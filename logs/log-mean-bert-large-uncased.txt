2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : layer 0
2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : ********************************************************************************
2019-03-12 11:26:54,154 : ***** Transfer task : STS12 *****


2019-03-12 11:26:54,192 : loading BERT model bert-large-uncased
2019-03-12 11:26:54,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:26:54,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:26:54,212 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgfknkd3o
2019-03-12 11:27:01,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:27:19,371 : MSRpar : pearson = 0.3694, spearman = 0.4125
2019-03-12 11:27:22,929 : MSRvid : pearson = 0.6101, spearman = 0.6140
2019-03-12 11:27:25,791 : SMTeuroparl : pearson = 0.4900, spearman = 0.5962
2019-03-12 11:27:30,838 : surprise.OnWN : pearson = 0.6480, spearman = 0.6743
2019-03-12 11:27:33,755 : surprise.SMTnews : pearson = 0.5107, spearman = 0.4448
2019-03-12 11:27:33,755 : ALL (weighted average) : Pearson = 0.5307,             Spearman = 0.5556
2019-03-12 11:27:33,755 : ALL (average) : Pearson = 0.5256,             Spearman = 0.5483

2019-03-12 11:27:33,755 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 11:27:33,765 : loading BERT model bert-large-uncased
2019-03-12 11:27:33,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:27:33,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:27:33,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvgo6l1eu
2019-03-12 11:27:41,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:27:49,783 : FNWN : pearson = 0.3613, spearman = 0.3538
2019-03-12 11:27:53,872 : headlines : pearson = 0.6704, spearman = 0.6550
2019-03-12 11:27:56,934 : OnWN : pearson = 0.4572, spearman = 0.5039
2019-03-12 11:27:56,934 : ALL (weighted average) : Pearson = 0.5517,             Spearman = 0.5605
2019-03-12 11:27:56,934 : ALL (average) : Pearson = 0.4963,             Spearman = 0.5042

2019-03-12 11:27:56,935 : ***** Transfer task : STS14 *****


2019-03-12 11:27:57,014 : loading BERT model bert-large-uncased
2019-03-12 11:27:57,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:27:57,034 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:27:57,035 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnu1drpv1
2019-03-12 11:28:04,501 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:28:12,775 : deft-forum : pearson = 0.3532, spearman = 0.3665
2019-03-12 11:28:15,847 : deft-news : pearson = 0.7074, spearman = 0.6912
2019-03-12 11:28:20,416 : headlines : pearson = 0.6368, spearman = 0.6020
2019-03-12 11:28:24,880 : images : pearson = 0.5992, spearman = 0.6007
2019-03-12 11:28:29,229 : OnWN : pearson = 0.5840, spearman = 0.6409
2019-03-12 11:28:34,701 : tweet-news : pearson = 0.6065, spearman = 0.5892
2019-03-12 11:28:34,701 : ALL (weighted average) : Pearson = 0.5843,             Spearman = 0.5859
2019-03-12 11:28:34,702 : ALL (average) : Pearson = 0.5812,             Spearman = 0.5818

2019-03-12 11:28:34,702 : ***** Transfer task : STS15 *****


2019-03-12 11:28:34,741 : loading BERT model bert-large-uncased
2019-03-12 11:28:34,741 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:28:34,761 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:28:34,761 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaz3esln1
2019-03-12 11:28:42,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:28:51,314 : answers-forums : pearson = 0.4448, spearman = 0.4370
2019-03-12 11:28:55,619 : answers-students : pearson = 0.6985, spearman = 0.7112
2019-03-12 11:28:59,377 : belief : pearson = 0.5352, spearman = 0.5384
2019-03-12 11:29:03,848 : headlines : pearson = 0.6888, spearman = 0.6843
2019-03-12 11:29:08,196 : images : pearson = 0.7030, spearman = 0.7161
2019-03-12 11:29:08,197 : ALL (weighted average) : Pearson = 0.6451,             Spearman = 0.6499
2019-03-12 11:29:08,197 : ALL (average) : Pearson = 0.6141,             Spearman = 0.6174

2019-03-12 11:29:08,197 : ***** Transfer task : STS16 *****


2019-03-12 11:29:08,303 : loading BERT model bert-large-uncased
2019-03-12 11:29:08,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:29:08,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:29:08,325 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzskzcy4d
2019-03-12 11:29:15,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:29:22,813 : answer-answer : pearson = 0.4594, spearman = 0.4956
2019-03-12 11:29:24,125 : headlines : pearson = 0.6971, spearman = 0.7019
2019-03-12 11:29:25,999 : plagiarism : pearson = 0.6689, spearman = 0.6723
2019-03-12 11:29:28,914 : postediting : pearson = 0.7533, spearman = 0.7875
2019-03-12 11:29:30,209 : question-question : pearson = 0.4889, spearman = 0.4852
2019-03-12 11:29:30,209 : ALL (weighted average) : Pearson = 0.6156,             Spearman = 0.6314
2019-03-12 11:29:30,209 : ALL (average) : Pearson = 0.6135,             Spearman = 0.6285

2019-03-12 11:29:30,209 : ***** Transfer task : MR *****


2019-03-12 11:29:30,263 : loading BERT model bert-large-uncased
2019-03-12 11:29:30,263 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:29:30,323 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:29:30,323 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxpy9rg6e
2019-03-12 11:29:37,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:29:43,158 : Generating sentence embeddings
2019-03-12 11:30:42,738 : Generated sentence embeddings
2019-03-12 11:30:42,739 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:31:05,280 : Best param found at split 1: l2reg = 1e-05                 with score 75.09
2019-03-12 11:31:27,792 : Best param found at split 2: l2reg = 0.001                 with score 74.77
2019-03-12 11:31:51,290 : Best param found at split 3: l2reg = 0.0001                 with score 75.65
2019-03-12 11:32:11,451 : Best param found at split 4: l2reg = 0.001                 with score 74.53
2019-03-12 11:32:56,268 : Best param found at split 5: l2reg = 0.0001                 with score 74.79
2019-03-12 11:32:59,320 : Dev acc : 74.97 Test acc : 74.26

2019-03-12 11:32:59,321 : ***** Transfer task : CR *****


2019-03-12 11:32:59,348 : loading BERT model bert-large-uncased
2019-03-12 11:32:59,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:59,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:59,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0_zd7208
2019-03-12 11:33:06,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:33:12,473 : Generating sentence embeddings
2019-03-12 11:33:30,414 : Generated sentence embeddings
2019-03-12 11:33:30,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:33:47,695 : Best param found at split 1: l2reg = 0.001                 with score 79.0
2019-03-12 11:34:06,247 : Best param found at split 2: l2reg = 1e-05                 with score 78.54
2019-03-12 11:34:26,121 : Best param found at split 3: l2reg = 1e-05                 with score 79.4
2019-03-12 11:34:46,363 : Best param found at split 4: l2reg = 1e-05                 with score 79.58
2019-03-12 11:35:05,217 : Best param found at split 5: l2reg = 1e-05                 with score 80.44
2019-03-12 11:35:06,324 : Dev acc : 79.39 Test acc : 76.82

2019-03-12 11:35:06,325 : ***** Transfer task : MPQA *****


2019-03-12 11:35:06,362 : loading BERT model bert-large-uncased
2019-03-12 11:35:06,362 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:35:06,380 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:35:06,380 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9hxrtlb3
2019-03-12 11:35:13,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:35:19,414 : Generating sentence embeddings
2019-03-12 11:35:44,626 : Generated sentence embeddings
2019-03-12 11:35:44,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:36:35,272 : Best param found at split 1: l2reg = 1e-05                 with score 87.29
2019-03-12 11:37:31,438 : Best param found at split 2: l2reg = 1e-05                 with score 87.38
2019-03-12 11:38:30,785 : Best param found at split 3: l2reg = 0.001                 with score 88.12
2019-03-12 11:38:44,943 : Best param found at split 4: l2reg = 0.001                 with score 88.32
2019-03-12 11:39:05,664 : Best param found at split 5: l2reg = 0.01                 with score 87.78
2019-03-12 11:39:06,858 : Dev acc : 87.78 Test acc : 87.84

2019-03-12 11:39:06,859 : ***** Transfer task : SUBJ *****


2019-03-12 11:39:06,877 : loading BERT model bert-large-uncased
2019-03-12 11:39:06,877 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:39:06,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:39:06,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptrulp4xp
2019-03-12 11:39:14,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:39:20,034 : Generating sentence embeddings
2019-03-12 11:40:14,583 : Generated sentence embeddings
2019-03-12 11:40:14,584 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:40:30,634 : Best param found at split 1: l2reg = 0.0001                 with score 91.6
2019-03-12 11:40:49,175 : Best param found at split 2: l2reg = 0.001                 with score 91.79
2019-03-12 11:41:06,645 : Best param found at split 3: l2reg = 0.001                 with score 91.52
2019-03-12 11:41:23,855 : Best param found at split 4: l2reg = 0.0001                 with score 91.69
2019-03-12 11:41:41,051 : Best param found at split 5: l2reg = 0.0001                 with score 91.11
2019-03-12 11:41:42,152 : Dev acc : 91.54 Test acc : 91.08

2019-03-12 11:41:42,153 : ***** Transfer task : SST Binary classification *****


2019-03-12 11:41:42,295 : loading BERT model bert-large-uncased
2019-03-12 11:41:42,295 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:41:42,317 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:41:42,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_135p650
2019-03-12 11:41:49,841 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:41:55,236 : Computing embedding for train
2019-03-12 11:44:55,249 : Computed train embeddings
2019-03-12 11:44:55,249 : Computing embedding for dev
2019-03-12 11:44:58,940 : Computed dev embeddings
2019-03-12 11:44:58,941 : Computing embedding for test
2019-03-12 11:45:07,178 : Computed test embeddings
2019-03-12 11:45:07,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:45:47,929 : [('reg:1e-05', 78.78), ('reg:0.0001', 78.78), ('reg:0.001', 77.98), ('reg:0.01', 78.1)]
2019-03-12 11:45:47,930 : Validation : best param found is reg = 1e-05 with score             78.78
2019-03-12 11:45:47,930 : Evaluating...
2019-03-12 11:45:58,225 : 
Dev acc : 78.78 Test acc : 79.68 for             SST Binary classification

2019-03-12 11:45:58,226 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 11:45:58,280 : loading BERT model bert-large-uncased
2019-03-12 11:45:58,280 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:45:58,303 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:45:58,303 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp054mlhv9
2019-03-12 11:46:05,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:46:11,047 : Computing embedding for train
2019-03-12 11:46:49,121 : Computed train embeddings
2019-03-12 11:46:49,121 : Computing embedding for dev
2019-03-12 11:46:53,969 : Computed dev embeddings
2019-03-12 11:46:53,969 : Computing embedding for test
2019-03-12 11:47:03,774 : Computed test embeddings
2019-03-12 11:47:03,774 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:47:07,887 : [('reg:1e-05', 38.15), ('reg:0.0001', 38.33), ('reg:0.001', 38.33), ('reg:0.01', 37.33)]
2019-03-12 11:47:07,887 : Validation : best param found is reg = 0.0001 with score             38.33
2019-03-12 11:47:07,887 : Evaluating...
2019-03-12 11:47:08,831 : 
Dev acc : 38.33 Test acc : 42.81 for             SST Fine-Grained classification

2019-03-12 11:47:08,831 : ***** Transfer task : TREC *****


2019-03-12 11:47:08,845 : loading BERT model bert-large-uncased
2019-03-12 11:47:08,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:47:08,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:47:08,868 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb2z9obk8
2019-03-12 11:47:16,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:47:34,951 : Computed train embeddings
2019-03-12 11:47:36,126 : Computed test embeddings
2019-03-12 11:47:36,127 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:47:48,828 : [('reg:1e-05', 76.8), ('reg:0.0001', 76.67), ('reg:0.001', 76.67), ('reg:0.01', 63.21)]
2019-03-12 11:47:48,828 : Cross-validation : best param found is reg = 1e-05             with score 76.8
2019-03-12 11:47:48,828 : Evaluating...
2019-03-12 11:47:49,825 : 
Dev acc : 76.8 Test acc : 82.8             for TREC

2019-03-12 11:47:49,826 : ***** Transfer task : MRPC *****


2019-03-12 11:47:49,849 : loading BERT model bert-large-uncased
2019-03-12 11:47:49,849 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:47:49,869 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:47:49,869 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxzp1x167
2019-03-12 11:47:57,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:48:02,971 : Computing embedding for train
2019-03-12 11:48:42,479 : Computed train embeddings
2019-03-12 11:48:42,479 : Computing embedding for test
2019-03-12 11:49:00,213 : Computed test embeddings
2019-03-12 11:49:00,235 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:49:10,181 : [('reg:1e-05', 74.48), ('reg:0.0001', 73.87), ('reg:0.001', 73.95), ('reg:0.01', 72.62)]
2019-03-12 11:49:10,181 : Cross-validation : best param found is reg = 1e-05             with score 74.48
2019-03-12 11:49:10,181 : Evaluating...
2019-03-12 11:49:10,635 : Dev acc : 74.48 Test acc 70.49; Test F1 76.64 for MRPC.

2019-03-12 11:49:10,636 : ***** Transfer task : SICK-Entailment*****


2019-03-12 11:49:10,660 : loading BERT model bert-large-uncased
2019-03-12 11:49:10,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:49:10,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:49:10,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn206t26w
2019-03-12 11:49:18,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:49:23,860 : Computing embedding for train
2019-03-12 11:49:45,474 : Computed train embeddings
2019-03-12 11:49:45,474 : Computing embedding for dev
2019-03-12 11:49:48,471 : Computed dev embeddings
2019-03-12 11:49:48,471 : Computing embedding for test
2019-03-12 11:50:11,854 : Computed test embeddings
2019-03-12 11:50:11,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:50:15,005 : [('reg:1e-05', 82.8), ('reg:0.0001', 83.2), ('reg:0.001', 82.6), ('reg:0.01', 75.8)]
2019-03-12 11:50:15,005 : Validation : best param found is reg = 0.0001 with score             83.2
2019-03-12 11:50:15,005 : Evaluating...
2019-03-12 11:50:15,538 : 
Dev acc : 83.2 Test acc : 81.51 for                        SICK entailment

2019-03-12 11:50:15,539 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 11:50:15,565 : loading BERT model bert-large-uncased
2019-03-12 11:50:15,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:50:15,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:50:15,584 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_bou7x_5
2019-03-12 11:50:23,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:50:28,410 : Computing embedding for train
2019-03-12 11:50:49,732 : Computed train embeddings
2019-03-12 11:50:49,732 : Computing embedding for dev
2019-03-12 11:50:52,458 : Computed dev embeddings
2019-03-12 11:50:52,458 : Computing embedding for test
2019-03-12 11:51:14,138 : Computed test embeddings
2019-03-12 11:51:45,445 : Dev : Pearson 0.8267945113916719
2019-03-12 11:51:45,445 : Test : Pearson 0.8326844192710106 Spearman 0.7623653843272574 MSE 0.3131011466402972                        for SICK Relatedness

2019-03-12 11:51:45,446 : 

***** Transfer task : STSBenchmark*****


2019-03-12 11:51:45,480 : loading BERT model bert-large-uncased
2019-03-12 11:51:45,480 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:51:45,509 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:51:45,509 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp60v72li4
2019-03-12 11:51:53,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:51:58,427 : Computing embedding for train
2019-03-12 11:52:31,583 : Computed train embeddings
2019-03-12 11:52:31,583 : Computing embedding for dev
2019-03-12 11:52:41,441 : Computed dev embeddings
2019-03-12 11:52:41,441 : Computing embedding for test
2019-03-12 11:52:49,026 : Computed test embeddings
2019-03-12 11:53:20,254 : Dev : Pearson 0.752286296427256
2019-03-12 11:53:20,254 : Test : Pearson 0.71805280364465 Spearman 0.7104742010387665 MSE 1.3396487399759156                        for SICK Relatedness

2019-03-12 11:53:20,254 : ***** Transfer task : SNLI Entailment*****


2019-03-12 11:53:25,103 : loading BERT model bert-large-uncased
2019-03-12 11:53:25,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:53:25,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:53:25,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpny8r4u0l
2019-03-12 11:53:32,718 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:53:38,815 : PROGRESS (encoding): 0.00%
2019-03-12 11:58:53,519 : PROGRESS (encoding): 14.56%
2019-03-12 12:04:51,370 : PROGRESS (encoding): 29.12%
2019-03-12 12:10:48,007 : PROGRESS (encoding): 43.69%
2019-03-12 12:17:02,155 : PROGRESS (encoding): 58.25%
2019-03-12 12:23:56,301 : PROGRESS (encoding): 72.81%
2019-03-12 12:30:48,266 : PROGRESS (encoding): 87.37%
2019-03-12 12:37:57,973 : PROGRESS (encoding): 0.00%
2019-03-12 12:38:53,790 : PROGRESS (encoding): 0.00%
2019-03-12 12:39:45,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:40:41,596 : [('reg:1e-09', 67.05)]
2019-03-12 12:40:41,596 : Validation : best param found is reg = 1e-09 with score             67.05
2019-03-12 12:40:41,596 : Evaluating...
2019-03-12 12:41:39,343 : Dev acc : 67.05 Test acc : 66.98 for SNLI

2019-03-12 12:41:39,343 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 12:41:39,557 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 12:41:40,561 : loading BERT model bert-large-uncased
2019-03-12 12:41:40,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:41:40,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:41:40,590 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp28bfd84r
2019-03-12 12:41:48,126 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:41:53,568 : Computing embeddings for train/dev/test
2019-03-12 12:48:20,087 : Computed embeddings
2019-03-12 12:48:20,087 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:49:13,725 : [('reg:1e-05', 96.58), ('reg:0.0001', 93.78), ('reg:0.001', 85.8), ('reg:0.01', 82.08)]
2019-03-12 12:49:13,725 : Validation : best param found is reg = 1e-05 with score             96.58
2019-03-12 12:49:13,726 : Evaluating...
2019-03-12 12:49:26,853 : 
Dev acc : 96.6 Test acc : 97.2 for LENGTH classification

2019-03-12 12:49:26,854 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 12:49:27,222 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 12:49:27,273 : loading BERT model bert-large-uncased
2019-03-12 12:49:27,274 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:49:27,304 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:49:27,304 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjspy41rb
2019-03-12 12:49:34,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:49:40,278 : Computing embeddings for train/dev/test
2019-03-12 12:55:37,608 : Computed embeddings
2019-03-12 12:55:37,608 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:56:34,867 : [('reg:1e-05', 76.01), ('reg:0.0001', 22.75), ('reg:0.001', 1.53), ('reg:0.01', 0.59)]
2019-03-12 12:56:34,868 : Validation : best param found is reg = 1e-05 with score             76.01
2019-03-12 12:56:34,868 : Evaluating...
2019-03-12 12:56:55,231 : 
Dev acc : 76.0 Test acc : 75.9 for WORDCONTENT classification

2019-03-12 12:56:55,232 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 12:56:55,599 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 12:56:55,669 : loading BERT model bert-large-uncased
2019-03-12 12:56:55,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:56:55,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:56:55,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_gpngbdk
2019-03-12 12:57:03,171 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:57:08,824 : Computing embeddings for train/dev/test
2019-03-12 13:02:46,654 : Computed embeddings
2019-03-12 13:02:46,654 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:03:22,126 : [('reg:1e-05', 31.99), ('reg:0.0001', 31.78), ('reg:0.001', 30.87), ('reg:0.01', 27.42)]
2019-03-12 13:03:22,127 : Validation : best param found is reg = 1e-05 with score             31.99
2019-03-12 13:03:22,127 : Evaluating...
2019-03-12 13:03:31,947 : 
Dev acc : 32.0 Test acc : 31.9 for DEPTH classification

2019-03-12 13:03:31,948 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 13:03:32,333 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 13:03:32,400 : loading BERT model bert-large-uncased
2019-03-12 13:03:32,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:03:32,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:03:32,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp24pblf4l
2019-03-12 13:03:40,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:03:45,324 : Computing embeddings for train/dev/test
2019-03-12 13:09:02,539 : Computed embeddings
2019-03-12 13:09:02,539 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:09:55,036 : [('reg:1e-05', 52.78), ('reg:0.0001', 52.27), ('reg:0.001', 46.06), ('reg:0.01', 33.77)]
2019-03-12 13:09:55,036 : Validation : best param found is reg = 1e-05 with score             52.78
2019-03-12 13:09:55,036 : Evaluating...
2019-03-12 13:10:07,414 : 
Dev acc : 52.8 Test acc : 51.8 for TOPCONSTITUENTS classification

2019-03-12 13:10:07,415 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 13:10:07,762 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 13:10:07,827 : loading BERT model bert-large-uncased
2019-03-12 13:10:07,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:10:07,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:10:07,948 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv6tlwyng
2019-03-12 13:10:15,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:10:20,678 : Computing embeddings for train/dev/test
2019-03-12 13:17:45,297 : Computed embeddings
2019-03-12 13:17:45,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:19:34,311 : [('reg:1e-05', 50.06), ('reg:0.0001', 50.1), ('reg:0.001', 50.06), ('reg:0.01', 50.05)]
2019-03-12 13:19:34,311 : Validation : best param found is reg = 0.0001 with score             50.1
2019-03-12 13:19:34,311 : Evaluating...
2019-03-12 13:19:44,383 : 
Dev acc : 50.1 Test acc : 50.1 for BIGRAMSHIFT classification

2019-03-12 13:19:44,384 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 13:19:44,941 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 13:19:45,033 : loading BERT model bert-large-uncased
2019-03-12 13:19:45,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:19:45,064 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:19:45,064 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoi_450vg
2019-03-12 13:19:52,537 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:19:57,840 : Computing embeddings for train/dev/test
2019-03-12 13:25:33,272 : Computed embeddings
2019-03-12 13:25:33,272 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:26:24,487 : [('reg:1e-05', 84.35), ('reg:0.0001', 84.8), ('reg:0.001', 85.62), ('reg:0.01', 85.18)]
2019-03-12 13:26:24,487 : Validation : best param found is reg = 0.001 with score             85.62
2019-03-12 13:26:24,487 : Evaluating...
2019-03-12 13:26:37,508 : 
Dev acc : 85.6 Test acc : 84.5 for TENSE classification

2019-03-12 13:26:37,509 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 13:26:37,963 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 13:26:38,027 : loading BERT model bert-large-uncased
2019-03-12 13:26:38,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:26:38,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:26:38,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr_dqjhsi
2019-03-12 13:26:45,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:26:50,903 : Computing embeddings for train/dev/test
2019-03-12 13:32:43,415 : Computed embeddings
2019-03-12 13:32:43,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:33:30,535 : [('reg:1e-05', 79.12), ('reg:0.0001', 79.58), ('reg:0.001', 81.15), ('reg:0.01', 79.32)]
2019-03-12 13:33:30,535 : Validation : best param found is reg = 0.001 with score             81.15
2019-03-12 13:33:30,535 : Evaluating...
2019-03-12 13:33:45,070 : 
Dev acc : 81.2 Test acc : 79.8 for SUBJNUMBER classification

2019-03-12 13:33:45,071 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 13:33:45,491 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 13:33:45,561 : loading BERT model bert-large-uncased
2019-03-12 13:33:45,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:33:45,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:33:45,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcg7p_b5j
2019-03-12 13:33:53,120 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:33:58,460 : Computing embeddings for train/dev/test
2019-03-12 13:39:43,818 : Computed embeddings
2019-03-12 13:39:43,818 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:40:25,400 : [('reg:1e-05', 80.52), ('reg:0.0001', 80.55), ('reg:0.001', 79.8), ('reg:0.01', 80.35)]
2019-03-12 13:40:25,400 : Validation : best param found is reg = 0.0001 with score             80.55
2019-03-12 13:40:25,400 : Evaluating...
2019-03-12 13:40:35,944 : 
Dev acc : 80.5 Test acc : 81.6 for OBJNUMBER classification

2019-03-12 13:40:35,945 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 13:40:36,564 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 13:40:36,634 : loading BERT model bert-large-uncased
2019-03-12 13:40:36,634 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:40:36,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:40:36,662 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5pv0z1vh
2019-03-12 13:40:44,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:40:49,858 : Computing embeddings for train/dev/test
2019-03-12 13:47:36,024 : Computed embeddings
2019-03-12 13:47:36,024 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:48:21,641 : [('reg:1e-05', 50.2), ('reg:0.0001', 50.18), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-03-12 13:48:21,641 : Validation : best param found is reg = 1e-05 with score             50.2
2019-03-12 13:48:21,641 : Evaluating...
2019-03-12 13:48:31,723 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-03-12 13:48:31,724 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 13:48:32,118 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 13:48:32,198 : loading BERT model bert-large-uncased
2019-03-12 13:48:32,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:48:32,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:48:32,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyuj39mjt
2019-03-12 13:48:39,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:48:45,321 : Computing embeddings for train/dev/test
2019-03-12 13:55:27,835 : Computed embeddings
2019-03-12 13:55:27,835 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:56:13,400 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.01), ('reg:0.01', 50.0)]
2019-03-12 13:56:13,400 : Validation : best param found is reg = 0.001 with score             50.01
2019-03-12 13:56:13,400 : Evaluating...
2019-03-12 13:56:24,918 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-03-12 13:56:24,920 : total results: {'STS12': {'MSRpar': {'pearson': (0.36940635786993514, 1.1566190979545216e-25), 'spearman': SpearmanrResult(correlation=0.4124861726736546, pvalue=3.5769596030354996e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6100679691503511, 1.169720452586486e-77), 'spearman': SpearmanrResult(correlation=0.6139616077417059, pvalue=6.723787914683201e-79), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4899577582099963, 4.3340456180245233e-29), 'spearman': SpearmanrResult(correlation=0.5961611102517371, pvalue=1.6454048349408732e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6480134746665399, 1.5722648544846239e-90), 'spearman': SpearmanrResult(correlation=0.6743053395271504, pvalue=1.3824146655088163e-100), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5106861414094586, 6.923002289218144e-28), 'spearman': SpearmanrResult(correlation=0.44481785906302157, pvalue=8.772584781885766e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5256263402612562, 'wmean': 0.5306532280263455}, 'spearman': {'mean': 0.5483464178514539, 'wmean': 0.5555614914184608}}}, 'STS13': {'FNWN': {'pearson': (0.36131409797686026, 3.2573763711194235e-07), 'spearman': SpearmanrResult(correlation=0.3538386606714223, pvalue=5.891420478532654e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6704498640617633, 4.7876158621611155e-99), 'spearman': SpearmanrResult(correlation=0.6550147486404417, pvalue=4.141162967628425e-93), 'nsamples': 750}, 'OnWN': {'pearson': (0.4571773566735294, 2.5333282668553097e-30), 'spearman': SpearmanrResult(correlation=0.503886451288883, pvalue=1.8620052022384365e-37), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.496313772904051, 'wmean': 0.5517348397718661}, 'spearman': {'mean': 0.504246620200249, 'wmean': 0.5605445783468623}}}, 'STS14': {'deft-forum': {'pearson': (0.35323940830219563, 1.13746540308215e-14), 'spearman': SpearmanrResult(correlation=0.3665478752136955, pvalue=9.327699846097083e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7074126947234468, 8.014291417463316e-47), 'spearman': SpearmanrResult(correlation=0.6911790773068247, pvalue=6.163950098977837e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6368288911566509, 1.5094800293396722e-86), 'spearman': SpearmanrResult(correlation=0.6020120327421548, pvalue=3.807007689813852e-75), 'nsamples': 750}, 'images': {'pearson': (0.5992460014455616, 2.6720530050978777e-74), 'spearman': SpearmanrResult(correlation=0.6007340824826948, pvalue=9.388528412513143e-75), 'nsamples': 750}, 'OnWN': {'pearson': (0.5839968274579665, 8.862928583193181e-70), 'spearman': SpearmanrResult(correlation=0.6409261468430026, pvalue=5.48583982093473e-88), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6064892586434757, 1.5599245133160332e-76), 'spearman': SpearmanrResult(correlation=0.5891852774553988, pvalue=2.7314230137404317e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5812021802882162, 'wmean': 0.5842939403148701}, 'spearman': {'mean': 0.5817640820072951, 'wmean': 0.5858515791148396}}}, 'STS15': {'answers-forums': {'pearson': (0.4448467880584809, 1.2669883457203613e-19), 'spearman': SpearmanrResult(correlation=0.4370372918227552, pvalue=6.349522001496648e-19), 'nsamples': 375}, 'answers-students': {'pearson': (0.6984818690325181, 8.43508068997302e-111), 'spearman': SpearmanrResult(correlation=0.7111938794922954, pvalue=1.3582841676397428e-116), 'nsamples': 750}, 'belief': {'pearson': (0.5351730333229254, 3.5697186251126644e-29), 'spearman': SpearmanrResult(correlation=0.5384412896101894, pvalue=1.415088676803574e-29), 'nsamples': 375}, 'headlines': {'pearson': (0.6887826999828527, 1.394917371487843e-106), 'spearman': SpearmanrResult(correlation=0.6843487602483843, pvalue=1.0425393045498995e-104), 'nsamples': 750}, 'images': {'pearson': (0.7029807672883019, 8.150491979256406e-113), 'spearman': SpearmanrResult(correlation=0.7161294063794299, pvalue=6.281284811224705e-119), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6140530315370157, 'wmean': 0.6450638117485941}, 'spearman': {'mean': 0.6174301255106108, 'wmean': 0.6498528342091455}}}, 'STS16': {'answer-answer': {'pearson': (0.45944664736070623, 1.1422912927170433e-14), 'spearman': SpearmanrResult(correlation=0.4956277785936782, pvalue=3.758039314965771e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6970802859069369, 1.4867535054254284e-37), 'spearman': SpearmanrResult(correlation=0.7019411751251479, pvalue=2.8511503374821034e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6688827182144514, 3.3831274821194444e-31), 'spearman': SpearmanrResult(correlation=0.6723414892355442, pvalue=1.287473615021012e-31), 'nsamples': 230}, 'postediting': {'pearson': (0.7533213258782716, 6.115400157939951e-46), 'spearman': SpearmanrResult(correlation=0.7875161029131764, pvalue=8.728411280004414e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.48893989681605515, 5.853688913071666e-14), 'spearman': SpearmanrResult(correlation=0.4851748737959791, pvalue=9.700353263866506e-14), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6135341748352843, 'wmean': 0.6156107139617406}, 'spearman': {'mean': 0.6285202839327052, 'wmean': 0.6314222838341534}}}, 'MR': {'devacc': 74.97, 'acc': 74.26, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.39, 'acc': 76.82, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.78, 'acc': 87.84, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.54, 'acc': 91.08, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.78, 'acc': 79.68, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.33, 'acc': 42.81, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.8, 'acc': 82.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.48, 'acc': 70.49, 'f1': 76.64, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 83.2, 'acc': 81.51, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8267945113916719, 'pearson': 0.8326844192710106, 'spearman': 0.7623653843272574, 'mse': 0.3131011466402972, 'yhat': array([3.64422856, 4.09021284, 1.25474513, ..., 3.26807373, 4.38476189,        4.47073013]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.752286296427256, 'pearson': 0.71805280364465, 'spearman': 0.7104742010387665, 'mse': 1.3396487399759156, 'yhat': array([1.45593862, 1.69853937, 2.1598454 , ..., 3.94225722, 3.83652108,        3.17432962]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.05, 'acc': 66.98, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 96.58, 'acc': 97.22, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 76.01, 'acc': 75.92, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.99, 'acc': 31.95, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.78, 'acc': 51.76, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.1, 'acc': 50.1, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.62, 'acc': 84.5, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.15, 'acc': 79.85, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.55, 'acc': 81.64, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.2, 'acc': 49.88, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.01, 'acc': 50.02, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 13:56:24,920 : STS12 p=0.5307, STS12 s=0.5556, STS13 p=0.5517, STS13 s=0.5605, STS14 p=0.5843, STS14 s=0.5859, STS15 p=0.6451, STS15 s=0.6499, STS 16 p=0.6156, STS16 s=0.6314, STS B p=0.7181, STS B s=0.7105, STS B m=1.3396, SICK-R p=0.8327, SICK-R s=0.7624, SICK-P m=0.3131
2019-03-12 13:56:24,920 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 13:56:24,920 : 0.5307,0.5556,0.5517,0.5605,0.5843,0.5859,0.6451,0.6499,0.6156,0.6314,0.7181,0.7105,1.3396,0.8327,0.7624,0.3131
2019-03-12 13:56:24,920 : MR=74.26, CR=76.82, SUBJ=91.08, MPQA=87.84, SST-B=79.68, SST-F=42.81, TREC=82.80, SICK-E=81.51, SNLI=66.98, MRPC=70.49, MRPC f=76.64
2019-03-12 13:56:24,920 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 13:56:24,920 : 74.26,76.82,91.08,87.84,79.68,42.81,82.80,81.51,66.98,70.49,76.64
2019-03-12 13:56:24,920 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 13:56:24,920 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 13:56:24,921 : na,na,na,na,na,na,na,na,na,na
2019-03-12 13:56:24,921 : SentLen=97.22, WC=75.92, TreeDepth=31.95, TopConst=51.76, BShift=50.10, Tense=84.50, SubjNum=79.85, ObjNum=81.64, SOMO=49.88, CoordInv=50.02, average=65.28
2019-03-12 13:56:24,921 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 13:56:24,921 : 97.22,75.92,31.95,51.76,50.10,84.50,79.85,81.64,49.88,50.02,65.28
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:24,921 : layer 1
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:24,921 : ********************************************************************************
2019-03-12 13:56:25,012 : ***** Transfer task : STS12 *****


2019-03-12 13:56:25,024 : loading BERT model bert-large-uncased
2019-03-12 13:56:25,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:56:25,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:56:25,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp13f8t0yd
2019-03-12 13:56:32,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:56:44,477 : MSRpar : pearson = 0.3954, spearman = 0.4378
2019-03-12 13:56:47,612 : MSRvid : pearson = 0.7040, spearman = 0.7026
2019-03-12 13:56:49,755 : SMTeuroparl : pearson = 0.5055, spearman = 0.6023
2019-03-12 13:56:54,257 : surprise.OnWN : pearson = 0.6901, spearman = 0.6895
2019-03-12 13:56:56,876 : surprise.SMTnews : pearson = 0.5335, spearman = 0.4716
2019-03-12 13:56:56,876 : ALL (weighted average) : Pearson = 0.5750,             Spearman = 0.5911
2019-03-12 13:56:56,876 : ALL (average) : Pearson = 0.5657,             Spearman = 0.5808

2019-03-12 13:56:56,876 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 13:56:56,884 : loading BERT model bert-large-uncased
2019-03-12 13:56:56,884 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:56:56,902 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:56:56,902 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw1m3m25i
2019-03-12 13:57:04,338 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:57:11,934 : FNWN : pearson = 0.4326, spearman = 0.4425
2019-03-12 13:57:15,646 : headlines : pearson = 0.7005, spearman = 0.6837
2019-03-12 13:57:18,632 : OnWN : pearson = 0.5950, spearman = 0.6153
2019-03-12 13:57:18,632 : ALL (weighted average) : Pearson = 0.6273,             Spearman = 0.6277
2019-03-12 13:57:18,632 : ALL (average) : Pearson = 0.5760,             Spearman = 0.5805

2019-03-12 13:57:18,632 : ***** Transfer task : STS14 *****


2019-03-12 13:57:18,650 : loading BERT model bert-large-uncased
2019-03-12 13:57:18,650 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:57:18,669 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:57:18,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7e4fghpc
2019-03-12 13:57:26,125 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:57:34,307 : deft-forum : pearson = 0.3850, spearman = 0.3884
2019-03-12 13:57:37,161 : deft-news : pearson = 0.7417, spearman = 0.6965
2019-03-12 13:57:41,275 : headlines : pearson = 0.6596, spearman = 0.6175
2019-03-12 13:57:45,359 : images : pearson = 0.7092, spearman = 0.6873
2019-03-12 13:57:49,365 : OnWN : pearson = 0.6887, spearman = 0.7257
2019-03-12 13:57:54,729 : tweet-news : pearson = 0.6609, spearman = 0.6417
2019-03-12 13:57:54,730 : ALL (weighted average) : Pearson = 0.6492,             Spearman = 0.6368
2019-03-12 13:57:54,730 : ALL (average) : Pearson = 0.6409,             Spearman = 0.6262

2019-03-12 13:57:54,730 : ***** Transfer task : STS15 *****


2019-03-12 13:57:54,783 : loading BERT model bert-large-uncased
2019-03-12 13:57:54,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:57:54,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:57:54,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf25brpxs
2019-03-12 13:58:02,307 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:58:11,403 : answers-forums : pearson = 0.5446, spearman = 0.5356
2019-03-12 13:58:15,395 : answers-students : pearson = 0.7272, spearman = 0.7356
2019-03-12 13:58:19,184 : belief : pearson = 0.6428, spearman = 0.6465
2019-03-12 13:58:23,295 : headlines : pearson = 0.7187, spearman = 0.7108
2019-03-12 13:58:27,472 : images : pearson = 0.7786, spearman = 0.7872
2019-03-12 13:58:27,472 : ALL (weighted average) : Pearson = 0.7046,             Spearman = 0.7061
2019-03-12 13:58:27,472 : ALL (average) : Pearson = 0.6824,             Spearman = 0.6831

2019-03-12 13:58:27,472 : ***** Transfer task : STS16 *****


2019-03-12 13:58:27,547 : loading BERT model bert-large-uncased
2019-03-12 13:58:27,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:58:27,565 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:58:27,565 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmai157x1
2019-03-12 13:58:35,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:58:42,111 : answer-answer : pearson = 0.5265, spearman = 0.5385
2019-03-12 13:58:43,333 : headlines : pearson = 0.7114, spearman = 0.7174
2019-03-12 13:58:45,025 : plagiarism : pearson = 0.7714, spearman = 0.7726
2019-03-12 13:58:47,749 : postediting : pearson = 0.8211, spearman = 0.8271
2019-03-12 13:58:48,898 : question-question : pearson = 0.5436, spearman = 0.5395
2019-03-12 13:58:48,898 : ALL (weighted average) : Pearson = 0.6765,             Spearman = 0.6810
2019-03-12 13:58:48,899 : ALL (average) : Pearson = 0.6748,             Spearman = 0.6790

2019-03-12 13:58:48,899 : ***** Transfer task : MR *****


2019-03-12 13:58:48,926 : loading BERT model bert-large-uncased
2019-03-12 13:58:48,926 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:58:48,951 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:58:48,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv7npjuie
2019-03-12 13:58:56,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:59:01,936 : Generating sentence embeddings
2019-03-12 13:59:57,515 : Generated sentence embeddings
2019-03-12 13:59:57,516 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:00:15,977 : Best param found at split 1: l2reg = 0.0001                 with score 74.98
2019-03-12 14:00:30,978 : Best param found at split 2: l2reg = 0.01                 with score 74.24
2019-03-12 14:00:47,740 : Best param found at split 3: l2reg = 0.0001                 with score 75.22
2019-03-12 14:01:03,776 : Best param found at split 4: l2reg = 1e-05                 with score 74.51
2019-03-12 14:01:22,143 : Best param found at split 5: l2reg = 0.001                 with score 73.99
2019-03-12 14:01:23,216 : Dev acc : 74.59 Test acc : 74.1

2019-03-12 14:01:23,217 : ***** Transfer task : CR *****


2019-03-12 14:01:23,225 : loading BERT model bert-large-uncased
2019-03-12 14:01:23,225 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:01:23,245 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:01:23,245 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2f4s_qy1
2019-03-12 14:01:30,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:01:36,420 : Generating sentence embeddings
2019-03-12 14:01:51,723 : Generated sentence embeddings
2019-03-12 14:01:51,723 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:01:58,628 : Best param found at split 1: l2reg = 0.001                 with score 79.66
2019-03-12 14:02:06,466 : Best param found at split 2: l2reg = 0.01                 with score 80.16
2019-03-12 14:02:13,464 : Best param found at split 3: l2reg = 0.0001                 with score 79.8
2019-03-12 14:02:20,057 : Best param found at split 4: l2reg = 1e-05                 with score 79.77
2019-03-12 14:02:25,434 : Best param found at split 5: l2reg = 0.0001                 with score 81.33
2019-03-12 14:02:25,548 : Dev acc : 80.14 Test acc : 78.86

2019-03-12 14:02:25,549 : ***** Transfer task : MPQA *****


2019-03-12 14:02:25,554 : loading BERT model bert-large-uncased
2019-03-12 14:02:25,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:02:25,605 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:02:25,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeygpiu5k
2019-03-12 14:02:33,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:02:38,470 : Generating sentence embeddings
2019-03-12 14:02:51,445 : Generated sentence embeddings
2019-03-12 14:02:51,446 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:03:08,935 : Best param found at split 1: l2reg = 1e-05                 with score 87.11
2019-03-12 14:03:27,433 : Best param found at split 2: l2reg = 0.001                 with score 86.33
2019-03-12 14:03:47,984 : Best param found at split 3: l2reg = 0.001                 with score 87.73
2019-03-12 14:04:04,679 : Best param found at split 4: l2reg = 0.01                 with score 87.73
2019-03-12 14:04:20,222 : Best param found at split 5: l2reg = 0.01                 with score 87.54
2019-03-12 14:04:21,281 : Dev acc : 87.29 Test acc : 87.49

2019-03-12 14:04:21,282 : ***** Transfer task : SUBJ *****


2019-03-12 14:04:21,297 : loading BERT model bert-large-uncased
2019-03-12 14:04:21,297 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:04:21,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:04:21,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsjkm3job
2019-03-12 14:04:28,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:04:34,194 : Generating sentence embeddings
2019-03-12 14:05:27,812 : Generated sentence embeddings
2019-03-12 14:05:27,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:05:44,525 : Best param found at split 1: l2reg = 0.001                 with score 92.05
2019-03-12 14:06:05,134 : Best param found at split 2: l2reg = 0.001                 with score 91.91
2019-03-12 14:06:24,274 : Best param found at split 3: l2reg = 0.001                 with score 91.66
2019-03-12 14:06:45,674 : Best param found at split 4: l2reg = 0.001                 with score 91.99
2019-03-12 14:07:05,743 : Best param found at split 5: l2reg = 0.001                 with score 91.48
2019-03-12 14:07:06,512 : Dev acc : 91.82 Test acc : 90.78

2019-03-12 14:07:06,513 : ***** Transfer task : SST Binary classification *****


2019-03-12 14:07:06,606 : loading BERT model bert-large-uncased
2019-03-12 14:07:06,606 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:07:06,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:07:06,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4w8jmt4x
2019-03-12 14:07:14,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:07:19,705 : Computing embedding for train
2019-03-12 14:10:27,213 : Computed train embeddings
2019-03-12 14:10:27,213 : Computing embedding for dev
2019-03-12 14:10:31,019 : Computed dev embeddings
2019-03-12 14:10:31,019 : Computing embedding for test
2019-03-12 14:10:38,814 : Computed test embeddings
2019-03-12 14:10:38,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:11:07,052 : [('reg:1e-05', 78.9), ('reg:0.0001', 79.13), ('reg:0.001', 79.93), ('reg:0.01', 77.75)]
2019-03-12 14:11:07,052 : Validation : best param found is reg = 0.001 with score             79.93
2019-03-12 14:11:07,053 : Evaluating...
2019-03-12 14:11:15,903 : 
Dev acc : 79.93 Test acc : 79.57 for             SST Binary classification

2019-03-12 14:11:15,904 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 14:11:15,955 : loading BERT model bert-large-uncased
2019-03-12 14:11:15,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:11:15,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:11:15,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfvqnwz9k
2019-03-12 14:11:23,501 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:11:28,842 : Computing embedding for train
2019-03-12 14:12:08,002 : Computed train embeddings
2019-03-12 14:12:08,002 : Computing embedding for dev
2019-03-12 14:12:12,892 : Computed dev embeddings
2019-03-12 14:12:12,893 : Computing embedding for test
2019-03-12 14:12:22,949 : Computed test embeddings
2019-03-12 14:12:22,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:12:28,649 : [('reg:1e-05', 39.06), ('reg:0.0001', 38.87), ('reg:0.001', 37.51), ('reg:0.01', 35.51)]
2019-03-12 14:12:28,649 : Validation : best param found is reg = 1e-05 with score             39.06
2019-03-12 14:12:28,649 : Evaluating...
2019-03-12 14:12:29,878 : 
Dev acc : 39.06 Test acc : 40.72 for             SST Fine-Grained classification

2019-03-12 14:12:29,879 : ***** Transfer task : TREC *****


2019-03-12 14:12:29,891 : loading BERT model bert-large-uncased
2019-03-12 14:12:29,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:12:29,910 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:12:29,910 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgloieav3
2019-03-12 14:12:37,364 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:12:57,490 : Computed train embeddings
2019-03-12 14:12:58,625 : Computed test embeddings
2019-03-12 14:12:58,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:13:12,713 : [('reg:1e-05', 77.97), ('reg:0.0001', 78.08), ('reg:0.001', 76.29), ('reg:0.01', 70.51)]
2019-03-12 14:13:12,713 : Cross-validation : best param found is reg = 0.0001             with score 78.08
2019-03-12 14:13:12,713 : Evaluating...
2019-03-12 14:13:13,586 : 
Dev acc : 78.08 Test acc : 87.0             for TREC

2019-03-12 14:13:13,587 : ***** Transfer task : MRPC *****


2019-03-12 14:13:13,610 : loading BERT model bert-large-uncased
2019-03-12 14:13:13,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:13:13,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:13:13,631 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp06he2zyt
2019-03-12 14:13:21,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:13:26,581 : Computing embedding for train
2019-03-12 14:14:05,873 : Computed train embeddings
2019-03-12 14:14:05,874 : Computing embedding for test
2019-03-12 14:14:23,725 : Computed test embeddings
2019-03-12 14:14:23,745 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:14:32,141 : [('reg:1e-05', 74.02), ('reg:0.0001', 73.77), ('reg:0.001', 73.65), ('reg:0.01', 71.81)]
2019-03-12 14:14:32,141 : Cross-validation : best param found is reg = 1e-05             with score 74.02
2019-03-12 14:14:32,141 : Evaluating...
2019-03-12 14:14:32,393 : Dev acc : 74.02 Test acc 73.16; Test F1 79.39 for MRPC.

2019-03-12 14:14:32,393 : ***** Transfer task : SICK-Entailment*****


2019-03-12 14:14:32,454 : loading BERT model bert-large-uncased
2019-03-12 14:14:32,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:14:32,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:14:32,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiswk0tgq
2019-03-12 14:14:39,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:14:45,408 : Computing embedding for train
2019-03-12 14:15:06,886 : Computed train embeddings
2019-03-12 14:15:06,886 : Computing embedding for dev
2019-03-12 14:15:09,818 : Computed dev embeddings
2019-03-12 14:15:09,818 : Computing embedding for test
2019-03-12 14:15:33,232 : Computed test embeddings
2019-03-12 14:15:33,269 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:15:35,932 : [('reg:1e-05', 81.2), ('reg:0.0001', 80.6), ('reg:0.001', 80.4), ('reg:0.01', 77.6)]
2019-03-12 14:15:35,933 : Validation : best param found is reg = 1e-05 with score             81.2
2019-03-12 14:15:35,933 : Evaluating...
2019-03-12 14:15:36,681 : 
Dev acc : 81.2 Test acc : 80.6 for                        SICK entailment

2019-03-12 14:15:36,682 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 14:15:36,708 : loading BERT model bert-large-uncased
2019-03-12 14:15:36,709 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:15:36,765 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:15:36,765 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxg35cb9j
2019-03-12 14:15:44,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:15:49,607 : Computing embedding for train
2019-03-12 14:16:11,156 : Computed train embeddings
2019-03-12 14:16:11,156 : Computing embedding for dev
2019-03-12 14:16:14,135 : Computed dev embeddings
2019-03-12 14:16:14,135 : Computing embedding for test
2019-03-12 14:16:37,733 : Computed test embeddings
2019-03-12 14:17:04,586 : Dev : Pearson 0.8354438756647905
2019-03-12 14:17:04,587 : Test : Pearson 0.839752942612643 Spearman 0.7622122635091367 MSE 0.30010561773495414                        for SICK Relatedness

2019-03-12 14:17:04,587 : 

***** Transfer task : STSBenchmark*****


2019-03-12 14:17:04,627 : loading BERT model bert-large-uncased
2019-03-12 14:17:04,627 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:17:04,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:17:04,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1xl7g51r
2019-03-12 14:17:12,125 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:17:17,485 : Computing embedding for train
2019-03-12 14:17:49,346 : Computed train embeddings
2019-03-12 14:17:49,346 : Computing embedding for dev
2019-03-12 14:17:59,728 : Computed dev embeddings
2019-03-12 14:17:59,729 : Computing embedding for test
2019-03-12 14:18:08,030 : Computed test embeddings
2019-03-12 14:18:36,831 : Dev : Pearson 0.7455117344228814
2019-03-12 14:18:36,831 : Test : Pearson 0.7034210361703115 Spearman 0.7000800066610328 MSE 1.3312902679564271                        for SICK Relatedness

2019-03-12 14:18:36,831 : ***** Transfer task : SNLI Entailment*****


2019-03-12 14:18:41,975 : loading BERT model bert-large-uncased
2019-03-12 14:18:41,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:18:42,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:18:42,054 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6prd2pkz
2019-03-12 14:18:49,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:18:55,637 : PROGRESS (encoding): 0.00%
2019-03-12 14:24:12,058 : PROGRESS (encoding): 14.56%
2019-03-12 14:30:04,567 : PROGRESS (encoding): 29.12%
2019-03-12 14:36:02,335 : PROGRESS (encoding): 43.69%
2019-03-12 14:42:14,217 : PROGRESS (encoding): 58.25%
2019-03-12 14:49:11,371 : PROGRESS (encoding): 72.81%
2019-03-12 14:58:10,309 : PROGRESS (encoding): 87.37%
2019-03-12 15:05:52,721 : PROGRESS (encoding): 0.00%
2019-03-12 15:06:47,741 : PROGRESS (encoding): 0.00%
2019-03-12 15:07:41,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:08:25,311 : [('reg:1e-09', 66.55)]
2019-03-12 15:08:25,311 : Validation : best param found is reg = 1e-09 with score             66.55
2019-03-12 15:08:25,311 : Evaluating...
2019-03-12 15:09:13,487 : Dev acc : 66.55 Test acc : 67.19 for SNLI

2019-03-12 15:09:13,487 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 15:09:13,695 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 15:09:14,768 : loading BERT model bert-large-uncased
2019-03-12 15:09:14,768 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:09:14,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:09:14,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpskt95bvk
2019-03-12 15:09:22,261 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:09:27,784 : Computing embeddings for train/dev/test
2019-03-12 15:15:52,438 : Computed embeddings
2019-03-12 15:15:52,438 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:16:39,548 : [('reg:1e-05', 95.41), ('reg:0.0001', 93.62), ('reg:0.001', 84.27), ('reg:0.01', 84.4)]
2019-03-12 15:16:39,548 : Validation : best param found is reg = 1e-05 with score             95.41
2019-03-12 15:16:39,548 : Evaluating...
2019-03-12 15:16:56,534 : 
Dev acc : 95.4 Test acc : 95.6 for LENGTH classification

2019-03-12 15:16:56,535 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 15:16:56,789 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 15:16:56,837 : loading BERT model bert-large-uncased
2019-03-12 15:16:56,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:16:56,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:16:56,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd3duv_3v
2019-03-12 15:17:04,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:17:09,557 : Computing embeddings for train/dev/test
2019-03-12 15:23:11,883 : Computed embeddings
2019-03-12 15:23:11,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:24:12,290 : [('reg:1e-05', 89.82), ('reg:0.0001', 53.58), ('reg:0.001', 5.32), ('reg:0.01', 1.33)]
2019-03-12 15:24:12,290 : Validation : best param found is reg = 1e-05 with score             89.82
2019-03-12 15:24:12,291 : Evaluating...
2019-03-12 15:24:28,302 : 
Dev acc : 89.8 Test acc : 90.2 for WORDCONTENT classification

2019-03-12 15:24:28,304 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 15:24:28,854 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 15:24:28,923 : loading BERT model bert-large-uncased
2019-03-12 15:24:28,924 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:24:28,951 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:24:28,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprtl4rmu8
2019-03-12 15:24:36,427 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:24:41,965 : Computing embeddings for train/dev/test
2019-03-12 15:30:20,762 : Computed embeddings
2019-03-12 15:30:20,762 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:31:00,054 : [('reg:1e-05', 32.3), ('reg:0.0001', 32.25), ('reg:0.001', 30.24), ('reg:0.01', 26.8)]
2019-03-12 15:31:00,054 : Validation : best param found is reg = 1e-05 with score             32.3
2019-03-12 15:31:00,054 : Evaluating...
2019-03-12 15:31:11,665 : 
Dev acc : 32.3 Test acc : 32.6 for DEPTH classification

2019-03-12 15:31:11,666 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 15:31:12,041 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 15:31:12,109 : loading BERT model bert-large-uncased
2019-03-12 15:31:12,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:31:12,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:31:12,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_rnrkx7d
2019-03-12 15:31:19,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:31:25,151 : Computing embeddings for train/dev/test
2019-03-12 15:36:43,337 : Computed embeddings
2019-03-12 15:36:43,337 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:37:43,832 : [('reg:1e-05', 57.72), ('reg:0.0001', 55.22), ('reg:0.001', 47.5), ('reg:0.01', 35.7)]
2019-03-12 15:37:43,832 : Validation : best param found is reg = 1e-05 with score             57.72
2019-03-12 15:37:43,832 : Evaluating...
2019-03-12 15:38:00,294 : 
Dev acc : 57.7 Test acc : 56.8 for TOPCONSTITUENTS classification

2019-03-12 15:38:00,295 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 15:38:00,691 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 15:38:00,764 : loading BERT model bert-large-uncased
2019-03-12 15:38:00,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:38:00,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:38:00,800 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm_f6ggfr
2019-03-12 15:38:08,284 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:38:13,757 : Computing embeddings for train/dev/test
2019-03-12 15:43:54,757 : Computed embeddings
2019-03-12 15:43:54,757 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:44:54,226 : [('reg:1e-05', 52.79), ('reg:0.0001', 53.0), ('reg:0.001', 53.23), ('reg:0.01', 51.66)]
2019-03-12 15:44:54,226 : Validation : best param found is reg = 0.001 with score             53.23
2019-03-12 15:44:54,226 : Evaluating...
2019-03-12 15:45:10,650 : 
Dev acc : 53.2 Test acc : 52.0 for BIGRAMSHIFT classification

2019-03-12 15:45:10,651 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 15:45:11,050 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 15:45:11,115 : loading BERT model bert-large-uncased
2019-03-12 15:45:11,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:45:11,146 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:45:11,146 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqdgszm14
2019-03-12 15:45:18,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:45:24,000 : Computing embeddings for train/dev/test
2019-03-12 15:50:57,615 : Computed embeddings
2019-03-12 15:50:57,615 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:51:45,020 : [('reg:1e-05', 85.23), ('reg:0.0001', 85.57), ('reg:0.001', 85.71), ('reg:0.01', 85.59)]
2019-03-12 15:51:45,020 : Validation : best param found is reg = 0.001 with score             85.71
2019-03-12 15:51:45,020 : Evaluating...
2019-03-12 15:51:55,157 : 
Dev acc : 85.7 Test acc : 83.9 for TENSE classification

2019-03-12 15:51:55,158 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 15:51:55,589 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 15:51:55,652 : loading BERT model bert-large-uncased
2019-03-12 15:51:55,652 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:51:55,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:51:55,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpysbt1s2r
2019-03-12 15:52:03,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:52:08,787 : Computing embeddings for train/dev/test
2019-03-12 15:58:03,625 : Computed embeddings
2019-03-12 15:58:03,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:59:09,677 : [('reg:1e-05', 81.21), ('reg:0.0001', 81.26), ('reg:0.001', 81.71), ('reg:0.01', 79.91)]
2019-03-12 15:59:09,677 : Validation : best param found is reg = 0.001 with score             81.71
2019-03-12 15:59:09,677 : Evaluating...
2019-03-12 15:59:26,755 : 
Dev acc : 81.7 Test acc : 80.4 for SUBJNUMBER classification

2019-03-12 15:59:26,756 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 15:59:27,161 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 15:59:27,228 : loading BERT model bert-large-uncased
2019-03-12 15:59:27,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:59:27,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:59:27,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsteul8f5
2019-03-12 15:59:34,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:59:40,334 : Computing embeddings for train/dev/test
2019-03-12 16:05:29,078 : Computed embeddings
2019-03-12 16:05:29,078 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:06:11,015 : [('reg:1e-05', 79.77), ('reg:0.0001', 79.77), ('reg:0.001', 79.75), ('reg:0.01', 78.82)]
2019-03-12 16:06:11,015 : Validation : best param found is reg = 1e-05 with score             79.77
2019-03-12 16:06:11,015 : Evaluating...
2019-03-12 16:06:21,460 : 
Dev acc : 79.8 Test acc : 81.4 for OBJNUMBER classification

2019-03-12 16:06:21,461 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 16:06:22,026 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 16:06:22,094 : loading BERT model bert-large-uncased
2019-03-12 16:06:22,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:06:22,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:06:22,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5kflc9fe
2019-03-12 16:06:29,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:06:35,064 : Computing embeddings for train/dev/test
2019-03-12 16:13:17,355 : Computed embeddings
2019-03-12 16:13:17,355 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:13:59,004 : [('reg:1e-05', 50.74), ('reg:0.0001', 50.71), ('reg:0.001', 50.44), ('reg:0.01', 50.57)]
2019-03-12 16:13:59,004 : Validation : best param found is reg = 1e-05 with score             50.74
2019-03-12 16:13:59,004 : Evaluating...
2019-03-12 16:14:07,506 : 
Dev acc : 50.7 Test acc : 50.9 for ODDMANOUT classification

2019-03-12 16:14:07,506 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 16:14:07,912 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 16:14:07,993 : loading BERT model bert-large-uncased
2019-03-12 16:14:07,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:14:08,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:14:08,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzwezha_d
2019-03-12 16:14:15,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:14:20,898 : Computing embeddings for train/dev/test
2019-03-12 16:20:57,027 : Computed embeddings
2019-03-12 16:20:57,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:21:35,874 : [('reg:1e-05', 52.02), ('reg:0.0001', 51.92), ('reg:0.001', 51.59), ('reg:0.01', 50.32)]
2019-03-12 16:21:35,874 : Validation : best param found is reg = 1e-05 with score             52.02
2019-03-12 16:21:35,874 : Evaluating...
2019-03-12 16:21:46,775 : 
Dev acc : 52.0 Test acc : 51.5 for COORDINATIONINVERSION classification

2019-03-12 16:21:46,777 : total results: {'STS12': {'MSRpar': {'pearson': (0.39539117343702485, 1.788530624064311e-29), 'spearman': SpearmanrResult(correlation=0.4377542248604886, pvalue=1.8520958676267397e-36), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7039628818110519, 2.9259192939507706e-113), 'spearman': SpearmanrResult(correlation=0.7025854333264111, pvalue=1.2295965588058835e-112), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5055129286960114, 3.8072183774228156e-31), 'spearman': SpearmanrResult(correlation=0.6023385415462084, pvalue=1.1626400214986153e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6901487882377094, 3.6348824428893204e-107), 'spearman': SpearmanrResult(correlation=0.689473906279114, pvalue=7.070265186900136e-107), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5335365573510141, 9.84000125946313e-31), 'spearman': SpearmanrResult(correlation=0.47164566094216287, pvalue=1.716105985974858e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5657104659065623, 'wmean': 0.5749802616695185}, 'spearman': {'mean': 0.5807595533908769, 'wmean': 0.5910618348246921}}}, 'STS13': {'FNWN': {'pearson': (0.4325763244490023, 5.096406218167701e-10), 'spearman': SpearmanrResult(correlation=0.4425488983387399, pvalue=1.8177818578393726e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.7005200265213892, 1.042200928505704e-111), 'spearman': SpearmanrResult(correlation=0.683660705160016, pvalue=2.022240184249457e-104), 'nsamples': 750}, 'OnWN': {'pearson': (0.5950094703607265, 5.073272763605536e-55), 'spearman': SpearmanrResult(correlation=0.6153099798288021, pvalue=9.631723560647136e-60), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5760352737770393, 'wmean': 0.6272981720561807}, 'spearman': {'mean': 0.5805065277758527, 'wmean': 0.6277174462266611}}}, 'STS14': {'deft-forum': {'pearson': (0.3849921513568189, 2.3916192832959986e-17), 'spearman': SpearmanrResult(correlation=0.3884116684623624, pvalue=1.181521673869109e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7417250430698297, 1.2568141807429641e-53), 'spearman': SpearmanrResult(correlation=0.6965430865510629, pvalue=7.204838956660527e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.6595983996361676, 7.776719798082764e-95), 'spearman': SpearmanrResult(correlation=0.6175440216857504, pvalue=4.68695031081772e-80), 'nsamples': 750}, 'images': {'pearson': (0.7092346483383731, 1.112417104745707e-115), 'spearman': SpearmanrResult(correlation=0.6872991415959305, pvalue=5.9589176968148e-106), 'nsamples': 750}, 'OnWN': {'pearson': (0.688738923949113, 1.4561659977942166e-106), 'spearman': SpearmanrResult(correlation=0.7256787856482042, pvalue=1.3665307132801224e-123), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6609372494665562, 2.403238707185934e-95), 'spearman': SpearmanrResult(correlation=0.6417066928506941, pvalue=2.9006756616578984e-88), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6408710693028098, 'wmean': 0.6492389058864466}, 'spearman': {'mean': 0.6261972327990007, 'wmean': 0.6367785754956845}}}, 'STS15': {'answers-forums': {'pearson': (0.5445794337848953, 2.4215677879962e-30), 'spearman': SpearmanrResult(correlation=0.5355591368292604, pvalue=3.201757099504343e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.7271809471142733, 2.4220070356688898e-124), 'spearman': SpearmanrResult(correlation=0.7356023315303833, pvalue=1.1924651958352138e-128), 'nsamples': 750}, 'belief': {'pearson': (0.6427837623137305, 4.300432949575823e-45), 'spearman': SpearmanrResult(correlation=0.6465231449523652, pvalue=9.179898825770191e-46), 'nsamples': 375}, 'headlines': {'pearson': (0.7187253239041946, 3.5466616787295834e-120), 'spearman': SpearmanrResult(correlation=0.7107614638712099, pvalue=2.163786637777686e-116), 'nsamples': 750}, 'images': {'pearson': (0.7786285399772501, 1.5153534767356045e-153), 'spearman': SpearmanrResult(correlation=0.7871571831775396, pvalue=3.7270221749220157e-159), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6823796014188688, 'wmean': 0.7045541022612577}, 'spearman': {'mean': 0.6831206520721517, 'wmean': 0.7061405298674863}}}, 'STS16': {'answer-answer': {'pearson': (0.526543723312768, 1.6130772213338203e-19), 'spearman': SpearmanrResult(correlation=0.5385277929291408, pvalue=1.6715654953222818e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.7113740303592108, 1.049276818152944e-39), 'spearman': SpearmanrResult(correlation=0.7173750121981078, pvalue=1.19708337741728e-40), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7714086500967817, 1.1887523873875592e-46), 'spearman': SpearmanrResult(correlation=0.7726432597125761, pvalue=6.930714955512487e-47), 'nsamples': 230}, 'postediting': {'pearson': (0.8211214313543934, 7.159517206076051e-61), 'spearman': SpearmanrResult(correlation=0.8271481173907431, pvalue=1.6763887947571e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.5436354378698081, 1.8049517306866774e-17), 'spearman': SpearmanrResult(correlation=0.5394872974245676, pvalue=3.511236767609316e-17), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6748166545985923, 'wmean': 0.6764516564657741}, 'spearman': {'mean': 0.679036295931027, 'wmean': 0.6810264527658508}}}, 'MR': {'devacc': 74.59, 'acc': 74.1, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.14, 'acc': 78.86, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.29, 'acc': 87.49, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.82, 'acc': 90.78, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.93, 'acc': 79.57, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.06, 'acc': 40.72, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 78.08, 'acc': 87.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.02, 'acc': 73.16, 'f1': 79.39, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.2, 'acc': 80.6, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8354438756647905, 'pearson': 0.839752942612643, 'spearman': 0.7622122635091367, 'mse': 0.30010561773495414, 'yhat': array([3.37821431, 4.36124347, 1.24240771, ..., 3.28791326, 4.31352507,        4.31045928]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7455117344228814, 'pearson': 0.7034210361703115, 'spearman': 0.7000800066610328, 'mse': 1.3312902679564271, 'yhat': array([1.38799564, 1.34775623, 2.05434765, ..., 3.80808361, 3.87684956,        3.23708851]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.55, 'acc': 67.19, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 95.41, 'acc': 95.61, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 89.82, 'acc': 90.21, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.3, 'acc': 32.61, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.72, 'acc': 56.78, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 53.23, 'acc': 51.97, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.71, 'acc': 83.93, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.71, 'acc': 80.36, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.77, 'acc': 81.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.74, 'acc': 50.94, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 52.02, 'acc': 51.45, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 16:21:46,778 : STS12 p=0.5750, STS12 s=0.5911, STS13 p=0.6273, STS13 s=0.6277, STS14 p=0.6492, STS14 s=0.6368, STS15 p=0.7046, STS15 s=0.7061, STS 16 p=0.6765, STS16 s=0.6810, STS B p=0.7034, STS B s=0.7001, STS B m=1.3313, SICK-R p=0.8398, SICK-R s=0.7622, SICK-P m=0.3001
2019-03-12 16:21:46,778 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 16:21:46,778 : 0.5750,0.5911,0.6273,0.6277,0.6492,0.6368,0.7046,0.7061,0.6765,0.6810,0.7034,0.7001,1.3313,0.8398,0.7622,0.3001
2019-03-12 16:21:46,778 : MR=74.10, CR=78.86, SUBJ=90.78, MPQA=87.49, SST-B=79.57, SST-F=40.72, TREC=87.00, SICK-E=80.60, SNLI=67.19, MRPC=73.16, MRPC f=79.39
2019-03-12 16:21:46,778 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 16:21:46,778 : 74.10,78.86,90.78,87.49,79.57,40.72,87.00,80.60,67.19,73.16,79.39
2019-03-12 16:21:46,778 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 16:21:46,778 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 16:21:46,778 : na,na,na,na,na,na,na,na,na,na
2019-03-12 16:21:46,778 : SentLen=95.61, WC=90.21, TreeDepth=32.61, TopConst=56.78, BShift=51.97, Tense=83.93, SubjNum=80.36, ObjNum=81.42, SOMO=50.94, CoordInv=51.45, average=67.53
2019-03-12 16:21:46,778 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 16:21:46,778 : 95.61,90.21,32.61,56.78,51.97,83.93,80.36,81.42,50.94,51.45,67.53
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,778 : layer 2
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,778 : ********************************************************************************
2019-03-12 16:21:46,870 : ***** Transfer task : STS12 *****


2019-03-12 16:21:46,882 : loading BERT model bert-large-uncased
2019-03-12 16:21:46,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:21:46,900 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:21:46,900 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0ln3lcm3
2019-03-12 16:21:54,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:07,215 : MSRpar : pearson = 0.4016, spearman = 0.4421
2019-03-12 16:22:10,284 : MSRvid : pearson = 0.6909, spearman = 0.6907
2019-03-12 16:22:13,112 : SMTeuroparl : pearson = 0.5067, spearman = 0.6058
2019-03-12 16:22:17,792 : surprise.OnWN : pearson = 0.6960, spearman = 0.6911
2019-03-12 16:22:20,606 : surprise.SMTnews : pearson = 0.5428, spearman = 0.4969
2019-03-12 16:22:20,607 : ALL (weighted average) : Pearson = 0.5761,             Spearman = 0.5934
2019-03-12 16:22:20,607 : ALL (average) : Pearson = 0.5676,             Spearman = 0.5853

2019-03-12 16:22:20,607 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 16:22:20,615 : loading BERT model bert-large-uncased
2019-03-12 16:22:20,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:22:20,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:22:20,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps20mr_19
2019-03-12 16:22:28,119 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:35,556 : FNWN : pearson = 0.4253, spearman = 0.4419
2019-03-12 16:22:39,110 : headlines : pearson = 0.6903, spearman = 0.6729
2019-03-12 16:22:41,970 : OnWN : pearson = 0.5716, spearman = 0.5958
2019-03-12 16:22:41,970 : ALL (weighted average) : Pearson = 0.6125,             Spearman = 0.6150
2019-03-12 16:22:41,970 : ALL (average) : Pearson = 0.5624,             Spearman = 0.5702

2019-03-12 16:22:41,970 : ***** Transfer task : STS14 *****


2019-03-12 16:22:41,986 : loading BERT model bert-large-uncased
2019-03-12 16:22:41,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:22:42,005 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:22:42,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpre180y_1
2019-03-12 16:22:49,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:57,248 : deft-forum : pearson = 0.3828, spearman = 0.3855
2019-03-12 16:23:00,166 : deft-news : pearson = 0.7467, spearman = 0.7025
2019-03-12 16:23:04,116 : headlines : pearson = 0.6470, spearman = 0.6040
2019-03-12 16:23:08,168 : images : pearson = 0.7087, spearman = 0.6900
2019-03-12 16:23:12,074 : OnWN : pearson = 0.6847, spearman = 0.7196
2019-03-12 16:23:17,334 : tweet-news : pearson = 0.6497, spearman = 0.6281
2019-03-12 16:23:17,335 : ALL (weighted average) : Pearson = 0.6437,             Spearman = 0.6308
2019-03-12 16:23:17,335 : ALL (average) : Pearson = 0.6366,             Spearman = 0.6216

2019-03-12 16:23:17,335 : ***** Transfer task : STS15 *****


2019-03-12 16:23:17,367 : loading BERT model bert-large-uncased
2019-03-12 16:23:17,367 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:23:17,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:23:17,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuefdxo74
2019-03-12 16:23:24,833 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:23:33,823 : answers-forums : pearson = 0.5552, spearman = 0.5418
2019-03-12 16:23:37,243 : answers-students : pearson = 0.7174, spearman = 0.7242
2019-03-12 16:23:40,801 : belief : pearson = 0.6451, spearman = 0.6617
2019-03-12 16:23:44,727 : headlines : pearson = 0.7096, spearman = 0.7004
2019-03-12 16:23:48,619 : images : pearson = 0.7804, spearman = 0.7900
2019-03-12 16:23:48,619 : ALL (weighted average) : Pearson = 0.7019,             Spearman = 0.7041
2019-03-12 16:23:48,619 : ALL (average) : Pearson = 0.6815,             Spearman = 0.6836

2019-03-12 16:23:48,620 : ***** Transfer task : STS16 *****


2019-03-12 16:23:48,689 : loading BERT model bert-large-uncased
2019-03-12 16:23:48,689 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:23:48,707 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:23:48,707 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf846uadk
2019-03-12 16:23:56,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:24:03,054 : answer-answer : pearson = 0.5256, spearman = 0.5425
2019-03-12 16:24:04,260 : headlines : pearson = 0.7112, spearman = 0.7193
2019-03-12 16:24:05,748 : plagiarism : pearson = 0.7487, spearman = 0.7507
2019-03-12 16:24:08,274 : postediting : pearson = 0.8184, spearman = 0.8276
2019-03-12 16:24:09,513 : question-question : pearson = 0.5396, spearman = 0.5366
2019-03-12 16:24:09,514 : ALL (weighted average) : Pearson = 0.6705,             Spearman = 0.6776
2019-03-12 16:24:09,514 : ALL (average) : Pearson = 0.6687,             Spearman = 0.6754

2019-03-12 16:24:09,514 : ***** Transfer task : MR *****


2019-03-12 16:24:09,529 : loading BERT model bert-large-uncased
2019-03-12 16:24:09,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:24:09,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:24:09,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0o1boipy
2019-03-12 16:24:16,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:24:22,273 : Generating sentence embeddings
2019-03-12 16:25:17,294 : Generated sentence embeddings
2019-03-12 16:25:17,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:25:34,616 : Best param found at split 1: l2reg = 0.001                 with score 75.47
2019-03-12 16:25:51,413 : Best param found at split 2: l2reg = 0.001                 with score 75.04
2019-03-12 16:26:09,461 : Best param found at split 3: l2reg = 0.0001                 with score 75.42
2019-03-12 16:26:26,930 : Best param found at split 4: l2reg = 1e-05                 with score 74.81
2019-03-12 16:26:43,910 : Best param found at split 5: l2reg = 0.001                 with score 74.91
2019-03-12 16:26:44,850 : Dev acc : 75.13 Test acc : 74.4

2019-03-12 16:26:44,851 : ***** Transfer task : CR *****


2019-03-12 16:26:44,858 : loading BERT model bert-large-uncased
2019-03-12 16:26:44,858 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:26:44,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:26:44,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk_37a6et
2019-03-12 16:26:52,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:26:57,948 : Generating sentence embeddings
2019-03-12 16:27:13,156 : Generated sentence embeddings
2019-03-12 16:27:13,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:27:19,042 : Best param found at split 1: l2reg = 1e-05                 with score 79.56
2019-03-12 16:27:25,126 : Best param found at split 2: l2reg = 0.001                 with score 80.36
2019-03-12 16:27:33,077 : Best param found at split 3: l2reg = 0.0001                 with score 79.97
2019-03-12 16:27:40,538 : Best param found at split 4: l2reg = 0.001                 with score 79.77
2019-03-12 16:27:48,942 : Best param found at split 5: l2reg = 0.01                 with score 80.5
2019-03-12 16:27:49,322 : Dev acc : 80.03 Test acc : 79.28

2019-03-12 16:27:49,323 : ***** Transfer task : MPQA *****


2019-03-12 16:27:49,329 : loading BERT model bert-large-uncased
2019-03-12 16:27:49,329 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:27:49,380 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:27:49,380 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv_a_idhd
2019-03-12 16:27:56,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:28:02,248 : Generating sentence embeddings
2019-03-12 16:28:15,321 : Generated sentence embeddings
2019-03-12 16:28:15,321 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:28:32,935 : Best param found at split 1: l2reg = 0.0001                 with score 87.01
2019-03-12 16:28:53,690 : Best param found at split 2: l2reg = 0.001                 with score 87.26
2019-03-12 16:29:15,070 : Best param found at split 3: l2reg = 0.001                 with score 87.7
2019-03-12 16:29:36,830 : Best param found at split 4: l2reg = 0.001                 with score 87.35
2019-03-12 16:29:59,304 : Best param found at split 5: l2reg = 0.01                 with score 86.61
2019-03-12 16:30:00,518 : Dev acc : 87.19 Test acc : 87.52

2019-03-12 16:30:00,519 : ***** Transfer task : SUBJ *****


2019-03-12 16:30:00,536 : loading BERT model bert-large-uncased
2019-03-12 16:30:00,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:30:00,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:30:00,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq8hk75x9
2019-03-12 16:30:08,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:30:13,666 : Generating sentence embeddings
2019-03-12 16:31:13,131 : Generated sentence embeddings
2019-03-12 16:31:13,132 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:31:56,556 : Best param found at split 1: l2reg = 0.001                 with score 92.22
2019-03-12 16:32:45,330 : Best param found at split 2: l2reg = 0.001                 with score 92.26
2019-03-12 16:33:39,940 : Best param found at split 3: l2reg = 0.0001                 with score 92.31
2019-03-12 16:34:29,226 : Best param found at split 4: l2reg = 0.001                 with score 92.58
2019-03-12 16:35:18,663 : Best param found at split 5: l2reg = 0.001                 with score 91.98
2019-03-12 16:35:21,568 : Dev acc : 92.27 Test acc : 91.64

2019-03-12 16:35:21,569 : ***** Transfer task : SST Binary classification *****


2019-03-12 16:35:21,659 : loading BERT model bert-large-uncased
2019-03-12 16:35:21,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:35:21,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:35:21,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2crgyv_d
2019-03-12 16:35:29,181 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:34,663 : Computing embedding for train
2019-03-12 16:38:40,360 : Computed train embeddings
2019-03-12 16:38:40,360 : Computing embedding for dev
2019-03-12 16:38:42,596 : Computed dev embeddings
2019-03-12 16:38:42,596 : Computing embedding for test
2019-03-12 16:38:47,323 : Computed test embeddings
2019-03-12 16:38:47,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:39:01,719 : [('reg:1e-05', 79.82), ('reg:0.0001', 79.93), ('reg:0.001', 78.9), ('reg:0.01', 78.21)]
2019-03-12 16:39:01,719 : Validation : best param found is reg = 0.0001 with score             79.93
2019-03-12 16:39:01,719 : Evaluating...
2019-03-12 16:39:06,007 : 
Dev acc : 79.93 Test acc : 80.12 for             SST Binary classification

2019-03-12 16:39:06,007 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 16:39:06,061 : loading BERT model bert-large-uncased
2019-03-12 16:39:06,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:39:06,082 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:39:06,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd89cnh4u
2019-03-12 16:39:13,580 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:39:18,899 : Computing embedding for train
2019-03-12 16:39:41,101 : Computed train embeddings
2019-03-12 16:39:41,101 : Computing embedding for dev
2019-03-12 16:39:43,999 : Computed dev embeddings
2019-03-12 16:39:43,999 : Computing embedding for test
2019-03-12 16:39:49,713 : Computed test embeddings
2019-03-12 16:39:49,713 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:39:51,900 : [('reg:1e-05', 37.42), ('reg:0.0001', 37.42), ('reg:0.001', 37.42), ('reg:0.01', 38.78)]
2019-03-12 16:39:51,900 : Validation : best param found is reg = 0.01 with score             38.78
2019-03-12 16:39:51,900 : Evaluating...
2019-03-12 16:39:52,448 : 
Dev acc : 38.78 Test acc : 39.41 for             SST Fine-Grained classification

2019-03-12 16:39:52,448 : ***** Transfer task : TREC *****


2019-03-12 16:39:52,462 : loading BERT model bert-large-uncased
2019-03-12 16:39:52,462 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:39:52,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:39:52,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk5i36kn2
2019-03-12 16:39:59,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:40:12,862 : Computed train embeddings
2019-03-12 16:40:13,460 : Computed test embeddings
2019-03-12 16:40:13,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:40:20,364 : [('reg:1e-05', 77.81), ('reg:0.0001', 78.04), ('reg:0.001', 78.08), ('reg:0.01', 70.21)]
2019-03-12 16:40:20,364 : Cross-validation : best param found is reg = 0.001             with score 78.08
2019-03-12 16:40:20,364 : Evaluating...
2019-03-12 16:40:20,700 : 
Dev acc : 78.08 Test acc : 87.6             for TREC

2019-03-12 16:40:20,701 : ***** Transfer task : MRPC *****


2019-03-12 16:40:20,721 : loading BERT model bert-large-uncased
2019-03-12 16:40:20,721 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:40:20,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:40:20,744 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprj9r4rfl
2019-03-12 16:40:28,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:40:33,556 : Computing embedding for train
2019-03-12 16:40:56,061 : Computed train embeddings
2019-03-12 16:40:56,061 : Computing embedding for test
2019-03-12 16:41:05,933 : Computed test embeddings
2019-03-12 16:41:05,954 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:41:10,384 : [('reg:1e-05', 73.8), ('reg:0.0001', 73.87), ('reg:0.001', 73.48), ('reg:0.01', 72.45)]
2019-03-12 16:41:10,384 : Cross-validation : best param found is reg = 0.0001             with score 73.87
2019-03-12 16:41:10,384 : Evaluating...
2019-03-12 16:41:10,703 : Dev acc : 73.87 Test acc 73.91; Test F1 81.34 for MRPC.

2019-03-12 16:41:10,703 : ***** Transfer task : SICK-Entailment*****


2019-03-12 16:41:10,766 : loading BERT model bert-large-uncased
2019-03-12 16:41:10,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:41:10,785 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:41:10,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp55iu1rr7
2019-03-12 16:41:18,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:41:23,482 : Computing embedding for train
2019-03-12 16:41:34,882 : Computed train embeddings
2019-03-12 16:41:34,882 : Computing embedding for dev
2019-03-12 16:41:36,439 : Computed dev embeddings
2019-03-12 16:41:36,439 : Computing embedding for test
2019-03-12 16:41:48,683 : Computed test embeddings
2019-03-12 16:41:48,719 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:41:49,758 : [('reg:1e-05', 78.8), ('reg:0.0001', 79.0), ('reg:0.001', 75.4), ('reg:0.01', 76.4)]
2019-03-12 16:41:49,758 : Validation : best param found is reg = 0.0001 with score             79.0
2019-03-12 16:41:49,758 : Evaluating...
2019-03-12 16:41:50,221 : 
Dev acc : 79.0 Test acc : 77.43 for                        SICK entailment

2019-03-12 16:41:50,222 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 16:41:50,248 : loading BERT model bert-large-uncased
2019-03-12 16:41:50,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:41:50,303 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:41:50,303 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz4i4s987
2019-03-12 16:41:57,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:42:02,970 : Computing embedding for train
2019-03-12 16:42:14,403 : Computed train embeddings
2019-03-12 16:42:14,403 : Computing embedding for dev
2019-03-12 16:42:15,964 : Computed dev embeddings
2019-03-12 16:42:15,964 : Computing embedding for test
2019-03-12 16:42:28,221 : Computed test embeddings
2019-03-12 16:42:43,253 : Dev : Pearson 0.8431614377116466
2019-03-12 16:42:43,253 : Test : Pearson 0.8299549852547955 Spearman 0.7503269631005324 MSE 0.31694086510680375                        for SICK Relatedness

2019-03-12 16:42:43,254 : 

***** Transfer task : STSBenchmark*****


2019-03-12 16:42:43,292 : loading BERT model bert-large-uncased
2019-03-12 16:42:43,293 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:42:43,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:42:43,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbmk4_6xb
2019-03-12 16:42:50,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:42:56,071 : Computing embedding for train
2019-03-12 16:43:14,856 : Computed train embeddings
2019-03-12 16:43:14,856 : Computing embedding for dev
2019-03-12 16:43:20,552 : Computed dev embeddings
2019-03-12 16:43:20,552 : Computing embedding for test
2019-03-12 16:43:25,203 : Computed test embeddings
2019-03-12 16:43:41,681 : Dev : Pearson 0.7571468299593779
2019-03-12 16:43:41,681 : Test : Pearson 0.7229356882064915 Spearman 0.7213734873873658 MSE 1.2983851384470984                        for SICK Relatedness

2019-03-12 16:43:41,681 : ***** Transfer task : SNLI Entailment*****


2019-03-12 16:43:46,782 : loading BERT model bert-large-uncased
2019-03-12 16:43:46,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:43:46,918 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:43:46,918 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm6r0je_x
2019-03-12 16:43:54,375 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:44:00,000 : PROGRESS (encoding): 0.00%
2019-03-12 16:46:49,122 : PROGRESS (encoding): 14.56%
2019-03-12 16:50:02,715 : PROGRESS (encoding): 29.12%
2019-03-12 16:53:14,939 : PROGRESS (encoding): 43.69%
2019-03-12 16:56:39,506 : PROGRESS (encoding): 58.25%
2019-03-12 17:00:26,718 : PROGRESS (encoding): 72.81%
2019-03-12 17:04:12,755 : PROGRESS (encoding): 87.37%
2019-03-12 17:08:17,544 : PROGRESS (encoding): 0.00%
2019-03-12 17:08:48,291 : PROGRESS (encoding): 0.00%
2019-03-12 17:09:17,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:09:49,494 : [('reg:1e-09', 66.68)]
2019-03-12 17:09:49,494 : Validation : best param found is reg = 1e-09 with score             66.68
2019-03-12 17:09:49,494 : Evaluating...
2019-03-12 17:10:20,140 : Dev acc : 66.68 Test acc : 66.81 for SNLI

2019-03-12 17:10:20,141 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 17:10:20,342 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 17:10:21,439 : loading BERT model bert-large-uncased
2019-03-12 17:10:21,439 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:10:21,466 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:10:21,466 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0qsz5uss
2019-03-12 17:10:28,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:10:34,239 : Computing embeddings for train/dev/test
2019-03-12 17:14:08,850 : Computed embeddings
2019-03-12 17:14:08,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:14:37,596 : [('reg:1e-05', 94.9), ('reg:0.0001', 91.42), ('reg:0.001', 82.9), ('reg:0.01', 81.47)]
2019-03-12 17:14:37,596 : Validation : best param found is reg = 1e-05 with score             94.9
2019-03-12 17:14:37,596 : Evaluating...
2019-03-12 17:14:45,012 : 
Dev acc : 94.9 Test acc : 95.2 for LENGTH classification

2019-03-12 17:14:45,012 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 17:14:45,393 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 17:14:45,438 : loading BERT model bert-large-uncased
2019-03-12 17:14:45,438 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:14:45,470 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:14:45,470 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw2ni2i0g
2019-03-12 17:14:52,919 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:14:58,130 : Computing embeddings for train/dev/test
2019-03-12 17:18:16,209 : Computed embeddings
2019-03-12 17:18:16,210 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:18:51,604 : [('reg:1e-05', 87.02), ('reg:0.0001', 57.68), ('reg:0.001', 5.55), ('reg:0.01', 1.35)]
2019-03-12 17:18:51,604 : Validation : best param found is reg = 1e-05 with score             87.02
2019-03-12 17:18:51,604 : Evaluating...
2019-03-12 17:18:56,559 : 
Dev acc : 87.0 Test acc : 87.1 for WORDCONTENT classification

2019-03-12 17:18:56,561 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 17:18:56,942 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 17:18:57,009 : loading BERT model bert-large-uncased
2019-03-12 17:18:57,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:18:57,034 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:18:57,034 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeouj5zi5
2019-03-12 17:19:04,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:19:09,855 : Computing embeddings for train/dev/test
2019-03-12 17:22:16,438 : Computed embeddings
2019-03-12 17:22:16,438 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:22:34,510 : [('reg:1e-05', 32.67), ('reg:0.0001', 32.42), ('reg:0.001', 30.06), ('reg:0.01', 26.57)]
2019-03-12 17:22:34,510 : Validation : best param found is reg = 1e-05 with score             32.67
2019-03-12 17:22:34,510 : Evaluating...
2019-03-12 17:22:39,174 : 
Dev acc : 32.7 Test acc : 32.4 for DEPTH classification

2019-03-12 17:22:39,175 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 17:22:39,597 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 17:22:39,664 : loading BERT model bert-large-uncased
2019-03-12 17:22:39,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:22:39,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:22:39,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcxjs0ks2
2019-03-12 17:22:47,250 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:22:52,444 : Computing embeddings for train/dev/test
2019-03-12 17:25:45,195 : Computed embeddings
2019-03-12 17:25:45,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:26:15,576 : [('reg:1e-05', 58.31), ('reg:0.0001', 56.21), ('reg:0.001', 46.66), ('reg:0.01', 36.05)]
2019-03-12 17:26:15,576 : Validation : best param found is reg = 1e-05 with score             58.31
2019-03-12 17:26:15,576 : Evaluating...
2019-03-12 17:26:21,129 : 
Dev acc : 58.3 Test acc : 57.6 for TOPCONSTITUENTS classification

2019-03-12 17:26:21,130 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 17:26:21,472 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 17:26:21,537 : loading BERT model bert-large-uncased
2019-03-12 17:26:21,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:26:21,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:26:21,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxktqow69
2019-03-12 17:26:29,092 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:26:34,354 : Computing embeddings for train/dev/test
2019-03-12 17:29:41,529 : Computed embeddings
2019-03-12 17:29:41,529 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:30:15,181 : [('reg:1e-05', 53.17), ('reg:0.0001', 53.22), ('reg:0.001', 52.74), ('reg:0.01', 51.36)]
2019-03-12 17:30:15,181 : Validation : best param found is reg = 0.0001 with score             53.22
2019-03-12 17:30:15,181 : Evaluating...
2019-03-12 17:30:23,689 : 
Dev acc : 53.2 Test acc : 53.2 for BIGRAMSHIFT classification

2019-03-12 17:30:23,690 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 17:30:24,243 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 17:30:24,308 : loading BERT model bert-large-uncased
2019-03-12 17:30:24,309 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:30:24,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:30:24,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgqeiyanw
2019-03-12 17:30:31,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:30:37,106 : Computing embeddings for train/dev/test
2019-03-12 17:33:39,664 : Computed embeddings
2019-03-12 17:33:39,664 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:34:05,294 : [('reg:1e-05', 85.06), ('reg:0.0001', 85.25), ('reg:0.001', 86.2), ('reg:0.01', 85.59)]
2019-03-12 17:34:05,294 : Validation : best param found is reg = 0.001 with score             86.2
2019-03-12 17:34:05,294 : Evaluating...
2019-03-12 17:34:11,726 : 
Dev acc : 86.2 Test acc : 84.2 for TENSE classification

2019-03-12 17:34:11,727 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 17:34:12,144 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 17:34:12,207 : loading BERT model bert-large-uncased
2019-03-12 17:34:12,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:34:12,232 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:34:12,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3h3ditg5
2019-03-12 17:34:19,747 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:34:25,008 : Computing embeddings for train/dev/test
2019-03-12 17:37:39,178 : Computed embeddings
2019-03-12 17:37:39,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:38:12,956 : [('reg:1e-05', 81.11), ('reg:0.0001', 81.06), ('reg:0.001', 80.96), ('reg:0.01', 79.22)]
2019-03-12 17:38:12,956 : Validation : best param found is reg = 1e-05 with score             81.11
2019-03-12 17:38:12,956 : Evaluating...
2019-03-12 17:38:21,547 : 
Dev acc : 81.1 Test acc : 79.4 for SUBJNUMBER classification

2019-03-12 17:38:21,548 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 17:38:21,952 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 17:38:22,017 : loading BERT model bert-large-uncased
2019-03-12 17:38:22,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:38:22,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:38:22,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_o33ou6t
2019-03-12 17:38:29,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:38:34,745 : Computing embeddings for train/dev/test
2019-03-12 17:41:44,158 : Computed embeddings
2019-03-12 17:41:44,158 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:42:10,220 : [('reg:1e-05', 79.77), ('reg:0.0001', 79.72), ('reg:0.001', 80.3), ('reg:0.01', 79.22)]
2019-03-12 17:42:10,220 : Validation : best param found is reg = 0.001 with score             80.3
2019-03-12 17:42:10,220 : Evaluating...
2019-03-12 17:42:17,421 : 
Dev acc : 80.3 Test acc : 82.0 for OBJNUMBER classification

2019-03-12 17:42:17,422 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 17:42:18,003 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 17:42:18,071 : loading BERT model bert-large-uncased
2019-03-12 17:42:18,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:42:18,098 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:42:18,098 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpts9c31gs
2019-03-12 17:42:25,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:42:30,874 : Computing embeddings for train/dev/test
2019-03-12 17:46:09,868 : Computed embeddings
2019-03-12 17:46:09,868 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:46:33,015 : [('reg:1e-05', 51.62), ('reg:0.0001', 51.48), ('reg:0.001', 51.64), ('reg:0.01', 51.2)]
2019-03-12 17:46:33,015 : Validation : best param found is reg = 0.001 with score             51.64
2019-03-12 17:46:33,015 : Evaluating...
2019-03-12 17:46:39,461 : 
Dev acc : 51.6 Test acc : 51.5 for ODDMANOUT classification

2019-03-12 17:46:39,462 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 17:46:39,849 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 17:46:39,924 : loading BERT model bert-large-uncased
2019-03-12 17:46:39,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:46:40,047 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:46:40,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwnjsxq6q
2019-03-12 17:46:47,532 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:46:52,778 : Computing embeddings for train/dev/test
2019-03-12 17:50:30,213 : Computed embeddings
2019-03-12 17:50:30,213 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:50:57,947 : [('reg:1e-05', 51.35), ('reg:0.0001', 51.26), ('reg:0.001', 53.17), ('reg:0.01', 50.45)]
2019-03-12 17:50:57,948 : Validation : best param found is reg = 0.001 with score             53.17
2019-03-12 17:50:57,948 : Evaluating...
2019-03-12 17:51:05,610 : 
Dev acc : 53.2 Test acc : 52.2 for COORDINATIONINVERSION classification

2019-03-12 17:51:05,612 : total results: {'STS12': {'MSRpar': {'pearson': (0.4015946361585901, 1.9552012230440775e-30), 'spearman': SpearmanrResult(correlation=0.4420686827999462, pvalue=3.1545851410267852e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6909145600832092, 1.7047747074331152e-107), 'spearman': SpearmanrResult(correlation=0.6906857912016157, pvalue=2.1380028514922894e-107), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5066846118401819, 2.6387733550327362e-31), 'spearman': SpearmanrResult(correlation=0.6057905423075826, pvalue=2.578374372479769e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6959697653864673, 1.0836392336816198e-109), 'spearman': SpearmanrResult(correlation=0.6910935081731997, pvalue=1.427835589625919e-107), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5427979218302642, 5.968804785289901e-32), 'spearman': SpearmanrResult(correlation=0.4968733420033361, pvalue=2.8836938334123565e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5675922990597425, 'wmean': 0.5760951830328568}, 'spearman': {'mean': 0.5853023732971361, 'wmean': 0.5933707557945891}}}, 'STS13': {'FNWN': {'pearson': (0.4252752678146874, 1.061568272453368e-09), 'spearman': SpearmanrResult(correlation=0.4418994144201343, pvalue=1.946008484036593e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.690271801081509, 3.2191241476493386e-107), 'spearman': SpearmanrResult(correlation=0.672895917500917, pvalue=5.083074545589983e-100), 'nsamples': 750}, 'OnWN': {'pearson': (0.571596370346218, 5.6187139372466735e-50), 'spearman': SpearmanrResult(correlation=0.5957960855291605, pvalue=3.377324001088102e-55), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5623811464141381, 'wmean': 0.6124976267948906}, 'spearman': {'mean': 0.5701971391500705, 'wmean': 0.6149550209553015}}}, 'STS14': {'deft-forum': {'pearson': (0.3827666667978496, 3.7677858449888216e-17), 'spearman': SpearmanrResult(correlation=0.38546425068097245, pvalue=2.1708214775948407e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7467024938725054, 1.0514944346897839e-54), 'spearman': SpearmanrResult(correlation=0.7024748795123987, pvalue=6.344709222693408e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.6469557465928595, 3.8046215934865256e-90), 'spearman': SpearmanrResult(correlation=0.6039860072722735, pvalue=9.366313191895434e-76), 'nsamples': 750}, 'images': {'pearson': (0.7087416390579239, 1.8831326724553154e-115), 'spearman': SpearmanrResult(correlation=0.6899654937576177, pvalue=4.3555704508771804e-107), 'nsamples': 750}, 'OnWN': {'pearson': (0.6846833592145315, 7.548782574138826e-105), 'spearman': SpearmanrResult(correlation=0.7196466845456945, pvalue=1.268786785792341e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6496671138525747, 3.92144015824993e-91), 'spearman': SpearmanrResult(correlation=0.6281197076065732, pvalue=1.4702439996951559e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6365861698980408, 'wmean': 0.6436777712691204}, 'spearman': {'mean': 0.6216095038959216, 'wmean': 0.6307972790791404}}}, 'STS15': {'answers-forums': {'pearson': (0.5551827589729114, 1.0517313547133567e-31), 'spearman': SpearmanrResult(correlation=0.5418333064076911, pvalue=5.358586348223899e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.7173867690970621, 1.567515273953861e-119), 'spearman': SpearmanrResult(correlation=0.7242493796994316, pvalue=7.015074220285493e-123), 'nsamples': 750}, 'belief': {'pearson': (0.6450573693302987, 1.6859663537815476e-45), 'spearman': SpearmanrResult(correlation=0.6616500523001532, pvalue=1.416121536827611e-48), 'nsamples': 375}, 'headlines': {'pearson': (0.7095595181118662, 7.858866901025584e-116), 'spearman': SpearmanrResult(correlation=0.700418358185016, pvalue=1.1572694613856191e-111), 'nsamples': 750}, 'images': {'pearson': (0.7803546149165392, 1.1632592325966045e-154), 'spearman': SpearmanrResult(correlation=0.7899590640997806, pvalue=4.697704264242906e-161), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6815082060857355, 'wmean': 0.7018552415692682}, 'spearman': {'mean': 0.6836220321384145, 'wmean': 0.7040921203345376}}}, 'STS16': {'answer-answer': {'pearson': (0.5256217218075189, 1.9133510522037725e-19), 'spearman': SpearmanrResult(correlation=0.5424816530665527, pvalue=7.755487490876374e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.7111998785353347, 1.1165787107229894e-39), 'spearman': SpearmanrResult(correlation=0.7192786186352075, pvalue=5.941953927068815e-41), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7487119767620494, 1.3683028733240205e-42), 'spearman': SpearmanrResult(correlation=0.7507367839239083, pvalue=6.191137767674019e-43), 'nsamples': 230}, 'postediting': {'pearson': (0.8184278016230145, 3.6637470625449766e-60), 'spearman': SpearmanrResult(correlation=0.8276298713666982, pvalue=1.233995052607146e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.5395764957706058, 3.4616853723786986e-17), 'spearman': SpearmanrResult(correlation=0.5366275509096499, pvalue=5.5257343560258585e-17), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6687075748997047, 'wmean': 0.6705466382476827}, 'spearman': {'mean': 0.6753508955804033, 'wmean': 0.6776197495574713}}}, 'MR': {'devacc': 75.13, 'acc': 74.4, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.03, 'acc': 79.28, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.19, 'acc': 87.52, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.27, 'acc': 91.64, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.93, 'acc': 80.12, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.78, 'acc': 39.41, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 78.08, 'acc': 87.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.87, 'acc': 73.91, 'f1': 81.34, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.0, 'acc': 77.43, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8431614377116466, 'pearson': 0.8299549852547955, 'spearman': 0.7503269631005324, 'mse': 0.31694086510680375, 'yhat': array([2.95292809, 4.28523876, 1.23541864, ..., 3.40440985, 4.36634849,        4.54639145]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7571468299593779, 'pearson': 0.7229356882064915, 'spearman': 0.7213734873873658, 'mse': 1.2983851384470984, 'yhat': array([1.91471128, 1.09540215, 2.47946832, ..., 3.91209357, 3.87048943,        3.32925831]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.68, 'acc': 66.81, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 94.9, 'acc': 95.2, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 87.02, 'acc': 87.09, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.67, 'acc': 32.41, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.31, 'acc': 57.62, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 53.22, 'acc': 53.16, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.2, 'acc': 84.18, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.11, 'acc': 79.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.3, 'acc': 82.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.64, 'acc': 51.54, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.17, 'acc': 52.17, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 17:51:05,612 : STS12 p=0.5761, STS12 s=0.5934, STS13 p=0.6125, STS13 s=0.6150, STS14 p=0.6437, STS14 s=0.6308, STS15 p=0.7019, STS15 s=0.7041, STS 16 p=0.6705, STS16 s=0.6776, STS B p=0.7229, STS B s=0.7214, STS B m=1.2984, SICK-R p=0.8300, SICK-R s=0.7503, SICK-P m=0.3169
2019-03-12 17:51:05,612 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 17:51:05,612 : 0.5761,0.5934,0.6125,0.6150,0.6437,0.6308,0.7019,0.7041,0.6705,0.6776,0.7229,0.7214,1.2984,0.8300,0.7503,0.3169
2019-03-12 17:51:05,612 : MR=74.40, CR=79.28, SUBJ=91.64, MPQA=87.52, SST-B=80.12, SST-F=39.41, TREC=87.60, SICK-E=77.43, SNLI=66.81, MRPC=73.91, MRPC f=81.34
2019-03-12 17:51:05,612 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 17:51:05,612 : 74.40,79.28,91.64,87.52,80.12,39.41,87.60,77.43,66.81,73.91,81.34
2019-03-12 17:51:05,612 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 17:51:05,612 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 17:51:05,612 : na,na,na,na,na,na,na,na,na,na
2019-03-12 17:51:05,612 : SentLen=95.20, WC=87.09, TreeDepth=32.41, TopConst=57.62, BShift=53.16, Tense=84.18, SubjNum=79.38, ObjNum=82.00, SOMO=51.54, CoordInv=52.17, average=67.47
2019-03-12 17:51:05,612 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 17:51:05,612 : 95.20,87.09,32.41,57.62,53.16,84.18,79.38,82.00,51.54,52.17,67.47
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,612 : layer 3
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,612 : ********************************************************************************
2019-03-12 17:51:05,699 : ***** Transfer task : STS12 *****


2019-03-12 17:51:05,711 : loading BERT model bert-large-uncased
2019-03-12 17:51:05,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:05,728 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:05,729 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqhzbgqmx
2019-03-12 17:51:13,185 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:51:22,596 : MSRpar : pearson = 0.4057, spearman = 0.4420
2019-03-12 17:51:24,246 : MSRvid : pearson = 0.6727, spearman = 0.6723
2019-03-12 17:51:25,668 : SMTeuroparl : pearson = 0.5023, spearman = 0.6082
2019-03-12 17:51:28,377 : surprise.OnWN : pearson = 0.6965, spearman = 0.6915
2019-03-12 17:51:29,817 : surprise.SMTnews : pearson = 0.5396, spearman = 0.5105
2019-03-12 17:51:29,817 : ALL (weighted average) : Pearson = 0.5717,             Spearman = 0.5911
2019-03-12 17:51:29,818 : ALL (average) : Pearson = 0.5633,             Spearman = 0.5849

2019-03-12 17:51:29,818 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 17:51:29,836 : loading BERT model bert-large-uncased
2019-03-12 17:51:29,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:29,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:29,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsd01x0_4
2019-03-12 17:51:37,429 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:51:44,585 : FNWN : pearson = 0.4274, spearman = 0.4365
2019-03-12 17:51:46,489 : headlines : pearson = 0.6805, spearman = 0.6623
2019-03-12 17:51:47,970 : OnWN : pearson = 0.5480, spearman = 0.5759
2019-03-12 17:51:47,970 : ALL (weighted average) : Pearson = 0.5990,             Spearman = 0.6015
2019-03-12 17:51:47,970 : ALL (average) : Pearson = 0.5520,             Spearman = 0.5582

2019-03-12 17:51:47,970 : ***** Transfer task : STS14 *****


2019-03-12 17:51:47,987 : loading BERT model bert-large-uncased
2019-03-12 17:51:47,987 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:48,004 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:48,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprld8gs08
2019-03-12 17:51:55,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:52:02,718 : deft-forum : pearson = 0.3791, spearman = 0.3832
2019-03-12 17:52:04,365 : deft-news : pearson = 0.7444, spearman = 0.7005
2019-03-12 17:52:06,554 : headlines : pearson = 0.6412, spearman = 0.5982
2019-03-12 17:52:08,645 : images : pearson = 0.6923, spearman = 0.6775
2019-03-12 17:52:10,786 : OnWN : pearson = 0.6731, spearman = 0.7111
2019-03-12 17:52:13,659 : tweet-news : pearson = 0.6432, spearman = 0.6209
2019-03-12 17:52:13,659 : ALL (weighted average) : Pearson = 0.6350,             Spearman = 0.6236
2019-03-12 17:52:13,659 : ALL (average) : Pearson = 0.6289,             Spearman = 0.6152

2019-03-12 17:52:13,659 : ***** Transfer task : STS15 *****


2019-03-12 17:52:13,693 : loading BERT model bert-large-uncased
2019-03-12 17:52:13,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:52:13,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:52:13,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9cfvx2ws
2019-03-12 17:52:21,230 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:52:28,542 : answers-forums : pearson = 0.5376, spearman = 0.5311
2019-03-12 17:52:30,643 : answers-students : pearson = 0.7097, spearman = 0.7143
2019-03-12 17:52:32,703 : belief : pearson = 0.6377, spearman = 0.6647
2019-03-12 17:52:34,964 : headlines : pearson = 0.7008, spearman = 0.6898
2019-03-12 17:52:37,109 : images : pearson = 0.7692, spearman = 0.7775
2019-03-12 17:52:37,110 : ALL (weighted average) : Pearson = 0.6918,             Spearman = 0.6949
2019-03-12 17:52:37,110 : ALL (average) : Pearson = 0.6710,             Spearman = 0.6755

2019-03-12 17:52:37,110 : ***** Transfer task : STS16 *****


2019-03-12 17:52:37,180 : loading BERT model bert-large-uncased
2019-03-12 17:52:37,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:52:37,198 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:52:37,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4ovsk35h
2019-03-12 17:52:44,678 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:52:50,730 : answer-answer : pearson = 0.4968, spearman = 0.5187
2019-03-12 17:52:51,394 : headlines : pearson = 0.7025, spearman = 0.7083
2019-03-12 17:52:52,282 : plagiarism : pearson = 0.7556, spearman = 0.7590
2019-03-12 17:52:53,782 : postediting : pearson = 0.8158, spearman = 0.8250
2019-03-12 17:52:54,390 : question-question : pearson = 0.5342, spearman = 0.5272
2019-03-12 17:52:54,390 : ALL (weighted average) : Pearson = 0.6624,             Spearman = 0.6696
2019-03-12 17:52:54,390 : ALL (average) : Pearson = 0.6610,             Spearman = 0.6676

2019-03-12 17:52:54,391 : ***** Transfer task : MR *****


2019-03-12 17:52:54,409 : loading BERT model bert-large-uncased
2019-03-12 17:52:54,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:52:54,428 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:52:54,428 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp03m8soyy
2019-03-12 17:53:01,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:53:07,139 : Generating sentence embeddings
2019-03-12 17:53:38,793 : Generated sentence embeddings
2019-03-12 17:53:38,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:53:48,861 : Best param found at split 1: l2reg = 0.001                 with score 75.63
2019-03-12 17:53:58,961 : Best param found at split 2: l2reg = 0.001                 with score 75.56
2019-03-12 17:54:09,649 : Best param found at split 3: l2reg = 0.001                 with score 75.87
2019-03-12 17:54:18,049 : Best param found at split 4: l2reg = 0.001                 with score 75.22
2019-03-12 17:54:28,008 : Best param found at split 5: l2reg = 0.001                 with score 74.63
2019-03-12 17:54:28,795 : Dev acc : 75.38 Test acc : 75.28

2019-03-12 17:54:28,796 : ***** Transfer task : CR *****


2019-03-12 17:54:28,804 : loading BERT model bert-large-uncased
2019-03-12 17:54:28,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:54:28,823 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:54:28,823 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpecj4l0hw
2019-03-12 17:54:36,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:54:41,538 : Generating sentence embeddings
2019-03-12 17:54:49,887 : Generated sentence embeddings
2019-03-12 17:54:49,888 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:54:53,264 : Best param found at split 1: l2reg = 0.001                 with score 79.96
2019-03-12 17:54:56,654 : Best param found at split 2: l2reg = 0.01                 with score 79.89
2019-03-12 17:55:00,566 : Best param found at split 3: l2reg = 1e-05                 with score 79.9
2019-03-12 17:55:03,801 : Best param found at split 4: l2reg = 0.001                 with score 79.87
2019-03-12 17:55:06,587 : Best param found at split 5: l2reg = 0.001                 with score 80.47
2019-03-12 17:55:06,728 : Dev acc : 80.02 Test acc : 79.18

2019-03-12 17:55:06,729 : ***** Transfer task : MPQA *****


2019-03-12 17:55:06,735 : loading BERT model bert-large-uncased
2019-03-12 17:55:06,735 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:55:06,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:55:06,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph55dbg6x
2019-03-12 17:55:14,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:55:19,566 : Generating sentence embeddings
2019-03-12 17:55:27,188 : Generated sentence embeddings
2019-03-12 17:55:27,188 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:55:37,482 : Best param found at split 1: l2reg = 0.001                 with score 87.19
2019-03-12 17:55:47,959 : Best param found at split 2: l2reg = 0.001                 with score 86.98
2019-03-12 17:56:00,422 : Best param found at split 3: l2reg = 0.001                 with score 87.86
2019-03-12 17:56:09,114 : Best param found at split 4: l2reg = 0.001                 with score 87.73
2019-03-12 17:56:20,301 : Best param found at split 5: l2reg = 0.0001                 with score 86.79
2019-03-12 17:56:20,815 : Dev acc : 87.31 Test acc : 87.66

2019-03-12 17:56:20,816 : ***** Transfer task : SUBJ *****


2019-03-12 17:56:20,830 : loading BERT model bert-large-uncased
2019-03-12 17:56:20,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:56:20,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:56:20,851 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb_q4f44n
2019-03-12 17:56:28,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:33,627 : Generating sentence embeddings
2019-03-12 17:57:04,645 : Generated sentence embeddings
2019-03-12 17:57:04,646 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:57:11,503 : Best param found at split 1: l2reg = 0.001                 with score 92.6
2019-03-12 17:57:18,720 : Best param found at split 2: l2reg = 0.001                 with score 92.75
2019-03-12 17:57:26,403 : Best param found at split 3: l2reg = 0.001                 with score 92.44
2019-03-12 17:57:35,651 : Best param found at split 4: l2reg = 0.001                 with score 92.81
2019-03-12 17:57:45,764 : Best param found at split 5: l2reg = 0.001                 with score 92.45
2019-03-12 17:57:46,562 : Dev acc : 92.61 Test acc : 92.56

2019-03-12 17:57:46,563 : ***** Transfer task : SST Binary classification *****


2019-03-12 17:57:46,656 : loading BERT model bert-large-uncased
2019-03-12 17:57:46,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:46,729 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:46,730 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj86wsda1
2019-03-12 17:57:54,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:57:59,675 : Computing embedding for train
2019-03-12 17:59:40,486 : Computed train embeddings
2019-03-12 17:59:40,486 : Computing embedding for dev
2019-03-12 17:59:42,684 : Computed dev embeddings
2019-03-12 17:59:42,684 : Computing embedding for test
2019-03-12 17:59:47,309 : Computed test embeddings
2019-03-12 17:59:47,309 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:00:02,257 : [('reg:1e-05', 78.67), ('reg:0.0001', 78.9), ('reg:0.001', 79.24), ('reg:0.01', 77.64)]
2019-03-12 18:00:02,257 : Validation : best param found is reg = 0.001 with score             79.24
2019-03-12 18:00:02,257 : Evaluating...
2019-03-12 18:00:06,555 : 
Dev acc : 79.24 Test acc : 79.85 for             SST Binary classification

2019-03-12 18:00:06,556 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 18:00:06,606 : loading BERT model bert-large-uncased
2019-03-12 18:00:06,606 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:00:06,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:00:06,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1jebkpgs
2019-03-12 18:00:14,102 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:00:19,313 : Computing embedding for train
2019-03-12 18:00:41,384 : Computed train embeddings
2019-03-12 18:00:41,384 : Computing embedding for dev
2019-03-12 18:00:44,269 : Computed dev embeddings
2019-03-12 18:00:44,269 : Computing embedding for test
2019-03-12 18:00:49,952 : Computed test embeddings
2019-03-12 18:00:49,953 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:00:52,147 : [('reg:1e-05', 38.96), ('reg:0.0001', 38.96), ('reg:0.001', 39.15), ('reg:0.01', 38.78)]
2019-03-12 18:00:52,147 : Validation : best param found is reg = 0.001 with score             39.15
2019-03-12 18:00:52,147 : Evaluating...
2019-03-12 18:00:52,697 : 
Dev acc : 39.15 Test acc : 41.36 for             SST Fine-Grained classification

2019-03-12 18:00:52,697 : ***** Transfer task : TREC *****


2019-03-12 18:00:52,710 : loading BERT model bert-large-uncased
2019-03-12 18:00:52,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:00:52,729 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:00:52,729 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpex5c2bl0
2019-03-12 18:01:00,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:01:13,052 : Computed train embeddings
2019-03-12 18:01:13,645 : Computed test embeddings
2019-03-12 18:01:13,645 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 18:01:20,149 : [('reg:1e-05', 79.05), ('reg:0.0001', 79.33), ('reg:0.001', 78.41), ('reg:0.01', 70.86)]
2019-03-12 18:01:20,149 : Cross-validation : best param found is reg = 0.0001             with score 79.33
2019-03-12 18:01:20,149 : Evaluating...
2019-03-12 18:01:20,480 : 
Dev acc : 79.33 Test acc : 89.6             for TREC

2019-03-12 18:01:20,481 : ***** Transfer task : MRPC *****


2019-03-12 18:01:20,503 : loading BERT model bert-large-uncased
2019-03-12 18:01:20,503 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:01:20,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:01:20,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9_a4jbx7
2019-03-12 18:01:28,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:01:33,250 : Computing embedding for train
2019-03-12 18:01:55,640 : Computed train embeddings
2019-03-12 18:01:55,640 : Computing embedding for test
2019-03-12 18:02:05,461 : Computed test embeddings
2019-03-12 18:02:05,482 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 18:02:10,078 : [('reg:1e-05', 72.87), ('reg:0.0001', 72.91), ('reg:0.001', 72.92), ('reg:0.01', 71.96)]
2019-03-12 18:02:10,078 : Cross-validation : best param found is reg = 0.001             with score 72.92
2019-03-12 18:02:10,078 : Evaluating...
2019-03-12 18:02:10,338 : Dev acc : 72.92 Test acc 72.29; Test F1 78.96 for MRPC.

2019-03-12 18:02:10,339 : ***** Transfer task : SICK-Entailment*****


2019-03-12 18:02:10,400 : loading BERT model bert-large-uncased
2019-03-12 18:02:10,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:02:10,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:02:10,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmper1k50dt
2019-03-12 18:02:17,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:02:23,040 : Computing embedding for train
2019-03-12 18:02:34,416 : Computed train embeddings
2019-03-12 18:02:34,416 : Computing embedding for dev
2019-03-12 18:02:35,967 : Computed dev embeddings
2019-03-12 18:02:35,967 : Computing embedding for test
2019-03-12 18:02:48,169 : Computed test embeddings
2019-03-12 18:02:48,205 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:02:49,602 : [('reg:1e-05', 78.8), ('reg:0.0001', 77.0), ('reg:0.001', 77.8), ('reg:0.01', 75.8)]
2019-03-12 18:02:49,603 : Validation : best param found is reg = 1e-05 with score             78.8
2019-03-12 18:02:49,603 : Evaluating...
2019-03-12 18:02:50,042 : 
Dev acc : 78.8 Test acc : 78.57 for                        SICK entailment

2019-03-12 18:02:50,042 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 18:02:50,069 : loading BERT model bert-large-uncased
2019-03-12 18:02:50,069 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:02:50,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:02:50,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi774feiw
2019-03-12 18:02:57,615 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:03:02,870 : Computing embedding for train
2019-03-12 18:03:14,259 : Computed train embeddings
2019-03-12 18:03:14,259 : Computing embedding for dev
2019-03-12 18:03:15,811 : Computed dev embeddings
2019-03-12 18:03:15,812 : Computing embedding for test
2019-03-12 18:03:28,013 : Computed test embeddings
2019-03-12 18:03:42,967 : Dev : Pearson 0.8254411416021022
2019-03-12 18:03:42,967 : Test : Pearson 0.8263719509304407 Spearman 0.7441235378498161 MSE 0.32270641654821064                        for SICK Relatedness

2019-03-12 18:03:42,970 : 

***** Transfer task : STSBenchmark*****


2019-03-12 18:03:43,038 : loading BERT model bert-large-uncased
2019-03-12 18:03:43,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:03:43,057 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:03:43,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjl0daqx0
2019-03-12 18:03:50,511 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:03:55,782 : Computing embedding for train
2019-03-12 18:04:14,488 : Computed train embeddings
2019-03-12 18:04:14,489 : Computing embedding for dev
2019-03-12 18:04:20,172 : Computed dev embeddings
2019-03-12 18:04:20,172 : Computing embedding for test
2019-03-12 18:04:24,811 : Computed test embeddings
2019-03-12 18:04:43,824 : Dev : Pearson 0.755704413527787
2019-03-12 18:04:43,824 : Test : Pearson 0.7213009127116944 Spearman 0.7209462139765899 MSE 1.3397562806194179                        for SICK Relatedness

2019-03-12 18:04:43,824 : ***** Transfer task : SNLI Entailment*****


2019-03-12 18:04:48,856 : loading BERT model bert-large-uncased
2019-03-12 18:04:48,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:04:48,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:04:48,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph9guk08h
2019-03-12 18:04:56,350 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:05:02,099 : PROGRESS (encoding): 0.00%
2019-03-12 18:07:49,704 : PROGRESS (encoding): 14.56%
2019-03-12 18:11:02,889 : PROGRESS (encoding): 29.12%
2019-03-12 18:14:14,733 : PROGRESS (encoding): 43.69%
2019-03-12 18:17:38,577 : PROGRESS (encoding): 58.25%
2019-03-12 18:21:25,515 : PROGRESS (encoding): 72.81%
2019-03-12 18:25:11,351 : PROGRESS (encoding): 87.37%
2019-03-12 18:29:15,578 : PROGRESS (encoding): 0.00%
2019-03-12 18:29:46,213 : PROGRESS (encoding): 0.00%
2019-03-12 18:30:15,815 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:30:47,801 : [('reg:1e-09', 67.64)]
2019-03-12 18:30:47,801 : Validation : best param found is reg = 1e-09 with score             67.64
2019-03-12 18:30:47,801 : Evaluating...
2019-03-12 18:31:21,458 : Dev acc : 67.64 Test acc : 67.88 for SNLI

2019-03-12 18:31:21,458 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 18:31:21,666 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 18:31:22,753 : loading BERT model bert-large-uncased
2019-03-12 18:31:22,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:31:22,780 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:31:22,780 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7835wm6r
2019-03-12 18:31:30,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:31:35,386 : Computing embeddings for train/dev/test
2019-03-12 18:35:09,058 : Computed embeddings
2019-03-12 18:35:09,058 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:35:34,006 : [('reg:1e-05', 93.6), ('reg:0.0001', 90.8), ('reg:0.001', 87.2), ('reg:0.01', 77.58)]
2019-03-12 18:35:34,006 : Validation : best param found is reg = 1e-05 with score             93.6
2019-03-12 18:35:34,006 : Evaluating...
2019-03-12 18:35:38,559 : 
Dev acc : 93.6 Test acc : 94.1 for LENGTH classification

2019-03-12 18:35:38,560 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 18:35:38,813 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 18:35:38,858 : loading BERT model bert-large-uncased
2019-03-12 18:35:38,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:35:38,888 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:35:38,888 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn38ifhxl
2019-03-12 18:35:46,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:35:51,514 : Computing embeddings for train/dev/test
2019-03-12 18:39:09,304 : Computed embeddings
2019-03-12 18:39:09,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:39:31,697 : [('reg:1e-05', 88.59), ('reg:0.0001', 62.79), ('reg:0.001', 5.01), ('reg:0.01', 1.14)]
2019-03-12 18:39:31,697 : Validation : best param found is reg = 1e-05 with score             88.59
2019-03-12 18:39:31,697 : Evaluating...
2019-03-12 18:39:36,243 : 
Dev acc : 88.6 Test acc : 88.7 for WORDCONTENT classification

2019-03-12 18:39:36,244 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 18:39:36,775 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 18:39:36,840 : loading BERT model bert-large-uncased
2019-03-12 18:39:36,840 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:39:36,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:39:36,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9q9j2j4l
2019-03-12 18:39:44,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:39:49,646 : Computing embeddings for train/dev/test
2019-03-12 18:42:56,390 : Computed embeddings
2019-03-12 18:42:56,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:43:22,474 : [('reg:1e-05', 33.29), ('reg:0.0001', 32.82), ('reg:0.001', 30.73), ('reg:0.01', 25.77)]
2019-03-12 18:43:22,475 : Validation : best param found is reg = 1e-05 with score             33.29
2019-03-12 18:43:22,475 : Evaluating...
2019-03-12 18:43:30,021 : 
Dev acc : 33.3 Test acc : 33.8 for DEPTH classification

2019-03-12 18:43:30,022 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 18:43:30,390 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 18:43:30,453 : loading BERT model bert-large-uncased
2019-03-12 18:43:30,453 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:43:30,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:43:30,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt8sag9ow
2019-03-12 18:43:38,026 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:43:43,288 : Computing embeddings for train/dev/test
2019-03-12 18:46:35,201 : Computed embeddings
2019-03-12 18:46:35,201 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:47:03,835 : [('reg:1e-05', 64.65), ('reg:0.0001', 62.8), ('reg:0.001', 54.53), ('reg:0.01', 38.74)]
2019-03-12 18:47:03,835 : Validation : best param found is reg = 1e-05 with score             64.65
2019-03-12 18:47:03,835 : Evaluating...
2019-03-12 18:47:10,279 : 
Dev acc : 64.7 Test acc : 65.6 for TOPCONSTITUENTS classification

2019-03-12 18:47:10,280 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 18:47:10,653 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 18:47:10,719 : loading BERT model bert-large-uncased
2019-03-12 18:47:10,719 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:47:10,748 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:47:10,749 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeqna9sw5
2019-03-12 18:47:18,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:47:23,424 : Computing embeddings for train/dev/test
2019-03-12 18:50:30,327 : Computed embeddings
2019-03-12 18:50:30,327 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:51:03,663 : [('reg:1e-05', 62.27), ('reg:0.0001', 62.18), ('reg:0.001', 60.59), ('reg:0.01', 56.45)]
2019-03-12 18:51:03,663 : Validation : best param found is reg = 1e-05 with score             62.27
2019-03-12 18:51:03,663 : Evaluating...
2019-03-12 18:51:12,037 : 
Dev acc : 62.3 Test acc : 62.3 for BIGRAMSHIFT classification

2019-03-12 18:51:12,038 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 18:51:12,424 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 18:51:12,489 : loading BERT model bert-large-uncased
2019-03-12 18:51:12,489 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:51:12,519 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:51:12,519 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo13iutq_
2019-03-12 18:51:19,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:51:25,313 : Computing embeddings for train/dev/test
2019-03-12 18:54:27,627 : Computed embeddings
2019-03-12 18:54:27,627 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:54:51,627 : [('reg:1e-05', 85.29), ('reg:0.0001', 85.5), ('reg:0.001', 86.15), ('reg:0.01', 86.15)]
2019-03-12 18:54:51,627 : Validation : best param found is reg = 0.001 with score             86.15
2019-03-12 18:54:51,627 : Evaluating...
2019-03-12 18:54:58,237 : 
Dev acc : 86.2 Test acc : 84.5 for TENSE classification

2019-03-12 18:54:58,238 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 18:54:58,641 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 18:54:58,703 : loading BERT model bert-large-uncased
2019-03-12 18:54:58,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:54:58,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:54:58,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppa91ji6x
2019-03-12 18:55:06,272 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:55:11,555 : Computing embeddings for train/dev/test
2019-03-12 18:58:24,971 : Computed embeddings
2019-03-12 18:58:24,971 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:58:55,237 : [('reg:1e-05', 81.36), ('reg:0.0001', 81.24), ('reg:0.001', 81.14), ('reg:0.01', 79.3)]
2019-03-12 18:58:55,237 : Validation : best param found is reg = 1e-05 with score             81.36
2019-03-12 18:58:55,238 : Evaluating...
2019-03-12 18:59:01,753 : 
Dev acc : 81.4 Test acc : 79.9 for SUBJNUMBER classification

2019-03-12 18:59:01,754 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 18:59:02,196 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 18:59:02,268 : loading BERT model bert-large-uncased
2019-03-12 18:59:02,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:59:02,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:59:02,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy4wt3sx5
2019-03-12 18:59:09,983 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:59:15,363 : Computing embeddings for train/dev/test
2019-03-12 19:02:24,536 : Computed embeddings
2019-03-12 19:02:24,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:02:48,133 : [('reg:1e-05', 79.92), ('reg:0.0001', 80.08), ('reg:0.001', 79.9), ('reg:0.01', 78.41)]
2019-03-12 19:02:48,134 : Validation : best param found is reg = 0.0001 with score             80.08
2019-03-12 19:02:48,134 : Evaluating...
2019-03-12 19:02:54,551 : 
Dev acc : 80.1 Test acc : 81.7 for OBJNUMBER classification

2019-03-12 19:02:54,552 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 19:02:55,112 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 19:02:55,181 : loading BERT model bert-large-uncased
2019-03-12 19:02:55,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:02:55,208 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:02:55,208 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph2ibi122
2019-03-12 19:03:02,676 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:03:07,989 : Computing embeddings for train/dev/test
2019-03-12 19:06:47,254 : Computed embeddings
2019-03-12 19:06:47,254 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:07:14,750 : [('reg:1e-05', 52.61), ('reg:0.0001', 52.69), ('reg:0.001', 53.23), ('reg:0.01', 52.2)]
2019-03-12 19:07:14,750 : Validation : best param found is reg = 0.001 with score             53.23
2019-03-12 19:07:14,750 : Evaluating...
2019-03-12 19:07:22,260 : 
Dev acc : 53.2 Test acc : 52.5 for ODDMANOUT classification

2019-03-12 19:07:22,261 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 19:07:22,635 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 19:07:22,716 : loading BERT model bert-large-uncased
2019-03-12 19:07:22,716 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:07:22,745 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:07:22,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmbkcc453
2019-03-12 19:07:30,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:07:35,638 : Computing embeddings for train/dev/test
2019-03-12 19:11:12,772 : Computed embeddings
2019-03-12 19:11:12,772 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:11:46,734 : [('reg:1e-05', 56.92), ('reg:0.0001', 56.79), ('reg:0.001', 55.54), ('reg:0.01', 52.48)]
2019-03-12 19:11:46,735 : Validation : best param found is reg = 1e-05 with score             56.92
2019-03-12 19:11:46,735 : Evaluating...
2019-03-12 19:11:55,344 : 
Dev acc : 56.9 Test acc : 56.0 for COORDINATIONINVERSION classification

2019-03-12 19:11:55,346 : total results: {'STS12': {'MSRpar': {'pearson': (0.4056868923806338, 4.424069138732259e-31), 'spearman': SpearmanrResult(correlation=0.44195422560993014, pvalue=3.3073375470659672e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6726653539634431, 6.285425338914557e-100), 'spearman': SpearmanrResult(correlation=0.672303801560192, pvalue=8.765168993729398e-100), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5022686601079528, 1.0428704979995864e-30), 'spearman': SpearmanrResult(correlation=0.6081592569849047, pvalue=9.075285911363852e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6965126393293711, 6.255263505985522e-110), 'spearman': SpearmanrResult(correlation=0.6914723650915854, pvalue=9.80611193859378e-108), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5395909075152121, 1.5908083181591778e-31), 'spearman': SpearmanrResult(correlation=0.5105465018145896, pvalue=7.195158318894088e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5633448906593225, 'wmean': 0.5717460589907356}, 'spearman': {'mean': 0.5848872302122403, 'wmean': 0.5911039084222565}}}, 'STS13': {'FNWN': {'pearson': (0.4273527866828245, 8.630533774218012e-10), 'spearman': SpearmanrResult(correlation=0.4365013032228351, pvalue=3.410220300170888e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6804718988047448, 4.2563966380130075e-103), 'spearman': SpearmanrResult(correlation=0.6622935532319324, pvalue=7.268763185507433e-96), 'nsamples': 750}, 'OnWN': {'pearson': (0.5480385907713743, 2.705705383434135e-45), 'spearman': SpearmanrResult(correlation=0.5758937947242138, pvalue=7.146454730867879e-51), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5519544254196479, 'wmean': 0.5990488334729023}, 'spearman': {'mean': 0.5582295503929937, 'wmean': 0.6015302200488993}}}, 'STS14': {'deft-forum': {'pearson': (0.3790508662929694, 7.985849955748377e-17), 'spearman': SpearmanrResult(correlation=0.3832482982262954, pvalue=3.415817170471392e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7443509846342178, 3.4194336507725686e-54), 'spearman': SpearmanrResult(correlation=0.7004612616023315, pvalue=1.4572511673609257e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.6412472342577024, 4.221808247773925e-88), 'spearman': SpearmanrResult(correlation=0.5981984932766526, pvalue=5.561317697874759e-74), 'nsamples': 750}, 'images': {'pearson': (0.6923194761061298, 4.2238938920404456e-108), 'spearman': SpearmanrResult(correlation=0.6774675804750443, pvalue=7.250440576936676e-102), 'nsamples': 750}, 'OnWN': {'pearson': (0.6730736757809411, 4.3149849618707e-100), 'spearman': SpearmanrResult(correlation=0.7110585456978217, pvalue=1.5715244080912847e-116), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6431759618758245, 8.69712079752867e-89), 'spearman': SpearmanrResult(correlation=0.620941989228869, pvalue=3.6293918458937516e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6288696998246309, 'wmean': 0.6349974523300133}, 'spearman': {'mean': 0.6152293614178358, 'wmean': 0.6235600184510195}}}, 'STS15': {'answers-forums': {'pearson': (0.5375863156707764, 1.8043935664078323e-29), 'spearman': SpearmanrResult(correlation=0.5311159330960535, pvalue=1.110256139524108e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.7097434926830458, 6.453686270512846e-116), 'spearman': SpearmanrResult(correlation=0.7142760536082504, pvalue=4.793700605482758e-118), 'nsamples': 750}, 'belief': {'pearson': (0.6377098121436529, 3.378623995805306e-44), 'spearman': SpearmanrResult(correlation=0.6646608293586274, pvalue=3.730927138445623e-49), 'nsamples': 375}, 'headlines': {'pearson': (0.7007507793967561, 8.21577777263512e-112), 'spearman': SpearmanrResult(correlation=0.6898189519194027, pvalue=5.032762200913873e-107), 'nsamples': 750}, 'images': {'pearson': (0.7691667863523279, 1.3107231286330683e-147), 'spearman': SpearmanrResult(correlation=0.7775079278606637, pvalue=7.923514920276418e-153), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6709914372493119, 'wmean': 0.6918272805848361}, 'spearman': {'mean': 0.6754759391685996, 'wmean': 0.6948728286539143}}}, 'STS16': {'answer-answer': {'pearson': (0.49683410113404564, 3.069170028526398e-17), 'spearman': SpearmanrResult(correlation=0.5186525653630465, pvalue=6.838431380660635e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7024751146852555, 2.373292144517125e-38), 'spearman': SpearmanrResult(correlation=0.7082787772558686, pvalue=3.1462131657588674e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7555556579966992, 9.09086794881091e-44), 'spearman': SpearmanrResult(correlation=0.7589944837529524, pvalue=2.2496016094871176e-44), 'nsamples': 230}, 'postediting': {'pearson': (0.815835057174027, 1.7194044278916707e-59), 'spearman': SpearmanrResult(correlation=0.824999075741483, pvalue=6.500712307571033e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.5342373502145177, 8.045638197139905e-17), 'spearman': SpearmanrResult(correlation=0.5271593264932184, pvalue=2.405805079184242e-16), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6609874562409089, 'wmean': 0.6624024677312089}, 'spearman': {'mean': 0.6676168457213137, 'wmean': 0.6695986274199904}}}, 'MR': {'devacc': 75.38, 'acc': 75.28, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.02, 'acc': 79.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.31, 'acc': 87.66, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.61, 'acc': 92.56, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.24, 'acc': 79.85, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.15, 'acc': 41.36, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.33, 'acc': 89.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.92, 'acc': 72.29, 'f1': 78.96, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.8, 'acc': 78.57, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8254411416021022, 'pearson': 0.8263719509304407, 'spearman': 0.7441235378498161, 'mse': 0.32270641654821064, 'yhat': array([3.4287564 , 4.52870369, 1.30684858, ..., 3.62642465, 4.28992659,        4.56757607]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.755704413527787, 'pearson': 0.7213009127116944, 'spearman': 0.7209462139765899, 'mse': 1.3397562806194179, 'yhat': array([1.59553094, 1.68301558, 2.05439807, ..., 3.75680432, 3.84525169,        3.41069025]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.64, 'acc': 67.88, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 93.6, 'acc': 94.13, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 88.59, 'acc': 88.66, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.29, 'acc': 33.8, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 64.65, 'acc': 65.59, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 62.27, 'acc': 62.26, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.15, 'acc': 84.49, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.36, 'acc': 79.86, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.08, 'acc': 81.71, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.23, 'acc': 52.5, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.92, 'acc': 55.95, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 19:11:55,346 : STS12 p=0.5717, STS12 s=0.5911, STS13 p=0.5990, STS13 s=0.6015, STS14 p=0.6350, STS14 s=0.6236, STS15 p=0.6918, STS15 s=0.6949, STS 16 p=0.6624, STS16 s=0.6696, STS B p=0.7213, STS B s=0.7209, STS B m=1.3398, SICK-R p=0.8264, SICK-R s=0.7441, SICK-P m=0.3227
2019-03-12 19:11:55,346 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 19:11:55,346 : 0.5717,0.5911,0.5990,0.6015,0.6350,0.6236,0.6918,0.6949,0.6624,0.6696,0.7213,0.7209,1.3398,0.8264,0.7441,0.3227
2019-03-12 19:11:55,346 : MR=75.28, CR=79.18, SUBJ=92.56, MPQA=87.66, SST-B=79.85, SST-F=41.36, TREC=89.60, SICK-E=78.57, SNLI=67.88, MRPC=72.29, MRPC f=78.96
2019-03-12 19:11:55,346 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 19:11:55,346 : 75.28,79.18,92.56,87.66,79.85,41.36,89.60,78.57,67.88,72.29,78.96
2019-03-12 19:11:55,346 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 19:11:55,346 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 19:11:55,346 : na,na,na,na,na,na,na,na,na,na
2019-03-12 19:11:55,346 : SentLen=94.13, WC=88.66, TreeDepth=33.80, TopConst=65.59, BShift=62.26, Tense=84.49, SubjNum=79.86, ObjNum=81.71, SOMO=52.50, CoordInv=55.95, average=69.90
2019-03-12 19:11:55,346 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 19:11:55,346 : 94.13,88.66,33.80,65.59,62.26,84.49,79.86,81.71,52.50,55.95,69.90
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,346 : layer 4
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,346 : ********************************************************************************
2019-03-12 19:11:55,438 : ***** Transfer task : STS12 *****


2019-03-12 19:11:55,450 : loading BERT model bert-large-uncased
2019-03-12 19:11:55,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:11:55,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:11:55,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvwk6vyrn
2019-03-12 19:12:02,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:12:12,432 : MSRpar : pearson = 0.4116, spearman = 0.4432
2019-03-12 19:12:14,083 : MSRvid : pearson = 0.6550, spearman = 0.6561
2019-03-12 19:12:15,502 : SMTeuroparl : pearson = 0.4986, spearman = 0.5970
2019-03-12 19:12:18,217 : surprise.OnWN : pearson = 0.7053, spearman = 0.6943
2019-03-12 19:12:19,652 : surprise.SMTnews : pearson = 0.5708, spearman = 0.5404
2019-03-12 19:12:19,652 : ALL (weighted average) : Pearson = 0.5745,             Spearman = 0.5904
2019-03-12 19:12:19,652 : ALL (average) : Pearson = 0.5683,             Spearman = 0.5862

2019-03-12 19:12:19,652 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 19:12:19,661 : loading BERT model bert-large-uncased
2019-03-12 19:12:19,661 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:12:19,678 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:12:19,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy9j_yrcb
2019-03-12 19:12:27,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:12:33,908 : FNWN : pearson = 0.4227, spearman = 0.4512
2019-03-12 19:12:35,811 : headlines : pearson = 0.6772, spearman = 0.6573
2019-03-12 19:12:37,287 : OnWN : pearson = 0.5575, spearman = 0.5829
2019-03-12 19:12:37,287 : ALL (weighted average) : Pearson = 0.6004,             Spearman = 0.6035
2019-03-12 19:12:37,287 : ALL (average) : Pearson = 0.5525,             Spearman = 0.5638

2019-03-12 19:12:37,287 : ***** Transfer task : STS14 *****


2019-03-12 19:12:37,302 : loading BERT model bert-large-uncased
2019-03-12 19:12:37,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:12:37,320 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:12:37,320 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphhjmvdug
2019-03-12 19:12:44,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:12:51,506 : deft-forum : pearson = 0.3909, spearman = 0.3902
2019-03-12 19:12:53,154 : deft-news : pearson = 0.7455, spearman = 0.7011
2019-03-12 19:12:55,336 : headlines : pearson = 0.6368, spearman = 0.5901
2019-03-12 19:12:57,425 : images : pearson = 0.6888, spearman = 0.6726
2019-03-12 19:12:59,568 : OnWN : pearson = 0.6873, spearman = 0.7223
2019-03-12 19:13:02,444 : tweet-news : pearson = 0.6775, spearman = 0.6467
2019-03-12 19:13:02,444 : ALL (weighted average) : Pearson = 0.6446,             Spearman = 0.6292
2019-03-12 19:13:02,444 : ALL (average) : Pearson = 0.6378,             Spearman = 0.6205

2019-03-12 19:13:02,445 : ***** Transfer task : STS15 *****


2019-03-12 19:13:02,487 : loading BERT model bert-large-uncased
2019-03-12 19:13:02,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:02,511 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:02,511 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr0q85rcg
2019-03-12 19:13:10,100 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:18,369 : answers-forums : pearson = 0.5675, spearman = 0.5544
2019-03-12 19:13:20,469 : answers-students : pearson = 0.7159, spearman = 0.7202
2019-03-12 19:13:22,527 : belief : pearson = 0.6667, spearman = 0.6908
2019-03-12 19:13:24,801 : headlines : pearson = 0.6961, spearman = 0.6832
2019-03-12 19:13:26,945 : images : pearson = 0.7684, spearman = 0.7760
2019-03-12 19:13:26,945 : ALL (weighted average) : Pearson = 0.6994,             Spearman = 0.7005
2019-03-12 19:13:26,945 : ALL (average) : Pearson = 0.6829,             Spearman = 0.6849

2019-03-12 19:13:26,945 : ***** Transfer task : STS16 *****


2019-03-12 19:13:27,020 : loading BERT model bert-large-uncased
2019-03-12 19:13:27,020 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:27,038 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:27,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0096jo4w
2019-03-12 19:13:34,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:40,812 : answer-answer : pearson = 0.5098, spearman = 0.5171
2019-03-12 19:13:41,475 : headlines : pearson = 0.7020, spearman = 0.7040
2019-03-12 19:13:42,362 : plagiarism : pearson = 0.7649, spearman = 0.7693
2019-03-12 19:13:43,863 : postediting : pearson = 0.8282, spearman = 0.8369
2019-03-12 19:13:44,472 : question-question : pearson = 0.5119, spearman = 0.5122
2019-03-12 19:13:44,472 : ALL (weighted average) : Pearson = 0.6655,             Spearman = 0.6702
2019-03-12 19:13:44,472 : ALL (average) : Pearson = 0.6634,             Spearman = 0.6679

2019-03-12 19:13:44,472 : ***** Transfer task : MR *****


2019-03-12 19:13:44,487 : loading BERT model bert-large-uncased
2019-03-12 19:13:44,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:44,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:44,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4lpibonx
2019-03-12 19:13:51,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:57,214 : Generating sentence embeddings
2019-03-12 19:14:29,106 : Generated sentence embeddings
2019-03-12 19:14:29,107 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:14:39,130 : Best param found at split 1: l2reg = 0.001                 with score 75.8
2019-03-12 19:14:48,612 : Best param found at split 2: l2reg = 0.001                 with score 75.67
2019-03-12 19:14:58,789 : Best param found at split 3: l2reg = 0.001                 with score 76.52
2019-03-12 19:15:09,715 : Best param found at split 4: l2reg = 0.001                 with score 75.57
2019-03-12 19:15:19,719 : Best param found at split 5: l2reg = 0.001                 with score 75.9
2019-03-12 19:15:20,335 : Dev acc : 75.89 Test acc : 75.25

2019-03-12 19:15:20,336 : ***** Transfer task : CR *****


2019-03-12 19:15:20,343 : loading BERT model bert-large-uncased
2019-03-12 19:15:20,343 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:15:20,363 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:15:20,363 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqp30tnun
2019-03-12 19:15:27,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:15:33,142 : Generating sentence embeddings
2019-03-12 19:15:41,513 : Generated sentence embeddings
2019-03-12 19:15:41,513 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:15:44,339 : Best param found at split 1: l2reg = 0.01                 with score 79.46
2019-03-12 19:15:47,444 : Best param found at split 2: l2reg = 0.001                 with score 79.73
2019-03-12 19:15:51,387 : Best param found at split 3: l2reg = 1e-05                 with score 80.5
2019-03-12 19:15:55,228 : Best param found at split 4: l2reg = 0.001                 with score 79.58
2019-03-12 19:15:58,847 : Best param found at split 5: l2reg = 1e-05                 with score 80.5
2019-03-12 19:15:59,010 : Dev acc : 79.95 Test acc : 79.57

2019-03-12 19:15:59,010 : ***** Transfer task : MPQA *****


2019-03-12 19:15:59,016 : loading BERT model bert-large-uncased
2019-03-12 19:15:59,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:15:59,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:15:59,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl61ar8lv
2019-03-12 19:16:06,467 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:16:11,733 : Generating sentence embeddings
2019-03-12 19:16:19,357 : Generated sentence embeddings
2019-03-12 19:16:19,358 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:16:27,335 : Best param found at split 1: l2reg = 0.001                 with score 87.13
2019-03-12 19:16:37,864 : Best param found at split 2: l2reg = 1e-05                 with score 87.08
2019-03-12 19:16:48,033 : Best param found at split 3: l2reg = 1e-05                 with score 87.81
2019-03-12 19:16:55,401 : Best param found at split 4: l2reg = 0.001                 with score 87.83
2019-03-12 19:17:06,091 : Best param found at split 5: l2reg = 0.0001                 with score 87.32
2019-03-12 19:17:06,526 : Dev acc : 87.43 Test acc : 87.18

2019-03-12 19:17:06,527 : ***** Transfer task : SUBJ *****


2019-03-12 19:17:06,543 : loading BERT model bert-large-uncased
2019-03-12 19:17:06,543 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:17:06,562 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:17:06,562 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6xr8vqng
2019-03-12 19:17:14,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:17:19,343 : Generating sentence embeddings
2019-03-12 19:17:50,392 : Generated sentence embeddings
2019-03-12 19:17:50,393 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:17:58,368 : Best param found at split 1: l2reg = 0.001                 with score 93.24
2019-03-12 19:18:08,553 : Best param found at split 2: l2reg = 0.001                 with score 93.48
2019-03-12 19:18:18,109 : Best param found at split 3: l2reg = 0.001                 with score 93.2
2019-03-12 19:18:28,079 : Best param found at split 4: l2reg = 0.0001                 with score 93.3
2019-03-12 19:18:38,192 : Best param found at split 5: l2reg = 0.001                 with score 93.25
2019-03-12 19:18:38,866 : Dev acc : 93.29 Test acc : 92.88

2019-03-12 19:18:38,867 : ***** Transfer task : SST Binary classification *****


2019-03-12 19:18:38,960 : loading BERT model bert-large-uncased
2019-03-12 19:18:38,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:18:39,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:18:39,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2gqgvt09
2019-03-12 19:18:46,490 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:51,855 : Computing embedding for train
2019-03-12 19:20:32,658 : Computed train embeddings
2019-03-12 19:20:32,658 : Computing embedding for dev
2019-03-12 19:20:34,859 : Computed dev embeddings
2019-03-12 19:20:34,859 : Computing embedding for test
2019-03-12 19:20:39,486 : Computed test embeddings
2019-03-12 19:20:39,486 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:20:56,725 : [('reg:1e-05', 79.59), ('reg:0.0001', 79.36), ('reg:0.001', 79.36), ('reg:0.01', 77.29)]
2019-03-12 19:20:56,725 : Validation : best param found is reg = 1e-05 with score             79.59
2019-03-12 19:20:56,725 : Evaluating...
2019-03-12 19:21:01,060 : 
Dev acc : 79.59 Test acc : 81.0 for             SST Binary classification

2019-03-12 19:21:01,061 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 19:21:01,116 : loading BERT model bert-large-uncased
2019-03-12 19:21:01,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:21:01,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:21:01,137 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoxebm1i5
2019-03-12 19:21:08,615 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:21:13,923 : Computing embedding for train
2019-03-12 19:21:35,926 : Computed train embeddings
2019-03-12 19:21:35,926 : Computing embedding for dev
2019-03-12 19:21:38,833 : Computed dev embeddings
2019-03-12 19:21:38,833 : Computing embedding for test
2019-03-12 19:21:44,509 : Computed test embeddings
2019-03-12 19:21:44,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:21:46,988 : [('reg:1e-05', 40.78), ('reg:0.0001', 38.06), ('reg:0.001', 38.51), ('reg:0.01', 40.24)]
2019-03-12 19:21:46,989 : Validation : best param found is reg = 1e-05 with score             40.78
2019-03-12 19:21:46,989 : Evaluating...
2019-03-12 19:21:47,723 : 
Dev acc : 40.78 Test acc : 42.17 for             SST Fine-Grained classification

2019-03-12 19:21:47,723 : ***** Transfer task : TREC *****


2019-03-12 19:21:47,737 : loading BERT model bert-large-uncased
2019-03-12 19:21:47,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:21:47,756 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:21:47,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmnyp6ot8
2019-03-12 19:21:55,181 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:22:08,059 : Computed train embeddings
2019-03-12 19:22:08,653 : Computed test embeddings
2019-03-12 19:22:08,653 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:22:15,866 : [('reg:1e-05', 79.64), ('reg:0.0001', 79.86), ('reg:0.001', 79.09), ('reg:0.01', 71.79)]
2019-03-12 19:22:15,866 : Cross-validation : best param found is reg = 0.0001             with score 79.86
2019-03-12 19:22:15,866 : Evaluating...
2019-03-12 19:22:16,378 : 
Dev acc : 79.86 Test acc : 89.2             for TREC

2019-03-12 19:22:16,378 : ***** Transfer task : MRPC *****


2019-03-12 19:22:16,399 : loading BERT model bert-large-uncased
2019-03-12 19:22:16,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:22:16,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:22:16,422 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8j8kz8i8
2019-03-12 19:22:23,844 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:22:29,051 : Computing embedding for train
2019-03-12 19:22:51,444 : Computed train embeddings
2019-03-12 19:22:51,444 : Computing embedding for test
2019-03-12 19:23:01,252 : Computed test embeddings
2019-03-12 19:23:01,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:23:06,286 : [('reg:1e-05', 74.02), ('reg:0.0001', 74.02), ('reg:0.001', 73.72), ('reg:0.01', 72.52)]
2019-03-12 19:23:06,286 : Cross-validation : best param found is reg = 1e-05             with score 74.02
2019-03-12 19:23:06,287 : Evaluating...
2019-03-12 19:23:06,608 : Dev acc : 74.02 Test acc 73.39; Test F1 81.6 for MRPC.

2019-03-12 19:23:06,608 : ***** Transfer task : SICK-Entailment*****


2019-03-12 19:23:06,670 : loading BERT model bert-large-uncased
2019-03-12 19:23:06,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:23:06,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:23:06,689 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0013_8cn
2019-03-12 19:23:14,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:23:19,340 : Computing embedding for train
2019-03-12 19:23:30,711 : Computed train embeddings
2019-03-12 19:23:30,711 : Computing embedding for dev
2019-03-12 19:23:32,262 : Computed dev embeddings
2019-03-12 19:23:32,262 : Computing embedding for test
2019-03-12 19:23:44,457 : Computed test embeddings
2019-03-12 19:23:44,493 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:23:46,340 : [('reg:1e-05', 80.2), ('reg:0.0001', 78.8), ('reg:0.001', 77.4), ('reg:0.01', 77.0)]
2019-03-12 19:23:46,340 : Validation : best param found is reg = 1e-05 with score             80.2
2019-03-12 19:23:46,341 : Evaluating...
2019-03-12 19:23:46,794 : 
Dev acc : 80.2 Test acc : 79.34 for                        SICK entailment

2019-03-12 19:23:46,794 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 19:23:46,820 : loading BERT model bert-large-uncased
2019-03-12 19:23:46,820 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:23:46,877 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:23:46,877 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcpalbhdd
2019-03-12 19:23:54,361 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:23:59,689 : Computing embedding for train
2019-03-12 19:24:11,073 : Computed train embeddings
2019-03-12 19:24:11,073 : Computing embedding for dev
2019-03-12 19:24:12,626 : Computed dev embeddings
2019-03-12 19:24:12,627 : Computing embedding for test
2019-03-12 19:24:24,847 : Computed test embeddings
2019-03-12 19:24:39,956 : Dev : Pearson 0.8271325062906784
2019-03-12 19:24:39,956 : Test : Pearson 0.8348584819940886 Spearman 0.7592363998882938 MSE 0.3090053338937124                        for SICK Relatedness

2019-03-12 19:24:39,957 : 

***** Transfer task : STSBenchmark*****


2019-03-12 19:24:40,024 : loading BERT model bert-large-uncased
2019-03-12 19:24:40,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:24:40,044 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:24:40,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy6_paja8
2019-03-12 19:24:47,487 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:24:52,795 : Computing embedding for train
2019-03-12 19:25:11,502 : Computed train embeddings
2019-03-12 19:25:11,502 : Computing embedding for dev
2019-03-12 19:25:17,175 : Computed dev embeddings
2019-03-12 19:25:17,176 : Computing embedding for test
2019-03-12 19:25:21,842 : Computed test embeddings
2019-03-12 19:25:40,203 : Dev : Pearson 0.7519112208175096
2019-03-12 19:25:40,204 : Test : Pearson 0.7306076844908262 Spearman 0.7262856243205148 MSE 1.3001242051446231                        for SICK Relatedness

2019-03-12 19:25:40,204 : ***** Transfer task : SNLI Entailment*****


2019-03-12 19:25:45,250 : loading BERT model bert-large-uncased
2019-03-12 19:25:45,251 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:25:45,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:25:45,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpev51ak1h
2019-03-12 19:25:52,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:25:58,518 : PROGRESS (encoding): 0.00%
2019-03-12 19:28:45,998 : PROGRESS (encoding): 14.56%
2019-03-12 19:31:59,144 : PROGRESS (encoding): 29.12%
2019-03-12 19:35:10,331 : PROGRESS (encoding): 43.69%
2019-03-12 19:38:34,045 : PROGRESS (encoding): 58.25%
2019-03-12 19:42:20,321 : PROGRESS (encoding): 72.81%
2019-03-12 19:46:05,753 : PROGRESS (encoding): 87.37%
2019-03-12 19:50:09,608 : PROGRESS (encoding): 0.00%
2019-03-12 19:50:40,288 : PROGRESS (encoding): 0.00%
2019-03-12 19:51:09,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:51:58,740 : [('reg:1e-09', 67.02)]
2019-03-12 19:51:58,740 : Validation : best param found is reg = 1e-09 with score             67.02
2019-03-12 19:51:58,740 : Evaluating...
2019-03-12 19:52:45,316 : Dev acc : 67.02 Test acc : 66.18 for SNLI

2019-03-12 19:52:45,317 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 19:52:45,517 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 19:52:46,584 : loading BERT model bert-large-uncased
2019-03-12 19:52:46,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:52:46,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:52:46,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt_v6ltqp
2019-03-12 19:52:54,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:52:59,338 : Computing embeddings for train/dev/test
2019-03-12 19:56:32,137 : Computed embeddings
2019-03-12 19:56:32,137 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:57:01,066 : [('reg:1e-05', 93.43), ('reg:0.0001', 91.24), ('reg:0.001', 86.1), ('reg:0.01', 77.52)]
2019-03-12 19:57:01,066 : Validation : best param found is reg = 1e-05 with score             93.43
2019-03-12 19:57:01,066 : Evaluating...
2019-03-12 19:57:09,592 : 
Dev acc : 93.4 Test acc : 93.6 for LENGTH classification

2019-03-12 19:57:09,593 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 19:57:09,974 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 19:57:10,019 : loading BERT model bert-large-uncased
2019-03-12 19:57:10,019 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:57:10,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:57:10,052 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv2tsw0a1
2019-03-12 19:57:17,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:57:22,809 : Computing embeddings for train/dev/test
2019-03-12 20:00:39,844 : Computed embeddings
2019-03-12 20:00:39,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:01:07,072 : [('reg:1e-05', 88.36), ('reg:0.0001', 61.25), ('reg:0.001', 4.34), ('reg:0.01', 1.17)]
2019-03-12 20:01:07,072 : Validation : best param found is reg = 1e-05 with score             88.36
2019-03-12 20:01:07,072 : Evaluating...
2019-03-12 20:01:13,519 : 
Dev acc : 88.4 Test acc : 88.1 for WORDCONTENT classification

2019-03-12 20:01:13,520 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 20:01:13,895 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 20:01:13,961 : loading BERT model bert-large-uncased
2019-03-12 20:01:13,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:01:13,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:01:13,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpti66eky7
2019-03-12 20:01:21,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:01:26,736 : Computing embeddings for train/dev/test
2019-03-12 20:04:32,569 : Computed embeddings
2019-03-12 20:04:32,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:04:58,196 : [('reg:1e-05', 34.67), ('reg:0.0001', 34.08), ('reg:0.001', 31.49), ('reg:0.01', 25.69)]
2019-03-12 20:04:58,196 : Validation : best param found is reg = 1e-05 with score             34.67
2019-03-12 20:04:58,196 : Evaluating...
2019-03-12 20:05:04,097 : 
Dev acc : 34.7 Test acc : 34.5 for DEPTH classification

2019-03-12 20:05:04,097 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 20:05:04,476 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 20:05:04,540 : loading BERT model bert-large-uncased
2019-03-12 20:05:04,540 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:05:04,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:05:04,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc4td751h
2019-03-12 20:05:12,116 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:05:17,333 : Computing embeddings for train/dev/test
2019-03-12 20:08:08,945 : Computed embeddings
2019-03-12 20:08:08,945 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:08:40,878 : [('reg:1e-05', 69.22), ('reg:0.0001', 66.87), ('reg:0.001', 59.31), ('reg:0.01', 40.52)]
2019-03-12 20:08:40,878 : Validation : best param found is reg = 1e-05 with score             69.22
2019-03-12 20:08:40,878 : Evaluating...
2019-03-12 20:08:49,167 : 
Dev acc : 69.2 Test acc : 69.7 for TOPCONSTITUENTS classification

2019-03-12 20:08:49,168 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 20:08:49,509 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 20:08:49,575 : loading BERT model bert-large-uncased
2019-03-12 20:08:49,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:08:49,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:08:49,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnzxvph7h
2019-03-12 20:08:57,128 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:09:02,369 : Computing embeddings for train/dev/test
2019-03-12 20:12:08,620 : Computed embeddings
2019-03-12 20:12:08,620 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:12:39,747 : [('reg:1e-05', 72.84), ('reg:0.0001', 72.78), ('reg:0.001', 72.46), ('reg:0.01', 69.72)]
2019-03-12 20:12:39,747 : Validation : best param found is reg = 1e-05 with score             72.84
2019-03-12 20:12:39,747 : Evaluating...
2019-03-12 20:12:48,338 : 
Dev acc : 72.8 Test acc : 72.7 for BIGRAMSHIFT classification

2019-03-12 20:12:48,339 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 20:12:48,893 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 20:12:48,957 : loading BERT model bert-large-uncased
2019-03-12 20:12:48,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:12:48,987 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:12:48,987 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpun6cz_as
2019-03-12 20:12:56,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:13:01,745 : Computing embeddings for train/dev/test
2019-03-12 20:16:04,083 : Computed embeddings
2019-03-12 20:16:04,083 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:16:29,052 : [('reg:1e-05', 86.88), ('reg:0.0001', 87.09), ('reg:0.001', 87.65), ('reg:0.01', 87.13)]
2019-03-12 20:16:29,052 : Validation : best param found is reg = 0.001 with score             87.65
2019-03-12 20:16:29,052 : Evaluating...
2019-03-12 20:16:35,654 : 
Dev acc : 87.7 Test acc : 86.1 for TENSE classification

2019-03-12 20:16:35,655 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 20:16:36,072 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 20:16:36,135 : loading BERT model bert-large-uncased
2019-03-12 20:16:36,135 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:16:36,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:16:36,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_4v96_0q
2019-03-12 20:16:43,666 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:16:48,882 : Computing embeddings for train/dev/test
2019-03-12 20:20:02,462 : Computed embeddings
2019-03-12 20:20:02,462 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:20:31,383 : [('reg:1e-05', 81.02), ('reg:0.0001', 80.97), ('reg:0.001', 80.76), ('reg:0.01', 77.05)]
2019-03-12 20:20:31,383 : Validation : best param found is reg = 1e-05 with score             81.02
2019-03-12 20:20:31,383 : Evaluating...
2019-03-12 20:20:38,820 : 
Dev acc : 81.0 Test acc : 79.6 for SUBJNUMBER classification

2019-03-12 20:20:38,821 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 20:20:39,242 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 20:20:39,311 : loading BERT model bert-large-uncased
2019-03-12 20:20:39,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:20:39,435 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:20:39,436 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplnirspiz
2019-03-12 20:20:46,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:20:52,153 : Computing embeddings for train/dev/test
2019-03-12 20:24:01,054 : Computed embeddings
2019-03-12 20:24:01,054 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:24:27,348 : [('reg:1e-05', 79.67), ('reg:0.0001', 79.73), ('reg:0.001', 79.76), ('reg:0.01', 78.02)]
2019-03-12 20:24:27,348 : Validation : best param found is reg = 0.001 with score             79.76
2019-03-12 20:24:27,349 : Evaluating...
2019-03-12 20:24:34,830 : 
Dev acc : 79.8 Test acc : 81.5 for OBJNUMBER classification

2019-03-12 20:24:34,831 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 20:24:35,411 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 20:24:35,479 : loading BERT model bert-large-uncased
2019-03-12 20:24:35,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:24:35,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:24:35,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmhsvqzn7
2019-03-12 20:24:42,995 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:24:48,248 : Computing embeddings for train/dev/test
2019-03-12 20:28:26,895 : Computed embeddings
2019-03-12 20:28:26,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:28:48,550 : [('reg:1e-05', 53.98), ('reg:0.0001', 53.92), ('reg:0.001', 53.62), ('reg:0.01', 52.83)]
2019-03-12 20:28:48,551 : Validation : best param found is reg = 1e-05 with score             53.98
2019-03-12 20:28:48,551 : Evaluating...
2019-03-12 20:28:54,367 : 
Dev acc : 54.0 Test acc : 52.8 for ODDMANOUT classification

2019-03-12 20:28:54,368 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 20:28:54,764 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 20:28:54,842 : loading BERT model bert-large-uncased
2019-03-12 20:28:54,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:28:54,969 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:28:54,970 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7e611y8e
2019-03-12 20:29:02,491 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:29:07,817 : Computing embeddings for train/dev/test
2019-03-12 20:32:45,295 : Computed embeddings
2019-03-12 20:32:45,295 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:33:18,913 : [('reg:1e-05', 57.43), ('reg:0.0001', 57.47), ('reg:0.001', 56.72), ('reg:0.01', 53.58)]
2019-03-12 20:33:18,913 : Validation : best param found is reg = 0.0001 with score             57.47
2019-03-12 20:33:18,913 : Evaluating...
2019-03-12 20:33:28,013 : 
Dev acc : 57.5 Test acc : 57.4 for COORDINATIONINVERSION classification

2019-03-12 20:33:28,015 : total results: {'STS12': {'MSRpar': {'pearson': (0.41163770722858173, 4.9113198842434695e-32), 'spearman': SpearmanrResult(correlation=0.4432432249031209, pvalue=1.9397655978917577e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6549859050059283, 4.245121696124364e-93), 'spearman': SpearmanrResult(correlation=0.6561211736362448, pvalue=1.5965384682306073e-93), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4985893163218764, 3.2277662187245496e-30), 'spearman': SpearmanrResult(correlation=0.5970028137280501, pvalue=1.1506260983969633e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.705263885594012, 7.482383919213938e-114), 'spearman': SpearmanrResult(correlation=0.6943146350904537, pvalue=5.743346687918958e-109), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5708162663769485, 7.147051599341832e-36), 'spearman': SpearmanrResult(correlation=0.540415474639931, pvalue=1.2376356822801783e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5682586161054693, 'wmean': 0.5744928603113049}, 'spearman': {'mean': 0.5862194643995601, 'wmean': 0.5903826708831634}}}, 'STS13': {'FNWN': {'pearson': (0.42270803958501707, 1.3683973052110302e-09), 'spearman': SpearmanrResult(correlation=0.4512140294668026, pvalue=7.220367041637531e-11), 'nsamples': 189}, 'headlines': {'pearson': (0.6772455270226058, 8.928742860034235e-102), 'spearman': SpearmanrResult(correlation=0.6573152606039377, pvalue=5.68162930861437e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.5575393654208837, 3.8791394195107427e-47), 'spearman': SpearmanrResult(correlation=0.5828571503024997, pvalue=2.3689178415872267e-52), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5524976440095021, 'wmean': 0.6004036991664256}, 'spearman': {'mean': 0.5637954801244134, 'wmean': 0.6034991722279209}}}, 'STS14': {'deft-forum': {'pearson': (0.3908755887517121, 7.072032241570706e-18), 'spearman': SpearmanrResult(correlation=0.3902105154825495, pvalue=8.126333601752873e-18), 'nsamples': 450}, 'deft-news': {'pearson': (0.7454604167242871, 1.9634859874191014e-54), 'spearman': SpearmanrResult(correlation=0.7011111747555753, pvalue=1.1150898923594224e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.636833782238241, 1.503563892510511e-86), 'spearman': SpearmanrResult(correlation=0.5900610963789825, pvalue=1.5086620371537861e-71), 'nsamples': 750}, 'images': {'pearson': (0.6888244636890148, 1.3388775842948007e-106), 'spearman': SpearmanrResult(correlation=0.6725806152401549, pvalue=6.795208887373744e-100), 'nsamples': 750}, 'OnWN': {'pearson': (0.6872600442804233, 6.190636268293199e-106), 'spearman': SpearmanrResult(correlation=0.7223033829029277, pvalue=6.397442890612219e-122), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6774984755361073, 7.043310067812737e-102), 'spearman': SpearmanrResult(correlation=0.6467003596492814, pvalue=4.707033496688538e-90), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6377921285366309, 'wmean': 0.6446252571369057}, 'spearman': {'mean': 0.6204945240682452, 'wmean': 0.6292432466726213}}}, 'STS15': {'answers-forums': {'pearson': (0.5674754075183089, 2.401015124125771e-33), 'spearman': SpearmanrResult(correlation=0.5544246774313539, pvalue=1.3210428008813838e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.7158681265317931, 8.373485771110066e-119), 'spearman': SpearmanrResult(correlation=0.7201586133967183, pvalue=7.15425032333777e-121), 'nsamples': 750}, 'belief': {'pearson': (0.6667209506515812, 1.484360650977804e-49), 'spearman': SpearmanrResult(correlation=0.6907651490851309, pvalue=1.7845556834501473e-54), 'nsamples': 375}, 'headlines': {'pearson': (0.6961476046449415, 9.052493543111818e-110), 'spearman': SpearmanrResult(correlation=0.6831801038781058, pvalue=3.2088196605618355e-104), 'nsamples': 750}, 'images': {'pearson': (0.7684180983128411, 3.7596188333151836e-147), 'spearman': SpearmanrResult(correlation=0.7760126362263594, pvalue=7.095985655651798e-152), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6829260375318931, 'wmean': 0.6993830021436304}, 'spearman': {'mean': 0.6849082360035336, 'wmean': 0.7004865666898564}}}, 'STS16': {'answer-answer': {'pearson': (0.509826648929311, 3.2924641722760673e-18), 'spearman': SpearmanrResult(correlation=0.5170562287123429, pvalue=9.117666882595047e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7020358816606175, 2.7599545765092655e-38), 'spearman': SpearmanrResult(correlation=0.703985117841313, pvalue=1.4095480521931145e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7648794240211566, 1.9508388518546868e-45), 'spearman': SpearmanrResult(correlation=0.769330260321114, pvalue=2.9257525029683856e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8282189116020645, 8.47337913414009e-63), 'spearman': SpearmanrResult(correlation=0.8369314933511379, pvalue=2.742618918420871e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.5118799185113504, 2.350413684606269e-15), 'spearman': SpearmanrResult(correlation=0.5121884999343189, pvalue=2.2471895150852177e-15), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6633681569449, 'wmean': 0.6655088434116195}, 'spearman': {'mean': 0.6678983200320454, 'wmean': 0.6701764057109852}}}, 'MR': {'devacc': 75.89, 'acc': 75.25, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.95, 'acc': 79.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.43, 'acc': 87.18, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.29, 'acc': 92.88, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.59, 'acc': 81.0, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.78, 'acc': 42.17, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.86, 'acc': 89.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.02, 'acc': 73.39, 'f1': 81.6, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 79.34, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8271325062906784, 'pearson': 0.8348584819940886, 'spearman': 0.7592363998882938, 'mse': 0.3090053338937124, 'yhat': array([3.14202797, 4.34598511, 1.51056673, ..., 3.57067999, 4.28228588,        4.46569308]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7519112208175096, 'pearson': 0.7306076844908262, 'spearman': 0.7262856243205148, 'mse': 1.3001242051446231, 'yhat': array([1.33153052, 1.1771637 , 1.88938864, ..., 3.82205366, 3.73626411,        3.23737022]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.02, 'acc': 66.18, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 93.43, 'acc': 93.64, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 88.36, 'acc': 88.12, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.67, 'acc': 34.55, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.22, 'acc': 69.74, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 72.84, 'acc': 72.68, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.65, 'acc': 86.09, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.02, 'acc': 79.59, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.76, 'acc': 81.46, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.98, 'acc': 52.75, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.47, 'acc': 57.42, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 20:33:28,015 : STS12 p=0.5745, STS12 s=0.5904, STS13 p=0.6004, STS13 s=0.6035, STS14 p=0.6446, STS14 s=0.6292, STS15 p=0.6994, STS15 s=0.7005, STS 16 p=0.6655, STS16 s=0.6702, STS B p=0.7306, STS B s=0.7263, STS B m=1.3001, SICK-R p=0.8349, SICK-R s=0.7592, SICK-P m=0.3090
2019-03-12 20:33:28,015 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 20:33:28,015 : 0.5745,0.5904,0.6004,0.6035,0.6446,0.6292,0.6994,0.7005,0.6655,0.6702,0.7306,0.7263,1.3001,0.8349,0.7592,0.3090
2019-03-12 20:33:28,015 : MR=75.25, CR=79.57, SUBJ=92.88, MPQA=87.18, SST-B=81.00, SST-F=42.17, TREC=89.20, SICK-E=79.34, SNLI=66.18, MRPC=73.39, MRPC f=81.60
2019-03-12 20:33:28,015 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 20:33:28,015 : 75.25,79.57,92.88,87.18,81.00,42.17,89.20,79.34,66.18,73.39,81.60
2019-03-12 20:33:28,015 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 20:33:28,016 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 20:33:28,016 : na,na,na,na,na,na,na,na,na,na
2019-03-12 20:33:28,016 : SentLen=93.64, WC=88.12, TreeDepth=34.55, TopConst=69.74, BShift=72.68, Tense=86.09, SubjNum=79.59, ObjNum=81.46, SOMO=52.75, CoordInv=57.42, average=71.60
2019-03-12 20:33:28,016 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 20:33:28,016 : 93.64,88.12,34.55,69.74,72.68,86.09,79.59,81.46,52.75,57.42,71.60
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,016 : layer 5
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,016 : ********************************************************************************
2019-03-12 20:33:28,103 : ***** Transfer task : STS12 *****


2019-03-12 20:33:28,116 : loading BERT model bert-large-uncased
2019-03-12 20:33:28,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:33:28,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:33:28,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4es6w_0z
2019-03-12 20:33:35,582 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:33:44,871 : MSRpar : pearson = 0.4124, spearman = 0.4419
2019-03-12 20:33:46,523 : MSRvid : pearson = 0.6519, spearman = 0.6510
2019-03-12 20:33:47,942 : SMTeuroparl : pearson = 0.4961, spearman = 0.5845
2019-03-12 20:33:50,654 : surprise.OnWN : pearson = 0.7056, spearman = 0.6910
2019-03-12 20:33:52,091 : surprise.SMTnews : pearson = 0.5777, spearman = 0.5486
2019-03-12 20:33:52,091 : ALL (weighted average) : Pearson = 0.5746,             Spearman = 0.5872
2019-03-12 20:33:52,091 : ALL (average) : Pearson = 0.5688,             Spearman = 0.5834

2019-03-12 20:33:52,092 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 20:33:52,099 : loading BERT model bert-large-uncased
2019-03-12 20:33:52,099 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:33:52,117 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:33:52,117 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp310bfstj
2019-03-12 20:33:59,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:06,397 : FNWN : pearson = 0.4419, spearman = 0.4650
2019-03-12 20:34:08,301 : headlines : pearson = 0.6777, spearman = 0.6573
2019-03-12 20:34:09,778 : OnWN : pearson = 0.5794, spearman = 0.6038
2019-03-12 20:34:09,778 : ALL (weighted average) : Pearson = 0.6112,             Spearman = 0.6131
2019-03-12 20:34:09,778 : ALL (average) : Pearson = 0.5663,             Spearman = 0.5754

2019-03-12 20:34:09,778 : ***** Transfer task : STS14 *****


2019-03-12 20:34:09,795 : loading BERT model bert-large-uncased
2019-03-12 20:34:09,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:34:09,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:34:09,813 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzfgh82wi
2019-03-12 20:34:17,307 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:24,006 : deft-forum : pearson = 0.3946, spearman = 0.3935
2019-03-12 20:34:25,653 : deft-news : pearson = 0.7467, spearman = 0.7029
2019-03-12 20:34:27,839 : headlines : pearson = 0.6364, spearman = 0.5885
2019-03-12 20:34:29,929 : images : pearson = 0.6979, spearman = 0.6781
2019-03-12 20:34:32,072 : OnWN : pearson = 0.6989, spearman = 0.7320
2019-03-12 20:34:34,948 : tweet-news : pearson = 0.7035, spearman = 0.6626
2019-03-12 20:34:34,948 : ALL (weighted average) : Pearson = 0.6545,             Spearman = 0.6357
2019-03-12 20:34:34,948 : ALL (average) : Pearson = 0.6464,             Spearman = 0.6263

2019-03-12 20:34:34,948 : ***** Transfer task : STS15 *****


2019-03-12 20:34:34,982 : loading BERT model bert-large-uncased
2019-03-12 20:34:34,982 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:34:35,001 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:34:35,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp822sxmfb
2019-03-12 20:34:42,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:49,762 : answers-forums : pearson = 0.5914, spearman = 0.5726
2019-03-12 20:34:51,861 : answers-students : pearson = 0.7266, spearman = 0.7305
2019-03-12 20:34:53,922 : belief : pearson = 0.6964, spearman = 0.7140
2019-03-12 20:34:56,183 : headlines : pearson = 0.6960, spearman = 0.6841
2019-03-12 20:34:58,328 : images : pearson = 0.7721, spearman = 0.7784
2019-03-12 20:34:58,328 : ALL (weighted average) : Pearson = 0.7096,             Spearman = 0.7091
2019-03-12 20:34:58,328 : ALL (average) : Pearson = 0.6965,             Spearman = 0.6959

2019-03-12 20:34:58,328 : ***** Transfer task : STS16 *****


2019-03-12 20:34:58,398 : loading BERT model bert-large-uncased
2019-03-12 20:34:58,398 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:34:58,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:34:58,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprigavuvt
2019-03-12 20:35:05,870 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:35:12,026 : answer-answer : pearson = 0.5237, spearman = 0.5303
2019-03-12 20:35:12,690 : headlines : pearson = 0.6997, spearman = 0.6998
2019-03-12 20:35:13,578 : plagiarism : pearson = 0.7810, spearman = 0.7868
2019-03-12 20:35:15,082 : postediting : pearson = 0.8297, spearman = 0.8430
2019-03-12 20:35:15,691 : question-question : pearson = 0.4995, spearman = 0.4996
2019-03-12 20:35:15,692 : ALL (weighted average) : Pearson = 0.6693,             Spearman = 0.6746
2019-03-12 20:35:15,692 : ALL (average) : Pearson = 0.6667,             Spearman = 0.6719

2019-03-12 20:35:15,692 : ***** Transfer task : MR *****


2019-03-12 20:35:15,711 : loading BERT model bert-large-uncased
2019-03-12 20:35:15,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:35:15,729 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:35:15,729 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz_sj_vxm
2019-03-12 20:35:23,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:35:28,526 : Generating sentence embeddings
2019-03-12 20:36:00,173 : Generated sentence embeddings
2019-03-12 20:36:00,173 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:36:09,548 : Best param found at split 1: l2reg = 1e-05                 with score 75.76
2019-03-12 20:36:19,518 : Best param found at split 2: l2reg = 0.001                 with score 75.86
2019-03-12 20:36:28,697 : Best param found at split 3: l2reg = 0.001                 with score 76.67
2019-03-12 20:36:39,873 : Best param found at split 4: l2reg = 0.001                 with score 76.04
2019-03-12 20:36:50,546 : Best param found at split 5: l2reg = 0.001                 with score 76.27
2019-03-12 20:36:51,160 : Dev acc : 76.12 Test acc : 75.27

2019-03-12 20:36:51,161 : ***** Transfer task : CR *****


2019-03-12 20:36:51,169 : loading BERT model bert-large-uncased
2019-03-12 20:36:51,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:36:51,188 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:36:51,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnslnrfx4
2019-03-12 20:36:58,597 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:37:03,869 : Generating sentence embeddings
2019-03-12 20:37:12,234 : Generated sentence embeddings
2019-03-12 20:37:12,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:37:16,167 : Best param found at split 1: l2reg = 0.001                 with score 80.32
2019-03-12 20:37:18,804 : Best param found at split 2: l2reg = 1e-05                 with score 79.23
2019-03-12 20:37:22,583 : Best param found at split 3: l2reg = 0.01                 with score 78.94
2019-03-12 20:37:26,469 : Best param found at split 4: l2reg = 0.0001                 with score 80.27
2019-03-12 20:37:30,469 : Best param found at split 5: l2reg = 0.0001                 with score 80.34
2019-03-12 20:37:30,718 : Dev acc : 79.82 Test acc : 78.46

2019-03-12 20:37:30,719 : ***** Transfer task : MPQA *****


2019-03-12 20:37:30,725 : loading BERT model bert-large-uncased
2019-03-12 20:37:30,725 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:37:30,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:37:30,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp67v_eai1
2019-03-12 20:37:38,182 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:37:43,481 : Generating sentence embeddings
2019-03-12 20:37:51,123 : Generated sentence embeddings
2019-03-12 20:37:51,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:38:00,811 : Best param found at split 1: l2reg = 0.001                 with score 87.47
2019-03-12 20:38:12,362 : Best param found at split 2: l2reg = 0.001                 with score 87.64
2019-03-12 20:38:24,315 : Best param found at split 3: l2reg = 0.0001                 with score 88.03
2019-03-12 20:38:35,517 : Best param found at split 4: l2reg = 0.001                 with score 88.43
2019-03-12 20:38:47,883 : Best param found at split 5: l2reg = 0.001                 with score 87.98
2019-03-12 20:38:48,277 : Dev acc : 87.91 Test acc : 87.92

2019-03-12 20:38:48,277 : ***** Transfer task : SUBJ *****


2019-03-12 20:38:48,292 : loading BERT model bert-large-uncased
2019-03-12 20:38:48,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:38:48,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:38:48,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj80t9aej
2019-03-12 20:38:55,786 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:39:01,165 : Generating sentence embeddings
2019-03-12 20:39:32,189 : Generated sentence embeddings
2019-03-12 20:39:32,189 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:39:41,532 : Best param found at split 1: l2reg = 0.001                 with score 93.66
2019-03-12 20:39:51,468 : Best param found at split 2: l2reg = 0.001                 with score 94.0
2019-03-12 20:40:01,419 : Best param found at split 3: l2reg = 0.001                 with score 93.8
2019-03-12 20:40:11,934 : Best param found at split 4: l2reg = 0.001                 with score 93.86
2019-03-12 20:40:21,637 : Best param found at split 5: l2reg = 0.001                 with score 93.68
2019-03-12 20:40:22,295 : Dev acc : 93.8 Test acc : 93.42

2019-03-12 20:40:22,296 : ***** Transfer task : SST Binary classification *****


2019-03-12 20:40:22,386 : loading BERT model bert-large-uncased
2019-03-12 20:40:22,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:40:22,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:40:22,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv9b9df0i
2019-03-12 20:40:29,930 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:40:35,268 : Computing embedding for train
2019-03-12 20:42:16,065 : Computed train embeddings
2019-03-12 20:42:16,065 : Computing embedding for dev
2019-03-12 20:42:18,267 : Computed dev embeddings
2019-03-12 20:42:18,267 : Computing embedding for test
2019-03-12 20:42:22,895 : Computed test embeddings
2019-03-12 20:42:22,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:42:41,574 : [('reg:1e-05', 80.16), ('reg:0.0001', 80.05), ('reg:0.001', 77.52), ('reg:0.01', 77.98)]
2019-03-12 20:42:41,575 : Validation : best param found is reg = 1e-05 with score             80.16
2019-03-12 20:42:41,575 : Evaluating...
2019-03-12 20:42:46,260 : 
Dev acc : 80.16 Test acc : 79.74 for             SST Binary classification

2019-03-12 20:42:46,261 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 20:42:46,310 : loading BERT model bert-large-uncased
2019-03-12 20:42:46,311 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:42:46,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:42:46,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbkxibf_j
2019-03-12 20:42:53,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:42:59,123 : Computing embedding for train
2019-03-12 20:43:21,149 : Computed train embeddings
2019-03-12 20:43:21,149 : Computing embedding for dev
2019-03-12 20:43:24,033 : Computed dev embeddings
2019-03-12 20:43:24,033 : Computing embedding for test
2019-03-12 20:43:29,712 : Computed test embeddings
2019-03-12 20:43:29,712 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:43:32,045 : [('reg:1e-05', 37.87), ('reg:0.0001', 38.06), ('reg:0.001', 39.06), ('reg:0.01', 39.42)]
2019-03-12 20:43:32,045 : Validation : best param found is reg = 0.01 with score             39.42
2019-03-12 20:43:32,045 : Evaluating...
2019-03-12 20:43:32,604 : 
Dev acc : 39.42 Test acc : 40.27 for             SST Fine-Grained classification

2019-03-12 20:43:32,605 : ***** Transfer task : TREC *****


2019-03-12 20:43:32,617 : loading BERT model bert-large-uncased
2019-03-12 20:43:32,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:43:32,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:43:32,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplxc4gz8f
2019-03-12 20:43:40,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:43:52,949 : Computed train embeddings
2019-03-12 20:43:53,541 : Computed test embeddings
2019-03-12 20:43:53,541 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:44:00,330 : [('reg:1e-05', 79.2), ('reg:0.0001', 79.16), ('reg:0.001', 77.84), ('reg:0.01', 67.57)]
2019-03-12 20:44:00,330 : Cross-validation : best param found is reg = 1e-05             with score 79.2
2019-03-12 20:44:00,330 : Evaluating...
2019-03-12 20:44:00,778 : 
Dev acc : 79.2 Test acc : 89.0             for TREC

2019-03-12 20:44:00,779 : ***** Transfer task : MRPC *****


2019-03-12 20:44:00,802 : loading BERT model bert-large-uncased
2019-03-12 20:44:00,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:44:00,823 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:44:00,823 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi9shhr1q
2019-03-12 20:44:08,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:44:13,804 : Computing embedding for train
2019-03-12 20:44:36,196 : Computed train embeddings
2019-03-12 20:44:36,196 : Computing embedding for test
2019-03-12 20:44:46,006 : Computed test embeddings
2019-03-12 20:44:46,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:44:50,807 : [('reg:1e-05', 73.36), ('reg:0.0001', 73.33), ('reg:0.001', 73.5), ('reg:0.01', 72.64)]
2019-03-12 20:44:50,807 : Cross-validation : best param found is reg = 0.001             with score 73.5
2019-03-12 20:44:50,807 : Evaluating...
2019-03-12 20:44:51,192 : Dev acc : 73.5 Test acc 67.48; Test F1 71.71 for MRPC.

2019-03-12 20:44:51,192 : ***** Transfer task : SICK-Entailment*****


2019-03-12 20:44:51,252 : loading BERT model bert-large-uncased
2019-03-12 20:44:51,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:44:51,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:44:51,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcj2wdeg0
2019-03-12 20:44:58,705 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:45:04,051 : Computing embedding for train
2019-03-12 20:45:15,403 : Computed train embeddings
2019-03-12 20:45:15,403 : Computing embedding for dev
2019-03-12 20:45:16,954 : Computed dev embeddings
2019-03-12 20:45:16,954 : Computing embedding for test
2019-03-12 20:45:29,151 : Computed test embeddings
2019-03-12 20:45:29,189 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:45:30,717 : [('reg:1e-05', 80.2), ('reg:0.0001', 78.8), ('reg:0.001', 78.8), ('reg:0.01', 72.6)]
2019-03-12 20:45:30,717 : Validation : best param found is reg = 1e-05 with score             80.2
2019-03-12 20:45:30,717 : Evaluating...
2019-03-12 20:45:31,144 : 
Dev acc : 80.2 Test acc : 80.21 for                        SICK entailment

2019-03-12 20:45:31,145 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 20:45:31,172 : loading BERT model bert-large-uncased
2019-03-12 20:45:31,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:45:31,231 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:45:31,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3obe1buc
2019-03-12 20:45:38,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:45:43,991 : Computing embedding for train
2019-03-12 20:45:55,374 : Computed train embeddings
2019-03-12 20:45:55,374 : Computing embedding for dev
2019-03-12 20:45:56,934 : Computed dev embeddings
2019-03-12 20:45:56,934 : Computing embedding for test
2019-03-12 20:46:09,159 : Computed test embeddings
2019-03-12 20:46:24,357 : Dev : Pearson 0.8191452280060787
2019-03-12 20:46:24,357 : Test : Pearson 0.8308858367329588 Spearman 0.7561527212267206 MSE 0.3160365194614587                        for SICK Relatedness

2019-03-12 20:46:24,358 : 

***** Transfer task : STSBenchmark*****


2019-03-12 20:46:24,426 : loading BERT model bert-large-uncased
2019-03-12 20:46:24,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:46:24,447 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:46:24,448 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoiqy4hhc
2019-03-12 20:46:31,890 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:46:37,191 : Computing embedding for train
2019-03-12 20:46:55,897 : Computed train embeddings
2019-03-12 20:46:55,897 : Computing embedding for dev
2019-03-12 20:47:01,575 : Computed dev embeddings
2019-03-12 20:47:01,575 : Computing embedding for test
2019-03-12 20:47:06,226 : Computed test embeddings
2019-03-12 20:47:25,119 : Dev : Pearson 0.7543606565613428
2019-03-12 20:47:25,119 : Test : Pearson 0.7248012909744357 Spearman 0.7181595471638691 MSE 1.3391881648357686                        for SICK Relatedness

2019-03-12 20:47:25,120 : ***** Transfer task : SNLI Entailment*****


2019-03-12 20:47:30,181 : loading BERT model bert-large-uncased
2019-03-12 20:47:30,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:47:30,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:47:30,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf5bpknui
2019-03-12 20:47:37,682 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:47:43,300 : PROGRESS (encoding): 0.00%
2019-03-12 20:50:30,796 : PROGRESS (encoding): 14.56%
2019-03-12 20:53:43,391 : PROGRESS (encoding): 29.12%
2019-03-12 20:56:55,131 : PROGRESS (encoding): 43.69%
2019-03-12 21:00:18,908 : PROGRESS (encoding): 58.25%
2019-03-12 21:04:05,481 : PROGRESS (encoding): 72.81%
2019-03-12 21:07:50,827 : PROGRESS (encoding): 87.37%
2019-03-12 21:11:54,640 : PROGRESS (encoding): 0.00%
2019-03-12 21:12:25,318 : PROGRESS (encoding): 0.00%
2019-03-12 21:12:54,811 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:13:24,606 : [('reg:1e-09', 62.68)]
2019-03-12 21:13:24,606 : Validation : best param found is reg = 1e-09 with score             62.68
2019-03-12 21:13:24,606 : Evaluating...
2019-03-12 21:13:55,694 : Dev acc : 62.68 Test acc : 62.66 for SNLI

2019-03-12 21:13:55,695 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 21:13:55,905 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 21:13:56,940 : loading BERT model bert-large-uncased
2019-03-12 21:13:56,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:13:56,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:13:56,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpou9o5ptw
2019-03-12 21:14:04,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:14:09,812 : Computing embeddings for train/dev/test
2019-03-12 21:17:42,838 : Computed embeddings
2019-03-12 21:17:42,838 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:18:06,053 : [('reg:1e-05', 91.4), ('reg:0.0001', 90.31), ('reg:0.001', 83.55), ('reg:0.01', 77.01)]
2019-03-12 21:18:06,053 : Validation : best param found is reg = 1e-05 with score             91.4
2019-03-12 21:18:06,053 : Evaluating...
2019-03-12 21:18:11,349 : 
Dev acc : 91.4 Test acc : 91.2 for LENGTH classification

2019-03-12 21:18:11,350 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 21:18:11,598 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 21:18:11,646 : loading BERT model bert-large-uncased
2019-03-12 21:18:11,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:18:11,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:18:11,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpesoucv4l
2019-03-12 21:18:19,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:18:24,242 : Computing embeddings for train/dev/test
2019-03-12 21:21:41,369 : Computed embeddings
2019-03-12 21:21:41,370 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:22:09,958 : [('reg:1e-05', 83.15), ('reg:0.0001', 50.18), ('reg:0.001', 3.62), ('reg:0.01', 0.95)]
2019-03-12 21:22:09,958 : Validation : best param found is reg = 1e-05 with score             83.15
2019-03-12 21:22:09,959 : Evaluating...
2019-03-12 21:22:14,522 : 
Dev acc : 83.2 Test acc : 83.8 for WORDCONTENT classification

2019-03-12 21:22:14,523 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 21:22:15,055 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 21:22:15,120 : loading BERT model bert-large-uncased
2019-03-12 21:22:15,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:22:15,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:22:15,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkw5f2jlm
2019-03-12 21:22:22,566 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:22:28,687 : Computing embeddings for train/dev/test
2019-03-12 21:25:33,884 : Computed embeddings
2019-03-12 21:25:33,885 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:25:54,680 : [('reg:1e-05', 35.41), ('reg:0.0001', 34.97), ('reg:0.001', 32.34), ('reg:0.01', 26.54)]
2019-03-12 21:25:54,680 : Validation : best param found is reg = 1e-05 with score             35.41
2019-03-12 21:25:54,680 : Evaluating...
2019-03-12 21:26:01,156 : 
Dev acc : 35.4 Test acc : 35.2 for DEPTH classification

2019-03-12 21:26:01,157 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 21:26:01,523 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 21:26:01,586 : loading BERT model bert-large-uncased
2019-03-12 21:26:01,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:26:01,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:26:01,695 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsbymve2v
2019-03-12 21:26:09,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:26:14,452 : Computing embeddings for train/dev/test
2019-03-12 21:29:05,990 : Computed embeddings
2019-03-12 21:29:05,990 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:29:35,447 : [('reg:1e-05', 70.17), ('reg:0.0001', 66.98), ('reg:0.001', 59.62), ('reg:0.01', 42.45)]
2019-03-12 21:29:35,447 : Validation : best param found is reg = 1e-05 with score             70.17
2019-03-12 21:29:35,448 : Evaluating...
2019-03-12 21:29:42,568 : 
Dev acc : 70.2 Test acc : 70.5 for TOPCONSTITUENTS classification

2019-03-12 21:29:42,569 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 21:29:42,939 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 21:29:43,004 : loading BERT model bert-large-uncased
2019-03-12 21:29:43,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:29:43,033 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:29:43,033 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6z94cfja
2019-03-12 21:29:50,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:29:55,763 : Computing embeddings for train/dev/test
2019-03-12 21:33:01,663 : Computed embeddings
2019-03-12 21:33:01,663 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:33:25,352 : [('reg:1e-05', 73.82), ('reg:0.0001', 73.94), ('reg:0.001', 74.41), ('reg:0.01', 68.34)]
2019-03-12 21:33:25,353 : Validation : best param found is reg = 0.001 with score             74.41
2019-03-12 21:33:25,353 : Evaluating...
2019-03-12 21:33:31,623 : 
Dev acc : 74.4 Test acc : 74.1 for BIGRAMSHIFT classification

2019-03-12 21:33:31,624 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 21:33:32,011 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 21:33:32,077 : loading BERT model bert-large-uncased
2019-03-12 21:33:32,077 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:33:32,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:33:32,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5wnd7el9
2019-03-12 21:33:39,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:33:44,915 : Computing embeddings for train/dev/test
2019-03-12 21:36:47,249 : Computed embeddings
2019-03-12 21:36:47,249 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:37:12,963 : [('reg:1e-05', 87.09), ('reg:0.0001', 87.34), ('reg:0.001', 88.02), ('reg:0.01', 87.65)]
2019-03-12 21:37:12,963 : Validation : best param found is reg = 0.001 with score             88.02
2019-03-12 21:37:12,963 : Evaluating...
2019-03-12 21:37:18,318 : 
Dev acc : 88.0 Test acc : 86.7 for TENSE classification

2019-03-12 21:37:18,320 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 21:37:18,722 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 21:37:18,784 : loading BERT model bert-large-uncased
2019-03-12 21:37:18,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:37:18,899 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:37:18,899 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8olc6bs7
2019-03-12 21:37:26,364 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:37:31,634 : Computing embeddings for train/dev/test
2019-03-12 21:40:44,449 : Computed embeddings
2019-03-12 21:40:44,449 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:41:11,676 : [('reg:1e-05', 81.52), ('reg:0.0001', 81.55), ('reg:0.001', 80.64), ('reg:0.01', 78.49)]
2019-03-12 21:41:11,676 : Validation : best param found is reg = 0.0001 with score             81.55
2019-03-12 21:41:11,676 : Evaluating...
2019-03-12 21:41:18,181 : 
Dev acc : 81.5 Test acc : 80.3 for SUBJNUMBER classification

2019-03-12 21:41:18,182 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 21:41:18,580 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 21:41:18,647 : loading BERT model bert-large-uncased
2019-03-12 21:41:18,648 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:41:18,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:41:18,762 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcag9a76t
2019-03-12 21:41:26,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:41:31,528 : Computing embeddings for train/dev/test
2019-03-12 21:44:40,227 : Computed embeddings
2019-03-12 21:44:40,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:45:01,251 : [('reg:1e-05', 79.76), ('reg:0.0001', 79.77), ('reg:0.001', 79.07), ('reg:0.01', 77.46)]
2019-03-12 21:45:01,252 : Validation : best param found is reg = 0.0001 with score             79.77
2019-03-12 21:45:01,252 : Evaluating...
2019-03-12 21:45:06,627 : 
Dev acc : 79.8 Test acc : 81.1 for OBJNUMBER classification

2019-03-12 21:45:06,628 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 21:45:07,190 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 21:45:07,259 : loading BERT model bert-large-uncased
2019-03-12 21:45:07,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:45:07,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:45:07,287 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphnue8lr2
2019-03-12 21:45:14,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:45:20,023 : Computing embeddings for train/dev/test
2019-03-12 21:48:58,805 : Computed embeddings
2019-03-12 21:48:58,805 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:49:20,011 : [('reg:1e-05', 55.6), ('reg:0.0001', 55.47), ('reg:0.001', 55.24), ('reg:0.01', 54.29)]
2019-03-12 21:49:20,011 : Validation : best param found is reg = 1e-05 with score             55.6
2019-03-12 21:49:20,011 : Evaluating...
2019-03-12 21:49:26,441 : 
Dev acc : 55.6 Test acc : 55.2 for ODDMANOUT classification

2019-03-12 21:49:26,442 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 21:49:26,818 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 21:49:26,898 : loading BERT model bert-large-uncased
2019-03-12 21:49:26,899 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:49:26,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:49:26,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkszvesl6
2019-03-12 21:49:34,389 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:49:39,643 : Computing embeddings for train/dev/test
2019-03-12 21:53:16,579 : Computed embeddings
2019-03-12 21:53:16,579 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:53:50,484 : [('reg:1e-05', 57.69), ('reg:0.0001', 57.51), ('reg:0.001', 56.03), ('reg:0.01', 51.07)]
2019-03-12 21:53:50,484 : Validation : best param found is reg = 1e-05 with score             57.69
2019-03-12 21:53:50,484 : Evaluating...
2019-03-12 21:54:00,162 : 
Dev acc : 57.7 Test acc : 57.3 for COORDINATIONINVERSION classification

2019-03-12 21:54:00,164 : total results: {'STS12': {'MSRpar': {'pearson': (0.41241217904806143, 3.6773675233083894e-32), 'spearman': SpearmanrResult(correlation=0.4418656194168363, pvalue=3.430608397606134e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6519482616725533, 5.692513509168965e-92), 'spearman': SpearmanrResult(correlation=0.6510053682735248, pvalue=1.2665448656830582e-91), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49610888801366243, 6.860301811508978e-30), 'spearman': SpearmanrResult(correlation=0.5844659268313838, pvalue=2.128341535737563e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.7056216935503231, 5.1356364798774646e-114), 'spearman': SpearmanrResult(correlation=0.6909675273874237, pvalue=1.6176506841266136e-107), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5777397271519736, 6.7001818576400005e-37), 'spearman': SpearmanrResult(correlation=0.5485558863578236, pvalue=9.99854034152963e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5687661498873148, 'wmean': 0.5745555763948237}, 'spearman': {'mean': 0.5833720656533984, 'wmean': 0.5872015911778364}}}, 'STS13': {'FNWN': {'pearson': (0.4419275008738714, 1.940286505152069e-10), 'spearman': SpearmanrResult(correlation=0.465040285890451, pvalue=1.566526870319954e-11), 'nsamples': 189}, 'headlines': {'pearson': (0.6776676959680155, 6.009022550011238e-102), 'spearman': SpearmanrResult(correlation=0.6573445583280487, pvalue=5.539082155772303e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.5793695440133635, 1.3183350513613699e-51), 'spearman': SpearmanrResult(correlation=0.6037599502327451, pvalue=5.148169604665344e-57), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5663215802850835, 'wmean': 0.6112009225551135}, 'spearman': {'mean': 0.575381598150415, 'wmean': 0.6130735765732679}}}, 'STS14': {'deft-forum': {'pearson': (0.3946181819368902, 3.216395058799495e-18), 'spearman': SpearmanrResult(correlation=0.3934599155791763, pvalue=4.108967909391216e-18), 'nsamples': 450}, 'deft-news': {'pearson': (0.7467484480207041, 1.0274058580014662e-54), 'spearman': SpearmanrResult(correlation=0.702916686912806, pvalue=5.2816681021197934e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.6363990804247365, 2.1309765049457838e-86), 'spearman': SpearmanrResult(correlation=0.5885230667911785, pvalue=4.273542617400838e-71), 'nsamples': 750}, 'images': {'pearson': (0.6979259302444408, 1.487528541111941e-110), 'spearman': SpearmanrResult(correlation=0.6781151643255043, pvalue=3.94634858652697e-102), 'nsamples': 750}, 'OnWN': {'pearson': (0.6989394514918865, 5.282929535863546e-111), 'spearman': SpearmanrResult(correlation=0.7320219374516365, pvalue=8.470401379884176e-127), 'nsamples': 750}, 'tweet-news': {'pearson': (0.7035240029360714, 4.6271312756086643e-113), 'spearman': SpearmanrResult(correlation=0.6626479796086604, pvalue=5.312515376196671e-96), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6463591825091216, 'wmean': 0.6544517506935101}, 'spearman': {'mean': 0.6262807917781603, 'wmean': 0.6357101544579216}}}, 'STS15': {'answers-forums': {'pearson': (0.5913962622885444, 9.579292702819386e-37), 'spearman': SpearmanrResult(correlation=0.5725747755234168, pvalue=4.7757924398433594e-34), 'nsamples': 375}, 'answers-students': {'pearson': (0.7265625425904645, 4.944645105822533e-124), 'spearman': SpearmanrResult(correlation=0.7304675341325327, pvalue=5.277358047243815e-126), 'nsamples': 750}, 'belief': {'pearson': (0.6963825707370399, 1.076077467900282e-55), 'spearman': SpearmanrResult(correlation=0.7140472870962032, pvalue=1.0117787496214679e-59), 'nsamples': 375}, 'headlines': {'pearson': (0.6959565255500747, 1.0982420537069591e-109), 'spearman': SpearmanrResult(correlation=0.6841295807315824, pvalue=1.2877703775402537e-104), 'nsamples': 750}, 'images': {'pearson': (0.7720794403410471, 2.091859719609425e-149), 'spearman': SpearmanrResult(correlation=0.778378835474632, pvalue=2.1925997647352484e-153), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6964754683014341, 'wmean': 0.7096219812485947}, 'spearman': {'mean': 0.6959196025916735, 'wmean': 0.7090717454121392}}}, 'STS16': {'answer-answer': {'pearson': (0.523718851942635, 2.717035152625836e-19), 'spearman': SpearmanrResult(correlation=0.5303056370765529, pvalue=7.994674967868275e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6996724977563329, 6.188144066895852e-38), 'spearman': SpearmanrResult(correlation=0.6998369193452271, pvalue=5.85162206116486e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7810392643975804, 1.611219472557334e-48), 'spearman': SpearmanrResult(correlation=0.7868230774618488, pvalue=1.0943183099197685e-49), 'nsamples': 230}, 'postediting': {'pearson': (0.8297373283072205, 3.1939828882333415e-63), 'spearman': SpearmanrResult(correlation=0.8430284041042243, pvalue=4.043386963920431e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.49952915169954204, 1.3673068082869073e-14), 'spearman': SpearmanrResult(correlation=0.4996489934461546, pvalue=1.3445994614473796e-14), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6667394188206622, 'wmean': 0.669257649206042}, 'spearman': {'mean': 0.6719286062868016, 'wmean': 0.6745800192093662}}}, 'MR': {'devacc': 76.12, 'acc': 75.27, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.82, 'acc': 78.46, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.91, 'acc': 87.92, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.8, 'acc': 93.42, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.16, 'acc': 79.74, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.42, 'acc': 40.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.2, 'acc': 89.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.5, 'acc': 67.48, 'f1': 71.71, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 80.21, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8191452280060787, 'pearson': 0.8308858367329588, 'spearman': 0.7561527212267206, 'mse': 0.3160365194614587, 'yhat': array([3.30416331, 4.27844291, 1.43008109, ..., 3.46560282, 4.44347441,        4.37643886]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7543606565613428, 'pearson': 0.7248012909744357, 'spearman': 0.7181595471638691, 'mse': 1.3391881648357686, 'yhat': array([1.90258645, 1.18761657, 2.09815148, ..., 3.85667867, 3.67144089,        3.36891771]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.68, 'acc': 62.66, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.4, 'acc': 91.25, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 83.15, 'acc': 83.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.41, 'acc': 35.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.17, 'acc': 70.46, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 74.41, 'acc': 74.13, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.02, 'acc': 86.71, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.55, 'acc': 80.29, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.77, 'acc': 81.07, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.6, 'acc': 55.23, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.69, 'acc': 57.33, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 21:54:00,164 : STS12 p=0.5746, STS12 s=0.5872, STS13 p=0.6112, STS13 s=0.6131, STS14 p=0.6545, STS14 s=0.6357, STS15 p=0.7096, STS15 s=0.7091, STS 16 p=0.6693, STS16 s=0.6746, STS B p=0.7248, STS B s=0.7182, STS B m=1.3392, SICK-R p=0.8309, SICK-R s=0.7562, SICK-P m=0.3160
2019-03-12 21:54:00,164 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 21:54:00,164 : 0.5746,0.5872,0.6112,0.6131,0.6545,0.6357,0.7096,0.7091,0.6693,0.6746,0.7248,0.7182,1.3392,0.8309,0.7562,0.3160
2019-03-12 21:54:00,164 : MR=75.27, CR=78.46, SUBJ=93.42, MPQA=87.92, SST-B=79.74, SST-F=40.27, TREC=89.00, SICK-E=80.21, SNLI=62.66, MRPC=67.48, MRPC f=71.71
2019-03-12 21:54:00,164 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 21:54:00,164 : 75.27,78.46,93.42,87.92,79.74,40.27,89.00,80.21,62.66,67.48,71.71
2019-03-12 21:54:00,164 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 21:54:00,164 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 21:54:00,164 : na,na,na,na,na,na,na,na,na,na
2019-03-12 21:54:00,164 : SentLen=91.25, WC=83.80, TreeDepth=35.25, TopConst=70.46, BShift=74.13, Tense=86.71, SubjNum=80.29, ObjNum=81.07, SOMO=55.23, CoordInv=57.33, average=71.55
2019-03-12 21:54:00,164 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 21:54:00,164 : 91.25,83.80,35.25,70.46,74.13,86.71,80.29,81.07,55.23,57.33,71.55
2019-03-12 21:54:00,164 : ********************************************************************************
2019-03-12 21:54:00,164 : ********************************************************************************
2019-03-12 21:54:00,164 : ********************************************************************************
2019-03-12 21:54:00,164 : layer 6
2019-03-12 21:54:00,164 : ********************************************************************************
2019-03-12 21:54:00,164 : ********************************************************************************
2019-03-12 21:54:00,165 : ********************************************************************************
2019-03-12 21:54:00,258 : ***** Transfer task : STS12 *****


2019-03-12 21:54:00,271 : loading BERT model bert-large-uncased
2019-03-12 21:54:00,271 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:54:00,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:54:00,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2vimml0z
2019-03-12 21:54:07,747 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:54:17,130 : MSRpar : pearson = 0.3892, spearman = 0.4242
2019-03-12 21:54:18,778 : MSRvid : pearson = 0.6452, spearman = 0.6463
2019-03-12 21:54:20,196 : SMTeuroparl : pearson = 0.5098, spearman = 0.5994
2019-03-12 21:54:22,902 : surprise.OnWN : pearson = 0.7083, spearman = 0.6937
2019-03-12 21:54:24,338 : surprise.SMTnews : pearson = 0.5850, spearman = 0.5596
2019-03-12 21:54:24,338 : ALL (weighted average) : Pearson = 0.5709,             Spearman = 0.5861
2019-03-12 21:54:24,338 : ALL (average) : Pearson = 0.5675,             Spearman = 0.5846

2019-03-12 21:54:24,338 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 21:54:24,347 : loading BERT model bert-large-uncased
2019-03-12 21:54:24,347 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:54:24,364 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:54:24,364 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2zp8eu52
2019-03-12 21:54:31,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:54:38,468 : FNWN : pearson = 0.4689, spearman = 0.4903
2019-03-12 21:54:40,371 : headlines : pearson = 0.6759, spearman = 0.6543
2019-03-12 21:54:41,846 : OnWN : pearson = 0.6047, spearman = 0.6242
2019-03-12 21:54:41,846 : ALL (weighted average) : Pearson = 0.6232,             Spearman = 0.6224
2019-03-12 21:54:41,846 : ALL (average) : Pearson = 0.5832,             Spearman = 0.5896

2019-03-12 21:54:41,846 : ***** Transfer task : STS14 *****


2019-03-12 21:54:41,861 : loading BERT model bert-large-uncased
2019-03-12 21:54:41,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:54:41,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:54:41,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd75zwaxc
2019-03-12 21:54:49,388 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:54:56,036 : deft-forum : pearson = 0.3917, spearman = 0.3920
2019-03-12 21:54:57,679 : deft-news : pearson = 0.7393, spearman = 0.6983
2019-03-12 21:54:59,862 : headlines : pearson = 0.6339, spearman = 0.5820
2019-03-12 21:55:01,947 : images : pearson = 0.6845, spearman = 0.6632
2019-03-12 21:55:04,086 : OnWN : pearson = 0.7172, spearman = 0.7470
2019-03-12 21:55:06,963 : tweet-news : pearson = 0.7283, spearman = 0.6758
2019-03-12 21:55:06,964 : ALL (weighted average) : Pearson = 0.6589,             Spearman = 0.6365
2019-03-12 21:55:06,964 : ALL (average) : Pearson = 0.6492,             Spearman = 0.6264

2019-03-12 21:55:06,964 : ***** Transfer task : STS15 *****


2019-03-12 21:55:07,029 : loading BERT model bert-large-uncased
2019-03-12 21:55:07,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:55:07,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:55:07,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxt3cw3bs
2019-03-12 21:55:14,502 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:55:21,566 : answers-forums : pearson = 0.6045, spearman = 0.5829
2019-03-12 21:55:23,658 : answers-students : pearson = 0.7267, spearman = 0.7321
2019-03-12 21:55:25,712 : belief : pearson = 0.7295, spearman = 0.7488
2019-03-12 21:55:27,965 : headlines : pearson = 0.6952, spearman = 0.6792
2019-03-12 21:55:30,110 : images : pearson = 0.7723, spearman = 0.7794
2019-03-12 21:55:30,110 : ALL (weighted average) : Pearson = 0.7153,             Spearman = 0.7141
2019-03-12 21:55:30,110 : ALL (average) : Pearson = 0.7056,             Spearman = 0.7045

2019-03-12 21:55:30,110 : ***** Transfer task : STS16 *****


2019-03-12 21:55:30,180 : loading BERT model bert-large-uncased
2019-03-12 21:55:30,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:55:30,197 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:55:30,198 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1md7reaa
2019-03-12 21:55:37,639 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:55:43,752 : answer-answer : pearson = 0.5292, spearman = 0.5364
2019-03-12 21:55:44,413 : headlines : pearson = 0.6931, spearman = 0.6928
2019-03-12 21:55:45,299 : plagiarism : pearson = 0.7907, spearman = 0.8012
2019-03-12 21:55:46,798 : postediting : pearson = 0.8321, spearman = 0.8503
2019-03-12 21:55:47,406 : question-question : pearson = 0.4999, spearman = 0.4973
2019-03-12 21:55:47,406 : ALL (weighted average) : Pearson = 0.6715,             Spearman = 0.6783
2019-03-12 21:55:47,406 : ALL (average) : Pearson = 0.6690,             Spearman = 0.6756

2019-03-12 21:55:47,406 : ***** Transfer task : MR *****


2019-03-12 21:55:47,421 : loading BERT model bert-large-uncased
2019-03-12 21:55:47,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:55:47,441 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:55:47,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw1y0at1d
2019-03-12 21:55:54,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:56:00,225 : Generating sentence embeddings
2019-03-12 21:56:31,831 : Generated sentence embeddings
2019-03-12 21:56:31,832 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:56:41,326 : Best param found at split 1: l2reg = 1e-05                 with score 75.74
2019-03-12 21:56:52,200 : Best param found at split 2: l2reg = 1e-05                 with score 76.11
2019-03-12 21:57:02,727 : Best param found at split 3: l2reg = 1e-05                 with score 76.76
2019-03-12 21:57:13,069 : Best param found at split 4: l2reg = 0.001                 with score 76.31
2019-03-12 21:57:23,217 : Best param found at split 5: l2reg = 1e-05                 with score 75.84
2019-03-12 21:57:23,746 : Dev acc : 76.15 Test acc : 75.0

2019-03-12 21:57:23,747 : ***** Transfer task : CR *****


2019-03-12 21:57:23,755 : loading BERT model bert-large-uncased
2019-03-12 21:57:23,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:57:23,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:57:23,775 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe8j6ksb2
2019-03-12 21:57:31,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:57:36,441 : Generating sentence embeddings
2019-03-12 21:57:44,803 : Generated sentence embeddings
2019-03-12 21:57:44,803 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:57:48,605 : Best param found at split 1: l2reg = 0.01                 with score 80.36
2019-03-12 21:57:52,170 : Best param found at split 2: l2reg = 0.001                 with score 79.99
2019-03-12 21:57:56,224 : Best param found at split 3: l2reg = 1e-05                 with score 80.03
2019-03-12 21:57:59,946 : Best param found at split 4: l2reg = 1e-05                 with score 79.74
2019-03-12 21:58:03,659 : Best param found at split 5: l2reg = 0.01                 with score 79.94
2019-03-12 21:58:03,848 : Dev acc : 80.01 Test acc : 78.36

2019-03-12 21:58:03,848 : ***** Transfer task : MPQA *****


2019-03-12 21:58:03,854 : loading BERT model bert-large-uncased
2019-03-12 21:58:03,854 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:58:03,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:58:03,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7cjqlwma
2019-03-12 21:58:11,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:58:16,680 : Generating sentence embeddings
2019-03-12 21:58:24,300 : Generated sentence embeddings
2019-03-12 21:58:24,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:58:33,475 : Best param found at split 1: l2reg = 0.0001                 with score 87.34
2019-03-12 21:58:44,946 : Best param found at split 2: l2reg = 1e-05                 with score 87.48
2019-03-12 21:58:53,849 : Best param found at split 3: l2reg = 0.001                 with score 88.13
2019-03-12 21:59:01,873 : Best param found at split 4: l2reg = 0.001                 with score 87.71
2019-03-12 21:59:13,217 : Best param found at split 5: l2reg = 0.0001                 with score 87.47
2019-03-12 21:59:13,769 : Dev acc : 87.63 Test acc : 87.21

2019-03-12 21:59:13,770 : ***** Transfer task : SUBJ *****


2019-03-12 21:59:13,786 : loading BERT model bert-large-uncased
2019-03-12 21:59:13,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:59:13,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:59:13,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmposdio17s
2019-03-12 21:59:21,295 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:59:26,622 : Generating sentence embeddings
2019-03-12 21:59:57,612 : Generated sentence embeddings
2019-03-12 21:59:57,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 22:00:06,117 : Best param found at split 1: l2reg = 0.0001                 with score 94.02
2019-03-12 22:00:15,929 : Best param found at split 2: l2reg = 0.001                 with score 94.14
2019-03-12 22:00:25,273 : Best param found at split 3: l2reg = 0.001                 with score 93.88
2019-03-12 22:00:35,820 : Best param found at split 4: l2reg = 0.001                 with score 94.11
2019-03-12 22:00:45,961 : Best param found at split 5: l2reg = 0.001                 with score 94.09
2019-03-12 22:00:46,394 : Dev acc : 94.05 Test acc : 92.95

2019-03-12 22:00:46,395 : ***** Transfer task : SST Binary classification *****


2019-03-12 22:00:46,486 : loading BERT model bert-large-uncased
2019-03-12 22:00:46,486 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:00:46,561 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:00:46,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkyhb8wxv
2019-03-12 22:00:54,039 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:00:59,427 : Computing embedding for train
2019-03-12 22:02:40,120 : Computed train embeddings
2019-03-12 22:02:40,120 : Computing embedding for dev
2019-03-12 22:02:42,313 : Computed dev embeddings
2019-03-12 22:02:42,314 : Computing embedding for test
2019-03-12 22:02:46,928 : Computed test embeddings
2019-03-12 22:02:46,928 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:03:13,321 : [('reg:1e-05', 79.47), ('reg:0.0001', 79.82), ('reg:0.001', 77.52), ('reg:0.01', 78.21)]
2019-03-12 22:03:13,321 : Validation : best param found is reg = 0.0001 with score             79.82
2019-03-12 22:03:13,321 : Evaluating...
2019-03-12 22:03:21,255 : 
Dev acc : 79.82 Test acc : 80.94 for             SST Binary classification

2019-03-12 22:03:21,256 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 22:03:21,311 : loading BERT model bert-large-uncased
2019-03-12 22:03:21,311 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:03:21,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:03:21,332 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7narelq6
2019-03-12 22:03:28,837 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:03:34,065 : Computing embedding for train
2019-03-12 22:03:55,987 : Computed train embeddings
2019-03-12 22:03:55,987 : Computing embedding for dev
2019-03-12 22:03:58,854 : Computed dev embeddings
2019-03-12 22:03:58,854 : Computing embedding for test
2019-03-12 22:04:04,509 : Computed test embeddings
2019-03-12 22:04:04,509 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:04:07,004 : [('reg:1e-05', 38.06), ('reg:0.0001', 39.15), ('reg:0.001', 38.33), ('reg:0.01', 40.42)]
2019-03-12 22:04:07,004 : Validation : best param found is reg = 0.01 with score             40.42
2019-03-12 22:04:07,004 : Evaluating...
2019-03-12 22:04:07,642 : 
Dev acc : 40.42 Test acc : 41.36 for             SST Fine-Grained classification

2019-03-12 22:04:07,643 : ***** Transfer task : TREC *****


2019-03-12 22:04:07,657 : loading BERT model bert-large-uncased
2019-03-12 22:04:07,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:04:07,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:04:07,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2rjqjcxg
2019-03-12 22:04:15,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:04:28,107 : Computed train embeddings
2019-03-12 22:04:28,697 : Computed test embeddings
2019-03-12 22:04:28,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 22:04:34,527 : [('reg:1e-05', 80.56), ('reg:0.0001', 80.36), ('reg:0.001', 78.61), ('reg:0.01', 70.86)]
2019-03-12 22:04:34,527 : Cross-validation : best param found is reg = 1e-05             with score 80.56
2019-03-12 22:04:34,527 : Evaluating...
2019-03-12 22:04:34,816 : 
Dev acc : 80.56 Test acc : 90.6             for TREC

2019-03-12 22:04:34,817 : ***** Transfer task : MRPC *****


2019-03-12 22:04:34,837 : loading BERT model bert-large-uncased
2019-03-12 22:04:34,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:04:34,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:04:34,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8orltlml
2019-03-12 22:04:42,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:04:47,556 : Computing embedding for train
2019-03-12 22:05:09,910 : Computed train embeddings
2019-03-12 22:05:09,910 : Computing embedding for test
2019-03-12 22:05:19,694 : Computed test embeddings
2019-03-12 22:05:19,715 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 22:05:24,417 : [('reg:1e-05', 74.12), ('reg:0.0001', 74.04), ('reg:0.001', 73.99), ('reg:0.01', 73.6)]
2019-03-12 22:05:24,417 : Cross-validation : best param found is reg = 1e-05             with score 74.12
2019-03-12 22:05:24,417 : Evaluating...
2019-03-12 22:05:24,807 : Dev acc : 74.12 Test acc 70.03; Test F1 75.13 for MRPC.

2019-03-12 22:05:24,808 : ***** Transfer task : SICK-Entailment*****


2019-03-12 22:05:24,889 : loading BERT model bert-large-uncased
2019-03-12 22:05:24,889 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:05:24,910 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:05:24,910 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy7b88tn2
2019-03-12 22:05:32,384 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:05:37,616 : Computing embedding for train
2019-03-12 22:05:48,954 : Computed train embeddings
2019-03-12 22:05:48,954 : Computing embedding for dev
2019-03-12 22:05:50,500 : Computed dev embeddings
2019-03-12 22:05:50,500 : Computing embedding for test
2019-03-12 22:06:02,671 : Computed test embeddings
2019-03-12 22:06:02,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:06:04,101 : [('reg:1e-05', 79.8), ('reg:0.0001', 80.2), ('reg:0.001', 80.4), ('reg:0.01', 77.8)]
2019-03-12 22:06:04,102 : Validation : best param found is reg = 0.001 with score             80.4
2019-03-12 22:06:04,102 : Evaluating...
2019-03-12 22:06:04,456 : 
Dev acc : 80.4 Test acc : 79.18 for                        SICK entailment

2019-03-12 22:06:04,457 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 22:06:04,483 : loading BERT model bert-large-uncased
2019-03-12 22:06:04,484 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:06:04,541 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:06:04,541 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiuz0peh_
2019-03-12 22:06:11,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:06:17,313 : Computing embedding for train
2019-03-12 22:06:28,683 : Computed train embeddings
2019-03-12 22:06:28,683 : Computing embedding for dev
2019-03-12 22:06:30,233 : Computed dev embeddings
2019-03-12 22:06:30,233 : Computing embedding for test
2019-03-12 22:06:42,424 : Computed test embeddings
2019-03-12 22:06:56,791 : Dev : Pearson 0.8145256806760229
2019-03-12 22:06:56,791 : Test : Pearson 0.8279778870623246 Spearman 0.752438973329266 MSE 0.32227769860374256                        for SICK Relatedness

2019-03-12 22:06:56,792 : 

***** Transfer task : STSBenchmark*****


2019-03-12 22:06:56,831 : loading BERT model bert-large-uncased
2019-03-12 22:06:56,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:06:56,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:06:56,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz217t5by
2019-03-12 22:07:04,333 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:07:09,595 : Computing embedding for train
2019-03-12 22:07:28,295 : Computed train embeddings
2019-03-12 22:07:28,296 : Computing embedding for dev
2019-03-12 22:07:33,963 : Computed dev embeddings
2019-03-12 22:07:33,963 : Computing embedding for test
2019-03-12 22:07:38,585 : Computed test embeddings
2019-03-12 22:07:57,674 : Dev : Pearson 0.7493072387292938
2019-03-12 22:07:57,675 : Test : Pearson 0.7243887991143818 Spearman 0.720316457354009 MSE 1.3233994750715283                        for SICK Relatedness

2019-03-12 22:07:57,675 : ***** Transfer task : SNLI Entailment*****


2019-03-12 22:08:02,776 : loading BERT model bert-large-uncased
2019-03-12 22:08:02,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:08:02,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:08:02,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvfy_z0_s
2019-03-12 22:08:10,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:08:16,016 : PROGRESS (encoding): 0.00%
2019-03-12 22:11:03,185 : PROGRESS (encoding): 14.56%
2019-03-12 22:14:15,771 : PROGRESS (encoding): 29.12%
2019-03-12 22:17:27,375 : PROGRESS (encoding): 43.69%
2019-03-12 22:20:50,854 : PROGRESS (encoding): 58.25%
2019-03-12 22:24:37,745 : PROGRESS (encoding): 72.81%
2019-03-12 22:28:23,182 : PROGRESS (encoding): 87.37%
2019-03-12 22:32:27,098 : PROGRESS (encoding): 0.00%
2019-03-12 22:32:57,761 : PROGRESS (encoding): 0.00%
2019-03-12 22:33:27,232 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:33:51,518 : [('reg:1e-09', 65.99)]
2019-03-12 22:33:51,518 : Validation : best param found is reg = 1e-09 with score             65.99
2019-03-12 22:33:51,518 : Evaluating...
2019-03-12 22:34:16,898 : Dev acc : 65.99 Test acc : 66.6 for SNLI

2019-03-12 22:34:16,898 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 22:34:17,103 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 22:34:18,194 : loading BERT model bert-large-uncased
2019-03-12 22:34:18,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:34:18,221 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:34:18,222 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwo1ji1bc
2019-03-12 22:34:25,696 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:34:31,087 : Computing embeddings for train/dev/test
2019-03-12 22:38:03,640 : Computed embeddings
2019-03-12 22:38:03,640 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:38:30,515 : [('reg:1e-05', 90.54), ('reg:0.0001', 88.81), ('reg:0.001', 84.89), ('reg:0.01', 76.08)]
2019-03-12 22:38:30,515 : Validation : best param found is reg = 1e-05 with score             90.54
2019-03-12 22:38:30,515 : Evaluating...
2019-03-12 22:38:37,038 : 
Dev acc : 90.5 Test acc : 90.6 for LENGTH classification

2019-03-12 22:38:37,039 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 22:38:37,413 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 22:38:37,458 : loading BERT model bert-large-uncased
2019-03-12 22:38:37,458 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:38:37,488 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:38:37,488 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9lpp0bhf
2019-03-12 22:38:44,923 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:38:50,218 : Computing embeddings for train/dev/test
2019-03-12 22:42:06,237 : Computed embeddings
2019-03-12 22:42:06,237 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:42:37,131 : [('reg:1e-05', 77.83), ('reg:0.0001', 43.45), ('reg:0.001', 3.38), ('reg:0.01', 1.01)]
2019-03-12 22:42:37,131 : Validation : best param found is reg = 1e-05 with score             77.83
2019-03-12 22:42:37,131 : Evaluating...
2019-03-12 22:42:43,801 : 
Dev acc : 77.8 Test acc : 77.8 for WORDCONTENT classification

2019-03-12 22:42:43,802 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 22:42:44,178 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 22:42:44,244 : loading BERT model bert-large-uncased
2019-03-12 22:42:44,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:42:44,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:42:44,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9tdfgjy3
2019-03-12 22:42:51,729 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:42:57,031 : Computing embeddings for train/dev/test
2019-03-12 22:46:01,659 : Computed embeddings
2019-03-12 22:46:01,659 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:46:27,628 : [('reg:1e-05', 35.02), ('reg:0.0001', 34.76), ('reg:0.001', 31.83), ('reg:0.01', 26.65)]
2019-03-12 22:46:27,628 : Validation : best param found is reg = 1e-05 with score             35.02
2019-03-12 22:46:27,628 : Evaluating...
2019-03-12 22:46:34,064 : 
Dev acc : 35.0 Test acc : 35.0 for DEPTH classification

2019-03-12 22:46:34,065 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 22:46:34,446 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 22:46:34,510 : loading BERT model bert-large-uncased
2019-03-12 22:46:34,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:46:34,618 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:46:34,618 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuk_bof9i
2019-03-12 22:46:42,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:46:47,402 : Computing embeddings for train/dev/test
2019-03-12 22:49:37,897 : Computed embeddings
2019-03-12 22:49:37,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:50:05,565 : [('reg:1e-05', 69.88), ('reg:0.0001', 67.9), ('reg:0.001', 60.74), ('reg:0.01', 42.99)]
2019-03-12 22:50:05,565 : Validation : best param found is reg = 1e-05 with score             69.88
2019-03-12 22:50:05,565 : Evaluating...
2019-03-12 22:50:13,290 : 
Dev acc : 69.9 Test acc : 69.6 for TOPCONSTITUENTS classification

2019-03-12 22:50:13,291 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 22:50:13,634 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 22:50:13,700 : loading BERT model bert-large-uncased
2019-03-12 22:50:13,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:50:13,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:50:13,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp04vj469d
2019-03-12 22:50:21,280 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:50:26,500 : Computing embeddings for train/dev/test
2019-03-12 22:53:31,534 : Computed embeddings
2019-03-12 22:53:31,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:54:03,864 : [('reg:1e-05', 74.7), ('reg:0.0001', 74.82), ('reg:0.001', 74.38), ('reg:0.01', 70.63)]
2019-03-12 22:54:03,864 : Validation : best param found is reg = 0.0001 with score             74.82
2019-03-12 22:54:03,864 : Evaluating...
2019-03-12 22:54:11,344 : 
Dev acc : 74.8 Test acc : 74.0 for BIGRAMSHIFT classification

2019-03-12 22:54:11,345 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 22:54:11,895 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 22:54:11,960 : loading BERT model bert-large-uncased
2019-03-12 22:54:11,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:54:11,989 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:54:11,989 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcg890vt5
2019-03-12 22:54:19,451 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:54:24,778 : Computing embeddings for train/dev/test
2019-03-12 22:57:25,714 : Computed embeddings
2019-03-12 22:57:25,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:57:46,540 : [('reg:1e-05', 87.58), ('reg:0.0001', 87.91), ('reg:0.001', 88.57), ('reg:0.01', 88.14)]
2019-03-12 22:57:46,540 : Validation : best param found is reg = 0.001 with score             88.57
2019-03-12 22:57:46,540 : Evaluating...
2019-03-12 22:57:52,970 : 
Dev acc : 88.6 Test acc : 87.2 for TENSE classification

2019-03-12 22:57:52,971 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 22:57:53,392 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 22:57:53,456 : loading BERT model bert-large-uncased
2019-03-12 22:57:53,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:57:53,481 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:57:53,482 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp69aau0i_
2019-03-12 22:58:00,958 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:58:06,243 : Computing embeddings for train/dev/test
2019-03-12 23:01:18,402 : Computed embeddings
2019-03-12 23:01:18,402 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:01:47,726 : [('reg:1e-05', 81.91), ('reg:0.0001', 81.82), ('reg:0.001', 81.55), ('reg:0.01', 79.93)]
2019-03-12 23:01:47,726 : Validation : best param found is reg = 1e-05 with score             81.91
2019-03-12 23:01:47,727 : Evaluating...
2019-03-12 23:01:53,256 : 
Dev acc : 81.9 Test acc : 80.7 for SUBJNUMBER classification

2019-03-12 23:01:53,257 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 23:01:53,664 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 23:01:53,730 : loading BERT model bert-large-uncased
2019-03-12 23:01:53,731 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:01:53,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:01:53,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpttkgid4e
2019-03-12 23:02:01,333 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:02:06,694 : Computing embeddings for train/dev/test
2019-03-12 23:05:14,763 : Computed embeddings
2019-03-12 23:05:14,763 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:05:37,546 : [('reg:1e-05', 81.5), ('reg:0.0001', 81.6), ('reg:0.001', 81.53), ('reg:0.01', 79.36)]
2019-03-12 23:05:37,547 : Validation : best param found is reg = 0.0001 with score             81.6
2019-03-12 23:05:37,547 : Evaluating...
2019-03-12 23:05:42,940 : 
Dev acc : 81.6 Test acc : 82.3 for OBJNUMBER classification

2019-03-12 23:05:42,941 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 23:05:43,520 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 23:05:43,588 : loading BERT model bert-large-uncased
2019-03-12 23:05:43,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:05:43,615 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:05:43,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3dgsocw3
2019-03-12 23:05:51,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:05:56,462 : Computing embeddings for train/dev/test
2019-03-12 23:09:34,568 : Computed embeddings
2019-03-12 23:09:34,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:09:59,843 : [('reg:1e-05', 55.86), ('reg:0.0001', 55.88), ('reg:0.001', 55.85), ('reg:0.01', 55.43)]
2019-03-12 23:09:59,843 : Validation : best param found is reg = 0.0001 with score             55.88
2019-03-12 23:09:59,843 : Evaluating...
2019-03-12 23:10:04,723 : 
Dev acc : 55.9 Test acc : 56.0 for ODDMANOUT classification

2019-03-12 23:10:04,724 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 23:10:05,133 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 23:10:05,212 : loading BERT model bert-large-uncased
2019-03-12 23:10:05,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:10:05,345 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:10:05,345 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz4508tdx
2019-03-12 23:10:12,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:18,244 : Computing embeddings for train/dev/test
2019-03-12 23:13:54,484 : Computed embeddings
2019-03-12 23:13:54,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:14:21,247 : [('reg:1e-05', 58.41), ('reg:0.0001', 53.37), ('reg:0.001', 52.61), ('reg:0.01', 53.75)]
2019-03-12 23:14:21,248 : Validation : best param found is reg = 1e-05 with score             58.41
2019-03-12 23:14:21,248 : Evaluating...
2019-03-12 23:14:30,831 : 
Dev acc : 58.4 Test acc : 58.2 for COORDINATIONINVERSION classification

2019-03-12 23:14:30,833 : total results: {'STS12': {'MSRpar': {'pearson': (0.3891577144039914, 1.5779556013914533e-28), 'spearman': SpearmanrResult(correlation=0.42416728209959637, pvalue=4.143354536197415e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.645169668079545, 1.6784837612183307e-89), 'spearman': SpearmanrResult(correlation=0.6463168456795616, pvalue=6.477260179393002e-90), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5098474062984808, 9.739968988111585e-32), 'spearman': SpearmanrResult(correlation=0.5993546386151697, pvalue=4.211694473006393e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.70833440739344, 2.9064119715920387e-115), 'spearman': SpearmanrResult(correlation=0.6936977636566548, pvalue=1.066218813838654e-108), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5849607035139953, 5.338982569118579e-38), 'spearman': SpearmanrResult(correlation=0.5595944570078457, pvalue=2.9487466417254323e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5674939799378904, 'wmean': 0.570918797490611}, 'spearman': {'mean': 0.5846261974117656, 'wmean': 0.5860741267848626}}}, 'STS13': {'FNWN': {'pearson': (0.4689152160000559, 1.008217412581516e-11), 'spearman': SpearmanrResult(correlation=0.4902837390451424, pvalue=8.014370189540725e-13), 'nsamples': 189}, 'headlines': {'pearson': (0.6759029127971612, 3.132159840217945e-101), 'spearman': SpearmanrResult(correlation=0.6542953788545521, pvalue=7.679314056719845e-93), 'nsamples': 750}, 'OnWN': {'pearson': (0.6047393408017, 3.052533437150545e-57), 'spearman': SpearmanrResult(correlation=0.6242362562094528, pvalue=6.261027671871347e-62), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5831858231996391, 'wmean': 0.6232072870744234}, 'spearman': {'mean': 0.589605124703049, 'wmean': 0.6223878003692994}}}, 'STS14': {'deft-forum': {'pearson': (0.391723690124394, 5.920883698033915e-18), 'spearman': SpearmanrResult(correlation=0.3919902187972955, pvalue=5.598767744359389e-18), 'nsamples': 450}, 'deft-news': {'pearson': (0.7392708593948037, 4.182473665325738e-53), 'spearman': SpearmanrResult(correlation=0.6982927328858745, pvalue=3.5405593085001126e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.6339212674719992, 1.5390572861325918e-85), 'spearman': SpearmanrResult(correlation=0.5819570990056477, pvalue=3.421740485874282e-69), 'nsamples': 750}, 'images': {'pearson': (0.6845472880652522, 8.608373028905829e-105), 'spearman': SpearmanrResult(correlation=0.6631707053231081, pvalue=3.343055687696625e-96), 'nsamples': 750}, 'OnWN': {'pearson': (0.7171619457856796, 2.0102209293975072e-119), 'spearman': SpearmanrResult(correlation=0.7469682542928378, pvalue=9.843141262241859e-135), 'nsamples': 750}, 'tweet-news': {'pearson': (0.7283098879563341, 6.548082829592748e-125), 'spearman': SpearmanrResult(correlation=0.6757756691807005, pvalue=3.526549712776236e-101), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6491558231330771, 'wmean': 0.6589365894223645}, 'spearman': {'mean': 0.6263591132475773, 'wmean': 0.6364765904470042}}}, 'STS15': {'answers-forums': {'pearson': (0.6044560072493378, 1.0067161714988106e-38), 'spearman': SpearmanrResult(correlation=0.5828947396974796, pvalue=1.6657269308662595e-35), 'nsamples': 375}, 'answers-students': {'pearson': (0.7267474788028663, 3.995120596013082e-124), 'spearman': SpearmanrResult(correlation=0.7320981225878869, pvalue=7.741403100337358e-127), 'nsamples': 750}, 'belief': {'pearson': (0.7295493942356427, 1.624168199973672e-63), 'spearman': SpearmanrResult(correlation=0.7487967498825572, pvalue=1.3065445447800554e-68), 'nsamples': 375}, 'headlines': {'pearson': (0.6951746314885757, 2.4179630715562724e-109), 'spearman': SpearmanrResult(correlation=0.6791554479958603, pvalue=1.4804889276129056e-102), 'nsamples': 750}, 'images': {'pearson': (0.77225124454783, 1.6356921426610587e-149), 'spearman': SpearmanrResult(correlation=0.7793665534014235, pvalue=5.070860925631321e-154), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.7056357512648506, 'wmean': 0.7152940138954406}, 'spearman': {'mean': 0.7044623227130413, 'wmean': 0.7141164671937973}}}, 'STS16': {'answer-answer': {'pearson': (0.5291837677879863, 9.865318746956206e-20), 'spearman': SpearmanrResult(correlation=0.536439732555764, pvalue=2.4974239883612023e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6931417742889392, 5.5322763315904284e-37), 'spearman': SpearmanrResult(correlation=0.6928236532192302, pvalue=6.146059510831782e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7907315305421091, 1.6945353834551234e-50), 'spearman': SpearmanrResult(correlation=0.8012428659821046, pvalue=9.15620419846367e-53), 'nsamples': 230}, 'postediting': {'pearson': (0.8320979047603616, 6.873882539987163e-64), 'spearman': SpearmanrResult(correlation=0.850348551039736, pvalue=2.0034509003381622e-69), 'nsamples': 244}, 'question-question': {'pearson': (0.49985363640829744, 1.3066727614680861e-14), 'spearman': SpearmanrResult(correlation=0.497325449402784, pvalue=1.858234308503736e-14), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6690017227575387, 'wmean': 0.6714793672948076}, 'spearman': {'mean': 0.6756360504399237, 'wmean': 0.6783142548697418}}}, 'MR': {'devacc': 76.15, 'acc': 75.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.01, 'acc': 78.36, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.63, 'acc': 87.21, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.05, 'acc': 92.95, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.82, 'acc': 80.94, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.42, 'acc': 41.36, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.56, 'acc': 90.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.12, 'acc': 70.03, 'f1': 75.13, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.4, 'acc': 79.18, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8145256806760229, 'pearson': 0.8279778870623246, 'spearman': 0.752438973329266, 'mse': 0.32227769860374256, 'yhat': array([3.38769394, 4.45637212, 1.37161224, ..., 3.60829364, 4.34977342,        4.27605926]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7493072387292938, 'pearson': 0.7243887991143818, 'spearman': 0.720316457354009, 'mse': 1.3233994750715283, 'yhat': array([1.59783218, 1.15983087, 2.3679814 , ..., 3.88510189, 3.47221163,        3.62418506]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.99, 'acc': 66.6, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 90.54, 'acc': 90.56, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 77.83, 'acc': 77.83, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.02, 'acc': 35.05, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.88, 'acc': 69.6, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 74.82, 'acc': 74.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.57, 'acc': 87.2, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.91, 'acc': 80.7, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.6, 'acc': 82.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.88, 'acc': 56.0, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.41, 'acc': 58.17, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 23:14:30,833 : STS12 p=0.5709, STS12 s=0.5861, STS13 p=0.6232, STS13 s=0.6224, STS14 p=0.6589, STS14 s=0.6365, STS15 p=0.7153, STS15 s=0.7141, STS 16 p=0.6715, STS16 s=0.6783, STS B p=0.7244, STS B s=0.7203, STS B m=1.3234, SICK-R p=0.8280, SICK-R s=0.7524, SICK-P m=0.3223
2019-03-12 23:14:30,833 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 23:14:30,833 : 0.5709,0.5861,0.6232,0.6224,0.6589,0.6365,0.7153,0.7141,0.6715,0.6783,0.7244,0.7203,1.3234,0.8280,0.7524,0.3223
2019-03-12 23:14:30,833 : MR=75.00, CR=78.36, SUBJ=92.95, MPQA=87.21, SST-B=80.94, SST-F=41.36, TREC=90.60, SICK-E=79.18, SNLI=66.60, MRPC=70.03, MRPC f=75.13
2019-03-12 23:14:30,833 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 23:14:30,833 : 75.00,78.36,92.95,87.21,80.94,41.36,90.60,79.18,66.60,70.03,75.13
2019-03-12 23:14:30,833 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 23:14:30,833 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 23:14:30,833 : na,na,na,na,na,na,na,na,na,na
2019-03-12 23:14:30,833 : SentLen=90.56, WC=77.83, TreeDepth=35.05, TopConst=69.60, BShift=74.00, Tense=87.20, SubjNum=80.70, ObjNum=82.35, SOMO=56.00, CoordInv=58.17, average=71.15
2019-03-12 23:14:30,833 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 23:14:30,833 : 90.56,77.83,35.05,69.60,74.00,87.20,80.70,82.35,56.00,58.17,71.15
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,833 : layer 7
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,833 : ********************************************************************************
2019-03-12 23:14:30,921 : ***** Transfer task : STS12 *****


2019-03-12 23:14:30,934 : loading BERT model bert-large-uncased
2019-03-12 23:14:30,934 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:14:30,952 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:14:30,952 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphq07lv_v
2019-03-12 23:14:38,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:14:47,825 : MSRpar : pearson = 0.3844, spearman = 0.4237
2019-03-12 23:14:49,473 : MSRvid : pearson = 0.5749, spearman = 0.5790
2019-03-12 23:14:50,891 : SMTeuroparl : pearson = 0.5076, spearman = 0.5997
2019-03-12 23:14:53,593 : surprise.OnWN : pearson = 0.6722, spearman = 0.6764
2019-03-12 23:14:55,027 : surprise.SMTnews : pearson = 0.5837, spearman = 0.5526
2019-03-12 23:14:55,027 : ALL (weighted average) : Pearson = 0.5436,             Spearman = 0.5647
2019-03-12 23:14:55,027 : ALL (average) : Pearson = 0.5446,             Spearman = 0.5663

2019-03-12 23:14:55,027 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 23:14:55,040 : loading BERT model bert-large-uncased
2019-03-12 23:14:55,040 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:14:55,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:14:55,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnrexvfm0
2019-03-12 23:15:02,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:15:09,118 : FNWN : pearson = 0.4476, spearman = 0.4604
2019-03-12 23:15:11,019 : headlines : pearson = 0.6652, spearman = 0.6437
2019-03-12 23:15:12,493 : OnWN : pearson = 0.5487, spearman = 0.5733
2019-03-12 23:15:12,494 : ALL (weighted average) : Pearson = 0.5942,             Spearman = 0.5942
2019-03-12 23:15:12,494 : ALL (average) : Pearson = 0.5538,             Spearman = 0.5591

2019-03-12 23:15:12,494 : ***** Transfer task : STS14 *****


2019-03-12 23:15:12,510 : loading BERT model bert-large-uncased
2019-03-12 23:15:12,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:15:12,528 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:15:12,528 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvjhcd28_
2019-03-12 23:15:19,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:15:26,658 : deft-forum : pearson = 0.3520, spearman = 0.3725
2019-03-12 23:15:28,302 : deft-news : pearson = 0.7375, spearman = 0.6977
2019-03-12 23:15:30,482 : headlines : pearson = 0.6218, spearman = 0.5698
2019-03-12 23:15:32,565 : images : pearson = 0.6196, spearman = 0.6081
2019-03-12 23:15:34,705 : OnWN : pearson = 0.6763, spearman = 0.7118
2019-03-12 23:15:37,575 : tweet-news : pearson = 0.6890, spearman = 0.6456
2019-03-12 23:15:37,575 : ALL (weighted average) : Pearson = 0.6226,             Spearman = 0.6076
2019-03-12 23:15:37,575 : ALL (average) : Pearson = 0.6160,             Spearman = 0.6009

2019-03-12 23:15:37,575 : ***** Transfer task : STS15 *****


2019-03-12 23:15:37,621 : loading BERT model bert-large-uncased
2019-03-12 23:15:37,622 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:15:37,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:15:37,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb2fv8xme
2019-03-12 23:15:45,104 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:15:52,405 : answers-forums : pearson = 0.5535, spearman = 0.5359
2019-03-12 23:15:54,501 : answers-students : pearson = 0.7064, spearman = 0.7192
2019-03-12 23:15:56,555 : belief : pearson = 0.6792, spearman = 0.7032
2019-03-12 23:15:58,810 : headlines : pearson = 0.6823, spearman = 0.6697
2019-03-12 23:16:00,947 : images : pearson = 0.7318, spearman = 0.7414
2019-03-12 23:16:00,948 : ALL (weighted average) : Pearson = 0.6842,             Spearman = 0.6875
2019-03-12 23:16:00,948 : ALL (average) : Pearson = 0.6706,             Spearman = 0.6739

2019-03-12 23:16:00,948 : ***** Transfer task : STS16 *****


2019-03-12 23:16:01,022 : loading BERT model bert-large-uncased
2019-03-12 23:16:01,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:16:01,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:16:01,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1zobogt3
2019-03-12 23:16:08,525 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:16:14,643 : answer-answer : pearson = 0.4930, spearman = 0.5120
2019-03-12 23:16:15,305 : headlines : pearson = 0.6713, spearman = 0.6761
2019-03-12 23:16:16,191 : plagiarism : pearson = 0.7619, spearman = 0.7703
2019-03-12 23:16:17,688 : postediting : pearson = 0.7944, spearman = 0.8311
2019-03-12 23:16:18,296 : question-question : pearson = 0.4148, spearman = 0.4085
2019-03-12 23:16:18,296 : ALL (weighted average) : Pearson = 0.6308,             Spearman = 0.6440
2019-03-12 23:16:18,296 : ALL (average) : Pearson = 0.6271,             Spearman = 0.6396

2019-03-12 23:16:18,297 : ***** Transfer task : MR *****


2019-03-12 23:16:18,315 : loading BERT model bert-large-uncased
2019-03-12 23:16:18,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:16:18,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:16:18,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyqrxyps2
2019-03-12 23:16:25,774 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:16:31,168 : Generating sentence embeddings
2019-03-12 23:17:02,739 : Generated sentence embeddings
2019-03-12 23:17:02,739 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:17:10,717 : Best param found at split 1: l2reg = 0.01                 with score 75.52
2019-03-12 23:17:18,279 : Best param found at split 2: l2reg = 0.001                 with score 75.62
2019-03-12 23:17:26,340 : Best param found at split 3: l2reg = 1e-05                 with score 76.3
2019-03-12 23:17:36,999 : Best param found at split 4: l2reg = 0.001                 with score 76.13
2019-03-12 23:17:45,986 : Best param found at split 5: l2reg = 0.0001                 with score 76.12
2019-03-12 23:17:46,577 : Dev acc : 75.94 Test acc : 74.53

2019-03-12 23:17:46,578 : ***** Transfer task : CR *****


2019-03-12 23:17:46,586 : loading BERT model bert-large-uncased
2019-03-12 23:17:46,586 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:17:46,605 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:17:46,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpap7dmyff
2019-03-12 23:17:54,050 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:17:59,230 : Generating sentence embeddings
2019-03-12 23:18:07,572 : Generated sentence embeddings
2019-03-12 23:18:07,572 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:18:11,500 : Best param found at split 1: l2reg = 0.01                 with score 80.62
2019-03-12 23:18:15,037 : Best param found at split 2: l2reg = 0.0001                 with score 80.03
2019-03-12 23:18:18,991 : Best param found at split 3: l2reg = 0.001                 with score 80.53
2019-03-12 23:18:23,218 : Best param found at split 4: l2reg = 0.01                 with score 80.4
2019-03-12 23:18:27,261 : Best param found at split 5: l2reg = 0.001                 with score 80.27
2019-03-12 23:18:27,423 : Dev acc : 80.37 Test acc : 78.78

2019-03-12 23:18:27,423 : ***** Transfer task : MPQA *****


2019-03-12 23:18:27,430 : loading BERT model bert-large-uncased
2019-03-12 23:18:27,430 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:18:27,482 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:18:27,482 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc0lutmqj
2019-03-12 23:18:34,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:18:40,313 : Generating sentence embeddings
2019-03-12 23:18:47,945 : Generated sentence embeddings
2019-03-12 23:18:47,946 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:18:58,713 : Best param found at split 1: l2reg = 0.001                 with score 87.51
2019-03-12 23:19:09,534 : Best param found at split 2: l2reg = 1e-05                 with score 84.82
2019-03-12 23:19:21,283 : Best param found at split 3: l2reg = 0.01                 with score 87.9
2019-03-12 23:19:31,870 : Best param found at split 4: l2reg = 0.001                 with score 87.44
2019-03-12 23:19:42,942 : Best param found at split 5: l2reg = 0.001                 with score 86.83
2019-03-12 23:19:43,698 : Dev acc : 86.9 Test acc : 87.73

2019-03-12 23:19:43,699 : ***** Transfer task : SUBJ *****


2019-03-12 23:19:43,714 : loading BERT model bert-large-uncased
2019-03-12 23:19:43,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:19:43,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:19:43,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_xdd6s20
2019-03-12 23:19:51,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:19:56,480 : Generating sentence embeddings
2019-03-12 23:20:27,422 : Generated sentence embeddings
2019-03-12 23:20:27,422 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:20:35,136 : Best param found at split 1: l2reg = 0.001                 with score 94.25
2019-03-12 23:20:42,321 : Best param found at split 2: l2reg = 1e-05                 with score 94.38
2019-03-12 23:20:51,271 : Best param found at split 3: l2reg = 0.001                 with score 93.95
2019-03-12 23:21:00,868 : Best param found at split 4: l2reg = 0.001                 with score 94.34
2019-03-12 23:21:10,824 : Best param found at split 5: l2reg = 0.001                 with score 93.99
2019-03-12 23:21:11,239 : Dev acc : 94.18 Test acc : 93.37

2019-03-12 23:21:11,240 : ***** Transfer task : SST Binary classification *****


2019-03-12 23:21:11,332 : loading BERT model bert-large-uncased
2019-03-12 23:21:11,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:21:11,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:21:11,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoharmov8
2019-03-12 23:21:18,864 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:21:24,329 : Computing embedding for train
2019-03-12 23:23:04,895 : Computed train embeddings
2019-03-12 23:23:04,895 : Computing embedding for dev
2019-03-12 23:23:07,091 : Computed dev embeddings
2019-03-12 23:23:07,091 : Computing embedding for test
2019-03-12 23:23:11,704 : Computed test embeddings
2019-03-12 23:23:11,704 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:23:31,632 : [('reg:1e-05', 79.01), ('reg:0.0001', 79.01), ('reg:0.001', 79.36), ('reg:0.01', 79.01)]
2019-03-12 23:23:31,632 : Validation : best param found is reg = 0.001 with score             79.36
2019-03-12 23:23:31,632 : Evaluating...
2019-03-12 23:23:38,070 : 
Dev acc : 79.36 Test acc : 80.94 for             SST Binary classification

2019-03-12 23:23:38,071 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 23:23:38,121 : loading BERT model bert-large-uncased
2019-03-12 23:23:38,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:23:38,143 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:23:38,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfu57v6th
2019-03-12 23:23:45,622 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:23:50,901 : Computing embedding for train
2019-03-12 23:24:12,825 : Computed train embeddings
2019-03-12 23:24:12,825 : Computing embedding for dev
2019-03-12 23:24:15,696 : Computed dev embeddings
2019-03-12 23:24:15,697 : Computing embedding for test
2019-03-12 23:24:21,357 : Computed test embeddings
2019-03-12 23:24:21,357 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:24:23,998 : [('reg:1e-05', 39.96), ('reg:0.0001', 39.51), ('reg:0.001', 39.42), ('reg:0.01', 41.96)]
2019-03-12 23:24:23,998 : Validation : best param found is reg = 0.01 with score             41.96
2019-03-12 23:24:23,998 : Evaluating...
2019-03-12 23:24:24,634 : 
Dev acc : 41.96 Test acc : 43.17 for             SST Fine-Grained classification

2019-03-12 23:24:24,635 : ***** Transfer task : TREC *****


2019-03-12 23:24:24,647 : loading BERT model bert-large-uncased
2019-03-12 23:24:24,647 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:24:24,666 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:24:24,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppu3i36ns
2019-03-12 23:24:32,119 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:24:44,985 : Computed train embeddings
2019-03-12 23:24:45,574 : Computed test embeddings
2019-03-12 23:24:45,575 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:24:52,176 : [('reg:1e-05', 80.56), ('reg:0.0001', 80.56), ('reg:0.001', 79.59), ('reg:0.01', 71.81)]
2019-03-12 23:24:52,176 : Cross-validation : best param found is reg = 1e-05             with score 80.56
2019-03-12 23:24:52,176 : Evaluating...
2019-03-12 23:24:52,592 : 
Dev acc : 80.56 Test acc : 85.8             for TREC

2019-03-12 23:24:52,593 : ***** Transfer task : MRPC *****


2019-03-12 23:24:52,616 : loading BERT model bert-large-uncased
2019-03-12 23:24:52,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:24:52,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:24:52,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdrgg9rpp
2019-03-12 23:25:00,093 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:25:05,523 : Computing embedding for train
2019-03-12 23:25:27,846 : Computed train embeddings
2019-03-12 23:25:27,846 : Computing embedding for test
2019-03-12 23:25:37,633 : Computed test embeddings
2019-03-12 23:25:37,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:25:42,732 : [('reg:1e-05', 73.77), ('reg:0.0001', 73.85), ('reg:0.001', 74.02), ('reg:0.01', 72.96)]
2019-03-12 23:25:42,732 : Cross-validation : best param found is reg = 0.001             with score 74.02
2019-03-12 23:25:42,732 : Evaluating...
2019-03-12 23:25:43,000 : Dev acc : 74.02 Test acc 72.06; Test F1 78.41 for MRPC.

2019-03-12 23:25:43,000 : ***** Transfer task : SICK-Entailment*****


2019-03-12 23:25:43,061 : loading BERT model bert-large-uncased
2019-03-12 23:25:43,062 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:25:43,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:25:43,081 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_3fsopuh
2019-03-12 23:25:50,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:25:55,738 : Computing embedding for train
2019-03-12 23:26:07,086 : Computed train embeddings
2019-03-12 23:26:07,086 : Computing embedding for dev
2019-03-12 23:26:08,636 : Computed dev embeddings
2019-03-12 23:26:08,636 : Computing embedding for test
2019-03-12 23:26:20,818 : Computed test embeddings
2019-03-12 23:26:20,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:26:21,842 : [('reg:1e-05', 80.0), ('reg:0.0001', 79.0), ('reg:0.001', 79.4), ('reg:0.01', 76.8)]
2019-03-12 23:26:21,842 : Validation : best param found is reg = 1e-05 with score             80.0
2019-03-12 23:26:21,842 : Evaluating...
2019-03-12 23:26:22,106 : 
Dev acc : 80.0 Test acc : 78.12 for                        SICK entailment

2019-03-12 23:26:22,106 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 23:26:22,133 : loading BERT model bert-large-uncased
2019-03-12 23:26:22,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:26:22,191 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:26:22,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjypotd4j
2019-03-12 23:26:29,627 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:26:34,777 : Computing embedding for train
2019-03-12 23:26:46,137 : Computed train embeddings
2019-03-12 23:26:46,138 : Computing embedding for dev
2019-03-12 23:26:47,688 : Computed dev embeddings
2019-03-12 23:26:47,688 : Computing embedding for test
2019-03-12 23:26:59,879 : Computed test embeddings
2019-03-12 23:27:14,813 : Dev : Pearson 0.813434978589731
2019-03-12 23:27:14,813 : Test : Pearson 0.8193763145953737 Spearman 0.7459903728035668 MSE 0.33606803274517927                        for SICK Relatedness

2019-03-12 23:27:14,813 : 

***** Transfer task : STSBenchmark*****


2019-03-12 23:27:14,853 : loading BERT model bert-large-uncased
2019-03-12 23:27:14,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:27:14,882 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:27:14,882 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgmbsazpd
2019-03-12 23:27:22,359 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:27:27,632 : Computing embedding for train
2019-03-12 23:27:46,305 : Computed train embeddings
2019-03-12 23:27:46,305 : Computing embedding for dev
2019-03-12 23:27:51,975 : Computed dev embeddings
2019-03-12 23:27:51,975 : Computing embedding for test
2019-03-12 23:27:56,604 : Computed test embeddings
2019-03-12 23:28:15,444 : Dev : Pearson 0.7251457848300936
2019-03-12 23:28:15,444 : Test : Pearson 0.7044395437021884 Spearman 0.6970304206904813 MSE 1.3763964531881154                        for SICK Relatedness

2019-03-12 23:28:15,444 : ***** Transfer task : SNLI Entailment*****


2019-03-12 23:28:20,507 : loading BERT model bert-large-uncased
2019-03-12 23:28:20,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:28:20,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:28:20,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz42a091g
2019-03-12 23:28:28,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:28:33,649 : PROGRESS (encoding): 0.00%
2019-03-12 23:31:20,418 : PROGRESS (encoding): 14.56%
2019-03-12 23:34:32,478 : PROGRESS (encoding): 29.12%
2019-03-12 23:37:43,484 : PROGRESS (encoding): 43.69%
2019-03-12 23:41:06,547 : PROGRESS (encoding): 58.25%
2019-03-12 23:44:52,597 : PROGRESS (encoding): 72.81%
2019-03-12 23:48:37,571 : PROGRESS (encoding): 87.37%
2019-03-12 23:52:40,982 : PROGRESS (encoding): 0.00%
2019-03-12 23:53:11,602 : PROGRESS (encoding): 0.00%
2019-03-12 23:53:41,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:54:12,162 : [('reg:1e-09', 66.86)]
2019-03-12 23:54:12,162 : Validation : best param found is reg = 1e-09 with score             66.86
2019-03-12 23:54:12,162 : Evaluating...
2019-03-12 23:54:44,856 : Dev acc : 66.86 Test acc : 66.97 for SNLI

2019-03-12 23:54:44,856 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 23:54:45,064 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 23:54:46,129 : loading BERT model bert-large-uncased
2019-03-12 23:54:46,130 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:54:46,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:54:46,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3sqjnsbu
2019-03-12 23:54:53,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:54:58,957 : Computing embeddings for train/dev/test
2019-03-12 23:58:31,423 : Computed embeddings
2019-03-12 23:58:31,423 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:58:58,492 : [('reg:1e-05', 90.48), ('reg:0.0001', 88.76), ('reg:0.001', 85.27), ('reg:0.01', 78.66)]
2019-03-12 23:58:58,493 : Validation : best param found is reg = 1e-05 with score             90.48
2019-03-12 23:58:58,493 : Evaluating...
2019-03-12 23:59:04,721 : 
Dev acc : 90.5 Test acc : 90.4 for LENGTH classification

2019-03-12 23:59:04,722 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 23:59:04,975 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 23:59:05,022 : loading BERT model bert-large-uncased
2019-03-12 23:59:05,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:59:05,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:59:05,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm3xsmlmu
2019-03-12 23:59:12,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:59:17,785 : Computing embeddings for train/dev/test
2019-03-13 00:02:33,850 : Computed embeddings
2019-03-13 00:02:33,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:02:59,379 : [('reg:1e-05', 75.91), ('reg:0.0001', 33.7), ('reg:0.001', 2.52), ('reg:0.01', 0.97)]
2019-03-13 00:02:59,380 : Validation : best param found is reg = 1e-05 with score             75.91
2019-03-13 00:02:59,380 : Evaluating...
2019-03-13 00:03:10,087 : 
Dev acc : 75.9 Test acc : 75.3 for WORDCONTENT classification

2019-03-13 00:03:10,089 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 00:03:10,622 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 00:03:10,688 : loading BERT model bert-large-uncased
2019-03-13 00:03:10,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:03:10,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:03:10,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnr99a5r2
2019-03-13 00:03:18,157 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:03:23,342 : Computing embeddings for train/dev/test
2019-03-13 00:06:28,186 : Computed embeddings
2019-03-13 00:06:28,186 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:06:51,826 : [('reg:1e-05', 34.77), ('reg:0.0001', 34.57), ('reg:0.001', 33.12), ('reg:0.01', 27.58)]
2019-03-13 00:06:51,826 : Validation : best param found is reg = 1e-05 with score             34.77
2019-03-13 00:06:51,826 : Evaluating...
2019-03-13 00:06:57,530 : 
Dev acc : 34.8 Test acc : 35.2 for DEPTH classification

2019-03-13 00:06:57,531 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 00:06:57,902 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 00:06:57,965 : loading BERT model bert-large-uncased
2019-03-13 00:06:57,965 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:06:58,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:06:58,076 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdnuphwnd
2019-03-13 00:07:05,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:07:10,911 : Computing embeddings for train/dev/test
2019-03-13 00:10:01,603 : Computed embeddings
2019-03-13 00:10:01,603 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:10:34,462 : [('reg:1e-05', 70.19), ('reg:0.0001', 66.39), ('reg:0.001', 61.91), ('reg:0.01', 42.78)]
2019-03-13 00:10:34,462 : Validation : best param found is reg = 1e-05 with score             70.19
2019-03-13 00:10:34,462 : Evaluating...
2019-03-13 00:10:43,170 : 
Dev acc : 70.2 Test acc : 70.2 for TOPCONSTITUENTS classification

2019-03-13 00:10:43,171 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 00:10:43,552 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 00:10:43,619 : loading BERT model bert-large-uncased
2019-03-13 00:10:43,620 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:10:43,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:10:43,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpic6mzotv
2019-03-13 00:10:51,174 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:10:56,525 : Computing embeddings for train/dev/test
2019-03-13 00:14:01,798 : Computed embeddings
2019-03-13 00:14:01,799 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:14:24,666 : [('reg:1e-05', 75.18), ('reg:0.0001', 75.21), ('reg:0.001', 74.78), ('reg:0.01', 71.74)]
2019-03-13 00:14:24,666 : Validation : best param found is reg = 0.0001 with score             75.21
2019-03-13 00:14:24,666 : Evaluating...
2019-03-13 00:14:31,100 : 
Dev acc : 75.2 Test acc : 74.3 for BIGRAMSHIFT classification

2019-03-13 00:14:31,101 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 00:14:31,494 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 00:14:31,561 : loading BERT model bert-large-uncased
2019-03-13 00:14:31,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:14:31,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:14:31,592 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj_ruewhv
2019-03-13 00:14:39,046 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:14:44,245 : Computing embeddings for train/dev/test
2019-03-13 00:17:45,798 : Computed embeddings
2019-03-13 00:17:45,798 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:18:10,946 : [('reg:1e-05', 88.42), ('reg:0.0001', 88.59), ('reg:0.001', 88.73), ('reg:0.01', 88.73)]
2019-03-13 00:18:10,947 : Validation : best param found is reg = 0.001 with score             88.73
2019-03-13 00:18:10,947 : Evaluating...
2019-03-13 00:18:17,143 : 
Dev acc : 88.7 Test acc : 87.5 for TENSE classification

2019-03-13 00:18:17,144 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 00:18:17,553 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 00:18:17,616 : loading BERT model bert-large-uncased
2019-03-13 00:18:17,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:18:17,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:18:17,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3oxdk49c
2019-03-13 00:18:25,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:18:30,555 : Computing embeddings for train/dev/test
2019-03-13 00:21:42,709 : Computed embeddings
2019-03-13 00:21:42,710 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:22:05,109 : [('reg:1e-05', 80.22), ('reg:0.0001', 80.38), ('reg:0.001', 80.82), ('reg:0.01', 79.09)]
2019-03-13 00:22:05,109 : Validation : best param found is reg = 0.001 with score             80.82
2019-03-13 00:22:05,109 : Evaluating...
2019-03-13 00:22:10,524 : 
Dev acc : 80.8 Test acc : 79.8 for SUBJNUMBER classification

2019-03-13 00:22:10,525 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 00:22:10,921 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 00:22:10,987 : loading BERT model bert-large-uncased
2019-03-13 00:22:10,987 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:22:11,101 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:22:11,101 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1crjh2qz
2019-03-13 00:22:18,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:22:23,875 : Computing embeddings for train/dev/test
2019-03-13 00:25:32,069 : Computed embeddings
2019-03-13 00:25:32,069 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:25:48,747 : [('reg:1e-05', 81.05), ('reg:0.0001', 80.99), ('reg:0.001', 80.24), ('reg:0.01', 79.81)]
2019-03-13 00:25:48,747 : Validation : best param found is reg = 1e-05 with score             81.05
2019-03-13 00:25:48,747 : Evaluating...
2019-03-13 00:25:54,162 : 
Dev acc : 81.0 Test acc : 82.0 for OBJNUMBER classification

2019-03-13 00:25:54,163 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 00:25:54,734 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 00:25:54,805 : loading BERT model bert-large-uncased
2019-03-13 00:25:54,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:25:54,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:25:54,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi_ygrtew
2019-03-13 00:26:02,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:26:07,648 : Computing embeddings for train/dev/test
2019-03-13 00:29:46,310 : Computed embeddings
2019-03-13 00:29:46,310 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:30:14,285 : [('reg:1e-05', 57.3), ('reg:0.0001', 57.42), ('reg:0.001', 57.3), ('reg:0.01', 55.89)]
2019-03-13 00:30:14,285 : Validation : best param found is reg = 0.0001 with score             57.42
2019-03-13 00:30:14,285 : Evaluating...
2019-03-13 00:30:21,834 : 
Dev acc : 57.4 Test acc : 57.2 for ODDMANOUT classification

2019-03-13 00:30:21,835 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 00:30:22,210 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 00:30:22,291 : loading BERT model bert-large-uncased
2019-03-13 00:30:22,291 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:30:22,321 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:30:22,321 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5n3v_stq
2019-03-13 00:30:29,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:30:35,220 : Computing embeddings for train/dev/test
2019-03-13 00:34:11,368 : Computed embeddings
2019-03-13 00:34:11,368 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:34:40,167 : [('reg:1e-05', 53.01), ('reg:0.0001', 52.81), ('reg:0.001', 57.73), ('reg:0.01', 53.89)]
2019-03-13 00:34:40,167 : Validation : best param found is reg = 0.001 with score             57.73
2019-03-13 00:34:40,167 : Evaluating...
2019-03-13 00:34:47,719 : 
Dev acc : 57.7 Test acc : 57.6 for COORDINATIONINVERSION classification

2019-03-13 00:34:47,721 : total results: {'STS12': {'MSRpar': {'pearson': (0.3844221048125534, 7.998979586115083e-28), 'spearman': SpearmanrResult(correlation=0.42369653810298746, pvalue=4.975761445489464e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5748551682349486, 3.506825927345827e-67), 'spearman': SpearmanrResult(correlation=0.5790031244911934, pvalue=2.379900842658138e-68), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5076326527871655, 1.9594423000232914e-31), 'spearman': SpearmanrResult(correlation=0.5996877027118789, pvalue=3.650487346688212e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6722240313332524, 9.431866411466705e-100), 'spearman': SpearmanrResult(correlation=0.6763544850282729, pvalue=2.055158638763561e-101), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5837455319424658, 8.20847539939877e-38), 'spearman': SpearmanrResult(correlation=0.5525977973585361, pvalue=2.793839202985774e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5445758978220772, 'wmean': 0.5436112719304756}, 'spearman': {'mean': 0.5662679295385737, 'wmean': 0.5646826857810967}}}, 'STS13': {'FNWN': {'pearson': (0.4475762641912349, 1.0672672722061452e-10), 'spearman': SpearmanrResult(correlation=0.4603638234916025, pvalue=2.6468645147378766e-11), 'nsamples': 189}, 'headlines': {'pearson': (0.6651789543615275, 5.591501679220846e-97), 'spearman': SpearmanrResult(correlation=0.6436551611471819, pvalue=5.863239614878871e-89), 'nsamples': 750}, 'OnWN': {'pearson': (0.5486933898273628, 2.028203656130894e-45), 'spearman': SpearmanrResult(correlation=0.5732981975317641, pvalue=2.4921302966531564e-50), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5538162027933752, 'wmean': 0.594195414264293}, 'spearman': {'mean': 0.5591057273901828, 'wmean': 0.5942469482104127}}}, 'STS14': {'deft-forum': {'pearson': (0.35204368834127325, 1.4159660925052627e-14), 'spearman': SpearmanrResult(correlation=0.3724928651049828, pvalue=2.9375557472269377e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7374954171267534, 9.89750400196672e-53), 'spearman': SpearmanrResult(correlation=0.6977251717417745, pvalue=4.4607359594892314e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.621810638761565, 1.8776189303465544e-81), 'spearman': SpearmanrResult(correlation=0.5697731917063192, pvalue=8.990887381948868e-66), 'nsamples': 750}, 'images': {'pearson': (0.6196331020082624, 9.759455003609491e-81), 'spearman': SpearmanrResult(correlation=0.6080701837973319, pvalue=4.987516583465052e-77), 'nsamples': 750}, 'OnWN': {'pearson': (0.6763039911602574, 2.1543890438195726e-101), 'spearman': SpearmanrResult(correlation=0.71179067251865, pvalue=7.133110173919823e-117), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6889935344129904, 1.1340180296904136e-106), 'spearman': SpearmanrResult(correlation=0.6455765050130116, pvalue=1.1980292190201445e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6160467286351835, 'wmean': 0.622593129239708}, 'spearman': {'mean': 0.6009047649803451, 'wmean': 0.6075592681590024}}}, 'STS15': {'answers-forums': {'pearson': (0.55353732752474, 1.7238174368972427e-31), 'spearman': SpearmanrResult(correlation=0.535904766753362, pvalue=2.9043148116063996e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.7064035195463368, 2.2521533774813252e-114), 'spearman': SpearmanrResult(correlation=0.719174382789806, pvalue=2.150103450016541e-120), 'nsamples': 750}, 'belief': {'pearson': (0.6791625411153729, 4.835475709480663e-52), 'spearman': SpearmanrResult(correlation=0.7031735274151455, pvalue=3.3043121743707084e-57), 'nsamples': 375}, 'headlines': {'pearson': (0.6822562150240019, 7.77530118754341e-104), 'spearman': SpearmanrResult(correlation=0.6697353895464444, pvalue=9.180289691804034e-99), 'nsamples': 750}, 'images': {'pearson': (0.7317970471626276, 1.1045828457786337e-126), 'spearman': SpearmanrResult(correlation=0.7413919223181442, pvalue=1.0417508763235056e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6706313300746158, 'wmean': 0.6842016790132557}, 'spearman': {'mean': 0.6738759977645803, 'wmean': 0.6874602104346621}}}, 'STS16': {'answer-answer': {'pearson': (0.49301919555607454, 5.806892165702815e-17), 'spearman': SpearmanrResult(correlation=0.5120330646065783, pvalue=2.232218545179184e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6713226139239917, 5.560822539825088e-34), 'spearman': SpearmanrResult(correlation=0.676126789598332, pvalue=1.2772929461231943e-34), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7619354446762713, 6.68780880739143e-45), 'spearman': SpearmanrResult(correlation=0.7703171964773488, pvalue=1.9099112077467124e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.7944431028331311, 2.5047135339598362e-54), 'spearman': SpearmanrResult(correlation=0.8311186079856142, pvalue=1.3038237105331022e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.4147515982249148, 4.2782810237385535e-10), 'spearman': SpearmanrResult(correlation=0.40846645779775453, pvalue=8.271386324383648e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6270943910428767, 'wmean': 0.6308250926932129}, 'spearman': {'mean': 0.6396124232931255, 'wmean': 0.6439691013811688}}}, 'MR': {'devacc': 75.94, 'acc': 74.53, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.37, 'acc': 78.78, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.9, 'acc': 87.73, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.18, 'acc': 93.37, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.36, 'acc': 80.94, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.96, 'acc': 43.17, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.56, 'acc': 85.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.02, 'acc': 72.06, 'f1': 78.41, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.0, 'acc': 78.12, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.813434978589731, 'pearson': 0.8193763145953737, 'spearman': 0.7459903728035668, 'mse': 0.33606803274517927, 'yhat': array([3.48733332, 4.44418401, 1.65743874, ..., 3.74659201, 4.40735359,        4.30589636]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7251457848300936, 'pearson': 0.7044395437021884, 'spearman': 0.6970304206904813, 'mse': 1.3763964531881154, 'yhat': array([1.60912432, 1.40531827, 2.54164273, ..., 3.81824633, 3.54485392,        3.5694074 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.86, 'acc': 66.97, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 90.48, 'acc': 90.41, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 75.91, 'acc': 75.29, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.77, 'acc': 35.21, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.19, 'acc': 70.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 75.21, 'acc': 74.27, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.73, 'acc': 87.53, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.82, 'acc': 79.79, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.05, 'acc': 82.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.42, 'acc': 57.2, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.73, 'acc': 57.6, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 00:34:47,722 : STS12 p=0.5436, STS12 s=0.5647, STS13 p=0.5942, STS13 s=0.5942, STS14 p=0.6226, STS14 s=0.6076, STS15 p=0.6842, STS15 s=0.6875, STS 16 p=0.6308, STS16 s=0.6440, STS B p=0.7044, STS B s=0.6970, STS B m=1.3764, SICK-R p=0.8194, SICK-R s=0.7460, SICK-P m=0.3361
2019-03-13 00:34:47,722 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 00:34:47,722 : 0.5436,0.5647,0.5942,0.5942,0.6226,0.6076,0.6842,0.6875,0.6308,0.6440,0.7044,0.6970,1.3764,0.8194,0.7460,0.3361
2019-03-13 00:34:47,722 : MR=74.53, CR=78.78, SUBJ=93.37, MPQA=87.73, SST-B=80.94, SST-F=43.17, TREC=85.80, SICK-E=78.12, SNLI=66.97, MRPC=72.06, MRPC f=78.41
2019-03-13 00:34:47,722 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 00:34:47,722 : 74.53,78.78,93.37,87.73,80.94,43.17,85.80,78.12,66.97,72.06,78.41
2019-03-13 00:34:47,722 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 00:34:47,722 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 00:34:47,722 : na,na,na,na,na,na,na,na,na,na
2019-03-13 00:34:47,722 : SentLen=90.41, WC=75.29, TreeDepth=35.21, TopConst=70.19, BShift=74.27, Tense=87.53, SubjNum=79.79, ObjNum=82.05, SOMO=57.20, CoordInv=57.60, average=70.95
2019-03-13 00:34:47,722 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 00:34:47,722 : 90.41,75.29,35.21,70.19,74.27,87.53,79.79,82.05,57.20,57.60,70.95
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,722 : layer 8
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,722 : ********************************************************************************
2019-03-13 00:34:47,814 : ***** Transfer task : STS12 *****


2019-03-13 00:34:47,827 : loading BERT model bert-large-uncased
2019-03-13 00:34:47,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:34:47,844 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:34:47,845 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcq8g1nbl
2019-03-13 00:34:55,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:35:04,585 : MSRpar : pearson = 0.3919, spearman = 0.4311
2019-03-13 00:35:06,236 : MSRvid : pearson = 0.6024, spearman = 0.6063
2019-03-13 00:35:07,655 : SMTeuroparl : pearson = 0.5167, spearman = 0.5993
2019-03-13 00:35:10,367 : surprise.OnWN : pearson = 0.6811, spearman = 0.6786
2019-03-13 00:35:11,802 : surprise.SMTnews : pearson = 0.5831, spearman = 0.5546
2019-03-13 00:35:11,802 : ALL (weighted average) : Pearson = 0.5555,             Spearman = 0.5738
2019-03-13 00:35:11,802 : ALL (average) : Pearson = 0.5550,             Spearman = 0.5740

2019-03-13 00:35:11,802 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 00:35:11,811 : loading BERT model bert-large-uncased
2019-03-13 00:35:11,811 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:35:11,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:35:11,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_zgqdz57
2019-03-13 00:35:19,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:35:25,833 : FNWN : pearson = 0.4683, spearman = 0.4815
2019-03-13 00:35:27,736 : headlines : pearson = 0.6706, spearman = 0.6498
2019-03-13 00:35:29,211 : OnWN : pearson = 0.5363, spearman = 0.5606
2019-03-13 00:35:29,211 : ALL (weighted average) : Pearson = 0.5949,             Spearman = 0.5952
2019-03-13 00:35:29,211 : ALL (average) : Pearson = 0.5584,             Spearman = 0.5640

2019-03-13 00:35:29,211 : ***** Transfer task : STS14 *****


2019-03-13 00:35:29,226 : loading BERT model bert-large-uncased
2019-03-13 00:35:29,226 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:35:29,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:35:29,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu8l4o7dd
2019-03-13 00:35:36,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:35:43,434 : deft-forum : pearson = 0.3351, spearman = 0.3616
2019-03-13 00:35:45,080 : deft-news : pearson = 0.7440, spearman = 0.7075
2019-03-13 00:35:47,265 : headlines : pearson = 0.6244, spearman = 0.5695
2019-03-13 00:35:49,352 : images : pearson = 0.6346, spearman = 0.6213
2019-03-13 00:35:51,495 : OnWN : pearson = 0.6736, spearman = 0.7094
2019-03-13 00:35:54,371 : tweet-news : pearson = 0.6801, spearman = 0.6385
2019-03-13 00:35:54,371 : ALL (weighted average) : Pearson = 0.6223,             Spearman = 0.6077
2019-03-13 00:35:54,371 : ALL (average) : Pearson = 0.6153,             Spearman = 0.6013

2019-03-13 00:35:54,372 : ***** Transfer task : STS15 *****


2019-03-13 00:35:54,420 : loading BERT model bert-large-uncased
2019-03-13 00:35:54,420 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:35:54,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:35:54,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp46cl2hjj
2019-03-13 00:36:01,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:36:08,886 : answers-forums : pearson = 0.5496, spearman = 0.5400
2019-03-13 00:36:10,983 : answers-students : pearson = 0.7052, spearman = 0.7170
2019-03-13 00:36:13,041 : belief : pearson = 0.6587, spearman = 0.6808
2019-03-13 00:36:15,302 : headlines : pearson = 0.6808, spearman = 0.6691
2019-03-13 00:36:17,447 : images : pearson = 0.7355, spearman = 0.7456
2019-03-13 00:36:17,447 : ALL (weighted average) : Pearson = 0.6814,             Spearman = 0.6855
2019-03-13 00:36:17,447 : ALL (average) : Pearson = 0.6660,             Spearman = 0.6705

2019-03-13 00:36:17,447 : ***** Transfer task : STS16 *****


2019-03-13 00:36:17,517 : loading BERT model bert-large-uncased
2019-03-13 00:36:17,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:36:17,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:36:17,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzconf81z
2019-03-13 00:36:24,983 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:36:31,026 : answer-answer : pearson = 0.4926, spearman = 0.5258
2019-03-13 00:36:31,688 : headlines : pearson = 0.6811, spearman = 0.6872
2019-03-13 00:36:32,575 : plagiarism : pearson = 0.7644, spearman = 0.7674
2019-03-13 00:36:34,077 : postediting : pearson = 0.7915, spearman = 0.8237
2019-03-13 00:36:34,685 : question-question : pearson = 0.4123, spearman = 0.4065
2019-03-13 00:36:34,685 : ALL (weighted average) : Pearson = 0.6322,             Spearman = 0.6468
2019-03-13 00:36:34,685 : ALL (average) : Pearson = 0.6284,             Spearman = 0.6421

2019-03-13 00:36:34,685 : ***** Transfer task : MR *****


2019-03-13 00:36:34,700 : loading BERT model bert-large-uncased
2019-03-13 00:36:34,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:36:34,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:36:34,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplf1n0rlc
2019-03-13 00:36:42,181 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:36:47,385 : Generating sentence embeddings
2019-03-13 00:37:18,983 : Generated sentence embeddings
2019-03-13 00:37:18,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:37:28,820 : Best param found at split 1: l2reg = 0.01                 with score 76.04
2019-03-13 00:37:39,466 : Best param found at split 2: l2reg = 0.0001                 with score 76.04
2019-03-13 00:37:49,448 : Best param found at split 3: l2reg = 0.001                 with score 76.33
2019-03-13 00:38:00,954 : Best param found at split 4: l2reg = 0.0001                 with score 76.1
2019-03-13 00:38:11,430 : Best param found at split 5: l2reg = 0.0001                 with score 75.63
2019-03-13 00:38:11,939 : Dev acc : 76.03 Test acc : 75.08

2019-03-13 00:38:11,941 : ***** Transfer task : CR *****


2019-03-13 00:38:11,949 : loading BERT model bert-large-uncased
2019-03-13 00:38:11,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:38:11,969 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:38:11,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcnq09647
2019-03-13 00:38:19,429 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:38:24,690 : Generating sentence embeddings
2019-03-13 00:38:33,048 : Generated sentence embeddings
2019-03-13 00:38:33,048 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:38:35,677 : Best param found at split 1: l2reg = 0.01                 with score 80.72
2019-03-13 00:38:38,475 : Best param found at split 2: l2reg = 0.0001                 with score 79.66
2019-03-13 00:38:41,229 : Best param found at split 3: l2reg = 1e-05                 with score 80.33
2019-03-13 00:38:43,991 : Best param found at split 4: l2reg = 0.001                 with score 79.87
2019-03-13 00:38:47,251 : Best param found at split 5: l2reg = 0.0001                 with score 80.21
2019-03-13 00:38:47,439 : Dev acc : 80.16 Test acc : 79.5

2019-03-13 00:38:47,439 : ***** Transfer task : MPQA *****


2019-03-13 00:38:47,445 : loading BERT model bert-large-uncased
2019-03-13 00:38:47,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:38:47,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:38:47,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgimnpu3m
2019-03-13 00:38:54,948 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:39:00,119 : Generating sentence embeddings
2019-03-13 00:39:07,734 : Generated sentence embeddings
2019-03-13 00:39:07,734 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:39:17,088 : Best param found at split 1: l2reg = 1e-05                 with score 86.82
2019-03-13 00:39:28,417 : Best param found at split 2: l2reg = 0.001                 with score 87.45
2019-03-13 00:39:39,979 : Best param found at split 3: l2reg = 0.001                 with score 87.59
2019-03-13 00:39:51,247 : Best param found at split 4: l2reg = 1e-05                 with score 87.44
2019-03-13 00:40:02,516 : Best param found at split 5: l2reg = 0.001                 with score 87.14
2019-03-13 00:40:03,128 : Dev acc : 87.29 Test acc : 87.59

2019-03-13 00:40:03,129 : ***** Transfer task : SUBJ *****


2019-03-13 00:40:03,146 : loading BERT model bert-large-uncased
2019-03-13 00:40:03,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:40:03,165 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:40:03,165 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl2p3wmtn
2019-03-13 00:40:10,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:40:16,016 : Generating sentence embeddings
2019-03-13 00:40:46,946 : Generated sentence embeddings
2019-03-13 00:40:46,946 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:40:56,244 : Best param found at split 1: l2reg = 0.001                 with score 94.11
2019-03-13 00:41:05,729 : Best param found at split 2: l2reg = 0.001                 with score 94.16
2019-03-13 00:41:15,170 : Best param found at split 3: l2reg = 0.0001                 with score 94.06
2019-03-13 00:41:22,812 : Best param found at split 4: l2reg = 0.001                 with score 94.44
2019-03-13 00:41:29,924 : Best param found at split 5: l2reg = 0.001                 with score 93.86
2019-03-13 00:41:30,282 : Dev acc : 94.13 Test acc : 93.81

2019-03-13 00:41:30,283 : ***** Transfer task : SST Binary classification *****


2019-03-13 00:41:30,374 : loading BERT model bert-large-uncased
2019-03-13 00:41:30,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:41:30,448 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:41:30,448 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbjtuon20
2019-03-13 00:41:37,861 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:41:43,150 : Computing embedding for train
2019-03-13 00:43:23,685 : Computed train embeddings
2019-03-13 00:43:23,686 : Computing embedding for dev
2019-03-13 00:43:25,877 : Computed dev embeddings
2019-03-13 00:43:25,877 : Computing embedding for test
2019-03-13 00:43:30,490 : Computed test embeddings
2019-03-13 00:43:30,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:43:50,584 : [('reg:1e-05', 79.93), ('reg:0.0001', 79.24), ('reg:0.001', 78.67), ('reg:0.01', 79.93)]
2019-03-13 00:43:50,585 : Validation : best param found is reg = 1e-05 with score             79.93
2019-03-13 00:43:50,585 : Evaluating...
2019-03-13 00:43:54,860 : 
Dev acc : 79.93 Test acc : 80.45 for             SST Binary classification

2019-03-13 00:43:54,861 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 00:43:54,915 : loading BERT model bert-large-uncased
2019-03-13 00:43:54,915 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:43:54,935 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:43:54,935 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgrk42urb
2019-03-13 00:44:02,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:44:07,806 : Computing embedding for train
2019-03-13 00:44:29,779 : Computed train embeddings
2019-03-13 00:44:29,779 : Computing embedding for dev
2019-03-13 00:44:32,650 : Computed dev embeddings
2019-03-13 00:44:32,650 : Computing embedding for test
2019-03-13 00:44:38,308 : Computed test embeddings
2019-03-13 00:44:38,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:44:40,510 : [('reg:1e-05', 39.96), ('reg:0.0001', 39.96), ('reg:0.001', 39.42), ('reg:0.01', 40.42)]
2019-03-13 00:44:40,511 : Validation : best param found is reg = 0.01 with score             40.42
2019-03-13 00:44:40,511 : Evaluating...
2019-03-13 00:44:41,080 : 
Dev acc : 40.42 Test acc : 40.9 for             SST Fine-Grained classification

2019-03-13 00:44:41,081 : ***** Transfer task : TREC *****


2019-03-13 00:44:41,094 : loading BERT model bert-large-uncased
2019-03-13 00:44:41,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:44:41,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:44:41,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2jpu9tfl
2019-03-13 00:44:48,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:45:01,452 : Computed train embeddings
2019-03-13 00:45:02,042 : Computed test embeddings
2019-03-13 00:45:02,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:45:09,089 : [('reg:1e-05', 81.86), ('reg:0.0001', 81.55), ('reg:0.001', 80.23), ('reg:0.01', 72.45)]
2019-03-13 00:45:09,090 : Cross-validation : best param found is reg = 1e-05             with score 81.86
2019-03-13 00:45:09,090 : Evaluating...
2019-03-13 00:45:09,601 : 
Dev acc : 81.86 Test acc : 89.8             for TREC

2019-03-13 00:45:09,602 : ***** Transfer task : MRPC *****


2019-03-13 00:45:09,623 : loading BERT model bert-large-uncased
2019-03-13 00:45:09,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:45:09,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:45:09,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbrp2izt4
2019-03-13 00:45:17,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:45:22,296 : Computing embedding for train
2019-03-13 00:45:44,622 : Computed train embeddings
2019-03-13 00:45:44,622 : Computing embedding for test
2019-03-13 00:45:54,398 : Computed test embeddings
2019-03-13 00:45:54,419 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:45:58,172 : [('reg:1e-05', 74.14), ('reg:0.0001', 73.36), ('reg:0.001', 74.24), ('reg:0.01', 72.35)]
2019-03-13 00:45:58,173 : Cross-validation : best param found is reg = 0.001             with score 74.24
2019-03-13 00:45:58,173 : Evaluating...
2019-03-13 00:45:58,396 : Dev acc : 74.24 Test acc 72.93; Test F1 79.03 for MRPC.

2019-03-13 00:45:58,396 : ***** Transfer task : SICK-Entailment*****


2019-03-13 00:45:58,458 : loading BERT model bert-large-uncased
2019-03-13 00:45:58,459 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:45:58,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:45:58,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn1e2pz1x
2019-03-13 00:46:05,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:46:11,270 : Computing embedding for train
2019-03-13 00:46:22,617 : Computed train embeddings
2019-03-13 00:46:22,617 : Computing embedding for dev
2019-03-13 00:46:24,162 : Computed dev embeddings
2019-03-13 00:46:24,162 : Computing embedding for test
2019-03-13 00:46:36,325 : Computed test embeddings
2019-03-13 00:46:36,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:46:37,647 : [('reg:1e-05', 79.8), ('reg:0.0001', 80.2), ('reg:0.001', 77.6), ('reg:0.01', 75.0)]
2019-03-13 00:46:37,647 : Validation : best param found is reg = 0.0001 with score             80.2
2019-03-13 00:46:37,647 : Evaluating...
2019-03-13 00:46:37,997 : 
Dev acc : 80.2 Test acc : 78.34 for                        SICK entailment

2019-03-13 00:46:37,998 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 00:46:38,025 : loading BERT model bert-large-uncased
2019-03-13 00:46:38,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:46:38,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:46:38,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp733oz8h4
2019-03-13 00:46:45,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:46:50,815 : Computing embedding for train
2019-03-13 00:47:02,156 : Computed train embeddings
2019-03-13 00:47:02,157 : Computing embedding for dev
2019-03-13 00:47:03,704 : Computed dev embeddings
2019-03-13 00:47:03,704 : Computing embedding for test
2019-03-13 00:47:15,885 : Computed test embeddings
2019-03-13 00:47:25,391 : Dev : Pearson 0.8088319131852874
2019-03-13 00:47:25,391 : Test : Pearson 0.8216146081523439 Spearman 0.7497211812287896 MSE 0.3317290449985093                        for SICK Relatedness

2019-03-13 00:47:25,392 : 

***** Transfer task : STSBenchmark*****


2019-03-13 00:47:25,441 : loading BERT model bert-large-uncased
2019-03-13 00:47:25,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:47:25,470 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:47:25,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn52izsf5
2019-03-13 00:47:32,934 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:47:38,207 : Computing embedding for train
2019-03-13 00:47:56,916 : Computed train embeddings
2019-03-13 00:47:56,916 : Computing embedding for dev
2019-03-13 00:48:02,585 : Computed dev embeddings
2019-03-13 00:48:02,585 : Computing embedding for test
2019-03-13 00:48:07,211 : Computed test embeddings
2019-03-13 00:48:26,816 : Dev : Pearson 0.7519299960720257
2019-03-13 00:48:26,816 : Test : Pearson 0.7099554538206362 Spearman 0.7046519173709499 MSE 1.3644039268276928                        for SICK Relatedness

2019-03-13 00:48:26,816 : ***** Transfer task : SNLI Entailment*****


2019-03-13 00:48:31,533 : loading BERT model bert-large-uncased
2019-03-13 00:48:31,533 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:48:32,529 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:48:32,529 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp85gjwgo
2019-03-13 00:48:39,984 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:48:45,591 : PROGRESS (encoding): 0.00%
2019-03-13 00:51:32,259 : PROGRESS (encoding): 14.56%
2019-03-13 00:54:43,747 : PROGRESS (encoding): 29.12%
2019-03-13 00:57:54,660 : PROGRESS (encoding): 43.69%
2019-03-13 01:01:17,817 : PROGRESS (encoding): 58.25%
2019-03-13 01:05:03,994 : PROGRESS (encoding): 72.81%
2019-03-13 01:08:48,915 : PROGRESS (encoding): 87.37%
2019-03-13 01:12:52,486 : PROGRESS (encoding): 0.00%
2019-03-13 01:13:23,110 : PROGRESS (encoding): 0.00%
2019-03-13 01:13:52,560 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:14:30,019 : [('reg:1e-09', 66.88)]
2019-03-13 01:14:30,019 : Validation : best param found is reg = 1e-09 with score             66.88
2019-03-13 01:14:30,019 : Evaluating...
2019-03-13 01:15:05,695 : Dev acc : 66.88 Test acc : 66.64 for SNLI

2019-03-13 01:15:05,695 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 01:15:05,905 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 01:15:06,887 : loading BERT model bert-large-uncased
2019-03-13 01:15:06,887 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:15:06,914 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:15:06,914 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmxkm4cjr
2019-03-13 01:15:14,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:15:19,829 : Computing embeddings for train/dev/test
2019-03-13 01:18:52,160 : Computed embeddings
2019-03-13 01:18:52,160 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:19:19,579 : [('reg:1e-05', 91.33), ('reg:0.0001', 89.85), ('reg:0.001', 86.06), ('reg:0.01', 79.17)]
2019-03-13 01:19:19,579 : Validation : best param found is reg = 1e-05 with score             91.33
2019-03-13 01:19:19,579 : Evaluating...
2019-03-13 01:19:25,475 : 
Dev acc : 91.3 Test acc : 91.7 for LENGTH classification

2019-03-13 01:19:25,476 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 01:19:25,844 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 01:19:25,890 : loading BERT model bert-large-uncased
2019-03-13 01:19:25,890 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:19:25,921 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:19:25,921 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmcddzvjs
2019-03-13 01:19:33,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:19:38,862 : Computing embeddings for train/dev/test
2019-03-13 01:22:55,073 : Computed embeddings
2019-03-13 01:22:55,074 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:23:28,153 : [('reg:1e-05', 77.19), ('reg:0.0001', 34.49), ('reg:0.001', 2.53), ('reg:0.01', 1.03)]
2019-03-13 01:23:28,153 : Validation : best param found is reg = 1e-05 with score             77.19
2019-03-13 01:23:28,153 : Evaluating...
2019-03-13 01:23:38,140 : 
Dev acc : 77.2 Test acc : 77.4 for WORDCONTENT classification

2019-03-13 01:23:38,141 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 01:23:38,508 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 01:23:38,575 : loading BERT model bert-large-uncased
2019-03-13 01:23:38,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:23:38,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:23:38,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvo49ubh7
2019-03-13 01:23:46,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:23:51,321 : Computing embeddings for train/dev/test
2019-03-13 01:26:55,851 : Computed embeddings
2019-03-13 01:26:55,851 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:27:19,167 : [('reg:1e-05', 35.02), ('reg:0.0001', 34.82), ('reg:0.001', 33.0), ('reg:0.01', 27.64)]
2019-03-13 01:27:19,167 : Validation : best param found is reg = 1e-05 with score             35.02
2019-03-13 01:27:19,167 : Evaluating...
2019-03-13 01:27:26,734 : 
Dev acc : 35.0 Test acc : 35.3 for DEPTH classification

2019-03-13 01:27:26,735 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 01:27:27,120 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 01:27:27,183 : loading BERT model bert-large-uncased
2019-03-13 01:27:27,183 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:27:27,290 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:27:27,290 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjyucnxfn
2019-03-13 01:27:34,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:27:40,067 : Computing embeddings for train/dev/test
2019-03-13 01:30:30,508 : Computed embeddings
2019-03-13 01:30:30,508 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:30:56,821 : [('reg:1e-05', 68.52), ('reg:0.0001', 66.57), ('reg:0.001', 57.78), ('reg:0.01', 42.56)]
2019-03-13 01:30:56,821 : Validation : best param found is reg = 1e-05 with score             68.52
2019-03-13 01:30:56,821 : Evaluating...
2019-03-13 01:31:04,448 : 
Dev acc : 68.5 Test acc : 68.6 for TOPCONSTITUENTS classification

2019-03-13 01:31:04,449 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 01:31:04,800 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 01:31:04,868 : loading BERT model bert-large-uncased
2019-03-13 01:31:04,868 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:31:04,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:31:04,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0_uwpyok
2019-03-13 01:31:12,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:31:17,802 : Computing embeddings for train/dev/test
2019-03-13 01:34:23,283 : Computed embeddings
2019-03-13 01:34:23,283 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:34:52,855 : [('reg:1e-05', 74.58), ('reg:0.0001', 74.29), ('reg:0.001', 74.53), ('reg:0.01', 68.94)]
2019-03-13 01:34:52,855 : Validation : best param found is reg = 1e-05 with score             74.58
2019-03-13 01:34:52,855 : Evaluating...
2019-03-13 01:35:01,281 : 
Dev acc : 74.6 Test acc : 73.7 for BIGRAMSHIFT classification

2019-03-13 01:35:01,282 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 01:35:01,694 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 01:35:01,760 : loading BERT model bert-large-uncased
2019-03-13 01:35:01,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:35:01,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:35:01,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphsy5qc8d
2019-03-13 01:35:09,293 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:35:14,583 : Computing embeddings for train/dev/test
2019-03-13 01:38:16,031 : Computed embeddings
2019-03-13 01:38:16,031 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:38:41,512 : [('reg:1e-05', 88.66), ('reg:0.0001', 88.74), ('reg:0.001', 89.22), ('reg:0.01', 88.94)]
2019-03-13 01:38:41,512 : Validation : best param found is reg = 0.001 with score             89.22
2019-03-13 01:38:41,512 : Evaluating...
2019-03-13 01:38:46,337 : 
Dev acc : 89.2 Test acc : 88.0 for TENSE classification

2019-03-13 01:38:46,338 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 01:38:46,769 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 01:38:46,834 : loading BERT model bert-large-uncased
2019-03-13 01:38:46,834 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:38:46,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:38:46,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg6g9oao6
2019-03-13 01:38:54,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:38:59,616 : Computing embeddings for train/dev/test
2019-03-13 01:42:11,667 : Computed embeddings
2019-03-13 01:42:11,667 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:42:36,279 : [('reg:1e-05', 82.31), ('reg:0.0001', 82.24), ('reg:0.001', 81.84), ('reg:0.01', 81.86)]
2019-03-13 01:42:36,279 : Validation : best param found is reg = 1e-05 with score             82.31
2019-03-13 01:42:36,279 : Evaluating...
2019-03-13 01:42:41,912 : 
Dev acc : 82.3 Test acc : 81.6 for SUBJNUMBER classification

2019-03-13 01:42:41,913 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 01:42:42,332 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 01:42:42,402 : loading BERT model bert-large-uncased
2019-03-13 01:42:42,402 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:42:42,522 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:42:42,522 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoq2vqssu
2019-03-13 01:42:50,030 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:42:55,250 : Computing embeddings for train/dev/test
2019-03-13 01:46:03,351 : Computed embeddings
2019-03-13 01:46:03,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:46:25,349 : [('reg:1e-05', 81.65), ('reg:0.0001', 81.83), ('reg:0.001', 81.49), ('reg:0.01', 80.42)]
2019-03-13 01:46:25,349 : Validation : best param found is reg = 0.0001 with score             81.83
2019-03-13 01:46:25,349 : Evaluating...
2019-03-13 01:46:30,818 : 
Dev acc : 81.8 Test acc : 82.4 for OBJNUMBER classification

2019-03-13 01:46:30,819 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 01:46:31,222 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 01:46:31,295 : loading BERT model bert-large-uncased
2019-03-13 01:46:31,295 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:46:31,427 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:46:31,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo18st1zp
2019-03-13 01:46:38,889 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:46:44,256 : Computing embeddings for train/dev/test
2019-03-13 01:50:22,525 : Computed embeddings
2019-03-13 01:50:22,525 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:50:55,324 : [('reg:1e-05', 59.15), ('reg:0.0001', 59.18), ('reg:0.001', 59.02), ('reg:0.01', 57.78)]
2019-03-13 01:50:55,324 : Validation : best param found is reg = 0.0001 with score             59.18
2019-03-13 01:50:55,324 : Evaluating...
2019-03-13 01:51:04,023 : 
Dev acc : 59.2 Test acc : 59.0 for ODDMANOUT classification

2019-03-13 01:51:04,024 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 01:51:04,615 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 01:51:04,691 : loading BERT model bert-large-uncased
2019-03-13 01:51:04,692 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:51:04,723 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:51:04,723 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptj_ns4m_
2019-03-13 01:51:12,214 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:51:17,530 : Computing embeddings for train/dev/test
2019-03-13 01:54:53,506 : Computed embeddings
2019-03-13 01:54:53,506 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:55:22,517 : [('reg:1e-05', 55.51), ('reg:0.0001', 55.11), ('reg:0.001', 54.29), ('reg:0.01', 53.78)]
2019-03-13 01:55:22,518 : Validation : best param found is reg = 1e-05 with score             55.51
2019-03-13 01:55:22,518 : Evaluating...
2019-03-13 01:55:30,012 : 
Dev acc : 55.5 Test acc : 54.8 for COORDINATIONINVERSION classification

2019-03-13 01:55:30,014 : total results: {'STS12': {'MSRpar': {'pearson': (0.39192659191298096, 6.033391580563516e-29), 'spearman': SpearmanrResult(correlation=0.43113298244425197, pvalue=2.667296046328879e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6024123147195609, 2.867026233600046e-75), 'spearman': SpearmanrResult(correlation=0.6062691689382637, pvalue=1.82736499065187e-76), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5167234737339551, 1.0758660636931902e-32), 'spearman': SpearmanrResult(correlation=0.5992701613451913, pvalue=4.367145380241129e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6810845160801603, 2.37762763404189e-103), 'spearman': SpearmanrResult(correlation=0.6786176663970619, pvalue=2.458897691059672e-102), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5830854723560426, 1.0361017497557262e-37), 'spearman': SpearmanrResult(correlation=0.5545790430833619, pvalue=1.485958362600199e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5550464737605398, 'wmean': 0.5554680646552359}, 'spearman': {'mean': 0.5739738044416262, 'wmean': 0.5737956581667912}}}, 'STS13': {'FNWN': {'pearson': (0.46827555638941565, 1.084708304253718e-11), 'spearman': SpearmanrResult(correlation=0.4814760600062746, pvalue=2.3248063344080287e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6705746502948566, 4.272250939218515e-99), 'spearman': SpearmanrResult(correlation=0.6497845400666407, pvalue=3.5520337378051785e-91), 'nsamples': 750}, 'OnWN': {'pearson': (0.5362852692904506, 4.2929728420144205e-43), 'spearman': SpearmanrResult(correlation=0.5605914558680576, pvalue=9.633933533385064e-48), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5583784919915743, 'wmean': 0.5948607359671232}, 'spearman': {'mean': 0.5639506853136577, 'wmean': 0.5952194580887645}}}, 'STS14': {'deft-forum': {'pearson': (0.335138285972559, 2.8393014482578904e-13), 'spearman': SpearmanrResult(correlation=0.3615681137322711, pvalue=2.4107639252020326e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.7439606292414017, 4.153632997302357e-54), 'spearman': SpearmanrResult(correlation=0.7074756496244131, pvalue=7.80347457449278e-47), 'nsamples': 300}, 'headlines': {'pearson': (0.624391757220527, 2.6167894359859152e-82), 'spearman': SpearmanrResult(correlation=0.5694874734395313, pvalue=1.0771731143165167e-65), 'nsamples': 750}, 'images': {'pearson': (0.6346474660178847, 8.637875570275976e-86), 'spearman': SpearmanrResult(correlation=0.621311331003234, pvalue=2.743109145293859e-81), 'nsamples': 750}, 'OnWN': {'pearson': (0.6735507306172331, 2.778389740276431e-100), 'spearman': SpearmanrResult(correlation=0.7093682564524499, pvalue=9.643387340422636e-116), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6801163434717309, 5.963995887081193e-103), 'spearman': SpearmanrResult(correlation=0.6384672884683369, pvalue=4.034574907578871e-87), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6153008687568894, 'wmean': 0.6222747041214942}, 'spearman': {'mean': 0.6012796854533727, 'wmean': 0.6077130954905361}}}, 'STS15': {'answers-forums': {'pearson': (0.5496278631979494, 5.51557284248982e-31), 'spearman': SpearmanrResult(correlation=0.5400359612175931, pvalue=8.976602327003755e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.705176605341348, 8.20109729980993e-114), 'spearman': SpearmanrResult(correlation=0.7169945126118952, pvalue=2.418968692908503e-119), 'nsamples': 750}, 'belief': {'pearson': (0.6587479472684935, 5.04847343542328e-48), 'spearman': SpearmanrResult(correlation=0.6808393401993107, pvalue=2.186761415272919e-52), 'nsamples': 375}, 'headlines': {'pearson': (0.6808483548264448, 2.9765170719353117e-103), 'spearman': SpearmanrResult(correlation=0.6691306287708109, pvalue=1.5905954234300208e-98), 'nsamples': 750}, 'images': {'pearson': (0.7354726779878717, 1.3932136945055778e-128), 'spearman': SpearmanrResult(correlation=0.7455718573112708, pvalue=5.727564073872663e-134), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6659746897244215, 'wmean': 0.6814213858472215}, 'spearman': {'mean': 0.6705144600221762, 'wmean': 0.6855336623506073}}}, 'STS16': {'answer-answer': {'pearson': (0.49260145635706853, 6.223834323419517e-17), 'spearman': SpearmanrResult(correlation=0.5258492746919344, pvalue=1.8345002824135992e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6810902126279749, 2.7128417181303716e-35), 'spearman': SpearmanrResult(correlation=0.6871826142533811, pvalue=3.8823434817226346e-36), 'nsamples': 249}, 'plagiarism': {'pearson': (0.764364123104614, 2.423487625602419e-45), 'spearman': SpearmanrResult(correlation=0.7673598948883875, pvalue=6.8120979981670425e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.7914676249456448, 1.1705024876465017e-53), 'spearman': SpearmanrResult(correlation=0.8237222182469517, pvalue=1.4416897951268846e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.41227792537407415, 5.554715053501028e-10), 'spearman': SpearmanrResult(correlation=0.40648029957917753, pvalue=1.0158429011949761e-09), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6283602684818753, 'wmean': 0.6322092479452286}, 'spearman': {'mean': 0.6421188603319665, 'wmean': 0.6468040188950057}}}, 'MR': {'devacc': 76.03, 'acc': 75.08, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.16, 'acc': 79.5, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.29, 'acc': 87.59, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.13, 'acc': 93.81, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.93, 'acc': 80.45, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.42, 'acc': 40.9, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.86, 'acc': 89.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.24, 'acc': 72.93, 'f1': 79.03, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 78.34, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8088319131852874, 'pearson': 0.8216146081523439, 'spearman': 0.7497211812287896, 'mse': 0.3317290449985093, 'yhat': array([3.7530256 , 4.35368037, 1.36708753, ..., 3.78054124, 4.31090627,        4.59232244]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7519299960720257, 'pearson': 0.7099554538206362, 'spearman': 0.7046519173709499, 'mse': 1.3644039268276928, 'yhat': array([1.23450766, 1.86819881, 2.19123518, ..., 3.96565738, 3.49015793,        3.61852725]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.88, 'acc': 66.64, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.33, 'acc': 91.68, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 77.19, 'acc': 77.41, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.02, 'acc': 35.31, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.52, 'acc': 68.57, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 74.58, 'acc': 73.71, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.22, 'acc': 87.99, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.31, 'acc': 81.64, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.83, 'acc': 82.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.18, 'acc': 59.0, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.51, 'acc': 54.82, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 01:55:30,014 : STS12 p=0.5555, STS12 s=0.5738, STS13 p=0.5949, STS13 s=0.5952, STS14 p=0.6223, STS14 s=0.6077, STS15 p=0.6814, STS15 s=0.6855, STS 16 p=0.6322, STS16 s=0.6468, STS B p=0.7100, STS B s=0.7047, STS B m=1.3644, SICK-R p=0.8216, SICK-R s=0.7497, SICK-P m=0.3317
2019-03-13 01:55:30,014 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 01:55:30,014 : 0.5555,0.5738,0.5949,0.5952,0.6223,0.6077,0.6814,0.6855,0.6322,0.6468,0.7100,0.7047,1.3644,0.8216,0.7497,0.3317
2019-03-13 01:55:30,014 : MR=75.08, CR=79.50, SUBJ=93.81, MPQA=87.59, SST-B=80.45, SST-F=40.90, TREC=89.80, SICK-E=78.34, SNLI=66.64, MRPC=72.93, MRPC f=79.03
2019-03-13 01:55:30,014 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 01:55:30,014 : 75.08,79.50,93.81,87.59,80.45,40.90,89.80,78.34,66.64,72.93,79.03
2019-03-13 01:55:30,014 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 01:55:30,014 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 01:55:30,014 : na,na,na,na,na,na,na,na,na,na
2019-03-13 01:55:30,014 : SentLen=91.68, WC=77.41, TreeDepth=35.31, TopConst=68.57, BShift=73.71, Tense=87.99, SubjNum=81.64, ObjNum=82.42, SOMO=59.00, CoordInv=54.82, average=71.26
2019-03-13 01:55:30,014 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 01:55:30,014 : 91.68,77.41,35.31,68.57,73.71,87.99,81.64,82.42,59.00,54.82,71.26
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,014 : layer 9
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,014 : ********************************************************************************
2019-03-13 01:55:30,108 : ***** Transfer task : STS12 *****


2019-03-13 01:55:30,120 : loading BERT model bert-large-uncased
2019-03-13 01:55:30,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:55:30,138 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:55:30,138 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph_721zqu
2019-03-13 01:55:37,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:55:46,928 : MSRpar : pearson = 0.3876, spearman = 0.4320
2019-03-13 01:55:48,579 : MSRvid : pearson = 0.6051, spearman = 0.6088
2019-03-13 01:55:49,999 : SMTeuroparl : pearson = 0.5261, spearman = 0.6191
2019-03-13 01:55:52,708 : surprise.OnWN : pearson = 0.6725, spearman = 0.6711
2019-03-13 01:55:54,147 : surprise.SMTnews : pearson = 0.5826, spearman = 0.5488
2019-03-13 01:55:54,147 : ALL (weighted average) : Pearson = 0.5543,             Spearman = 0.5750
2019-03-13 01:55:54,148 : ALL (average) : Pearson = 0.5548,             Spearman = 0.5760

2019-03-13 01:55:54,148 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 01:55:54,158 : loading BERT model bert-large-uncased
2019-03-13 01:55:54,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:55:54,176 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:55:54,176 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpur4xg_95
2019-03-13 01:56:01,641 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:56:08,363 : FNWN : pearson = 0.4702, spearman = 0.4777
2019-03-13 01:56:10,270 : headlines : pearson = 0.6750, spearman = 0.6581
2019-03-13 01:56:11,746 : OnWN : pearson = 0.5211, spearman = 0.5474
2019-03-13 01:56:11,747 : ALL (weighted average) : Pearson = 0.5916,             Spearman = 0.5940
2019-03-13 01:56:11,747 : ALL (average) : Pearson = 0.5554,             Spearman = 0.5610

2019-03-13 01:56:11,747 : ***** Transfer task : STS14 *****


2019-03-13 01:56:11,762 : loading BERT model bert-large-uncased
2019-03-13 01:56:11,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:56:11,780 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:56:11,781 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzd3yw345
2019-03-13 01:56:19,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:56:25,997 : deft-forum : pearson = 0.3110, spearman = 0.3405
2019-03-13 01:56:27,644 : deft-news : pearson = 0.7533, spearman = 0.7177
2019-03-13 01:56:29,825 : headlines : pearson = 0.6272, spearman = 0.5764
2019-03-13 01:56:31,917 : images : pearson = 0.6324, spearman = 0.6168
2019-03-13 01:56:34,057 : OnWN : pearson = 0.6719, spearman = 0.7079
2019-03-13 01:56:36,929 : tweet-news : pearson = 0.6619, spearman = 0.6282
2019-03-13 01:56:36,930 : ALL (weighted average) : Pearson = 0.6163,             Spearman = 0.6042
2019-03-13 01:56:36,930 : ALL (average) : Pearson = 0.6096,             Spearman = 0.5979

2019-03-13 01:56:36,930 : ***** Transfer task : STS15 *****


2019-03-13 01:56:36,964 : loading BERT model bert-large-uncased
2019-03-13 01:56:36,964 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:56:36,982 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:56:36,982 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4d0ig7ow
2019-03-13 01:56:44,441 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:56:51,700 : answers-forums : pearson = 0.5299, spearman = 0.5204
2019-03-13 01:56:53,795 : answers-students : pearson = 0.6984, spearman = 0.7125
2019-03-13 01:56:55,852 : belief : pearson = 0.6448, spearman = 0.6714
2019-03-13 01:56:58,112 : headlines : pearson = 0.6788, spearman = 0.6667
2019-03-13 01:57:00,257 : images : pearson = 0.7424, spearman = 0.7531
2019-03-13 01:57:00,258 : ALL (weighted average) : Pearson = 0.6767,             Spearman = 0.6820
2019-03-13 01:57:00,258 : ALL (average) : Pearson = 0.6589,             Spearman = 0.6648

2019-03-13 01:57:00,258 : ***** Transfer task : STS16 *****


2019-03-13 01:57:00,327 : loading BERT model bert-large-uncased
2019-03-13 01:57:00,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:57:00,345 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:57:00,345 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8wo_tn1p
2019-03-13 01:57:07,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:57:14,017 : answer-answer : pearson = 0.4938, spearman = 0.5364
2019-03-13 01:57:14,680 : headlines : pearson = 0.6840, spearman = 0.6923
2019-03-13 01:57:15,567 : plagiarism : pearson = 0.7416, spearman = 0.7455
2019-03-13 01:57:17,067 : postediting : pearson = 0.7894, spearman = 0.8193
2019-03-13 01:57:17,676 : question-question : pearson = 0.4280, spearman = 0.4255
2019-03-13 01:57:17,676 : ALL (weighted average) : Pearson = 0.6310,             Spearman = 0.6484
2019-03-13 01:57:17,676 : ALL (average) : Pearson = 0.6274,             Spearman = 0.6438

2019-03-13 01:57:17,676 : ***** Transfer task : MR *****


2019-03-13 01:57:17,695 : loading BERT model bert-large-uncased
2019-03-13 01:57:17,695 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:57:17,714 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:57:17,714 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx5wpa1b9
2019-03-13 01:57:25,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:57:30,541 : Generating sentence embeddings
2019-03-13 01:58:02,132 : Generated sentence embeddings
2019-03-13 01:58:02,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:58:11,493 : Best param found at split 1: l2reg = 0.01                 with score 76.21
2019-03-13 01:58:21,868 : Best param found at split 2: l2reg = 1e-05                 with score 76.0
2019-03-13 01:58:32,035 : Best param found at split 3: l2reg = 0.0001                 with score 76.55
2019-03-13 01:58:42,665 : Best param found at split 4: l2reg = 0.0001                 with score 75.92
2019-03-13 01:58:54,089 : Best param found at split 5: l2reg = 1e-05                 with score 76.21
2019-03-13 01:58:54,615 : Dev acc : 76.18 Test acc : 74.85

2019-03-13 01:58:54,616 : ***** Transfer task : CR *****


2019-03-13 01:58:54,623 : loading BERT model bert-large-uncased
2019-03-13 01:58:54,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:58:54,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:58:54,644 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptn2e043f
2019-03-13 01:59:02,135 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:59:07,499 : Generating sentence embeddings
2019-03-13 01:59:15,849 : Generated sentence embeddings
2019-03-13 01:59:15,849 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:59:19,802 : Best param found at split 1: l2reg = 0.01                 with score 80.29
2019-03-13 01:59:23,578 : Best param found at split 2: l2reg = 0.001                 with score 79.23
2019-03-13 01:59:27,309 : Best param found at split 3: l2reg = 0.0001                 with score 79.24
2019-03-13 01:59:31,106 : Best param found at split 4: l2reg = 0.001                 with score 79.77
2019-03-13 01:59:34,806 : Best param found at split 5: l2reg = 0.001                 with score 80.04
2019-03-13 01:59:34,962 : Dev acc : 79.71 Test acc : 77.96

2019-03-13 01:59:34,962 : ***** Transfer task : MPQA *****


2019-03-13 01:59:34,968 : loading BERT model bert-large-uncased
2019-03-13 01:59:34,968 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:59:34,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:59:34,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwzfcewlu
2019-03-13 01:59:42,432 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:59:47,748 : Generating sentence embeddings
2019-03-13 01:59:55,347 : Generated sentence embeddings
2019-03-13 01:59:55,348 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:00:04,078 : Best param found at split 1: l2reg = 1e-05                 with score 87.12
2019-03-13 02:00:12,185 : Best param found at split 2: l2reg = 0.001                 with score 87.13
2019-03-13 02:00:22,235 : Best param found at split 3: l2reg = 0.001                 with score 87.19
2019-03-13 02:00:32,924 : Best param found at split 4: l2reg = 0.001                 with score 87.44
2019-03-13 02:00:43,783 : Best param found at split 5: l2reg = 0.001                 with score 87.06
2019-03-13 02:00:44,388 : Dev acc : 87.19 Test acc : 87.32

2019-03-13 02:00:44,389 : ***** Transfer task : SUBJ *****


2019-03-13 02:00:44,405 : loading BERT model bert-large-uncased
2019-03-13 02:00:44,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:00:44,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:00:44,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp90xq8wca
2019-03-13 02:00:51,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:00:57,262 : Generating sentence embeddings
2019-03-13 02:01:28,255 : Generated sentence embeddings
2019-03-13 02:01:28,256 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:01:37,080 : Best param found at split 1: l2reg = 0.001                 with score 94.01
2019-03-13 02:01:47,070 : Best param found at split 2: l2reg = 0.0001                 with score 94.34
2019-03-13 02:01:56,134 : Best param found at split 3: l2reg = 0.001                 with score 93.91
2019-03-13 02:02:06,776 : Best param found at split 4: l2reg = 0.001                 with score 94.62
2019-03-13 02:02:15,326 : Best param found at split 5: l2reg = 0.001                 with score 93.98
2019-03-13 02:02:15,761 : Dev acc : 94.17 Test acc : 94.05

2019-03-13 02:02:15,762 : ***** Transfer task : SST Binary classification *****


2019-03-13 02:02:15,891 : loading BERT model bert-large-uncased
2019-03-13 02:02:15,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:02:15,914 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:02:15,914 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_yxxe1vw
2019-03-13 02:02:23,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:02:28,568 : Computing embedding for train
2019-03-13 02:04:09,068 : Computed train embeddings
2019-03-13 02:04:09,068 : Computing embedding for dev
2019-03-13 02:04:11,257 : Computed dev embeddings
2019-03-13 02:04:11,257 : Computing embedding for test
2019-03-13 02:04:15,866 : Computed test embeddings
2019-03-13 02:04:15,866 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:04:37,191 : [('reg:1e-05', 79.82), ('reg:0.0001', 79.7), ('reg:0.001', 80.05), ('reg:0.01', 79.7)]
2019-03-13 02:04:37,192 : Validation : best param found is reg = 0.001 with score             80.05
2019-03-13 02:04:37,192 : Evaluating...
2019-03-13 02:04:42,209 : 
Dev acc : 80.05 Test acc : 81.44 for             SST Binary classification

2019-03-13 02:04:42,209 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 02:04:42,263 : loading BERT model bert-large-uncased
2019-03-13 02:04:42,263 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:04:42,286 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:04:42,286 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfvzg0cat
2019-03-13 02:04:49,742 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:04:55,055 : Computing embedding for train
2019-03-13 02:05:16,983 : Computed train embeddings
2019-03-13 02:05:16,983 : Computing embedding for dev
2019-03-13 02:05:19,852 : Computed dev embeddings
2019-03-13 02:05:19,852 : Computing embedding for test
2019-03-13 02:05:25,505 : Computed test embeddings
2019-03-13 02:05:25,506 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:05:27,282 : [('reg:1e-05', 39.96), ('reg:0.0001', 40.05), ('reg:0.001', 39.87), ('reg:0.01', 40.69)]
2019-03-13 02:05:27,282 : Validation : best param found is reg = 0.01 with score             40.69
2019-03-13 02:05:27,282 : Evaluating...
2019-03-13 02:05:27,745 : 
Dev acc : 40.69 Test acc : 43.39 for             SST Fine-Grained classification

2019-03-13 02:05:27,746 : ***** Transfer task : TREC *****


2019-03-13 02:05:27,758 : loading BERT model bert-large-uncased
2019-03-13 02:05:27,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:27,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:27,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjdq4jb7z
2019-03-13 02:05:35,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:05:48,165 : Computed train embeddings
2019-03-13 02:05:48,756 : Computed test embeddings
2019-03-13 02:05:48,756 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:05:55,061 : [('reg:1e-05', 82.34), ('reg:0.0001', 82.25), ('reg:0.001', 81.02), ('reg:0.01', 70.29)]
2019-03-13 02:05:55,061 : Cross-validation : best param found is reg = 1e-05             with score 82.34
2019-03-13 02:05:55,062 : Evaluating...
2019-03-13 02:05:55,470 : 
Dev acc : 82.34 Test acc : 90.8             for TREC

2019-03-13 02:05:55,470 : ***** Transfer task : MRPC *****


2019-03-13 02:05:55,493 : loading BERT model bert-large-uncased
2019-03-13 02:05:55,493 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:55,514 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:55,514 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy2g9i3rh
2019-03-13 02:06:03,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:06:08,189 : Computing embedding for train
2019-03-13 02:06:30,508 : Computed train embeddings
2019-03-13 02:06:30,509 : Computing embedding for test
2019-03-13 02:06:40,284 : Computed test embeddings
2019-03-13 02:06:40,305 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:06:45,333 : [('reg:1e-05', 72.03), ('reg:0.0001', 72.33), ('reg:0.001', 71.56), ('reg:0.01', 73.11)]
2019-03-13 02:06:45,333 : Cross-validation : best param found is reg = 0.01             with score 73.11
2019-03-13 02:06:45,333 : Evaluating...
2019-03-13 02:06:45,598 : Dev acc : 73.11 Test acc 74.03; Test F1 82.12 for MRPC.

2019-03-13 02:06:45,598 : ***** Transfer task : SICK-Entailment*****


2019-03-13 02:06:45,659 : loading BERT model bert-large-uncased
2019-03-13 02:06:45,659 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:06:45,678 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:06:45,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjx85h5ob
2019-03-13 02:06:53,110 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:06:58,351 : Computing embedding for train
2019-03-13 02:07:09,663 : Computed train embeddings
2019-03-13 02:07:09,663 : Computing embedding for dev
2019-03-13 02:07:11,209 : Computed dev embeddings
2019-03-13 02:07:11,210 : Computing embedding for test
2019-03-13 02:07:23,361 : Computed test embeddings
2019-03-13 02:07:23,397 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:07:24,765 : [('reg:1e-05', 80.0), ('reg:0.0001', 76.6), ('reg:0.001', 78.6), ('reg:0.01', 77.4)]
2019-03-13 02:07:24,766 : Validation : best param found is reg = 1e-05 with score             80.0
2019-03-13 02:07:24,766 : Evaluating...
2019-03-13 02:07:25,168 : 
Dev acc : 80.0 Test acc : 78.12 for                        SICK entailment

2019-03-13 02:07:25,168 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 02:07:25,195 : loading BERT model bert-large-uncased
2019-03-13 02:07:25,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:07:25,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:07:25,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpawmbkzxi
2019-03-13 02:07:32,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:07:37,981 : Computing embedding for train
2019-03-13 02:07:49,327 : Computed train embeddings
2019-03-13 02:07:49,327 : Computing embedding for dev
2019-03-13 02:07:50,874 : Computed dev embeddings
2019-03-13 02:07:50,875 : Computing embedding for test
2019-03-13 02:08:03,048 : Computed test embeddings
2019-03-13 02:08:17,815 : Dev : Pearson 0.8060989379460587
2019-03-13 02:08:17,815 : Test : Pearson 0.8177269795184101 Spearman 0.7463583193207101 MSE 0.33907965674021096                        for SICK Relatedness

2019-03-13 02:08:17,816 : 

***** Transfer task : STSBenchmark*****


2019-03-13 02:08:17,855 : loading BERT model bert-large-uncased
2019-03-13 02:08:17,855 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:08:17,884 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:08:17,884 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdfy0ofa2
2019-03-13 02:08:25,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:08:30,619 : Computing embedding for train
2019-03-13 02:08:49,258 : Computed train embeddings
2019-03-13 02:08:49,258 : Computing embedding for dev
2019-03-13 02:08:54,926 : Computed dev embeddings
2019-03-13 02:08:54,926 : Computing embedding for test
2019-03-13 02:08:59,547 : Computed test embeddings
2019-03-13 02:09:14,830 : Dev : Pearson 0.754990337982739
2019-03-13 02:09:14,830 : Test : Pearson 0.7152881418254856 Spearman 0.7076707779750138 MSE 1.3744486369943396                        for SICK Relatedness

2019-03-13 02:09:14,830 : ***** Transfer task : SNLI Entailment*****


2019-03-13 02:09:19,882 : loading BERT model bert-large-uncased
2019-03-13 02:09:19,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:09:19,952 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:09:19,952 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr0tlh6gm
2019-03-13 02:09:27,425 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:09:33,138 : PROGRESS (encoding): 0.00%
2019-03-13 02:12:19,825 : PROGRESS (encoding): 14.56%
2019-03-13 02:15:31,406 : PROGRESS (encoding): 29.12%
2019-03-13 02:18:43,093 : PROGRESS (encoding): 43.69%
2019-03-13 02:22:06,193 : PROGRESS (encoding): 58.25%
2019-03-13 02:25:52,458 : PROGRESS (encoding): 72.81%
2019-03-13 02:29:37,429 : PROGRESS (encoding): 87.37%
2019-03-13 02:33:41,212 : PROGRESS (encoding): 0.00%
2019-03-13 02:34:11,876 : PROGRESS (encoding): 0.00%
2019-03-13 02:34:41,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:35:12,674 : [('reg:1e-09', 61.26)]
2019-03-13 02:35:12,674 : Validation : best param found is reg = 1e-09 with score             61.26
2019-03-13 02:35:12,674 : Evaluating...
2019-03-13 02:35:40,326 : Dev acc : 61.26 Test acc : 61.45 for SNLI

2019-03-13 02:35:40,326 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 02:35:40,533 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 02:35:41,599 : loading BERT model bert-large-uncased
2019-03-13 02:35:41,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:35:41,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:35:41,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpom4x1bn_
2019-03-13 02:35:49,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:35:54,365 : Computing embeddings for train/dev/test
2019-03-13 02:39:26,613 : Computed embeddings
2019-03-13 02:39:26,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:39:56,030 : [('reg:1e-05', 91.66), ('reg:0.0001', 91.21), ('reg:0.001', 86.23), ('reg:0.01', 78.86)]
2019-03-13 02:39:56,030 : Validation : best param found is reg = 1e-05 with score             91.66
2019-03-13 02:39:56,030 : Evaluating...
2019-03-13 02:40:02,649 : 
Dev acc : 91.7 Test acc : 92.3 for LENGTH classification

2019-03-13 02:40:02,650 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 02:40:02,932 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 02:40:02,982 : loading BERT model bert-large-uncased
2019-03-13 02:40:02,983 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:40:03,017 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:40:03,017 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpptockduv
2019-03-13 02:40:10,557 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:40:15,766 : Computing embeddings for train/dev/test
2019-03-13 02:43:31,568 : Computed embeddings
2019-03-13 02:43:31,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:44:00,964 : [('reg:1e-05', 71.5), ('reg:0.0001', 34.56), ('reg:0.001', 2.66), ('reg:0.01', 1.15)]
2019-03-13 02:44:00,964 : Validation : best param found is reg = 1e-05 with score             71.5
2019-03-13 02:44:00,964 : Evaluating...
2019-03-13 02:44:07,632 : 
Dev acc : 71.5 Test acc : 71.1 for WORDCONTENT classification

2019-03-13 02:44:07,634 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 02:44:08,187 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 02:44:08,256 : loading BERT model bert-large-uncased
2019-03-13 02:44:08,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:44:08,283 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:44:08,283 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl_hfpqo6
2019-03-13 02:44:15,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:44:21,022 : Computing embeddings for train/dev/test
2019-03-13 02:47:25,271 : Computed embeddings
2019-03-13 02:47:25,272 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:47:50,301 : [('reg:1e-05', 35.71), ('reg:0.0001', 35.64), ('reg:0.001', 33.02), ('reg:0.01', 27.6)]
2019-03-13 02:47:50,301 : Validation : best param found is reg = 1e-05 with score             35.71
2019-03-13 02:47:50,301 : Evaluating...
2019-03-13 02:47:56,863 : 
Dev acc : 35.7 Test acc : 36.1 for DEPTH classification

2019-03-13 02:47:56,864 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 02:47:57,237 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 02:47:57,301 : loading BERT model bert-large-uncased
2019-03-13 02:47:57,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:47:57,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:47:57,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8wx4dafr
2019-03-13 02:48:04,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:48:10,212 : Computing embeddings for train/dev/test
2019-03-13 02:51:00,828 : Computed embeddings
2019-03-13 02:51:00,828 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:51:32,680 : [('reg:1e-05', 68.91), ('reg:0.0001', 66.12), ('reg:0.001', 57.39), ('reg:0.01', 42.78)]
2019-03-13 02:51:32,680 : Validation : best param found is reg = 1e-05 with score             68.91
2019-03-13 02:51:32,680 : Evaluating...
2019-03-13 02:51:41,413 : 
Dev acc : 68.9 Test acc : 69.5 for TOPCONSTITUENTS classification

2019-03-13 02:51:41,414 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 02:51:41,789 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 02:51:41,855 : loading BERT model bert-large-uncased
2019-03-13 02:51:41,855 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:51:41,885 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:51:41,885 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnov00j9v
2019-03-13 02:51:49,389 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:51:54,600 : Computing embeddings for train/dev/test
2019-03-13 02:54:59,706 : Computed embeddings
2019-03-13 02:54:59,707 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:55:36,842 : [('reg:1e-05', 74.2), ('reg:0.0001', 73.96), ('reg:0.001', 73.37), ('reg:0.01', 70.2)]
2019-03-13 02:55:36,842 : Validation : best param found is reg = 1e-05 with score             74.2
2019-03-13 02:55:36,842 : Evaluating...
2019-03-13 02:55:43,510 : 
Dev acc : 74.2 Test acc : 73.2 for BIGRAMSHIFT classification

2019-03-13 02:55:43,511 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 02:55:43,900 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 02:55:43,965 : loading BERT model bert-large-uncased
2019-03-13 02:55:43,966 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:55:43,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:55:43,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3gad9v17
2019-03-13 02:55:51,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:55:56,828 : Computing embeddings for train/dev/test
2019-03-13 02:58:57,615 : Computed embeddings
2019-03-13 02:58:57,615 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:59:23,566 : [('reg:1e-05', 88.9), ('reg:0.0001', 89.0), ('reg:0.001', 89.5), ('reg:0.01', 89.04)]
2019-03-13 02:59:23,567 : Validation : best param found is reg = 0.001 with score             89.5
2019-03-13 02:59:23,567 : Evaluating...
2019-03-13 02:59:29,018 : 
Dev acc : 89.5 Test acc : 88.3 for TENSE classification

2019-03-13 02:59:29,020 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 02:59:29,423 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 02:59:29,486 : loading BERT model bert-large-uncased
2019-03-13 02:59:29,486 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:59:29,604 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:59:29,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8k3u2tfy
2019-03-13 02:59:37,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:59:42,380 : Computing embeddings for train/dev/test
2019-03-13 03:02:53,837 : Computed embeddings
2019-03-13 03:02:53,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:03:18,240 : [('reg:1e-05', 84.37), ('reg:0.0001', 84.46), ('reg:0.001', 84.37), ('reg:0.01', 81.56)]
2019-03-13 03:03:18,240 : Validation : best param found is reg = 0.0001 with score             84.46
2019-03-13 03:03:18,240 : Evaluating...
2019-03-13 03:03:24,630 : 
Dev acc : 84.5 Test acc : 83.4 for SUBJNUMBER classification

2019-03-13 03:03:24,631 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 03:03:25,052 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 03:03:25,122 : loading BERT model bert-large-uncased
2019-03-13 03:03:25,122 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:03:25,250 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:03:25,250 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4o8h94ge
2019-03-13 03:03:32,717 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:03:37,954 : Computing embeddings for train/dev/test
2019-03-13 03:06:45,792 : Computed embeddings
2019-03-13 03:06:45,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:07:10,047 : [('reg:1e-05', 83.04), ('reg:0.0001', 82.98), ('reg:0.001', 83.0), ('reg:0.01', 81.46)]
2019-03-13 03:07:10,047 : Validation : best param found is reg = 1e-05 with score             83.04
2019-03-13 03:07:10,047 : Evaluating...
2019-03-13 03:07:16,512 : 
Dev acc : 83.0 Test acc : 83.3 for OBJNUMBER classification

2019-03-13 03:07:16,513 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 03:07:17,071 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 03:07:17,139 : loading BERT model bert-large-uncased
2019-03-13 03:07:17,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:07:17,166 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:07:17,167 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ah07whm
2019-03-13 03:07:24,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:07:29,839 : Computing embeddings for train/dev/test
2019-03-13 03:11:07,802 : Computed embeddings
2019-03-13 03:11:07,803 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:11:32,001 : [('reg:1e-05', 59.02), ('reg:0.0001', 59.03), ('reg:0.001', 59.38), ('reg:0.01', 58.45)]
2019-03-13 03:11:32,002 : Validation : best param found is reg = 0.001 with score             59.38
2019-03-13 03:11:32,002 : Evaluating...
2019-03-13 03:11:38,421 : 
Dev acc : 59.4 Test acc : 60.1 for ODDMANOUT classification

2019-03-13 03:11:38,422 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 03:11:38,795 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 03:11:38,875 : loading BERT model bert-large-uncased
2019-03-13 03:11:38,875 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:11:38,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:11:38,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt6j_3_a8
2019-03-13 03:11:46,374 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:11:51,567 : Computing embeddings for train/dev/test
2019-03-13 03:15:27,443 : Computed embeddings
2019-03-13 03:15:27,443 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:15:59,236 : [('reg:1e-05', 58.71), ('reg:0.0001', 58.5), ('reg:0.001', 57.75), ('reg:0.01', 54.42)]
2019-03-13 03:15:59,236 : Validation : best param found is reg = 1e-05 with score             58.71
2019-03-13 03:15:59,236 : Evaluating...
2019-03-13 03:16:07,993 : 
Dev acc : 58.7 Test acc : 58.6 for COORDINATIONINVERSION classification

2019-03-13 03:16:07,995 : total results: {'STS12': {'MSRpar': {'pearson': (0.3875743864012331, 2.7231444256221037e-28), 'spearman': SpearmanrResult(correlation=0.43203904352843525, pvalue=1.8580735225256934e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6050915698779998, 4.252183067923542e-76), 'spearman': SpearmanrResult(correlation=0.6087613306907182, pvalue=3.0234835321500306e-77), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5260755422060491, 4.953536924588856e-34), 'spearman': SpearmanrResult(correlation=0.6190891189018046, pvalue=6.531172416052505e-50), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6724818986996306, 7.441206050657048e-100), 'spearman': SpearmanrResult(correlation=0.67114223007703, pvalue=2.543167123393714e-99), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5826299322567101, 1.2163809629742389e-37), 'spearman': SpearmanrResult(correlation=0.5487737283137181, pvalue=9.338654126861937e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5547706658883245, 'wmean': 0.5543111029849265}, 'spearman': {'mean': 0.5759610903023412, 'wmean': 0.5749934286985969}}}, 'STS13': {'FNWN': {'pearson': (0.47017397540847816, 8.72699788720556e-12), 'spearman': SpearmanrResult(correlation=0.47765577599669895, pvalue=3.65534874581584e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6750236205150734, 7.099662887315367e-101), 'spearman': SpearmanrResult(correlation=0.6580862975502794, pvalue=2.908348340163384e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.5211023705778218, 2.2396506559234356e-40), 'spearman': SpearmanrResult(correlation=0.5473988781129495, pvalue=3.583431769513567e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5554333221671244, 'wmean': 0.5916460177551103}, 'spearman': {'mean': 0.5610469838866426, 'wmean': 0.5939549569649669}}}, 'STS14': {'deft-forum': {'pearson': (0.3109742085851694, 1.5155834955301996e-11), 'spearman': SpearmanrResult(correlation=0.3405091743388568, pvalue=1.1170004423641276e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7533313588692445, 3.5245078882486675e-56), 'spearman': SpearmanrResult(correlation=0.7176852956012785, pvalue=9.398715406948678e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.6271820725927006, 3.0442547210341654e-83), 'spearman': SpearmanrResult(correlation=0.5764239111407707, pvalue=1.273533026171803e-67), 'nsamples': 750}, 'images': {'pearson': (0.6324105061444886, 5.092896530932922e-85), 'spearman': SpearmanrResult(correlation=0.6168409476104594, pvalue=7.926528065228153e-80), 'nsamples': 750}, 'OnWN': {'pearson': (0.6718852006542464, 1.2874139550901728e-99), 'spearman': SpearmanrResult(correlation=0.7079239618252441, pvalue=4.497708482957847e-115), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6619122332142553, 1.0179928099068854e-95), 'spearman': SpearmanrResult(correlation=0.6282302109794882, pvalue=1.3491665972561291e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6096159300100173, 'wmean': 0.6162614162608981}, 'spearman': {'mean': 0.5979355835826828, 'wmean': 0.6041597308799576}}}, 'STS15': {'answers-forums': {'pearson': (0.5299235189688772, 1.5452424335027e-28), 'spearman': SpearmanrResult(correlation=0.5204059132040995, pvalue=2.064483329507504e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6984215031831301, 8.971584762726534e-111), 'spearman': SpearmanrResult(correlation=0.7125007906860268, pvalue=3.3076517072585306e-117), 'nsamples': 750}, 'belief': {'pearson': (0.6447687606078354, 1.8995983313247925e-45), 'spearman': SpearmanrResult(correlation=0.6714234105592548, pvalue=1.7609873820311262e-50), 'nsamples': 375}, 'headlines': {'pearson': (0.6788039300306773, 2.062906412373699e-102), 'spearman': SpearmanrResult(correlation=0.6666977978831448, pvalue=1.4326531780788658e-97), 'nsamples': 750}, 'images': {'pearson': (0.7423680349412958, 3.11874759448329e-132), 'spearman': SpearmanrResult(correlation=0.7530710007237543, pvalue=3.895349900840784e-138), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6588571495463631, 'wmean': 0.676734901985865}, 'spearman': {'mean': 0.6648197826112561, 'wmean': 0.6820460627936508}}}, 'STS16': {'answer-answer': {'pearson': (0.49381406717951465, 5.087785906800631e-17), 'spearman': SpearmanrResult(correlation=0.5364267239916721, pvalue=2.503656852021402e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.684012764223594, 1.0739116368499898e-35), 'spearman': SpearmanrResult(correlation=0.6923368124758428, pvalue=7.217863175789754e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7415872296992905, 2.100639575203463e-41), 'spearman': SpearmanrResult(correlation=0.7455330753681672, pvalue=4.6806654774808095e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.7893528485400949, 3.4489229464868485e-53), 'spearman': SpearmanrResult(correlation=0.8192590818471039, pvalue=2.2200875987148748e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.4280384809079678, 1.0143176392366806e-10), 'spearman': SpearmanrResult(correlation=0.42553858218210494, pvalue=1.3362093437222604e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6273610781100925, 'wmean': 0.6310077164752588}, 'spearman': {'mean': 0.6438188551729782, 'wmean': 0.6483588880116368}}}, 'MR': {'devacc': 76.18, 'acc': 74.85, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.71, 'acc': 77.96, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.19, 'acc': 87.32, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.17, 'acc': 94.05, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.05, 'acc': 81.44, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.69, 'acc': 43.39, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.34, 'acc': 90.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.11, 'acc': 74.03, 'f1': 82.12, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.0, 'acc': 78.12, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8060989379460587, 'pearson': 0.8177269795184101, 'spearman': 0.7463583193207101, 'mse': 0.33907965674021096, 'yhat': array([3.7130002 , 4.59193498, 1.6576462 , ..., 3.65744201, 4.49250294,        4.5880787 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.754990337982739, 'pearson': 0.7152881418254856, 'spearman': 0.7076707779750138, 'mse': 1.3744486369943396, 'yhat': array([1.35680152, 1.49467524, 2.30666895, ..., 4.08956173, 3.51451648,        3.45343972]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.26, 'acc': 61.45, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.66, 'acc': 92.3, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 71.5, 'acc': 71.09, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.71, 'acc': 36.14, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.91, 'acc': 69.47, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 74.2, 'acc': 73.2, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.5, 'acc': 88.34, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.46, 'acc': 83.4, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.04, 'acc': 83.34, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.38, 'acc': 60.07, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.71, 'acc': 58.56, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 03:16:07,995 : STS12 p=0.5543, STS12 s=0.5750, STS13 p=0.5916, STS13 s=0.5940, STS14 p=0.6163, STS14 s=0.6042, STS15 p=0.6767, STS15 s=0.6820, STS 16 p=0.6310, STS16 s=0.6484, STS B p=0.7153, STS B s=0.7077, STS B m=1.3744, SICK-R p=0.8177, SICK-R s=0.7464, SICK-P m=0.3391
2019-03-13 03:16:07,995 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 03:16:07,995 : 0.5543,0.5750,0.5916,0.5940,0.6163,0.6042,0.6767,0.6820,0.6310,0.6484,0.7153,0.7077,1.3744,0.8177,0.7464,0.3391
2019-03-13 03:16:07,995 : MR=74.85, CR=77.96, SUBJ=94.05, MPQA=87.32, SST-B=81.44, SST-F=43.39, TREC=90.80, SICK-E=78.12, SNLI=61.45, MRPC=74.03, MRPC f=82.12
2019-03-13 03:16:07,995 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 03:16:07,995 : 74.85,77.96,94.05,87.32,81.44,43.39,90.80,78.12,61.45,74.03,82.12
2019-03-13 03:16:07,995 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 03:16:07,995 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 03:16:07,995 : na,na,na,na,na,na,na,na,na,na
2019-03-13 03:16:07,995 : SentLen=92.30, WC=71.09, TreeDepth=36.14, TopConst=69.47, BShift=73.20, Tense=88.34, SubjNum=83.40, ObjNum=83.34, SOMO=60.07, CoordInv=58.56, average=71.59
2019-03-13 03:16:07,995 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 03:16:07,995 : 92.30,71.09,36.14,69.47,73.20,88.34,83.40,83.34,60.07,58.56,71.59
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:07,995 : layer 10
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:07,995 : ********************************************************************************
2019-03-13 03:16:08,086 : ***** Transfer task : STS12 *****


2019-03-13 03:16:08,098 : loading BERT model bert-large-uncased
2019-03-13 03:16:08,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:16:08,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:16:08,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp064p3t86
2019-03-13 03:16:15,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:16:24,992 : MSRpar : pearson = 0.3886, spearman = 0.4349
2019-03-13 03:16:26,651 : MSRvid : pearson = 0.5907, spearman = 0.5946
2019-03-13 03:16:28,081 : SMTeuroparl : pearson = 0.5307, spearman = 0.6257
2019-03-13 03:16:30,788 : surprise.OnWN : pearson = 0.6690, spearman = 0.6729
2019-03-13 03:16:32,223 : surprise.SMTnews : pearson = 0.5948, spearman = 0.5510
2019-03-13 03:16:32,223 : ALL (weighted average) : Pearson = 0.5525,             Spearman = 0.5739
2019-03-13 03:16:32,223 : ALL (average) : Pearson = 0.5547,             Spearman = 0.5758

2019-03-13 03:16:32,223 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 03:16:32,232 : loading BERT model bert-large-uncased
2019-03-13 03:16:32,232 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:16:32,249 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:16:32,249 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp90mzcfap
2019-03-13 03:16:39,680 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:16:46,276 : FNWN : pearson = 0.4516, spearman = 0.4480
2019-03-13 03:16:48,179 : headlines : pearson = 0.6723, spearman = 0.6547
2019-03-13 03:16:49,654 : OnWN : pearson = 0.4904, spearman = 0.5208
2019-03-13 03:16:49,654 : ALL (weighted average) : Pearson = 0.5765,             Spearman = 0.5786
2019-03-13 03:16:49,654 : ALL (average) : Pearson = 0.5381,             Spearman = 0.5412

2019-03-13 03:16:49,654 : ***** Transfer task : STS14 *****


2019-03-13 03:16:49,669 : loading BERT model bert-large-uncased
2019-03-13 03:16:49,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:16:49,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:16:49,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2jome5gi
2019-03-13 03:16:57,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:17:03,875 : deft-forum : pearson = 0.2963, spearman = 0.3452
2019-03-13 03:17:05,522 : deft-news : pearson = 0.7486, spearman = 0.7131
2019-03-13 03:17:07,704 : headlines : pearson = 0.6199, spearman = 0.5727
2019-03-13 03:17:09,794 : images : pearson = 0.6197, spearman = 0.6063
2019-03-13 03:17:11,938 : OnWN : pearson = 0.6451, spearman = 0.6883
2019-03-13 03:17:14,815 : tweet-news : pearson = 0.6474, spearman = 0.6190
2019-03-13 03:17:14,815 : ALL (weighted average) : Pearson = 0.6019,             Spearman = 0.5957
2019-03-13 03:17:14,815 : ALL (average) : Pearson = 0.5962,             Spearman = 0.5908

2019-03-13 03:17:14,816 : ***** Transfer task : STS15 *****


2019-03-13 03:17:14,866 : loading BERT model bert-large-uncased
2019-03-13 03:17:14,866 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:17:14,884 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:17:14,885 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6lklzn20
2019-03-13 03:17:22,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:17:29,442 : answers-forums : pearson = 0.5165, spearman = 0.5065
2019-03-13 03:17:31,540 : answers-students : pearson = 0.6984, spearman = 0.7130
2019-03-13 03:17:33,598 : belief : pearson = 0.6298, spearman = 0.6631
2019-03-13 03:17:35,861 : headlines : pearson = 0.6723, spearman = 0.6622
2019-03-13 03:17:38,005 : images : pearson = 0.7398, spearman = 0.7511
2019-03-13 03:17:38,005 : ALL (weighted average) : Pearson = 0.6709,             Spearman = 0.6778
2019-03-13 03:17:38,005 : ALL (average) : Pearson = 0.6514,             Spearman = 0.6592

2019-03-13 03:17:38,005 : ***** Transfer task : STS16 *****


2019-03-13 03:17:38,075 : loading BERT model bert-large-uncased
2019-03-13 03:17:38,075 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:17:38,093 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:17:38,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpksrqvskl
2019-03-13 03:17:45,632 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:17:51,777 : answer-answer : pearson = 0.4982, spearman = 0.5442
2019-03-13 03:17:52,441 : headlines : pearson = 0.6864, spearman = 0.6921
2019-03-13 03:17:53,327 : plagiarism : pearson = 0.7380, spearman = 0.7473
2019-03-13 03:17:54,828 : postediting : pearson = 0.7819, spearman = 0.8102
2019-03-13 03:17:55,437 : question-question : pearson = 0.4062, spearman = 0.4096
2019-03-13 03:17:55,437 : ALL (weighted average) : Pearson = 0.6264,             Spearman = 0.6456
2019-03-13 03:17:55,437 : ALL (average) : Pearson = 0.6222,             Spearman = 0.6407

2019-03-13 03:17:55,437 : ***** Transfer task : MR *****


2019-03-13 03:17:55,452 : loading BERT model bert-large-uncased
2019-03-13 03:17:55,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:17:55,472 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:17:55,473 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkzo1vp3q
2019-03-13 03:18:02,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:18:08,325 : Generating sentence embeddings
2019-03-13 03:18:39,901 : Generated sentence embeddings
2019-03-13 03:18:39,902 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:18:49,632 : Best param found at split 1: l2reg = 0.001                 with score 76.66
2019-03-13 03:19:00,338 : Best param found at split 2: l2reg = 0.001                 with score 76.72
2019-03-13 03:19:10,777 : Best param found at split 3: l2reg = 1e-05                 with score 77.19
2019-03-13 03:19:21,418 : Best param found at split 4: l2reg = 0.01                 with score 76.66
2019-03-13 03:19:28,931 : Best param found at split 5: l2reg = 0.001                 with score 76.95
2019-03-13 03:19:29,434 : Dev acc : 76.84 Test acc : 75.32

2019-03-13 03:19:29,435 : ***** Transfer task : CR *****


2019-03-13 03:19:29,442 : loading BERT model bert-large-uncased
2019-03-13 03:19:29,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:19:29,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:19:29,462 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0ajkuq5w
2019-03-13 03:19:36,945 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:19:42,153 : Generating sentence embeddings
2019-03-13 03:19:50,517 : Generated sentence embeddings
2019-03-13 03:19:50,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:19:53,406 : Best param found at split 1: l2reg = 0.001                 with score 80.26
2019-03-13 03:19:56,215 : Best param found at split 2: l2reg = 1e-05                 with score 79.4
2019-03-13 03:19:59,049 : Best param found at split 3: l2reg = 0.0001                 with score 79.47
2019-03-13 03:20:02,240 : Best param found at split 4: l2reg = 0.0001                 with score 80.01
2019-03-13 03:20:05,814 : Best param found at split 5: l2reg = 0.01                 with score 80.01
2019-03-13 03:20:06,033 : Dev acc : 79.83 Test acc : 78.12

2019-03-13 03:20:06,034 : ***** Transfer task : MPQA *****


2019-03-13 03:20:06,040 : loading BERT model bert-large-uncased
2019-03-13 03:20:06,040 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:20:06,090 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:20:06,091 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi2090t0j
2019-03-13 03:20:13,573 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:20:18,842 : Generating sentence embeddings
2019-03-13 03:20:26,455 : Generated sentence embeddings
2019-03-13 03:20:26,456 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:20:36,644 : Best param found at split 1: l2reg = 0.0001                 with score 87.06
2019-03-13 03:20:47,395 : Best param found at split 2: l2reg = 0.001                 with score 85.93
2019-03-13 03:20:58,479 : Best param found at split 3: l2reg = 1e-05                 with score 86.72
2019-03-13 03:21:08,738 : Best param found at split 4: l2reg = 0.001                 with score 86.66
2019-03-13 03:21:20,458 : Best param found at split 5: l2reg = 0.0001                 with score 86.58
2019-03-13 03:21:21,321 : Dev acc : 86.59 Test acc : 87.32

2019-03-13 03:21:21,322 : ***** Transfer task : SUBJ *****


2019-03-13 03:21:21,340 : loading BERT model bert-large-uncased
2019-03-13 03:21:21,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:21:21,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:21:21,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp72egd_nv
2019-03-13 03:21:28,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:21:34,176 : Generating sentence embeddings
2019-03-13 03:22:05,133 : Generated sentence embeddings
2019-03-13 03:22:05,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:22:11,466 : Best param found at split 1: l2reg = 0.001                 with score 94.12
2019-03-13 03:22:19,672 : Best param found at split 2: l2reg = 0.001                 with score 94.41
2019-03-13 03:22:29,436 : Best param found at split 3: l2reg = 0.001                 with score 93.85
2019-03-13 03:22:40,201 : Best param found at split 4: l2reg = 0.001                 with score 94.56
2019-03-13 03:22:47,400 : Best param found at split 5: l2reg = 0.001                 with score 94.11
2019-03-13 03:22:47,814 : Dev acc : 94.21 Test acc : 94.14

2019-03-13 03:22:47,815 : ***** Transfer task : SST Binary classification *****


2019-03-13 03:22:47,907 : loading BERT model bert-large-uncased
2019-03-13 03:22:47,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:22:47,980 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:22:47,980 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6fs3dr2h
2019-03-13 03:22:55,436 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:23:00,699 : Computing embedding for train
2019-03-13 03:24:41,182 : Computed train embeddings
2019-03-13 03:24:41,182 : Computing embedding for dev
2019-03-13 03:24:43,385 : Computed dev embeddings
2019-03-13 03:24:43,385 : Computing embedding for test
2019-03-13 03:24:48,001 : Computed test embeddings
2019-03-13 03:24:48,001 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:25:04,124 : [('reg:1e-05', 80.28), ('reg:0.0001', 80.28), ('reg:0.001', 80.85), ('reg:0.01', 80.73)]
2019-03-13 03:25:04,124 : Validation : best param found is reg = 0.001 with score             80.85
2019-03-13 03:25:04,124 : Evaluating...
2019-03-13 03:25:08,460 : 
Dev acc : 80.85 Test acc : 80.62 for             SST Binary classification

2019-03-13 03:25:08,460 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 03:25:08,515 : loading BERT model bert-large-uncased
2019-03-13 03:25:08,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:25:08,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:25:08,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzy65slx9
2019-03-13 03:25:16,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:25:21,189 : Computing embedding for train
2019-03-13 03:25:43,177 : Computed train embeddings
2019-03-13 03:25:43,177 : Computing embedding for dev
2019-03-13 03:25:46,053 : Computed dev embeddings
2019-03-13 03:25:46,053 : Computing embedding for test
2019-03-13 03:25:51,717 : Computed test embeddings
2019-03-13 03:25:51,717 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:25:54,029 : [('reg:1e-05', 40.33), ('reg:0.0001', 40.24), ('reg:0.001', 40.51), ('reg:0.01', 41.96)]
2019-03-13 03:25:54,029 : Validation : best param found is reg = 0.01 with score             41.96
2019-03-13 03:25:54,029 : Evaluating...
2019-03-13 03:25:54,668 : 
Dev acc : 41.96 Test acc : 42.99 for             SST Fine-Grained classification

2019-03-13 03:25:54,668 : ***** Transfer task : TREC *****


2019-03-13 03:25:54,682 : loading BERT model bert-large-uncased
2019-03-13 03:25:54,682 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:25:54,700 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:25:54,701 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpul07m6uu
2019-03-13 03:26:02,192 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:14,960 : Computed train embeddings
2019-03-13 03:26:15,550 : Computed test embeddings
2019-03-13 03:26:15,551 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:26:22,042 : [('reg:1e-05', 81.14), ('reg:0.0001', 80.98), ('reg:0.001', 79.6), ('reg:0.01', 70.2)]
2019-03-13 03:26:22,042 : Cross-validation : best param found is reg = 1e-05             with score 81.14
2019-03-13 03:26:22,042 : Evaluating...
2019-03-13 03:26:22,536 : 
Dev acc : 81.14 Test acc : 89.8             for TREC

2019-03-13 03:26:22,537 : ***** Transfer task : MRPC *****


2019-03-13 03:26:22,558 : loading BERT model bert-large-uncased
2019-03-13 03:26:22,558 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:26:22,582 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:26:22,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqi14nydc
2019-03-13 03:26:30,126 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:35,539 : Computing embedding for train
2019-03-13 03:26:57,878 : Computed train embeddings
2019-03-13 03:26:57,878 : Computing embedding for test
2019-03-13 03:27:07,677 : Computed test embeddings
2019-03-13 03:27:07,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:27:12,495 : [('reg:1e-05', 73.19), ('reg:0.0001', 72.5), ('reg:0.001', 72.06), ('reg:0.01', 72.87)]
2019-03-13 03:27:12,495 : Cross-validation : best param found is reg = 1e-05             with score 73.19
2019-03-13 03:27:12,495 : Evaluating...
2019-03-13 03:27:12,799 : Dev acc : 73.19 Test acc 70.96; Test F1 76.6 for MRPC.

2019-03-13 03:27:12,799 : ***** Transfer task : SICK-Entailment*****


2019-03-13 03:27:12,862 : loading BERT model bert-large-uncased
2019-03-13 03:27:12,862 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:27:12,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:27:12,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeaysmhp1
2019-03-13 03:27:20,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:27:25,616 : Computing embedding for train
2019-03-13 03:27:36,934 : Computed train embeddings
2019-03-13 03:27:36,934 : Computing embedding for dev
2019-03-13 03:27:38,479 : Computed dev embeddings
2019-03-13 03:27:38,479 : Computing embedding for test
2019-03-13 03:27:50,647 : Computed test embeddings
2019-03-13 03:27:50,684 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:27:52,123 : [('reg:1e-05', 79.8), ('reg:0.0001', 80.0), ('reg:0.001', 79.2), ('reg:0.01', 70.2)]
2019-03-13 03:27:52,123 : Validation : best param found is reg = 0.0001 with score             80.0
2019-03-13 03:27:52,123 : Evaluating...
2019-03-13 03:27:52,474 : 
Dev acc : 80.0 Test acc : 77.98 for                        SICK entailment

2019-03-13 03:27:52,474 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 03:27:52,500 : loading BERT model bert-large-uncased
2019-03-13 03:27:52,500 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:27:52,558 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:27:52,558 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp5c36a6e
2019-03-13 03:28:00,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:28:05,265 : Computing embedding for train
2019-03-13 03:28:16,590 : Computed train embeddings
2019-03-13 03:28:16,590 : Computing embedding for dev
2019-03-13 03:28:18,138 : Computed dev embeddings
2019-03-13 03:28:18,138 : Computing embedding for test
2019-03-13 03:28:30,315 : Computed test embeddings
2019-03-13 03:28:48,592 : Dev : Pearson 0.8190763959733466
2019-03-13 03:28:48,592 : Test : Pearson 0.8100282501465923 Spearman 0.7382063625674621 MSE 0.35022027133320544                        for SICK Relatedness

2019-03-13 03:28:48,593 : 

***** Transfer task : STSBenchmark*****


2019-03-13 03:28:48,631 : loading BERT model bert-large-uncased
2019-03-13 03:28:48,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:28:48,661 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:28:48,661 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8r8ri0aq
2019-03-13 03:28:56,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:29:01,363 : Computing embedding for train
2019-03-13 03:29:19,999 : Computed train embeddings
2019-03-13 03:29:19,999 : Computing embedding for dev
2019-03-13 03:29:25,652 : Computed dev embeddings
2019-03-13 03:29:25,652 : Computing embedding for test
2019-03-13 03:29:30,270 : Computed test embeddings
2019-03-13 03:29:48,222 : Dev : Pearson 0.7552060059242556
2019-03-13 03:29:48,222 : Test : Pearson 0.7170741799038057 Spearman 0.7101229682603117 MSE 1.4009896542142546                        for SICK Relatedness

2019-03-13 03:29:48,223 : ***** Transfer task : SNLI Entailment*****


2019-03-13 03:29:53,326 : loading BERT model bert-large-uncased
2019-03-13 03:29:53,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:29:53,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:29:53,449 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmgebxyt3
2019-03-13 03:30:00,908 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:30:06,595 : PROGRESS (encoding): 0.00%
2019-03-13 03:32:53,317 : PROGRESS (encoding): 14.56%
2019-03-13 03:36:04,562 : PROGRESS (encoding): 29.12%
2019-03-13 03:39:15,724 : PROGRESS (encoding): 43.69%
2019-03-13 03:42:38,792 : PROGRESS (encoding): 58.25%
2019-03-13 03:46:24,881 : PROGRESS (encoding): 72.81%
2019-03-13 03:50:09,725 : PROGRESS (encoding): 87.37%
2019-03-13 03:54:13,396 : PROGRESS (encoding): 0.00%
2019-03-13 03:54:44,057 : PROGRESS (encoding): 0.00%
2019-03-13 03:55:13,522 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:55:42,559 : [('reg:1e-09', 68.94)]
2019-03-13 03:55:42,559 : Validation : best param found is reg = 1e-09 with score             68.94
2019-03-13 03:55:42,559 : Evaluating...
2019-03-13 03:56:15,054 : Dev acc : 68.94 Test acc : 69.33 for SNLI

2019-03-13 03:56:15,054 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 03:56:15,256 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 03:56:16,328 : loading BERT model bert-large-uncased
2019-03-13 03:56:16,329 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:56:16,355 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:56:16,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpltj7c3v4
2019-03-13 03:56:23,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:56:29,116 : Computing embeddings for train/dev/test
2019-03-13 04:00:01,108 : Computed embeddings
2019-03-13 04:00:01,109 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:00:31,669 : [('reg:1e-05', 92.8), ('reg:0.0001', 91.59), ('reg:0.001', 85.01), ('reg:0.01', 80.15)]
2019-03-13 04:00:31,669 : Validation : best param found is reg = 1e-05 with score             92.8
2019-03-13 04:00:31,669 : Evaluating...
2019-03-13 04:00:39,644 : 
Dev acc : 92.8 Test acc : 93.6 for LENGTH classification

2019-03-13 04:00:39,645 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 04:00:40,020 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 04:00:40,064 : loading BERT model bert-large-uncased
2019-03-13 04:00:40,064 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:00:40,094 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:00:40,094 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpadl4c1su
2019-03-13 04:00:47,527 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:00:52,864 : Computing embeddings for train/dev/test
2019-03-13 04:04:08,492 : Computed embeddings
2019-03-13 04:04:08,492 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:04:38,518 : [('reg:1e-05', 72.0), ('reg:0.0001', 30.2), ('reg:0.001', 2.26), ('reg:0.01', 1.05)]
2019-03-13 04:04:38,518 : Validation : best param found is reg = 1e-05 with score             72.0
2019-03-13 04:04:38,518 : Evaluating...
2019-03-13 04:04:47,282 : 
Dev acc : 72.0 Test acc : 71.8 for WORDCONTENT classification

2019-03-13 04:04:47,284 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 04:04:47,656 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 04:04:47,721 : loading BERT model bert-large-uncased
2019-03-13 04:04:47,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:04:47,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:04:47,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppr8yzn_a
2019-03-13 04:04:55,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:05:00,488 : Computing embeddings for train/dev/test
2019-03-13 04:08:04,616 : Computed embeddings
2019-03-13 04:08:04,617 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:08:28,087 : [('reg:1e-05', 35.78), ('reg:0.0001', 35.45), ('reg:0.001', 33.11), ('reg:0.01', 27.76)]
2019-03-13 04:08:28,088 : Validation : best param found is reg = 1e-05 with score             35.78
2019-03-13 04:08:28,088 : Evaluating...
2019-03-13 04:08:34,986 : 
Dev acc : 35.8 Test acc : 36.7 for DEPTH classification

2019-03-13 04:08:34,987 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 04:08:35,367 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 04:08:35,430 : loading BERT model bert-large-uncased
2019-03-13 04:08:35,430 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:08:35,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:08:35,540 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3lzy342p
2019-03-13 04:08:43,046 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:08:48,258 : Computing embeddings for train/dev/test
2019-03-13 04:11:38,540 : Computed embeddings
2019-03-13 04:11:38,540 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:12:04,075 : [('reg:1e-05', 70.7), ('reg:0.0001', 68.08), ('reg:0.001', 59.55), ('reg:0.01', 44.01)]
2019-03-13 04:12:04,075 : Validation : best param found is reg = 1e-05 with score             70.7
2019-03-13 04:12:04,075 : Evaluating...
2019-03-13 04:12:09,500 : 
Dev acc : 70.7 Test acc : 70.4 for TOPCONSTITUENTS classification

2019-03-13 04:12:09,501 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 04:12:09,840 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 04:12:09,906 : loading BERT model bert-large-uncased
2019-03-13 04:12:09,906 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:12:10,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:12:10,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv8risny9
2019-03-13 04:12:17,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:12:22,856 : Computing embeddings for train/dev/test
2019-03-13 04:15:27,898 : Computed embeddings
2019-03-13 04:15:27,898 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:16:05,924 : [('reg:1e-05', 82.63), ('reg:0.0001', 82.5), ('reg:0.001', 81.93), ('reg:0.01', 78.78)]
2019-03-13 04:16:05,924 : Validation : best param found is reg = 1e-05 with score             82.63
2019-03-13 04:16:05,924 : Evaluating...
2019-03-13 04:16:16,801 : 
Dev acc : 82.6 Test acc : 81.7 for BIGRAMSHIFT classification

2019-03-13 04:16:16,802 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 04:16:17,370 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 04:16:17,436 : loading BERT model bert-large-uncased
2019-03-13 04:16:17,437 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:16:17,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:16:17,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpirkk91fx
2019-03-13 04:16:24,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:16:30,225 : Computing embeddings for train/dev/test
2019-03-13 04:19:30,917 : Computed embeddings
2019-03-13 04:19:30,918 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:19:57,815 : [('reg:1e-05', 88.77), ('reg:0.0001', 89.0), ('reg:0.001', 89.48), ('reg:0.01', 88.71)]
2019-03-13 04:19:57,815 : Validation : best param found is reg = 0.001 with score             89.48
2019-03-13 04:19:57,815 : Evaluating...
2019-03-13 04:20:04,312 : 
Dev acc : 89.5 Test acc : 88.3 for TENSE classification

2019-03-13 04:20:04,314 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 04:20:04,739 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 04:20:04,804 : loading BERT model bert-large-uncased
2019-03-13 04:20:04,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:20:04,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:20:04,830 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaephzoi5
2019-03-13 04:20:12,273 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:20:17,627 : Computing embeddings for train/dev/test
2019-03-13 04:23:29,104 : Computed embeddings
2019-03-13 04:23:29,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:23:54,801 : [('reg:1e-05', 84.31), ('reg:0.0001', 84.12), ('reg:0.001', 84.45), ('reg:0.01', 82.03)]
2019-03-13 04:23:54,801 : Validation : best param found is reg = 0.001 with score             84.45
2019-03-13 04:23:54,801 : Evaluating...
2019-03-13 04:24:01,456 : 
Dev acc : 84.5 Test acc : 83.0 for SUBJNUMBER classification

2019-03-13 04:24:01,457 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 04:24:01,862 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 04:24:01,928 : loading BERT model bert-large-uncased
2019-03-13 04:24:01,928 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:24:02,044 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:24:02,044 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp80cqkl4b
2019-03-13 04:24:09,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:24:14,851 : Computing embeddings for train/dev/test
2019-03-13 04:27:22,658 : Computed embeddings
2019-03-13 04:27:22,658 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:27:54,490 : [('reg:1e-05', 83.04), ('reg:0.0001', 83.23), ('reg:0.001', 83.3), ('reg:0.01', 82.15)]
2019-03-13 04:27:54,491 : Validation : best param found is reg = 0.001 with score             83.3
2019-03-13 04:27:54,491 : Evaluating...
2019-03-13 04:28:02,104 : 
Dev acc : 83.3 Test acc : 83.8 for OBJNUMBER classification

2019-03-13 04:28:02,106 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 04:28:02,684 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 04:28:02,752 : loading BERT model bert-large-uncased
2019-03-13 04:28:02,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:28:02,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:28:02,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph8r37hw0
2019-03-13 04:28:10,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:28:15,659 : Computing embeddings for train/dev/test
2019-03-13 04:31:53,323 : Computed embeddings
2019-03-13 04:31:53,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:32:16,525 : [('reg:1e-05', 60.37), ('reg:0.0001', 60.34), ('reg:0.001', 60.63), ('reg:0.01', 59.87)]
2019-03-13 04:32:16,525 : Validation : best param found is reg = 0.001 with score             60.63
2019-03-13 04:32:16,525 : Evaluating...
2019-03-13 04:32:22,946 : 
Dev acc : 60.6 Test acc : 61.3 for ODDMANOUT classification

2019-03-13 04:32:22,947 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 04:32:23,334 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 04:32:23,410 : loading BERT model bert-large-uncased
2019-03-13 04:32:23,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:32:23,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:32:23,533 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuz4i6kvb
2019-03-13 04:32:31,012 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:32:36,323 : Computing embeddings for train/dev/test
2019-03-13 04:36:12,318 : Computed embeddings
2019-03-13 04:36:12,318 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:36:43,556 : [('reg:1e-05', 58.71), ('reg:0.0001', 58.46), ('reg:0.001', 57.74), ('reg:0.01', 53.95)]
2019-03-13 04:36:43,556 : Validation : best param found is reg = 1e-05 with score             58.71
2019-03-13 04:36:43,556 : Evaluating...
2019-03-13 04:36:52,283 : 
Dev acc : 58.7 Test acc : 59.5 for COORDINATIONINVERSION classification

2019-03-13 04:36:52,285 : total results: {'STS12': {'MSRpar': {'pearson': (0.3885738511916066, 1.930331011129629e-28), 'spearman': SpearmanrResult(correlation=0.43485350829903147, pvalue=6.0021763441226356e-36), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5906803971544119, 9.904316802682025e-72), 'spearman': SpearmanrResult(correlation=0.5945567081282139, pvalue=6.963236854743341e-73), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5306749587356433, 1.0520626254812897e-34), 'spearman': SpearmanrResult(correlation=0.6256856549856834, pvalue=3.0221801085581883e-51), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6689592395995371, 1.8582564980088063e-98), 'spearman': SpearmanrResult(correlation=0.6729471711259819, pvalue=4.848612596391602e-100), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5948335514757885, 1.514100112026066e-39), 'spearman': SpearmanrResult(correlation=0.5509636667105269, pvalue=4.687946367208409e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5547443996313974, 'wmean': 0.5524705627598671}, 'spearman': {'mean': 0.5758013418498875, 'wmean': 0.5739357333722166}}}, 'STS13': {'FNWN': {'pearson': (0.4515660825364047, 6.950695269456417e-11), 'spearman': SpearmanrResult(correlation=0.4479746281937575, pvalue=1.0227882469001003e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6722941281971224, 8.843447304897953e-100), 'spearman': SpearmanrResult(correlation=0.6546922031908883, pvalue=5.463446628274377e-93), 'nsamples': 750}, 'OnWN': {'pearson': (0.49042060123588455, 2.758021147825766e-35), 'spearman': SpearmanrResult(correlation=0.5207953517064302, pvalue=2.53352197689304e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5380936039898039, 'wmean': 0.5764616953603691}, 'spearman': {'mean': 0.5411540610303587, 'wmean': 0.5785683662860625}}}, 'STS14': {'deft-forum': {'pearson': (0.29626549046484174, 1.438454580067141e-10), 'spearman': SpearmanrResult(correlation=0.34520719931758276, pvalue=4.865923839915872e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7485686955181301, 4.086132946086774e-55), 'spearman': SpearmanrResult(correlation=0.7131245523056897, pvalue=6.930744666587478e-48), 'nsamples': 300}, 'headlines': {'pearson': (0.6198733980475307, 8.14160446585144e-81), 'spearman': SpearmanrResult(correlation=0.5726612906036965, pvalue=1.4326723346433473e-66), 'nsamples': 750}, 'images': {'pearson': (0.6196880971809213, 9.363030113973883e-81), 'spearman': SpearmanrResult(correlation=0.6063283759846797, pvalue=1.751231395904944e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.6451254860173584, 1.741036919577187e-89), 'spearman': SpearmanrResult(correlation=0.6883353757120901, pvalue=2.1631842094130753e-106), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6474136028865567, 2.596393835269458e-90), 'spearman': SpearmanrResult(correlation=0.6189862960898089, pvalue=1.58841984178145e-80), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5961557950192232, 'wmean': 0.6018574713237049}, 'spearman': {'mean': 0.5907738483355912, 'wmean': 0.5957370957806202}}}, 'STS15': {'answers-forums': {'pearson': (0.516457734825617, 5.908919484153257e-27), 'spearman': SpearmanrResult(correlation=0.506505287933911, pvalue=7.881912972515849e-26), 'nsamples': 375}, 'answers-students': {'pearson': (0.6984236026878394, 8.952367052769475e-111), 'spearman': SpearmanrResult(correlation=0.7130207416929304, pvalue=1.8814581735415388e-117), 'nsamples': 750}, 'belief': {'pearson': (0.6298404253058407, 7.663457954306552e-43), 'spearman': SpearmanrResult(correlation=0.6630927201406616, pvalue=7.487998111403808e-49), 'nsamples': 375}, 'headlines': {'pearson': (0.6723282356946864, 8.570502995814186e-100), 'spearman': SpearmanrResult(correlation=0.662241729491309, pvalue=7.609464338417013e-96), 'nsamples': 750}, 'images': {'pearson': (0.7397543923642728, 7.783298021405552e-131), 'spearman': SpearmanrResult(correlation=0.7510805843816162, pvalue=5.142637663251379e-137), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6513608781756512, 'wmean': 0.6709138277031319}, 'spearman': {'mean': 0.6591882127280856, 'wmean': 0.6777855149007854}}}, 'STS16': {'answer-answer': {'pearson': (0.4982120024949094, 2.4330276134203667e-17), 'spearman': SpearmanrResult(correlation=0.5441820582299914, pvalue=5.556746146817693e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6864114414595429, 4.9786798474231247e-36), 'spearman': SpearmanrResult(correlation=0.6921242025445368, pvalue=7.741996188056624e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7380316174762088, 7.939696181934426e-41), 'spearman': SpearmanrResult(correlation=0.7473481361265623, pvalue=2.3244247663348613e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.7819096635651434, 1.404357755165717e-51), 'spearman': SpearmanrResult(correlation=0.8101819991071095, pvalue=4.6056443121687476e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.40619501310029227, 1.0461594410669871e-09), 'spearman': SpearmanrResult(correlation=0.40959040779741435, pvalue=7.358909584725629e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6221519476192194, 'wmean': 0.6263830398183114}, 'spearman': {'mean': 0.6406853607611228, 'wmean': 0.6456492778624883}}}, 'MR': {'devacc': 76.84, 'acc': 75.32, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.83, 'acc': 78.12, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.59, 'acc': 87.32, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.21, 'acc': 94.14, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.85, 'acc': 80.62, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.96, 'acc': 42.99, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.14, 'acc': 89.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.19, 'acc': 70.96, 'f1': 76.6, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.0, 'acc': 77.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8190763959733466, 'pearson': 0.8100282501465923, 'spearman': 0.7382063625674621, 'mse': 0.35022027133320544, 'yhat': array([4.10413563, 4.80193806, 1.5509379 , ..., 3.47174907, 4.52383013,        4.50798157]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7552060059242556, 'pearson': 0.7170741799038057, 'spearman': 0.7101229682603117, 'mse': 1.4009896542142546, 'yhat': array([1.66075507, 1.31662756, 2.14855225, ..., 4.05415517, 3.7431529 ,        3.66114583]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.94, 'acc': 69.33, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 92.8, 'acc': 93.58, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 72.0, 'acc': 71.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.78, 'acc': 36.71, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.7, 'acc': 70.37, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 82.63, 'acc': 81.7, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.48, 'acc': 88.29, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.45, 'acc': 83.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.3, 'acc': 83.81, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.63, 'acc': 61.28, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.71, 'acc': 59.54, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 04:36:52,285 : STS12 p=0.5525, STS12 s=0.5739, STS13 p=0.5765, STS13 s=0.5786, STS14 p=0.6019, STS14 s=0.5957, STS15 p=0.6709, STS15 s=0.6778, STS 16 p=0.6264, STS16 s=0.6456, STS B p=0.7171, STS B s=0.7101, STS B m=1.4010, SICK-R p=0.8100, SICK-R s=0.7382, SICK-P m=0.3502
2019-03-13 04:36:52,285 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 04:36:52,285 : 0.5525,0.5739,0.5765,0.5786,0.6019,0.5957,0.6709,0.6778,0.6264,0.6456,0.7171,0.7101,1.4010,0.8100,0.7382,0.3502
2019-03-13 04:36:52,285 : MR=75.32, CR=78.12, SUBJ=94.14, MPQA=87.32, SST-B=80.62, SST-F=42.99, TREC=89.80, SICK-E=77.98, SNLI=69.33, MRPC=70.96, MRPC f=76.60
2019-03-13 04:36:52,285 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 04:36:52,285 : 75.32,78.12,94.14,87.32,80.62,42.99,89.80,77.98,69.33,70.96,76.60
2019-03-13 04:36:52,285 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 04:36:52,285 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 04:36:52,285 : na,na,na,na,na,na,na,na,na,na
2019-03-13 04:36:52,285 : SentLen=93.58, WC=71.80, TreeDepth=36.71, TopConst=70.37, BShift=81.70, Tense=88.29, SubjNum=83.00, ObjNum=83.81, SOMO=61.28, CoordInv=59.54, average=73.01
2019-03-13 04:36:52,285 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 04:36:52,285 : 93.58,71.80,36.71,70.37,81.70,88.29,83.00,83.81,61.28,59.54,73.01
2019-03-13 04:36:52,285 : ********************************************************************************
2019-03-13 04:36:52,286 : ********************************************************************************
2019-03-13 04:36:52,286 : ********************************************************************************
2019-03-13 04:36:52,286 : layer 11
2019-03-13 04:36:52,286 : ********************************************************************************
2019-03-13 04:36:52,286 : ********************************************************************************
2019-03-13 04:36:52,286 : ********************************************************************************
2019-03-13 04:36:52,373 : ***** Transfer task : STS12 *****


2019-03-13 04:36:52,385 : loading BERT model bert-large-uncased
2019-03-13 04:36:52,385 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:36:52,402 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:36:52,402 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwklpdmws
2019-03-13 04:36:59,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:37:09,233 : MSRpar : pearson = 0.3647, spearman = 0.4115
2019-03-13 04:37:10,884 : MSRvid : pearson = 0.5440, spearman = 0.5486
2019-03-13 04:37:12,306 : SMTeuroparl : pearson = 0.5247, spearman = 0.6176
2019-03-13 04:37:15,017 : surprise.OnWN : pearson = 0.6367, spearman = 0.6583
2019-03-13 04:37:16,453 : surprise.SMTnews : pearson = 0.6099, spearman = 0.5295
2019-03-13 04:37:16,453 : ALL (weighted average) : Pearson = 0.5287,             Spearman = 0.5497
2019-03-13 04:37:16,453 : ALL (average) : Pearson = 0.5360,             Spearman = 0.5531

2019-03-13 04:37:16,453 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 04:37:16,461 : loading BERT model bert-large-uncased
2019-03-13 04:37:16,461 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:37:16,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:37:16,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp16lmusi3
2019-03-13 04:37:23,905 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:37:30,581 : FNWN : pearson = 0.4063, spearman = 0.3998
2019-03-13 04:37:32,483 : headlines : pearson = 0.6482, spearman = 0.6271
2019-03-13 04:37:33,956 : OnWN : pearson = 0.4513, spearman = 0.4825
2019-03-13 04:37:33,956 : ALL (weighted average) : Pearson = 0.5441,             Spearman = 0.5444
2019-03-13 04:37:33,956 : ALL (average) : Pearson = 0.5019,             Spearman = 0.5031

2019-03-13 04:37:33,956 : ***** Transfer task : STS14 *****


2019-03-13 04:37:33,973 : loading BERT model bert-large-uncased
2019-03-13 04:37:33,973 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:37:33,990 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:37:33,990 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1jid13d_
2019-03-13 04:37:41,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:37:48,212 : deft-forum : pearson = 0.2870, spearman = 0.3281
2019-03-13 04:37:49,856 : deft-news : pearson = 0.7305, spearman = 0.6956
2019-03-13 04:37:52,032 : headlines : pearson = 0.5992, spearman = 0.5530
2019-03-13 04:37:54,117 : images : pearson = 0.5436, spearman = 0.5409
2019-03-13 04:37:56,258 : OnWN : pearson = 0.6203, spearman = 0.6621
2019-03-13 04:37:59,133 : tweet-news : pearson = 0.6254, spearman = 0.5973
2019-03-13 04:37:59,133 : ALL (weighted average) : Pearson = 0.5706,             Spearman = 0.5657
2019-03-13 04:37:59,133 : ALL (average) : Pearson = 0.5676,             Spearman = 0.5628

2019-03-13 04:37:59,133 : ***** Transfer task : STS15 *****


2019-03-13 04:37:59,167 : loading BERT model bert-large-uncased
2019-03-13 04:37:59,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:37:59,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:37:59,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp529h8slf
2019-03-13 04:38:06,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:38:13,878 : answers-forums : pearson = 0.5076, spearman = 0.4946
2019-03-13 04:38:15,974 : answers-students : pearson = 0.6836, spearman = 0.7000
2019-03-13 04:38:18,029 : belief : pearson = 0.5984, spearman = 0.6389
2019-03-13 04:38:20,289 : headlines : pearson = 0.6535, spearman = 0.6430
2019-03-13 04:38:22,430 : images : pearson = 0.6984, spearman = 0.7123
2019-03-13 04:38:22,430 : ALL (weighted average) : Pearson = 0.6471,             Spearman = 0.6555
2019-03-13 04:38:22,430 : ALL (average) : Pearson = 0.6283,             Spearman = 0.6377

2019-03-13 04:38:22,430 : ***** Transfer task : STS16 *****


2019-03-13 04:38:22,500 : loading BERT model bert-large-uncased
2019-03-13 04:38:22,500 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:38:22,519 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:38:22,519 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyfujn2fh
2019-03-13 04:38:30,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:38:36,076 : answer-answer : pearson = 0.4617, spearman = 0.5011
2019-03-13 04:38:36,740 : headlines : pearson = 0.6799, spearman = 0.6816
2019-03-13 04:38:37,626 : plagiarism : pearson = 0.7217, spearman = 0.7309
2019-03-13 04:38:39,126 : postediting : pearson = 0.7684, spearman = 0.8070
2019-03-13 04:38:39,734 : question-question : pearson = 0.3021, spearman = 0.3230
2019-03-13 04:38:39,734 : ALL (weighted average) : Pearson = 0.5929,             Spearman = 0.6151
2019-03-13 04:38:39,734 : ALL (average) : Pearson = 0.5868,             Spearman = 0.6087

2019-03-13 04:38:39,734 : ***** Transfer task : MR *****


2019-03-13 04:38:39,753 : loading BERT model bert-large-uncased
2019-03-13 04:38:39,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:38:39,772 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:38:39,772 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphi8glk09
2019-03-13 04:38:47,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:38:52,505 : Generating sentence embeddings
2019-03-13 04:39:24,124 : Generated sentence embeddings
2019-03-13 04:39:24,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:39:33,479 : Best param found at split 1: l2reg = 0.01                 with score 76.7
2019-03-13 04:39:44,092 : Best param found at split 2: l2reg = 0.001                 with score 77.16
2019-03-13 04:39:54,135 : Best param found at split 3: l2reg = 0.0001                 with score 76.94
2019-03-13 04:40:04,528 : Best param found at split 4: l2reg = 0.001                 with score 77.05
2019-03-13 04:40:15,210 : Best param found at split 5: l2reg = 0.0001                 with score 76.91
2019-03-13 04:40:15,966 : Dev acc : 76.95 Test acc : 75.94

2019-03-13 04:40:15,967 : ***** Transfer task : CR *****


2019-03-13 04:40:15,975 : loading BERT model bert-large-uncased
2019-03-13 04:40:15,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:40:15,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:40:15,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplm7v1ueu
2019-03-13 04:40:23,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:40:28,831 : Generating sentence embeddings
2019-03-13 04:40:37,182 : Generated sentence embeddings
2019-03-13 04:40:37,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:40:40,972 : Best param found at split 1: l2reg = 0.001                 with score 80.13
2019-03-13 04:40:44,342 : Best param found at split 2: l2reg = 0.001                 with score 80.36
2019-03-13 04:40:47,510 : Best param found at split 3: l2reg = 1e-05                 with score 80.83
2019-03-13 04:40:50,313 : Best param found at split 4: l2reg = 0.001                 with score 80.9
2019-03-13 04:40:53,216 : Best param found at split 5: l2reg = 1e-05                 with score 80.24
2019-03-13 04:40:53,403 : Dev acc : 80.49 Test acc : 79.68

2019-03-13 04:40:53,404 : ***** Transfer task : MPQA *****


2019-03-13 04:40:53,410 : loading BERT model bert-large-uncased
2019-03-13 04:40:53,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:40:53,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:40:53,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpteip6fil
2019-03-13 04:41:00,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:41:06,162 : Generating sentence embeddings
2019-03-13 04:41:13,754 : Generated sentence embeddings
2019-03-13 04:41:13,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:41:24,199 : Best param found at split 1: l2reg = 1e-05                 with score 86.36
2019-03-13 04:41:35,073 : Best param found at split 2: l2reg = 0.001                 with score 86.48
2019-03-13 04:41:45,857 : Best param found at split 3: l2reg = 0.001                 with score 87.11
2019-03-13 04:41:55,885 : Best param found at split 4: l2reg = 1e-05                 with score 87.2
2019-03-13 04:42:06,596 : Best param found at split 5: l2reg = 0.001                 with score 86.53
2019-03-13 04:42:07,248 : Dev acc : 86.74 Test acc : 86.14

2019-03-13 04:42:07,249 : ***** Transfer task : SUBJ *****


2019-03-13 04:42:07,264 : loading BERT model bert-large-uncased
2019-03-13 04:42:07,264 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:42:07,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:42:07,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpau45vuy8
2019-03-13 04:42:14,771 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:42:20,027 : Generating sentence embeddings
2019-03-13 04:42:50,974 : Generated sentence embeddings
2019-03-13 04:42:50,974 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:42:59,520 : Best param found at split 1: l2reg = 1e-05                 with score 94.25
2019-03-13 04:43:08,513 : Best param found at split 2: l2reg = 0.0001                 with score 94.44
2019-03-13 04:43:19,126 : Best param found at split 3: l2reg = 0.001                 with score 93.86
2019-03-13 04:43:29,381 : Best param found at split 4: l2reg = 0.0001                 with score 94.38
2019-03-13 04:43:38,177 : Best param found at split 5: l2reg = 0.001                 with score 94.2
2019-03-13 04:43:38,480 : Dev acc : 94.23 Test acc : 94.02

2019-03-13 04:43:38,481 : ***** Transfer task : SST Binary classification *****


2019-03-13 04:43:38,573 : loading BERT model bert-large-uncased
2019-03-13 04:43:38,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:43:38,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:43:38,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl6kg7t_l
2019-03-13 04:43:46,119 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:43:51,473 : Computing embedding for train
2019-03-13 04:45:31,919 : Computed train embeddings
2019-03-13 04:45:31,919 : Computing embedding for dev
2019-03-13 04:45:34,110 : Computed dev embeddings
2019-03-13 04:45:34,110 : Computing embedding for test
2019-03-13 04:45:38,721 : Computed test embeddings
2019-03-13 04:45:38,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:45:56,185 : [('reg:1e-05', 81.65), ('reg:0.0001', 81.54), ('reg:0.001', 81.42), ('reg:0.01', 80.28)]
2019-03-13 04:45:56,185 : Validation : best param found is reg = 1e-05 with score             81.65
2019-03-13 04:45:56,185 : Evaluating...
2019-03-13 04:46:01,159 : 
Dev acc : 81.65 Test acc : 82.59 for             SST Binary classification

2019-03-13 04:46:01,159 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 04:46:01,209 : loading BERT model bert-large-uncased
2019-03-13 04:46:01,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:46:01,231 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:46:01,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt9c_leie
2019-03-13 04:46:08,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:46:13,955 : Computing embedding for train
2019-03-13 04:46:35,920 : Computed train embeddings
2019-03-13 04:46:35,920 : Computing embedding for dev
2019-03-13 04:46:38,791 : Computed dev embeddings
2019-03-13 04:46:38,791 : Computing embedding for test
2019-03-13 04:46:44,451 : Computed test embeddings
2019-03-13 04:46:44,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:46:46,670 : [('reg:1e-05', 40.05), ('reg:0.0001', 40.24), ('reg:0.001', 40.69), ('reg:0.01', 41.6)]
2019-03-13 04:46:46,670 : Validation : best param found is reg = 0.01 with score             41.6
2019-03-13 04:46:46,671 : Evaluating...
2019-03-13 04:46:47,136 : 
Dev acc : 41.6 Test acc : 43.44 for             SST Fine-Grained classification

2019-03-13 04:46:47,136 : ***** Transfer task : TREC *****


2019-03-13 04:46:47,149 : loading BERT model bert-large-uncased
2019-03-13 04:46:47,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:46:47,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:46:47,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzdriccjg
2019-03-13 04:46:54,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:47:07,503 : Computed train embeddings
2019-03-13 04:47:08,093 : Computed test embeddings
2019-03-13 04:47:08,094 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:47:13,782 : [('reg:1e-05', 81.64), ('reg:0.0001', 81.27), ('reg:0.001', 79.95), ('reg:0.01', 71.1)]
2019-03-13 04:47:13,782 : Cross-validation : best param found is reg = 1e-05             with score 81.64
2019-03-13 04:47:13,782 : Evaluating...
2019-03-13 04:47:14,148 : 
Dev acc : 81.64 Test acc : 91.0             for TREC

2019-03-13 04:47:14,149 : ***** Transfer task : MRPC *****


2019-03-13 04:47:14,170 : loading BERT model bert-large-uncased
2019-03-13 04:47:14,170 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:47:14,191 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:47:14,191 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxsxbd_fe
2019-03-13 04:47:21,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:47:26,939 : Computing embedding for train
2019-03-13 04:47:49,278 : Computed train embeddings
2019-03-13 04:47:49,279 : Computing embedding for test
2019-03-13 04:47:59,055 : Computed test embeddings
2019-03-13 04:47:59,076 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:48:03,504 : [('reg:1e-05', 73.18), ('reg:0.0001', 73.5), ('reg:0.001', 71.79), ('reg:0.01', 71.67)]
2019-03-13 04:48:03,504 : Cross-validation : best param found is reg = 0.0001             with score 73.5
2019-03-13 04:48:03,505 : Evaluating...
2019-03-13 04:48:03,717 : Dev acc : 73.5 Test acc 69.51; Test F1 74.64 for MRPC.

2019-03-13 04:48:03,718 : ***** Transfer task : SICK-Entailment*****


2019-03-13 04:48:03,779 : loading BERT model bert-large-uncased
2019-03-13 04:48:03,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:48:03,799 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:48:03,799 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfa9y2ucb
2019-03-13 04:48:11,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:48:16,765 : Computing embedding for train
2019-03-13 04:48:28,096 : Computed train embeddings
2019-03-13 04:48:28,096 : Computing embedding for dev
2019-03-13 04:48:29,642 : Computed dev embeddings
2019-03-13 04:48:29,642 : Computing embedding for test
2019-03-13 04:48:41,800 : Computed test embeddings
2019-03-13 04:48:41,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:48:43,238 : [('reg:1e-05', 79.8), ('reg:0.0001', 76.0), ('reg:0.001', 76.4), ('reg:0.01', 74.8)]
2019-03-13 04:48:43,238 : Validation : best param found is reg = 1e-05 with score             79.8
2019-03-13 04:48:43,238 : Evaluating...
2019-03-13 04:48:43,588 : 
Dev acc : 79.8 Test acc : 78.12 for                        SICK entailment

2019-03-13 04:48:43,588 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 04:48:43,615 : loading BERT model bert-large-uncased
2019-03-13 04:48:43,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:48:43,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:48:43,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoopb9ep4
2019-03-13 04:48:51,111 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:48:56,334 : Computing embedding for train
2019-03-13 04:49:07,695 : Computed train embeddings
2019-03-13 04:49:07,695 : Computing embedding for dev
2019-03-13 04:49:09,242 : Computed dev embeddings
2019-03-13 04:49:09,242 : Computing embedding for test
2019-03-13 04:49:21,422 : Computed test embeddings
2019-03-13 04:49:36,438 : Dev : Pearson 0.7992919411300246
2019-03-13 04:49:36,438 : Test : Pearson 0.8130749645419224 Spearman 0.7427452871671081 MSE 0.3470419727425767                        for SICK Relatedness

2019-03-13 04:49:36,439 : 

***** Transfer task : STSBenchmark*****


2019-03-13 04:49:36,478 : loading BERT model bert-large-uncased
2019-03-13 04:49:36,478 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:49:36,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:49:36,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn9ub76lc
2019-03-13 04:49:43,938 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:49:49,257 : Computing embedding for train
2019-03-13 04:50:07,879 : Computed train embeddings
2019-03-13 04:50:07,879 : Computing embedding for dev
2019-03-13 04:50:13,534 : Computed dev embeddings
2019-03-13 04:50:13,534 : Computing embedding for test
2019-03-13 04:50:18,161 : Computed test embeddings
2019-03-13 04:50:38,834 : Dev : Pearson 0.744241892017086
2019-03-13 04:50:38,834 : Test : Pearson 0.7138656990594764 Spearman 0.7067666234194238 MSE 1.437241022631547                        for SICK Relatedness

2019-03-13 04:50:38,835 : ***** Transfer task : SNLI Entailment*****


2019-03-13 04:50:43,926 : loading BERT model bert-large-uncased
2019-03-13 04:50:43,926 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:50:44,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:50:44,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplilgy605
2019-03-13 04:50:51,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:50:57,155 : PROGRESS (encoding): 0.00%
2019-03-13 04:53:43,708 : PROGRESS (encoding): 14.56%
2019-03-13 04:56:55,095 : PROGRESS (encoding): 29.12%
2019-03-13 05:00:05,950 : PROGRESS (encoding): 43.69%
2019-03-13 05:03:29,254 : PROGRESS (encoding): 58.25%
2019-03-13 05:07:15,423 : PROGRESS (encoding): 72.81%
2019-03-13 05:11:00,486 : PROGRESS (encoding): 87.37%
2019-03-13 05:15:03,986 : PROGRESS (encoding): 0.00%
2019-03-13 05:15:34,573 : PROGRESS (encoding): 0.00%
2019-03-13 05:16:04,011 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:16:29,061 : [('reg:1e-09', 66.52)]
2019-03-13 05:16:29,061 : Validation : best param found is reg = 1e-09 with score             66.52
2019-03-13 05:16:29,062 : Evaluating...
2019-03-13 05:16:55,224 : Dev acc : 66.52 Test acc : 66.35 for SNLI

2019-03-13 05:16:55,224 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 05:16:55,434 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 05:16:56,506 : loading BERT model bert-large-uncased
2019-03-13 05:16:56,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:16:56,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:16:56,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvm2ixq_8
2019-03-13 05:17:03,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:17:09,366 : Computing embeddings for train/dev/test
2019-03-13 05:20:41,474 : Computed embeddings
2019-03-13 05:20:41,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:21:11,763 : [('reg:1e-05', 92.36), ('reg:0.0001', 92.33), ('reg:0.001', 87.97), ('reg:0.01', 82.66)]
2019-03-13 05:21:11,763 : Validation : best param found is reg = 1e-05 with score             92.36
2019-03-13 05:21:11,763 : Evaluating...
2019-03-13 05:21:19,306 : 
Dev acc : 92.4 Test acc : 93.7 for LENGTH classification

2019-03-13 05:21:19,307 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 05:21:19,561 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 05:21:19,607 : loading BERT model bert-large-uncased
2019-03-13 05:21:19,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:21:19,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:21:19,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdtzyah7b
2019-03-13 05:21:27,074 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:21:32,237 : Computing embeddings for train/dev/test
2019-03-13 05:24:47,534 : Computed embeddings
2019-03-13 05:24:47,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:25:18,412 : [('reg:1e-05', 63.46), ('reg:0.0001', 22.16), ('reg:0.001', 2.02), ('reg:0.01', 0.97)]
2019-03-13 05:25:18,412 : Validation : best param found is reg = 1e-05 with score             63.46
2019-03-13 05:25:18,412 : Evaluating...
2019-03-13 05:25:27,029 : 
Dev acc : 63.5 Test acc : 63.2 for WORDCONTENT classification

2019-03-13 05:25:27,030 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 05:25:27,558 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 05:25:27,624 : loading BERT model bert-large-uncased
2019-03-13 05:25:27,624 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:25:27,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:25:27,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeqfenjwl
2019-03-13 05:25:35,099 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:25:40,287 : Computing embeddings for train/dev/test
2019-03-13 05:28:44,262 : Computed embeddings
2019-03-13 05:28:44,262 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:29:06,640 : [('reg:1e-05', 38.52), ('reg:0.0001', 37.78), ('reg:0.001', 35.01), ('reg:0.01', 28.81)]
2019-03-13 05:29:06,641 : Validation : best param found is reg = 1e-05 with score             38.52
2019-03-13 05:29:06,641 : Evaluating...
2019-03-13 05:29:11,301 : 
Dev acc : 38.5 Test acc : 38.9 for DEPTH classification

2019-03-13 05:29:11,302 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 05:29:11,675 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 05:29:11,739 : loading BERT model bert-large-uncased
2019-03-13 05:29:11,739 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:29:11,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:29:11,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6dg5mer2
2019-03-13 05:29:19,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:29:24,612 : Computing embeddings for train/dev/test
2019-03-13 05:32:14,751 : Computed embeddings
2019-03-13 05:32:14,751 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:32:36,857 : [('reg:1e-05', 75.62), ('reg:0.0001', 70.36), ('reg:0.001', 62.12), ('reg:0.01', 47.67)]
2019-03-13 05:32:36,858 : Validation : best param found is reg = 1e-05 with score             75.62
2019-03-13 05:32:36,858 : Evaluating...
2019-03-13 05:32:43,494 : 
Dev acc : 75.6 Test acc : 76.2 for TOPCONSTITUENTS classification

2019-03-13 05:32:43,496 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 05:32:43,898 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 05:32:43,966 : loading BERT model bert-large-uncased
2019-03-13 05:32:43,966 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:32:43,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:32:43,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplgvq45lb
2019-03-13 05:32:51,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:32:56,738 : Computing embeddings for train/dev/test
2019-03-13 05:36:01,740 : Computed embeddings
2019-03-13 05:36:01,740 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:36:35,058 : [('reg:1e-05', 87.0), ('reg:0.0001', 87.54), ('reg:0.001', 86.84), ('reg:0.01', 84.64)]
2019-03-13 05:36:35,058 : Validation : best param found is reg = 0.0001 with score             87.54
2019-03-13 05:36:35,059 : Evaluating...
2019-03-13 05:36:44,186 : 
Dev acc : 87.5 Test acc : 87.0 for BIGRAMSHIFT classification

2019-03-13 05:36:44,187 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 05:36:44,574 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 05:36:44,638 : loading BERT model bert-large-uncased
2019-03-13 05:36:44,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:36:44,669 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:36:44,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp06t8rv4k
2019-03-13 05:36:52,133 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:36:57,447 : Computing embeddings for train/dev/test
2019-03-13 05:39:58,073 : Computed embeddings
2019-03-13 05:39:58,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:40:24,686 : [('reg:1e-05', 88.93), ('reg:0.0001', 89.04), ('reg:0.001', 89.67), ('reg:0.01', 89.06)]
2019-03-13 05:40:24,686 : Validation : best param found is reg = 0.001 with score             89.67
2019-03-13 05:40:24,686 : Evaluating...
2019-03-13 05:40:31,169 : 
Dev acc : 89.7 Test acc : 88.3 for TENSE classification

2019-03-13 05:40:31,170 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 05:40:31,603 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 05:40:31,670 : loading BERT model bert-large-uncased
2019-03-13 05:40:31,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:40:31,785 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:40:31,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp31fi8ibs
2019-03-13 05:40:39,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:40:44,527 : Computing embeddings for train/dev/test
2019-03-13 05:43:55,608 : Computed embeddings
2019-03-13 05:43:55,608 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:44:19,753 : [('reg:1e-05', 84.94), ('reg:0.0001', 85.14), ('reg:0.001', 84.99), ('reg:0.01', 80.89)]
2019-03-13 05:44:19,753 : Validation : best param found is reg = 0.0001 with score             85.14
2019-03-13 05:44:19,754 : Evaluating...
2019-03-13 05:44:25,642 : 
Dev acc : 85.1 Test acc : 83.7 for SUBJNUMBER classification

2019-03-13 05:44:25,643 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 05:44:26,040 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 05:44:26,106 : loading BERT model bert-large-uncased
2019-03-13 05:44:26,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:44:26,221 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:44:26,221 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_0ao9axr
2019-03-13 05:44:33,685 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:44:38,952 : Computing embeddings for train/dev/test
2019-03-13 05:47:46,550 : Computed embeddings
2019-03-13 05:47:46,550 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:48:11,167 : [('reg:1e-05', 83.99), ('reg:0.0001', 83.96), ('reg:0.001', 83.83), ('reg:0.01', 82.83)]
2019-03-13 05:48:11,167 : Validation : best param found is reg = 1e-05 with score             83.99
2019-03-13 05:48:11,167 : Evaluating...
2019-03-13 05:48:15,769 : 
Dev acc : 84.0 Test acc : 84.3 for OBJNUMBER classification

2019-03-13 05:48:15,770 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 05:48:16,330 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 05:48:16,398 : loading BERT model bert-large-uncased
2019-03-13 05:48:16,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:48:16,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:48:16,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9cc6xtm3
2019-03-13 05:48:23,881 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:48:29,163 : Computing embeddings for train/dev/test
2019-03-13 05:52:06,926 : Computed embeddings
2019-03-13 05:52:06,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:52:32,978 : [('reg:1e-05', 61.98), ('reg:0.0001', 62.05), ('reg:0.001', 62.11), ('reg:0.01', 61.31)]
2019-03-13 05:52:32,978 : Validation : best param found is reg = 0.001 with score             62.11
2019-03-13 05:52:32,978 : Evaluating...
2019-03-13 05:52:39,567 : 
Dev acc : 62.1 Test acc : 62.6 for ODDMANOUT classification

2019-03-13 05:52:39,568 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 05:52:39,952 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 05:52:40,038 : loading BERT model bert-large-uncased
2019-03-13 05:52:40,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:52:40,071 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:52:40,071 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppzj7zvt0
2019-03-13 05:52:47,579 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:52:52,945 : Computing embeddings for train/dev/test
2019-03-13 05:56:28,734 : Computed embeddings
2019-03-13 05:56:28,734 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:56:54,968 : [('reg:1e-05', 55.13), ('reg:0.0001', 54.73), ('reg:0.001', 53.16), ('reg:0.01', 50.87)]
2019-03-13 05:56:54,968 : Validation : best param found is reg = 1e-05 with score             55.13
2019-03-13 05:56:54,968 : Evaluating...
2019-03-13 05:57:02,457 : 
Dev acc : 55.1 Test acc : 55.3 for COORDINATIONINVERSION classification

2019-03-13 05:57:02,459 : total results: {'STS12': {'MSRpar': {'pearson': (0.36466710392746904, 5.2692298394830055e-25), 'spearman': SpearmanrResult(correlation=0.4115224923955744, pvalue=5.127010149333757e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.544000639433232, 5.4282163315372785e-59), 'spearman': SpearmanrResult(correlation=0.5485852964288234, pvalue=3.7266308863213553e-60), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5247045362994543, 7.825331753847745e-34), 'spearman': SpearmanrResult(correlation=0.6175818622189201, pvalue=1.3047103775808436e-49), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.636683958861385, 1.6957045033177056e-86), 'spearman': SpearmanrResult(correlation=0.6583256285665899, pvalue=2.361557988843585e-94), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6098709480368556, 5.22500159754855e-42), 'spearman': SpearmanrResult(correlation=0.5294676535136087, pvalue=3.2805182915413716e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5359854373116792, 'wmean': 0.5286974475851736}, 'spearman': {'mean': 0.5530965866247033, 'wmean': 0.5497273911047795}}}, 'STS13': {'FNWN': {'pearson': (0.4063415200081926, 6.5732579871299455e-09), 'spearman': SpearmanrResult(correlation=0.39980145266162936, pvalue=1.202376911082002e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6482073050790218, 1.3366892067250093e-90), 'spearman': SpearmanrResult(correlation=0.6270840568097338, pvalue=3.2844389819810358e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.45129412115103185, 1.685273718610833e-29), 'spearman': SpearmanrResult(correlation=0.4824760939339276, pvalue=4.749654745050462e-34), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5019476487460821, 'wmean': 0.544086685371029}, 'spearman': {'mean': 0.5031205344684303, 'wmean': 0.5443630705715211}}}, 'STS14': {'deft-forum': {'pearson': (0.28696150402735754, 5.598420211485767e-10), 'spearman': SpearmanrResult(correlation=0.3281119820600311, pvalue=9.363769186996138e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.730478927763781, 2.784201575677772e-51), 'spearman': SpearmanrResult(correlation=0.6956154022967067, pvalue=1.047904012159019e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5991544785394352, 2.8490844515369137e-74), 'spearman': SpearmanrResult(correlation=0.5529694582146328, pvalue=2.7661731912855624e-61), 'nsamples': 750}, 'images': {'pearson': (0.5435875258627505, 6.896094361961706e-59), 'spearman': SpearmanrResult(correlation=0.5408921948694541, pvalue=3.260293031027184e-58), 'nsamples': 750}, 'OnWN': {'pearson': (0.6202565762991975, 6.096040911454017e-81), 'spearman': SpearmanrResult(correlation=0.6620525413336009, pvalue=8.993814525287409e-96), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6254200245675237, 1.1873436217136854e-82), 'spearman': SpearmanrResult(correlation=0.5972650857311541, pvalue=1.0662222144417562e-73), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5676431728433409, 'wmean': 0.5705574157581667}, 'spearman': {'mean': 0.5628177774175965, 'wmean': 0.5656585260607087}}}, 'STS15': {'answers-forums': {'pearson': (0.5076135919328009, 5.931424312242327e-26), 'spearman': SpearmanrResult(correlation=0.4945914602183817, pvalue=1.5697412866131531e-24), 'nsamples': 375}, 'answers-students': {'pearson': (0.6835789161608451, 2.187678204589036e-104), 'spearman': SpearmanrResult(correlation=0.6999515158499252, pvalue=1.8708412711404935e-111), 'nsamples': 750}, 'belief': {'pearson': (0.5984450357967414, 8.409758232698565e-38), 'spearman': SpearmanrResult(correlation=0.6388939784401191, pvalue=2.0956320315696645e-44), 'nsamples': 375}, 'headlines': {'pearson': (0.6534566818764063, 1.5742908059955637e-92), 'spearman': SpearmanrResult(correlation=0.6429898297359373, pvalue=1.0134575760666922e-88), 'nsamples': 750}, 'images': {'pearson': (0.6984170737004913, 9.012264694543137e-111), 'spearman': SpearmanrResult(correlation=0.712299161757074, pvalue=4.1151883572644464e-117), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.628302259893457, 'wmean': 0.6471204964006285}, 'spearman': {'mean': 0.6377451892002874, 'wmean': 0.6554958066680467}}}, 'STS16': {'answer-answer': {'pearson': (0.46166965239801494, 8.194788416537456e-15), 'spearman': SpearmanrResult(correlation=0.5010683317701623, pvalue=1.4982536638201753e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6799327382305191, 3.904129731712109e-35), 'spearman': SpearmanrResult(correlation=0.6816152784364485, pvalue=2.2986080228315927e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7217344786271207, 2.6972093378628057e-38), 'spearman': SpearmanrResult(correlation=0.7308978642351291, pvalue=1.0727719410221783e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7683614694044937, 8.336716181855854e-49), 'spearman': SpearmanrResult(correlation=0.8070292335218593, pvalue=2.747940793886509e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.30214240308002005, 8.740961147333827e-06), 'spearman': SpearmanrResult(correlation=0.32299203510176616, pvalue=1.8431278200006157e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5867681483480337, 'wmean': 0.5929125079183419}, 'spearman': {'mean': 0.6087205486130731, 'wmean': 0.6151100655058848}}}, 'MR': {'devacc': 76.95, 'acc': 75.94, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.49, 'acc': 79.68, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.74, 'acc': 86.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.23, 'acc': 94.02, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.65, 'acc': 82.59, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.6, 'acc': 43.44, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.64, 'acc': 91.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.5, 'acc': 69.51, 'f1': 74.64, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.8, 'acc': 78.12, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7992919411300246, 'pearson': 0.8130749645419224, 'spearman': 0.7427452871671081, 'mse': 0.3470419727425767, 'yhat': array([3.89142056, 4.63149529, 1.50102371, ..., 3.39773076, 4.43847772,        4.55902246]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.744241892017086, 'pearson': 0.7138656990594764, 'spearman': 0.7067666234194238, 'mse': 1.437241022631547, 'yhat': array([1.80820853, 1.39796896, 2.53625114, ..., 4.02517081, 3.71552526,        3.5279906 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.52, 'acc': 66.35, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 92.36, 'acc': 93.65, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 63.46, 'acc': 63.24, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.52, 'acc': 38.85, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.62, 'acc': 76.24, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.54, 'acc': 87.03, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.67, 'acc': 88.28, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.14, 'acc': 83.73, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.99, 'acc': 84.29, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.11, 'acc': 62.6, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.13, 'acc': 55.27, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 05:57:02,459 : STS12 p=0.5287, STS12 s=0.5497, STS13 p=0.5441, STS13 s=0.5444, STS14 p=0.5706, STS14 s=0.5657, STS15 p=0.6471, STS15 s=0.6555, STS 16 p=0.5929, STS16 s=0.6151, STS B p=0.7139, STS B s=0.7068, STS B m=1.4372, SICK-R p=0.8131, SICK-R s=0.7427, SICK-P m=0.3470
2019-03-13 05:57:02,459 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 05:57:02,459 : 0.5287,0.5497,0.5441,0.5444,0.5706,0.5657,0.6471,0.6555,0.5929,0.6151,0.7139,0.7068,1.4372,0.8131,0.7427,0.3470
2019-03-13 05:57:02,459 : MR=75.94, CR=79.68, SUBJ=94.02, MPQA=86.14, SST-B=82.59, SST-F=43.44, TREC=91.00, SICK-E=78.12, SNLI=66.35, MRPC=69.51, MRPC f=74.64
2019-03-13 05:57:02,459 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 05:57:02,459 : 75.94,79.68,94.02,86.14,82.59,43.44,91.00,78.12,66.35,69.51,74.64
2019-03-13 05:57:02,459 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 05:57:02,459 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 05:57:02,459 : na,na,na,na,na,na,na,na,na,na
2019-03-13 05:57:02,459 : SentLen=93.65, WC=63.24, TreeDepth=38.85, TopConst=76.24, BShift=87.03, Tense=88.28, SubjNum=83.73, ObjNum=84.29, SOMO=62.60, CoordInv=55.27, average=73.32
2019-03-13 05:57:02,459 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 05:57:02,460 : 93.65,63.24,38.85,76.24,87.03,88.28,83.73,84.29,62.60,55.27,73.32
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,460 : layer 12
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,460 : ********************************************************************************
2019-03-13 05:57:02,552 : ***** Transfer task : STS12 *****


2019-03-13 05:57:02,564 : loading BERT model bert-large-uncased
2019-03-13 05:57:02,564 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:57:02,581 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:57:02,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplj8zwsgv
2019-03-13 05:57:10,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:57:19,338 : MSRpar : pearson = 0.3588, spearman = 0.4017
2019-03-13 05:57:20,988 : MSRvid : pearson = 0.5010, spearman = 0.5093
2019-03-13 05:57:22,407 : SMTeuroparl : pearson = 0.5087, spearman = 0.6038
2019-03-13 05:57:25,118 : surprise.OnWN : pearson = 0.6105, spearman = 0.6404
2019-03-13 05:57:26,553 : surprise.SMTnews : pearson = 0.6256, spearman = 0.5276
2019-03-13 05:57:26,553 : ALL (weighted average) : Pearson = 0.5103,             Spearman = 0.5313
2019-03-13 05:57:26,553 : ALL (average) : Pearson = 0.5209,             Spearman = 0.5366

2019-03-13 05:57:26,553 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 05:57:26,561 : loading BERT model bert-large-uncased
2019-03-13 05:57:26,562 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:57:26,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:57:26,579 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpapkqi0pd
2019-03-13 05:57:34,009 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:57:40,707 : FNWN : pearson = 0.3612, spearman = 0.3579
2019-03-13 05:57:42,608 : headlines : pearson = 0.6297, spearman = 0.6091
2019-03-13 05:57:44,085 : OnWN : pearson = 0.4311, spearman = 0.4594
2019-03-13 05:57:44,085 : ALL (weighted average) : Pearson = 0.5216,             Spearman = 0.5215
2019-03-13 05:57:44,085 : ALL (average) : Pearson = 0.4740,             Spearman = 0.4755

2019-03-13 05:57:44,085 : ***** Transfer task : STS14 *****


2019-03-13 05:57:44,100 : loading BERT model bert-large-uncased
2019-03-13 05:57:44,100 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:57:44,118 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:57:44,118 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbf2uaa0e
2019-03-13 05:57:51,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:57:58,273 : deft-forum : pearson = 0.2702, spearman = 0.3229
2019-03-13 05:57:59,920 : deft-news : pearson = 0.7253, spearman = 0.6937
2019-03-13 05:58:02,101 : headlines : pearson = 0.5813, spearman = 0.5392
2019-03-13 05:58:04,192 : images : pearson = 0.4950, spearman = 0.5024
2019-03-13 05:58:06,332 : OnWN : pearson = 0.6010, spearman = 0.6430
2019-03-13 05:58:09,206 : tweet-news : pearson = 0.6097, spearman = 0.5850
2019-03-13 05:58:09,206 : ALL (weighted average) : Pearson = 0.5479,             Spearman = 0.5482
2019-03-13 05:58:09,206 : ALL (average) : Pearson = 0.5471,             Spearman = 0.5477

2019-03-13 05:58:09,207 : ***** Transfer task : STS15 *****


2019-03-13 05:58:09,255 : loading BERT model bert-large-uncased
2019-03-13 05:58:09,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:58:09,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:58:09,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3_7ljj6g
2019-03-13 05:58:16,821 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:58:23,868 : answers-forums : pearson = 0.4893, spearman = 0.4789
2019-03-13 05:58:25,966 : answers-students : pearson = 0.6814, spearman = 0.6956
2019-03-13 05:58:28,022 : belief : pearson = 0.5729, spearman = 0.6208
2019-03-13 05:58:30,284 : headlines : pearson = 0.6338, spearman = 0.6267
2019-03-13 05:58:32,429 : images : pearson = 0.6698, spearman = 0.6871
2019-03-13 05:58:32,430 : ALL (weighted average) : Pearson = 0.6290,             Spearman = 0.6398
2019-03-13 05:58:32,430 : ALL (average) : Pearson = 0.6094,             Spearman = 0.6218

2019-03-13 05:58:32,430 : ***** Transfer task : STS16 *****


2019-03-13 05:58:32,499 : loading BERT model bert-large-uncased
2019-03-13 05:58:32,499 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:58:32,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:58:32,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsf96y335
2019-03-13 05:58:39,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:58:46,144 : answer-answer : pearson = 0.4428, spearman = 0.4845
2019-03-13 05:58:46,807 : headlines : pearson = 0.6668, spearman = 0.6640
2019-03-13 05:58:47,693 : plagiarism : pearson = 0.7086, spearman = 0.7155
2019-03-13 05:58:49,192 : postediting : pearson = 0.7589, spearman = 0.8083
2019-03-13 05:58:49,802 : question-question : pearson = 0.2602, spearman = 0.2746
2019-03-13 05:58:49,802 : ALL (weighted average) : Pearson = 0.5742,             Spearman = 0.5966
2019-03-13 05:58:49,802 : ALL (average) : Pearson = 0.5674,             Spearman = 0.5893

2019-03-13 05:58:49,802 : ***** Transfer task : MR *****


2019-03-13 05:58:49,817 : loading BERT model bert-large-uncased
2019-03-13 05:58:49,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:58:49,837 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:58:49,837 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp90it4nr1
2019-03-13 05:58:57,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:59:02,583 : Generating sentence embeddings
2019-03-13 05:59:34,172 : Generated sentence embeddings
2019-03-13 05:59:34,173 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:59:42,713 : Best param found at split 1: l2reg = 0.001                 with score 77.44
2019-03-13 05:59:52,891 : Best param found at split 2: l2reg = 0.001                 with score 77.63
2019-03-13 06:00:01,750 : Best param found at split 3: l2reg = 0.001                 with score 77.8
2019-03-13 06:00:11,951 : Best param found at split 4: l2reg = 0.001                 with score 77.44
2019-03-13 06:00:22,809 : Best param found at split 5: l2reg = 0.01                 with score 76.83
2019-03-13 06:00:23,527 : Dev acc : 77.43 Test acc : 77.51

2019-03-13 06:00:23,528 : ***** Transfer task : CR *****


2019-03-13 06:00:23,536 : loading BERT model bert-large-uncased
2019-03-13 06:00:23,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:00:23,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:00:23,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk7ucquhv
2019-03-13 06:00:31,068 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:00:36,339 : Generating sentence embeddings
2019-03-13 06:00:44,695 : Generated sentence embeddings
2019-03-13 06:00:44,696 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:00:48,946 : Best param found at split 1: l2reg = 0.01                 with score 81.05
2019-03-13 06:00:52,622 : Best param found at split 2: l2reg = 0.001                 with score 80.69
2019-03-13 06:00:56,323 : Best param found at split 3: l2reg = 0.001                 with score 81.49
2019-03-13 06:01:00,032 : Best param found at split 4: l2reg = 0.001                 with score 80.4
2019-03-13 06:01:04,294 : Best param found at split 5: l2reg = 0.001                 with score 81.0
2019-03-13 06:01:04,483 : Dev acc : 80.93 Test acc : 79.89

2019-03-13 06:01:04,484 : ***** Transfer task : MPQA *****


2019-03-13 06:01:04,490 : loading BERT model bert-large-uncased
2019-03-13 06:01:04,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:01:04,540 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:01:04,540 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9cuxrl2n
2019-03-13 06:01:11,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:01:17,173 : Generating sentence embeddings
2019-03-13 06:01:24,783 : Generated sentence embeddings
2019-03-13 06:01:24,784 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:01:35,178 : Best param found at split 1: l2reg = 1e-05                 with score 86.94
2019-03-13 06:01:46,421 : Best param found at split 2: l2reg = 1e-05                 with score 86.42
2019-03-13 06:01:56,425 : Best param found at split 3: l2reg = 0.0001                 with score 87.18
2019-03-13 06:02:06,448 : Best param found at split 4: l2reg = 1e-05                 with score 86.38
2019-03-13 06:02:17,055 : Best param found at split 5: l2reg = 0.01                 with score 84.2
2019-03-13 06:02:17,679 : Dev acc : 86.22 Test acc : 86.35

2019-03-13 06:02:17,679 : ***** Transfer task : SUBJ *****


2019-03-13 06:02:17,696 : loading BERT model bert-large-uncased
2019-03-13 06:02:17,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:02:17,716 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:02:17,716 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_vgwqx0j
2019-03-13 06:02:25,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:02:30,501 : Generating sentence embeddings
2019-03-13 06:03:01,417 : Generated sentence embeddings
2019-03-13 06:03:01,418 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:03:10,664 : Best param found at split 1: l2reg = 0.001                 with score 94.74
2019-03-13 06:03:21,230 : Best param found at split 2: l2reg = 1e-05                 with score 94.68
2019-03-13 06:03:30,798 : Best param found at split 3: l2reg = 1e-05                 with score 94.2
2019-03-13 06:03:41,644 : Best param found at split 4: l2reg = 0.001                 with score 95.05
2019-03-13 06:03:50,296 : Best param found at split 5: l2reg = 0.001                 with score 94.41
2019-03-13 06:03:50,719 : Dev acc : 94.62 Test acc : 94.28

2019-03-13 06:03:50,720 : ***** Transfer task : SST Binary classification *****


2019-03-13 06:03:50,812 : loading BERT model bert-large-uncased
2019-03-13 06:03:50,812 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:03:50,888 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:03:50,888 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb7aagf2k
2019-03-13 06:03:58,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:04:03,708 : Computing embedding for train
2019-03-13 06:05:44,124 : Computed train embeddings
2019-03-13 06:05:44,124 : Computing embedding for dev
2019-03-13 06:05:46,314 : Computed dev embeddings
2019-03-13 06:05:46,314 : Computing embedding for test
2019-03-13 06:05:50,925 : Computed test embeddings
2019-03-13 06:05:50,925 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:06:04,390 : [('reg:1e-05', 82.57), ('reg:0.0001', 82.8), ('reg:0.001', 82.68), ('reg:0.01', 80.05)]
2019-03-13 06:06:04,390 : Validation : best param found is reg = 0.0001 with score             82.8
2019-03-13 06:06:04,390 : Evaluating...
2019-03-13 06:06:08,445 : 
Dev acc : 82.8 Test acc : 81.22 for             SST Binary classification

2019-03-13 06:06:08,446 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 06:06:08,500 : loading BERT model bert-large-uncased
2019-03-13 06:06:08,500 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:06:08,520 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:06:08,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwdzxpe5q
2019-03-13 06:06:15,979 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:06:21,212 : Computing embedding for train
2019-03-13 06:06:43,180 : Computed train embeddings
2019-03-13 06:06:43,180 : Computing embedding for dev
2019-03-13 06:06:46,053 : Computed dev embeddings
2019-03-13 06:06:46,053 : Computing embedding for test
2019-03-13 06:06:51,714 : Computed test embeddings
2019-03-13 06:06:51,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:06:54,234 : [('reg:1e-05', 41.24), ('reg:0.0001', 41.05), ('reg:0.001', 41.6), ('reg:0.01', 41.42)]
2019-03-13 06:06:54,234 : Validation : best param found is reg = 0.001 with score             41.6
2019-03-13 06:06:54,234 : Evaluating...
2019-03-13 06:06:54,873 : 
Dev acc : 41.6 Test acc : 42.08 for             SST Fine-Grained classification

2019-03-13 06:06:54,873 : ***** Transfer task : TREC *****


2019-03-13 06:06:54,887 : loading BERT model bert-large-uncased
2019-03-13 06:06:54,887 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:06:54,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:06:54,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoc_5ydpy
2019-03-13 06:07:02,360 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:07:15,170 : Computed train embeddings
2019-03-13 06:07:15,760 : Computed test embeddings
2019-03-13 06:07:15,761 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:07:20,769 : [('reg:1e-05', 83.0), ('reg:0.0001', 82.85), ('reg:0.001', 81.23), ('reg:0.01', 73.74)]
2019-03-13 06:07:20,769 : Cross-validation : best param found is reg = 1e-05             with score 83.0
2019-03-13 06:07:20,769 : Evaluating...
2019-03-13 06:07:21,142 : 
Dev acc : 83.0 Test acc : 91.0             for TREC

2019-03-13 06:07:21,142 : ***** Transfer task : MRPC *****


2019-03-13 06:07:21,162 : loading BERT model bert-large-uncased
2019-03-13 06:07:21,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:07:21,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:07:21,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi6ikwh8p
2019-03-13 06:07:28,622 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:07:33,873 : Computing embedding for train
2019-03-13 06:07:56,170 : Computed train embeddings
2019-03-13 06:07:56,171 : Computing embedding for test
2019-03-13 06:08:05,944 : Computed test embeddings
2019-03-13 06:08:05,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:08:11,416 : [('reg:1e-05', 74.04), ('reg:0.0001', 74.02), ('reg:0.001', 73.45), ('reg:0.01', 71.54)]
2019-03-13 06:08:11,416 : Cross-validation : best param found is reg = 1e-05             with score 74.04
2019-03-13 06:08:11,416 : Evaluating...
2019-03-13 06:08:11,726 : Dev acc : 74.04 Test acc 74.9; Test F1 82.0 for MRPC.

2019-03-13 06:08:11,726 : ***** Transfer task : SICK-Entailment*****


2019-03-13 06:08:11,790 : loading BERT model bert-large-uncased
2019-03-13 06:08:11,791 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:08:11,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:08:11,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp42t8vdw4
2019-03-13 06:08:19,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:08:24,474 : Computing embedding for train
2019-03-13 06:08:35,784 : Computed train embeddings
2019-03-13 06:08:35,784 : Computing embedding for dev
2019-03-13 06:08:37,326 : Computed dev embeddings
2019-03-13 06:08:37,326 : Computing embedding for test
2019-03-13 06:08:49,464 : Computed test embeddings
2019-03-13 06:08:49,500 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:08:50,993 : [('reg:1e-05', 78.2), ('reg:0.0001', 78.2), ('reg:0.001', 77.2), ('reg:0.01', 74.0)]
2019-03-13 06:08:50,993 : Validation : best param found is reg = 1e-05 with score             78.2
2019-03-13 06:08:50,994 : Evaluating...
2019-03-13 06:08:51,374 : 
Dev acc : 78.2 Test acc : 77.59 for                        SICK entailment

2019-03-13 06:08:51,374 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 06:08:51,401 : loading BERT model bert-large-uncased
2019-03-13 06:08:51,401 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:08:51,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:08:51,459 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8h8gmzvj
2019-03-13 06:08:58,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:04,357 : Computing embedding for train
2019-03-13 06:09:15,684 : Computed train embeddings
2019-03-13 06:09:15,684 : Computing embedding for dev
2019-03-13 06:09:17,229 : Computed dev embeddings
2019-03-13 06:09:17,229 : Computing embedding for test
2019-03-13 06:09:29,391 : Computed test embeddings
2019-03-13 06:09:46,531 : Dev : Pearson 0.7980966118478369
2019-03-13 06:09:46,531 : Test : Pearson 0.8051427439946067 Spearman 0.7348797485777515 MSE 0.3611494285957364                        for SICK Relatedness

2019-03-13 06:09:46,532 : 

***** Transfer task : STSBenchmark*****


2019-03-13 06:09:46,570 : loading BERT model bert-large-uncased
2019-03-13 06:09:46,571 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:09:46,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:09:46,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp05d_qm2g
2019-03-13 06:09:54,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:59,363 : Computing embedding for train
2019-03-13 06:10:17,988 : Computed train embeddings
2019-03-13 06:10:17,988 : Computing embedding for dev
2019-03-13 06:10:23,634 : Computed dev embeddings
2019-03-13 06:10:23,634 : Computing embedding for test
2019-03-13 06:10:28,244 : Computed test embeddings
2019-03-13 06:10:41,554 : Dev : Pearson 0.7373039359686192
2019-03-13 06:10:41,554 : Test : Pearson 0.6943437666469298 Spearman 0.6886272379751508 MSE 1.44120065079097                        for SICK Relatedness

2019-03-13 06:10:41,554 : ***** Transfer task : SNLI Entailment*****


2019-03-13 06:10:46,697 : loading BERT model bert-large-uncased
2019-03-13 06:10:46,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:10:46,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:10:46,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjd85w2bf
2019-03-13 06:10:54,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:10:59,948 : PROGRESS (encoding): 0.00%
2019-03-13 06:13:46,390 : PROGRESS (encoding): 14.56%
2019-03-13 06:16:56,827 : PROGRESS (encoding): 29.12%
2019-03-13 06:20:07,319 : PROGRESS (encoding): 43.69%
2019-03-13 06:23:30,016 : PROGRESS (encoding): 58.25%
2019-03-13 06:27:15,622 : PROGRESS (encoding): 72.81%
2019-03-13 06:31:00,118 : PROGRESS (encoding): 87.37%
2019-03-13 06:35:03,117 : PROGRESS (encoding): 0.00%
2019-03-13 06:35:33,757 : PROGRESS (encoding): 0.00%
2019-03-13 06:36:03,192 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:36:29,332 : [('reg:1e-09', 69.61)]
2019-03-13 06:36:29,332 : Validation : best param found is reg = 1e-09 with score             69.61
2019-03-13 06:36:29,333 : Evaluating...
2019-03-13 06:36:56,049 : Dev acc : 69.61 Test acc : 69.28 for SNLI

2019-03-13 06:36:56,049 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 06:36:56,254 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 06:36:57,281 : loading BERT model bert-large-uncased
2019-03-13 06:36:57,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:36:57,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:36:57,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnjfmbm8z
2019-03-13 06:37:04,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:37:09,965 : Computing embeddings for train/dev/test
2019-03-13 06:40:41,758 : Computed embeddings
2019-03-13 06:40:41,758 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:41:10,645 : [('reg:1e-05', 92.32), ('reg:0.0001', 91.72), ('reg:0.001', 87.47), ('reg:0.01', 81.61)]
2019-03-13 06:41:10,645 : Validation : best param found is reg = 1e-05 with score             92.32
2019-03-13 06:41:10,645 : Evaluating...
2019-03-13 06:41:17,858 : 
Dev acc : 92.3 Test acc : 93.3 for LENGTH classification

2019-03-13 06:41:17,859 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 06:41:18,235 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 06:41:18,280 : loading BERT model bert-large-uncased
2019-03-13 06:41:18,280 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:41:18,310 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:41:18,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyel4se1f
2019-03-13 06:41:25,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:41:30,992 : Computing embeddings for train/dev/test
2019-03-13 06:44:46,291 : Computed embeddings
2019-03-13 06:44:46,291 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:45:15,897 : [('reg:1e-05', 60.45), ('reg:0.0001', 21.33), ('reg:0.001', 1.79), ('reg:0.01', 0.88)]
2019-03-13 06:45:15,897 : Validation : best param found is reg = 1e-05 with score             60.45
2019-03-13 06:45:15,897 : Evaluating...
2019-03-13 06:45:22,177 : 
Dev acc : 60.5 Test acc : 60.5 for WORDCONTENT classification

2019-03-13 06:45:22,178 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 06:45:22,559 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 06:45:22,626 : loading BERT model bert-large-uncased
2019-03-13 06:45:22,627 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:45:22,652 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:45:22,652 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkh8mvugl
2019-03-13 06:45:30,078 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:45:35,373 : Computing embeddings for train/dev/test
2019-03-13 06:48:39,206 : Computed embeddings
2019-03-13 06:48:39,206 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:48:58,283 : [('reg:1e-05', 38.38), ('reg:0.0001', 38.17), ('reg:0.001', 35.76), ('reg:0.01', 29.99)]
2019-03-13 06:48:58,283 : Validation : best param found is reg = 1e-05 with score             38.38
2019-03-13 06:48:58,283 : Evaluating...
2019-03-13 06:49:03,687 : 
Dev acc : 38.4 Test acc : 39.2 for DEPTH classification

2019-03-13 06:49:03,688 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 06:49:04,081 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 06:49:04,146 : loading BERT model bert-large-uncased
2019-03-13 06:49:04,147 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:49:04,263 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:49:04,263 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpavp_e140
2019-03-13 06:49:11,749 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:49:16,933 : Computing embeddings for train/dev/test
2019-03-13 06:52:07,358 : Computed embeddings
2019-03-13 06:52:07,358 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:52:37,994 : [('reg:1e-05', 76.97), ('reg:0.0001', 73.94), ('reg:0.001', 65.06), ('reg:0.01', 52.66)]
2019-03-13 06:52:37,994 : Validation : best param found is reg = 1e-05 with score             76.97
2019-03-13 06:52:37,994 : Evaluating...
2019-03-13 06:52:46,833 : 
Dev acc : 77.0 Test acc : 77.3 for TOPCONSTITUENTS classification

2019-03-13 06:52:46,834 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 06:52:47,172 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 06:52:47,238 : loading BERT model bert-large-uncased
2019-03-13 06:52:47,238 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:52:47,355 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:52:47,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1a0mf373
2019-03-13 06:52:54,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:53:00,108 : Computing embeddings for train/dev/test
2019-03-13 06:56:04,655 : Computed embeddings
2019-03-13 06:56:04,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:56:42,039 : [('reg:1e-05', 90.48), ('reg:0.0001', 90.6), ('reg:0.001', 89.61), ('reg:0.01', 88.44)]
2019-03-13 06:56:42,039 : Validation : best param found is reg = 0.0001 with score             90.6
2019-03-13 06:56:42,040 : Evaluating...
2019-03-13 06:56:51,685 : 
Dev acc : 90.6 Test acc : 89.9 for BIGRAMSHIFT classification

2019-03-13 06:56:51,686 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 06:56:52,240 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 06:56:52,304 : loading BERT model bert-large-uncased
2019-03-13 06:56:52,304 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:56:52,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:56:52,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwy0vqv29
2019-03-13 06:56:59,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:57:05,113 : Computing embeddings for train/dev/test
2019-03-13 07:00:05,663 : Computed embeddings
2019-03-13 07:00:05,663 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:00:30,282 : [('reg:1e-05', 89.2), ('reg:0.0001', 89.22), ('reg:0.001', 89.49), ('reg:0.01', 89.19)]
2019-03-13 07:00:30,283 : Validation : best param found is reg = 0.001 with score             89.49
2019-03-13 07:00:30,283 : Evaluating...
2019-03-13 07:00:36,687 : 
Dev acc : 89.5 Test acc : 87.9 for TENSE classification

2019-03-13 07:00:36,688 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 07:00:37,107 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 07:00:37,170 : loading BERT model bert-large-uncased
2019-03-13 07:00:37,170 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:00:37,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:00:37,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmasppngp
2019-03-13 07:00:44,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:00:49,892 : Computing embeddings for train/dev/test
2019-03-13 07:04:01,447 : Computed embeddings
2019-03-13 07:04:01,447 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:04:31,414 : [('reg:1e-05', 86.21), ('reg:0.0001', 86.49), ('reg:0.001', 85.25), ('reg:0.01', 82.69)]
2019-03-13 07:04:31,415 : Validation : best param found is reg = 0.0001 with score             86.49
2019-03-13 07:04:31,415 : Evaluating...
2019-03-13 07:04:40,986 : 
Dev acc : 86.5 Test acc : 85.7 for SUBJNUMBER classification

2019-03-13 07:04:40,987 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 07:04:41,389 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 07:04:41,455 : loading BERT model bert-large-uncased
2019-03-13 07:04:41,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:04:41,570 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:04:41,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgl8moy8h
2019-03-13 07:04:49,089 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:04:54,361 : Computing embeddings for train/dev/test
2019-03-13 07:08:02,020 : Computed embeddings
2019-03-13 07:08:02,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:08:25,682 : [('reg:1e-05', 83.83), ('reg:0.0001', 83.89), ('reg:0.001', 83.91), ('reg:0.01', 82.68)]
2019-03-13 07:08:25,682 : Validation : best param found is reg = 0.001 with score             83.91
2019-03-13 07:08:25,682 : Evaluating...
2019-03-13 07:08:32,172 : 
Dev acc : 83.9 Test acc : 85.0 for OBJNUMBER classification

2019-03-13 07:08:32,173 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 07:08:32,746 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 07:08:32,813 : loading BERT model bert-large-uncased
2019-03-13 07:08:32,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:08:32,839 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:08:32,839 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpitfop775
2019-03-13 07:08:40,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:08:45,493 : Computing embeddings for train/dev/test
2019-03-13 07:12:23,238 : Computed embeddings
2019-03-13 07:12:23,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:12:50,475 : [('reg:1e-05', 63.98), ('reg:0.0001', 63.91), ('reg:0.001', 64.19), ('reg:0.01', 63.04)]
2019-03-13 07:12:50,475 : Validation : best param found is reg = 0.001 with score             64.19
2019-03-13 07:12:50,475 : Evaluating...
2019-03-13 07:12:56,956 : 
Dev acc : 64.2 Test acc : 65.4 for ODDMANOUT classification

2019-03-13 07:12:56,957 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 07:12:57,343 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 07:12:57,419 : loading BERT model bert-large-uncased
2019-03-13 07:12:57,419 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:12:57,543 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:12:57,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8sq_e8f0
2019-03-13 07:13:04,950 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:13:10,195 : Computing embeddings for train/dev/test
2019-03-13 07:16:46,138 : Computed embeddings
2019-03-13 07:16:46,138 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:17:14,174 : [('reg:1e-05', 63.14), ('reg:0.0001', 63.13), ('reg:0.001', 61.62), ('reg:0.01', 55.98)]
2019-03-13 07:17:14,174 : Validation : best param found is reg = 1e-05 with score             63.14
2019-03-13 07:17:14,174 : Evaluating...
2019-03-13 07:17:19,511 : 
Dev acc : 63.1 Test acc : 62.2 for COORDINATIONINVERSION classification

2019-03-13 07:17:19,513 : total results: {'STS12': {'MSRpar': {'pearson': (0.35884214085425364, 3.282574623481407e-24), 'spearman': SpearmanrResult(correlation=0.4016507248896717, pvalue=1.9160490212764448e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5010024511751742, 6.587846779405341e-49), 'spearman': SpearmanrResult(correlation=0.5093391119792559, pvalue=9.434767675851466e-51), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5087426027930185, 1.3812432988886133e-31), 'spearman': SpearmanrResult(correlation=0.6038321016498434, pvalue=6.073227575684785e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6105005733966449, 8.533182906882817e-78), 'spearman': SpearmanrResult(correlation=0.640436585626689, pvalue=8.17349138300863e-88), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6256216535226699, 9.858874683549609e-45), 'spearman': SpearmanrResult(correlation=0.5275635584408699, pvalue=5.7311292287103434e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5209418843483522, 'wmean': 0.5102621520293099}, 'spearman': {'mean': 0.5365644165172659, 'wmean': 0.5312826934835577}}}, 'STS13': {'FNWN': {'pearson': (0.3611760021865095, 3.293683542861391e-07), 'spearman': SpearmanrResult(correlation=0.35792444016487235, pvalue=4.2694960606662565e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6296877172402486, 4.328730459659361e-84), 'spearman': SpearmanrResult(correlation=0.6090985995418614, pvalue=2.367208751271978e-77), 'nsamples': 750}, 'OnWN': {'pearson': (0.43110506026363127, 8.546375676547773e-27), 'spearman': SpearmanrResult(correlation=0.45943588756933196, pvalue=1.2119278646620532e-30), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.47398959323012985, 'wmean': 0.5215853274342226}, 'spearman': {'mean': 0.4754863090920219, 'wmean': 0.5214768011826347}}}, 'STS14': {'deft-forum': {'pearson': (0.27016513609251813, 5.753423942208414e-09), 'spearman': SpearmanrResult(correlation=0.3228720380379014, pvalue=2.235320642022666e-12), 'nsamples': 450}, 'deft-news': {'pearson': (0.7252599802088199, 3.114393558505212e-50), 'spearman': SpearmanrResult(correlation=0.6937282058493252, pvalue=2.2355746407050145e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5812644066128261, 5.401801730100467e-69), 'spearman': SpearmanrResult(correlation=0.5392199200137562, pvalue=8.48734238429534e-58), 'nsamples': 750}, 'images': {'pearson': (0.4950457909905744, 1.2748544032080924e-47), 'spearman': SpearmanrResult(correlation=0.5024316209063048, pvalue=3.2079638071766546e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.6010058368193956, 7.751477825998976e-75), 'spearman': SpearmanrResult(correlation=0.6429787786599409, pvalue=1.0226999157977657e-88), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6097420984793622, 1.4829261206094034e-77), 'spearman': SpearmanrResult(correlation=0.5849805500291422, pvalue=4.6041513929528845e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.547080541533916, 'wmean': 0.5478522413282394}, 'spearman': {'mean': 0.547701852249395, 'wmean': 0.548165074954323}}}, 'STS15': {'answers-forums': {'pearson': (0.48926994490320724, 5.752855674164224e-24), 'spearman': SpearmanrResult(correlation=0.47886942326265736, pvalue=6.826753194387311e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.6814224184863692, 1.7234032122613842e-103), 'spearman': SpearmanrResult(correlation=0.6956357430911603, pvalue=1.5186238874831363e-109), 'nsamples': 750}, 'belief': {'pearson': (0.5728559645616091, 4.365308348363129e-34), 'spearman': SpearmanrResult(correlation=0.6208016267544892, pvalue=2.478730885757082e-41), 'nsamples': 375}, 'headlines': {'pearson': (0.6338167943334339, 1.672196373341968e-85), 'spearman': SpearmanrResult(correlation=0.6267014653247901, pvalue=4.416539939770165e-83), 'nsamples': 750}, 'images': {'pearson': (0.6697532675460871, 9.032151551557992e-99), 'spearman': SpearmanrResult(correlation=0.6871273511197948, pvalue=7.046052139484801e-106), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6094236779661413, 'wmean': 0.6290138587745746}, 'spearman': {'mean': 0.6218271219105784, 'wmean': 0.6398250211360796}}}, 'STS16': {'answer-answer': {'pearson': (0.4427948994507156, 1.275075676852215e-13), 'spearman': SpearmanrResult(correlation=0.4845035979288217, pvalue=2.3430066943206847e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.6667795880390112, 2.1793346051531924e-33), 'spearman': SpearmanrResult(correlation=0.6639628667045537, pvalue=5.022148661105496e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7085836296256812, 2.2149990753214228e-36), 'spearman': SpearmanrResult(correlation=0.715453565847107, pvalue=2.2845124015661806e-37), 'nsamples': 230}, 'postediting': {'pearson': (0.7588843423771167, 5.637588187758664e-47), 'spearman': SpearmanrResult(correlation=0.808265875032311, pvalue=1.3691653427944274e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.26017757828132576, 0.00014208208788829189), 'spearman': SpearmanrResult(correlation=0.274552400840072, pvalue=5.747776324270504e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.56744400755477, 'wmean': 0.5742134486483269}, 'spearman': {'mean': 0.5893476612705731, 'wmean': 0.5965795220165667}}}, 'MR': {'devacc': 77.43, 'acc': 77.51, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.93, 'acc': 79.89, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.22, 'acc': 86.35, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.62, 'acc': 94.28, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.8, 'acc': 81.22, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.6, 'acc': 42.08, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.0, 'acc': 91.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.04, 'acc': 74.9, 'f1': 82.0, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 77.59, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7980966118478369, 'pearson': 0.8051427439946067, 'spearman': 0.7348797485777515, 'mse': 0.3611494285957364, 'yhat': array([3.52346926, 4.66724058, 1.29062617, ..., 3.12854898, 4.49971698,        4.48248503]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7373039359686192, 'pearson': 0.6943437666469298, 'spearman': 0.6886272379751508, 'mse': 1.44120065079097, 'yhat': array([1.76151583, 1.30872268, 2.50648415, ..., 3.90347162, 3.78099641,        3.67755009]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 69.61, 'acc': 69.28, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 92.32, 'acc': 93.29, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 60.45, 'acc': 60.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.38, 'acc': 39.17, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.97, 'acc': 77.34, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.6, 'acc': 89.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.49, 'acc': 87.94, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.49, 'acc': 85.74, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.91, 'acc': 84.99, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.19, 'acc': 65.4, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 63.14, 'acc': 62.25, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 07:17:19,513 : STS12 p=0.5103, STS12 s=0.5313, STS13 p=0.5216, STS13 s=0.5215, STS14 p=0.5479, STS14 s=0.5482, STS15 p=0.6290, STS15 s=0.6398, STS 16 p=0.5742, STS16 s=0.5966, STS B p=0.6943, STS B s=0.6886, STS B m=1.4412, SICK-R p=0.8051, SICK-R s=0.7349, SICK-P m=0.3611
2019-03-13 07:17:19,513 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 07:17:19,513 : 0.5103,0.5313,0.5216,0.5215,0.5479,0.5482,0.6290,0.6398,0.5742,0.5966,0.6943,0.6886,1.4412,0.8051,0.7349,0.3611
2019-03-13 07:17:19,513 : MR=77.51, CR=79.89, SUBJ=94.28, MPQA=86.35, SST-B=81.22, SST-F=42.08, TREC=91.00, SICK-E=77.59, SNLI=69.28, MRPC=74.90, MRPC f=82.00
2019-03-13 07:17:19,513 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 07:17:19,513 : 77.51,79.89,94.28,86.35,81.22,42.08,91.00,77.59,69.28,74.90,82.00
2019-03-13 07:17:19,513 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 07:17:19,513 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 07:17:19,513 : na,na,na,na,na,na,na,na,na,na
2019-03-13 07:17:19,513 : SentLen=93.29, WC=60.49, TreeDepth=39.17, TopConst=77.34, BShift=89.89, Tense=87.94, SubjNum=85.74, ObjNum=84.99, SOMO=65.40, CoordInv=62.25, average=74.65
2019-03-13 07:17:19,513 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 07:17:19,513 : 93.29,60.49,39.17,77.34,89.89,87.94,85.74,84.99,65.40,62.25,74.65
2019-03-13 07:17:19,513 : ********************************************************************************
2019-03-13 07:17:19,513 : ********************************************************************************
2019-03-13 07:17:19,513 : ********************************************************************************
2019-03-13 07:17:19,513 : layer 13
2019-03-13 07:17:19,514 : ********************************************************************************
2019-03-13 07:17:19,514 : ********************************************************************************
2019-03-13 07:17:19,514 : ********************************************************************************
2019-03-13 07:17:19,603 : ***** Transfer task : STS12 *****


2019-03-13 07:17:19,616 : loading BERT model bert-large-uncased
2019-03-13 07:17:19,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:17:19,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:17:19,633 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpilqgma9x
2019-03-13 07:17:27,055 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:17:36,338 : MSRpar : pearson = 0.3448, spearman = 0.3844
2019-03-13 07:17:37,991 : MSRvid : pearson = 0.4741, spearman = 0.4881
2019-03-13 07:17:39,415 : SMTeuroparl : pearson = 0.5038, spearman = 0.5946
2019-03-13 07:17:42,127 : surprise.OnWN : pearson = 0.5992, spearman = 0.6302
2019-03-13 07:17:43,561 : surprise.SMTnews : pearson = 0.6618, spearman = 0.5474
2019-03-13 07:17:43,561 : ALL (weighted average) : Pearson = 0.5016,             Spearman = 0.5207
2019-03-13 07:17:43,562 : ALL (average) : Pearson = 0.5168,             Spearman = 0.5289

2019-03-13 07:17:43,562 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 07:17:43,569 : loading BERT model bert-large-uncased
2019-03-13 07:17:43,570 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:17:43,587 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:17:43,587 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphya3o0mt
2019-03-13 07:17:51,049 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:17:57,569 : FNWN : pearson = 0.3176, spearman = 0.3160
2019-03-13 07:17:59,473 : headlines : pearson = 0.6181, spearman = 0.5988
2019-03-13 07:18:00,952 : OnWN : pearson = 0.4156, spearman = 0.4455
2019-03-13 07:18:00,952 : ALL (weighted average) : Pearson = 0.5045,             Spearman = 0.5058
2019-03-13 07:18:00,952 : ALL (average) : Pearson = 0.4504,             Spearman = 0.4534

2019-03-13 07:18:00,952 : ***** Transfer task : STS14 *****


2019-03-13 07:18:00,969 : loading BERT model bert-large-uncased
2019-03-13 07:18:00,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:18:00,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:18:00,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8m7031cq
2019-03-13 07:18:08,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:18:15,183 : deft-forum : pearson = 0.2602, spearman = 0.3048
2019-03-13 07:18:16,832 : deft-news : pearson = 0.7202, spearman = 0.6879
2019-03-13 07:18:19,015 : headlines : pearson = 0.5617, spearman = 0.5195
2019-03-13 07:18:21,105 : images : pearson = 0.4781, spearman = 0.4886
2019-03-13 07:18:23,246 : OnWN : pearson = 0.5910, spearman = 0.6317
2019-03-13 07:18:26,121 : tweet-news : pearson = 0.6191, spearman = 0.5862
2019-03-13 07:18:26,122 : ALL (weighted average) : Pearson = 0.5388,             Spearman = 0.5368
2019-03-13 07:18:26,122 : ALL (average) : Pearson = 0.5384,             Spearman = 0.5364

2019-03-13 07:18:26,122 : ***** Transfer task : STS15 *****


2019-03-13 07:18:26,156 : loading BERT model bert-large-uncased
2019-03-13 07:18:26,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:18:26,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:18:26,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu0eps368
2019-03-13 07:18:33,665 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:18:40,920 : answers-forums : pearson = 0.5075, spearman = 0.4968
2019-03-13 07:18:43,014 : answers-students : pearson = 0.6864, spearman = 0.6984
2019-03-13 07:18:45,071 : belief : pearson = 0.5775, spearman = 0.6169
2019-03-13 07:18:47,331 : headlines : pearson = 0.6243, spearman = 0.6178
2019-03-13 07:18:49,471 : images : pearson = 0.6645, spearman = 0.6826
2019-03-13 07:18:49,471 : ALL (weighted average) : Pearson = 0.6294,             Spearman = 0.6389
2019-03-13 07:18:49,471 : ALL (average) : Pearson = 0.6120,             Spearman = 0.6225

2019-03-13 07:18:49,471 : ***** Transfer task : STS16 *****


2019-03-13 07:18:49,541 : loading BERT model bert-large-uncased
2019-03-13 07:18:49,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:18:49,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:18:49,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5oroih8c
2019-03-13 07:18:56,992 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:19:03,197 : answer-answer : pearson = 0.4392, spearman = 0.4761
2019-03-13 07:19:03,860 : headlines : pearson = 0.6574, spearman = 0.6558
2019-03-13 07:19:04,747 : plagiarism : pearson = 0.7074, spearman = 0.7171
2019-03-13 07:19:06,248 : postediting : pearson = 0.7757, spearman = 0.8260
2019-03-13 07:19:06,856 : question-question : pearson = 0.2290, spearman = 0.2432
2019-03-13 07:19:06,856 : ALL (weighted average) : Pearson = 0.5692,             Spearman = 0.5915
2019-03-13 07:19:06,856 : ALL (average) : Pearson = 0.5617,             Spearman = 0.5837

2019-03-13 07:19:06,856 : ***** Transfer task : MR *****


2019-03-13 07:19:06,875 : loading BERT model bert-large-uncased
2019-03-13 07:19:06,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:19:06,894 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:19:06,895 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqrrd1xbv
2019-03-13 07:19:14,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:19:19,849 : Generating sentence embeddings
2019-03-13 07:19:51,453 : Generated sentence embeddings
2019-03-13 07:19:51,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:20:00,103 : Best param found at split 1: l2reg = 1e-05                 with score 77.81
2019-03-13 07:20:09,834 : Best param found at split 2: l2reg = 1e-05                 with score 77.76
2019-03-13 07:20:20,327 : Best param found at split 3: l2reg = 1e-05                 with score 78.17
2019-03-13 07:20:31,131 : Best param found at split 4: l2reg = 0.01                 with score 77.51
2019-03-13 07:20:42,072 : Best param found at split 5: l2reg = 0.0001                 with score 77.37
2019-03-13 07:20:42,631 : Dev acc : 77.72 Test acc : 77.08

2019-03-13 07:20:42,632 : ***** Transfer task : CR *****


2019-03-13 07:20:42,640 : loading BERT model bert-large-uncased
2019-03-13 07:20:42,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:20:42,659 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:20:42,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp345saz1c
2019-03-13 07:20:50,135 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:20:55,416 : Generating sentence embeddings
2019-03-13 07:21:03,756 : Generated sentence embeddings
2019-03-13 07:21:03,757 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:21:07,440 : Best param found at split 1: l2reg = 0.001                 with score 81.98
2019-03-13 07:21:11,271 : Best param found at split 2: l2reg = 0.0001                 with score 82.01
2019-03-13 07:21:14,745 : Best param found at split 3: l2reg = 1e-05                 with score 82.78
2019-03-13 07:21:18,592 : Best param found at split 4: l2reg = 0.01                 with score 81.76
2019-03-13 07:21:22,601 : Best param found at split 5: l2reg = 0.0001                 with score 81.96
2019-03-13 07:21:22,794 : Dev acc : 82.1 Test acc : 80.5

2019-03-13 07:21:22,795 : ***** Transfer task : MPQA *****


2019-03-13 07:21:22,801 : loading BERT model bert-large-uncased
2019-03-13 07:21:22,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:21:22,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:21:22,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxbr7l_xu
2019-03-13 07:21:30,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:21:35,569 : Generating sentence embeddings
2019-03-13 07:21:43,162 : Generated sentence embeddings
2019-03-13 07:21:43,162 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:21:54,483 : Best param found at split 1: l2reg = 0.0001                 with score 86.89
2019-03-13 07:22:05,368 : Best param found at split 2: l2reg = 0.001                 with score 86.22
2019-03-13 07:22:16,542 : Best param found at split 3: l2reg = 0.0001                 with score 85.5
2019-03-13 07:22:28,039 : Best param found at split 4: l2reg = 0.01                 with score 86.28
2019-03-13 07:22:35,414 : Best param found at split 5: l2reg = 1e-05                 with score 85.25
2019-03-13 07:22:35,906 : Dev acc : 86.03 Test acc : 86.35

2019-03-13 07:22:35,907 : ***** Transfer task : SUBJ *****


2019-03-13 07:22:35,921 : loading BERT model bert-large-uncased
2019-03-13 07:22:35,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:22:35,942 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:22:35,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpivwoism2
2019-03-13 07:22:43,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:22:49,073 : Generating sentence embeddings
2019-03-13 07:23:19,999 : Generated sentence embeddings
2019-03-13 07:23:19,999 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:23:28,471 : Best param found at split 1: l2reg = 0.001                 with score 94.85
2019-03-13 07:23:35,813 : Best param found at split 2: l2reg = 0.001                 with score 94.88
2019-03-13 07:23:42,686 : Best param found at split 3: l2reg = 1e-05                 with score 94.24
2019-03-13 07:23:51,908 : Best param found at split 4: l2reg = 0.0001                 with score 95.03
2019-03-13 07:24:01,902 : Best param found at split 5: l2reg = 0.001                 with score 94.8
2019-03-13 07:24:02,394 : Dev acc : 94.76 Test acc : 94.52

2019-03-13 07:24:02,395 : ***** Transfer task : SST Binary classification *****


2019-03-13 07:24:02,487 : loading BERT model bert-large-uncased
2019-03-13 07:24:02,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:24:02,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:24:02,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2i53rjlq
2019-03-13 07:24:09,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:24:15,226 : Computing embedding for train
2019-03-13 07:25:55,598 : Computed train embeddings
2019-03-13 07:25:55,598 : Computing embedding for dev
2019-03-13 07:25:57,785 : Computed dev embeddings
2019-03-13 07:25:57,785 : Computing embedding for test
2019-03-13 07:26:02,388 : Computed test embeddings
2019-03-13 07:26:02,389 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:26:21,889 : [('reg:1e-05', 82.34), ('reg:0.0001', 82.45), ('reg:0.001', 82.11), ('reg:0.01', 79.93)]
2019-03-13 07:26:21,889 : Validation : best param found is reg = 0.0001 with score             82.45
2019-03-13 07:26:21,889 : Evaluating...
2019-03-13 07:26:25,419 : 
Dev acc : 82.45 Test acc : 82.59 for             SST Binary classification

2019-03-13 07:26:25,419 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 07:26:25,469 : loading BERT model bert-large-uncased
2019-03-13 07:26:25,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:26:25,491 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:26:25,492 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9g5puqd_
2019-03-13 07:26:32,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:26:38,246 : Computing embedding for train
2019-03-13 07:27:00,212 : Computed train embeddings
2019-03-13 07:27:00,212 : Computing embedding for dev
2019-03-13 07:27:03,078 : Computed dev embeddings
2019-03-13 07:27:03,078 : Computing embedding for test
2019-03-13 07:27:08,732 : Computed test embeddings
2019-03-13 07:27:08,732 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:27:10,934 : [('reg:1e-05', 40.51), ('reg:0.0001', 40.69), ('reg:0.001', 40.87), ('reg:0.01', 40.6)]
2019-03-13 07:27:10,934 : Validation : best param found is reg = 0.001 with score             40.87
2019-03-13 07:27:10,934 : Evaluating...
2019-03-13 07:27:11,487 : 
Dev acc : 40.87 Test acc : 44.34 for             SST Fine-Grained classification

2019-03-13 07:27:11,488 : ***** Transfer task : TREC *****


2019-03-13 07:27:11,502 : loading BERT model bert-large-uncased
2019-03-13 07:27:11,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:27:11,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:27:11,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo8t3z4gn
2019-03-13 07:27:18,961 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:27:31,776 : Computed train embeddings
2019-03-13 07:27:32,366 : Computed test embeddings
2019-03-13 07:27:32,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:27:38,912 : [('reg:1e-05', 84.3), ('reg:0.0001', 84.3), ('reg:0.001', 82.99), ('reg:0.01', 75.94)]
2019-03-13 07:27:38,912 : Cross-validation : best param found is reg = 1e-05             with score 84.3
2019-03-13 07:27:38,912 : Evaluating...
2019-03-13 07:27:39,441 : 
Dev acc : 84.3 Test acc : 93.4             for TREC

2019-03-13 07:27:39,442 : ***** Transfer task : MRPC *****


2019-03-13 07:27:39,463 : loading BERT model bert-large-uncased
2019-03-13 07:27:39,464 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:27:39,484 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:27:39,484 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8s72jpht
2019-03-13 07:27:46,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:27:52,080 : Computing embedding for train
2019-03-13 07:28:14,383 : Computed train embeddings
2019-03-13 07:28:14,383 : Computing embedding for test
2019-03-13 07:28:24,141 : Computed test embeddings
2019-03-13 07:28:24,161 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:28:29,081 : [('reg:1e-05', 73.28), ('reg:0.0001', 73.18), ('reg:0.001', 73.21), ('reg:0.01', 71.54)]
2019-03-13 07:28:29,081 : Cross-validation : best param found is reg = 1e-05             with score 73.28
2019-03-13 07:28:29,081 : Evaluating...
2019-03-13 07:28:29,389 : Dev acc : 73.28 Test acc 71.07; Test F1 76.67 for MRPC.

2019-03-13 07:28:29,389 : ***** Transfer task : SICK-Entailment*****


2019-03-13 07:28:29,451 : loading BERT model bert-large-uncased
2019-03-13 07:28:29,451 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:28:29,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:28:29,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5di6cjd5
2019-03-13 07:28:36,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:28:42,322 : Computing embedding for train
2019-03-13 07:28:53,637 : Computed train embeddings
2019-03-13 07:28:53,638 : Computing embedding for dev
2019-03-13 07:28:55,180 : Computed dev embeddings
2019-03-13 07:28:55,180 : Computing embedding for test
2019-03-13 07:29:07,317 : Computed test embeddings
2019-03-13 07:29:07,354 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:29:08,778 : [('reg:1e-05', 77.8), ('reg:0.0001', 78.2), ('reg:0.001', 76.8), ('reg:0.01', 75.6)]
2019-03-13 07:29:08,778 : Validation : best param found is reg = 0.0001 with score             78.2
2019-03-13 07:29:08,778 : Evaluating...
2019-03-13 07:29:09,128 : 
Dev acc : 78.2 Test acc : 77.55 for                        SICK entailment

2019-03-13 07:29:09,128 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 07:29:09,155 : loading BERT model bert-large-uncased
2019-03-13 07:29:09,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:29:09,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:29:09,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpotjhtsal
2019-03-13 07:29:16,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:29:21,945 : Computing embedding for train
2019-03-13 07:29:33,282 : Computed train embeddings
2019-03-13 07:29:33,282 : Computing embedding for dev
2019-03-13 07:29:34,829 : Computed dev embeddings
2019-03-13 07:29:34,829 : Computing embedding for test
2019-03-13 07:29:46,992 : Computed test embeddings
2019-03-13 07:30:05,118 : Dev : Pearson 0.8044901404146084
2019-03-13 07:30:05,121 : Test : Pearson 0.8099672832262849 Spearman 0.7431908965016985 MSE 0.3517029447802231                        for SICK Relatedness

2019-03-13 07:30:05,121 : 

***** Transfer task : STSBenchmark*****


2019-03-13 07:30:05,189 : loading BERT model bert-large-uncased
2019-03-13 07:30:05,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:30:05,208 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:30:05,208 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwel4kndu
2019-03-13 07:30:12,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:30:17,916 : Computing embedding for train
2019-03-13 07:30:36,549 : Computed train embeddings
2019-03-13 07:30:36,549 : Computing embedding for dev
2019-03-13 07:30:42,200 : Computed dev embeddings
2019-03-13 07:30:42,200 : Computing embedding for test
2019-03-13 07:30:46,816 : Computed test embeddings
2019-03-13 07:31:03,639 : Dev : Pearson 0.7278409934614452
2019-03-13 07:31:03,639 : Test : Pearson 0.6782240423898888 Spearman 0.6717399896722528 MSE 1.4817138022246898                        for SICK Relatedness

2019-03-13 07:31:03,639 : ***** Transfer task : SNLI Entailment*****


2019-03-13 07:31:08,655 : loading BERT model bert-large-uncased
2019-03-13 07:31:08,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:31:08,794 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:31:08,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj0yytjid
2019-03-13 07:31:16,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:31:21,967 : PROGRESS (encoding): 0.00%
2019-03-13 07:34:08,499 : PROGRESS (encoding): 14.56%
2019-03-13 07:37:19,039 : PROGRESS (encoding): 29.12%
2019-03-13 07:40:29,831 : PROGRESS (encoding): 43.69%
2019-03-13 07:43:52,772 : PROGRESS (encoding): 58.25%
2019-03-13 07:47:38,869 : PROGRESS (encoding): 72.81%
2019-03-13 07:51:23,549 : PROGRESS (encoding): 87.37%
2019-03-13 07:55:26,821 : PROGRESS (encoding): 0.00%
2019-03-13 07:55:57,504 : PROGRESS (encoding): 0.00%
2019-03-13 07:56:26,990 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:57:04,061 : [('reg:1e-09', 68.86)]
2019-03-13 07:57:04,062 : Validation : best param found is reg = 1e-09 with score             68.86
2019-03-13 07:57:04,062 : Evaluating...
2019-03-13 07:57:40,380 : Dev acc : 68.86 Test acc : 69.4 for SNLI

2019-03-13 07:57:40,380 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 07:57:40,590 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 07:57:41,578 : loading BERT model bert-large-uncased
2019-03-13 07:57:41,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:57:41,604 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:57:41,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfmuoctl8
2019-03-13 07:57:49,057 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:57:54,256 : Computing embeddings for train/dev/test
2019-03-13 08:01:25,852 : Computed embeddings
2019-03-13 08:01:25,852 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:01:54,795 : [('reg:1e-05', 91.06), ('reg:0.0001', 90.69), ('reg:0.001', 84.24), ('reg:0.01', 78.9)]
2019-03-13 08:01:54,795 : Validation : best param found is reg = 1e-05 with score             91.06
2019-03-13 08:01:54,795 : Evaluating...
2019-03-13 08:02:01,857 : 
Dev acc : 91.1 Test acc : 91.6 for LENGTH classification

2019-03-13 08:02:01,857 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 08:02:02,109 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 08:02:02,157 : loading BERT model bert-large-uncased
2019-03-13 08:02:02,157 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:02:02,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:02:02,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvbwk3zag
2019-03-13 08:02:09,636 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:02:14,904 : Computing embeddings for train/dev/test
2019-03-13 08:05:30,230 : Computed embeddings
2019-03-13 08:05:30,230 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:06:01,985 : [('reg:1e-05', 61.93), ('reg:0.0001', 24.46), ('reg:0.001', 2.04), ('reg:0.01', 0.9)]
2019-03-13 08:06:01,985 : Validation : best param found is reg = 1e-05 with score             61.93
2019-03-13 08:06:01,985 : Evaluating...
2019-03-13 08:06:11,878 : 
Dev acc : 61.9 Test acc : 61.5 for WORDCONTENT classification

2019-03-13 08:06:11,880 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 08:06:12,407 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 08:06:12,475 : loading BERT model bert-large-uncased
2019-03-13 08:06:12,475 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:06:12,500 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:06:12,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_jp6z3le
2019-03-13 08:06:19,997 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:06:25,317 : Computing embeddings for train/dev/test
2019-03-13 08:09:28,930 : Computed embeddings
2019-03-13 08:09:28,930 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:09:50,557 : [('reg:1e-05', 39.85), ('reg:0.0001', 39.67), ('reg:0.001', 37.61), ('reg:0.01', 31.34)]
2019-03-13 08:09:50,557 : Validation : best param found is reg = 1e-05 with score             39.85
2019-03-13 08:09:50,557 : Evaluating...
2019-03-13 08:09:55,916 : 
Dev acc : 39.9 Test acc : 40.8 for DEPTH classification

2019-03-13 08:09:55,917 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 08:09:56,323 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 08:09:56,386 : loading BERT model bert-large-uncased
2019-03-13 08:09:56,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:09:56,414 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:09:56,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp22evxh6i
2019-03-13 08:10:03,857 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:10:09,061 : Computing embeddings for train/dev/test
2019-03-13 08:12:59,325 : Computed embeddings
2019-03-13 08:12:59,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:13:27,337 : [('reg:1e-05', 78.46), ('reg:0.0001', 75.69), ('reg:0.001', 69.69), ('reg:0.01', 56.5)]
2019-03-13 08:13:27,338 : Validation : best param found is reg = 1e-05 with score             78.46
2019-03-13 08:13:27,338 : Evaluating...
2019-03-13 08:13:33,761 : 
Dev acc : 78.5 Test acc : 79.5 for TOPCONSTITUENTS classification

2019-03-13 08:13:33,762 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 08:13:34,120 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 08:13:34,187 : loading BERT model bert-large-uncased
2019-03-13 08:13:34,187 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:13:34,218 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:13:34,218 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbbu1vtop
2019-03-13 08:13:41,682 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:13:46,893 : Computing embeddings for train/dev/test
2019-03-13 08:16:51,694 : Computed embeddings
2019-03-13 08:16:51,694 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:17:25,747 : [('reg:1e-05', 91.69), ('reg:0.0001', 91.5), ('reg:0.001', 91.69), ('reg:0.01', 89.66)]
2019-03-13 08:17:25,747 : Validation : best param found is reg = 1e-05 with score             91.69
2019-03-13 08:17:25,747 : Evaluating...
2019-03-13 08:17:35,325 : 
Dev acc : 91.7 Test acc : 91.4 for BIGRAMSHIFT classification

2019-03-13 08:17:35,326 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 08:17:35,714 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 08:17:35,779 : loading BERT model bert-large-uncased
2019-03-13 08:17:35,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:17:35,807 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:17:35,807 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj94yiraa
2019-03-13 08:17:43,244 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:17:48,522 : Computing embeddings for train/dev/test
2019-03-13 08:20:48,799 : Computed embeddings
2019-03-13 08:20:48,799 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:21:17,856 : [('reg:1e-05', 89.08), ('reg:0.0001', 89.22), ('reg:0.001', 89.41), ('reg:0.01', 88.61)]
2019-03-13 08:21:17,856 : Validation : best param found is reg = 0.001 with score             89.41
2019-03-13 08:21:17,856 : Evaluating...
2019-03-13 08:21:24,479 : 
Dev acc : 89.4 Test acc : 87.8 for TENSE classification

2019-03-13 08:21:24,480 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 08:21:24,886 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 08:21:24,949 : loading BERT model bert-large-uncased
2019-03-13 08:21:24,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:21:25,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:21:25,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp7gc1t2a
2019-03-13 08:21:32,551 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:21:37,808 : Computing embeddings for train/dev/test
2019-03-13 08:24:48,638 : Computed embeddings
2019-03-13 08:24:48,639 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:25:13,728 : [('reg:1e-05', 86.96), ('reg:0.0001', 87.09), ('reg:0.001', 86.66), ('reg:0.01', 85.18)]
2019-03-13 08:25:13,729 : Validation : best param found is reg = 0.0001 with score             87.09
2019-03-13 08:25:13,729 : Evaluating...
2019-03-13 08:25:20,140 : 
Dev acc : 87.1 Test acc : 86.6 for SUBJNUMBER classification

2019-03-13 08:25:20,141 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 08:25:20,792 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 08:25:20,865 : loading BERT model bert-large-uncased
2019-03-13 08:25:20,865 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:25:20,895 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:25:20,895 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnv8sumf9
2019-03-13 08:25:28,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:25:33,668 : Computing embeddings for train/dev/test
2019-03-13 08:28:41,065 : Computed embeddings
2019-03-13 08:28:41,066 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:29:06,012 : [('reg:1e-05', 84.06), ('reg:0.0001', 84.07), ('reg:0.001', 84.06), ('reg:0.01', 82.52)]
2019-03-13 08:29:06,012 : Validation : best param found is reg = 0.0001 with score             84.07
2019-03-13 08:29:06,013 : Evaluating...
2019-03-13 08:29:11,399 : 
Dev acc : 84.1 Test acc : 85.0 for OBJNUMBER classification

2019-03-13 08:29:11,400 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 08:29:11,775 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 08:29:11,844 : loading BERT model bert-large-uncased
2019-03-13 08:29:11,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:29:11,970 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:29:11,970 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkc7uor80
2019-03-13 08:29:19,429 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:29:24,638 : Computing embeddings for train/dev/test
2019-03-13 08:33:02,187 : Computed embeddings
2019-03-13 08:33:02,187 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:33:30,683 : [('reg:1e-05', 64.24), ('reg:0.0001', 64.13), ('reg:0.001', 64.02), ('reg:0.01', 63.88)]
2019-03-13 08:33:30,683 : Validation : best param found is reg = 1e-05 with score             64.24
2019-03-13 08:33:30,683 : Evaluating...
2019-03-13 08:33:35,962 : 
Dev acc : 64.2 Test acc : 65.6 for ODDMANOUT classification

2019-03-13 08:33:35,963 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 08:33:36,406 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 08:33:36,482 : loading BERT model bert-large-uncased
2019-03-13 08:33:36,482 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:33:36,511 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:33:36,511 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4_ss9e4b
2019-03-13 08:33:44,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:33:49,258 : Computing embeddings for train/dev/test
2019-03-13 08:37:24,860 : Computed embeddings
2019-03-13 08:37:24,860 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:37:54,554 : [('reg:1e-05', 65.56), ('reg:0.0001', 65.7), ('reg:0.001', 65.05), ('reg:0.01', 60.73)]
2019-03-13 08:37:54,554 : Validation : best param found is reg = 0.0001 with score             65.7
2019-03-13 08:37:54,554 : Evaluating...
2019-03-13 08:38:00,596 : 
Dev acc : 65.7 Test acc : 66.0 for COORDINATIONINVERSION classification

2019-03-13 08:38:00,599 : total results: {'STS12': {'MSRpar': {'pearson': (0.3448419329268657, 2.288325434373783e-22), 'spearman': SpearmanrResult(correlation=0.38437680529908563, pvalue=8.123110029008135e-28), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4741417251999089, 2.6640070346421994e-43), 'spearman': SpearmanrResult(correlation=0.4880837941740523, pvalue=3.780424703088134e-46), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5037828672715906, 6.524704856976314e-31), 'spearman': SpearmanrResult(correlation=0.5945996965545068, pvalue=3.185844338833362e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.599223048274977, 2.715395097865514e-74), 'spearman': SpearmanrResult(correlation=0.6302324277791789, pvalue=2.825996697360672e-84), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6617621576813398, 1.2992980742021593e-51), 'spearman': SpearmanrResult(correlation=0.5473645595083976, pvalue=1.451157668196711e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5167503462709364, 'wmean': 0.5015876662785806}, 'spearman': {'mean': 0.5289314566630443, 'wmean': 0.5207012517379688}}}, 'STS13': {'FNWN': {'pearson': (0.3175622686600988, 8.48375407602266e-06), 'spearman': SpearmanrResult(correlation=0.31602069993483933, pvalue=9.431988519259602e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.6180647527229888, 3.173266695227802e-80), 'spearman': SpearmanrResult(correlation=0.5987822415770733, pvalue=3.697638331164728e-74), 'nsamples': 750}, 'OnWN': {'pearson': (0.4155871818990956, 7.774928616274804e-25), 'spearman': SpearmanrResult(correlation=0.44551832532764263, pvalue=1.0451311644397669e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4504047344273944, 'wmean': 0.5044748282429286}, 'spearman': {'mean': 0.45344042227985176, 'wmean': 0.5058335826528647}}}, 'STS14': {'deft-forum': {'pearson': (0.2601588579714301, 2.1414012072791612e-08), 'spearman': SpearmanrResult(correlation=0.3048185653288775, pvalue=3.94706565914194e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7201528044743258, 3.13631324783951e-49), 'spearman': SpearmanrResult(correlation=0.6878780971781401, pvalue=2.2567949423998723e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.56173454674119, 1.357976498258868e-63), 'spearman': SpearmanrResult(correlation=0.5194918877615391, pvalue=4.560155639908965e-53), 'nsamples': 750}, 'images': {'pearson': (0.4780669352525935, 4.3346644833562855e-44), 'spearman': SpearmanrResult(correlation=0.48861928114516634, pvalue=2.9208838454163137e-46), 'nsamples': 750}, 'OnWN': {'pearson': (0.5910276730928959, 7.819222660650227e-72), 'spearman': SpearmanrResult(correlation=0.6316654476546261, pvalue=9.166575317468593e-85), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6190710465618864, 1.4903063395241898e-80), 'spearman': SpearmanrResult(correlation=0.5862132410844033, pvalue=2.020046239518293e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5383686440157204, 'wmean': 0.5388113276442309}, 'spearman': {'mean': 0.536447753358792, 'wmean': 0.5368064471428634}}}, 'STS15': {'answers-forums': {'pearson': (0.5074882722768349, 6.125521950500062e-26), 'spearman': SpearmanrResult(correlation=0.49675733074857414, pvalue=9.19210068841499e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.6864282891119322, 1.3918087196249334e-105), 'spearman': SpearmanrResult(correlation=0.698376029273007, pvalue=9.398059488054606e-111), 'nsamples': 750}, 'belief': {'pearson': (0.5774720066392705, 9.859999387719975e-35), 'spearman': SpearmanrResult(correlation=0.6169233494523768, pvalue=1.0642832579835927e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6243427450754016, 2.717033080645768e-82), 'spearman': SpearmanrResult(correlation=0.6178168489996527, pvalue=3.821066047547805e-80), 'nsamples': 750}, 'images': {'pearson': (0.6645164358621531, 1.0101848985568448e-96), 'spearman': SpearmanrResult(correlation=0.6826446213631294, pvalue=5.3616623761840436e-104), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6120495497931184, 'wmean': 0.6294419023768849}, 'spearman': {'mean': 0.6225036359673479, 'wmean': 0.6389194599340662}}}, 'STS16': {'answer-answer': {'pearson': (0.43921185354246195, 2.1067826913747205e-13), 'spearman': SpearmanrResult(correlation=0.47611158739651976, pvalue=8.92391473294413e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.6573858284434481, 3.4067766595846634e-32), 'spearman': SpearmanrResult(correlation=0.6558383422339339, pvalue=5.308305308028722e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.707355452127954, 3.301662438713664e-36), 'spearman': SpearmanrResult(correlation=0.7171438818832165, pvalue=1.2928933563108874e-37), 'nsamples': 230}, 'postediting': {'pearson': (0.7756735811419363, 2.806755082028714e-50), 'spearman': SpearmanrResult(correlation=0.8260027996774727, pvalue=3.4599105823144018e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.22898710275952588, 0.0008530005824613385), 'spearman': SpearmanrResult(correlation=0.24320229578702324, pvalue=0.0003877667346123939), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5617227636030652, 'wmean': 0.5691933341880326}, 'spearman': {'mean': 0.5836597813956332, 'wmean': 0.5915287910530325}}}, 'MR': {'devacc': 77.72, 'acc': 77.08, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 82.1, 'acc': 80.5, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.03, 'acc': 86.35, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.76, 'acc': 94.52, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.45, 'acc': 82.59, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.87, 'acc': 44.34, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.3, 'acc': 93.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.28, 'acc': 71.07, 'f1': 76.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 77.55, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8044901404146084, 'pearson': 0.8099672832262849, 'spearman': 0.7431908965016985, 'mse': 0.3517029447802231, 'yhat': array([3.33392899, 4.67942907, 1.35417358, ..., 3.00104408, 4.42112039,        4.46849323]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7278409934614452, 'pearson': 0.6782240423898888, 'spearman': 0.6717399896722528, 'mse': 1.4817138022246898, 'yhat': array([1.51453329, 1.44800862, 2.32213095, ..., 3.89888808, 3.89240068,        3.54006207]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.86, 'acc': 69.4, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.06, 'acc': 91.64, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 61.93, 'acc': 61.55, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 39.85, 'acc': 40.75, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 78.46, 'acc': 79.48, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.69, 'acc': 91.36, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.41, 'acc': 87.81, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.09, 'acc': 86.56, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 84.07, 'acc': 85.03, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.24, 'acc': 65.6, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 65.7, 'acc': 65.98, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 08:38:00,599 : STS12 p=0.5016, STS12 s=0.5207, STS13 p=0.5045, STS13 s=0.5058, STS14 p=0.5388, STS14 s=0.5368, STS15 p=0.6294, STS15 s=0.6389, STS 16 p=0.5692, STS16 s=0.5915, STS B p=0.6782, STS B s=0.6717, STS B m=1.4817, SICK-R p=0.8100, SICK-R s=0.7432, SICK-P m=0.3517
2019-03-13 08:38:00,599 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 08:38:00,599 : 0.5016,0.5207,0.5045,0.5058,0.5388,0.5368,0.6294,0.6389,0.5692,0.5915,0.6782,0.6717,1.4817,0.8100,0.7432,0.3517
2019-03-13 08:38:00,599 : MR=77.08, CR=80.50, SUBJ=94.52, MPQA=86.35, SST-B=82.59, SST-F=44.34, TREC=93.40, SICK-E=77.55, SNLI=69.40, MRPC=71.07, MRPC f=76.67
2019-03-13 08:38:00,599 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 08:38:00,599 : 77.08,80.50,94.52,86.35,82.59,44.34,93.40,77.55,69.40,71.07,76.67
2019-03-13 08:38:00,599 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 08:38:00,599 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 08:38:00,599 : na,na,na,na,na,na,na,na,na,na
2019-03-13 08:38:00,599 : SentLen=91.64, WC=61.55, TreeDepth=40.75, TopConst=79.48, BShift=91.36, Tense=87.81, SubjNum=86.56, ObjNum=85.03, SOMO=65.60, CoordInv=65.98, average=75.58
2019-03-13 08:38:00,599 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 08:38:00,599 : 91.64,61.55,40.75,79.48,91.36,87.81,86.56,85.03,65.60,65.98,75.58
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,599 : layer 14
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,599 : ********************************************************************************
2019-03-13 08:38:00,691 : ***** Transfer task : STS12 *****


2019-03-13 08:38:00,704 : loading BERT model bert-large-uncased
2019-03-13 08:38:00,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:38:00,721 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:38:00,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgx1ushll
2019-03-13 08:38:08,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:17,517 : MSRpar : pearson = 0.3399, spearman = 0.3775
2019-03-13 08:38:19,165 : MSRvid : pearson = 0.4683, spearman = 0.4817
2019-03-13 08:38:20,581 : SMTeuroparl : pearson = 0.5209, spearman = 0.6104
2019-03-13 08:38:23,287 : surprise.OnWN : pearson = 0.6105, spearman = 0.6317
2019-03-13 08:38:24,719 : surprise.SMTnews : pearson = 0.6838, spearman = 0.5628
2019-03-13 08:38:24,719 : ALL (weighted average) : Pearson = 0.5071,             Spearman = 0.5222
2019-03-13 08:38:24,719 : ALL (average) : Pearson = 0.5247,             Spearman = 0.5328

2019-03-13 08:38:24,720 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 08:38:24,728 : loading BERT model bert-large-uncased
2019-03-13 08:38:24,728 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:38:24,745 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:38:24,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm2ykpltn
2019-03-13 08:38:32,189 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:38,858 : FNWN : pearson = 0.3439, spearman = 0.3578
2019-03-13 08:38:40,757 : headlines : pearson = 0.6160, spearman = 0.5964
2019-03-13 08:38:42,231 : OnWN : pearson = 0.4651, spearman = 0.4866
2019-03-13 08:38:42,231 : ALL (weighted average) : Pearson = 0.5253,             Spearman = 0.5252
2019-03-13 08:38:42,231 : ALL (average) : Pearson = 0.4750,             Spearman = 0.4802

2019-03-13 08:38:42,231 : ***** Transfer task : STS14 *****


2019-03-13 08:38:42,246 : loading BERT model bert-large-uncased
2019-03-13 08:38:42,246 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:38:42,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:38:42,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprwgk1hmw
2019-03-13 08:38:49,704 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:56,336 : deft-forum : pearson = 0.2777, spearman = 0.2976
2019-03-13 08:38:57,980 : deft-news : pearson = 0.7261, spearman = 0.6899
2019-03-13 08:39:00,157 : headlines : pearson = 0.5588, spearman = 0.5146
2019-03-13 08:39:02,245 : images : pearson = 0.4981, spearman = 0.5006
2019-03-13 08:39:04,382 : OnWN : pearson = 0.6339, spearman = 0.6671
2019-03-13 08:39:07,250 : tweet-news : pearson = 0.6596, spearman = 0.6104
2019-03-13 08:39:07,250 : ALL (weighted average) : Pearson = 0.5615,             Spearman = 0.5494
2019-03-13 08:39:07,250 : ALL (average) : Pearson = 0.5590,             Spearman = 0.5467

2019-03-13 08:39:07,250 : ***** Transfer task : STS15 *****


2019-03-13 08:39:07,282 : loading BERT model bert-large-uncased
2019-03-13 08:39:07,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:39:07,299 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:39:07,299 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbhb70ryn
2019-03-13 08:39:14,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:39:21,787 : answers-forums : pearson = 0.5636, spearman = 0.5524
2019-03-13 08:39:23,877 : answers-students : pearson = 0.7057, spearman = 0.7176
2019-03-13 08:39:25,930 : belief : pearson = 0.6314, spearman = 0.6574
2019-03-13 08:39:28,186 : headlines : pearson = 0.6202, spearman = 0.6114
2019-03-13 08:39:30,324 : images : pearson = 0.6746, spearman = 0.6858
2019-03-13 08:39:30,324 : ALL (weighted average) : Pearson = 0.6495,             Spearman = 0.6549
2019-03-13 08:39:30,324 : ALL (average) : Pearson = 0.6391,             Spearman = 0.6449

2019-03-13 08:39:30,324 : ***** Transfer task : STS16 *****


2019-03-13 08:39:30,393 : loading BERT model bert-large-uncased
2019-03-13 08:39:30,394 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:39:30,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:39:30,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgj0nu9w6
2019-03-13 08:39:37,877 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:39:44,033 : answer-answer : pearson = 0.5242, spearman = 0.5445
2019-03-13 08:39:44,695 : headlines : pearson = 0.6531, spearman = 0.6540
2019-03-13 08:39:45,580 : plagiarism : pearson = 0.7565, spearman = 0.7664
2019-03-13 08:39:47,077 : postediting : pearson = 0.8082, spearman = 0.8419
2019-03-13 08:39:47,685 : question-question : pearson = 0.2591, spearman = 0.2574
2019-03-13 08:39:47,685 : ALL (weighted average) : Pearson = 0.6080,             Spearman = 0.6211
2019-03-13 08:39:47,685 : ALL (average) : Pearson = 0.6002,             Spearman = 0.6128

2019-03-13 08:39:47,685 : ***** Transfer task : MR *****


2019-03-13 08:39:47,701 : loading BERT model bert-large-uncased
2019-03-13 08:39:47,701 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:39:47,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:39:47,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp49jaev_h
2019-03-13 08:39:55,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:40:00,414 : Generating sentence embeddings
2019-03-13 08:40:31,974 : Generated sentence embeddings
2019-03-13 08:40:31,975 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:40:41,699 : Best param found at split 1: l2reg = 0.001                 with score 78.27
2019-03-13 08:40:50,278 : Best param found at split 2: l2reg = 0.001                 with score 78.63
2019-03-13 08:40:59,170 : Best param found at split 3: l2reg = 0.001                 with score 79.16
2019-03-13 08:41:07,105 : Best param found at split 4: l2reg = 0.001                 with score 78.75
2019-03-13 08:41:17,353 : Best param found at split 5: l2reg = 0.0001                 with score 78.52
2019-03-13 08:41:17,792 : Dev acc : 78.67 Test acc : 77.84

2019-03-13 08:41:17,793 : ***** Transfer task : CR *****


2019-03-13 08:41:17,801 : loading BERT model bert-large-uncased
2019-03-13 08:41:17,801 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:41:17,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:41:17,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz_06esxg
2019-03-13 08:41:25,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:41:30,543 : Generating sentence embeddings
2019-03-13 08:41:38,894 : Generated sentence embeddings
2019-03-13 08:41:38,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:41:42,347 : Best param found at split 1: l2reg = 0.001                 with score 82.91
2019-03-13 08:41:46,327 : Best param found at split 2: l2reg = 1e-05                 with score 83.6
2019-03-13 08:41:50,438 : Best param found at split 3: l2reg = 0.0001                 with score 83.28
2019-03-13 08:41:54,152 : Best param found at split 4: l2reg = 0.0001                 with score 83.12
2019-03-13 08:41:58,120 : Best param found at split 5: l2reg = 0.001                 with score 83.68
2019-03-13 08:41:58,311 : Dev acc : 83.32 Test acc : 80.16

2019-03-13 08:41:58,312 : ***** Transfer task : MPQA *****


2019-03-13 08:41:58,318 : loading BERT model bert-large-uncased
2019-03-13 08:41:58,318 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:41:58,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:41:58,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfoh5y40e
2019-03-13 08:42:05,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:42:11,004 : Generating sentence embeddings
2019-03-13 08:42:18,604 : Generated sentence embeddings
2019-03-13 08:42:18,604 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:42:28,342 : Best param found at split 1: l2reg = 0.001                 with score 85.11
2019-03-13 08:42:37,846 : Best param found at split 2: l2reg = 0.0001                 with score 85.77
2019-03-13 08:42:47,287 : Best param found at split 3: l2reg = 0.001                 with score 85.52
2019-03-13 08:42:58,687 : Best param found at split 4: l2reg = 0.01                 with score 86.58
2019-03-13 08:43:07,628 : Best param found at split 5: l2reg = 0.001                 with score 84.65
2019-03-13 08:43:08,308 : Dev acc : 85.53 Test acc : 86.07

2019-03-13 08:43:08,308 : ***** Transfer task : SUBJ *****


2019-03-13 08:43:08,325 : loading BERT model bert-large-uncased
2019-03-13 08:43:08,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:43:08,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:43:08,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptbbijrqn
2019-03-13 08:43:15,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:43:21,122 : Generating sentence embeddings
2019-03-13 08:43:52,047 : Generated sentence embeddings
2019-03-13 08:43:52,048 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:44:01,470 : Best param found at split 1: l2reg = 0.001                 with score 95.32
2019-03-13 08:44:11,760 : Best param found at split 2: l2reg = 0.001                 with score 95.25
2019-03-13 08:44:21,334 : Best param found at split 3: l2reg = 1e-05                 with score 94.72
2019-03-13 08:44:32,470 : Best param found at split 4: l2reg = 0.001                 with score 95.48
2019-03-13 08:44:42,363 : Best param found at split 5: l2reg = 0.001                 with score 95.24
2019-03-13 08:44:42,827 : Dev acc : 95.2 Test acc : 94.75

2019-03-13 08:44:42,828 : ***** Transfer task : SST Binary classification *****


2019-03-13 08:44:42,917 : loading BERT model bert-large-uncased
2019-03-13 08:44:42,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:44:42,992 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:44:42,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuyd9l6fq
2019-03-13 08:44:50,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:44:55,757 : Computing embedding for train
2019-03-13 08:46:36,037 : Computed train embeddings
2019-03-13 08:46:36,038 : Computing embedding for dev
2019-03-13 08:46:38,224 : Computed dev embeddings
2019-03-13 08:46:38,224 : Computing embedding for test
2019-03-13 08:46:42,826 : Computed test embeddings
2019-03-13 08:46:42,827 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:46:58,497 : [('reg:1e-05', 83.14), ('reg:0.0001', 83.03), ('reg:0.001', 82.8), ('reg:0.01', 82.68)]
2019-03-13 08:46:58,498 : Validation : best param found is reg = 1e-05 with score             83.14
2019-03-13 08:46:58,498 : Evaluating...
2019-03-13 08:47:02,020 : 
Dev acc : 83.14 Test acc : 83.25 for             SST Binary classification

2019-03-13 08:47:02,021 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 08:47:02,075 : loading BERT model bert-large-uncased
2019-03-13 08:47:02,075 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:47:02,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:47:02,097 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp1znn9u2
2019-03-13 08:47:09,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:47:14,801 : Computing embedding for train
2019-03-13 08:47:36,735 : Computed train embeddings
2019-03-13 08:47:36,735 : Computing embedding for dev
2019-03-13 08:47:39,603 : Computed dev embeddings
2019-03-13 08:47:39,603 : Computing embedding for test
2019-03-13 08:47:45,255 : Computed test embeddings
2019-03-13 08:47:45,255 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:47:48,072 : [('reg:1e-05', 41.14), ('reg:0.0001', 41.14), ('reg:0.001', 41.51), ('reg:0.01', 40.78)]
2019-03-13 08:47:48,072 : Validation : best param found is reg = 0.001 with score             41.51
2019-03-13 08:47:48,072 : Evaluating...
2019-03-13 08:47:48,709 : 
Dev acc : 41.51 Test acc : 41.95 for             SST Fine-Grained classification

2019-03-13 08:47:48,710 : ***** Transfer task : TREC *****


2019-03-13 08:47:48,723 : loading BERT model bert-large-uncased
2019-03-13 08:47:48,723 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:47:48,742 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:47:48,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps6tftx_a
2019-03-13 08:47:56,199 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:48:09,422 : Computed train embeddings
2019-03-13 08:48:10,012 : Computed test embeddings
2019-03-13 08:48:10,012 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:48:16,281 : [('reg:1e-05', 83.1), ('reg:0.0001', 82.96), ('reg:0.001', 82.02), ('reg:0.01', 76.23)]
2019-03-13 08:48:16,281 : Cross-validation : best param found is reg = 1e-05             with score 83.1
2019-03-13 08:48:16,281 : Evaluating...
2019-03-13 08:48:16,786 : 
Dev acc : 83.1 Test acc : 94.2             for TREC

2019-03-13 08:48:16,786 : ***** Transfer task : MRPC *****


2019-03-13 08:48:16,806 : loading BERT model bert-large-uncased
2019-03-13 08:48:16,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:48:16,830 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:48:16,830 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpst1ei4dv
2019-03-13 08:48:24,273 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:48:29,518 : Computing embedding for train
2019-03-13 08:48:51,780 : Computed train embeddings
2019-03-13 08:48:51,780 : Computing embedding for test
2019-03-13 08:49:01,553 : Computed test embeddings
2019-03-13 08:49:01,574 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:49:06,325 : [('reg:1e-05', 73.8), ('reg:0.0001', 73.92), ('reg:0.001', 73.7), ('reg:0.01', 72.18)]
2019-03-13 08:49:06,325 : Cross-validation : best param found is reg = 0.0001             with score 73.92
2019-03-13 08:49:06,325 : Evaluating...
2019-03-13 08:49:06,640 : Dev acc : 73.92 Test acc 73.8; Test F1 79.23 for MRPC.

2019-03-13 08:49:06,640 : ***** Transfer task : SICK-Entailment*****


2019-03-13 08:49:06,703 : loading BERT model bert-large-uncased
2019-03-13 08:49:06,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:49:06,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:49:06,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcldv3eso
2019-03-13 08:49:14,155 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:49:19,414 : Computing embedding for train
2019-03-13 08:49:30,708 : Computed train embeddings
2019-03-13 08:49:30,708 : Computing embedding for dev
2019-03-13 08:49:32,252 : Computed dev embeddings
2019-03-13 08:49:32,252 : Computing embedding for test
2019-03-13 08:49:44,398 : Computed test embeddings
2019-03-13 08:49:44,437 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:49:45,988 : [('reg:1e-05', 79.8), ('reg:0.0001', 79.8), ('reg:0.001', 79.8), ('reg:0.01', 75.4)]
2019-03-13 08:49:45,988 : Validation : best param found is reg = 1e-05 with score             79.8
2019-03-13 08:49:45,988 : Evaluating...
2019-03-13 08:49:46,368 : 
Dev acc : 79.8 Test acc : 78.24 for                        SICK entailment

2019-03-13 08:49:46,369 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 08:49:46,396 : loading BERT model bert-large-uncased
2019-03-13 08:49:46,396 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:49:46,455 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:49:46,456 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuky6czz0
2019-03-13 08:49:53,900 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:49:59,287 : Computing embedding for train
2019-03-13 08:50:10,601 : Computed train embeddings
2019-03-13 08:50:10,601 : Computing embedding for dev
2019-03-13 08:50:12,147 : Computed dev embeddings
2019-03-13 08:50:12,147 : Computing embedding for test
2019-03-13 08:50:24,305 : Computed test embeddings
2019-03-13 08:50:39,352 : Dev : Pearson 0.7899878073780807
2019-03-13 08:50:39,352 : Test : Pearson 0.8091985614104109 Spearman 0.7441180746823515 MSE 0.3515561247518121                        for SICK Relatedness

2019-03-13 08:50:39,353 : 

***** Transfer task : STSBenchmark*****


2019-03-13 08:50:39,420 : loading BERT model bert-large-uncased
2019-03-13 08:50:39,420 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:50:39,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:50:39,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptnpnicux
2019-03-13 08:50:46,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:50:52,084 : Computing embedding for train
2019-03-13 08:51:10,760 : Computed train embeddings
2019-03-13 08:51:10,761 : Computing embedding for dev
2019-03-13 08:51:16,411 : Computed dev embeddings
2019-03-13 08:51:16,411 : Computing embedding for test
2019-03-13 08:51:21,027 : Computed test embeddings
2019-03-13 08:51:35,792 : Dev : Pearson 0.7166532532005799
2019-03-13 08:51:35,792 : Test : Pearson 0.6709550518245768 Spearman 0.6644842309131127 MSE 1.4907862389791562                        for SICK Relatedness

2019-03-13 08:51:35,792 : ***** Transfer task : SNLI Entailment*****


2019-03-13 08:51:40,881 : loading BERT model bert-large-uncased
2019-03-13 08:51:40,881 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:51:41,004 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:51:41,004 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps4uf1oom
2019-03-13 08:51:48,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:51:54,237 : PROGRESS (encoding): 0.00%
2019-03-13 08:54:40,607 : PROGRESS (encoding): 14.56%
2019-03-13 08:57:50,786 : PROGRESS (encoding): 29.12%
2019-03-13 09:01:01,042 : PROGRESS (encoding): 43.69%
2019-03-13 09:04:23,721 : PROGRESS (encoding): 58.25%
2019-03-13 09:08:09,387 : PROGRESS (encoding): 72.81%
2019-03-13 09:11:53,914 : PROGRESS (encoding): 87.37%
2019-03-13 09:15:56,944 : PROGRESS (encoding): 0.00%
2019-03-13 09:16:27,579 : PROGRESS (encoding): 0.00%
2019-03-13 09:16:57,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:17:34,660 : [('reg:1e-09', 69.51)]
2019-03-13 09:17:34,660 : Validation : best param found is reg = 1e-09 with score             69.51
2019-03-13 09:17:34,660 : Evaluating...
2019-03-13 09:18:10,405 : Dev acc : 69.51 Test acc : 70.1 for SNLI

2019-03-13 09:18:10,406 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 09:18:10,614 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 09:18:11,632 : loading BERT model bert-large-uncased
2019-03-13 09:18:11,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:18:11,660 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:18:11,660 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgscaijsy
2019-03-13 09:18:19,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:18:24,512 : Computing embeddings for train/dev/test
2019-03-13 09:21:56,063 : Computed embeddings
2019-03-13 09:21:56,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:22:27,251 : [('reg:1e-05', 89.61), ('reg:0.0001', 88.52), ('reg:0.001', 81.86), ('reg:0.01', 73.57)]
2019-03-13 09:22:27,252 : Validation : best param found is reg = 1e-05 with score             89.61
2019-03-13 09:22:27,252 : Evaluating...
2019-03-13 09:22:34,848 : 
Dev acc : 89.6 Test acc : 90.0 for LENGTH classification

2019-03-13 09:22:34,849 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 09:22:35,228 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 09:22:35,274 : loading BERT model bert-large-uncased
2019-03-13 09:22:35,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:22:35,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:22:35,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2c2exlfe
2019-03-13 09:22:42,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:22:47,917 : Computing embeddings for train/dev/test
2019-03-13 09:26:03,020 : Computed embeddings
2019-03-13 09:26:03,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:26:36,250 : [('reg:1e-05', 62.47), ('reg:0.0001', 30.76), ('reg:0.001', 2.83), ('reg:0.01', 0.92)]
2019-03-13 09:26:36,250 : Validation : best param found is reg = 1e-05 with score             62.47
2019-03-13 09:26:36,250 : Evaluating...
2019-03-13 09:26:47,335 : 
Dev acc : 62.5 Test acc : 62.9 for WORDCONTENT classification

2019-03-13 09:26:47,336 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 09:26:47,713 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 09:26:47,779 : loading BERT model bert-large-uncased
2019-03-13 09:26:47,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:26:47,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:26:47,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7t6xo0fl
2019-03-13 09:26:55,217 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:27:00,393 : Computing embeddings for train/dev/test
2019-03-13 09:30:03,847 : Computed embeddings
2019-03-13 09:30:03,848 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:30:23,547 : [('reg:1e-05', 39.01), ('reg:0.0001', 38.81), ('reg:0.001', 36.76), ('reg:0.01', 30.9)]
2019-03-13 09:30:23,547 : Validation : best param found is reg = 1e-05 with score             39.01
2019-03-13 09:30:23,547 : Evaluating...
2019-03-13 09:30:28,916 : 
Dev acc : 39.0 Test acc : 40.0 for DEPTH classification

2019-03-13 09:30:28,917 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 09:30:29,299 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 09:30:29,362 : loading BERT model bert-large-uncased
2019-03-13 09:30:29,362 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:30:29,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:30:29,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4nwovuc9
2019-03-13 09:30:36,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:30:41,975 : Computing embeddings for train/dev/test
2019-03-13 09:33:31,986 : Computed embeddings
2019-03-13 09:33:31,986 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:33:56,775 : [('reg:1e-05', 77.86), ('reg:0.0001', 75.89), ('reg:0.001', 68.82), ('reg:0.01', 58.93)]
2019-03-13 09:33:56,775 : Validation : best param found is reg = 1e-05 with score             77.86
2019-03-13 09:33:56,775 : Evaluating...
2019-03-13 09:34:03,281 : 
Dev acc : 77.9 Test acc : 78.0 for TOPCONSTITUENTS classification

2019-03-13 09:34:03,282 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 09:34:03,638 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 09:34:03,704 : loading BERT model bert-large-uncased
2019-03-13 09:34:03,705 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:34:03,733 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:34:03,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi9r47_bf
2019-03-13 09:34:11,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:34:16,445 : Computing embeddings for train/dev/test
2019-03-13 09:37:21,302 : Computed embeddings
2019-03-13 09:37:21,302 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:37:53,397 : [('reg:1e-05', 92.45), ('reg:0.0001', 92.44), ('reg:0.001', 92.22), ('reg:0.01', 90.45)]
2019-03-13 09:37:53,397 : Validation : best param found is reg = 1e-05 with score             92.45
2019-03-13 09:37:53,397 : Evaluating...
2019-03-13 09:37:59,420 : 
Dev acc : 92.5 Test acc : 92.5 for BIGRAMSHIFT classification

2019-03-13 09:37:59,421 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 09:37:59,804 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 09:37:59,869 : loading BERT model bert-large-uncased
2019-03-13 09:37:59,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:37:59,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:37:59,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps9mv_411
2019-03-13 09:38:07,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:38:12,984 : Computing embeddings for train/dev/test
2019-03-13 09:41:13,577 : Computed embeddings
2019-03-13 09:41:13,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:41:44,991 : [('reg:1e-05', 88.79), ('reg:0.0001', 88.92), ('reg:0.001', 89.14), ('reg:0.01', 88.54)]
2019-03-13 09:41:44,991 : Validation : best param found is reg = 0.001 with score             89.14
2019-03-13 09:41:44,991 : Evaluating...
2019-03-13 09:41:51,411 : 
Dev acc : 89.1 Test acc : 87.5 for TENSE classification

2019-03-13 09:41:51,412 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 09:41:52,011 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 09:41:52,074 : loading BERT model bert-large-uncased
2019-03-13 09:41:52,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:41:52,100 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:41:52,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj_xf0otw
2019-03-13 09:41:59,632 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:42:04,998 : Computing embeddings for train/dev/test
2019-03-13 09:45:15,943 : Computed embeddings
2019-03-13 09:45:15,943 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:45:45,077 : [('reg:1e-05', 85.87), ('reg:0.0001', 85.9), ('reg:0.001', 86.55), ('reg:0.01', 85.18)]
2019-03-13 09:45:45,077 : Validation : best param found is reg = 0.001 with score             86.55
2019-03-13 09:45:45,077 : Evaluating...
2019-03-13 09:45:54,752 : 
Dev acc : 86.5 Test acc : 86.0 for SUBJNUMBER classification

2019-03-13 09:45:54,753 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 09:45:55,146 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 09:45:55,212 : loading BERT model bert-large-uncased
2019-03-13 09:45:55,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:45:55,327 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:45:55,327 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmputepaary
2019-03-13 09:46:02,804 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:46:08,015 : Computing embeddings for train/dev/test
2019-03-13 09:49:15,564 : Computed embeddings
2019-03-13 09:49:15,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:49:37,205 : [('reg:1e-05', 83.86), ('reg:0.0001', 83.84), ('reg:0.001', 83.68), ('reg:0.01', 82.42)]
2019-03-13 09:49:37,206 : Validation : best param found is reg = 1e-05 with score             83.86
2019-03-13 09:49:37,206 : Evaluating...
2019-03-13 09:49:42,760 : 
Dev acc : 83.9 Test acc : 85.0 for OBJNUMBER classification

2019-03-13 09:49:42,761 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 09:49:43,188 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 09:49:43,257 : loading BERT model bert-large-uncased
2019-03-13 09:49:43,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:49:43,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:49:43,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3c3f5irz
2019-03-13 09:49:50,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:49:56,145 : Computing embeddings for train/dev/test
2019-03-13 09:53:34,013 : Computed embeddings
2019-03-13 09:53:34,013 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:54:01,726 : [('reg:1e-05', 66.37), ('reg:0.0001', 66.45), ('reg:0.001', 66.71), ('reg:0.01', 65.04)]
2019-03-13 09:54:01,726 : Validation : best param found is reg = 0.001 with score             66.71
2019-03-13 09:54:01,726 : Evaluating...
2019-03-13 09:54:06,878 : 
Dev acc : 66.7 Test acc : 66.6 for ODDMANOUT classification

2019-03-13 09:54:06,879 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 09:54:07,249 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 09:54:07,330 : loading BERT model bert-large-uncased
2019-03-13 09:54:07,331 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:54:07,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:54:07,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdaqmjez5
2019-03-13 09:54:14,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:54:20,085 : Computing embeddings for train/dev/test
2019-03-13 09:57:56,121 : Computed embeddings
2019-03-13 09:57:56,121 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:58:29,478 : [('reg:1e-05', 69.16), ('reg:0.0001', 69.13), ('reg:0.001', 68.71), ('reg:0.01', 65.27)]
2019-03-13 09:58:29,479 : Validation : best param found is reg = 1e-05 with score             69.16
2019-03-13 09:58:29,479 : Evaluating...
2019-03-13 09:58:38,559 : 
Dev acc : 69.2 Test acc : 68.5 for COORDINATIONINVERSION classification

2019-03-13 09:58:38,561 : total results: {'STS12': {'MSRpar': {'pearson': (0.33987057458903824, 9.817259471715311e-22), 'spearman': SpearmanrResult(correlation=0.37749546189579936, pvalue=8.194911760530223e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.46834698886678633, 3.7251860746090467e-42), 'spearman': SpearmanrResult(correlation=0.4817271273980678, pvalue=7.804351214866286e-45), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5208847386272759, 2.7672199716179167e-33), 'spearman': SpearmanrResult(correlation=0.6103935347330892, pvalue=3.3620024252657396e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6105472762974596, 8.24729126540836e-78), 'spearman': SpearmanrResult(correlation=0.6317445115888525, pvalue=8.613001459196494e-85), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6838380566538815, 2.6227408126806903e-56), 'spearman': SpearmanrResult(correlation=0.5627974664527113, pvalue=1.034659869190753e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5246975270068883, 'wmean': 0.5070820815475487}, 'spearman': {'mean': 0.532831620413704, 'wmean': 0.5221853755531402}}}, 'STS13': {'FNWN': {'pearson': (0.34389987284374235, 1.2656217465926716e-06), 'spearman': SpearmanrResult(correlation=0.357801492591803, pvalue=4.3113503899342124e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6159714212131514, 1.5153124145773112e-79), 'spearman': SpearmanrResult(correlation=0.5963699341506296, pvalue=1.9863769770197554e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.46506778264287846, 1.881466813109005e-31), 'spearman': SpearmanrResult(correlation=0.48655429128561034, pvalue=1.112230305900632e-34), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4749796922332574, 'wmean': 0.5252524452933238}, 'spearman': {'mean': 0.48024190600934763, 'wmean': 0.5252392600827002}}}, 'STS14': {'deft-forum': {'pearson': (0.2777155842711035, 2.058237718261336e-09), 'spearman': SpearmanrResult(correlation=0.2975540609777718, pvalue=1.1870318940434867e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.7260768737388098, 2.1420634036584426e-50), 'spearman': SpearmanrResult(correlation=0.6899297750708119, pvalue=1.0093991885204645e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.5588162731663757, 8.115225032985573e-63), 'spearman': SpearmanrResult(correlation=0.5145961524551071, pvalue=6.101102439690085e-52), 'nsamples': 750}, 'images': {'pearson': (0.498076156575902, 2.844679886575895e-48), 'spearman': SpearmanrResult(correlation=0.5005556292227046, pvalue=8.244152226476402e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.6339320726530905, 1.525904369724726e-85), 'spearman': SpearmanrResult(correlation=0.6670709292745884, pvalue=1.0240492225936529e-97), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6596384885322644, 7.50868253565053e-95), 'spearman': SpearmanrResult(correlation=0.6104310753775362, pvalue=8.976969925469717e-78), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5590425748229243, 'wmean': 0.5615046181971637}, 'spearman': {'mean': 0.5466896037297534, 'wmean': 0.5494316265889848}}}, 'STS15': {'answers-forums': {'pearson': (0.5636073109168408, 8.022814160793477e-33), 'spearman': SpearmanrResult(correlation=0.5524227925188147, pvalue=2.4052856414297716e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.7056649029901607, 4.907266177932219e-114), 'spearman': SpearmanrResult(correlation=0.7175581037369915, pvalue=1.2966157084581733e-119), 'nsamples': 750}, 'belief': {'pearson': (0.6314149546196649, 4.133338674160331e-43), 'spearman': SpearmanrResult(correlation=0.6574181975464446, pvalue=8.996460026521681e-48), 'nsamples': 375}, 'headlines': {'pearson': (0.6202282995520384, 6.227691597563021e-81), 'spearman': SpearmanrResult(correlation=0.6114497467012198, pvalue=4.2642342642478538e-78), 'nsamples': 750}, 'images': {'pearson': (0.674590204852144, 1.0616033253463157e-100), 'spearman': SpearmanrResult(correlation=0.6857624278838791, pvalue=2.657029455830918e-105), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6391011345861697, 'wmean': 0.649498635040649}, 'spearman': {'mean': 0.6449222536774699, 'wmean': 0.6549226933386799}}}, 'STS16': {'answer-answer': {'pearson': (0.5242186340080573, 2.478487034904838e-19), 'spearman': SpearmanrResult(correlation=0.544543324638486, pvalue=5.175508075627396e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6531142065344788, 1.1515588558242804e-31), 'spearman': SpearmanrResult(correlation=0.6540243469786843, pvalue=8.898223324305797e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7565157541826422, 6.170082287682358e-44), 'spearman': SpearmanrResult(correlation=0.7663524184365155, pvalue=1.0460602379243098e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.8081621164438203, 1.451862759151014e-57), 'spearman': SpearmanrResult(correlation=0.841912410546012, pvalue=8.866772733720411e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.25906342242562813, 0.00015208346872610338), 'spearman': SpearmanrResult(correlation=0.2574099877422228, pvalue=0.00016814447119044672), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6002148267189253, 'wmean': 0.6080197349295008}, 'spearman': {'mean': 0.6128484976683841, 'wmean': 0.6211234727720218}}}, 'MR': {'devacc': 78.67, 'acc': 77.84, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.32, 'acc': 80.16, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.53, 'acc': 86.07, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.2, 'acc': 94.75, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.14, 'acc': 83.25, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.51, 'acc': 41.95, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.1, 'acc': 94.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.92, 'acc': 73.8, 'f1': 79.23, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.8, 'acc': 78.24, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7899878073780807, 'pearson': 0.8091985614104109, 'spearman': 0.7441180746823515, 'mse': 0.3515561247518121, 'yhat': array([3.84781305, 4.76065595, 1.52086025, ..., 3.05493279, 4.5604348 ,        4.56039564]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7166532532005799, 'pearson': 0.6709550518245768, 'spearman': 0.6644842309131127, 'mse': 1.4907862389791562, 'yhat': array([2.10774246, 1.47827227, 2.28926147, ..., 4.04877544, 3.91959557,        3.31173913]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 69.51, 'acc': 70.1, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 89.61, 'acc': 90.01, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 62.47, 'acc': 62.87, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 39.01, 'acc': 39.99, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 77.86, 'acc': 78.02, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.45, 'acc': 92.47, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.14, 'acc': 87.49, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.55, 'acc': 86.04, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.86, 'acc': 85.03, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.71, 'acc': 66.56, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.16, 'acc': 68.45, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 09:58:38,561 : STS12 p=0.5071, STS12 s=0.5222, STS13 p=0.5253, STS13 s=0.5252, STS14 p=0.5615, STS14 s=0.5494, STS15 p=0.6495, STS15 s=0.6549, STS 16 p=0.6080, STS16 s=0.6211, STS B p=0.6710, STS B s=0.6645, STS B m=1.4908, SICK-R p=0.8092, SICK-R s=0.7441, SICK-P m=0.3516
2019-03-13 09:58:38,561 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 09:58:38,561 : 0.5071,0.5222,0.5253,0.5252,0.5615,0.5494,0.6495,0.6549,0.6080,0.6211,0.6710,0.6645,1.4908,0.8092,0.7441,0.3516
2019-03-13 09:58:38,561 : MR=77.84, CR=80.16, SUBJ=94.75, MPQA=86.07, SST-B=83.25, SST-F=41.95, TREC=94.20, SICK-E=78.24, SNLI=70.10, MRPC=73.80, MRPC f=79.23
2019-03-13 09:58:38,561 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 09:58:38,561 : 77.84,80.16,94.75,86.07,83.25,41.95,94.20,78.24,70.10,73.80,79.23
2019-03-13 09:58:38,561 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 09:58:38,561 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 09:58:38,561 : na,na,na,na,na,na,na,na,na,na
2019-03-13 09:58:38,561 : SentLen=90.01, WC=62.87, TreeDepth=39.99, TopConst=78.02, BShift=92.47, Tense=87.49, SubjNum=86.04, ObjNum=85.03, SOMO=66.56, CoordInv=68.45, average=75.69
2019-03-13 09:58:38,561 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 09:58:38,562 : 90.01,62.87,39.99,78.02,92.47,87.49,86.04,85.03,66.56,68.45,75.69
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,562 : layer 15
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,562 : ********************************************************************************
2019-03-13 09:58:38,653 : ***** Transfer task : STS12 *****


2019-03-13 09:58:38,666 : loading BERT model bert-large-uncased
2019-03-13 09:58:38,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:58:38,683 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:58:38,683 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp17hrkgnp
2019-03-13 09:58:46,161 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:58:55,605 : MSRpar : pearson = 0.3705, spearman = 0.4066
2019-03-13 09:58:57,256 : MSRvid : pearson = 0.5009, spearman = 0.5118
2019-03-13 09:58:58,675 : SMTeuroparl : pearson = 0.5314, spearman = 0.6111
2019-03-13 09:59:01,385 : surprise.OnWN : pearson = 0.6144, spearman = 0.6296
2019-03-13 09:59:02,820 : surprise.SMTnews : pearson = 0.6898, spearman = 0.5802
2019-03-13 09:59:02,820 : ALL (weighted average) : Pearson = 0.5256,             Spearman = 0.5383
2019-03-13 09:59:02,821 : ALL (average) : Pearson = 0.5414,             Spearman = 0.5478

2019-03-13 09:59:02,821 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 09:59:02,829 : loading BERT model bert-large-uncased
2019-03-13 09:59:02,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:02,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:02,847 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfr4gaavh
2019-03-13 09:59:10,330 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:16,967 : FNWN : pearson = 0.3836, spearman = 0.3988
2019-03-13 09:59:18,868 : headlines : pearson = 0.6209, spearman = 0.6003
2019-03-13 09:59:20,342 : OnWN : pearson = 0.4725, spearman = 0.4931
2019-03-13 09:59:20,342 : ALL (weighted average) : Pearson = 0.5355,             Spearman = 0.5348
2019-03-13 09:59:20,343 : ALL (average) : Pearson = 0.4923,             Spearman = 0.4974

2019-03-13 09:59:20,343 : ***** Transfer task : STS14 *****


2019-03-13 09:59:20,359 : loading BERT model bert-large-uncased
2019-03-13 09:59:20,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:20,376 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:20,376 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnhtr43ci
2019-03-13 09:59:27,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:34,530 : deft-forum : pearson = 0.2898, spearman = 0.3041
2019-03-13 09:59:36,174 : deft-news : pearson = 0.7356, spearman = 0.6939
2019-03-13 09:59:38,353 : headlines : pearson = 0.5644, spearman = 0.5193
2019-03-13 09:59:40,438 : images : pearson = 0.5301, spearman = 0.5226
2019-03-13 09:59:42,579 : OnWN : pearson = 0.6448, spearman = 0.6742
2019-03-13 09:59:45,455 : tweet-news : pearson = 0.6712, spearman = 0.6227
2019-03-13 09:59:45,455 : ALL (weighted average) : Pearson = 0.5757,             Spearman = 0.5598
2019-03-13 09:59:45,455 : ALL (average) : Pearson = 0.5727,             Spearman = 0.5562

2019-03-13 09:59:45,455 : ***** Transfer task : STS15 *****


2019-03-13 09:59:45,486 : loading BERT model bert-large-uncased
2019-03-13 09:59:45,486 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:45,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:45,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp__b110un
2019-03-13 09:59:52,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:00:00,192 : answers-forums : pearson = 0.5845, spearman = 0.5782
2019-03-13 10:00:02,286 : answers-students : pearson = 0.7085, spearman = 0.7208
2019-03-13 10:00:04,344 : belief : pearson = 0.6683, spearman = 0.6893
2019-03-13 10:00:06,605 : headlines : pearson = 0.6253, spearman = 0.6162
2019-03-13 10:00:08,747 : images : pearson = 0.6895, spearman = 0.6964
2019-03-13 10:00:08,748 : ALL (weighted average) : Pearson = 0.6624,             Spearman = 0.6668
2019-03-13 10:00:08,748 : ALL (average) : Pearson = 0.6552,             Spearman = 0.6602

2019-03-13 10:00:08,748 : ***** Transfer task : STS16 *****


2019-03-13 10:00:08,818 : loading BERT model bert-large-uncased
2019-03-13 10:00:08,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:00:08,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:00:08,836 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkujbgbyz
2019-03-13 10:00:16,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:00:22,392 : answer-answer : pearson = 0.5857, spearman = 0.6066
2019-03-13 10:00:23,053 : headlines : pearson = 0.6662, spearman = 0.6674
2019-03-13 10:00:23,936 : plagiarism : pearson = 0.7762, spearman = 0.7826
2019-03-13 10:00:25,434 : postediting : pearson = 0.8164, spearman = 0.8437
2019-03-13 10:00:26,039 : question-question : pearson = 0.2937, spearman = 0.2812
2019-03-13 10:00:26,039 : ALL (weighted average) : Pearson = 0.6355,             Spearman = 0.6449
2019-03-13 10:00:26,039 : ALL (average) : Pearson = 0.6276,             Spearman = 0.6363

2019-03-13 10:00:26,039 : ***** Transfer task : MR *****


2019-03-13 10:00:26,055 : loading BERT model bert-large-uncased
2019-03-13 10:00:26,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:00:26,109 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:00:26,110 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppq4jjc0d
2019-03-13 10:00:33,562 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:00:38,791 : Generating sentence embeddings
2019-03-13 10:01:10,380 : Generated sentence embeddings
2019-03-13 10:01:10,381 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:01:20,293 : Best param found at split 1: l2reg = 0.001                 with score 79.87
2019-03-13 10:01:30,089 : Best param found at split 2: l2reg = 0.0001                 with score 79.77
2019-03-13 10:01:38,944 : Best param found at split 3: l2reg = 1e-05                 with score 79.61
2019-03-13 10:01:46,520 : Best param found at split 4: l2reg = 0.01                 with score 79.65
2019-03-13 10:01:55,019 : Best param found at split 5: l2reg = 0.01                 with score 79.41
2019-03-13 10:01:55,461 : Dev acc : 79.66 Test acc : 78.62

2019-03-13 10:01:55,462 : ***** Transfer task : CR *****


2019-03-13 10:01:55,470 : loading BERT model bert-large-uncased
2019-03-13 10:01:55,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:01:55,489 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:01:55,489 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphjblm628
2019-03-13 10:02:02,911 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:02:08,192 : Generating sentence embeddings
2019-03-13 10:02:16,545 : Generated sentence embeddings
2019-03-13 10:02:16,545 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:02:19,315 : Best param found at split 1: l2reg = 0.001                 with score 84.2
2019-03-13 10:02:22,170 : Best param found at split 2: l2reg = 0.001                 with score 84.43
2019-03-13 10:02:25,802 : Best param found at split 3: l2reg = 0.001                 with score 84.77
2019-03-13 10:02:28,541 : Best param found at split 4: l2reg = 1e-05                 with score 84.21
2019-03-13 10:02:31,607 : Best param found at split 5: l2reg = 0.001                 with score 84.84
2019-03-13 10:02:31,796 : Dev acc : 84.49 Test acc : 83.71

2019-03-13 10:02:31,797 : ***** Transfer task : MPQA *****


2019-03-13 10:02:31,836 : loading BERT model bert-large-uncased
2019-03-13 10:02:31,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:02:31,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:02:31,854 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqun30st8
2019-03-13 10:02:39,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:02:44,565 : Generating sentence embeddings
2019-03-13 10:02:52,178 : Generated sentence embeddings
2019-03-13 10:02:52,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:03:02,549 : Best param found at split 1: l2reg = 0.0001                 with score 84.94
2019-03-13 10:03:13,446 : Best param found at split 2: l2reg = 1e-05                 with score 85.36
2019-03-13 10:03:23,971 : Best param found at split 3: l2reg = 0.001                 with score 85.94
2019-03-13 10:03:34,988 : Best param found at split 4: l2reg = 1e-05                 with score 85.46
2019-03-13 10:03:44,820 : Best param found at split 5: l2reg = 1e-05                 with score 84.08
2019-03-13 10:03:45,461 : Dev acc : 85.16 Test acc : 85.64

2019-03-13 10:03:45,462 : ***** Transfer task : SUBJ *****


2019-03-13 10:03:45,479 : loading BERT model bert-large-uncased
2019-03-13 10:03:45,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:03:45,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:03:45,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo9pcdo7j
2019-03-13 10:03:53,009 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:03:58,310 : Generating sentence embeddings
2019-03-13 10:04:29,259 : Generated sentence embeddings
2019-03-13 10:04:29,259 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:04:39,043 : Best param found at split 1: l2reg = 0.001                 with score 95.44
2019-03-13 10:04:47,549 : Best param found at split 2: l2reg = 0.001                 with score 95.68
2019-03-13 10:04:57,548 : Best param found at split 3: l2reg = 1e-05                 with score 95.25
2019-03-13 10:05:07,545 : Best param found at split 4: l2reg = 0.001                 with score 95.79
2019-03-13 10:05:16,648 : Best param found at split 5: l2reg = 0.001                 with score 95.45
2019-03-13 10:05:17,065 : Dev acc : 95.52 Test acc : 95.05

2019-03-13 10:05:17,066 : ***** Transfer task : SST Binary classification *****


2019-03-13 10:05:17,208 : loading BERT model bert-large-uncased
2019-03-13 10:05:17,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:05:17,231 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:05:17,231 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp65dy3rqf
2019-03-13 10:05:24,635 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:05:29,831 : Computing embedding for train
2019-03-13 10:07:10,207 : Computed train embeddings
2019-03-13 10:07:10,207 : Computing embedding for dev
2019-03-13 10:07:12,396 : Computed dev embeddings
2019-03-13 10:07:12,396 : Computing embedding for test
2019-03-13 10:07:17,005 : Computed test embeddings
2019-03-13 10:07:17,005 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:07:38,244 : [('reg:1e-05', 82.91), ('reg:0.0001', 82.91), ('reg:0.001', 82.91), ('reg:0.01', 82.91)]
2019-03-13 10:07:38,244 : Validation : best param found is reg = 1e-05 with score             82.91
2019-03-13 10:07:38,244 : Evaluating...
2019-03-13 10:07:44,070 : 
Dev acc : 82.91 Test acc : 83.58 for             SST Binary classification

2019-03-13 10:07:44,070 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 10:07:44,121 : loading BERT model bert-large-uncased
2019-03-13 10:07:44,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:07:44,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:07:44,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvtau0d_5
2019-03-13 10:07:51,594 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:07:56,776 : Computing embedding for train
2019-03-13 10:08:18,683 : Computed train embeddings
2019-03-13 10:08:18,683 : Computing embedding for dev
2019-03-13 10:08:21,547 : Computed dev embeddings
2019-03-13 10:08:21,547 : Computing embedding for test
2019-03-13 10:08:27,197 : Computed test embeddings
2019-03-13 10:08:27,197 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:08:29,440 : [('reg:1e-05', 41.87), ('reg:0.0001', 41.87), ('reg:0.001', 42.51), ('reg:0.01', 41.51)]
2019-03-13 10:08:29,440 : Validation : best param found is reg = 0.001 with score             42.51
2019-03-13 10:08:29,440 : Evaluating...
2019-03-13 10:08:29,992 : 
Dev acc : 42.51 Test acc : 44.12 for             SST Fine-Grained classification

2019-03-13 10:08:29,993 : ***** Transfer task : TREC *****


2019-03-13 10:08:30,006 : loading BERT model bert-large-uncased
2019-03-13 10:08:30,006 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:08:30,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:08:30,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd4yjjs41
2019-03-13 10:08:37,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:08:50,372 : Computed train embeddings
2019-03-13 10:08:50,963 : Computed test embeddings
2019-03-13 10:08:50,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:08:58,131 : [('reg:1e-05', 85.84), ('reg:0.0001', 85.54), ('reg:0.001', 83.27), ('reg:0.01', 77.61)]
2019-03-13 10:08:58,132 : Cross-validation : best param found is reg = 1e-05             with score 85.84
2019-03-13 10:08:58,132 : Evaluating...
2019-03-13 10:08:58,749 : 
Dev acc : 85.84 Test acc : 94.0             for TREC

2019-03-13 10:08:58,750 : ***** Transfer task : MRPC *****


2019-03-13 10:08:58,800 : loading BERT model bert-large-uncased
2019-03-13 10:08:58,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:08:58,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:08:58,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8sm0rnm_
2019-03-13 10:09:06,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:09:11,590 : Computing embedding for train
2019-03-13 10:09:33,893 : Computed train embeddings
2019-03-13 10:09:33,893 : Computing embedding for test
2019-03-13 10:09:43,662 : Computed test embeddings
2019-03-13 10:09:43,684 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:09:48,114 : [('reg:1e-05', 74.73), ('reg:0.0001', 74.7), ('reg:0.001', 74.58), ('reg:0.01', 73.45)]
2019-03-13 10:09:48,115 : Cross-validation : best param found is reg = 1e-05             with score 74.73
2019-03-13 10:09:48,115 : Evaluating...
2019-03-13 10:09:48,425 : Dev acc : 74.73 Test acc 74.55; Test F1 80.11 for MRPC.

2019-03-13 10:09:48,425 : ***** Transfer task : SICK-Entailment*****


2019-03-13 10:09:48,449 : loading BERT model bert-large-uncased
2019-03-13 10:09:48,449 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:09:48,505 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:09:48,505 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpugtkkkvr
2019-03-13 10:09:55,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:10:01,215 : Computing embedding for train
2019-03-13 10:10:12,549 : Computed train embeddings
2019-03-13 10:10:12,549 : Computing embedding for dev
2019-03-13 10:10:14,095 : Computed dev embeddings
2019-03-13 10:10:14,095 : Computing embedding for test
2019-03-13 10:10:26,240 : Computed test embeddings
2019-03-13 10:10:26,277 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:10:27,759 : [('reg:1e-05', 79.6), ('reg:0.0001', 78.8), ('reg:0.001', 79.8), ('reg:0.01', 77.8)]
2019-03-13 10:10:27,759 : Validation : best param found is reg = 0.001 with score             79.8
2019-03-13 10:10:27,759 : Evaluating...
2019-03-13 10:10:28,111 : 
Dev acc : 79.8 Test acc : 77.73 for                        SICK entailment

2019-03-13 10:10:28,112 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 10:10:28,139 : loading BERT model bert-large-uncased
2019-03-13 10:10:28,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:10:28,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:10:28,158 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmvf6okqr
2019-03-13 10:10:35,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:10:40,822 : Computing embedding for train
2019-03-13 10:10:52,163 : Computed train embeddings
2019-03-13 10:10:52,163 : Computing embedding for dev
2019-03-13 10:10:53,708 : Computed dev embeddings
2019-03-13 10:10:53,708 : Computing embedding for test
2019-03-13 10:11:05,918 : Computed test embeddings
2019-03-13 10:11:22,276 : Dev : Pearson 0.8096067833834681
2019-03-13 10:11:22,276 : Test : Pearson 0.8027287940707535 Spearman 0.7361870867504796 MSE 0.36194869599084717                        for SICK Relatedness

2019-03-13 10:11:22,277 : 

***** Transfer task : STSBenchmark*****


2019-03-13 10:11:22,315 : loading BERT model bert-large-uncased
2019-03-13 10:11:22,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:11:22,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:11:22,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpudyyl6mg
2019-03-13 10:11:29,774 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:11:34,993 : Computing embedding for train
2019-03-13 10:11:53,625 : Computed train embeddings
2019-03-13 10:11:53,625 : Computing embedding for dev
2019-03-13 10:11:59,275 : Computed dev embeddings
2019-03-13 10:11:59,275 : Computing embedding for test
2019-03-13 10:12:03,906 : Computed test embeddings
2019-03-13 10:12:22,751 : Dev : Pearson 0.7249477427395513
2019-03-13 10:12:22,752 : Test : Pearson 0.6767919742600336 Spearman 0.6724063129857487 MSE 1.4364472664746044                        for SICK Relatedness

2019-03-13 10:12:22,752 : ***** Transfer task : SNLI Entailment*****


2019-03-13 10:12:27,541 : loading BERT model bert-large-uncased
2019-03-13 10:12:27,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:12:27,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:12:27,662 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaappd488
2019-03-13 10:12:35,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:12:40,866 : PROGRESS (encoding): 0.00%
2019-03-13 10:15:27,163 : PROGRESS (encoding): 14.56%
2019-03-13 10:18:37,339 : PROGRESS (encoding): 29.12%
2019-03-13 10:21:47,780 : PROGRESS (encoding): 43.69%
2019-03-13 10:25:10,586 : PROGRESS (encoding): 58.25%
2019-03-13 10:28:56,488 : PROGRESS (encoding): 72.81%
2019-03-13 10:32:41,150 : PROGRESS (encoding): 87.37%
2019-03-13 10:36:44,273 : PROGRESS (encoding): 0.00%
2019-03-13 10:37:14,939 : PROGRESS (encoding): 0.00%
2019-03-13 10:37:44,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:38:15,059 : [('reg:1e-09', 70.27)]
2019-03-13 10:38:15,059 : Validation : best param found is reg = 1e-09 with score             70.27
2019-03-13 10:38:15,059 : Evaluating...
2019-03-13 10:38:44,971 : Dev acc : 70.27 Test acc : 70.98 for SNLI

2019-03-13 10:38:44,971 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 10:38:45,185 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 10:38:46,192 : loading BERT model bert-large-uncased
2019-03-13 10:38:46,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:38:46,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:38:46,218 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb9pafgy5
2019-03-13 10:38:53,659 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:38:59,015 : Computing embeddings for train/dev/test
2019-03-13 10:42:31,099 : Computed embeddings
2019-03-13 10:42:31,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:43:00,769 : [('reg:1e-05', 87.15), ('reg:0.0001', 86.45), ('reg:0.001', 80.84), ('reg:0.01', 68.59)]
2019-03-13 10:43:00,769 : Validation : best param found is reg = 1e-05 with score             87.15
2019-03-13 10:43:00,769 : Evaluating...
2019-03-13 10:43:08,264 : 
Dev acc : 87.2 Test acc : 87.9 for LENGTH classification

2019-03-13 10:43:08,265 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 10:43:08,618 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 10:43:08,667 : loading BERT model bert-large-uncased
2019-03-13 10:43:08,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:43:08,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:43:08,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw0hh6o0i
2019-03-13 10:43:16,157 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:43:21,844 : Computing embeddings for train/dev/test
2019-03-13 10:46:37,042 : Computed embeddings
2019-03-13 10:46:37,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:47:11,474 : [('reg:1e-05', 60.33), ('reg:0.0001', 34.11), ('reg:0.001', 4.09), ('reg:0.01', 1.04)]
2019-03-13 10:47:11,474 : Validation : best param found is reg = 1e-05 with score             60.33
2019-03-13 10:47:11,474 : Evaluating...
2019-03-13 10:47:22,458 : 
Dev acc : 60.3 Test acc : 61.3 for WORDCONTENT classification

2019-03-13 10:47:22,459 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 10:47:22,816 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 10:47:22,882 : loading BERT model bert-large-uncased
2019-03-13 10:47:22,883 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:47:22,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:47:22,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp54djs_iy
2019-03-13 10:47:30,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:47:35,572 : Computing embeddings for train/dev/test
2019-03-13 10:50:39,179 : Computed embeddings
2019-03-13 10:50:39,180 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:50:59,087 : [('reg:1e-05', 38.58), ('reg:0.0001', 38.41), ('reg:0.001', 36.84), ('reg:0.01', 31.31)]
2019-03-13 10:50:59,087 : Validation : best param found is reg = 1e-05 with score             38.58
2019-03-13 10:50:59,087 : Evaluating...
2019-03-13 10:51:04,490 : 
Dev acc : 38.6 Test acc : 38.4 for DEPTH classification

2019-03-13 10:51:04,491 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 10:51:04,869 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 10:51:04,932 : loading BERT model bert-large-uncased
2019-03-13 10:51:04,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:51:05,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:51:05,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp34__jhjl
2019-03-13 10:51:12,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:51:17,706 : Computing embeddings for train/dev/test
2019-03-13 10:54:08,135 : Computed embeddings
2019-03-13 10:54:08,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:54:34,626 : [('reg:1e-05', 76.79), ('reg:0.0001', 75.32), ('reg:0.001', 68.85), ('reg:0.01', 60.35)]
2019-03-13 10:54:34,626 : Validation : best param found is reg = 1e-05 with score             76.79
2019-03-13 10:54:34,626 : Evaluating...
2019-03-13 10:54:41,235 : 
Dev acc : 76.8 Test acc : 77.3 for TOPCONSTITUENTS classification

2019-03-13 10:54:41,236 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 10:54:41,580 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 10:54:41,646 : loading BERT model bert-large-uncased
2019-03-13 10:54:41,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:54:41,763 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:54:41,764 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz5st0y1o
2019-03-13 10:54:49,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:54:54,554 : Computing embeddings for train/dev/test
2019-03-13 10:57:59,570 : Computed embeddings
2019-03-13 10:57:59,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:58:29,062 : [('reg:1e-05', 92.69), ('reg:0.0001', 92.73), ('reg:0.001', 92.56), ('reg:0.01', 91.54)]
2019-03-13 10:58:29,062 : Validation : best param found is reg = 0.0001 with score             92.73
2019-03-13 10:58:29,062 : Evaluating...
2019-03-13 10:58:36,645 : 
Dev acc : 92.7 Test acc : 92.5 for BIGRAMSHIFT classification

2019-03-13 10:58:36,646 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 10:58:37,200 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 10:58:37,265 : loading BERT model bert-large-uncased
2019-03-13 10:58:37,265 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:58:37,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:58:37,295 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprn1ih14x
2019-03-13 10:58:44,785 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:58:50,086 : Computing embeddings for train/dev/test
2019-03-13 11:01:50,701 : Computed embeddings
2019-03-13 11:01:50,701 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:02:17,478 : [('reg:1e-05', 89.3), ('reg:0.0001', 89.33), ('reg:0.001', 89.39), ('reg:0.01', 88.38)]
2019-03-13 11:02:17,478 : Validation : best param found is reg = 0.001 with score             89.39
2019-03-13 11:02:17,478 : Evaluating...
2019-03-13 11:02:23,899 : 
Dev acc : 89.4 Test acc : 88.2 for TENSE classification

2019-03-13 11:02:23,900 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 11:02:24,289 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 11:02:24,356 : loading BERT model bert-large-uncased
2019-03-13 11:02:24,356 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:02:24,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:02:24,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9rmfvrni
2019-03-13 11:02:31,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:02:37,154 : Computing embeddings for train/dev/test
2019-03-13 11:05:48,294 : Computed embeddings
2019-03-13 11:05:48,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:06:22,073 : [('reg:1e-05', 87.09), ('reg:0.0001', 87.12), ('reg:0.001', 87.05), ('reg:0.01', 86.04)]
2019-03-13 11:06:22,073 : Validation : best param found is reg = 0.0001 with score             87.12
2019-03-13 11:06:22,073 : Evaluating...
2019-03-13 11:06:30,415 : 
Dev acc : 87.1 Test acc : 87.4 for SUBJNUMBER classification

2019-03-13 11:06:30,416 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 11:06:30,826 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 11:06:30,892 : loading BERT model bert-large-uncased
2019-03-13 11:06:30,892 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:06:30,921 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:06:30,921 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1pln00f1
2019-03-13 11:06:38,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:06:43,672 : Computing embeddings for train/dev/test
2019-03-13 11:09:51,402 : Computed embeddings
2019-03-13 11:09:51,402 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:10:11,535 : [('reg:1e-05', 83.66), ('reg:0.0001', 83.71), ('reg:0.001', 83.66), ('reg:0.01', 82.59)]
2019-03-13 11:10:11,535 : Validation : best param found is reg = 0.0001 with score             83.71
2019-03-13 11:10:11,535 : Evaluating...
2019-03-13 11:10:16,876 : 
Dev acc : 83.7 Test acc : 84.6 for OBJNUMBER classification

2019-03-13 11:10:16,877 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 11:10:17,287 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 11:10:17,359 : loading BERT model bert-large-uncased
2019-03-13 11:10:17,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:10:17,494 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:10:17,494 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp669sqacz
2019-03-13 11:10:24,929 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:10:30,255 : Computing embeddings for train/dev/test
2019-03-13 11:14:08,266 : Computed embeddings
2019-03-13 11:14:08,267 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:14:37,846 : [('reg:1e-05', 66.28), ('reg:0.0001', 66.25), ('reg:0.001', 66.57), ('reg:0.01', 66.09)]
2019-03-13 11:14:37,847 : Validation : best param found is reg = 0.001 with score             66.57
2019-03-13 11:14:37,847 : Evaluating...
2019-03-13 11:14:44,184 : 
Dev acc : 66.6 Test acc : 66.5 for ODDMANOUT classification

2019-03-13 11:14:44,185 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 11:14:44,583 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 11:14:44,660 : loading BERT model bert-large-uncased
2019-03-13 11:14:44,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:14:44,785 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:14:44,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkc95sncx
2019-03-13 11:14:52,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:14:57,509 : Computing embeddings for train/dev/test
2019-03-13 11:18:33,960 : Computed embeddings
2019-03-13 11:18:33,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:18:58,871 : [('reg:1e-05', 71.98), ('reg:0.0001', 71.87), ('reg:0.001', 71.68), ('reg:0.01', 69.57)]
2019-03-13 11:18:58,872 : Validation : best param found is reg = 1e-05 with score             71.98
2019-03-13 11:18:58,872 : Evaluating...
2019-03-13 11:19:06,399 : 
Dev acc : 72.0 Test acc : 71.8 for COORDINATIONINVERSION classification

2019-03-13 11:19:06,401 : total results: {'STS12': {'MSRpar': {'pearson': (0.37048492916799763, 8.161594164102186e-26), 'spearman': SpearmanrResult(correlation=0.40655642438320877, pvalue=3.2175974648345584e-31), 'nsamples': 750}, 'MSRvid': {'pearson': (0.500856151669345, 7.090079892904679e-49), 'spearman': SpearmanrResult(correlation=0.5117955782069227, pvalue=2.639844994328035e-51), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5314334644478494, 8.12983851485068e-35), 'spearman': SpearmanrResult(correlation=0.6111192684828852, pvalue=2.4309494918622443e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6144180007516277, 4.798174937995862e-79), 'spearman': SpearmanrResult(correlation=0.6295662557883331, pvalue=4.759970066251389e-84), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6897827055286737, 1.2091940999224167e-57), 'spearman': SpearmanrResult(correlation=0.5801904305303464, pvalue=2.859506562208706e-37), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5413950503130988, 'wmean': 0.5255696817500745}, 'spearman': {'mean': 0.5478455914783392, 'wmean': 0.5382684748388356}}}, 'STS13': {'FNWN': {'pearson': (0.3836003039883796, 5.078758303480767e-08), 'spearman': SpearmanrResult(correlation=0.3987688712327352, pvalue=1.3210895939303869e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6209394134100865, 3.6364805741049465e-81), 'spearman': SpearmanrResult(correlation=0.6003125237892133, pvalue=1.2633500307145086e-74), 'nsamples': 750}, 'OnWN': {'pearson': (0.4724669490725089, 1.5435890895735843e-32), 'spearman': SpearmanrResult(correlation=0.4930669121747537, pvalue=1.0510326568979972e-35), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.492335555490325, 'wmean': 0.5355059839606975}, 'spearman': {'mean': 0.4973827690655674, 'wmean': 0.5348081648232892}}}, 'STS14': {'deft-forum': {'pearson': (0.2897740049116306, 3.731832217420194e-10), 'spearman': SpearmanrResult(correlation=0.3041311905393117, pvalue=4.386244467005709e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7355883072395316, 2.4773760219180284e-52), 'spearman': SpearmanrResult(correlation=0.6938677591188743, pvalue=2.1141831920348042e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5644390397502249, 2.549391324075956e-64), 'spearman': SpearmanrResult(correlation=0.5193448358916076, pvalue=4.932667952572013e-53), 'nsamples': 750}, 'images': {'pearson': (0.5301046881285022, 1.4227799666663183e-55), 'spearman': SpearmanrResult(correlation=0.5226470606224831, pvalue=8.379882807101435e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.6448149092446046, 2.2513238833326362e-89), 'spearman': SpearmanrResult(correlation=0.6741876180198612, pvalue=1.5416682722260438e-100), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6712013912424526, 2.4091507364369946e-99), 'spearman': SpearmanrResult(correlation=0.6227372558621241, pvalue=9.27438648966549e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5726537234194912, 'wmean': 0.575731950841715}, 'spearman': {'mean': 0.5561526200090436, 'wmean': 0.5597885176734425}}}, 'STS15': {'answers-forums': {'pearson': (0.5844659412080561, 9.888471413366385e-36), 'spearman': SpearmanrResult(correlation=0.578232683135729, pvalue=7.698633789918877e-35), 'nsamples': 375}, 'answers-students': {'pearson': (0.7084966642765405, 2.445119066682815e-115), 'spearman': SpearmanrResult(correlation=0.7208468315869175, pvalue=3.30502241044569e-121), 'nsamples': 750}, 'belief': {'pearson': (0.6683207193283001, 7.2192529945888e-50), 'spearman': SpearmanrResult(correlation=0.689256273959539, pvalue=3.7532159293574194e-54), 'nsamples': 375}, 'headlines': {'pearson': (0.6253484596462481, 1.2545945645135343e-82), 'spearman': SpearmanrResult(correlation=0.6162446196634044, pvalue=1.236452176688313e-79), 'nsamples': 750}, 'images': {'pearson': (0.6894585029228747, 7.178294336642364e-107), 'spearman': SpearmanrResult(correlation=0.6963946444042491, pvalue=7.049525956880822e-110), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6552180574764038, 'wmean': 0.6624242392784603}, 'spearman': {'mean': 0.6601950105499678, 'wmean': 0.6668076435505512}}}, 'STS16': {'answer-answer': {'pearson': (0.585739926466757, 8.620337790006376e-25), 'spearman': SpearmanrResult(correlation=0.6065540346446999, pvalue=6.507049543828022e-27), 'nsamples': 254}, 'headlines': {'pearson': (0.6661552383517148, 2.624403060626877e-33), 'spearman': SpearmanrResult(correlation=0.6674109606174363, pvalue=1.8051392302515537e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7761871780603501, 1.4450864910857043e-47), 'spearman': SpearmanrResult(correlation=0.7825511969520959, pvalue=8.040561294019856e-49), 'nsamples': 230}, 'postediting': {'pearson': (0.8164189809346806, 1.2164251708002501e-59), 'spearman': SpearmanrResult(correlation=0.8437179406645913, pvalue=2.4814491354002456e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.29368534990539313, 1.5898826697184584e-05), 'spearman': SpearmanrResult(correlation=0.28123986986723193, pvalue=3.707014637566212e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6276373347437791, 'wmean': 0.6355481586039655}, 'spearman': {'mean': 0.6362948005492111, 'wmean': 0.644926761902942}}}, 'MR': {'devacc': 79.66, 'acc': 78.62, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 84.49, 'acc': 83.71, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.16, 'acc': 85.64, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.52, 'acc': 95.05, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.91, 'acc': 83.58, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 44.12, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 85.84, 'acc': 94.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.73, 'acc': 74.55, 'f1': 80.11, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.8, 'acc': 77.73, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8096067833834681, 'pearson': 0.8027287940707535, 'spearman': 0.7361870867504796, 'mse': 0.36194869599084717, 'yhat': array([4.16589376, 4.61979593, 1.38839046, ..., 3.16112818, 4.63689652,        4.23084298]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7249477427395513, 'pearson': 0.6767919742600336, 'spearman': 0.6724063129857487, 'mse': 1.4364472664746044, 'yhat': array([1.71489129, 1.32462052, 2.78339806, ..., 4.04414733, 3.99988036,        3.37530977]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 70.27, 'acc': 70.98, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 87.15, 'acc': 87.9, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 60.33, 'acc': 61.34, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.58, 'acc': 38.35, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.79, 'acc': 77.31, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.73, 'acc': 92.5, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.39, 'acc': 88.18, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.12, 'acc': 87.37, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.71, 'acc': 84.6, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.57, 'acc': 66.52, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.98, 'acc': 71.79, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 11:19:06,401 : STS12 p=0.5256, STS12 s=0.5383, STS13 p=0.5355, STS13 s=0.5348, STS14 p=0.5757, STS14 s=0.5598, STS15 p=0.6624, STS15 s=0.6668, STS 16 p=0.6355, STS16 s=0.6449, STS B p=0.6768, STS B s=0.6724, STS B m=1.4364, SICK-R p=0.8027, SICK-R s=0.7362, SICK-P m=0.3619
2019-03-13 11:19:06,401 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 11:19:06,401 : 0.5256,0.5383,0.5355,0.5348,0.5757,0.5598,0.6624,0.6668,0.6355,0.6449,0.6768,0.6724,1.4364,0.8027,0.7362,0.3619
2019-03-13 11:19:06,401 : MR=78.62, CR=83.71, SUBJ=95.05, MPQA=85.64, SST-B=83.58, SST-F=44.12, TREC=94.00, SICK-E=77.73, SNLI=70.98, MRPC=74.55, MRPC f=80.11
2019-03-13 11:19:06,401 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 11:19:06,401 : 78.62,83.71,95.05,85.64,83.58,44.12,94.00,77.73,70.98,74.55,80.11
2019-03-13 11:19:06,401 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 11:19:06,401 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 11:19:06,401 : na,na,na,na,na,na,na,na,na,na
2019-03-13 11:19:06,401 : SentLen=87.90, WC=61.34, TreeDepth=38.35, TopConst=77.31, BShift=92.50, Tense=88.18, SubjNum=87.37, ObjNum=84.60, SOMO=66.52, CoordInv=71.79, average=75.59
2019-03-13 11:19:06,401 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 11:19:06,401 : 87.90,61.34,38.35,77.31,92.50,88.18,87.37,84.60,66.52,71.79,75.59
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,402 : layer 16
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,402 : ********************************************************************************
2019-03-13 11:19:06,495 : ***** Transfer task : STS12 *****


2019-03-13 11:19:06,533 : loading BERT model bert-large-uncased
2019-03-13 11:19:06,533 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:19:06,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:19:06,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4pndbgnr
2019-03-13 11:19:13,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:19:23,302 : MSRpar : pearson = 0.3738, spearman = 0.4140
2019-03-13 11:19:24,953 : MSRvid : pearson = 0.4108, spearman = 0.4346
2019-03-13 11:19:26,373 : SMTeuroparl : pearson = 0.5550, spearman = 0.6271
2019-03-13 11:19:29,090 : surprise.OnWN : pearson = 0.5756, spearman = 0.5963
2019-03-13 11:19:30,529 : surprise.SMTnews : pearson = 0.6964, spearman = 0.5944
2019-03-13 11:19:30,529 : ALL (weighted average) : Pearson = 0.4996,             Spearman = 0.5176
2019-03-13 11:19:30,529 : ALL (average) : Pearson = 0.5223,             Spearman = 0.5333

2019-03-13 11:19:30,529 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 11:19:30,539 : loading BERT model bert-large-uncased
2019-03-13 11:19:30,539 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:19:30,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:19:30,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp06qdg5a0
2019-03-13 11:19:37,997 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:19:44,695 : FNWN : pearson = 0.3374, spearman = 0.3565
2019-03-13 11:19:46,599 : headlines : pearson = 0.6069, spearman = 0.5861
2019-03-13 11:19:48,075 : OnWN : pearson = 0.4392, spearman = 0.4545
2019-03-13 11:19:48,075 : ALL (weighted average) : Pearson = 0.5102,             Spearman = 0.5080
2019-03-13 11:19:48,075 : ALL (average) : Pearson = 0.4611,             Spearman = 0.4657

2019-03-13 11:19:48,075 : ***** Transfer task : STS14 *****


2019-03-13 11:19:48,119 : loading BERT model bert-large-uncased
2019-03-13 11:19:48,119 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:19:48,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:19:48,137 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu0myequ0
2019-03-13 11:19:55,575 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:02,313 : deft-forum : pearson = 0.2561, spearman = 0.2862
2019-03-13 11:20:03,959 : deft-news : pearson = 0.7327, spearman = 0.6874
2019-03-13 11:20:06,144 : headlines : pearson = 0.5482, spearman = 0.5090
2019-03-13 11:20:08,230 : images : pearson = 0.4553, spearman = 0.4527
2019-03-13 11:20:10,370 : OnWN : pearson = 0.6137, spearman = 0.6390
2019-03-13 11:20:13,252 : tweet-news : pearson = 0.6525, spearman = 0.6086
2019-03-13 11:20:13,252 : ALL (weighted average) : Pearson = 0.5433,             Spearman = 0.5312
2019-03-13 11:20:13,252 : ALL (average) : Pearson = 0.5431,             Spearman = 0.5305

2019-03-13 11:20:13,253 : ***** Transfer task : STS15 *****


2019-03-13 11:20:13,284 : loading BERT model bert-large-uncased
2019-03-13 11:20:13,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:13,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:13,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvdwlahlh
2019-03-13 11:20:20,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:27,943 : answers-forums : pearson = 0.5679, spearman = 0.5636
2019-03-13 11:20:30,043 : answers-students : pearson = 0.6811, spearman = 0.7020
2019-03-13 11:20:32,100 : belief : pearson = 0.6468, spearman = 0.6700
2019-03-13 11:20:34,364 : headlines : pearson = 0.6118, spearman = 0.6052
2019-03-13 11:20:36,512 : images : pearson = 0.5859, spearman = 0.5949
2019-03-13 11:20:36,512 : ALL (weighted average) : Pearson = 0.6215,             Spearman = 0.6297
2019-03-13 11:20:36,512 : ALL (average) : Pearson = 0.6187,             Spearman = 0.6271

2019-03-13 11:20:36,512 : ***** Transfer task : STS16 *****


2019-03-13 11:20:36,550 : loading BERT model bert-large-uncased
2019-03-13 11:20:36,550 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:36,568 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:36,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp16vwim5t
2019-03-13 11:20:44,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:50,197 : answer-answer : pearson = 0.5468, spearman = 0.5605
2019-03-13 11:20:50,860 : headlines : pearson = 0.6620, spearman = 0.6644
2019-03-13 11:20:51,748 : plagiarism : pearson = 0.7663, spearman = 0.7717
2019-03-13 11:20:53,247 : postediting : pearson = 0.8023, spearman = 0.8346
2019-03-13 11:20:53,856 : question-question : pearson = 0.2503, spearman = 0.2363
2019-03-13 11:20:53,856 : ALL (weighted average) : Pearson = 0.6139,             Spearman = 0.6225
2019-03-13 11:20:53,856 : ALL (average) : Pearson = 0.6055,             Spearman = 0.6135

2019-03-13 11:20:53,856 : ***** Transfer task : MR *****


2019-03-13 11:20:53,902 : loading BERT model bert-large-uncased
2019-03-13 11:20:53,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:53,923 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:53,923 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc3hflda8
2019-03-13 11:21:01,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:21:06,753 : Generating sentence embeddings
2019-03-13 11:21:38,359 : Generated sentence embeddings
2019-03-13 11:21:38,359 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:21:46,910 : Best param found at split 1: l2reg = 0.001                 with score 81.16
2019-03-13 11:21:58,355 : Best param found at split 2: l2reg = 1e-05                 with score 81.04
2019-03-13 11:22:09,821 : Best param found at split 3: l2reg = 0.001                 with score 81.76
2019-03-13 11:22:19,813 : Best param found at split 4: l2reg = 0.0001                 with score 81.27
2019-03-13 11:22:31,345 : Best param found at split 5: l2reg = 0.001                 with score 81.38
2019-03-13 11:22:31,924 : Dev acc : 81.32 Test acc : 79.82

2019-03-13 11:22:31,925 : ***** Transfer task : CR *****


2019-03-13 11:22:31,932 : loading BERT model bert-large-uncased
2019-03-13 11:22:31,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:22:31,984 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:22:31,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprph2knz7
2019-03-13 11:22:39,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:22:44,814 : Generating sentence embeddings
2019-03-13 11:22:53,161 : Generated sentence embeddings
2019-03-13 11:22:53,161 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:22:56,233 : Best param found at split 1: l2reg = 1e-05                 with score 87.02
2019-03-13 11:22:59,113 : Best param found at split 2: l2reg = 0.001                 with score 87.25
2019-03-13 11:23:02,024 : Best param found at split 3: l2reg = 0.001                 with score 87.85
2019-03-13 11:23:04,959 : Best param found at split 4: l2reg = 0.001                 with score 86.69
2019-03-13 11:23:08,724 : Best param found at split 5: l2reg = 1e-05                 with score 87.45
2019-03-13 11:23:08,930 : Dev acc : 87.25 Test acc : 86.62

2019-03-13 11:23:08,931 : ***** Transfer task : MPQA *****


2019-03-13 11:23:08,936 : loading BERT model bert-large-uncased
2019-03-13 11:23:08,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:23:08,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:23:08,956 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo3hth8y9
2019-03-13 11:23:16,377 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:23:21,705 : Generating sentence embeddings
2019-03-13 11:23:29,317 : Generated sentence embeddings
2019-03-13 11:23:29,317 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:23:38,288 : Best param found at split 1: l2reg = 1e-05                 with score 83.63
2019-03-13 11:23:49,169 : Best param found at split 2: l2reg = 1e-05                 with score 85.64
2019-03-13 11:23:57,847 : Best param found at split 3: l2reg = 0.001                 with score 85.2
2019-03-13 11:24:07,519 : Best param found at split 4: l2reg = 0.001                 with score 84.16
2019-03-13 11:24:15,367 : Best param found at split 5: l2reg = 0.0001                 with score 84.39
2019-03-13 11:24:15,825 : Dev acc : 84.6 Test acc : 85.78

2019-03-13 11:24:15,825 : ***** Transfer task : SUBJ *****


2019-03-13 11:24:15,841 : loading BERT model bert-large-uncased
2019-03-13 11:24:15,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:24:15,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:24:15,896 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv89hoycz
2019-03-13 11:24:23,387 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:24:28,668 : Generating sentence embeddings
2019-03-13 11:24:59,659 : Generated sentence embeddings
2019-03-13 11:24:59,660 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:25:08,793 : Best param found at split 1: l2reg = 0.001                 with score 95.9
2019-03-13 11:25:16,835 : Best param found at split 2: l2reg = 0.0001                 with score 96.05
2019-03-13 11:25:24,753 : Best param found at split 3: l2reg = 0.0001                 with score 95.45
2019-03-13 11:25:33,712 : Best param found at split 4: l2reg = 0.001                 with score 95.86
2019-03-13 11:25:43,235 : Best param found at split 5: l2reg = 0.001                 with score 95.81
2019-03-13 11:25:43,653 : Dev acc : 95.81 Test acc : 95.08

2019-03-13 11:25:43,654 : ***** Transfer task : SST Binary classification *****


2019-03-13 11:25:43,794 : loading BERT model bert-large-uncased
2019-03-13 11:25:43,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:25:43,817 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:25:43,817 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpap6e_qo8
2019-03-13 11:25:51,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:25:56,569 : Computing embedding for train
2019-03-13 11:27:37,171 : Computed train embeddings
2019-03-13 11:27:37,171 : Computing embedding for dev
2019-03-13 11:27:39,363 : Computed dev embeddings
2019-03-13 11:27:39,363 : Computing embedding for test
2019-03-13 11:27:43,980 : Computed test embeddings
2019-03-13 11:27:43,980 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:27:59,644 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.75), ('reg:0.001', 83.72), ('reg:0.01', 83.26)]
2019-03-13 11:27:59,644 : Validation : best param found is reg = 1e-05 with score             84.75
2019-03-13 11:27:59,644 : Evaluating...
2019-03-13 11:28:04,046 : 
Dev acc : 84.75 Test acc : 85.34 for             SST Binary classification

2019-03-13 11:28:04,046 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 11:28:04,097 : loading BERT model bert-large-uncased
2019-03-13 11:28:04,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:28:04,119 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:28:04,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxn3qt6no
2019-03-13 11:28:11,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:28:17,035 : Computing embedding for train
2019-03-13 11:28:39,055 : Computed train embeddings
2019-03-13 11:28:39,056 : Computing embedding for dev
2019-03-13 11:28:41,934 : Computed dev embeddings
2019-03-13 11:28:41,934 : Computing embedding for test
2019-03-13 11:28:47,598 : Computed test embeddings
2019-03-13 11:28:47,599 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:28:50,107 : [('reg:1e-05', 44.41), ('reg:0.0001', 44.32), ('reg:0.001', 43.78), ('reg:0.01', 42.87)]
2019-03-13 11:28:50,107 : Validation : best param found is reg = 1e-05 with score             44.41
2019-03-13 11:28:50,107 : Evaluating...
2019-03-13 11:28:50,748 : 
Dev acc : 44.41 Test acc : 46.02 for             SST Fine-Grained classification

2019-03-13 11:28:50,749 : ***** Transfer task : TREC *****


2019-03-13 11:28:50,762 : loading BERT model bert-large-uncased
2019-03-13 11:28:50,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:28:50,781 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:28:50,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnneo4tqn
2019-03-13 11:28:58,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:29:11,300 : Computed train embeddings
2019-03-13 11:29:11,891 : Computed test embeddings
2019-03-13 11:29:11,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:29:19,715 : [('reg:1e-05', 85.58), ('reg:0.0001', 85.69), ('reg:0.001', 84.45), ('reg:0.01', 80.65)]
2019-03-13 11:29:19,715 : Cross-validation : best param found is reg = 0.0001             with score 85.69
2019-03-13 11:29:19,715 : Evaluating...
2019-03-13 11:29:20,218 : 
Dev acc : 85.69 Test acc : 94.6             for TREC

2019-03-13 11:29:20,219 : ***** Transfer task : MRPC *****


2019-03-13 11:29:20,241 : loading BERT model bert-large-uncased
2019-03-13 11:29:20,241 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:29:20,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:29:20,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1bvcfbqx
2019-03-13 11:29:27,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:29:33,052 : Computing embedding for train
2019-03-13 11:29:55,349 : Computed train embeddings
2019-03-13 11:29:55,349 : Computing embedding for test
2019-03-13 11:30:05,128 : Computed test embeddings
2019-03-13 11:30:05,150 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:30:09,978 : [('reg:1e-05', 75.0), ('reg:0.0001', 75.07), ('reg:0.001', 74.44), ('reg:0.01', 73.8)]
2019-03-13 11:30:09,978 : Cross-validation : best param found is reg = 0.0001             with score 75.07
2019-03-13 11:30:09,978 : Evaluating...
2019-03-13 11:30:10,287 : Dev acc : 75.07 Test acc 74.72; Test F1 80.59 for MRPC.

2019-03-13 11:30:10,287 : ***** Transfer task : SICK-Entailment*****


2019-03-13 11:30:10,311 : loading BERT model bert-large-uncased
2019-03-13 11:30:10,311 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:30:10,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:30:10,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7_5zsgy8
2019-03-13 11:30:17,744 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:30:23,027 : Computing embedding for train
2019-03-13 11:30:34,359 : Computed train embeddings
2019-03-13 11:30:34,359 : Computing embedding for dev
2019-03-13 11:30:35,905 : Computed dev embeddings
2019-03-13 11:30:35,905 : Computing embedding for test
2019-03-13 11:30:48,100 : Computed test embeddings
2019-03-13 11:30:48,136 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:30:49,542 : [('reg:1e-05', 79.0), ('reg:0.0001', 79.2), ('reg:0.001', 79.2), ('reg:0.01', 78.6)]
2019-03-13 11:30:49,542 : Validation : best param found is reg = 0.0001 with score             79.2
2019-03-13 11:30:49,542 : Evaluating...
2019-03-13 11:30:49,927 : 
Dev acc : 79.2 Test acc : 77.98 for                        SICK entailment

2019-03-13 11:30:49,927 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 11:30:49,954 : loading BERT model bert-large-uncased
2019-03-13 11:30:49,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:30:49,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:30:49,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgs2pdwjz
2019-03-13 11:30:57,487 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:31:02,712 : Computing embedding for train
2019-03-13 11:31:14,047 : Computed train embeddings
2019-03-13 11:31:14,047 : Computing embedding for dev
2019-03-13 11:31:15,596 : Computed dev embeddings
2019-03-13 11:31:15,596 : Computing embedding for test
2019-03-13 11:31:27,777 : Computed test embeddings
2019-03-13 11:31:43,101 : Dev : Pearson 0.7877867205781303
2019-03-13 11:31:43,101 : Test : Pearson 0.8091600147204489 Spearman 0.7452840761592932 MSE 0.3532671232445474                        for SICK Relatedness

2019-03-13 11:31:43,102 : 

***** Transfer task : STSBenchmark*****


2019-03-13 11:31:43,171 : loading BERT model bert-large-uncased
2019-03-13 11:31:43,171 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:31:43,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:31:43,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8gc35gc4
2019-03-13 11:31:50,676 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:31:55,918 : Computing embedding for train
2019-03-13 11:32:14,572 : Computed train embeddings
2019-03-13 11:32:14,572 : Computing embedding for dev
2019-03-13 11:32:20,240 : Computed dev embeddings
2019-03-13 11:32:20,240 : Computing embedding for test
2019-03-13 11:32:24,860 : Computed test embeddings
2019-03-13 11:32:43,792 : Dev : Pearson 0.7170316622393728
2019-03-13 11:32:43,792 : Test : Pearson 0.6794656485154934 Spearman 0.6749805732072844 MSE 1.4538880496472235                        for SICK Relatedness

2019-03-13 11:32:43,792 : ***** Transfer task : SNLI Entailment*****


2019-03-13 11:32:48,755 : loading BERT model bert-large-uncased
2019-03-13 11:32:48,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:32:48,876 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:32:48,876 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp60k2yly0
2019-03-13 11:32:56,330 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:33:02,006 : PROGRESS (encoding): 0.00%
2019-03-13 11:35:48,400 : PROGRESS (encoding): 14.56%
2019-03-13 11:38:58,894 : PROGRESS (encoding): 29.12%
2019-03-13 11:42:09,909 : PROGRESS (encoding): 43.69%
2019-03-13 11:45:33,042 : PROGRESS (encoding): 58.25%
2019-03-13 11:49:19,089 : PROGRESS (encoding): 72.81%
2019-03-13 11:53:04,071 : PROGRESS (encoding): 87.37%
2019-03-13 11:57:07,480 : PROGRESS (encoding): 0.00%
2019-03-13 11:57:38,082 : PROGRESS (encoding): 0.00%
2019-03-13 11:58:07,483 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:58:33,648 : [('reg:1e-09', 64.74)]
2019-03-13 11:58:33,649 : Validation : best param found is reg = 1e-09 with score             64.74
2019-03-13 11:58:33,649 : Evaluating...
2019-03-13 11:59:01,275 : Dev acc : 64.74 Test acc : 64.62 for SNLI

2019-03-13 11:59:01,275 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 11:59:01,496 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 11:59:02,528 : loading BERT model bert-large-uncased
2019-03-13 11:59:02,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:59:02,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:59:02,556 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2pwc3dg2
2019-03-13 11:59:10,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:59:15,349 : Computing embeddings for train/dev/test
2019-03-13 12:02:47,689 : Computed embeddings
2019-03-13 12:02:47,689 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:03:18,619 : [('reg:1e-05', 87.17), ('reg:0.0001', 84.36), ('reg:0.001', 78.52), ('reg:0.01', 64.71)]
2019-03-13 12:03:18,619 : Validation : best param found is reg = 1e-05 with score             87.17
2019-03-13 12:03:18,619 : Evaluating...
2019-03-13 12:03:27,346 : 
Dev acc : 87.2 Test acc : 87.0 for LENGTH classification

2019-03-13 12:03:27,347 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 12:03:27,707 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 12:03:27,752 : loading BERT model bert-large-uncased
2019-03-13 12:03:27,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:03:27,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:03:27,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph9khhj60
2019-03-13 12:03:35,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:03:40,493 : Computing embeddings for train/dev/test
2019-03-13 12:06:56,297 : Computed embeddings
2019-03-13 12:06:56,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:07:25,186 : [('reg:1e-05', 57.9), ('reg:0.0001', 33.67), ('reg:0.001', 3.84), ('reg:0.01', 0.95)]
2019-03-13 12:07:25,186 : Validation : best param found is reg = 1e-05 with score             57.9
2019-03-13 12:07:25,186 : Evaluating...
2019-03-13 12:07:32,907 : 
Dev acc : 57.9 Test acc : 58.8 for WORDCONTENT classification

2019-03-13 12:07:32,908 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 12:07:33,267 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 12:07:33,333 : loading BERT model bert-large-uncased
2019-03-13 12:07:33,333 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:07:33,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:07:33,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeg5otqtv
2019-03-13 12:07:40,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:07:46,085 : Computing embeddings for train/dev/test
2019-03-13 12:10:50,420 : Computed embeddings
2019-03-13 12:10:50,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:11:11,549 : [('reg:1e-05', 37.82), ('reg:0.0001', 37.54), ('reg:0.001', 36.08), ('reg:0.01', 30.46)]
2019-03-13 12:11:11,549 : Validation : best param found is reg = 1e-05 with score             37.82
2019-03-13 12:11:11,549 : Evaluating...
2019-03-13 12:11:16,405 : 
Dev acc : 37.8 Test acc : 38.1 for DEPTH classification

2019-03-13 12:11:16,406 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 12:11:16,791 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 12:11:16,854 : loading BERT model bert-large-uncased
2019-03-13 12:11:16,854 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:11:16,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:11:16,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7hb74qr2
2019-03-13 12:11:24,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:11:29,563 : Computing embeddings for train/dev/test
2019-03-13 12:14:20,381 : Computed embeddings
2019-03-13 12:14:20,381 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:14:47,340 : [('reg:1e-05', 76.0), ('reg:0.0001', 74.41), ('reg:0.001', 68.19), ('reg:0.01', 60.22)]
2019-03-13 12:14:47,340 : Validation : best param found is reg = 1e-05 with score             76.0
2019-03-13 12:14:47,340 : Evaluating...
2019-03-13 12:14:53,902 : 
Dev acc : 76.0 Test acc : 75.9 for TOPCONSTITUENTS classification

2019-03-13 12:14:53,903 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 12:14:54,251 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 12:14:54,316 : loading BERT model bert-large-uncased
2019-03-13 12:14:54,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:14:54,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:14:54,435 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz7bx2uwk
2019-03-13 12:15:01,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:15:07,188 : Computing embeddings for train/dev/test
2019-03-13 12:18:12,616 : Computed embeddings
2019-03-13 12:18:12,616 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:18:39,275 : [('reg:1e-05', 93.15), ('reg:0.0001', 93.21), ('reg:0.001', 92.73), ('reg:0.01', 91.61)]
2019-03-13 12:18:39,275 : Validation : best param found is reg = 0.0001 with score             93.21
2019-03-13 12:18:39,275 : Evaluating...
2019-03-13 12:18:44,958 : 
Dev acc : 93.2 Test acc : 92.7 for BIGRAMSHIFT classification

2019-03-13 12:18:44,959 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 12:18:45,350 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 12:18:45,417 : loading BERT model bert-large-uncased
2019-03-13 12:18:45,417 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:18:45,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:18:45,545 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx9x6rkjl
2019-03-13 12:18:53,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:18:58,584 : Computing embeddings for train/dev/test
2019-03-13 12:21:59,483 : Computed embeddings
2019-03-13 12:21:59,483 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:22:25,674 : [('reg:1e-05', 89.02), ('reg:0.0001', 89.05), ('reg:0.001', 89.27), ('reg:0.01', 89.35)]
2019-03-13 12:22:25,674 : Validation : best param found is reg = 0.01 with score             89.35
2019-03-13 12:22:25,674 : Evaluating...
2019-03-13 12:22:32,241 : 
Dev acc : 89.3 Test acc : 87.3 for TENSE classification

2019-03-13 12:22:32,242 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 12:22:32,809 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 12:22:32,872 : loading BERT model bert-large-uncased
2019-03-13 12:22:32,873 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:22:32,900 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:22:32,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpno4f8tzp
2019-03-13 12:22:40,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:22:45,779 : Computing embeddings for train/dev/test
2019-03-13 12:25:57,381 : Computed embeddings
2019-03-13 12:25:57,381 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:26:22,607 : [('reg:1e-05', 86.53), ('reg:0.0001', 86.54), ('reg:0.001', 86.17), ('reg:0.01', 85.04)]
2019-03-13 12:26:22,607 : Validation : best param found is reg = 0.0001 with score             86.54
2019-03-13 12:26:22,607 : Evaluating...
2019-03-13 12:26:29,082 : 
Dev acc : 86.5 Test acc : 86.2 for SUBJNUMBER classification

2019-03-13 12:26:29,083 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 12:26:29,505 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 12:26:29,571 : loading BERT model bert-large-uncased
2019-03-13 12:26:29,571 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:26:29,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:26:29,597 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp04sagohz
2019-03-13 12:26:37,106 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:26:42,326 : Computing embeddings for train/dev/test
2019-03-13 12:29:50,617 : Computed embeddings
2019-03-13 12:29:50,617 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:30:10,629 : [('reg:1e-05', 83.39), ('reg:0.0001', 83.38), ('reg:0.001', 83.11), ('reg:0.01', 82.24)]
2019-03-13 12:30:10,629 : Validation : best param found is reg = 1e-05 with score             83.39
2019-03-13 12:30:10,629 : Evaluating...
2019-03-13 12:30:16,005 : 
Dev acc : 83.4 Test acc : 83.8 for OBJNUMBER classification

2019-03-13 12:30:16,006 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 12:30:16,392 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 12:30:16,461 : loading BERT model bert-large-uncased
2019-03-13 12:30:16,461 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:30:16,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:30:16,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0y0j7avf
2019-03-13 12:30:24,043 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:30:29,286 : Computing embeddings for train/dev/test
2019-03-13 12:34:08,149 : Computed embeddings
2019-03-13 12:34:08,149 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:34:33,937 : [('reg:1e-05', 68.98), ('reg:0.0001', 68.92), ('reg:0.001', 68.65), ('reg:0.01', 66.43)]
2019-03-13 12:34:33,938 : Validation : best param found is reg = 1e-05 with score             68.98
2019-03-13 12:34:33,938 : Evaluating...
2019-03-13 12:34:40,421 : 
Dev acc : 69.0 Test acc : 68.3 for ODDMANOUT classification

2019-03-13 12:34:40,422 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 12:34:41,034 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 12:34:41,109 : loading BERT model bert-large-uncased
2019-03-13 12:34:41,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:34:41,139 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:34:41,139 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl2yuao1t
2019-03-13 12:34:48,621 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:34:53,933 : Computing embeddings for train/dev/test
2019-03-13 12:38:31,527 : Computed embeddings
2019-03-13 12:38:31,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:39:00,345 : [('reg:1e-05', 72.88), ('reg:0.0001', 72.91), ('reg:0.001', 72.7), ('reg:0.01', 71.38)]
2019-03-13 12:39:00,345 : Validation : best param found is reg = 0.0001 with score             72.91
2019-03-13 12:39:00,346 : Evaluating...
2019-03-13 12:39:07,281 : 
Dev acc : 72.9 Test acc : 72.3 for COORDINATIONINVERSION classification

2019-03-13 12:39:07,283 : total results: {'STS12': {'MSRpar': {'pearson': (0.37379814528051064, 2.7736577870259893e-26), 'spearman': SpearmanrResult(correlation=0.41397963814092065, pvalue=2.0426844790352605e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.41076975158025647, 6.78635813195469e-32), 'spearman': SpearmanrResult(correlation=0.43457001831043995, pvalue=6.728989005652876e-36), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5550274740402285, 1.9125536325359673e-38), 'spearman': SpearmanrResult(correlation=0.6270645995043826, pvalue=1.5749249325340138e-51), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5755561976700836, 2.2316527957173142e-67), 'spearman': SpearmanrResult(correlation=0.5963388807000286, pvalue=2.0296461598674796e-73), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6964100687476139, 3.582380682523141e-59), 'spearman': SpearmanrResult(correlation=0.5944329704613752, pvalue=1.7537876183809904e-39), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5223123274637386, 'wmean': 0.4995876122628381}, 'spearman': {'mean': 0.5332772214234294, 'wmean': 0.5175893852156184}}}, 'STS13': {'FNWN': {'pearson': (0.33735098587840845, 2.065171412577451e-06), 'spearman': SpearmanrResult(correlation=0.35654974375004606, pvalue=4.7604352103967125e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6068554509716303, 1.1985134793825854e-76), 'spearman': SpearmanrResult(correlation=0.5861200726242746, pvalue=2.1500920201879456e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.43923486567146436, 7.315689702220005e-28), 'spearman': SpearmanrResult(correlation=0.4545098092071194, pvalue=6.0093648597539175e-30), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46114710084050103, 'wmean': 0.5102077894676222}, 'spearman': {'mean': 0.46572654186047996, 'wmean': 0.5079719726681058}}}, 'STS14': {'deft-forum': {'pearson': (0.25613475614613, 3.5778354000357744e-08), 'spearman': SpearmanrResult(correlation=0.28623812742716914, pvalue=6.20948031331649e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.7326676676130598, 9.944140714832961e-52), 'spearman': SpearmanrResult(correlation=0.6874009006711008, pvalue=2.7186290956409484e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.5481555131586525, 4.7988610540162274e-60), 'spearman': SpearmanrResult(correlation=0.5090451258843511, pvalue=1.0980698502975705e-50), 'nsamples': 750}, 'images': {'pearson': (0.4553008771698396, 1.178418210710177e-39), 'spearman': SpearmanrResult(correlation=0.45265039625678477, pvalue=3.6825999619974814e-39), 'nsamples': 750}, 'OnWN': {'pearson': (0.6137425685733641, 7.904193454252097e-79), 'spearman': SpearmanrResult(correlation=0.6390355179213149, pvalue=2.5482653199209585e-87), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6524784723325828, 3.6262182108150945e-92), 'spearman': SpearmanrResult(correlation=0.6086224161230149, pvalue=3.343809040246494e-77), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5430799758322714, 'wmean': 0.5432850703934682}, 'spearman': {'mean': 0.5304987473806225, 'wmean': 0.5312113385820415}}}, 'STS15': {'answers-forums': {'pearson': (0.567946213114127, 2.0708668981263283e-33), 'spearman': SpearmanrResult(correlation=0.5635984289619543, pvalue=8.044922699305884e-33), 'nsamples': 375}, 'answers-students': {'pearson': (0.6811045906272852, 2.332632142905073e-103), 'spearman': SpearmanrResult(correlation=0.702002145782213, pvalue=2.2527010028353866e-112), 'nsamples': 750}, 'belief': {'pearson': (0.6467885648670545, 8.220079918184401e-46), 'spearman': SpearmanrResult(correlation=0.6700344924873646, pvalue=3.3186805652063044e-50), 'nsamples': 375}, 'headlines': {'pearson': (0.6118183391817622, 3.255164412214552e-78), 'spearman': SpearmanrResult(correlation=0.6051630852060864, pvalue=4.0399948206449155e-76), 'nsamples': 750}, 'images': {'pearson': (0.5858641256819193, 2.551806194818353e-70), 'spearman': SpearmanrResult(correlation=0.5948892739314011, pvalue=5.535509655936822e-73), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6187043666944296, 'wmean': 0.6215386111203893}, 'spearman': {'mean': 0.6271374852738039, 'wmean': 0.62971774141109}}}, 'STS16': {'answer-answer': {'pearson': (0.5468016679626497, 3.312438718873192e-21), 'spearman': SpearmanrResult(correlation=0.5604832757301565, pvalue=2.0612924398583006e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6620398655312849, 8.834132648469633e-33), 'spearman': SpearmanrResult(correlation=0.6643957016326827, pvalue=4.4200950739110425e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7662615686269305, 1.0871966559169934e-45), 'spearman': SpearmanrResult(correlation=0.77167836680044, pvalue=1.0568865248423416e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8022964199939498, 3.7757240061222315e-56), 'spearman': SpearmanrResult(correlation=0.8345855609464872, pvalue=1.326840411236697e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.2502629851741584, 0.0002575423803640225), 'spearman': SpearmanrResult(correlation=0.236318687778386, pvalue=0.0005713695887892444), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6055325014577947, 'wmean': 0.6138625643709275}, 'spearman': {'mean': 0.6134923185776305, 'wmean': 0.6225234306262433}}}, 'MR': {'devacc': 81.32, 'acc': 79.82, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.25, 'acc': 86.62, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.6, 'acc': 85.78, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.81, 'acc': 95.08, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.75, 'acc': 85.34, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.41, 'acc': 46.02, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 85.69, 'acc': 94.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 75.07, 'acc': 74.72, 'f1': 80.59, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.2, 'acc': 77.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7877867205781303, 'pearson': 0.8091600147204489, 'spearman': 0.7452840761592932, 'mse': 0.3532671232445474, 'yhat': array([3.44134323, 4.42879025, 1.46527759, ..., 3.22681362, 4.34687836,        4.28845238]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7170316622393728, 'pearson': 0.6794656485154934, 'spearman': 0.6749805732072844, 'mse': 1.4538880496472235, 'yhat': array([1.59568993, 1.52969154, 2.51399006, ..., 3.96352967, 4.00481117,        3.27351911]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.74, 'acc': 64.62, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 87.17, 'acc': 86.96, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 57.9, 'acc': 58.76, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.82, 'acc': 38.1, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.0, 'acc': 75.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 93.21, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.35, 'acc': 87.34, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.54, 'acc': 86.24, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.39, 'acc': 83.79, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 68.98, 'acc': 68.34, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.91, 'acc': 72.28, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 12:39:07,283 : STS12 p=0.4996, STS12 s=0.5176, STS13 p=0.5102, STS13 s=0.5080, STS14 p=0.5433, STS14 s=0.5312, STS15 p=0.6215, STS15 s=0.6297, STS 16 p=0.6139, STS16 s=0.6225, STS B p=0.6795, STS B s=0.6750, STS B m=1.4539, SICK-R p=0.8092, SICK-R s=0.7453, SICK-P m=0.3533
2019-03-13 12:39:07,283 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 12:39:07,283 : 0.4996,0.5176,0.5102,0.5080,0.5433,0.5312,0.6215,0.6297,0.6139,0.6225,0.6795,0.6750,1.4539,0.8092,0.7453,0.3533
2019-03-13 12:39:07,283 : MR=79.82, CR=86.62, SUBJ=95.08, MPQA=85.78, SST-B=85.34, SST-F=46.02, TREC=94.60, SICK-E=77.98, SNLI=64.62, MRPC=74.72, MRPC f=80.59
2019-03-13 12:39:07,283 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 12:39:07,283 : 79.82,86.62,95.08,85.78,85.34,46.02,94.60,77.98,64.62,74.72,80.59
2019-03-13 12:39:07,283 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 12:39:07,284 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 12:39:07,284 : na,na,na,na,na,na,na,na,na,na
2019-03-13 12:39:07,284 : SentLen=86.96, WC=58.76, TreeDepth=38.10, TopConst=75.92, BShift=92.68, Tense=87.34, SubjNum=86.24, ObjNum=83.79, SOMO=68.34, CoordInv=72.28, average=75.04
2019-03-13 12:39:07,284 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 12:39:07,284 : 86.96,58.76,38.10,75.92,92.68,87.34,86.24,83.79,68.34,72.28,75.04
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,284 : layer 17
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,284 : ********************************************************************************
2019-03-13 12:39:07,376 : ***** Transfer task : STS12 *****


2019-03-13 12:39:07,388 : loading BERT model bert-large-uncased
2019-03-13 12:39:07,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:39:07,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:39:07,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3gb8to5d
2019-03-13 12:39:14,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:39:24,165 : MSRpar : pearson = 0.3746, spearman = 0.4169
2019-03-13 12:39:25,817 : MSRvid : pearson = 0.3840, spearman = 0.4074
2019-03-13 12:39:27,240 : SMTeuroparl : pearson = 0.5133, spearman = 0.5935
2019-03-13 12:39:29,955 : surprise.OnWN : pearson = 0.5811, spearman = 0.5996
2019-03-13 12:39:31,397 : surprise.SMTnews : pearson = 0.6607, spearman = 0.6015
2019-03-13 12:39:31,397 : ALL (weighted average) : Pearson = 0.4839,             Spearman = 0.5085
2019-03-13 12:39:31,397 : ALL (average) : Pearson = 0.5027,             Spearman = 0.5238

2019-03-13 12:39:31,398 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 12:39:31,406 : loading BERT model bert-large-uncased
2019-03-13 12:39:31,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:39:31,424 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:39:31,424 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppg3e9gbp
2019-03-13 12:39:38,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:39:45,472 : FNWN : pearson = 0.3434, spearman = 0.3615
2019-03-13 12:39:47,378 : headlines : pearson = 0.5783, spearman = 0.5592
2019-03-13 12:39:48,857 : OnWN : pearson = 0.4487, spearman = 0.4696
2019-03-13 12:39:48,857 : ALL (weighted average) : Pearson = 0.5003,             Spearman = 0.5008
2019-03-13 12:39:48,858 : ALL (average) : Pearson = 0.4568,             Spearman = 0.4634

2019-03-13 12:39:48,858 : ***** Transfer task : STS14 *****


2019-03-13 12:39:48,901 : loading BERT model bert-large-uncased
2019-03-13 12:39:48,901 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:39:48,919 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:39:48,919 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyksmm__h
2019-03-13 12:39:56,369 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:03,067 : deft-forum : pearson = 0.2586, spearman = 0.2893
2019-03-13 12:40:04,715 : deft-news : pearson = 0.7294, spearman = 0.6920
2019-03-13 12:40:06,900 : headlines : pearson = 0.5249, spearman = 0.4850
2019-03-13 12:40:08,987 : images : pearson = 0.3921, spearman = 0.4077
2019-03-13 12:40:11,133 : OnWN : pearson = 0.6257, spearman = 0.6512
2019-03-13 12:40:14,014 : tweet-news : pearson = 0.6395, spearman = 0.5857
2019-03-13 12:40:14,014 : ALL (weighted average) : Pearson = 0.5258,             Spearman = 0.5160
2019-03-13 12:40:14,014 : ALL (average) : Pearson = 0.5284,             Spearman = 0.5185

2019-03-13 12:40:14,014 : ***** Transfer task : STS15 *****


2019-03-13 12:40:14,046 : loading BERT model bert-large-uncased
2019-03-13 12:40:14,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:14,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:14,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppm37xsmk
2019-03-13 12:40:21,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:28,771 : answers-forums : pearson = 0.5801, spearman = 0.5807
2019-03-13 12:40:30,877 : answers-students : pearson = 0.6856, spearman = 0.7085
2019-03-13 12:40:32,939 : belief : pearson = 0.6604, spearman = 0.6808
2019-03-13 12:40:35,205 : headlines : pearson = 0.5886, spearman = 0.5859
2019-03-13 12:40:37,353 : images : pearson = 0.5474, spearman = 0.5687
2019-03-13 12:40:37,353 : ALL (weighted average) : Pearson = 0.6105,             Spearman = 0.6235
2019-03-13 12:40:37,353 : ALL (average) : Pearson = 0.6124,             Spearman = 0.6249

2019-03-13 12:40:37,354 : ***** Transfer task : STS16 *****


2019-03-13 12:40:37,392 : loading BERT model bert-large-uncased
2019-03-13 12:40:37,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:37,410 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:37,410 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_7atqga6
2019-03-13 12:40:44,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:51,121 : answer-answer : pearson = 0.5619, spearman = 0.5768
2019-03-13 12:40:51,785 : headlines : pearson = 0.6439, spearman = 0.6474
2019-03-13 12:40:52,673 : plagiarism : pearson = 0.7553, spearman = 0.7682
2019-03-13 12:40:54,176 : postediting : pearson = 0.7845, spearman = 0.8135
2019-03-13 12:40:54,785 : question-question : pearson = 0.2126, spearman = 0.2210
2019-03-13 12:40:54,785 : ALL (weighted average) : Pearson = 0.6009,             Spearman = 0.6147
2019-03-13 12:40:54,786 : ALL (average) : Pearson = 0.5917,             Spearman = 0.6054

2019-03-13 12:40:54,786 : ***** Transfer task : MR *****


2019-03-13 12:40:54,848 : loading BERT model bert-large-uncased
2019-03-13 12:40:54,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:54,868 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:54,869 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjyuni0j0
2019-03-13 12:41:02,304 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:41:07,655 : Generating sentence embeddings
2019-03-13 12:41:39,346 : Generated sentence embeddings
2019-03-13 12:41:39,347 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:41:49,142 : Best param found at split 1: l2reg = 0.001                 with score 82.93
2019-03-13 12:41:58,790 : Best param found at split 2: l2reg = 0.0001                 with score 82.57
2019-03-13 12:42:09,090 : Best param found at split 3: l2reg = 0.001                 with score 83.36
2019-03-13 12:42:19,785 : Best param found at split 4: l2reg = 0.01                 with score 82.78
2019-03-13 12:42:28,584 : Best param found at split 5: l2reg = 0.0001                 with score 82.88
2019-03-13 12:42:28,966 : Dev acc : 82.9 Test acc : 81.64

2019-03-13 12:42:28,967 : ***** Transfer task : CR *****


2019-03-13 12:42:28,974 : loading BERT model bert-large-uncased
2019-03-13 12:42:28,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:42:29,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:42:29,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfg5zfyy6
2019-03-13 12:42:36,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:42:41,792 : Generating sentence embeddings
2019-03-13 12:42:50,139 : Generated sentence embeddings
2019-03-13 12:42:50,139 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:42:53,574 : Best param found at split 1: l2reg = 0.001                 with score 88.84
2019-03-13 12:42:57,088 : Best param found at split 2: l2reg = 1e-05                 with score 88.71
2019-03-13 12:43:00,928 : Best param found at split 3: l2reg = 0.01                 with score 89.44
2019-03-13 12:43:04,422 : Best param found at split 4: l2reg = 0.01                 with score 88.22
2019-03-13 12:43:08,294 : Best param found at split 5: l2reg = 0.001                 with score 89.04
2019-03-13 12:43:08,420 : Dev acc : 88.85 Test acc : 87.76

2019-03-13 12:43:08,420 : ***** Transfer task : MPQA *****


2019-03-13 12:43:08,426 : loading BERT model bert-large-uncased
2019-03-13 12:43:08,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:43:08,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:43:08,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjljsk7cg
2019-03-13 12:43:15,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:43:21,230 : Generating sentence embeddings
2019-03-13 12:43:28,850 : Generated sentence embeddings
2019-03-13 12:43:28,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:43:37,456 : Best param found at split 1: l2reg = 0.01                 with score 82.2
2019-03-13 12:43:47,490 : Best param found at split 2: l2reg = 0.001                 with score 84.97
2019-03-13 12:43:55,183 : Best param found at split 3: l2reg = 1e-05                 with score 84.73
2019-03-13 12:44:03,953 : Best param found at split 4: l2reg = 0.001                 with score 84.93
2019-03-13 12:44:13,932 : Best param found at split 5: l2reg = 0.0001                 with score 83.53
2019-03-13 12:44:14,526 : Dev acc : 84.07 Test acc : 85.49

2019-03-13 12:44:14,527 : ***** Transfer task : SUBJ *****


2019-03-13 12:44:14,545 : loading BERT model bert-large-uncased
2019-03-13 12:44:14,546 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:44:14,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:44:14,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcem4r6io
2019-03-13 12:44:22,080 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:44:27,329 : Generating sentence embeddings
2019-03-13 12:44:58,374 : Generated sentence embeddings
2019-03-13 12:44:58,375 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:45:06,840 : Best param found at split 1: l2reg = 1e-05                 with score 95.95
2019-03-13 12:45:17,246 : Best param found at split 2: l2reg = 0.0001                 with score 95.94
2019-03-13 12:45:26,846 : Best param found at split 3: l2reg = 0.001                 with score 95.56
2019-03-13 12:45:35,931 : Best param found at split 4: l2reg = 1e-05                 with score 96.11
2019-03-13 12:45:44,087 : Best param found at split 5: l2reg = 0.001                 with score 95.65
2019-03-13 12:45:44,569 : Dev acc : 95.84 Test acc : 95.15

2019-03-13 12:45:44,570 : ***** Transfer task : SST Binary classification *****


2019-03-13 12:45:44,709 : loading BERT model bert-large-uncased
2019-03-13 12:45:44,709 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:45:44,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:45:44,733 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxb_yihma
2019-03-13 12:45:52,150 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:45:57,406 : Computing embedding for train
2019-03-13 12:47:38,076 : Computed train embeddings
2019-03-13 12:47:38,076 : Computing embedding for dev
2019-03-13 12:47:40,276 : Computed dev embeddings
2019-03-13 12:47:40,276 : Computing embedding for test
2019-03-13 12:47:44,905 : Computed test embeddings
2019-03-13 12:47:44,905 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:48:03,367 : [('reg:1e-05', 86.81), ('reg:0.0001', 86.47), ('reg:0.001', 86.24), ('reg:0.01', 84.52)]
2019-03-13 12:48:03,368 : Validation : best param found is reg = 1e-05 with score             86.81
2019-03-13 12:48:03,368 : Evaluating...
2019-03-13 12:48:08,336 : 
Dev acc : 86.81 Test acc : 87.26 for             SST Binary classification

2019-03-13 12:48:08,337 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 12:48:08,387 : loading BERT model bert-large-uncased
2019-03-13 12:48:08,387 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:48:08,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:48:08,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpccxqob2o
2019-03-13 12:48:15,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:48:21,179 : Computing embedding for train
2019-03-13 12:48:43,235 : Computed train embeddings
2019-03-13 12:48:43,235 : Computing embedding for dev
2019-03-13 12:48:46,117 : Computed dev embeddings
2019-03-13 12:48:46,117 : Computing embedding for test
2019-03-13 12:48:51,790 : Computed test embeddings
2019-03-13 12:48:51,790 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:48:54,367 : [('reg:1e-05', 46.59), ('reg:0.0001', 46.87), ('reg:0.001', 47.05), ('reg:0.01', 44.96)]
2019-03-13 12:48:54,367 : Validation : best param found is reg = 0.001 with score             47.05
2019-03-13 12:48:54,367 : Evaluating...
2019-03-13 12:48:55,009 : 
Dev acc : 47.05 Test acc : 47.78 for             SST Fine-Grained classification

2019-03-13 12:48:55,009 : ***** Transfer task : TREC *****


2019-03-13 12:48:55,022 : loading BERT model bert-large-uncased
2019-03-13 12:48:55,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:48:55,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:48:55,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcpgv6ubu
2019-03-13 12:49:02,491 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:49:15,423 : Computed train embeddings
2019-03-13 12:49:16,015 : Computed test embeddings
2019-03-13 12:49:16,015 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:49:22,962 : [('reg:1e-05', 86.96), ('reg:0.0001', 87.01), ('reg:0.001', 86.19), ('reg:0.01', 78.56)]
2019-03-13 12:49:22,963 : Cross-validation : best param found is reg = 0.0001             with score 87.01
2019-03-13 12:49:22,963 : Evaluating...
2019-03-13 12:49:23,208 : 
Dev acc : 87.01 Test acc : 93.6             for TREC

2019-03-13 12:49:23,209 : ***** Transfer task : MRPC *****


2019-03-13 12:49:23,229 : loading BERT model bert-large-uncased
2019-03-13 12:49:23,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:49:23,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:49:23,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpihnksu1p
2019-03-13 12:49:30,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:49:36,094 : Computing embedding for train
2019-03-13 12:49:58,449 : Computed train embeddings
2019-03-13 12:49:58,449 : Computing embedding for test
2019-03-13 12:50:08,246 : Computed test embeddings
2019-03-13 12:50:08,267 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:50:12,671 : [('reg:1e-05', 74.46), ('reg:0.0001', 74.34), ('reg:0.001', 74.34), ('reg:0.01', 74.09)]
2019-03-13 12:50:12,672 : Cross-validation : best param found is reg = 1e-05             with score 74.46
2019-03-13 12:50:12,672 : Evaluating...
2019-03-13 12:50:12,904 : Dev acc : 74.46 Test acc 74.2; Test F1 79.98 for MRPC.

2019-03-13 12:50:12,904 : ***** Transfer task : SICK-Entailment*****


2019-03-13 12:50:12,933 : loading BERT model bert-large-uncased
2019-03-13 12:50:12,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:50:12,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:50:12,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps2bbbvfi
2019-03-13 12:50:20,466 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:50:25,801 : Computing embedding for train
2019-03-13 12:50:37,157 : Computed train embeddings
2019-03-13 12:50:37,157 : Computing embedding for dev
2019-03-13 12:50:38,704 : Computed dev embeddings
2019-03-13 12:50:38,704 : Computing embedding for test
2019-03-13 12:50:50,913 : Computed test embeddings
2019-03-13 12:50:50,950 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:50:52,342 : [('reg:1e-05', 78.2), ('reg:0.0001', 78.4), ('reg:0.001', 77.2), ('reg:0.01', 78.2)]
2019-03-13 12:50:52,342 : Validation : best param found is reg = 0.0001 with score             78.4
2019-03-13 12:50:52,342 : Evaluating...
2019-03-13 12:50:52,693 : 
Dev acc : 78.4 Test acc : 78.12 for                        SICK entailment

2019-03-13 12:50:52,694 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 12:50:52,720 : loading BERT model bert-large-uncased
2019-03-13 12:50:52,720 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:50:52,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:50:52,740 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_mxgl9bc
2019-03-13 12:51:00,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:51:05,575 : Computing embedding for train
2019-03-13 12:51:16,928 : Computed train embeddings
2019-03-13 12:51:16,928 : Computing embedding for dev
2019-03-13 12:51:18,481 : Computed dev embeddings
2019-03-13 12:51:18,481 : Computing embedding for test
2019-03-13 12:51:30,688 : Computed test embeddings
2019-03-13 12:51:45,805 : Dev : Pearson 0.7848223738808127
2019-03-13 12:51:45,805 : Test : Pearson 0.8009161158023148 Spearman 0.7373365248577304 MSE 0.3668932184983793                        for SICK Relatedness

2019-03-13 12:51:45,806 : 

***** Transfer task : STSBenchmark*****


2019-03-13 12:51:45,846 : loading BERT model bert-large-uncased
2019-03-13 12:51:45,846 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:51:45,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:51:45,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3vbdgf4o
2019-03-13 12:51:53,363 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:51:58,647 : Computing embedding for train
2019-03-13 12:52:17,335 : Computed train embeddings
2019-03-13 12:52:17,335 : Computing embedding for dev
2019-03-13 12:52:23,005 : Computed dev embeddings
2019-03-13 12:52:23,006 : Computing embedding for test
2019-03-13 12:52:27,645 : Computed test embeddings
2019-03-13 12:52:46,412 : Dev : Pearson 0.7263815849949483
2019-03-13 12:52:46,412 : Test : Pearson 0.6806907836450307 Spearman 0.6804006052818833 MSE 1.4247065119465931                        for SICK Relatedness

2019-03-13 12:52:46,413 : ***** Transfer task : SNLI Entailment*****


2019-03-13 12:52:51,317 : loading BERT model bert-large-uncased
2019-03-13 12:52:51,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:52:51,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:52:51,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd09enrqt
2019-03-13 12:52:58,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:53:04,536 : PROGRESS (encoding): 0.00%
2019-03-13 12:55:51,725 : PROGRESS (encoding): 14.56%
2019-03-13 12:59:03,235 : PROGRESS (encoding): 29.12%
2019-03-13 13:02:14,296 : PROGRESS (encoding): 43.69%
2019-03-13 13:05:37,527 : PROGRESS (encoding): 58.25%
2019-03-13 13:09:23,925 : PROGRESS (encoding): 72.81%
2019-03-13 13:13:09,017 : PROGRESS (encoding): 87.37%
2019-03-13 13:17:12,569 : PROGRESS (encoding): 0.00%
2019-03-13 13:17:43,216 : PROGRESS (encoding): 0.00%
2019-03-13 13:18:12,675 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:18:50,066 : [('reg:1e-09', 66.22)]
2019-03-13 13:18:50,066 : Validation : best param found is reg = 1e-09 with score             66.22
2019-03-13 13:18:50,066 : Evaluating...
2019-03-13 13:19:24,725 : Dev acc : 66.22 Test acc : 66.8 for SNLI

2019-03-13 13:19:24,725 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 13:19:24,937 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 13:19:26,014 : loading BERT model bert-large-uncased
2019-03-13 13:19:26,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:19:26,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:19:26,041 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm1tinf_e
2019-03-13 13:19:33,526 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:19:38,815 : Computing embeddings for train/dev/test
2019-03-13 13:23:11,151 : Computed embeddings
2019-03-13 13:23:11,151 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:23:39,775 : [('reg:1e-05', 84.2), ('reg:0.0001', 81.97), ('reg:0.001', 74.45), ('reg:0.01', 59.95)]
2019-03-13 13:23:39,776 : Validation : best param found is reg = 1e-05 with score             84.2
2019-03-13 13:23:39,776 : Evaluating...
2019-03-13 13:23:47,609 : 
Dev acc : 84.2 Test acc : 84.5 for LENGTH classification

2019-03-13 13:23:47,609 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 13:23:47,970 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 13:23:48,015 : loading BERT model bert-large-uncased
2019-03-13 13:23:48,015 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:23:48,044 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:23:48,044 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpayxono4n
2019-03-13 13:23:55,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:24:00,790 : Computing embeddings for train/dev/test
2019-03-13 13:27:16,882 : Computed embeddings
2019-03-13 13:27:16,882 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:27:47,699 : [('reg:1e-05', 55.42), ('reg:0.0001', 34.5), ('reg:0.001', 4.61), ('reg:0.01', 0.94)]
2019-03-13 13:27:47,700 : Validation : best param found is reg = 1e-05 with score             55.42
2019-03-13 13:27:47,700 : Evaluating...
2019-03-13 13:27:57,853 : 
Dev acc : 55.4 Test acc : 56.1 for WORDCONTENT classification

2019-03-13 13:27:57,854 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 13:27:58,212 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 13:27:58,278 : loading BERT model bert-large-uncased
2019-03-13 13:27:58,278 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:27:58,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:27:58,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfpvev863
2019-03-13 13:28:05,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:28:11,073 : Computing embeddings for train/dev/test
2019-03-13 13:31:16,166 : Computed embeddings
2019-03-13 13:31:16,166 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:31:35,795 : [('reg:1e-05', 37.64), ('reg:0.0001', 37.31), ('reg:0.001', 35.35), ('reg:0.01', 29.25)]
2019-03-13 13:31:35,795 : Validation : best param found is reg = 1e-05 with score             37.64
2019-03-13 13:31:35,795 : Evaluating...
2019-03-13 13:31:40,327 : 
Dev acc : 37.6 Test acc : 37.2 for DEPTH classification

2019-03-13 13:31:40,328 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 13:31:40,709 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 13:31:40,772 : loading BERT model bert-large-uncased
2019-03-13 13:31:40,772 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:31:40,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:31:40,800 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp99hx_26n
2019-03-13 13:31:48,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:31:53,503 : Computing embeddings for train/dev/test
2019-03-13 13:34:45,332 : Computed embeddings
2019-03-13 13:34:45,332 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:35:11,739 : [('reg:1e-05', 73.42), ('reg:0.0001', 71.93), ('reg:0.001', 68.4), ('reg:0.01', 58.2)]
2019-03-13 13:35:11,739 : Validation : best param found is reg = 1e-05 with score             73.42
2019-03-13 13:35:11,739 : Evaluating...
2019-03-13 13:35:17,537 : 
Dev acc : 73.4 Test acc : 73.1 for TOPCONSTITUENTS classification

2019-03-13 13:35:17,538 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 13:35:17,884 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 13:35:17,949 : loading BERT model bert-large-uncased
2019-03-13 13:35:17,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:35:18,067 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:35:18,067 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp79ik764a
2019-03-13 13:35:25,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:35:30,847 : Computing embeddings for train/dev/test
2019-03-13 13:38:37,280 : Computed embeddings
2019-03-13 13:38:37,280 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:39:07,298 : [('reg:1e-05', 92.88), ('reg:0.0001', 92.87), ('reg:0.001', 92.6), ('reg:0.01', 91.59)]
2019-03-13 13:39:07,298 : Validation : best param found is reg = 1e-05 with score             92.88
2019-03-13 13:39:07,299 : Evaluating...
2019-03-13 13:39:12,800 : 
Dev acc : 92.9 Test acc : 92.4 for BIGRAMSHIFT classification

2019-03-13 13:39:12,801 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 13:39:13,191 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 13:39:13,258 : loading BERT model bert-large-uncased
2019-03-13 13:39:13,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:39:13,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:39:13,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprjxas0fe
2019-03-13 13:39:20,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:39:26,024 : Computing embeddings for train/dev/test
2019-03-13 13:42:27,554 : Computed embeddings
2019-03-13 13:42:27,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:42:50,049 : [('reg:1e-05', 89.82), ('reg:0.0001', 89.78), ('reg:0.001', 90.2), ('reg:0.01', 90.05)]
2019-03-13 13:42:50,049 : Validation : best param found is reg = 0.001 with score             90.2
2019-03-13 13:42:50,049 : Evaluating...
2019-03-13 13:42:55,480 : 
Dev acc : 90.2 Test acc : 87.9 for TENSE classification

2019-03-13 13:42:55,481 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 13:42:56,045 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 13:42:56,107 : loading BERT model bert-large-uncased
2019-03-13 13:42:56,108 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:42:56,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:42:56,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6leeeeo1
2019-03-13 13:43:03,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:43:08,971 : Computing embeddings for train/dev/test
2019-03-13 13:46:21,083 : Computed embeddings
2019-03-13 13:46:21,083 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:46:45,722 : [('reg:1e-05', 86.31), ('reg:0.0001', 86.26), ('reg:0.001', 85.12), ('reg:0.01', 84.31)]
2019-03-13 13:46:45,722 : Validation : best param found is reg = 1e-05 with score             86.31
2019-03-13 13:46:45,722 : Evaluating...
2019-03-13 13:46:52,135 : 
Dev acc : 86.3 Test acc : 86.0 for SUBJNUMBER classification

2019-03-13 13:46:52,136 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 13:46:52,557 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 13:46:52,623 : loading BERT model bert-large-uncased
2019-03-13 13:46:52,624 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:46:52,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:46:52,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpljwm6n_q
2019-03-13 13:47:00,148 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:47:05,451 : Computing embeddings for train/dev/test
2019-03-13 13:50:14,514 : Computed embeddings
2019-03-13 13:50:14,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:50:38,632 : [('reg:1e-05', 83.5), ('reg:0.0001', 83.45), ('reg:0.001', 83.34), ('reg:0.01', 82.21)]
2019-03-13 13:50:38,632 : Validation : best param found is reg = 1e-05 with score             83.5
2019-03-13 13:50:38,632 : Evaluating...
2019-03-13 13:50:44,014 : 
Dev acc : 83.5 Test acc : 84.4 for OBJNUMBER classification

2019-03-13 13:50:44,015 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 13:50:44,435 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 13:50:44,506 : loading BERT model bert-large-uncased
2019-03-13 13:50:44,506 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:50:44,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:50:44,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfpjoymsc
2019-03-13 13:50:52,117 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:50:57,503 : Computing embeddings for train/dev/test
2019-03-13 13:54:37,095 : Computed embeddings
2019-03-13 13:54:37,095 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:55:01,036 : [('reg:1e-05', 69.16), ('reg:0.0001', 69.23), ('reg:0.001', 68.91), ('reg:0.01', 67.28)]
2019-03-13 13:55:01,036 : Validation : best param found is reg = 0.0001 with score             69.23
2019-03-13 13:55:01,036 : Evaluating...
2019-03-13 13:55:07,439 : 
Dev acc : 69.2 Test acc : 68.0 for ODDMANOUT classification

2019-03-13 13:55:07,440 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 13:55:08,044 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 13:55:08,119 : loading BERT model bert-large-uncased
2019-03-13 13:55:08,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:55:08,148 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:55:08,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzcll4hpa
2019-03-13 13:55:15,615 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:55:20,826 : Computing embeddings for train/dev/test
2019-03-13 13:58:59,540 : Computed embeddings
2019-03-13 13:58:59,540 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:59:29,237 : [('reg:1e-05', 74.23), ('reg:0.0001', 74.27), ('reg:0.001', 73.89), ('reg:0.01', 71.57)]
2019-03-13 13:59:29,237 : Validation : best param found is reg = 0.0001 with score             74.27
2019-03-13 13:59:29,237 : Evaluating...
2019-03-13 13:59:36,898 : 
Dev acc : 74.3 Test acc : 74.0 for COORDINATIONINVERSION classification

2019-03-13 13:59:36,900 : total results: {'STS12': {'MSRpar': {'pearson': (0.37461162285015115, 2.1239132243460453e-26), 'spearman': SpearmanrResult(correlation=0.4168911482517721, pvalue=6.797046842454279e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3839689523859136, 9.330139136513747e-28), 'spearman': SpearmanrResult(correlation=0.40739511120333133, pvalue=2.3646112249509395e-31), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5132626966035078, 3.281293577906393e-32), 'spearman': SpearmanrResult(correlation=0.593529696091236, pvalue=4.999906945149442e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5810958959201794, 6.035384842580376e-69), 'spearman': SpearmanrResult(correlation=0.5996259131735437, pvalue=2.046920237110279e-74), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6607176583114119, 2.1183033457234773e-51), 'spearman': SpearmanrResult(correlation=0.6014646841920172, pvalue=1.2897954755756478e-40), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5027313652142328, 'wmean': 0.48390324220542036}, 'spearman': {'mean': 0.5237813105823801, 'wmean': 0.508477692718783}}}, 'STS13': {'FNWN': {'pearson': (0.3434121131507094, 1.313135794091018e-06), 'spearman': SpearmanrResult(correlation=0.3614551737306252, pvalue=3.2206816246614497e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.5783346727965378, 3.6810681053805424e-68), 'spearman': SpearmanrResult(correlation=0.5592273526544148, pvalue=6.315366340817817e-63), 'nsamples': 750}, 'OnWN': {'pearson': (0.4487493507415408, 3.7819010649720626e-29), 'spearman': SpearmanrResult(correlation=0.4696070891439687, pvalue=4.0871159189615724e-32), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.45683204556292933, 'wmean': 0.5002695198325946}, 'spearman': {'mean': 0.46342987184300294, 'wmean': 0.5007900795571104}}}, 'STS14': {'deft-forum': {'pearson': (0.25855724925284607, 2.6295028364439225e-08), 'spearman': SpearmanrResult(correlation=0.28929973987389335, pvalue=3.9972500074225107e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.7294301741182402, 4.5435632852841316e-51), 'spearman': SpearmanrResult(correlation=0.6920375413589005, pvalue=4.3856592239240607e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5249345659558011, 2.426416476705698e-54), 'spearman': SpearmanrResult(correlation=0.4850310529656425, pvalue=1.6308417461254162e-45), 'nsamples': 750}, 'images': {'pearson': (0.3920753982081356, 5.728085906385879e-29), 'spearman': SpearmanrResult(correlation=0.40771470191099113, pvalue=2.1022492925434716e-31), 'nsamples': 750}, 'OnWN': {'pearson': (0.6256585048258899, 9.880934434098311e-83), 'spearman': SpearmanrResult(correlation=0.6512478869891886, pvalue=1.0313518518266478e-91), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6394983797281967, 1.7513919583499112e-87), 'spearman': SpearmanrResult(correlation=0.5857092938431223, pvalue=2.8302006106016957e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5283590453481849, 'wmean': 0.5258146535834054}, 'spearman': {'mean': 0.5185067028236231, 'wmean': 0.5160195592353681}}}, 'STS15': {'answers-forums': {'pearson': (0.5800766484585819, 4.2144445072793134e-35), 'spearman': SpearmanrResult(correlation=0.5807415297407167, pvalue=3.388285356839926e-35), 'nsamples': 375}, 'answers-students': {'pearson': (0.6855614236923965, 3.2286314497520674e-105), 'spearman': SpearmanrResult(correlation=0.7084617233292049, pvalue=2.537858753729442e-115), 'nsamples': 750}, 'belief': {'pearson': (0.6604103037068341, 2.4416699863153414e-48), 'spearman': SpearmanrResult(correlation=0.6807955323166872, pvalue=2.2327222885814177e-52), 'nsamples': 375}, 'headlines': {'pearson': (0.5886199972991376, 4.002774165258446e-71), 'spearman': SpearmanrResult(correlation=0.585946713007419, pvalue=2.414632419160079e-70), 'nsamples': 750}, 'images': {'pearson': (0.5473839806978478, 7.548963194310106e-60), 'spearman': SpearmanrResult(correlation=0.5686504761401695, pvalue=1.8270715982534052e-65), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6124104707709596, 'wmean': 0.6104522194430224}, 'spearman': {'mean': 0.6249191949068394, 'wmean': 0.6234568608763739}}}, 'STS16': {'answer-answer': {'pearson': (0.5619224243483099, 1.5276487475984302e-22), 'spearman': SpearmanrResult(correlation=0.5768350407936501, pvalue=6.2760173007695006e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6439419127157453, 1.4746238218924862e-30), 'spearman': SpearmanrResult(correlation=0.6473830814202356, pvalue=5.72370796545616e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7553279974741623, 9.963276358719733e-44), 'spearman': SpearmanrResult(correlation=0.7681734909887558, pvalue=4.810206911407996e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.7845017566405722, 3.9276341274700735e-52), 'spearman': SpearmanrResult(correlation=0.8135216014517299, pvalue=6.692875115218836e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.21264246763354328, 0.0019939696409698193), 'spearman': SpearmanrResult(correlation=0.22099440940303594, pvalue=0.0013018597125619724), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5916673117624666, 'wmean': 0.6008903674750917}, 'spearman': {'mean': 0.6053815248114816, 'wmean': 0.6147398759545499}}}, 'MR': {'devacc': 82.9, 'acc': 81.64, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.85, 'acc': 87.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.07, 'acc': 85.49, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.84, 'acc': 95.15, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.81, 'acc': 87.26, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 47.05, 'acc': 47.78, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 87.01, 'acc': 93.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.46, 'acc': 74.2, 'f1': 79.98, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.4, 'acc': 78.12, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7848223738808127, 'pearson': 0.8009161158023148, 'spearman': 0.7373365248577304, 'mse': 0.3668932184983793, 'yhat': array([3.23246971, 4.54600776, 1.33325878, ..., 3.10069052, 4.40029962,        4.12808245]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7263815849949483, 'pearson': 0.6806907836450307, 'spearman': 0.6804006052818833, 'mse': 1.4247065119465931, 'yhat': array([1.68884107, 1.24671888, 2.38430385, ..., 4.03154623, 4.11053058,        3.39697209]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.22, 'acc': 66.8, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 84.2, 'acc': 84.5, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 55.42, 'acc': 56.09, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.64, 'acc': 37.2, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 73.42, 'acc': 73.12, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.88, 'acc': 92.43, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.2, 'acc': 87.86, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.31, 'acc': 86.02, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.5, 'acc': 84.4, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 69.23, 'acc': 68.03, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 74.27, 'acc': 73.99, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 13:59:36,900 : STS12 p=0.4839, STS12 s=0.5085, STS13 p=0.5003, STS13 s=0.5008, STS14 p=0.5258, STS14 s=0.5160, STS15 p=0.6105, STS15 s=0.6235, STS 16 p=0.6009, STS16 s=0.6147, STS B p=0.6807, STS B s=0.6804, STS B m=1.4247, SICK-R p=0.8009, SICK-R s=0.7373, SICK-P m=0.3669
2019-03-13 13:59:36,900 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 13:59:36,900 : 0.4839,0.5085,0.5003,0.5008,0.5258,0.5160,0.6105,0.6235,0.6009,0.6147,0.6807,0.6804,1.4247,0.8009,0.7373,0.3669
2019-03-13 13:59:36,900 : MR=81.64, CR=87.76, SUBJ=95.15, MPQA=85.49, SST-B=87.26, SST-F=47.78, TREC=93.60, SICK-E=78.12, SNLI=66.80, MRPC=74.20, MRPC f=79.98
2019-03-13 13:59:36,900 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 13:59:36,900 : 81.64,87.76,95.15,85.49,87.26,47.78,93.60,78.12,66.80,74.20,79.98
2019-03-13 13:59:36,900 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 13:59:36,900 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 13:59:36,900 : na,na,na,na,na,na,na,na,na,na
2019-03-13 13:59:36,900 : SentLen=84.50, WC=56.09, TreeDepth=37.20, TopConst=73.12, BShift=92.43, Tense=87.86, SubjNum=86.02, ObjNum=84.40, SOMO=68.03, CoordInv=73.99, average=74.36
2019-03-13 13:59:36,900 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 13:59:36,900 : 84.50,56.09,37.20,73.12,92.43,87.86,86.02,84.40,68.03,73.99,74.36
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,900 : layer 18
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,900 : ********************************************************************************
2019-03-13 13:59:36,993 : ***** Transfer task : STS12 *****


2019-03-13 13:59:37,005 : loading BERT model bert-large-uncased
2019-03-13 13:59:37,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:59:37,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:59:37,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg_zvj2jq
2019-03-13 13:59:44,444 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:59:53,765 : MSRpar : pearson = 0.3707, spearman = 0.4116
2019-03-13 13:59:55,420 : MSRvid : pearson = 0.3783, spearman = 0.4058
2019-03-13 13:59:56,843 : SMTeuroparl : pearson = 0.5235, spearman = 0.6014
2019-03-13 13:59:59,561 : surprise.OnWN : pearson = 0.5670, spearman = 0.5901
2019-03-13 14:00:01,001 : surprise.SMTnews : pearson = 0.6703, spearman = 0.6161
2019-03-13 14:00:01,001 : ALL (weighted average) : Pearson = 0.4809,             Spearman = 0.5076
2019-03-13 14:00:01,001 : ALL (average) : Pearson = 0.5020,             Spearman = 0.5250

2019-03-13 14:00:01,001 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 14:00:01,009 : loading BERT model bert-large-uncased
2019-03-13 14:00:01,010 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:00:01,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:00:01,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpze4wdv9b
2019-03-13 14:00:08,471 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:00:15,189 : FNWN : pearson = 0.3366, spearman = 0.3500
2019-03-13 14:00:17,094 : headlines : pearson = 0.5806, spearman = 0.5615
2019-03-13 14:00:18,574 : OnWN : pearson = 0.4666, spearman = 0.4783
2019-03-13 14:00:18,574 : ALL (weighted average) : Pearson = 0.5072,             Spearman = 0.5037
2019-03-13 14:00:18,574 : ALL (average) : Pearson = 0.4613,             Spearman = 0.4633

2019-03-13 14:00:18,574 : ***** Transfer task : STS14 *****


2019-03-13 14:00:18,618 : loading BERT model bert-large-uncased
2019-03-13 14:00:18,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:00:18,635 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:00:18,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpparhz4bq
2019-03-13 14:00:26,104 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:00:32,708 : deft-forum : pearson = 0.2479, spearman = 0.2741
2019-03-13 14:00:34,357 : deft-news : pearson = 0.7273, spearman = 0.6922
2019-03-13 14:00:36,542 : headlines : pearson = 0.5295, spearman = 0.4903
2019-03-13 14:00:38,645 : images : pearson = 0.4284, spearman = 0.4254
2019-03-13 14:00:40,792 : OnWN : pearson = 0.6380, spearman = 0.6633
2019-03-13 14:00:43,674 : tweet-news : pearson = 0.6429, spearman = 0.5878
2019-03-13 14:00:43,674 : ALL (weighted average) : Pearson = 0.5357,             Spearman = 0.5216
2019-03-13 14:00:43,674 : ALL (average) : Pearson = 0.5357,             Spearman = 0.5222

2019-03-13 14:00:43,675 : ***** Transfer task : STS15 *****


2019-03-13 14:00:43,725 : loading BERT model bert-large-uncased
2019-03-13 14:00:43,725 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:00:43,775 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:00:43,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuz4xzfk4
2019-03-13 14:00:51,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:00:58,421 : answers-forums : pearson = 0.6160, spearman = 0.6126
2019-03-13 14:01:00,522 : answers-students : pearson = 0.6929, spearman = 0.7129
2019-03-13 14:01:02,586 : belief : pearson = 0.6701, spearman = 0.6899
2019-03-13 14:01:04,857 : headlines : pearson = 0.5906, spearman = 0.5873
2019-03-13 14:01:07,009 : images : pearson = 0.5731, spearman = 0.5871
2019-03-13 14:01:07,009 : ALL (weighted average) : Pearson = 0.6249,             Spearman = 0.6346
2019-03-13 14:01:07,009 : ALL (average) : Pearson = 0.6285,             Spearman = 0.6380

2019-03-13 14:01:07,010 : ***** Transfer task : STS16 *****


2019-03-13 14:01:07,047 : loading BERT model bert-large-uncased
2019-03-13 14:01:07,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:01:07,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:01:07,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb3dbw4fj
2019-03-13 14:01:14,577 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:01:20,825 : answer-answer : pearson = 0.5642, spearman = 0.5696
2019-03-13 14:01:21,488 : headlines : pearson = 0.6519, spearman = 0.6564
2019-03-13 14:01:22,375 : plagiarism : pearson = 0.7676, spearman = 0.7792
2019-03-13 14:01:23,878 : postediting : pearson = 0.7927, spearman = 0.8191
2019-03-13 14:01:24,488 : question-question : pearson = 0.2236, spearman = 0.2243
2019-03-13 14:01:24,488 : ALL (weighted average) : Pearson = 0.6091,             Spearman = 0.6190
2019-03-13 14:01:24,488 : ALL (average) : Pearson = 0.6000,             Spearman = 0.6097

2019-03-13 14:01:24,488 : ***** Transfer task : MR *****


2019-03-13 14:01:24,534 : loading BERT model bert-large-uncased
2019-03-13 14:01:24,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:01:24,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:01:24,554 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5r93qou9
2019-03-13 14:01:32,058 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:01:37,423 : Generating sentence embeddings
2019-03-13 14:02:09,140 : Generated sentence embeddings
2019-03-13 14:02:09,141 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:02:18,865 : Best param found at split 1: l2reg = 0.0001                 with score 83.79
2019-03-13 14:02:26,515 : Best param found at split 2: l2reg = 1e-05                 with score 83.6
2019-03-13 14:02:34,752 : Best param found at split 3: l2reg = 0.01                 with score 84.11
2019-03-13 14:02:44,965 : Best param found at split 4: l2reg = 0.0001                 with score 83.88
2019-03-13 14:02:55,059 : Best param found at split 5: l2reg = 1e-05                 with score 83.62
2019-03-13 14:02:55,499 : Dev acc : 83.8 Test acc : 82.55

2019-03-13 14:02:55,500 : ***** Transfer task : CR *****


2019-03-13 14:02:55,508 : loading BERT model bert-large-uncased
2019-03-13 14:02:55,508 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:02:55,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:02:55,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzwa22h0y
2019-03-13 14:03:03,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:03:08,432 : Generating sentence embeddings
2019-03-13 14:03:16,803 : Generated sentence embeddings
2019-03-13 14:03:16,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:03:20,320 : Best param found at split 1: l2reg = 0.01                 with score 88.97
2019-03-13 14:03:24,132 : Best param found at split 2: l2reg = 0.0001                 with score 89.57
2019-03-13 14:03:27,894 : Best param found at split 3: l2reg = 0.01                 with score 90.03
2019-03-13 14:03:31,799 : Best param found at split 4: l2reg = 0.001                 with score 89.57
2019-03-13 14:03:35,449 : Best param found at split 5: l2reg = 0.01                 with score 89.47
2019-03-13 14:03:35,607 : Dev acc : 89.52 Test acc : 88.26

2019-03-13 14:03:35,607 : ***** Transfer task : MPQA *****


2019-03-13 14:03:35,613 : loading BERT model bert-large-uncased
2019-03-13 14:03:35,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:03:35,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:03:35,633 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeb6dq7v7
2019-03-13 14:03:43,076 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:03:48,343 : Generating sentence embeddings
2019-03-13 14:03:55,949 : Generated sentence embeddings
2019-03-13 14:03:55,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:04:06,191 : Best param found at split 1: l2reg = 1e-05                 with score 83.6
2019-03-13 14:04:16,683 : Best param found at split 2: l2reg = 1e-05                 with score 86.34
2019-03-13 14:04:27,780 : Best param found at split 3: l2reg = 0.001                 with score 84.51
2019-03-13 14:04:37,299 : Best param found at split 4: l2reg = 0.001                 with score 85.9
2019-03-13 14:04:46,301 : Best param found at split 5: l2reg = 0.0001                 with score 83.17
2019-03-13 14:04:46,941 : Dev acc : 84.7 Test acc : 85.34

2019-03-13 14:04:46,942 : ***** Transfer task : SUBJ *****


2019-03-13 14:04:46,960 : loading BERT model bert-large-uncased
2019-03-13 14:04:46,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:04:47,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:04:47,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj1g0yrnv
2019-03-13 14:04:54,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:05:00,202 : Generating sentence embeddings
2019-03-13 14:05:31,237 : Generated sentence embeddings
2019-03-13 14:05:31,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:05:38,390 : Best param found at split 1: l2reg = 0.001                 with score 95.89
2019-03-13 14:05:46,008 : Best param found at split 2: l2reg = 0.0001                 with score 96.18
2019-03-13 14:05:54,758 : Best param found at split 3: l2reg = 0.0001                 with score 95.6
2019-03-13 14:06:03,874 : Best param found at split 4: l2reg = 0.001                 with score 96.11
2019-03-13 14:06:13,450 : Best param found at split 5: l2reg = 0.001                 with score 95.96
2019-03-13 14:06:13,947 : Dev acc : 95.95 Test acc : 95.89

2019-03-13 14:06:13,948 : ***** Transfer task : SST Binary classification *****


2019-03-13 14:06:14,086 : loading BERT model bert-large-uncased
2019-03-13 14:06:14,087 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:06:14,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:06:14,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg1xhid2y
2019-03-13 14:06:21,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:06:26,801 : Computing embedding for train
2019-03-13 14:08:07,588 : Computed train embeddings
2019-03-13 14:08:07,588 : Computing embedding for dev
2019-03-13 14:08:09,784 : Computed dev embeddings
2019-03-13 14:08:09,784 : Computing embedding for test
2019-03-13 14:08:14,410 : Computed test embeddings
2019-03-13 14:08:14,410 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:08:32,497 : [('reg:1e-05', 87.04), ('reg:0.0001', 87.5), ('reg:0.001', 86.47), ('reg:0.01', 85.67)]
2019-03-13 14:08:32,497 : Validation : best param found is reg = 0.0001 with score             87.5
2019-03-13 14:08:32,498 : Evaluating...
2019-03-13 14:08:37,607 : 
Dev acc : 87.5 Test acc : 88.52 for             SST Binary classification

2019-03-13 14:08:37,607 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 14:08:37,670 : loading BERT model bert-large-uncased
2019-03-13 14:08:37,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:08:37,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:08:37,694 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8l86o7_d
2019-03-13 14:08:45,172 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:08:50,460 : Computing embedding for train
2019-03-13 14:09:12,512 : Computed train embeddings
2019-03-13 14:09:12,513 : Computing embedding for dev
2019-03-13 14:09:15,393 : Computed dev embeddings
2019-03-13 14:09:15,393 : Computing embedding for test
2019-03-13 14:09:21,065 : Computed test embeddings
2019-03-13 14:09:21,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:09:23,492 : [('reg:1e-05', 45.5), ('reg:0.0001', 45.69), ('reg:0.001', 46.14), ('reg:0.01', 46.14)]
2019-03-13 14:09:23,492 : Validation : best param found is reg = 0.001 with score             46.14
2019-03-13 14:09:23,492 : Evaluating...
2019-03-13 14:09:24,115 : 
Dev acc : 46.14 Test acc : 49.32 for             SST Fine-Grained classification

2019-03-13 14:09:24,116 : ***** Transfer task : TREC *****


2019-03-13 14:09:24,129 : loading BERT model bert-large-uncased
2019-03-13 14:09:24,129 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:09:24,148 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:09:24,148 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpklgci128
2019-03-13 14:09:31,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:09:44,599 : Computed train embeddings
2019-03-13 14:09:45,191 : Computed test embeddings
2019-03-13 14:09:45,191 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:09:52,451 : [('reg:1e-05', 86.22), ('reg:0.0001', 86.43), ('reg:0.001', 84.19), ('reg:0.01', 79.18)]
2019-03-13 14:09:52,451 : Cross-validation : best param found is reg = 0.0001             with score 86.43
2019-03-13 14:09:52,451 : Evaluating...
2019-03-13 14:09:52,961 : 
Dev acc : 86.43 Test acc : 92.6             for TREC

2019-03-13 14:09:52,961 : ***** Transfer task : MRPC *****


2019-03-13 14:09:52,983 : loading BERT model bert-large-uncased
2019-03-13 14:09:52,983 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:09:53,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:09:53,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz1yirif6
2019-03-13 14:10:00,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:10:05,741 : Computing embedding for train
2019-03-13 14:10:28,116 : Computed train embeddings
2019-03-13 14:10:28,116 : Computing embedding for test
2019-03-13 14:10:37,917 : Computed test embeddings
2019-03-13 14:10:37,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:10:41,711 : [('reg:1e-05', 74.14), ('reg:0.0001', 73.97), ('reg:0.001', 74.43), ('reg:0.01', 73.26)]
2019-03-13 14:10:41,711 : Cross-validation : best param found is reg = 0.001             with score 74.43
2019-03-13 14:10:41,712 : Evaluating...
2019-03-13 14:10:41,951 : Dev acc : 74.43 Test acc 75.65; Test F1 81.94 for MRPC.

2019-03-13 14:10:41,951 : ***** Transfer task : SICK-Entailment*****


2019-03-13 14:10:41,974 : loading BERT model bert-large-uncased
2019-03-13 14:10:41,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:10:41,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:10:41,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzkr4xxwe
2019-03-13 14:10:49,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:10:54,761 : Computing embedding for train
2019-03-13 14:11:06,147 : Computed train embeddings
2019-03-13 14:11:06,147 : Computing embedding for dev
2019-03-13 14:11:07,696 : Computed dev embeddings
2019-03-13 14:11:07,696 : Computing embedding for test
2019-03-13 14:11:19,916 : Computed test embeddings
2019-03-13 14:11:19,954 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:11:21,417 : [('reg:1e-05', 77.6), ('reg:0.0001', 77.6), ('reg:0.001', 78.2), ('reg:0.01', 78.2)]
2019-03-13 14:11:21,417 : Validation : best param found is reg = 0.001 with score             78.2
2019-03-13 14:11:21,418 : Evaluating...
2019-03-13 14:11:21,816 : 
Dev acc : 78.2 Test acc : 75.1 for                        SICK entailment

2019-03-13 14:11:21,816 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 14:11:21,842 : loading BERT model bert-large-uncased
2019-03-13 14:11:21,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:11:21,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:11:21,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkw97miyk
2019-03-13 14:11:29,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:11:34,570 : Computing embedding for train
2019-03-13 14:11:45,955 : Computed train embeddings
2019-03-13 14:11:45,956 : Computing embedding for dev
2019-03-13 14:11:47,510 : Computed dev embeddings
2019-03-13 14:11:47,510 : Computing embedding for test
2019-03-13 14:11:59,716 : Computed test embeddings
2019-03-13 14:12:14,463 : Dev : Pearson 0.784216861158798
2019-03-13 14:12:14,463 : Test : Pearson 0.8004781372207789 Spearman 0.7413725257018436 MSE 0.36665186357041335                        for SICK Relatedness

2019-03-13 14:12:14,466 : 

***** Transfer task : STSBenchmark*****


2019-03-13 14:12:14,534 : loading BERT model bert-large-uncased
2019-03-13 14:12:14,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:12:14,601 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:12:14,601 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp675ck6ht
2019-03-13 14:12:22,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:12:27,316 : Computing embedding for train
2019-03-13 14:12:46,024 : Computed train embeddings
2019-03-13 14:12:46,024 : Computing embedding for dev
2019-03-13 14:12:51,702 : Computed dev embeddings
2019-03-13 14:12:51,702 : Computing embedding for test
2019-03-13 14:12:56,344 : Computed test embeddings
2019-03-13 14:13:12,646 : Dev : Pearson 0.72494929147405
2019-03-13 14:13:12,646 : Test : Pearson 0.6810220966317319 Spearman 0.6810141573832883 MSE 1.4501844075012633                        for SICK Relatedness

2019-03-13 14:13:12,646 : ***** Transfer task : SNLI Entailment*****


2019-03-13 14:13:17,666 : loading BERT model bert-large-uncased
2019-03-13 14:13:17,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:13:17,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:13:17,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyvc5drtl
2019-03-13 14:13:25,242 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:13:31,002 : PROGRESS (encoding): 0.00%
2019-03-13 14:16:18,499 : PROGRESS (encoding): 14.56%
2019-03-13 14:19:30,571 : PROGRESS (encoding): 29.12%
2019-03-13 14:22:41,895 : PROGRESS (encoding): 43.69%
2019-03-13 14:26:05,567 : PROGRESS (encoding): 58.25%
2019-03-13 14:29:52,523 : PROGRESS (encoding): 72.81%
2019-03-13 14:33:38,049 : PROGRESS (encoding): 87.37%
2019-03-13 14:37:42,176 : PROGRESS (encoding): 0.00%
2019-03-13 14:38:12,827 : PROGRESS (encoding): 0.00%
2019-03-13 14:38:42,259 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:39:13,561 : [('reg:1e-09', 68.16)]
2019-03-13 14:39:13,561 : Validation : best param found is reg = 1e-09 with score             68.16
2019-03-13 14:39:13,561 : Evaluating...
2019-03-13 14:39:45,746 : Dev acc : 68.16 Test acc : 68.68 for SNLI

2019-03-13 14:39:45,746 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 14:39:45,955 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 14:39:47,000 : loading BERT model bert-large-uncased
2019-03-13 14:39:47,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:39:47,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:39:47,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmmcq8cah
2019-03-13 14:39:54,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:39:59,725 : Computing embeddings for train/dev/test
2019-03-13 14:43:33,102 : Computed embeddings
2019-03-13 14:43:33,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:44:04,007 : [('reg:1e-05', 80.92), ('reg:0.0001', 79.5), ('reg:0.001', 72.18), ('reg:0.01', 58.17)]
2019-03-13 14:44:04,008 : Validation : best param found is reg = 1e-05 with score             80.92
2019-03-13 14:44:04,008 : Evaluating...
2019-03-13 14:44:11,587 : 
Dev acc : 80.9 Test acc : 82.1 for LENGTH classification

2019-03-13 14:44:11,588 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 14:44:11,948 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 14:44:11,992 : loading BERT model bert-large-uncased
2019-03-13 14:44:11,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:44:12,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:44:12,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppffe3hca
2019-03-13 14:44:19,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:44:24,754 : Computing embeddings for train/dev/test
2019-03-13 14:47:41,570 : Computed embeddings
2019-03-13 14:47:41,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:48:10,402 : [('reg:1e-05', 52.69), ('reg:0.0001', 32.47), ('reg:0.001', 4.41), ('reg:0.01', 0.86)]
2019-03-13 14:48:10,402 : Validation : best param found is reg = 1e-05 with score             52.69
2019-03-13 14:48:10,402 : Evaluating...
2019-03-13 14:48:20,340 : 
Dev acc : 52.7 Test acc : 53.1 for WORDCONTENT classification

2019-03-13 14:48:20,341 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 14:48:20,697 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 14:48:20,763 : loading BERT model bert-large-uncased
2019-03-13 14:48:20,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:48:20,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:48:20,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5vmom4zy
2019-03-13 14:48:28,218 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:48:33,545 : Computing embeddings for train/dev/test
2019-03-13 14:51:39,276 : Computed embeddings
2019-03-13 14:51:39,276 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:51:58,072 : [('reg:1e-05', 36.63), ('reg:0.0001', 36.09), ('reg:0.001', 34.36), ('reg:0.01', 28.58)]
2019-03-13 14:51:58,072 : Validation : best param found is reg = 1e-05 with score             36.63
2019-03-13 14:51:58,072 : Evaluating...
2019-03-13 14:52:03,699 : 
Dev acc : 36.6 Test acc : 36.4 for DEPTH classification

2019-03-13 14:52:03,700 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 14:52:04,085 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 14:52:04,148 : loading BERT model bert-large-uncased
2019-03-13 14:52:04,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:52:04,175 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:52:04,176 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkv2o_lcx
2019-03-13 14:52:11,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:52:16,817 : Computing embeddings for train/dev/test
2019-03-13 14:55:08,686 : Computed embeddings
2019-03-13 14:55:08,686 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:55:39,479 : [('reg:1e-05', 74.66), ('reg:0.0001', 72.98), ('reg:0.001', 67.96), ('reg:0.01', 58.12)]
2019-03-13 14:55:39,480 : Validation : best param found is reg = 1e-05 with score             74.66
2019-03-13 14:55:39,480 : Evaluating...
2019-03-13 14:55:45,653 : 
Dev acc : 74.7 Test acc : 74.7 for TOPCONSTITUENTS classification

2019-03-13 14:55:45,654 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 14:55:46,006 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 14:55:46,071 : loading BERT model bert-large-uncased
2019-03-13 14:55:46,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:55:46,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:55:46,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4139cw8m
2019-03-13 14:55:53,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:55:58,925 : Computing embeddings for train/dev/test
2019-03-13 14:59:05,325 : Computed embeddings
2019-03-13 14:59:05,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:59:32,033 : [('reg:1e-05', 92.53), ('reg:0.0001', 92.61), ('reg:0.001', 92.34), ('reg:0.01', 91.28)]
2019-03-13 14:59:32,033 : Validation : best param found is reg = 0.0001 with score             92.61
2019-03-13 14:59:32,033 : Evaluating...
2019-03-13 14:59:38,481 : 
Dev acc : 92.6 Test acc : 92.2 for BIGRAMSHIFT classification

2019-03-13 14:59:38,482 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 14:59:38,861 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 14:59:38,926 : loading BERT model bert-large-uncased
2019-03-13 14:59:38,927 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:59:39,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:59:39,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfo30m1po
2019-03-13 14:59:46,466 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:59:51,763 : Computing embeddings for train/dev/test
2019-03-13 15:02:53,297 : Computed embeddings
2019-03-13 15:02:53,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:03:11,645 : [('reg:1e-05', 89.59), ('reg:0.0001', 89.74), ('reg:0.001', 90.25), ('reg:0.01', 90.34)]
2019-03-13 15:03:11,645 : Validation : best param found is reg = 0.01 with score             90.34
2019-03-13 15:03:11,645 : Evaluating...
2019-03-13 15:03:16,569 : 
Dev acc : 90.3 Test acc : 88.4 for TENSE classification

2019-03-13 15:03:16,570 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 15:03:17,135 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 15:03:17,198 : loading BERT model bert-large-uncased
2019-03-13 15:03:17,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:03:17,224 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:03:17,224 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz0090y01
2019-03-13 15:03:24,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:03:29,984 : Computing embeddings for train/dev/test
2019-03-13 15:06:42,366 : Computed embeddings
2019-03-13 15:06:42,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:07:13,336 : [('reg:1e-05', 86.18), ('reg:0.0001', 86.19), ('reg:0.001', 86.0), ('reg:0.01', 85.09)]
2019-03-13 15:07:13,336 : Validation : best param found is reg = 0.0001 with score             86.19
2019-03-13 15:07:13,336 : Evaluating...
2019-03-13 15:07:23,182 : 
Dev acc : 86.2 Test acc : 85.9 for SUBJNUMBER classification

2019-03-13 15:07:23,184 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 15:07:23,612 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 15:07:23,679 : loading BERT model bert-large-uncased
2019-03-13 15:07:23,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:07:23,705 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:07:23,705 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9mnrj7ss
2019-03-13 15:07:31,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:07:36,475 : Computing embeddings for train/dev/test
2019-03-13 15:10:45,068 : Computed embeddings
2019-03-13 15:10:45,068 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:11:11,522 : [('reg:1e-05', 82.51), ('reg:0.0001', 83.1), ('reg:0.001', 83.13), ('reg:0.01', 81.78)]
2019-03-13 15:11:11,522 : Validation : best param found is reg = 0.001 with score             83.13
2019-03-13 15:11:11,522 : Evaluating...
2019-03-13 15:11:20,239 : 
Dev acc : 83.1 Test acc : 83.6 for OBJNUMBER classification

2019-03-13 15:11:20,240 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 15:11:20,629 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 15:11:20,697 : loading BERT model bert-large-uncased
2019-03-13 15:11:20,698 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:11:20,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:11:20,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp69r2lrqs
2019-03-13 15:11:28,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:11:33,496 : Computing embeddings for train/dev/test
2019-03-13 15:15:13,097 : Computed embeddings
2019-03-13 15:15:13,097 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:15:37,467 : [('reg:1e-05', 69.44), ('reg:0.0001', 69.46), ('reg:0.001', 69.35), ('reg:0.01', 68.27)]
2019-03-13 15:15:37,467 : Validation : best param found is reg = 0.0001 with score             69.46
2019-03-13 15:15:37,467 : Evaluating...
2019-03-13 15:15:44,969 : 
Dev acc : 69.5 Test acc : 69.0 for ODDMANOUT classification

2019-03-13 15:15:44,970 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 15:15:45,586 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 15:15:45,663 : loading BERT model bert-large-uncased
2019-03-13 15:15:45,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:15:45,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:15:45,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdrzcw6n8
2019-03-13 15:15:53,155 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:15:58,347 : Computing embeddings for train/dev/test
2019-03-13 15:19:37,104 : Computed embeddings
2019-03-13 15:19:37,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:20:06,311 : [('reg:1e-05', 73.81), ('reg:0.0001', 76.1), ('reg:0.001', 75.68), ('reg:0.01', 72.41)]
2019-03-13 15:20:06,311 : Validation : best param found is reg = 0.0001 with score             76.1
2019-03-13 15:20:06,311 : Evaluating...
2019-03-13 15:20:13,813 : 
Dev acc : 76.1 Test acc : 76.0 for COORDINATIONINVERSION classification

2019-03-13 15:20:13,815 : total results: {'STS12': {'MSRpar': {'pearson': (0.3707038049791377, 7.602879115171908e-26), 'spearman': SpearmanrResult(correlation=0.4115762080247153, pvalue=5.02530717345725e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3782763442802274, 6.321721459542812e-27), 'spearman': SpearmanrResult(correlation=0.40583674449211804, pvalue=4.1881147845737445e-31), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5235091947250954, 1.163880485689415e-33), 'spearman': SpearmanrResult(correlation=0.6013766685901701, pvalue=1.7631434466807906e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5670099594417175, 5.1240582783559806e-65), 'spearman': SpearmanrResult(correlation=0.5901352701416611, pvalue=1.434573190163741e-71), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6702543209480017, 2.2678179697943587e-53), 'spearman': SpearmanrResult(correlation=0.616125893922224, pvalue=4.51487912227769e-43), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5019507248748359, 'wmean': 0.4809249600910179}, 'spearman': {'mean': 0.5250101570341777, 'wmean': 0.507569912983181}}}, 'STS13': {'FNWN': {'pearson': (0.33657714365481056, 2.1865766852482022e-06), 'spearman': SpearmanrResult(correlation=0.3500344133018121, pvalue=7.919119960531427e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.5806382641450767, 8.153999233113003e-69), 'spearman': SpearmanrResult(correlation=0.5615146576028933, pvalue=1.5547585362631001e-63), 'nsamples': 750}, 'OnWN': {'pearson': (0.4665592834896737, 1.142143756071408e-31), 'spearman': SpearmanrResult(correlation=0.47826828354790946, pvalue=2.0813024327614913e-33), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46125823042985364, 'wmean': 0.5072210241981825}, 'spearman': {'mean': 0.463272451484205, 'wmean': 0.5037340029243931}}}, 'STS14': {'deft-forum': {'pearson': (0.24792029530844634, 9.932214505132468e-08), 'spearman': SpearmanrResult(correlation=0.2741388727312806, pvalue=3.3626067409904352e-09), 'nsamples': 450}, 'deft-news': {'pearson': (0.7273339606257515, 1.2010223320851095e-50), 'spearman': SpearmanrResult(correlation=0.692159066215637, pvalue=4.1789474355568106e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5295427404802844, 1.941152442885146e-55), 'spearman': SpearmanrResult(correlation=0.4903120750610987, pvalue=1.2884455565560306e-46), 'nsamples': 750}, 'images': {'pearson': (0.4283932458561552, 7.905777168648923e-35), 'spearman': SpearmanrResult(correlation=0.42535361342673944, pvalue=2.608708032511861e-34), 'nsamples': 750}, 'OnWN': {'pearson': (0.63797615382425, 5.997049875708941e-87), 'spearman': SpearmanrResult(correlation=0.6633036287604179, pvalue=2.9711654401256782e-96), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6428809116562826, 1.1082984348992411e-88), 'spearman': SpearmanrResult(correlation=0.5878444196122096, pvalue=6.7537819495282e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5356745512918617, 'wmean': 0.5356957626504681}, 'spearman': {'mean': 0.5221852793012306, 'wmean': 0.5216321373970978}}}, 'STS15': {'answers-forums': {'pearson': (0.6159575456528816, 1.5250434951937576e-40), 'spearman': SpearmanrResult(correlation=0.6125833281526014, pvalue=5.307134062797317e-40), 'nsamples': 375}, 'answers-students': {'pearson': (0.6929161002350928, 2.3298458288305637e-108), 'spearman': SpearmanrResult(correlation=0.7128869879342673, pvalue=2.175594019495577e-117), 'nsamples': 750}, 'belief': {'pearson': (0.6700540291912815, 3.289309039072246e-50), 'spearman': SpearmanrResult(correlation=0.6899431311676825, pvalue=2.67715514734105e-54), 'nsamples': 375}, 'headlines': {'pearson': (0.590553925003044, 1.0793964871909406e-71), 'spearman': SpearmanrResult(correlation=0.5873245967085778, pvalue=9.582365255439645e-71), 'nsamples': 750}, 'images': {'pearson': (0.5731098066661786, 1.0753783576435692e-66), 'spearman': SpearmanrResult(correlation=0.5870735645453264, pvalue=1.1343368125723186e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6285182813496958, 'wmean': 0.6248964048315993}, 'spearman': {'mean': 0.6379623217016912, 'wmean': 0.6346370947120784}}}, 'STS16': {'answer-answer': {'pearson': (0.5642330249774883, 9.414328098899521e-23), 'spearman': SpearmanrResult(correlation=0.5696405615043207, pvalue=2.9873961908298373e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6518918262912118, 1.6258546961171686e-31), 'spearman': SpearmanrResult(correlation=0.6564481743230192, pvalue=4.4584792212894104e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7676011142969921, 6.1452739014270656e-46), 'spearman': SpearmanrResult(correlation=0.7792216651275599, pvalue=3.6887954361665986e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.792719141113391, 6.138527049253512e-54), 'spearman': SpearmanrResult(correlation=0.8191015692427916, pvalue=2.4416240144906118e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.22358404835250323, 0.0011370131174969237), 'spearman': SpearmanrResult(correlation=0.224263169288884, pvalue=0.0010970747111759317), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6000058310063172, 'wmean': 0.609053158445567}, 'spearman': {'mean': 0.609735027897315, 'wmean': 0.6189688585872564}}}, 'MR': {'devacc': 83.8, 'acc': 82.55, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.52, 'acc': 88.26, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.7, 'acc': 85.34, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.95, 'acc': 95.89, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.5, 'acc': 88.52, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 46.14, 'acc': 49.32, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.43, 'acc': 92.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.43, 'acc': 75.65, 'f1': 81.94, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 75.1, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.784216861158798, 'pearson': 0.8004781372207789, 'spearman': 0.7413725257018436, 'mse': 0.36665186357041335, 'yhat': array([3.47860403, 4.74209998, 1.45417555, ..., 3.23252535, 4.49940887,        4.30132611]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.72494929147405, 'pearson': 0.6810220966317319, 'spearman': 0.6810141573832883, 'mse': 1.4501844075012633, 'yhat': array([1.66871006, 1.49893345, 2.70581697, ..., 4.0981553 , 3.60013901,        3.45800229]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.16, 'acc': 68.68, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 80.92, 'acc': 82.1, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 52.69, 'acc': 53.09, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 36.63, 'acc': 36.43, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.66, 'acc': 74.72, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.61, 'acc': 92.19, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.34, 'acc': 88.38, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.19, 'acc': 85.91, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.13, 'acc': 83.62, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 69.46, 'acc': 69.04, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 76.1, 'acc': 76.0, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 15:20:13,815 : STS12 p=0.4809, STS12 s=0.5076, STS13 p=0.5072, STS13 s=0.5037, STS14 p=0.5357, STS14 s=0.5216, STS15 p=0.6249, STS15 s=0.6346, STS 16 p=0.6091, STS16 s=0.6190, STS B p=0.6810, STS B s=0.6810, STS B m=1.4502, SICK-R p=0.8005, SICK-R s=0.7414, SICK-P m=0.3667
2019-03-13 15:20:13,815 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 15:20:13,815 : 0.4809,0.5076,0.5072,0.5037,0.5357,0.5216,0.6249,0.6346,0.6091,0.6190,0.6810,0.6810,1.4502,0.8005,0.7414,0.3667
2019-03-13 15:20:13,815 : MR=82.55, CR=88.26, SUBJ=95.89, MPQA=85.34, SST-B=88.52, SST-F=49.32, TREC=92.60, SICK-E=75.10, SNLI=68.68, MRPC=75.65, MRPC f=81.94
2019-03-13 15:20:13,815 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 15:20:13,815 : 82.55,88.26,95.89,85.34,88.52,49.32,92.60,75.10,68.68,75.65,81.94
2019-03-13 15:20:13,815 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 15:20:13,815 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 15:20:13,815 : na,na,na,na,na,na,na,na,na,na
2019-03-13 15:20:13,815 : SentLen=82.10, WC=53.09, TreeDepth=36.43, TopConst=74.72, BShift=92.19, Tense=88.38, SubjNum=85.91, ObjNum=83.62, SOMO=69.04, CoordInv=76.00, average=74.15
2019-03-13 15:20:13,815 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 15:20:13,815 : 82.10,53.09,36.43,74.72,92.19,88.38,85.91,83.62,69.04,76.00,74.15
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,815 : layer 19
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,815 : ********************************************************************************
2019-03-13 15:20:13,907 : ***** Transfer task : STS12 *****


2019-03-13 15:20:13,919 : loading BERT model bert-large-uncased
2019-03-13 15:20:13,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:20:13,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:20:13,937 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy41jipew
2019-03-13 15:20:21,367 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:20:30,693 : MSRpar : pearson = 0.3703, spearman = 0.4091
2019-03-13 15:20:32,347 : MSRvid : pearson = 0.3660, spearman = 0.3982
2019-03-13 15:20:33,770 : SMTeuroparl : pearson = 0.5204, spearman = 0.5951
2019-03-13 15:20:36,490 : surprise.OnWN : pearson = 0.5567, spearman = 0.5863
2019-03-13 15:20:37,929 : surprise.SMTnews : pearson = 0.6752, spearman = 0.6276
2019-03-13 15:20:37,929 : ALL (weighted average) : Pearson = 0.4756,             Spearman = 0.5048
2019-03-13 15:20:37,930 : ALL (average) : Pearson = 0.4977,             Spearman = 0.5233

2019-03-13 15:20:37,930 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 15:20:37,938 : loading BERT model bert-large-uncased
2019-03-13 15:20:37,938 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:20:37,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:20:37,956 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgkafof8e
2019-03-13 15:20:45,381 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:20:52,389 : FNWN : pearson = 0.3642, spearman = 0.3751
2019-03-13 15:20:54,295 : headlines : pearson = 0.5814, spearman = 0.5619
2019-03-13 15:20:55,773 : OnWN : pearson = 0.4669, spearman = 0.4781
2019-03-13 15:20:55,773 : ALL (weighted average) : Pearson = 0.5112,             Spearman = 0.5070
2019-03-13 15:20:55,773 : ALL (average) : Pearson = 0.4708,             Spearman = 0.4717

2019-03-13 15:20:55,773 : ***** Transfer task : STS14 *****


2019-03-13 15:20:55,816 : loading BERT model bert-large-uncased
2019-03-13 15:20:55,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:20:55,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:20:55,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps03f22m9
2019-03-13 15:21:03,249 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:21:10,058 : deft-forum : pearson = 0.2488, spearman = 0.2717
2019-03-13 15:21:11,706 : deft-news : pearson = 0.7192, spearman = 0.6838
2019-03-13 15:21:13,894 : headlines : pearson = 0.5374, spearman = 0.4975
2019-03-13 15:21:15,988 : images : pearson = 0.4546, spearman = 0.4442
2019-03-13 15:21:18,136 : OnWN : pearson = 0.6423, spearman = 0.6664
2019-03-13 15:21:21,023 : tweet-news : pearson = 0.6497, spearman = 0.5893
2019-03-13 15:21:21,023 : ALL (weighted average) : Pearson = 0.5442,             Spearman = 0.5268
2019-03-13 15:21:21,023 : ALL (average) : Pearson = 0.5420,             Spearman = 0.5255

2019-03-13 15:21:21,024 : ***** Transfer task : STS15 *****


2019-03-13 15:21:21,074 : loading BERT model bert-large-uncased
2019-03-13 15:21:21,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:21:21,123 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:21:21,123 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgpk5xno1
2019-03-13 15:21:28,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:21:35,785 : answers-forums : pearson = 0.6201, spearman = 0.6150
2019-03-13 15:21:37,885 : answers-students : pearson = 0.6941, spearman = 0.7117
2019-03-13 15:21:39,949 : belief : pearson = 0.6866, spearman = 0.6994
2019-03-13 15:21:42,211 : headlines : pearson = 0.6066, spearman = 0.6037
2019-03-13 15:21:44,357 : images : pearson = 0.5858, spearman = 0.5957
2019-03-13 15:21:44,357 : ALL (weighted average) : Pearson = 0.6350,             Spearman = 0.6421
2019-03-13 15:21:44,357 : ALL (average) : Pearson = 0.6387,             Spearman = 0.6451

2019-03-13 15:21:44,358 : ***** Transfer task : STS16 *****


2019-03-13 15:21:44,395 : loading BERT model bert-large-uncased
2019-03-13 15:21:44,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:21:44,413 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:21:44,413 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnedkaum7
2019-03-13 15:21:51,853 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:21:58,000 : answer-answer : pearson = 0.5482, spearman = 0.5538
2019-03-13 15:21:58,664 : headlines : pearson = 0.6520, spearman = 0.6525
2019-03-13 15:21:59,551 : plagiarism : pearson = 0.7762, spearman = 0.7904
2019-03-13 15:22:01,056 : postediting : pearson = 0.7997, spearman = 0.8187
2019-03-13 15:22:01,666 : question-question : pearson = 0.2565, spearman = 0.2571
2019-03-13 15:22:01,666 : ALL (weighted average) : Pearson = 0.6145,             Spearman = 0.6226
2019-03-13 15:22:01,666 : ALL (average) : Pearson = 0.6065,             Spearman = 0.6145

2019-03-13 15:22:01,666 : ***** Transfer task : MR *****


2019-03-13 15:22:01,712 : loading BERT model bert-large-uncased
2019-03-13 15:22:01,712 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:22:01,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:22:01,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz1t7fmbh
2019-03-13 15:22:09,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:22:14,418 : Generating sentence embeddings
2019-03-13 15:22:46,123 : Generated sentence embeddings
2019-03-13 15:22:46,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:22:56,894 : Best param found at split 1: l2reg = 0.001                 with score 83.99
2019-03-13 15:23:07,391 : Best param found at split 2: l2reg = 0.0001                 with score 84.06
2019-03-13 15:23:16,812 : Best param found at split 3: l2reg = 0.001                 with score 84.09
2019-03-13 15:23:27,795 : Best param found at split 4: l2reg = 0.01                 with score 84.21
2019-03-13 15:23:37,823 : Best param found at split 5: l2reg = 0.001                 with score 84.01
2019-03-13 15:23:38,273 : Dev acc : 84.07 Test acc : 82.64

2019-03-13 15:23:38,274 : ***** Transfer task : CR *****


2019-03-13 15:23:38,282 : loading BERT model bert-large-uncased
2019-03-13 15:23:38,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:23:38,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:23:38,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqvwzqnhr
2019-03-13 15:23:45,807 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:23:51,227 : Generating sentence embeddings
2019-03-13 15:23:59,584 : Generated sentence embeddings
2019-03-13 15:23:59,584 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:24:02,614 : Best param found at split 1: l2reg = 0.001                 with score 89.04
2019-03-13 15:24:06,348 : Best param found at split 2: l2reg = 0.001                 with score 89.14
2019-03-13 15:24:09,888 : Best param found at split 3: l2reg = 0.01                 with score 90.6
2019-03-13 15:24:13,299 : Best param found at split 4: l2reg = 0.001                 with score 89.28
2019-03-13 15:24:16,664 : Best param found at split 5: l2reg = 0.001                 with score 89.9
2019-03-13 15:24:16,781 : Dev acc : 89.59 Test acc : 89.03

2019-03-13 15:24:16,781 : ***** Transfer task : MPQA *****


2019-03-13 15:24:16,786 : loading BERT model bert-large-uncased
2019-03-13 15:24:16,786 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:24:16,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:24:16,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7xqknz5x
2019-03-13 15:24:24,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:24:29,543 : Generating sentence embeddings
2019-03-13 15:24:37,177 : Generated sentence embeddings
2019-03-13 15:24:37,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:24:46,547 : Best param found at split 1: l2reg = 0.001                 with score 82.6
2019-03-13 15:24:56,194 : Best param found at split 2: l2reg = 0.0001                 with score 85.86
2019-03-13 15:25:06,385 : Best param found at split 3: l2reg = 0.001                 with score 85.53
2019-03-13 15:25:17,408 : Best param found at split 4: l2reg = 1e-05                 with score 85.46
2019-03-13 15:25:26,886 : Best param found at split 5: l2reg = 0.01                 with score 84.11
2019-03-13 15:25:27,579 : Dev acc : 84.71 Test acc : 85.96

2019-03-13 15:25:27,580 : ***** Transfer task : SUBJ *****


2019-03-13 15:25:27,596 : loading BERT model bert-large-uncased
2019-03-13 15:25:27,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:25:27,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:25:27,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcnnceqe8
2019-03-13 15:25:35,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:25:40,470 : Generating sentence embeddings
2019-03-13 15:26:11,521 : Generated sentence embeddings
2019-03-13 15:26:11,522 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:26:21,654 : Best param found at split 1: l2reg = 0.001                 with score 96.0
2019-03-13 15:26:32,445 : Best param found at split 2: l2reg = 0.001                 with score 96.09
2019-03-13 15:26:43,533 : Best param found at split 3: l2reg = 0.001                 with score 95.69
2019-03-13 15:26:53,484 : Best param found at split 4: l2reg = 0.001                 with score 96.11
2019-03-13 15:27:02,912 : Best param found at split 5: l2reg = 0.001                 with score 96.02
2019-03-13 15:27:03,408 : Dev acc : 95.98 Test acc : 95.73

2019-03-13 15:27:03,409 : ***** Transfer task : SST Binary classification *****


2019-03-13 15:27:03,548 : loading BERT model bert-large-uncased
2019-03-13 15:27:03,548 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:27:03,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:27:03,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpevyrl1lt
2019-03-13 15:27:11,126 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:27:16,559 : Computing embedding for train
2019-03-13 15:28:57,335 : Computed train embeddings
2019-03-13 15:28:57,335 : Computing embedding for dev
2019-03-13 15:28:59,535 : Computed dev embeddings
2019-03-13 15:28:59,535 : Computing embedding for test
2019-03-13 15:29:04,166 : Computed test embeddings
2019-03-13 15:29:04,166 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:29:19,567 : [('reg:1e-05', 88.53), ('reg:0.0001', 88.42), ('reg:0.001', 87.61), ('reg:0.01', 86.47)]
2019-03-13 15:29:19,567 : Validation : best param found is reg = 1e-05 with score             88.53
2019-03-13 15:29:19,568 : Evaluating...
2019-03-13 15:29:24,568 : 
Dev acc : 88.53 Test acc : 88.63 for             SST Binary classification

2019-03-13 15:29:24,568 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 15:29:24,618 : loading BERT model bert-large-uncased
2019-03-13 15:29:24,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:29:24,640 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:29:24,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk3kx8k26
2019-03-13 15:29:32,041 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:29:37,253 : Computing embedding for train
2019-03-13 15:29:59,329 : Computed train embeddings
2019-03-13 15:29:59,329 : Computing embedding for dev
2019-03-13 15:30:02,207 : Computed dev embeddings
2019-03-13 15:30:02,207 : Computing embedding for test
2019-03-13 15:30:07,886 : Computed test embeddings
2019-03-13 15:30:07,886 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:30:10,355 : [('reg:1e-05', 48.77), ('reg:0.0001', 48.96), ('reg:0.001', 48.41), ('reg:0.01', 47.05)]
2019-03-13 15:30:10,355 : Validation : best param found is reg = 0.0001 with score             48.96
2019-03-13 15:30:10,355 : Evaluating...
2019-03-13 15:30:10,996 : 
Dev acc : 48.96 Test acc : 47.83 for             SST Fine-Grained classification

2019-03-13 15:30:10,996 : ***** Transfer task : TREC *****


2019-03-13 15:30:11,009 : loading BERT model bert-large-uncased
2019-03-13 15:30:11,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:30:11,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:30:11,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfsmoc17p
2019-03-13 15:30:18,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:30:31,359 : Computed train embeddings
2019-03-13 15:30:31,952 : Computed test embeddings
2019-03-13 15:30:31,953 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:30:38,882 : [('reg:1e-05', 86.34), ('reg:0.0001', 86.3), ('reg:0.001', 84.92), ('reg:0.01', 79.31)]
2019-03-13 15:30:38,882 : Cross-validation : best param found is reg = 1e-05             with score 86.34
2019-03-13 15:30:38,882 : Evaluating...
2019-03-13 15:30:39,254 : 
Dev acc : 86.34 Test acc : 91.2             for TREC

2019-03-13 15:30:39,255 : ***** Transfer task : MRPC *****


2019-03-13 15:30:39,277 : loading BERT model bert-large-uncased
2019-03-13 15:30:39,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:30:39,339 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:30:39,339 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd95xzxug
2019-03-13 15:30:46,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:30:52,086 : Computing embedding for train
2019-03-13 15:31:14,468 : Computed train embeddings
2019-03-13 15:31:14,468 : Computing embedding for test
2019-03-13 15:31:24,274 : Computed test embeddings
2019-03-13 15:31:24,295 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:31:29,223 : [('reg:1e-05', 74.14), ('reg:0.0001', 74.29), ('reg:0.001', 73.99), ('reg:0.01', 73.21)]
2019-03-13 15:31:29,224 : Cross-validation : best param found is reg = 0.0001             with score 74.29
2019-03-13 15:31:29,224 : Evaluating...
2019-03-13 15:31:29,529 : Dev acc : 74.29 Test acc 75.94; Test F1 82.23 for MRPC.

2019-03-13 15:31:29,529 : ***** Transfer task : SICK-Entailment*****


2019-03-13 15:31:29,567 : loading BERT model bert-large-uncased
2019-03-13 15:31:29,567 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:31:29,587 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:31:29,587 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3qmrvcg_
2019-03-13 15:31:37,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:31:42,269 : Computing embedding for train
2019-03-13 15:31:53,625 : Computed train embeddings
2019-03-13 15:31:53,625 : Computing embedding for dev
2019-03-13 15:31:55,175 : Computed dev embeddings
2019-03-13 15:31:55,175 : Computing embedding for test
2019-03-13 15:32:07,394 : Computed test embeddings
2019-03-13 15:32:07,431 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:32:08,856 : [('reg:1e-05', 78.8), ('reg:0.0001', 78.2), ('reg:0.001', 79.6), ('reg:0.01', 76.8)]
2019-03-13 15:32:08,856 : Validation : best param found is reg = 0.001 with score             79.6
2019-03-13 15:32:08,856 : Evaluating...
2019-03-13 15:32:09,204 : 
Dev acc : 79.6 Test acc : 77.51 for                        SICK entailment

2019-03-13 15:32:09,205 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 15:32:09,231 : loading BERT model bert-large-uncased
2019-03-13 15:32:09,231 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:32:09,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:32:09,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyouy8_vo
2019-03-13 15:32:16,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:32:21,930 : Computing embedding for train
2019-03-13 15:32:33,322 : Computed train embeddings
2019-03-13 15:32:33,323 : Computing embedding for dev
2019-03-13 15:32:34,880 : Computed dev embeddings
2019-03-13 15:32:34,880 : Computing embedding for test
2019-03-13 15:32:47,108 : Computed test embeddings
2019-03-13 15:32:56,702 : Dev : Pearson 0.7811201214029939
2019-03-13 15:32:56,702 : Test : Pearson 0.7892367108128031 Spearman 0.7352322396774894 MSE 0.3855214771531424                        for SICK Relatedness

2019-03-13 15:32:56,703 : 

***** Transfer task : STSBenchmark*****


2019-03-13 15:32:56,753 : loading BERT model bert-large-uncased
2019-03-13 15:32:56,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:32:56,816 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:32:56,816 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdyfcouvv
2019-03-13 15:33:04,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:33:09,510 : Computing embedding for train
2019-03-13 15:33:28,230 : Computed train embeddings
2019-03-13 15:33:28,230 : Computing embedding for dev
2019-03-13 15:33:33,919 : Computed dev embeddings
2019-03-13 15:33:33,919 : Computing embedding for test
2019-03-13 15:33:38,563 : Computed test embeddings
2019-03-13 15:33:57,321 : Dev : Pearson 0.7144192948839738
2019-03-13 15:33:57,321 : Test : Pearson 0.6682812765808908 Spearman 0.6668320859812771 MSE 1.4485572113073588                        for SICK Relatedness

2019-03-13 15:33:57,322 : ***** Transfer task : SNLI Entailment*****


2019-03-13 15:34:02,240 : loading BERT model bert-large-uncased
2019-03-13 15:34:02,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:34:02,363 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:34:02,363 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbu9rvyfc
2019-03-13 15:34:09,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:34:15,554 : PROGRESS (encoding): 0.00%
2019-03-13 15:37:03,165 : PROGRESS (encoding): 14.56%
2019-03-13 15:40:15,084 : PROGRESS (encoding): 29.12%
2019-03-13 15:43:26,098 : PROGRESS (encoding): 43.69%
2019-03-13 15:46:49,728 : PROGRESS (encoding): 58.25%
2019-03-13 15:50:36,406 : PROGRESS (encoding): 72.81%
2019-03-13 15:54:21,765 : PROGRESS (encoding): 87.37%
2019-03-13 15:58:25,669 : PROGRESS (encoding): 0.00%
2019-03-13 15:58:56,297 : PROGRESS (encoding): 0.00%
2019-03-13 15:59:25,773 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:00:06,090 : [('reg:1e-09', 66.16)]
2019-03-13 16:00:06,090 : Validation : best param found is reg = 1e-09 with score             66.16
2019-03-13 16:00:06,090 : Evaluating...
2019-03-13 16:00:48,041 : Dev acc : 66.16 Test acc : 67.3 for SNLI

2019-03-13 16:00:48,041 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 16:00:48,248 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 16:00:49,302 : loading BERT model bert-large-uncased
2019-03-13 16:00:49,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:00:49,328 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:00:49,328 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo9zl60d5
2019-03-13 16:00:56,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:01:01,924 : Computing embeddings for train/dev/test
2019-03-13 16:04:35,064 : Computed embeddings
2019-03-13 16:04:35,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:05:05,465 : [('reg:1e-05', 81.0), ('reg:0.0001', 77.6), ('reg:0.001', 70.89), ('reg:0.01', 57.03)]
2019-03-13 16:05:05,466 : Validation : best param found is reg = 1e-05 with score             81.0
2019-03-13 16:05:05,466 : Evaluating...
2019-03-13 16:05:15,108 : 
Dev acc : 81.0 Test acc : 80.4 for LENGTH classification

2019-03-13 16:05:15,109 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 16:05:15,474 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 16:05:15,519 : loading BERT model bert-large-uncased
2019-03-13 16:05:15,519 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:05:15,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:05:15,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1v0om1tw
2019-03-13 16:05:23,013 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:05:28,348 : Computing embeddings for train/dev/test
2019-03-13 16:08:45,251 : Computed embeddings
2019-03-13 16:08:45,251 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:09:15,468 : [('reg:1e-05', 49.42), ('reg:0.0001', 29.2), ('reg:0.001', 3.42), ('reg:0.01', 0.9)]
2019-03-13 16:09:15,468 : Validation : best param found is reg = 1e-05 with score             49.42
2019-03-13 16:09:15,468 : Evaluating...
2019-03-13 16:09:21,612 : 
Dev acc : 49.4 Test acc : 49.7 for WORDCONTENT classification

2019-03-13 16:09:21,613 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 16:09:21,972 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 16:09:22,038 : loading BERT model bert-large-uncased
2019-03-13 16:09:22,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:09:22,062 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:09:22,062 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa5a6ihb2
2019-03-13 16:09:29,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:09:34,778 : Computing embeddings for train/dev/test
2019-03-13 16:12:40,339 : Computed embeddings
2019-03-13 16:12:40,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:13:03,641 : [('reg:1e-05', 35.53), ('reg:0.0001', 35.37), ('reg:0.001', 34.21), ('reg:0.01', 27.82)]
2019-03-13 16:13:03,641 : Validation : best param found is reg = 1e-05 with score             35.53
2019-03-13 16:13:03,641 : Evaluating...
2019-03-13 16:13:10,240 : 
Dev acc : 35.5 Test acc : 35.8 for DEPTH classification

2019-03-13 16:13:10,241 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 16:13:10,624 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 16:13:10,686 : loading BERT model bert-large-uncased
2019-03-13 16:13:10,686 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:13:10,715 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:13:10,715 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoe3kipqq
2019-03-13 16:13:18,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:13:23,547 : Computing embeddings for train/dev/test
2019-03-13 16:16:14,885 : Computed embeddings
2019-03-13 16:16:14,886 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:16:42,488 : [('reg:1e-05', 74.02), ('reg:0.0001', 72.45), ('reg:0.001', 65.86), ('reg:0.01', 56.81)]
2019-03-13 16:16:42,488 : Validation : best param found is reg = 1e-05 with score             74.02
2019-03-13 16:16:42,488 : Evaluating...
2019-03-13 16:16:51,191 : 
Dev acc : 74.0 Test acc : 73.7 for TOPCONSTITUENTS classification

2019-03-13 16:16:51,192 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 16:16:51,543 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 16:16:51,609 : loading BERT model bert-large-uncased
2019-03-13 16:16:51,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:16:51,727 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:16:51,727 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp__gp28kl
2019-03-13 16:16:59,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:17:04,403 : Computing embeddings for train/dev/test
2019-03-13 16:20:10,009 : Computed embeddings
2019-03-13 16:20:10,010 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:20:35,016 : [('reg:1e-05', 92.02), ('reg:0.0001', 91.9), ('reg:0.001', 91.33), ('reg:0.01', 90.78)]
2019-03-13 16:20:35,016 : Validation : best param found is reg = 1e-05 with score             92.02
2019-03-13 16:20:35,016 : Evaluating...
2019-03-13 16:20:41,704 : 
Dev acc : 92.0 Test acc : 91.7 for BIGRAMSHIFT classification

2019-03-13 16:20:41,705 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 16:20:42,083 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 16:20:42,148 : loading BERT model bert-large-uncased
2019-03-13 16:20:42,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:20:42,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:20:42,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps8k88k8o
2019-03-13 16:20:49,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:20:54,981 : Computing embeddings for train/dev/test
2019-03-13 16:23:56,279 : Computed embeddings
2019-03-13 16:23:56,280 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:24:21,540 : [('reg:1e-05', 89.84), ('reg:0.0001', 89.92), ('reg:0.001', 90.22), ('reg:0.01', 90.33)]
2019-03-13 16:24:21,540 : Validation : best param found is reg = 0.01 with score             90.33
2019-03-13 16:24:21,540 : Evaluating...
2019-03-13 16:24:26,684 : 
Dev acc : 90.3 Test acc : 88.5 for TENSE classification

2019-03-13 16:24:26,686 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 16:24:27,295 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 16:24:27,360 : loading BERT model bert-large-uncased
2019-03-13 16:24:27,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:24:27,389 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:24:27,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv2p3wqde
2019-03-13 16:24:34,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:24:40,125 : Computing embeddings for train/dev/test
2019-03-13 16:27:52,349 : Computed embeddings
2019-03-13 16:27:52,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:28:23,819 : [('reg:1e-05', 85.78), ('reg:0.0001', 85.69), ('reg:0.001', 85.47), ('reg:0.01', 84.5)]
2019-03-13 16:28:23,820 : Validation : best param found is reg = 1e-05 with score             85.78
2019-03-13 16:28:23,820 : Evaluating...
2019-03-13 16:28:33,081 : 
Dev acc : 85.8 Test acc : 85.7 for SUBJNUMBER classification

2019-03-13 16:28:33,082 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 16:28:33,507 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 16:28:33,574 : loading BERT model bert-large-uncased
2019-03-13 16:28:33,574 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:28:33,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:28:33,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzihfglmk
2019-03-13 16:28:41,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:28:46,471 : Computing embeddings for train/dev/test
2019-03-13 16:31:55,285 : Computed embeddings
2019-03-13 16:31:55,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:32:20,950 : [('reg:1e-05', 82.14), ('reg:0.0001', 82.12), ('reg:0.001', 82.13), ('reg:0.01', 81.36)]
2019-03-13 16:32:20,951 : Validation : best param found is reg = 1e-05 with score             82.14
2019-03-13 16:32:20,951 : Evaluating...
2019-03-13 16:32:27,355 : 
Dev acc : 82.1 Test acc : 83.5 for OBJNUMBER classification

2019-03-13 16:32:27,357 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 16:32:27,748 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 16:32:27,818 : loading BERT model bert-large-uncased
2019-03-13 16:32:27,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:32:27,942 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:32:27,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuph1q70h
2019-03-13 16:32:35,454 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:32:41,078 : Computing embeddings for train/dev/test
2019-03-13 16:36:21,641 : Computed embeddings
2019-03-13 16:36:21,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:36:49,860 : [('reg:1e-05', 69.78), ('reg:0.0001', 69.8), ('reg:0.001', 69.42), ('reg:0.01', 67.56)]
2019-03-13 16:36:49,860 : Validation : best param found is reg = 0.0001 with score             69.8
2019-03-13 16:36:49,860 : Evaluating...
2019-03-13 16:36:55,195 : 
Dev acc : 69.8 Test acc : 68.8 for ODDMANOUT classification

2019-03-13 16:36:55,196 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 16:36:55,806 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 16:36:55,881 : loading BERT model bert-large-uncased
2019-03-13 16:36:55,881 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:36:55,911 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:36:55,911 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_pugnvn1
2019-03-13 16:37:03,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:37:08,734 : Computing embeddings for train/dev/test
2019-03-13 16:40:48,341 : Computed embeddings
2019-03-13 16:40:48,341 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:41:22,231 : [('reg:1e-05', 76.14), ('reg:0.0001', 76.08), ('reg:0.001', 75.73), ('reg:0.01', 69.6)]
2019-03-13 16:41:22,231 : Validation : best param found is reg = 1e-05 with score             76.14
2019-03-13 16:41:22,231 : Evaluating...
2019-03-13 16:41:31,727 : 
Dev acc : 76.1 Test acc : 76.1 for COORDINATIONINVERSION classification

2019-03-13 16:41:31,729 : total results: {'STS12': {'MSRpar': {'pearson': (0.37034538847482473, 8.538797692767322e-26), 'spearman': SpearmanrResult(correlation=0.4091382898126876, pvalue=1.243080071563694e-31), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3660287100484679, 3.417160924052256e-25), 'spearman': SpearmanrResult(correlation=0.3982325046437129, pvalue=6.527158875340067e-30), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5204274129118188, 3.2155302992420354e-33), 'spearman': SpearmanrResult(correlation=0.5951374010796573, pvalue=2.5385401925827448e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5566818949788626, 2.966846095119694e-62), 'spearman': SpearmanrResult(correlation=0.5863280953562751, pvalue=1.8704525866370117e-70), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6752279373621032, 1.9886888012743743e-54), 'spearman': SpearmanrResult(correlation=0.6275547710550533, pvalue=4.453163315727011e-45), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49774226875521543, 'wmean': 0.47557404268359726}, 'spearman': {'mean': 0.5232782123894772, 'wmean': 0.504773676996794}}}, 'STS13': {'FNWN': {'pearson': (0.36421459286490176, 2.577710228149928e-07), 'spearman': SpearmanrResult(correlation=0.3751005724924398, pvalue=1.0489434615394335e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.581398909591965, 4.94393493692142e-69), 'spearman': SpearmanrResult(correlation=0.5618583070804678, pvalue=1.2583311258549755e-63), 'nsamples': 750}, 'OnWN': {'pearson': (0.46685203795831853, 1.0352521322112993e-31), 'spearman': SpearmanrResult(correlation=0.47807326819180945, pvalue=2.227710449078131e-33), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4708218468050618, 'wmean': 0.5111931556933712}, 'spearman': {'mean': 0.471677382588239, 'wmean': 0.5069912279780181}}}, 'STS14': {'deft-forum': {'pearson': (0.24876225802940205, 8.96002547564527e-08), 'spearman': SpearmanrResult(correlation=0.27172409120119695, pvalue=4.6651769612073776e-09), 'nsamples': 450}, 'deft-news': {'pearson': (0.7192051567008693, 4.787206089477484e-49), 'spearman': SpearmanrResult(correlation=0.6837725156628198, pvalue=1.1068631769090906e-42), 'nsamples': 300}, 'headlines': {'pearson': (0.5374028049483909, 2.3858223092895194e-57), 'spearman': SpearmanrResult(correlation=0.4975210940977847, pvalue=3.7483721559915165e-48), 'nsamples': 750}, 'images': {'pearson': (0.45464859143239256, 1.561305171868775e-39), 'spearman': SpearmanrResult(correlation=0.4442394983607555, pvalue=1.2822183399132982e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.6423474874947528, 1.7167272222989964e-88), 'spearman': SpearmanrResult(correlation=0.666416849625232, pvalue=1.8441505165353494e-97), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6496934041152227, 3.835545468716646e-91), 'spearman': SpearmanrResult(correlation=0.589347955061374, pvalue=2.446608300322772e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5420099504535051, 'wmean': 0.5442063410977496}, 'spearman': {'mean': 0.5255036673348604, 'wmean': 0.5268137716261985}}}, 'STS15': {'answers-forums': {'pearson': (0.6201081926258378, 3.2212812867499557e-41), 'spearman': SpearmanrResult(correlation=0.6150160972577926, pvalue=2.1629559081696475e-40), 'nsamples': 375}, 'answers-students': {'pearson': (0.6941420747283854, 6.829567184867376e-109), 'spearman': SpearmanrResult(correlation=0.7117240820114662, pvalue=7.665222303219906e-117), 'nsamples': 750}, 'belief': {'pearson': (0.6865943739915902, 1.3778660209832858e-53), 'spearman': SpearmanrResult(correlation=0.6994187169558198, pvalue=2.294991034900621e-56), 'nsamples': 375}, 'headlines': {'pearson': (0.6065761943606901, 1.4653575146941223e-76), 'spearman': SpearmanrResult(correlation=0.6037371363139006, pvalue=1.1183705415275607e-75), 'nsamples': 750}, 'images': {'pearson': (0.5858334429575311, 2.6047205484468996e-70), 'spearman': SpearmanrResult(correlation=0.5957050662185436, pvalue=3.149282570640932e-73), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6386508557328069, 'wmean': 0.6349757488388301}, 'spearman': {'mean': 0.6451202197515046, 'wmean': 0.6420959229126791}}}, 'STS16': {'answer-answer': {'pearson': (0.54822093622133, 2.4980394946545665e-21), 'spearman': SpearmanrResult(correlation=0.5538057939451646, pvalue=8.1221623060396485e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6519632733183172, 1.593475253986275e-31), 'spearman': SpearmanrResult(correlation=0.652503384590972, pvalue=1.3684384409087375e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.776246921578451, 1.40704681224207e-47), 'spearman': SpearmanrResult(correlation=0.7904426783394102, pvalue=1.9476507812048113e-50), 'nsamples': 230}, 'postediting': {'pearson': (0.7996541316794572, 1.5811958330693794e-55), 'spearman': SpearmanrResult(correlation=0.8187203552270359, pvalue=3.0724742299272495e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.2564527715543531, 0.00017815275419565745), 'spearman': SpearmanrResult(correlation=0.25707527958270526, pvalue=0.0001715827800496267), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6065076068703817, 'wmean': 0.6145345718416273}, 'spearman': {'mean': 0.6145094983370576, 'wmean': 0.622629283770211}}}, 'MR': {'devacc': 84.07, 'acc': 82.64, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.59, 'acc': 89.03, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.71, 'acc': 85.96, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.98, 'acc': 95.73, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 88.53, 'acc': 88.63, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 48.96, 'acc': 47.83, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.34, 'acc': 91.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.29, 'acc': 75.94, 'f1': 82.23, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 77.51, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7811201214029939, 'pearson': 0.7892367108128031, 'spearman': 0.7352322396774894, 'mse': 0.3855214771531424, 'yhat': array([3.590808  , 4.66315036, 2.15569116, ..., 3.33954753, 4.5779474 ,        4.27232586]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7144192948839738, 'pearson': 0.6682812765808908, 'spearman': 0.6668320859812771, 'mse': 1.4485572113073588, 'yhat': array([1.38898854, 1.69374066, 2.56361344, ..., 3.91188991, 3.89292009,        3.55673366]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.16, 'acc': 67.3, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 81.0, 'acc': 80.37, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 49.42, 'acc': 49.68, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.53, 'acc': 35.83, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.02, 'acc': 73.69, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 92.02, 'acc': 91.66, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.33, 'acc': 88.45, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.78, 'acc': 85.69, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.14, 'acc': 83.53, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 69.8, 'acc': 68.8, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 76.14, 'acc': 76.12, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 16:41:31,729 : STS12 p=0.4756, STS12 s=0.5048, STS13 p=0.5112, STS13 s=0.5070, STS14 p=0.5442, STS14 s=0.5268, STS15 p=0.6350, STS15 s=0.6421, STS 16 p=0.6145, STS16 s=0.6226, STS B p=0.6683, STS B s=0.6668, STS B m=1.4486, SICK-R p=0.7892, SICK-R s=0.7352, SICK-P m=0.3855
2019-03-13 16:41:31,729 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 16:41:31,729 : 0.4756,0.5048,0.5112,0.5070,0.5442,0.5268,0.6350,0.6421,0.6145,0.6226,0.6683,0.6668,1.4486,0.7892,0.7352,0.3855
2019-03-13 16:41:31,729 : MR=82.64, CR=89.03, SUBJ=95.73, MPQA=85.96, SST-B=88.63, SST-F=47.83, TREC=91.20, SICK-E=77.51, SNLI=67.30, MRPC=75.94, MRPC f=82.23
2019-03-13 16:41:31,729 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 16:41:31,729 : 82.64,89.03,95.73,85.96,88.63,47.83,91.20,77.51,67.30,75.94,82.23
2019-03-13 16:41:31,729 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 16:41:31,729 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 16:41:31,729 : na,na,na,na,na,na,na,na,na,na
2019-03-13 16:41:31,729 : SentLen=80.37, WC=49.68, TreeDepth=35.83, TopConst=73.69, BShift=91.66, Tense=88.45, SubjNum=85.69, ObjNum=83.53, SOMO=68.80, CoordInv=76.12, average=73.38
2019-03-13 16:41:31,729 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 16:41:31,729 : 80.37,49.68,35.83,73.69,91.66,88.45,85.69,83.53,68.80,76.12,73.38
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,729 : layer 20
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,729 : ********************************************************************************
2019-03-13 16:41:31,820 : ***** Transfer task : STS12 *****


2019-03-13 16:41:31,833 : loading BERT model bert-large-uncased
2019-03-13 16:41:31,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:41:31,850 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:41:31,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpop7_uj2a
2019-03-13 16:41:39,291 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:41:48,684 : MSRpar : pearson = 0.3832, spearman = 0.4230
2019-03-13 16:41:50,337 : MSRvid : pearson = 0.3667, spearman = 0.4027
2019-03-13 16:41:51,758 : SMTeuroparl : pearson = 0.5117, spearman = 0.5878
2019-03-13 16:41:54,473 : surprise.OnWN : pearson = 0.5352, spearman = 0.5724
2019-03-13 16:41:55,912 : surprise.SMTnews : pearson = 0.6633, spearman = 0.6240
2019-03-13 16:41:55,913 : ALL (weighted average) : Pearson = 0.4708,             Spearman = 0.5043
2019-03-13 16:41:55,913 : ALL (average) : Pearson = 0.4920,             Spearman = 0.5220

2019-03-13 16:41:55,913 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 16:41:55,921 : loading BERT model bert-large-uncased
2019-03-13 16:41:55,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:41:55,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:41:55,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprsh23o3p
2019-03-13 16:42:03,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:42:09,977 : FNWN : pearson = 0.3760, spearman = 0.3903
2019-03-13 16:42:11,883 : headlines : pearson = 0.5825, spearman = 0.5654
2019-03-13 16:42:13,361 : OnWN : pearson = 0.4592, spearman = 0.4736
2019-03-13 16:42:13,361 : ALL (weighted average) : Pearson = 0.5104,             Spearman = 0.5090
2019-03-13 16:42:13,361 : ALL (average) : Pearson = 0.4726,             Spearman = 0.4764

2019-03-13 16:42:13,361 : ***** Transfer task : STS14 *****


2019-03-13 16:42:13,405 : loading BERT model bert-large-uncased
2019-03-13 16:42:13,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:42:13,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:42:13,423 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8b4w95mb
2019-03-13 16:42:20,895 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:42:27,565 : deft-forum : pearson = 0.2451, spearman = 0.2629
2019-03-13 16:42:29,214 : deft-news : pearson = 0.7319, spearman = 0.6899
2019-03-13 16:42:31,404 : headlines : pearson = 0.5435, spearman = 0.5075
2019-03-13 16:42:33,499 : images : pearson = 0.4897, spearman = 0.4746
2019-03-13 16:42:35,648 : OnWN : pearson = 0.6368, spearman = 0.6608
2019-03-13 16:42:38,532 : tweet-news : pearson = 0.6482, spearman = 0.5901
2019-03-13 16:42:38,532 : ALL (weighted average) : Pearson = 0.5516,             Spearman = 0.5334
2019-03-13 16:42:38,532 : ALL (average) : Pearson = 0.5492,             Spearman = 0.5310

2019-03-13 16:42:38,533 : ***** Transfer task : STS15 *****


2019-03-13 16:42:38,582 : loading BERT model bert-large-uncased
2019-03-13 16:42:38,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:42:38,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:42:38,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfgb53mt9
2019-03-13 16:42:46,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:42:53,264 : answers-forums : pearson = 0.6179, spearman = 0.6108
2019-03-13 16:42:55,362 : answers-students : pearson = 0.6559, spearman = 0.6764
2019-03-13 16:42:57,427 : belief : pearson = 0.6956, spearman = 0.7076
2019-03-13 16:42:59,694 : headlines : pearson = 0.6242, spearman = 0.6207
2019-03-13 16:43:01,845 : images : pearson = 0.6064, spearman = 0.6137
2019-03-13 16:43:01,845 : ALL (weighted average) : Pearson = 0.6358,             Spearman = 0.6425
2019-03-13 16:43:01,845 : ALL (average) : Pearson = 0.6400,             Spearman = 0.6458

2019-03-13 16:43:01,845 : ***** Transfer task : STS16 *****


2019-03-13 16:43:01,883 : loading BERT model bert-large-uncased
2019-03-13 16:43:01,883 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:43:01,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:43:01,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2gy5jn2u
2019-03-13 16:43:09,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:43:15,595 : answer-answer : pearson = 0.5589, spearman = 0.5616
2019-03-13 16:43:16,259 : headlines : pearson = 0.6622, spearman = 0.6672
2019-03-13 16:43:17,149 : plagiarism : pearson = 0.7802, spearman = 0.7920
2019-03-13 16:43:18,656 : postediting : pearson = 0.7975, spearman = 0.8149
2019-03-13 16:43:19,265 : question-question : pearson = 0.2721, spearman = 0.2731
2019-03-13 16:43:19,265 : ALL (weighted average) : Pearson = 0.6221,             Spearman = 0.6297
2019-03-13 16:43:19,265 : ALL (average) : Pearson = 0.6142,             Spearman = 0.6218

2019-03-13 16:43:19,265 : ***** Transfer task : MR *****


2019-03-13 16:43:19,310 : loading BERT model bert-large-uncased
2019-03-13 16:43:19,311 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:43:19,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:43:19,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps3sunstw
2019-03-13 16:43:26,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:43:32,012 : Generating sentence embeddings
2019-03-13 16:44:03,739 : Generated sentence embeddings
2019-03-13 16:44:03,740 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:44:14,077 : Best param found at split 1: l2reg = 0.001                 with score 84.09
2019-03-13 16:44:24,543 : Best param found at split 2: l2reg = 0.001                 with score 84.27
2019-03-13 16:44:32,835 : Best param found at split 3: l2reg = 0.001                 with score 84.38
2019-03-13 16:44:43,081 : Best param found at split 4: l2reg = 0.001                 with score 84.64
2019-03-13 16:44:52,169 : Best param found at split 5: l2reg = 0.0001                 with score 84.34
2019-03-13 16:44:52,617 : Dev acc : 84.34 Test acc : 84.21

2019-03-13 16:44:52,618 : ***** Transfer task : CR *****


2019-03-13 16:44:52,626 : loading BERT model bert-large-uncased
2019-03-13 16:44:52,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:44:52,682 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:44:52,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg30ipqz9
2019-03-13 16:45:00,125 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:45:05,409 : Generating sentence embeddings
2019-03-13 16:45:13,787 : Generated sentence embeddings
2019-03-13 16:45:13,787 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:45:17,127 : Best param found at split 1: l2reg = 0.01                 with score 89.73
2019-03-13 16:45:20,780 : Best param found at split 2: l2reg = 0.01                 with score 89.37
2019-03-13 16:45:24,609 : Best param found at split 3: l2reg = 0.01                 with score 90.26
2019-03-13 16:45:28,149 : Best param found at split 4: l2reg = 0.001                 with score 89.8
2019-03-13 16:45:31,336 : Best param found at split 5: l2reg = 0.001                 with score 90.17
2019-03-13 16:45:31,455 : Dev acc : 89.87 Test acc : 89.4

2019-03-13 16:45:31,455 : ***** Transfer task : MPQA *****


2019-03-13 16:45:31,460 : loading BERT model bert-large-uncased
2019-03-13 16:45:31,460 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:45:31,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:45:31,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4u472owo
2019-03-13 16:45:38,908 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:45:44,325 : Generating sentence embeddings
2019-03-13 16:45:51,942 : Generated sentence embeddings
2019-03-13 16:45:51,943 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:45:58,766 : Best param found at split 1: l2reg = 1e-05                 with score 83.12
2019-03-13 16:46:08,692 : Best param found at split 2: l2reg = 0.001                 with score 85.41
2019-03-13 16:46:18,747 : Best param found at split 3: l2reg = 0.001                 with score 85.36
2019-03-13 16:46:26,850 : Best param found at split 4: l2reg = 1e-05                 with score 85.28
2019-03-13 16:46:35,214 : Best param found at split 5: l2reg = 0.0001                 with score 82.67
2019-03-13 16:46:35,732 : Dev acc : 84.37 Test acc : 85.64

2019-03-13 16:46:35,733 : ***** Transfer task : SUBJ *****


2019-03-13 16:46:35,749 : loading BERT model bert-large-uncased
2019-03-13 16:46:35,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:46:35,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:46:35,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgobohji5
2019-03-13 16:46:43,268 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:46:48,705 : Generating sentence embeddings
2019-03-13 16:47:19,774 : Generated sentence embeddings
2019-03-13 16:47:19,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:47:29,244 : Best param found at split 1: l2reg = 0.001                 with score 96.09
2019-03-13 16:47:39,537 : Best param found at split 2: l2reg = 0.001                 with score 96.19
2019-03-13 16:47:49,372 : Best param found at split 3: l2reg = 0.001                 with score 95.66
2019-03-13 16:47:59,510 : Best param found at split 4: l2reg = 0.001                 with score 96.19
2019-03-13 16:48:09,730 : Best param found at split 5: l2reg = 0.001                 with score 96.02
2019-03-13 16:48:10,314 : Dev acc : 96.03 Test acc : 95.73

2019-03-13 16:48:10,315 : ***** Transfer task : SST Binary classification *****


2019-03-13 16:48:10,454 : loading BERT model bert-large-uncased
2019-03-13 16:48:10,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:48:10,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:48:10,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ljkjjhe
2019-03-13 16:48:17,950 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:48:23,151 : Computing embedding for train
2019-03-13 16:50:04,037 : Computed train embeddings
2019-03-13 16:50:04,037 : Computing embedding for dev
2019-03-13 16:50:06,238 : Computed dev embeddings
2019-03-13 16:50:06,238 : Computing embedding for test
2019-03-13 16:50:10,870 : Computed test embeddings
2019-03-13 16:50:10,871 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:50:28,474 : [('reg:1e-05', 87.84), ('reg:0.0001', 88.42), ('reg:0.001', 89.22), ('reg:0.01', 87.61)]
2019-03-13 16:50:28,474 : Validation : best param found is reg = 0.001 with score             89.22
2019-03-13 16:50:28,474 : Evaluating...
2019-03-13 16:50:33,592 : 
Dev acc : 89.22 Test acc : 88.47 for             SST Binary classification

2019-03-13 16:50:33,592 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 16:50:33,641 : loading BERT model bert-large-uncased
2019-03-13 16:50:33,641 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:50:33,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:50:33,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplsq21sub
2019-03-13 16:50:41,051 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:50:46,337 : Computing embedding for train
2019-03-13 16:51:08,407 : Computed train embeddings
2019-03-13 16:51:08,407 : Computing embedding for dev
2019-03-13 16:51:11,292 : Computed dev embeddings
2019-03-13 16:51:11,292 : Computing embedding for test
2019-03-13 16:51:16,977 : Computed test embeddings
2019-03-13 16:51:16,978 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:51:19,883 : [('reg:1e-05', 47.96), ('reg:0.0001', 47.87), ('reg:0.001', 46.87), ('reg:0.01', 47.41)]
2019-03-13 16:51:19,883 : Validation : best param found is reg = 1e-05 with score             47.96
2019-03-13 16:51:19,883 : Evaluating...
2019-03-13 16:51:20,702 : 
Dev acc : 47.96 Test acc : 46.56 for             SST Fine-Grained classification

2019-03-13 16:51:20,703 : ***** Transfer task : TREC *****


2019-03-13 16:51:20,715 : loading BERT model bert-large-uncased
2019-03-13 16:51:20,716 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:51:20,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:51:20,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9i4uw2z8
2019-03-13 16:51:28,201 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:51:41,089 : Computed train embeddings
2019-03-13 16:51:41,681 : Computed test embeddings
2019-03-13 16:51:41,682 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:51:49,274 : [('reg:1e-05', 86.77), ('reg:0.0001', 86.59), ('reg:0.001', 85.34), ('reg:0.01', 77.0)]
2019-03-13 16:51:49,275 : Cross-validation : best param found is reg = 1e-05             with score 86.77
2019-03-13 16:51:49,275 : Evaluating...
2019-03-13 16:51:49,781 : 
Dev acc : 86.77 Test acc : 94.4             for TREC

2019-03-13 16:51:49,782 : ***** Transfer task : MRPC *****


2019-03-13 16:51:49,803 : loading BERT model bert-large-uncased
2019-03-13 16:51:49,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:51:49,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:51:49,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfbv6mvbw
2019-03-13 16:51:57,342 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:52:02,791 : Computing embedding for train
2019-03-13 16:52:25,173 : Computed train embeddings
2019-03-13 16:52:25,173 : Computing embedding for test
2019-03-13 16:52:34,993 : Computed test embeddings
2019-03-13 16:52:35,015 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:52:39,654 : [('reg:1e-05', 74.07), ('reg:0.0001', 74.26), ('reg:0.001', 74.31), ('reg:0.01', 73.28)]
2019-03-13 16:52:39,654 : Cross-validation : best param found is reg = 0.001             with score 74.31
2019-03-13 16:52:39,654 : Evaluating...
2019-03-13 16:52:39,878 : Dev acc : 74.31 Test acc 74.96; Test F1 82.19 for MRPC.

2019-03-13 16:52:39,878 : ***** Transfer task : SICK-Entailment*****


2019-03-13 16:52:39,901 : loading BERT model bert-large-uncased
2019-03-13 16:52:39,902 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:52:39,922 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:52:39,922 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfodhx_an
2019-03-13 16:52:47,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:52:52,611 : Computing embedding for train
2019-03-13 16:53:03,981 : Computed train embeddings
2019-03-13 16:53:03,981 : Computing embedding for dev
2019-03-13 16:53:05,533 : Computed dev embeddings
2019-03-13 16:53:05,533 : Computing embedding for test
2019-03-13 16:53:17,775 : Computed test embeddings
2019-03-13 16:53:17,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:53:19,330 : [('reg:1e-05', 76.2), ('reg:0.0001', 76.4), ('reg:0.001', 75.2), ('reg:0.01', 76.8)]
2019-03-13 16:53:19,331 : Validation : best param found is reg = 0.01 with score             76.8
2019-03-13 16:53:19,331 : Evaluating...
2019-03-13 16:53:19,730 : 
Dev acc : 76.8 Test acc : 75.95 for                        SICK entailment

2019-03-13 16:53:19,731 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 16:53:19,758 : loading BERT model bert-large-uncased
2019-03-13 16:53:19,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:53:19,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:53:19,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx8as5f2s
2019-03-13 16:53:27,175 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:53:32,526 : Computing embedding for train
2019-03-13 16:53:43,909 : Computed train embeddings
2019-03-13 16:53:43,909 : Computing embedding for dev
2019-03-13 16:53:45,464 : Computed dev embeddings
2019-03-13 16:53:45,464 : Computing embedding for test
2019-03-13 16:53:57,696 : Computed test embeddings
2019-03-13 16:54:15,572 : Dev : Pearson 0.7905127265781522
2019-03-13 16:54:15,572 : Test : Pearson 0.7761838542307102 Spearman 0.7172156596106803 MSE 0.4050631270949628                        for SICK Relatedness

2019-03-13 16:54:15,573 : 

***** Transfer task : STSBenchmark*****


2019-03-13 16:54:15,643 : loading BERT model bert-large-uncased
2019-03-13 16:54:15,643 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:54:15,701 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:54:15,701 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4qb0erqx
2019-03-13 16:54:23,198 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:54:28,556 : Computing embedding for train
2019-03-13 16:54:47,270 : Computed train embeddings
2019-03-13 16:54:47,270 : Computing embedding for dev
2019-03-13 16:54:52,957 : Computed dev embeddings
2019-03-13 16:54:52,957 : Computing embedding for test
2019-03-13 16:54:57,608 : Computed test embeddings
2019-03-13 16:55:16,883 : Dev : Pearson 0.7007456662712341
2019-03-13 16:55:16,883 : Test : Pearson 0.6555238874321705 Spearman 0.6517097491851377 MSE 1.4610273806041196                        for SICK Relatedness

2019-03-13 16:55:16,884 : ***** Transfer task : SNLI Entailment*****


2019-03-13 16:55:21,861 : loading BERT model bert-large-uncased
2019-03-13 16:55:21,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:55:21,991 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:55:21,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkclc4idi
2019-03-13 16:55:29,404 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:55:35,308 : PROGRESS (encoding): 0.00%
2019-03-13 16:58:23,215 : PROGRESS (encoding): 14.56%
2019-03-13 17:01:36,547 : PROGRESS (encoding): 29.12%
2019-03-13 17:04:48,201 : PROGRESS (encoding): 43.69%
2019-03-13 17:08:12,439 : PROGRESS (encoding): 58.25%
2019-03-13 17:11:59,823 : PROGRESS (encoding): 72.81%
2019-03-13 17:15:45,768 : PROGRESS (encoding): 87.37%
2019-03-13 17:19:50,428 : PROGRESS (encoding): 0.00%
2019-03-13 17:20:21,137 : PROGRESS (encoding): 0.00%
2019-03-13 17:20:50,817 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:21:27,116 : [('reg:1e-09', 66.08)]
2019-03-13 17:21:27,117 : Validation : best param found is reg = 1e-09 with score             66.08
2019-03-13 17:21:27,117 : Evaluating...
2019-03-13 17:22:03,699 : Dev acc : 66.08 Test acc : 66.72 for SNLI

2019-03-13 17:22:03,699 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 17:22:03,909 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 17:22:04,955 : loading BERT model bert-large-uncased
2019-03-13 17:22:04,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:22:04,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:22:04,980 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa64g_hon
2019-03-13 17:22:12,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:22:17,659 : Computing embeddings for train/dev/test
2019-03-13 17:25:52,274 : Computed embeddings
2019-03-13 17:25:52,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:26:27,532 : [('reg:1e-05', 77.9), ('reg:0.0001', 75.34), ('reg:0.001', 68.22), ('reg:0.01', 55.48)]
2019-03-13 17:26:27,532 : Validation : best param found is reg = 1e-05 with score             77.9
2019-03-13 17:26:27,532 : Evaluating...
2019-03-13 17:26:37,254 : 
Dev acc : 77.9 Test acc : 77.1 for LENGTH classification

2019-03-13 17:26:37,255 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 17:26:37,614 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 17:26:37,659 : loading BERT model bert-large-uncased
2019-03-13 17:26:37,659 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:26:37,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:26:37,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpznbengo1
2019-03-13 17:26:45,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:26:50,449 : Computing embeddings for train/dev/test
2019-03-13 17:30:08,184 : Computed embeddings
2019-03-13 17:30:08,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:30:38,816 : [('reg:1e-05', 47.72), ('reg:0.0001', 27.15), ('reg:0.001', 3.51), ('reg:0.01', 0.88)]
2019-03-13 17:30:38,817 : Validation : best param found is reg = 1e-05 with score             47.72
2019-03-13 17:30:38,817 : Evaluating...
2019-03-13 17:30:47,045 : 
Dev acc : 47.7 Test acc : 48.4 for WORDCONTENT classification

2019-03-13 17:30:47,047 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 17:30:47,411 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 17:30:47,478 : loading BERT model bert-large-uncased
2019-03-13 17:30:47,478 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:30:47,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:30:47,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq7bfm6m5
2019-03-13 17:30:54,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:31:00,286 : Computing embeddings for train/dev/test
2019-03-13 17:34:07,333 : Computed embeddings
2019-03-13 17:34:07,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:34:24,242 : [('reg:1e-05', 34.31), ('reg:0.0001', 34.22), ('reg:0.001', 33.28), ('reg:0.01', 27.92)]
2019-03-13 17:34:24,242 : Validation : best param found is reg = 1e-05 with score             34.31
2019-03-13 17:34:24,242 : Evaluating...
2019-03-13 17:34:29,232 : 
Dev acc : 34.3 Test acc : 34.6 for DEPTH classification

2019-03-13 17:34:29,233 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 17:34:29,631 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 17:34:29,696 : loading BERT model bert-large-uncased
2019-03-13 17:34:29,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:34:29,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:34:29,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjis0rlzj
2019-03-13 17:34:37,156 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:34:42,488 : Computing embeddings for train/dev/test
2019-03-13 17:37:35,907 : Computed embeddings
2019-03-13 17:37:35,907 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:38:06,945 : [('reg:1e-05', 72.41), ('reg:0.0001', 70.84), ('reg:0.001', 64.74), ('reg:0.01', 55.22)]
2019-03-13 17:38:06,945 : Validation : best param found is reg = 1e-05 with score             72.41
2019-03-13 17:38:06,945 : Evaluating...
2019-03-13 17:38:15,722 : 
Dev acc : 72.4 Test acc : 73.1 for TOPCONSTITUENTS classification

2019-03-13 17:38:15,723 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 17:38:16,075 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 17:38:16,141 : loading BERT model bert-large-uncased
2019-03-13 17:38:16,141 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:38:16,261 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:38:16,262 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcj1v_lw8
2019-03-13 17:38:23,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:38:29,078 : Computing embeddings for train/dev/test
2019-03-13 17:41:35,275 : Computed embeddings
2019-03-13 17:41:35,275 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:41:57,520 : [('reg:1e-05', 91.08), ('reg:0.0001', 91.11), ('reg:0.001', 90.96), ('reg:0.01', 90.03)]
2019-03-13 17:41:57,520 : Validation : best param found is reg = 0.0001 with score             91.11
2019-03-13 17:41:57,520 : Evaluating...
2019-03-13 17:42:02,830 : 
Dev acc : 91.1 Test acc : 90.8 for BIGRAMSHIFT classification

2019-03-13 17:42:02,831 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 17:42:03,232 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 17:42:03,300 : loading BERT model bert-large-uncased
2019-03-13 17:42:03,300 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:42:03,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:42:03,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptenlnc2d
2019-03-13 17:42:10,949 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:42:16,267 : Computing embeddings for train/dev/test
2019-03-13 17:45:18,261 : Computed embeddings
2019-03-13 17:45:18,262 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:45:41,893 : [('reg:1e-05', 89.47), ('reg:0.0001', 89.61), ('reg:0.001', 90.2), ('reg:0.01', 90.57)]
2019-03-13 17:45:41,893 : Validation : best param found is reg = 0.01 with score             90.57
2019-03-13 17:45:41,893 : Evaluating...
2019-03-13 17:45:46,926 : 
Dev acc : 90.6 Test acc : 88.4 for TENSE classification

2019-03-13 17:45:46,927 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 17:45:47,540 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 17:45:47,609 : loading BERT model bert-large-uncased
2019-03-13 17:45:47,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:45:47,638 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:45:47,638 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0zasy0p7
2019-03-13 17:45:55,171 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:46:00,498 : Computing embeddings for train/dev/test
2019-03-13 17:49:13,697 : Computed embeddings
2019-03-13 17:49:13,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:49:44,928 : [('reg:1e-05', 85.59), ('reg:0.0001', 85.6), ('reg:0.001', 85.56), ('reg:0.01', 84.13)]
2019-03-13 17:49:44,929 : Validation : best param found is reg = 0.0001 with score             85.6
2019-03-13 17:49:44,929 : Evaluating...
2019-03-13 17:49:53,624 : 
Dev acc : 85.6 Test acc : 85.0 for SUBJNUMBER classification

2019-03-13 17:49:53,625 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 17:49:54,059 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 17:49:54,127 : loading BERT model bert-large-uncased
2019-03-13 17:49:54,127 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:49:54,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:49:54,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0a6cb9bf
2019-03-13 17:50:01,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:50:06,996 : Computing embeddings for train/dev/test
2019-03-13 17:53:16,519 : Computed embeddings
2019-03-13 17:53:16,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:53:39,933 : [('reg:1e-05', 81.55), ('reg:0.0001', 82.01), ('reg:0.001', 81.48), ('reg:0.01', 80.51)]
2019-03-13 17:53:39,933 : Validation : best param found is reg = 0.0001 with score             82.01
2019-03-13 17:53:39,933 : Evaluating...
2019-03-13 17:53:47,867 : 
Dev acc : 82.0 Test acc : 82.6 for OBJNUMBER classification

2019-03-13 17:53:47,868 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 17:53:48,261 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 17:53:48,330 : loading BERT model bert-large-uncased
2019-03-13 17:53:48,330 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:53:48,456 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:53:48,456 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdh2ikux7
2019-03-13 17:53:55,947 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:54:01,174 : Computing embeddings for train/dev/test
2019-03-13 17:57:42,454 : Computed embeddings
2019-03-13 17:57:42,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:58:11,602 : [('reg:1e-05', 69.37), ('reg:0.0001', 69.37), ('reg:0.001', 68.84), ('reg:0.01', 66.61)]
2019-03-13 17:58:11,602 : Validation : best param found is reg = 1e-05 with score             69.37
2019-03-13 17:58:11,602 : Evaluating...
2019-03-13 17:58:19,162 : 
Dev acc : 69.4 Test acc : 69.1 for ODDMANOUT classification

2019-03-13 17:58:19,163 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 17:58:19,802 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 17:58:19,880 : loading BERT model bert-large-uncased
2019-03-13 17:58:19,880 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:58:19,909 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:58:19,909 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5bb04i80
2019-03-13 17:58:27,389 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:58:32,811 : Computing embeddings for train/dev/test
2019-03-13 18:02:12,170 : Computed embeddings
2019-03-13 18:02:12,170 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:02:48,912 : [('reg:1e-05', 75.85), ('reg:0.0001', 75.75), ('reg:0.001', 75.56), ('reg:0.01', 67.96)]
2019-03-13 18:02:48,913 : Validation : best param found is reg = 1e-05 with score             75.85
2019-03-13 18:02:48,913 : Evaluating...
2019-03-13 18:02:59,545 : 
Dev acc : 75.8 Test acc : 74.9 for COORDINATIONINVERSION classification

2019-03-13 18:02:59,547 : total results: {'STS12': {'MSRpar': {'pearson': (0.38316292878259023, 1.2261453775366467e-27), 'spearman': SpearmanrResult(correlation=0.4230376474465523, pvalue=6.425894175260434e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3667224167469957, 2.7383842183933227e-25), 'spearman': SpearmanrResult(correlation=0.40272828397988375, pvalue=1.2981014250090257e-30), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5117283914906623, 5.357721395897863e-32), 'spearman': SpearmanrResult(correlation=0.5877578893959744, pvalue=5.524484177623589e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.535246403979801, 8.06820512719289e-57), 'spearman': SpearmanrResult(correlation=0.5724162926885477, pvalue=1.6754041556714897e-66), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.663312309033319, 6.267368788955271e-52), 'spearman': SpearmanrResult(correlation=0.623970568109533, pvalue=1.935151308440396e-44), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49203449000667365, 'wmean': 0.4708474115606655}, 'spearman': {'mean': 0.5219821363240983, 'wmean': 0.5043055971668898}}}, 'STS13': {'FNWN': {'pearson': (0.3759777720364524, 9.742219174964741e-08), 'spearman': SpearmanrResult(correlation=0.39030508902868466, pvalue=2.8241520109689147e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.5824876460604878, 2.410179513144543e-69), 'spearman': SpearmanrResult(correlation=0.5653852275040929, pvalue=1.4147939966521405e-64), 'nsamples': 750}, 'OnWN': {'pearson': (0.45919280207569285, 1.3123702985453095e-30), 'spearman': SpearmanrResult(correlation=0.4735579204957964, pvalue=1.0620768316394809e-32), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4725527400575443, 'wmean': 0.5103551302831459}, 'spearman': {'mean': 0.47641607900952465, 'wmean': 0.5089817172350886}}}, 'STS14': {'deft-forum': {'pearson': (0.24514999271337248, 1.3902753894783655e-07), 'spearman': SpearmanrResult(correlation=0.2628799648884726, pvalue=1.50596705003804e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.7319365014063787, 1.4041899588145784e-51), 'spearman': SpearmanrResult(correlation=0.6898605548685314, pvalue=1.0372885322759892e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.5435198090369424, 7.171798119489786e-59), 'spearman': SpearmanrResult(correlation=0.507540855349978, pvalue=2.381316016528849e-50), 'nsamples': 750}, 'images': {'pearson': (0.48965724758034657, 1.769298315899911e-46), 'spearman': SpearmanrResult(correlation=0.4746453264050567, pvalue=2.113162098252209e-43), 'nsamples': 750}, 'OnWN': {'pearson': (0.6368303163352631, 1.5077537788075625e-86), 'spearman': SpearmanrResult(correlation=0.6607928838365007, pvalue=2.7284477980541744e-95), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6482315618236366, 1.309799271469251e-90), 'spearman': SpearmanrResult(correlation=0.5901458418445078, pvalue=1.4243125734593097e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.54922090481599, 'wmean': 0.5516207061933527}, 'spearman': {'mean': 0.5309775711988411, 'wmean': 0.5333594216633079}}}, 'STS15': {'answers-forums': {'pearson': (0.6179104945427871, 7.359008150886837e-41), 'spearman': SpearmanrResult(correlation=0.6107874657323645, pvalue=1.024360893434572e-39), 'nsamples': 375}, 'answers-students': {'pearson': (0.6558850319941727, 1.9573703157724005e-93), 'spearman': SpearmanrResult(correlation=0.6763941956178148, pvalue=1.9803276968475418e-101), 'nsamples': 750}, 'belief': {'pearson': (0.6956463078930167, 1.5606710087789897e-55), 'spearman': SpearmanrResult(correlation=0.7076456034553729, pvalue=3.1561444378517863e-58), 'nsamples': 375}, 'headlines': {'pearson': (0.624206240346736, 3.016823334103044e-82), 'spearman': SpearmanrResult(correlation=0.6207125483813629, pvalue=4.31803435197095e-81), 'nsamples': 750}, 'images': {'pearson': (0.6064282335657079, 1.629910258600567e-76), 'spearman': SpearmanrResult(correlation=0.6136952073535435, pvalue=8.185374404871757e-79), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.640015261668484, 'wmean': 0.6358244767811296}, 'spearman': {'mean': 0.6458470041080917, 'wmean': 0.6425046214866474}}}, 'STS16': {'answer-answer': {'pearson': (0.5589153284723924, 2.8522199114131172e-22), 'spearman': SpearmanrResult(correlation=0.5615771101336537, pvalue=1.6417299084984113e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6622314399208589, 8.352381352967151e-33), 'spearman': SpearmanrResult(correlation=0.6671811349988512, pvalue=1.933368656965636e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7801542995403817, 2.4139663730387116e-48), 'spearman': SpearmanrResult(correlation=0.7920237801206675, pvalue=9.065517533015282e-51), 'nsamples': 230}, 'postediting': {'pearson': (0.7974923843952013, 5.022998499357221e-55), 'spearman': SpearmanrResult(correlation=0.814917431232497, pvalue=2.954497405507159e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.272114255627029, 6.725587996708446e-05), 'spearman': SpearmanrResult(correlation=0.2731472959864174, pvalue=6.29361050880475e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6141815415911728, 'wmean': 0.6220536526855376}, 'spearman': {'mean': 0.6217693504944174, 'wmean': 0.6297316999142547}}}, 'MR': {'devacc': 84.34, 'acc': 84.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.87, 'acc': 89.4, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.37, 'acc': 85.64, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 96.03, 'acc': 95.73, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 89.22, 'acc': 88.47, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 47.96, 'acc': 46.56, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.77, 'acc': 94.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.31, 'acc': 74.96, 'f1': 82.19, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.8, 'acc': 75.95, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7905127265781522, 'pearson': 0.7761838542307102, 'spearman': 0.7172156596106803, 'mse': 0.4050631270949628, 'yhat': array([3.71698159, 4.33467519, 2.56461457, ..., 3.15469661, 4.84617977,        4.62214116]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7007456662712341, 'pearson': 0.6555238874321705, 'spearman': 0.6517097491851377, 'mse': 1.4610273806041196, 'yhat': array([1.58596624, 1.62764268, 3.14005029, ..., 3.81913578, 4.06361211,        3.6435947 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.08, 'acc': 66.72, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 77.9, 'acc': 77.12, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 47.72, 'acc': 48.36, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.31, 'acc': 34.59, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.41, 'acc': 73.12, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.11, 'acc': 90.76, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.57, 'acc': 88.43, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.6, 'acc': 85.01, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.01, 'acc': 82.58, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 69.37, 'acc': 69.06, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 75.85, 'acc': 74.91, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 18:02:59,547 : STS12 p=0.4708, STS12 s=0.5043, STS13 p=0.5104, STS13 s=0.5090, STS14 p=0.5516, STS14 s=0.5334, STS15 p=0.6358, STS15 s=0.6425, STS 16 p=0.6221, STS16 s=0.6297, STS B p=0.6555, STS B s=0.6517, STS B m=1.4610, SICK-R p=0.7762, SICK-R s=0.7172, SICK-P m=0.4051
2019-03-13 18:02:59,547 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 18:02:59,547 : 0.4708,0.5043,0.5104,0.5090,0.5516,0.5334,0.6358,0.6425,0.6221,0.6297,0.6555,0.6517,1.4610,0.7762,0.7172,0.4051
2019-03-13 18:02:59,548 : MR=84.21, CR=89.40, SUBJ=95.73, MPQA=85.64, SST-B=88.47, SST-F=46.56, TREC=94.40, SICK-E=75.95, SNLI=66.72, MRPC=74.96, MRPC f=82.19
2019-03-13 18:02:59,548 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 18:02:59,548 : 84.21,89.40,95.73,85.64,88.47,46.56,94.40,75.95,66.72,74.96,82.19
2019-03-13 18:02:59,548 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 18:02:59,548 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 18:02:59,548 : na,na,na,na,na,na,na,na,na,na
2019-03-13 18:02:59,548 : SentLen=77.12, WC=48.36, TreeDepth=34.59, TopConst=73.12, BShift=90.76, Tense=88.43, SubjNum=85.01, ObjNum=82.58, SOMO=69.06, CoordInv=74.91, average=72.39
2019-03-13 18:02:59,548 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 18:02:59,548 : 77.12,48.36,34.59,73.12,90.76,88.43,85.01,82.58,69.06,74.91,72.39
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,548 : layer 21
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,548 : ********************************************************************************
2019-03-13 18:02:59,639 : ***** Transfer task : STS12 *****


2019-03-13 18:02:59,652 : loading BERT model bert-large-uncased
2019-03-13 18:02:59,652 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:02:59,669 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:02:59,669 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgnk69w42
2019-03-13 18:03:07,162 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:03:16,480 : MSRpar : pearson = 0.3799, spearman = 0.4312
2019-03-13 18:03:18,130 : MSRvid : pearson = 0.3927, spearman = 0.4336
2019-03-13 18:03:19,549 : SMTeuroparl : pearson = 0.5192, spearman = 0.5811
2019-03-13 18:03:22,261 : surprise.OnWN : pearson = 0.5205, spearman = 0.5587
2019-03-13 18:03:23,700 : surprise.SMTnews : pearson = 0.6540, spearman = 0.6324
2019-03-13 18:03:23,700 : ALL (weighted average) : Pearson = 0.4727,             Spearman = 0.5105
2019-03-13 18:03:23,700 : ALL (average) : Pearson = 0.4932,             Spearman = 0.5274

2019-03-13 18:03:23,700 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 18:03:23,709 : loading BERT model bert-large-uncased
2019-03-13 18:03:23,709 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:03:23,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:03:23,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2im30zjf
2019-03-13 18:03:31,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:03:37,868 : FNWN : pearson = 0.3713, spearman = 0.3791
2019-03-13 18:03:39,773 : headlines : pearson = 0.5911, spearman = 0.5840
2019-03-13 18:03:41,252 : OnWN : pearson = 0.5125, spearman = 0.5250
2019-03-13 18:03:41,252 : ALL (weighted average) : Pearson = 0.5340,             Spearman = 0.5361
2019-03-13 18:03:41,252 : ALL (average) : Pearson = 0.4916,             Spearman = 0.4960

2019-03-13 18:03:41,252 : ***** Transfer task : STS14 *****


2019-03-13 18:03:41,296 : loading BERT model bert-large-uncased
2019-03-13 18:03:41,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:03:41,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:03:41,313 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ez1qs5g
2019-03-13 18:03:48,762 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:03:55,459 : deft-forum : pearson = 0.2312, spearman = 0.2562
2019-03-13 18:03:57,107 : deft-news : pearson = 0.7503, spearman = 0.7001
2019-03-13 18:03:59,288 : headlines : pearson = 0.5492, spearman = 0.5213
2019-03-13 18:04:01,378 : images : pearson = 0.5436, spearman = 0.5192
2019-03-13 18:04:03,524 : OnWN : pearson = 0.6739, spearman = 0.6938
2019-03-13 18:04:06,404 : tweet-news : pearson = 0.6142, spearman = 0.5699
2019-03-13 18:04:06,404 : ALL (weighted average) : Pearson = 0.5639,             Spearman = 0.5476
2019-03-13 18:04:06,404 : ALL (average) : Pearson = 0.5604,             Spearman = 0.5434

2019-03-13 18:04:06,404 : ***** Transfer task : STS15 *****


2019-03-13 18:04:06,436 : loading BERT model bert-large-uncased
2019-03-13 18:04:06,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:04:06,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:04:06,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxio8weq8
2019-03-13 18:04:13,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:04:21,101 : answers-forums : pearson = 0.6213, spearman = 0.6132
2019-03-13 18:04:23,201 : answers-students : pearson = 0.5960, spearman = 0.6211
2019-03-13 18:04:25,262 : belief : pearson = 0.6696, spearman = 0.7008
2019-03-13 18:04:27,528 : headlines : pearson = 0.6330, spearman = 0.6332
2019-03-13 18:04:29,676 : images : pearson = 0.6385, spearman = 0.6526
2019-03-13 18:04:29,676 : ALL (weighted average) : Pearson = 0.6282,             Spearman = 0.6410
2019-03-13 18:04:29,676 : ALL (average) : Pearson = 0.6317,             Spearman = 0.6442

2019-03-13 18:04:29,676 : ***** Transfer task : STS16 *****


2019-03-13 18:04:29,714 : loading BERT model bert-large-uncased
2019-03-13 18:04:29,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:04:29,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:04:29,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp97f3pgmf
2019-03-13 18:04:37,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:04:43,312 : answer-answer : pearson = 0.5265, spearman = 0.5324
2019-03-13 18:04:43,978 : headlines : pearson = 0.6384, spearman = 0.6617
2019-03-13 18:04:44,868 : plagiarism : pearson = 0.7831, spearman = 0.7917
2019-03-13 18:04:46,374 : postediting : pearson = 0.7839, spearman = 0.8093
2019-03-13 18:04:46,985 : question-question : pearson = 0.3726, spearman = 0.3758
2019-03-13 18:04:46,985 : ALL (weighted average) : Pearson = 0.6256,             Spearman = 0.6392
2019-03-13 18:04:46,985 : ALL (average) : Pearson = 0.6209,             Spearman = 0.6342

2019-03-13 18:04:46,985 : ***** Transfer task : MR *****


2019-03-13 18:04:47,030 : loading BERT model bert-large-uncased
2019-03-13 18:04:47,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:04:47,050 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:04:47,050 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpujo0s47l
2019-03-13 18:04:54,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:04:59,720 : Generating sentence embeddings
2019-03-13 18:05:31,444 : Generated sentence embeddings
2019-03-13 18:05:31,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:05:42,386 : Best param found at split 1: l2reg = 0.001                 with score 83.99
2019-03-13 18:05:52,068 : Best param found at split 2: l2reg = 0.01                 with score 83.72
2019-03-13 18:06:01,689 : Best param found at split 3: l2reg = 0.01                 with score 84.11
2019-03-13 18:06:11,524 : Best param found at split 4: l2reg = 0.0001                 with score 83.95
2019-03-13 18:06:21,182 : Best param found at split 5: l2reg = 0.01                 with score 84.0
2019-03-13 18:06:21,566 : Dev acc : 83.95 Test acc : 82.86

2019-03-13 18:06:21,567 : ***** Transfer task : CR *****


2019-03-13 18:06:21,574 : loading BERT model bert-large-uncased
2019-03-13 18:06:21,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:06:21,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:06:21,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphaj5g8dq
2019-03-13 18:06:29,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:06:34,291 : Generating sentence embeddings
2019-03-13 18:06:42,650 : Generated sentence embeddings
2019-03-13 18:06:42,650 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:06:45,736 : Best param found at split 1: l2reg = 0.001                 with score 89.63
2019-03-13 18:06:49,375 : Best param found at split 2: l2reg = 0.0001                 with score 89.2
2019-03-13 18:06:53,516 : Best param found at split 3: l2reg = 0.01                 with score 90.07
2019-03-13 18:06:57,011 : Best param found at split 4: l2reg = 0.001                 with score 89.67
2019-03-13 18:07:00,721 : Best param found at split 5: l2reg = 0.001                 with score 90.17
2019-03-13 18:07:00,894 : Dev acc : 89.75 Test acc : 88.87

2019-03-13 18:07:00,894 : ***** Transfer task : MPQA *****


2019-03-13 18:07:00,900 : loading BERT model bert-large-uncased
2019-03-13 18:07:00,900 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:07:00,920 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:07:00,920 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8pb_4ulu
2019-03-13 18:07:08,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:07:13,659 : Generating sentence embeddings
2019-03-13 18:07:21,291 : Generated sentence embeddings
2019-03-13 18:07:21,291 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:07:30,630 : Best param found at split 1: l2reg = 0.0001                 with score 83.16
2019-03-13 18:07:41,728 : Best param found at split 2: l2reg = 0.0001                 with score 85.61
2019-03-13 18:07:51,052 : Best param found at split 3: l2reg = 1e-05                 with score 84.83
2019-03-13 18:08:01,882 : Best param found at split 4: l2reg = 1e-05                 with score 85.09
2019-03-13 18:08:11,726 : Best param found at split 5: l2reg = 0.0001                 with score 84.73
2019-03-13 18:08:12,334 : Dev acc : 84.68 Test acc : 85.61

2019-03-13 18:08:12,334 : ***** Transfer task : SUBJ *****


2019-03-13 18:08:12,350 : loading BERT model bert-large-uncased
2019-03-13 18:08:12,351 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:08:12,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:08:12,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvzbgzzrd
2019-03-13 18:08:19,794 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:08:25,019 : Generating sentence embeddings
2019-03-13 18:08:56,061 : Generated sentence embeddings
2019-03-13 18:08:56,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:09:04,405 : Best param found at split 1: l2reg = 0.001                 with score 96.2
2019-03-13 18:09:15,029 : Best param found at split 2: l2reg = 0.0001                 with score 96.36
2019-03-13 18:09:24,971 : Best param found at split 3: l2reg = 1e-05                 with score 95.79
2019-03-13 18:09:34,822 : Best param found at split 4: l2reg = 0.001                 with score 96.21
2019-03-13 18:09:44,052 : Best param found at split 5: l2reg = 1e-05                 with score 95.91
2019-03-13 18:09:44,470 : Dev acc : 96.09 Test acc : 95.75

2019-03-13 18:09:44,471 : ***** Transfer task : SST Binary classification *****


2019-03-13 18:09:44,608 : loading BERT model bert-large-uncased
2019-03-13 18:09:44,608 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:09:44,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:09:44,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplel7wa_t
2019-03-13 18:09:52,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:09:57,467 : Computing embedding for train
2019-03-13 18:11:38,378 : Computed train embeddings
2019-03-13 18:11:38,379 : Computing embedding for dev
2019-03-13 18:11:40,579 : Computed dev embeddings
2019-03-13 18:11:40,579 : Computing embedding for test
2019-03-13 18:11:45,209 : Computed test embeddings
2019-03-13 18:11:45,209 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:12:00,664 : [('reg:1e-05', 87.96), ('reg:0.0001', 87.96), ('reg:0.001', 88.07), ('reg:0.01', 87.39)]
2019-03-13 18:12:00,664 : Validation : best param found is reg = 0.001 with score             88.07
2019-03-13 18:12:00,664 : Evaluating...
2019-03-13 18:12:04,951 : 
Dev acc : 88.07 Test acc : 87.86 for             SST Binary classification

2019-03-13 18:12:04,952 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 18:12:05,002 : loading BERT model bert-large-uncased
2019-03-13 18:12:05,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:12:05,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:12:05,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx2nrqrdc
2019-03-13 18:12:12,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:12:17,720 : Computing embedding for train
2019-03-13 18:12:39,809 : Computed train embeddings
2019-03-13 18:12:39,809 : Computing embedding for dev
2019-03-13 18:12:42,692 : Computed dev embeddings
2019-03-13 18:12:42,692 : Computing embedding for test
2019-03-13 18:12:48,374 : Computed test embeddings
2019-03-13 18:12:48,374 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:12:50,853 : [('reg:1e-05', 48.05), ('reg:0.0001', 47.68), ('reg:0.001', 47.77), ('reg:0.01', 47.96)]
2019-03-13 18:12:50,853 : Validation : best param found is reg = 1e-05 with score             48.05
2019-03-13 18:12:50,853 : Evaluating...
2019-03-13 18:12:51,507 : 
Dev acc : 48.05 Test acc : 48.91 for             SST Fine-Grained classification

2019-03-13 18:12:51,507 : ***** Transfer task : TREC *****


2019-03-13 18:12:51,520 : loading BERT model bert-large-uncased
2019-03-13 18:12:51,520 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:12:51,540 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:12:51,540 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphn27r1xc
2019-03-13 18:12:58,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:13:11,845 : Computed train embeddings
2019-03-13 18:13:12,438 : Computed test embeddings
2019-03-13 18:13:12,438 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:13:17,954 : [('reg:1e-05', 84.9), ('reg:0.0001', 84.85), ('reg:0.001', 82.83), ('reg:0.01', 78.25)]
2019-03-13 18:13:17,954 : Cross-validation : best param found is reg = 1e-05             with score 84.9
2019-03-13 18:13:17,954 : Evaluating...
2019-03-13 18:13:18,294 : 
Dev acc : 84.9 Test acc : 86.0             for TREC

2019-03-13 18:13:18,294 : ***** Transfer task : MRPC *****


2019-03-13 18:13:18,316 : loading BERT model bert-large-uncased
2019-03-13 18:13:18,316 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:13:18,376 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:13:18,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb4ijklud
2019-03-13 18:13:25,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:13:31,087 : Computing embedding for train
2019-03-13 18:13:53,498 : Computed train embeddings
2019-03-13 18:13:53,498 : Computing embedding for test
2019-03-13 18:14:03,318 : Computed test embeddings
2019-03-13 18:14:03,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:14:08,262 : [('reg:1e-05', 73.65), ('reg:0.0001', 74.07), ('reg:0.001', 73.31), ('reg:0.01', 72.25)]
2019-03-13 18:14:08,262 : Cross-validation : best param found is reg = 0.0001             with score 74.07
2019-03-13 18:14:08,262 : Evaluating...
2019-03-13 18:14:08,574 : Dev acc : 74.07 Test acc 73.51; Test F1 79.79 for MRPC.

2019-03-13 18:14:08,575 : ***** Transfer task : SICK-Entailment*****


2019-03-13 18:14:08,598 : loading BERT model bert-large-uncased
2019-03-13 18:14:08,598 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:14:08,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:14:08,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpss7vybf0
2019-03-13 18:14:16,022 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:14:21,279 : Computing embedding for train
2019-03-13 18:14:32,643 : Computed train embeddings
2019-03-13 18:14:32,643 : Computing embedding for dev
2019-03-13 18:14:34,193 : Computed dev embeddings
2019-03-13 18:14:34,193 : Computing embedding for test
2019-03-13 18:14:46,416 : Computed test embeddings
2019-03-13 18:14:46,452 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:14:48,072 : [('reg:1e-05', 76.2), ('reg:0.0001', 76.8), ('reg:0.001', 78.0), ('reg:0.01', 72.4)]
2019-03-13 18:14:48,072 : Validation : best param found is reg = 0.001 with score             78.0
2019-03-13 18:14:48,072 : Evaluating...
2019-03-13 18:14:48,474 : 
Dev acc : 78.0 Test acc : 75.97 for                        SICK entailment

2019-03-13 18:14:48,474 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 18:14:48,500 : loading BERT model bert-large-uncased
2019-03-13 18:14:48,501 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:14:48,520 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:14:48,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbi_uryzw
2019-03-13 18:14:55,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:15:01,214 : Computing embedding for train
2019-03-13 18:15:12,613 : Computed train embeddings
2019-03-13 18:15:12,613 : Computing embedding for dev
2019-03-13 18:15:14,170 : Computed dev embeddings
2019-03-13 18:15:14,170 : Computing embedding for test
2019-03-13 18:15:26,402 : Computed test embeddings
2019-03-13 18:15:40,161 : Dev : Pearson 0.7682861979632477
2019-03-13 18:15:40,161 : Test : Pearson 0.792077768959647 Spearman 0.7355562752658711 MSE 0.3797260551421253                        for SICK Relatedness

2019-03-13 18:15:40,162 : 

***** Transfer task : STSBenchmark*****


2019-03-13 18:15:40,211 : loading BERT model bert-large-uncased
2019-03-13 18:15:40,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:15:40,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:15:40,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7sl60nc7
2019-03-13 18:15:47,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:15:52,959 : Computing embedding for train
2019-03-13 18:16:11,692 : Computed train embeddings
2019-03-13 18:16:11,692 : Computing embedding for dev
2019-03-13 18:16:17,388 : Computed dev embeddings
2019-03-13 18:16:17,388 : Computing embedding for test
2019-03-13 18:16:22,043 : Computed test embeddings
2019-03-13 18:16:41,716 : Dev : Pearson 0.6882760834625685
2019-03-13 18:16:41,716 : Test : Pearson 0.6466284980724861 Spearman 0.640834939463817 MSE 1.519066492815737                        for SICK Relatedness

2019-03-13 18:16:41,716 : ***** Transfer task : SNLI Entailment*****


2019-03-13 18:16:46,706 : loading BERT model bert-large-uncased
2019-03-13 18:16:46,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:16:46,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:16:46,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvmty1yy1
2019-03-13 18:16:54,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:17:00,087 : PROGRESS (encoding): 0.00%
2019-03-13 18:19:47,574 : PROGRESS (encoding): 14.56%
2019-03-13 18:23:00,437 : PROGRESS (encoding): 29.12%
2019-03-13 18:26:12,122 : PROGRESS (encoding): 43.69%
2019-03-13 18:29:36,240 : PROGRESS (encoding): 58.25%
2019-03-13 18:33:23,502 : PROGRESS (encoding): 72.81%
2019-03-13 18:37:09,344 : PROGRESS (encoding): 87.37%
2019-03-13 18:41:13,908 : PROGRESS (encoding): 0.00%
2019-03-13 18:41:44,544 : PROGRESS (encoding): 0.00%
2019-03-13 18:42:14,066 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:42:40,047 : [('reg:1e-09', 65.58)]
2019-03-13 18:42:40,048 : Validation : best param found is reg = 1e-09 with score             65.58
2019-03-13 18:42:40,048 : Evaluating...
2019-03-13 18:43:06,690 : Dev acc : 65.58 Test acc : 65.55 for SNLI

2019-03-13 18:43:06,690 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 18:43:06,899 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 18:43:07,914 : loading BERT model bert-large-uncased
2019-03-13 18:43:07,914 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:43:07,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:43:07,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppjz9x6j_
2019-03-13 18:43:15,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:43:20,712 : Computing embeddings for train/dev/test
2019-03-13 18:46:56,007 : Computed embeddings
2019-03-13 18:46:56,007 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:47:29,001 : [('reg:1e-05', 73.91), ('reg:0.0001', 71.6), ('reg:0.001', 63.36), ('reg:0.01', 49.65)]
2019-03-13 18:47:29,001 : Validation : best param found is reg = 1e-05 with score             73.91
2019-03-13 18:47:29,001 : Evaluating...
2019-03-13 18:47:38,720 : 
Dev acc : 73.9 Test acc : 73.4 for LENGTH classification

2019-03-13 18:47:38,721 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 18:47:39,092 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 18:47:39,140 : loading BERT model bert-large-uncased
2019-03-13 18:47:39,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:47:39,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:47:39,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmputg0ykfm
2019-03-13 18:47:46,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:47:51,921 : Computing embeddings for train/dev/test
2019-03-13 18:51:09,319 : Computed embeddings
2019-03-13 18:51:09,319 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:51:41,135 : [('reg:1e-05', 46.43), ('reg:0.0001', 24.82), ('reg:0.001', 3.26), ('reg:0.01', 0.92)]
2019-03-13 18:51:41,135 : Validation : best param found is reg = 1e-05 with score             46.43
2019-03-13 18:51:41,136 : Evaluating...
2019-03-13 18:51:48,400 : 
Dev acc : 46.4 Test acc : 46.7 for WORDCONTENT classification

2019-03-13 18:51:48,401 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 18:51:48,772 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 18:51:48,841 : loading BERT model bert-large-uncased
2019-03-13 18:51:48,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:51:48,865 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:51:48,866 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvjiqxgl1
2019-03-13 18:51:56,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:52:01,568 : Computing embeddings for train/dev/test
2019-03-13 18:55:08,242 : Computed embeddings
2019-03-13 18:55:08,243 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:55:30,017 : [('reg:1e-05', 33.09), ('reg:0.0001', 32.84), ('reg:0.001', 31.48), ('reg:0.01', 26.69)]
2019-03-13 18:55:30,017 : Validation : best param found is reg = 1e-05 with score             33.09
2019-03-13 18:55:30,017 : Evaluating...
2019-03-13 18:55:35,611 : 
Dev acc : 33.1 Test acc : 32.6 for DEPTH classification

2019-03-13 18:55:35,612 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 18:55:36,003 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 18:55:36,067 : loading BERT model bert-large-uncased
2019-03-13 18:55:36,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:55:36,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:55:36,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2psgghlj
2019-03-13 18:55:43,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:55:48,911 : Computing embeddings for train/dev/test
2019-03-13 18:58:41,252 : Computed embeddings
2019-03-13 18:58:41,252 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:59:14,259 : [('reg:1e-05', 69.24), ('reg:0.0001', 67.16), ('reg:0.001', 64.48), ('reg:0.01', 51.45)]
2019-03-13 18:59:14,259 : Validation : best param found is reg = 1e-05 with score             69.24
2019-03-13 18:59:14,259 : Evaluating...
2019-03-13 18:59:23,179 : 
Dev acc : 69.2 Test acc : 69.4 for TOPCONSTITUENTS classification

2019-03-13 18:59:23,180 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 18:59:23,531 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 18:59:23,597 : loading BERT model bert-large-uncased
2019-03-13 18:59:23,597 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:59:23,717 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:59:23,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk2uheupa
2019-03-13 18:59:31,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:59:36,539 : Computing embeddings for train/dev/test
2019-03-13 19:02:42,384 : Computed embeddings
2019-03-13 19:02:42,384 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:03:10,302 : [('reg:1e-05', 91.79), ('reg:0.0001', 91.67), ('reg:0.001', 91.23), ('reg:0.01', 89.43)]
2019-03-13 19:03:10,302 : Validation : best param found is reg = 1e-05 with score             91.79
2019-03-13 19:03:10,302 : Evaluating...
2019-03-13 19:03:16,836 : 
Dev acc : 91.8 Test acc : 91.2 for BIGRAMSHIFT classification

2019-03-13 19:03:16,837 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 19:03:17,216 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 19:03:17,281 : loading BERT model bert-large-uncased
2019-03-13 19:03:17,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:03:17,401 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:03:17,401 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgneh9ae0
2019-03-13 19:03:24,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:03:30,226 : Computing embeddings for train/dev/test
2019-03-13 19:06:32,241 : Computed embeddings
2019-03-13 19:06:32,241 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:06:53,021 : [('reg:1e-05', 89.39), ('reg:0.0001', 89.57), ('reg:0.001', 90.09), ('reg:0.01', 90.71)]
2019-03-13 19:06:53,021 : Validation : best param found is reg = 0.01 with score             90.71
2019-03-13 19:06:53,022 : Evaluating...
2019-03-13 19:06:59,954 : 
Dev acc : 90.7 Test acc : 88.8 for TENSE classification

2019-03-13 19:06:59,955 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 19:07:00,566 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 19:07:00,634 : loading BERT model bert-large-uncased
2019-03-13 19:07:00,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:07:00,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:07:00,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvt64mlrh
2019-03-13 19:07:08,166 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:07:13,468 : Computing embeddings for train/dev/test
2019-03-13 19:10:27,447 : Computed embeddings
2019-03-13 19:10:27,448 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:10:59,646 : [('reg:1e-05', 84.88), ('reg:0.0001', 84.91), ('reg:0.001', 84.34), ('reg:0.01', 82.83)]
2019-03-13 19:10:59,646 : Validation : best param found is reg = 0.0001 with score             84.91
2019-03-13 19:10:59,646 : Evaluating...
2019-03-13 19:11:08,539 : 
Dev acc : 84.9 Test acc : 84.4 for SUBJNUMBER classification

2019-03-13 19:11:08,540 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 19:11:08,961 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 19:11:09,028 : loading BERT model bert-large-uncased
2019-03-13 19:11:09,028 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:11:09,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:11:09,054 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9i8bl_bl
2019-03-13 19:11:16,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:11:21,708 : Computing embeddings for train/dev/test
2019-03-13 19:14:31,994 : Computed embeddings
2019-03-13 19:14:31,994 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:14:55,870 : [('reg:1e-05', 80.49), ('reg:0.0001', 80.6), ('reg:0.001', 80.6), ('reg:0.01', 79.39)]
2019-03-13 19:14:55,870 : Validation : best param found is reg = 0.0001 with score             80.6
2019-03-13 19:14:55,870 : Evaluating...
2019-03-13 19:15:01,021 : 
Dev acc : 80.6 Test acc : 81.7 for OBJNUMBER classification

2019-03-13 19:15:01,022 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 19:15:01,417 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 19:15:01,487 : loading BERT model bert-large-uncased
2019-03-13 19:15:01,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:15:01,623 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:15:01,623 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf_ek9gl4
2019-03-13 19:15:09,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:15:14,310 : Computing embeddings for train/dev/test
2019-03-13 19:18:56,618 : Computed embeddings
2019-03-13 19:18:56,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:19:21,821 : [('reg:1e-05', 68.63), ('reg:0.0001', 68.69), ('reg:0.001', 67.82), ('reg:0.01', 66.31)]
2019-03-13 19:19:21,821 : Validation : best param found is reg = 0.0001 with score             68.69
2019-03-13 19:19:21,821 : Evaluating...
2019-03-13 19:19:27,927 : 
Dev acc : 68.7 Test acc : 68.5 for ODDMANOUT classification

2019-03-13 19:19:27,928 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 19:19:28,534 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 19:19:28,609 : loading BERT model bert-large-uncased
2019-03-13 19:19:28,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:19:28,639 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:19:28,639 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprj638r7_
2019-03-13 19:19:36,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:19:41,355 : Computing embeddings for train/dev/test
2019-03-13 19:23:21,489 : Computed embeddings
2019-03-13 19:23:21,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:23:49,214 : [('reg:1e-05', 73.12), ('reg:0.0001', 72.9), ('reg:0.001', 72.71), ('reg:0.01', 64.04)]
2019-03-13 19:23:49,214 : Validation : best param found is reg = 1e-05 with score             73.12
2019-03-13 19:23:49,215 : Evaluating...
2019-03-13 19:23:58,836 : 
Dev acc : 73.1 Test acc : 73.2 for COORDINATIONINVERSION classification

2019-03-13 19:23:58,838 : total results: {'STS12': {'MSRpar': {'pearson': (0.3798698566343639, 3.7143541104024105e-27), 'spearman': SpearmanrResult(correlation=0.43119640314881286, pvalue=2.6007388323323097e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.39265897736908045, 4.671488090997068e-29), 'spearman': SpearmanrResult(correlation=0.4336306080427865, pvalue=9.8199983265733e-36), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5191591610551092, 4.870488504381052e-33), 'spearman': SpearmanrResult(correlation=0.5810680277699969, pvalue=8.427018265160154e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5205380474725876, 2.6054717886193426e-53), 'spearman': SpearmanrResult(correlation=0.5586996059065943, pvalue=8.713246723119456e-63), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.653973604735595, 4.746466100524831e-50), 'spearman': SpearmanrResult(correlation=0.6324205267277874, pvalue=5.875327276742953e-46), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49323992945334727, 'wmean': 0.4726607735909978}, 'spearman': {'mean': 0.5274030343191957, 'wmean': 0.5105183326043954}}}, 'STS13': {'FNWN': {'pearson': (0.3713165901022158, 1.439181719228139e-07), 'spearman': SpearmanrResult(correlation=0.3791186417282574, pvalue=7.463863800286364e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.5910582706778682, 7.657948035065008e-72), 'spearman': SpearmanrResult(correlation=0.5840094548838239, pvalue=8.788856717794896e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.5124842926796762, 6.801876445624118e-39), 'spearman': SpearmanrResult(correlation=0.5249819174028173, pvalue=4.664991004567323e-41), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.49161971781992003, 'wmean': 0.5339841511540122}, 'spearman': {'mean': 0.49603667133829954, 'wmean': 0.536116913408326}}}, 'STS14': {'deft-forum': {'pearson': (0.23115475605982053, 7.152062861090851e-07), 'spearman': SpearmanrResult(correlation=0.25617112989829816, pvalue=3.561412081764544e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.750284574313989, 1.7008894420592748e-55), 'spearman': SpearmanrResult(correlation=0.7000570690385333, pvalue=1.7205139864385335e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5492011701373681, 2.5921797995497315e-60), 'spearman': SpearmanrResult(correlation=0.5212984233292934, pvalue=1.732477596364339e-53), 'nsamples': 750}, 'images': {'pearson': (0.5436219487021066, 6.760010543769655e-59), 'spearman': SpearmanrResult(correlation=0.5192164163724992, pvalue=5.282621444417554e-53), 'nsamples': 750}, 'OnWN': {'pearson': (0.6738957223124292, 2.0198027816118285e-100), 'spearman': SpearmanrResult(correlation=0.6937834550917127, pvalue=9.785065916300589e-109), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6141553242617354, 5.82700161562411e-79), 'spearman': SpearmanrResult(correlation=0.5699424315284276, pvalue=8.077526668216702e-66), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5603855826312415, 'wmean': 0.5639361697550255}, 'spearman': {'mean': 0.5434114875431274, 'wmean': 0.5475932463752651}}}, 'STS15': {'answers-forums': {'pearson': (0.6213169110667408, 2.0393022250235726e-41), 'spearman': SpearmanrResult(correlation=0.6131861311307502, pvalue=4.2519315253604685e-40), 'nsamples': 375}, 'answers-students': {'pearson': (0.5959962031637928, 2.5741042982860712e-73), 'spearman': SpearmanrResult(correlation=0.6210565368163186, pvalue=3.3276766317628286e-81), 'nsamples': 750}, 'belief': {'pearson': (0.6695849748489875, 4.0711414986766596e-50), 'spearman': SpearmanrResult(correlation=0.7008416489347749, pvalue=1.1049889820721223e-56), 'nsamples': 375}, 'headlines': {'pearson': (0.632952883449495, 3.316780276293466e-85), 'spearman': SpearmanrResult(correlation=0.6331647662151482, pvalue=2.804508390640263e-85), 'nsamples': 750}, 'images': {'pearson': (0.6384785522000231, 3.9980316562014526e-87), 'spearman': SpearmanrResult(correlation=0.6525814630325468, pvalue=3.321737628549421e-92), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6316659049458078, 'wmean': 0.6282196454427939}, 'spearman': {'mean': 0.6441661092259077, 'wmean': 0.640954164024194}}}, 'STS16': {'answer-answer': {'pearson': (0.5264867399223092, 1.6302110344154836e-19), 'spearman': SpearmanrResult(correlation=0.5323717687276032, pvalue=5.416834887321329e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6383678452457687, 6.658402273058316e-30), 'spearman': SpearmanrResult(correlation=0.6617170835995916, pvalue=9.708599126542826e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7830636623490655, 6.344825926569681e-49), 'spearman': SpearmanrResult(correlation=0.7917156756861069, pvalue=1.0527797480495224e-50), 'nsamples': 230}, 'postediting': {'pearson': (0.7839174525659554, 5.242420447858958e-52), 'spearman': SpearmanrResult(correlation=0.8092855177953323, pvalue=7.679511091890814e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.37257563404965804, 2.7718210589642747e-08), 'spearman': SpearmanrResult(correlation=0.37584847823645334, pvalue=2.047213315391694e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6208822668265515, 'wmean': 0.6255733842236254}, 'spearman': {'mean': 0.6341877048090175, 'wmean': 0.6392096009902141}}}, 'MR': {'devacc': 83.95, 'acc': 82.86, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.75, 'acc': 88.87, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.68, 'acc': 85.61, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 96.09, 'acc': 95.75, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 88.07, 'acc': 87.86, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 48.05, 'acc': 48.91, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.9, 'acc': 86.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.07, 'acc': 73.51, 'f1': 79.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 75.97, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7682861979632477, 'pearson': 0.792077768959647, 'spearman': 0.7355562752658711, 'mse': 0.3797260551421253, 'yhat': array([3.47179921, 4.5729149 , 2.10999782, ..., 3.22521979, 4.53567378,        4.34249692]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6882760834625685, 'pearson': 0.6466284980724861, 'spearman': 0.640834939463817, 'mse': 1.519066492815737, 'yhat': array([1.61417641, 3.62554723, 3.3766959 , ..., 3.80530794, 3.61694537,        3.37677433]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.58, 'acc': 65.55, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 73.91, 'acc': 73.39, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 46.43, 'acc': 46.72, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.09, 'acc': 32.6, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.24, 'acc': 69.38, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.79, 'acc': 91.21, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.71, 'acc': 88.76, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.91, 'acc': 84.39, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.6, 'acc': 81.66, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 68.69, 'acc': 68.51, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.12, 'acc': 73.17, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 19:23:58,838 : STS12 p=0.4727, STS12 s=0.5105, STS13 p=0.5340, STS13 s=0.5361, STS14 p=0.5639, STS14 s=0.5476, STS15 p=0.6282, STS15 s=0.6410, STS 16 p=0.6256, STS16 s=0.6392, STS B p=0.6466, STS B s=0.6408, STS B m=1.5191, SICK-R p=0.7921, SICK-R s=0.7356, SICK-P m=0.3797
2019-03-13 19:23:58,839 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 19:23:58,839 : 0.4727,0.5105,0.5340,0.5361,0.5639,0.5476,0.6282,0.6410,0.6256,0.6392,0.6466,0.6408,1.5191,0.7921,0.7356,0.3797
2019-03-13 19:23:58,839 : MR=82.86, CR=88.87, SUBJ=95.75, MPQA=85.61, SST-B=87.86, SST-F=48.91, TREC=86.00, SICK-E=75.97, SNLI=65.55, MRPC=73.51, MRPC f=79.79
2019-03-13 19:23:58,839 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 19:23:58,839 : 82.86,88.87,95.75,85.61,87.86,48.91,86.00,75.97,65.55,73.51,79.79
2019-03-13 19:23:58,839 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 19:23:58,839 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 19:23:58,839 : na,na,na,na,na,na,na,na,na,na
2019-03-13 19:23:58,839 : SentLen=73.39, WC=46.72, TreeDepth=32.60, TopConst=69.38, BShift=91.21, Tense=88.76, SubjNum=84.39, ObjNum=81.66, SOMO=68.51, CoordInv=73.17, average=70.98
2019-03-13 19:23:58,839 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 19:23:58,839 : 73.39,46.72,32.60,69.38,91.21,88.76,84.39,81.66,68.51,73.17,70.98
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,839 : layer 22
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,839 : ********************************************************************************
2019-03-13 19:23:58,930 : ***** Transfer task : STS12 *****


2019-03-13 19:23:58,942 : loading BERT model bert-large-uncased
2019-03-13 19:23:58,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:23:58,960 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:23:58,960 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd5eh_mi1
2019-03-13 19:24:06,378 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:24:15,694 : MSRpar : pearson = 0.3717, spearman = 0.4343
2019-03-13 19:24:17,350 : MSRvid : pearson = 0.4332, spearman = 0.4761
2019-03-13 19:24:18,775 : SMTeuroparl : pearson = 0.5132, spearman = 0.5585
2019-03-13 19:24:21,494 : surprise.OnWN : pearson = 0.4913, spearman = 0.5443
2019-03-13 19:24:22,936 : surprise.SMTnews : pearson = 0.6365, spearman = 0.6232
2019-03-13 19:24:22,936 : ALL (weighted average) : Pearson = 0.4703,             Spearman = 0.5135
2019-03-13 19:24:22,936 : ALL (average) : Pearson = 0.4892,             Spearman = 0.5273

2019-03-13 19:24:22,936 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 19:24:22,945 : loading BERT model bert-large-uncased
2019-03-13 19:24:22,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:24:22,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:24:22,962 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjdqvbazg
2019-03-13 19:24:30,386 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:24:37,043 : FNWN : pearson = 0.3659, spearman = 0.3687
2019-03-13 19:24:38,952 : headlines : pearson = 0.5763, spearman = 0.5960
2019-03-13 19:24:40,432 : OnWN : pearson = 0.5133, spearman = 0.5321
2019-03-13 19:24:40,432 : ALL (weighted average) : Pearson = 0.5262,             Spearman = 0.5435
2019-03-13 19:24:40,432 : ALL (average) : Pearson = 0.4852,             Spearman = 0.4989

2019-03-13 19:24:40,432 : ***** Transfer task : STS14 *****


2019-03-13 19:24:40,476 : loading BERT model bert-large-uncased
2019-03-13 19:24:40,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:24:40,494 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:24:40,494 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyn_wuekn
2019-03-13 19:24:47,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:24:54,623 : deft-forum : pearson = 0.2158, spearman = 0.2393
2019-03-13 19:24:56,275 : deft-news : pearson = 0.7413, spearman = 0.7017
2019-03-13 19:24:58,462 : headlines : pearson = 0.5330, spearman = 0.5301
2019-03-13 19:25:00,554 : images : pearson = 0.5457, spearman = 0.5253
2019-03-13 19:25:02,706 : OnWN : pearson = 0.6585, spearman = 0.6875
2019-03-13 19:25:05,589 : tweet-news : pearson = 0.5733, spearman = 0.5437
2019-03-13 19:25:05,590 : ALL (weighted average) : Pearson = 0.5473,             Spearman = 0.5422
2019-03-13 19:25:05,590 : ALL (average) : Pearson = 0.5446,             Spearman = 0.5380

2019-03-13 19:25:05,591 : ***** Transfer task : STS15 *****


2019-03-13 19:25:05,641 : loading BERT model bert-large-uncased
2019-03-13 19:25:05,641 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:25:05,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:25:05,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv3hb_85n
2019-03-13 19:25:13,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:25:20,312 : answers-forums : pearson = 0.6049, spearman = 0.5976
2019-03-13 19:25:22,415 : answers-students : pearson = 0.5412, spearman = 0.5771
2019-03-13 19:25:24,478 : belief : pearson = 0.6436, spearman = 0.6857
2019-03-13 19:25:26,743 : headlines : pearson = 0.6209, spearman = 0.6367
2019-03-13 19:25:28,892 : images : pearson = 0.6416, spearman = 0.6556
2019-03-13 19:25:28,892 : ALL (weighted average) : Pearson = 0.6070,             Spearman = 0.6278
2019-03-13 19:25:28,892 : ALL (average) : Pearson = 0.6104,             Spearman = 0.6305

2019-03-13 19:25:28,892 : ***** Transfer task : STS16 *****


2019-03-13 19:25:28,929 : loading BERT model bert-large-uncased
2019-03-13 19:25:28,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:25:28,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:25:28,948 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgyoo9tp9
2019-03-13 19:25:36,349 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:25:42,560 : answer-answer : pearson = 0.5013, spearman = 0.5071
2019-03-13 19:25:43,225 : headlines : pearson = 0.6029, spearman = 0.6516
2019-03-13 19:25:44,113 : plagiarism : pearson = 0.7842, spearman = 0.7925
2019-03-13 19:25:45,619 : postediting : pearson = 0.7728, spearman = 0.8050
2019-03-13 19:25:46,229 : question-question : pearson = 0.4380, spearman = 0.4325
2019-03-13 19:25:46,229 : ALL (weighted average) : Pearson = 0.6222,             Spearman = 0.6409
2019-03-13 19:25:46,229 : ALL (average) : Pearson = 0.6198,             Spearman = 0.6377

2019-03-13 19:25:46,230 : ***** Transfer task : MR *****


2019-03-13 19:25:46,275 : loading BERT model bert-large-uncased
2019-03-13 19:25:46,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:25:46,295 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:25:46,295 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgpap3c9a
2019-03-13 19:25:53,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:25:58,948 : Generating sentence embeddings
2019-03-13 19:26:30,695 : Generated sentence embeddings
2019-03-13 19:26:30,695 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:26:41,368 : Best param found at split 1: l2reg = 0.001                 with score 84.28
2019-03-13 19:26:51,069 : Best param found at split 2: l2reg = 0.01                 with score 83.86
2019-03-13 19:26:59,417 : Best param found at split 3: l2reg = 0.01                 with score 84.28
2019-03-13 19:27:09,022 : Best param found at split 4: l2reg = 0.001                 with score 84.53
2019-03-13 19:27:18,909 : Best param found at split 5: l2reg = 1e-05                 with score 84.08
2019-03-13 19:27:19,228 : Dev acc : 84.21 Test acc : 83.05

2019-03-13 19:27:19,229 : ***** Transfer task : CR *****


2019-03-13 19:27:19,236 : loading BERT model bert-large-uncased
2019-03-13 19:27:19,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:27:19,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:27:19,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpag1pehj8
2019-03-13 19:27:26,753 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:27:31,984 : Generating sentence embeddings
2019-03-13 19:27:40,370 : Generated sentence embeddings
2019-03-13 19:27:40,370 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:27:44,105 : Best param found at split 1: l2reg = 0.01                 with score 89.93
2019-03-13 19:27:47,607 : Best param found at split 2: l2reg = 0.01                 with score 89.53
2019-03-13 19:27:51,324 : Best param found at split 3: l2reg = 0.001                 with score 90.0
2019-03-13 19:27:54,727 : Best param found at split 4: l2reg = 0.001                 with score 89.61
2019-03-13 19:27:58,222 : Best param found at split 5: l2reg = 0.01                 with score 90.0
2019-03-13 19:27:58,442 : Dev acc : 89.81 Test acc : 89.61

2019-03-13 19:27:58,442 : ***** Transfer task : MPQA *****


2019-03-13 19:27:58,448 : loading BERT model bert-large-uncased
2019-03-13 19:27:58,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:27:58,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:27:58,468 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdjfpb_9q
2019-03-13 19:28:05,937 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:28:11,347 : Generating sentence embeddings
2019-03-13 19:28:18,990 : Generated sentence embeddings
2019-03-13 19:28:18,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:28:26,641 : Best param found at split 1: l2reg = 1e-05                 with score 82.79
2019-03-13 19:28:35,234 : Best param found at split 2: l2reg = 0.001                 with score 86.02
2019-03-13 19:28:46,192 : Best param found at split 3: l2reg = 1e-05                 with score 84.93
2019-03-13 19:28:57,434 : Best param found at split 4: l2reg = 0.01                 with score 83.96
2019-03-13 19:29:07,381 : Best param found at split 5: l2reg = 0.0001                 with score 84.53
2019-03-13 19:29:08,072 : Dev acc : 84.45 Test acc : 85.04

2019-03-13 19:29:08,073 : ***** Transfer task : SUBJ *****


2019-03-13 19:29:08,090 : loading BERT model bert-large-uncased
2019-03-13 19:29:08,090 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:29:08,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:29:08,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp37x_o96q
2019-03-13 19:29:15,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:29:20,993 : Generating sentence embeddings
2019-03-13 19:29:52,070 : Generated sentence embeddings
2019-03-13 19:29:52,070 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:30:02,069 : Best param found at split 1: l2reg = 0.001                 with score 96.14
2019-03-13 19:30:11,432 : Best param found at split 2: l2reg = 0.001                 with score 96.12
2019-03-13 19:30:20,007 : Best param found at split 3: l2reg = 1e-05                 with score 95.48
2019-03-13 19:30:27,440 : Best param found at split 4: l2reg = 0.001                 with score 96.06
2019-03-13 19:30:35,802 : Best param found at split 5: l2reg = 0.01                 with score 95.88
2019-03-13 19:30:36,211 : Dev acc : 95.94 Test acc : 95.76

2019-03-13 19:30:36,212 : ***** Transfer task : SST Binary classification *****


2019-03-13 19:30:36,348 : loading BERT model bert-large-uncased
2019-03-13 19:30:36,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:30:36,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:30:36,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7pt6iuep
2019-03-13 19:30:43,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:30:48,984 : Computing embedding for train
2019-03-13 19:32:29,978 : Computed train embeddings
2019-03-13 19:32:29,978 : Computing embedding for dev
2019-03-13 19:32:32,181 : Computed dev embeddings
2019-03-13 19:32:32,181 : Computing embedding for test
2019-03-13 19:32:36,815 : Computed test embeddings
2019-03-13 19:32:36,815 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:32:53,384 : [('reg:1e-05', 88.76), ('reg:0.0001', 88.65), ('reg:0.001', 88.3), ('reg:0.01', 87.16)]
2019-03-13 19:32:53,384 : Validation : best param found is reg = 1e-05 with score             88.76
2019-03-13 19:32:53,384 : Evaluating...
2019-03-13 19:32:57,667 : 
Dev acc : 88.76 Test acc : 87.75 for             SST Binary classification

2019-03-13 19:32:57,668 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 19:32:57,718 : loading BERT model bert-large-uncased
2019-03-13 19:32:57,718 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:32:57,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:32:57,740 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppt3q0fwp
2019-03-13 19:33:05,183 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:33:10,434 : Computing embedding for train
2019-03-13 19:33:32,539 : Computed train embeddings
2019-03-13 19:33:32,539 : Computing embedding for dev
2019-03-13 19:33:35,424 : Computed dev embeddings
2019-03-13 19:33:35,424 : Computing embedding for test
2019-03-13 19:33:41,117 : Computed test embeddings
2019-03-13 19:33:41,117 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:33:42,961 : [('reg:1e-05', 45.87), ('reg:0.0001', 46.14), ('reg:0.001', 46.41), ('reg:0.01', 46.59)]
2019-03-13 19:33:42,961 : Validation : best param found is reg = 0.01 with score             46.59
2019-03-13 19:33:42,961 : Evaluating...
2019-03-13 19:33:43,366 : 
Dev acc : 46.59 Test acc : 47.19 for             SST Fine-Grained classification

2019-03-13 19:33:43,366 : ***** Transfer task : TREC *****


2019-03-13 19:33:43,379 : loading BERT model bert-large-uncased
2019-03-13 19:33:43,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:33:43,398 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:33:43,399 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpncsre_ez
2019-03-13 19:33:50,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:34:03,744 : Computed train embeddings
2019-03-13 19:34:04,338 : Computed test embeddings
2019-03-13 19:34:04,338 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:34:09,523 : [('reg:1e-05', 83.86), ('reg:0.0001', 83.64), ('reg:0.001', 83.51), ('reg:0.01', 78.56)]
2019-03-13 19:34:09,524 : Cross-validation : best param found is reg = 1e-05             with score 83.86
2019-03-13 19:34:09,524 : Evaluating...
2019-03-13 19:34:09,937 : 
Dev acc : 83.86 Test acc : 91.8             for TREC

2019-03-13 19:34:09,938 : ***** Transfer task : MRPC *****


2019-03-13 19:34:09,959 : loading BERT model bert-large-uncased
2019-03-13 19:34:09,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:34:10,019 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:34:10,020 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphgzbaj9i
2019-03-13 19:34:17,527 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:34:22,825 : Computing embedding for train
2019-03-13 19:34:45,258 : Computed train embeddings
2019-03-13 19:34:45,258 : Computing embedding for test
2019-03-13 19:34:55,089 : Computed test embeddings
2019-03-13 19:34:55,110 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:34:59,037 : [('reg:1e-05', 70.58), ('reg:0.0001', 71.37), ('reg:0.001', 72.5), ('reg:0.01', 71.79)]
2019-03-13 19:34:59,038 : Cross-validation : best param found is reg = 0.001             with score 72.5
2019-03-13 19:34:59,038 : Evaluating...
2019-03-13 19:34:59,268 : Dev acc : 72.5 Test acc 72.23; Test F1 79.09 for MRPC.

2019-03-13 19:34:59,268 : ***** Transfer task : SICK-Entailment*****


2019-03-13 19:34:59,292 : loading BERT model bert-large-uncased
2019-03-13 19:34:59,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:34:59,312 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:34:59,312 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpix1satby
2019-03-13 19:35:06,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:35:12,044 : Computing embedding for train
2019-03-13 19:35:23,412 : Computed train embeddings
2019-03-13 19:35:23,412 : Computing embedding for dev
2019-03-13 19:35:24,968 : Computed dev embeddings
2019-03-13 19:35:24,968 : Computing embedding for test
2019-03-13 19:35:37,234 : Computed test embeddings
2019-03-13 19:35:37,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:35:38,753 : [('reg:1e-05', 78.0), ('reg:0.0001', 77.2), ('reg:0.001', 77.4), ('reg:0.01', 71.8)]
2019-03-13 19:35:38,753 : Validation : best param found is reg = 1e-05 with score             78.0
2019-03-13 19:35:38,753 : Evaluating...
2019-03-13 19:35:39,152 : 
Dev acc : 78.0 Test acc : 75.1 for                        SICK entailment

2019-03-13 19:35:39,153 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 19:35:39,180 : loading BERT model bert-large-uncased
2019-03-13 19:35:39,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:35:39,200 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:35:39,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdsmr36kh
2019-03-13 19:35:46,613 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:35:51,899 : Computing embedding for train
2019-03-13 19:36:03,303 : Computed train embeddings
2019-03-13 19:36:03,303 : Computing embedding for dev
2019-03-13 19:36:04,861 : Computed dev embeddings
2019-03-13 19:36:04,861 : Computing embedding for test
2019-03-13 19:36:17,115 : Computed test embeddings
2019-03-13 19:36:31,769 : Dev : Pearson 0.7644597377787808
2019-03-13 19:36:31,769 : Test : Pearson 0.784667153220663 Spearman 0.7221421298984706 MSE 0.39132625885103056                        for SICK Relatedness

2019-03-13 19:36:31,770 : 

***** Transfer task : STSBenchmark*****


2019-03-13 19:36:31,822 : loading BERT model bert-large-uncased
2019-03-13 19:36:31,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:36:31,884 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:36:31,884 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvon98cs6
2019-03-13 19:36:39,337 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:36:44,543 : Computing embedding for train
2019-03-13 19:37:03,278 : Computed train embeddings
2019-03-13 19:37:03,278 : Computing embedding for dev
2019-03-13 19:37:08,959 : Computed dev embeddings
2019-03-13 19:37:08,959 : Computing embedding for test
2019-03-13 19:37:13,603 : Computed test embeddings
2019-03-13 19:37:31,988 : Dev : Pearson 0.7048328970895658
2019-03-13 19:37:31,988 : Test : Pearson 0.6470187002848534 Spearman 0.6394949006980304 MSE 1.5065972582561518                        for SICK Relatedness

2019-03-13 19:37:31,988 : ***** Transfer task : SNLI Entailment*****


2019-03-13 19:37:36,911 : loading BERT model bert-large-uncased
2019-03-13 19:37:36,911 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:37:37,032 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:37:37,032 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe0qc6hp1
2019-03-13 19:37:44,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:37:50,266 : PROGRESS (encoding): 0.00%
2019-03-13 19:40:38,182 : PROGRESS (encoding): 14.56%
2019-03-13 19:43:51,044 : PROGRESS (encoding): 29.12%
2019-03-13 19:47:02,667 : PROGRESS (encoding): 43.69%
2019-03-13 19:50:26,898 : PROGRESS (encoding): 58.25%
2019-03-13 19:54:14,075 : PROGRESS (encoding): 72.81%
2019-03-13 19:58:00,336 : PROGRESS (encoding): 87.37%
2019-03-13 20:02:04,880 : PROGRESS (encoding): 0.00%
2019-03-13 20:02:35,593 : PROGRESS (encoding): 0.00%
2019-03-13 20:03:05,312 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:03:31,650 : [('reg:1e-09', 65.36)]
2019-03-13 20:03:31,650 : Validation : best param found is reg = 1e-09 with score             65.36
2019-03-13 20:03:31,650 : Evaluating...
2019-03-13 20:03:58,626 : Dev acc : 65.36 Test acc : 65.58 for SNLI

2019-03-13 20:03:58,626 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 20:03:58,837 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 20:03:59,931 : loading BERT model bert-large-uncased
2019-03-13 20:03:59,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:03:59,958 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:03:59,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpddz1dcb3
2019-03-13 20:04:07,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:04:12,878 : Computing embeddings for train/dev/test
2019-03-13 20:07:48,221 : Computed embeddings
2019-03-13 20:07:48,221 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:08:21,471 : [('reg:1e-05', 71.57), ('reg:0.0001', 67.31), ('reg:0.001', 60.84), ('reg:0.01', 48.21)]
2019-03-13 20:08:21,472 : Validation : best param found is reg = 1e-05 with score             71.57
2019-03-13 20:08:21,472 : Evaluating...
2019-03-13 20:08:31,353 : 
Dev acc : 71.6 Test acc : 69.8 for LENGTH classification

2019-03-13 20:08:31,354 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 20:08:31,720 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 20:08:31,765 : loading BERT model bert-large-uncased
2019-03-13 20:08:31,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:08:31,797 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:08:31,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl0mx50dt
2019-03-13 20:08:39,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:08:44,616 : Computing embeddings for train/dev/test
2019-03-13 20:12:02,716 : Computed embeddings
2019-03-13 20:12:02,716 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:12:32,492 : [('reg:1e-05', 45.98), ('reg:0.0001', 20.96), ('reg:0.001', 2.71), ('reg:0.01', 0.94)]
2019-03-13 20:12:32,493 : Validation : best param found is reg = 1e-05 with score             45.98
2019-03-13 20:12:32,493 : Evaluating...
2019-03-13 20:12:43,544 : 
Dev acc : 46.0 Test acc : 46.8 for WORDCONTENT classification

2019-03-13 20:12:43,545 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 20:12:43,897 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 20:12:43,963 : loading BERT model bert-large-uncased
2019-03-13 20:12:43,963 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:12:44,058 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:12:44,058 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf83t_a0j
2019-03-13 20:12:51,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:12:56,654 : Computing embeddings for train/dev/test
2019-03-13 20:16:03,767 : Computed embeddings
2019-03-13 20:16:03,767 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:16:25,878 : [('reg:1e-05', 32.3), ('reg:0.0001', 32.15), ('reg:0.001', 30.83), ('reg:0.01', 26.41)]
2019-03-13 20:16:25,878 : Validation : best param found is reg = 1e-05 with score             32.3
2019-03-13 20:16:25,878 : Evaluating...
2019-03-13 20:16:31,166 : 
Dev acc : 32.3 Test acc : 32.0 for DEPTH classification

2019-03-13 20:16:31,167 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 20:16:31,546 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 20:16:31,609 : loading BERT model bert-large-uncased
2019-03-13 20:16:31,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:16:31,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:16:31,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn720iaoy
2019-03-13 20:16:39,182 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:16:44,391 : Computing embeddings for train/dev/test
2019-03-13 20:19:36,502 : Computed embeddings
2019-03-13 20:19:36,503 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:20:04,430 : [('reg:1e-05', 65.47), ('reg:0.0001', 65.54), ('reg:0.001', 62.88), ('reg:0.01', 46.03)]
2019-03-13 20:20:04,430 : Validation : best param found is reg = 0.0001 with score             65.54
2019-03-13 20:20:04,430 : Evaluating...
2019-03-13 20:20:12,144 : 
Dev acc : 65.5 Test acc : 64.7 for TOPCONSTITUENTS classification

2019-03-13 20:20:12,145 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 20:20:12,515 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 20:20:12,581 : loading BERT model bert-large-uncased
2019-03-13 20:20:12,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:20:12,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:20:12,611 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5gg_c16e
2019-03-13 20:20:20,065 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:20:25,382 : Computing embeddings for train/dev/test
2019-03-13 20:23:31,497 : Computed embeddings
2019-03-13 20:23:31,497 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:23:58,371 : [('reg:1e-05', 91.14), ('reg:0.0001', 91.22), ('reg:0.001', 91.19), ('reg:0.01', 89.71)]
2019-03-13 20:23:58,371 : Validation : best param found is reg = 0.0001 with score             91.22
2019-03-13 20:23:58,371 : Evaluating...
2019-03-13 20:24:05,903 : 
Dev acc : 91.2 Test acc : 90.9 for BIGRAMSHIFT classification

2019-03-13 20:24:05,905 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 20:24:06,294 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 20:24:06,359 : loading BERT model bert-large-uncased
2019-03-13 20:24:06,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:24:06,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:24:06,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4egvg0z_
2019-03-13 20:24:13,857 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:24:19,109 : Computing embeddings for train/dev/test
2019-03-13 20:27:21,408 : Computed embeddings
2019-03-13 20:27:21,408 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:27:47,281 : [('reg:1e-05', 89.75), ('reg:0.0001', 89.81), ('reg:0.001', 90.37), ('reg:0.01', 90.74)]
2019-03-13 20:27:47,281 : Validation : best param found is reg = 0.01 with score             90.74
2019-03-13 20:27:47,281 : Evaluating...
2019-03-13 20:27:53,699 : 
Dev acc : 90.7 Test acc : 88.7 for TENSE classification

2019-03-13 20:27:53,700 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 20:27:54,129 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 20:27:54,194 : loading BERT model bert-large-uncased
2019-03-13 20:27:54,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:27:54,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:27:54,313 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp51ffun0p
2019-03-13 20:28:01,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:28:07,016 : Computing embeddings for train/dev/test
2019-03-13 20:31:20,463 : Computed embeddings
2019-03-13 20:31:20,463 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:31:49,419 : [('reg:1e-05', 83.81), ('reg:0.0001', 83.95), ('reg:0.001', 83.26), ('reg:0.01', 81.69)]
2019-03-13 20:31:49,419 : Validation : best param found is reg = 0.0001 with score             83.95
2019-03-13 20:31:49,419 : Evaluating...
2019-03-13 20:31:55,859 : 
Dev acc : 84.0 Test acc : 83.1 for SUBJNUMBER classification

2019-03-13 20:31:55,860 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 20:31:56,261 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 20:31:56,327 : loading BERT model bert-large-uncased
2019-03-13 20:31:56,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:31:56,441 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:31:56,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxbkv2j2o
2019-03-13 20:32:03,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:32:09,327 : Computing embeddings for train/dev/test
2019-03-13 20:35:20,575 : Computed embeddings
2019-03-13 20:35:20,575 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:35:45,457 : [('reg:1e-05', 79.54), ('reg:0.0001', 79.48), ('reg:0.001', 79.35), ('reg:0.01', 78.75)]
2019-03-13 20:35:45,458 : Validation : best param found is reg = 1e-05 with score             79.54
2019-03-13 20:35:45,458 : Evaluating...
2019-03-13 20:35:51,568 : 
Dev acc : 79.5 Test acc : 80.5 for OBJNUMBER classification

2019-03-13 20:35:51,569 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 20:35:52,130 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 20:35:52,199 : loading BERT model bert-large-uncased
2019-03-13 20:35:52,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:35:52,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:35:52,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmqpvkhwr
2019-03-13 20:35:59,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:36:04,957 : Computing embeddings for train/dev/test
2019-03-13 20:39:47,455 : Computed embeddings
2019-03-13 20:39:47,456 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:40:11,026 : [('reg:1e-05', 67.4), ('reg:0.0001', 67.34), ('reg:0.001', 67.29), ('reg:0.01', 66.78)]
2019-03-13 20:40:11,026 : Validation : best param found is reg = 1e-05 with score             67.4
2019-03-13 20:40:11,026 : Evaluating...
2019-03-13 20:40:17,152 : 
Dev acc : 67.4 Test acc : 67.0 for ODDMANOUT classification

2019-03-13 20:40:17,153 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 20:40:17,530 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 20:40:17,610 : loading BERT model bert-large-uncased
2019-03-13 20:40:17,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:40:17,639 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:40:17,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphficv8gr
2019-03-13 20:40:25,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:40:30,346 : Computing embeddings for train/dev/test
2019-03-13 20:44:10,727 : Computed embeddings
2019-03-13 20:44:10,727 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:44:37,769 : [('reg:1e-05', 71.89), ('reg:0.0001', 71.98), ('reg:0.001', 71.97), ('reg:0.01', 61.15)]
2019-03-13 20:44:37,770 : Validation : best param found is reg = 0.0001 with score             71.98
2019-03-13 20:44:37,770 : Evaluating...
2019-03-13 20:44:46,238 : 
Dev acc : 72.0 Test acc : 72.6 for COORDINATIONINVERSION classification

2019-03-13 20:44:46,240 : total results: {'STS12': {'MSRpar': {'pearson': (0.37166084344931977, 5.572372148193854e-26), 'spearman': SpearmanrResult(correlation=0.43429608839851264, pvalue=7.51401756173129e-36), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4331586722579445, 1.1868278122506968e-35), 'spearman': SpearmanrResult(correlation=0.476078733603418, pvalue=1.090604564216041e-43), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5132059838748462, 3.341451012798705e-32), 'spearman': SpearmanrResult(correlation=0.5585117190558412, pvalue=5.259367704742099e-39), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.49130861370309603, 7.941140401274823e-47), 'spearman': SpearmanrResult(correlation=0.5442772781081244, pvalue=4.623485637651986e-59), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6365094007432025, 1.0413644832803674e-46), 'spearman': SpearmanrResult(correlation=0.6232484441674865, pvalue=2.595749655297571e-44), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48916870280568175, 'wmean': 0.4702782801006635}, 'spearman': {'mean': 0.5272824526666766, 'wmean': 0.5135206188391246}}}, 'STS13': {'FNWN': {'pearson': (0.36593147143544236, 2.241819979636939e-07), 'spearman': SpearmanrResult(correlation=0.36868502558176947, pvalue=1.789023934738497e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.5762868094550438, 1.3917169487235909e-67), 'spearman': SpearmanrResult(correlation=0.596015229534916, pvalue=2.540382640683831e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.5132703118498283, 5.002316021182505e-39), 'spearman': SpearmanrResult(correlation=0.5320585099005146, pvalue=2.5301226646709307e-42), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4851628642467715, 'wmean': 0.5262138667602235}, 'spearman': {'mean': 0.4989195883390667, 'wmean': 0.5434518106935535}}}, 'STS14': {'deft-forum': {'pearson': (0.21581389565451597, 3.838796576538509e-06), 'spearman': SpearmanrResult(correlation=0.23932604281461647, pvalue=2.782691781881417e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.7412694447476901, 1.5727285346748892e-53), 'spearman': SpearmanrResult(correlation=0.701742614350011, pvalue=8.591861367301885e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.5329973874783531, 2.8482813355024023e-56), 'spearman': SpearmanrResult(correlation=0.5301378793674346, pvalue=1.3968851375071452e-55), 'nsamples': 750}, 'images': {'pearson': (0.5456884126548269, 2.0346066768646313e-59), 'spearman': SpearmanrResult(correlation=0.5252704733988495, pvalue=2.0210480568790258e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.6585266152533995, 1.9823381453219746e-94), 'spearman': SpearmanrResult(correlation=0.6875461985375855, pvalue=4.681808753776079e-106), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5732967614293207, 9.540553897963981e-67), 'spearman': SpearmanrResult(correlation=0.5437225570781722, pvalue=6.377391285130089e-59), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5445987528696844, 'wmean': 0.5473010584215372}, 'spearman': {'mean': 0.5379576275911114, 'wmean': 0.5421939559621632}}}, 'STS15': {'answers-forums': {'pearson': (0.6049304594229006, 8.49779985502474e-39), 'spearman': SpearmanrResult(correlation=0.5976014214760081, pvalue=1.1287867239388062e-37), 'nsamples': 375}, 'answers-students': {'pearson': (0.5411510373790641, 2.810168269660091e-58), 'spearman': SpearmanrResult(correlation=0.5770876966187566, pvalue=8.282384148986171e-68), 'nsamples': 750}, 'belief': {'pearson': (0.6435739266899996, 3.1086544477171633e-45), 'spearman': SpearmanrResult(correlation=0.6856678501146054, pvalue=2.159611394664927e-53), 'nsamples': 375}, 'headlines': {'pearson': (0.6208718436536435, 3.827437168331824e-81), 'spearman': SpearmanrResult(correlation=0.6367339167016204, pvalue=1.6290639664571726e-86), 'nsamples': 750}, 'images': {'pearson': (0.6415579218788396, 3.275731686025927e-88), 'spearman': SpearmanrResult(correlation=0.655594446015219, pvalue=2.5145489551254917e-93), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6104170378048894, 'wmean': 0.6069582489919992}, 'spearman': {'mean': 0.630537066185242, 'wmean': 0.6277626737827257}}}, 'STS16': {'answer-answer': {'pearson': (0.501297270386462, 1.440864554337338e-17), 'spearman': SpearmanrResult(correlation=0.5071147123600682, pvalue=5.2882078702949515e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6029175385768658, 4.89992620778903e-26), 'spearman': SpearmanrResult(correlation=0.65161354172453, pvalue=1.758277411141245e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7842242021070917, 3.701927893885837e-49), 'spearman': SpearmanrResult(correlation=0.7924751656418692, pvalue=7.278582927120275e-51), 'nsamples': 230}, 'postediting': {'pearson': (0.7727987160737627, 1.080986848118179e-49), 'spearman': SpearmanrResult(correlation=0.8050469206399297, pvalue=8.307136725766209e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.4380064552833331, 3.3039548979384366e-11), 'spearman': SpearmanrResult(correlation=0.4324964954021551, pvalue=6.169770941407446e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.619848836485503, 'wmean': 0.622203858469348}, 'spearman': {'mean': 0.6377493671537104, 'wmean': 0.6409371105410527}}}, 'MR': {'devacc': 84.21, 'acc': 83.05, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.81, 'acc': 89.61, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.45, 'acc': 85.04, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.94, 'acc': 95.76, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 88.76, 'acc': 87.75, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 46.59, 'acc': 47.19, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.86, 'acc': 91.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.5, 'acc': 72.23, 'f1': 79.09, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 75.1, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7644597377787808, 'pearson': 0.784667153220663, 'spearman': 0.7221421298984706, 'mse': 0.39132625885103056, 'yhat': array([3.2827879 , 4.64254399, 2.36089989, ..., 3.04233202, 4.60477184,        4.4339797 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7048328970895658, 'pearson': 0.6470187002848534, 'spearman': 0.6394949006980304, 'mse': 1.5065972582561518, 'yhat': array([1.52695262, 1.18454991, 3.53945854, ..., 3.7047078 , 3.56591163,        3.33650918]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.36, 'acc': 65.58, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 71.57, 'acc': 69.84, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 45.98, 'acc': 46.78, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.3, 'acc': 32.0, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 65.54, 'acc': 64.71, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.22, 'acc': 90.9, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.74, 'acc': 88.7, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.95, 'acc': 83.13, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.54, 'acc': 80.51, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.4, 'acc': 67.03, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.98, 'acc': 72.59, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 20:44:46,240 : STS12 p=0.4703, STS12 s=0.5135, STS13 p=0.5262, STS13 s=0.5435, STS14 p=0.5473, STS14 s=0.5422, STS15 p=0.6070, STS15 s=0.6278, STS 16 p=0.6222, STS16 s=0.6409, STS B p=0.6470, STS B s=0.6395, STS B m=1.5066, SICK-R p=0.7847, SICK-R s=0.7221, SICK-P m=0.3913
2019-03-13 20:44:46,240 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 20:44:46,240 : 0.4703,0.5135,0.5262,0.5435,0.5473,0.5422,0.6070,0.6278,0.6222,0.6409,0.6470,0.6395,1.5066,0.7847,0.7221,0.3913
2019-03-13 20:44:46,240 : MR=83.05, CR=89.61, SUBJ=95.76, MPQA=85.04, SST-B=87.75, SST-F=47.19, TREC=91.80, SICK-E=75.10, SNLI=65.58, MRPC=72.23, MRPC f=79.09
2019-03-13 20:44:46,240 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 20:44:46,240 : 83.05,89.61,95.76,85.04,87.75,47.19,91.80,75.10,65.58,72.23,79.09
2019-03-13 20:44:46,240 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 20:44:46,240 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 20:44:46,240 : na,na,na,na,na,na,na,na,na,na
2019-03-13 20:44:46,240 : SentLen=69.84, WC=46.78, TreeDepth=32.00, TopConst=64.71, BShift=90.90, Tense=88.70, SubjNum=83.13, ObjNum=80.51, SOMO=67.03, CoordInv=72.59, average=69.62
2019-03-13 20:44:46,240 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 20:44:46,240 : 69.84,46.78,32.00,64.71,90.90,88.70,83.13,80.51,67.03,72.59,69.62
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,240 : layer 23
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,240 : ********************************************************************************
2019-03-13 20:44:46,332 : ***** Transfer task : STS12 *****


2019-03-13 20:44:46,345 : loading BERT model bert-large-uncased
2019-03-13 20:44:46,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:44:46,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:44:46,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj5m5jkb4
2019-03-13 20:44:53,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:45:03,064 : MSRpar : pearson = 0.3458, spearman = 0.4227
2019-03-13 20:45:04,723 : MSRvid : pearson = 0.4487, spearman = 0.5138
2019-03-13 20:45:06,148 : SMTeuroparl : pearson = 0.4588, spearman = 0.5075
2019-03-13 20:45:08,871 : surprise.OnWN : pearson = 0.4351, spearman = 0.5248
2019-03-13 20:45:10,313 : surprise.SMTnews : pearson = 0.6013, spearman = 0.6209
2019-03-13 20:45:10,313 : ALL (weighted average) : Pearson = 0.4417,             Spearman = 0.5073
2019-03-13 20:45:10,313 : ALL (average) : Pearson = 0.4580,             Spearman = 0.5179

2019-03-13 20:45:10,313 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 20:45:10,321 : loading BERT model bert-large-uncased
2019-03-13 20:45:10,322 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:45:10,339 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:45:10,339 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpml2mg6ed
2019-03-13 20:45:17,747 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:45:24,289 : FNWN : pearson = 0.3360, spearman = 0.3411
2019-03-13 20:45:26,200 : headlines : pearson = 0.5366, spearman = 0.6024
2019-03-13 20:45:27,685 : OnWN : pearson = 0.4756, spearman = 0.5227
2019-03-13 20:45:27,685 : ALL (weighted average) : Pearson = 0.4885,             Spearman = 0.5397
2019-03-13 20:45:27,685 : ALL (average) : Pearson = 0.4494,             Spearman = 0.4887

2019-03-13 20:45:27,685 : ***** Transfer task : STS14 *****


2019-03-13 20:45:27,700 : loading BERT model bert-large-uncased
2019-03-13 20:45:27,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:45:27,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:45:27,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz6aff2vr
2019-03-13 20:45:35,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:45:41,846 : deft-forum : pearson = 0.1758, spearman = 0.2298
2019-03-13 20:45:43,501 : deft-news : pearson = 0.7189, spearman = 0.6982
2019-03-13 20:45:45,694 : headlines : pearson = 0.4726, spearman = 0.5352
2019-03-13 20:45:47,793 : images : pearson = 0.5163, spearman = 0.5397
2019-03-13 20:45:49,946 : OnWN : pearson = 0.5906, spearman = 0.6703
2019-03-13 20:45:52,840 : tweet-news : pearson = 0.5108, spearman = 0.5133
2019-03-13 20:45:52,840 : ALL (weighted average) : Pearson = 0.4967,             Spearman = 0.5351
2019-03-13 20:45:52,840 : ALL (average) : Pearson = 0.4975,             Spearman = 0.5311

2019-03-13 20:45:52,841 : ***** Transfer task : STS15 *****


2019-03-13 20:45:52,890 : loading BERT model bert-large-uncased
2019-03-13 20:45:52,890 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:45:52,910 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:45:52,910 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7j1clorf
2019-03-13 20:46:00,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:46:07,445 : answers-forums : pearson = 0.5642, spearman = 0.5579
2019-03-13 20:46:09,555 : answers-students : pearson = 0.4430, spearman = 0.5119
2019-03-13 20:46:11,626 : belief : pearson = 0.5885, spearman = 0.6485
2019-03-13 20:46:13,898 : headlines : pearson = 0.5855, spearman = 0.6354
2019-03-13 20:46:16,051 : images : pearson = 0.6488, spearman = 0.6657
2019-03-13 20:46:16,052 : ALL (weighted average) : Pearson = 0.5634,             Spearman = 0.6041
2019-03-13 20:46:16,052 : ALL (average) : Pearson = 0.5660,             Spearman = 0.6039

2019-03-13 20:46:16,052 : ***** Transfer task : STS16 *****


2019-03-13 20:46:16,121 : loading BERT model bert-large-uncased
2019-03-13 20:46:16,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:46:16,139 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:46:16,139 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjvyc5540
2019-03-13 20:46:23,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:46:29,695 : answer-answer : pearson = 0.4444, spearman = 0.4682
2019-03-13 20:46:30,363 : headlines : pearson = 0.5514, spearman = 0.6401
2019-03-13 20:46:31,254 : plagiarism : pearson = 0.7675, spearman = 0.7774
2019-03-13 20:46:32,762 : postediting : pearson = 0.7508, spearman = 0.7959
2019-03-13 20:46:33,372 : question-question : pearson = 0.4700, spearman = 0.4658
2019-03-13 20:46:33,373 : ALL (weighted average) : Pearson = 0.5971,             Spearman = 0.6313
2019-03-13 20:46:33,373 : ALL (average) : Pearson = 0.5968,             Spearman = 0.6295

2019-03-13 20:46:33,373 : ***** Transfer task : MR *****


2019-03-13 20:46:33,388 : loading BERT model bert-large-uncased
2019-03-13 20:46:33,388 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:46:33,408 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:46:33,408 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphw4bbang
2019-03-13 20:46:40,886 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:46:46,055 : Generating sentence embeddings
2019-03-13 20:47:17,851 : Generated sentence embeddings
2019-03-13 20:47:17,852 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:47:28,418 : Best param found at split 1: l2reg = 0.01                 with score 84.43
2019-03-13 20:47:39,314 : Best param found at split 2: l2reg = 1e-05                 with score 83.68
2019-03-13 20:47:49,429 : Best param found at split 3: l2reg = 0.001                 with score 84.28
2019-03-13 20:48:00,517 : Best param found at split 4: l2reg = 0.01                 with score 83.77
2019-03-13 20:48:10,995 : Best param found at split 5: l2reg = 1e-05                 with score 83.86
2019-03-13 20:48:11,434 : Dev acc : 84.0 Test acc : 82.52

2019-03-13 20:48:11,435 : ***** Transfer task : CR *****


2019-03-13 20:48:11,442 : loading BERT model bert-large-uncased
2019-03-13 20:48:11,443 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:48:11,463 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:48:11,463 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4k5gliwe
2019-03-13 20:48:18,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:48:24,112 : Generating sentence embeddings
2019-03-13 20:48:32,506 : Generated sentence embeddings
2019-03-13 20:48:32,506 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:48:35,964 : Best param found at split 1: l2reg = 0.0001                 with score 89.8
2019-03-13 20:48:39,618 : Best param found at split 2: l2reg = 0.01                 with score 89.43
2019-03-13 20:48:43,740 : Best param found at split 3: l2reg = 0.0001                 with score 89.97
2019-03-13 20:48:47,220 : Best param found at split 4: l2reg = 0.001                 with score 89.47
2019-03-13 20:48:50,907 : Best param found at split 5: l2reg = 1e-05                 with score 90.0
2019-03-13 20:48:51,096 : Dev acc : 89.73 Test acc : 88.13

2019-03-13 20:48:51,096 : ***** Transfer task : MPQA *****


2019-03-13 20:48:51,102 : loading BERT model bert-large-uncased
2019-03-13 20:48:51,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:48:51,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:48:51,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprpfmz24l
2019-03-13 20:48:58,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:49:03,785 : Generating sentence embeddings
2019-03-13 20:49:11,433 : Generated sentence embeddings
2019-03-13 20:49:11,434 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:49:20,822 : Best param found at split 1: l2reg = 0.001                 with score 81.81
2019-03-13 20:49:31,495 : Best param found at split 2: l2reg = 0.0001                 with score 85.62
2019-03-13 20:49:41,688 : Best param found at split 3: l2reg = 0.001                 with score 83.95
2019-03-13 20:49:52,943 : Best param found at split 4: l2reg = 0.0001                 with score 84.96
2019-03-13 20:50:01,342 : Best param found at split 5: l2reg = 0.01                 with score 83.46
2019-03-13 20:50:01,792 : Dev acc : 83.96 Test acc : 84.53

2019-03-13 20:50:01,793 : ***** Transfer task : SUBJ *****


2019-03-13 20:50:01,809 : loading BERT model bert-large-uncased
2019-03-13 20:50:01,809 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:50:01,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:50:01,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6xv4wr5o
2019-03-13 20:50:09,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:50:14,768 : Generating sentence embeddings
2019-03-13 20:50:45,873 : Generated sentence embeddings
2019-03-13 20:50:45,873 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:50:56,177 : Best param found at split 1: l2reg = 0.001                 with score 96.02
2019-03-13 20:51:04,793 : Best param found at split 2: l2reg = 0.0001                 with score 95.9
2019-03-13 20:51:12,324 : Best param found at split 3: l2reg = 0.001                 with score 95.74
2019-03-13 20:51:21,851 : Best param found at split 4: l2reg = 0.001                 with score 96.1
2019-03-13 20:51:31,670 : Best param found at split 5: l2reg = 0.001                 with score 95.88
2019-03-13 20:51:32,142 : Dev acc : 95.93 Test acc : 95.43

2019-03-13 20:51:32,143 : ***** Transfer task : SST Binary classification *****


2019-03-13 20:51:32,235 : loading BERT model bert-large-uncased
2019-03-13 20:51:32,235 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:51:32,310 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:51:32,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3qd6tr4f
2019-03-13 20:51:39,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:51:44,912 : Computing embedding for train
2019-03-13 20:53:26,033 : Computed train embeddings
2019-03-13 20:53:26,033 : Computing embedding for dev
2019-03-13 20:53:28,242 : Computed dev embeddings
2019-03-13 20:53:28,242 : Computing embedding for test
2019-03-13 20:53:32,884 : Computed test embeddings
2019-03-13 20:53:32,884 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:53:48,358 : [('reg:1e-05', 87.39), ('reg:0.0001', 87.5), ('reg:0.001', 87.73), ('reg:0.01', 86.7)]
2019-03-13 20:53:48,358 : Validation : best param found is reg = 0.001 with score             87.73
2019-03-13 20:53:48,358 : Evaluating...
2019-03-13 20:53:51,968 : 
Dev acc : 87.73 Test acc : 88.19 for             SST Binary classification

2019-03-13 20:53:51,968 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 20:53:52,023 : loading BERT model bert-large-uncased
2019-03-13 20:53:52,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:53:52,044 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:53:52,044 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpitkproip
2019-03-13 20:53:59,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:54:04,790 : Computing embedding for train
2019-03-13 20:54:26,894 : Computed train embeddings
2019-03-13 20:54:26,894 : Computing embedding for dev
2019-03-13 20:54:29,786 : Computed dev embeddings
2019-03-13 20:54:29,786 : Computing embedding for test
2019-03-13 20:54:35,489 : Computed test embeddings
2019-03-13 20:54:35,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:54:37,838 : [('reg:1e-05', 47.23), ('reg:0.0001', 47.14), ('reg:0.001', 46.5), ('reg:0.01', 44.41)]
2019-03-13 20:54:37,838 : Validation : best param found is reg = 1e-05 with score             47.23
2019-03-13 20:54:37,838 : Evaluating...
2019-03-13 20:54:38,383 : 
Dev acc : 47.23 Test acc : 49.05 for             SST Fine-Grained classification

2019-03-13 20:54:38,383 : ***** Transfer task : TREC *****


2019-03-13 20:54:38,396 : loading BERT model bert-large-uncased
2019-03-13 20:54:38,396 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:54:38,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:54:38,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuxcvh1uj
2019-03-13 20:54:45,840 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:54:58,707 : Computed train embeddings
2019-03-13 20:54:59,300 : Computed test embeddings
2019-03-13 20:54:59,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 20:55:04,679 : [('reg:1e-05', 83.22), ('reg:0.0001', 83.03), ('reg:0.001', 82.26), ('reg:0.01', 76.71)]
2019-03-13 20:55:04,679 : Cross-validation : best param found is reg = 1e-05             with score 83.22
2019-03-13 20:55:04,679 : Evaluating...
2019-03-13 20:55:04,998 : 
Dev acc : 83.22 Test acc : 92.4             for TREC

2019-03-13 20:55:04,999 : ***** Transfer task : MRPC *****


2019-03-13 20:55:05,019 : loading BERT model bert-large-uncased
2019-03-13 20:55:05,019 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:55:05,041 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:55:05,041 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3yqcpius
2019-03-13 20:55:12,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:55:17,798 : Computing embedding for train
2019-03-13 20:55:40,249 : Computed train embeddings
2019-03-13 20:55:40,249 : Computing embedding for test
2019-03-13 20:55:50,101 : Computed test embeddings
2019-03-13 20:55:50,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 20:55:53,959 : [('reg:1e-05', 71.39), ('reg:0.0001', 71.86), ('reg:0.001', 71.32), ('reg:0.01', 70.76)]
2019-03-13 20:55:53,959 : Cross-validation : best param found is reg = 0.0001             with score 71.86
2019-03-13 20:55:53,959 : Evaluating...
2019-03-13 20:55:54,220 : Dev acc : 71.86 Test acc 73.33; Test F1 82.33 for MRPC.

2019-03-13 20:55:54,220 : ***** Transfer task : SICK-Entailment*****


2019-03-13 20:55:54,286 : loading BERT model bert-large-uncased
2019-03-13 20:55:54,286 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:55:54,306 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:55:54,307 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkm_kpmug
2019-03-13 20:56:01,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:56:07,088 : Computing embedding for train
2019-03-13 20:56:18,496 : Computed train embeddings
2019-03-13 20:56:18,496 : Computing embedding for dev
2019-03-13 20:56:20,047 : Computed dev embeddings
2019-03-13 20:56:20,048 : Computing embedding for test
2019-03-13 20:56:32,271 : Computed test embeddings
2019-03-13 20:56:32,307 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:56:33,415 : [('reg:1e-05', 73.8), ('reg:0.0001', 75.2), ('reg:0.001', 76.4), ('reg:0.01', 74.0)]
2019-03-13 20:56:33,415 : Validation : best param found is reg = 0.001 with score             76.4
2019-03-13 20:56:33,415 : Evaluating...
2019-03-13 20:56:33,708 : 
Dev acc : 76.4 Test acc : 74.37 for                        SICK entailment

2019-03-13 20:56:33,708 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 20:56:33,735 : loading BERT model bert-large-uncased
2019-03-13 20:56:33,735 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:56:33,791 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:56:33,791 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwrj32mej
2019-03-13 20:56:41,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:56:46,627 : Computing embedding for train
2019-03-13 20:56:58,003 : Computed train embeddings
2019-03-13 20:56:58,004 : Computing embedding for dev
2019-03-13 20:56:59,559 : Computed dev embeddings
2019-03-13 20:56:59,559 : Computing embedding for test
2019-03-13 20:57:11,778 : Computed test embeddings
2019-03-13 20:57:29,624 : Dev : Pearson 0.7644946292533992
2019-03-13 20:57:29,624 : Test : Pearson 0.7831249018747991 Spearman 0.7159012893490161 MSE 0.3985220171592307                        for SICK Relatedness

2019-03-13 20:57:29,625 : 

***** Transfer task : STSBenchmark*****


2019-03-13 20:57:29,670 : loading BERT model bert-large-uncased
2019-03-13 20:57:29,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:57:29,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:57:29,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj2qjyzix
2019-03-13 20:57:37,144 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:57:42,628 : Computing embedding for train
2019-03-13 20:58:01,350 : Computed train embeddings
2019-03-13 20:58:01,350 : Computing embedding for dev
2019-03-13 20:58:07,025 : Computed dev embeddings
2019-03-13 20:58:07,025 : Computing embedding for test
2019-03-13 20:58:11,681 : Computed test embeddings
2019-03-13 20:58:30,329 : Dev : Pearson 0.7137797973383886
2019-03-13 20:58:30,329 : Test : Pearson 0.6478599459659695 Spearman 0.6444843744758815 MSE 1.5073912679189432                        for SICK Relatedness

2019-03-13 20:58:30,329 : ***** Transfer task : SNLI Entailment*****


2019-03-13 20:58:34,991 : loading BERT model bert-large-uncased
2019-03-13 20:58:34,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:58:35,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:58:35,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp98b6g3ub
2019-03-13 20:58:43,435 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:58:49,276 : PROGRESS (encoding): 0.00%
2019-03-13 21:01:37,171 : PROGRESS (encoding): 14.56%
2019-03-13 21:04:50,446 : PROGRESS (encoding): 29.12%
2019-03-13 21:08:02,439 : PROGRESS (encoding): 43.69%
2019-03-13 21:11:26,270 : PROGRESS (encoding): 58.25%
2019-03-13 21:15:13,438 : PROGRESS (encoding): 72.81%
2019-03-13 21:18:59,414 : PROGRESS (encoding): 87.37%
2019-03-13 21:23:03,988 : PROGRESS (encoding): 0.00%
2019-03-13 21:23:34,777 : PROGRESS (encoding): 0.00%
2019-03-13 21:24:04,381 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:24:41,824 : [('reg:1e-09', 68.48)]
2019-03-13 21:24:41,824 : Validation : best param found is reg = 1e-09 with score             68.48
2019-03-13 21:24:41,824 : Evaluating...
2019-03-13 21:25:19,033 : Dev acc : 68.48 Test acc : 68.95 for SNLI

2019-03-13 21:25:19,033 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 21:25:19,239 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 21:25:20,228 : loading BERT model bert-large-uncased
2019-03-13 21:25:20,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:25:20,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:25:20,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9uyuh7t7
2019-03-13 21:25:27,727 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:25:33,044 : Computing embeddings for train/dev/test
2019-03-13 21:29:07,032 : Computed embeddings
2019-03-13 21:29:07,032 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:29:46,040 : [('reg:1e-05', 68.57), ('reg:0.0001', 66.22), ('reg:0.001', 58.01), ('reg:0.01', 49.13)]
2019-03-13 21:29:46,040 : Validation : best param found is reg = 1e-05 with score             68.57
2019-03-13 21:29:46,040 : Evaluating...
2019-03-13 21:29:53,620 : 
Dev acc : 68.6 Test acc : 68.0 for LENGTH classification

2019-03-13 21:29:53,620 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 21:29:53,986 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 21:29:54,031 : loading BERT model bert-large-uncased
2019-03-13 21:29:54,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:29:54,059 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:29:54,059 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp75wgshz_
2019-03-13 21:30:01,525 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:30:06,757 : Computing embeddings for train/dev/test
2019-03-13 21:33:24,445 : Computed embeddings
2019-03-13 21:33:24,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:33:55,974 : [('reg:1e-05', 42.66), ('reg:0.0001', 17.21), ('reg:0.001', 2.66), ('reg:0.01', 0.87)]
2019-03-13 21:33:55,974 : Validation : best param found is reg = 1e-05 with score             42.66
2019-03-13 21:33:55,974 : Evaluating...
2019-03-13 21:34:04,082 : 
Dev acc : 42.7 Test acc : 43.2 for WORDCONTENT classification

2019-03-13 21:34:04,083 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 21:34:04,447 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 21:34:04,515 : loading BERT model bert-large-uncased
2019-03-13 21:34:04,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:34:04,541 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:34:04,541 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpujyd_pg9
2019-03-13 21:34:11,944 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:34:17,263 : Computing embeddings for train/dev/test
2019-03-13 21:37:23,768 : Computed embeddings
2019-03-13 21:37:23,768 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:37:44,465 : [('reg:1e-05', 31.51), ('reg:0.0001', 31.36), ('reg:0.001', 30.13), ('reg:0.01', 26.05)]
2019-03-13 21:37:44,465 : Validation : best param found is reg = 1e-05 with score             31.51
2019-03-13 21:37:44,465 : Evaluating...
2019-03-13 21:37:49,823 : 
Dev acc : 31.5 Test acc : 30.8 for DEPTH classification

2019-03-13 21:37:49,824 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 21:37:50,211 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 21:37:50,275 : loading BERT model bert-large-uncased
2019-03-13 21:37:50,276 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:37:50,386 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:37:50,386 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsz6wn71s
2019-03-13 21:37:58,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:38:03,371 : Computing embeddings for train/dev/test
2019-03-13 21:40:54,801 : Computed embeddings
2019-03-13 21:40:54,801 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:41:23,706 : [('reg:1e-05', 63.37), ('reg:0.0001', 63.76), ('reg:0.001', 58.87), ('reg:0.01', 40.14)]
2019-03-13 21:41:23,707 : Validation : best param found is reg = 0.0001 with score             63.76
2019-03-13 21:41:23,707 : Evaluating...
2019-03-13 21:41:29,273 : 
Dev acc : 63.8 Test acc : 63.5 for TOPCONSTITUENTS classification

2019-03-13 21:41:29,275 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 21:41:29,620 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 21:41:29,688 : loading BERT model bert-large-uncased
2019-03-13 21:41:29,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:41:29,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:41:29,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph6vap05n
2019-03-13 21:41:37,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:41:42,645 : Computing embeddings for train/dev/test
2019-03-13 21:44:48,410 : Computed embeddings
2019-03-13 21:44:48,410 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:45:23,034 : [('reg:1e-05', 90.74), ('reg:0.0001', 90.59), ('reg:0.001', 90.53), ('reg:0.01', 89.03)]
2019-03-13 21:45:23,035 : Validation : best param found is reg = 1e-05 with score             90.74
2019-03-13 21:45:23,035 : Evaluating...
2019-03-13 21:45:31,804 : 
Dev acc : 90.7 Test acc : 90.2 for BIGRAMSHIFT classification

2019-03-13 21:45:31,805 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 21:45:32,212 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 21:45:32,277 : loading BERT model bert-large-uncased
2019-03-13 21:45:32,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:45:32,306 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:45:32,307 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7uy_nvl3
2019-03-13 21:45:39,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:45:45,075 : Computing embeddings for train/dev/test
2019-03-13 21:48:46,984 : Computed embeddings
2019-03-13 21:48:46,985 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:49:10,942 : [('reg:1e-05', 89.56), ('reg:0.0001', 89.64), ('reg:0.001', 90.08), ('reg:0.01', 90.46)]
2019-03-13 21:49:10,942 : Validation : best param found is reg = 0.01 with score             90.46
2019-03-13 21:49:10,943 : Evaluating...
2019-03-13 21:49:17,519 : 
Dev acc : 90.5 Test acc : 88.7 for TENSE classification

2019-03-13 21:49:17,520 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 21:49:17,976 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 21:49:18,046 : loading BERT model bert-large-uncased
2019-03-13 21:49:18,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:49:18,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:49:18,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq_7yttt0
2019-03-13 21:49:25,527 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:49:30,808 : Computing embeddings for train/dev/test
2019-03-13 21:52:44,385 : Computed embeddings
2019-03-13 21:52:44,385 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:53:10,160 : [('reg:1e-05', 82.67), ('reg:0.0001', 82.65), ('reg:0.001', 82.72), ('reg:0.01', 82.11)]
2019-03-13 21:53:10,160 : Validation : best param found is reg = 0.001 with score             82.72
2019-03-13 21:53:10,161 : Evaluating...
2019-03-13 21:53:17,780 : 
Dev acc : 82.7 Test acc : 82.4 for SUBJNUMBER classification

2019-03-13 21:53:17,781 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 21:53:18,182 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 21:53:18,248 : loading BERT model bert-large-uncased
2019-03-13 21:53:18,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:53:18,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:53:18,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi2e8sr70
2019-03-13 21:53:25,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:53:31,126 : Computing embeddings for train/dev/test
2019-03-13 21:56:42,325 : Computed embeddings
2019-03-13 21:56:42,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:57:05,381 : [('reg:1e-05', 79.56), ('reg:0.0001', 79.59), ('reg:0.001', 79.82), ('reg:0.01', 78.76)]
2019-03-13 21:57:05,381 : Validation : best param found is reg = 0.001 with score             79.82
2019-03-13 21:57:05,381 : Evaluating...
2019-03-13 21:57:10,794 : 
Dev acc : 79.8 Test acc : 80.6 for OBJNUMBER classification

2019-03-13 21:57:10,795 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 21:57:11,181 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 21:57:11,251 : loading BERT model bert-large-uncased
2019-03-13 21:57:11,252 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:57:11,377 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:57:11,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwgtp0olk
2019-03-13 21:57:18,808 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:57:24,053 : Computing embeddings for train/dev/test
2019-03-13 22:01:06,230 : Computed embeddings
2019-03-13 22:01:06,231 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:01:30,426 : [('reg:1e-05', 65.88), ('reg:0.0001', 65.93), ('reg:0.001', 65.8), ('reg:0.01', 67.04)]
2019-03-13 22:01:30,426 : Validation : best param found is reg = 0.01 with score             67.04
2019-03-13 22:01:30,426 : Evaluating...
2019-03-13 22:01:37,345 : 
Dev acc : 67.0 Test acc : 67.0 for ODDMANOUT classification

2019-03-13 22:01:37,346 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 22:01:37,939 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 22:01:38,015 : loading BERT model bert-large-uncased
2019-03-13 22:01:38,015 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:01:38,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:01:38,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfycpb52i
2019-03-13 22:01:45,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:01:50,760 : Computing embeddings for train/dev/test
2019-03-13 22:05:30,981 : Computed embeddings
2019-03-13 22:05:30,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:05:58,953 : [('reg:1e-05', 71.52), ('reg:0.0001', 71.42), ('reg:0.001', 67.77), ('reg:0.01', 59.95)]
2019-03-13 22:05:58,954 : Validation : best param found is reg = 1e-05 with score             71.52
2019-03-13 22:05:58,954 : Evaluating...
2019-03-13 22:06:06,375 : 
Dev acc : 71.5 Test acc : 71.3 for COORDINATIONINVERSION classification

2019-03-13 22:06:06,377 : total results: {'STS12': {'MSRpar': {'pearson': (0.3458241588788612, 1.7108061543053073e-22), 'spearman': SpearmanrResult(correlation=0.4227143708326042, pvalue=7.283511624375254e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4487430817181869, 1.939855489702924e-38), 'spearman': SpearmanrResult(correlation=0.513833512477641, pvalue=9.104043205183888e-52), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.45884444990276985, 2.7974202451389053e-25), 'spearman': SpearmanrResult(correlation=0.5074511979538768, pvalue=2.074464504100982e-31), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4351328156954107, 5.36233678015509e-36), 'spearman': SpearmanrResult(correlation=0.524756126096267, pvalue=2.673633258321853e-54), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6013091955780376, 1.3673776823413113e-40), 'spearman': SpearmanrResult(correlation=0.6208614401391325, pvalue=6.815721718753493e-44), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.45797074035465324, 'wmean': 0.44170109837849186}, 'spearman': {'mean': 0.5179233294999043, 'wmean': 0.5072785783562508}}}, 'STS13': {'FNWN': {'pearson': (0.3360063900670254, 2.2804446505934264e-06), 'spearman': SpearmanrResult(correlation=0.34105567676997833, pvalue=1.5676707591005347e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.536637789407867, 3.679562006271558e-57), 'spearman': SpearmanrResult(correlation=0.6024255189295855, pvalue=2.840313270034237e-75), 'nsamples': 750}, 'OnWN': {'pearson': (0.47556786714282956, 5.314577136023875e-33), 'spearman': SpearmanrResult(correlation=0.5227431139082911, pvalue=1.1563902052994783e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.44940401553924064, 'wmean': 0.488518082163797}, 'spearman': {'mean': 0.48874143653595165, 'wmean': 0.5396916993395109}}}, 'STS14': {'deft-forum': {'pearson': (0.17578177279804288, 0.00017843183094892553), 'spearman': SpearmanrResult(correlation=0.22983204297496307, pvalue=8.30603177441043e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.718908652386414, 5.4625048339229716e-49), 'spearman': SpearmanrResult(correlation=0.6981531796163251, pvalue=3.74769495428546e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.47258240668635565, 5.4446998154322975e-43), 'spearman': SpearmanrResult(correlation=0.5351953769603115, pvalue=8.303318452234566e-57), 'nsamples': 750}, 'images': {'pearson': (0.5163470919852374, 2.4246619258591633e-52), 'spearman': SpearmanrResult(correlation=0.5396626635213443, pvalue=6.59157264473921e-58), 'nsamples': 750}, 'OnWN': {'pearson': (0.590642457787327, 1.0163248088864783e-71), 'spearman': SpearmanrResult(correlation=0.6703458554802871, pvalue=5.264141506688197e-99), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5107689486125134, 4.5009701210547543e-51), 'spearman': SpearmanrResult(correlation=0.5132920425018458, pvalue=1.2088783465915567e-51), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.49750522170931505, 'wmean': 0.4966746859409649}, 'spearman': {'mean': 0.5310801935091795, 'wmean': 0.5351312872190593}}}, 'STS15': {'answers-forums': {'pearson': (0.5641611645262875, 6.756623092148642e-33), 'spearman': SpearmanrResult(correlation=0.5579188500978262, pvalue=4.596542231928313e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.4430167826810861, 2.130737047641342e-37), 'spearman': SpearmanrResult(correlation=0.5119455867360585, pvalue=2.4414776673995767e-51), 'nsamples': 750}, 'belief': {'pearson': (0.5884699332067376, 2.5845623787931156e-36), 'spearman': SpearmanrResult(correlation=0.6485060131618569, pvalue=4.0121062900673434e-46), 'nsamples': 375}, 'headlines': {'pearson': (0.5855134561561106, 3.225981480037665e-70), 'spearman': SpearmanrResult(correlation=0.6354134229959678, pvalue=4.689169652881143e-86), 'nsamples': 750}, 'images': {'pearson': (0.6487804775729488, 8.265488343141833e-91), 'spearman': SpearmanrResult(correlation=0.665690209146647, pvalue=3.5387696461793865e-97), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5659883628286341, 'wmean': 0.5634065663191645}, 'spearman': {'mean': 0.6038948164276713, 'wmean': 0.6040654126271288}}}, 'STS16': {'answer-answer': {'pearson': (0.4443820064160053, 1.0188605663211797e-13), 'spearman': SpearmanrResult(correlation=0.4682176190323071, pvalue=3.0376755288417807e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.551386347468026, 3.310992167806207e-21), 'spearman': SpearmanrResult(correlation=0.6400986036088536, pvalue=4.183439778085005e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7675040450636386, 6.405476785405133e-46), 'spearman': SpearmanrResult(correlation=0.7774176259912141, pvalue=8.328435587035285e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.7508374830892999, 1.73710097617267e-45), 'spearman': SpearmanrResult(correlation=0.7959472164088903, pvalue=1.1377398889147871e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.4700003482480425, 6.984968215356058e-13), 'spearman': SpearmanrResult(correlation=0.4658053126047003, pvalue=1.1852886749544497e-12), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5968220460570024, 'wmean': 0.5970730009877494}, 'spearman': {'mean': 0.6294972755291931, 'wmean': 0.6312667054375559}}}, 'MR': {'devacc': 84.0, 'acc': 82.52, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.73, 'acc': 88.13, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.96, 'acc': 84.53, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.93, 'acc': 95.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.73, 'acc': 88.19, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 47.23, 'acc': 49.05, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.22, 'acc': 92.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.86, 'acc': 73.33, 'f1': 82.33, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 74.37, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7644946292533992, 'pearson': 0.7831249018747991, 'spearman': 0.7159012893490161, 'mse': 0.3985220171592307, 'yhat': array([3.23380223, 4.14615487, 1.90570342, ..., 3.05599472, 4.59383917,        4.637386  ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7137797973383886, 'pearson': 0.6478599459659695, 'spearman': 0.6444843744758815, 'mse': 1.5073912679189432, 'yhat': array([1.63096599, 1.01579503, 3.13249268, ..., 3.70238071, 3.65628676,        3.28466292]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.48, 'acc': 68.95, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 68.57, 'acc': 67.99, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 42.66, 'acc': 43.25, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.51, 'acc': 30.78, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.76, 'acc': 63.55, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.74, 'acc': 90.25, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.46, 'acc': 88.65, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.72, 'acc': 82.39, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.82, 'acc': 80.59, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.04, 'acc': 66.98, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.52, 'acc': 71.28, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 22:06:06,378 : STS12 p=0.4417, STS12 s=0.5073, STS13 p=0.4885, STS13 s=0.5397, STS14 p=0.4967, STS14 s=0.5351, STS15 p=0.5634, STS15 s=0.6041, STS 16 p=0.5971, STS16 s=0.6313, STS B p=0.6479, STS B s=0.6445, STS B m=1.5074, SICK-R p=0.7831, SICK-R s=0.7159, SICK-P m=0.3985
2019-03-13 22:06:06,378 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 22:06:06,378 : 0.4417,0.5073,0.4885,0.5397,0.4967,0.5351,0.5634,0.6041,0.5971,0.6313,0.6479,0.6445,1.5074,0.7831,0.7159,0.3985
2019-03-13 22:06:06,378 : MR=82.52, CR=88.13, SUBJ=95.43, MPQA=84.53, SST-B=88.19, SST-F=49.05, TREC=92.40, SICK-E=74.37, SNLI=68.95, MRPC=73.33, MRPC f=82.33
2019-03-13 22:06:06,378 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 22:06:06,378 : 82.52,88.13,95.43,84.53,88.19,49.05,92.40,74.37,68.95,73.33,82.33
2019-03-13 22:06:06,378 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 22:06:06,378 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 22:06:06,378 : na,na,na,na,na,na,na,na,na,na
2019-03-13 22:06:06,378 : SentLen=67.99, WC=43.25, TreeDepth=30.78, TopConst=63.55, BShift=90.25, Tense=88.65, SubjNum=82.39, ObjNum=80.59, SOMO=66.98, CoordInv=71.28, average=68.57
2019-03-13 22:06:06,378 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 22:06:06,378 : 67.99,43.25,30.78,63.55,90.25,88.65,82.39,80.59,66.98,71.28,68.57
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,378 : layer 24
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,378 : ********************************************************************************
2019-03-13 22:06:06,470 : ***** Transfer task : STS12 *****


2019-03-13 22:06:06,482 : loading BERT model bert-large-uncased
2019-03-13 22:06:06,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:06:06,500 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:06:06,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwdh3x6od
2019-03-13 22:06:13,932 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:06:23,322 : MSRpar : pearson = 0.3044, spearman = 0.3677
2019-03-13 22:06:24,981 : MSRvid : pearson = 0.4257, spearman = 0.4788
2019-03-13 22:06:26,409 : SMTeuroparl : pearson = 0.4292, spearman = 0.5222
2019-03-13 22:06:29,137 : surprise.OnWN : pearson = 0.4504, spearman = 0.5212
2019-03-13 22:06:30,580 : surprise.SMTnews : pearson = 0.5482, spearman = 0.5838
2019-03-13 22:06:30,581 : ALL (weighted average) : Pearson = 0.4186,             Spearman = 0.4821
2019-03-13 22:06:30,581 : ALL (average) : Pearson = 0.4316,             Spearman = 0.4947

2019-03-13 22:06:30,581 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 22:06:30,591 : loading BERT model bert-large-uncased
2019-03-13 22:06:30,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:06:30,608 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:06:30,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp13qqswxi
2019-03-13 22:06:38,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:06:44,605 : FNWN : pearson = 0.4584, spearman = 0.4373
2019-03-13 22:06:46,519 : headlines : pearson = 0.5629, spearman = 0.5819
2019-03-13 22:06:48,002 : OnWN : pearson = 0.4964, spearman = 0.5132
2019-03-13 22:06:48,002 : ALL (weighted average) : Pearson = 0.5248,             Spearman = 0.5380
2019-03-13 22:06:48,002 : ALL (average) : Pearson = 0.5059,             Spearman = 0.5108

2019-03-13 22:06:48,002 : ***** Transfer task : STS14 *****


2019-03-13 22:06:48,017 : loading BERT model bert-large-uncased
2019-03-13 22:06:48,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:06:48,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:06:48,035 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu8uut6rm
2019-03-13 22:06:55,581 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:07:02,358 : deft-forum : pearson = 0.1700, spearman = 0.2118
2019-03-13 22:07:04,009 : deft-news : pearson = 0.7157, spearman = 0.6705
2019-03-13 22:07:06,196 : headlines : pearson = 0.4913, spearman = 0.4894
2019-03-13 22:07:08,291 : images : pearson = 0.5003, spearman = 0.5092
2019-03-13 22:07:10,444 : OnWN : pearson = 0.6188, spearman = 0.6586
2019-03-13 22:07:13,336 : tweet-news : pearson = 0.5321, spearman = 0.5183
2019-03-13 22:07:13,336 : ALL (weighted average) : Pearson = 0.5061,             Spearman = 0.5142
2019-03-13 22:07:13,336 : ALL (average) : Pearson = 0.5047,             Spearman = 0.5096

2019-03-13 22:07:13,336 : ***** Transfer task : STS15 *****


2019-03-13 22:07:13,370 : loading BERT model bert-large-uncased
2019-03-13 22:07:13,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:07:13,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:07:13,389 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3x_vhnf7
2019-03-13 22:07:20,858 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:07:28,084 : answers-forums : pearson = 0.5073, spearman = 0.5018
2019-03-13 22:07:30,188 : answers-students : pearson = 0.4860, spearman = 0.4919
2019-03-13 22:07:32,260 : belief : pearson = 0.6052, spearman = 0.6456
2019-03-13 22:07:34,537 : headlines : pearson = 0.6083, spearman = 0.6209
2019-03-13 22:07:36,697 : images : pearson = 0.6374, spearman = 0.6507
2019-03-13 22:07:36,697 : ALL (weighted average) : Pearson = 0.5720,             Spearman = 0.5843
2019-03-13 22:07:36,698 : ALL (average) : Pearson = 0.5689,             Spearman = 0.5822

2019-03-13 22:07:36,698 : ***** Transfer task : STS16 *****


2019-03-13 22:07:36,768 : loading BERT model bert-large-uncased
2019-03-13 22:07:36,768 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:07:36,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:07:36,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo3ajzvv2
2019-03-13 22:07:44,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:07:50,414 : answer-answer : pearson = 0.4725, spearman = 0.4795
2019-03-13 22:07:51,079 : headlines : pearson = 0.5595, spearman = 0.6122
2019-03-13 22:07:51,968 : plagiarism : pearson = 0.7482, spearman = 0.7654
2019-03-13 22:07:53,476 : postediting : pearson = 0.7234, spearman = 0.7520
2019-03-13 22:07:54,087 : question-question : pearson = 0.4614, spearman = 0.4581
2019-03-13 22:07:54,087 : ALL (weighted average) : Pearson = 0.5939,             Spearman = 0.6151
2019-03-13 22:07:54,087 : ALL (average) : Pearson = 0.5930,             Spearman = 0.6134

2019-03-13 22:07:54,087 : ***** Transfer task : MR *****


2019-03-13 22:07:54,106 : loading BERT model bert-large-uncased
2019-03-13 22:07:54,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:07:54,124 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:07:54,124 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppy2io8zr
2019-03-13 22:08:01,534 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:08:06,841 : Generating sentence embeddings
2019-03-13 22:08:38,585 : Generated sentence embeddings
2019-03-13 22:08:38,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:08:48,164 : Best param found at split 1: l2reg = 0.0001                 with score 83.84
2019-03-13 22:08:58,206 : Best param found at split 2: l2reg = 0.001                 with score 83.74
2019-03-13 22:09:09,293 : Best param found at split 3: l2reg = 0.001                 with score 83.85
2019-03-13 22:09:20,508 : Best param found at split 4: l2reg = 0.001                 with score 83.29
2019-03-13 22:09:30,798 : Best param found at split 5: l2reg = 0.0001                 with score 83.92
2019-03-13 22:09:31,324 : Dev acc : 83.73 Test acc : 82.74

2019-03-13 22:09:31,325 : ***** Transfer task : CR *****


2019-03-13 22:09:31,332 : loading BERT model bert-large-uncased
2019-03-13 22:09:31,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:09:31,352 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:09:31,353 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo7og7uge
2019-03-13 22:09:38,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:09:44,121 : Generating sentence embeddings
2019-03-13 22:09:52,500 : Generated sentence embeddings
2019-03-13 22:09:52,501 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:09:56,225 : Best param found at split 1: l2reg = 0.001                 with score 88.94
2019-03-13 22:10:00,201 : Best param found at split 2: l2reg = 0.0001                 with score 88.37
2019-03-13 22:10:03,685 : Best param found at split 3: l2reg = 0.0001                 with score 88.81
2019-03-13 22:10:07,448 : Best param found at split 4: l2reg = 0.01                 with score 88.68
2019-03-13 22:10:11,324 : Best param found at split 5: l2reg = 0.001                 with score 88.45
2019-03-13 22:10:11,575 : Dev acc : 88.65 Test acc : 87.87

2019-03-13 22:10:11,575 : ***** Transfer task : MPQA *****


2019-03-13 22:10:11,581 : loading BERT model bert-large-uncased
2019-03-13 22:10:11,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:10:11,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:10:11,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1ogq04d2
2019-03-13 22:10:19,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:10:24,433 : Generating sentence embeddings
2019-03-13 22:10:32,061 : Generated sentence embeddings
2019-03-13 22:10:32,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:10:41,979 : Best param found at split 1: l2reg = 1e-05                 with score 83.85
2019-03-13 22:10:53,749 : Best param found at split 2: l2reg = 1e-05                 with score 84.31
2019-03-13 22:11:04,646 : Best param found at split 3: l2reg = 0.001                 with score 83.94
2019-03-13 22:11:13,125 : Best param found at split 4: l2reg = 0.0001                 with score 83.81
2019-03-13 22:11:21,801 : Best param found at split 5: l2reg = 0.001                 with score 83.06
2019-03-13 22:11:22,298 : Dev acc : 83.79 Test acc : 85.55

2019-03-13 22:11:22,299 : ***** Transfer task : SUBJ *****


2019-03-13 22:11:22,314 : loading BERT model bert-large-uncased
2019-03-13 22:11:22,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:11:22,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:11:22,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk9c2c9p4
2019-03-13 22:11:29,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:11:35,192 : Generating sentence embeddings
2019-03-13 22:12:06,270 : Generated sentence embeddings
2019-03-13 22:12:06,270 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:12:16,728 : Best param found at split 1: l2reg = 0.0001                 with score 95.85
2019-03-13 22:12:27,326 : Best param found at split 2: l2reg = 0.0001                 with score 95.59
2019-03-13 22:12:38,366 : Best param found at split 3: l2reg = 0.001                 with score 95.39
2019-03-13 22:12:49,212 : Best param found at split 4: l2reg = 0.001                 with score 95.86
2019-03-13 22:12:59,380 : Best param found at split 5: l2reg = 0.0001                 with score 95.56
2019-03-13 22:12:59,873 : Dev acc : 95.65 Test acc : 95.43

2019-03-13 22:12:59,874 : ***** Transfer task : SST Binary classification *****


2019-03-13 22:13:00,004 : loading BERT model bert-large-uncased
2019-03-13 22:13:00,004 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:13:00,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:13:00,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnwlznhkj
2019-03-13 22:13:07,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:13:12,841 : Computing embedding for train
2019-03-13 22:14:53,715 : Computed train embeddings
2019-03-13 22:14:53,715 : Computing embedding for dev
2019-03-13 22:14:55,916 : Computed dev embeddings
2019-03-13 22:14:55,916 : Computing embedding for test
2019-03-13 22:15:00,548 : Computed test embeddings
2019-03-13 22:15:00,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:15:16,880 : [('reg:1e-05', 87.39), ('reg:0.0001', 87.27), ('reg:0.001', 87.27), ('reg:0.01', 86.01)]
2019-03-13 22:15:16,880 : Validation : best param found is reg = 1e-05 with score             87.39
2019-03-13 22:15:16,880 : Evaluating...
2019-03-13 22:15:21,209 : 
Dev acc : 87.39 Test acc : 87.15 for             SST Binary classification

2019-03-13 22:15:21,209 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 22:15:21,261 : loading BERT model bert-large-uncased
2019-03-13 22:15:21,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:15:21,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:15:21,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptn18gvzy
2019-03-13 22:15:28,764 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:15:34,047 : Computing embedding for train
2019-03-13 22:15:56,119 : Computed train embeddings
2019-03-13 22:15:56,119 : Computing embedding for dev
2019-03-13 22:15:59,004 : Computed dev embeddings
2019-03-13 22:15:59,005 : Computing embedding for test
2019-03-13 22:16:04,694 : Computed test embeddings
2019-03-13 22:16:04,694 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:16:07,164 : [('reg:1e-05', 45.41), ('reg:0.0001', 45.23), ('reg:0.001', 44.78), ('reg:0.01', 43.42)]
2019-03-13 22:16:07,165 : Validation : best param found is reg = 1e-05 with score             45.41
2019-03-13 22:16:07,165 : Evaluating...
2019-03-13 22:16:07,805 : 
Dev acc : 45.41 Test acc : 47.38 for             SST Fine-Grained classification

2019-03-13 22:16:07,805 : ***** Transfer task : TREC *****


2019-03-13 22:16:07,819 : loading BERT model bert-large-uncased
2019-03-13 22:16:07,819 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:16:07,839 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:16:07,839 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp34ny8jx0
2019-03-13 22:16:15,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:16:28,167 : Computed train embeddings
2019-03-13 22:16:28,761 : Computed test embeddings
2019-03-13 22:16:28,761 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:16:36,707 : [('reg:1e-05', 80.08), ('reg:0.0001', 79.93), ('reg:0.001', 77.99), ('reg:0.01', 71.96)]
2019-03-13 22:16:36,707 : Cross-validation : best param found is reg = 1e-05             with score 80.08
2019-03-13 22:16:36,707 : Evaluating...
2019-03-13 22:16:37,160 : 
Dev acc : 80.08 Test acc : 86.4             for TREC

2019-03-13 22:16:37,161 : ***** Transfer task : MRPC *****


2019-03-13 22:16:37,183 : loading BERT model bert-large-uncased
2019-03-13 22:16:37,183 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:16:37,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:16:37,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3y67jfbv
2019-03-13 22:16:44,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:16:50,087 : Computing embedding for train
2019-03-13 22:17:12,473 : Computed train embeddings
2019-03-13 22:17:12,473 : Computing embedding for test
2019-03-13 22:17:22,277 : Computed test embeddings
2019-03-13 22:17:22,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:17:26,838 : [('reg:1e-05', 71.54), ('reg:0.0001', 71.54), ('reg:0.001', 71.83), ('reg:0.01', 71.2)]
2019-03-13 22:17:26,838 : Cross-validation : best param found is reg = 0.001             with score 71.83
2019-03-13 22:17:26,838 : Evaluating...
2019-03-13 22:17:27,146 : Dev acc : 71.83 Test acc 72.0; Test F1 79.47 for MRPC.

2019-03-13 22:17:27,146 : ***** Transfer task : SICK-Entailment*****


2019-03-13 22:17:27,207 : loading BERT model bert-large-uncased
2019-03-13 22:17:27,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:17:27,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:17:27,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzdf53x27
2019-03-13 22:17:34,675 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:17:40,026 : Computing embedding for train
2019-03-13 22:17:51,376 : Computed train embeddings
2019-03-13 22:17:51,376 : Computing embedding for dev
2019-03-13 22:17:52,927 : Computed dev embeddings
2019-03-13 22:17:52,927 : Computing embedding for test
2019-03-13 22:18:05,119 : Computed test embeddings
2019-03-13 22:18:05,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:18:06,677 : [('reg:1e-05', 76.2), ('reg:0.0001', 75.4), ('reg:0.001', 75.0), ('reg:0.01', 76.2)]
2019-03-13 22:18:06,678 : Validation : best param found is reg = 1e-05 with score             76.2
2019-03-13 22:18:06,678 : Evaluating...
2019-03-13 22:18:07,080 : 
Dev acc : 76.2 Test acc : 73.82 for                        SICK entailment

2019-03-13 22:18:07,080 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 22:18:07,107 : loading BERT model bert-large-uncased
2019-03-13 22:18:07,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:18:07,165 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:18:07,165 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18xl709_
2019-03-13 22:18:14,666 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:18:20,017 : Computing embedding for train
2019-03-13 22:18:31,399 : Computed train embeddings
2019-03-13 22:18:31,399 : Computing embedding for dev
2019-03-13 22:18:32,954 : Computed dev embeddings
2019-03-13 22:18:32,954 : Computing embedding for test
2019-03-13 22:18:45,169 : Computed test embeddings
2019-03-13 22:19:00,685 : Dev : Pearson 0.7689746145090945
2019-03-13 22:19:00,686 : Test : Pearson 0.7822540114069367 Spearman 0.7135476359514117 MSE 0.39552314519716064                        for SICK Relatedness

2019-03-13 22:19:00,687 : 

***** Transfer task : STSBenchmark*****


2019-03-13 22:19:00,753 : loading BERT model bert-large-uncased
2019-03-13 22:19:00,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:19:00,773 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:19:00,773 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgoamisz6
2019-03-13 22:19:08,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:19:13,503 : Computing embedding for train
2019-03-13 22:19:32,209 : Computed train embeddings
2019-03-13 22:19:32,209 : Computing embedding for dev
2019-03-13 22:19:37,889 : Computed dev embeddings
2019-03-13 22:19:37,889 : Computing embedding for test
2019-03-13 22:19:42,528 : Computed test embeddings
2019-03-13 22:20:01,786 : Dev : Pearson 0.6883357347810786
2019-03-13 22:20:01,786 : Test : Pearson 0.6598083100710219 Spearman 0.6510881154395841 MSE 1.500980622520064                        for SICK Relatedness

2019-03-13 22:20:01,786 : ***** Transfer task : SNLI Entailment*****


2019-03-13 22:20:06,916 : loading BERT model bert-large-uncased
2019-03-13 22:20:06,916 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:20:07,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:20:07,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0u4pzizi
2019-03-13 22:20:14,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:20:20,195 : PROGRESS (encoding): 0.00%
2019-03-13 22:23:07,803 : PROGRESS (encoding): 14.56%
2019-03-13 22:26:20,927 : PROGRESS (encoding): 29.12%
2019-03-13 22:29:32,567 : PROGRESS (encoding): 43.69%
2019-03-13 22:32:56,382 : PROGRESS (encoding): 58.25%
2019-03-13 22:36:43,160 : PROGRESS (encoding): 72.81%
2019-03-13 22:40:28,804 : PROGRESS (encoding): 87.37%
2019-03-13 22:44:33,072 : PROGRESS (encoding): 0.00%
2019-03-13 22:45:03,753 : PROGRESS (encoding): 0.00%
2019-03-13 22:45:33,299 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:46:05,684 : [('reg:1e-09', 62.17)]
2019-03-13 22:46:05,684 : Validation : best param found is reg = 1e-09 with score             62.17
2019-03-13 22:46:05,684 : Evaluating...
2019-03-13 22:46:34,701 : Dev acc : 62.17 Test acc : 62.01 for SNLI

2019-03-13 22:46:34,702 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 22:46:34,908 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 22:46:35,974 : loading BERT model bert-large-uncased
2019-03-13 22:46:35,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:46:36,001 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:46:36,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph7o6i_ki
2019-03-13 22:46:43,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:46:48,874 : Computing embeddings for train/dev/test
2019-03-13 22:50:23,526 : Computed embeddings
2019-03-13 22:50:23,526 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:50:56,929 : [('reg:1e-05', 66.89), ('reg:0.0001', 65.73), ('reg:0.001', 54.35), ('reg:0.01', 41.17)]
2019-03-13 22:50:56,930 : Validation : best param found is reg = 1e-05 with score             66.89
2019-03-13 22:50:56,930 : Evaluating...
2019-03-13 22:51:06,639 : 
Dev acc : 66.9 Test acc : 67.2 for LENGTH classification

2019-03-13 22:51:06,640 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 22:51:06,894 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 22:51:06,940 : loading BERT model bert-large-uncased
2019-03-13 22:51:06,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:51:06,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:51:06,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7yjdnnpy
2019-03-13 22:51:14,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:51:19,873 : Computing embeddings for train/dev/test
2019-03-13 22:54:37,406 : Computed embeddings
2019-03-13 22:54:37,406 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:55:07,265 : [('reg:1e-05', 28.8), ('reg:0.0001', 7.36), ('reg:0.001', 1.23), ('reg:0.01', 0.46)]
2019-03-13 22:55:07,265 : Validation : best param found is reg = 1e-05 with score             28.8
2019-03-13 22:55:07,266 : Evaluating...
2019-03-13 22:55:13,334 : 
Dev acc : 28.8 Test acc : 28.9 for WORDCONTENT classification

2019-03-13 22:55:13,335 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 22:55:13,852 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 22:55:13,918 : loading BERT model bert-large-uncased
2019-03-13 22:55:13,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:55:13,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:55:13,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbv46us8g
2019-03-13 22:55:21,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:55:26,714 : Computing embeddings for train/dev/test
2019-03-13 22:58:32,328 : Computed embeddings
2019-03-13 22:58:32,328 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:58:53,935 : [('reg:1e-05', 30.01), ('reg:0.0001', 30.41), ('reg:0.001', 28.04), ('reg:0.01', 25.25)]
2019-03-13 22:58:53,936 : Validation : best param found is reg = 0.0001 with score             30.41
2019-03-13 22:58:53,936 : Evaluating...
2019-03-13 22:58:59,301 : 
Dev acc : 30.4 Test acc : 29.9 for DEPTH classification

2019-03-13 22:58:59,302 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 22:58:59,712 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 22:58:59,776 : loading BERT model bert-large-uncased
2019-03-13 22:58:59,776 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:58:59,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:58:59,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsnnokxy6
2019-03-13 22:59:07,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:59:12,526 : Computing embeddings for train/dev/test
2019-03-13 23:02:02,962 : Computed embeddings
2019-03-13 23:02:02,962 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:02:32,892 : [('reg:1e-05', 61.0), ('reg:0.0001', 61.13), ('reg:0.001', 50.21), ('reg:0.01', 32.32)]
2019-03-13 23:02:32,892 : Validation : best param found is reg = 0.0001 with score             61.13
2019-03-13 23:02:32,893 : Evaluating...
2019-03-13 23:02:40,538 : 
Dev acc : 61.1 Test acc : 60.4 for TOPCONSTITUENTS classification

2019-03-13 23:02:40,540 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 23:02:40,893 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 23:02:40,960 : loading BERT model bert-large-uncased
2019-03-13 23:02:40,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:02:40,989 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:02:40,989 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp09z68t_1
2019-03-13 23:02:48,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:02:53,655 : Computing embeddings for train/dev/test
2019-03-13 23:05:58,349 : Computed embeddings
2019-03-13 23:05:58,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:06:26,580 : [('reg:1e-05', 90.43), ('reg:0.0001', 90.53), ('reg:0.001', 89.98), ('reg:0.01', 87.77)]
2019-03-13 23:06:26,580 : Validation : best param found is reg = 0.0001 with score             90.53
2019-03-13 23:06:26,580 : Evaluating...
2019-03-13 23:06:31,501 : 
Dev acc : 90.5 Test acc : 90.4 for BIGRAMSHIFT classification

2019-03-13 23:06:31,502 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 23:06:31,885 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 23:06:31,949 : loading BERT model bert-large-uncased
2019-03-13 23:06:31,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:06:31,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:06:31,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpanzb2rqh
2019-03-13 23:06:39,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:06:44,725 : Computing embeddings for train/dev/test
2019-03-13 23:09:45,289 : Computed embeddings
2019-03-13 23:09:45,289 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:10:11,204 : [('reg:1e-05', 89.61), ('reg:0.0001', 89.8), ('reg:0.001', 89.97), ('reg:0.01', 90.21)]
2019-03-13 23:10:11,204 : Validation : best param found is reg = 0.01 with score             90.21
2019-03-13 23:10:11,204 : Evaluating...
2019-03-13 23:10:17,688 : 
Dev acc : 90.2 Test acc : 88.3 for TENSE classification

2019-03-13 23:10:17,689 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 23:10:18,088 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 23:10:18,150 : loading BERT model bert-large-uncased
2019-03-13 23:10:18,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:10:18,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:10:18,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9tajt2dy
2019-03-13 23:10:25,699 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:10:30,996 : Computing embeddings for train/dev/test
2019-03-13 23:13:42,454 : Computed embeddings
2019-03-13 23:13:42,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:14:12,629 : [('reg:1e-05', 83.24), ('reg:0.0001', 82.67), ('reg:0.001', 82.54), ('reg:0.01', 80.76)]
2019-03-13 23:14:12,629 : Validation : best param found is reg = 1e-05 with score             83.24
2019-03-13 23:14:12,629 : Evaluating...
2019-03-13 23:14:19,951 : 
Dev acc : 83.2 Test acc : 82.1 for SUBJNUMBER classification

2019-03-13 23:14:19,952 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 23:14:20,546 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 23:14:20,612 : loading BERT model bert-large-uncased
2019-03-13 23:14:20,612 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:14:20,638 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:14:20,639 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdpn766lb
2019-03-13 23:14:28,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:14:33,318 : Computing embeddings for train/dev/test
2019-03-13 23:17:40,934 : Computed embeddings
2019-03-13 23:17:40,934 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:18:12,028 : [('reg:1e-05', 79.21), ('reg:0.0001', 79.59), ('reg:0.001', 78.95), ('reg:0.01', 76.36)]
2019-03-13 23:18:12,028 : Validation : best param found is reg = 0.0001 with score             79.59
2019-03-13 23:18:12,028 : Evaluating...
2019-03-13 23:18:21,666 : 
Dev acc : 79.6 Test acc : 80.4 for OBJNUMBER classification

2019-03-13 23:18:21,667 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 23:18:22,046 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 23:18:22,116 : loading BERT model bert-large-uncased
2019-03-13 23:18:22,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:18:22,241 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:18:22,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphbbmy8ci
2019-03-13 23:18:29,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:18:34,975 : Computing embeddings for train/dev/test
2019-03-13 23:22:12,824 : Computed embeddings
2019-03-13 23:22:12,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:22:35,379 : [('reg:1e-05', 66.7), ('reg:0.0001', 66.59), ('reg:0.001', 66.07), ('reg:0.01', 65.13)]
2019-03-13 23:22:35,379 : Validation : best param found is reg = 1e-05 with score             66.7
2019-03-13 23:22:35,379 : Evaluating...
2019-03-13 23:22:40,763 : 
Dev acc : 66.7 Test acc : 67.1 for ODDMANOUT classification

2019-03-13 23:22:40,764 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 23:22:41,201 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 23:22:41,277 : loading BERT model bert-large-uncased
2019-03-13 23:22:41,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:22:41,306 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:22:41,306 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7q3m3okr
2019-03-13 23:22:48,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:22:53,992 : Computing embeddings for train/dev/test
2019-03-13 23:26:29,797 : Computed embeddings
2019-03-13 23:26:29,798 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:26:55,952 : [('reg:1e-05', 66.95), ('reg:0.0001', 71.04), ('reg:0.001', 64.78), ('reg:0.01', 65.85)]
2019-03-13 23:26:55,952 : Validation : best param found is reg = 0.0001 with score             71.04
2019-03-13 23:26:55,952 : Evaluating...
2019-03-13 23:27:03,458 : 
Dev acc : 71.0 Test acc : 70.2 for COORDINATIONINVERSION classification

2019-03-13 23:27:03,460 : total results: {'STS12': {'MSRpar': {'pearson': (0.3044175261102851, 1.51385518571357e-17), 'spearman': SpearmanrResult(correlation=0.36768198956962583, pvalue=2.0140834430791066e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4256774899679186, 2.2984336324703226e-34), 'spearman': SpearmanrResult(correlation=0.47878361549079224, pvalue=3.1035944030342317e-44), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.42916779302662017, 5.416558498692324e-22), 'spearman': SpearmanrResult(correlation=0.5221911489383557, pvalue=1.7997808817287163e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4503823215592157, 9.686558887884147e-39), 'spearman': SpearmanrResult(correlation=0.5211571031051375, pvalue=1.869126605945157e-53), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5481549954079781, 1.1335704816354464e-32), 'spearman': SpearmanrResult(correlation=0.583783464371067, pvalue=8.099219234080772e-38), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4315600252144035, 'wmean': 0.41861642966379226}, 'spearman': {'mean': 0.4947194642949956, 'wmean': 0.48208892238446843}}}, 'STS13': {'FNWN': {'pearson': (0.4584291205570517, 3.2807070007195085e-11), 'spearman': SpearmanrResult(correlation=0.4372764074878376, pvalue=3.1481937143782136e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.5628853060348102, 6.677358316013817e-64), 'spearman': SpearmanrResult(correlation=0.5819351348501622, pvalue=3.4716975322172364e-69), 'nsamples': 750}, 'OnWN': {'pearson': (0.4963723235759326, 3.112523997467316e-36), 'spearman': SpearmanrResult(correlation=0.5131917955771484, pvalue=5.158435304111108e-39), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5058955833892648, 'wmean': 0.5248479712249925}, 'spearman': {'mean': 0.5108011126383828, 'wmean': 0.5379981263144021}}}, 'STS14': {'deft-forum': {'pearson': (0.1699535776375917, 0.0002927719343689025), 'spearman': SpearmanrResult(correlation=0.21179091614890372, pvalue=5.8489449994864674e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.7157385630062746, 2.2158253281483255e-48), 'spearman': SpearmanrResult(correlation=0.670458421514227, pvalue=1.6124186792680335e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.49128101353957887, 8.04847223366158e-47), 'spearman': SpearmanrResult(correlation=0.48943692858182375, pvalue=1.9682281637927545e-46), 'nsamples': 750}, 'images': {'pearson': (0.500306526483269, 9.340824925190385e-49), 'spearman': SpearmanrResult(correlation=0.5091739404732023, pvalue=1.0274561931265547e-50), 'nsamples': 750}, 'OnWN': {'pearson': (0.6188025604662609, 1.8237549650138704e-80), 'spearman': SpearmanrResult(correlation=0.6586177285814163, pvalue=1.8310317558864167e-94), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5320879277967739, 4.730820160030547e-56), 'spearman': SpearmanrResult(correlation=0.5183194330605786, pvalue=8.519527484433592e-53), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5046950281549581, 'wmean': 0.5061491200141895}, 'spearman': {'mean': 0.5096328947266919, 'wmean': 0.5141611897984107}}}, 'STS15': {'answers-forums': {'pearson': (0.5072915335379428, 6.442953971892329e-26), 'spearman': SpearmanrResult(correlation=0.5017706897081863, pvalue=2.6245092132442574e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.48603939290459147, 1.007902163294709e-45), 'spearman': SpearmanrResult(correlation=0.4918576345429477, pvalue=6.078419909319901e-47), 'nsamples': 750}, 'belief': {'pearson': (0.6052374750048953, 7.614021319658576e-39), 'spearman': SpearmanrResult(correlation=0.6455934790608191, pvalue=1.3503752921494006e-45), 'nsamples': 375}, 'headlines': {'pearson': (0.6082643571691104, 4.3337893076716395e-77), 'spearman': SpearmanrResult(correlation=0.6208706916676685, pvalue=3.8307778180033813e-81), 'nsamples': 750}, 'images': {'pearson': (0.6374364577518288, 9.262682640699027e-87), 'spearman': SpearmanrResult(correlation=0.6507008768111759, pvalue=1.638761964378702e-91), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5688538432736738, 'wmean': 0.5720011780242374}, 'spearman': {'mean': 0.5821586743581595, 'wmean': 0.5842778218515736}}}, 'STS16': {'answer-answer': {'pearson': (0.4725272352116438, 1.5624609365467e-15), 'spearman': SpearmanrResult(correlation=0.47954882170285434, pvalue=5.183202690750595e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.5594547622772005, 6.585752042246314e-22), 'spearman': SpearmanrResult(correlation=0.6121519739232584, pvalue=5.367518310996682e-27), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7482109924237133, 1.6629908573340507e-42), 'spearman': SpearmanrResult(correlation=0.7654030559918125, pvalue=1.5639791617709193e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7234197873826982, 8.170729211521653e-41), 'spearman': SpearmanrResult(correlation=0.7519509298256718, pvalue=1.0895352053596562e-45), 'nsamples': 244}, 'question-question': {'pearson': (0.4613863938380664, 2.0529522390753264e-12), 'spearman': SpearmanrResult(correlation=0.45806284345874115, pvalue=3.0871409662792192e-12), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5929998342266644, 'wmean': 0.5938944909289787}, 'spearman': {'mean': 0.6134235249804676, 'wmean': 0.6150801907739242}}}, 'MR': {'devacc': 83.73, 'acc': 82.74, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.65, 'acc': 87.87, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.79, 'acc': 85.55, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.65, 'acc': 95.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.39, 'acc': 87.15, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.41, 'acc': 47.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.08, 'acc': 86.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.83, 'acc': 72.0, 'f1': 79.47, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 73.82, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7689746145090945, 'pearson': 0.7822540114069367, 'spearman': 0.7135476359514117, 'mse': 0.39552314519716064, 'yhat': array([4.08953614, 4.01990735, 1.69401022, ..., 3.00560965, 4.50787848,        4.26743921]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6883357347810786, 'pearson': 0.6598083100710219, 'spearman': 0.6510881154395841, 'mse': 1.500980622520064, 'yhat': array([1.7348404 , 1.2927134 , 2.84344768, ..., 3.85338039, 3.7922926 ,        3.46665515]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.17, 'acc': 62.01, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 66.89, 'acc': 67.17, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.8, 'acc': 28.93, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.41, 'acc': 29.91, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.13, 'acc': 60.41, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.53, 'acc': 90.41, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.21, 'acc': 88.26, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.24, 'acc': 82.07, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.59, 'acc': 80.38, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.7, 'acc': 67.14, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.04, 'acc': 70.22, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 23:27:03,460 : STS12 p=0.4186, STS12 s=0.4821, STS13 p=0.5248, STS13 s=0.5380, STS14 p=0.5061, STS14 s=0.5142, STS15 p=0.5720, STS15 s=0.5843, STS 16 p=0.5939, STS16 s=0.6151, STS B p=0.6598, STS B s=0.6511, STS B m=1.5010, SICK-R p=0.7823, SICK-R s=0.7135, SICK-P m=0.3955
2019-03-13 23:27:03,460 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 23:27:03,460 : 0.4186,0.4821,0.5248,0.5380,0.5061,0.5142,0.5720,0.5843,0.5939,0.6151,0.6598,0.6511,1.5010,0.7823,0.7135,0.3955
2019-03-13 23:27:03,460 : MR=82.74, CR=87.87, SUBJ=95.43, MPQA=85.55, SST-B=87.15, SST-F=47.38, TREC=86.40, SICK-E=73.82, SNLI=62.01, MRPC=72.00, MRPC f=79.47
2019-03-13 23:27:03,460 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 23:27:03,460 : 82.74,87.87,95.43,85.55,87.15,47.38,86.40,73.82,62.01,72.00,79.47
2019-03-13 23:27:03,460 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 23:27:03,460 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 23:27:03,460 : na,na,na,na,na,na,na,na,na,na
2019-03-13 23:27:03,460 : SentLen=67.17, WC=28.93, TreeDepth=29.91, TopConst=60.41, BShift=90.41, Tense=88.26, SubjNum=82.07, ObjNum=80.38, SOMO=67.14, CoordInv=70.22, average=66.49
2019-03-13 23:27:03,460 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 23:27:03,460 : 67.17,28.93,29.91,60.41,90.41,88.26,82.07,80.38,67.14,70.22,66.49
