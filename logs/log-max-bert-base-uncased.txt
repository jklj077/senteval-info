2019-02-15 14:18:39,814 : ********************************************************************************
2019-02-15 14:18:39,814 : ********************************************************************************
2019-02-15 14:18:39,814 : ********************************************************************************
2019-02-15 14:18:39,814 : layer 0
2019-02-15 14:18:39,815 : ********************************************************************************
2019-02-15 14:18:39,815 : ********************************************************************************
2019-02-15 14:18:39,815 : ********************************************************************************
2019-02-15 14:18:39,815 : ***** Transfer task : STS12 *****


2019-02-15 14:18:39,854 : loading BERT model bert-base-uncased
2019-02-15 14:18:39,854 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:18:39,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:18:39,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp974pfd1g
2019-02-15 14:18:42,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:18:47,755 : MSRpar : pearson = 0.3923, spearman = 0.4113
2019-02-15 14:18:48,393 : MSRvid : pearson = 0.7107, spearman = 0.7137
2019-02-15 14:18:48,937 : SMTeuroparl : pearson = 0.4897, spearman = 0.5861
2019-02-15 14:18:49,908 : surprise.OnWN : pearson = 0.6185, spearman = 0.6561
2019-02-15 14:18:50,454 : surprise.SMTnews : pearson = 0.4909, spearman = 0.4206
2019-02-15 14:18:50,454 : ALL (weighted average) : Pearson = 0.5508,             Spearman = 0.5704
2019-02-15 14:18:50,454 : ALL (average) : Pearson = 0.5404,             Spearman = 0.5576

2019-02-15 14:18:50,454 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 14:18:50,462 : loading BERT model bert-base-uncased
2019-02-15 14:18:50,463 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:18:50,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:18:50,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp77hjxqnd
2019-02-15 14:18:52,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:18:54,803 : FNWN : pearson = 0.2444, spearman = 0.2393
2019-02-15 14:18:55,545 : headlines : pearson = 0.6611, spearman = 0.6567
2019-02-15 14:18:56,101 : OnWN : pearson = 0.5890, spearman = 0.5899
2019-02-15 14:18:56,101 : ALL (weighted average) : Pearson = 0.5816,             Spearman = 0.5791
2019-02-15 14:18:56,102 : ALL (average) : Pearson = 0.4982,             Spearman = 0.4953

2019-02-15 14:18:56,102 : ***** Transfer task : STS14 *****


2019-02-15 14:18:56,133 : loading BERT model bert-base-uncased
2019-02-15 14:18:56,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:18:56,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:18:56,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmvf4kdt5
2019-02-15 14:18:58,551 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:00,624 : deft-forum : pearson = 0.3605, spearman = 0.3567
2019-02-15 14:19:01,221 : deft-news : pearson = 0.6842, spearman = 0.6511
2019-02-15 14:19:02,054 : headlines : pearson = 0.6246, spearman = 0.6106
2019-02-15 14:19:02,857 : images : pearson = 0.6964, spearman = 0.6920
2019-02-15 14:19:03,668 : OnWN : pearson = 0.6505, spearman = 0.6802
2019-02-15 14:19:04,733 : tweet-news : pearson = 0.5959, spearman = 0.5888
2019-02-15 14:19:04,733 : ALL (weighted average) : Pearson = 0.6115,             Spearman = 0.6092
2019-02-15 14:19:04,734 : ALL (average) : Pearson = 0.6020,             Spearman = 0.5966

2019-02-15 14:19:04,734 : ***** Transfer task : STS15 *****


2019-02-15 14:19:04,769 : loading BERT model bert-base-uncased
2019-02-15 14:19:04,769 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:04,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:04,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpigjblxop
2019-02-15 14:19:07,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:09,424 : answers-forums : pearson = 0.5416, spearman = 0.5191
2019-02-15 14:19:10,243 : answers-students : pearson = 0.6904, spearman = 0.6997
2019-02-15 14:19:11,055 : belief : pearson = 0.5498, spearman = 0.5968
2019-02-15 14:19:12,012 : headlines : pearson = 0.6513, spearman = 0.6541
2019-02-15 14:19:12,900 : images : pearson = 0.7736, spearman = 0.7847
2019-02-15 14:19:12,900 : ALL (weighted average) : Pearson = 0.6652,             Spearman = 0.6741
2019-02-15 14:19:12,900 : ALL (average) : Pearson = 0.6413,             Spearman = 0.6509

2019-02-15 14:19:12,901 : ***** Transfer task : STS16 *****


2019-02-15 14:19:12,978 : loading BERT model bert-base-uncased
2019-02-15 14:19:12,978 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:12,997 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:12,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpusbn_x_y
2019-02-15 14:19:15,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:17,487 : answer-answer : pearson = 0.4273, spearman = 0.4269
2019-02-15 14:19:18,041 : headlines : pearson = 0.6523, spearman = 0.6671
2019-02-15 14:19:18,610 : plagiarism : pearson = 0.6856, spearman = 0.6913
2019-02-15 14:19:19,236 : postediting : pearson = 0.7946, spearman = 0.8281
2019-02-15 14:19:19,481 : question-question : pearson = 0.6301, spearman = 0.6317
2019-02-15 14:19:19,481 : ALL (weighted average) : Pearson = 0.6359,             Spearman = 0.6472
2019-02-15 14:19:19,481 : ALL (average) : Pearson = 0.6380,             Spearman = 0.6490

2019-02-15 14:19:19,481 : ***** Transfer task : MR *****


2019-02-15 14:19:19,502 : loading BERT model bert-base-uncased
2019-02-15 14:19:19,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:19,522 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:19,522 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4m6ogjm1
2019-02-15 14:19:21,963 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:23,539 : Generating sentence embeddings
2019-02-15 14:19:37,356 : Generated sentence embeddings
2019-02-15 14:19:37,357 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:19:50,764 : Best param found at split 1: l2reg = 1e-05                 with score 51.25
2019-02-15 14:19:59,666 : Best param found at split 2: l2reg = 0.01                 with score 50.28
2019-02-15 14:20:19,953 : Best param found at split 3: l2reg = 0.001                 with score 52.52
2019-02-15 14:20:31,068 : Best param found at split 4: l2reg = 0.0001                 with score 52.42
2019-02-15 14:20:42,735 : Best param found at split 5: l2reg = 1e-05                 with score 51.35
2019-02-15 14:20:43,497 : Dev acc : 51.56 Test acc : 50.32

2019-02-15 14:20:43,498 : ***** Transfer task : CR *****


2019-02-15 14:20:43,506 : loading BERT model bert-base-uncased
2019-02-15 14:20:43,506 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:20:43,526 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:20:43,526 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsreto0fe
2019-02-15 14:20:45,970 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:20:47,410 : Generating sentence embeddings
2019-02-15 14:20:51,478 : Generated sentence embeddings
2019-02-15 14:20:51,478 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:20:54,372 : Best param found at split 1: l2reg = 0.001                 with score 66.08
2019-02-15 14:20:57,946 : Best param found at split 2: l2reg = 0.001                 with score 64.89
2019-02-15 14:21:02,022 : Best param found at split 3: l2reg = 0.001                 with score 66.19
2019-02-15 14:21:05,586 : Best param found at split 4: l2reg = 0.0001                 with score 66.1
2019-02-15 14:21:10,252 : Best param found at split 5: l2reg = 0.001                 with score 67.53
2019-02-15 14:21:10,720 : Dev acc : 66.16 Test acc : 65.11

2019-02-15 14:21:10,721 : ***** Transfer task : MPQA *****


2019-02-15 14:21:10,726 : loading BERT model bert-base-uncased
2019-02-15 14:21:10,727 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:21:10,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:21:10,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn56lclwb
2019-02-15 14:21:13,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:21:14,713 : Generating sentence embeddings
2019-02-15 14:21:19,720 : Generated sentence embeddings
2019-02-15 14:21:19,720 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:21:34,019 : Best param found at split 1: l2reg = 0.001                 with score 84.3
2019-02-15 14:21:49,919 : Best param found at split 2: l2reg = 1e-05                 with score 84.37
2019-02-15 14:22:05,714 : Best param found at split 3: l2reg = 0.001                 with score 85.1
2019-02-15 14:22:18,981 : Best param found at split 4: l2reg = 0.01                 with score 85.6
2019-02-15 14:22:34,033 : Best param found at split 5: l2reg = 0.001                 with score 85.98
2019-02-15 14:22:34,465 : Dev acc : 85.07 Test acc : 84.82

2019-02-15 14:22:34,466 : ***** Transfer task : SUBJ *****


2019-02-15 14:22:34,483 : loading BERT model bert-base-uncased
2019-02-15 14:22:34,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:22:34,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:22:34,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp53himgnj
2019-02-15 14:22:36,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:22:38,410 : Generating sentence embeddings
2019-02-15 14:22:57,533 : Generated sentence embeddings
2019-02-15 14:22:57,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:23:08,201 : Best param found at split 1: l2reg = 1e-05                 with score 50.39
2019-02-15 14:23:21,640 : Best param found at split 2: l2reg = 0.0001                 with score 54.22
2019-02-15 14:23:32,680 : Best param found at split 3: l2reg = 1e-05                 with score 53.97
2019-02-15 14:23:43,418 : Best param found at split 4: l2reg = 1e-05                 with score 52.05
2019-02-15 14:24:01,605 : Best param found at split 5: l2reg = 1e-05                 with score 52.1
2019-02-15 14:24:02,490 : Dev acc : 52.55 Test acc : 55.43

2019-02-15 14:24:02,491 : ***** Transfer task : SST Binary classification *****


2019-02-15 14:24:02,628 : loading BERT model bert-base-uncased
2019-02-15 14:24:02,628 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:24:02,654 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:24:02,654 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpseinwyv_
2019-02-15 14:24:05,123 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:24:06,625 : Computing embedding for train
2019-02-15 14:25:15,483 : Computed train embeddings
2019-02-15 14:25:15,483 : Computing embedding for dev
2019-02-15 14:25:16,534 : Computed dev embeddings
2019-02-15 14:25:16,534 : Computing embedding for test
2019-02-15 14:25:18,804 : Computed test embeddings
2019-02-15 14:25:18,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:25:52,407 : [('reg:1e-05', 69.5), ('reg:0.0001', 68.92), ('reg:0.001', 68.46), ('reg:0.01', 69.04)]
2019-02-15 14:25:52,407 : Validation : best param found is reg = 1e-05 with score             69.5
2019-02-15 14:25:52,407 : Evaluating...
2019-02-15 14:25:57,800 : 
Dev acc : 69.5 Test acc : 68.81 for             SST Binary classification

2019-02-15 14:25:57,801 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 14:25:57,850 : loading BERT model bert-base-uncased
2019-02-15 14:25:57,850 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:25:57,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:25:57,871 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw0jiuln1
2019-02-15 14:26:00,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:01,774 : Computing embedding for train
2019-02-15 14:26:16,395 : Computed train embeddings
2019-02-15 14:26:16,395 : Computing embedding for dev
2019-02-15 14:26:18,336 : Computed dev embeddings
2019-02-15 14:26:18,336 : Computing embedding for test
2019-02-15 14:26:22,041 : Computed test embeddings
2019-02-15 14:26:22,041 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:26:25,135 : [('reg:1e-05', 31.7), ('reg:0.0001', 27.97), ('reg:0.001', 26.43), ('reg:0.01', 27.79)]
2019-02-15 14:26:25,136 : Validation : best param found is reg = 1e-05 with score             31.7
2019-02-15 14:26:25,136 : Evaluating...
2019-02-15 14:26:25,947 : 
Dev acc : 31.7 Test acc : 32.67 for             SST Fine-Grained classification

2019-02-15 14:26:25,948 : ***** Transfer task : TREC *****


2019-02-15 14:26:25,961 : loading BERT model bert-base-uncased
2019-02-15 14:26:25,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:26:25,982 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:26:25,982 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpar7jm1pz
2019-02-15 14:26:28,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:34,663 : Computed train embeddings
2019-02-15 14:26:35,062 : Computed test embeddings
2019-02-15 14:26:35,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 14:26:48,236 : [('reg:1e-05', 50.8), ('reg:0.0001', 53.27), ('reg:0.001', 49.97), ('reg:0.01', 49.84)]
2019-02-15 14:26:48,236 : Cross-validation : best param found is reg = 0.0001             with score 53.27
2019-02-15 14:26:48,236 : Evaluating...
2019-02-15 14:26:48,640 : 
Dev acc : 53.27 Test acc : 65.4             for TREC

2019-02-15 14:26:48,640 : ***** Transfer task : MRPC *****


2019-02-15 14:26:48,664 : loading BERT model bert-base-uncased
2019-02-15 14:26:48,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:26:48,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:26:48,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeb2h67hi
2019-02-15 14:26:51,106 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:52,635 : Computing embedding for train
2019-02-15 14:27:03,624 : Computed train embeddings
2019-02-15 14:27:03,624 : Computing embedding for test
2019-02-15 14:27:08,330 : Computed test embeddings
2019-02-15 14:27:08,347 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 14:27:15,874 : [('reg:1e-05', 71.57), ('reg:0.0001', 70.24), ('reg:0.001', 69.97), ('reg:0.01', 71.37)]
2019-02-15 14:27:15,875 : Cross-validation : best param found is reg = 1e-05             with score 71.57
2019-02-15 14:27:15,875 : Evaluating...
2019-02-15 14:27:16,213 : Dev acc : 71.57 Test acc 69.39; Test F1 80.97 for MRPC.

2019-02-15 14:27:16,213 : ***** Transfer task : SICK-Entailment*****


2019-02-15 14:27:16,239 : loading BERT model bert-base-uncased
2019-02-15 14:27:16,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:27:16,300 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:27:16,300 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxrp_ew9w
2019-02-15 14:27:18,747 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:27:20,212 : Computing embedding for train
2019-02-15 14:27:26,487 : Computed train embeddings
2019-02-15 14:27:26,488 : Computing embedding for dev
2019-02-15 14:27:27,287 : Computed dev embeddings
2019-02-15 14:27:27,287 : Computing embedding for test
2019-02-15 14:27:34,036 : Computed test embeddings
2019-02-15 14:27:34,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:27:35,683 : [('reg:1e-05', 75.4), ('reg:0.0001', 72.2), ('reg:0.001', 71.6), ('reg:0.01', 72.6)]
2019-02-15 14:27:35,683 : Validation : best param found is reg = 1e-05 with score             75.4
2019-02-15 14:27:35,683 : Evaluating...
2019-02-15 14:27:36,077 : 
Dev acc : 75.4 Test acc : 74.93 for                        SICK entailment

2019-02-15 14:27:36,077 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 14:27:36,106 : loading BERT model bert-base-uncased
2019-02-15 14:27:36,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:27:36,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:27:36,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7rvo5ajt
2019-02-15 14:27:38,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:27:40,113 : Computing embedding for train
2019-02-15 14:27:47,397 : Computed train embeddings
2019-02-15 14:27:47,398 : Computing embedding for dev
2019-02-15 14:27:48,344 : Computed dev embeddings
2019-02-15 14:27:48,345 : Computing embedding for test
2019-02-15 14:27:58,480 : Computed test embeddings
2019-02-15 14:28:21,005 : Dev : Pearson 0.788086601269239
2019-02-15 14:28:21,005 : Test : Pearson 0.7968666730442842 Spearman 0.7240256291999169 MSE 0.3764490400607171                        for SICK Relatedness

2019-02-15 14:28:21,006 : 

***** Transfer task : STSBenchmark*****


2019-02-15 14:28:21,046 : loading BERT model bert-base-uncased
2019-02-15 14:28:21,047 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:28:21,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:28:21,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd8l0o0li
2019-02-15 14:28:23,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:28:24,946 : Computing embedding for train
2019-02-15 14:28:32,404 : Computed train embeddings
2019-02-15 14:28:32,404 : Computing embedding for dev
2019-02-15 14:28:34,521 : Computed dev embeddings
2019-02-15 14:28:34,521 : Computing embedding for test
2019-02-15 14:28:37,032 : Computed test embeddings
2019-02-15 14:29:21,119 : Dev : Pearson 0.6965712054703311
2019-02-15 14:29:21,119 : Test : Pearson 0.6436464192455066 Spearman 0.6408201822625577 MSE 1.4333519342854175                        for SICK Relatedness

2019-02-15 14:29:21,120 : ***** Transfer task : SNLI Entailment*****


2019-02-15 14:29:25,976 : loading BERT model bert-base-uncased
2019-02-15 14:29:25,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:29:26,104 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:29:26,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjcfdbujc
2019-02-15 14:29:28,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:29:30,238 : PROGRESS (encoding): 0.00%
2019-02-15 14:31:30,870 : PROGRESS (encoding): 14.56%
2019-02-15 14:33:40,757 : PROGRESS (encoding): 29.12%
2019-02-15 14:35:54,948 : PROGRESS (encoding): 43.69%
2019-02-15 14:38:17,521 : PROGRESS (encoding): 58.25%
2019-02-15 14:40:52,483 : PROGRESS (encoding): 72.81%
2019-02-15 14:43:31,950 : PROGRESS (encoding): 87.37%
2019-02-15 14:45:55,622 : PROGRESS (encoding): 0.00%
2019-02-15 14:46:09,749 : PROGRESS (encoding): 0.00%
2019-02-15 14:46:23,706 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:47:00,912 : [('reg:1e-09', 54.79)]
2019-02-15 14:47:00,912 : Validation : best param found is reg = 1e-09 with score             54.79
2019-02-15 14:47:00,912 : Evaluating...
2019-02-15 14:48:16,797 : Dev acc : 54.79 Test acc : 55.22 for SNLI

2019-02-15 14:48:16,798 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 14:48:25,620 : loading BERT model bert-base-uncased
2019-02-15 14:48:25,620 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:48:25,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:48:25,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsdqkc3y1
2019-02-15 14:48:28,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:48:29,608 : Computing embedding for train
2019-02-15 14:59:45,481 : Computed train embeddings
2019-02-15 14:59:45,481 : Computing embedding for dev
2019-02-15 15:00:05,530 : Computed dev embeddings
2019-02-15 15:00:05,530 : Computing embedding for test
2019-02-15 15:00:25,369 : Computed test embeddings
2019-02-15 15:00:25,385 : prepare data
2019-02-15 15:00:25,457 : start epoch
2019-02-15 15:01:08,889 : samples : 64000
2019-02-15 15:01:19,428 : Image to text: 6.14, 18.4, 28.42, 31.0
2019-02-15 15:01:26,999 : Text to Image: 4.992, 16.364, 25.232, 35.0
2019-02-15 15:02:10,736 : samples : 128000
2019-02-15 15:02:21,232 : Image to text: 6.8, 20.7, 31.28, 26.0
2019-02-15 15:02:28,814 : Text to Image: 5.14, 17.516, 27.08, 30.0
2019-02-15 15:03:11,950 : samples : 192000
2019-02-15 15:03:22,459 : Image to text: 6.26, 19.78, 29.14, 28.0
2019-02-15 15:03:29,978 : Text to Image: 5.104, 16.672, 25.772, 32.0
2019-02-15 15:04:14,308 : samples : 256000
2019-02-15 15:04:24,687 : Image to text: 7.14, 21.54, 32.28, 24.0
2019-02-15 15:04:32,640 : Text to Image: 5.552, 17.812, 27.424, 30.0
2019-02-15 15:05:15,158 : samples : 320000
2019-02-15 15:05:25,391 : Image to text: 7.46, 22.94, 34.38, 22.0
2019-02-15 15:05:32,818 : Text to Image: 6.428, 20.384, 30.876, 26.0
2019-02-15 15:06:15,417 : samples : 384000
2019-02-15 15:06:25,826 : Image to text: 7.32, 23.0, 34.16, 22.0
2019-02-15 15:06:33,360 : Text to Image: 6.376, 19.8, 29.976, 27.0
2019-02-15 15:07:16,400 : samples : 448000
2019-02-15 15:07:26,936 : Image to text: 8.0, 23.66, 35.1, 22.0
2019-02-15 15:07:34,591 : Text to Image: 6.232, 19.188, 29.408, 28.0
2019-02-15 15:08:17,976 : samples : 512000
2019-02-15 15:08:28,489 : Image to text: 8.14, 23.94, 35.08, 22.0
2019-02-15 15:08:36,028 : Text to Image: 6.992, 21.016, 31.844, 25.0
2019-02-15 15:09:13,027 : Epoch 1 finished
2019-02-15 15:09:13,472 : Image to text: 23.3, 52.2, 69.7, 5.0
2019-02-15 15:09:13,812 : Text to Image: 18.22, 47.98, 65.42, 6.0
2019-02-15 15:09:14,261 : Image to text: 24.1, 52.8, 69.5, 5.0
2019-02-15 15:09:14,613 : Text to Image: 17.8, 46.96, 64.54, 6.0
2019-02-15 15:09:15,067 : Image to text: 22.7, 53.3, 67.9, 5.0
2019-02-15 15:09:15,419 : Text to Image: 18.66, 48.58, 64.62, 6.0
2019-02-15 15:09:15,873 : Image to text: 25.1, 52.8, 69.2, 5.0
2019-02-15 15:09:16,221 : Text to Image: 18.28, 46.94, 64.0, 6.0
2019-02-15 15:09:16,673 : Image to text: 22.5, 52.2, 66.8, 5.0
2019-02-15 15:09:17,006 : Text to Image: 18.24, 48.08, 64.36, 6.0
2019-02-15 15:09:17,006 : Dev mean Text to Image: 18.240000000000002, 47.708, 64.58800000000001, 6.0
2019-02-15 15:09:17,006 : Dev mean Image to text: 23.54, 52.66, 68.62, 5.0
2019-02-15 15:09:17,007 : start epoch
2019-02-15 15:10:00,200 : samples : 64000
2019-02-15 15:10:10,782 : Image to text: 9.2, 25.4, 36.9, 19.0
2019-02-15 15:10:18,344 : Text to Image: 7.444, 22.144, 33.136, 23.0
2019-02-15 15:11:01,114 : samples : 128000
2019-02-15 15:11:11,688 : Image to text: 8.32, 24.5, 36.54, 20.0
2019-02-15 15:11:19,330 : Text to Image: 7.08, 21.54, 32.26, 24.0
2019-02-15 15:12:03,102 : samples : 192000
2019-02-15 15:12:13,644 : Image to text: 8.42, 24.62, 35.5, 21.0
2019-02-15 15:12:21,235 : Text to Image: 6.68, 20.556, 30.94, 25.0
2019-02-15 15:13:04,502 : samples : 256000
2019-02-15 15:13:15,069 : Image to text: 9.3, 25.9, 37.28, 19.0
2019-02-15 15:13:22,652 : Text to Image: 7.868, 23.192, 34.496, 21.0
2019-02-15 15:14:05,379 : samples : 320000
2019-02-15 15:14:15,981 : Image to text: 8.9, 26.34, 37.94, 18.0
2019-02-15 15:14:23,910 : Text to Image: 7.448, 22.824, 33.584, 23.0
2019-02-15 15:15:07,654 : samples : 384000
2019-02-15 15:15:18,158 : Image to text: 8.96, 26.48, 38.02, 19.0
2019-02-15 15:15:25,725 : Text to Image: 7.204, 22.392, 33.116, 23.0
2019-02-15 15:16:09,084 : samples : 448000
2019-02-15 15:16:19,684 : Image to text: 9.28, 25.62, 36.84, 20.0
2019-02-15 15:16:27,245 : Text to Image: 7.032, 21.656, 32.656, 23.0
2019-02-15 15:17:10,569 : samples : 512000
2019-02-15 15:17:21,058 : Image to text: 9.24, 25.9, 37.58, 19.0
2019-02-15 15:17:28,612 : Text to Image: 7.336, 21.772, 32.736, 23.0
2019-02-15 15:18:05,318 : Epoch 2 finished
2019-02-15 15:18:05,758 : Image to text: 24.1, 54.8, 69.5, 5.0
2019-02-15 15:18:06,103 : Text to Image: 20.14, 52.58, 69.38, 5.0
2019-02-15 15:18:06,542 : Image to text: 22.9, 54.3, 69.7, 5.0
2019-02-15 15:18:06,888 : Text to Image: 19.84, 51.68, 68.94, 5.0
2019-02-15 15:18:07,332 : Image to text: 24.0, 54.4, 71.2, 4.0
2019-02-15 15:18:07,678 : Text to Image: 21.34, 52.96, 69.62, 5.0
2019-02-15 15:18:08,126 : Image to text: 24.6, 55.6, 71.4, 4.0
2019-02-15 15:18:08,475 : Text to Image: 20.82, 51.02, 68.62, 5.0
2019-02-15 15:18:08,938 : Image to text: 24.0, 54.5, 69.8, 5.0
2019-02-15 15:18:09,286 : Text to Image: 20.4, 52.38, 67.92, 5.0
2019-02-15 15:18:09,286 : Dev mean Text to Image: 20.507999999999996, 52.124, 68.896, 5.0
2019-02-15 15:18:09,286 : Dev mean Image to text: 23.919999999999998, 54.720000000000006, 70.32000000000001, 4.6
2019-02-15 15:18:09,287 : start epoch
2019-02-15 15:18:52,960 : samples : 64000
2019-02-15 15:19:03,521 : Image to text: 9.48, 27.4, 39.2, 17.0
2019-02-15 15:19:11,082 : Text to Image: 7.824, 23.284, 34.572, 22.0
2019-02-15 15:19:54,251 : samples : 128000
2019-02-15 15:20:04,764 : Image to text: 9.06, 25.86, 37.76, 19.0
2019-02-15 15:20:12,417 : Text to Image: 7.224, 22.556, 33.548, 22.0
2019-02-15 15:20:56,729 : samples : 192000
2019-02-15 15:21:07,387 : Image to text: 9.56, 26.7, 37.86, 18.0
2019-02-15 15:21:15,049 : Text to Image: 7.188, 22.396, 33.724, 22.0
2019-02-15 15:21:59,212 : samples : 256000
2019-02-15 15:22:09,544 : Image to text: 9.48, 26.6, 38.88, 19.0
2019-02-15 15:22:16,987 : Text to Image: 7.56, 22.84, 33.984, 22.0
2019-02-15 15:23:14,887 : samples : 320000
2019-02-15 15:23:25,404 : Image to text: 9.78, 26.62, 38.74, 18.0
2019-02-15 15:23:32,961 : Text to Image: 7.84, 23.54, 34.332, 22.0
2019-02-15 15:24:15,560 : samples : 384000
2019-02-15 15:24:25,947 : Image to text: 9.9, 27.14, 39.34, 17.0
2019-02-15 15:24:33,560 : Text to Image: 8.428, 24.284, 35.696, 20.0
2019-02-15 15:25:16,470 : samples : 448000
2019-02-15 15:25:27,054 : Image to text: 9.46, 27.24, 38.8, 18.0
2019-02-15 15:25:34,665 : Text to Image: 8.028, 23.716, 35.024, 21.0
2019-02-15 15:26:18,057 : samples : 512000
2019-02-15 15:26:28,496 : Image to text: 9.94, 26.54, 38.54, 19.0
2019-02-15 15:26:36,099 : Text to Image: 7.808, 23.28, 34.22, 22.0
2019-02-15 15:27:13,327 : Epoch 3 finished
2019-02-15 15:27:13,783 : Image to text: 25.7, 55.5, 71.2, 4.0
2019-02-15 15:27:14,127 : Text to Image: 20.28, 52.36, 69.74, 5.0
2019-02-15 15:27:14,575 : Image to text: 24.1, 56.6, 72.5, 4.0
2019-02-15 15:27:14,912 : Text to Image: 21.0, 50.98, 68.4, 5.0
2019-02-15 15:27:15,367 : Image to text: 25.7, 56.5, 72.0, 4.0
2019-02-15 15:27:15,722 : Text to Image: 20.98, 52.56, 69.74, 5.0
2019-02-15 15:27:16,199 : Image to text: 25.9, 56.9, 72.0, 4.0
2019-02-15 15:27:16,539 : Text to Image: 20.08, 50.96, 68.9, 5.0
2019-02-15 15:27:16,992 : Image to text: 26.1, 57.1, 71.9, 4.0
2019-02-15 15:27:17,348 : Text to Image: 20.9, 53.02, 70.08, 5.0
2019-02-15 15:27:17,348 : Dev mean Text to Image: 20.648, 51.976, 69.372, 5.0
2019-02-15 15:27:17,348 : Dev mean Image to text: 25.5, 56.519999999999996, 71.92, 4.0
2019-02-15 15:27:17,349 : start epoch
2019-02-15 15:27:59,972 : samples : 64000
2019-02-15 15:28:10,427 : Image to text: 9.34, 26.72, 39.34, 18.0
2019-02-15 15:28:17,980 : Text to Image: 8.072, 24.204, 35.304, 20.0
2019-02-15 15:29:01,480 : samples : 128000
2019-02-15 15:29:11,994 : Image to text: 10.36, 28.76, 41.28, 16.0
2019-02-15 15:29:19,567 : Text to Image: 8.652, 24.516, 35.52, 21.0
2019-02-15 15:30:03,171 : samples : 192000
2019-02-15 15:30:13,706 : Image to text: 9.78, 27.72, 39.7, 17.0
2019-02-15 15:30:21,240 : Text to Image: 7.772, 23.028, 34.36, 22.0
2019-02-15 15:31:04,326 : samples : 256000
2019-02-15 15:31:14,870 : Image to text: 10.34, 28.68, 40.88, 17.0
2019-02-15 15:31:22,453 : Text to Image: 8.52, 24.396, 35.5, 20.0
2019-02-15 15:32:05,453 : samples : 320000
2019-02-15 15:32:16,005 : Image to text: 9.94, 28.4, 40.06, 17.0
2019-02-15 15:32:23,941 : Text to Image: 8.86, 24.604, 36.044, 20.0
2019-02-15 15:33:06,908 : samples : 384000
2019-02-15 15:33:17,317 : Image to text: 10.18, 28.52, 40.36, 17.0
2019-02-15 15:33:24,736 : Text to Image: 8.328, 24.528, 35.624, 20.0
2019-02-15 15:34:08,194 : samples : 448000
2019-02-15 15:34:18,601 : Image to text: 10.0, 28.22, 40.26, 17.0
2019-02-15 15:34:26,113 : Text to Image: 8.972, 25.704, 37.024, 19.0
2019-02-15 15:35:09,721 : samples : 512000
2019-02-15 15:35:20,256 : Image to text: 9.84, 28.74, 41.04, 17.0
2019-02-15 15:35:27,843 : Text to Image: 8.34, 24.388, 35.876, 20.0
2019-02-15 15:36:04,222 : Epoch 4 finished
2019-02-15 15:36:04,651 : Image to text: 27.1, 57.9, 73.7, 4.0
2019-02-15 15:36:04,963 : Text to Image: 22.14, 54.76, 72.74, 5.0
2019-02-15 15:36:05,378 : Image to text: 27.3, 58.7, 72.2, 4.0
2019-02-15 15:36:05,683 : Text to Image: 21.68, 54.46, 70.9, 5.0
2019-02-15 15:36:06,097 : Image to text: 26.2, 59.6, 75.6, 4.0
2019-02-15 15:36:06,417 : Text to Image: 22.48, 55.16, 71.42, 5.0
2019-02-15 15:36:06,853 : Image to text: 25.7, 58.5, 74.1, 4.0
2019-02-15 15:36:07,188 : Text to Image: 21.1, 54.02, 70.72, 5.0
2019-02-15 15:36:07,651 : Image to text: 25.8, 58.8, 73.2, 4.0
2019-02-15 15:36:07,991 : Text to Image: 22.08, 53.98, 70.58, 5.0
2019-02-15 15:36:07,992 : Dev mean Text to Image: 21.896, 54.476000000000006, 71.27199999999999, 5.0
2019-02-15 15:36:07,992 : Dev mean Image to text: 26.419999999999998, 58.699999999999996, 73.75999999999999, 4.0
2019-02-15 15:36:07,992 : start epoch
2019-02-15 15:36:51,334 : samples : 64000
2019-02-15 15:37:01,684 : Image to text: 10.18, 28.18, 39.88, 17.0
2019-02-15 15:37:09,132 : Text to Image: 8.612, 24.956, 36.248, 20.0
2019-02-15 15:37:52,393 : samples : 128000
2019-02-15 15:38:02,841 : Image to text: 10.22, 28.64, 41.04, 16.0
2019-02-15 15:38:10,496 : Text to Image: 9.088, 25.804, 37.016, 19.0
2019-02-15 15:38:54,500 : samples : 192000
2019-02-15 15:39:05,051 : Image to text: 10.06, 28.66, 39.56, 17.0
2019-02-15 15:39:12,519 : Text to Image: 8.796, 25.132, 36.56, 20.0
2019-02-15 15:40:02,276 : samples : 256000
2019-02-15 15:40:14,985 : Image to text: 10.04, 28.14, 39.92, 17.0
2019-02-15 15:40:23,883 : Text to Image: 8.828, 25.02, 36.644, 20.0
2019-02-15 15:41:07,163 : samples : 320000
2019-02-15 15:41:17,725 : Image to text: 10.28, 28.64, 40.86, 17.0
2019-02-15 15:41:25,289 : Text to Image: 8.712, 25.452, 36.728, 19.0
2019-02-15 15:42:08,290 : samples : 384000
2019-02-15 15:42:18,887 : Image to text: 10.18, 28.48, 40.94, 16.0
2019-02-15 15:42:26,764 : Text to Image: 8.88, 25.372, 37.056, 19.0
2019-02-15 15:43:10,074 : samples : 448000
2019-02-15 15:43:20,611 : Image to text: 10.1, 29.02, 41.66, 16.0
2019-02-15 15:43:28,440 : Text to Image: 8.748, 25.476, 37.0, 19.0
2019-02-15 15:44:11,674 : samples : 512000
2019-02-15 15:44:22,037 : Image to text: 9.74, 29.7, 41.56, 16.0
2019-02-15 15:44:29,524 : Text to Image: 8.904, 25.52, 37.092, 19.0
2019-02-15 15:45:05,964 : Epoch 5 finished
2019-02-15 15:45:06,403 : Image to text: 27.9, 58.3, 73.8, 4.0
2019-02-15 15:45:06,742 : Text to Image: 21.86, 55.12, 71.06, 5.0
2019-02-15 15:45:07,181 : Image to text: 26.8, 58.0, 72.6, 4.0
2019-02-15 15:45:07,529 : Text to Image: 21.56, 53.66, 71.14, 5.0
2019-02-15 15:45:08,000 : Image to text: 27.1, 60.2, 74.7, 4.0
2019-02-15 15:45:08,334 : Text to Image: 22.66, 54.36, 70.92, 5.0
2019-02-15 15:45:08,796 : Image to text: 27.8, 59.8, 75.4, 4.0
2019-02-15 15:45:09,135 : Text to Image: 21.88, 53.7, 70.48, 5.0
2019-02-15 15:45:09,575 : Image to text: 26.0, 59.2, 74.1, 4.0
2019-02-15 15:45:09,908 : Text to Image: 21.48, 53.2, 69.32, 5.0
2019-02-15 15:45:09,908 : Dev mean Text to Image: 21.887999999999998, 54.008, 70.584, 5.0
2019-02-15 15:45:09,908 : Dev mean Image to text: 27.12, 59.099999999999994, 74.11999999999999, 4.0
2019-02-15 15:45:09,909 : start epoch
2019-02-15 15:45:52,849 : samples : 64000
2019-02-15 15:46:03,356 : Image to text: 10.4, 29.16, 41.16, 16.0
2019-02-15 15:46:11,368 : Text to Image: 8.636, 25.232, 36.928, 19.0
2019-02-15 15:46:54,888 : samples : 128000
2019-02-15 15:47:05,277 : Image to text: 10.3, 28.9, 41.64, 15.0
2019-02-15 15:47:12,798 : Text to Image: 9.08, 25.16, 36.732, 20.0
2019-02-15 15:47:55,854 : samples : 192000
2019-02-15 15:48:07,438 : Image to text: 10.16, 28.8, 41.12, 16.0
2019-02-15 15:48:17,450 : Text to Image: 8.504, 25.128, 36.952, 19.0
2019-02-15 15:49:03,095 : samples : 256000
2019-02-15 15:49:15,734 : Image to text: 10.64, 28.78, 41.16, 16.0
2019-02-15 15:49:25,775 : Text to Image: 8.832, 24.892, 36.444, 20.0
2019-02-15 15:50:08,906 : samples : 320000
2019-02-15 15:50:19,095 : Image to text: 10.36, 29.82, 42.54, 15.0
2019-02-15 15:50:27,338 : Text to Image: 9.152, 25.944, 37.468, 19.0
2019-02-15 15:51:11,880 : samples : 384000
2019-02-15 15:51:24,532 : Image to text: 10.32, 29.5, 41.68, 16.0
2019-02-15 15:51:34,599 : Text to Image: 8.848, 26.016, 37.604, 19.0
2019-02-15 15:52:17,564 : samples : 448000
2019-02-15 15:52:27,905 : Image to text: 10.92, 29.88, 42.48, 15.0
2019-02-15 15:52:35,121 : Text to Image: 9.088, 25.712, 37.032, 19.0
2019-02-15 15:53:18,482 : samples : 512000
2019-02-15 15:53:31,115 : Image to text: 11.0, 29.86, 41.88, 15.0
2019-02-15 15:53:41,278 : Text to Image: 8.796, 25.432, 36.94, 19.0
2019-02-15 15:54:18,403 : Epoch 6 finished
2019-02-15 15:54:18,880 : Image to text: 30.3, 59.4, 75.0, 4.0
2019-02-15 15:54:19,237 : Text to Image: 22.66, 56.02, 73.62, 4.0
2019-02-15 15:54:19,698 : Image to text: 26.0, 59.6, 74.7, 4.0
2019-02-15 15:54:20,064 : Text to Image: 22.74, 54.32, 71.84, 5.0
2019-02-15 15:54:20,518 : Image to text: 27.4, 60.0, 76.6, 4.0
2019-02-15 15:54:20,881 : Text to Image: 23.14, 56.54, 73.44, 4.0
2019-02-15 15:54:21,340 : Image to text: 27.2, 60.2, 74.3, 4.0
2019-02-15 15:54:21,712 : Text to Image: 22.18, 55.12, 71.54, 5.0
2019-02-15 15:54:22,176 : Image to text: 27.6, 61.6, 75.1, 4.0
2019-02-15 15:54:22,553 : Text to Image: 22.82, 55.62, 71.4, 4.0
2019-02-15 15:54:22,553 : Dev mean Text to Image: 22.708, 55.524, 72.368, 4.4
2019-02-15 15:54:22,554 : Dev mean Image to text: 27.7, 60.16, 75.14, 4.0
2019-02-15 15:54:22,555 : start epoch
2019-02-15 15:55:05,807 : samples : 64000
2019-02-15 15:55:18,358 : Image to text: 10.72, 30.62, 43.3, 14.0
2019-02-15 15:55:28,320 : Text to Image: 9.42, 25.964, 37.768, 19.0
2019-02-15 15:56:13,313 : samples : 128000
2019-02-15 15:56:23,572 : Image to text: 10.6, 28.9, 41.56, 16.0
2019-02-15 15:56:30,760 : Text to Image: 8.784, 25.472, 36.996, 19.0
2019-02-15 15:57:24,638 : samples : 192000
2019-02-15 15:57:37,211 : Image to text: 11.0, 30.18, 43.0, 15.0
2019-02-15 15:57:47,277 : Text to Image: 9.276, 26.384, 38.016, 18.0
2019-02-15 15:58:31,329 : samples : 256000
2019-02-15 15:58:41,633 : Image to text: 10.7, 31.48, 43.7, 14.0
2019-02-15 15:58:48,979 : Text to Image: 9.86, 26.784, 38.608, 18.0
2019-02-15 15:59:32,525 : samples : 320000
2019-02-15 15:59:45,098 : Image to text: 10.5, 29.56, 41.9, 15.0
2019-02-15 15:59:55,178 : Text to Image: 9.256, 26.048, 37.532, 19.0
2019-02-15 16:00:39,519 : samples : 384000
2019-02-15 16:00:49,770 : Image to text: 11.04, 29.78, 42.84, 14.0
2019-02-15 16:00:57,204 : Text to Image: 9.616, 26.688, 38.376, 18.0
2019-02-15 16:01:40,977 : samples : 448000
2019-02-15 16:01:53,558 : Image to text: 10.66, 30.08, 42.86, 15.0
2019-02-15 16:02:03,634 : Text to Image: 9.82, 26.984, 38.92, 17.0
2019-02-15 16:02:47,907 : samples : 512000
2019-02-15 16:02:58,184 : Image to text: 11.16, 30.56, 42.98, 14.0
2019-02-15 16:03:05,590 : Text to Image: 8.984, 26.02, 37.664, 19.0
2019-02-15 16:03:43,000 : Epoch 7 finished
2019-02-15 16:03:43,919 : Image to text: 28.7, 60.3, 76.1, 4.0
2019-02-15 16:03:44,656 : Text to Image: 22.42, 55.08, 73.36, 5.0
2019-02-15 16:03:45,563 : Image to text: 26.0, 58.0, 74.2, 4.0
2019-02-15 16:03:46,333 : Text to Image: 22.1, 54.12, 71.66, 5.0
2019-02-15 16:03:47,282 : Image to text: 27.7, 58.8, 75.6, 4.0
2019-02-15 16:03:48,036 : Text to Image: 22.48, 56.06, 72.82, 4.0
2019-02-15 16:03:48,924 : Image to text: 27.0, 60.3, 75.0, 4.0
2019-02-15 16:03:49,696 : Text to Image: 21.58, 54.54, 72.06, 5.0
2019-02-15 16:03:50,634 : Image to text: 26.6, 59.1, 74.4, 4.0
2019-02-15 16:03:51,416 : Text to Image: 22.1, 54.7, 71.24, 5.0
2019-02-15 16:03:51,417 : Dev mean Text to Image: 22.136000000000003, 54.900000000000006, 72.228, 4.8
2019-02-15 16:03:51,417 : Dev mean Image to text: 27.200000000000003, 59.29999999999999, 75.06, 4.0
2019-02-15 16:03:51,417 : start epoch
2019-02-15 16:04:37,231 : samples : 64000
2019-02-15 16:04:49,341 : Image to text: 10.3, 29.64, 41.96, 15.0
2019-02-15 16:04:56,688 : Text to Image: 9.2, 25.972, 37.364, 19.0
2019-02-15 16:05:39,294 : samples : 128000
2019-02-15 16:05:51,923 : Image to text: 11.06, 30.34, 43.04, 14.0
2019-02-15 16:06:01,966 : Text to Image: 9.692, 27.012, 38.84, 18.0
2019-02-15 16:06:47,918 : samples : 192000
2019-02-15 16:07:00,560 : Image to text: 10.96, 30.02, 42.32, 15.0
2019-02-15 16:07:10,616 : Text to Image: 9.044, 26.192, 38.08, 18.0
2019-02-15 16:07:53,658 : samples : 256000
2019-02-15 16:08:04,062 : Image to text: 10.78, 30.52, 43.64, 14.0
2019-02-15 16:08:14,028 : Text to Image: 9.436, 26.76, 38.504, 18.0
2019-02-15 16:08:58,870 : samples : 320000
2019-02-15 16:09:11,471 : Image to text: 10.66, 29.94, 42.42, 15.0
2019-02-15 16:09:21,507 : Text to Image: 9.6, 27.164, 38.588, 18.0
2019-02-15 16:10:04,914 : samples : 384000
2019-02-15 16:10:15,176 : Image to text: 11.4, 30.46, 43.36, 14.0
2019-02-15 16:10:22,525 : Text to Image: 9.504, 27.084, 38.428, 18.0
2019-02-15 16:11:05,589 : samples : 448000
2019-02-15 16:11:18,421 : Image to text: 10.88, 30.74, 43.52, 14.0
2019-02-15 16:11:26,301 : Text to Image: 9.412, 26.628, 38.328, 18.0
2019-02-15 16:12:08,600 : samples : 512000
2019-02-15 16:12:18,872 : Image to text: 11.22, 30.16, 43.44, 14.0
2019-02-15 16:12:26,283 : Text to Image: 9.484, 26.612, 38.344, 18.0
2019-02-15 16:13:02,388 : Epoch 8 finished
2019-02-15 16:13:02,851 : Image to text: 28.7, 59.8, 75.6, 4.0
2019-02-15 16:13:03,213 : Text to Image: 22.02, 55.66, 73.12, 4.0
2019-02-15 16:13:03,673 : Image to text: 27.5, 59.6, 75.3, 4.0
2019-02-15 16:13:04,046 : Text to Image: 22.64, 55.3, 71.42, 5.0
2019-02-15 16:13:04,510 : Image to text: 26.1, 59.1, 75.5, 4.0
2019-02-15 16:13:04,875 : Text to Image: 22.7, 56.2, 72.86, 4.0
2019-02-15 16:13:05,341 : Image to text: 28.4, 60.1, 75.5, 4.0
2019-02-15 16:13:05,710 : Text to Image: 22.44, 54.62, 71.76, 5.0
2019-02-15 16:13:06,174 : Image to text: 27.8, 58.7, 74.1, 4.0
2019-02-15 16:13:06,549 : Text to Image: 22.38, 55.0, 71.22, 4.0
2019-02-15 16:13:06,549 : Dev mean Text to Image: 22.436, 55.356, 72.07600000000001, 4.4
2019-02-15 16:13:06,549 : Dev mean Image to text: 27.700000000000003, 59.46, 75.2, 4.0
2019-02-15 16:13:06,549 : start epoch
2019-02-15 16:13:53,466 : samples : 64000
2019-02-15 16:14:05,332 : Image to text: 11.42, 30.84, 43.76, 14.0
2019-02-15 16:14:13,991 : Text to Image: 9.576, 26.92, 38.684, 18.0
2019-02-15 16:15:00,002 : samples : 128000
2019-02-15 16:15:10,260 : Image to text: 11.5, 30.84, 43.98, 14.0
2019-02-15 16:15:17,621 : Text to Image: 9.04, 26.004, 37.628, 18.0
2019-02-15 16:16:00,093 : samples : 192000
2019-02-15 16:16:10,396 : Image to text: 11.48, 31.12, 44.22, 14.0
2019-02-15 16:16:17,755 : Text to Image: 9.456, 26.884, 38.68, 17.0
2019-02-15 16:17:02,342 : samples : 256000
2019-02-15 16:17:15,050 : Image to text: 11.2, 30.42, 43.14, 14.0
2019-02-15 16:17:22,601 : Text to Image: 9.504, 26.82, 38.8, 18.0
2019-02-15 16:18:05,974 : samples : 320000
2019-02-15 16:18:16,762 : Image to text: 11.28, 30.82, 43.18, 15.0
2019-02-15 16:18:24,254 : Text to Image: 9.304, 26.896, 38.696, 18.0
2019-02-15 16:19:08,238 : samples : 384000
2019-02-15 16:19:19,185 : Image to text: 11.14, 30.8, 43.66, 14.0
2019-02-15 16:19:29,135 : Text to Image: 9.472, 27.236, 39.144, 17.0
2019-02-15 16:20:11,622 : samples : 448000
2019-02-15 16:20:22,015 : Image to text: 11.48, 31.28, 43.72, 14.0
2019-02-15 16:20:29,138 : Text to Image: 9.644, 27.348, 38.944, 18.0
2019-02-15 16:21:12,115 : samples : 512000
2019-02-15 16:21:22,345 : Image to text: 10.9, 30.42, 43.28, 14.0
2019-02-15 16:21:29,664 : Text to Image: 9.352, 26.708, 38.74, 18.0
2019-02-15 16:22:07,383 : Epoch 9 finished
2019-02-15 16:22:08,225 : Image to text: 30.4, 60.1, 76.0, 3.0
2019-02-15 16:22:09,013 : Text to Image: 23.4, 57.76, 74.4, 4.0
2019-02-15 16:22:09,965 : Image to text: 27.8, 61.1, 76.8, 4.0
2019-02-15 16:22:10,720 : Text to Image: 23.52, 56.76, 73.78, 4.0
2019-02-15 16:22:11,613 : Image to text: 28.3, 60.9, 76.1, 4.0
2019-02-15 16:22:12,331 : Text to Image: 24.72, 57.8, 74.76, 4.0
2019-02-15 16:22:13,248 : Image to text: 27.9, 60.9, 75.4, 4.0
2019-02-15 16:22:14,022 : Text to Image: 23.98, 56.88, 74.3, 4.0
2019-02-15 16:22:14,980 : Image to text: 26.5, 62.5, 75.2, 3.0
2019-02-15 16:22:15,765 : Text to Image: 24.96, 57.18, 72.52, 4.0
2019-02-15 16:22:15,765 : Dev mean Text to Image: 24.116, 57.276, 73.95200000000001, 4.0
2019-02-15 16:22:15,765 : Dev mean Image to text: 28.180000000000003, 61.1, 75.9, 3.6
2019-02-15 16:22:15,766 : start epoch
2019-02-15 16:23:00,755 : samples : 64000
2019-02-15 16:23:13,286 : Image to text: 11.34, 31.26, 43.56, 14.0
2019-02-15 16:23:23,262 : Text to Image: 9.604, 27.044, 38.956, 18.0
2019-02-15 16:24:07,945 : samples : 128000
2019-02-15 16:24:20,511 : Image to text: 11.28, 31.3, 44.02, 14.0
2019-02-15 16:24:30,513 : Text to Image: 10.316, 27.8, 39.472, 17.0
2019-02-15 16:25:15,631 : samples : 192000
2019-02-15 16:25:28,200 : Image to text: 11.34, 30.94, 43.3, 14.0
2019-02-15 16:25:38,142 : Text to Image: 9.732, 27.36, 38.92, 17.0
2019-02-15 16:26:23,006 : samples : 256000
2019-02-15 16:26:35,618 : Image to text: 10.7, 30.88, 44.3, 14.0
2019-02-15 16:26:45,626 : Text to Image: 9.748, 27.108, 38.836, 17.0
2019-02-15 16:27:30,085 : samples : 320000
2019-02-15 16:27:42,743 : Image to text: 10.66, 30.36, 42.32, 15.0
2019-02-15 16:27:52,819 : Text to Image: 9.332, 26.804, 38.692, 18.0
2019-02-15 16:28:37,679 : samples : 384000
2019-02-15 16:28:50,301 : Image to text: 11.28, 32.02, 43.78, 14.0
2019-02-15 16:29:00,327 : Text to Image: 9.716, 27.116, 39.216, 17.0
2019-02-15 16:29:44,632 : samples : 448000
2019-02-15 16:29:57,286 : Image to text: 11.34, 32.18, 43.7, 14.0
2019-02-15 16:30:07,319 : Text to Image: 9.992, 27.512, 39.064, 18.0
2019-02-15 16:30:54,734 : samples : 512000
2019-02-15 16:31:08,038 : Image to text: 11.62, 30.88, 44.4, 14.0
2019-02-15 16:31:18,332 : Text to Image: 9.844, 27.296, 39.536, 17.0
2019-02-15 16:31:55,519 : Epoch 10 finished
2019-02-15 16:31:56,467 : Image to text: 30.8, 61.4, 76.1, 3.0
2019-02-15 16:31:57,225 : Text to Image: 23.8, 58.56, 75.12, 4.0
2019-02-15 16:31:58,207 : Image to text: 29.8, 60.9, 75.7, 4.0
2019-02-15 16:31:58,968 : Text to Image: 23.08, 57.04, 74.18, 4.0
2019-02-15 16:31:59,917 : Image to text: 28.0, 62.5, 78.9, 3.0
2019-02-15 16:32:00,669 : Text to Image: 24.38, 58.54, 75.0, 4.0
2019-02-15 16:32:01,824 : Image to text: 28.3, 63.0, 77.5, 3.0
2019-02-15 16:32:02,566 : Text to Image: 24.0, 56.52, 73.62, 4.0
2019-02-15 16:32:03,522 : Image to text: 26.8, 60.1, 76.4, 4.0
2019-02-15 16:32:04,290 : Text to Image: 23.82, 57.5, 73.84, 4.0
2019-02-15 16:32:04,290 : Dev mean Text to Image: 23.816, 57.632, 74.352, 4.0
2019-02-15 16:32:04,290 : Dev mean Image to text: 28.74, 61.58, 76.92, 3.4000000000000004
2019-02-15 16:32:04,290 : start epoch
2019-02-15 16:32:49,734 : samples : 64000
2019-02-15 16:33:02,470 : Image to text: 11.48, 31.16, 44.32, 14.0
2019-02-15 16:33:12,582 : Text to Image: 9.512, 26.928, 38.348, 18.0
2019-02-15 16:33:55,288 : samples : 128000
2019-02-15 16:34:05,685 : Image to text: 11.08, 31.02, 43.6, 14.0
2019-02-15 16:34:13,117 : Text to Image: 9.416, 26.6, 38.38, 18.0
2019-02-15 16:34:55,169 : samples : 192000
2019-02-15 16:35:07,348 : Image to text: 11.44, 32.02, 44.72, 13.0
2019-02-15 16:35:17,730 : Text to Image: 9.796, 27.264, 39.064, 18.0
2019-02-15 16:36:03,166 : samples : 256000
2019-02-15 16:36:16,037 : Image to text: 11.5, 31.08, 44.12, 14.0
2019-02-15 16:36:26,529 : Text to Image: 9.592, 27.012, 38.732, 18.0
2019-02-15 16:37:11,589 : samples : 320000
2019-02-15 16:37:24,491 : Image to text: 11.4, 32.06, 44.16, 14.0
2019-02-15 16:37:34,913 : Text to Image: 9.492, 26.92, 39.0, 18.0
2019-02-15 16:38:20,265 : samples : 384000
2019-02-15 16:38:33,115 : Image to text: 11.04, 30.96, 43.76, 14.0
2019-02-15 16:38:43,601 : Text to Image: 9.5, 26.824, 38.684, 18.0
2019-02-15 16:39:29,368 : samples : 448000
2019-02-15 16:39:42,300 : Image to text: 11.6, 30.94, 43.76, 14.0
2019-02-15 16:39:52,751 : Text to Image: 9.732, 27.732, 39.328, 17.0
2019-02-15 16:40:38,306 : samples : 512000
2019-02-15 16:40:51,164 : Image to text: 11.8, 31.38, 44.76, 13.0
2019-02-15 16:41:01,590 : Text to Image: 9.7, 27.272, 38.932, 18.0
2019-02-15 16:41:40,275 : Epoch 11 finished
2019-02-15 16:41:41,346 : Image to text: 30.0, 61.3, 76.6, 4.0
2019-02-15 16:41:42,271 : Text to Image: 22.94, 56.78, 73.46, 4.0
2019-02-15 16:41:43,447 : Image to text: 29.0, 62.2, 75.7, 3.0
2019-02-15 16:41:44,416 : Text to Image: 22.58, 55.24, 71.98, 4.0
2019-02-15 16:41:45,499 : Image to text: 27.0, 61.7, 77.2, 4.0
2019-02-15 16:41:46,369 : Text to Image: 23.02, 55.84, 72.44, 4.0
2019-02-15 16:41:47,397 : Image to text: 29.1, 60.9, 76.2, 4.0
2019-02-15 16:41:48,296 : Text to Image: 22.16, 54.96, 72.6, 5.0
2019-02-15 16:41:49,402 : Image to text: 25.6, 60.4, 76.0, 4.0
2019-02-15 16:41:50,323 : Text to Image: 23.38, 55.84, 71.6, 4.0
2019-02-15 16:41:50,324 : Dev mean Text to Image: 22.816000000000003, 55.732000000000006, 72.416, 4.2
2019-02-15 16:41:50,324 : Dev mean Image to text: 28.140000000000004, 61.300000000000004, 76.34, 3.8
2019-02-15 16:41:50,324 : start epoch
2019-02-15 16:42:36,435 : samples : 64000
2019-02-15 16:42:49,255 : Image to text: 11.62, 32.98, 45.22, 13.0
2019-02-15 16:42:59,751 : Text to Image: 9.76, 27.428, 39.428, 17.0
2019-02-15 16:43:42,437 : samples : 128000
2019-02-15 16:43:52,704 : Image to text: 10.94, 31.36, 43.2, 14.0
2019-02-15 16:44:00,091 : Text to Image: 9.772, 26.856, 38.772, 18.0
2019-02-15 16:44:42,907 : samples : 192000
2019-02-15 16:44:53,176 : Image to text: 11.28, 31.46, 44.04, 14.0
2019-02-15 16:45:00,565 : Text to Image: 9.684, 27.12, 38.804, 18.0
2019-02-15 16:45:43,899 : samples : 256000
2019-02-15 16:45:54,156 : Image to text: 11.08, 31.74, 43.6, 14.0
2019-02-15 16:46:01,597 : Text to Image: 9.676, 27.064, 38.98, 17.0
2019-02-15 16:46:44,502 : samples : 320000
2019-02-15 16:46:54,770 : Image to text: 10.72, 30.44, 43.74, 14.0
2019-02-15 16:47:02,274 : Text to Image: 9.428, 27.352, 39.032, 18.0
2019-02-15 16:47:49,723 : samples : 384000
2019-02-15 16:48:02,525 : Image to text: 12.0, 32.48, 45.36, 13.0
2019-02-15 16:48:11,296 : Text to Image: 10.06, 27.892, 39.256, 17.0
2019-02-15 16:48:54,861 : samples : 448000
2019-02-15 16:49:05,121 : Image to text: 11.5, 31.58, 44.68, 14.0
2019-02-15 16:49:12,550 : Text to Image: 9.96, 27.612, 39.804, 17.0
2019-02-15 16:49:55,481 : samples : 512000
2019-02-15 16:50:05,763 : Image to text: 11.84, 31.74, 45.02, 13.0
2019-02-15 16:50:13,178 : Text to Image: 9.92, 27.932, 39.944, 17.0
2019-02-15 16:50:50,157 : Epoch 12 finished
2019-02-15 16:50:50,594 : Image to text: 30.8, 61.1, 75.6, 4.0
2019-02-15 16:50:50,940 : Text to Image: 23.32, 58.4, 74.86, 4.0
2019-02-15 16:50:51,379 : Image to text: 29.9, 63.0, 76.7, 3.0
2019-02-15 16:50:51,718 : Text to Image: 24.22, 57.62, 73.96, 4.0
2019-02-15 16:50:52,168 : Image to text: 27.5, 61.7, 78.5, 4.0
2019-02-15 16:50:52,530 : Text to Image: 24.06, 58.4, 74.52, 4.0
2019-02-15 16:50:52,979 : Image to text: 27.2, 61.9, 75.2, 3.0
2019-02-15 16:50:53,323 : Text to Image: 23.26, 56.52, 73.82, 4.0
2019-02-15 16:50:53,764 : Image to text: 27.3, 60.2, 75.7, 4.0
2019-02-15 16:50:54,103 : Text to Image: 24.2, 57.52, 73.24, 4.0
2019-02-15 16:50:54,103 : Dev mean Text to Image: 23.811999999999998, 57.69200000000001, 74.07999999999998, 4.0
2019-02-15 16:50:54,103 : Dev mean Image to text: 28.54, 61.57999999999999, 76.34, 3.6000000000000005
2019-02-15 16:50:58,219 : 
Test scores | Image to text:             28.880000000000003, 61.800000000000004, 76.78, 3.4
2019-02-15 16:50:58,219 : Test scores | Text to image:             24.036, 57.284000000000006, 74.016, 4.0

2019-02-15 16:50:58,312 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 16:50:58,528 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 16:50:59,198 : loading BERT model bert-base-uncased
2019-02-15 16:50:59,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:50:59,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:50:59,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp73_vbrgi
2019-02-15 16:51:01,705 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:51:03,165 : Computing embeddings for train/dev/test
2019-02-15 16:52:36,736 : Computed embeddings
2019-02-15 16:52:36,736 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:53:28,393 : [('reg:1e-05', 82.13), ('reg:0.0001', 82.54), ('reg:0.001', 79.82), ('reg:0.01', 64.53)]
2019-02-15 16:53:28,394 : Validation : best param found is reg = 0.0001 with score             82.54
2019-02-15 16:53:28,394 : Evaluating...
2019-02-15 16:53:40,848 : 
Dev acc : 82.5 Test acc : 82.4 for LENGTH classification

2019-02-15 16:53:40,849 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 16:53:41,239 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 16:53:41,288 : loading BERT model bert-base-uncased
2019-02-15 16:53:41,288 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:53:41,321 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:53:41,321 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvocpqzav
2019-02-15 16:53:43,801 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:53:45,269 : Computing embeddings for train/dev/test
2019-02-15 16:55:14,626 : Computed embeddings
2019-02-15 16:55:14,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:56:10,627 : [('reg:1e-05', 56.42), ('reg:0.0001', 12.92), ('reg:0.001', 0.42), ('reg:0.01', 0.19)]
2019-02-15 16:56:10,627 : Validation : best param found is reg = 1e-05 with score             56.42
2019-02-15 16:56:10,627 : Evaluating...
2019-02-15 16:56:31,558 : 
Dev acc : 56.4 Test acc : 55.9 for WORDCONTENT classification

2019-02-15 16:56:31,559 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 16:56:31,905 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 16:56:31,985 : loading BERT model bert-base-uncased
2019-02-15 16:56:31,985 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:56:32,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:56:32,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmputh6bbzx
2019-02-15 16:56:34,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:56:35,965 : Computing embeddings for train/dev/test
2019-02-15 16:57:59,318 : Computed embeddings
2019-02-15 16:57:59,318 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:58:43,735 : [('reg:1e-05', 23.0), ('reg:0.0001', 22.93), ('reg:0.001', 21.73), ('reg:0.01', 18.98)]
2019-02-15 16:58:43,735 : Validation : best param found is reg = 1e-05 with score             23.0
2019-02-15 16:58:43,735 : Evaluating...
2019-02-15 16:58:57,812 : 
Dev acc : 23.0 Test acc : 22.6 for DEPTH classification

2019-02-15 16:58:57,813 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 16:58:58,230 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 16:58:58,295 : loading BERT model bert-base-uncased
2019-02-15 16:58:58,295 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:58:58,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:58:58,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1dtxth2i
2019-02-15 16:59:00,900 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:59:02,358 : Computing embeddings for train/dev/test
2019-02-15 17:00:20,620 : Computed embeddings
2019-02-15 17:00:20,620 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:01:14,224 : [('reg:1e-05', 43.53), ('reg:0.0001', 44.54), ('reg:0.001', 44.43), ('reg:0.01', 24.0)]
2019-02-15 17:01:14,225 : Validation : best param found is reg = 0.0001 with score             44.54
2019-02-15 17:01:14,225 : Evaluating...
2019-02-15 17:01:26,837 : 
Dev acc : 44.5 Test acc : 44.3 for TOPCONSTITUENTS classification

2019-02-15 17:01:26,838 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 17:01:27,427 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 17:01:27,498 : loading BERT model bert-base-uncased
2019-02-15 17:01:27,498 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:01:27,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:01:27,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3jb98zcn
2019-02-15 17:01:30,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:01:31,497 : Computing embeddings for train/dev/test
2019-02-15 17:02:55,552 : Computed embeddings
2019-02-15 17:02:55,553 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:03:42,830 : [('reg:1e-05', 51.79), ('reg:0.0001', 51.79), ('reg:0.001', 50.18), ('reg:0.01', 50.0)]
2019-02-15 17:03:42,830 : Validation : best param found is reg = 1e-05 with score             51.79
2019-02-15 17:03:42,831 : Evaluating...
2019-02-15 17:03:54,667 : 
Dev acc : 51.8 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-15 17:03:54,668 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 17:03:55,084 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 17:03:55,159 : loading BERT model bert-base-uncased
2019-02-15 17:03:55,159 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:03:55,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:03:55,298 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6vy2_g6q
2019-02-15 17:03:57,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:03:59,270 : Computing embeddings for train/dev/test
2019-02-15 17:05:26,404 : Computed embeddings
2019-02-15 17:05:26,404 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:06:28,103 : [('reg:1e-05', 74.41), ('reg:0.0001', 74.51), ('reg:0.001', 76.33), ('reg:0.01', 72.98)]
2019-02-15 17:06:28,104 : Validation : best param found is reg = 0.001 with score             76.33
2019-02-15 17:06:28,104 : Evaluating...
2019-02-15 17:06:43,932 : 
Dev acc : 76.3 Test acc : 73.5 for TENSE classification

2019-02-15 17:06:43,933 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 17:06:44,563 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 17:06:44,631 : loading BERT model bert-base-uncased
2019-02-15 17:06:44,631 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:06:44,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:06:44,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2kaygd_8
2019-02-15 17:06:47,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:06:48,617 : Computing embeddings for train/dev/test
2019-02-15 17:08:15,286 : Computed embeddings
2019-02-15 17:08:15,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:09:05,116 : [('reg:1e-05', 72.33), ('reg:0.0001', 72.29), ('reg:0.001', 72.08), ('reg:0.01', 68.78)]
2019-02-15 17:09:05,116 : Validation : best param found is reg = 1e-05 with score             72.33
2019-02-15 17:09:05,116 : Evaluating...
2019-02-15 17:09:18,648 : 
Dev acc : 72.3 Test acc : 71.1 for SUBJNUMBER classification

2019-02-15 17:09:18,649 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 17:09:19,301 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 17:09:19,376 : loading BERT model bert-base-uncased
2019-02-15 17:09:19,376 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:09:19,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:09:19,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphhr40d7e
2019-02-15 17:09:21,912 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:09:23,410 : Computing embeddings for train/dev/test
2019-02-15 17:10:49,233 : Computed embeddings
2019-02-15 17:10:49,233 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:11:42,174 : [('reg:1e-05', 62.0), ('reg:0.0001', 61.64), ('reg:0.001', 67.98), ('reg:0.01', 56.01)]
2019-02-15 17:11:42,174 : Validation : best param found is reg = 0.001 with score             67.98
2019-02-15 17:11:42,174 : Evaluating...
2019-02-15 17:11:54,578 : 
Dev acc : 68.0 Test acc : 68.5 for OBJNUMBER classification

2019-02-15 17:11:54,579 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 17:11:55,031 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 17:11:55,108 : loading BERT model bert-base-uncased
2019-02-15 17:11:55,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:11:55,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:11:55,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxm1yg5os
2019-02-15 17:11:57,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:11:59,124 : Computing embeddings for train/dev/test
2019-02-15 17:13:37,963 : Computed embeddings
2019-02-15 17:13:37,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:14:17,954 : [('reg:1e-05', 50.23), ('reg:0.0001', 50.24), ('reg:0.001', 50.19), ('reg:0.01', 50.31)]
2019-02-15 17:14:17,955 : Validation : best param found is reg = 0.01 with score             50.31
2019-02-15 17:14:17,955 : Evaluating...
2019-02-15 17:14:28,402 : 
Dev acc : 50.3 Test acc : 49.8 for ODDMANOUT classification

2019-02-15 17:14:28,403 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 17:14:28,852 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 17:14:28,933 : loading BERT model bert-base-uncased
2019-02-15 17:14:28,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:14:29,071 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:14:29,071 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk0bd1a_6
2019-02-15 17:14:31,593 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:14:33,053 : Computing embeddings for train/dev/test
2019-02-15 17:16:11,180 : Computed embeddings
2019-02-15 17:16:11,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:16:45,091 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 17:16:45,092 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 17:16:45,092 : Evaluating...
2019-02-15 17:16:52,835 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-02-15 17:16:52,837 : total results: {'STS12': {'MSRpar': {'pearson': (0.39231158157851875, 5.274625528025561e-29), 'spearman': SpearmanrResult(correlation=0.41133509330874707, pvalue=5.498056061923203e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7107481932308285, 2.1948997187354176e-116), 'spearman': SpearmanrResult(correlation=0.7136955104174543, pvalue=9.030146640820396e-118), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4897457082726784, 4.6153305190058883e-29), 'spearman': SpearmanrResult(correlation=0.5860508017604955, pvalue=1.113973036028007e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6184714853260592, 2.338744754187602e-80), 'spearman': SpearmanrResult(correlation=0.6561186784128675, pvalue=1.5999812174613138e-93), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4909052252305271, 1.37174231664998e-25), 'spearman': SpearmanrResult(correlation=0.4205804010956017, pvalue=1.5563522533347399e-18), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5404364387277225, 'wmean': 0.5507763545899917}, 'spearman': {'mean': 0.5575560969990332, 'wmean': 0.5703574194496506}}}, 'STS13': {'FNWN': {'pearson': (0.2443752076325935, 0.0007018781033651067), 'spearman': SpearmanrResult(correlation=0.23928715954847454, pvalue=0.0009127017679584543), 'nsamples': 189}, 'headlines': {'pearson': (0.6611179772258328, 2.0499908212567183e-95), 'spearman': SpearmanrResult(correlation=0.6567408395182543, pvalue=9.345034613667561e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.5889810947294624, 1.1054796413677528e-53), 'spearman': SpearmanrResult(correlation=0.5898880190531226, pvalue=6.982570013511401e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4981580931959629, 'wmean': 0.5816291942034421}, 'spearman': {'mean': 0.4953053393732838, 'wmean': 0.5791387209881028}}}, 'STS14': {'deft-forum': {'pearson': (0.36045651660363986, 2.9732327689101337e-15), 'spearman': SpearmanrResult(correlation=0.3567293793570378, pvalue=5.970503042233836e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.6842009741600031, 9.387601617302659e-43), 'spearman': SpearmanrResult(correlation=0.6511114862633486, pvalue=1.4383490238246118e-37), 'nsamples': 300}, 'headlines': {'pearson': (0.6245594771673342, 2.3007982677098274e-82), 'spearman': SpearmanrResult(correlation=0.6106461229923437, pvalue=7.673253966081052e-78), 'nsamples': 750}, 'images': {'pearson': (0.6963990208220597, 7.018347547260832e-110), 'spearman': SpearmanrResult(correlation=0.6919959875925529, pvalue=5.8283485420049815e-108), 'nsamples': 750}, 'OnWN': {'pearson': (0.6504567223997998, 2.0143979016798115e-91), 'spearman': SpearmanrResult(correlation=0.680159885748345, pvalue=5.722796515125585e-103), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5959294164627614, 2.6960387912890152e-73), 'spearman': SpearmanrResult(correlation=0.5888364709087633, pvalue=3.4581263114371886e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6020003546025997, 'wmean': 0.611459787295628}, 'spearman': {'mean': 0.5965798888103985, 'wmean': 0.6092241378723134}}}, 'STS15': {'answers-forums': {'pearson': (0.5415993896194873, 5.7317499662483094e-30), 'spearman': SpearmanrResult(correlation=0.5190650625420267, pvalue=2.955160375324995e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6903602108687453, 2.9499038399067926e-107), 'spearman': SpearmanrResult(correlation=0.6996805484472093, pvalue=2.4713215748660936e-111), 'nsamples': 750}, 'belief': {'pearson': (0.5498365987491456, 5.185490373985195e-31), 'spearman': SpearmanrResult(correlation=0.5968070579954796, pvalue=1.4881003551127607e-37), 'nsamples': 375}, 'headlines': {'pearson': (0.6512991316747175, 9.875186961608847e-92), 'spearman': SpearmanrResult(correlation=0.6541133643998895, pvalue=8.97564872468903e-93), 'nsamples': 750}, 'images': {'pearson': (0.7735549847298093, 2.511545613079099e-150), 'spearman': SpearmanrResult(correlation=0.7846970457649921, pvalue=1.6428659469558314e-157), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.641330063128381, 'wmean': 0.6652330803643971}, 'spearman': {'mean': 0.6508726158299194, 'wmean': 0.674106754720211}}}, 'STS16': {'answer-answer': {'pearson': (0.42731274377393036, 1.0708706668731246e-12), 'spearman': SpearmanrResult(correlation=0.4269161713871067, pvalue=1.1292706557556726e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6522747329842059, 1.4595974600703958e-31), 'spearman': SpearmanrResult(correlation=0.6670536830455703, pvalue=2.0083125371534743e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6855602956462044, 2.834374749923506e-33), 'spearman': SpearmanrResult(correlation=0.691332245009346, pvalue=5.021953185071478e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7945551784965781, 2.3622190130892694e-54), 'spearman': SpearmanrResult(correlation=0.8280797876885901, pvalue=9.261298717895888e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.6301465046425283, 1.5755139317840046e-24), 'spearman': SpearmanrResult(correlation=0.631724833191767, pvalue=1.1159144029650878e-24), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6379698911086894, 'wmean': 0.6359230998766662}, 'spearman': {'mean': 0.649021344064476, 'wmean': 0.6472356233523753}}}, 'MR': {'devacc': 51.56, 'acc': 50.32, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 66.16, 'acc': 65.11, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.07, 'acc': 84.82, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 52.55, 'acc': 55.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 69.5, 'acc': 68.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 31.7, 'acc': 32.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 53.27, 'acc': 65.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.57, 'acc': 69.39, 'f1': 80.97, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.4, 'acc': 74.93, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.788086601269239, 'pearson': 0.7968666730442842, 'spearman': 0.7240256291999169, 'mse': 0.3764490400607171, 'yhat': array([2.89386674, 4.46304165, 1.5904312 , ..., 3.1766855 , 4.6135616 ,        4.69510072]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6965712054703311, 'pearson': 0.6436464192455066, 'spearman': 0.6408201822625577, 'mse': 1.4333519342854175, 'yhat': array([1.82179285, 1.77609072, 2.57698044, ..., 3.34350985, 3.75869264,        3.00943342]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 54.79, 'acc': 55.22, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 323.04, 'acc': [(28.880000000000003, 61.800000000000004, 76.78, 3.4), (24.036, 57.284000000000006, 74.016, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 82.54, 'acc': 82.37, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 56.42, 'acc': 55.9, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 23.0, 'acc': 22.65, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 44.54, 'acc': 44.28, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 51.79, 'acc': 50.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 76.33, 'acc': 73.48, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 72.33, 'acc': 71.06, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 67.98, 'acc': 68.47, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.31, 'acc': 49.75, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 17:16:52,837 : STS12 p=0.5508, STS12 s=0.5704, STS13 p=0.5816, STS13 s=0.5791, STS14 p=0.6115, STS14 s=0.6092, STS15 p=0.6652, STS15 s=0.6741, STS 16 p=0.6359, STS16 s=0.6472, STS B p=0.6436, STS B s=0.6408, STS B m=1.4334, SICK-R p=0.7969, SICK-R s=0.7240, SICK-P m=0.3764
2019-02-15 17:16:52,837 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 17:16:52,837 : 0.5508,0.5704,0.5816,0.5791,0.6115,0.6092,0.6652,0.6741,0.6359,0.6472,0.6436,0.6408,1.4334,0.7969,0.7240,0.3764
2019-02-15 17:16:52,837 : MR=50.32, CR=65.11, SUBJ=55.43, MPQA=84.82, SST-B=68.81, SST-F=32.67, TREC=65.40, SICK-E=74.93, SNLI=55.22, MRPC=69.39, MRPC f=80.97
2019-02-15 17:16:52,838 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 17:16:52,838 : 50.32,65.11,55.43,84.82,68.81,32.67,65.40,74.93,55.22,69.39,80.97
2019-02-15 17:16:52,838 : COCO r1i2t=28.88, COCO r5i2t=61.80, COCO r10i2t=76.78, COCO medr_i2t=3.40, COCO r1t2i=24.04, COCO r5t2i=57.28, COCO r10t2i=74.02, COCO medr_t2i=4.00
2019-02-15 17:16:52,838 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 17:16:52,838 : 28.88,61.80,76.78,3.40,24.04,57.28,74.02,4.00
2019-02-15 17:16:52,838 : SentLen=82.37, WC=55.90, TreeDepth=22.65, TopConst=44.28, BShift=50.01, Tense=73.48, SubjNum=71.06, ObjNum=68.47, SOMO=49.75, CoordInv=50.00, average=56.80
2019-02-15 17:16:52,838 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 17:16:52,838 : 82.37,55.90,22.65,44.28,50.01,73.48,71.06,68.47,49.75,50.00,56.80
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,838 : layer 1
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,838 : ********************************************************************************
2019-02-15 17:16:52,927 : ***** Transfer task : STS12 *****


2019-02-15 17:16:52,940 : loading BERT model bert-base-uncased
2019-02-15 17:16:52,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:16:52,957 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:16:52,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfadd2skt
2019-02-15 17:16:55,432 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:16:58,641 : MSRpar : pearson = 0.3807, spearman = 0.4126
2019-02-15 17:16:59,390 : MSRvid : pearson = 0.7642, spearman = 0.7660
2019-02-15 17:17:00,027 : SMTeuroparl : pearson = 0.5010, spearman = 0.5961
2019-02-15 17:17:01,173 : surprise.OnWN : pearson = 0.6267, spearman = 0.6508
2019-02-15 17:17:01,812 : surprise.SMTnews : pearson = 0.5402, spearman = 0.4553
2019-02-15 17:17:01,812 : ALL (weighted average) : Pearson = 0.5708,             Spearman = 0.5879
2019-02-15 17:17:01,812 : ALL (average) : Pearson = 0.5626,             Spearman = 0.5761

2019-02-15 17:17:01,812 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 17:17:01,823 : loading BERT model bert-base-uncased
2019-02-15 17:17:01,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:17:01,844 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:17:01,844 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp15oks285
2019-02-15 17:17:04,300 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:17:06,340 : FNWN : pearson = 0.2299, spearman = 0.2380
2019-02-15 17:17:07,207 : headlines : pearson = 0.6854, spearman = 0.6782
2019-02-15 17:17:07,869 : OnWN : pearson = 0.6428, spearman = 0.6410
2019-02-15 17:17:07,870 : ALL (weighted average) : Pearson = 0.6121,             Spearman = 0.6088
2019-02-15 17:17:07,870 : ALL (average) : Pearson = 0.5194,             Spearman = 0.5191

2019-02-15 17:17:07,870 : ***** Transfer task : STS14 *****


2019-02-15 17:17:07,885 : loading BERT model bert-base-uncased
2019-02-15 17:17:07,885 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:17:07,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:17:07,904 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr78dwhp1
2019-02-15 17:17:10,381 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:17:12,519 : deft-forum : pearson = 0.3957, spearman = 0.4066
2019-02-15 17:17:13,244 : deft-news : pearson = 0.7135, spearman = 0.6762
2019-02-15 17:17:14,225 : headlines : pearson = 0.6616, spearman = 0.6395
2019-02-15 17:17:15,173 : images : pearson = 0.7106, spearman = 0.6974
2019-02-15 17:17:16,138 : OnWN : pearson = 0.7054, spearman = 0.7297
2019-02-15 17:17:17,421 : tweet-news : pearson = 0.6203, spearman = 0.5995
2019-02-15 17:17:17,422 : ALL (weighted average) : Pearson = 0.6442,             Spearman = 0.6361
2019-02-15 17:17:17,422 : ALL (average) : Pearson = 0.6345,             Spearman = 0.6248

2019-02-15 17:17:17,422 : ***** Transfer task : STS15 *****


2019-02-15 17:17:17,489 : loading BERT model bert-base-uncased
2019-02-15 17:17:17,489 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:17:17,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:17:17,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpos_uj6hu
2019-02-15 17:17:20,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:17:22,355 : answers-forums : pearson = 0.5281, spearman = 0.4979
2019-02-15 17:17:23,287 : answers-students : pearson = 0.7205, spearman = 0.7333
2019-02-15 17:17:24,141 : belief : pearson = 0.5512, spearman = 0.6034
2019-02-15 17:17:25,138 : headlines : pearson = 0.6862, spearman = 0.6878
2019-02-15 17:17:26,122 : images : pearson = 0.7865, spearman = 0.7969
2019-02-15 17:17:26,122 : ALL (weighted average) : Pearson = 0.6832,             Spearman = 0.6922
2019-02-15 17:17:26,123 : ALL (average) : Pearson = 0.6545,             Spearman = 0.6639

2019-02-15 17:17:26,123 : ***** Transfer task : STS16 *****


2019-02-15 17:17:26,198 : loading BERT model bert-base-uncased
2019-02-15 17:17:26,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:17:26,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:17:26,217 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9cb4l6qa
2019-02-15 17:17:28,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:17:30,534 : answer-answer : pearson = 0.4181, spearman = 0.4365
2019-02-15 17:17:30,852 : headlines : pearson = 0.6912, spearman = 0.7036
2019-02-15 17:17:31,251 : plagiarism : pearson = 0.7137, spearman = 0.7265
2019-02-15 17:17:31,879 : postediting : pearson = 0.7915, spearman = 0.8410
2019-02-15 17:17:32,157 : question-question : pearson = 0.5770, spearman = 0.5761
2019-02-15 17:17:32,157 : ALL (weighted average) : Pearson = 0.6376,             Spearman = 0.6566
2019-02-15 17:17:32,157 : ALL (average) : Pearson = 0.6383,             Spearman = 0.6567

2019-02-15 17:17:32,157 : ***** Transfer task : MR *****


2019-02-15 17:17:32,177 : loading BERT model bert-base-uncased
2019-02-15 17:17:32,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:17:32,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:17:32,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj5nrjmdv
2019-02-15 17:17:34,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:17:36,189 : Generating sentence embeddings
2019-02-15 17:17:49,936 : Generated sentence embeddings
2019-02-15 17:17:49,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 17:18:06,085 : Best param found at split 1: l2reg = 0.01                 with score 59.37
2019-02-15 17:18:26,385 : Best param found at split 2: l2reg = 0.001                 with score 57.25
2019-02-15 17:18:48,052 : Best param found at split 3: l2reg = 0.0001                 with score 59.25
2019-02-15 17:19:05,564 : Best param found at split 4: l2reg = 0.001                 with score 56.15
2019-02-15 17:19:25,730 : Best param found at split 5: l2reg = 1e-05                 with score 57.51
2019-02-15 17:19:27,017 : Dev acc : 57.91 Test acc : 56.86

2019-02-15 17:19:27,018 : ***** Transfer task : CR *****


2019-02-15 17:19:27,030 : loading BERT model bert-base-uncased
2019-02-15 17:19:27,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:19:27,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:19:27,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprubdxq4d
2019-02-15 17:19:29,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:19:31,025 : Generating sentence embeddings
2019-02-15 17:19:34,709 : Generated sentence embeddings
2019-02-15 17:19:34,710 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 17:19:41,130 : Best param found at split 1: l2reg = 0.01                 with score 67.7
2019-02-15 17:19:45,259 : Best param found at split 2: l2reg = 0.01                 with score 72.11
2019-02-15 17:19:51,196 : Best param found at split 3: l2reg = 0.001                 with score 67.62
2019-02-15 17:19:57,861 : Best param found at split 4: l2reg = 1e-05                 with score 72.23
2019-02-15 17:20:02,952 : Best param found at split 5: l2reg = 0.0001                 with score 70.28
2019-02-15 17:20:03,314 : Dev acc : 69.99 Test acc : 64.93

2019-02-15 17:20:03,314 : ***** Transfer task : MPQA *****


2019-02-15 17:20:03,324 : loading BERT model bert-base-uncased
2019-02-15 17:20:03,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:20:03,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:20:03,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzyzc50aw
2019-02-15 17:20:05,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:20:07,320 : Generating sentence embeddings
2019-02-15 17:20:11,103 : Generated sentence embeddings
2019-02-15 17:20:11,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 17:20:30,297 : Best param found at split 1: l2reg = 0.0001                 with score 86.83
2019-02-15 17:20:49,561 : Best param found at split 2: l2reg = 1e-05                 with score 87.03
2019-02-15 17:21:04,137 : Best param found at split 3: l2reg = 0.01                 with score 87.11
2019-02-15 17:21:16,400 : Best param found at split 4: l2reg = 0.0001                 with score 86.34
2019-02-15 17:21:30,076 : Best param found at split 5: l2reg = 0.0001                 with score 87.27
2019-02-15 17:21:30,738 : Dev acc : 86.92 Test acc : 84.98

2019-02-15 17:21:30,739 : ***** Transfer task : SUBJ *****


2019-02-15 17:21:30,761 : loading BERT model bert-base-uncased
2019-02-15 17:21:30,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:21:30,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:21:30,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_aquhu1q
2019-02-15 17:21:33,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:21:35,913 : Generating sentence embeddings
2019-02-15 17:21:51,333 : Generated sentence embeddings
2019-02-15 17:21:51,334 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 17:22:09,861 : Best param found at split 1: l2reg = 1e-05                 with score 83.07
2019-02-15 17:22:32,986 : Best param found at split 2: l2reg = 1e-05                 with score 83.85
2019-02-15 17:22:54,634 : Best param found at split 3: l2reg = 0.0001                 with score 85.81
2019-02-15 17:23:17,517 : Best param found at split 4: l2reg = 0.0001                 with score 87.28
2019-02-15 17:23:39,179 : Best param found at split 5: l2reg = 1e-05                 with score 85.49
2019-02-15 17:23:39,835 : Dev acc : 85.1 Test acc : 85.1

2019-02-15 17:23:39,836 : ***** Transfer task : SST Binary classification *****


2019-02-15 17:23:39,967 : loading BERT model bert-base-uncased
2019-02-15 17:23:39,967 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:23:39,992 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:23:39,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbrmhj4ce
2019-02-15 17:23:42,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:23:43,937 : Computing embedding for train
2019-02-15 17:24:28,764 : Computed train embeddings
2019-02-15 17:24:28,765 : Computing embedding for dev
2019-02-15 17:24:29,726 : Computed dev embeddings
2019-02-15 17:24:29,726 : Computing embedding for test
2019-02-15 17:24:31,670 : Computed test embeddings
2019-02-15 17:24:31,670 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:24:56,604 : [('reg:1e-05', 74.89), ('reg:0.0001', 75.0), ('reg:0.001', 74.31), ('reg:0.01', 65.71)]
2019-02-15 17:24:56,604 : Validation : best param found is reg = 0.0001 with score             75.0
2019-02-15 17:24:56,604 : Evaluating...
2019-02-15 17:25:01,358 : 
Dev acc : 75.0 Test acc : 76.06 for             SST Binary classification

2019-02-15 17:25:01,359 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 17:25:01,413 : loading BERT model bert-base-uncased
2019-02-15 17:25:01,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:25:01,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:25:01,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp9ea66cw
2019-02-15 17:25:03,911 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:25:05,403 : Computing embedding for train
2019-02-15 17:25:14,854 : Computed train embeddings
2019-02-15 17:25:14,854 : Computing embedding for dev
2019-02-15 17:25:16,074 : Computed dev embeddings
2019-02-15 17:25:16,074 : Computing embedding for test
2019-02-15 17:25:18,519 : Computed test embeddings
2019-02-15 17:25:18,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:25:22,631 : [('reg:1e-05', 32.24), ('reg:0.0001', 28.52), ('reg:0.001', 31.7), ('reg:0.01', 29.06)]
2019-02-15 17:25:22,632 : Validation : best param found is reg = 1e-05 with score             32.24
2019-02-15 17:25:22,632 : Evaluating...
2019-02-15 17:25:23,743 : 
Dev acc : 32.24 Test acc : 28.33 for             SST Fine-Grained classification

2019-02-15 17:25:23,744 : ***** Transfer task : TREC *****


2019-02-15 17:25:23,764 : loading BERT model bert-base-uncased
2019-02-15 17:25:23,764 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:25:23,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:25:23,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5zhoxz6b
2019-02-15 17:25:26,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:25:31,170 : Computed train embeddings
2019-02-15 17:25:31,430 : Computed test embeddings
2019-02-15 17:25:31,430 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 17:25:45,344 : [('reg:1e-05', 61.07), ('reg:0.0001', 57.27), ('reg:0.001', 60.85), ('reg:0.01', 58.04)]
2019-02-15 17:25:45,344 : Cross-validation : best param found is reg = 1e-05             with score 61.07
2019-02-15 17:25:45,344 : Evaluating...
2019-02-15 17:25:46,129 : 
Dev acc : 61.07 Test acc : 72.0             for TREC

2019-02-15 17:25:46,129 : ***** Transfer task : MRPC *****


2019-02-15 17:25:46,152 : loading BERT model bert-base-uncased
2019-02-15 17:25:46,152 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:25:46,172 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:25:46,172 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0ocyi56g
2019-02-15 17:25:48,618 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:25:50,087 : Computing embedding for train
2019-02-15 17:26:00,390 : Computed train embeddings
2019-02-15 17:26:00,390 : Computing embedding for test
2019-02-15 17:26:04,497 : Computed test embeddings
2019-02-15 17:26:04,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 17:26:10,944 : [('reg:1e-05', 70.51), ('reg:0.0001', 71.05), ('reg:0.001', 72.2), ('reg:0.01', 71.71)]
2019-02-15 17:26:10,944 : Cross-validation : best param found is reg = 0.001             with score 72.2
2019-02-15 17:26:10,944 : Evaluating...
2019-02-15 17:26:11,299 : Dev acc : 72.2 Test acc 72.0; Test F1 81.4 for MRPC.

2019-02-15 17:26:11,299 : ***** Transfer task : SICK-Entailment*****


2019-02-15 17:26:11,371 : loading BERT model bert-base-uncased
2019-02-15 17:26:11,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:26:11,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:26:11,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpul3mqzjx
2019-02-15 17:26:13,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:26:15,361 : Computing embedding for train
2019-02-15 17:26:20,558 : Computed train embeddings
2019-02-15 17:26:20,558 : Computing embedding for dev
2019-02-15 17:26:21,232 : Computed dev embeddings
2019-02-15 17:26:21,232 : Computing embedding for test
2019-02-15 17:26:26,757 : Computed test embeddings
2019-02-15 17:26:26,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:26:29,424 : [('reg:1e-05', 74.4), ('reg:0.0001', 70.6), ('reg:0.001', 73.6), ('reg:0.01', 74.6)]
2019-02-15 17:26:29,424 : Validation : best param found is reg = 0.01 with score             74.6
2019-02-15 17:26:29,425 : Evaluating...
2019-02-15 17:26:30,114 : 
Dev acc : 74.6 Test acc : 72.8 for                        SICK entailment

2019-02-15 17:26:30,115 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 17:26:30,147 : loading BERT model bert-base-uncased
2019-02-15 17:26:30,147 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:26:30,171 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:26:30,171 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpls4w5hto
2019-02-15 17:26:32,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:26:34,135 : Computing embedding for train
2019-02-15 17:26:39,303 : Computed train embeddings
2019-02-15 17:26:39,303 : Computing embedding for dev
2019-02-15 17:26:39,999 : Computed dev embeddings
2019-02-15 17:26:39,999 : Computing embedding for test
2019-02-15 17:26:45,581 : Computed test embeddings
2019-02-15 17:27:09,345 : Dev : Pearson 0.7992206545120942
2019-02-15 17:27:09,345 : Test : Pearson 0.8061241353457513 Spearman 0.7395668721473344 MSE 0.35651854256819004                        for SICK Relatedness

2019-02-15 17:27:09,346 : 

***** Transfer task : STSBenchmark*****


2019-02-15 17:27:09,393 : loading BERT model bert-base-uncased
2019-02-15 17:27:09,393 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:27:09,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:27:09,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa11wgraw
2019-02-15 17:27:11,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:27:13,403 : Computing embedding for train
2019-02-15 17:27:21,670 : Computed train embeddings
2019-02-15 17:27:21,671 : Computing embedding for dev
2019-02-15 17:27:24,115 : Computed dev embeddings
2019-02-15 17:27:24,115 : Computing embedding for test
2019-02-15 17:27:26,124 : Computed test embeddings
2019-02-15 17:27:56,306 : Dev : Pearson 0.7017670416428679
2019-02-15 17:27:56,307 : Test : Pearson 0.6680418259700962 Spearman 0.6642506073298712 MSE 1.3527185259098462                        for SICK Relatedness

2019-02-15 17:27:56,307 : ***** Transfer task : SNLI Entailment*****


2019-02-15 17:28:01,172 : loading BERT model bert-base-uncased
2019-02-15 17:28:01,172 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:28:01,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:28:01,305 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp764m192q
2019-02-15 17:28:03,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:28:05,473 : PROGRESS (encoding): 0.00%
2019-02-15 17:29:23,121 : PROGRESS (encoding): 14.56%
2019-02-15 17:30:49,830 : PROGRESS (encoding): 29.12%
2019-02-15 17:32:17,612 : PROGRESS (encoding): 43.69%
2019-02-15 17:33:51,760 : PROGRESS (encoding): 58.25%
2019-02-15 17:35:36,658 : PROGRESS (encoding): 72.81%
2019-02-15 17:37:19,842 : PROGRESS (encoding): 87.37%
2019-02-15 17:39:14,293 : PROGRESS (encoding): 0.00%
2019-02-15 17:39:27,701 : PROGRESS (encoding): 0.00%
2019-02-15 17:39:41,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 17:40:22,824 : [('reg:1e-09', 59.83)]
2019-02-15 17:40:22,824 : Validation : best param found is reg = 1e-09 with score             59.83
2019-02-15 17:40:22,824 : Evaluating...
2019-02-15 17:41:04,720 : Dev acc : 59.83 Test acc : 60.57 for SNLI

2019-02-15 17:41:04,720 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 17:41:13,859 : loading BERT model bert-base-uncased
2019-02-15 17:41:13,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 17:41:13,909 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 17:41:13,909 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmysi3xj8
2019-02-15 17:41:16,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 17:41:17,866 : Computing embedding for train
2019-02-15 17:48:46,906 : Computed train embeddings
2019-02-15 17:48:46,906 : Computing embedding for dev
2019-02-15 17:49:06,092 : Computed dev embeddings
2019-02-15 17:49:06,092 : Computing embedding for test
2019-02-15 17:49:26,728 : Computed test embeddings
2019-02-15 17:49:26,744 : prepare data
2019-02-15 17:49:26,806 : start epoch
2019-02-15 17:50:09,609 : samples : 64000
2019-02-15 17:50:20,145 : Image to text: 6.84, 20.68, 30.52, 27.0
2019-02-15 17:50:27,861 : Text to Image: 5.384, 17.128, 26.704, 31.0
2019-02-15 17:51:11,161 : samples : 128000
2019-02-15 17:51:21,651 : Image to text: 8.62, 24.8, 36.52, 21.0
2019-02-15 17:51:29,495 : Text to Image: 6.8, 20.88, 31.54, 25.0
2019-02-15 17:52:12,166 : samples : 192000
2019-02-15 17:52:22,713 : Image to text: 8.22, 23.04, 34.26, 22.0
2019-02-15 17:52:30,577 : Text to Image: 5.992, 18.644, 28.896, 28.0
2019-02-15 17:53:13,535 : samples : 256000
2019-02-15 17:53:24,050 : Image to text: 8.38, 24.16, 36.2, 21.0
2019-02-15 17:53:31,775 : Text to Image: 6.74, 20.94, 31.612, 25.0
2019-02-15 17:54:14,979 : samples : 320000
2019-02-15 17:54:25,434 : Image to text: 9.1, 26.16, 37.84, 18.0
2019-02-15 17:54:33,154 : Text to Image: 7.292, 22.184, 33.536, 22.0
2019-02-15 17:55:23,651 : samples : 384000
2019-02-15 17:55:35,079 : Image to text: 8.52, 24.98, 36.66, 20.0
2019-02-15 17:55:42,801 : Text to Image: 6.488, 20.256, 30.66, 25.0
2019-02-15 17:56:25,558 : samples : 448000
2019-02-15 17:56:36,004 : Image to text: 9.1, 26.32, 37.76, 19.0
2019-02-15 17:56:43,781 : Text to Image: 7.284, 22.444, 33.548, 23.0
2019-02-15 17:57:26,421 : samples : 512000
2019-02-15 17:57:36,821 : Image to text: 8.92, 25.76, 37.82, 18.0
2019-02-15 17:57:44,385 : Text to Image: 7.144, 22.44, 33.416, 23.0
2019-02-15 17:58:20,685 : Epoch 1 finished
2019-02-15 17:58:21,146 : Image to text: 24.9, 55.5, 70.8, 4.0
2019-02-15 17:58:21,497 : Text to Image: 20.56, 50.9, 68.64, 5.0
2019-02-15 17:58:21,955 : Image to text: 23.5, 57.7, 71.2, 4.0
2019-02-15 17:58:22,304 : Text to Image: 19.52, 50.62, 67.54, 5.0
2019-02-15 17:58:22,767 : Image to text: 27.1, 57.3, 72.4, 4.0
2019-02-15 17:58:23,106 : Text to Image: 20.8, 50.96, 68.08, 5.0
2019-02-15 17:58:23,560 : Image to text: 25.8, 57.4, 73.0, 4.0
2019-02-15 17:58:23,910 : Text to Image: 19.94, 50.38, 67.78, 5.0
2019-02-15 17:58:24,381 : Image to text: 24.1, 57.4, 71.7, 4.0
2019-02-15 17:58:24,725 : Text to Image: 19.42, 50.62, 67.34, 5.0
2019-02-15 17:58:24,726 : Dev mean Text to Image: 20.048000000000002, 50.696, 67.876, 5.0
2019-02-15 17:58:24,726 : Dev mean Image to text: 25.08, 57.06, 71.82, 4.0
2019-02-15 17:58:24,726 : start epoch
2019-02-15 17:59:07,733 : samples : 64000
2019-02-15 17:59:18,132 : Image to text: 9.72, 27.48, 39.8, 17.0
2019-02-15 17:59:25,952 : Text to Image: 7.836, 23.316, 34.724, 21.0
2019-02-15 18:00:08,701 : samples : 128000
2019-02-15 18:00:19,158 : Image to text: 10.0, 28.12, 40.38, 17.0
2019-02-15 18:00:27,142 : Text to Image: 7.86, 24.028, 35.3, 21.0
2019-02-15 18:01:11,094 : samples : 192000
2019-02-15 18:01:21,582 : Image to text: 10.14, 27.24, 39.84, 17.0
2019-02-15 18:01:29,266 : Text to Image: 7.952, 23.572, 34.976, 21.0
2019-02-15 18:02:12,035 : samples : 256000
2019-02-15 18:02:22,460 : Image to text: 9.58, 28.56, 40.54, 16.0
2019-02-15 18:02:30,086 : Text to Image: 8.248, 24.4, 36.24, 20.0
2019-02-15 18:03:13,056 : samples : 320000
2019-02-15 18:03:23,436 : Image to text: 10.06, 28.66, 41.46, 16.0
2019-02-15 18:03:31,047 : Text to Image: 8.336, 24.388, 35.86, 20.0
2019-02-15 18:04:14,529 : samples : 384000
2019-02-15 18:04:24,827 : Image to text: 9.76, 27.84, 39.82, 17.0
2019-02-15 18:04:32,451 : Text to Image: 8.0, 24.636, 36.288, 20.0
2019-02-15 18:05:15,211 : samples : 448000
2019-02-15 18:05:25,589 : Image to text: 11.02, 28.38, 41.68, 16.0
2019-02-15 18:05:33,179 : Text to Image: 8.648, 25.068, 36.936, 19.0
2019-02-15 18:06:16,368 : samples : 512000
2019-02-15 18:06:26,498 : Image to text: 10.3, 29.3, 41.82, 16.0
2019-02-15 18:06:36,044 : Text to Image: 8.404, 24.94, 36.372, 20.0
2019-02-15 18:07:14,451 : Epoch 2 finished
2019-02-15 18:07:15,446 : Image to text: 27.5, 60.3, 75.5, 4.0
2019-02-15 18:07:16,201 : Text to Image: 22.16, 55.72, 72.54, 4.0
2019-02-15 18:07:17,158 : Image to text: 26.5, 59.9, 73.7, 4.0
2019-02-15 18:07:17,962 : Text to Image: 21.68, 54.24, 71.76, 5.0
2019-02-15 18:07:18,933 : Image to text: 26.4, 60.7, 75.5, 4.0
2019-02-15 18:07:19,719 : Text to Image: 22.26, 55.36, 71.78, 4.0
2019-02-15 18:07:20,674 : Image to text: 27.4, 59.3, 73.4, 4.0
2019-02-15 18:07:21,410 : Text to Image: 22.38, 54.94, 71.12, 5.0
2019-02-15 18:07:22,391 : Image to text: 26.9, 60.6, 74.9, 4.0
2019-02-15 18:07:23,116 : Text to Image: 22.5, 53.66, 70.42, 5.0
2019-02-15 18:07:23,116 : Dev mean Text to Image: 22.196, 54.784, 71.524, 4.6
2019-02-15 18:07:23,116 : Dev mean Image to text: 26.939999999999998, 60.16, 74.6, 4.0
2019-02-15 18:07:23,117 : start epoch
2019-02-15 18:08:08,039 : samples : 64000
2019-02-15 18:08:18,285 : Image to text: 10.7, 29.46, 42.08, 15.0
2019-02-15 18:08:25,634 : Text to Image: 8.664, 25.068, 36.76, 20.0
2019-02-15 18:09:08,532 : samples : 128000
2019-02-15 18:09:21,086 : Image to text: 9.56, 27.56, 40.34, 17.0
2019-02-15 18:09:31,058 : Text to Image: 8.012, 24.2, 35.708, 20.0
2019-02-15 18:10:16,693 : samples : 192000
2019-02-15 18:10:29,310 : Image to text: 10.24, 30.22, 42.52, 15.0
2019-02-15 18:10:36,680 : Text to Image: 8.332, 25.068, 36.416, 20.0
2019-02-15 18:11:18,382 : samples : 256000
2019-02-15 18:11:28,216 : Image to text: 11.8, 29.94, 42.04, 15.0
2019-02-15 18:11:37,810 : Text to Image: 8.896, 26.176, 38.136, 18.0
2019-02-15 18:12:30,319 : samples : 320000
2019-02-15 18:12:42,907 : Image to text: 10.36, 29.04, 41.74, 15.0
2019-02-15 18:12:52,966 : Text to Image: 8.38, 24.904, 36.636, 20.0
2019-02-15 18:13:35,700 : samples : 384000
2019-02-15 18:13:45,807 : Image to text: 10.7, 29.1, 41.64, 15.0
2019-02-15 18:13:55,598 : Text to Image: 8.832, 25.62, 37.144, 19.0
2019-02-15 18:14:40,282 : samples : 448000
2019-02-15 18:14:52,876 : Image to text: 10.8, 31.34, 43.84, 14.0
2019-02-15 18:15:02,892 : Text to Image: 9.216, 26.748, 38.416, 18.0
2019-02-15 18:15:45,299 : samples : 512000
2019-02-15 18:15:55,623 : Image to text: 11.04, 29.92, 42.42, 15.0
2019-02-15 18:16:03,063 : Text to Image: 8.94, 26.232, 38.26, 18.0
2019-02-15 18:16:40,555 : Epoch 3 finished
2019-02-15 18:16:41,458 : Image to text: 28.3, 58.0, 75.2, 4.0
2019-02-15 18:16:42,206 : Text to Image: 22.8, 56.02, 73.52, 4.0
2019-02-15 18:16:43,149 : Image to text: 28.3, 61.4, 73.9, 4.0
2019-02-15 18:16:43,919 : Text to Image: 22.54, 54.34, 72.14, 5.0
2019-02-15 18:16:44,862 : Image to text: 27.1, 61.6, 79.3, 4.0
2019-02-15 18:16:45,620 : Text to Image: 21.72, 56.62, 72.72, 4.0
2019-02-15 18:16:46,525 : Image to text: 29.4, 61.6, 75.7, 4.0
2019-02-15 18:16:47,314 : Text to Image: 23.28, 54.64, 72.16, 5.0
2019-02-15 18:16:48,222 : Image to text: 29.3, 61.6, 75.5, 4.0
2019-02-15 18:16:48,992 : Text to Image: 22.78, 55.34, 72.0, 5.0
2019-02-15 18:16:48,992 : Dev mean Text to Image: 22.624000000000002, 55.391999999999996, 72.50800000000001, 4.6
2019-02-15 18:16:48,992 : Dev mean Image to text: 28.48, 60.84, 75.92, 4.0
2019-02-15 18:16:48,993 : start epoch
2019-02-15 18:17:34,858 : samples : 64000
2019-02-15 18:17:46,321 : Image to text: 11.4, 30.62, 43.06, 14.0
2019-02-15 18:17:53,650 : Text to Image: 9.484, 27.08, 38.88, 18.0
2019-02-15 18:18:36,513 : samples : 128000
2019-02-15 18:18:49,079 : Image to text: 11.38, 31.0, 44.04, 14.0
2019-02-15 18:18:59,115 : Text to Image: 9.068, 26.36, 38.14, 18.0
2019-02-15 18:19:44,761 : samples : 192000
2019-02-15 18:19:56,079 : Image to text: 10.62, 31.04, 43.98, 14.0
2019-02-15 18:20:03,464 : Text to Image: 8.796, 26.112, 37.936, 19.0
2019-02-15 18:20:45,875 : samples : 256000
2019-02-15 18:20:56,189 : Image to text: 12.06, 30.72, 43.58, 14.0
2019-02-15 18:21:03,277 : Text to Image: 9.496, 26.86, 38.78, 18.0
2019-02-15 18:21:47,818 : samples : 320000
2019-02-15 18:22:00,428 : Image to text: 11.48, 30.86, 43.5, 14.0
2019-02-15 18:22:10,458 : Text to Image: 9.672, 27.188, 38.868, 18.0
2019-02-15 18:22:54,280 : samples : 384000
2019-02-15 18:23:04,534 : Image to text: 10.58, 29.92, 43.28, 14.0
2019-02-15 18:23:11,946 : Text to Image: 8.784, 26.424, 38.336, 18.0
2019-02-15 18:23:55,271 : samples : 448000
2019-02-15 18:24:07,826 : Image to text: 11.86, 31.52, 44.42, 14.0
2019-02-15 18:24:17,847 : Text to Image: 9.504, 27.288, 39.404, 17.0
2019-02-15 18:25:03,329 : samples : 512000
2019-02-15 18:25:14,021 : Image to text: 11.52, 31.84, 43.96, 14.0
2019-02-15 18:25:21,515 : Text to Image: 9.236, 26.812, 38.676, 18.0
2019-02-15 18:25:57,805 : Epoch 4 finished
2019-02-15 18:25:58,257 : Image to text: 27.4, 60.9, 76.1, 4.0
2019-02-15 18:25:58,628 : Text to Image: 23.44, 56.56, 74.1, 4.0
2019-02-15 18:25:59,087 : Image to text: 26.7, 59.9, 75.7, 4.0
2019-02-15 18:25:59,463 : Text to Image: 23.44, 56.12, 73.0, 4.0
2019-02-15 18:25:59,920 : Image to text: 28.8, 63.1, 78.1, 3.0
2019-02-15 18:26:00,295 : Text to Image: 23.76, 57.04, 73.38, 4.0
2019-02-15 18:26:00,765 : Image to text: 28.8, 63.4, 77.5, 3.0
2019-02-15 18:26:01,138 : Text to Image: 22.92, 56.32, 72.64, 4.0
2019-02-15 18:26:01,596 : Image to text: 28.7, 60.3, 74.7, 3.0
2019-02-15 18:26:01,959 : Text to Image: 23.14, 55.82, 72.44, 4.0
2019-02-15 18:26:01,959 : Dev mean Text to Image: 23.340000000000003, 56.372, 73.112, 4.0
2019-02-15 18:26:01,959 : Dev mean Image to text: 28.08, 61.519999999999996, 76.42, 3.4000000000000004
2019-02-15 18:26:01,959 : start epoch
2019-02-15 18:26:45,097 : samples : 64000
2019-02-15 18:26:57,713 : Image to text: 12.06, 31.22, 44.1, 13.0
2019-02-15 18:27:07,776 : Text to Image: 9.608, 27.76, 39.884, 17.0
2019-02-15 18:27:53,320 : samples : 128000
2019-02-15 18:28:03,607 : Image to text: 12.2, 32.0, 44.8, 14.0
2019-02-15 18:28:10,841 : Text to Image: 9.412, 27.584, 39.66, 17.0
2019-02-15 18:29:02,517 : samples : 192000
2019-02-15 18:29:15,104 : Image to text: 11.44, 31.94, 44.14, 14.0
2019-02-15 18:29:25,166 : Text to Image: 9.54, 28.144, 40.176, 17.0
2019-02-15 18:30:10,504 : samples : 256000
2019-02-15 18:30:21,806 : Image to text: 10.78, 30.58, 43.42, 15.0
2019-02-15 18:30:29,176 : Text to Image: 9.436, 27.44, 39.408, 17.0
2019-02-15 18:31:12,117 : samples : 320000
2019-02-15 18:31:23,128 : Image to text: 11.52, 31.34, 43.4, 14.0
2019-02-15 18:31:31,739 : Text to Image: 9.324, 26.804, 39.024, 18.0
2019-02-15 18:32:15,192 : samples : 384000
2019-02-15 18:32:25,497 : Image to text: 11.14, 31.54, 44.02, 14.0
2019-02-15 18:32:32,958 : Text to Image: 9.664, 27.428, 39.468, 17.0
2019-02-15 18:33:15,644 : samples : 448000
2019-02-15 18:33:25,972 : Image to text: 11.74, 32.2, 44.96, 13.0
2019-02-15 18:33:33,158 : Text to Image: 9.536, 27.472, 39.544, 17.0
2019-02-15 18:34:16,016 : samples : 512000
2019-02-15 18:34:26,220 : Image to text: 12.1, 32.32, 45.38, 13.0
2019-02-15 18:34:33,617 : Text to Image: 9.804, 27.72, 39.872, 17.0
2019-02-15 18:35:09,908 : Epoch 5 finished
2019-02-15 18:35:10,387 : Image to text: 29.4, 62.2, 77.1, 3.0
2019-02-15 18:35:10,757 : Text to Image: 23.46, 56.98, 74.78, 4.0
2019-02-15 18:35:11,209 : Image to text: 27.9, 60.5, 74.9, 3.0
2019-02-15 18:35:11,583 : Text to Image: 23.82, 56.06, 73.64, 4.0
2019-02-15 18:35:12,036 : Image to text: 28.8, 64.2, 78.2, 3.0
2019-02-15 18:35:12,401 : Text to Image: 24.96, 58.22, 73.88, 4.0
2019-02-15 18:35:12,852 : Image to text: 29.4, 63.6, 76.7, 3.0
2019-02-15 18:35:13,215 : Text to Image: 24.0, 57.52, 73.38, 4.0
2019-02-15 18:35:13,679 : Image to text: 29.4, 64.3, 76.9, 3.0
2019-02-15 18:35:14,046 : Text to Image: 23.92, 56.46, 72.28, 4.0
2019-02-15 18:35:14,046 : Dev mean Text to Image: 24.032000000000004, 57.048, 73.592, 4.0
2019-02-15 18:35:14,046 : Dev mean Image to text: 28.979999999999997, 62.959999999999994, 76.75999999999999, 3.0
2019-02-15 18:35:14,047 : start epoch
2019-02-15 18:35:57,384 : samples : 64000
2019-02-15 18:36:07,894 : Image to text: 11.3, 31.64, 44.88, 13.0
2019-02-15 18:36:15,286 : Text to Image: 9.172, 26.816, 38.684, 18.0
2019-02-15 18:36:57,969 : samples : 128000
2019-02-15 18:37:08,190 : Image to text: 12.16, 32.68, 45.5, 13.0
2019-02-15 18:37:15,538 : Text to Image: 10.056, 27.964, 39.984, 17.0
2019-02-15 18:37:59,636 : samples : 192000
2019-02-15 18:38:12,293 : Image to text: 11.94, 31.42, 44.1, 14.0
2019-02-15 18:38:22,453 : Text to Image: 9.244, 26.984, 38.984, 18.0
2019-02-15 18:39:04,937 : samples : 256000
2019-02-15 18:39:15,785 : Image to text: 11.64, 32.86, 45.52, 13.0
2019-02-15 18:39:25,964 : Text to Image: 9.836, 27.724, 39.74, 17.0
2019-02-15 18:40:09,069 : samples : 320000
2019-02-15 18:40:19,620 : Image to text: 11.5, 32.48, 44.6, 14.0
2019-02-15 18:40:27,786 : Text to Image: 9.74, 28.02, 40.012, 17.0
2019-02-15 18:41:11,202 : samples : 384000
2019-02-15 18:41:21,490 : Image to text: 11.58, 31.66, 44.88, 13.0
2019-02-15 18:41:28,887 : Text to Image: 9.936, 28.02, 40.328, 17.0
2019-02-15 18:42:12,054 : samples : 448000
2019-02-15 18:42:22,172 : Image to text: 12.08, 32.28, 45.96, 13.0
2019-02-15 18:42:30,292 : Text to Image: 10.096, 28.208, 40.608, 16.0
2019-02-15 18:43:15,390 : samples : 512000
2019-02-15 18:43:27,885 : Image to text: 12.0, 32.24, 45.18, 13.0
2019-02-15 18:43:37,803 : Text to Image: 9.8, 27.964, 40.52, 16.0
2019-02-15 18:44:16,389 : Epoch 6 finished
2019-02-15 18:44:17,278 : Image to text: 30.2, 62.9, 77.6, 3.0
2019-02-15 18:44:18,023 : Text to Image: 25.28, 59.02, 76.58, 4.0
2019-02-15 18:44:18,910 : Image to text: 29.7, 61.8, 77.6, 3.0
2019-02-15 18:44:19,644 : Text to Image: 24.34, 58.42, 75.0, 4.0
2019-02-15 18:44:20,524 : Image to text: 30.0, 65.9, 79.1, 3.0
2019-02-15 18:44:21,263 : Text to Image: 26.12, 59.7, 75.46, 4.0
2019-02-15 18:44:22,181 : Image to text: 31.0, 63.5, 76.3, 3.0
2019-02-15 18:44:22,904 : Text to Image: 24.08, 58.18, 74.38, 4.0
2019-02-15 18:44:23,807 : Image to text: 30.1, 64.7, 77.2, 3.0
2019-02-15 18:44:24,546 : Text to Image: 24.38, 56.9, 74.12, 4.0
2019-02-15 18:44:24,546 : Dev mean Text to Image: 24.839999999999996, 58.44399999999999, 75.108, 4.0
2019-02-15 18:44:24,546 : Dev mean Image to text: 30.2, 63.75999999999999, 77.56, 3.0
2019-02-15 18:44:24,546 : start epoch
2019-02-15 18:45:10,149 : samples : 64000
2019-02-15 18:45:23,156 : Image to text: 10.96, 32.44, 46.3, 12.0
2019-02-15 18:45:33,618 : Text to Image: 10.144, 28.296, 40.8, 16.0
2019-02-15 18:46:21,052 : samples : 128000
2019-02-15 18:46:33,614 : Image to text: 11.92, 32.16, 45.56, 13.0
2019-02-15 18:46:43,629 : Text to Image: 10.164, 28.524, 40.924, 16.0
2019-02-15 18:47:28,379 : samples : 192000
2019-02-15 18:47:40,977 : Image to text: 11.62, 32.62, 45.1, 13.0
2019-02-15 18:47:50,956 : Text to Image: 9.384, 27.396, 39.76, 17.0
2019-02-15 18:48:35,708 : samples : 256000
2019-02-15 18:48:48,384 : Image to text: 12.06, 32.78, 46.06, 13.0
2019-02-15 18:48:58,378 : Text to Image: 10.336, 29.024, 41.38, 16.0
2019-02-15 18:49:42,230 : samples : 320000
2019-02-15 18:49:54,829 : Image to text: 12.18, 32.1, 45.3, 13.0
2019-02-15 18:50:04,865 : Text to Image: 10.064, 28.42, 40.648, 16.0
2019-02-15 18:50:48,936 : samples : 384000
2019-02-15 18:51:01,497 : Image to text: 12.34, 32.26, 45.98, 13.0
2019-02-15 18:51:11,473 : Text to Image: 10.328, 28.768, 40.956, 16.0
2019-02-15 18:51:55,698 : samples : 448000
2019-02-15 18:52:08,321 : Image to text: 12.3, 32.86, 46.7, 12.0
2019-02-15 18:52:18,358 : Text to Image: 10.352, 29.008, 41.604, 16.0
2019-02-15 18:53:03,038 : samples : 512000
2019-02-15 18:53:15,657 : Image to text: 13.0, 33.2, 46.58, 12.0
2019-02-15 18:53:25,742 : Text to Image: 10.424, 28.852, 40.98, 16.0
2019-02-15 18:54:04,566 : Epoch 7 finished
2019-02-15 18:54:05,032 : Image to text: 28.9, 61.0, 76.7, 4.0
2019-02-15 18:54:05,390 : Text to Image: 24.0, 58.76, 75.64, 4.0
2019-02-15 18:54:05,853 : Image to text: 28.4, 61.4, 75.9, 3.0
2019-02-15 18:54:06,218 : Text to Image: 23.02, 56.72, 74.42, 4.0
2019-02-15 18:54:06,678 : Image to text: 27.7, 63.8, 79.3, 4.0
2019-02-15 18:54:07,045 : Text to Image: 25.62, 59.16, 75.08, 4.0
2019-02-15 18:54:07,504 : Image to text: 27.9, 63.0, 78.2, 3.0
2019-02-15 18:54:07,874 : Text to Image: 24.08, 57.7, 74.28, 4.0
2019-02-15 18:54:08,333 : Image to text: 28.0, 62.2, 77.0, 3.0
2019-02-15 18:54:08,698 : Text to Image: 24.38, 57.06, 72.94, 4.0
2019-02-15 18:54:08,698 : Dev mean Text to Image: 24.22, 57.879999999999995, 74.472, 4.0
2019-02-15 18:54:08,699 : Dev mean Image to text: 28.18, 62.28, 77.42, 3.4000000000000004
2019-02-15 18:54:08,699 : start epoch
2019-02-15 18:54:51,866 : samples : 64000
2019-02-15 18:55:02,181 : Image to text: 12.64, 32.78, 45.84, 13.0
2019-02-15 18:55:09,564 : Text to Image: 9.992, 28.284, 40.08, 16.0
2019-02-15 18:55:53,145 : samples : 128000
2019-02-15 18:56:06,082 : Image to text: 13.0, 33.06, 46.02, 13.0
2019-02-15 18:56:16,556 : Text to Image: 10.088, 28.408, 41.016, 16.0
2019-02-15 18:57:02,014 : samples : 192000
2019-02-15 18:57:14,894 : Image to text: 11.52, 32.9, 46.14, 13.0
2019-02-15 18:57:25,299 : Text to Image: 9.592, 28.14, 40.424, 16.0
2019-02-15 18:58:11,795 : samples : 256000
2019-02-15 18:58:24,632 : Image to text: 12.5, 32.54, 45.68, 13.0
2019-02-15 18:58:35,083 : Text to Image: 10.288, 29.128, 41.2, 16.0
2019-02-15 18:59:21,426 : samples : 320000
2019-02-15 18:59:34,302 : Image to text: 12.36, 33.14, 46.14, 13.0
2019-02-15 18:59:44,785 : Text to Image: 10.36, 29.084, 41.404, 16.0
2019-02-15 19:00:31,399 : samples : 384000
2019-02-15 19:00:44,224 : Image to text: 12.02, 33.16, 46.68, 12.0
2019-02-15 19:00:54,706 : Text to Image: 10.452, 29.012, 41.172, 16.0
2019-02-15 19:01:40,512 : samples : 448000
2019-02-15 19:01:53,280 : Image to text: 12.58, 33.1, 46.54, 12.0
2019-02-15 19:02:03,685 : Text to Image: 10.184, 28.4, 40.34, 16.0
2019-02-15 19:02:58,081 : samples : 512000
2019-02-15 19:03:10,128 : Image to text: 11.8, 32.62, 45.44, 13.0
2019-02-15 19:03:20,499 : Text to Image: 10.248, 28.52, 40.616, 16.0
2019-02-15 19:03:59,290 : Epoch 8 finished
2019-02-15 19:03:59,714 : Image to text: 29.8, 60.7, 78.5, 4.0
2019-02-15 19:04:00,045 : Text to Image: 24.12, 57.7, 74.7, 4.0
2019-02-15 19:04:00,486 : Image to text: 29.9, 62.4, 77.6, 3.0
2019-02-15 19:04:00,810 : Text to Image: 23.1, 57.04, 74.3, 4.0
2019-02-15 19:04:01,255 : Image to text: 29.0, 64.2, 80.4, 3.0
2019-02-15 19:04:01,590 : Text to Image: 24.44, 58.48, 74.78, 4.0
2019-02-15 19:04:02,039 : Image to text: 29.9, 63.5, 77.4, 3.0
2019-02-15 19:04:02,377 : Text to Image: 24.04, 57.36, 74.26, 4.0
2019-02-15 19:04:02,830 : Image to text: 29.9, 63.8, 76.8, 3.0
2019-02-15 19:04:03,169 : Text to Image: 23.4, 56.62, 73.66, 4.0
2019-02-15 19:04:03,169 : Dev mean Text to Image: 23.82, 57.44, 74.34, 4.0
2019-02-15 19:04:03,169 : Dev mean Image to text: 29.7, 62.919999999999995, 78.14, 3.2
2019-02-15 19:04:03,169 : start epoch
2019-02-15 19:04:46,339 : samples : 64000
2019-02-15 19:04:56,845 : Image to text: 12.28, 33.54, 45.82, 13.0
2019-02-15 19:05:04,447 : Text to Image: 10.136, 28.532, 40.812, 16.0
2019-02-15 19:05:46,580 : samples : 128000
2019-02-15 19:05:57,058 : Image to text: 12.58, 34.16, 46.82, 12.0
2019-02-15 19:06:04,597 : Text to Image: 10.256, 28.808, 40.86, 16.0
2019-02-15 19:06:47,494 : samples : 192000
2019-02-15 19:06:58,018 : Image to text: 12.64, 34.06, 46.78, 12.0
2019-02-15 19:07:05,609 : Text to Image: 10.352, 29.172, 41.436, 16.0
2019-02-15 19:07:48,300 : samples : 256000
2019-02-15 19:07:58,933 : Image to text: 12.54, 33.8, 46.28, 13.0
2019-02-15 19:08:06,495 : Text to Image: 10.484, 29.368, 41.672, 16.0
2019-02-15 19:08:49,099 : samples : 320000
2019-02-15 19:08:59,578 : Image to text: 12.44, 33.64, 46.42, 12.0
2019-02-15 19:09:07,176 : Text to Image: 10.536, 29.244, 41.564, 16.0
2019-02-15 19:09:49,731 : samples : 384000
2019-02-15 19:10:00,199 : Image to text: 12.44, 33.92, 46.98, 12.0
2019-02-15 19:10:07,781 : Text to Image: 10.272, 28.744, 41.124, 16.0
2019-02-15 19:10:50,086 : samples : 448000
2019-02-15 19:11:00,573 : Image to text: 12.32, 33.22, 46.04, 12.0
2019-02-15 19:11:08,143 : Text to Image: 10.244, 29.208, 41.34, 16.0
2019-02-15 19:11:51,355 : samples : 512000
2019-02-15 19:12:01,833 : Image to text: 12.14, 33.22, 46.62, 12.0
2019-02-15 19:12:09,418 : Text to Image: 9.944, 28.608, 41.072, 16.0
2019-02-15 19:12:46,201 : Epoch 9 finished
2019-02-15 19:12:46,678 : Image to text: 29.9, 64.5, 78.9, 3.0
2019-02-15 19:12:47,021 : Text to Image: 26.42, 61.12, 77.2, 4.0
2019-02-15 19:12:47,464 : Image to text: 31.0, 65.7, 77.4, 3.0
2019-02-15 19:12:47,797 : Text to Image: 25.6, 58.98, 76.2, 4.0
2019-02-15 19:12:48,247 : Image to text: 31.6, 65.6, 81.2, 3.0
2019-02-15 19:12:48,579 : Text to Image: 27.14, 60.8, 76.66, 4.0
2019-02-15 19:12:49,018 : Image to text: 30.3, 65.2, 79.6, 3.0
2019-02-15 19:12:49,359 : Text to Image: 26.1, 59.44, 76.08, 4.0
2019-02-15 19:12:49,799 : Image to text: 29.8, 65.3, 77.6, 3.0
2019-02-15 19:12:50,138 : Text to Image: 26.32, 59.92, 74.76, 4.0
2019-02-15 19:12:50,138 : Dev mean Text to Image: 26.316, 60.052, 76.18, 4.0
2019-02-15 19:12:50,138 : Dev mean Image to text: 30.520000000000003, 65.25999999999999, 78.94, 3.0
2019-02-15 19:12:50,139 : start epoch
2019-02-15 19:13:32,444 : samples : 64000
2019-02-15 19:13:42,983 : Image to text: 12.98, 33.08, 47.06, 12.0
2019-02-15 19:13:50,578 : Text to Image: 10.152, 28.472, 40.524, 16.0
2019-02-15 19:14:32,850 : samples : 128000
2019-02-15 19:14:43,302 : Image to text: 12.5, 33.42, 47.18, 12.0
2019-02-15 19:14:50,919 : Text to Image: 10.568, 29.816, 41.86, 15.0
2019-02-15 19:15:33,921 : samples : 192000
2019-02-15 19:15:44,463 : Image to text: 11.9, 32.98, 45.64, 13.0
2019-02-15 19:15:52,015 : Text to Image: 10.216, 28.344, 41.016, 16.0
2019-02-15 19:16:34,489 : samples : 256000
2019-02-15 19:16:45,019 : Image to text: 12.56, 32.96, 46.88, 12.0
2019-02-15 19:16:52,594 : Text to Image: 10.248, 28.788, 41.04, 16.0
2019-02-15 19:17:34,887 : samples : 320000
2019-02-15 19:17:45,400 : Image to text: 11.88, 32.9, 45.72, 13.0
2019-02-15 19:17:52,953 : Text to Image: 10.128, 28.52, 41.116, 16.0
2019-02-15 19:18:35,298 : samples : 384000
2019-02-15 19:18:45,876 : Image to text: 12.42, 33.14, 47.14, 12.0
2019-02-15 19:18:53,293 : Text to Image: 10.628, 29.616, 42.092, 15.0
2019-02-15 19:19:46,255 : samples : 448000
2019-02-15 19:19:56,682 : Image to text: 12.68, 33.84, 47.22, 12.0
2019-02-15 19:20:04,245 : Text to Image: 10.656, 29.684, 42.148, 15.0
2019-02-15 19:20:46,745 : samples : 512000
2019-02-15 19:20:57,167 : Image to text: 12.94, 33.32, 47.1, 12.0
2019-02-15 19:21:04,787 : Text to Image: 10.584, 29.612, 42.16, 15.0
2019-02-15 19:21:41,311 : Epoch 10 finished
2019-02-15 19:21:41,759 : Image to text: 32.4, 63.6, 79.2, 3.0
2019-02-15 19:21:42,096 : Text to Image: 26.38, 60.4, 77.18, 4.0
2019-02-15 19:21:42,535 : Image to text: 30.6, 64.9, 78.2, 3.0
2019-02-15 19:21:42,872 : Text to Image: 25.76, 59.1, 76.02, 4.0
2019-02-15 19:21:43,303 : Image to text: 31.8, 66.6, 81.6, 3.0
2019-02-15 19:21:43,648 : Text to Image: 26.26, 61.36, 76.5, 4.0
2019-02-15 19:21:44,098 : Image to text: 30.2, 66.6, 78.1, 3.0
2019-02-15 19:21:44,427 : Text to Image: 26.34, 59.42, 75.68, 4.0
2019-02-15 19:21:44,853 : Image to text: 28.9, 64.6, 79.0, 3.0
2019-02-15 19:21:45,181 : Text to Image: 25.34, 59.28, 75.3, 4.0
2019-02-15 19:21:45,181 : Dev mean Text to Image: 26.016, 59.912, 76.136, 4.0
2019-02-15 19:21:45,181 : Dev mean Image to text: 30.78, 65.26, 79.22, 3.0
2019-02-15 19:21:45,182 : start epoch
2019-02-15 19:22:28,477 : samples : 64000
2019-02-15 19:22:39,066 : Image to text: 12.64, 34.54, 47.58, 12.0
2019-02-15 19:22:46,657 : Text to Image: 10.556, 29.676, 41.936, 15.0
2019-02-15 19:23:28,908 : samples : 128000
2019-02-15 19:23:39,369 : Image to text: 12.82, 33.66, 47.22, 12.0
2019-02-15 19:23:46,991 : Text to Image: 10.432, 28.896, 41.56, 15.0
2019-02-15 19:24:29,434 : samples : 192000
2019-02-15 19:24:39,961 : Image to text: 12.0, 32.9, 47.02, 12.0
2019-02-15 19:24:47,525 : Text to Image: 10.352, 28.86, 40.752, 16.0
2019-02-15 19:25:30,278 : samples : 256000
2019-02-15 19:25:40,842 : Image to text: 12.96, 33.52, 47.3, 12.0
2019-02-15 19:25:48,438 : Text to Image: 10.548, 29.084, 41.068, 16.0
2019-02-15 19:26:31,181 : samples : 320000
2019-02-15 19:26:41,706 : Image to text: 12.68, 33.6, 47.26, 12.0
2019-02-15 19:26:49,262 : Text to Image: 10.544, 29.148, 41.552, 15.0
2019-02-15 19:27:32,355 : samples : 384000
2019-02-15 19:27:42,821 : Image to text: 13.18, 34.38, 46.86, 12.0
2019-02-15 19:27:50,373 : Text to Image: 10.776, 29.588, 41.948, 15.0
2019-02-15 19:28:32,963 : samples : 448000
2019-02-15 19:28:43,475 : Image to text: 12.52, 33.28, 46.66, 12.0
2019-02-15 19:28:51,031 : Text to Image: 10.548, 29.536, 41.676, 15.0
2019-02-15 19:29:33,501 : samples : 512000
2019-02-15 19:29:43,962 : Image to text: 12.98, 33.46, 46.6, 12.0
2019-02-15 19:29:51,508 : Text to Image: 10.644, 29.64, 42.084, 15.0
2019-02-15 19:30:28,251 : Epoch 11 finished
2019-02-15 19:30:28,681 : Image to text: 30.2, 63.5, 80.2, 3.0
2019-02-15 19:30:29,012 : Text to Image: 25.32, 59.68, 76.96, 4.0
2019-02-15 19:30:29,441 : Image to text: 31.0, 62.4, 76.9, 3.0
2019-02-15 19:30:29,755 : Text to Image: 25.56, 58.02, 75.16, 4.0
2019-02-15 19:30:30,203 : Image to text: 31.2, 67.1, 81.9, 3.0
2019-02-15 19:30:30,537 : Text to Image: 26.06, 60.0, 76.5, 4.0
2019-02-15 19:30:30,981 : Image to text: 31.8, 63.7, 77.9, 3.0
2019-02-15 19:30:31,326 : Text to Image: 25.04, 59.2, 75.32, 4.0
2019-02-15 19:30:31,775 : Image to text: 29.8, 63.8, 78.0, 3.0
2019-02-15 19:30:32,124 : Text to Image: 25.28, 58.8, 74.86, 4.0
2019-02-15 19:30:32,124 : Dev mean Text to Image: 25.452, 59.13999999999999, 75.76, 4.0
2019-02-15 19:30:32,124 : Dev mean Image to text: 30.8, 64.1, 78.98, 3.0
2019-02-15 19:30:32,124 : start epoch
2019-02-15 19:31:14,385 : samples : 64000
2019-02-15 19:31:24,895 : Image to text: 12.96, 34.52, 48.34, 11.0
2019-02-15 19:31:32,500 : Text to Image: 10.744, 29.456, 42.1, 15.0
2019-02-15 19:32:14,934 : samples : 128000
2019-02-15 19:32:25,505 : Image to text: 12.04, 32.86, 45.84, 13.0
2019-02-15 19:32:33,063 : Text to Image: 9.924, 28.62, 40.688, 16.0
2019-02-15 19:33:16,303 : samples : 192000
2019-02-15 19:33:26,868 : Image to text: 13.38, 34.58, 48.36, 11.0
2019-02-15 19:33:34,447 : Text to Image: 10.848, 30.016, 42.42, 15.0
2019-02-15 19:34:16,754 : samples : 256000
2019-02-15 19:34:27,215 : Image to text: 13.04, 33.78, 46.4, 12.0
2019-02-15 19:34:34,834 : Text to Image: 10.508, 29.58, 41.828, 15.0
2019-02-15 19:35:16,994 : samples : 320000
2019-02-15 19:35:27,461 : Image to text: 12.28, 33.46, 46.16, 13.0
2019-02-15 19:35:34,971 : Text to Image: 10.504, 29.348, 41.64, 15.0
2019-02-15 19:36:24,867 : samples : 384000
2019-02-15 19:36:36,473 : Image to text: 12.26, 33.78, 47.14, 12.0
2019-02-15 19:36:44,042 : Text to Image: 10.708, 29.172, 41.924, 16.0
2019-02-15 19:37:27,188 : samples : 448000
2019-02-15 19:37:37,672 : Image to text: 13.62, 34.66, 47.8, 12.0
2019-02-15 19:37:45,228 : Text to Image: 10.68, 29.724, 42.48, 15.0
2019-02-15 19:38:27,773 : samples : 512000
2019-02-15 19:38:38,232 : Image to text: 12.96, 35.02, 47.66, 12.0
2019-02-15 19:38:45,791 : Text to Image: 10.68, 30.18, 42.556, 15.0
2019-02-15 19:39:21,849 : Epoch 12 finished
2019-02-15 19:39:22,289 : Image to text: 30.4, 63.0, 79.2, 3.0
2019-02-15 19:39:22,625 : Text to Image: 26.12, 60.62, 77.08, 4.0
2019-02-15 19:39:23,074 : Image to text: 32.0, 64.2, 77.8, 3.0
2019-02-15 19:39:23,409 : Text to Image: 25.84, 59.3, 76.18, 4.0
2019-02-15 19:39:23,866 : Image to text: 30.9, 64.9, 80.4, 3.0
2019-02-15 19:39:24,204 : Text to Image: 26.04, 61.46, 77.14, 4.0
2019-02-15 19:39:24,645 : Image to text: 31.3, 64.6, 78.2, 3.0
2019-02-15 19:39:24,981 : Text to Image: 25.38, 59.32, 76.36, 4.0
2019-02-15 19:39:25,437 : Image to text: 31.0, 64.3, 77.8, 3.0
2019-02-15 19:39:25,772 : Text to Image: 26.5, 59.3, 75.46, 4.0
2019-02-15 19:39:25,772 : Dev mean Text to Image: 25.976, 60.0, 76.44399999999999, 4.0
2019-02-15 19:39:25,772 : Dev mean Image to text: 31.12, 64.2, 78.68, 3.0
2019-02-15 19:39:29,710 : 
Test scores | Image to text:             31.4, 64.8, 79.06, 3.2
2019-02-15 19:39:29,710 : Test scores | Text to image:             25.34, 59.696, 76.18, 4.0

2019-02-15 19:39:29,817 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 19:39:30,034 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 19:39:30,711 : loading BERT model bert-base-uncased
2019-02-15 19:39:30,712 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:39:30,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:39:30,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ltpy_1m
2019-02-15 19:39:33,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:39:34,657 : Computing embeddings for train/dev/test
2019-02-15 19:41:10,032 : Computed embeddings
2019-02-15 19:41:10,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:41:53,912 : [('reg:1e-05', 70.47), ('reg:0.0001', 77.76), ('reg:0.001', 74.24), ('reg:0.01', 57.84)]
2019-02-15 19:41:53,913 : Validation : best param found is reg = 0.0001 with score             77.76
2019-02-15 19:41:53,913 : Evaluating...
2019-02-15 19:42:06,589 : 
Dev acc : 77.8 Test acc : 78.4 for LENGTH classification

2019-02-15 19:42:06,590 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 19:42:06,921 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 19:42:06,968 : loading BERT model bert-base-uncased
2019-02-15 19:42:06,968 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:42:07,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:42:07,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgp29_9mh
2019-02-15 19:42:09,556 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:42:10,994 : Computing embeddings for train/dev/test
2019-02-15 19:43:38,766 : Computed embeddings
2019-02-15 19:43:38,766 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:44:40,619 : [('reg:1e-05', 33.49), ('reg:0.0001', 13.49), ('reg:0.001', 0.46), ('reg:0.01', 0.28)]
2019-02-15 19:44:40,619 : Validation : best param found is reg = 1e-05 with score             33.49
2019-02-15 19:44:40,619 : Evaluating...
2019-02-15 19:45:01,019 : 
Dev acc : 33.5 Test acc : 34.1 for WORDCONTENT classification

2019-02-15 19:45:01,020 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 19:45:01,399 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 19:45:01,468 : loading BERT model bert-base-uncased
2019-02-15 19:45:01,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:45:01,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:45:01,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ds5er7e
2019-02-15 19:45:04,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:45:05,567 : Computing embeddings for train/dev/test
2019-02-15 19:46:29,096 : Computed embeddings
2019-02-15 19:46:29,097 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:47:12,479 : [('reg:1e-05', 25.61), ('reg:0.0001', 25.18), ('reg:0.001', 27.54), ('reg:0.01', 24.65)]
2019-02-15 19:47:12,479 : Validation : best param found is reg = 0.001 with score             27.54
2019-02-15 19:47:12,479 : Evaluating...
2019-02-15 19:47:25,020 : 
Dev acc : 27.5 Test acc : 27.6 for DEPTH classification

2019-02-15 19:47:25,021 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 19:47:25,604 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 19:47:25,669 : loading BERT model bert-base-uncased
2019-02-15 19:47:25,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:47:25,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:47:25,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf7x19ixk
2019-02-15 19:47:28,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:47:29,654 : Computing embeddings for train/dev/test
2019-02-15 19:48:46,460 : Computed embeddings
2019-02-15 19:48:46,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:49:30,835 : [('reg:1e-05', 49.58), ('reg:0.0001', 48.09), ('reg:0.001', 46.62), ('reg:0.01', 34.79)]
2019-02-15 19:49:30,836 : Validation : best param found is reg = 1e-05 with score             49.58
2019-02-15 19:49:30,836 : Evaluating...
2019-02-15 19:49:41,406 : 
Dev acc : 49.6 Test acc : 49.9 for TOPCONSTITUENTS classification

2019-02-15 19:49:41,408 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 19:49:41,832 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 19:49:41,903 : loading BERT model bert-base-uncased
2019-02-15 19:49:41,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:49:41,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:49:41,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaxdpicgq
2019-02-15 19:49:44,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:49:45,981 : Computing embeddings for train/dev/test
2019-02-15 19:51:10,290 : Computed embeddings
2019-02-15 19:51:10,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:52:01,352 : [('reg:1e-05', 54.65), ('reg:0.0001', 54.62), ('reg:0.001', 54.62), ('reg:0.01', 50.16)]
2019-02-15 19:52:01,352 : Validation : best param found is reg = 1e-05 with score             54.65
2019-02-15 19:52:01,352 : Evaluating...
2019-02-15 19:52:14,098 : 
Dev acc : 54.6 Test acc : 53.6 for BIGRAMSHIFT classification

2019-02-15 19:52:14,099 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 19:52:14,521 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 19:52:14,591 : loading BERT model bert-base-uncased
2019-02-15 19:52:14,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:52:14,724 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:52:14,724 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfstr4j4y
2019-02-15 19:52:17,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:52:18,675 : Computing embeddings for train/dev/test
2019-02-15 19:53:46,105 : Computed embeddings
2019-02-15 19:53:46,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:54:37,427 : [('reg:1e-05', 82.43), ('reg:0.0001', 82.62), ('reg:0.001', 82.3), ('reg:0.01', 81.13)]
2019-02-15 19:54:37,427 : Validation : best param found is reg = 0.0001 with score             82.62
2019-02-15 19:54:37,427 : Evaluating...
2019-02-15 19:54:48,995 : 
Dev acc : 82.6 Test acc : 81.7 for TENSE classification

2019-02-15 19:54:48,997 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 19:54:49,399 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 19:54:49,469 : loading BERT model bert-base-uncased
2019-02-15 19:54:49,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:54:49,588 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:54:49,588 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgx4nlpko
2019-02-15 19:54:52,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:54:53,459 : Computing embeddings for train/dev/test
2019-02-15 19:56:20,113 : Computed embeddings
2019-02-15 19:56:20,113 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:57:10,858 : [('reg:1e-05', 76.33), ('reg:0.0001', 76.61), ('reg:0.001', 76.55), ('reg:0.01', 72.23)]
2019-02-15 19:57:10,859 : Validation : best param found is reg = 0.0001 with score             76.61
2019-02-15 19:57:10,859 : Evaluating...
2019-02-15 19:57:25,988 : 
Dev acc : 76.6 Test acc : 75.1 for SUBJNUMBER classification

2019-02-15 19:57:25,989 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 19:57:26,643 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 19:57:26,719 : loading BERT model bert-base-uncased
2019-02-15 19:57:26,719 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 19:57:26,754 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 19:57:26,754 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu2fd538f
2019-02-15 19:57:29,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 19:57:30,700 : Computing embeddings for train/dev/test
2019-02-15 19:58:56,082 : Computed embeddings
2019-02-15 19:58:56,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 19:59:46,200 : [('reg:1e-05', 74.08), ('reg:0.0001', 74.14), ('reg:0.001', 74.22), ('reg:0.01', 70.35)]
2019-02-15 19:59:46,200 : Validation : best param found is reg = 0.001 with score             74.22
2019-02-15 19:59:46,200 : Evaluating...
2019-02-15 19:59:59,955 : 
Dev acc : 74.2 Test acc : 74.8 for OBJNUMBER classification

2019-02-15 19:59:59,956 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 20:00:00,614 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 20:00:00,690 : loading BERT model bert-base-uncased
2019-02-15 20:00:00,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:00:00,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:00:00,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp87vtx17l
2019-02-15 20:00:03,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:00:04,661 : Computing embeddings for train/dev/test
2019-02-15 20:01:42,411 : Computed embeddings
2019-02-15 20:01:42,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:02:23,268 : [('reg:1e-05', 51.96), ('reg:0.0001', 51.8), ('reg:0.001', 51.63), ('reg:0.01', 51.64)]
2019-02-15 20:02:23,268 : Validation : best param found is reg = 1e-05 with score             51.96
2019-02-15 20:02:23,268 : Evaluating...
2019-02-15 20:02:33,557 : 
Dev acc : 52.0 Test acc : 51.4 for ODDMANOUT classification

2019-02-15 20:02:33,558 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 20:02:34,033 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 20:02:34,116 : loading BERT model bert-base-uncased
2019-02-15 20:02:34,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:02:34,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:02:34,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwpk7wm0_
2019-02-15 20:02:36,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:02:38,082 : Computing embeddings for train/dev/test
2019-02-15 20:04:16,627 : Computed embeddings
2019-02-15 20:04:16,627 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:05:05,771 : [('reg:1e-05', 50.38), ('reg:0.0001', 50.36), ('reg:0.001', 53.59), ('reg:0.01', 50.0)]
2019-02-15 20:05:05,771 : Validation : best param found is reg = 0.001 with score             53.59
2019-02-15 20:05:05,771 : Evaluating...
2019-02-15 20:05:20,105 : 
Dev acc : 53.6 Test acc : 52.8 for COORDINATIONINVERSION classification

2019-02-15 20:05:20,107 : total results: {'STS12': {'MSRpar': {'pearson': (0.38069638861979616, 2.8157054993071588e-27), 'spearman': SpearmanrResult(correlation=0.4125865398681763, pvalue=3.445092319161489e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7642346648213648, 1.2611967356420944e-144), 'spearman': SpearmanrResult(correlation=0.7660416614520523, pvalue=1.0383598356083297e-145), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5009663626201865, 1.558060622826133e-30), 'spearman': SpearmanrResult(correlation=0.5960898148899993, pvalue=1.6959359791444293e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6266758965382246, 4.504759056839864e-83), 'spearman': SpearmanrResult(correlation=0.6507550207364325, pvalue=1.5654109013263755e-91), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5401865335976657, 1.3270698620685984e-31), 'spearman': SpearmanrResult(correlation=0.4552595274414354, pvalue=8.265472671887956e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5625519692394475, 'wmean': 0.570844015390178}, 'spearman': {'mean': 0.5761465128776192, 'wmean': 0.5879315292876893}}}, 'STS13': {'FNWN': {'pearson': (0.22992137199438617, 0.001459209753732531), 'spearman': SpearmanrResult(correlation=0.23804877457335558, pvalue=0.0009721479618058084), 'nsamples': 189}, 'headlines': {'pearson': (0.6854022846599871, 3.766749933132417e-105), 'spearman': SpearmanrResult(correlation=0.6782259536883557, pvalue=3.5557642413927836e-102), 'nsamples': 750}, 'OnWN': {'pearson': (0.6428401750383972, 1.0065426137124007e-66), 'spearman': SpearmanrResult(correlation=0.6409609338456004, pvalue=3.1781216734650553e-66), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5193879438975901, 'wmean': 0.6120934606656467}, 'spearman': {'mean': 0.5190785540357705, 'wmean': 0.6088265116986752}}}, 'STS14': {'deft-forum': {'pearson': (0.3957486400249637, 2.530200224604935e-18), 'spearman': SpearmanrResult(correlation=0.406576800256275, pvalue=2.423468939694758e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.713501632358904, 5.884127498758998e-48), 'spearman': SpearmanrResult(correlation=0.6762143818073906, pvalue=1.932688842306899e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6616185437275139, 1.319061244646489e-95), 'spearman': SpearmanrResult(correlation=0.6394692718378472, pvalue=1.7932194128148609e-87), 'nsamples': 750}, 'images': {'pearson': (0.7105853051793857, 2.615109456359279e-116), 'spearman': SpearmanrResult(correlation=0.6973818391152311, pvalue=2.5884604195410694e-110), 'nsamples': 750}, 'OnWN': {'pearson': (0.7054077063175149, 6.432395192699002e-114), 'spearman': SpearmanrResult(correlation=0.7296906069490945, pvalue=1.31055551381845e-125), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6203133997860553, 5.839799721799749e-81), 'spearman': SpearmanrResult(correlation=0.5994941621901851, pvalue=2.2452144435558347e-74), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6345292045657229, 'wmean': 0.6441549583938019}, 'spearman': {'mean': 0.6248045103593373, 'wmean': 0.6360935425938158}}}, 'STS15': {'answers-forums': {'pearson': (0.5281476328208886, 2.5221490673201502e-28), 'spearman': SpearmanrResult(correlation=0.49788047188875023, pvalue=6.954086517580838e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.7205325141256118, 4.704096924251792e-121), 'spearman': SpearmanrResult(correlation=0.7332885936881175, pvalue=1.8894523240862917e-127), 'nsamples': 750}, 'belief': {'pearson': (0.5512099979070266, 3.4512852879396676e-31), 'spearman': SpearmanrResult(correlation=0.6033820307662148, pvalue=1.4758934809909849e-38), 'nsamples': 375}, 'headlines': {'pearson': (0.6861978223006284, 1.7412493094083542e-105), 'spearman': SpearmanrResult(correlation=0.6878476717414747, pvalue=3.486943326710338e-106), 'nsamples': 750}, 'images': {'pearson': (0.7864628950553668, 1.090432133284442e-158), 'spearman': SpearmanrResult(correlation=0.7969144616866566, pvalue=6.734336175793431e-166), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6545101724419045, 'wmean': 0.6832180117113912}, 'spearman': {'mean': 0.6638626459542427, 'wmean': 0.6921704946109328}}}, 'STS16': {'answer-answer': {'pearson': (0.4181422049357414, 3.5923090241615974e-12), 'spearman': SpearmanrResult(correlation=0.4364707758759629, pvalue=3.0813110749723756e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.691238633964886, 1.036041045975562e-36), 'spearman': SpearmanrResult(correlation=0.7036303351133384, pvalue=1.5935730668111904e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.713662104896694, 4.157780116519914e-37), 'spearman': SpearmanrResult(correlation=0.7265403156630837, pvalue=5.0529779703772695e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.791515094590184, 1.142287595967173e-53), 'spearman': SpearmanrResult(correlation=0.8409924699101883, pvalue=1.686134294775216e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.5769991377697243, 6.026447714152873e-20), 'spearman': SpearmanrResult(correlation=0.576105708908422, pvalue=7.080789690193989e-20), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.638311435231446, 'wmean': 0.6375980833988633}, 'spearman': {'mean': 0.656747921094199, 'wmean': 0.6566442318197059}}}, 'MR': {'devacc': 57.91, 'acc': 56.86, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.99, 'acc': 64.93, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.92, 'acc': 84.98, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 85.1, 'acc': 85.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.0, 'acc': 76.06, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.24, 'acc': 28.33, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 61.07, 'acc': 72.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.2, 'acc': 72.0, 'f1': 81.4, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.6, 'acc': 72.8, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7992206545120942, 'pearson': 0.8061241353457513, 'spearman': 0.7395668721473344, 'mse': 0.35651854256819004, 'yhat': array([3.34722076, 4.19541263, 1.40789467, ..., 3.03857946, 4.20813792,        4.43069232]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7017670416428679, 'pearson': 0.6680418259700962, 'spearman': 0.6642506073298712, 'mse': 1.3527185259098462, 'yhat': array([1.35163078, 1.6973677 , 2.56751219, ..., 3.71423694, 4.3902917 ,        3.49068192]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.83, 'acc': 60.57, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 337.32400000000007, 'acc': [(31.4, 64.8, 79.06, 3.2), (25.34, 59.696, 76.18, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 77.76, 'acc': 78.4, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 33.49, 'acc': 34.07, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.54, 'acc': 27.6, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.58, 'acc': 49.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 54.65, 'acc': 53.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.62, 'acc': 81.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.61, 'acc': 75.11, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.22, 'acc': 74.83, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.96, 'acc': 51.4, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.59, 'acc': 52.82, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 20:05:20,108 : STS12 p=0.5708, STS12 s=0.5879, STS13 p=0.6121, STS13 s=0.6088, STS14 p=0.6442, STS14 s=0.6361, STS15 p=0.6832, STS15 s=0.6922, STS 16 p=0.6376, STS16 s=0.6566, STS B p=0.6680, STS B s=0.6643, STS B m=1.3527, SICK-R p=0.8061, SICK-R s=0.7396, SICK-P m=0.3565
2019-02-15 20:05:20,108 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 20:05:20,108 : 0.5708,0.5879,0.6121,0.6088,0.6442,0.6361,0.6832,0.6922,0.6376,0.6566,0.6680,0.6643,1.3527,0.8061,0.7396,0.3565
2019-02-15 20:05:20,108 : MR=56.86, CR=64.93, SUBJ=85.10, MPQA=84.98, SST-B=76.06, SST-F=28.33, TREC=72.00, SICK-E=72.80, SNLI=60.57, MRPC=72.00, MRPC f=81.40
2019-02-15 20:05:20,108 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 20:05:20,108 : 56.86,64.93,85.10,84.98,76.06,28.33,72.00,72.80,60.57,72.00,81.40
2019-02-15 20:05:20,108 : COCO r1i2t=31.40, COCO r5i2t=64.80, COCO r10i2t=79.06, COCO medr_i2t=3.20, COCO r1t2i=25.34, COCO r5t2i=59.70, COCO r10t2i=76.18, COCO medr_t2i=4.00
2019-02-15 20:05:20,108 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 20:05:20,108 : 31.40,64.80,79.06,3.20,25.34,59.70,76.18,4.00
2019-02-15 20:05:20,108 : SentLen=78.40, WC=34.07, TreeDepth=27.60, TopConst=49.92, BShift=53.62, Tense=81.68, SubjNum=75.11, ObjNum=74.83, SOMO=51.40, CoordInv=52.82, average=57.95
2019-02-15 20:05:20,108 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 20:05:20,108 : 78.40,34.07,27.60,49.92,53.62,81.68,75.11,74.83,51.40,52.82,57.95
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,108 : layer 2
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,108 : ********************************************************************************
2019-02-15 20:05:20,205 : ***** Transfer task : STS12 *****


2019-02-15 20:05:20,217 : loading BERT model bert-base-uncased
2019-02-15 20:05:20,217 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:20,235 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:20,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpghtnu0tg
2019-02-15 20:05:22,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:05:26,038 : MSRpar : pearson = 0.3635, spearman = 0.3998
2019-02-15 20:05:26,815 : MSRvid : pearson = 0.7558, spearman = 0.7582
2019-02-15 20:05:27,446 : SMTeuroparl : pearson = 0.4896, spearman = 0.5895
2019-02-15 20:05:28,629 : surprise.OnWN : pearson = 0.6216, spearman = 0.6451
2019-02-15 20:05:29,272 : surprise.SMTnews : pearson = 0.5225, spearman = 0.4373
2019-02-15 20:05:29,272 : ALL (weighted average) : Pearson = 0.5595,             Spearman = 0.5783
2019-02-15 20:05:29,272 : ALL (average) : Pearson = 0.5506,             Spearman = 0.5660

2019-02-15 20:05:29,272 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 20:05:29,282 : loading BERT model bert-base-uncased
2019-02-15 20:05:29,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:29,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:29,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe7yw9eq3
2019-02-15 20:05:31,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:05:33,806 : FNWN : pearson = 0.2181, spearman = 0.2092
2019-02-15 20:05:34,676 : headlines : pearson = 0.6957, spearman = 0.6874
2019-02-15 20:05:35,365 : OnWN : pearson = 0.6474, spearman = 0.6457
2019-02-15 20:05:35,365 : ALL (weighted average) : Pearson = 0.6175,             Spearman = 0.6115
2019-02-15 20:05:35,365 : ALL (average) : Pearson = 0.5204,             Spearman = 0.5141

2019-02-15 20:05:35,365 : ***** Transfer task : STS14 *****


2019-02-15 20:05:35,381 : loading BERT model bert-base-uncased
2019-02-15 20:05:35,381 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:35,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:35,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9v9lb4on
2019-02-15 20:05:37,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:05:40,019 : deft-forum : pearson = 0.4194, spearman = 0.4359
2019-02-15 20:05:40,733 : deft-news : pearson = 0.7179, spearman = 0.6858
2019-02-15 20:05:41,697 : headlines : pearson = 0.6601, spearman = 0.6348
2019-02-15 20:05:42,653 : images : pearson = 0.6962, spearman = 0.6862
2019-02-15 20:05:43,607 : OnWN : pearson = 0.7154, spearman = 0.7384
2019-02-15 20:05:44,867 : tweet-news : pearson = 0.6095, spearman = 0.5925
2019-02-15 20:05:44,867 : ALL (weighted average) : Pearson = 0.6440,             Spearman = 0.6375
2019-02-15 20:05:44,867 : ALL (average) : Pearson = 0.6364,             Spearman = 0.6289

2019-02-15 20:05:44,867 : ***** Transfer task : STS15 *****


2019-02-15 20:05:44,903 : loading BERT model bert-base-uncased
2019-02-15 20:05:44,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:44,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:44,924 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp94cbs65u
2019-02-15 20:05:47,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:05:49,710 : answers-forums : pearson = 0.5480, spearman = 0.5250
2019-02-15 20:05:50,616 : answers-students : pearson = 0.7161, spearman = 0.7245
2019-02-15 20:05:51,475 : belief : pearson = 0.5699, spearman = 0.6127
2019-02-15 20:05:52,481 : headlines : pearson = 0.6977, spearman = 0.7011
2019-02-15 20:05:53,480 : images : pearson = 0.7854, spearman = 0.7933
2019-02-15 20:05:53,480 : ALL (weighted average) : Pearson = 0.6895,             Spearman = 0.6969
2019-02-15 20:05:53,481 : ALL (average) : Pearson = 0.6634,             Spearman = 0.6713

2019-02-15 20:05:53,481 : ***** Transfer task : STS16 *****


2019-02-15 20:05:53,575 : loading BERT model bert-base-uncased
2019-02-15 20:05:53,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:53,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:53,597 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg8145v2j
2019-02-15 20:05:56,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:05:57,952 : answer-answer : pearson = 0.4391, spearman = 0.4474
2019-02-15 20:05:58,266 : headlines : pearson = 0.7000, spearman = 0.7123
2019-02-15 20:05:58,661 : plagiarism : pearson = 0.7244, spearman = 0.7326
2019-02-15 20:05:59,293 : postediting : pearson = 0.7973, spearman = 0.8435
2019-02-15 20:05:59,582 : question-question : pearson = 0.5814, spearman = 0.5890
2019-02-15 20:05:59,582 : ALL (weighted average) : Pearson = 0.6480,             Spearman = 0.6648
2019-02-15 20:05:59,582 : ALL (average) : Pearson = 0.6484,             Spearman = 0.6650

2019-02-15 20:05:59,582 : ***** Transfer task : MR *****


2019-02-15 20:05:59,602 : loading BERT model bert-base-uncased
2019-02-15 20:05:59,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:05:59,624 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:05:59,624 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj0dqjzlx
2019-02-15 20:06:02,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:06:03,568 : Generating sentence embeddings
2019-02-15 20:06:17,745 : Generated sentence embeddings
2019-02-15 20:06:17,746 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 20:06:36,893 : Best param found at split 1: l2reg = 1e-05                 with score 60.89
2019-02-15 20:06:58,509 : Best param found at split 2: l2reg = 1e-05                 with score 60.05
2019-02-15 20:07:19,609 : Best param found at split 3: l2reg = 0.0001                 with score 58.59
2019-02-15 20:07:37,874 : Best param found at split 4: l2reg = 1e-05                 with score 57.7
2019-02-15 20:07:59,810 : Best param found at split 5: l2reg = 0.0001                 with score 61.66
2019-02-15 20:08:00,630 : Dev acc : 59.78 Test acc : 62.16

2019-02-15 20:08:00,631 : ***** Transfer task : CR *****


2019-02-15 20:08:00,639 : loading BERT model bert-base-uncased
2019-02-15 20:08:00,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:08:00,660 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:08:00,660 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxj87bha9
2019-02-15 20:08:03,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:08:04,621 : Generating sentence embeddings
2019-02-15 20:08:08,306 : Generated sentence embeddings
2019-02-15 20:08:08,306 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 20:08:13,462 : Best param found at split 1: l2reg = 0.01                 with score 69.09
2019-02-15 20:08:18,068 : Best param found at split 2: l2reg = 0.0001                 with score 67.54
2019-02-15 20:08:24,018 : Best param found at split 3: l2reg = 0.0001                 with score 68.41
2019-02-15 20:08:30,723 : Best param found at split 4: l2reg = 0.001                 with score 73.12
2019-02-15 20:08:36,000 : Best param found at split 5: l2reg = 1e-05                 with score 72.16
2019-02-15 20:08:36,200 : Dev acc : 70.06 Test acc : 67.23

2019-02-15 20:08:36,200 : ***** Transfer task : MPQA *****


2019-02-15 20:08:36,207 : loading BERT model bert-base-uncased
2019-02-15 20:08:36,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:08:36,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:08:36,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu33_pga0
2019-02-15 20:08:38,729 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:08:40,271 : Generating sentence embeddings
2019-02-15 20:08:44,050 : Generated sentence embeddings
2019-02-15 20:08:44,050 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 20:09:00,603 : Best param found at split 1: l2reg = 0.0001                 with score 87.01
2019-02-15 20:09:15,481 : Best param found at split 2: l2reg = 0.01                 with score 86.95
2019-02-15 20:09:27,859 : Best param found at split 3: l2reg = 1e-05                 with score 86.62
2019-02-15 20:09:39,943 : Best param found at split 4: l2reg = 0.001                 with score 86.06
2019-02-15 20:09:53,213 : Best param found at split 5: l2reg = 0.0001                 with score 86.74
2019-02-15 20:09:53,934 : Dev acc : 86.68 Test acc : 86.58

2019-02-15 20:09:53,935 : ***** Transfer task : SUBJ *****


2019-02-15 20:09:53,961 : loading BERT model bert-base-uncased
2019-02-15 20:09:53,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:09:53,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:09:53,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph3axln9b
2019-02-15 20:09:57,391 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:09:59,470 : Generating sentence embeddings
2019-02-15 20:10:13,360 : Generated sentence embeddings
2019-02-15 20:10:13,361 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 20:10:34,106 : Best param found at split 1: l2reg = 0.0001                 with score 88.26
2019-02-15 20:10:54,432 : Best param found at split 2: l2reg = 1e-05                 with score 87.61
2019-02-15 20:11:15,191 : Best param found at split 3: l2reg = 1e-05                 with score 88.6
2019-02-15 20:11:34,554 : Best param found at split 4: l2reg = 1e-05                 with score 89.76
2019-02-15 20:11:54,999 : Best param found at split 5: l2reg = 0.0001                 with score 88.25
2019-02-15 20:11:56,157 : Dev acc : 88.5 Test acc : 89.54

2019-02-15 20:11:56,158 : ***** Transfer task : SST Binary classification *****


2019-02-15 20:11:56,301 : loading BERT model bert-base-uncased
2019-02-15 20:11:56,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:11:56,330 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:11:56,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp931g4wv
2019-02-15 20:11:58,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:12:00,223 : Computing embedding for train
2019-02-15 20:12:45,049 : Computed train embeddings
2019-02-15 20:12:45,049 : Computing embedding for dev
2019-02-15 20:12:46,010 : Computed dev embeddings
2019-02-15 20:12:46,010 : Computing embedding for test
2019-02-15 20:12:48,052 : Computed test embeddings
2019-02-15 20:12:48,052 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:13:15,137 : [('reg:1e-05', 76.26), ('reg:0.0001', 76.38), ('reg:0.001', 76.49), ('reg:0.01', 72.71)]
2019-02-15 20:13:15,137 : Validation : best param found is reg = 0.001 with score             76.49
2019-02-15 20:13:15,138 : Evaluating...
2019-02-15 20:13:21,938 : 
Dev acc : 76.49 Test acc : 77.38 for             SST Binary classification

2019-02-15 20:13:21,939 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 20:13:21,991 : loading BERT model bert-base-uncased
2019-02-15 20:13:21,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:13:22,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:13:22,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0yte0mrh
2019-02-15 20:13:24,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:13:26,022 : Computing embedding for train
2019-02-15 20:13:35,466 : Computed train embeddings
2019-02-15 20:13:35,466 : Computing embedding for dev
2019-02-15 20:13:36,692 : Computed dev embeddings
2019-02-15 20:13:36,692 : Computing embedding for test
2019-02-15 20:13:39,116 : Computed test embeddings
2019-02-15 20:13:39,116 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:13:42,722 : [('reg:1e-05', 34.33), ('reg:0.0001', 30.52), ('reg:0.001', 33.51), ('reg:0.01', 37.51)]
2019-02-15 20:13:42,722 : Validation : best param found is reg = 0.01 with score             37.51
2019-02-15 20:13:42,722 : Evaluating...
2019-02-15 20:13:43,372 : 
Dev acc : 37.51 Test acc : 36.61 for             SST Fine-Grained classification

2019-02-15 20:13:43,372 : ***** Transfer task : TREC *****


2019-02-15 20:13:43,389 : loading BERT model bert-base-uncased
2019-02-15 20:13:43,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:13:43,413 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:13:43,413 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnp45gtt3
2019-02-15 20:13:45,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:13:50,797 : Computed train embeddings
2019-02-15 20:13:51,067 : Computed test embeddings
2019-02-15 20:13:51,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 20:14:03,505 : [('reg:1e-05', 67.34), ('reg:0.0001', 57.26), ('reg:0.001', 63.83), ('reg:0.01', 53.77)]
2019-02-15 20:14:03,505 : Cross-validation : best param found is reg = 1e-05             with score 67.34
2019-02-15 20:14:03,505 : Evaluating...
2019-02-15 20:14:04,258 : 
Dev acc : 67.34 Test acc : 70.6             for TREC

2019-02-15 20:14:04,258 : ***** Transfer task : MRPC *****


2019-02-15 20:14:04,312 : loading BERT model bert-base-uncased
2019-02-15 20:14:04,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:14:04,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:14:04,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpepfc7i98
2019-02-15 20:14:06,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:14:08,230 : Computing embedding for train
2019-02-15 20:14:18,288 : Computed train embeddings
2019-02-15 20:14:18,288 : Computing embedding for test
2019-02-15 20:14:22,406 : Computed test embeddings
2019-02-15 20:14:22,423 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 20:14:29,795 : [('reg:1e-05', 71.44), ('reg:0.0001', 70.85), ('reg:0.001', 70.66), ('reg:0.01', 71.83)]
2019-02-15 20:14:29,795 : Cross-validation : best param found is reg = 0.01             with score 71.83
2019-02-15 20:14:29,795 : Evaluating...
2019-02-15 20:14:30,264 : Dev acc : 71.83 Test acc 71.36; Test F1 80.24 for MRPC.

2019-02-15 20:14:30,264 : ***** Transfer task : SICK-Entailment*****


2019-02-15 20:14:30,299 : loading BERT model bert-base-uncased
2019-02-15 20:14:30,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:14:30,366 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:14:30,366 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3ogqggw6
2019-02-15 20:14:32,870 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:14:34,359 : Computing embedding for train
2019-02-15 20:14:39,528 : Computed train embeddings
2019-02-15 20:14:39,528 : Computing embedding for dev
2019-02-15 20:14:40,181 : Computed dev embeddings
2019-02-15 20:14:40,181 : Computing embedding for test
2019-02-15 20:14:45,696 : Computed test embeddings
2019-02-15 20:14:45,726 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:14:48,267 : [('reg:1e-05', 75.0), ('reg:0.0001', 77.8), ('reg:0.001', 76.6), ('reg:0.01', 76.2)]
2019-02-15 20:14:48,267 : Validation : best param found is reg = 0.0001 with score             77.8
2019-02-15 20:14:48,267 : Evaluating...
2019-02-15 20:14:48,572 : 
Dev acc : 77.8 Test acc : 75.4 for                        SICK entailment

2019-02-15 20:14:48,573 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 20:14:48,601 : loading BERT model bert-base-uncased
2019-02-15 20:14:48,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:14:48,624 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:14:48,624 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzsxzpvik
2019-02-15 20:14:51,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:14:52,565 : Computing embedding for train
2019-02-15 20:14:57,779 : Computed train embeddings
2019-02-15 20:14:57,779 : Computing embedding for dev
2019-02-15 20:14:58,467 : Computed dev embeddings
2019-02-15 20:14:58,468 : Computing embedding for test
2019-02-15 20:15:03,955 : Computed test embeddings
2019-02-15 20:15:28,672 : Dev : Pearson 0.7978968172552805
2019-02-15 20:15:28,672 : Test : Pearson 0.801687707607353 Spearman 0.7333321394894639 MSE 0.36452502670947                        for SICK Relatedness

2019-02-15 20:15:28,673 : 

***** Transfer task : STSBenchmark*****


2019-02-15 20:15:28,756 : loading BERT model bert-base-uncased
2019-02-15 20:15:28,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:15:28,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:15:28,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoak63m7r
2019-02-15 20:15:31,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:15:32,649 : Computing embedding for train
2019-02-15 20:15:41,034 : Computed train embeddings
2019-02-15 20:15:41,034 : Computing embedding for dev
2019-02-15 20:15:43,456 : Computed dev embeddings
2019-02-15 20:15:43,457 : Computing embedding for test
2019-02-15 20:15:45,442 : Computed test embeddings
2019-02-15 20:16:15,808 : Dev : Pearson 0.6805422197170821
2019-02-15 20:16:15,809 : Test : Pearson 0.6539986199305371 Spearman 0.648530565583907 MSE 1.4058035217290834                        for SICK Relatedness

2019-02-15 20:16:15,809 : ***** Transfer task : SNLI Entailment*****


2019-02-15 20:16:21,024 : loading BERT model bert-base-uncased
2019-02-15 20:16:21,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:16:21,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:16:21,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmvpqwcvz
2019-02-15 20:16:23,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:16:25,416 : PROGRESS (encoding): 0.00%
2019-02-15 20:17:43,499 : PROGRESS (encoding): 14.56%
2019-02-15 20:19:10,816 : PROGRESS (encoding): 29.12%
2019-02-15 20:20:38,278 : PROGRESS (encoding): 43.69%
2019-02-15 20:22:12,295 : PROGRESS (encoding): 58.25%
2019-02-15 20:23:57,246 : PROGRESS (encoding): 72.81%
2019-02-15 20:25:40,903 : PROGRESS (encoding): 87.37%
2019-02-15 20:27:35,048 : PROGRESS (encoding): 0.00%
2019-02-15 20:27:48,963 : PROGRESS (encoding): 0.00%
2019-02-15 20:28:02,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 20:28:38,379 : [('reg:1e-09', 62.81)]
2019-02-15 20:28:38,380 : Validation : best param found is reg = 1e-09 with score             62.81
2019-02-15 20:28:38,380 : Evaluating...
2019-02-15 20:29:13,785 : Dev acc : 62.81 Test acc : 62.9 for SNLI

2019-02-15 20:29:13,785 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 20:29:23,140 : loading BERT model bert-base-uncased
2019-02-15 20:29:23,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 20:29:23,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 20:29:23,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphlbi99uh
2019-02-15 20:29:25,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 20:29:27,154 : Computing embedding for train
2019-02-15 20:36:55,833 : Computed train embeddings
2019-02-15 20:36:55,833 : Computing embedding for dev
2019-02-15 20:37:15,149 : Computed dev embeddings
2019-02-15 20:37:15,149 : Computing embedding for test
2019-02-15 20:37:35,437 : Computed test embeddings
2019-02-15 20:37:35,453 : prepare data
2019-02-15 20:37:35,515 : start epoch
2019-02-15 20:38:18,465 : samples : 64000
2019-02-15 20:38:29,053 : Image to text: 7.2, 20.2, 30.6, 25.0
2019-02-15 20:38:36,809 : Text to Image: 5.32, 16.84, 26.816, 31.0
2019-02-15 20:39:19,963 : samples : 128000
2019-02-15 20:39:30,491 : Image to text: 8.5, 25.3, 36.44, 20.0
2019-02-15 20:39:38,046 : Text to Image: 6.848, 21.212, 32.12, 24.0
2019-02-15 20:40:20,810 : samples : 192000
2019-02-15 20:40:31,325 : Image to text: 7.98, 23.8, 34.98, 21.0
2019-02-15 20:40:38,999 : Text to Image: 6.376, 19.452, 29.888, 26.0
2019-02-15 20:41:21,629 : samples : 256000
2019-02-15 20:41:32,114 : Image to text: 8.42, 25.4, 36.06, 20.0
2019-02-15 20:41:39,742 : Text to Image: 7.148, 21.74, 32.536, 24.0
2019-02-15 20:42:22,880 : samples : 320000
2019-02-15 20:42:33,356 : Image to text: 8.5, 25.26, 37.38, 19.0
2019-02-15 20:42:41,056 : Text to Image: 6.924, 21.1, 31.68, 24.0
2019-02-15 20:43:29,923 : samples : 384000
2019-02-15 20:43:42,569 : Image to text: 8.76, 25.22, 36.52, 20.0
2019-02-15 20:43:50,228 : Text to Image: 6.692, 20.968, 31.82, 24.0
2019-02-15 20:44:33,204 : samples : 448000
2019-02-15 20:44:43,624 : Image to text: 9.16, 26.96, 38.56, 18.0
2019-02-15 20:44:51,142 : Text to Image: 7.508, 22.624, 34.024, 22.0
2019-02-15 20:45:33,754 : samples : 512000
2019-02-15 20:45:44,078 : Image to text: 9.92, 26.9, 39.72, 18.0
2019-02-15 20:45:51,594 : Text to Image: 7.688, 22.972, 34.14, 22.0
2019-02-15 20:46:28,118 : Epoch 1 finished
2019-02-15 20:46:28,582 : Image to text: 25.8, 57.2, 72.5, 4.0
2019-02-15 20:46:28,936 : Text to Image: 19.96, 51.0, 69.24, 5.0
2019-02-15 20:46:29,413 : Image to text: 23.8, 57.3, 70.7, 4.0
2019-02-15 20:46:29,766 : Text to Image: 19.64, 50.62, 68.58, 5.0
2019-02-15 20:46:30,236 : Image to text: 26.3, 57.5, 73.2, 4.0
2019-02-15 20:46:30,592 : Text to Image: 20.34, 50.54, 68.32, 5.0
2019-02-15 20:46:31,056 : Image to text: 24.6, 57.8, 73.6, 4.0
2019-02-15 20:46:31,396 : Text to Image: 20.9, 51.22, 69.08, 5.0
2019-02-15 20:46:31,852 : Image to text: 25.7, 56.1, 71.3, 4.0
2019-02-15 20:46:32,193 : Text to Image: 19.42, 51.8, 68.5, 5.0
2019-02-15 20:46:32,193 : Dev mean Text to Image: 20.052, 51.036, 68.744, 5.0
2019-02-15 20:46:32,193 : Dev mean Image to text: 25.240000000000002, 57.17999999999999, 72.26, 4.0
2019-02-15 20:46:32,193 : start epoch
2019-02-15 20:47:14,549 : samples : 64000
2019-02-15 20:47:25,075 : Image to text: 9.9, 28.04, 39.92, 17.0
2019-02-15 20:47:32,834 : Text to Image: 7.992, 23.776, 34.876, 21.0
2019-02-15 20:48:15,467 : samples : 128000
2019-02-15 20:48:25,983 : Image to text: 9.9, 28.16, 40.42, 16.0
2019-02-15 20:48:33,634 : Text to Image: 8.016, 24.324, 35.708, 20.0
2019-02-15 20:49:15,917 : samples : 192000
2019-02-15 20:49:26,439 : Image to text: 10.74, 28.86, 41.48, 16.0
2019-02-15 20:49:34,177 : Text to Image: 8.476, 24.6, 36.168, 20.0
2019-02-15 20:50:16,606 : samples : 256000
2019-02-15 20:50:27,163 : Image to text: 10.3, 28.44, 40.86, 16.0
2019-02-15 20:50:34,758 : Text to Image: 8.176, 24.292, 36.02, 20.0
2019-02-15 20:51:17,529 : samples : 320000
2019-02-15 20:51:28,138 : Image to text: 10.48, 29.84, 41.32, 16.0
2019-02-15 20:51:35,859 : Text to Image: 8.368, 24.82, 36.364, 20.0
2019-02-15 20:52:18,722 : samples : 384000
2019-02-15 20:52:29,240 : Image to text: 10.12, 28.6, 40.6, 16.0
2019-02-15 20:52:37,082 : Text to Image: 8.096, 24.9, 36.72, 19.0
2019-02-15 20:53:19,792 : samples : 448000
2019-02-15 20:53:30,179 : Image to text: 10.64, 28.54, 41.16, 16.0
2019-02-15 20:53:37,733 : Text to Image: 8.288, 24.652, 36.372, 20.0
2019-02-15 20:54:20,756 : samples : 512000
2019-02-15 20:54:31,140 : Image to text: 10.44, 30.04, 42.6, 15.0
2019-02-15 20:54:38,703 : Text to Image: 8.608, 25.22, 36.832, 19.0
2019-02-15 20:55:14,727 : Epoch 2 finished
2019-02-15 20:55:15,180 : Image to text: 28.6, 58.8, 76.4, 4.0
2019-02-15 20:55:15,521 : Text to Image: 21.94, 54.92, 72.88, 4.0
2019-02-15 20:55:15,978 : Image to text: 28.4, 60.7, 74.9, 4.0
2019-02-15 20:55:16,331 : Text to Image: 22.06, 54.4, 71.92, 5.0
2019-02-15 20:55:16,798 : Image to text: 28.7, 62.0, 75.8, 4.0
2019-02-15 20:55:17,142 : Text to Image: 23.0, 54.84, 71.42, 5.0
2019-02-15 20:55:17,607 : Image to text: 26.8, 59.6, 74.8, 4.0
2019-02-15 20:55:17,950 : Text to Image: 22.44, 55.28, 71.56, 5.0
2019-02-15 20:55:18,420 : Image to text: 29.0, 60.6, 75.2, 3.0
2019-02-15 20:55:18,766 : Text to Image: 22.9, 54.7, 70.84, 5.0
2019-02-15 20:55:18,766 : Dev mean Text to Image: 22.468000000000004, 54.82799999999999, 71.724, 4.8
2019-02-15 20:55:18,766 : Dev mean Image to text: 28.3, 60.34, 75.42, 3.8000000000000003
2019-02-15 20:55:18,766 : start epoch
2019-02-15 20:56:01,393 : samples : 64000
2019-02-15 20:56:11,864 : Image to text: 10.22, 29.54, 41.72, 15.0
2019-02-15 20:56:19,538 : Text to Image: 8.496, 24.864, 36.6, 20.0
2019-02-15 20:57:01,915 : samples : 128000
2019-02-15 20:57:12,385 : Image to text: 10.42, 28.32, 41.18, 16.0
2019-02-15 20:57:20,089 : Text to Image: 8.08, 23.844, 35.384, 20.0
2019-02-15 20:58:02,789 : samples : 192000
2019-02-15 20:58:13,268 : Image to text: 10.8, 29.84, 42.98, 15.0
2019-02-15 20:58:21,024 : Text to Image: 8.36, 25.1, 36.796, 19.0
2019-02-15 20:59:03,349 : samples : 256000
2019-02-15 20:59:13,867 : Image to text: 11.32, 30.42, 42.68, 15.0
2019-02-15 20:59:21,617 : Text to Image: 8.716, 25.844, 37.696, 18.0
2019-02-15 21:00:09,376 : samples : 320000
2019-02-15 21:00:22,365 : Image to text: 11.0, 29.36, 42.06, 15.0
2019-02-15 21:00:30,945 : Text to Image: 8.436, 25.512, 37.192, 19.0
2019-02-15 21:01:13,894 : samples : 384000
2019-02-15 21:01:24,455 : Image to text: 10.66, 29.66, 41.96, 15.0
2019-02-15 21:01:32,316 : Text to Image: 8.856, 26.028, 37.724, 18.0
2019-02-15 21:02:15,099 : samples : 448000
2019-02-15 21:02:25,466 : Image to text: 11.3, 30.92, 43.96, 14.0
2019-02-15 21:02:33,046 : Text to Image: 9.092, 26.384, 38.048, 18.0
2019-02-15 21:03:15,733 : samples : 512000
2019-02-15 21:03:26,145 : Image to text: 10.18, 29.6, 42.42, 15.0
2019-02-15 21:03:33,702 : Text to Image: 9.332, 26.836, 38.696, 18.0
2019-02-15 21:04:10,297 : Epoch 3 finished
2019-02-15 21:04:10,748 : Image to text: 28.3, 59.6, 74.6, 4.0
2019-02-15 21:04:11,090 : Text to Image: 23.28, 57.16, 74.44, 4.0
2019-02-15 21:04:11,548 : Image to text: 27.7, 59.2, 74.2, 4.0
2019-02-15 21:04:11,901 : Text to Image: 22.28, 55.82, 73.34, 5.0
2019-02-15 21:04:12,360 : Image to text: 27.2, 62.1, 76.6, 4.0
2019-02-15 21:04:12,706 : Text to Image: 24.08, 57.66, 73.68, 4.0
2019-02-15 21:04:13,174 : Image to text: 27.7, 62.6, 76.8, 3.0
2019-02-15 21:04:13,518 : Text to Image: 23.32, 56.9, 73.34, 4.0
2019-02-15 21:04:13,983 : Image to text: 29.7, 61.6, 75.3, 3.0
2019-02-15 21:04:14,329 : Text to Image: 24.12, 57.02, 73.12, 4.0
2019-02-15 21:04:14,329 : Dev mean Text to Image: 23.415999999999997, 56.91199999999999, 73.584, 4.2
2019-02-15 21:04:14,330 : Dev mean Image to text: 28.119999999999997, 61.02, 75.5, 3.6000000000000005
2019-02-15 21:04:14,330 : start epoch
2019-02-15 21:04:57,321 : samples : 64000
2019-02-15 21:05:07,847 : Image to text: 11.1, 30.1, 43.5, 14.0
2019-02-15 21:05:15,569 : Text to Image: 8.996, 26.664, 39.016, 17.0
2019-02-15 21:05:58,268 : samples : 128000
2019-02-15 21:06:08,803 : Image to text: 10.98, 31.0, 44.12, 13.0
2019-02-15 21:06:16,526 : Text to Image: 9.22, 26.428, 38.352, 18.0
2019-02-15 21:06:59,434 : samples : 192000
2019-02-15 21:07:09,946 : Image to text: 10.82, 30.38, 43.3, 14.0
2019-02-15 21:07:17,745 : Text to Image: 8.888, 25.792, 37.528, 18.0
2019-02-15 21:08:00,145 : samples : 256000
2019-02-15 21:08:10,641 : Image to text: 11.62, 30.92, 43.88, 14.0
2019-02-15 21:08:18,492 : Text to Image: 9.332, 26.48, 38.832, 18.0
2019-02-15 21:09:01,861 : samples : 320000
2019-02-15 21:09:12,351 : Image to text: 11.54, 30.84, 44.04, 14.0
2019-02-15 21:09:20,082 : Text to Image: 9.532, 27.228, 39.34, 17.0
2019-02-15 21:10:02,740 : samples : 384000
2019-02-15 21:10:13,188 : Image to text: 10.62, 29.86, 42.48, 15.0
2019-02-15 21:10:20,920 : Text to Image: 8.912, 26.024, 37.792, 18.0
2019-02-15 21:11:03,651 : samples : 448000
2019-02-15 21:11:14,076 : Image to text: 11.62, 31.8, 44.8, 14.0
2019-02-15 21:11:21,642 : Text to Image: 9.676, 27.72, 39.96, 17.0
2019-02-15 21:12:04,101 : samples : 512000
2019-02-15 21:12:14,645 : Image to text: 11.58, 31.74, 45.0, 13.0
2019-02-15 21:12:22,248 : Text to Image: 9.544, 26.94, 39.432, 17.0
2019-02-15 21:12:59,017 : Epoch 4 finished
2019-02-15 21:12:59,490 : Image to text: 29.9, 61.1, 77.0, 3.0
2019-02-15 21:12:59,849 : Text to Image: 23.58, 56.66, 74.32, 4.0
2019-02-15 21:13:00,313 : Image to text: 29.3, 60.7, 75.4, 4.0
2019-02-15 21:13:00,690 : Text to Image: 23.38, 56.24, 73.74, 4.0
2019-02-15 21:13:01,155 : Image to text: 28.5, 62.5, 77.4, 3.0
2019-02-15 21:13:01,499 : Text to Image: 24.16, 58.02, 74.52, 4.0
2019-02-15 21:13:01,970 : Image to text: 28.6, 62.5, 77.2, 3.0
2019-02-15 21:13:02,327 : Text to Image: 24.34, 57.0, 73.16, 4.0
2019-02-15 21:13:02,807 : Image to text: 30.4, 63.1, 76.7, 3.0
2019-02-15 21:13:03,113 : Text to Image: 24.12, 57.18, 72.96, 4.0
2019-02-15 21:13:03,113 : Dev mean Text to Image: 23.915999999999997, 57.019999999999996, 73.74, 4.0
2019-02-15 21:13:03,113 : Dev mean Image to text: 29.339999999999996, 61.980000000000004, 76.74000000000001, 3.2
2019-02-15 21:13:03,114 : start epoch
2019-02-15 21:13:45,492 : samples : 64000
2019-02-15 21:13:55,918 : Image to text: 12.02, 30.66, 43.82, 14.0
2019-02-15 21:14:03,611 : Text to Image: 9.892, 27.152, 39.216, 17.0
2019-02-15 21:14:46,637 : samples : 128000
2019-02-15 21:14:57,080 : Image to text: 11.62, 31.54, 44.9, 13.0
2019-02-15 21:15:04,738 : Text to Image: 9.584, 27.636, 39.892, 17.0
2019-02-15 21:15:47,225 : samples : 192000
2019-02-15 21:15:57,656 : Image to text: 11.26, 31.58, 44.36, 13.0
2019-02-15 21:16:05,282 : Text to Image: 9.784, 27.956, 40.208, 16.0
2019-02-15 21:16:50,898 : samples : 256000
2019-02-15 21:17:03,549 : Image to text: 11.24, 31.22, 44.62, 14.0
2019-02-15 21:17:13,187 : Text to Image: 9.628, 27.152, 39.028, 17.0
2019-02-15 21:17:57,012 : samples : 320000
2019-02-15 21:18:07,423 : Image to text: 11.6, 30.24, 42.6, 14.0
2019-02-15 21:18:15,046 : Text to Image: 8.756, 25.692, 37.488, 19.0
2019-02-15 21:18:57,984 : samples : 384000
2019-02-15 21:19:08,396 : Image to text: 11.54, 31.68, 44.24, 14.0
2019-02-15 21:19:16,019 : Text to Image: 9.54, 27.4, 39.492, 17.0
2019-02-15 21:19:59,051 : samples : 448000
2019-02-15 21:20:09,438 : Image to text: 11.7, 32.32, 45.78, 13.0
2019-02-15 21:20:16,997 : Text to Image: 9.548, 27.408, 39.332, 17.0
2019-02-15 21:20:59,726 : samples : 512000
2019-02-15 21:21:10,139 : Image to text: 11.84, 31.96, 44.56, 13.0
2019-02-15 21:21:17,706 : Text to Image: 9.788, 27.788, 39.684, 17.0
2019-02-15 21:21:53,599 : Epoch 5 finished
2019-02-15 21:21:54,059 : Image to text: 29.1, 61.4, 75.8, 3.0
2019-02-15 21:21:54,399 : Text to Image: 22.82, 56.8, 74.34, 4.0
2019-02-15 21:21:54,851 : Image to text: 29.9, 61.4, 74.7, 3.0
2019-02-15 21:21:55,191 : Text to Image: 23.66, 56.72, 72.94, 4.0
2019-02-15 21:21:55,652 : Image to text: 29.0, 62.5, 76.9, 3.0
2019-02-15 21:21:56,023 : Text to Image: 24.6, 58.42, 73.38, 4.0
2019-02-15 21:21:56,447 : Image to text: 30.2, 63.5, 77.3, 3.0
2019-02-15 21:21:56,750 : Text to Image: 24.16, 57.54, 73.76, 4.0
2019-02-15 21:21:57,153 : Image to text: 29.6, 62.3, 76.8, 3.0
2019-02-15 21:21:57,456 : Text to Image: 23.38, 57.14, 73.08, 4.0
2019-02-15 21:21:57,456 : Dev mean Text to Image: 23.723999999999997, 57.324, 73.5, 4.0
2019-02-15 21:21:57,456 : Dev mean Image to text: 29.560000000000002, 62.220000000000006, 76.30000000000001, 3.0
2019-02-15 21:21:57,456 : start epoch
2019-02-15 21:22:40,158 : samples : 64000
2019-02-15 21:22:50,527 : Image to text: 11.88, 31.38, 45.06, 13.0
2019-02-15 21:22:58,137 : Text to Image: 9.952, 27.844, 39.984, 16.0
2019-02-15 21:23:40,660 : samples : 128000
2019-02-15 21:23:51,078 : Image to text: 12.6, 32.66, 45.56, 13.0
2019-02-15 21:23:58,735 : Text to Image: 10.068, 28.124, 40.028, 17.0
2019-02-15 21:24:41,720 : samples : 192000
2019-02-15 21:24:52,202 : Image to text: 11.44, 31.42, 44.42, 14.0
2019-02-15 21:24:59,864 : Text to Image: 9.112, 26.604, 38.88, 18.0
2019-02-15 21:25:42,779 : samples : 256000
2019-02-15 21:25:53,156 : Image to text: 12.32, 32.54, 45.58, 13.0
2019-02-15 21:26:00,759 : Text to Image: 10.364, 28.172, 40.536, 16.0
2019-02-15 21:26:43,696 : samples : 320000
2019-02-15 21:26:54,035 : Image to text: 11.8, 32.36, 45.44, 13.0
2019-02-15 21:27:01,723 : Text to Image: 9.984, 28.004, 40.008, 17.0
2019-02-15 21:27:44,929 : samples : 384000
2019-02-15 21:27:55,327 : Image to text: 11.04, 32.14, 45.84, 13.0
2019-02-15 21:28:02,914 : Text to Image: 10.068, 28.544, 40.676, 16.0
2019-02-15 21:28:45,932 : samples : 448000
2019-02-15 21:28:56,029 : Image to text: 12.36, 32.86, 45.4, 13.0
2019-02-15 21:29:05,400 : Text to Image: 10.256, 28.368, 40.504, 16.0
2019-02-15 21:29:50,477 : samples : 512000
2019-02-15 21:30:03,141 : Image to text: 12.22, 32.42, 45.9, 13.0
2019-02-15 21:30:13,202 : Text to Image: 9.812, 27.476, 39.948, 17.0
2019-02-15 21:30:50,630 : Epoch 6 finished
2019-02-15 21:30:51,107 : Image to text: 31.0, 63.2, 78.9, 3.0
2019-02-15 21:30:51,484 : Text to Image: 25.48, 59.74, 75.98, 4.0
2019-02-15 21:30:51,941 : Image to text: 28.1, 64.5, 78.0, 3.0
2019-02-15 21:30:52,313 : Text to Image: 24.72, 58.12, 75.08, 4.0
2019-02-15 21:30:52,774 : Image to text: 31.1, 63.5, 79.0, 3.0
2019-02-15 21:30:53,142 : Text to Image: 25.8, 60.06, 76.14, 4.0
2019-02-15 21:30:53,607 : Image to text: 28.8, 65.2, 79.0, 3.0
2019-02-15 21:30:53,977 : Text to Image: 25.18, 59.1, 75.68, 4.0
2019-02-15 21:30:54,442 : Image to text: 29.6, 63.1, 77.4, 3.0
2019-02-15 21:30:54,834 : Text to Image: 25.48, 58.52, 73.82, 4.0
2019-02-15 21:30:54,834 : Dev mean Text to Image: 25.331999999999997, 59.108000000000004, 75.34, 4.0
2019-02-15 21:30:54,834 : Dev mean Image to text: 29.72, 63.89999999999999, 78.46000000000001, 3.0
2019-02-15 21:30:54,835 : start epoch
2019-02-15 21:31:37,721 : samples : 64000
2019-02-15 21:31:50,281 : Image to text: 11.86, 32.52, 45.7, 13.0
2019-02-15 21:32:00,296 : Text to Image: 10.084, 28.32, 40.816, 16.0
2019-02-15 21:32:46,125 : samples : 128000
2019-02-15 21:32:58,557 : Image to text: 12.38, 32.58, 46.1, 13.0
2019-02-15 21:33:05,810 : Text to Image: 10.144, 28.132, 41.244, 16.0
2019-02-15 21:33:52,309 : samples : 192000
2019-02-15 21:34:05,330 : Image to text: 11.9, 32.5, 46.24, 13.0
2019-02-15 21:34:15,403 : Text to Image: 9.96, 28.284, 40.46, 16.0
2019-02-15 21:35:00,639 : samples : 256000
2019-02-15 21:35:13,254 : Image to text: 12.36, 32.7, 46.22, 13.0
2019-02-15 21:35:21,256 : Text to Image: 10.344, 28.856, 41.156, 16.0
2019-02-15 21:36:03,829 : samples : 320000
2019-02-15 21:36:15,211 : Image to text: 12.12, 32.46, 45.74, 13.0
2019-02-15 21:36:25,188 : Text to Image: 10.292, 28.524, 40.792, 16.0
2019-02-15 21:37:10,092 : samples : 384000
2019-02-15 21:37:22,689 : Image to text: 12.28, 32.14, 45.12, 13.0
2019-02-15 21:37:30,980 : Text to Image: 10.464, 28.752, 41.236, 16.0
2019-02-15 21:38:13,682 : samples : 448000
2019-02-15 21:38:23,984 : Image to text: 12.46, 32.5, 44.84, 13.0
2019-02-15 21:38:31,456 : Text to Image: 10.34, 28.984, 41.7, 15.0
2019-02-15 21:39:15,070 : samples : 512000
2019-02-15 21:39:27,623 : Image to text: 12.52, 33.82, 46.64, 12.0
2019-02-15 21:39:37,697 : Text to Image: 10.248, 28.616, 40.888, 16.0
2019-02-15 21:40:15,918 : Epoch 7 finished
2019-02-15 21:40:16,410 : Image to text: 31.1, 61.1, 77.3, 3.0
2019-02-15 21:40:16,772 : Text to Image: 23.48, 58.0, 75.62, 4.0
2019-02-15 21:40:17,223 : Image to text: 30.1, 62.2, 75.2, 3.0
2019-02-15 21:40:17,573 : Text to Image: 23.66, 56.66, 74.34, 4.0
2019-02-15 21:40:18,032 : Image to text: 28.0, 62.3, 77.0, 3.0
2019-02-15 21:40:18,401 : Text to Image: 24.86, 59.5, 74.78, 4.0
2019-02-15 21:40:18,862 : Image to text: 27.9, 62.7, 78.5, 3.0
2019-02-15 21:40:19,242 : Text to Image: 24.06, 57.9, 74.7, 4.0
2019-02-15 21:40:19,703 : Image to text: 29.5, 62.8, 76.2, 3.0
2019-02-15 21:40:20,080 : Text to Image: 24.02, 57.76, 74.06, 4.0
2019-02-15 21:40:20,080 : Dev mean Text to Image: 24.016, 57.964, 74.7, 4.0
2019-02-15 21:40:20,080 : Dev mean Image to text: 29.32, 62.22, 76.83999999999999, 3.0
2019-02-15 21:40:20,080 : start epoch
2019-02-15 21:41:02,575 : samples : 64000
2019-02-15 21:41:15,125 : Image to text: 12.36, 33.08, 45.86, 13.0
2019-02-15 21:41:25,122 : Text to Image: 9.624, 27.912, 40.1, 16.0
2019-02-15 21:42:10,061 : samples : 128000
2019-02-15 21:42:22,643 : Image to text: 12.74, 32.42, 45.88, 13.0
2019-02-15 21:42:30,324 : Text to Image: 9.656, 28.06, 40.612, 16.0
2019-02-15 21:43:12,692 : samples : 192000
2019-02-15 21:43:23,027 : Image to text: 11.94, 33.0, 46.14, 12.0
2019-02-15 21:43:30,239 : Text to Image: 9.976, 28.588, 40.892, 16.0
2019-02-15 21:44:14,145 : samples : 256000
2019-02-15 21:44:26,759 : Image to text: 12.22, 32.66, 46.08, 13.0
2019-02-15 21:44:36,816 : Text to Image: 10.436, 29.22, 41.648, 15.0
2019-02-15 21:45:20,888 : samples : 320000
2019-02-15 21:45:31,187 : Image to text: 12.72, 33.16, 46.72, 12.0
2019-02-15 21:45:38,637 : Text to Image: 10.448, 29.108, 41.628, 15.0
2019-02-15 21:46:22,089 : samples : 384000
2019-02-15 21:46:34,658 : Image to text: 12.3, 33.24, 46.24, 12.0
2019-02-15 21:46:44,641 : Text to Image: 10.48, 28.792, 41.28, 16.0
2019-02-15 21:47:30,389 : samples : 448000
2019-02-15 21:47:41,742 : Image to text: 12.36, 32.94, 46.42, 12.0
2019-02-15 21:47:49,142 : Text to Image: 10.316, 28.748, 40.884, 16.0
2019-02-15 21:48:31,662 : samples : 512000
2019-02-15 21:48:41,912 : Image to text: 12.04, 32.5, 46.04, 13.0
2019-02-15 21:48:49,267 : Text to Image: 10.296, 28.612, 40.752, 16.0
2019-02-15 21:49:26,622 : Epoch 8 finished
2019-02-15 21:49:27,540 : Image to text: 30.1, 62.4, 78.3, 3.0
2019-02-15 21:49:28,339 : Text to Image: 24.0, 57.98, 75.22, 4.0
2019-02-15 21:49:29,270 : Image to text: 29.4, 63.6, 77.5, 3.0
2019-02-15 21:49:30,088 : Text to Image: 23.56, 57.56, 74.98, 4.0
2019-02-15 21:49:31,043 : Image to text: 29.1, 64.7, 77.9, 3.0
2019-02-15 21:49:31,809 : Text to Image: 24.94, 59.26, 75.42, 4.0
2019-02-15 21:49:32,772 : Image to text: 30.3, 63.0, 77.2, 4.0
2019-02-15 21:49:33,529 : Text to Image: 24.4, 58.34, 74.44, 4.0
2019-02-15 21:49:34,522 : Image to text: 30.2, 63.7, 77.5, 3.0
2019-02-15 21:49:35,312 : Text to Image: 23.74, 57.46, 74.12, 4.0
2019-02-15 21:49:35,312 : Dev mean Text to Image: 24.128, 58.120000000000005, 74.836, 4.0
2019-02-15 21:49:35,312 : Dev mean Image to text: 29.82, 63.480000000000004, 77.68, 3.1999999999999997
2019-02-15 21:49:35,312 : start epoch
2019-02-15 21:50:21,302 : samples : 64000
2019-02-15 21:50:34,011 : Image to text: 11.52, 31.78, 45.38, 13.0
2019-02-15 21:50:42,725 : Text to Image: 9.94, 28.168, 40.036, 16.0
2019-02-15 21:51:29,419 : samples : 128000
2019-02-15 21:51:42,048 : Image to text: 13.06, 33.06, 46.42, 12.0
2019-02-15 21:51:52,118 : Text to Image: 10.116, 28.208, 40.384, 16.0
2019-02-15 21:52:37,786 : samples : 192000
2019-02-15 21:52:49,210 : Image to text: 12.7, 33.56, 46.92, 12.0
2019-02-15 21:52:56,614 : Text to Image: 10.792, 29.096, 41.712, 15.0
2019-02-15 21:53:39,398 : samples : 256000
2019-02-15 21:53:49,940 : Image to text: 12.6, 33.58, 46.78, 12.0
2019-02-15 21:53:58,157 : Text to Image: 10.492, 29.044, 41.392, 16.0
2019-02-15 21:54:41,576 : samples : 320000
2019-02-15 21:54:51,897 : Image to text: 12.5, 33.86, 46.52, 12.0
2019-02-15 21:54:59,325 : Text to Image: 10.576, 28.764, 41.144, 16.0
2019-02-15 21:55:42,035 : samples : 384000
2019-02-15 21:55:52,331 : Image to text: 13.06, 33.42, 45.98, 13.0
2019-02-15 21:55:59,660 : Text to Image: 10.604, 29.076, 41.16, 16.0
2019-02-15 21:56:42,323 : samples : 448000
2019-02-15 21:56:52,756 : Image to text: 12.88, 34.42, 47.22, 12.0
2019-02-15 21:57:00,808 : Text to Image: 10.5, 29.384, 41.812, 15.0
2019-02-15 21:57:43,573 : samples : 512000
2019-02-15 21:57:53,879 : Image to text: 12.62, 33.36, 47.18, 12.0
2019-02-15 21:58:01,262 : Text to Image: 10.34, 28.672, 41.392, 16.0
2019-02-15 21:58:38,314 : Epoch 9 finished
2019-02-15 21:58:39,417 : Image to text: 32.5, 63.5, 79.1, 3.0
2019-02-15 21:58:40,277 : Text to Image: 25.74, 60.52, 77.78, 4.0
2019-02-15 21:58:41,339 : Image to text: 31.4, 64.1, 78.0, 3.0
2019-02-15 21:58:42,185 : Text to Image: 25.66, 59.04, 75.72, 4.0
2019-02-15 21:58:43,213 : Image to text: 30.8, 65.0, 78.6, 3.0
2019-02-15 21:58:44,094 : Text to Image: 27.4, 60.9, 76.6, 4.0
2019-02-15 21:58:44,877 : Image to text: 30.9, 65.5, 78.4, 3.0
2019-02-15 21:58:45,250 : Text to Image: 26.1, 59.54, 75.6, 4.0
2019-02-15 21:58:45,713 : Image to text: 33.0, 65.2, 79.5, 3.0
2019-02-15 21:58:46,083 : Text to Image: 26.38, 59.82, 75.3, 4.0
2019-02-15 21:58:46,083 : Dev mean Text to Image: 26.255999999999997, 59.964, 76.2, 4.0
2019-02-15 21:58:46,083 : Dev mean Image to text: 31.72, 64.66, 78.72, 3.0
2019-02-15 21:58:46,083 : start epoch
2019-02-15 21:59:28,542 : samples : 64000
2019-02-15 21:59:38,847 : Image to text: 12.84, 33.2, 46.5, 12.0
2019-02-15 21:59:46,230 : Text to Image: 10.36, 28.448, 40.972, 16.0
2019-02-15 22:00:29,397 : samples : 128000
2019-02-15 22:00:42,092 : Image to text: 12.24, 33.58, 46.44, 12.0
2019-02-15 22:00:52,253 : Text to Image: 10.756, 29.56, 41.884, 15.0
2019-02-15 22:01:35,796 : samples : 192000
2019-02-15 22:01:46,674 : Image to text: 12.46, 33.26, 46.34, 13.0
2019-02-15 22:01:56,758 : Text to Image: 10.456, 28.928, 41.336, 16.0
2019-02-15 22:02:39,839 : samples : 256000
2019-02-15 22:02:50,372 : Image to text: 12.94, 33.56, 46.92, 12.0
2019-02-15 22:02:58,711 : Text to Image: 10.592, 29.008, 41.368, 16.0
2019-02-15 22:03:42,234 : samples : 320000
2019-02-15 22:03:52,664 : Image to text: 12.64, 32.6, 45.86, 13.0
2019-02-15 22:04:00,095 : Text to Image: 10.148, 28.56, 41.384, 16.0
2019-02-15 22:04:43,248 : samples : 384000
2019-02-15 22:04:53,497 : Image to text: 13.24, 34.8, 47.9, 12.0
2019-02-15 22:05:00,646 : Text to Image: 10.396, 28.984, 41.5, 15.0
2019-02-15 22:05:45,356 : samples : 448000
2019-02-15 22:05:57,846 : Image to text: 12.72, 33.84, 47.08, 12.0
2019-02-15 22:06:07,766 : Text to Image: 10.768, 29.34, 42.024, 15.0
2019-02-15 22:06:52,759 : samples : 512000
2019-02-15 22:07:05,197 : Image to text: 12.74, 34.02, 47.42, 12.0
2019-02-15 22:07:15,335 : Text to Image: 10.596, 29.144, 41.932, 15.0
2019-02-15 22:08:00,172 : Epoch 10 finished
2019-02-15 22:08:01,062 : Image to text: 31.9, 64.2, 79.3, 3.0
2019-02-15 22:08:01,852 : Text to Image: 26.46, 61.04, 77.48, 4.0
2019-02-15 22:08:02,813 : Image to text: 30.3, 65.0, 77.0, 3.0
2019-02-15 22:08:03,521 : Text to Image: 24.78, 59.2, 76.1, 4.0
2019-02-15 22:08:04,472 : Image to text: 29.7, 66.0, 80.4, 3.0
2019-02-15 22:08:05,309 : Text to Image: 26.72, 61.26, 76.54, 4.0
2019-02-15 22:08:06,341 : Image to text: 29.8, 64.7, 79.6, 3.0
2019-02-15 22:08:07,058 : Text to Image: 26.1, 59.74, 76.12, 4.0
2019-02-15 22:08:07,926 : Image to text: 32.5, 66.3, 79.4, 3.0
2019-02-15 22:08:08,640 : Text to Image: 26.0, 59.94, 75.42, 4.0
2019-02-15 22:08:08,640 : Dev mean Text to Image: 26.012, 60.236, 76.33200000000001, 4.0
2019-02-15 22:08:08,640 : Dev mean Image to text: 30.840000000000003, 65.24000000000001, 79.14, 3.0
2019-02-15 22:08:08,641 : start epoch
2019-02-15 22:08:53,630 : samples : 64000
2019-02-15 22:09:06,101 : Image to text: 12.94, 33.92, 46.44, 12.0
2019-02-15 22:09:16,024 : Text to Image: 10.684, 29.748, 42.312, 15.0
2019-02-15 22:10:01,217 : samples : 128000
2019-02-15 22:10:13,766 : Image to text: 12.34, 33.82, 47.22, 12.0
2019-02-15 22:10:23,725 : Text to Image: 10.372, 29.144, 41.732, 15.0
2019-02-15 22:11:08,198 : samples : 192000
2019-02-15 22:11:20,828 : Image to text: 13.08, 33.56, 47.42, 12.0
2019-02-15 22:11:30,893 : Text to Image: 10.512, 28.856, 41.164, 16.0
2019-02-15 22:12:15,695 : samples : 256000
2019-02-15 22:12:28,272 : Image to text: 12.94, 33.56, 46.9, 12.0
2019-02-15 22:12:38,303 : Text to Image: 10.98, 29.012, 41.876, 15.0
2019-02-15 22:13:22,748 : samples : 320000
2019-02-15 22:13:35,366 : Image to text: 12.2, 33.98, 47.12, 12.0
2019-02-15 22:13:45,417 : Text to Image: 10.452, 28.972, 41.56, 15.0
2019-02-15 22:14:30,020 : samples : 384000
2019-02-15 22:14:42,658 : Image to text: 12.86, 35.12, 47.6, 12.0
2019-02-15 22:14:52,774 : Text to Image: 11.012, 29.692, 42.344, 15.0
2019-02-15 22:15:39,099 : samples : 448000
2019-02-15 22:15:51,718 : Image to text: 13.44, 33.68, 47.28, 12.0
2019-02-15 22:16:01,779 : Text to Image: 10.592, 29.668, 42.404, 15.0
2019-02-15 22:16:48,116 : samples : 512000
2019-02-15 22:16:58,714 : Image to text: 12.66, 33.34, 46.7, 12.0
2019-02-15 22:17:06,307 : Text to Image: 10.74, 29.54, 42.476, 15.0
2019-02-15 22:17:43,510 : Epoch 11 finished
2019-02-15 22:17:44,000 : Image to text: 31.6, 62.8, 78.5, 3.0
2019-02-15 22:17:44,376 : Text to Image: 25.44, 59.7, 76.64, 4.0
2019-02-15 22:17:44,846 : Image to text: 31.1, 65.1, 78.2, 3.0
2019-02-15 22:17:45,217 : Text to Image: 25.2, 58.64, 75.9, 4.0
2019-02-15 22:17:45,686 : Image to text: 31.0, 64.6, 80.4, 3.0
2019-02-15 22:17:46,052 : Text to Image: 25.58, 61.4, 76.78, 4.0
2019-02-15 22:17:46,519 : Image to text: 33.4, 66.4, 80.3, 3.0
2019-02-15 22:17:46,891 : Text to Image: 24.78, 59.12, 75.76, 4.0
2019-02-15 22:17:47,354 : Image to text: 30.9, 65.2, 78.9, 3.0
2019-02-15 22:17:47,721 : Text to Image: 25.78, 58.78, 75.28, 4.0
2019-02-15 22:17:47,721 : Dev mean Text to Image: 25.356, 59.528, 76.072, 4.0
2019-02-15 22:17:47,721 : Dev mean Image to text: 31.6, 64.82000000000001, 79.26, 3.0
2019-02-15 22:17:51,882 : 
Test scores | Image to text:             30.599999999999998, 65.08, 78.5, 3.0
2019-02-15 22:17:51,882 : Test scores | Text to image:             25.560000000000002, 59.68799999999999, 76.24799999999999, 4.0

2019-02-15 22:17:52,000 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 22:17:52,229 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 22:17:52,923 : loading BERT model bert-base-uncased
2019-02-15 22:17:52,923 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:17:52,958 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:17:52,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprvag3bce
2019-02-15 22:17:55,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:17:57,016 : Computing embeddings for train/dev/test
2019-02-15 22:19:36,410 : Computed embeddings
2019-02-15 22:19:36,410 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:20:26,845 : [('reg:1e-05', 70.8), ('reg:0.0001', 59.68), ('reg:0.001', 65.67), ('reg:0.01', 55.2)]
2019-02-15 22:20:26,845 : Validation : best param found is reg = 1e-05 with score             70.8
2019-02-15 22:20:26,845 : Evaluating...
2019-02-15 22:20:48,350 : 
Dev acc : 70.8 Test acc : 71.6 for LENGTH classification

2019-02-15 22:20:48,351 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 22:20:48,717 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 22:20:48,763 : loading BERT model bert-base-uncased
2019-02-15 22:20:48,763 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:20:48,866 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:20:48,866 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl6aokkcx
2019-02-15 22:20:51,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:20:52,979 : Computing embeddings for train/dev/test
2019-02-15 22:23:10,607 : Computed embeddings
2019-02-15 22:23:10,607 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:24:32,696 : [('reg:1e-05', 36.57), ('reg:0.0001', 27.15), ('reg:0.001', 0.77), ('reg:0.01', 0.2)]
2019-02-15 22:24:32,696 : Validation : best param found is reg = 1e-05 with score             36.57
2019-02-15 22:24:32,697 : Evaluating...
2019-02-15 22:24:58,864 : 
Dev acc : 36.6 Test acc : 36.4 for WORDCONTENT classification

2019-02-15 22:24:58,865 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 22:24:59,405 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 22:24:59,471 : loading BERT model bert-base-uncased
2019-02-15 22:24:59,472 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:24:59,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:24:59,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb72f81at
2019-02-15 22:25:02,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:25:03,687 : Computing embeddings for train/dev/test
2019-02-15 22:27:21,530 : Computed embeddings
2019-02-15 22:27:21,530 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:29:08,251 : [('reg:1e-05', 25.76), ('reg:0.0001', 27.13), ('reg:0.001', 22.85), ('reg:0.01', 25.28)]
2019-02-15 22:29:08,252 : Validation : best param found is reg = 0.0001 with score             27.13
2019-02-15 22:29:08,252 : Evaluating...
2019-02-15 22:29:37,387 : 
Dev acc : 27.1 Test acc : 26.9 for DEPTH classification

2019-02-15 22:29:37,388 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 22:29:37,985 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 22:29:38,055 : loading BERT model bert-base-uncased
2019-02-15 22:29:38,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:29:38,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:29:38,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwmee5grr
2019-02-15 22:29:40,626 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:29:42,217 : Computing embeddings for train/dev/test
2019-02-15 22:31:29,967 : Computed embeddings
2019-02-15 22:31:29,968 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:32:30,362 : [('reg:1e-05', 53.37), ('reg:0.0001', 46.28), ('reg:0.001', 48.14), ('reg:0.01', 37.65)]
2019-02-15 22:32:30,362 : Validation : best param found is reg = 1e-05 with score             53.37
2019-02-15 22:32:30,362 : Evaluating...
2019-02-15 22:32:51,705 : 
Dev acc : 53.4 Test acc : 53.8 for TOPCONSTITUENTS classification

2019-02-15 22:32:51,706 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 22:32:52,123 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 22:32:52,194 : loading BERT model bert-base-uncased
2019-02-15 22:32:52,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:32:52,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:32:52,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpergemduj
2019-02-15 22:32:54,759 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:32:56,264 : Computing embeddings for train/dev/test
2019-02-15 22:34:21,159 : Computed embeddings
2019-02-15 22:34:21,159 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:35:42,633 : [('reg:1e-05', 61.4), ('reg:0.0001', 61.37), ('reg:0.001', 61.27), ('reg:0.01', 56.91)]
2019-02-15 22:35:42,633 : Validation : best param found is reg = 1e-05 with score             61.4
2019-02-15 22:35:42,633 : Evaluating...
2019-02-15 22:36:06,237 : 
Dev acc : 61.4 Test acc : 61.5 for BIGRAMSHIFT classification

2019-02-15 22:36:06,238 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 22:36:06,658 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 22:36:06,727 : loading BERT model bert-base-uncased
2019-02-15 22:36:06,727 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:36:06,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:36:06,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5bzko8qe
2019-02-15 22:36:09,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:36:10,922 : Computing embeddings for train/dev/test
2019-02-15 22:37:33,287 : Computed embeddings
2019-02-15 22:37:33,288 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:38:16,508 : [('reg:1e-05', 85.13), ('reg:0.0001', 85.08), ('reg:0.001', 85.12), ('reg:0.01', 85.07)]
2019-02-15 22:38:16,508 : Validation : best param found is reg = 1e-05 with score             85.13
2019-02-15 22:38:16,509 : Evaluating...
2019-02-15 22:38:27,461 : 
Dev acc : 85.1 Test acc : 83.6 for TENSE classification

2019-02-15 22:38:27,462 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 22:38:27,944 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 22:38:28,011 : loading BERT model bert-base-uncased
2019-02-15 22:38:28,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:38:28,146 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:38:28,146 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptd8qsqpy
2019-02-15 22:38:30,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:38:32,257 : Computing embeddings for train/dev/test
2019-02-15 22:39:59,857 : Computed embeddings
2019-02-15 22:39:59,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:40:55,367 : [('reg:1e-05', 75.24), ('reg:0.0001', 75.25), ('reg:0.001', 75.16), ('reg:0.01', 75.59)]
2019-02-15 22:40:55,367 : Validation : best param found is reg = 0.01 with score             75.59
2019-02-15 22:40:55,367 : Evaluating...
2019-02-15 22:41:05,283 : 
Dev acc : 75.6 Test acc : 74.5 for SUBJNUMBER classification

2019-02-15 22:41:05,284 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 22:41:06,035 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 22:41:06,126 : loading BERT model bert-base-uncased
2019-02-15 22:41:06,126 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:41:06,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:41:06,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp439pxhgl
2019-02-15 22:41:09,072 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:41:11,092 : Computing embeddings for train/dev/test
2019-02-15 22:42:41,208 : Computed embeddings
2019-02-15 22:42:41,208 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:43:43,329 : [('reg:1e-05', 73.87), ('reg:0.0001', 73.83), ('reg:0.001', 74.13), ('reg:0.01', 71.64)]
2019-02-15 22:43:43,329 : Validation : best param found is reg = 0.001 with score             74.13
2019-02-15 22:43:43,329 : Evaluating...
2019-02-15 22:44:01,095 : 
Dev acc : 74.1 Test acc : 74.3 for OBJNUMBER classification

2019-02-15 22:44:01,096 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 22:44:01,571 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 22:44:01,645 : loading BERT model bert-base-uncased
2019-02-15 22:44:01,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:44:01,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:44:01,680 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpas1se_4u
2019-02-15 22:44:04,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:44:05,769 : Computing embeddings for train/dev/test
2019-02-15 22:45:45,784 : Computed embeddings
2019-02-15 22:45:45,784 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:46:33,920 : [('reg:1e-05', 53.19), ('reg:0.0001', 53.21), ('reg:0.001', 53.2), ('reg:0.01', 53.43)]
2019-02-15 22:46:33,920 : Validation : best param found is reg = 0.01 with score             53.43
2019-02-15 22:46:33,920 : Evaluating...
2019-02-15 22:46:45,776 : 
Dev acc : 53.4 Test acc : 52.3 for ODDMANOUT classification

2019-02-15 22:46:45,777 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 22:46:46,234 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 22:46:46,319 : loading BERT model bert-base-uncased
2019-02-15 22:46:46,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:46:46,355 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:46:46,355 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdejzfj_r
2019-02-15 22:46:48,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:46:50,351 : Computing embeddings for train/dev/test
2019-02-15 22:48:27,782 : Computed embeddings
2019-02-15 22:48:27,782 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:49:25,648 : [('reg:1e-05', 50.16), ('reg:0.0001', 50.15), ('reg:0.001', 50.09), ('reg:0.01', 50.12)]
2019-02-15 22:49:25,648 : Validation : best param found is reg = 1e-05 with score             50.16
2019-02-15 22:49:25,648 : Evaluating...
2019-02-15 22:49:37,684 : 
Dev acc : 50.2 Test acc : 50.2 for COORDINATIONINVERSION classification

2019-02-15 22:49:37,686 : total results: {'STS12': {'MSRpar': {'pearson': (0.3634798165699589, 7.673853849198575e-25), 'spearman': SpearmanrResult(correlation=0.3997694419428671, pvalue=3.768303683766432e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7557501835215009, 1.1617117874998997e-139), 'spearman': SpearmanrResult(correlation=0.7581860004545258, pvalue=4.5808026483292326e-141), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48961696941393046, 4.7948323518174923e-29), 'spearman': SpearmanrResult(correlation=0.5894611547256319, pvalue=2.7326498951014033e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6215585200458827, 2.27392023941898e-81), 'spearman': SpearmanrResult(correlation=0.6451086182352678, pvalue=1.765525820392722e-89), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5225457373554884, 2.451699030501157e-29), 'spearman': SpearmanrResult(correlation=0.43725411380960116, pvalue=4.618036235212196e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5505902453813523, 'wmean': 0.5594663218368213}, 'spearman': {'mean': 0.5659558658335787, 'wmean': 0.5782899314361619}}}, 'STS13': {'FNWN': {'pearson': (0.21807781148377023, 0.0025735827001728434), 'spearman': SpearmanrResult(correlation=0.20919975019973863, pvalue=0.003864963970377738), 'nsamples': 189}, 'headlines': {'pearson': (0.695725960220636, 1.3863848432541341e-109), 'spearman': SpearmanrResult(correlation=0.6873607457725012, pvalue=5.611211509716322e-106), 'nsamples': 750}, 'OnWN': {'pearson': (0.6474096221519898, 5.942014461117843e-68), 'spearman': SpearmanrResult(correlation=0.6456511757253226, pvalue=1.7755784341377614e-67), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5204044646187987, 'wmean': 0.6174719830421173}, 'spearman': {'mean': 0.5140705572325208, 'wmean': 0.6115130811326882}}}, 'STS14': {'deft-forum': {'pearson': (0.41942977486781613, 1.3365097935154013e-20), 'spearman': SpearmanrResult(correlation=0.43591941927240585, pvalue=2.692676977963053e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7178704520501891, 8.65922235191604e-49), 'spearman': SpearmanrResult(correlation=0.6857823498319265, pvalue=5.098535573857949e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.6601415931441176, 4.832788244917321e-95), 'spearman': SpearmanrResult(correlation=0.6347780481115839, pvalue=7.784471956664213e-86), 'nsamples': 750}, 'images': {'pearson': (0.6962274893278043, 8.349489595758187e-110), 'spearman': SpearmanrResult(correlation=0.6861508005433177, pvalue=1.8226277867111263e-105), 'nsamples': 750}, 'OnWN': {'pearson': (0.7154079514698037, 1.3882487803546206e-118), 'spearman': SpearmanrResult(correlation=0.738362218281524, pvalue=4.251350780170232e-130), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6095389619574102, 1.7190514323920226e-77), 'spearman': SpearmanrResult(correlation=0.5925470533527082, pvalue=2.770376411813423e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6364360371361902, 'wmean': 0.6440244083279802}, 'spearman': {'mean': 0.6289233148989111, 'wmean': 0.6375405423570696}}}, 'STS15': {'answers-forums': {'pearson': (0.5479755689987685, 8.975859061246794e-31), 'spearman': SpearmanrResult(correlation=0.5250348925920255, pvalue=5.911555088648052e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.7160583558855695, 6.792276605625499e-119), 'spearman': SpearmanrResult(correlation=0.7245155203484552, pvalue=5.1772729938540527e-123), 'nsamples': 750}, 'belief': {'pearson': (0.5698822147925212, 1.1243267897527341e-33), 'spearman': SpearmanrResult(correlation=0.6126539366003817, pvalue=5.171230340304649e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6977468190119592, 1.7853416015797153e-110), 'spearman': SpearmanrResult(correlation=0.7010552535594983, pvalue=6.000538979801117e-112), 'nsamples': 750}, 'images': {'pearson': (0.7854117957038306, 5.497123701788706e-158), 'spearman': SpearmanrResult(correlation=0.7932605149017111, pvalue=2.489299104462609e-163), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6634149508785299, 'wmean': 0.6895364656242511}, 'spearman': {'mean': 0.6713040236004143, 'wmean': 0.6969189258514671}}}, 'STS16': {'answer-answer': {'pearson': (0.43911276605393923, 2.1360648175300349e-13), 'spearman': SpearmanrResult(correlation=0.4473942529805447, pvalue=6.634625212330925e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.6999669069894656, 5.598427941943264e-38), 'spearman': SpearmanrResult(correlation=0.712333943836619, pvalue=7.442370559591558e-40), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7243775499613481, 1.0784243533502777e-38), 'spearman': SpearmanrResult(correlation=0.7326007048417494, pvalue=5.806453070698871e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7973381347057792, 5.4519019037435585e-55), 'spearman': SpearmanrResult(correlation=0.843466590764093, pvalue=2.965630311314217e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.5813783850113474, 2.714799073798569e-20), 'spearman': SpearmanrResult(correlation=0.5890341462779458, pvalue=6.541021236966973e-21), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.648434748544376, 'wmean': 0.6479694993632119}, 'spearman': {'mean': 0.6649659277401904, 'wmean': 0.6647735574237}}}, 'MR': {'devacc': 59.78, 'acc': 62.16, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.06, 'acc': 67.23, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.68, 'acc': 86.58, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 88.5, 'acc': 89.54, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.49, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.51, 'acc': 36.61, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.34, 'acc': 70.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.83, 'acc': 71.36, 'f1': 80.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.8, 'acc': 75.4, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7978968172552805, 'pearson': 0.801687707607353, 'spearman': 0.7333321394894639, 'mse': 0.36452502670947, 'yhat': array([3.26779042, 4.06687002, 1.40914109, ..., 3.07057839, 4.77973782,        4.71874024]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6805422197170821, 'pearson': 0.6539986199305371, 'spearman': 0.648530565583907, 'mse': 1.4058035217290834, 'yhat': array([1.35363002, 1.90169274, 2.79531034, ..., 4.12400527, 4.40375328,        3.40761603]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.81, 'acc': 62.9, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 337.79999999999995, 'acc': [(30.599999999999998, 65.08, 78.5, 3.0), (25.560000000000002, 59.68799999999999, 76.24799999999999, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 70.8, 'acc': 71.61, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 36.57, 'acc': 36.38, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.13, 'acc': 26.94, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.37, 'acc': 53.75, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 61.4, 'acc': 61.55, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.13, 'acc': 83.57, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.59, 'acc': 74.5, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.13, 'acc': 74.3, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.43, 'acc': 52.3, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.16, 'acc': 50.23, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 22:49:37,686 : STS12 p=0.5595, STS12 s=0.5783, STS13 p=0.6175, STS13 s=0.6115, STS14 p=0.6440, STS14 s=0.6375, STS15 p=0.6895, STS15 s=0.6969, STS 16 p=0.6480, STS16 s=0.6648, STS B p=0.6540, STS B s=0.6485, STS B m=1.4058, SICK-R p=0.8017, SICK-R s=0.7333, SICK-P m=0.3645
2019-02-15 22:49:37,686 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 22:49:37,686 : 0.5595,0.5783,0.6175,0.6115,0.6440,0.6375,0.6895,0.6969,0.6480,0.6648,0.6540,0.6485,1.4058,0.8017,0.7333,0.3645
2019-02-15 22:49:37,686 : MR=62.16, CR=67.23, SUBJ=89.54, MPQA=86.58, SST-B=77.38, SST-F=36.61, TREC=70.60, SICK-E=75.40, SNLI=62.90, MRPC=71.36, MRPC f=80.24
2019-02-15 22:49:37,686 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 22:49:37,686 : 62.16,67.23,89.54,86.58,77.38,36.61,70.60,75.40,62.90,71.36,80.24
2019-02-15 22:49:37,686 : COCO r1i2t=30.60, COCO r5i2t=65.08, COCO r10i2t=78.50, COCO medr_i2t=3.00, COCO r1t2i=25.56, COCO r5t2i=59.69, COCO r10t2i=76.25, COCO medr_t2i=4.00
2019-02-15 22:49:37,686 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 22:49:37,686 : 30.60,65.08,78.50,3.00,25.56,59.69,76.25,4.00
2019-02-15 22:49:37,686 : SentLen=71.61, WC=36.38, TreeDepth=26.94, TopConst=53.75, BShift=61.55, Tense=83.57, SubjNum=74.50, ObjNum=74.30, SOMO=52.30, CoordInv=50.23, average=58.51
2019-02-15 22:49:37,686 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 22:49:37,686 : 71.61,36.38,26.94,53.75,61.55,83.57,74.50,74.30,52.30,50.23,58.51
2019-02-15 22:49:37,686 : ********************************************************************************
2019-02-15 22:49:37,686 : ********************************************************************************
2019-02-15 22:49:37,686 : ********************************************************************************
2019-02-15 22:49:37,686 : layer 3
2019-02-15 22:49:37,687 : ********************************************************************************
2019-02-15 22:49:37,687 : ********************************************************************************
2019-02-15 22:49:37,687 : ********************************************************************************
2019-02-15 22:49:37,786 : ***** Transfer task : STS12 *****


2019-02-15 22:49:37,799 : loading BERT model bert-base-uncased
2019-02-15 22:49:37,799 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:49:37,820 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:49:37,820 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_yrgegiz
2019-02-15 22:49:40,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:49:43,592 : MSRpar : pearson = 0.3371, spearman = 0.3784
2019-02-15 22:49:44,377 : MSRvid : pearson = 0.7220, spearman = 0.7276
2019-02-15 22:49:45,065 : SMTeuroparl : pearson = 0.5059, spearman = 0.6118
2019-02-15 22:49:46,257 : surprise.OnWN : pearson = 0.5872, spearman = 0.6119
2019-02-15 22:49:46,930 : surprise.SMTnews : pearson = 0.4989, spearman = 0.4198
2019-02-15 22:49:46,931 : ALL (weighted average) : Pearson = 0.5360,             Spearman = 0.5588
2019-02-15 22:49:46,931 : ALL (average) : Pearson = 0.5302,             Spearman = 0.5499

2019-02-15 22:49:46,931 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 22:49:46,941 : loading BERT model bert-base-uncased
2019-02-15 22:49:46,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:49:46,959 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:49:46,959 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprkn89tly
2019-02-15 22:49:49,457 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:49:51,539 : FNWN : pearson = 0.1986, spearman = 0.1935
2019-02-15 22:49:52,467 : headlines : pearson = 0.6651, spearman = 0.6574
2019-02-15 22:49:53,130 : OnWN : pearson = 0.5921, spearman = 0.5917
2019-02-15 22:49:53,130 : ALL (weighted average) : Pearson = 0.5790,             Spearman = 0.5744
2019-02-15 22:49:53,130 : ALL (average) : Pearson = 0.4853,             Spearman = 0.4809

2019-02-15 22:49:53,130 : ***** Transfer task : STS14 *****


2019-02-15 22:49:53,148 : loading BERT model bert-base-uncased
2019-02-15 22:49:53,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:49:53,170 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:49:53,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0t329y7b
2019-02-15 22:49:55,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:49:57,869 : deft-forum : pearson = 0.4096, spearman = 0.4298
2019-02-15 22:49:58,584 : deft-news : pearson = 0.7105, spearman = 0.6741
2019-02-15 22:49:59,568 : headlines : pearson = 0.6249, spearman = 0.6011
2019-02-15 22:50:00,531 : images : pearson = 0.6389, spearman = 0.6320
2019-02-15 22:50:01,508 : OnWN : pearson = 0.6723, spearman = 0.7020
2019-02-15 22:50:02,738 : tweet-news : pearson = 0.5777, spearman = 0.5568
2019-02-15 22:50:02,738 : ALL (weighted average) : Pearson = 0.6087,             Spearman = 0.6039
2019-02-15 22:50:02,738 : ALL (average) : Pearson = 0.6056,             Spearman = 0.5993

2019-02-15 22:50:02,738 : ***** Transfer task : STS15 *****


2019-02-15 22:50:02,779 : loading BERT model bert-base-uncased
2019-02-15 22:50:02,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:50:02,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:50:02,802 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk78_tl3r
2019-02-15 22:50:05,386 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:50:07,728 : answers-forums : pearson = 0.5039, spearman = 0.4646
2019-02-15 22:50:08,662 : answers-students : pearson = 0.6941, spearman = 0.7016
2019-02-15 22:50:09,526 : belief : pearson = 0.5406, spearman = 0.5836
2019-02-15 22:50:10,543 : headlines : pearson = 0.6665, spearman = 0.6701
2019-02-15 22:50:11,516 : images : pearson = 0.7591, spearman = 0.7681
2019-02-15 22:50:11,517 : ALL (weighted average) : Pearson = 0.6605,             Spearman = 0.6659
2019-02-15 22:50:11,517 : ALL (average) : Pearson = 0.6328,             Spearman = 0.6376

2019-02-15 22:50:11,517 : ***** Transfer task : STS16 *****


2019-02-15 22:50:11,596 : loading BERT model bert-base-uncased
2019-02-15 22:50:11,597 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:50:11,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:50:11,618 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphxrb4ky1
2019-02-15 22:50:14,182 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:50:16,102 : answer-answer : pearson = 0.3998, spearman = 0.4188
2019-02-15 22:50:16,420 : headlines : pearson = 0.6680, spearman = 0.6783
2019-02-15 22:50:16,832 : plagiarism : pearson = 0.6982, spearman = 0.7059
2019-02-15 22:50:17,462 : postediting : pearson = 0.7731, spearman = 0.8313
2019-02-15 22:50:17,752 : question-question : pearson = 0.4767, spearman = 0.4860
2019-02-15 22:50:17,753 : ALL (weighted average) : Pearson = 0.6043,             Spearman = 0.6257
2019-02-15 22:50:17,753 : ALL (average) : Pearson = 0.6032,             Spearman = 0.6241

2019-02-15 22:50:17,753 : ***** Transfer task : MR *****


2019-02-15 22:50:17,774 : loading BERT model bert-base-uncased
2019-02-15 22:50:17,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:50:17,799 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:50:17,799 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_qn9y4qk
2019-02-15 22:50:20,339 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:50:21,952 : Generating sentence embeddings
2019-02-15 22:50:35,473 : Generated sentence embeddings
2019-02-15 22:50:35,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 22:50:57,594 : Best param found at split 1: l2reg = 1e-05                 with score 59.09
2019-02-15 22:51:24,751 : Best param found at split 2: l2reg = 0.01                 with score 58.99
2019-02-15 22:51:52,248 : Best param found at split 3: l2reg = 0.01                 with score 54.22
2019-02-15 22:52:13,150 : Best param found at split 4: l2reg = 0.01                 with score 56.27
2019-02-15 22:52:33,831 : Best param found at split 5: l2reg = 1e-05                 with score 59.44
2019-02-15 22:52:35,368 : Dev acc : 57.6 Test acc : 59.16

2019-02-15 22:52:35,369 : ***** Transfer task : CR *****


2019-02-15 22:52:35,383 : loading BERT model bert-base-uncased
2019-02-15 22:52:35,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:52:35,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:52:35,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxhrq3v4h
2019-02-15 22:52:37,916 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:52:39,435 : Generating sentence embeddings
2019-02-15 22:52:43,533 : Generated sentence embeddings
2019-02-15 22:52:43,533 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 22:52:51,076 : Best param found at split 1: l2reg = 0.01                 with score 71.58
2019-02-15 22:52:58,150 : Best param found at split 2: l2reg = 0.001                 with score 69.23
2019-02-15 22:53:04,976 : Best param found at split 3: l2reg = 0.001                 with score 68.91
2019-02-15 22:53:11,992 : Best param found at split 4: l2reg = 0.001                 with score 71.8
2019-02-15 22:53:19,802 : Best param found at split 5: l2reg = 1e-05                 with score 72.36
2019-02-15 22:53:20,200 : Dev acc : 70.78 Test acc : 70.18

2019-02-15 22:53:20,200 : ***** Transfer task : MPQA *****


2019-02-15 22:53:20,209 : loading BERT model bert-base-uncased
2019-02-15 22:53:20,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:53:20,245 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:53:20,245 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpha95ue30
2019-02-15 22:53:22,785 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:53:24,358 : Generating sentence embeddings
2019-02-15 22:53:28,157 : Generated sentence embeddings
2019-02-15 22:53:28,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 22:53:52,749 : Best param found at split 1: l2reg = 0.0001                 with score 85.91
2019-02-15 22:54:15,211 : Best param found at split 2: l2reg = 0.001                 with score 86.87
2019-02-15 22:54:39,497 : Best param found at split 3: l2reg = 0.01                 with score 85.9
2019-02-15 22:55:03,689 : Best param found at split 4: l2reg = 0.001                 with score 86.74
2019-02-15 22:55:27,570 : Best param found at split 5: l2reg = 0.01                 with score 86.47
2019-02-15 22:55:28,401 : Dev acc : 86.38 Test acc : 85.08

2019-02-15 22:55:28,402 : ***** Transfer task : SUBJ *****


2019-02-15 22:55:28,420 : loading BERT model bert-base-uncased
2019-02-15 22:55:28,420 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:55:28,442 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:55:28,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8lcwhz96
2019-02-15 22:55:30,945 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:55:32,516 : Generating sentence embeddings
2019-02-15 22:55:45,832 : Generated sentence embeddings
2019-02-15 22:55:45,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 22:56:12,003 : Best param found at split 1: l2reg = 0.0001                 with score 85.0
2019-02-15 22:56:39,060 : Best param found at split 2: l2reg = 0.01                 with score 88.34
2019-02-15 22:57:07,655 : Best param found at split 3: l2reg = 1e-05                 with score 87.39
2019-02-15 22:57:35,363 : Best param found at split 4: l2reg = 0.01                 with score 88.56
2019-02-15 22:57:59,785 : Best param found at split 5: l2reg = 1e-05                 with score 83.94
2019-02-15 22:58:00,561 : Dev acc : 86.65 Test acc : 89.43

2019-02-15 22:58:00,562 : ***** Transfer task : SST Binary classification *****


2019-02-15 22:58:00,694 : loading BERT model bert-base-uncased
2019-02-15 22:58:00,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:58:00,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:58:00,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwjfwzpd8
2019-02-15 22:58:03,412 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:58:05,426 : Computing embedding for train
2019-02-15 22:58:53,839 : Computed train embeddings
2019-02-15 22:58:53,839 : Computing embedding for dev
2019-02-15 22:58:54,778 : Computed dev embeddings
2019-02-15 22:58:54,778 : Computing embedding for test
2019-02-15 22:58:56,823 : Computed test embeddings
2019-02-15 22:58:56,823 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:59:36,434 : [('reg:1e-05', 75.8), ('reg:0.0001', 75.23), ('reg:0.001', 74.66), ('reg:0.01', 60.89)]
2019-02-15 22:59:36,435 : Validation : best param found is reg = 1e-05 with score             75.8
2019-02-15 22:59:36,435 : Evaluating...
2019-02-15 22:59:46,737 : 
Dev acc : 75.8 Test acc : 73.92 for             SST Binary classification

2019-02-15 22:59:46,737 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 22:59:46,798 : loading BERT model bert-base-uncased
2019-02-15 22:59:46,798 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:59:46,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:59:46,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbp1khhra
2019-02-15 22:59:49,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:59:50,873 : Computing embedding for train
2019-02-15 23:00:01,291 : Computed train embeddings
2019-02-15 23:00:01,291 : Computing embedding for dev
2019-02-15 23:00:02,511 : Computed dev embeddings
2019-02-15 23:00:02,512 : Computing embedding for test
2019-02-15 23:00:04,938 : Computed test embeddings
2019-02-15 23:00:04,939 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 23:00:11,043 : [('reg:1e-05', 36.78), ('reg:0.0001', 37.69), ('reg:0.001', 29.61), ('reg:0.01', 37.6)]
2019-02-15 23:00:11,044 : Validation : best param found is reg = 0.0001 with score             37.69
2019-02-15 23:00:11,044 : Evaluating...
2019-02-15 23:00:12,312 : 
Dev acc : 37.69 Test acc : 36.88 for             SST Fine-Grained classification

2019-02-15 23:00:12,313 : ***** Transfer task : TREC *****


2019-02-15 23:00:12,335 : loading BERT model bert-base-uncased
2019-02-15 23:00:12,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:00:12,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:00:12,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp76pogh83
2019-02-15 23:00:14,916 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:00:19,884 : Computed train embeddings
2019-02-15 23:00:20,152 : Computed test embeddings
2019-02-15 23:00:20,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 23:00:35,628 : [('reg:1e-05', 59.61), ('reg:0.0001', 61.32), ('reg:0.001', 62.09), ('reg:0.01', 61.47)]
2019-02-15 23:00:35,629 : Cross-validation : best param found is reg = 0.001             with score 62.09
2019-02-15 23:00:35,629 : Evaluating...
2019-02-15 23:00:36,617 : 
Dev acc : 62.09 Test acc : 75.2             for TREC

2019-02-15 23:00:36,617 : ***** Transfer task : MRPC *****


2019-02-15 23:00:36,676 : loading BERT model bert-base-uncased
2019-02-15 23:00:36,676 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:00:36,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:00:36,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi3pnxjw9
2019-02-15 23:00:39,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:00:40,873 : Computing embedding for train
2019-02-15 23:00:50,637 : Computed train embeddings
2019-02-15 23:00:50,637 : Computing embedding for test
2019-02-15 23:00:54,896 : Computed test embeddings
2019-02-15 23:00:54,912 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 23:01:06,155 : [('reg:1e-05', 70.58), ('reg:0.0001', 70.51), ('reg:0.001', 70.29), ('reg:0.01', 70.46)]
2019-02-15 23:01:06,155 : Cross-validation : best param found is reg = 1e-05             with score 70.58
2019-02-15 23:01:06,155 : Evaluating...
2019-02-15 23:01:06,739 : Dev acc : 70.58 Test acc 67.71; Test F1 73.26 for MRPC.

2019-02-15 23:01:06,740 : ***** Transfer task : SICK-Entailment*****


2019-02-15 23:01:06,767 : loading BERT model bert-base-uncased
2019-02-15 23:01:06,768 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:01:06,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:01:06,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnpwntu7d
2019-02-15 23:01:09,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:01:10,864 : Computing embedding for train
2019-02-15 23:01:16,054 : Computed train embeddings
2019-02-15 23:01:16,054 : Computing embedding for dev
2019-02-15 23:01:16,724 : Computed dev embeddings
2019-02-15 23:01:16,724 : Computing embedding for test
2019-02-15 23:01:22,184 : Computed test embeddings
2019-02-15 23:01:22,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 23:01:24,173 : [('reg:1e-05', 79.6), ('reg:0.0001', 77.4), ('reg:0.001', 76.8), ('reg:0.01', 77.6)]
2019-02-15 23:01:24,174 : Validation : best param found is reg = 1e-05 with score             79.6
2019-02-15 23:01:24,174 : Evaluating...
2019-02-15 23:01:24,664 : 
Dev acc : 79.6 Test acc : 78.53 for                        SICK entailment

2019-02-15 23:01:24,665 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 23:01:24,693 : loading BERT model bert-base-uncased
2019-02-15 23:01:24,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:01:24,717 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:01:24,717 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprmqy412s
2019-02-15 23:01:27,298 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:01:28,827 : Computing embedding for train
2019-02-15 23:01:34,020 : Computed train embeddings
2019-02-15 23:01:34,020 : Computing embedding for dev
2019-02-15 23:01:34,711 : Computed dev embeddings
2019-02-15 23:01:34,711 : Computing embedding for test
2019-02-15 23:01:40,263 : Computed test embeddings
2019-02-15 23:02:08,713 : Dev : Pearson 0.791443666955994
2019-02-15 23:02:08,713 : Test : Pearson 0.7852568825300805 Spearman 0.7194472533832414 MSE 0.394842839137381                        for SICK Relatedness

2019-02-15 23:02:08,714 : 

***** Transfer task : STSBenchmark*****


2019-02-15 23:02:08,796 : loading BERT model bert-base-uncased
2019-02-15 23:02:08,797 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:02:08,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:02:08,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8la6j728
2019-02-15 23:02:11,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:02:12,913 : Computing embedding for train
2019-02-15 23:02:21,226 : Computed train embeddings
2019-02-15 23:02:21,226 : Computing embedding for dev
2019-02-15 23:02:23,693 : Computed dev embeddings
2019-02-15 23:02:23,693 : Computing embedding for test
2019-02-15 23:02:25,700 : Computed test embeddings
2019-02-15 23:03:02,059 : Dev : Pearson 0.6448378815406637
2019-02-15 23:03:02,059 : Test : Pearson 0.6322046135664458 Spearman 0.6266734611653674 MSE 1.4739610306651059                        for SICK Relatedness

2019-02-15 23:03:02,059 : ***** Transfer task : SNLI Entailment*****


2019-02-15 23:03:07,222 : loading BERT model bert-base-uncased
2019-02-15 23:03:07,223 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:03:07,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:03:07,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw7deorc3
2019-02-15 23:03:09,901 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:03:11,596 : PROGRESS (encoding): 0.00%
2019-02-15 23:04:30,346 : PROGRESS (encoding): 14.56%
2019-02-15 23:05:58,123 : PROGRESS (encoding): 29.12%
2019-02-15 23:07:26,058 : PROGRESS (encoding): 43.69%
2019-02-15 23:09:01,476 : PROGRESS (encoding): 58.25%
2019-02-15 23:10:46,623 : PROGRESS (encoding): 72.81%
2019-02-15 23:12:31,404 : PROGRESS (encoding): 87.37%
2019-02-15 23:14:22,200 : PROGRESS (encoding): 0.00%
2019-02-15 23:14:35,643 : PROGRESS (encoding): 0.00%
2019-02-15 23:14:48,805 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 23:15:40,483 : [('reg:1e-09', 67.59)]
2019-02-15 23:15:40,483 : Validation : best param found is reg = 1e-09 with score             67.59
2019-02-15 23:15:40,483 : Evaluating...
2019-02-15 23:16:39,689 : Dev acc : 67.59 Test acc : 67.1 for SNLI

2019-02-15 23:16:42,473 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 23:16:52,097 : loading BERT model bert-base-uncased
2019-02-15 23:16:52,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 23:16:52,155 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 23:16:52,155 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp353vpaa6
2019-02-15 23:16:54,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 23:16:56,301 : Computing embedding for train
2019-02-15 23:24:28,052 : Computed train embeddings
2019-02-15 23:24:28,052 : Computing embedding for dev
2019-02-15 23:24:47,137 : Computed dev embeddings
2019-02-15 23:24:47,137 : Computing embedding for test
2019-02-15 23:25:06,992 : Computed test embeddings
2019-02-15 23:25:07,009 : prepare data
2019-02-15 23:25:07,075 : start epoch
2019-02-15 23:25:51,618 : samples : 64000
2019-02-15 23:26:02,306 : Image to text: 6.74, 19.76, 29.94, 27.0
2019-02-15 23:26:09,993 : Text to Image: 5.536, 17.4, 27.484, 31.0
2019-02-15 23:26:54,627 : samples : 128000
2019-02-15 23:27:05,263 : Image to text: 8.26, 24.64, 35.86, 21.0
2019-02-15 23:27:12,972 : Text to Image: 6.408, 20.232, 30.86, 26.0
2019-02-15 23:27:57,157 : samples : 192000
2019-02-15 23:28:07,672 : Image to text: 7.6, 22.46, 33.56, 23.0
2019-02-15 23:28:15,316 : Text to Image: 5.668, 18.252, 28.064, 28.0
2019-02-15 23:29:00,171 : samples : 256000
2019-02-15 23:29:10,757 : Image to text: 8.3, 24.4, 36.32, 20.0
2019-02-15 23:29:18,624 : Text to Image: 6.596, 20.768, 31.364, 25.0
2019-02-15 23:30:03,424 : samples : 320000
2019-02-15 23:30:14,119 : Image to text: 8.96, 25.82, 37.74, 19.0
2019-02-15 23:30:22,035 : Text to Image: 7.536, 21.924, 32.724, 23.0
2019-02-15 23:31:06,249 : samples : 384000
2019-02-15 23:31:16,790 : Image to text: 8.4, 25.62, 36.94, 19.0
2019-02-15 23:31:24,677 : Text to Image: 6.764, 21.216, 32.044, 24.0
2019-02-15 23:32:08,564 : samples : 448000
2019-02-15 23:32:19,261 : Image to text: 8.9, 25.3, 36.64, 19.0
2019-02-15 23:32:28,115 : Text to Image: 6.82, 21.576, 32.852, 23.0
2019-02-15 23:33:21,214 : samples : 512000
2019-02-15 23:33:31,846 : Image to text: 8.44, 25.5, 37.32, 19.0
2019-02-15 23:33:39,667 : Text to Image: 7.024, 21.96, 33.244, 23.0
2019-02-15 23:34:17,301 : Epoch 1 finished
2019-02-15 23:34:17,765 : Image to text: 25.3, 56.5, 72.5, 4.0
2019-02-15 23:34:18,116 : Text to Image: 18.86, 50.68, 68.12, 5.0
2019-02-15 23:34:18,556 : Image to text: 23.4, 55.0, 69.7, 5.0
2019-02-15 23:34:18,892 : Text to Image: 19.92, 49.74, 67.48, 6.0
2019-02-15 23:34:19,340 : Image to text: 23.7, 57.2, 72.5, 4.0
2019-02-15 23:34:19,679 : Text to Image: 19.62, 50.38, 66.62, 5.0
2019-02-15 23:34:20,143 : Image to text: 25.3, 54.8, 71.2, 4.0
2019-02-15 23:34:20,489 : Text to Image: 19.4, 51.32, 68.06, 5.0
2019-02-15 23:34:20,957 : Image to text: 24.6, 55.5, 71.3, 4.0
2019-02-15 23:34:21,310 : Text to Image: 19.22, 50.46, 67.6, 5.0
2019-02-15 23:34:21,310 : Dev mean Text to Image: 19.404, 50.516, 67.57600000000001, 5.2
2019-02-15 23:34:21,310 : Dev mean Image to text: 24.46, 55.800000000000004, 71.44, 4.2
2019-02-15 23:34:21,310 : start epoch
2019-02-15 23:35:06,006 : samples : 64000
2019-02-15 23:35:16,659 : Image to text: 10.14, 27.3, 38.52, 18.0
2019-02-15 23:35:24,377 : Text to Image: 7.564, 22.604, 33.748, 23.0
2019-02-15 23:36:09,115 : samples : 128000
2019-02-15 23:36:19,751 : Image to text: 9.76, 26.64, 38.42, 18.0
2019-02-15 23:36:27,398 : Text to Image: 7.512, 23.088, 34.62, 21.0
2019-02-15 23:37:11,544 : samples : 192000
2019-02-15 23:37:22,173 : Image to text: 10.24, 27.52, 39.82, 16.0
2019-02-15 23:37:29,873 : Text to Image: 8.464, 24.352, 35.88, 20.0
2019-02-15 23:38:14,507 : samples : 256000
2019-02-15 23:38:25,204 : Image to text: 9.7, 27.02, 39.58, 16.0
2019-02-15 23:38:32,972 : Text to Image: 8.076, 24.384, 35.616, 20.0
2019-02-15 23:39:17,937 : samples : 320000
2019-02-15 23:39:28,554 : Image to text: 10.5, 28.14, 40.68, 16.0
2019-02-15 23:39:36,387 : Text to Image: 8.024, 23.96, 35.488, 21.0
2019-02-15 23:40:20,743 : samples : 384000
2019-02-15 23:40:31,251 : Image to text: 10.18, 27.6, 39.84, 17.0
2019-02-15 23:40:38,967 : Text to Image: 7.812, 23.932, 35.424, 20.0
2019-02-15 23:41:23,832 : samples : 448000
2019-02-15 23:41:34,438 : Image to text: 10.12, 27.88, 39.46, 18.0
2019-02-15 23:41:42,185 : Text to Image: 7.788, 23.06, 34.588, 21.0
2019-02-15 23:42:27,606 : samples : 512000
2019-02-15 23:42:38,177 : Image to text: 10.76, 28.58, 40.72, 16.0
2019-02-15 23:42:45,867 : Text to Image: 8.48, 24.776, 36.692, 20.0
2019-02-15 23:43:23,481 : Epoch 2 finished
2019-02-15 23:43:23,949 : Image to text: 26.7, 59.8, 75.5, 4.0
2019-02-15 23:43:24,286 : Text to Image: 22.24, 54.5, 71.84, 5.0
2019-02-15 23:43:24,758 : Image to text: 27.4, 58.5, 74.6, 4.0
2019-02-15 23:43:25,096 : Text to Image: 21.34, 53.6, 71.04, 5.0
2019-02-15 23:43:25,534 : Image to text: 26.7, 60.1, 74.6, 4.0
2019-02-15 23:43:25,878 : Text to Image: 22.38, 54.88, 70.56, 5.0
2019-02-15 23:43:26,333 : Image to text: 28.6, 61.4, 76.0, 4.0
2019-02-15 23:43:26,680 : Text to Image: 21.56, 54.8, 70.86, 5.0
2019-02-15 23:43:27,137 : Image to text: 26.6, 59.7, 74.7, 4.0
2019-02-15 23:43:27,485 : Text to Image: 21.98, 54.2, 70.76, 5.0
2019-02-15 23:43:27,485 : Dev mean Text to Image: 21.9, 54.396, 71.012, 5.0
2019-02-15 23:43:27,485 : Dev mean Image to text: 27.200000000000003, 59.89999999999999, 75.08, 4.0
2019-02-15 23:43:27,485 : start epoch
2019-02-15 23:44:12,214 : samples : 64000
2019-02-15 23:44:22,846 : Image to text: 9.88, 27.98, 41.18, 16.0
2019-02-15 23:44:30,543 : Text to Image: 8.372, 24.624, 36.116, 20.0
2019-02-15 23:45:15,196 : samples : 128000
2019-02-15 23:45:25,936 : Image to text: 9.24, 28.08, 39.74, 17.0
2019-02-15 23:45:33,657 : Text to Image: 7.764, 23.588, 34.716, 21.0
2019-02-15 23:46:17,381 : samples : 192000
2019-02-15 23:46:28,040 : Image to text: 10.04, 29.2, 41.36, 16.0
2019-02-15 23:46:35,767 : Text to Image: 7.792, 23.78, 35.252, 21.0
2019-02-15 23:47:20,707 : samples : 256000
2019-02-15 23:47:31,227 : Image to text: 10.42, 29.66, 41.6, 15.0
2019-02-15 23:47:38,941 : Text to Image: 8.4, 25.064, 36.548, 19.0
2019-02-15 23:48:24,092 : samples : 320000
2019-02-15 23:48:34,725 : Image to text: 10.06, 28.94, 41.44, 16.0
2019-02-15 23:48:42,439 : Text to Image: 8.616, 25.516, 37.184, 19.0
2019-02-15 23:49:26,065 : samples : 384000
2019-02-15 23:49:37,982 : Image to text: 10.64, 28.98, 41.92, 15.0
2019-02-15 23:49:46,846 : Text to Image: 8.484, 25.076, 36.916, 19.0
2019-02-15 23:50:35,883 : samples : 448000
2019-02-15 23:50:46,459 : Image to text: 11.18, 29.88, 42.96, 15.0
2019-02-15 23:50:54,254 : Text to Image: 8.772, 25.588, 37.276, 19.0
2019-02-15 23:51:39,453 : samples : 512000
2019-02-15 23:51:49,843 : Image to text: 10.12, 30.44, 41.9, 15.0
2019-02-15 23:51:57,434 : Text to Image: 9.108, 26.064, 37.84, 18.0
2019-02-15 23:52:35,125 : Epoch 3 finished
2019-02-15 23:52:35,580 : Image to text: 26.8, 60.2, 76.1, 4.0
2019-02-15 23:52:35,932 : Text to Image: 22.78, 55.88, 73.28, 4.0
2019-02-15 23:52:36,400 : Image to text: 26.9, 58.4, 74.2, 4.0
2019-02-15 23:52:36,753 : Text to Image: 22.0, 54.82, 71.64, 5.0
2019-02-15 23:52:37,203 : Image to text: 25.0, 60.8, 75.6, 4.0
2019-02-15 23:52:37,543 : Text to Image: 22.64, 55.9, 72.72, 4.0
2019-02-15 23:52:37,998 : Image to text: 29.7, 61.7, 76.6, 3.0
2019-02-15 23:52:38,340 : Text to Image: 23.96, 56.16, 72.74, 4.0
2019-02-15 23:52:38,791 : Image to text: 28.9, 60.6, 76.4, 3.0
2019-02-15 23:52:39,131 : Text to Image: 23.72, 56.38, 72.04, 4.0
2019-02-15 23:52:39,131 : Dev mean Text to Image: 23.02, 55.828, 72.48400000000001, 4.2
2019-02-15 23:52:39,131 : Dev mean Image to text: 27.46, 60.34, 75.78, 3.6000000000000005
2019-02-15 23:52:39,132 : start epoch
2019-02-15 23:53:23,814 : samples : 64000
2019-02-15 23:53:34,547 : Image to text: 11.2, 29.28, 42.14, 15.0
2019-02-15 23:53:42,280 : Text to Image: 9.04, 26.34, 38.62, 18.0
2019-02-15 23:54:26,608 : samples : 128000
2019-02-15 23:54:37,310 : Image to text: 11.16, 29.94, 43.06, 14.0
2019-02-15 23:54:45,047 : Text to Image: 9.092, 26.288, 37.936, 18.0
2019-02-15 23:55:28,822 : samples : 192000
2019-02-15 23:55:39,577 : Image to text: 10.44, 29.04, 41.9, 15.0
2019-02-15 23:55:47,268 : Text to Image: 8.64, 25.296, 37.28, 19.0
2019-02-15 23:56:32,379 : samples : 256000
2019-02-15 23:56:42,941 : Image to text: 10.86, 29.64, 42.66, 15.0
2019-02-15 23:56:50,776 : Text to Image: 9.68, 26.6, 38.692, 18.0
2019-02-15 23:57:35,571 : samples : 320000
2019-02-15 23:57:46,014 : Image to text: 10.72, 30.54, 43.3, 14.0
2019-02-15 23:57:53,580 : Text to Image: 9.292, 26.812, 39.152, 18.0
2019-02-15 23:58:38,606 : samples : 384000
2019-02-15 23:58:49,164 : Image to text: 10.42, 29.04, 41.98, 16.0
2019-02-15 23:58:56,869 : Text to Image: 8.644, 25.296, 37.168, 19.0
2019-02-15 23:59:41,442 : samples : 448000
2019-02-15 23:59:51,987 : Image to text: 10.82, 31.0, 43.4, 14.0
2019-02-15 23:59:59,727 : Text to Image: 9.608, 26.876, 38.964, 18.0
2019-02-16 00:00:44,466 : samples : 512000
2019-02-16 00:00:54,884 : Image to text: 11.18, 30.62, 43.7, 14.0
2019-02-16 00:01:02,472 : Text to Image: 9.412, 26.856, 38.68, 17.0
2019-02-16 00:01:40,548 : Epoch 4 finished
2019-02-16 00:01:41,011 : Image to text: 27.3, 62.3, 77.7, 4.0
2019-02-16 00:01:41,360 : Text to Image: 22.52, 56.68, 73.8, 4.0
2019-02-16 00:01:41,819 : Image to text: 26.3, 60.2, 75.2, 4.0
2019-02-16 00:01:42,171 : Text to Image: 22.72, 55.2, 72.78, 5.0
2019-02-16 00:01:42,652 : Image to text: 27.2, 61.7, 77.3, 4.0
2019-02-16 00:01:43,029 : Text to Image: 23.7, 56.9, 73.88, 4.0
2019-02-16 00:01:43,496 : Image to text: 28.9, 62.3, 78.2, 3.0
2019-02-16 00:01:43,844 : Text to Image: 23.62, 56.22, 72.68, 4.0
2019-02-16 00:01:44,311 : Image to text: 29.9, 61.5, 76.9, 3.0
2019-02-16 00:01:44,661 : Text to Image: 23.18, 56.8, 72.9, 4.0
2019-02-16 00:01:44,661 : Dev mean Text to Image: 23.148, 56.36, 73.208, 4.2
2019-02-16 00:01:44,661 : Dev mean Image to text: 27.919999999999998, 61.60000000000001, 77.06, 3.6000000000000005
2019-02-16 00:01:44,662 : start epoch
2019-02-16 00:02:29,154 : samples : 64000
2019-02-16 00:02:39,869 : Image to text: 11.26, 31.0, 42.9, 15.0
2019-02-16 00:02:47,537 : Text to Image: 9.4, 26.46, 38.28, 18.0
2019-02-16 00:03:31,861 : samples : 128000
2019-02-16 00:03:42,469 : Image to text: 11.76, 31.14, 43.64, 14.0
2019-02-16 00:03:50,044 : Text to Image: 9.52, 27.044, 39.14, 17.0
2019-02-16 00:04:34,244 : samples : 192000
2019-02-16 00:04:44,921 : Image to text: 11.2, 30.1, 42.96, 15.0
2019-02-16 00:04:52,664 : Text to Image: 9.236, 26.9, 38.9, 18.0
2019-02-16 00:05:37,547 : samples : 256000
2019-02-16 00:05:48,127 : Image to text: 11.02, 29.88, 43.2, 14.0
2019-02-16 00:05:55,872 : Text to Image: 9.376, 26.824, 38.644, 17.0
2019-02-16 00:06:43,299 : samples : 320000
2019-02-16 00:06:55,619 : Image to text: 11.16, 29.86, 42.08, 14.0
2019-02-16 00:07:04,303 : Text to Image: 8.644, 25.872, 37.544, 19.0
2019-02-16 00:07:50,155 : samples : 384000
2019-02-16 00:08:00,707 : Image to text: 11.02, 30.68, 43.0, 15.0
2019-02-16 00:08:08,405 : Text to Image: 9.572, 27.068, 39.112, 17.0
2019-02-16 00:08:53,071 : samples : 448000
2019-02-16 00:09:03,615 : Image to text: 11.7, 30.36, 43.96, 14.0
2019-02-16 00:09:11,244 : Text to Image: 9.36, 26.872, 38.844, 17.0
2019-02-16 00:09:55,292 : samples : 512000
2019-02-16 00:10:05,793 : Image to text: 11.54, 31.24, 44.12, 14.0
2019-02-16 00:10:13,405 : Text to Image: 9.76, 27.608, 39.656, 17.0
2019-02-16 00:10:51,340 : Epoch 5 finished
2019-02-16 00:10:51,789 : Image to text: 28.7, 60.9, 78.2, 4.0
2019-02-16 00:10:52,126 : Text to Image: 22.66, 56.0, 74.2, 4.0
2019-02-16 00:10:52,575 : Image to text: 27.2, 60.3, 74.9, 4.0
2019-02-16 00:10:52,916 : Text to Image: 22.68, 55.82, 72.78, 4.0
2019-02-16 00:10:53,371 : Image to text: 29.0, 61.7, 77.2, 3.0
2019-02-16 00:10:53,716 : Text to Image: 23.48, 56.44, 72.24, 4.0
2019-02-16 00:10:54,189 : Image to text: 29.7, 62.5, 76.4, 3.0
2019-02-16 00:10:54,549 : Text to Image: 24.02, 57.14, 73.56, 4.0
2019-02-16 00:10:55,022 : Image to text: 29.0, 62.6, 75.9, 3.0
2019-02-16 00:10:55,404 : Text to Image: 24.02, 56.44, 73.12, 4.0
2019-02-16 00:10:55,404 : Dev mean Text to Image: 23.372, 56.367999999999995, 73.17999999999999, 4.0
2019-02-16 00:10:55,404 : Dev mean Image to text: 28.720000000000002, 61.599999999999994, 76.52000000000001, 3.4000000000000004
2019-02-16 00:10:55,405 : start epoch
2019-02-16 00:11:40,471 : samples : 64000
2019-02-16 00:11:51,172 : Image to text: 11.42, 30.86, 43.38, 14.0
2019-02-16 00:11:58,939 : Text to Image: 9.772, 27.396, 39.488, 17.0
2019-02-16 00:12:43,686 : samples : 128000
2019-02-16 00:12:54,257 : Image to text: 11.58, 31.62, 44.0, 14.0
2019-02-16 00:13:01,925 : Text to Image: 9.96, 27.744, 39.884, 17.0
2019-02-16 00:13:47,062 : samples : 192000
2019-02-16 00:13:57,710 : Image to text: 11.64, 30.44, 42.9, 15.0
2019-02-16 00:14:05,508 : Text to Image: 9.012, 26.5, 38.524, 18.0
2019-02-16 00:14:50,182 : samples : 256000
2019-02-16 00:15:00,672 : Image to text: 11.96, 31.7, 44.24, 14.0
2019-02-16 00:15:08,346 : Text to Image: 9.82, 27.24, 39.328, 17.0
2019-02-16 00:15:52,722 : samples : 320000
2019-02-16 00:16:03,118 : Image to text: 12.2, 30.56, 43.66, 14.0
2019-02-16 00:16:10,792 : Text to Image: 9.632, 26.844, 38.824, 18.0
2019-02-16 00:16:56,467 : samples : 384000
2019-02-16 00:17:07,025 : Image to text: 11.42, 32.0, 44.98, 13.0
2019-02-16 00:17:14,637 : Text to Image: 9.964, 27.96, 39.84, 17.0
2019-02-16 00:17:59,843 : samples : 448000
2019-02-16 00:18:10,367 : Image to text: 11.66, 31.1, 44.16, 14.0
2019-02-16 00:18:17,984 : Text to Image: 9.664, 27.444, 39.56, 17.0
2019-02-16 00:19:02,009 : samples : 512000
2019-02-16 00:19:12,431 : Image to text: 12.14, 31.84, 43.64, 14.0
2019-02-16 00:19:20,063 : Text to Image: 9.244, 26.672, 38.456, 17.0
2019-02-16 00:19:57,758 : Epoch 6 finished
2019-02-16 00:19:58,198 : Image to text: 28.8, 62.8, 78.1, 4.0
2019-02-16 00:19:58,547 : Text to Image: 24.24, 58.5, 75.74, 4.0
2019-02-16 00:19:58,989 : Image to text: 27.1, 61.7, 75.9, 3.0
2019-02-16 00:19:59,339 : Text to Image: 23.46, 57.34, 74.28, 4.0
2019-02-16 00:19:59,777 : Image to text: 30.0, 61.7, 77.0, 3.0
2019-02-16 00:20:00,113 : Text to Image: 25.12, 58.62, 74.94, 4.0
2019-02-16 00:20:00,557 : Image to text: 30.8, 64.0, 79.2, 3.0
2019-02-16 00:20:00,898 : Text to Image: 24.86, 57.76, 74.58, 4.0
2019-02-16 00:20:01,370 : Image to text: 30.4, 66.0, 77.7, 3.0
2019-02-16 00:20:01,702 : Text to Image: 24.7, 57.76, 73.6, 4.0
2019-02-16 00:20:01,702 : Dev mean Text to Image: 24.476, 57.995999999999995, 74.628, 4.0
2019-02-16 00:20:01,702 : Dev mean Image to text: 29.42, 63.239999999999995, 77.58000000000001, 3.2
2019-02-16 00:20:01,703 : start epoch
2019-02-16 00:20:46,119 : samples : 64000
2019-02-16 00:20:56,833 : Image to text: 11.78, 32.12, 45.08, 13.0
2019-02-16 00:21:04,531 : Text to Image: 10.092, 28.32, 40.46, 16.0
2019-02-16 00:21:48,142 : samples : 128000
2019-02-16 00:21:58,814 : Image to text: 11.32, 31.3, 43.54, 14.0
2019-02-16 00:22:06,590 : Text to Image: 9.612, 27.028, 38.936, 17.0
2019-02-16 00:22:50,816 : samples : 192000
2019-02-16 00:23:01,528 : Image to text: 11.52, 31.08, 44.64, 13.0
2019-02-16 00:23:09,225 : Text to Image: 9.748, 27.244, 39.764, 17.0
2019-02-16 00:23:59,684 : samples : 256000
2019-02-16 00:24:11,425 : Image to text: 11.62, 31.78, 43.8, 14.0
2019-02-16 00:24:19,629 : Text to Image: 9.836, 28.284, 40.728, 16.0
2019-02-16 00:25:03,451 : samples : 320000
2019-02-16 00:25:13,920 : Image to text: 12.54, 31.38, 43.98, 14.0
2019-02-16 00:25:21,553 : Text to Image: 9.756, 27.796, 40.16, 17.0
2019-02-16 00:26:06,492 : samples : 384000
2019-02-16 00:26:17,014 : Image to text: 11.98, 31.56, 43.56, 14.0
2019-02-16 00:26:24,646 : Text to Image: 9.896, 27.856, 39.956, 17.0
2019-02-16 00:27:09,818 : samples : 448000
2019-02-16 00:27:20,272 : Image to text: 11.88, 31.92, 43.92, 14.0
2019-02-16 00:27:27,763 : Text to Image: 9.888, 27.968, 40.492, 16.0
2019-02-16 00:28:12,510 : samples : 512000
2019-02-16 00:28:22,980 : Image to text: 12.26, 32.72, 44.76, 13.0
2019-02-16 00:28:30,588 : Text to Image: 10.196, 27.952, 40.444, 16.0
2019-02-16 00:29:08,929 : Epoch 7 finished
2019-02-16 00:29:09,395 : Image to text: 28.8, 60.7, 76.7, 3.0
2019-02-16 00:29:09,741 : Text to Image: 22.62, 58.52, 75.34, 4.0
2019-02-16 00:29:10,184 : Image to text: 27.6, 59.5, 75.3, 4.0
2019-02-16 00:29:10,548 : Text to Image: 22.9, 56.84, 74.02, 4.0
2019-02-16 00:29:11,005 : Image to text: 28.9, 60.5, 76.9, 4.0
2019-02-16 00:29:11,367 : Text to Image: 23.52, 58.34, 74.3, 4.0
2019-02-16 00:29:11,828 : Image to text: 29.5, 62.9, 77.1, 3.0
2019-02-16 00:29:12,187 : Text to Image: 23.94, 57.46, 74.94, 4.0
2019-02-16 00:29:12,645 : Image to text: 27.7, 62.4, 77.2, 3.0
2019-02-16 00:29:12,986 : Text to Image: 23.58, 57.22, 73.86, 4.0
2019-02-16 00:29:12,986 : Dev mean Text to Image: 23.311999999999998, 57.676, 74.492, 4.0
2019-02-16 00:29:12,986 : Dev mean Image to text: 28.5, 61.2, 76.64, 3.4000000000000004
2019-02-16 00:29:12,986 : start epoch
2019-02-16 00:29:57,629 : samples : 64000
2019-02-16 00:30:08,387 : Image to text: 11.72, 32.36, 45.22, 13.0
2019-02-16 00:30:16,107 : Text to Image: 9.548, 27.192, 39.24, 17.0
2019-02-16 00:31:00,011 : samples : 128000
2019-02-16 00:31:10,702 : Image to text: 12.52, 32.08, 44.62, 13.0
2019-02-16 00:31:18,407 : Text to Image: 9.648, 27.38, 39.72, 17.0
2019-02-16 00:32:02,398 : samples : 192000
2019-02-16 00:32:13,042 : Image to text: 11.96, 31.78, 45.5, 13.0
2019-02-16 00:32:20,741 : Text to Image: 9.868, 27.98, 40.516, 16.0
2019-02-16 00:33:06,515 : samples : 256000
2019-02-16 00:33:19,216 : Image to text: 12.26, 32.26, 44.74, 13.0
2019-02-16 00:33:29,289 : Text to Image: 10.112, 28.452, 40.62, 16.0
2019-02-16 00:34:15,290 : samples : 320000
2019-02-16 00:34:25,828 : Image to text: 12.12, 32.3, 45.66, 13.0
2019-02-16 00:34:33,555 : Text to Image: 10.136, 28.64, 41.144, 16.0
2019-02-16 00:35:18,090 : samples : 384000
2019-02-16 00:35:29,739 : Image to text: 12.06, 32.8, 45.24, 13.0
2019-02-16 00:35:39,777 : Text to Image: 10.32, 28.556, 40.72, 16.0
2019-02-16 00:36:26,519 : samples : 448000
2019-02-16 00:36:39,211 : Image to text: 12.8, 32.68, 45.54, 13.0
2019-02-16 00:36:49,276 : Text to Image: 10.076, 28.26, 40.8, 16.0
2019-02-16 00:37:32,959 : samples : 512000
2019-02-16 00:37:43,448 : Image to text: 11.32, 31.62, 44.58, 13.0
2019-02-16 00:37:50,845 : Text to Image: 10.004, 27.904, 39.8, 17.0
2019-02-16 00:38:29,386 : Epoch 8 finished
2019-02-16 00:38:30,324 : Image to text: 28.9, 62.6, 78.7, 3.0
2019-02-16 00:38:31,076 : Text to Image: 23.14, 57.32, 74.28, 4.0
2019-02-16 00:38:32,065 : Image to text: 27.9, 60.2, 75.5, 3.0
2019-02-16 00:38:32,769 : Text to Image: 22.82, 56.38, 73.88, 4.0
2019-02-16 00:38:33,701 : Image to text: 26.6, 61.4, 77.3, 4.0
2019-02-16 00:38:34,482 : Text to Image: 23.7, 57.7, 74.76, 4.0
2019-02-16 00:38:35,434 : Image to text: 30.1, 63.4, 78.0, 3.0
2019-02-16 00:38:36,189 : Text to Image: 24.18, 57.64, 74.22, 4.0
2019-02-16 00:38:37,137 : Image to text: 30.7, 63.4, 77.5, 3.0
2019-02-16 00:38:37,857 : Text to Image: 23.84, 57.08, 73.86, 4.0
2019-02-16 00:38:37,857 : Dev mean Text to Image: 23.536, 57.224000000000004, 74.2, 4.0
2019-02-16 00:38:37,857 : Dev mean Image to text: 28.84, 62.2, 77.4, 3.2
2019-02-16 00:38:37,857 : start epoch
2019-02-16 00:39:24,120 : samples : 64000
2019-02-16 00:39:35,832 : Image to text: 12.28, 32.06, 45.4, 13.0
2019-02-16 00:39:43,417 : Text to Image: 10.2, 27.836, 39.8, 16.0
2019-02-16 00:40:27,237 : samples : 128000
2019-02-16 00:40:38,757 : Image to text: 12.38, 33.06, 45.46, 13.0
2019-02-16 00:40:49,036 : Text to Image: 9.964, 28.004, 40.028, 17.0
2019-02-16 00:41:41,251 : samples : 192000
2019-02-16 00:41:53,894 : Image to text: 12.62, 32.44, 46.06, 13.0
2019-02-16 00:42:02,141 : Text to Image: 10.22, 28.608, 41.268, 16.0
2019-02-16 00:42:46,300 : samples : 256000
2019-02-16 00:42:56,902 : Image to text: 12.16, 32.1, 45.46, 13.0
2019-02-16 00:43:04,490 : Text to Image: 10.34, 28.648, 40.804, 16.0
2019-02-16 00:43:49,742 : samples : 320000
2019-02-16 00:44:02,428 : Image to text: 12.04, 32.6, 46.14, 13.0
2019-02-16 00:44:12,536 : Text to Image: 10.156, 27.864, 40.42, 16.0
2019-02-16 00:44:59,150 : samples : 384000
2019-02-16 00:45:09,812 : Image to text: 12.2, 32.48, 44.7, 14.0
2019-02-16 00:45:17,190 : Text to Image: 10.096, 28.58, 40.728, 16.0
2019-02-16 00:46:01,463 : samples : 448000
2019-02-16 00:46:12,043 : Image to text: 12.2, 32.26, 45.02, 13.0
2019-02-16 00:46:22,049 : Text to Image: 10.032, 28.304, 40.728, 16.0
2019-02-16 00:47:08,635 : samples : 512000
2019-02-16 00:47:21,325 : Image to text: 11.98, 31.92, 45.34, 13.0
2019-02-16 00:47:31,452 : Text to Image: 10.232, 28.712, 41.5, 16.0
2019-02-16 00:48:09,470 : Epoch 9 finished
2019-02-16 00:48:09,957 : Image to text: 30.2, 64.1, 78.7, 3.0
2019-02-16 00:48:10,346 : Text to Image: 25.08, 59.46, 76.66, 4.0
2019-02-16 00:48:10,824 : Image to text: 28.9, 63.1, 76.5, 3.0
2019-02-16 00:48:11,209 : Text to Image: 24.28, 58.84, 75.16, 4.0
2019-02-16 00:48:11,671 : Image to text: 30.2, 63.7, 78.9, 3.0
2019-02-16 00:48:12,044 : Text to Image: 26.14, 60.02, 75.82, 4.0
2019-02-16 00:48:12,508 : Image to text: 30.8, 64.0, 79.3, 3.0
2019-02-16 00:48:12,892 : Text to Image: 25.9, 59.66, 76.08, 4.0
2019-02-16 00:48:13,359 : Image to text: 31.6, 66.5, 78.7, 3.0
2019-02-16 00:48:13,735 : Text to Image: 25.82, 59.86, 75.0, 4.0
2019-02-16 00:48:13,735 : Dev mean Text to Image: 25.444000000000003, 59.568000000000005, 75.744, 4.0
2019-02-16 00:48:13,736 : Dev mean Image to text: 30.34, 64.28, 78.42, 3.0
2019-02-16 00:48:13,736 : start epoch
2019-02-16 00:48:57,617 : samples : 64000
2019-02-16 00:49:08,787 : Image to text: 12.36, 32.18, 44.9, 13.0
2019-02-16 00:49:18,814 : Text to Image: 10.14, 28.072, 40.596, 16.0
2019-02-16 00:50:05,387 : samples : 128000
2019-02-16 00:50:18,083 : Image to text: 12.28, 32.8, 46.1, 13.0
2019-02-16 00:50:28,131 : Text to Image: 10.748, 29.128, 41.836, 15.0
2019-02-16 00:51:12,499 : samples : 192000
2019-02-16 00:51:22,848 : Image to text: 11.74, 32.6, 45.44, 13.0
2019-02-16 00:51:30,376 : Text to Image: 10.028, 28.096, 40.484, 16.0
2019-02-16 00:52:15,694 : samples : 256000
2019-02-16 00:52:28,311 : Image to text: 12.0, 33.0, 46.02, 13.0
2019-02-16 00:52:38,370 : Text to Image: 10.324, 28.512, 40.924, 16.0
2019-02-16 00:53:25,226 : samples : 320000
2019-02-16 00:53:35,903 : Image to text: 12.4, 32.4, 45.12, 13.0
2019-02-16 00:53:43,508 : Text to Image: 9.968, 28.44, 40.872, 16.0
2019-02-16 00:54:27,228 : samples : 384000
2019-02-16 00:54:37,887 : Image to text: 12.34, 32.96, 45.98, 12.0
2019-02-16 00:54:46,083 : Text to Image: 10.272, 28.28, 40.72, 16.0
2019-02-16 00:55:32,375 : samples : 448000
2019-02-16 00:55:45,089 : Image to text: 12.68, 33.16, 45.98, 12.0
2019-02-16 00:55:55,184 : Text to Image: 10.496, 28.892, 41.688, 15.0
2019-02-16 00:56:40,720 : samples : 512000
2019-02-16 00:56:51,375 : Image to text: 12.24, 33.3, 46.16, 12.0
2019-02-16 00:56:59,015 : Text to Image: 10.472, 28.972, 41.652, 16.0
2019-02-16 00:57:36,081 : Epoch 10 finished
2019-02-16 00:57:36,986 : Image to text: 30.2, 63.6, 78.4, 3.0
2019-02-16 00:57:37,752 : Text to Image: 25.54, 60.66, 77.1, 4.0
2019-02-16 00:57:38,653 : Image to text: 27.7, 62.1, 76.4, 3.0
2019-02-16 00:57:39,387 : Text to Image: 24.4, 58.32, 74.86, 4.0
2019-02-16 00:57:40,289 : Image to text: 31.7, 63.5, 78.5, 3.0
2019-02-16 00:57:41,076 : Text to Image: 25.74, 60.22, 75.72, 4.0
2019-02-16 00:57:41,981 : Image to text: 31.0, 63.9, 78.7, 3.0
2019-02-16 00:57:42,762 : Text to Image: 25.52, 59.66, 76.28, 4.0
2019-02-16 00:57:43,659 : Image to text: 28.9, 63.8, 77.3, 3.0
2019-02-16 00:57:44,394 : Text to Image: 25.4, 59.18, 75.52, 4.0
2019-02-16 00:57:44,394 : Dev mean Text to Image: 25.32, 59.608000000000004, 75.89599999999999, 4.0
2019-02-16 00:57:44,394 : Dev mean Image to text: 29.9, 63.38, 77.86, 3.0
2019-02-16 00:57:44,394 : start epoch
2019-02-16 00:58:38,163 : samples : 64000
2019-02-16 00:58:50,800 : Image to text: 12.86, 33.16, 46.36, 12.0
2019-02-16 00:59:00,885 : Text to Image: 10.664, 29.168, 41.808, 15.0
2019-02-16 00:59:45,526 : samples : 128000
2019-02-16 00:59:56,213 : Image to text: 11.88, 32.76, 45.56, 13.0
2019-02-16 01:00:03,696 : Text to Image: 10.04, 28.348, 40.824, 16.0
2019-02-16 01:00:48,162 : samples : 192000
2019-02-16 01:01:00,103 : Image to text: 12.64, 32.82, 46.06, 12.0
2019-02-16 01:01:09,167 : Text to Image: 10.304, 28.316, 40.844, 16.0
2019-02-16 01:01:53,702 : samples : 256000
2019-02-16 01:02:04,367 : Image to text: 12.52, 33.24, 45.5, 13.0
2019-02-16 01:02:11,981 : Text to Image: 10.212, 28.412, 40.744, 16.0
2019-02-16 01:02:56,372 : samples : 320000
2019-02-16 01:03:06,639 : Image to text: 11.7, 32.3, 45.52, 13.0
2019-02-16 01:03:15,017 : Text to Image: 10.136, 28.548, 40.84, 16.0
2019-02-16 01:03:59,793 : samples : 384000
2019-02-16 01:04:10,413 : Image to text: 12.74, 33.22, 45.78, 13.0
2019-02-16 01:04:18,063 : Text to Image: 10.62, 28.916, 41.524, 16.0
2019-02-16 01:05:02,577 : samples : 448000
2019-02-16 01:05:13,283 : Image to text: 12.74, 32.7, 46.02, 13.0
2019-02-16 01:05:20,886 : Text to Image: 10.62, 29.248, 41.88, 15.0
2019-02-16 01:06:05,863 : samples : 512000
2019-02-16 01:06:16,297 : Image to text: 12.74, 32.54, 45.56, 13.0
2019-02-16 01:06:23,955 : Text to Image: 10.576, 29.18, 41.776, 15.0
2019-02-16 01:07:01,841 : Epoch 11 finished
2019-02-16 01:07:02,324 : Image to text: 30.2, 63.3, 79.5, 3.0
2019-02-16 01:07:02,692 : Text to Image: 24.8, 59.92, 76.16, 4.0
2019-02-16 01:07:03,156 : Image to text: 28.8, 61.9, 75.9, 3.0
2019-02-16 01:07:03,520 : Text to Image: 24.82, 57.46, 74.3, 4.0
2019-02-16 01:07:03,978 : Image to text: 28.7, 63.5, 78.3, 3.0
2019-02-16 01:07:04,350 : Text to Image: 24.8, 59.32, 75.62, 4.0
2019-02-16 01:07:04,826 : Image to text: 31.5, 64.3, 79.9, 3.0
2019-02-16 01:07:05,209 : Text to Image: 24.72, 59.22, 75.62, 4.0
2019-02-16 01:07:05,682 : Image to text: 28.5, 62.9, 78.3, 3.0
2019-02-16 01:07:06,064 : Text to Image: 25.56, 58.94, 75.24, 4.0
2019-02-16 01:07:06,064 : Dev mean Text to Image: 24.939999999999998, 58.97200000000001, 75.388, 4.0
2019-02-16 01:07:06,064 : Dev mean Image to text: 29.54, 63.17999999999999, 78.38, 3.0
2019-02-16 01:07:10,361 : 
Test scores | Image to text:             30.2, 64.88, 78.5, 3.0
2019-02-16 01:07:10,361 : Test scores | Text to image:             24.82, 58.792, 75.44, 4.0

2019-02-16 01:07:10,477 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 01:07:10,695 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 01:07:11,408 : loading BERT model bert-base-uncased
2019-02-16 01:07:11,408 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:07:11,441 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:07:11,441 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzk49tu14
2019-02-16 01:07:14,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:07:15,553 : Computing embeddings for train/dev/test
2019-02-16 01:09:04,797 : Computed embeddings
2019-02-16 01:09:04,797 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:10:13,671 : [('reg:1e-05', 66.57), ('reg:0.0001', 64.04), ('reg:0.001', 63.54), ('reg:0.01', 56.95)]
2019-02-16 01:10:13,671 : Validation : best param found is reg = 1e-05 with score             66.57
2019-02-16 01:10:13,672 : Evaluating...
2019-02-16 01:10:27,374 : 
Dev acc : 66.6 Test acc : 66.6 for LENGTH classification

2019-02-16 01:10:27,375 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 01:10:27,728 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 01:10:27,778 : loading BERT model bert-base-uncased
2019-02-16 01:10:27,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:10:27,892 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:10:27,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwxm2ng5p
2019-02-16 01:10:30,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:10:31,888 : Computing embeddings for train/dev/test
2019-02-16 01:12:25,931 : Computed embeddings
2019-02-16 01:12:25,931 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:13:29,767 : [('reg:1e-05', 48.54), ('reg:0.0001', 19.05), ('reg:0.001', 0.41), ('reg:0.01', 0.1)]
2019-02-16 01:13:29,768 : Validation : best param found is reg = 1e-05 with score             48.54
2019-02-16 01:13:29,768 : Evaluating...
2019-02-16 01:13:48,564 : 
Dev acc : 48.5 Test acc : 48.8 for WORDCONTENT classification

2019-02-16 01:13:48,565 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 01:13:49,130 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 01:13:49,200 : loading BERT model bert-base-uncased
2019-02-16 01:13:49,201 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:13:49,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:13:49,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpymv00day
2019-02-16 01:13:51,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:13:53,310 : Computing embeddings for train/dev/test
2019-02-16 01:15:36,901 : Computed embeddings
2019-02-16 01:15:36,901 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:17:13,231 : [('reg:1e-05', 27.91), ('reg:0.0001', 27.52), ('reg:0.001', 24.97), ('reg:0.01', 26.69)]
2019-02-16 01:17:13,231 : Validation : best param found is reg = 1e-05 with score             27.91
2019-02-16 01:17:13,231 : Evaluating...
2019-02-16 01:17:43,651 : 
Dev acc : 27.9 Test acc : 28.1 for DEPTH classification

2019-02-16 01:17:43,652 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 01:17:44,217 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 01:17:44,294 : loading BERT model bert-base-uncased
2019-02-16 01:17:44,295 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:17:44,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:17:44,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxp2wb406
2019-02-16 01:17:46,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:17:48,409 : Computing embeddings for train/dev/test
2019-02-16 01:19:37,044 : Computed embeddings
2019-02-16 01:19:37,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:21:10,973 : [('reg:1e-05', 52.22), ('reg:0.0001', 49.9), ('reg:0.001', 46.2), ('reg:0.01', 33.85)]
2019-02-16 01:21:10,974 : Validation : best param found is reg = 1e-05 with score             52.22
2019-02-16 01:21:10,974 : Evaluating...
2019-02-16 01:21:37,288 : 
Dev acc : 52.2 Test acc : 52.9 for TOPCONSTITUENTS classification

2019-02-16 01:21:37,290 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 01:21:37,711 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 01:21:37,778 : loading BERT model bert-base-uncased
2019-02-16 01:21:37,778 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:21:37,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:21:37,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzta32e9a
2019-02-16 01:21:40,384 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:21:41,985 : Computing embeddings for train/dev/test
2019-02-16 01:23:38,704 : Computed embeddings
2019-02-16 01:23:38,704 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:25:16,369 : [('reg:1e-05', 70.77), ('reg:0.0001', 70.62), ('reg:0.001', 75.07), ('reg:0.01', 73.32)]
2019-02-16 01:25:16,369 : Validation : best param found is reg = 0.001 with score             75.07
2019-02-16 01:25:16,369 : Evaluating...
2019-02-16 01:25:41,916 : 
Dev acc : 75.1 Test acc : 74.9 for BIGRAMSHIFT classification

2019-02-16 01:25:41,917 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 01:25:42,355 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 01:25:42,436 : loading BERT model bert-base-uncased
2019-02-16 01:25:42,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:25:42,574 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:25:42,574 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps34h86op
2019-02-16 01:25:45,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:25:46,683 : Computing embeddings for train/dev/test
2019-02-16 01:27:48,813 : Computed embeddings
2019-02-16 01:27:48,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:29:20,636 : [('reg:1e-05', 84.56), ('reg:0.0001', 84.55), ('reg:0.001', 84.72), ('reg:0.01', 84.24)]
2019-02-16 01:29:20,636 : Validation : best param found is reg = 0.001 with score             84.72
2019-02-16 01:29:20,636 : Evaluating...
2019-02-16 01:29:41,911 : 
Dev acc : 84.7 Test acc : 83.2 for TENSE classification

2019-02-16 01:29:41,912 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 01:29:42,339 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 01:29:42,410 : loading BERT model bert-base-uncased
2019-02-16 01:29:42,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:29:42,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:29:42,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl0e8ky9r
2019-02-16 01:29:45,084 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:29:46,632 : Computing embeddings for train/dev/test
2019-02-16 01:31:32,816 : Computed embeddings
2019-02-16 01:31:32,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:32:15,816 : [('reg:1e-05', 76.77), ('reg:0.0001', 76.86), ('reg:0.001', 76.86), ('reg:0.01', 74.77)]
2019-02-16 01:32:15,816 : Validation : best param found is reg = 0.0001 with score             76.86
2019-02-16 01:32:15,817 : Evaluating...
2019-02-16 01:32:23,934 : 
Dev acc : 76.9 Test acc : 75.8 for SUBJNUMBER classification

2019-02-16 01:32:23,936 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 01:32:24,835 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 01:32:24,935 : loading BERT model bert-base-uncased
2019-02-16 01:32:24,936 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:32:24,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:32:24,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwnr37mok
2019-02-16 01:32:28,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:32:30,382 : Computing embeddings for train/dev/test
2019-02-16 01:34:40,494 : Computed embeddings
2019-02-16 01:34:40,494 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:36:31,575 : [('reg:1e-05', 75.44), ('reg:0.0001', 75.12), ('reg:0.001', 75.28), ('reg:0.01', 73.9)]
2019-02-16 01:36:31,576 : Validation : best param found is reg = 1e-05 with score             75.44
2019-02-16 01:36:31,576 : Evaluating...
2019-02-16 01:37:01,673 : 
Dev acc : 75.4 Test acc : 76.0 for OBJNUMBER classification

2019-02-16 01:37:01,674 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 01:37:02,135 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 01:37:02,211 : loading BERT model bert-base-uncased
2019-02-16 01:37:02,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:37:02,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:37:02,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzi5xwakf
2019-02-16 01:37:04,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:37:06,405 : Computing embeddings for train/dev/test
2019-02-16 01:39:35,129 : Computed embeddings
2019-02-16 01:39:35,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:41:01,810 : [('reg:1e-05', 54.04), ('reg:0.0001', 54.05), ('reg:0.001', 54.07), ('reg:0.01', 54.16)]
2019-02-16 01:41:01,810 : Validation : best param found is reg = 0.01 with score             54.16
2019-02-16 01:41:01,810 : Evaluating...
2019-02-16 01:41:21,179 : 
Dev acc : 54.2 Test acc : 53.7 for ODDMANOUT classification

2019-02-16 01:41:21,180 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 01:41:21,612 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 01:41:21,692 : loading BERT model bert-base-uncased
2019-02-16 01:41:21,693 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:41:21,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:41:21,727 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzua7_pcs
2019-02-16 01:41:24,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:41:25,900 : Computing embeddings for train/dev/test
2019-02-16 01:43:42,159 : Computed embeddings
2019-02-16 01:43:42,159 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:44:43,118 : [('reg:1e-05', 55.39), ('reg:0.0001', 55.4), ('reg:0.001', 55.13), ('reg:0.01', 50.05)]
2019-02-16 01:44:43,118 : Validation : best param found is reg = 0.0001 with score             55.4
2019-02-16 01:44:43,118 : Evaluating...
2019-02-16 01:45:01,394 : 
Dev acc : 55.4 Test acc : 55.1 for COORDINATIONINVERSION classification

2019-02-16 01:45:01,397 : total results: {'STS12': {'MSRpar': {'pearson': (0.3371374024601905, 2.16204792579965e-21), 'spearman': SpearmanrResult(correlation=0.37835821963228683, pvalue=6.151770146981816e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7219649848677115, 9.377832797477276e-122), 'spearman': SpearmanrResult(correlation=0.7276171998366326, pvalue=1.462179004227035e-124), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5058900780829677, 3.383988020553368e-31), 'spearman': SpearmanrResult(correlation=0.6117555394468404, pvalue=1.8281546245001072e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5872227928562502, 1.0261105542691276e-70), 'spearman': SpearmanrResult(correlation=0.6119219008015674, pvalue=3.017149545160747e-78), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4988595141311005, 1.7043958373204728e-26), 'spearman': SpearmanrResult(correlation=0.41983308799244123, pvalue=1.813896697358388e-18), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5302149544796441, 'wmean': 0.5360335833708191}, 'spearman': {'mean': 0.5498971895419537, 'wmean': 0.5587941392914894}}}, 'STS13': {'FNWN': {'pearson': (0.1986497202199223, 0.0061396490765262815), 'spearman': SpearmanrResult(correlation=0.19348562488236565, pvalue=0.007639578361229931), 'nsamples': 189}, 'headlines': {'pearson': (0.6650815190106989, 6.100249749442635e-97), 'spearman': SpearmanrResult(correlation=0.6573867443227979, pvalue=5.340059954002557e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.592092600274722, 2.2716269158635486e-54), 'spearman': SpearmanrResult(correlation=0.5916855444993537, pvalue=2.7968244346607414e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4852746131684477, 'wmean': 0.5790132567558056}, 'spearman': {'mean': 0.48085263790150573, 'wmean': 0.5743629545393353}}}, 'STS14': {'deft-forum': {'pearson': (0.4096139598692221, 1.2357953810810065e-19), 'spearman': SpearmanrResult(correlation=0.4298129096199732, pvalue=1.1723479388408378e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7104545186683172, 2.1925134192304404e-47), 'spearman': SpearmanrResult(correlation=0.674121527910147, pvalue=4.2030409883844686e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6248657234122955, 1.8186050433501845e-82), 'spearman': SpearmanrResult(correlation=0.6010865555559064, pvalue=7.322383852057969e-75), 'nsamples': 750}, 'images': {'pearson': (0.6388911458350115, 2.86408387793588e-87), 'spearman': SpearmanrResult(correlation=0.6320273267933535, pvalue=6.891627944462661e-85), 'nsamples': 750}, 'OnWN': {'pearson': (0.672274403482019, 9.005225979504906e-100), 'spearman': SpearmanrResult(correlation=0.7020136221038634, pvalue=2.226057190011703e-112), 'nsamples': 750}, 'tweet-news': {'pearson': (0.577708241823742, 5.53458718551228e-68), 'spearman': SpearmanrResult(correlation=0.5567625004451875, pvalue=2.8255787917592806e-62), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6056346655151011, 'wmean': 0.6087379395883855}, 'spearman': {'mean': 0.5993040737380718, 'wmean': 0.6038852723668707}}}, 'STS15': {'answers-forums': {'pearson': (0.5038542601423485, 1.5493531705608256e-25), 'spearman': SpearmanrResult(correlation=0.464552481839159, pvalue=1.7960695466679514e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.6940773207970337, 7.2880116576492085e-109), 'spearman': SpearmanrResult(correlation=0.7015552937530586, pvalue=3.578546946112438e-112), 'nsamples': 750}, 'belief': {'pearson': (0.5406420820926097, 7.545740619714642e-30), 'spearman': SpearmanrResult(correlation=0.5836365567656353, pvalue=1.3026592271155328e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6665141215397001, 1.6898349447002245e-97), 'spearman': SpearmanrResult(correlation=0.670072746785518, pvalue=6.752327207492537e-99), 'nsamples': 750}, 'images': {'pearson': (0.7591257727046778, 1.3023512416012964e-141), 'spearman': SpearmanrResult(correlation=0.7680530145980171, pvalue=6.27585746036151e-147), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.632842711455274, 'wmean': 0.6604913465397226}, 'spearman': {'mean': 0.6375740187482777, 'wmean': 0.6659438936097477}}}, 'STS16': {'answer-answer': {'pearson': (0.39984267252105193, 3.611949400289921e-11), 'spearman': SpearmanrResult(correlation=0.4188248445219572, pvalue=3.2869414744136386e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6680059402682259, 1.5108790561694613e-33), 'spearman': SpearmanrResult(correlation=0.6783460208795807, pvalue=6.413220306437679e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6981691985608248, 6.129810389786437e-35), 'spearman': SpearmanrResult(correlation=0.7058627507329153, pvalue=5.348396991179477e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.7730944925913059, 9.418205517230193e-50), 'spearman': SpearmanrResult(correlation=0.8313356067330445, pvalue=1.1317969704684177e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.4767286954950688, 2.945899457793772e-13), 'spearman': SpearmanrResult(correlation=0.4860404290964914, pvalue=8.64163838701246e-14), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6031681998872955, 'wmean': 0.6043370886735862}, 'spearman': {'mean': 0.6240819303927978, 'wmean': 0.6256883980608708}}}, 'MR': {'devacc': 57.6, 'acc': 59.16, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.78, 'acc': 70.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.38, 'acc': 85.08, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.65, 'acc': 89.43, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.8, 'acc': 73.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.69, 'acc': 36.88, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 62.09, 'acc': 75.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.58, 'acc': 67.71, 'f1': 73.26, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 78.53, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791443666955994, 'pearson': 0.7852568825300805, 'spearman': 0.7194472533832414, 'mse': 0.394842839137381, 'yhat': array([3.01226072, 4.15230138, 1.1619529 , ..., 3.12687879, 4.55433739,        4.85673748]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6448378815406637, 'pearson': 0.6322046135664458, 'spearman': 0.6266734611653674, 'mse': 1.4739610306651059, 'yhat': array([1.04024258, 1.74381548, 1.91207808, ..., 4.08654689, 4.20948703,        3.64535713]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.59, 'acc': 67.1, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 333.79599999999994, 'acc': [(30.2, 64.88, 78.5, 3.0), (24.82, 58.792, 75.44, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 66.57, 'acc': 66.64, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 48.54, 'acc': 48.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.91, 'acc': 28.07, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.22, 'acc': 52.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 75.07, 'acc': 74.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.72, 'acc': 83.22, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.86, 'acc': 75.76, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.44, 'acc': 76.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.16, 'acc': 53.73, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.4, 'acc': 55.07, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 01:45:01,397 : STS12 p=0.5360, STS12 s=0.5588, STS13 p=0.5790, STS13 s=0.5744, STS14 p=0.6087, STS14 s=0.6039, STS15 p=0.6605, STS15 s=0.6659, STS 16 p=0.6043, STS16 s=0.6257, STS B p=0.6322, STS B s=0.6267, STS B m=1.4740, SICK-R p=0.7853, SICK-R s=0.7194, SICK-P m=0.3948
2019-02-16 01:45:01,397 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 01:45:01,397 : 0.5360,0.5588,0.5790,0.5744,0.6087,0.6039,0.6605,0.6659,0.6043,0.6257,0.6322,0.6267,1.4740,0.7853,0.7194,0.3948
2019-02-16 01:45:01,397 : MR=59.16, CR=70.18, SUBJ=89.43, MPQA=85.08, SST-B=73.92, SST-F=36.88, TREC=75.20, SICK-E=78.53, SNLI=67.10, MRPC=67.71, MRPC f=73.26
2019-02-16 01:45:01,397 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 01:45:01,397 : 59.16,70.18,89.43,85.08,73.92,36.88,75.20,78.53,67.10,67.71,73.26
2019-02-16 01:45:01,397 : COCO r1i2t=30.20, COCO r5i2t=64.88, COCO r10i2t=78.50, COCO medr_i2t=3.00, COCO r1t2i=24.82, COCO r5t2i=58.79, COCO r10t2i=75.44, COCO medr_t2i=4.00
2019-02-16 01:45:01,397 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 01:45:01,397 : 30.20,64.88,78.50,3.00,24.82,58.79,75.44,4.00
2019-02-16 01:45:01,397 : SentLen=66.64, WC=48.80, TreeDepth=28.07, TopConst=52.92, BShift=74.89, Tense=83.22, SubjNum=75.76, ObjNum=76.00, SOMO=53.73, CoordInv=55.07, average=61.51
2019-02-16 01:45:01,397 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 01:45:01,397 : 66.64,48.80,28.07,52.92,74.89,83.22,75.76,76.00,53.73,55.07,61.51
2019-02-16 01:45:01,397 : ********************************************************************************
2019-02-16 01:45:01,397 : ********************************************************************************
2019-02-16 01:45:01,397 : ********************************************************************************
2019-02-16 01:45:01,397 : layer 4
2019-02-16 01:45:01,397 : ********************************************************************************
2019-02-16 01:45:01,397 : ********************************************************************************
2019-02-16 01:45:01,398 : ********************************************************************************
2019-02-16 01:45:01,496 : ***** Transfer task : STS12 *****


2019-02-16 01:45:01,510 : loading BERT model bert-base-uncased
2019-02-16 01:45:01,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:01,532 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:01,532 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfx4_jc1w
2019-02-16 01:45:04,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:07,414 : MSRpar : pearson = 0.3122, spearman = 0.3482
2019-02-16 01:45:08,182 : MSRvid : pearson = 0.6635, spearman = 0.6741
2019-02-16 01:45:08,857 : SMTeuroparl : pearson = 0.5058, spearman = 0.6124
2019-02-16 01:45:10,023 : surprise.OnWN : pearson = 0.5652, spearman = 0.5960
2019-02-16 01:45:10,697 : surprise.SMTnews : pearson = 0.5155, spearman = 0.4414
2019-02-16 01:45:10,697 : ALL (weighted average) : Pearson = 0.5127,             Spearman = 0.5376
2019-02-16 01:45:10,697 : ALL (average) : Pearson = 0.5124,             Spearman = 0.5344

2019-02-16 01:45:10,698 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 01:45:10,709 : loading BERT model bert-base-uncased
2019-02-16 01:45:10,709 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:10,734 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:10,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppo40aos_
2019-02-16 01:45:13,257 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:15,416 : FNWN : pearson = 0.1599, spearman = 0.1611
2019-02-16 01:45:16,406 : headlines : pearson = 0.6460, spearman = 0.6406
2019-02-16 01:45:17,142 : OnWN : pearson = 0.5458, spearman = 0.5456
2019-02-16 01:45:17,142 : ALL (weighted average) : Pearson = 0.5473,             Spearman = 0.5446
2019-02-16 01:45:17,142 : ALL (average) : Pearson = 0.4506,             Spearman = 0.4491

2019-02-16 01:45:17,142 : ***** Transfer task : STS14 *****


2019-02-16 01:45:17,159 : loading BERT model bert-base-uncased
2019-02-16 01:45:17,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:17,178 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:17,178 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzpwxo3an
2019-02-16 01:45:19,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:21,902 : deft-forum : pearson = 0.3898, spearman = 0.4053
2019-02-16 01:45:22,614 : deft-news : pearson = 0.7204, spearman = 0.6878
2019-02-16 01:45:23,599 : headlines : pearson = 0.6031, spearman = 0.5777
2019-02-16 01:45:24,531 : images : pearson = 0.5948, spearman = 0.5932
2019-02-16 01:45:25,474 : OnWN : pearson = 0.6363, spearman = 0.6681
2019-02-16 01:45:26,718 : tweet-news : pearson = 0.5656, spearman = 0.5427
2019-02-16 01:45:26,718 : ALL (weighted average) : Pearson = 0.5844,             Spearman = 0.5800
2019-02-16 01:45:26,718 : ALL (average) : Pearson = 0.5850,             Spearman = 0.5791

2019-02-16 01:45:26,718 : ***** Transfer task : STS15 *****


2019-02-16 01:45:26,756 : loading BERT model bert-base-uncased
2019-02-16 01:45:26,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:26,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:26,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfvvc4vkv
2019-02-16 01:45:29,321 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:31,702 : answers-forums : pearson = 0.5056, spearman = 0.4758
2019-02-16 01:45:32,642 : answers-students : pearson = 0.6828, spearman = 0.6882
2019-02-16 01:45:33,504 : belief : pearson = 0.5274, spearman = 0.5690
2019-02-16 01:45:34,528 : headlines : pearson = 0.6427, spearman = 0.6496
2019-02-16 01:45:35,527 : images : pearson = 0.7302, spearman = 0.7400
2019-02-16 01:45:35,528 : ALL (weighted average) : Pearson = 0.6430,             Spearman = 0.6500
2019-02-16 01:45:35,528 : ALL (average) : Pearson = 0.6177,             Spearman = 0.6245

2019-02-16 01:45:35,528 : ***** Transfer task : STS16 *****


2019-02-16 01:45:35,605 : loading BERT model bert-base-uncased
2019-02-16 01:45:35,605 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:35,626 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:35,626 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgueofz2l
2019-02-16 01:45:38,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:40,109 : answer-answer : pearson = 0.4276, spearman = 0.4392
2019-02-16 01:45:40,408 : headlines : pearson = 0.6432, spearman = 0.6527
2019-02-16 01:45:40,820 : plagiarism : pearson = 0.6693, spearman = 0.6792
2019-02-16 01:45:41,451 : postediting : pearson = 0.7621, spearman = 0.8150
2019-02-16 01:45:41,745 : question-question : pearson = 0.4029, spearman = 0.4173
2019-02-16 01:45:41,745 : ALL (weighted average) : Pearson = 0.5842,             Spearman = 0.6040
2019-02-16 01:45:41,745 : ALL (average) : Pearson = 0.5810,             Spearman = 0.6007

2019-02-16 01:45:41,745 : ***** Transfer task : MR *****


2019-02-16 01:45:41,766 : loading BERT model bert-base-uncased
2019-02-16 01:45:41,767 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:45:41,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:45:41,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwf67ms52
2019-02-16 01:45:44,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:45:45,879 : Generating sentence embeddings
2019-02-16 01:45:59,505 : Generated sentence embeddings
2019-02-16 01:45:59,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:46:23,447 : Best param found at split 1: l2reg = 0.01                 with score 60.75
2019-02-16 01:46:48,406 : Best param found at split 2: l2reg = 0.001                 with score 58.98
2019-02-16 01:47:07,735 : Best param found at split 3: l2reg = 0.001                 with score 56.0
2019-02-16 01:47:30,155 : Best param found at split 4: l2reg = 1e-05                 with score 56.05
2019-02-16 01:47:56,358 : Best param found at split 5: l2reg = 1e-05                 with score 60.62
2019-02-16 01:47:58,082 : Dev acc : 58.48 Test acc : 58.67

2019-02-16 01:47:58,083 : ***** Transfer task : CR *****


2019-02-16 01:47:58,096 : loading BERT model bert-base-uncased
2019-02-16 01:47:58,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:47:58,125 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:47:58,125 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdsxmz_bp
2019-02-16 01:48:00,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:48:02,239 : Generating sentence embeddings
2019-02-16 01:48:05,988 : Generated sentence embeddings
2019-02-16 01:48:05,988 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:48:12,504 : Best param found at split 1: l2reg = 0.001                 with score 68.46
2019-02-16 01:48:18,511 : Best param found at split 2: l2reg = 0.01                 with score 67.04
2019-02-16 01:48:26,337 : Best param found at split 3: l2reg = 0.0001                 with score 69.2
2019-02-16 01:48:32,627 : Best param found at split 4: l2reg = 0.001                 with score 73.65
2019-02-16 01:48:37,791 : Best param found at split 5: l2reg = 1e-05                 with score 73.62
2019-02-16 01:48:37,969 : Dev acc : 70.39 Test acc : 66.38

2019-02-16 01:48:37,969 : ***** Transfer task : MPQA *****


2019-02-16 01:48:37,976 : loading BERT model bert-base-uncased
2019-02-16 01:48:37,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:48:37,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:48:37,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4oqyudib
2019-02-16 01:48:40,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:48:42,003 : Generating sentence embeddings
2019-02-16 01:48:45,829 : Generated sentence embeddings
2019-02-16 01:48:45,830 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:48:58,525 : Best param found at split 1: l2reg = 1e-05                 with score 86.09
2019-02-16 01:49:12,261 : Best param found at split 2: l2reg = 0.0001                 with score 87.0
2019-02-16 01:49:25,829 : Best param found at split 3: l2reg = 0.001                 with score 86.6
2019-02-16 01:49:39,585 : Best param found at split 4: l2reg = 0.0001                 with score 86.28
2019-02-16 01:49:55,569 : Best param found at split 5: l2reg = 0.01                 with score 86.52
2019-02-16 01:49:56,494 : Dev acc : 86.5 Test acc : 85.2

2019-02-16 01:49:56,496 : ***** Transfer task : SUBJ *****


2019-02-16 01:49:56,515 : loading BERT model bert-base-uncased
2019-02-16 01:49:56,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:49:56,536 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:49:56,536 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfdbsz44v
2019-02-16 01:49:59,050 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:50:00,500 : Generating sentence embeddings
2019-02-16 01:50:13,820 : Generated sentence embeddings
2019-02-16 01:50:13,821 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:50:38,961 : Best param found at split 1: l2reg = 1e-05                 with score 88.4
2019-02-16 01:51:03,562 : Best param found at split 2: l2reg = 0.0001                 with score 86.64
2019-02-16 01:51:28,234 : Best param found at split 3: l2reg = 0.01                 with score 87.7
2019-02-16 01:51:56,587 : Best param found at split 4: l2reg = 1e-05                 with score 89.39
2019-02-16 01:52:22,334 : Best param found at split 5: l2reg = 1e-05                 with score 87.49
2019-02-16 01:52:23,814 : Dev acc : 87.92 Test acc : 89.93

2019-02-16 01:52:23,815 : ***** Transfer task : SST Binary classification *****


2019-02-16 01:52:23,966 : loading BERT model bert-base-uncased
2019-02-16 01:52:23,966 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:52:23,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:52:23,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9pl6al71
2019-02-16 01:52:26,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:52:28,136 : Computing embedding for train
2019-02-16 01:53:13,624 : Computed train embeddings
2019-02-16 01:53:13,625 : Computing embedding for dev
2019-02-16 01:53:14,577 : Computed dev embeddings
2019-02-16 01:53:14,577 : Computing embedding for test
2019-02-16 01:53:16,548 : Computed test embeddings
2019-02-16 01:53:16,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:54:16,359 : [('reg:1e-05', 76.95), ('reg:0.0001', 76.83), ('reg:0.001', 76.83), ('reg:0.01', 62.73)]
2019-02-16 01:54:16,359 : Validation : best param found is reg = 1e-05 with score             76.95
2019-02-16 01:54:16,359 : Evaluating...
2019-02-16 01:54:33,368 : 
Dev acc : 76.95 Test acc : 76.22 for             SST Binary classification

2019-02-16 01:54:33,369 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 01:54:33,424 : loading BERT model bert-base-uncased
2019-02-16 01:54:33,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:54:33,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:54:33,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnm0s1cut
2019-02-16 01:54:35,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:54:37,471 : Computing embedding for train
2019-02-16 01:54:47,192 : Computed train embeddings
2019-02-16 01:54:47,192 : Computing embedding for dev
2019-02-16 01:54:48,418 : Computed dev embeddings
2019-02-16 01:54:48,418 : Computing embedding for test
2019-02-16 01:54:50,853 : Computed test embeddings
2019-02-16 01:54:50,853 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:54:55,566 : [('reg:1e-05', 31.88), ('reg:0.0001', 32.33), ('reg:0.001', 29.25), ('reg:0.01', 30.15)]
2019-02-16 01:54:55,566 : Validation : best param found is reg = 0.0001 with score             32.33
2019-02-16 01:54:55,566 : Evaluating...
2019-02-16 01:54:56,967 : 
Dev acc : 32.33 Test acc : 30.27 for             SST Fine-Grained classification

2019-02-16 01:54:56,968 : ***** Transfer task : TREC *****


2019-02-16 01:54:56,990 : loading BERT model bert-base-uncased
2019-02-16 01:54:56,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:54:57,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:54:57,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprmsbpmu8
2019-02-16 01:54:59,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:55:04,522 : Computed train embeddings
2019-02-16 01:55:04,776 : Computed test embeddings
2019-02-16 01:55:04,776 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 01:55:19,673 : [('reg:1e-05', 66.27), ('reg:0.0001', 62.52), ('reg:0.001', 63.85), ('reg:0.01', 59.75)]
2019-02-16 01:55:19,673 : Cross-validation : best param found is reg = 1e-05             with score 66.27
2019-02-16 01:55:19,673 : Evaluating...
2019-02-16 01:55:20,462 : 
Dev acc : 66.27 Test acc : 80.2             for TREC

2019-02-16 01:55:20,463 : ***** Transfer task : MRPC *****


2019-02-16 01:55:20,487 : loading BERT model bert-base-uncased
2019-02-16 01:55:20,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:55:20,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:55:20,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj5_0g049
2019-02-16 01:55:23,084 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:55:24,623 : Computing embedding for train
2019-02-16 01:55:34,341 : Computed train embeddings
2019-02-16 01:55:34,341 : Computing embedding for test
2019-02-16 01:55:38,794 : Computed test embeddings
2019-02-16 01:55:38,811 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 01:55:46,128 : [('reg:1e-05', 71.64), ('reg:0.0001', 70.95), ('reg:0.001', 71.02), ('reg:0.01', 70.58)]
2019-02-16 01:55:46,128 : Cross-validation : best param found is reg = 1e-05             with score 71.64
2019-02-16 01:55:46,128 : Evaluating...
2019-02-16 01:55:46,402 : Dev acc : 71.64 Test acc 72.93; Test F1 81.65 for MRPC.

2019-02-16 01:55:46,402 : ***** Transfer task : SICK-Entailment*****


2019-02-16 01:55:46,465 : loading BERT model bert-base-uncased
2019-02-16 01:55:46,465 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:55:46,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:55:46,486 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcpnlrx9j
2019-02-16 01:55:48,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:55:50,404 : Computing embedding for train
2019-02-16 01:55:55,511 : Computed train embeddings
2019-02-16 01:55:55,511 : Computing embedding for dev
2019-02-16 01:55:56,188 : Computed dev embeddings
2019-02-16 01:55:56,188 : Computing embedding for test
2019-02-16 01:56:01,739 : Computed test embeddings
2019-02-16 01:56:01,768 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:56:04,112 : [('reg:1e-05', 75.2), ('reg:0.0001', 71.4), ('reg:0.001', 76.2), ('reg:0.01', 71.6)]
2019-02-16 01:56:04,112 : Validation : best param found is reg = 0.001 with score             76.2
2019-02-16 01:56:04,112 : Evaluating...
2019-02-16 01:56:04,728 : 
Dev acc : 76.2 Test acc : 76.74 for                        SICK entailment

2019-02-16 01:56:04,729 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 01:56:04,763 : loading BERT model bert-base-uncased
2019-02-16 01:56:04,763 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:56:04,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:56:04,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc7eulfn_
2019-02-16 01:56:07,404 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:56:08,906 : Computing embedding for train
2019-02-16 01:56:14,082 : Computed train embeddings
2019-02-16 01:56:14,083 : Computing embedding for dev
2019-02-16 01:56:14,735 : Computed dev embeddings
2019-02-16 01:56:14,735 : Computing embedding for test
2019-02-16 01:56:20,267 : Computed test embeddings
2019-02-16 01:56:50,139 : Dev : Pearson 0.783497781215318
2019-02-16 01:56:50,139 : Test : Pearson 0.7914938097001635 Spearman 0.730879495418651 MSE 0.3827408204461735                        for SICK Relatedness

2019-02-16 01:56:50,140 : 

***** Transfer task : STSBenchmark*****


2019-02-16 01:56:50,181 : loading BERT model bert-base-uncased
2019-02-16 01:56:50,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:56:50,210 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:56:50,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkxehf4vu
2019-02-16 01:56:52,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:56:54,276 : Computing embedding for train
2019-02-16 01:57:02,569 : Computed train embeddings
2019-02-16 01:57:02,569 : Computing embedding for dev
2019-02-16 01:57:05,016 : Computed dev embeddings
2019-02-16 01:57:05,017 : Computing embedding for test
2019-02-16 01:57:06,998 : Computed test embeddings
2019-02-16 01:57:44,155 : Dev : Pearson 0.6325067065473199
2019-02-16 01:57:44,155 : Test : Pearson 0.6318460324632185 Spearman 0.6285202863684332 MSE 1.465290329959281                        for SICK Relatedness

2019-02-16 01:57:44,156 : ***** Transfer task : SNLI Entailment*****


2019-02-16 01:57:49,080 : loading BERT model bert-base-uncased
2019-02-16 01:57:49,080 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:57:49,220 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:57:49,220 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuihdicoy
2019-02-16 01:57:51,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:57:53,437 : PROGRESS (encoding): 0.00%
2019-02-16 01:59:12,376 : PROGRESS (encoding): 14.56%
2019-02-16 02:00:40,080 : PROGRESS (encoding): 29.12%
2019-02-16 02:02:07,861 : PROGRESS (encoding): 43.69%
2019-02-16 02:03:43,166 : PROGRESS (encoding): 58.25%
2019-02-16 02:05:28,447 : PROGRESS (encoding): 72.81%
2019-02-16 02:07:18,187 : PROGRESS (encoding): 87.37%
2019-02-16 02:09:09,089 : PROGRESS (encoding): 0.00%
2019-02-16 02:09:22,639 : PROGRESS (encoding): 0.00%
2019-02-16 02:09:35,777 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 02:10:26,125 : [('reg:1e-09', 63.56)]
2019-02-16 02:10:26,126 : Validation : best param found is reg = 1e-09 with score             63.56
2019-02-16 02:10:26,126 : Evaluating...
2019-02-16 02:11:12,875 : Dev acc : 63.56 Test acc : 63.04 for SNLI

2019-02-16 02:11:12,875 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 02:11:22,277 : loading BERT model bert-base-uncased
2019-02-16 02:11:22,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 02:11:22,327 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 02:11:22,327 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcb4rmwu5
2019-02-16 02:11:24,878 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 02:11:26,457 : Computing embedding for train
2019-02-16 02:18:59,281 : Computed train embeddings
2019-02-16 02:18:59,281 : Computing embedding for dev
2019-02-16 02:19:19,150 : Computed dev embeddings
2019-02-16 02:19:19,150 : Computing embedding for test
2019-02-16 02:19:39,071 : Computed test embeddings
2019-02-16 02:19:39,087 : prepare data
2019-02-16 02:19:39,157 : start epoch
2019-02-16 02:20:23,548 : samples : 64000
2019-02-16 02:20:34,234 : Image to text: 6.48, 19.74, 30.78, 26.0
2019-02-16 02:20:41,965 : Text to Image: 5.36, 17.54, 27.36, 31.0
2019-02-16 02:21:26,788 : samples : 128000
2019-02-16 02:21:37,480 : Image to text: 7.78, 23.46, 33.78, 23.0
2019-02-16 02:21:45,203 : Text to Image: 5.84, 18.776, 28.992, 28.0
2019-02-16 02:22:29,682 : samples : 192000
2019-02-16 02:22:40,269 : Image to text: 7.0, 22.68, 34.0, 24.0
2019-02-16 02:22:47,938 : Text to Image: 5.736, 18.452, 28.456, 28.0
2019-02-16 02:23:36,055 : samples : 256000
2019-02-16 02:23:48,214 : Image to text: 7.92, 24.46, 35.8, 20.0
2019-02-16 02:23:56,967 : Text to Image: 6.548, 20.432, 31.016, 26.0
2019-02-16 02:24:43,786 : samples : 320000
2019-02-16 02:24:54,432 : Image to text: 8.5, 24.86, 36.74, 20.0
2019-02-16 02:25:02,138 : Text to Image: 7.12, 21.764, 32.684, 23.0
2019-02-16 02:25:45,826 : samples : 384000
2019-02-16 02:25:56,438 : Image to text: 7.9, 24.62, 36.68, 21.0
2019-02-16 02:26:04,100 : Text to Image: 6.652, 20.844, 32.152, 24.0
2019-02-16 02:26:48,424 : samples : 448000
2019-02-16 02:26:59,179 : Image to text: 8.12, 24.56, 36.48, 20.0
2019-02-16 02:27:06,883 : Text to Image: 6.668, 20.54, 31.408, 25.0
2019-02-16 02:27:51,050 : samples : 512000
2019-02-16 02:28:01,720 : Image to text: 8.34, 25.4, 36.96, 20.0
2019-02-16 02:28:09,436 : Text to Image: 6.736, 21.092, 32.232, 24.0
2019-02-16 02:28:46,713 : Epoch 1 finished
2019-02-16 02:28:47,169 : Image to text: 23.1, 55.4, 71.1, 4.0
2019-02-16 02:28:47,524 : Text to Image: 19.02, 49.32, 67.06, 6.0
2019-02-16 02:28:47,985 : Image to text: 22.8, 56.5, 70.6, 4.0
2019-02-16 02:28:48,344 : Text to Image: 19.02, 49.14, 67.24, 6.0
2019-02-16 02:28:48,822 : Image to text: 23.6, 54.8, 71.3, 5.0
2019-02-16 02:28:49,182 : Text to Image: 19.24, 49.26, 66.62, 6.0
2019-02-16 02:28:49,648 : Image to text: 23.7, 55.1, 72.6, 4.0
2019-02-16 02:28:50,010 : Text to Image: 19.5, 51.24, 67.82, 5.0
2019-02-16 02:28:50,494 : Image to text: 23.9, 56.0, 69.0, 4.0
2019-02-16 02:28:50,835 : Text to Image: 19.4, 50.24, 67.24, 5.0
2019-02-16 02:28:50,835 : Dev mean Text to Image: 19.236, 49.84, 67.196, 5.6
2019-02-16 02:28:50,835 : Dev mean Image to text: 23.42, 55.56, 70.91999999999999, 4.2
2019-02-16 02:28:50,835 : start epoch
2019-02-16 02:29:35,106 : samples : 64000
2019-02-16 02:29:45,782 : Image to text: 9.48, 26.98, 38.02, 18.0
2019-02-16 02:29:53,449 : Text to Image: 7.076, 21.796, 32.62, 23.0
2019-02-16 02:30:37,792 : samples : 128000
2019-02-16 02:30:48,489 : Image to text: 9.0, 26.86, 38.2, 18.0
2019-02-16 02:30:56,264 : Text to Image: 7.236, 23.196, 34.68, 21.0
2019-02-16 02:31:40,992 : samples : 192000
2019-02-16 02:31:51,786 : Image to text: 8.98, 27.3, 40.08, 17.0
2019-02-16 02:31:59,524 : Text to Image: 7.888, 23.736, 35.236, 21.0
2019-02-16 02:32:43,790 : samples : 256000
2019-02-16 02:32:54,521 : Image to text: 9.4, 27.1, 39.58, 17.0
2019-02-16 02:33:02,238 : Text to Image: 8.296, 24.448, 36.192, 20.0
2019-02-16 02:33:46,429 : samples : 320000
2019-02-16 02:33:57,098 : Image to text: 9.14, 26.78, 39.44, 17.0
2019-02-16 02:34:04,870 : Text to Image: 7.536, 22.844, 34.2, 22.0
2019-02-16 02:34:48,856 : samples : 384000
2019-02-16 02:34:59,569 : Image to text: 9.58, 27.28, 39.34, 18.0
2019-02-16 02:35:07,302 : Text to Image: 7.676, 23.172, 34.808, 21.0
2019-02-16 02:35:51,825 : samples : 448000
2019-02-16 02:36:02,550 : Image to text: 9.82, 27.26, 39.52, 17.0
2019-02-16 02:36:10,263 : Text to Image: 7.772, 23.24, 34.768, 21.0
2019-02-16 02:36:55,081 : samples : 512000
2019-02-16 02:37:05,760 : Image to text: 10.16, 27.82, 41.06, 16.0
2019-02-16 02:37:13,416 : Text to Image: 8.544, 25.244, 36.908, 19.0
2019-02-16 02:37:50,991 : Epoch 2 finished
2019-02-16 02:37:51,448 : Image to text: 24.0, 59.2, 75.7, 4.0
2019-02-16 02:37:51,810 : Text to Image: 21.56, 54.14, 71.92, 5.0
2019-02-16 02:37:52,271 : Image to text: 23.6, 57.5, 72.4, 4.0
2019-02-16 02:37:52,628 : Text to Image: 20.72, 53.56, 70.8, 5.0
2019-02-16 02:37:53,086 : Image to text: 26.0, 59.5, 75.2, 4.0
2019-02-16 02:37:53,428 : Text to Image: 22.5, 53.7, 70.06, 5.0
2019-02-16 02:37:53,888 : Image to text: 26.3, 61.0, 75.6, 4.0
2019-02-16 02:37:54,229 : Text to Image: 22.78, 54.76, 71.16, 5.0
2019-02-16 02:37:54,686 : Image to text: 26.9, 57.2, 72.3, 4.0
2019-02-16 02:37:55,033 : Text to Image: 21.76, 54.38, 70.68, 5.0
2019-02-16 02:37:55,034 : Dev mean Text to Image: 21.864, 54.108000000000004, 70.92399999999999, 5.0
2019-02-16 02:37:55,034 : Dev mean Image to text: 25.359999999999996, 58.879999999999995, 74.24, 4.0
2019-02-16 02:37:55,034 : start epoch
2019-02-16 02:38:39,558 : samples : 64000
2019-02-16 02:38:50,325 : Image to text: 9.9, 28.54, 40.76, 16.0
2019-02-16 02:38:58,066 : Text to Image: 8.564, 24.84, 36.488, 19.0
2019-02-16 02:39:43,137 : samples : 128000
2019-02-16 02:39:53,805 : Image to text: 8.76, 26.38, 38.32, 18.0
2019-02-16 02:40:01,527 : Text to Image: 7.148, 22.296, 33.488, 22.0
2019-02-16 02:40:50,432 : samples : 192000
2019-02-16 02:41:02,430 : Image to text: 9.66, 28.06, 40.06, 17.0
2019-02-16 02:41:10,987 : Text to Image: 7.32, 23.04, 34.644, 21.0
2019-02-16 02:41:56,418 : samples : 256000
2019-02-16 02:42:07,090 : Image to text: 9.42, 28.32, 41.08, 16.0
2019-02-16 02:42:14,799 : Text to Image: 7.764, 24.108, 35.432, 21.0
2019-02-16 02:42:59,113 : samples : 320000
2019-02-16 02:43:09,750 : Image to text: 9.98, 28.9, 41.92, 15.0
2019-02-16 02:43:17,345 : Text to Image: 8.348, 24.84, 36.588, 20.0
2019-02-16 02:44:01,839 : samples : 384000
2019-02-16 02:44:12,531 : Image to text: 10.84, 29.36, 41.02, 16.0
2019-02-16 02:44:20,234 : Text to Image: 8.32, 24.328, 35.8, 20.0
2019-02-16 02:45:04,814 : samples : 448000
2019-02-16 02:45:15,536 : Image to text: 10.26, 29.28, 41.48, 16.0
2019-02-16 02:45:23,252 : Text to Image: 8.416, 25.236, 36.964, 19.0
2019-02-16 02:46:08,003 : samples : 512000
2019-02-16 02:46:18,610 : Image to text: 10.3, 28.9, 41.64, 16.0
2019-02-16 02:46:26,301 : Text to Image: 8.54, 25.564, 37.396, 18.0
2019-02-16 02:47:04,159 : Epoch 3 finished
2019-02-16 02:47:04,591 : Image to text: 26.8, 58.3, 73.7, 4.0
2019-02-16 02:47:04,929 : Text to Image: 21.36, 54.0, 71.82, 5.0
2019-02-16 02:47:05,364 : Image to text: 24.4, 59.2, 73.7, 4.0
2019-02-16 02:47:05,703 : Text to Image: 20.68, 53.64, 71.12, 5.0
2019-02-16 02:47:06,175 : Image to text: 25.7, 58.9, 75.7, 4.0
2019-02-16 02:47:06,530 : Text to Image: 21.4, 53.88, 70.98, 5.0
2019-02-16 02:47:06,989 : Image to text: 27.9, 61.4, 75.5, 4.0
2019-02-16 02:47:07,345 : Text to Image: 22.48, 53.82, 71.54, 5.0
2019-02-16 02:47:07,817 : Image to text: 27.4, 60.9, 74.6, 4.0
2019-02-16 02:47:08,175 : Text to Image: 22.12, 55.22, 71.3, 5.0
2019-02-16 02:47:08,175 : Dev mean Text to Image: 21.608, 54.111999999999995, 71.352, 5.0
2019-02-16 02:47:08,175 : Dev mean Image to text: 26.44, 59.74, 74.64, 4.0
2019-02-16 02:47:08,175 : start epoch
2019-02-16 02:47:52,549 : samples : 64000
2019-02-16 02:48:03,221 : Image to text: 10.42, 29.9, 42.12, 15.0
2019-02-16 02:48:10,932 : Text to Image: 8.872, 25.776, 38.012, 18.0
2019-02-16 02:48:55,823 : samples : 128000
2019-02-16 02:49:06,406 : Image to text: 10.72, 29.38, 42.58, 15.0
2019-02-16 02:49:14,083 : Text to Image: 8.772, 25.816, 37.648, 18.0
2019-02-16 02:49:58,454 : samples : 192000
2019-02-16 02:50:09,193 : Image to text: 10.38, 28.92, 41.74, 16.0
2019-02-16 02:50:16,938 : Text to Image: 8.388, 24.852, 36.368, 20.0
2019-02-16 02:51:01,835 : samples : 256000
2019-02-16 02:51:12,561 : Image to text: 9.94, 28.98, 41.82, 16.0
2019-02-16 02:51:20,326 : Text to Image: 9.004, 25.704, 37.616, 19.0
2019-02-16 02:52:04,660 : samples : 320000
2019-02-16 02:52:15,338 : Image to text: 10.64, 29.5, 42.96, 15.0
2019-02-16 02:52:23,139 : Text to Image: 9.256, 26.332, 38.36, 18.0
2019-02-16 02:53:07,818 : samples : 384000
2019-02-16 02:53:18,479 : Image to text: 10.06, 29.88, 42.12, 15.0
2019-02-16 02:53:26,313 : Text to Image: 8.956, 25.568, 37.496, 19.0
2019-02-16 02:54:11,198 : samples : 448000
2019-02-16 02:54:21,898 : Image to text: 10.56, 28.88, 41.94, 15.0
2019-02-16 02:54:29,735 : Text to Image: 9.456, 26.48, 38.54, 18.0
2019-02-16 02:55:13,695 : samples : 512000
2019-02-16 02:55:24,374 : Image to text: 11.08, 30.26, 42.82, 15.0
2019-02-16 02:55:32,312 : Text to Image: 9.128, 26.108, 37.932, 18.0
2019-02-16 02:56:11,047 : Epoch 4 finished
2019-02-16 02:56:11,505 : Image to text: 28.0, 62.1, 78.3, 3.0
2019-02-16 02:56:11,860 : Text to Image: 21.96, 56.2, 73.64, 4.0
2019-02-16 02:56:12,315 : Image to text: 26.3, 59.8, 73.2, 4.0
2019-02-16 02:56:12,671 : Text to Image: 22.08, 54.86, 72.06, 5.0
2019-02-16 02:56:13,143 : Image to text: 28.1, 60.5, 75.7, 4.0
2019-02-16 02:56:13,495 : Text to Image: 23.48, 56.46, 72.72, 4.0
2019-02-16 02:56:13,959 : Image to text: 27.1, 62.5, 77.2, 3.0
2019-02-16 02:56:14,305 : Text to Image: 23.32, 56.14, 71.94, 4.0
2019-02-16 02:56:14,773 : Image to text: 31.4, 62.1, 75.5, 3.0
2019-02-16 02:56:15,113 : Text to Image: 23.84, 55.66, 72.52, 4.0
2019-02-16 02:56:15,113 : Dev mean Text to Image: 22.936, 55.864, 72.576, 4.2
2019-02-16 02:56:15,113 : Dev mean Image to text: 28.18, 61.4, 75.97999999999999, 3.4000000000000004
2019-02-16 02:56:15,114 : start epoch
2019-02-16 02:56:59,618 : samples : 64000
2019-02-16 02:57:10,354 : Image to text: 10.76, 30.22, 42.5, 15.0
2019-02-16 02:57:17,950 : Text to Image: 9.012, 26.552, 38.416, 18.0
2019-02-16 02:58:08,636 : samples : 128000
2019-02-16 02:58:20,366 : Image to text: 10.8, 30.28, 42.98, 14.0
2019-02-16 02:58:28,079 : Text to Image: 9.284, 26.972, 39.108, 17.0
2019-02-16 02:59:13,432 : samples : 192000
2019-02-16 02:59:24,146 : Image to text: 10.74, 30.84, 43.06, 15.0
2019-02-16 02:59:31,871 : Text to Image: 9.324, 26.86, 38.816, 18.0
2019-02-16 03:00:17,433 : samples : 256000
2019-02-16 03:00:28,179 : Image to text: 10.3, 28.92, 41.46, 15.0
2019-02-16 03:00:36,056 : Text to Image: 8.588, 25.392, 37.392, 19.0
2019-02-16 03:01:20,092 : samples : 320000
2019-02-16 03:01:30,762 : Image to text: 10.76, 29.14, 42.12, 15.0
2019-02-16 03:01:38,610 : Text to Image: 8.604, 25.656, 37.356, 19.0
2019-02-16 03:02:23,121 : samples : 384000
2019-02-16 03:02:33,815 : Image to text: 11.74, 31.1, 43.54, 14.0
2019-02-16 03:02:41,710 : Text to Image: 9.204, 26.596, 38.744, 18.0
2019-02-16 03:03:25,702 : samples : 448000
2019-02-16 03:03:36,343 : Image to text: 11.04, 30.34, 43.38, 14.0
2019-02-16 03:03:44,317 : Text to Image: 8.88, 25.912, 37.98, 18.0
2019-02-16 03:04:28,927 : samples : 512000
2019-02-16 03:04:39,578 : Image to text: 11.0, 30.64, 43.4, 14.0
2019-02-16 03:04:47,541 : Text to Image: 9.488, 26.784, 39.268, 17.0
2019-02-16 03:05:25,916 : Epoch 5 finished
2019-02-16 03:05:26,361 : Image to text: 28.3, 61.7, 76.6, 3.0
2019-02-16 03:05:26,702 : Text to Image: 22.32, 55.86, 73.86, 4.0
2019-02-16 03:05:27,162 : Image to text: 26.7, 60.6, 73.9, 4.0
2019-02-16 03:05:27,509 : Text to Image: 22.68, 55.8, 72.8, 4.0
2019-02-16 03:05:27,960 : Image to text: 28.2, 59.0, 74.1, 4.0
2019-02-16 03:05:28,313 : Text to Image: 24.12, 56.34, 72.84, 4.0
2019-02-16 03:05:28,780 : Image to text: 27.9, 63.1, 77.9, 3.0
2019-02-16 03:05:29,134 : Text to Image: 23.5, 56.46, 73.88, 4.0
2019-02-16 03:05:29,623 : Image to text: 29.1, 62.4, 76.1, 3.0
2019-02-16 03:05:29,971 : Text to Image: 23.18, 56.52, 73.08, 4.0
2019-02-16 03:05:29,971 : Dev mean Text to Image: 23.16, 56.196000000000005, 73.292, 4.0
2019-02-16 03:05:29,971 : Dev mean Image to text: 28.04, 61.360000000000014, 75.72, 3.4000000000000004
2019-02-16 03:05:29,972 : start epoch
2019-02-16 03:06:14,304 : samples : 64000
2019-02-16 03:06:24,982 : Image to text: 11.3, 31.16, 43.84, 14.0
2019-02-16 03:06:32,667 : Text to Image: 9.352, 27.088, 39.144, 18.0
2019-02-16 03:07:16,345 : samples : 128000
2019-02-16 03:07:27,063 : Image to text: 11.1, 30.96, 44.24, 13.0
2019-02-16 03:07:34,805 : Text to Image: 9.332, 27.0, 38.924, 17.0
2019-02-16 03:08:19,030 : samples : 192000
2019-02-16 03:08:29,695 : Image to text: 11.3, 31.06, 43.54, 14.0
2019-02-16 03:08:37,556 : Text to Image: 9.208, 26.516, 38.532, 18.0
2019-02-16 03:09:22,548 : samples : 256000
2019-02-16 03:09:33,081 : Image to text: 11.64, 31.88, 44.5, 13.0
2019-02-16 03:09:40,944 : Text to Image: 9.48, 27.108, 39.34, 17.0
2019-02-16 03:10:24,468 : samples : 320000
2019-02-16 03:10:34,871 : Image to text: 11.32, 31.1, 44.26, 14.0
2019-02-16 03:10:42,445 : Text to Image: 9.392, 26.788, 38.984, 18.0
2019-02-16 03:11:25,321 : samples : 384000
2019-02-16 03:11:35,712 : Image to text: 11.26, 30.86, 44.06, 14.0
2019-02-16 03:11:43,269 : Text to Image: 9.556, 27.6, 39.58, 17.0
2019-02-16 03:12:26,579 : samples : 448000
2019-02-16 03:12:36,930 : Image to text: 11.24, 30.82, 43.9, 14.0
2019-02-16 03:12:44,467 : Text to Image: 9.62, 27.496, 39.432, 17.0
2019-02-16 03:13:27,031 : samples : 512000
2019-02-16 03:13:37,352 : Image to text: 11.26, 31.34, 44.66, 13.0
2019-02-16 03:13:44,926 : Text to Image: 9.164, 27.04, 39.02, 17.0
2019-02-16 03:14:21,389 : Epoch 6 finished
2019-02-16 03:14:21,820 : Image to text: 28.1, 62.4, 78.3, 4.0
2019-02-16 03:14:22,141 : Text to Image: 23.36, 56.68, 74.14, 4.0
2019-02-16 03:14:22,559 : Image to text: 27.4, 59.8, 75.1, 4.0
2019-02-16 03:14:22,880 : Text to Image: 22.16, 56.32, 72.94, 4.0
2019-02-16 03:14:23,309 : Image to text: 26.2, 60.9, 76.0, 4.0
2019-02-16 03:14:23,630 : Text to Image: 24.22, 57.2, 73.4, 4.0
2019-02-16 03:14:24,049 : Image to text: 26.5, 61.8, 77.0, 4.0
2019-02-16 03:14:24,372 : Text to Image: 23.76, 56.9, 73.3, 4.0
2019-02-16 03:14:24,801 : Image to text: 31.4, 62.9, 76.8, 3.0
2019-02-16 03:14:25,121 : Text to Image: 23.52, 57.44, 72.24, 4.0
2019-02-16 03:14:25,121 : Dev mean Text to Image: 23.404, 56.908, 73.204, 4.0
2019-02-16 03:14:25,121 : Dev mean Image to text: 27.92, 61.559999999999995, 76.63999999999999, 3.8000000000000003
2019-02-16 03:14:25,121 : start epoch
2019-02-16 03:15:18,287 : samples : 64000
2019-02-16 03:15:28,837 : Image to text: 11.82, 31.24, 44.44, 13.0
2019-02-16 03:15:36,406 : Text to Image: 9.608, 27.544, 39.78, 17.0
2019-02-16 03:16:19,280 : samples : 128000
2019-02-16 03:16:29,736 : Image to text: 11.22, 31.24, 42.98, 14.0
2019-02-16 03:16:37,299 : Text to Image: 9.376, 26.892, 39.008, 17.0
2019-02-16 03:17:20,582 : samples : 192000
2019-02-16 03:17:30,860 : Image to text: 11.58, 31.9, 44.36, 14.0
2019-02-16 03:17:38,274 : Text to Image: 9.344, 27.448, 39.456, 17.0
2019-02-16 03:18:20,913 : samples : 256000
2019-02-16 03:18:31,229 : Image to text: 11.36, 31.44, 43.64, 14.0
2019-02-16 03:18:38,697 : Text to Image: 9.536, 28.132, 40.36, 17.0
2019-02-16 03:19:21,849 : samples : 320000
2019-02-16 03:19:32,150 : Image to text: 11.68, 30.7, 43.2, 14.0
2019-02-16 03:19:39,561 : Text to Image: 9.468, 27.292, 39.604, 17.0
2019-02-16 03:20:23,904 : samples : 384000
2019-02-16 03:20:34,202 : Image to text: 11.62, 31.14, 44.46, 13.0
2019-02-16 03:20:41,649 : Text to Image: 9.94, 27.924, 40.444, 16.0
2019-02-16 03:21:24,268 : samples : 448000
2019-02-16 03:21:34,492 : Image to text: 11.36, 32.26, 44.96, 13.0
2019-02-16 03:21:41,927 : Text to Image: 9.988, 28.232, 40.312, 16.0
2019-02-16 03:22:24,677 : samples : 512000
2019-02-16 03:22:34,897 : Image to text: 11.4, 31.08, 44.64, 14.0
2019-02-16 03:22:42,361 : Text to Image: 9.928, 27.996, 40.332, 16.0
2019-02-16 03:23:18,844 : Epoch 7 finished
2019-02-16 03:23:19,269 : Image to text: 28.2, 61.1, 77.1, 4.0
2019-02-16 03:23:19,595 : Text to Image: 22.68, 56.82, 74.54, 4.0
2019-02-16 03:23:20,020 : Image to text: 25.6, 57.8, 74.0, 4.0
2019-02-16 03:23:20,360 : Text to Image: 22.74, 55.74, 73.22, 4.0
2019-02-16 03:23:20,790 : Image to text: 27.4, 59.7, 76.0, 4.0
2019-02-16 03:23:21,122 : Text to Image: 23.76, 56.4, 73.54, 4.0
2019-02-16 03:23:21,569 : Image to text: 28.7, 63.4, 76.5, 3.0
2019-02-16 03:23:21,902 : Text to Image: 23.1, 56.76, 73.96, 4.0
2019-02-16 03:23:22,340 : Image to text: 28.4, 62.1, 74.8, 3.0
2019-02-16 03:23:22,675 : Text to Image: 23.12, 56.48, 72.52, 4.0
2019-02-16 03:23:22,675 : Dev mean Text to Image: 23.08, 56.440000000000005, 73.55600000000001, 4.0
2019-02-16 03:23:22,675 : Dev mean Image to text: 27.659999999999997, 60.82, 75.67999999999999, 3.6000000000000005
2019-02-16 03:23:22,675 : start epoch
2019-02-16 03:24:05,136 : samples : 64000
2019-02-16 03:24:15,644 : Image to text: 11.5, 32.52, 45.58, 13.0
2019-02-16 03:24:23,231 : Text to Image: 9.216, 26.976, 39.292, 17.0
2019-02-16 03:25:06,540 : samples : 128000
2019-02-16 03:25:17,135 : Image to text: 11.48, 31.44, 44.86, 14.0
2019-02-16 03:25:24,661 : Text to Image: 9.328, 26.992, 39.336, 17.0
2019-02-16 03:26:07,521 : samples : 192000
2019-02-16 03:26:17,858 : Image to text: 11.42, 31.72, 44.42, 13.0
2019-02-16 03:26:25,339 : Text to Image: 9.628, 27.564, 40.028, 17.0
2019-02-16 03:27:07,858 : samples : 256000
2019-02-16 03:27:18,042 : Image to text: 11.64, 31.46, 44.72, 13.0
2019-02-16 03:27:25,493 : Text to Image: 9.932, 28.004, 40.344, 16.0
2019-02-16 03:28:08,341 : samples : 320000
2019-02-16 03:28:18,589 : Image to text: 11.86, 32.54, 45.04, 13.0
2019-02-16 03:28:25,988 : Text to Image: 10.084, 28.356, 40.88, 16.0
2019-02-16 03:29:08,788 : samples : 384000
2019-02-16 03:29:19,043 : Image to text: 11.74, 31.4, 44.38, 14.0
2019-02-16 03:29:26,502 : Text to Image: 9.66, 27.728, 39.972, 17.0
2019-02-16 03:30:09,409 : samples : 448000
2019-02-16 03:30:19,661 : Image to text: 11.92, 32.82, 45.82, 13.0
2019-02-16 03:30:27,131 : Text to Image: 9.66, 28.048, 40.292, 16.0
2019-02-16 03:31:10,665 : samples : 512000
2019-02-16 03:31:20,605 : Image to text: 10.98, 31.68, 45.1, 13.0
2019-02-16 03:31:27,883 : Text to Image: 9.848, 27.864, 40.328, 16.0
2019-02-16 03:32:14,641 : Epoch 8 finished
2019-02-16 03:32:15,081 : Image to text: 29.5, 62.6, 79.2, 3.0
2019-02-16 03:32:15,425 : Text to Image: 23.66, 57.26, 74.24, 4.0
2019-02-16 03:32:15,877 : Image to text: 28.6, 62.7, 75.5, 3.0
2019-02-16 03:32:16,235 : Text to Image: 23.24, 55.84, 73.46, 4.0
2019-02-16 03:32:16,682 : Image to text: 28.4, 62.2, 78.2, 3.0
2019-02-16 03:32:17,013 : Text to Image: 24.18, 58.26, 74.8, 4.0
2019-02-16 03:32:17,460 : Image to text: 29.1, 65.6, 77.7, 3.0
2019-02-16 03:32:17,790 : Text to Image: 24.12, 57.94, 73.34, 4.0
2019-02-16 03:32:18,231 : Image to text: 30.8, 63.7, 76.2, 3.0
2019-02-16 03:32:18,559 : Text to Image: 24.16, 57.38, 73.64, 4.0
2019-02-16 03:32:18,559 : Dev mean Text to Image: 23.872, 57.336, 73.896, 4.0
2019-02-16 03:32:18,560 : Dev mean Image to text: 29.28, 63.36, 77.36, 3.0
2019-02-16 03:32:18,560 : start epoch
2019-02-16 03:33:01,112 : samples : 64000
2019-02-16 03:33:11,668 : Image to text: 12.56, 32.2, 45.42, 13.0
2019-02-16 03:33:19,226 : Text to Image: 9.772, 28.032, 40.184, 17.0
2019-02-16 03:34:01,986 : samples : 128000
2019-02-16 03:34:12,523 : Image to text: 11.8, 32.88, 46.02, 13.0
2019-02-16 03:34:19,974 : Text to Image: 9.552, 27.376, 39.384, 17.0
2019-02-16 03:35:04,566 : samples : 192000
2019-02-16 03:35:17,237 : Image to text: 11.86, 32.28, 46.02, 12.0
2019-02-16 03:35:27,328 : Text to Image: 10.156, 28.572, 41.084, 16.0
2019-02-16 03:36:12,246 : samples : 256000
2019-02-16 03:36:22,500 : Image to text: 11.5, 32.26, 44.8, 13.0
2019-02-16 03:36:29,889 : Text to Image: 9.724, 27.904, 40.192, 16.0
2019-02-16 03:37:12,349 : samples : 320000
2019-02-16 03:37:24,521 : Image to text: 11.62, 31.26, 44.44, 14.0
2019-02-16 03:37:34,538 : Text to Image: 9.74, 27.396, 39.804, 17.0
2019-02-16 03:38:19,814 : samples : 384000
2019-02-16 03:38:32,432 : Image to text: 11.92, 31.92, 43.98, 14.0
2019-02-16 03:38:42,480 : Text to Image: 9.748, 28.16, 40.372, 16.0
2019-02-16 03:39:24,217 : samples : 448000
2019-02-16 03:39:34,456 : Image to text: 12.06, 31.68, 44.54, 13.0
2019-02-16 03:39:41,713 : Text to Image: 9.824, 28.268, 40.28, 16.0
2019-02-16 03:40:24,671 : samples : 512000
2019-02-16 03:40:37,338 : Image to text: 11.0, 31.08, 45.08, 13.0
2019-02-16 03:40:47,384 : Text to Image: 10.048, 28.416, 41.112, 16.0
2019-02-16 03:41:26,354 : Epoch 9 finished
2019-02-16 03:41:27,271 : Image to text: 29.4, 64.1, 79.7, 3.0
2019-02-16 03:41:28,021 : Text to Image: 25.56, 59.02, 75.88, 4.0
2019-02-16 03:41:28,999 : Image to text: 27.9, 61.6, 76.2, 4.0
2019-02-16 03:41:29,802 : Text to Image: 23.92, 58.12, 75.08, 4.0
2019-02-16 03:41:30,758 : Image to text: 30.5, 61.9, 76.9, 3.0
2019-02-16 03:41:31,220 : Text to Image: 26.02, 59.86, 75.48, 4.0
2019-02-16 03:41:31,689 : Image to text: 29.6, 65.2, 79.1, 3.0
2019-02-16 03:41:32,067 : Text to Image: 25.6, 59.64, 76.24, 4.0
2019-02-16 03:41:32,533 : Image to text: 31.8, 64.8, 77.7, 3.0
2019-02-16 03:41:32,910 : Text to Image: 25.56, 58.72, 74.7, 4.0
2019-02-16 03:41:32,910 : Dev mean Text to Image: 25.332, 59.072, 75.476, 4.0
2019-02-16 03:41:32,910 : Dev mean Image to text: 29.840000000000003, 63.519999999999996, 77.92, 3.2
2019-02-16 03:41:32,910 : start epoch
2019-02-16 03:42:15,260 : samples : 64000
2019-02-16 03:42:27,041 : Image to text: 12.0, 32.3, 44.86, 14.0
2019-02-16 03:42:37,072 : Text to Image: 9.536, 27.456, 39.668, 17.0
2019-02-16 03:43:21,956 : samples : 128000
2019-02-16 03:43:34,578 : Image to text: 11.54, 32.24, 45.32, 13.0
2019-02-16 03:43:42,674 : Text to Image: 10.212, 28.544, 40.856, 16.0
2019-02-16 03:44:25,141 : samples : 192000
2019-02-16 03:44:35,554 : Image to text: 11.32, 31.76, 44.66, 13.0
2019-02-16 03:44:42,963 : Text to Image: 9.732, 27.684, 40.204, 16.0
2019-02-16 03:45:26,715 : samples : 256000
2019-02-16 03:45:39,314 : Image to text: 11.3, 32.38, 45.72, 13.0
2019-02-16 03:45:49,380 : Text to Image: 9.828, 28.12, 40.544, 16.0
2019-02-16 03:46:33,619 : samples : 320000
2019-02-16 03:46:43,846 : Image to text: 10.88, 31.02, 44.66, 14.0
2019-02-16 03:46:51,193 : Text to Image: 9.516, 27.74, 40.424, 16.0
2019-02-16 03:47:35,023 : samples : 384000
2019-02-16 03:47:47,658 : Image to text: 12.46, 32.78, 46.18, 13.0
2019-02-16 03:47:57,729 : Text to Image: 10.2, 29.076, 41.556, 16.0
2019-02-16 03:48:47,078 : samples : 448000
2019-02-16 03:48:58,870 : Image to text: 11.9, 31.98, 45.38, 13.0
2019-02-16 03:49:07,371 : Text to Image: 10.016, 28.476, 41.16, 16.0
2019-02-16 03:49:51,688 : samples : 512000
2019-02-16 03:50:04,293 : Image to text: 12.36, 33.14, 46.06, 13.0
2019-02-16 03:50:14,347 : Text to Image: 10.2, 28.752, 41.2, 16.0
2019-02-16 03:50:52,001 : Epoch 10 finished
2019-02-16 03:50:52,469 : Image to text: 30.8, 64.2, 80.6, 3.0
2019-02-16 03:50:52,832 : Text to Image: 25.24, 59.56, 76.4, 4.0
2019-02-16 03:50:53,283 : Image to text: 29.3, 60.3, 76.6, 4.0
2019-02-16 03:50:53,649 : Text to Image: 24.32, 57.94, 75.2, 4.0
2019-02-16 03:50:54,110 : Image to text: 30.6, 61.5, 78.0, 3.0
2019-02-16 03:50:54,485 : Text to Image: 25.56, 59.78, 75.8, 4.0
2019-02-16 03:50:54,948 : Image to text: 29.1, 64.5, 78.8, 3.0
2019-02-16 03:50:55,328 : Text to Image: 25.46, 59.5, 75.24, 4.0
2019-02-16 03:50:55,810 : Image to text: 30.9, 64.1, 77.6, 3.0
2019-02-16 03:50:56,172 : Text to Image: 25.26, 59.12, 75.28, 4.0
2019-02-16 03:50:56,172 : Dev mean Text to Image: 25.168, 59.18, 75.584, 4.0
2019-02-16 03:50:56,172 : Dev mean Image to text: 30.14, 62.92, 78.32, 3.2
2019-02-16 03:50:56,172 : start epoch
2019-02-16 03:51:39,314 : samples : 64000
2019-02-16 03:51:51,906 : Image to text: 12.18, 32.02, 45.62, 13.0
2019-02-16 03:52:01,896 : Text to Image: 10.14, 28.488, 40.924, 16.0
2019-02-16 03:52:47,447 : samples : 128000
2019-02-16 03:52:59,640 : Image to text: 11.84, 32.54, 45.34, 13.0
2019-02-16 03:53:07,104 : Text to Image: 9.9, 28.568, 40.94, 16.0
2019-02-16 03:53:49,670 : samples : 192000
2019-02-16 03:53:59,903 : Image to text: 11.98, 32.72, 45.56, 13.0
2019-02-16 03:54:07,052 : Text to Image: 10.192, 28.452, 41.056, 16.0
2019-02-16 03:54:51,544 : samples : 256000
2019-02-16 03:55:04,173 : Image to text: 11.54, 32.0, 44.9, 13.0
2019-02-16 03:55:14,250 : Text to Image: 9.844, 28.288, 40.7, 16.0
2019-02-16 03:55:58,835 : samples : 320000
2019-02-16 03:56:09,120 : Image to text: 11.48, 32.1, 45.7, 13.0
2019-02-16 03:56:16,560 : Text to Image: 10.048, 28.892, 41.24, 16.0
2019-02-16 03:56:59,463 : samples : 384000
2019-02-16 03:57:12,059 : Image to text: 12.5, 32.46, 46.18, 12.0
2019-02-16 03:57:22,105 : Text to Image: 10.188, 29.004, 41.264, 16.0
2019-02-16 03:58:07,749 : samples : 448000
2019-02-16 03:58:20,386 : Image to text: 12.42, 32.78, 45.38, 13.0
2019-02-16 03:58:28,004 : Text to Image: 10.172, 28.928, 41.052, 16.0
2019-02-16 03:59:10,258 : samples : 512000
2019-02-16 03:59:21,434 : Image to text: 11.72, 32.14, 45.84, 13.0
2019-02-16 03:59:29,060 : Text to Image: 10.184, 28.3, 40.88, 16.0
2019-02-16 04:00:06,798 : Epoch 11 finished
2019-02-16 04:00:07,318 : Image to text: 29.0, 63.4, 79.7, 3.0
2019-02-16 04:00:07,695 : Text to Image: 24.86, 58.2, 75.46, 4.0
2019-02-16 04:00:08,145 : Image to text: 29.4, 60.9, 76.2, 3.0
2019-02-16 04:00:08,505 : Text to Image: 23.48, 56.52, 73.94, 4.0
2019-02-16 04:00:08,955 : Image to text: 28.5, 61.4, 76.8, 4.0
2019-02-16 04:00:09,325 : Text to Image: 24.62, 58.58, 75.56, 4.0
2019-02-16 04:00:09,786 : Image to text: 31.2, 63.3, 78.5, 3.0
2019-02-16 04:00:10,160 : Text to Image: 24.58, 58.78, 75.02, 4.0
2019-02-16 04:00:10,620 : Image to text: 29.4, 62.4, 77.3, 3.0
2019-02-16 04:00:10,992 : Text to Image: 25.16, 58.34, 74.52, 4.0
2019-02-16 04:00:10,992 : Dev mean Text to Image: 24.54, 58.084, 74.89999999999999, 4.0
2019-02-16 04:00:10,992 : Dev mean Image to text: 29.499999999999996, 62.28, 77.69999999999999, 3.2
2019-02-16 04:00:10,992 : start epoch
2019-02-16 04:00:53,572 : samples : 64000
2019-02-16 04:01:03,822 : Image to text: 11.72, 32.54, 45.68, 13.0
2019-02-16 04:01:11,201 : Text to Image: 10.26, 28.512, 41.272, 16.0
2019-02-16 04:01:53,961 : samples : 128000
2019-02-16 04:02:04,287 : Image to text: 11.3, 32.54, 45.66, 13.0
2019-02-16 04:02:11,661 : Text to Image: 9.788, 28.08, 40.2, 16.0
2019-02-16 04:02:54,156 : samples : 192000
2019-02-16 04:03:04,403 : Image to text: 11.86, 33.46, 45.9, 12.0
2019-02-16 04:03:11,841 : Text to Image: 10.384, 29.016, 41.388, 15.0
2019-02-16 04:03:54,338 : samples : 256000
2019-02-16 04:04:07,079 : Image to text: 12.58, 33.16, 46.28, 13.0
2019-02-16 04:04:14,941 : Text to Image: 10.316, 28.956, 41.404, 15.0
2019-02-16 04:04:57,767 : samples : 320000
2019-02-16 04:05:07,847 : Image to text: 12.44, 32.58, 45.4, 13.0
2019-02-16 04:05:15,065 : Text to Image: 10.224, 28.74, 40.916, 16.0
2019-02-16 04:06:07,792 : samples : 384000
2019-02-16 04:06:20,475 : Image to text: 12.2, 33.2, 45.94, 13.0
2019-02-16 04:06:27,816 : Text to Image: 10.18, 28.648, 41.052, 16.0
2019-02-16 04:07:11,385 : samples : 448000
2019-02-16 04:07:22,245 : Image to text: 12.02, 32.66, 45.74, 13.0
2019-02-16 04:07:29,639 : Text to Image: 10.428, 28.996, 41.336, 16.0
2019-02-16 04:08:13,432 : samples : 512000
2019-02-16 04:08:24,320 : Image to text: 12.02, 32.16, 45.82, 13.0
2019-02-16 04:08:34,400 : Text to Image: 10.336, 29.188, 41.428, 15.0
2019-02-16 04:09:10,743 : Epoch 12 finished
2019-02-16 04:09:11,711 : Image to text: 30.8, 64.2, 78.2, 3.0
2019-02-16 04:09:12,501 : Text to Image: 24.86, 58.44, 75.5, 4.0
2019-02-16 04:09:13,595 : Image to text: 28.5, 62.8, 78.3, 3.0
2019-02-16 04:09:14,501 : Text to Image: 23.76, 58.06, 75.24, 4.0
2019-02-16 04:09:15,433 : Image to text: 29.6, 61.9, 78.6, 3.0
2019-02-16 04:09:16,314 : Text to Image: 25.14, 59.6, 75.24, 4.0
2019-02-16 04:09:17,363 : Image to text: 29.7, 63.4, 78.2, 3.0
2019-02-16 04:09:18,211 : Text to Image: 24.54, 59.06, 76.0, 4.0
2019-02-16 04:09:19,169 : Image to text: 31.0, 64.6, 77.7, 3.0
2019-02-16 04:09:19,539 : Text to Image: 25.22, 59.48, 75.1, 4.0
2019-02-16 04:09:19,539 : Dev mean Text to Image: 24.704, 58.928, 75.416, 4.0
2019-02-16 04:09:19,539 : Dev mean Image to text: 29.919999999999998, 63.379999999999995, 78.2, 3.0
2019-02-16 04:09:19,539 : start epoch
2019-02-16 04:10:02,384 : samples : 64000
2019-02-16 04:10:14,940 : Image to text: 11.96, 32.88, 46.22, 12.0
2019-02-16 04:10:24,897 : Text to Image: 10.332, 29.12, 41.612, 15.0
2019-02-16 04:11:09,995 : samples : 128000
2019-02-16 04:11:22,599 : Image to text: 11.86, 32.78, 46.16, 13.0
2019-02-16 04:11:32,628 : Text to Image: 10.052, 28.352, 40.54, 16.0
2019-02-16 04:12:17,737 : samples : 192000
2019-02-16 04:12:30,283 : Image to text: 12.28, 33.28, 46.26, 13.0
2019-02-16 04:12:40,303 : Text to Image: 10.168, 28.956, 41.56, 15.0
2019-02-16 04:13:25,259 : samples : 256000
2019-02-16 04:13:37,852 : Image to text: 12.5, 33.68, 45.92, 13.0
2019-02-16 04:13:47,783 : Text to Image: 10.432, 29.54, 41.836, 15.0
2019-02-16 04:14:32,490 : samples : 320000
2019-02-16 04:14:45,117 : Image to text: 11.62, 32.34, 45.56, 13.0
2019-02-16 04:14:55,119 : Text to Image: 9.984, 28.52, 40.904, 16.0
2019-02-16 04:15:40,051 : samples : 384000
2019-02-16 04:15:52,654 : Image to text: 12.24, 32.5, 45.16, 13.0
2019-02-16 04:16:02,685 : Text to Image: 10.228, 28.56, 40.96, 16.0
2019-02-16 04:16:47,981 : samples : 448000
2019-02-16 04:17:00,548 : Image to text: 12.48, 33.54, 46.42, 12.0
2019-02-16 04:17:10,585 : Text to Image: 10.448, 29.164, 41.808, 15.0
2019-02-16 04:17:55,535 : samples : 512000
2019-02-16 04:18:08,183 : Image to text: 12.46, 32.98, 45.94, 13.0
2019-02-16 04:18:18,220 : Text to Image: 10.208, 28.584, 41.124, 16.0
2019-02-16 04:18:56,137 : Epoch 13 finished
2019-02-16 04:18:57,072 : Image to text: 30.5, 65.0, 79.8, 3.0
2019-02-16 04:18:57,825 : Text to Image: 25.78, 59.92, 76.12, 4.0
2019-02-16 04:18:58,760 : Image to text: 30.1, 60.9, 78.3, 4.0
2019-02-16 04:18:59,543 : Text to Image: 24.62, 58.82, 75.44, 4.0
2019-02-16 04:19:00,462 : Image to text: 29.3, 62.8, 78.3, 3.0
2019-02-16 04:19:01,228 : Text to Image: 26.12, 59.64, 75.68, 4.0
2019-02-16 04:19:02,204 : Image to text: 31.3, 65.6, 79.6, 3.0
2019-02-16 04:19:02,977 : Text to Image: 25.48, 59.66, 76.1, 4.0
2019-02-16 04:19:03,914 : Image to text: 32.0, 63.2, 77.5, 3.0
2019-02-16 04:19:04,660 : Text to Image: 25.98, 59.8, 75.6, 4.0
2019-02-16 04:19:04,661 : Dev mean Text to Image: 25.596000000000004, 59.568000000000005, 75.788, 4.0
2019-02-16 04:19:04,661 : Dev mean Image to text: 30.64, 63.49999999999999, 78.7, 3.2
2019-02-16 04:19:04,661 : start epoch
2019-02-16 04:19:48,714 : samples : 64000
2019-02-16 04:20:01,329 : Image to text: 12.3, 32.9, 45.68, 13.0
2019-02-16 04:20:11,381 : Text to Image: 10.2, 28.78, 41.712, 15.0
2019-02-16 04:20:56,557 : samples : 128000
2019-02-16 04:21:08,653 : Image to text: 12.22, 32.9, 46.06, 13.0
2019-02-16 04:21:18,858 : Text to Image: 10.28, 28.808, 41.328, 16.0
2019-02-16 04:22:01,380 : samples : 192000
2019-02-16 04:22:11,632 : Image to text: 11.96, 32.92, 46.7, 12.0
2019-02-16 04:22:20,114 : Text to Image: 10.344, 29.052, 41.272, 16.0
2019-02-16 04:23:11,286 : samples : 256000
2019-02-16 04:23:23,655 : Image to text: 12.4, 33.2, 46.5, 12.0
2019-02-16 04:23:34,109 : Text to Image: 10.212, 28.724, 41.5, 16.0
2019-02-16 04:24:19,290 : samples : 320000
2019-02-16 04:24:32,171 : Image to text: 12.18, 33.28, 46.84, 12.0
2019-02-16 04:24:42,648 : Text to Image: 10.552, 29.424, 41.6, 15.0
2019-02-16 04:25:28,385 : samples : 384000
2019-02-16 04:25:41,326 : Image to text: 11.76, 33.28, 45.88, 13.0
2019-02-16 04:25:51,792 : Text to Image: 10.22, 28.784, 41.204, 16.0
2019-02-16 04:26:37,369 : samples : 448000
2019-02-16 04:26:50,258 : Image to text: 11.72, 33.56, 46.58, 12.0
2019-02-16 04:27:00,739 : Text to Image: 10.268, 28.648, 40.744, 16.0
2019-02-16 04:27:46,236 : samples : 512000
2019-02-16 04:27:59,126 : Image to text: 11.68, 32.74, 46.28, 13.0
2019-02-16 04:28:09,573 : Text to Image: 10.172, 28.916, 41.372, 16.0
2019-02-16 04:28:48,805 : Epoch 14 finished
2019-02-16 04:28:49,862 : Image to text: 31.1, 65.5, 80.2, 3.0
2019-02-16 04:28:50,821 : Text to Image: 25.64, 60.12, 76.76, 4.0
2019-02-16 04:28:51,889 : Image to text: 29.3, 62.1, 76.8, 3.0
2019-02-16 04:28:52,718 : Text to Image: 25.02, 58.7, 75.8, 4.0
2019-02-16 04:28:53,807 : Image to text: 28.4, 64.9, 80.5, 3.0
2019-02-16 04:28:54,643 : Text to Image: 26.3, 60.46, 76.56, 4.0
2019-02-16 04:28:55,654 : Image to text: 30.2, 66.4, 79.3, 3.0
2019-02-16 04:28:56,514 : Text to Image: 26.5, 60.56, 76.5, 4.0
2019-02-16 04:28:57,540 : Image to text: 31.5, 64.3, 78.4, 3.0
2019-02-16 04:28:58,427 : Text to Image: 26.6, 59.96, 75.44, 4.0
2019-02-16 04:28:58,428 : Dev mean Text to Image: 26.012, 59.96000000000001, 76.21199999999999, 4.0
2019-02-16 04:28:58,428 : Dev mean Image to text: 30.1, 64.64, 79.04, 3.0
2019-02-16 04:28:58,428 : start epoch
2019-02-16 04:29:43,897 : samples : 64000
2019-02-16 04:29:56,783 : Image to text: 12.24, 33.68, 46.76, 12.0
2019-02-16 04:30:07,274 : Text to Image: 10.392, 28.992, 41.452, 15.0
2019-02-16 04:30:53,323 : samples : 128000
2019-02-16 04:31:06,114 : Image to text: 12.04, 33.44, 47.24, 12.0
2019-02-16 04:31:16,622 : Text to Image: 10.48, 29.164, 41.684, 15.0
2019-02-16 04:31:59,617 : samples : 192000
2019-02-16 04:32:09,941 : Image to text: 12.24, 33.4, 46.3, 12.0
2019-02-16 04:32:17,363 : Text to Image: 10.176, 28.5, 41.248, 16.0
2019-02-16 04:33:00,748 : samples : 256000
2019-02-16 04:33:11,081 : Image to text: 12.96, 33.84, 46.44, 12.0
2019-02-16 04:33:18,557 : Text to Image: 10.364, 29.388, 41.956, 15.0
2019-02-16 04:34:01,811 : samples : 320000
2019-02-16 04:34:12,110 : Image to text: 11.74, 32.6, 45.9, 13.0
2019-02-16 04:34:19,569 : Text to Image: 10.112, 28.824, 41.568, 16.0
2019-02-16 04:35:02,604 : samples : 384000
2019-02-16 04:35:12,995 : Image to text: 11.8, 33.14, 45.82, 13.0
2019-02-16 04:35:20,469 : Text to Image: 10.3, 28.94, 41.62, 15.0
2019-02-16 04:36:03,422 : samples : 448000
2019-02-16 04:36:13,687 : Image to text: 12.32, 32.68, 45.72, 13.0
2019-02-16 04:36:21,163 : Text to Image: 10.708, 29.812, 42.044, 15.0
2019-02-16 04:37:03,735 : samples : 512000
2019-02-16 04:37:14,050 : Image to text: 13.04, 33.44, 46.56, 12.0
2019-02-16 04:37:21,456 : Text to Image: 10.512, 29.244, 41.588, 15.0
2019-02-16 04:37:58,350 : Epoch 15 finished
2019-02-16 04:37:58,791 : Image to text: 31.1, 65.1, 81.4, 3.0
2019-02-16 04:37:59,123 : Text to Image: 25.54, 59.88, 76.62, 4.0
2019-02-16 04:37:59,564 : Image to text: 30.1, 64.0, 78.2, 3.0
2019-02-16 04:37:59,898 : Text to Image: 24.9, 58.48, 75.14, 4.0
2019-02-16 04:38:00,355 : Image to text: 28.6, 62.5, 78.5, 3.0
2019-02-16 04:38:00,693 : Text to Image: 25.68, 60.48, 76.02, 4.0
2019-02-16 04:38:01,149 : Image to text: 29.2, 64.5, 79.6, 3.0
2019-02-16 04:38:01,491 : Text to Image: 25.56, 59.4, 75.6, 4.0
2019-02-16 04:38:01,945 : Image to text: 32.7, 65.6, 77.8, 3.0
2019-02-16 04:38:02,287 : Text to Image: 25.5, 59.62, 74.94, 4.0
2019-02-16 04:38:02,287 : Dev mean Text to Image: 25.436, 59.571999999999996, 75.664, 4.0
2019-02-16 04:38:02,287 : Dev mean Image to text: 30.340000000000003, 64.34, 79.10000000000001, 3.0
2019-02-16 04:38:06,239 : 
Test scores | Image to text:             30.96, 64.5, 79.24, 3.0
2019-02-16 04:38:06,239 : Test scores | Text to image:             25.488, 59.36399999999999, 75.324, 4.0

2019-02-16 04:38:06,350 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 04:38:06,712 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 04:38:07,391 : loading BERT model bert-base-uncased
2019-02-16 04:38:07,391 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:38:07,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:38:07,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeez79kln
2019-02-16 04:38:09,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:38:11,406 : Computing embeddings for train/dev/test
2019-02-16 04:39:52,330 : Computed embeddings
2019-02-16 04:39:52,331 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:40:32,934 : [('reg:1e-05', 68.98), ('reg:0.0001', 67.34), ('reg:0.001', 60.38), ('reg:0.01', 67.69)]
2019-02-16 04:40:32,934 : Validation : best param found is reg = 1e-05 with score             68.98
2019-02-16 04:40:32,934 : Evaluating...
2019-02-16 04:40:45,602 : 
Dev acc : 69.0 Test acc : 67.7 for LENGTH classification

2019-02-16 04:40:45,602 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 04:40:45,967 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 04:40:46,016 : loading BERT model bert-base-uncased
2019-02-16 04:40:46,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:40:46,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:40:46,052 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk50h3gx4
2019-02-16 04:40:48,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:40:50,036 : Computing embeddings for train/dev/test
2019-02-16 04:42:19,647 : Computed embeddings
2019-02-16 04:42:19,647 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:43:26,457 : [('reg:1e-05', 31.55), ('reg:0.0001', 7.57), ('reg:0.001', 0.35), ('reg:0.01', 0.16)]
2019-02-16 04:43:26,457 : Validation : best param found is reg = 1e-05 with score             31.55
2019-02-16 04:43:26,457 : Evaluating...
2019-02-16 04:43:52,309 : 
Dev acc : 31.6 Test acc : 32.5 for WORDCONTENT classification

2019-02-16 04:43:52,310 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 04:43:52,715 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 04:43:52,793 : loading BERT model bert-base-uncased
2019-02-16 04:43:52,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:43:52,825 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:43:52,825 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqiyfaw4_
2019-02-16 04:43:55,342 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:43:56,810 : Computing embeddings for train/dev/test
2019-02-16 04:45:20,408 : Computed embeddings
2019-02-16 04:45:20,408 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:46:04,752 : [('reg:1e-05', 30.73), ('reg:0.0001', 32.37), ('reg:0.001', 27.3), ('reg:0.01', 27.4)]
2019-02-16 04:46:04,753 : Validation : best param found is reg = 0.0001 with score             32.37
2019-02-16 04:46:04,753 : Evaluating...
2019-02-16 04:46:16,457 : 
Dev acc : 32.4 Test acc : 32.2 for DEPTH classification

2019-02-16 04:46:16,458 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 04:46:16,883 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 04:46:16,952 : loading BERT model bert-base-uncased
2019-02-16 04:46:16,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:46:16,981 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:46:16,981 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpklbf4wo3
2019-02-16 04:46:19,481 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:46:21,008 : Computing embeddings for train/dev/test
2019-02-16 04:47:40,200 : Computed embeddings
2019-02-16 04:47:40,200 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:48:27,245 : [('reg:1e-05', 54.32), ('reg:0.0001', 51.41), ('reg:0.001', 51.41), ('reg:0.01', 45.3)]
2019-02-16 04:48:27,245 : Validation : best param found is reg = 1e-05 with score             54.32
2019-02-16 04:48:27,246 : Evaluating...
2019-02-16 04:48:37,659 : 
Dev acc : 54.3 Test acc : 53.9 for TOPCONSTITUENTS classification

2019-02-16 04:48:37,660 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 04:48:38,040 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 04:48:38,113 : loading BERT model bert-base-uncased
2019-02-16 04:48:38,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:48:38,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:48:38,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_5yov0yb
2019-02-16 04:48:40,721 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:48:42,199 : Computing embeddings for train/dev/test
2019-02-16 04:50:05,064 : Computed embeddings
2019-02-16 04:50:05,064 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:51:03,781 : [('reg:1e-05', 80.61), ('reg:0.0001', 80.76), ('reg:0.001', 80.54), ('reg:0.01', 80.17)]
2019-02-16 04:51:03,781 : Validation : best param found is reg = 0.0001 with score             80.76
2019-02-16 04:51:03,781 : Evaluating...
2019-02-16 04:51:21,638 : 
Dev acc : 80.8 Test acc : 80.0 for BIGRAMSHIFT classification

2019-02-16 04:51:21,640 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 04:51:22,049 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 04:51:22,117 : loading BERT model bert-base-uncased
2019-02-16 04:51:22,118 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:51:22,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:51:22,237 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphzlq9p93
2019-02-16 04:51:24,752 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:51:26,238 : Computing embeddings for train/dev/test
2019-02-16 04:52:48,907 : Computed embeddings
2019-02-16 04:52:48,907 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:53:38,208 : [('reg:1e-05', 85.44), ('reg:0.0001', 85.13), ('reg:0.001', 85.66), ('reg:0.01', 85.37)]
2019-02-16 04:53:38,208 : Validation : best param found is reg = 0.001 with score             85.66
2019-02-16 04:53:38,208 : Evaluating...
2019-02-16 04:53:49,676 : 
Dev acc : 85.7 Test acc : 84.5 for TENSE classification

2019-02-16 04:53:49,677 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 04:53:50,291 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 04:53:50,355 : loading BERT model bert-base-uncased
2019-02-16 04:53:50,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:53:50,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:53:50,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzacjs9e9
2019-02-16 04:53:52,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:53:54,415 : Computing embeddings for train/dev/test
2019-02-16 04:55:21,474 : Computed embeddings
2019-02-16 04:55:21,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:56:09,569 : [('reg:1e-05', 76.99), ('reg:0.0001', 76.94), ('reg:0.001', 76.64), ('reg:0.01', 77.54)]
2019-02-16 04:56:09,569 : Validation : best param found is reg = 0.01 with score             77.54
2019-02-16 04:56:09,569 : Evaluating...
2019-02-16 04:56:18,553 : 
Dev acc : 77.5 Test acc : 76.9 for SUBJNUMBER classification

2019-02-16 04:56:18,555 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 04:56:19,248 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 04:56:19,355 : loading BERT model bert-base-uncased
2019-02-16 04:56:19,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:56:19,410 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:56:19,410 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp43tnuo4t
2019-02-16 04:56:22,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:56:24,840 : Computing embeddings for train/dev/test
2019-02-16 04:57:53,083 : Computed embeddings
2019-02-16 04:57:53,083 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:59:06,690 : [('reg:1e-05', 77.24), ('reg:0.0001', 77.28), ('reg:0.001', 77.96), ('reg:0.01', 73.03)]
2019-02-16 04:59:06,691 : Validation : best param found is reg = 0.001 with score             77.96
2019-02-16 04:59:06,691 : Evaluating...
2019-02-16 04:59:24,137 : 
Dev acc : 78.0 Test acc : 78.3 for OBJNUMBER classification

2019-02-16 04:59:24,138 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 04:59:24,608 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 04:59:24,686 : loading BERT model bert-base-uncased
2019-02-16 04:59:24,686 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:59:24,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:59:24,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgxuaelrf
2019-02-16 04:59:27,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:59:28,688 : Computing embeddings for train/dev/test
2019-02-16 05:01:07,983 : Computed embeddings
2019-02-16 05:01:07,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:01:50,277 : [('reg:1e-05', 54.54), ('reg:0.0001', 54.56), ('reg:0.001', 54.65), ('reg:0.01', 54.5)]
2019-02-16 05:01:50,277 : Validation : best param found is reg = 0.001 with score             54.65
2019-02-16 05:01:50,278 : Evaluating...
2019-02-16 05:02:00,278 : 
Dev acc : 54.6 Test acc : 54.7 for ODDMANOUT classification

2019-02-16 05:02:00,279 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 05:02:00,693 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 05:02:00,775 : loading BERT model bert-base-uncased
2019-02-16 05:02:00,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:02:00,913 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:02:00,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpttt6stth
2019-02-16 05:02:03,419 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:02:04,876 : Computing embeddings for train/dev/test
2019-02-16 05:03:41,541 : Computed embeddings
2019-02-16 05:03:41,542 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:04:25,716 : [('reg:1e-05', 50.31), ('reg:0.0001', 50.31), ('reg:0.001', 50.23), ('reg:0.01', 50.0)]
2019-02-16 05:04:25,716 : Validation : best param found is reg = 1e-05 with score             50.31
2019-02-16 05:04:25,716 : Evaluating...
2019-02-16 05:04:39,133 : 
Dev acc : 50.3 Test acc : 50.4 for COORDINATIONINVERSION classification

2019-02-16 05:04:39,135 : total results: {'STS12': {'MSRpar': {'pearson': (0.3122166820186254, 2.0238226623146045e-18), 'spearman': SpearmanrResult(correlation=0.34822678766034754, pvalue=8.362187684453667e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6634805037477882, 2.5394182192443723e-96), 'spearman': SpearmanrResult(correlation=0.6740925700318483, pvalue=1.683477177106444e-100), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5058453097875151, 3.4316831861868068e-31), 'spearman': SpearmanrResult(correlation=0.6123676401491357, pvalue=1.388946941942825e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5651728410543488, 1.6149931781347085e-64), 'spearman': SpearmanrResult(correlation=0.5959767288109727, pvalue=2.6090811714756803e-73), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5154976526897959, 1.8135985642270956e-28), 'spearman': SpearmanrResult(correlation=0.4413779210959883, pvalue=1.8768122245103063e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5124425978596147, 'wmean': 0.5127152769405631}, 'spearman': {'mean': 0.5344083295496584, 'wmean': 0.5376153803806721}}}, 'STS13': {'FNWN': {'pearson': (0.15988281869751747, 0.02797760807566723), 'spearman': SpearmanrResult(correlation=0.16114595854290978, pvalue=0.02674690588898557), 'nsamples': 189}, 'headlines': {'pearson': (0.6460458682312775, 8.11394298301438e-90), 'spearman': SpearmanrResult(correlation=0.6405527343152857, pvalue=7.436229646980297e-88), 'nsamples': 750}, 'OnWN': {'pearson': (0.5458352928735449, 7.102851545873179e-45), 'spearman': SpearmanrResult(correlation=0.5455861681609885, pvalue=7.918273546071402e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4505879932674466, 'wmean': 0.5473105688062317}, 'spearman': {'mean': 0.44909495367306135, 'wmean': 0.5446299848262592}}}, 'STS14': {'deft-forum': {'pearson': (0.3898134305088139, 8.827991067139376e-18), 'spearman': SpearmanrResult(correlation=0.4053232422083518, pvalue=3.193671208542357e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.7203682996529029, 2.8480558859252036e-49), 'spearman': SpearmanrResult(correlation=0.6877514531424567, pvalue=2.3711898243761078e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.6031202936536042, 1.7345420665940543e-75), 'spearman': SpearmanrResult(correlation=0.5777406582022563, pvalue=5.419129335295342e-68), 'nsamples': 750}, 'images': {'pearson': (0.5947537491015102, 6.078294742748783e-73), 'spearman': SpearmanrResult(correlation=0.593220474481269, pvalue=1.7460016956454181e-72), 'nsamples': 750}, 'OnWN': {'pearson': (0.6362910606654603, 2.323681603474666e-86), 'spearman': SpearmanrResult(correlation=0.6681263929618395, pvalue=3.950887143982658e-98), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5655836193557078, 1.2501592945608691e-64), 'spearman': SpearmanrResult(correlation=0.5426678739059951, pvalue=1.1735026709460315e-58), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5849884088229999, 'wmean': 0.5843568201885464}, 'spearman': {'mean': 0.5791383491503614, 'wmean': 0.5800099852266707}}}, 'STS15': {'answers-forums': {'pearson': (0.5055628783243161, 1.0029242159781212e-25), 'spearman': SpearmanrResult(correlation=0.4757557206135593, pvalue=1.4085511798381865e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.6827747374948985, 4.733345776252249e-104), 'spearman': SpearmanrResult(correlation=0.6882034536503452, pvalue=2.4616242886889924e-106), 'nsamples': 750}, 'belief': {'pearson': (0.5273635517176553, 3.128346655719293e-28), 'spearman': SpearmanrResult(correlation=0.5689677859292421, pvalue=1.5010688684839677e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6426974847837373, 1.2883963035817603e-88), 'spearman': SpearmanrResult(correlation=0.6495925707135694, pvalue=4.1755382892681335e-91), 'nsamples': 750}, 'images': {'pearson': (0.7302124630732332, 7.116439870351464e-126), 'spearman': SpearmanrResult(correlation=0.7399826647120971, pvalue=5.88582670203036e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6177222230787681, 'wmean': 0.6430369750932137}, 'spearman': {'mean': 0.6245004391237625, 'wmean': 0.650035110586853}}}, 'STS16': {'answer-answer': {'pearson': (0.4276446413957827, 1.024268305111074e-12), 'spearman': SpearmanrResult(correlation=0.4391795305931522, pvalue=2.1162913606095836e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.6431868144792594, 1.8120267649410172e-30), 'spearman': SpearmanrResult(correlation=0.6526859005943635, pvalue=1.299724413444842e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6692523896012285, 3.053116324524378e-31), 'spearman': SpearmanrResult(correlation=0.6791628713184334, pvalue=1.8423526084946455e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.762109653940884, 1.373632377080932e-47), 'spearman': SpearmanrResult(correlation=0.8150003766996614, pvalue=2.8137526921894145e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.4029055695000011, 1.4655067700632883e-09), 'spearman': SpearmanrResult(correlation=0.41734693468607326, pvalue=3.2456599578159416e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5810198137834311, 'wmean': 0.5842034780060901}, 'spearman': {'mean': 0.6006751227783368, 'wmean': 0.60401640108432}}}, 'MR': {'devacc': 58.48, 'acc': 58.67, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.39, 'acc': 66.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.5, 'acc': 85.2, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 87.92, 'acc': 89.93, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.95, 'acc': 76.22, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.33, 'acc': 30.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.27, 'acc': 80.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.64, 'acc': 72.93, 'f1': 81.65, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 76.74, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.783497781215318, 'pearson': 0.7914938097001635, 'spearman': 0.730879495418651, 'mse': 0.3827408204461735, 'yhat': array([3.17077788, 4.14392142, 1.74857454, ..., 3.21604462, 4.31403944,        4.87393995]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6325067065473199, 'pearson': 0.6318460324632185, 'spearman': 0.6285202863684332, 'mse': 1.465290329959281, 'yhat': array([1.16553882, 1.52775248, 2.11774838, ..., 3.81075196, 4.92069276,        4.0493054 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.56, 'acc': 63.04, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 335.96400000000006, 'acc': [(30.96, 64.5, 79.24, 3.0), (25.488, 59.36399999999999, 75.324, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 68.98, 'acc': 67.73, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 31.55, 'acc': 32.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.37, 'acc': 32.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.32, 'acc': 53.86, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 80.76, 'acc': 80.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.66, 'acc': 84.54, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.54, 'acc': 76.94, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.96, 'acc': 78.27, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.65, 'acc': 54.74, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.31, 'acc': 50.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 05:04:39,135 : STS12 p=0.5127, STS12 s=0.5376, STS13 p=0.5473, STS13 s=0.5446, STS14 p=0.5844, STS14 s=0.5800, STS15 p=0.6430, STS15 s=0.6500, STS 16 p=0.5842, STS16 s=0.6040, STS B p=0.6318, STS B s=0.6285, STS B m=1.4653, SICK-R p=0.7915, SICK-R s=0.7309, SICK-P m=0.3827
2019-02-16 05:04:39,135 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 05:04:39,135 : 0.5127,0.5376,0.5473,0.5446,0.5844,0.5800,0.6430,0.6500,0.5842,0.6040,0.6318,0.6285,1.4653,0.7915,0.7309,0.3827
2019-02-16 05:04:39,135 : MR=58.67, CR=66.38, SUBJ=89.93, MPQA=85.20, SST-B=76.22, SST-F=30.27, TREC=80.20, SICK-E=76.74, SNLI=63.04, MRPC=72.93, MRPC f=81.65
2019-02-16 05:04:39,135 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 05:04:39,135 : 58.67,66.38,89.93,85.20,76.22,30.27,80.20,76.74,63.04,72.93,81.65
2019-02-16 05:04:39,135 : COCO r1i2t=30.96, COCO r5i2t=64.50, COCO r10i2t=79.24, COCO medr_i2t=3.00, COCO r1t2i=25.49, COCO r5t2i=59.36, COCO r10t2i=75.32, COCO medr_t2i=4.00
2019-02-16 05:04:39,135 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 05:04:39,135 : 30.96,64.50,79.24,3.00,25.49,59.36,75.32,4.00
2019-02-16 05:04:39,135 : SentLen=67.73, WC=32.49, TreeDepth=32.25, TopConst=53.86, BShift=80.01, Tense=84.54, SubjNum=76.94, ObjNum=78.27, SOMO=54.74, CoordInv=50.37, average=61.12
2019-02-16 05:04:39,135 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 05:04:39,135 : 67.73,32.49,32.25,53.86,80.01,84.54,76.94,78.27,54.74,50.37,61.12
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,136 : layer 5
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,136 : ********************************************************************************
2019-02-16 05:04:39,241 : ***** Transfer task : STS12 *****


2019-02-16 05:04:39,255 : loading BERT model bert-base-uncased
2019-02-16 05:04:39,256 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:04:39,276 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:04:39,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb4zuiax9
2019-02-16 05:04:41,782 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:04:45,079 : MSRpar : pearson = 0.3152, spearman = 0.3534
2019-02-16 05:04:45,850 : MSRvid : pearson = 0.6660, spearman = 0.6758
2019-02-16 05:04:46,495 : SMTeuroparl : pearson = 0.4891, spearman = 0.5965
2019-02-16 05:04:47,703 : surprise.OnWN : pearson = 0.5582, spearman = 0.5846
2019-02-16 05:04:48,358 : surprise.SMTnews : pearson = 0.5278, spearman = 0.4567
2019-02-16 05:04:48,358 : ALL (weighted average) : Pearson = 0.5115,             Spearman = 0.5361
2019-02-16 05:04:48,358 : ALL (average) : Pearson = 0.5113,             Spearman = 0.5334

2019-02-16 05:04:48,358 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 05:04:48,368 : loading BERT model bert-base-uncased
2019-02-16 05:04:48,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:04:48,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:04:48,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjziqg7nt
2019-02-16 05:04:50,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:04:53,019 : FNWN : pearson = 0.1763, spearman = 0.1839
2019-02-16 05:04:53,916 : headlines : pearson = 0.6502, spearman = 0.6435
2019-02-16 05:04:54,575 : OnWN : pearson = 0.5237, spearman = 0.5226
2019-02-16 05:04:54,575 : ALL (weighted average) : Pearson = 0.5432,             Spearman = 0.5404
2019-02-16 05:04:54,575 : ALL (average) : Pearson = 0.4501,             Spearman = 0.4500

2019-02-16 05:04:54,575 : ***** Transfer task : STS14 *****


2019-02-16 05:04:54,592 : loading BERT model bert-base-uncased
2019-02-16 05:04:54,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:04:54,611 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:04:54,611 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw_ui3aex
2019-02-16 05:04:57,147 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:04:59,314 : deft-forum : pearson = 0.3905, spearman = 0.4032
2019-02-16 05:05:00,033 : deft-news : pearson = 0.7182, spearman = 0.6926
2019-02-16 05:05:01,117 : headlines : pearson = 0.5975, spearman = 0.5698
2019-02-16 05:05:02,169 : images : pearson = 0.5945, spearman = 0.5882
2019-02-16 05:05:03,236 : OnWN : pearson = 0.6280, spearman = 0.6583
2019-02-16 05:05:04,619 : tweet-news : pearson = 0.5752, spearman = 0.5472
2019-02-16 05:05:04,619 : ALL (weighted average) : Pearson = 0.5834,             Spearman = 0.5765
2019-02-16 05:05:04,619 : ALL (average) : Pearson = 0.5840,             Spearman = 0.5765

2019-02-16 05:05:04,619 : ***** Transfer task : STS15 *****


2019-02-16 05:05:04,685 : loading BERT model bert-base-uncased
2019-02-16 05:05:04,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:05:04,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:05:04,705 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq5bjrvbz
2019-02-16 05:05:07,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:05:09,636 : answers-forums : pearson = 0.5000, spearman = 0.4693
2019-02-16 05:05:10,683 : answers-students : pearson = 0.6711, spearman = 0.6776
2019-02-16 05:05:11,636 : belief : pearson = 0.5408, spearman = 0.5792
2019-02-16 05:05:12,671 : headlines : pearson = 0.6493, spearman = 0.6552
2019-02-16 05:05:13,617 : images : pearson = 0.7323, spearman = 0.7411
2019-02-16 05:05:13,617 : ALL (weighted average) : Pearson = 0.6433,             Spearman = 0.6495
2019-02-16 05:05:13,617 : ALL (average) : Pearson = 0.6187,             Spearman = 0.6245

2019-02-16 05:05:13,617 : ***** Transfer task : STS16 *****


2019-02-16 05:05:13,685 : loading BERT model bert-base-uncased
2019-02-16 05:05:13,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:05:13,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:05:13,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqc403o3w
2019-02-16 05:05:16,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:05:18,061 : answer-answer : pearson = 0.4629, spearman = 0.4826
2019-02-16 05:05:18,363 : headlines : pearson = 0.6367, spearman = 0.6429
2019-02-16 05:05:18,748 : plagiarism : pearson = 0.6862, spearman = 0.6994
2019-02-16 05:05:19,366 : postediting : pearson = 0.7654, spearman = 0.8168
2019-02-16 05:05:19,645 : question-question : pearson = 0.3398, spearman = 0.3648
2019-02-16 05:05:19,645 : ALL (weighted average) : Pearson = 0.5832,             Spearman = 0.6063
2019-02-16 05:05:19,645 : ALL (average) : Pearson = 0.5782,             Spearman = 0.6013

2019-02-16 05:05:19,645 : ***** Transfer task : MR *****


2019-02-16 05:05:19,662 : loading BERT model bert-base-uncased
2019-02-16 05:05:19,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:05:19,684 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:05:19,685 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpue00oi4s
2019-02-16 05:05:22,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:05:23,723 : Generating sentence embeddings
2019-02-16 05:05:37,198 : Generated sentence embeddings
2019-02-16 05:05:37,198 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 05:05:56,154 : Best param found at split 1: l2reg = 0.001                 with score 65.75
2019-02-16 05:06:16,296 : Best param found at split 2: l2reg = 1e-05                 with score 59.68
2019-02-16 05:06:36,213 : Best param found at split 3: l2reg = 0.001                 with score 59.5
2019-02-16 05:06:52,758 : Best param found at split 4: l2reg = 1e-05                 with score 59.31
2019-02-16 05:07:14,284 : Best param found at split 5: l2reg = 1e-05                 with score 62.71
2019-02-16 05:07:15,588 : Dev acc : 61.39 Test acc : 65.13

2019-02-16 05:07:15,589 : ***** Transfer task : CR *****


2019-02-16 05:07:15,598 : loading BERT model bert-base-uncased
2019-02-16 05:07:15,598 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:07:15,622 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:07:15,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8jq82m0j
2019-02-16 05:07:18,117 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:07:19,557 : Generating sentence embeddings
2019-02-16 05:07:23,216 : Generated sentence embeddings
2019-02-16 05:07:23,217 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 05:07:28,709 : Best param found at split 1: l2reg = 1e-05                 with score 68.4
2019-02-16 05:07:34,564 : Best param found at split 2: l2reg = 0.0001                 with score 71.25
2019-02-16 05:07:40,473 : Best param found at split 3: l2reg = 0.01                 with score 69.7
2019-02-16 05:07:46,139 : Best param found at split 4: l2reg = 0.0001                 with score 72.72
2019-02-16 05:07:51,586 : Best param found at split 5: l2reg = 0.001                 with score 72.92
2019-02-16 05:07:51,770 : Dev acc : 71.0 Test acc : 66.68

2019-02-16 05:07:51,771 : ***** Transfer task : MPQA *****


2019-02-16 05:07:51,776 : loading BERT model bert-base-uncased
2019-02-16 05:07:51,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:07:51,797 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:07:51,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc8gce0xz
2019-02-16 05:07:54,262 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:07:55,737 : Generating sentence embeddings
2019-02-16 05:07:59,640 : Generated sentence embeddings
2019-02-16 05:07:59,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 05:08:18,732 : Best param found at split 1: l2reg = 0.0001                 with score 87.0
2019-02-16 05:08:37,911 : Best param found at split 2: l2reg = 1e-05                 with score 86.82
2019-02-16 05:08:57,756 : Best param found at split 3: l2reg = 0.0001                 with score 87.34
2019-02-16 05:09:15,573 : Best param found at split 4: l2reg = 0.0001                 with score 86.91
2019-02-16 05:09:35,676 : Best param found at split 5: l2reg = 1e-05                 with score 86.39
2019-02-16 05:09:37,082 : Dev acc : 86.89 Test acc : 85.9

2019-02-16 05:09:37,084 : ***** Transfer task : SUBJ *****


2019-02-16 05:09:37,107 : loading BERT model bert-base-uncased
2019-02-16 05:09:37,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:09:37,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:09:37,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf3zjycfe
2019-02-16 05:09:39,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:09:41,165 : Generating sentence embeddings
2019-02-16 05:09:55,388 : Generated sentence embeddings
2019-02-16 05:09:55,388 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 05:10:13,718 : Best param found at split 1: l2reg = 1e-05                 with score 90.85
2019-02-16 05:10:33,524 : Best param found at split 2: l2reg = 0.001                 with score 90.18
2019-02-16 05:10:51,943 : Best param found at split 3: l2reg = 0.01                 with score 90.55
2019-02-16 05:11:10,758 : Best param found at split 4: l2reg = 0.0001                 with score 90.78
2019-02-16 05:11:30,430 : Best param found at split 5: l2reg = 0.001                 with score 90.62
2019-02-16 05:11:31,557 : Dev acc : 90.6 Test acc : 90.55

2019-02-16 05:11:31,558 : ***** Transfer task : SST Binary classification *****


2019-02-16 05:11:31,703 : loading BERT model bert-base-uncased
2019-02-16 05:11:31,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:11:31,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:11:31,731 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkigvycku
2019-02-16 05:11:34,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:11:35,715 : Computing embedding for train
2019-02-16 05:12:21,666 : Computed train embeddings
2019-02-16 05:12:21,666 : Computing embedding for dev
2019-02-16 05:12:22,593 : Computed dev embeddings
2019-02-16 05:12:22,593 : Computing embedding for test
2019-02-16 05:12:24,616 : Computed test embeddings
2019-02-16 05:12:24,616 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:12:47,842 : [('reg:1e-05', 79.7), ('reg:0.0001', 79.59), ('reg:0.001', 79.13), ('reg:0.01', 77.06)]
2019-02-16 05:12:47,842 : Validation : best param found is reg = 1e-05 with score             79.7
2019-02-16 05:12:47,842 : Evaluating...
2019-02-16 05:12:53,818 : 
Dev acc : 79.7 Test acc : 77.38 for             SST Binary classification

2019-02-16 05:12:53,818 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 05:12:53,872 : loading BERT model bert-base-uncased
2019-02-16 05:12:53,872 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:12:53,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:12:53,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9opd4yuf
2019-02-16 05:12:56,452 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:12:58,496 : Computing embedding for train
2019-02-16 05:13:09,581 : Computed train embeddings
2019-02-16 05:13:09,581 : Computing embedding for dev
2019-02-16 05:13:10,927 : Computed dev embeddings
2019-02-16 05:13:10,928 : Computing embedding for test
2019-02-16 05:13:13,569 : Computed test embeddings
2019-02-16 05:13:13,569 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:13:16,217 : [('reg:1e-05', 31.24), ('reg:0.0001', 32.97), ('reg:0.001', 35.15), ('reg:0.01', 31.34)]
2019-02-16 05:13:16,218 : Validation : best param found is reg = 0.001 with score             35.15
2019-02-16 05:13:16,218 : Evaluating...
2019-02-16 05:13:16,877 : 
Dev acc : 35.15 Test acc : 38.55 for             SST Fine-Grained classification

2019-02-16 05:13:16,877 : ***** Transfer task : TREC *****


2019-02-16 05:13:16,898 : loading BERT model bert-base-uncased
2019-02-16 05:13:16,898 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:13:16,931 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:13:16,931 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpke8r4lgs
2019-02-16 05:13:20,006 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:13:25,752 : Computed train embeddings
2019-02-16 05:13:26,025 : Computed test embeddings
2019-02-16 05:13:26,025 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 05:13:33,943 : [('reg:1e-05', 64.52), ('reg:0.0001', 68.78), ('reg:0.001', 66.82), ('reg:0.01', 63.03)]
2019-02-16 05:13:33,943 : Cross-validation : best param found is reg = 0.0001             with score 68.78
2019-02-16 05:13:33,943 : Evaluating...
2019-02-16 05:13:34,518 : 
Dev acc : 68.78 Test acc : 76.8             for TREC

2019-02-16 05:13:34,519 : ***** Transfer task : MRPC *****


2019-02-16 05:13:34,558 : loading BERT model bert-base-uncased
2019-02-16 05:13:34,558 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:13:34,592 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:13:34,593 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdfw7quvw
2019-02-16 05:13:37,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:13:39,422 : Computing embedding for train
2019-02-16 05:13:49,204 : Computed train embeddings
2019-02-16 05:13:49,204 : Computing embedding for test
2019-02-16 05:13:53,433 : Computed test embeddings
2019-02-16 05:13:53,450 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 05:14:01,620 : [('reg:1e-05', 71.66), ('reg:0.0001', 71.84), ('reg:0.001', 71.71), ('reg:0.01', 70.88)]
2019-02-16 05:14:01,621 : Cross-validation : best param found is reg = 0.0001             with score 71.84
2019-02-16 05:14:01,621 : Evaluating...
2019-02-16 05:14:02,112 : Dev acc : 71.84 Test acc 72.52; Test F1 81.67 for MRPC.

2019-02-16 05:14:02,113 : ***** Transfer task : SICK-Entailment*****


2019-02-16 05:14:02,184 : loading BERT model bert-base-uncased
2019-02-16 05:14:02,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:14:02,207 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:14:02,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd1ocengu
2019-02-16 05:14:04,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:14:06,130 : Computing embedding for train
2019-02-16 05:14:11,840 : Computed train embeddings
2019-02-16 05:14:11,840 : Computing embedding for dev
2019-02-16 05:14:12,591 : Computed dev embeddings
2019-02-16 05:14:12,591 : Computing embedding for test
2019-02-16 05:14:18,384 : Computed test embeddings
2019-02-16 05:14:18,411 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:14:20,087 : [('reg:1e-05', 74.6), ('reg:0.0001', 80.4), ('reg:0.001', 76.4), ('reg:0.01', 77.2)]
2019-02-16 05:14:20,088 : Validation : best param found is reg = 0.0001 with score             80.4
2019-02-16 05:14:20,088 : Evaluating...
2019-02-16 05:14:20,794 : 
Dev acc : 80.4 Test acc : 78.16 for                        SICK entailment

2019-02-16 05:14:20,795 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 05:14:20,827 : loading BERT model bert-base-uncased
2019-02-16 05:14:20,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:14:20,850 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:14:20,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplbai1aad
2019-02-16 05:14:23,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:14:24,823 : Computing embedding for train
2019-02-16 05:14:29,997 : Computed train embeddings
2019-02-16 05:14:29,997 : Computing embedding for dev
2019-02-16 05:14:30,681 : Computed dev embeddings
2019-02-16 05:14:30,681 : Computing embedding for test
2019-02-16 05:14:36,204 : Computed test embeddings
2019-02-16 05:15:00,427 : Dev : Pearson 0.8044908751785099
2019-02-16 05:15:00,427 : Test : Pearson 0.7908200536926443 Spearman 0.7219444348371298 MSE 0.3820328401020975                        for SICK Relatedness

2019-02-16 05:15:00,431 : 

***** Transfer task : STSBenchmark*****


2019-02-16 05:15:00,484 : loading BERT model bert-base-uncased
2019-02-16 05:15:00,484 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:15:00,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:15:00,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbicb75y9
2019-02-16 05:15:03,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:15:04,533 : Computing embedding for train
2019-02-16 05:15:13,537 : Computed train embeddings
2019-02-16 05:15:13,537 : Computing embedding for dev
2019-02-16 05:15:16,245 : Computed dev embeddings
2019-02-16 05:15:16,245 : Computing embedding for test
2019-02-16 05:15:18,455 : Computed test embeddings
2019-02-16 05:15:46,929 : Dev : Pearson 0.6478805515062751
2019-02-16 05:15:46,929 : Test : Pearson 0.6420220664289261 Spearman 0.6402174377560115 MSE 1.5042491503707691                        for SICK Relatedness

2019-02-16 05:15:46,930 : ***** Transfer task : SNLI Entailment*****


2019-02-16 05:15:51,997 : loading BERT model bert-base-uncased
2019-02-16 05:15:51,998 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:15:52,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:15:52,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkkrxxwbb
2019-02-16 05:15:54,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:15:56,324 : PROGRESS (encoding): 0.00%
2019-02-16 05:17:15,161 : PROGRESS (encoding): 14.56%
2019-02-16 05:18:42,348 : PROGRESS (encoding): 29.12%
2019-02-16 05:20:09,469 : PROGRESS (encoding): 43.69%
2019-02-16 05:21:44,994 : PROGRESS (encoding): 58.25%
2019-02-16 05:23:29,097 : PROGRESS (encoding): 72.81%
2019-02-16 05:25:14,117 : PROGRESS (encoding): 87.37%
2019-02-16 05:27:03,722 : PROGRESS (encoding): 0.00%
2019-02-16 05:27:17,239 : PROGRESS (encoding): 0.00%
2019-02-16 05:27:30,355 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 05:28:04,791 : [('reg:1e-09', 55.9)]
2019-02-16 05:28:04,792 : Validation : best param found is reg = 1e-09 with score             55.9
2019-02-16 05:28:04,792 : Evaluating...
2019-02-16 05:28:41,861 : Dev acc : 55.9 Test acc : 55.93 for SNLI

2019-02-16 05:28:41,861 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 05:28:50,970 : loading BERT model bert-base-uncased
2019-02-16 05:28:50,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 05:28:51,019 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 05:28:51,020 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcnmb677e
2019-02-16 05:28:53,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 05:28:54,986 : Computing embedding for train
2019-02-16 05:36:27,091 : Computed train embeddings
2019-02-16 05:36:27,091 : Computing embedding for dev
2019-02-16 05:36:46,632 : Computed dev embeddings
2019-02-16 05:36:46,632 : Computing embedding for test
2019-02-16 05:37:06,566 : Computed test embeddings
2019-02-16 05:37:06,583 : prepare data
2019-02-16 05:37:06,649 : start epoch
2019-02-16 05:37:49,705 : samples : 64000
2019-02-16 05:38:00,169 : Image to text: 6.42, 19.34, 30.04, 28.0
2019-02-16 05:38:07,717 : Text to Image: 5.016, 16.704, 26.184, 32.0
2019-02-16 05:38:50,838 : samples : 128000
2019-02-16 05:39:01,256 : Image to text: 7.76, 23.98, 35.0, 22.0
2019-02-16 05:39:08,824 : Text to Image: 6.464, 20.376, 30.92, 26.0
2019-02-16 05:39:51,838 : samples : 192000
2019-02-16 05:40:02,330 : Image to text: 7.76, 22.48, 33.18, 23.0
2019-02-16 05:40:09,889 : Text to Image: 5.648, 18.076, 28.192, 29.0
2019-02-16 05:40:52,382 : samples : 256000
2019-02-16 05:41:02,845 : Image to text: 8.88, 25.18, 36.54, 20.0
2019-02-16 05:41:10,372 : Text to Image: 6.952, 21.268, 32.092, 25.0
2019-02-16 05:41:53,352 : samples : 320000
2019-02-16 05:42:03,867 : Image to text: 8.72, 25.2, 36.4, 20.0
2019-02-16 05:42:11,492 : Text to Image: 7.688, 22.704, 33.84, 22.0
2019-02-16 05:42:54,191 : samples : 384000
2019-02-16 05:43:04,693 : Image to text: 8.42, 25.72, 36.66, 20.0
2019-02-16 05:43:12,233 : Text to Image: 6.728, 21.336, 31.608, 25.0
2019-02-16 05:43:55,293 : samples : 448000
2019-02-16 05:44:05,807 : Image to text: 7.86, 23.42, 35.34, 21.0
2019-02-16 05:44:13,416 : Text to Image: 6.328, 20.244, 30.876, 25.0
2019-02-16 05:44:56,423 : samples : 512000
2019-02-16 05:45:06,877 : Image to text: 8.82, 25.88, 36.64, 20.0
2019-02-16 05:45:14,493 : Text to Image: 6.676, 21.924, 32.528, 24.0
2019-02-16 05:45:50,699 : Epoch 1 finished
2019-02-16 05:45:51,136 : Image to text: 23.4, 56.0, 71.9, 4.0
2019-02-16 05:45:51,481 : Text to Image: 19.44, 50.74, 67.72, 5.0
2019-02-16 05:45:51,927 : Image to text: 25.6, 56.2, 69.5, 4.0
2019-02-16 05:45:52,272 : Text to Image: 18.72, 49.1, 67.6, 6.0
2019-02-16 05:45:52,728 : Image to text: 23.8, 55.3, 70.3, 4.0
2019-02-16 05:45:53,072 : Text to Image: 19.58, 48.74, 66.44, 6.0
2019-02-16 05:45:53,543 : Image to text: 25.5, 57.6, 71.8, 4.0
2019-02-16 05:45:53,883 : Text to Image: 19.72, 50.72, 67.64, 5.0
2019-02-16 05:45:54,329 : Image to text: 24.4, 58.5, 71.3, 4.0
2019-02-16 05:45:54,671 : Text to Image: 19.32, 50.7, 67.8, 5.0
2019-02-16 05:45:54,671 : Dev mean Text to Image: 19.355999999999998, 50.0, 67.44, 5.4
2019-02-16 05:45:54,671 : Dev mean Image to text: 24.54, 56.72, 70.96000000000001, 4.0
2019-02-16 05:45:54,671 : start epoch
2019-02-16 05:46:37,029 : samples : 64000
2019-02-16 05:46:47,558 : Image to text: 8.74, 26.74, 38.94, 17.0
2019-02-16 05:46:55,922 : Text to Image: 7.176, 22.172, 33.144, 23.0
2019-02-16 05:47:46,766 : samples : 128000
2019-02-16 05:47:57,204 : Image to text: 9.02, 25.56, 38.32, 17.0
2019-02-16 05:48:04,760 : Text to Image: 7.968, 23.5, 34.708, 21.0
2019-02-16 05:48:47,795 : samples : 192000
2019-02-16 05:48:58,318 : Image to text: 9.74, 26.66, 38.64, 17.0
2019-02-16 05:49:05,874 : Text to Image: 7.788, 23.576, 34.948, 21.0
2019-02-16 05:49:48,613 : samples : 256000
2019-02-16 05:49:59,016 : Image to text: 9.12, 26.32, 39.26, 17.0
2019-02-16 05:50:06,602 : Text to Image: 8.432, 24.936, 36.56, 20.0
2019-02-16 05:50:48,897 : samples : 320000
2019-02-16 05:50:59,370 : Image to text: 9.42, 27.56, 40.0, 16.0
2019-02-16 05:51:06,905 : Text to Image: 7.8, 23.26, 34.716, 21.0
2019-02-16 05:51:49,424 : samples : 384000
2019-02-16 05:51:59,885 : Image to text: 10.3, 28.04, 39.8, 18.0
2019-02-16 05:52:07,476 : Text to Image: 7.996, 24.144, 35.844, 20.0
2019-02-16 05:52:50,147 : samples : 448000
2019-02-16 05:53:00,592 : Image to text: 9.46, 27.42, 39.46, 17.0
2019-02-16 05:53:08,102 : Text to Image: 7.932, 24.036, 35.764, 20.0
2019-02-16 05:53:51,160 : samples : 512000
2019-02-16 05:54:01,664 : Image to text: 10.2, 28.34, 40.6, 16.0
2019-02-16 05:54:09,240 : Text to Image: 8.668, 25.184, 36.756, 19.0
2019-02-16 05:54:45,821 : Epoch 2 finished
2019-02-16 05:54:46,266 : Image to text: 25.4, 58.4, 74.4, 4.0
2019-02-16 05:54:46,606 : Text to Image: 22.62, 54.38, 71.92, 5.0
2019-02-16 05:54:47,070 : Image to text: 24.6, 58.1, 72.0, 4.0
2019-02-16 05:54:47,414 : Text to Image: 20.76, 52.76, 70.86, 5.0
2019-02-16 05:54:47,864 : Image to text: 26.1, 59.3, 73.6, 4.0
2019-02-16 05:54:48,210 : Text to Image: 21.56, 53.76, 70.34, 5.0
2019-02-16 05:54:48,677 : Image to text: 25.4, 60.7, 75.2, 4.0
2019-02-16 05:54:49,022 : Text to Image: 22.72, 54.26, 70.3, 5.0
2019-02-16 05:54:49,472 : Image to text: 27.3, 60.0, 73.6, 4.0
2019-02-16 05:54:49,809 : Text to Image: 22.66, 54.46, 70.3, 5.0
2019-02-16 05:54:49,809 : Dev mean Text to Image: 22.064, 53.92399999999999, 70.744, 5.0
2019-02-16 05:54:49,809 : Dev mean Image to text: 25.76, 59.3, 73.75999999999999, 4.0
2019-02-16 05:54:49,809 : start epoch
2019-02-16 05:55:32,636 : samples : 64000
2019-02-16 05:55:43,104 : Image to text: 9.84, 27.74, 41.08, 16.0
2019-02-16 05:55:50,687 : Text to Image: 8.652, 25.2, 36.852, 19.0
2019-02-16 05:56:33,151 : samples : 128000
2019-02-16 05:56:43,659 : Image to text: 8.22, 25.42, 37.86, 18.0
2019-02-16 05:56:51,167 : Text to Image: 7.604, 23.184, 34.644, 22.0
2019-02-16 05:57:33,449 : samples : 192000
2019-02-16 05:57:43,978 : Image to text: 10.56, 29.0, 40.56, 16.0
2019-02-16 05:57:51,548 : Text to Image: 8.0, 24.58, 36.364, 20.0
2019-02-16 05:58:34,185 : samples : 256000
2019-02-16 05:58:44,743 : Image to text: 9.84, 27.84, 40.4, 17.0
2019-02-16 05:58:52,258 : Text to Image: 7.692, 23.496, 35.02, 21.0
2019-02-16 05:59:34,609 : samples : 320000
2019-02-16 05:59:45,176 : Image to text: 10.16, 28.8, 41.14, 16.0
2019-02-16 05:59:52,810 : Text to Image: 8.504, 24.964, 36.404, 20.0
2019-02-16 06:00:35,183 : samples : 384000
2019-02-16 06:00:45,748 : Image to text: 10.72, 28.62, 40.64, 16.0
2019-02-16 06:00:53,299 : Text to Image: 8.572, 25.152, 36.776, 19.0
2019-02-16 06:01:36,113 : samples : 448000
2019-02-16 06:01:46,672 : Image to text: 10.22, 28.82, 41.08, 15.0
2019-02-16 06:01:54,184 : Text to Image: 8.696, 25.76, 37.436, 19.0
2019-02-16 06:02:36,746 : samples : 512000
2019-02-16 06:02:47,296 : Image to text: 9.7, 28.42, 41.1, 15.0
2019-02-16 06:02:54,853 : Text to Image: 8.792, 26.112, 37.692, 19.0
2019-02-16 06:03:31,788 : Epoch 3 finished
2019-02-16 06:03:32,210 : Image to text: 25.0, 56.5, 75.0, 4.0
2019-02-16 06:03:32,533 : Text to Image: 22.42, 54.8, 71.88, 5.0
2019-02-16 06:03:32,949 : Image to text: 24.4, 59.1, 73.7, 4.0
2019-02-16 06:03:33,271 : Text to Image: 20.96, 52.92, 71.14, 5.0
2019-02-16 06:03:33,698 : Image to text: 25.3, 59.1, 73.9, 4.0
2019-02-16 06:03:34,020 : Text to Image: 20.86, 54.38, 71.04, 5.0
2019-02-16 06:03:34,440 : Image to text: 27.6, 60.3, 75.3, 4.0
2019-02-16 06:03:34,765 : Text to Image: 22.14, 54.72, 71.0, 5.0
2019-02-16 06:03:35,199 : Image to text: 27.8, 60.5, 73.9, 3.0
2019-02-16 06:03:35,524 : Text to Image: 22.34, 55.32, 71.02, 4.0
2019-02-16 06:03:35,524 : Dev mean Text to Image: 21.744, 54.428000000000004, 71.216, 4.8
2019-02-16 06:03:35,524 : Dev mean Image to text: 26.020000000000003, 59.1, 74.36, 3.8000000000000003
2019-02-16 06:03:35,525 : start epoch
2019-02-16 06:04:29,364 : samples : 64000
2019-02-16 06:04:39,869 : Image to text: 10.3, 29.52, 41.98, 15.0
2019-02-16 06:04:47,418 : Text to Image: 9.024, 26.204, 38.168, 18.0
2019-02-16 06:05:30,306 : samples : 128000
2019-02-16 06:05:40,702 : Image to text: 10.28, 29.6, 42.86, 15.0
2019-02-16 06:05:48,267 : Text to Image: 9.172, 26.168, 37.864, 18.0
2019-02-16 06:06:31,142 : samples : 192000
2019-02-16 06:06:41,709 : Image to text: 10.3, 28.56, 42.28, 15.0
2019-02-16 06:06:49,285 : Text to Image: 8.364, 25.156, 36.768, 19.0
2019-02-16 06:07:32,390 : samples : 256000
2019-02-16 06:07:42,802 : Image to text: 10.26, 29.54, 42.28, 15.0
2019-02-16 06:07:50,392 : Text to Image: 8.952, 26.24, 37.884, 18.0
2019-02-16 06:08:33,232 : samples : 320000
2019-02-16 06:08:43,721 : Image to text: 10.46, 29.9, 42.4, 14.0
2019-02-16 06:08:51,243 : Text to Image: 9.172, 26.332, 37.952, 18.0
2019-02-16 06:09:33,849 : samples : 384000
2019-02-16 06:09:44,417 : Image to text: 10.48, 29.54, 42.8, 15.0
2019-02-16 06:09:51,985 : Text to Image: 9.136, 26.188, 38.216, 18.0
2019-02-16 06:10:34,463 : samples : 448000
2019-02-16 06:10:44,923 : Image to text: 10.2, 29.5, 42.34, 15.0
2019-02-16 06:10:52,410 : Text to Image: 9.34, 26.836, 38.62, 18.0
2019-02-16 06:11:35,334 : samples : 512000
2019-02-16 06:11:45,851 : Image to text: 11.48, 30.2, 42.58, 15.0
2019-02-16 06:11:53,376 : Text to Image: 9.352, 26.544, 38.56, 18.0
2019-02-16 06:12:30,585 : Epoch 4 finished
2019-02-16 06:12:31,024 : Image to text: 27.2, 62.8, 77.6, 4.0
2019-02-16 06:12:31,355 : Text to Image: 22.8, 56.68, 73.58, 4.0
2019-02-16 06:12:31,784 : Image to text: 26.9, 60.1, 73.5, 4.0
2019-02-16 06:12:32,116 : Text to Image: 21.84, 54.02, 72.18, 5.0
2019-02-16 06:12:32,558 : Image to text: 27.1, 60.1, 76.2, 4.0
2019-02-16 06:12:32,908 : Text to Image: 23.44, 56.2, 72.78, 4.0
2019-02-16 06:12:33,344 : Image to text: 29.3, 62.3, 76.5, 3.0
2019-02-16 06:12:33,687 : Text to Image: 22.88, 55.46, 72.16, 4.0
2019-02-16 06:12:34,128 : Image to text: 28.0, 61.6, 74.4, 3.0
2019-02-16 06:12:34,468 : Text to Image: 23.14, 55.82, 71.76, 4.0
2019-02-16 06:12:34,468 : Dev mean Text to Image: 22.82, 55.636, 72.492, 4.2
2019-02-16 06:12:34,469 : Dev mean Image to text: 27.700000000000003, 61.379999999999995, 75.64, 3.6000000000000005
2019-02-16 06:12:34,470 : start epoch
2019-02-16 06:13:17,048 : samples : 64000
2019-02-16 06:13:27,639 : Image to text: 10.84, 29.86, 42.34, 15.0
2019-02-16 06:13:35,216 : Text to Image: 9.272, 26.836, 38.556, 18.0
2019-02-16 06:14:17,917 : samples : 128000
2019-02-16 06:14:28,464 : Image to text: 11.14, 30.04, 43.06, 15.0
2019-02-16 06:14:36,014 : Text to Image: 9.388, 27.06, 39.156, 17.0
2019-02-16 06:15:18,478 : samples : 192000
2019-02-16 06:15:28,961 : Image to text: 10.2, 29.48, 41.88, 15.0
2019-02-16 06:15:36,426 : Text to Image: 9.356, 26.628, 38.856, 17.0
2019-02-16 06:16:19,031 : samples : 256000
2019-02-16 06:16:29,511 : Image to text: 10.58, 29.36, 41.8, 15.0
2019-02-16 06:16:37,050 : Text to Image: 9.312, 26.196, 38.344, 18.0
2019-02-16 06:17:20,627 : samples : 320000
2019-02-16 06:17:31,171 : Image to text: 11.0, 30.3, 42.46, 15.0
2019-02-16 06:17:38,699 : Text to Image: 9.06, 26.164, 38.208, 18.0
2019-02-16 06:18:21,334 : samples : 384000
2019-02-16 06:18:31,900 : Image to text: 11.24, 30.58, 43.04, 15.0
2019-02-16 06:18:39,466 : Text to Image: 9.708, 27.26, 39.48, 17.0
2019-02-16 06:19:21,961 : samples : 448000
2019-02-16 06:19:32,434 : Image to text: 11.12, 30.82, 43.36, 14.0
2019-02-16 06:19:40,011 : Text to Image: 9.156, 26.476, 38.736, 17.0
2019-02-16 06:20:22,531 : samples : 512000
2019-02-16 06:20:32,801 : Image to text: 11.04, 29.98, 43.16, 14.0
2019-02-16 06:20:41,184 : Text to Image: 9.524, 27.1, 39.604, 17.0
2019-02-16 06:21:25,787 : Epoch 5 finished
2019-02-16 06:21:26,219 : Image to text: 28.2, 62.7, 77.4, 3.0
2019-02-16 06:21:26,551 : Text to Image: 23.62, 57.62, 74.22, 4.0
2019-02-16 06:21:26,979 : Image to text: 25.9, 59.8, 75.1, 4.0
2019-02-16 06:21:27,316 : Text to Image: 23.24, 55.2, 73.06, 4.0
2019-02-16 06:21:27,751 : Image to text: 26.9, 60.5, 76.2, 4.0
2019-02-16 06:21:28,093 : Text to Image: 24.08, 56.76, 73.86, 4.0
2019-02-16 06:21:28,538 : Image to text: 29.2, 64.0, 77.0, 3.0
2019-02-16 06:21:28,876 : Text to Image: 23.48, 57.44, 74.2, 4.0
2019-02-16 06:21:29,328 : Image to text: 30.7, 62.6, 75.4, 3.0
2019-02-16 06:21:29,673 : Text to Image: 23.48, 57.38, 73.24, 4.0
2019-02-16 06:21:29,673 : Dev mean Text to Image: 23.58, 56.879999999999995, 73.716, 4.0
2019-02-16 06:21:29,673 : Dev mean Image to text: 28.18, 61.92, 76.22, 3.4000000000000004
2019-02-16 06:21:29,674 : start epoch
2019-02-16 06:22:11,943 : samples : 64000
2019-02-16 06:22:22,490 : Image to text: 10.84, 31.46, 44.0, 14.0
2019-02-16 06:22:30,080 : Text to Image: 9.36, 27.112, 39.096, 17.0
2019-02-16 06:23:12,104 : samples : 128000
2019-02-16 06:23:22,596 : Image to text: 11.62, 30.8, 43.72, 14.0
2019-02-16 06:23:30,175 : Text to Image: 9.764, 27.12, 39.372, 17.0
2019-02-16 06:24:12,562 : samples : 192000
2019-02-16 06:24:23,129 : Image to text: 11.32, 30.34, 43.14, 14.0
2019-02-16 06:24:30,651 : Text to Image: 9.024, 26.688, 38.832, 17.0
2019-02-16 06:25:13,109 : samples : 256000
2019-02-16 06:25:23,674 : Image to text: 11.86, 31.02, 43.96, 14.0
2019-02-16 06:25:31,211 : Text to Image: 9.596, 27.38, 39.44, 17.0
2019-02-16 06:26:13,827 : samples : 320000
2019-02-16 06:26:24,408 : Image to text: 11.28, 30.58, 44.3, 14.0
2019-02-16 06:26:32,041 : Text to Image: 9.684, 27.192, 39.212, 17.0
2019-02-16 06:27:14,178 : samples : 384000
2019-02-16 06:27:24,708 : Image to text: 11.4, 31.54, 44.54, 14.0
2019-02-16 06:27:32,247 : Text to Image: 10.104, 27.796, 40.096, 16.0
2019-02-16 06:28:14,734 : samples : 448000
2019-02-16 06:28:25,343 : Image to text: 10.82, 31.4, 43.96, 14.0
2019-02-16 06:28:32,907 : Text to Image: 9.684, 26.932, 39.4, 17.0
2019-02-16 06:29:16,383 : samples : 512000
2019-02-16 06:29:26,796 : Image to text: 11.34, 31.32, 44.06, 14.0
2019-02-16 06:29:34,360 : Text to Image: 9.424, 27.176, 38.932, 17.0
2019-02-16 06:30:11,295 : Epoch 6 finished
2019-02-16 06:30:12,172 : Image to text: 28.2, 63.3, 78.8, 4.0
2019-02-16 06:30:12,936 : Text to Image: 23.32, 58.02, 75.5, 4.0
2019-02-16 06:30:13,821 : Image to text: 28.3, 61.0, 76.8, 4.0
2019-02-16 06:30:14,586 : Text to Image: 23.34, 56.36, 74.18, 4.0
2019-02-16 06:30:15,507 : Image to text: 26.8, 61.5, 77.1, 4.0
2019-02-16 06:30:16,291 : Text to Image: 24.58, 57.7, 74.84, 4.0
2019-02-16 06:30:17,276 : Image to text: 27.6, 63.4, 79.3, 3.0
2019-02-16 06:30:18,058 : Text to Image: 24.28, 57.84, 73.7, 4.0
2019-02-16 06:30:18,983 : Image to text: 30.5, 62.9, 76.6, 3.0
2019-02-16 06:30:19,733 : Text to Image: 23.58, 57.12, 73.54, 4.0
2019-02-16 06:30:19,733 : Dev mean Text to Image: 23.82, 57.40800000000001, 74.352, 4.0
2019-02-16 06:30:19,733 : Dev mean Image to text: 28.28, 62.419999999999995, 77.71999999999998, 3.6000000000000005
2019-02-16 06:30:19,733 : start epoch
2019-02-16 06:31:04,920 : samples : 64000
2019-02-16 06:31:17,550 : Image to text: 12.0, 32.0, 45.8, 13.0
2019-02-16 06:31:27,641 : Text to Image: 9.972, 27.968, 40.024, 16.0
2019-02-16 06:32:11,181 : samples : 128000
2019-02-16 06:32:21,394 : Image to text: 11.9, 31.06, 44.24, 14.0
2019-02-16 06:32:28,809 : Text to Image: 9.704, 27.464, 39.872, 17.0
2019-02-16 06:33:12,098 : samples : 192000
2019-02-16 06:33:24,703 : Image to text: 11.52, 31.18, 43.7, 14.0
2019-02-16 06:33:34,762 : Text to Image: 9.556, 27.596, 39.7, 17.0
2019-02-16 06:34:20,542 : samples : 256000
2019-02-16 06:34:30,817 : Image to text: 11.66, 32.04, 44.66, 14.0
2019-02-16 06:34:38,044 : Text to Image: 9.78, 28.252, 40.54, 16.0
2019-02-16 06:35:19,868 : samples : 320000
2019-02-16 06:35:29,954 : Image to text: 10.82, 30.72, 42.8, 14.0
2019-02-16 06:35:39,748 : Text to Image: 9.544, 27.844, 39.996, 17.0
2019-02-16 06:36:24,123 : samples : 384000
2019-02-16 06:36:36,788 : Image to text: 10.64, 30.94, 44.7, 13.0
2019-02-16 06:36:46,889 : Text to Image: 10.02, 28.044, 40.244, 17.0
2019-02-16 06:37:30,557 : samples : 448000
2019-02-16 06:37:42,169 : Image to text: 11.82, 31.9, 44.44, 14.0
2019-02-16 06:37:51,930 : Text to Image: 9.76, 27.976, 40.216, 16.0
2019-02-16 06:38:39,035 : samples : 512000
2019-02-16 06:38:51,631 : Image to text: 11.4, 31.26, 43.94, 13.0
2019-02-16 06:39:01,646 : Text to Image: 10.024, 28.328, 40.872, 16.0
2019-02-16 06:39:37,964 : Epoch 7 finished
2019-02-16 06:39:38,433 : Image to text: 28.9, 62.5, 77.9, 4.0
2019-02-16 06:39:38,811 : Text to Image: 23.62, 58.64, 75.88, 4.0
2019-02-16 06:39:39,280 : Image to text: 26.3, 59.9, 75.3, 4.0
2019-02-16 06:39:39,644 : Text to Image: 23.88, 57.44, 74.1, 4.0
2019-02-16 06:39:40,116 : Image to text: 26.2, 61.4, 76.3, 4.0
2019-02-16 06:39:40,486 : Text to Image: 24.3, 58.44, 74.9, 4.0
2019-02-16 06:39:40,956 : Image to text: 29.8, 64.1, 78.4, 3.0
2019-02-16 06:39:41,331 : Text to Image: 24.9, 58.5, 75.54, 4.0
2019-02-16 06:39:41,786 : Image to text: 29.5, 63.7, 76.4, 3.0
2019-02-16 06:39:42,159 : Text to Image: 24.74, 58.86, 74.02, 4.0
2019-02-16 06:39:42,159 : Dev mean Text to Image: 24.288, 58.376, 74.888, 4.0
2019-02-16 06:39:42,159 : Dev mean Image to text: 28.14, 62.32, 76.86, 3.6000000000000005
2019-02-16 06:39:42,160 : start epoch
2019-02-16 06:40:25,891 : samples : 64000
2019-02-16 06:40:38,477 : Image to text: 11.9, 31.5, 44.84, 14.0
2019-02-16 06:40:48,539 : Text to Image: 9.668, 27.552, 40.076, 16.0
2019-02-16 06:41:34,156 : samples : 128000
2019-02-16 06:41:44,455 : Image to text: 11.72, 31.66, 44.86, 13.0
2019-02-16 06:41:51,863 : Text to Image: 9.84, 27.772, 40.192, 16.0
2019-02-16 06:42:34,454 : samples : 192000
2019-02-16 06:42:44,682 : Image to text: 10.78, 31.24, 44.38, 14.0
2019-02-16 06:42:52,610 : Text to Image: 9.46, 27.428, 39.64, 16.0
2019-02-16 06:43:37,310 : samples : 256000
2019-02-16 06:43:49,998 : Image to text: 11.42, 31.24, 44.26, 13.0
2019-02-16 06:44:00,053 : Text to Image: 9.916, 27.992, 40.548, 16.0
2019-02-16 06:44:42,991 : samples : 320000
2019-02-16 06:44:53,362 : Image to text: 11.66, 32.08, 45.26, 13.0
2019-02-16 06:45:00,655 : Text to Image: 10.132, 28.58, 41.02, 16.0
2019-02-16 06:45:44,805 : samples : 384000
2019-02-16 06:45:57,427 : Image to text: 11.92, 32.6, 44.7, 13.0
2019-02-16 06:46:07,494 : Text to Image: 10.112, 27.832, 39.948, 17.0
2019-02-16 06:46:51,761 : samples : 448000
2019-02-16 06:47:02,080 : Image to text: 11.62, 31.68, 45.26, 13.0
2019-02-16 06:47:09,461 : Text to Image: 10.016, 28.096, 40.46, 16.0
2019-02-16 06:47:52,817 : samples : 512000
2019-02-16 06:48:05,372 : Image to text: 11.5, 31.98, 44.22, 14.0
2019-02-16 06:48:15,427 : Text to Image: 9.956, 27.848, 40.148, 17.0
2019-02-16 06:48:54,317 : Epoch 8 finished
2019-02-16 06:48:55,248 : Image to text: 28.6, 64.0, 80.0, 3.0
2019-02-16 06:48:56,043 : Text to Image: 24.62, 58.98, 75.3, 4.0
2019-02-16 06:48:56,981 : Image to text: 28.2, 62.3, 75.5, 3.0
2019-02-16 06:48:57,757 : Text to Image: 23.36, 56.22, 73.92, 4.0
2019-02-16 06:48:58,682 : Image to text: 27.5, 62.1, 78.1, 3.0
2019-02-16 06:48:59,051 : Text to Image: 24.4, 58.4, 75.26, 4.0
2019-02-16 06:48:59,515 : Image to text: 28.9, 63.4, 78.5, 3.0
2019-02-16 06:48:59,894 : Text to Image: 24.2, 58.2, 74.78, 4.0
2019-02-16 06:49:00,348 : Image to text: 31.3, 64.7, 77.6, 3.0
2019-02-16 06:49:00,710 : Text to Image: 24.7, 57.86, 73.86, 4.0
2019-02-16 06:49:00,710 : Dev mean Text to Image: 24.256, 57.932, 74.62400000000001, 4.0
2019-02-16 06:49:00,710 : Dev mean Image to text: 28.9, 63.3, 77.94, 3.0
2019-02-16 06:49:00,710 : start epoch
2019-02-16 06:49:43,548 : samples : 64000
2019-02-16 06:49:53,821 : Image to text: 11.66, 31.6, 44.52, 14.0
2019-02-16 06:50:01,097 : Text to Image: 9.948, 28.128, 40.336, 16.0
2019-02-16 06:50:45,786 : samples : 128000
2019-02-16 06:50:58,409 : Image to text: 11.92, 33.28, 45.88, 13.0
2019-02-16 06:51:08,507 : Text to Image: 9.736, 27.716, 40.264, 16.0
2019-02-16 06:51:53,064 : samples : 192000
2019-02-16 06:52:03,304 : Image to text: 11.54, 31.64, 46.08, 13.0
2019-02-16 06:52:10,754 : Text to Image: 10.164, 28.492, 40.88, 16.0
2019-02-16 06:52:53,756 : samples : 256000
2019-02-16 06:53:06,357 : Image to text: 11.82, 32.0, 45.0, 13.0
2019-02-16 06:53:16,402 : Text to Image: 9.924, 28.228, 40.368, 16.0
2019-02-16 06:54:02,367 : samples : 320000
2019-02-16 06:54:14,900 : Image to text: 11.78, 32.08, 45.22, 13.0
2019-02-16 06:54:24,199 : Text to Image: 9.764, 28.024, 40.348, 16.0
2019-02-16 06:55:15,487 : samples : 384000
2019-02-16 06:55:27,022 : Image to text: 12.06, 31.48, 45.1, 13.0
2019-02-16 06:55:35,230 : Text to Image: 9.944, 28.5, 40.664, 16.0
2019-02-16 06:56:18,911 : samples : 448000
2019-02-16 06:56:29,225 : Image to text: 11.5, 31.34, 44.46, 13.0
2019-02-16 06:56:36,631 : Text to Image: 10.128, 28.232, 40.504, 16.0
2019-02-16 06:57:19,158 : samples : 512000
2019-02-16 06:57:30,109 : Image to text: 11.52, 31.72, 45.3, 13.0
2019-02-16 06:57:37,540 : Text to Image: 10.048, 28.872, 41.124, 16.0
2019-02-16 06:58:13,985 : Epoch 9 finished
2019-02-16 06:58:14,457 : Image to text: 30.5, 64.0, 80.6, 3.0
2019-02-16 06:58:14,822 : Text to Image: 25.12, 59.46, 75.64, 4.0
2019-02-16 06:58:15,293 : Image to text: 29.2, 63.1, 78.6, 3.0
2019-02-16 06:58:15,663 : Text to Image: 23.98, 58.3, 74.98, 4.0
2019-02-16 06:58:16,132 : Image to text: 29.0, 62.8, 78.3, 3.0
2019-02-16 06:58:16,501 : Text to Image: 25.9, 59.18, 75.72, 4.0
2019-02-16 06:58:16,968 : Image to text: 30.3, 65.8, 79.2, 3.0
2019-02-16 06:58:17,344 : Text to Image: 25.34, 59.7, 75.26, 4.0
2019-02-16 06:58:17,792 : Image to text: 29.9, 64.3, 77.0, 3.0
2019-02-16 06:58:18,155 : Text to Image: 25.9, 59.32, 74.78, 4.0
2019-02-16 06:58:18,155 : Dev mean Text to Image: 25.247999999999998, 59.19200000000001, 75.276, 4.0
2019-02-16 06:58:18,155 : Dev mean Image to text: 29.779999999999998, 64.0, 78.74000000000001, 3.0
2019-02-16 06:58:18,156 : start epoch
2019-02-16 06:59:00,928 : samples : 64000
2019-02-16 06:59:11,203 : Image to text: 11.74, 31.48, 46.14, 13.0
2019-02-16 06:59:18,610 : Text to Image: 9.74, 27.712, 40.12, 16.0
2019-02-16 07:00:02,189 : samples : 128000
2019-02-16 07:00:12,462 : Image to text: 12.04, 32.68, 45.94, 13.0
2019-02-16 07:00:19,890 : Text to Image: 10.364, 28.876, 41.264, 15.0
2019-02-16 07:01:02,680 : samples : 192000
2019-02-16 07:01:12,904 : Image to text: 12.2, 32.4, 45.58, 13.0
2019-02-16 07:01:20,304 : Text to Image: 10.016, 28.456, 40.896, 16.0
2019-02-16 07:02:05,038 : samples : 256000
2019-02-16 07:02:16,634 : Image to text: 11.14, 32.12, 45.36, 13.0
2019-02-16 07:02:24,069 : Text to Image: 9.956, 28.268, 40.6, 16.0
2019-02-16 07:03:07,152 : samples : 320000
2019-02-16 07:03:17,888 : Image to text: 11.58, 31.8, 44.98, 13.0
2019-02-16 07:03:25,445 : Text to Image: 10.124, 28.404, 40.748, 16.0
2019-02-16 07:04:09,244 : samples : 384000
2019-02-16 07:04:21,754 : Image to text: 12.4, 33.06, 46.2, 13.0
2019-02-16 07:04:29,108 : Text to Image: 10.264, 28.828, 41.416, 16.0
2019-02-16 07:05:12,493 : samples : 448000
2019-02-16 07:05:22,783 : Image to text: 12.62, 32.52, 46.18, 13.0
2019-02-16 07:05:30,240 : Text to Image: 10.572, 29.072, 41.868, 15.0
2019-02-16 07:06:14,157 : samples : 512000
2019-02-16 07:06:26,739 : Image to text: 11.84, 32.3, 45.58, 13.0
2019-02-16 07:06:36,650 : Text to Image: 10.184, 28.408, 41.376, 16.0
2019-02-16 07:07:15,025 : Epoch 10 finished
2019-02-16 07:07:15,987 : Image to text: 30.2, 64.1, 79.2, 3.0
2019-02-16 07:07:16,703 : Text to Image: 25.74, 60.08, 76.58, 4.0
2019-02-16 07:07:17,679 : Image to text: 27.7, 62.1, 76.1, 3.0
2019-02-16 07:07:18,434 : Text to Image: 24.24, 57.92, 75.3, 4.0
2019-02-16 07:07:19,387 : Image to text: 28.0, 64.0, 77.6, 3.0
2019-02-16 07:07:20,165 : Text to Image: 25.5, 59.08, 76.1, 4.0
2019-02-16 07:07:21,090 : Image to text: 30.5, 64.7, 77.9, 3.0
2019-02-16 07:07:21,845 : Text to Image: 25.62, 58.8, 75.98, 4.0
2019-02-16 07:07:22,808 : Image to text: 28.9, 63.5, 77.3, 3.0
2019-02-16 07:07:23,562 : Text to Image: 25.9, 59.72, 75.42, 4.0
2019-02-16 07:07:23,562 : Dev mean Text to Image: 25.4, 59.11999999999999, 75.876, 4.0
2019-02-16 07:07:23,562 : Dev mean Image to text: 29.060000000000002, 63.68000000000001, 77.61999999999999, 3.0
2019-02-16 07:07:23,562 : start epoch
2019-02-16 07:08:08,384 : samples : 64000
2019-02-16 07:08:20,999 : Image to text: 11.92, 32.78, 45.48, 13.0
2019-02-16 07:08:31,030 : Text to Image: 10.348, 28.772, 41.228, 16.0
2019-02-16 07:09:16,243 : samples : 128000
2019-02-16 07:09:28,819 : Image to text: 11.66, 32.34, 45.52, 13.0
2019-02-16 07:09:38,816 : Text to Image: 9.848, 28.204, 40.648, 16.0
2019-02-16 07:10:23,988 : samples : 192000
2019-02-16 07:10:36,558 : Image to text: 12.44, 32.7, 46.1, 12.0
2019-02-16 07:10:46,595 : Text to Image: 10.192, 28.316, 40.86, 16.0
2019-02-16 07:11:36,053 : samples : 256000
2019-02-16 07:11:49,396 : Image to text: 12.72, 33.08, 46.1, 13.0
2019-02-16 07:11:59,672 : Text to Image: 10.392, 28.796, 41.272, 16.0
2019-02-16 07:12:44,860 : samples : 320000
2019-02-16 07:12:57,477 : Image to text: 12.36, 32.98, 46.02, 13.0
2019-02-16 07:13:07,522 : Text to Image: 10.54, 29.344, 41.448, 15.0
2019-02-16 07:13:52,507 : samples : 384000
2019-02-16 07:14:05,149 : Image to text: 12.02, 33.0, 46.26, 13.0
2019-02-16 07:14:15,188 : Text to Image: 10.476, 29.08, 41.628, 15.0
2019-02-16 07:14:59,614 : samples : 448000
2019-02-16 07:15:12,234 : Image to text: 11.88, 32.32, 45.56, 13.0
2019-02-16 07:15:22,240 : Text to Image: 10.344, 29.008, 41.276, 16.0
2019-02-16 07:16:06,683 : samples : 512000
2019-02-16 07:16:19,354 : Image to text: 11.66, 32.9, 45.68, 13.0
2019-02-16 07:16:29,453 : Text to Image: 10.056, 28.02, 40.72, 16.0
2019-02-16 07:17:08,102 : Epoch 11 finished
2019-02-16 07:17:09,101 : Image to text: 30.2, 64.2, 80.4, 3.0
2019-02-16 07:17:09,960 : Text to Image: 24.98, 59.82, 76.48, 4.0
2019-02-16 07:17:11,024 : Image to text: 28.8, 62.5, 76.4, 3.0
2019-02-16 07:17:11,926 : Text to Image: 23.84, 57.44, 75.06, 4.0
2019-02-16 07:17:12,971 : Image to text: 28.2, 62.0, 79.2, 4.0
2019-02-16 07:17:13,738 : Text to Image: 24.88, 59.62, 75.44, 4.0
2019-02-16 07:17:14,690 : Image to text: 30.9, 65.4, 78.8, 3.0
2019-02-16 07:17:15,435 : Text to Image: 25.62, 60.08, 75.78, 4.0
2019-02-16 07:17:16,373 : Image to text: 32.0, 64.2, 76.8, 3.0
2019-02-16 07:17:17,116 : Text to Image: 25.94, 58.88, 74.36, 4.0
2019-02-16 07:17:17,116 : Dev mean Text to Image: 25.052, 59.16799999999999, 75.424, 4.0
2019-02-16 07:17:17,116 : Dev mean Image to text: 30.020000000000003, 63.66000000000001, 78.32, 3.2
2019-02-16 07:17:17,116 : start epoch
2019-02-16 07:17:59,799 : samples : 64000
2019-02-16 07:18:10,086 : Image to text: 12.64, 32.7, 45.54, 13.0
2019-02-16 07:18:17,482 : Text to Image: 10.456, 28.868, 41.272, 16.0
2019-02-16 07:18:59,814 : samples : 128000
2019-02-16 07:19:10,120 : Image to text: 11.66, 32.42, 45.94, 13.0
2019-02-16 07:19:20,544 : Text to Image: 10.084, 28.12, 40.584, 16.0
2019-02-16 07:20:06,360 : samples : 192000
2019-02-16 07:20:19,253 : Image to text: 11.84, 32.1, 45.8, 13.0
2019-02-16 07:20:29,746 : Text to Image: 10.512, 29.04, 41.724, 15.0
2019-02-16 07:21:15,574 : samples : 256000
2019-02-16 07:21:28,431 : Image to text: 11.58, 32.2, 45.92, 13.0
2019-02-16 07:21:38,994 : Text to Image: 10.28, 28.812, 41.452, 15.0
2019-02-16 07:22:24,658 : samples : 320000
2019-02-16 07:22:37,590 : Image to text: 12.18, 32.66, 45.12, 13.0
2019-02-16 07:22:48,154 : Text to Image: 9.856, 28.028, 40.696, 16.0
2019-02-16 07:23:34,253 : samples : 384000
2019-02-16 07:23:47,195 : Image to text: 12.44, 32.6, 46.08, 13.0
2019-02-16 07:23:57,687 : Text to Image: 10.16, 28.768, 40.84, 16.0
2019-02-16 07:24:43,461 : samples : 448000
2019-02-16 07:24:56,352 : Image to text: 12.68, 32.86, 46.04, 13.0
2019-02-16 07:25:06,744 : Text to Image: 10.476, 28.956, 41.272, 15.0
2019-02-16 07:25:52,310 : samples : 512000
2019-02-16 07:26:05,233 : Image to text: 11.88, 32.46, 46.04, 12.0
2019-02-16 07:26:15,734 : Text to Image: 10.364, 29.284, 42.124, 15.0
2019-02-16 07:26:54,585 : Epoch 12 finished
2019-02-16 07:26:55,600 : Image to text: 30.3, 62.7, 77.9, 3.0
2019-02-16 07:26:56,479 : Text to Image: 24.52, 58.84, 76.22, 4.0
2019-02-16 07:26:57,520 : Image to text: 28.9, 61.9, 78.2, 3.0
2019-02-16 07:26:58,479 : Text to Image: 23.58, 57.5, 75.28, 4.0
2019-02-16 07:26:59,593 : Image to text: 29.4, 61.9, 78.2, 3.0
2019-02-16 07:27:00,469 : Text to Image: 24.5, 59.06, 75.94, 4.0
2019-02-16 07:27:01,582 : Image to text: 30.4, 63.2, 77.9, 3.0
2019-02-16 07:27:02,392 : Text to Image: 24.86, 59.4, 76.04, 4.0
2019-02-16 07:27:03,524 : Image to text: 28.8, 64.1, 76.5, 3.0
2019-02-16 07:27:04,454 : Text to Image: 26.06, 58.84, 75.68, 4.0
2019-02-16 07:27:04,454 : Dev mean Text to Image: 24.703999999999997, 58.727999999999994, 75.832, 4.0
2019-02-16 07:27:04,455 : Dev mean Image to text: 29.559999999999995, 62.76, 77.74, 3.0
2019-02-16 07:27:04,455 : start epoch
2019-02-16 07:27:47,901 : samples : 64000
2019-02-16 07:27:58,035 : Image to text: 12.4, 32.36, 45.58, 13.0
2019-02-16 07:28:05,692 : Text to Image: 10.328, 28.832, 41.496, 15.0
2019-02-16 07:28:58,056 : samples : 128000
2019-02-16 07:29:08,528 : Image to text: 11.58, 33.24, 46.26, 13.0
2019-02-16 07:29:16,092 : Text to Image: 9.84, 28.276, 40.628, 16.0
2019-02-16 07:29:58,356 : samples : 192000
2019-02-16 07:30:08,770 : Image to text: 12.16, 33.8, 46.58, 13.0
2019-02-16 07:30:16,394 : Text to Image: 10.388, 29.172, 42.0, 15.0
2019-02-16 07:30:59,458 : samples : 256000
2019-02-16 07:31:09,834 : Image to text: 12.56, 33.18, 46.88, 12.0
2019-02-16 07:31:17,451 : Text to Image: 10.616, 29.464, 42.112, 15.0
2019-02-16 07:32:00,385 : samples : 320000
2019-02-16 07:32:10,789 : Image to text: 12.3, 33.06, 46.06, 13.0
2019-02-16 07:32:18,330 : Text to Image: 10.416, 29.268, 41.808, 15.0
2019-02-16 07:33:01,600 : samples : 384000
2019-02-16 07:33:12,008 : Image to text: 12.22, 32.92, 45.36, 13.0
2019-02-16 07:33:19,678 : Text to Image: 10.012, 28.432, 40.984, 16.0
2019-02-16 07:34:03,103 : samples : 448000
2019-02-16 07:34:13,514 : Image to text: 12.48, 33.44, 45.72, 13.0
2019-02-16 07:34:21,098 : Text to Image: 10.6, 29.276, 41.876, 15.0
2019-02-16 07:35:04,212 : samples : 512000
2019-02-16 07:35:14,574 : Image to text: 12.52, 33.02, 46.78, 12.0
2019-02-16 07:35:22,116 : Text to Image: 10.412, 28.76, 41.24, 16.0
2019-02-16 07:35:57,902 : Epoch 13 finished
2019-02-16 07:35:58,322 : Image to text: 30.8, 64.2, 80.0, 3.0
2019-02-16 07:35:58,653 : Text to Image: 25.16, 60.66, 76.72, 4.0
2019-02-16 07:35:59,089 : Image to text: 30.3, 63.6, 78.7, 3.0
2019-02-16 07:35:59,417 : Text to Image: 24.52, 58.52, 76.24, 4.0
2019-02-16 07:35:59,862 : Image to text: 28.8, 64.1, 79.9, 3.0
2019-02-16 07:36:00,192 : Text to Image: 26.1, 60.52, 76.54, 4.0
2019-02-16 07:36:00,651 : Image to text: 30.6, 65.4, 78.8, 3.0
2019-02-16 07:36:00,995 : Text to Image: 26.34, 60.12, 76.32, 4.0
2019-02-16 07:36:01,463 : Image to text: 30.2, 65.0, 78.0, 3.0
2019-02-16 07:36:01,882 : Text to Image: 26.74, 60.04, 75.94, 4.0
2019-02-16 07:36:01,883 : Dev mean Text to Image: 25.772, 59.971999999999994, 76.352, 4.0
2019-02-16 07:36:01,883 : Dev mean Image to text: 30.14, 64.46000000000001, 79.08, 3.0
2019-02-16 07:36:01,883 : start epoch
2019-02-16 07:36:45,393 : samples : 64000
2019-02-16 07:36:55,719 : Image to text: 12.4, 32.7, 46.58, 12.0
2019-02-16 07:37:03,248 : Text to Image: 10.552, 29.316, 41.864, 15.0
2019-02-16 07:37:46,104 : samples : 128000
2019-02-16 07:37:56,442 : Image to text: 12.26, 33.6, 46.48, 12.0
2019-02-16 07:38:03,886 : Text to Image: 10.416, 28.76, 41.132, 16.0
2019-02-16 07:38:47,035 : samples : 192000
2019-02-16 07:38:57,348 : Image to text: 12.22, 33.62, 46.58, 12.0
2019-02-16 07:39:04,851 : Text to Image: 10.412, 29.076, 41.28, 15.0
2019-02-16 07:39:48,061 : samples : 256000
2019-02-16 07:39:58,422 : Image to text: 12.52, 33.24, 46.38, 12.0
2019-02-16 07:40:06,002 : Text to Image: 10.436, 29.232, 42.06, 15.0
2019-02-16 07:40:48,918 : samples : 320000
2019-02-16 07:40:59,221 : Image to text: 12.44, 33.7, 46.4, 13.0
2019-02-16 07:41:06,642 : Text to Image: 10.424, 29.368, 41.844, 15.0
2019-02-16 07:41:49,764 : samples : 384000
2019-02-16 07:42:00,049 : Image to text: 11.96, 33.0, 46.58, 12.0
2019-02-16 07:42:07,477 : Text to Image: 10.576, 29.4, 42.052, 15.0
2019-02-16 07:42:49,599 : samples : 448000
2019-02-16 07:42:59,941 : Image to text: 12.82, 32.94, 46.06, 12.0
2019-02-16 07:43:07,390 : Text to Image: 10.188, 28.712, 41.14, 16.0
2019-02-16 07:43:50,184 : samples : 512000
2019-02-16 07:44:00,530 : Image to text: 12.84, 33.44, 46.32, 12.0
2019-02-16 07:44:07,954 : Text to Image: 10.66, 29.656, 42.26, 15.0
2019-02-16 07:44:44,366 : Epoch 14 finished
2019-02-16 07:44:44,794 : Image to text: 30.8, 64.2, 81.3, 3.0
2019-02-16 07:44:45,115 : Text to Image: 26.68, 60.64, 77.06, 4.0
2019-02-16 07:44:45,560 : Image to text: 28.3, 64.5, 79.0, 3.0
2019-02-16 07:44:45,933 : Text to Image: 24.96, 58.82, 76.16, 4.0
2019-02-16 07:44:46,383 : Image to text: 28.8, 65.8, 79.7, 3.0
2019-02-16 07:44:46,899 : Text to Image: 26.24, 60.72, 77.02, 4.0
2019-02-16 07:44:47,350 : Image to text: 31.9, 65.3, 79.4, 3.0
2019-02-16 07:44:47,712 : Text to Image: 26.18, 61.34, 77.14, 4.0
2019-02-16 07:44:48,161 : Image to text: 32.6, 65.4, 78.4, 3.0
2019-02-16 07:44:48,525 : Text to Image: 26.7, 61.2, 76.12, 4.0
2019-02-16 07:44:48,525 : Dev mean Text to Image: 26.151999999999997, 60.544000000000004, 76.7, 4.0
2019-02-16 07:44:48,525 : Dev mean Image to text: 30.479999999999997, 65.04, 79.56, 3.0
2019-02-16 07:44:48,525 : start epoch
2019-02-16 07:45:41,605 : samples : 64000
2019-02-16 07:45:51,852 : Image to text: 12.4, 32.52, 45.74, 13.0
2019-02-16 07:45:59,332 : Text to Image: 10.364, 28.744, 41.448, 15.0
2019-02-16 07:46:41,494 : samples : 128000
2019-02-16 07:46:51,740 : Image to text: 12.38, 33.52, 46.12, 12.0
2019-02-16 07:46:59,187 : Text to Image: 10.36, 28.94, 41.736, 15.0
2019-02-16 07:47:42,078 : samples : 192000
2019-02-16 07:47:52,374 : Image to text: 12.42, 34.4, 46.68, 12.0
2019-02-16 07:47:59,689 : Text to Image: 10.424, 29.272, 41.628, 15.0
2019-02-16 07:48:43,252 : samples : 256000
2019-02-16 07:48:53,501 : Image to text: 12.92, 32.86, 45.76, 13.0
2019-02-16 07:49:00,961 : Text to Image: 10.396, 28.904, 41.472, 15.0
2019-02-16 07:49:44,002 : samples : 320000
2019-02-16 07:49:54,221 : Image to text: 12.06, 33.4, 47.24, 12.0
2019-02-16 07:50:01,638 : Text to Image: 10.456, 29.492, 42.148, 15.0
2019-02-16 07:50:44,765 : samples : 384000
2019-02-16 07:50:55,047 : Image to text: 12.34, 32.94, 46.34, 13.0
2019-02-16 07:51:02,455 : Text to Image: 10.204, 29.14, 41.724, 15.0
2019-02-16 07:51:45,305 : samples : 448000
2019-02-16 07:51:55,538 : Image to text: 12.1, 33.4, 46.52, 12.0
2019-02-16 07:52:02,911 : Text to Image: 10.692, 29.324, 41.784, 15.0
2019-02-16 07:52:46,065 : samples : 512000
2019-02-16 07:52:56,298 : Image to text: 12.58, 33.4, 46.3, 13.0
2019-02-16 07:53:03,702 : Text to Image: 10.568, 28.948, 41.512, 16.0
2019-02-16 07:53:39,504 : Epoch 15 finished
2019-02-16 07:53:39,975 : Image to text: 31.0, 65.4, 81.5, 3.0
2019-02-16 07:53:40,401 : Text to Image: 25.28, 60.36, 77.04, 4.0
2019-02-16 07:53:40,870 : Image to text: 30.2, 63.3, 79.7, 3.0
2019-02-16 07:53:41,309 : Text to Image: 24.86, 58.8, 75.46, 4.0
2019-02-16 07:53:41,774 : Image to text: 28.8, 64.4, 80.0, 3.0
2019-02-16 07:53:42,212 : Text to Image: 25.54, 59.36, 76.3, 4.0
2019-02-16 07:53:42,675 : Image to text: 30.4, 64.3, 78.4, 3.0
2019-02-16 07:53:43,109 : Text to Image: 24.72, 59.9, 76.14, 4.0
2019-02-16 07:53:43,595 : Image to text: 33.4, 65.3, 78.7, 3.0
2019-02-16 07:53:43,923 : Text to Image: 26.04, 59.74, 75.62, 4.0
2019-02-16 07:53:43,923 : Dev mean Text to Image: 25.287999999999997, 59.632, 76.112, 4.0
2019-02-16 07:53:43,923 : Dev mean Image to text: 30.759999999999998, 64.54, 79.66, 3.0
2019-02-16 07:53:47,738 : 
Test scores | Image to text:             30.439999999999998, 64.64, 79.24000000000001, 3.0
2019-02-16 07:53:47,738 : Test scores | Text to image:             25.943999999999996, 60.16399999999999, 76.092, 4.0

2019-02-16 07:53:47,850 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 07:53:48,086 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 07:53:48,797 : loading BERT model bert-base-uncased
2019-02-16 07:53:48,797 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:53:48,833 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:53:48,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp87n78z72
2019-02-16 07:53:51,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:53:52,802 : Computing embeddings for train/dev/test
2019-02-16 07:55:27,991 : Computed embeddings
2019-02-16 07:55:27,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:56:14,551 : [('reg:1e-05', 66.11), ('reg:0.0001', 65.38), ('reg:0.001', 67.24), ('reg:0.01', 49.99)]
2019-02-16 07:56:14,551 : Validation : best param found is reg = 0.001 with score             67.24
2019-02-16 07:56:14,551 : Evaluating...
2019-02-16 07:56:28,666 : 
Dev acc : 67.2 Test acc : 66.5 for LENGTH classification

2019-02-16 07:56:28,667 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 07:56:29,016 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 07:56:29,069 : loading BERT model bert-base-uncased
2019-02-16 07:56:29,069 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:56:29,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:56:29,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbgw5a2s0
2019-02-16 07:56:31,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:56:33,219 : Computing embeddings for train/dev/test
2019-02-16 07:58:02,912 : Computed embeddings
2019-02-16 07:58:02,912 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:59:02,676 : [('reg:1e-05', 33.8), ('reg:0.0001', 6.66), ('reg:0.001', 0.52), ('reg:0.01', 0.2)]
2019-02-16 07:59:02,676 : Validation : best param found is reg = 1e-05 with score             33.8
2019-02-16 07:59:02,676 : Evaluating...
2019-02-16 07:59:22,593 : 
Dev acc : 33.8 Test acc : 34.3 for WORDCONTENT classification

2019-02-16 07:59:22,595 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 07:59:22,979 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 07:59:23,049 : loading BERT model bert-base-uncased
2019-02-16 07:59:23,049 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:59:23,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:59:23,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpswahstdq
2019-02-16 07:59:25,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:59:27,150 : Computing embeddings for train/dev/test
2019-02-16 08:00:50,716 : Computed embeddings
2019-02-16 08:00:50,717 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:01:34,230 : [('reg:1e-05', 28.95), ('reg:0.0001', 27.8), ('reg:0.001', 30.22), ('reg:0.01', 25.79)]
2019-02-16 08:01:34,230 : Validation : best param found is reg = 0.001 with score             30.22
2019-02-16 08:01:34,230 : Evaluating...
2019-02-16 08:01:41,865 : 
Dev acc : 30.2 Test acc : 30.0 for DEPTH classification

2019-02-16 08:01:41,866 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 08:01:42,476 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 08:01:42,543 : loading BERT model bert-base-uncased
2019-02-16 08:01:42,543 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:01:42,575 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:01:42,575 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoa1vta47
2019-02-16 08:01:45,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:01:47,169 : Computing embeddings for train/dev/test
2019-02-16 08:03:09,416 : Computed embeddings
2019-02-16 08:03:09,416 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:03:59,704 : [('reg:1e-05', 57.94), ('reg:0.0001', 55.66), ('reg:0.001', 43.62), ('reg:0.01', 36.78)]
2019-02-16 08:03:59,704 : Validation : best param found is reg = 1e-05 with score             57.94
2019-02-16 08:03:59,704 : Evaluating...
2019-02-16 08:04:14,333 : 
Dev acc : 57.9 Test acc : 57.9 for TOPCONSTITUENTS classification

2019-02-16 08:04:14,334 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 08:04:14,928 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 08:04:15,001 : loading BERT model bert-base-uncased
2019-02-16 08:04:15,001 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:04:15,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:04:15,056 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf2m5u5p3
2019-02-16 08:04:17,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:04:19,013 : Computing embeddings for train/dev/test
2019-02-16 08:05:43,141 : Computed embeddings
2019-02-16 08:05:43,141 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:06:44,600 : [('reg:1e-05', 82.94), ('reg:0.0001', 82.9), ('reg:0.001', 83.25), ('reg:0.01', 80.8)]
2019-02-16 08:06:44,601 : Validation : best param found is reg = 0.001 with score             83.25
2019-02-16 08:06:44,601 : Evaluating...
2019-02-16 08:06:59,006 : 
Dev acc : 83.2 Test acc : 82.4 for BIGRAMSHIFT classification

2019-02-16 08:06:59,007 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 08:06:59,449 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 08:06:59,518 : loading BERT model bert-base-uncased
2019-02-16 08:06:59,519 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:06:59,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:06:59,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpleyv2ont
2019-02-16 08:07:02,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:07:03,487 : Computing embeddings for train/dev/test
2019-02-16 08:08:26,507 : Computed embeddings
2019-02-16 08:08:26,507 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:09:11,976 : [('reg:1e-05', 87.43), ('reg:0.0001', 87.37), ('reg:0.001', 87.41), ('reg:0.01', 87.37)]
2019-02-16 08:09:11,977 : Validation : best param found is reg = 1e-05 with score             87.43
2019-02-16 08:09:11,977 : Evaluating...
2019-02-16 08:09:24,201 : 
Dev acc : 87.4 Test acc : 86.2 for TENSE classification

2019-02-16 08:09:24,202 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 08:09:24,638 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 08:09:24,704 : loading BERT model bert-base-uncased
2019-02-16 08:09:24,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:09:24,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:09:24,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuvg6l9xx
2019-02-16 08:09:27,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:09:28,754 : Computing embeddings for train/dev/test
2019-02-16 08:10:55,642 : Computed embeddings
2019-02-16 08:10:55,642 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:11:47,322 : [('reg:1e-05', 79.97), ('reg:0.0001', 79.98), ('reg:0.001', 80.02), ('reg:0.01', 79.27)]
2019-02-16 08:11:47,323 : Validation : best param found is reg = 0.001 with score             80.02
2019-02-16 08:11:47,323 : Evaluating...
2019-02-16 08:12:00,446 : 
Dev acc : 80.0 Test acc : 79.4 for SUBJNUMBER classification

2019-02-16 08:12:00,447 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 08:12:00,910 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 08:12:00,982 : loading BERT model bert-base-uncased
2019-02-16 08:12:00,983 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:12:01,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:12:01,116 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6t5pqzge
2019-02-16 08:12:03,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:12:05,099 : Computing embeddings for train/dev/test
2019-02-16 08:13:31,026 : Computed embeddings
2019-02-16 08:13:31,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:14:22,063 : [('reg:1e-05', 76.58), ('reg:0.0001', 76.56), ('reg:0.001', 76.36), ('reg:0.01', 74.99)]
2019-02-16 08:14:22,063 : Validation : best param found is reg = 1e-05 with score             76.58
2019-02-16 08:14:22,063 : Evaluating...
2019-02-16 08:14:35,214 : 
Dev acc : 76.6 Test acc : 77.3 for OBJNUMBER classification

2019-02-16 08:14:35,215 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 08:14:35,635 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 08:14:35,711 : loading BERT model bert-base-uncased
2019-02-16 08:14:35,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:14:35,857 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:14:35,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpokfevyeb
2019-02-16 08:14:38,350 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:14:39,819 : Computing embeddings for train/dev/test
2019-02-16 08:16:17,599 : Computed embeddings
2019-02-16 08:16:17,599 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:16:57,420 : [('reg:1e-05', 56.47), ('reg:0.0001', 56.48), ('reg:0.001', 56.37), ('reg:0.01', 56.44)]
2019-02-16 08:16:57,420 : Validation : best param found is reg = 0.0001 with score             56.48
2019-02-16 08:16:57,420 : Evaluating...
2019-02-16 08:17:07,088 : 
Dev acc : 56.5 Test acc : 56.7 for ODDMANOUT classification

2019-02-16 08:17:07,089 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 08:17:07,748 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 08:17:07,833 : loading BERT model bert-base-uncased
2019-02-16 08:17:07,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:17:07,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:17:07,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnomzhaie
2019-02-16 08:17:10,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:17:11,851 : Computing embeddings for train/dev/test
2019-02-16 08:18:52,612 : Computed embeddings
2019-02-16 08:18:52,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:19:46,851 : [('reg:1e-05', 58.4), ('reg:0.0001', 58.29), ('reg:0.001', 57.41), ('reg:0.01', 55.08)]
2019-02-16 08:19:46,851 : Validation : best param found is reg = 1e-05 with score             58.4
2019-02-16 08:19:46,852 : Evaluating...
2019-02-16 08:20:07,152 : 
Dev acc : 58.4 Test acc : 57.4 for COORDINATIONINVERSION classification

2019-02-16 08:20:07,154 : total results: {'STS12': {'MSRpar': {'pearson': (0.31517962043380005, 9.274245985630103e-19), 'spearman': SpearmanrResult(correlation=0.35335482006504165, pvalue=1.7771813092395498e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.666022682741378, 2.6268758548865865e-97), 'spearman': SpearmanrResult(correlation=0.675760943288564, pvalue=3.5752723552564956e-101), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48905904327298066, 5.655951867635274e-29), 'spearman': SpearmanrResult(correlation=0.596543180327218, pvalue=1.398999641079036e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5581871937714659, 1.190310186904133e-62), 'spearman': SpearmanrResult(correlation=0.58460323320039, pvalue=5.920516996823856e-70), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5278132445700195, 5.327884022339016e-30), 'spearman': SpearmanrResult(correlation=0.4567146830166179, pvalue=5.908528657942639e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5112523569579288, 'wmean': 0.5114600090591116}, 'spearman': {'mean': 0.5333953719795663, 'wmean': 0.5361427688897427}}}, 'STS13': {'FNWN': {'pearson': (0.17628146696661798, 0.015248407716798664), 'spearman': SpearmanrResult(correlation=0.18385473165859867, pvalue=0.011327110494333856), 'nsamples': 189}, 'headlines': {'pearson': (0.6502119846052229, 2.4768845706869196e-91), 'spearman': SpearmanrResult(correlation=0.6435148626523652, pvalue=6.581182261830661e-89), 'nsamples': 750}, 'OnWN': {'pearson': (0.5236854073124746, 7.898132167816414e-41), 'spearman': SpearmanrResult(correlation=0.5225876340722075, pvalue=1.2313329200352398e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4500596196281051, 'wmean': 0.5431757994752708}, 'spearman': {'mean': 0.44998574279439046, 'wmean': 0.5403709026581717}}}, 'STS14': {'deft-forum': {'pearson': (0.39049257781677976, 7.661544465881755e-18), 'spearman': SpearmanrResult(correlation=0.4032201042040241, pvalue=5.0609955789961e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.718190490714152, 7.514422746929178e-49), 'spearman': SpearmanrResult(correlation=0.6926240212078038, pvalue=3.4733388372130707e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5974912505536703, 9.108397311231084e-74), 'spearman': SpearmanrResult(correlation=0.5697585278060356, pvalue=9.074702675085512e-66), 'nsamples': 750}, 'images': {'pearson': (0.594532470989574, 7.080587539077131e-73), 'spearman': SpearmanrResult(correlation=0.5881876545655778, pvalue=5.3589630419650775e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.627995217099557, 1.619643800656171e-83), 'spearman': SpearmanrResult(correlation=0.6582527861811847, pvalue=2.5161465966249743e-94), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5752404490761874, 2.7358645574348726e-67), 'spearman': SpearmanrResult(correlation=0.5471522079198559, pvalue=8.647526391818863e-60), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5839904093749867, 'wmean': 0.5833662261389435}, 'spearman': {'mean': 0.5765325503140803, 'wmean': 0.576466569495638}}}, 'STS15': {'answers-forums': {'pearson': (0.5000142411763683, 4.0812405923192728e-25), 'spearman': SpearmanrResult(correlation=0.46925023893679624, pvalue=6.24727918348043e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.671125651924859, 2.5820351951778227e-99), 'spearman': SpearmanrResult(correlation=0.6775773055627093, pvalue=6.541119019823477e-102), 'nsamples': 750}, 'belief': {'pearson': (0.5407949207950868, 7.222081814308047e-30), 'spearman': SpearmanrResult(correlation=0.5791611987437087, pvalue=5.686663981276436e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6492550986991714, 5.547013176252168e-91), 'spearman': SpearmanrResult(correlation=0.6551934739891613, pvalue=3.551196322065464e-93), 'nsamples': 750}, 'images': {'pearson': (0.7322802894807683, 6.241807260873921e-127), 'spearman': SpearmanrResult(correlation=0.7411027875984493, pvalue=1.4875081756685134e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6186940404152509, 'wmean': 0.6432664052726316}, 'spearman': {'mean': 0.624457000966165, 'wmean': 0.6495198214976431}}}, 'STS16': {'answer-answer': {'pearson': (0.4628567438561703, 6.856204659572955e-15), 'spearman': SpearmanrResult(correlation=0.4826169844622393, pvalue=3.174893646993659e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.6366892607275633, 1.0420635168267824e-29), 'spearman': SpearmanrResult(correlation=0.6429002045507601, pvalue=1.9591321244232668e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6862352028140658, 2.3199413209641038e-33), 'spearman': SpearmanrResult(correlation=0.6994336381984178, pvalue=4.127458448838043e-35), 'nsamples': 230}, 'postediting': {'pearson': (0.7654234813380592, 3.1446940402690925e-48), 'spearman': SpearmanrResult(correlation=0.8168159608142607, pvalue=9.607331237620113e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.33982046087234086, 4.806703303128953e-07), 'spearman': SpearmanrResult(correlation=0.3648131502171608, pvalue=5.6120053696216537e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5782050299216399, 'wmean': 0.5832387363209707}, 'spearman': {'mean': 0.6013159876485676, 'wmean': 0.6063116733610879}}}, 'MR': {'devacc': 61.39, 'acc': 65.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.0, 'acc': 66.68, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.89, 'acc': 85.9, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.6, 'acc': 90.55, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.7, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.15, 'acc': 38.55, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.78, 'acc': 76.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.84, 'acc': 72.52, 'f1': 81.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.4, 'acc': 78.16, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8044908751785099, 'pearson': 0.7908200536926443, 'spearman': 0.7219444348371298, 'mse': 0.3820328401020975, 'yhat': array([2.55963588, 4.11010255, 1.26987492, ..., 3.32369615, 4.11696814,        4.02663421]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6478805515062751, 'pearson': 0.6420220664289261, 'spearman': 0.6402174377560115, 'mse': 1.5042491503707691, 'yhat': array([1.44988881, 1.5051401 , 2.25148232, ..., 3.980443  , 4.38074323,        3.91794949]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 55.9, 'acc': 55.93, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 338.476, 'acc': [(30.439999999999998, 64.64, 79.24000000000001, 3.0), (25.943999999999996, 60.16399999999999, 76.092, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 67.24, 'acc': 66.52, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 33.8, 'acc': 34.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.22, 'acc': 29.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.94, 'acc': 57.93, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.25, 'acc': 82.38, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.43, 'acc': 86.21, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.02, 'acc': 79.4, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.58, 'acc': 77.34, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.48, 'acc': 56.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.4, 'acc': 57.44, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 08:20:07,154 : STS12 p=0.5115, STS12 s=0.5361, STS13 p=0.5432, STS13 s=0.5404, STS14 p=0.5834, STS14 s=0.5765, STS15 p=0.6433, STS15 s=0.6495, STS 16 p=0.5832, STS16 s=0.6063, STS B p=0.6420, STS B s=0.6402, STS B m=1.5042, SICK-R p=0.7908, SICK-R s=0.7219, SICK-P m=0.3820
2019-02-16 08:20:07,154 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 08:20:07,154 : 0.5115,0.5361,0.5432,0.5404,0.5834,0.5765,0.6433,0.6495,0.5832,0.6063,0.6420,0.6402,1.5042,0.7908,0.7219,0.3820
2019-02-16 08:20:07,155 : MR=65.13, CR=66.68, SUBJ=90.55, MPQA=85.90, SST-B=77.38, SST-F=38.55, TREC=76.80, SICK-E=78.16, SNLI=55.93, MRPC=72.52, MRPC f=81.67
2019-02-16 08:20:07,155 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 08:20:07,155 : 65.13,66.68,90.55,85.90,77.38,38.55,76.80,78.16,55.93,72.52,81.67
2019-02-16 08:20:07,155 : COCO r1i2t=30.44, COCO r5i2t=64.64, COCO r10i2t=79.24, COCO medr_i2t=3.00, COCO r1t2i=25.94, COCO r5t2i=60.16, COCO r10t2i=76.09, COCO medr_t2i=4.00
2019-02-16 08:20:07,155 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 08:20:07,155 : 30.44,64.64,79.24,3.00,25.94,60.16,76.09,4.00
2019-02-16 08:20:07,155 : SentLen=66.52, WC=34.31, TreeDepth=29.96, TopConst=57.93, BShift=82.38, Tense=86.21, SubjNum=79.40, ObjNum=77.34, SOMO=56.69, CoordInv=57.44, average=62.82
2019-02-16 08:20:07,155 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 08:20:07,155 : 66.52,34.31,29.96,57.93,82.38,86.21,79.40,77.34,56.69,57.44,62.82
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,155 : layer 6
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,155 : ********************************************************************************
2019-02-16 08:20:07,248 : ***** Transfer task : STS12 *****


2019-02-16 08:20:07,260 : loading BERT model bert-base-uncased
2019-02-16 08:20:07,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:07,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:07,279 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl_cwqnv3
2019-02-16 08:20:09,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:13,068 : MSRpar : pearson = 0.2980, spearman = 0.3358
2019-02-16 08:20:13,851 : MSRvid : pearson = 0.6319, spearman = 0.6432
2019-02-16 08:20:14,538 : SMTeuroparl : pearson = 0.4782, spearman = 0.5977
2019-02-16 08:20:15,721 : surprise.OnWN : pearson = 0.5490, spearman = 0.5763
2019-02-16 08:20:16,362 : surprise.SMTnews : pearson = 0.5509, spearman = 0.4631
2019-02-16 08:20:16,363 : ALL (weighted average) : Pearson = 0.4982,             Spearman = 0.5230
2019-02-16 08:20:16,363 : ALL (average) : Pearson = 0.5016,             Spearman = 0.5232

2019-02-16 08:20:16,363 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 08:20:16,371 : loading BERT model bert-base-uncased
2019-02-16 08:20:16,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:16,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:16,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaeiyc86u
2019-02-16 08:20:18,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:20,954 : FNWN : pearson = 0.1588, spearman = 0.1697
2019-02-16 08:20:21,852 : headlines : pearson = 0.6383, spearman = 0.6341
2019-02-16 08:20:22,531 : OnWN : pearson = 0.4911, spearman = 0.4920
2019-02-16 08:20:22,532 : ALL (weighted average) : Pearson = 0.5228,             Spearman = 0.5224
2019-02-16 08:20:22,532 : ALL (average) : Pearson = 0.4294,             Spearman = 0.4319

2019-02-16 08:20:22,532 : ***** Transfer task : STS14 *****


2019-02-16 08:20:22,550 : loading BERT model bert-base-uncased
2019-02-16 08:20:22,550 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:22,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:22,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkxubylbn
2019-02-16 08:20:25,139 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:27,251 : deft-forum : pearson = 0.3768, spearman = 0.3839
2019-02-16 08:20:27,968 : deft-news : pearson = 0.7247, spearman = 0.7009
2019-02-16 08:20:28,944 : headlines : pearson = 0.5853, spearman = 0.5592
2019-02-16 08:20:29,917 : images : pearson = 0.5667, spearman = 0.5654
2019-02-16 08:20:30,883 : OnWN : pearson = 0.6138, spearman = 0.6459
2019-02-16 08:20:32,153 : tweet-news : pearson = 0.5713, spearman = 0.5449
2019-02-16 08:20:32,153 : ALL (weighted average) : Pearson = 0.5706,             Spearman = 0.5652
2019-02-16 08:20:32,154 : ALL (average) : Pearson = 0.5731,             Spearman = 0.5667

2019-02-16 08:20:32,154 : ***** Transfer task : STS15 *****


2019-02-16 08:20:32,203 : loading BERT model bert-base-uncased
2019-02-16 08:20:32,203 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:32,222 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:32,223 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_j664v__
2019-02-16 08:20:34,752 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:37,100 : answers-forums : pearson = 0.4950, spearman = 0.4625
2019-02-16 08:20:38,022 : answers-students : pearson = 0.6638, spearman = 0.6721
2019-02-16 08:20:38,880 : belief : pearson = 0.5286, spearman = 0.5663
2019-02-16 08:20:39,998 : headlines : pearson = 0.6309, spearman = 0.6355
2019-02-16 08:20:41,067 : images : pearson = 0.7009, spearman = 0.7111
2019-02-16 08:20:41,067 : ALL (weighted average) : Pearson = 0.6268,             Spearman = 0.6333
2019-02-16 08:20:41,067 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6095

2019-02-16 08:20:41,067 : ***** Transfer task : STS16 *****


2019-02-16 08:20:41,138 : loading BERT model bert-base-uncased
2019-02-16 08:20:41,138 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:41,156 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:41,156 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwim4g2ni
2019-02-16 08:20:43,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:45,544 : answer-answer : pearson = 0.4941, spearman = 0.5171
2019-02-16 08:20:45,889 : headlines : pearson = 0.6267, spearman = 0.6334
2019-02-16 08:20:46,324 : plagiarism : pearson = 0.6797, spearman = 0.6808
2019-02-16 08:20:47,011 : postediting : pearson = 0.7637, spearman = 0.8170
2019-02-16 08:20:47,323 : question-question : pearson = 0.2585, spearman = 0.2875
2019-02-16 08:20:47,324 : ALL (weighted average) : Pearson = 0.5719,             Spearman = 0.5945
2019-02-16 08:20:47,324 : ALL (average) : Pearson = 0.5645,             Spearman = 0.5872

2019-02-16 08:20:47,324 : ***** Transfer task : MR *****


2019-02-16 08:20:47,340 : loading BERT model bert-base-uncased
2019-02-16 08:20:47,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:20:47,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:20:47,361 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbeis1w30
2019-02-16 08:20:49,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:20:51,317 : Generating sentence embeddings
2019-02-16 08:21:04,721 : Generated sentence embeddings
2019-02-16 08:21:04,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 08:21:25,315 : Best param found at split 1: l2reg = 0.0001                 with score 66.87
2019-02-16 08:21:45,934 : Best param found at split 2: l2reg = 1e-05                 with score 62.81
2019-02-16 08:22:07,527 : Best param found at split 3: l2reg = 0.0001                 with score 62.17
2019-02-16 08:22:25,418 : Best param found at split 4: l2reg = 1e-05                 with score 60.09
2019-02-16 08:22:44,961 : Best param found at split 5: l2reg = 0.0001                 with score 61.78
2019-02-16 08:22:46,409 : Dev acc : 62.74 Test acc : 65.62

2019-02-16 08:22:46,411 : ***** Transfer task : CR *****


2019-02-16 08:22:46,426 : loading BERT model bert-base-uncased
2019-02-16 08:22:46,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:22:46,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:22:46,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8plx698c
2019-02-16 08:22:48,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:22:50,378 : Generating sentence embeddings
2019-02-16 08:22:54,107 : Generated sentence embeddings
2019-02-16 08:22:54,108 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 08:22:58,362 : Best param found at split 1: l2reg = 0.0001                 with score 70.82
2019-02-16 08:23:03,952 : Best param found at split 2: l2reg = 0.0001                 with score 72.24
2019-02-16 08:23:09,757 : Best param found at split 3: l2reg = 0.0001                 with score 68.11
2019-02-16 08:23:15,056 : Best param found at split 4: l2reg = 0.01                 with score 70.74
2019-02-16 08:23:21,590 : Best param found at split 5: l2reg = 0.001                 with score 72.72
2019-02-16 08:23:21,823 : Dev acc : 70.93 Test acc : 68.0

2019-02-16 08:23:21,824 : ***** Transfer task : MPQA *****


2019-02-16 08:23:21,829 : loading BERT model bert-base-uncased
2019-02-16 08:23:21,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:23:21,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:23:21,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzlrs3sdz
2019-02-16 08:23:24,355 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:23:25,861 : Generating sentence embeddings
2019-02-16 08:23:30,021 : Generated sentence embeddings
2019-02-16 08:23:30,021 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 08:23:47,969 : Best param found at split 1: l2reg = 0.001                 with score 87.8
2019-02-16 08:24:06,574 : Best param found at split 2: l2reg = 0.001                 with score 87.33
2019-02-16 08:24:25,881 : Best param found at split 3: l2reg = 0.001                 with score 87.58
2019-02-16 08:24:45,218 : Best param found at split 4: l2reg = 1e-05                 with score 87.11
2019-02-16 08:25:06,390 : Best param found at split 5: l2reg = 0.0001                 with score 86.94
2019-02-16 08:25:07,409 : Dev acc : 87.35 Test acc : 87.33

2019-02-16 08:25:07,410 : ***** Transfer task : SUBJ *****


2019-02-16 08:25:07,426 : loading BERT model bert-base-uncased
2019-02-16 08:25:07,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:25:07,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:25:07,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphv4ljrer
2019-02-16 08:25:09,929 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:25:11,433 : Generating sentence embeddings
2019-02-16 08:25:24,640 : Generated sentence embeddings
2019-02-16 08:25:24,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 08:25:41,967 : Best param found at split 1: l2reg = 1e-05                 with score 91.4
2019-02-16 08:26:01,105 : Best param found at split 2: l2reg = 0.01                 with score 91.35
2019-02-16 08:26:18,418 : Best param found at split 3: l2reg = 0.01                 with score 91.2
2019-02-16 08:26:37,044 : Best param found at split 4: l2reg = 0.01                 with score 91.4
2019-02-16 08:26:56,492 : Best param found at split 5: l2reg = 0.0001                 with score 91.59
2019-02-16 08:26:57,956 : Dev acc : 91.39 Test acc : 90.06

2019-02-16 08:26:57,957 : ***** Transfer task : SST Binary classification *****


2019-02-16 08:26:58,091 : loading BERT model bert-base-uncased
2019-02-16 08:26:58,092 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:26:58,119 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:26:58,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyy8rm6ka
2019-02-16 08:27:00,638 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:27:02,126 : Computing embedding for train
2019-02-16 08:27:47,980 : Computed train embeddings
2019-02-16 08:27:47,980 : Computing embedding for dev
2019-02-16 08:27:48,915 : Computed dev embeddings
2019-02-16 08:27:48,915 : Computing embedding for test
2019-02-16 08:27:50,855 : Computed test embeddings
2019-02-16 08:27:50,855 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:28:14,485 : [('reg:1e-05', 80.05), ('reg:0.0001', 80.16), ('reg:0.001', 79.47), ('reg:0.01', 79.59)]
2019-02-16 08:28:14,485 : Validation : best param found is reg = 0.0001 with score             80.16
2019-02-16 08:28:14,485 : Evaluating...
2019-02-16 08:28:20,951 : 
Dev acc : 80.16 Test acc : 77.81 for             SST Binary classification

2019-02-16 08:28:20,951 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 08:28:21,020 : loading BERT model bert-base-uncased
2019-02-16 08:28:21,020 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:28:21,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:28:21,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprmlrxs1w
2019-02-16 08:28:23,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:28:25,064 : Computing embedding for train
2019-02-16 08:28:34,776 : Computed train embeddings
2019-02-16 08:28:34,776 : Computing embedding for dev
2019-02-16 08:28:36,131 : Computed dev embeddings
2019-02-16 08:28:36,131 : Computing embedding for test
2019-02-16 08:28:38,816 : Computed test embeddings
2019-02-16 08:28:38,817 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:28:42,568 : [('reg:1e-05', 39.33), ('reg:0.0001', 29.79), ('reg:0.001', 35.51), ('reg:0.01', 25.34)]
2019-02-16 08:28:42,568 : Validation : best param found is reg = 1e-05 with score             39.33
2019-02-16 08:28:42,568 : Evaluating...
2019-02-16 08:28:43,906 : 
Dev acc : 39.33 Test acc : 36.7 for             SST Fine-Grained classification

2019-02-16 08:28:43,907 : ***** Transfer task : TREC *****


2019-02-16 08:28:43,926 : loading BERT model bert-base-uncased
2019-02-16 08:28:43,926 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:28:43,946 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:28:43,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc19o6pdd
2019-02-16 08:28:46,466 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:28:51,308 : Computed train embeddings
2019-02-16 08:28:51,567 : Computed test embeddings
2019-02-16 08:28:51,567 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 08:29:04,135 : [('reg:1e-05', 62.73), ('reg:0.0001', 69.43), ('reg:0.001', 70.16), ('reg:0.01', 63.57)]
2019-02-16 08:29:04,135 : Cross-validation : best param found is reg = 0.001             with score 70.16
2019-02-16 08:29:04,135 : Evaluating...
2019-02-16 08:29:04,612 : 
Dev acc : 70.16 Test acc : 81.0             for TREC

2019-02-16 08:29:04,612 : ***** Transfer task : MRPC *****


2019-02-16 08:29:04,635 : loading BERT model bert-base-uncased
2019-02-16 08:29:04,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:29:04,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:29:04,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7b1z64ei
2019-02-16 08:29:07,150 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:29:08,666 : Computing embedding for train
2019-02-16 08:29:18,396 : Computed train embeddings
2019-02-16 08:29:18,397 : Computing embedding for test
2019-02-16 08:29:22,636 : Computed test embeddings
2019-02-16 08:29:22,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 08:29:29,876 : [('reg:1e-05', 70.49), ('reg:0.0001', 72.65), ('reg:0.001', 71.47), ('reg:0.01', 70.85)]
2019-02-16 08:29:29,877 : Cross-validation : best param found is reg = 0.0001             with score 72.65
2019-02-16 08:29:29,877 : Evaluating...
2019-02-16 08:29:30,597 : Dev acc : 72.65 Test acc 71.48; Test F1 81.78 for MRPC.

2019-02-16 08:29:30,597 : ***** Transfer task : SICK-Entailment*****


2019-02-16 08:29:30,669 : loading BERT model bert-base-uncased
2019-02-16 08:29:30,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:29:30,692 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:29:30,692 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwh7a44tj
2019-02-16 08:29:33,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:29:34,641 : Computing embedding for train
2019-02-16 08:29:40,280 : Computed train embeddings
2019-02-16 08:29:40,280 : Computing embedding for dev
2019-02-16 08:29:41,036 : Computed dev embeddings
2019-02-16 08:29:41,036 : Computing embedding for test
2019-02-16 08:29:47,148 : Computed test embeddings
2019-02-16 08:29:47,176 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:29:48,608 : [('reg:1e-05', 75.4), ('reg:0.0001', 78.2), ('reg:0.001', 74.8), ('reg:0.01', 76.8)]
2019-02-16 08:29:48,609 : Validation : best param found is reg = 0.0001 with score             78.2
2019-02-16 08:29:48,609 : Evaluating...
2019-02-16 08:29:49,352 : 
Dev acc : 78.2 Test acc : 74.91 for                        SICK entailment

2019-02-16 08:29:49,353 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 08:29:49,383 : loading BERT model bert-base-uncased
2019-02-16 08:29:49,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:29:49,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:29:49,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppugm934w
2019-02-16 08:29:51,933 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:29:53,340 : Computing embedding for train
2019-02-16 08:29:58,506 : Computed train embeddings
2019-02-16 08:29:58,506 : Computing embedding for dev
2019-02-16 08:29:59,198 : Computed dev embeddings
2019-02-16 08:29:59,198 : Computing embedding for test
2019-02-16 08:30:04,702 : Computed test embeddings
2019-02-16 08:30:29,325 : Dev : Pearson 0.8016525987092108
2019-02-16 08:30:29,325 : Test : Pearson 0.776170577689326 Spearman 0.7123677296437668 MSE 0.405752075275095                        for SICK Relatedness

2019-02-16 08:30:29,326 : 

***** Transfer task : STSBenchmark*****


2019-02-16 08:30:29,395 : loading BERT model bert-base-uncased
2019-02-16 08:30:29,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:30:29,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:30:29,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxfdpt663
2019-02-16 08:30:31,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:30:33,353 : Computing embedding for train
2019-02-16 08:30:41,894 : Computed train embeddings
2019-02-16 08:30:41,895 : Computing embedding for dev
2019-02-16 08:30:44,356 : Computed dev embeddings
2019-02-16 08:30:44,356 : Computing embedding for test
2019-02-16 08:30:46,343 : Computed test embeddings
2019-02-16 08:31:14,954 : Dev : Pearson 0.6419749514192992
2019-02-16 08:31:14,954 : Test : Pearson 0.6270375233457395 Spearman 0.6247146932789917 MSE 1.4881659884311775                        for SICK Relatedness

2019-02-16 08:31:14,954 : ***** Transfer task : SNLI Entailment*****


2019-02-16 08:31:20,026 : loading BERT model bert-base-uncased
2019-02-16 08:31:20,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:31:20,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:31:20,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1clpmy_4
2019-02-16 08:31:22,637 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:31:24,312 : PROGRESS (encoding): 0.00%
2019-02-16 08:32:43,817 : PROGRESS (encoding): 14.56%
2019-02-16 08:34:10,764 : PROGRESS (encoding): 29.12%
2019-02-16 08:35:38,938 : PROGRESS (encoding): 43.69%
2019-02-16 08:37:17,892 : PROGRESS (encoding): 58.25%
2019-02-16 08:39:01,835 : PROGRESS (encoding): 72.81%
2019-02-16 08:40:46,928 : PROGRESS (encoding): 87.37%
2019-02-16 08:42:37,715 : PROGRESS (encoding): 0.00%
2019-02-16 08:42:51,115 : PROGRESS (encoding): 0.00%
2019-02-16 08:43:04,147 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 08:43:46,498 : [('reg:1e-09', 58.24)]
2019-02-16 08:43:46,499 : Validation : best param found is reg = 1e-09 with score             58.24
2019-02-16 08:43:46,499 : Evaluating...
2019-02-16 08:44:29,297 : Dev acc : 58.24 Test acc : 58.71 for SNLI

2019-02-16 08:44:29,297 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 08:44:38,240 : loading BERT model bert-base-uncased
2019-02-16 08:44:38,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 08:44:38,293 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 08:44:38,293 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwdn1tuqf
2019-02-16 08:44:40,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 08:44:42,226 : Computing embedding for train
2019-02-16 08:52:13,400 : Computed train embeddings
2019-02-16 08:52:13,400 : Computing embedding for dev
2019-02-16 08:52:33,186 : Computed dev embeddings
2019-02-16 08:52:33,186 : Computing embedding for test
2019-02-16 08:52:54,641 : Computed test embeddings
2019-02-16 08:52:54,667 : prepare data
2019-02-16 08:52:54,737 : start epoch
2019-02-16 08:53:41,189 : samples : 64000
2019-02-16 08:53:51,691 : Image to text: 5.9, 19.06, 29.1, 29.0
2019-02-16 08:53:59,195 : Text to Image: 4.692, 15.912, 25.324, 34.0
2019-02-16 08:54:41,456 : samples : 128000
2019-02-16 08:54:51,924 : Image to text: 7.58, 22.88, 34.26, 23.0
2019-02-16 08:54:59,469 : Text to Image: 6.456, 20.04, 30.328, 27.0
2019-02-16 08:55:41,976 : samples : 192000
2019-02-16 08:55:52,484 : Image to text: 6.74, 21.44, 32.24, 24.0
2019-02-16 08:56:00,075 : Text to Image: 5.792, 18.372, 27.864, 30.0
2019-02-16 08:56:42,499 : samples : 256000
2019-02-16 08:56:52,971 : Image to text: 8.18, 23.58, 35.52, 20.0
2019-02-16 08:57:00,546 : Text to Image: 6.7, 20.568, 30.66, 26.0
2019-02-16 08:57:42,829 : samples : 320000
2019-02-16 08:57:53,310 : Image to text: 7.82, 24.46, 35.8, 20.0
2019-02-16 08:58:00,799 : Text to Image: 7.432, 22.512, 33.328, 23.0
2019-02-16 08:58:43,477 : samples : 384000
2019-02-16 08:58:54,074 : Image to text: 8.3, 24.88, 36.74, 20.0
2019-02-16 08:59:01,613 : Text to Image: 6.448, 20.504, 30.848, 26.0
2019-02-16 08:59:44,157 : samples : 448000
2019-02-16 08:59:54,669 : Image to text: 7.8, 22.6, 34.0, 21.0
2019-02-16 09:00:02,199 : Text to Image: 6.148, 19.7, 30.088, 26.0
2019-02-16 09:00:44,795 : samples : 512000
2019-02-16 09:00:55,316 : Image to text: 7.84, 25.72, 36.88, 20.0
2019-02-16 09:01:02,877 : Text to Image: 7.136, 21.968, 32.84, 24.0
2019-02-16 09:01:39,429 : Epoch 1 finished
2019-02-16 09:01:39,876 : Image to text: 25.1, 56.6, 71.6, 4.0
2019-02-16 09:01:40,217 : Text to Image: 19.2, 49.94, 67.88, 6.0
2019-02-16 09:01:40,650 : Image to text: 24.7, 54.7, 69.9, 5.0
2019-02-16 09:01:40,987 : Text to Image: 20.36, 49.42, 67.6, 6.0
2019-02-16 09:01:41,425 : Image to text: 23.1, 55.8, 71.4, 4.0
2019-02-16 09:01:41,761 : Text to Image: 19.76, 49.66, 66.84, 6.0
2019-02-16 09:01:42,199 : Image to text: 22.7, 56.9, 72.3, 4.0
2019-02-16 09:01:42,516 : Text to Image: 19.56, 50.92, 67.8, 5.0
2019-02-16 09:01:42,950 : Image to text: 24.7, 56.8, 72.3, 4.0
2019-02-16 09:01:43,292 : Text to Image: 18.9, 51.3, 67.78, 5.0
2019-02-16 09:01:43,292 : Dev mean Text to Image: 19.556, 50.248, 67.58, 5.6
2019-02-16 09:01:43,292 : Dev mean Image to text: 24.060000000000002, 56.16, 71.5, 4.2
2019-02-16 09:01:43,293 : start epoch
2019-02-16 09:02:26,058 : samples : 64000
2019-02-16 09:02:36,626 : Image to text: 9.4, 27.02, 39.14, 18.0
2019-02-16 09:02:44,149 : Text to Image: 7.232, 22.38, 33.632, 23.0
2019-02-16 09:03:27,024 : samples : 128000
2019-02-16 09:03:37,519 : Image to text: 8.24, 25.78, 36.94, 19.0
2019-02-16 09:03:45,055 : Text to Image: 7.22, 22.584, 33.644, 22.0
2019-02-16 09:04:27,540 : samples : 192000
2019-02-16 09:04:38,029 : Image to text: 8.74, 25.72, 38.32, 17.0
2019-02-16 09:04:45,576 : Text to Image: 7.972, 23.796, 35.024, 21.0
2019-02-16 09:05:28,389 : samples : 256000
2019-02-16 09:05:38,883 : Image to text: 9.1, 27.48, 40.12, 17.0
2019-02-16 09:05:46,424 : Text to Image: 8.088, 24.244, 35.556, 20.0
2019-02-16 09:06:29,451 : samples : 320000
2019-02-16 09:06:39,927 : Image to text: 9.12, 27.9, 40.92, 16.0
2019-02-16 09:06:47,477 : Text to Image: 7.856, 23.716, 35.036, 21.0
2019-02-16 09:07:29,734 : samples : 384000
2019-02-16 09:07:40,216 : Image to text: 9.38, 27.16, 39.14, 18.0
2019-02-16 09:07:47,781 : Text to Image: 7.684, 23.28, 34.748, 21.0
2019-02-16 09:08:31,018 : samples : 448000
2019-02-16 09:08:41,593 : Image to text: 8.74, 26.66, 39.38, 17.0
2019-02-16 09:08:49,189 : Text to Image: 7.692, 23.336, 34.588, 22.0
2019-02-16 09:09:33,036 : samples : 512000
2019-02-16 09:09:45,575 : Image to text: 9.8, 28.24, 40.42, 16.0
2019-02-16 09:09:54,447 : Text to Image: 8.5, 24.892, 36.252, 20.0
2019-02-16 09:10:33,125 : Epoch 2 finished
2019-02-16 09:10:33,558 : Image to text: 27.0, 58.6, 74.3, 4.0
2019-02-16 09:10:33,885 : Text to Image: 22.02, 53.64, 71.0, 5.0
2019-02-16 09:10:34,313 : Image to text: 26.1, 56.2, 71.9, 4.0
2019-02-16 09:10:34,643 : Text to Image: 21.52, 53.48, 71.0, 5.0
2019-02-16 09:10:35,099 : Image to text: 26.3, 57.2, 73.9, 4.0
2019-02-16 09:10:35,438 : Text to Image: 21.62, 52.86, 70.16, 5.0
2019-02-16 09:10:35,887 : Image to text: 24.4, 60.0, 75.9, 4.0
2019-02-16 09:10:36,225 : Text to Image: 21.58, 54.58, 71.04, 5.0
2019-02-16 09:10:36,669 : Image to text: 27.5, 57.8, 73.4, 4.0
2019-02-16 09:10:37,005 : Text to Image: 22.22, 54.14, 69.96, 5.0
2019-02-16 09:10:37,005 : Dev mean Text to Image: 21.791999999999998, 53.739999999999995, 70.632, 5.0
2019-02-16 09:10:37,005 : Dev mean Image to text: 26.26, 57.96000000000001, 73.88000000000001, 4.0
2019-02-16 09:10:37,005 : start epoch
2019-02-16 09:11:19,346 : samples : 64000
2019-02-16 09:11:29,847 : Image to text: 9.56, 28.68, 41.4, 16.0
2019-02-16 09:11:37,389 : Text to Image: 8.616, 25.252, 36.98, 19.0
2019-02-16 09:12:20,707 : samples : 128000
2019-02-16 09:12:31,265 : Image to text: 7.62, 25.28, 37.34, 19.0
2019-02-16 09:12:38,810 : Text to Image: 7.196, 22.488, 33.652, 23.0
2019-02-16 09:13:21,879 : samples : 192000
2019-02-16 09:13:32,404 : Image to text: 9.62, 28.2, 41.06, 16.0
2019-02-16 09:13:39,928 : Text to Image: 7.928, 24.244, 35.608, 20.0
2019-02-16 09:14:22,955 : samples : 256000
2019-02-16 09:14:33,524 : Image to text: 9.48, 28.7, 40.96, 16.0
2019-02-16 09:14:41,095 : Text to Image: 8.004, 24.44, 35.872, 20.0
2019-02-16 09:15:23,908 : samples : 320000
2019-02-16 09:15:34,456 : Image to text: 9.84, 29.36, 41.04, 16.0
2019-02-16 09:15:41,982 : Text to Image: 8.696, 25.084, 36.616, 19.0
2019-02-16 09:16:24,310 : samples : 384000
2019-02-16 09:16:34,775 : Image to text: 10.12, 29.66, 41.86, 16.0
2019-02-16 09:16:42,391 : Text to Image: 8.836, 24.988, 36.444, 20.0
2019-02-16 09:17:25,387 : samples : 448000
2019-02-16 09:17:38,013 : Image to text: 9.88, 29.28, 41.26, 15.0
2019-02-16 09:17:48,101 : Text to Image: 8.932, 25.76, 37.2, 19.0
2019-02-16 09:18:33,746 : samples : 512000
2019-02-16 09:18:46,355 : Image to text: 9.68, 28.26, 41.46, 16.0
2019-02-16 09:18:54,271 : Text to Image: 9.012, 26.108, 37.76, 19.0
2019-02-16 09:19:30,459 : Epoch 3 finished
2019-02-16 09:19:30,944 : Image to text: 26.2, 58.9, 72.8, 4.0
2019-02-16 09:19:31,314 : Text to Image: 21.92, 54.8, 71.64, 5.0
2019-02-16 09:19:31,785 : Image to text: 25.2, 58.3, 73.5, 4.0
2019-02-16 09:19:32,158 : Text to Image: 21.68, 53.6, 70.44, 5.0
2019-02-16 09:19:32,623 : Image to text: 24.9, 60.1, 74.5, 4.0
2019-02-16 09:19:32,994 : Text to Image: 21.02, 52.98, 70.48, 5.0
2019-02-16 09:19:33,444 : Image to text: 25.4, 60.7, 76.3, 4.0
2019-02-16 09:19:33,818 : Text to Image: 21.62, 53.36, 71.46, 5.0
2019-02-16 09:19:34,279 : Image to text: 27.5, 59.8, 74.0, 4.0
2019-02-16 09:19:34,663 : Text to Image: 22.12, 53.94, 70.54, 5.0
2019-02-16 09:19:34,663 : Dev mean Text to Image: 21.671999999999997, 53.73599999999999, 70.912, 5.0
2019-02-16 09:19:34,663 : Dev mean Image to text: 25.840000000000003, 59.559999999999995, 74.22, 4.0
2019-02-16 09:19:34,663 : start epoch
2019-02-16 09:20:18,248 : samples : 64000
2019-02-16 09:20:30,798 : Image to text: 10.36, 28.82, 41.44, 16.0
2019-02-16 09:20:40,851 : Text to Image: 8.384, 25.168, 36.944, 19.0
2019-02-16 09:21:26,458 : samples : 128000
2019-02-16 09:21:36,679 : Image to text: 9.66, 29.86, 42.18, 16.0
2019-02-16 09:21:44,041 : Text to Image: 8.664, 24.696, 36.852, 20.0
2019-02-16 09:22:25,731 : samples : 192000
2019-02-16 09:22:35,989 : Image to text: 9.78, 30.02, 42.44, 15.0
2019-02-16 09:22:42,956 : Text to Image: 8.004, 24.156, 35.752, 20.0
2019-02-16 09:23:27,323 : samples : 256000
2019-02-16 09:23:39,994 : Image to text: 9.94, 28.42, 40.98, 16.0
2019-02-16 09:23:50,113 : Text to Image: 8.636, 25.144, 36.764, 19.0
2019-02-16 09:24:33,586 : samples : 320000
2019-02-16 09:24:43,897 : Image to text: 10.36, 30.32, 42.82, 15.0
2019-02-16 09:24:51,294 : Text to Image: 9.304, 26.48, 38.584, 18.0
2019-02-16 09:25:35,204 : samples : 384000
2019-02-16 09:25:47,884 : Image to text: 9.94, 29.42, 42.42, 15.0
2019-02-16 09:25:57,950 : Text to Image: 8.756, 25.52, 37.36, 19.0
2019-02-16 09:26:47,933 : samples : 448000
2019-02-16 09:26:59,654 : Image to text: 10.44, 30.28, 42.58, 15.0
2019-02-16 09:27:07,657 : Text to Image: 9.404, 26.724, 38.344, 18.0
2019-02-16 09:27:52,029 : samples : 512000
2019-02-16 09:28:04,650 : Image to text: 10.38, 30.0, 42.9, 15.0
2019-02-16 09:28:14,670 : Text to Image: 9.18, 25.908, 37.396, 19.0
2019-02-16 09:28:51,844 : Epoch 4 finished
2019-02-16 09:28:52,316 : Image to text: 26.9, 61.0, 76.2, 4.0
2019-02-16 09:28:52,682 : Text to Image: 22.22, 55.52, 74.06, 4.0
2019-02-16 09:28:53,141 : Image to text: 28.7, 58.5, 72.5, 4.0
2019-02-16 09:28:53,516 : Text to Image: 22.34, 55.06, 72.14, 4.0
2019-02-16 09:28:53,974 : Image to text: 26.9, 60.1, 74.2, 4.0
2019-02-16 09:28:54,359 : Text to Image: 23.56, 56.32, 72.72, 4.0
2019-02-16 09:28:54,818 : Image to text: 28.1, 62.9, 76.1, 3.0
2019-02-16 09:28:55,192 : Text to Image: 22.82, 55.82, 72.68, 4.0
2019-02-16 09:28:55,652 : Image to text: 28.4, 62.0, 76.0, 3.0
2019-02-16 09:28:56,028 : Text to Image: 22.78, 56.14, 72.54, 4.0
2019-02-16 09:28:56,028 : Dev mean Text to Image: 22.744, 55.772, 72.828, 4.0
2019-02-16 09:28:56,028 : Dev mean Image to text: 27.8, 60.9, 75.0, 3.6000000000000005
2019-02-16 09:28:56,028 : start epoch
2019-02-16 09:29:38,415 : samples : 64000
2019-02-16 09:29:49,496 : Image to text: 10.4, 29.0, 41.78, 15.0
2019-02-16 09:29:59,488 : Text to Image: 8.92, 26.324, 38.156, 18.0
2019-02-16 09:30:44,427 : samples : 128000
2019-02-16 09:30:57,042 : Image to text: 10.32, 30.16, 43.22, 14.0
2019-02-16 09:31:06,936 : Text to Image: 9.216, 26.768, 38.84, 18.0
2019-02-16 09:31:49,737 : samples : 192000
2019-02-16 09:31:59,993 : Image to text: 10.08, 29.76, 42.1, 15.0
2019-02-16 09:32:07,455 : Text to Image: 9.172, 26.276, 37.988, 18.0
2019-02-16 09:32:52,235 : samples : 256000
2019-02-16 09:33:04,838 : Image to text: 9.66, 28.88, 41.22, 16.0
2019-02-16 09:33:14,922 : Text to Image: 8.524, 25.68, 37.568, 19.0
2019-02-16 09:33:58,450 : samples : 320000
2019-02-16 09:34:08,716 : Image to text: 10.3, 29.76, 41.9, 15.0
2019-02-16 09:34:16,134 : Text to Image: 8.592, 25.484, 37.168, 19.0
2019-02-16 09:34:59,922 : samples : 384000
2019-02-16 09:35:12,535 : Image to text: 10.66, 30.78, 43.38, 14.0
2019-02-16 09:35:22,603 : Text to Image: 9.888, 27.412, 39.636, 17.0
2019-02-16 09:36:07,537 : samples : 448000
2019-02-16 09:36:17,792 : Image to text: 10.64, 30.52, 43.32, 14.0
2019-02-16 09:36:25,201 : Text to Image: 9.244, 26.12, 38.512, 18.0
2019-02-16 09:37:07,698 : samples : 512000
2019-02-16 09:37:20,313 : Image to text: 10.1, 30.18, 42.44, 15.0
2019-02-16 09:37:30,355 : Text to Image: 9.444, 26.584, 38.832, 17.0
2019-02-16 09:38:09,466 : Epoch 5 finished
2019-02-16 09:38:10,411 : Image to text: 27.8, 60.8, 76.2, 3.0
2019-02-16 09:38:11,209 : Text to Image: 22.84, 55.7, 73.36, 4.0
2019-02-16 09:38:12,193 : Image to text: 25.9, 59.0, 73.3, 4.0
2019-02-16 09:38:13,033 : Text to Image: 23.2, 55.4, 72.86, 5.0
2019-02-16 09:38:13,994 : Image to text: 27.7, 61.7, 75.5, 4.0
2019-02-16 09:38:14,742 : Text to Image: 23.6, 55.86, 72.28, 4.0
2019-02-16 09:38:15,707 : Image to text: 26.3, 64.3, 79.0, 3.0
2019-02-16 09:38:16,488 : Text to Image: 23.84, 57.32, 74.04, 4.0
2019-02-16 09:38:17,471 : Image to text: 29.3, 61.5, 75.9, 3.0
2019-02-16 09:38:18,303 : Text to Image: 22.66, 56.56, 72.3, 4.0
2019-02-16 09:38:18,303 : Dev mean Text to Image: 23.227999999999998, 56.16799999999999, 72.968, 4.2
2019-02-16 09:38:18,303 : Dev mean Image to text: 27.4, 61.459999999999994, 75.98, 3.4000000000000004
2019-02-16 09:38:18,303 : start epoch
2019-02-16 09:39:02,130 : samples : 64000
2019-02-16 09:39:12,393 : Image to text: 10.8, 30.86, 43.16, 14.0
2019-02-16 09:39:19,768 : Text to Image: 9.22, 26.636, 38.416, 18.0
2019-02-16 09:40:03,218 : samples : 128000
2019-02-16 09:40:15,849 : Image to text: 10.62, 31.06, 43.46, 14.0
2019-02-16 09:40:25,934 : Text to Image: 9.316, 26.532, 38.216, 18.0
2019-02-16 09:41:11,331 : samples : 192000
2019-02-16 09:41:21,536 : Image to text: 10.52, 30.74, 44.2, 14.0
2019-02-16 09:41:28,971 : Text to Image: 9.352, 26.488, 38.456, 18.0
2019-02-16 09:42:11,549 : samples : 256000
2019-02-16 09:42:22,786 : Image to text: 11.16, 30.68, 43.78, 14.0
2019-02-16 09:42:30,782 : Text to Image: 9.344, 26.664, 38.528, 18.0
2019-02-16 09:43:13,777 : samples : 320000
2019-02-16 09:43:24,961 : Image to text: 10.64, 31.12, 43.12, 14.0
2019-02-16 09:43:33,547 : Text to Image: 9.436, 26.664, 38.58, 18.0
2019-02-16 09:44:22,252 : samples : 384000
2019-02-16 09:44:32,458 : Image to text: 9.8, 30.86, 44.66, 13.0
2019-02-16 09:44:39,895 : Text to Image: 9.7, 27.176, 39.152, 17.0
2019-02-16 09:45:23,095 : samples : 448000
2019-02-16 09:45:33,442 : Image to text: 10.84, 31.32, 43.42, 14.0
2019-02-16 09:45:40,797 : Text to Image: 9.496, 27.088, 39.096, 17.0
2019-02-16 09:46:23,267 : samples : 512000
2019-02-16 09:46:35,779 : Image to text: 10.56, 30.22, 42.96, 15.0
2019-02-16 09:46:44,027 : Text to Image: 9.148, 26.688, 38.76, 17.0
2019-02-16 09:47:20,414 : Epoch 6 finished
2019-02-16 09:47:20,877 : Image to text: 29.1, 63.4, 78.8, 3.0
2019-02-16 09:47:21,243 : Text to Image: 23.38, 57.48, 75.76, 4.0
2019-02-16 09:47:21,705 : Image to text: 30.2, 60.6, 74.6, 3.0
2019-02-16 09:47:22,076 : Text to Image: 24.22, 57.02, 73.8, 4.0
2019-02-16 09:47:22,542 : Image to text: 28.5, 62.7, 78.5, 3.0
2019-02-16 09:47:22,908 : Text to Image: 24.42, 57.78, 74.64, 4.0
2019-02-16 09:47:23,357 : Image to text: 29.2, 64.4, 78.1, 3.0
2019-02-16 09:47:23,718 : Text to Image: 24.5, 57.74, 74.36, 4.0
2019-02-16 09:47:24,167 : Image to text: 29.2, 64.4, 77.4, 3.0
2019-02-16 09:47:24,530 : Text to Image: 24.58, 57.98, 73.48, 4.0
2019-02-16 09:47:24,530 : Dev mean Text to Image: 24.220000000000002, 57.60000000000001, 74.408, 4.0
2019-02-16 09:47:24,530 : Dev mean Image to text: 29.24, 63.10000000000001, 77.47999999999999, 3.0
2019-02-16 09:47:24,530 : start epoch
2019-02-16 09:48:07,081 : samples : 64000
2019-02-16 09:48:18,740 : Image to text: 10.86, 32.04, 45.3, 13.0
2019-02-16 09:48:28,880 : Text to Image: 9.728, 27.612, 39.952, 17.0
2019-02-16 09:49:13,638 : samples : 128000
2019-02-16 09:49:23,965 : Image to text: 10.66, 31.14, 43.62, 14.0
2019-02-16 09:49:31,084 : Text to Image: 9.692, 27.4, 39.356, 17.0
2019-02-16 09:50:14,627 : samples : 192000
2019-02-16 09:50:27,394 : Image to text: 10.84, 31.18, 43.64, 14.0
2019-02-16 09:50:35,064 : Text to Image: 9.592, 27.276, 39.476, 17.0
2019-02-16 09:51:18,842 : samples : 256000
2019-02-16 09:51:29,094 : Image to text: 10.72, 31.2, 43.68, 14.0
2019-02-16 09:51:36,201 : Text to Image: 9.736, 27.692, 40.092, 17.0
2019-02-16 09:52:19,438 : samples : 320000
2019-02-16 09:52:29,909 : Image to text: 10.28, 31.02, 43.42, 14.0
2019-02-16 09:52:39,874 : Text to Image: 9.964, 27.78, 39.864, 17.0
2019-02-16 09:53:25,024 : samples : 384000
2019-02-16 09:53:37,594 : Image to text: 10.96, 31.48, 43.74, 14.0
2019-02-16 09:53:47,619 : Text to Image: 9.92, 27.876, 40.292, 17.0
2019-02-16 09:54:32,734 : samples : 448000
2019-02-16 09:54:45,308 : Image to text: 11.4, 31.44, 44.72, 14.0
2019-02-16 09:54:55,253 : Text to Image: 9.936, 28.216, 40.676, 16.0
2019-02-16 09:55:40,399 : samples : 512000
2019-02-16 09:55:52,981 : Image to text: 11.28, 32.66, 45.3, 13.0
2019-02-16 09:56:02,951 : Text to Image: 10.288, 28.668, 40.928, 16.0
2019-02-16 09:56:41,392 : Epoch 7 finished
2019-02-16 09:56:42,275 : Image to text: 27.2, 61.1, 76.7, 4.0
2019-02-16 09:56:43,074 : Text to Image: 22.02, 56.72, 74.38, 4.0
2019-02-16 09:56:43,969 : Image to text: 27.1, 58.6, 75.0, 4.0
2019-02-16 09:56:44,763 : Text to Image: 23.74, 57.08, 73.5, 4.0
2019-02-16 09:56:45,688 : Image to text: 26.5, 61.7, 76.2, 4.0
2019-02-16 09:56:46,482 : Text to Image: 23.42, 57.06, 73.84, 4.0
2019-02-16 09:56:47,398 : Image to text: 28.0, 64.4, 78.6, 3.0
2019-02-16 09:56:48,188 : Text to Image: 23.66, 57.9, 73.98, 4.0
2019-02-16 09:56:49,114 : Image to text: 28.3, 60.5, 75.8, 4.0
2019-02-16 09:56:49,874 : Text to Image: 23.5, 56.1, 72.7, 4.0
2019-02-16 09:56:49,874 : Dev mean Text to Image: 23.267999999999997, 56.971999999999994, 73.68, 4.0
2019-02-16 09:56:49,874 : Dev mean Image to text: 27.419999999999998, 61.260000000000005, 76.46, 3.8000000000000007
2019-02-16 09:56:49,874 : start epoch
2019-02-16 09:57:35,044 : samples : 64000
2019-02-16 09:57:47,648 : Image to text: 11.16, 32.22, 44.74, 14.0
2019-02-16 09:57:57,643 : Text to Image: 9.84, 27.54, 39.996, 17.0
2019-02-16 09:58:41,951 : samples : 128000
2019-02-16 09:58:54,579 : Image to text: 12.24, 32.2, 45.4, 13.0
2019-02-16 09:59:04,585 : Text to Image: 9.836, 27.52, 39.892, 17.0
2019-02-16 09:59:48,534 : samples : 192000
2019-02-16 10:00:01,133 : Image to text: 11.1, 31.84, 44.42, 14.0
2019-02-16 10:00:11,038 : Text to Image: 9.464, 27.736, 40.204, 16.0
2019-02-16 10:01:04,577 : samples : 256000
2019-02-16 10:01:17,219 : Image to text: 10.68, 30.62, 43.9, 14.0
2019-02-16 10:01:27,236 : Text to Image: 9.56, 27.744, 39.84, 17.0
2019-02-16 10:02:11,846 : samples : 320000
2019-02-16 10:02:24,466 : Image to text: 11.54, 32.42, 45.26, 13.0
2019-02-16 10:02:34,453 : Text to Image: 10.252, 28.5, 40.988, 16.0
2019-02-16 10:03:18,876 : samples : 384000
2019-02-16 10:03:31,578 : Image to text: 11.6, 32.62, 45.54, 13.0
2019-02-16 10:03:40,614 : Text to Image: 10.232, 28.224, 40.648, 16.0
2019-02-16 10:04:24,421 : samples : 448000
2019-02-16 10:04:34,739 : Image to text: 11.46, 32.16, 45.46, 13.0
2019-02-16 10:04:42,163 : Text to Image: 9.892, 27.484, 40.044, 17.0
2019-02-16 10:05:24,643 : samples : 512000
2019-02-16 10:05:34,922 : Image to text: 10.82, 31.74, 44.28, 14.0
2019-02-16 10:05:42,262 : Text to Image: 9.744, 27.476, 39.684, 17.0
2019-02-16 10:06:18,518 : Epoch 8 finished
2019-02-16 10:06:18,997 : Image to text: 27.7, 63.5, 77.7, 3.0
2019-02-16 10:06:19,357 : Text to Image: 22.78, 58.04, 74.9, 4.0
2019-02-16 10:06:19,821 : Image to text: 29.4, 60.3, 74.6, 3.0
2019-02-16 10:06:20,184 : Text to Image: 23.68, 56.78, 73.16, 4.0
2019-02-16 10:06:20,649 : Image to text: 26.2, 62.5, 77.2, 3.0
2019-02-16 10:06:21,010 : Text to Image: 24.28, 57.54, 74.2, 4.0
2019-02-16 10:06:21,479 : Image to text: 28.7, 62.7, 77.4, 3.0
2019-02-16 10:06:21,843 : Text to Image: 24.1, 58.84, 74.86, 4.0
2019-02-16 10:06:22,302 : Image to text: 30.6, 62.0, 77.5, 3.0
2019-02-16 10:06:22,654 : Text to Image: 24.14, 57.18, 73.72, 4.0
2019-02-16 10:06:22,654 : Dev mean Text to Image: 23.796, 57.675999999999995, 74.168, 4.0
2019-02-16 10:06:22,654 : Dev mean Image to text: 28.52, 62.199999999999996, 76.88000000000001, 3.0
2019-02-16 10:06:22,654 : start epoch
2019-02-16 10:07:05,381 : samples : 64000
2019-02-16 10:07:18,256 : Image to text: 11.24, 32.06, 45.14, 13.0
2019-02-16 10:07:28,727 : Text to Image: 9.78, 27.556, 39.872, 17.0
2019-02-16 10:08:14,545 : samples : 128000
2019-02-16 10:08:27,467 : Image to text: 11.58, 31.54, 45.3, 13.0
2019-02-16 10:08:37,821 : Text to Image: 9.436, 26.708, 38.876, 17.0
2019-02-16 10:09:23,643 : samples : 192000
2019-02-16 10:09:36,537 : Image to text: 10.88, 31.96, 45.38, 13.0
2019-02-16 10:09:46,951 : Text to Image: 10.384, 28.584, 40.508, 16.0
2019-02-16 10:10:33,069 : samples : 256000
2019-02-16 10:10:45,947 : Image to text: 10.92, 31.56, 44.66, 13.0
2019-02-16 10:10:56,438 : Text to Image: 9.432, 27.088, 39.156, 17.0
2019-02-16 10:11:42,784 : samples : 320000
2019-02-16 10:11:55,672 : Image to text: 11.26, 31.84, 44.36, 14.0
2019-02-16 10:12:06,122 : Text to Image: 9.936, 27.62, 39.728, 17.0
2019-02-16 10:12:51,934 : samples : 384000
2019-02-16 10:13:04,768 : Image to text: 11.32, 31.14, 44.34, 14.0
2019-02-16 10:13:15,187 : Text to Image: 9.776, 27.808, 40.492, 17.0
2019-02-16 10:14:01,151 : samples : 448000
2019-02-16 10:14:14,062 : Image to text: 11.54, 31.7, 44.68, 13.0
2019-02-16 10:14:23,105 : Text to Image: 9.76, 27.92, 39.708, 17.0
2019-02-16 10:15:08,408 : samples : 512000
2019-02-16 10:15:18,901 : Image to text: 11.04, 31.56, 44.18, 14.0
2019-02-16 10:15:26,489 : Text to Image: 9.952, 27.924, 40.264, 17.0
2019-02-16 10:16:03,334 : Epoch 9 finished
2019-02-16 10:16:03,770 : Image to text: 29.4, 63.1, 78.0, 3.0
2019-02-16 10:16:04,102 : Text to Image: 24.5, 59.34, 76.44, 4.0
2019-02-16 10:16:04,548 : Image to text: 28.0, 62.4, 76.4, 3.0
2019-02-16 10:16:04,890 : Text to Image: 25.26, 58.3, 74.54, 4.0
2019-02-16 10:16:05,330 : Image to text: 28.9, 63.3, 77.9, 3.0
2019-02-16 10:16:05,674 : Text to Image: 25.4, 59.4, 75.72, 4.0
2019-02-16 10:16:06,131 : Image to text: 29.9, 66.7, 79.4, 3.0
2019-02-16 10:16:06,477 : Text to Image: 25.36, 59.9, 75.68, 4.0
2019-02-16 10:16:06,927 : Image to text: 29.8, 63.2, 76.9, 3.0
2019-02-16 10:16:07,265 : Text to Image: 25.9, 59.0, 74.46, 4.0
2019-02-16 10:16:07,265 : Dev mean Text to Image: 25.284000000000002, 59.188, 75.36800000000001, 4.0
2019-02-16 10:16:07,265 : Dev mean Image to text: 29.2, 63.74000000000001, 77.72000000000001, 3.0
2019-02-16 10:16:07,266 : start epoch
2019-02-16 10:16:50,073 : samples : 64000
2019-02-16 10:17:00,080 : Image to text: 11.72, 32.04, 45.68, 13.0
2019-02-16 10:17:08,272 : Text to Image: 9.776, 27.412, 39.892, 17.0
2019-02-16 10:17:59,701 : samples : 128000
2019-02-16 10:18:09,996 : Image to text: 12.34, 32.72, 45.36, 13.0
2019-02-16 10:18:17,493 : Text to Image: 10.3, 28.516, 41.024, 16.0
2019-02-16 10:18:59,915 : samples : 192000
2019-02-16 10:19:10,181 : Image to text: 11.66, 32.38, 45.82, 13.0
2019-02-16 10:19:17,708 : Text to Image: 10.036, 28.256, 40.428, 16.0
2019-02-16 10:20:00,237 : samples : 256000
2019-02-16 10:20:10,602 : Image to text: 11.58, 32.06, 45.0, 13.0
2019-02-16 10:20:18,169 : Text to Image: 10.06, 28.148, 40.752, 16.0
2019-02-16 10:21:00,484 : samples : 320000
2019-02-16 10:21:10,807 : Image to text: 11.54, 32.1, 44.82, 14.0
2019-02-16 10:21:18,359 : Text to Image: 9.928, 27.752, 40.316, 17.0
2019-02-16 10:22:00,405 : samples : 384000
2019-02-16 10:22:10,674 : Image to text: 11.5, 33.72, 46.92, 12.0
2019-02-16 10:22:18,135 : Text to Image: 10.128, 28.376, 40.928, 16.0
2019-02-16 10:23:01,085 : samples : 448000
2019-02-16 10:23:11,452 : Image to text: 12.14, 32.18, 45.68, 13.0
2019-02-16 10:23:18,956 : Text to Image: 10.44, 28.788, 41.368, 16.0
2019-02-16 10:24:01,957 : samples : 512000
2019-02-16 10:24:12,496 : Image to text: 11.58, 31.98, 46.04, 13.0
2019-02-16 10:24:20,077 : Text to Image: 10.024, 28.2, 40.844, 16.0
2019-02-16 10:24:56,012 : Epoch 10 finished
2019-02-16 10:24:56,464 : Image to text: 28.6, 65.2, 79.1, 3.0
2019-02-16 10:24:56,805 : Text to Image: 25.0, 58.82, 76.82, 4.0
2019-02-16 10:24:57,252 : Image to text: 29.2, 61.4, 76.1, 4.0
2019-02-16 10:24:57,590 : Text to Image: 24.98, 58.18, 74.9, 4.0
2019-02-16 10:24:58,057 : Image to text: 29.6, 64.0, 78.6, 3.0
2019-02-16 10:24:58,381 : Text to Image: 24.42, 58.62, 74.86, 4.0
2019-02-16 10:24:58,847 : Image to text: 30.8, 65.2, 79.6, 3.0
2019-02-16 10:24:59,183 : Text to Image: 25.34, 59.28, 75.38, 4.0
2019-02-16 10:24:59,631 : Image to text: 30.4, 63.0, 78.4, 3.0
2019-02-16 10:24:59,971 : Text to Image: 24.52, 58.1, 74.62, 4.0
2019-02-16 10:24:59,971 : Dev mean Text to Image: 24.852, 58.599999999999994, 75.316, 4.0
2019-02-16 10:24:59,972 : Dev mean Image to text: 29.72, 63.760000000000005, 78.36, 3.2
2019-02-16 10:24:59,972 : start epoch
2019-02-16 10:25:44,218 : samples : 64000
2019-02-16 10:25:54,498 : Image to text: 12.14, 33.24, 46.62, 13.0
2019-02-16 10:26:01,957 : Text to Image: 10.38, 28.788, 41.456, 16.0
2019-02-16 10:26:45,355 : samples : 128000
2019-02-16 10:26:55,633 : Image to text: 11.36, 32.76, 46.3, 13.0
2019-02-16 10:27:03,157 : Text to Image: 9.764, 27.852, 40.452, 16.0
2019-02-16 10:27:45,742 : samples : 192000
2019-02-16 10:27:56,087 : Image to text: 11.46, 33.48, 46.08, 13.0
2019-02-16 10:28:03,546 : Text to Image: 10.168, 28.232, 40.5, 16.0
2019-02-16 10:28:46,327 : samples : 256000
2019-02-16 10:28:56,540 : Image to text: 11.44, 32.48, 45.72, 13.0
2019-02-16 10:29:04,040 : Text to Image: 10.264, 28.42, 40.58, 16.0
2019-02-16 10:29:47,490 : samples : 320000
2019-02-16 10:29:57,769 : Image to text: 11.7, 32.88, 46.04, 13.0
2019-02-16 10:30:05,291 : Text to Image: 10.2, 28.712, 40.98, 16.0
2019-02-16 10:30:47,760 : samples : 384000
2019-02-16 10:30:58,033 : Image to text: 11.34, 32.9, 45.3, 13.0
2019-02-16 10:31:05,508 : Text to Image: 10.624, 28.94, 41.616, 15.0
2019-02-16 10:31:47,781 : samples : 448000
2019-02-16 10:31:58,018 : Image to text: 11.62, 32.56, 45.56, 13.0
2019-02-16 10:32:05,491 : Text to Image: 10.444, 28.784, 40.956, 16.0
2019-02-16 10:32:48,561 : samples : 512000
2019-02-16 10:32:59,108 : Image to text: 11.66, 32.66, 45.84, 13.0
2019-02-16 10:33:06,639 : Text to Image: 10.064, 28.22, 40.748, 16.0
2019-02-16 10:33:42,639 : Epoch 11 finished
2019-02-16 10:33:43,065 : Image to text: 28.9, 64.2, 78.4, 3.0
2019-02-16 10:33:43,364 : Text to Image: 23.82, 58.74, 76.24, 4.0
2019-02-16 10:33:43,806 : Image to text: 29.0, 60.7, 75.1, 3.0
2019-02-16 10:33:44,148 : Text to Image: 24.34, 56.98, 74.16, 4.0
2019-02-16 10:33:44,574 : Image to text: 27.1, 61.6, 78.0, 4.0
2019-02-16 10:33:44,894 : Text to Image: 24.0, 58.32, 74.96, 4.0
2019-02-16 10:33:45,315 : Image to text: 31.6, 63.4, 78.4, 3.0
2019-02-16 10:33:45,643 : Text to Image: 23.96, 58.84, 74.98, 4.0
2019-02-16 10:33:46,060 : Image to text: 31.0, 62.8, 76.6, 3.0
2019-02-16 10:33:46,380 : Text to Image: 24.24, 57.64, 73.86, 4.0
2019-02-16 10:33:46,380 : Dev mean Text to Image: 24.072000000000003, 58.104, 74.84, 4.0
2019-02-16 10:33:46,380 : Dev mean Image to text: 29.52, 62.53999999999999, 77.3, 3.2
2019-02-16 10:33:46,380 : start epoch
2019-02-16 10:34:39,552 : samples : 64000
2019-02-16 10:34:50,012 : Image to text: 11.54, 32.58, 45.34, 13.0
2019-02-16 10:34:57,494 : Text to Image: 10.548, 28.848, 41.288, 16.0
2019-02-16 10:35:40,434 : samples : 128000
2019-02-16 10:35:50,730 : Image to text: 11.0, 32.48, 45.88, 13.0
2019-02-16 10:35:58,205 : Text to Image: 9.704, 27.576, 40.048, 17.0
2019-02-16 10:36:41,201 : samples : 192000
2019-02-16 10:36:51,405 : Image to text: 11.4, 32.24, 45.6, 13.0
2019-02-16 10:36:58,840 : Text to Image: 10.34, 28.832, 41.604, 16.0
2019-02-16 10:37:42,534 : samples : 256000
2019-02-16 10:37:52,812 : Image to text: 11.18, 33.12, 45.64, 13.0
2019-02-16 10:38:00,278 : Text to Image: 10.056, 28.616, 41.256, 16.0
2019-02-16 10:38:42,726 : samples : 320000
2019-02-16 10:38:53,035 : Image to text: 11.36, 31.24, 43.92, 14.0
2019-02-16 10:39:00,514 : Text to Image: 9.66, 27.956, 40.188, 16.0
2019-02-16 10:39:43,471 : samples : 384000
2019-02-16 10:39:53,709 : Image to text: 11.84, 33.12, 45.72, 13.0
2019-02-16 10:40:01,199 : Text to Image: 10.44, 28.932, 41.524, 16.0
2019-02-16 10:40:44,288 : samples : 448000
2019-02-16 10:40:54,519 : Image to text: 11.72, 32.9, 46.16, 13.0
2019-02-16 10:41:01,998 : Text to Image: 10.284, 28.42, 40.888, 16.0
2019-02-16 10:41:44,675 : samples : 512000
2019-02-16 10:41:55,258 : Image to text: 11.66, 32.32, 45.36, 13.0
2019-02-16 10:42:02,794 : Text to Image: 10.384, 28.908, 41.756, 15.0
2019-02-16 10:42:39,071 : Epoch 12 finished
2019-02-16 10:42:39,502 : Image to text: 29.3, 63.3, 78.5, 3.0
2019-02-16 10:42:39,819 : Text to Image: 23.96, 59.24, 76.48, 4.0
2019-02-16 10:42:40,269 : Image to text: 28.7, 62.7, 77.4, 3.0
2019-02-16 10:42:40,603 : Text to Image: 24.56, 57.96, 74.48, 4.0
2019-02-16 10:42:41,053 : Image to text: 30.1, 63.4, 77.0, 3.0
2019-02-16 10:42:41,403 : Text to Image: 25.28, 58.96, 75.68, 4.0
2019-02-16 10:42:41,857 : Image to text: 30.1, 66.2, 78.8, 3.0
2019-02-16 10:42:42,196 : Text to Image: 25.0, 60.06, 76.72, 4.0
2019-02-16 10:42:42,643 : Image to text: 30.3, 63.7, 78.3, 3.0
2019-02-16 10:42:42,983 : Text to Image: 25.0, 58.54, 74.66, 4.0
2019-02-16 10:42:42,983 : Dev mean Text to Image: 24.76, 58.952, 75.604, 4.0
2019-02-16 10:42:42,983 : Dev mean Image to text: 29.700000000000003, 63.86000000000001, 78.0, 3.0
2019-02-16 10:42:42,984 : start epoch
2019-02-16 10:43:25,870 : samples : 64000
2019-02-16 10:43:36,103 : Image to text: 11.66, 33.04, 46.0, 13.0
2019-02-16 10:43:43,552 : Text to Image: 10.204, 28.788, 41.216, 16.0
2019-02-16 10:44:26,006 : samples : 128000
2019-02-16 10:44:36,242 : Image to text: 11.64, 33.24, 46.16, 13.0
2019-02-16 10:44:43,698 : Text to Image: 9.996, 28.492, 40.852, 16.0
2019-02-16 10:45:27,415 : samples : 192000
2019-02-16 10:45:37,760 : Image to text: 12.26, 33.24, 46.2, 13.0
2019-02-16 10:45:45,181 : Text to Image: 10.3, 28.872, 41.632, 15.0
2019-02-16 10:46:27,678 : samples : 256000
2019-02-16 10:46:37,934 : Image to text: 12.64, 33.94, 46.82, 12.0
2019-02-16 10:46:45,423 : Text to Image: 10.604, 29.148, 41.72, 15.0
2019-02-16 10:47:27,381 : samples : 320000
2019-02-16 10:47:37,644 : Image to text: 11.74, 32.94, 45.62, 13.0
2019-02-16 10:47:45,080 : Text to Image: 10.056, 28.608, 41.052, 16.0
2019-02-16 10:48:27,698 : samples : 384000
2019-02-16 10:48:37,970 : Image to text: 11.48, 33.18, 45.72, 13.0
2019-02-16 10:48:45,448 : Text to Image: 9.896, 28.128, 40.744, 16.0
2019-02-16 10:49:28,850 : samples : 448000
2019-02-16 10:49:39,121 : Image to text: 12.02, 33.12, 46.38, 12.0
2019-02-16 10:49:46,539 : Text to Image: 10.416, 28.956, 41.38, 16.0
2019-02-16 10:50:29,591 : samples : 512000
2019-02-16 10:50:39,923 : Image to text: 11.84, 33.2, 46.3, 13.0
2019-02-16 10:50:47,275 : Text to Image: 10.54, 28.788, 41.42, 16.0
2019-02-16 10:51:34,228 : Epoch 13 finished
2019-02-16 10:51:34,665 : Image to text: 29.8, 62.8, 79.0, 3.0
2019-02-16 10:51:35,005 : Text to Image: 24.76, 60.2, 76.56, 4.0
2019-02-16 10:51:35,433 : Image to text: 31.1, 63.2, 76.6, 3.0
2019-02-16 10:51:35,760 : Text to Image: 25.12, 58.84, 75.46, 4.0
2019-02-16 10:51:36,190 : Image to text: 29.1, 63.9, 77.9, 3.0
2019-02-16 10:51:36,529 : Text to Image: 25.94, 59.48, 75.7, 4.0
2019-02-16 10:51:36,988 : Image to text: 30.4, 67.9, 78.9, 3.0
2019-02-16 10:51:37,326 : Text to Image: 25.68, 60.04, 76.5, 4.0
2019-02-16 10:51:37,767 : Image to text: 29.6, 63.4, 77.7, 3.0
2019-02-16 10:51:38,107 : Text to Image: 25.48, 58.78, 74.76, 4.0
2019-02-16 10:51:38,107 : Dev mean Text to Image: 25.396, 59.468, 75.79599999999999, 4.0
2019-02-16 10:51:38,107 : Dev mean Image to text: 30.0, 64.24000000000001, 78.02000000000001, 3.0
2019-02-16 10:51:38,108 : start epoch
2019-02-16 10:52:20,840 : samples : 64000
2019-02-16 10:52:31,207 : Image to text: 11.88, 33.48, 46.34, 12.0
2019-02-16 10:52:38,770 : Text to Image: 10.588, 29.116, 41.744, 15.0
2019-02-16 10:53:20,979 : samples : 128000
2019-02-16 10:53:31,269 : Image to text: 12.06, 33.3, 45.98, 13.0
2019-02-16 10:53:38,797 : Text to Image: 10.36, 28.856, 41.276, 16.0
2019-02-16 10:54:21,035 : samples : 192000
2019-02-16 10:54:31,324 : Image to text: 12.56, 33.5, 46.62, 12.0
2019-02-16 10:54:38,793 : Text to Image: 10.416, 28.9, 41.216, 16.0
2019-02-16 10:55:21,058 : samples : 256000
2019-02-16 10:55:31,368 : Image to text: 11.38, 32.56, 45.86, 13.0
2019-02-16 10:55:38,874 : Text to Image: 10.336, 28.808, 41.4, 16.0
2019-02-16 10:56:21,968 : samples : 320000
2019-02-16 10:56:32,274 : Image to text: 12.08, 34.12, 46.64, 13.0
2019-02-16 10:56:39,752 : Text to Image: 10.216, 29.024, 41.48, 15.0
2019-02-16 10:57:22,750 : samples : 384000
2019-02-16 10:57:33,072 : Image to text: 12.08, 33.74, 46.52, 13.0
2019-02-16 10:57:40,575 : Text to Image: 10.54, 29.012, 41.448, 16.0
2019-02-16 10:58:23,437 : samples : 448000
2019-02-16 10:58:33,766 : Image to text: 11.8, 33.34, 46.52, 12.0
2019-02-16 10:58:41,320 : Text to Image: 10.508, 28.868, 41.368, 16.0
2019-02-16 10:59:23,561 : samples : 512000
2019-02-16 10:59:34,048 : Image to text: 12.38, 34.36, 46.96, 13.0
2019-02-16 10:59:41,580 : Text to Image: 10.924, 29.536, 42.34, 15.0
2019-02-16 11:00:17,895 : Epoch 14 finished
2019-02-16 11:00:18,351 : Image to text: 29.4, 63.5, 79.4, 3.0
2019-02-16 11:00:18,690 : Text to Image: 24.92, 60.1, 77.24, 4.0
2019-02-16 11:00:19,139 : Image to text: 29.4, 63.0, 77.7, 3.0
2019-02-16 11:00:19,475 : Text to Image: 25.4, 59.16, 75.86, 4.0
2019-02-16 11:00:19,926 : Image to text: 29.6, 65.5, 79.5, 3.0
2019-02-16 11:00:20,277 : Text to Image: 25.5, 60.0, 76.4, 4.0
2019-02-16 11:00:20,725 : Image to text: 29.3, 67.5, 79.5, 3.0
2019-02-16 11:00:21,063 : Text to Image: 25.52, 61.18, 76.46, 4.0
2019-02-16 11:00:21,508 : Image to text: 32.4, 64.1, 78.0, 3.0
2019-02-16 11:00:21,848 : Text to Image: 26.04, 59.58, 75.16, 4.0
2019-02-16 11:00:21,848 : Dev mean Text to Image: 25.476, 60.00399999999999, 76.22399999999999, 4.0
2019-02-16 11:00:21,848 : Dev mean Image to text: 30.02, 64.72, 78.82, 3.0
2019-02-16 11:00:21,849 : start epoch
2019-02-16 11:01:04,846 : samples : 64000
2019-02-16 11:01:15,150 : Image to text: 11.94, 33.64, 45.98, 13.0
2019-02-16 11:01:22,732 : Text to Image: 10.208, 28.508, 41.164, 16.0
2019-02-16 11:02:05,493 : samples : 128000
2019-02-16 11:02:15,775 : Image to text: 12.0, 33.64, 46.08, 13.0
2019-02-16 11:02:23,328 : Text to Image: 10.468, 28.952, 41.52, 16.0
2019-02-16 11:03:05,569 : samples : 192000
2019-02-16 11:03:15,881 : Image to text: 12.18, 34.3, 47.12, 12.0
2019-02-16 11:03:23,448 : Text to Image: 10.672, 29.016, 41.736, 15.0
2019-02-16 11:04:06,152 : samples : 256000
2019-02-16 11:04:16,465 : Image to text: 12.24, 33.4, 47.08, 12.0
2019-02-16 11:04:23,954 : Text to Image: 10.644, 28.784, 41.544, 16.0
2019-02-16 11:05:07,359 : samples : 320000
2019-02-16 11:05:17,616 : Image to text: 11.92, 33.1, 46.24, 13.0
2019-02-16 11:05:25,106 : Text to Image: 10.332, 29.224, 41.744, 15.0
2019-02-16 11:06:07,245 : samples : 384000
2019-02-16 11:06:17,501 : Image to text: 11.82, 33.44, 46.1, 13.0
2019-02-16 11:06:24,999 : Text to Image: 10.504, 28.988, 41.58, 16.0
2019-02-16 11:07:07,553 : samples : 448000
2019-02-16 11:07:17,832 : Image to text: 12.58, 33.66, 46.56, 13.0
2019-02-16 11:07:25,331 : Text to Image: 10.796, 29.516, 42.068, 15.0
2019-02-16 11:08:14,142 : samples : 512000
2019-02-16 11:08:26,724 : Image to text: 12.16, 33.54, 47.0, 12.0
2019-02-16 11:08:34,243 : Text to Image: 10.192, 28.732, 41.128, 16.0
2019-02-16 11:09:10,292 : Epoch 15 finished
2019-02-16 11:09:10,736 : Image to text: 29.0, 65.9, 80.4, 3.0
2019-02-16 11:09:11,070 : Text to Image: 24.18, 59.84, 76.78, 4.0
2019-02-16 11:09:11,546 : Image to text: 29.9, 62.0, 78.0, 3.0
2019-02-16 11:09:11,887 : Text to Image: 24.12, 58.2, 75.0, 4.0
2019-02-16 11:09:12,335 : Image to text: 29.7, 65.3, 80.0, 3.0
2019-02-16 11:09:12,682 : Text to Image: 25.34, 59.38, 75.5, 4.0
2019-02-16 11:09:13,126 : Image to text: 29.4, 65.6, 79.3, 3.0
2019-02-16 11:09:13,486 : Text to Image: 24.84, 58.94, 75.68, 4.0
2019-02-16 11:09:13,951 : Image to text: 30.7, 64.5, 77.7, 3.0
2019-02-16 11:09:14,287 : Text to Image: 25.32, 58.44, 74.34, 4.0
2019-02-16 11:09:14,287 : Dev mean Text to Image: 24.759999999999998, 58.96000000000001, 75.46000000000001, 4.0
2019-02-16 11:09:14,287 : Dev mean Image to text: 29.74, 64.66, 79.08, 3.0
2019-02-16 11:09:18,292 : 
Test scores | Image to text:             30.020000000000003, 63.56, 78.44, 3.0
2019-02-16 11:09:18,292 : Test scores | Text to image:             25.528, 59.396, 75.83200000000001, 4.0

2019-02-16 11:09:18,390 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 11:09:18,781 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 11:09:19,480 : loading BERT model bert-base-uncased
2019-02-16 11:09:19,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:09:19,514 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:09:19,514 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm9y5ngti
2019-02-16 11:09:22,063 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:09:23,579 : Computing embeddings for train/dev/test
2019-02-16 11:10:58,563 : Computed embeddings
2019-02-16 11:10:58,563 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:11:46,807 : [('reg:1e-05', 67.08), ('reg:0.0001', 69.98), ('reg:0.001', 70.11), ('reg:0.01', 51.64)]
2019-02-16 11:11:46,807 : Validation : best param found is reg = 0.001 with score             70.11
2019-02-16 11:11:46,807 : Evaluating...
2019-02-16 11:11:56,825 : 
Dev acc : 70.1 Test acc : 70.8 for LENGTH classification

2019-02-16 11:11:56,825 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 11:11:57,182 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 11:11:57,233 : loading BERT model bert-base-uncased
2019-02-16 11:11:57,233 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:11:57,267 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:11:57,267 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppqnxtuvb
2019-02-16 11:11:59,782 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:12:01,328 : Computing embeddings for train/dev/test
2019-02-16 11:13:31,164 : Computed embeddings
2019-02-16 11:13:31,164 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:14:26,747 : [('reg:1e-05', 17.58), ('reg:0.0001', 6.67), ('reg:0.001', 0.49), ('reg:0.01', 0.13)]
2019-02-16 11:14:26,747 : Validation : best param found is reg = 1e-05 with score             17.58
2019-02-16 11:14:26,747 : Evaluating...
2019-02-16 11:14:45,577 : 
Dev acc : 17.6 Test acc : 17.7 for WORDCONTENT classification

2019-02-16 11:14:45,578 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 11:14:45,980 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 11:14:46,056 : loading BERT model bert-base-uncased
2019-02-16 11:14:46,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:14:46,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:14:46,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsbp_zii0
2019-02-16 11:14:48,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:14:50,153 : Computing embeddings for train/dev/test
2019-02-16 11:16:13,642 : Computed embeddings
2019-02-16 11:16:13,642 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:16:58,330 : [('reg:1e-05', 27.57), ('reg:0.0001', 26.61), ('reg:0.001', 29.3), ('reg:0.01', 25.41)]
2019-02-16 11:16:58,330 : Validation : best param found is reg = 0.001 with score             29.3
2019-02-16 11:16:58,330 : Evaluating...
2019-02-16 11:17:10,055 : 
Dev acc : 29.3 Test acc : 29.8 for DEPTH classification

2019-02-16 11:17:10,057 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 11:17:10,485 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 11:17:10,559 : loading BERT model bert-base-uncased
2019-02-16 11:17:10,559 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:17:10,692 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:17:10,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp64g8qp9w
2019-02-16 11:17:13,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:17:14,745 : Computing embeddings for train/dev/test
2019-02-16 11:18:33,100 : Computed embeddings
2019-02-16 11:18:33,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:19:16,853 : [('reg:1e-05', 59.11), ('reg:0.0001', 50.53), ('reg:0.001', 46.03), ('reg:0.01', 39.62)]
2019-02-16 11:19:16,853 : Validation : best param found is reg = 1e-05 with score             59.11
2019-02-16 11:19:16,854 : Evaluating...
2019-02-16 11:19:31,286 : 
Dev acc : 59.1 Test acc : 59.1 for TOPCONSTITUENTS classification

2019-02-16 11:19:31,288 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 11:19:31,830 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 11:19:31,896 : loading BERT model bert-base-uncased
2019-02-16 11:19:31,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:19:31,927 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:19:31,927 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9dd237id
2019-02-16 11:19:34,434 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:19:35,867 : Computing embeddings for train/dev/test
2019-02-16 11:20:59,670 : Computed embeddings
2019-02-16 11:20:59,670 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:22:02,484 : [('reg:1e-05', 84.31), ('reg:0.0001', 84.3), ('reg:0.001', 83.88), ('reg:0.01', 75.1)]
2019-02-16 11:22:02,484 : Validation : best param found is reg = 1e-05 with score             84.31
2019-02-16 11:22:02,484 : Evaluating...
2019-02-16 11:22:21,476 : 
Dev acc : 84.3 Test acc : 83.6 for BIGRAMSHIFT classification

2019-02-16 11:22:21,477 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 11:22:22,134 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 11:22:22,208 : loading BERT model bert-base-uncased
2019-02-16 11:22:22,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:22:22,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:22:22,243 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqodtwkam
2019-02-16 11:22:24,762 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:22:26,232 : Computing embeddings for train/dev/test
2019-02-16 11:23:48,961 : Computed embeddings
2019-02-16 11:23:48,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:24:26,969 : [('reg:1e-05', 87.33), ('reg:0.0001', 87.3), ('reg:0.001', 87.43), ('reg:0.01', 87.08)]
2019-02-16 11:24:26,969 : Validation : best param found is reg = 0.001 with score             87.43
2019-02-16 11:24:26,969 : Evaluating...
2019-02-16 11:24:34,595 : 
Dev acc : 87.4 Test acc : 86.0 for TENSE classification

2019-02-16 11:24:34,596 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 11:24:35,023 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 11:24:35,086 : loading BERT model bert-base-uncased
2019-02-16 11:24:35,087 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:24:35,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:24:35,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp09ht_id7
2019-02-16 11:24:37,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:24:39,901 : Computing embeddings for train/dev/test
2019-02-16 11:26:11,287 : Computed embeddings
2019-02-16 11:26:11,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:26:59,216 : [('reg:1e-05', 78.75), ('reg:0.0001', 78.76), ('reg:0.001', 78.73), ('reg:0.01', 80.03)]
2019-02-16 11:26:59,216 : Validation : best param found is reg = 0.01 with score             80.03
2019-02-16 11:26:59,216 : Evaluating...
2019-02-16 11:27:10,453 : 
Dev acc : 80.0 Test acc : 79.4 for SUBJNUMBER classification

2019-02-16 11:27:10,454 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 11:27:10,915 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 11:27:10,993 : loading BERT model bert-base-uncased
2019-02-16 11:27:10,994 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:27:11,128 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:27:11,128 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvx0on8mp
2019-02-16 11:27:13,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:27:15,097 : Computing embeddings for train/dev/test
2019-02-16 11:28:41,263 : Computed embeddings
2019-02-16 11:28:41,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:29:45,523 : [('reg:1e-05', 78.98), ('reg:0.0001', 78.99), ('reg:0.001', 79.01), ('reg:0.01', 76.19)]
2019-02-16 11:29:45,523 : Validation : best param found is reg = 0.001 with score             79.01
2019-02-16 11:29:45,523 : Evaluating...
2019-02-16 11:29:58,222 : 
Dev acc : 79.0 Test acc : 79.8 for OBJNUMBER classification

2019-02-16 11:29:58,223 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 11:29:58,887 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 11:29:58,962 : loading BERT model bert-base-uncased
2019-02-16 11:29:58,963 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:29:58,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:29:58,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2bk8ywnx
2019-02-16 11:30:01,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:30:02,962 : Computing embeddings for train/dev/test
2019-02-16 11:31:40,681 : Computed embeddings
2019-02-16 11:31:40,682 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:32:25,273 : [('reg:1e-05', 57.49), ('reg:0.0001', 57.5), ('reg:0.001', 57.42), ('reg:0.01', 57.94)]
2019-02-16 11:32:25,273 : Validation : best param found is reg = 0.01 with score             57.94
2019-02-16 11:32:25,273 : Evaluating...
2019-02-16 11:32:36,957 : 
Dev acc : 57.9 Test acc : 57.9 for ODDMANOUT classification

2019-02-16 11:32:36,958 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 11:32:37,394 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 11:32:37,479 : loading BERT model bert-base-uncased
2019-02-16 11:32:37,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:32:37,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:32:37,626 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp__cer_tm
2019-02-16 11:32:40,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:32:41,532 : Computing embeddings for train/dev/test
2019-02-16 11:34:20,353 : Computed embeddings
2019-02-16 11:34:20,353 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:35:33,203 : [('reg:1e-05', 63.88), ('reg:0.0001', 63.9), ('reg:0.001', 61.55), ('reg:0.01', 58.48)]
2019-02-16 11:35:33,203 : Validation : best param found is reg = 0.0001 with score             63.9
2019-02-16 11:35:33,203 : Evaluating...
2019-02-16 11:35:54,341 : 
Dev acc : 63.9 Test acc : 63.0 for COORDINATIONINVERSION classification

2019-02-16 11:35:54,343 : total results: {'STS12': {'MSRpar': {'pearson': (0.29800551466257164, 7.570030585662228e-17), 'spearman': SpearmanrResult(correlation=0.3357838994452631, pvalue=3.1871064299758365e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6318654782941254, 7.829773911024673e-85), 'spearman': SpearmanrResult(correlation=0.6431613848217653, pvalue=8.801965924341226e-89), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47823544208024454, 1.3131779577101142e-27), 'spearman': SpearmanrResult(correlation=0.5977241295275321, pvalue=8.461510031124956e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5489828624010895, 2.94839062559549e-60), 'spearman': SpearmanrResult(correlation=0.576296846708843, pvalue=1.382706352557444e-67), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5508526057190424, 4.855284161752994e-33), 'spearman': SpearmanrResult(correlation=0.4630552559461852, pvalue=1.3428181587715157e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5015883806314146, 'wmean': 0.49821127706405094}, 'spearman': {'mean': 0.5232043032899177, 'wmean': 0.5230199552147904}}}, 'STS13': {'FNWN': {'pearson': (0.15878847541711782, 0.029082685884684413), 'spearman': SpearmanrResult(correlation=0.16971754260450664, pvalue=0.019558304994350423), 'nsamples': 189}, 'headlines': {'pearson': (0.6382940187365744, 4.640487938195143e-87), 'spearman': SpearmanrResult(correlation=0.6341023378208493, pvalue=1.3328192244615853e-85), 'nsamples': 750}, 'OnWN': {'pearson': (0.49108460391801906, 2.166766313617893e-35), 'spearman': SpearmanrResult(correlation=0.49201404033687096, pvalue=1.5443738318460969e-35), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42938903269057044, 'wmean': 0.5228199991361832}, 'spearman': {'mean': 0.43194464025407564, 'wmean': 0.5224488303645823}}}, 'STS14': {'deft-forum': {'pearson': (0.37681352774139115, 1.2495312238373274e-16), 'spearman': SpearmanrResult(correlation=0.383859513476216, pvalue=3.015380888237665e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7247174111011488, 3.9903127055257606e-50), 'spearman': SpearmanrResult(correlation=0.7009095236196715, pvalue=1.2117275309064214e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5853450722123773, 3.609996219935461e-70), 'spearman': SpearmanrResult(correlation=0.5591613968853569, pvalue=6.574785945368578e-63), 'nsamples': 750}, 'images': {'pearson': (0.5666550070973194, 6.400075811623183e-65), 'spearman': SpearmanrResult(correlation=0.5653893535045296, pvalue=1.4111597758063578e-64), 'nsamples': 750}, 'OnWN': {'pearson': (0.6137588240461684, 7.809918431969947e-79), 'spearman': SpearmanrResult(correlation=0.6458595312704644, pvalue=9.472269417808565e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5713477879572377, 3.310547659736987e-66), 'spearman': SpearmanrResult(correlation=0.544947981924023, pvalue=3.1314373774303546e-59), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5731062716926072, 'wmean': 0.5706163544796795}, 'spearman': {'mean': 0.5666878834467103, 'wmean': 0.5652075562235944}}}, 'STS15': {'answers-forums': {'pearson': (0.49503490215467066, 1.4072728784951827e-24), 'spearman': SpearmanrResult(correlation=0.4625020392396925, pvalue=2.8333131790114137e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.6637879905082753, 1.9322987253315367e-96), 'spearman': SpearmanrResult(correlation=0.6720960260677997, pvalue=1.0608778070740942e-99), 'nsamples': 750}, 'belief': {'pearson': (0.5285552163169923, 2.2544883048585113e-28), 'spearman': SpearmanrResult(correlation=0.56628126483599, pvalue=3.490418536372085e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6308657310762676, 1.7194821994492406e-84), 'spearman': SpearmanrResult(correlation=0.6354755545054342, pvalue=4.4621277405323506e-86), 'nsamples': 750}, 'images': {'pearson': (0.7008867576098062, 7.140485822954236e-112), 'spearman': SpearmanrResult(correlation=0.7111338357577126, pvalue=1.4490824278051948e-116), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6038261195332024, 'wmean': 0.6268338846075452}, 'spearman': {'mean': 0.6094977440813258, 'wmean': 0.6332742670921969}}}, 'STS16': {'answer-answer': {'pearson': (0.49408045905989084, 4.8669118776915505e-17), 'spearman': SpearmanrResult(correlation=0.5171346517701539, pvalue=8.990042747896874e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6267219321937699, 1.4080780950613647e-28), 'spearman': SpearmanrResult(correlation=0.6334408836956236, pvalue=2.460030032657408e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6797395546341248, 1.559341013365047e-32), 'spearman': SpearmanrResult(correlation=0.6807850536909653, pvalue=1.1513361783574992e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.76367517122773, 6.86585145383923e-48), 'spearman': SpearmanrResult(correlation=0.8170363108936974, pvalue=8.425790255827406e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.2585292321209124, 0.00015710860303836283), 'spearman': SpearmanrResult(correlation=0.28750159011548565, pvalue=2.433354455981776e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5645492698472856, 'wmean': 0.5718884035212027}, 'spearman': {'mean': 0.5871796980331851, 'wmean': 0.5945229646972597}}}, 'MR': {'devacc': 62.74, 'acc': 65.62, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.93, 'acc': 68.0, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.35, 'acc': 87.33, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.39, 'acc': 90.06, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.16, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.33, 'acc': 36.7, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.16, 'acc': 81.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.65, 'acc': 71.48, 'f1': 81.78, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 74.91, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8016525987092108, 'pearson': 0.776170577689326, 'spearman': 0.7123677296437668, 'mse': 0.405752075275095, 'yhat': array([2.39989118, 4.23611851, 2.72161094, ..., 3.05343511, 4.25065465,        4.59093306]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6419749514192992, 'pearson': 0.6270375233457395, 'spearman': 0.6247146932789917, 'mse': 1.4881659884311775, 'yhat': array([1.26308335, 1.48939063, 2.2063324 , ..., 3.96600346, 3.4499822 ,        4.31422085]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.24, 'acc': 58.71, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 335.264, 'acc': [(30.020000000000003, 63.56, 78.44, 3.0), (25.528, 59.396, 75.83200000000001, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 70.11, 'acc': 70.79, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 17.58, 'acc': 17.69, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.3, 'acc': 29.84, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.11, 'acc': 59.07, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.31, 'acc': 83.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.43, 'acc': 86.03, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.03, 'acc': 79.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.01, 'acc': 79.83, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.94, 'acc': 57.94, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 63.9, 'acc': 63.02, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 11:35:54,343 : STS12 p=0.4982, STS12 s=0.5230, STS13 p=0.5228, STS13 s=0.5224, STS14 p=0.5706, STS14 s=0.5652, STS15 p=0.6268, STS15 s=0.6333, STS 16 p=0.5719, STS16 s=0.5945, STS B p=0.6270, STS B s=0.6247, STS B m=1.4882, SICK-R p=0.7762, SICK-R s=0.7124, SICK-P m=0.4058
2019-02-16 11:35:54,343 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 11:35:54,343 : 0.4982,0.5230,0.5228,0.5224,0.5706,0.5652,0.6268,0.6333,0.5719,0.5945,0.6270,0.6247,1.4882,0.7762,0.7124,0.4058
2019-02-16 11:35:54,343 : MR=65.62, CR=68.00, SUBJ=90.06, MPQA=87.33, SST-B=77.81, SST-F=36.70, TREC=81.00, SICK-E=74.91, SNLI=58.71, MRPC=71.48, MRPC f=81.78
2019-02-16 11:35:54,343 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 11:35:54,343 : 65.62,68.00,90.06,87.33,77.81,36.70,81.00,74.91,58.71,71.48,81.78
2019-02-16 11:35:54,344 : COCO r1i2t=30.02, COCO r5i2t=63.56, COCO r10i2t=78.44, COCO medr_i2t=3.00, COCO r1t2i=25.53, COCO r5t2i=59.40, COCO r10t2i=75.83, COCO medr_t2i=4.00
2019-02-16 11:35:54,344 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 11:35:54,344 : 30.02,63.56,78.44,3.00,25.53,59.40,75.83,4.00
2019-02-16 11:35:54,344 : SentLen=70.79, WC=17.69, TreeDepth=29.84, TopConst=59.07, BShift=83.62, Tense=86.03, SubjNum=79.38, ObjNum=79.83, SOMO=57.94, CoordInv=63.02, average=62.72
2019-02-16 11:35:54,344 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 11:35:54,344 : 70.79,17.69,29.84,59.07,83.62,86.03,79.38,79.83,57.94,63.02,62.72
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,344 : layer 7
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,344 : ********************************************************************************
2019-02-16 11:35:54,448 : ***** Transfer task : STS12 *****


2019-02-16 11:35:54,491 : loading BERT model bert-base-uncased
2019-02-16 11:35:54,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:35:54,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:35:54,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7_nmj84x
2019-02-16 11:35:57,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:00,302 : MSRpar : pearson = 0.2981, spearman = 0.3332
2019-02-16 11:36:01,111 : MSRvid : pearson = 0.6187, spearman = 0.6304
2019-02-16 11:36:01,780 : SMTeuroparl : pearson = 0.4711, spearman = 0.5950
2019-02-16 11:36:02,980 : surprise.OnWN : pearson = 0.5413, spearman = 0.5686
2019-02-16 11:36:03,657 : surprise.SMTnews : pearson = 0.5641, spearman = 0.4661
2019-02-16 11:36:03,658 : ALL (weighted average) : Pearson = 0.4939,             Spearman = 0.5175
2019-02-16 11:36:03,658 : ALL (average) : Pearson = 0.4987,             Spearman = 0.5187

2019-02-16 11:36:03,658 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 11:36:03,669 : loading BERT model bert-base-uncased
2019-02-16 11:36:03,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:36:03,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:36:03,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeczookft
2019-02-16 11:36:06,158 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:08,229 : FNWN : pearson = 0.1568, spearman = 0.1686
2019-02-16 11:36:09,132 : headlines : pearson = 0.6308, spearman = 0.6261
2019-02-16 11:36:09,792 : OnWN : pearson = 0.4739, spearman = 0.4654
2019-02-16 11:36:09,792 : ALL (weighted average) : Pearson = 0.5124,             Spearman = 0.5083
2019-02-16 11:36:09,792 : ALL (average) : Pearson = 0.4205,             Spearman = 0.4200

2019-02-16 11:36:09,792 : ***** Transfer task : STS14 *****


2019-02-16 11:36:09,808 : loading BERT model bert-base-uncased
2019-02-16 11:36:09,809 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:36:09,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:36:09,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqlin0dm4
2019-02-16 11:36:12,316 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:14,426 : deft-forum : pearson = 0.3724, spearman = 0.3795
2019-02-16 11:36:15,141 : deft-news : pearson = 0.7230, spearman = 0.7009
2019-02-16 11:36:16,117 : headlines : pearson = 0.5802, spearman = 0.5536
2019-02-16 11:36:17,084 : images : pearson = 0.5512, spearman = 0.5465
2019-02-16 11:36:18,054 : OnWN : pearson = 0.6084, spearman = 0.6377
2019-02-16 11:36:19,353 : tweet-news : pearson = 0.5872, spearman = 0.5591
2019-02-16 11:36:19,353 : ALL (weighted average) : Pearson = 0.5679,             Spearman = 0.5610
2019-02-16 11:36:19,353 : ALL (average) : Pearson = 0.5704,             Spearman = 0.5629

2019-02-16 11:36:19,353 : ***** Transfer task : STS15 *****


2019-02-16 11:36:19,423 : loading BERT model bert-base-uncased
2019-02-16 11:36:19,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:36:19,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:36:19,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7r1nmzv6
2019-02-16 11:36:21,902 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:24,232 : answers-forums : pearson = 0.5036, spearman = 0.4883
2019-02-16 11:36:25,195 : answers-students : pearson = 0.6577, spearman = 0.6660
2019-02-16 11:36:26,034 : belief : pearson = 0.5305, spearman = 0.5718
2019-02-16 11:36:27,065 : headlines : pearson = 0.6244, spearman = 0.6323
2019-02-16 11:36:28,043 : images : pearson = 0.6921, spearman = 0.7004
2019-02-16 11:36:28,044 : ALL (weighted average) : Pearson = 0.6228,             Spearman = 0.6322
2019-02-16 11:36:28,044 : ALL (average) : Pearson = 0.6017,             Spearman = 0.6118

2019-02-16 11:36:28,044 : ***** Transfer task : STS16 *****


2019-02-16 11:36:28,104 : loading BERT model bert-base-uncased
2019-02-16 11:36:28,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:36:28,124 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:36:28,124 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2v8xe73
2019-02-16 11:36:30,613 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:32,502 : answer-answer : pearson = 0.4951, spearman = 0.5140
2019-02-16 11:36:32,807 : headlines : pearson = 0.6285, spearman = 0.6356
2019-02-16 11:36:33,207 : plagiarism : pearson = 0.6729, spearman = 0.6785
2019-02-16 11:36:33,835 : postediting : pearson = 0.7649, spearman = 0.8134
2019-02-16 11:36:34,119 : question-question : pearson = 0.2300, spearman = 0.2468
2019-02-16 11:36:34,119 : ALL (weighted average) : Pearson = 0.5664,             Spearman = 0.5859
2019-02-16 11:36:34,119 : ALL (average) : Pearson = 0.5583,             Spearman = 0.5777

2019-02-16 11:36:34,119 : ***** Transfer task : MR *****


2019-02-16 11:36:34,173 : loading BERT model bert-base-uncased
2019-02-16 11:36:34,174 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:36:34,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:36:34,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi4ky5bt2
2019-02-16 11:36:36,705 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:36:38,216 : Generating sentence embeddings
2019-02-16 11:36:52,236 : Generated sentence embeddings
2019-02-16 11:36:52,236 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 11:37:09,563 : Best param found at split 1: l2reg = 0.01                 with score 64.51
2019-02-16 11:37:30,114 : Best param found at split 2: l2reg = 1e-05                 with score 66.04
2019-02-16 11:37:50,632 : Best param found at split 3: l2reg = 1e-05                 with score 63.99
2019-02-16 11:38:08,289 : Best param found at split 4: l2reg = 1e-05                 with score 66.54
2019-02-16 11:38:27,731 : Best param found at split 5: l2reg = 0.0001                 with score 60.94
2019-02-16 11:38:29,155 : Dev acc : 64.4 Test acc : 65.46

2019-02-16 11:38:29,156 : ***** Transfer task : CR *****


2019-02-16 11:38:29,164 : loading BERT model bert-base-uncased
2019-02-16 11:38:29,164 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:38:29,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:38:29,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp05nglbvj
2019-02-16 11:38:31,725 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:38:33,159 : Generating sentence embeddings
2019-02-16 11:38:36,879 : Generated sentence embeddings
2019-02-16 11:38:36,880 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 11:38:42,561 : Best param found at split 1: l2reg = 1e-05                 with score 72.74
2019-02-16 11:38:48,018 : Best param found at split 2: l2reg = 0.0001                 with score 76.31
2019-02-16 11:38:55,005 : Best param found at split 3: l2reg = 0.001                 with score 72.69
2019-02-16 11:39:00,621 : Best param found at split 4: l2reg = 0.01                 with score 75.97
2019-02-16 11:39:05,794 : Best param found at split 5: l2reg = 0.01                 with score 74.81
2019-02-16 11:39:05,988 : Dev acc : 74.5 Test acc : 73.72

2019-02-16 11:39:05,988 : ***** Transfer task : MPQA *****


2019-02-16 11:39:06,023 : loading BERT model bert-base-uncased
2019-02-16 11:39:06,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:39:06,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:39:06,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyruv_byo
2019-02-16 11:39:08,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:39:10,032 : Generating sentence embeddings
2019-02-16 11:39:13,821 : Generated sentence embeddings
2019-02-16 11:39:13,821 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 11:39:34,763 : Best param found at split 1: l2reg = 0.001                 with score 87.78
2019-02-16 11:39:51,755 : Best param found at split 2: l2reg = 0.01                 with score 87.88
2019-02-16 11:40:09,512 : Best param found at split 3: l2reg = 0.001                 with score 87.28
2019-02-16 11:40:26,248 : Best param found at split 4: l2reg = 0.001                 with score 87.61
2019-02-16 11:40:44,726 : Best param found at split 5: l2reg = 0.0001                 with score 87.45
2019-02-16 11:40:45,301 : Dev acc : 87.6 Test acc : 86.78

2019-02-16 11:40:45,302 : ***** Transfer task : SUBJ *****


2019-02-16 11:40:45,317 : loading BERT model bert-base-uncased
2019-02-16 11:40:45,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:40:45,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:40:45,341 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4cmziyx8
2019-02-16 11:40:47,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:40:49,341 : Generating sentence embeddings
2019-02-16 11:41:03,698 : Generated sentence embeddings
2019-02-16 11:41:03,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 11:41:20,743 : Best param found at split 1: l2reg = 1e-05                 with score 92.18
2019-02-16 11:41:33,896 : Best param found at split 2: l2reg = 0.01                 with score 91.68
2019-02-16 11:41:45,182 : Best param found at split 3: l2reg = 0.001                 with score 91.51
2019-02-16 11:41:57,735 : Best param found at split 4: l2reg = 0.001                 with score 91.93
2019-02-16 11:42:11,453 : Best param found at split 5: l2reg = 0.01                 with score 91.99
2019-02-16 11:42:12,042 : Dev acc : 91.86 Test acc : 91.94

2019-02-16 11:42:12,044 : ***** Transfer task : SST Binary classification *****


2019-02-16 11:42:12,189 : loading BERT model bert-base-uncased
2019-02-16 11:42:12,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:42:12,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:42:12,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfk1sj1u3
2019-02-16 11:42:15,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:42:17,791 : Computing embedding for train
2019-02-16 11:43:02,725 : Computed train embeddings
2019-02-16 11:43:02,725 : Computing embedding for dev
2019-02-16 11:43:03,768 : Computed dev embeddings
2019-02-16 11:43:03,768 : Computing embedding for test
2019-02-16 11:43:05,944 : Computed test embeddings
2019-02-16 11:43:05,944 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:43:30,158 : [('reg:1e-05', 81.42), ('reg:0.0001', 81.31), ('reg:0.001', 77.29), ('reg:0.01', 77.29)]
2019-02-16 11:43:30,159 : Validation : best param found is reg = 1e-05 with score             81.42
2019-02-16 11:43:30,159 : Evaluating...
2019-02-16 11:43:36,695 : 
Dev acc : 81.42 Test acc : 79.35 for             SST Binary classification

2019-02-16 11:43:36,695 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 11:43:36,753 : loading BERT model bert-base-uncased
2019-02-16 11:43:36,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:43:36,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:43:36,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfmylw4oz
2019-02-16 11:43:39,285 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:43:40,773 : Computing embedding for train
2019-02-16 11:43:50,336 : Computed train embeddings
2019-02-16 11:43:50,336 : Computing embedding for dev
2019-02-16 11:43:51,594 : Computed dev embeddings
2019-02-16 11:43:51,594 : Computing embedding for test
2019-02-16 11:43:54,059 : Computed test embeddings
2019-02-16 11:43:54,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:43:57,676 : [('reg:1e-05', 36.78), ('reg:0.0001', 35.6), ('reg:0.001', 36.88), ('reg:0.01', 38.15)]
2019-02-16 11:43:57,676 : Validation : best param found is reg = 0.01 with score             38.15
2019-02-16 11:43:57,676 : Evaluating...
2019-02-16 11:43:58,693 : 
Dev acc : 38.15 Test acc : 42.04 for             SST Fine-Grained classification

2019-02-16 11:43:58,693 : ***** Transfer task : TREC *****


2019-02-16 11:43:58,708 : loading BERT model bert-base-uncased
2019-02-16 11:43:58,708 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:43:58,733 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:43:58,733 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9aj2614j
2019-02-16 11:44:01,210 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:44:06,474 : Computed train embeddings
2019-02-16 11:44:06,771 : Computed test embeddings
2019-02-16 11:44:06,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 11:44:17,891 : [('reg:1e-05', 69.01), ('reg:0.0001', 73.22), ('reg:0.001', 72.31), ('reg:0.01', 60.52)]
2019-02-16 11:44:17,891 : Cross-validation : best param found is reg = 0.0001             with score 73.22
2019-02-16 11:44:17,891 : Evaluating...
2019-02-16 11:44:18,395 : 
Dev acc : 73.22 Test acc : 88.8             for TREC

2019-02-16 11:44:18,396 : ***** Transfer task : MRPC *****


2019-02-16 11:44:18,416 : loading BERT model bert-base-uncased
2019-02-16 11:44:18,416 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:44:18,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:44:18,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt_80roci
2019-02-16 11:44:20,900 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:44:22,367 : Computing embedding for train
2019-02-16 11:44:32,091 : Computed train embeddings
2019-02-16 11:44:32,091 : Computing embedding for test
2019-02-16 11:44:36,281 : Computed test embeddings
2019-02-16 11:44:36,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 11:44:43,487 : [('reg:1e-05', 72.03), ('reg:0.0001', 71.13), ('reg:0.001', 71.61), ('reg:0.01', 71.86)]
2019-02-16 11:44:43,487 : Cross-validation : best param found is reg = 1e-05             with score 72.03
2019-02-16 11:44:43,487 : Evaluating...
2019-02-16 11:44:43,875 : Dev acc : 72.03 Test acc 69.62; Test F1 75.24 for MRPC.

2019-02-16 11:44:43,876 : ***** Transfer task : SICK-Entailment*****


2019-02-16 11:44:43,945 : loading BERT model bert-base-uncased
2019-02-16 11:44:43,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:44:43,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:44:43,976 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwf8jfyhr
2019-02-16 11:44:46,489 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:44:47,975 : Computing embedding for train
2019-02-16 11:44:53,159 : Computed train embeddings
2019-02-16 11:44:53,159 : Computing embedding for dev
2019-02-16 11:44:53,842 : Computed dev embeddings
2019-02-16 11:44:53,842 : Computing embedding for test
2019-02-16 11:44:59,317 : Computed test embeddings
2019-02-16 11:44:59,345 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:45:01,380 : [('reg:1e-05', 74.0), ('reg:0.0001', 76.0), ('reg:0.001', 71.0), ('reg:0.01', 72.8)]
2019-02-16 11:45:01,380 : Validation : best param found is reg = 0.0001 with score             76.0
2019-02-16 11:45:01,380 : Evaluating...
2019-02-16 11:45:01,874 : 
Dev acc : 76.0 Test acc : 75.08 for                        SICK entailment

2019-02-16 11:45:01,874 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 11:45:01,906 : loading BERT model bert-base-uncased
2019-02-16 11:45:01,906 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:45:01,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:45:01,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe789jpom
2019-02-16 11:45:04,467 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:45:05,940 : Computing embedding for train
2019-02-16 11:45:11,096 : Computed train embeddings
2019-02-16 11:45:11,096 : Computing embedding for dev
2019-02-16 11:45:11,780 : Computed dev embeddings
2019-02-16 11:45:11,780 : Computing embedding for test
2019-02-16 11:45:17,309 : Computed test embeddings
2019-02-16 11:45:40,942 : Dev : Pearson 0.7874576173065023
2019-02-16 11:45:40,942 : Test : Pearson 0.7788641232113926 Spearman 0.7137650912235657 MSE 0.4040303642313609                        for SICK Relatedness

2019-02-16 11:45:40,943 : 

***** Transfer task : STSBenchmark*****


2019-02-16 11:45:40,984 : loading BERT model bert-base-uncased
2019-02-16 11:45:40,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:45:41,015 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:45:41,015 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwhb7fcnl
2019-02-16 11:45:43,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:45:44,989 : Computing embedding for train
2019-02-16 11:45:54,137 : Computed train embeddings
2019-02-16 11:45:54,137 : Computing embedding for dev
2019-02-16 11:45:56,856 : Computed dev embeddings
2019-02-16 11:45:56,856 : Computing embedding for test
2019-02-16 11:45:59,050 : Computed test embeddings
2019-02-16 11:46:28,831 : Dev : Pearson 0.646405449255224
2019-02-16 11:46:28,831 : Test : Pearson 0.6474794577567471 Spearman 0.6458808812620485 MSE 1.4307438215016695                        for SICK Relatedness

2019-02-16 11:46:28,832 : ***** Transfer task : SNLI Entailment*****


2019-02-16 11:46:33,811 : loading BERT model bert-base-uncased
2019-02-16 11:46:33,811 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:46:33,955 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:46:33,956 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1_kbhp25
2019-02-16 11:46:36,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:46:38,106 : PROGRESS (encoding): 0.00%
2019-02-16 11:47:56,691 : PROGRESS (encoding): 14.56%
2019-02-16 11:49:24,755 : PROGRESS (encoding): 29.12%
2019-02-16 11:50:52,188 : PROGRESS (encoding): 43.69%
2019-02-16 11:52:26,493 : PROGRESS (encoding): 58.25%
2019-02-16 11:54:12,161 : PROGRESS (encoding): 72.81%
2019-02-16 11:55:56,714 : PROGRESS (encoding): 87.37%
2019-02-16 11:57:46,348 : PROGRESS (encoding): 0.00%
2019-02-16 11:58:00,698 : PROGRESS (encoding): 0.00%
2019-02-16 11:58:13,541 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 11:58:41,672 : [('reg:1e-09', 65.33)]
2019-02-16 11:58:41,672 : Validation : best param found is reg = 1e-09 with score             65.33
2019-02-16 11:58:41,672 : Evaluating...
2019-02-16 11:59:08,415 : Dev acc : 65.33 Test acc : 65.34 for SNLI

2019-02-16 11:59:08,415 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 11:59:20,799 : loading BERT model bert-base-uncased
2019-02-16 11:59:20,799 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 11:59:20,850 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 11:59:20,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbu1n8i6h
2019-02-16 11:59:23,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 11:59:24,777 : Computing embedding for train
2019-02-16 12:06:57,623 : Computed train embeddings
2019-02-16 12:06:57,623 : Computing embedding for dev
2019-02-16 12:07:16,801 : Computed dev embeddings
2019-02-16 12:07:16,801 : Computing embedding for test
2019-02-16 12:07:36,688 : Computed test embeddings
2019-02-16 12:07:36,704 : prepare data
2019-02-16 12:07:36,769 : start epoch
2019-02-16 12:08:19,933 : samples : 64000
2019-02-16 12:08:30,476 : Image to text: 6.02, 19.98, 30.82, 28.0
2019-02-16 12:08:38,064 : Text to Image: 5.216, 17.32, 27.004, 32.0
2019-02-16 12:09:20,278 : samples : 128000
2019-02-16 12:09:30,738 : Image to text: 8.14, 24.06, 35.28, 21.0
2019-02-16 12:09:38,302 : Text to Image: 6.572, 20.3, 30.64, 26.0
2019-02-16 12:10:21,063 : samples : 192000
2019-02-16 12:10:31,469 : Image to text: 7.18, 22.4, 33.44, 23.0
2019-02-16 12:10:39,017 : Text to Image: 5.86, 19.072, 28.832, 28.0
2019-02-16 12:11:21,920 : samples : 256000
2019-02-16 12:11:32,405 : Image to text: 7.52, 24.94, 36.02, 22.0
2019-02-16 12:11:39,952 : Text to Image: 6.804, 20.888, 31.224, 26.0
2019-02-16 12:12:22,857 : samples : 320000
2019-02-16 12:12:33,340 : Image to text: 8.22, 25.9, 37.22, 19.0
2019-02-16 12:12:40,905 : Text to Image: 7.544, 22.584, 33.624, 23.0
2019-02-16 12:13:23,407 : samples : 384000
2019-02-16 12:13:33,877 : Image to text: 8.24, 25.46, 37.44, 19.0
2019-02-16 12:13:41,402 : Text to Image: 7.052, 21.772, 32.76, 24.0
2019-02-16 12:14:24,175 : samples : 448000
2019-02-16 12:14:34,504 : Image to text: 8.56, 24.92, 36.04, 20.0
2019-02-16 12:14:42,015 : Text to Image: 7.104, 21.76, 32.452, 24.0
2019-02-16 12:15:24,486 : samples : 512000
2019-02-16 12:15:34,575 : Image to text: 8.02, 25.54, 36.82, 20.0
2019-02-16 12:15:43,022 : Text to Image: 6.9, 21.928, 32.748, 24.0
2019-02-16 12:16:28,074 : Epoch 1 finished
2019-02-16 12:16:28,517 : Image to text: 25.4, 56.5, 72.1, 4.0
2019-02-16 12:16:28,852 : Text to Image: 20.28, 50.66, 67.88, 5.0
2019-02-16 12:16:29,291 : Image to text: 26.3, 58.5, 70.7, 4.0
2019-02-16 12:16:29,629 : Text to Image: 19.58, 50.58, 68.92, 5.0
2019-02-16 12:16:30,079 : Image to text: 24.2, 57.2, 73.1, 4.0
2019-02-16 12:16:30,412 : Text to Image: 20.32, 50.66, 67.94, 5.0
2019-02-16 12:16:30,868 : Image to text: 25.4, 57.9, 72.7, 4.0
2019-02-16 12:16:31,215 : Text to Image: 20.46, 51.34, 68.3, 5.0
2019-02-16 12:16:31,671 : Image to text: 24.2, 56.6, 72.2, 4.0
2019-02-16 12:16:32,020 : Text to Image: 20.3, 51.94, 68.76, 5.0
2019-02-16 12:16:32,020 : Dev mean Text to Image: 20.188000000000002, 51.035999999999994, 68.36, 5.0
2019-02-16 12:16:32,020 : Dev mean Image to text: 25.099999999999998, 57.339999999999996, 72.16, 4.0
2019-02-16 12:16:32,021 : start epoch
2019-02-16 12:17:14,732 : samples : 64000
2019-02-16 12:17:25,232 : Image to text: 9.44, 27.5, 39.28, 17.0
2019-02-16 12:17:32,716 : Text to Image: 7.36, 22.792, 33.892, 22.0
2019-02-16 12:18:15,591 : samples : 128000
2019-02-16 12:18:26,060 : Image to text: 9.22, 26.86, 39.04, 18.0
2019-02-16 12:18:33,633 : Text to Image: 7.568, 22.996, 34.44, 22.0
2019-02-16 12:19:15,804 : samples : 192000
2019-02-16 12:19:26,375 : Image to text: 9.38, 26.34, 38.98, 18.0
2019-02-16 12:19:33,876 : Text to Image: 7.812, 23.52, 34.54, 22.0
2019-02-16 12:20:16,650 : samples : 256000
2019-02-16 12:20:27,143 : Image to text: 8.84, 27.3, 38.74, 17.0
2019-02-16 12:20:34,696 : Text to Image: 8.292, 24.44, 36.024, 20.0
2019-02-16 12:21:17,015 : samples : 320000
2019-02-16 12:21:27,528 : Image to text: 9.7, 28.52, 40.32, 16.0
2019-02-16 12:21:35,086 : Text to Image: 8.172, 24.24, 35.62, 21.0
2019-02-16 12:22:17,373 : samples : 384000
2019-02-16 12:22:27,902 : Image to text: 9.58, 26.82, 38.58, 18.0
2019-02-16 12:22:35,412 : Text to Image: 7.26, 22.884, 34.38, 22.0
2019-02-16 12:23:18,858 : samples : 448000
2019-02-16 12:23:29,172 : Image to text: 9.62, 27.5, 38.94, 17.0
2019-02-16 12:23:36,608 : Text to Image: 7.544, 23.04, 34.124, 22.0
2019-02-16 12:24:19,658 : samples : 512000
2019-02-16 12:24:29,973 : Image to text: 9.58, 29.14, 41.56, 16.0
2019-02-16 12:24:37,495 : Text to Image: 8.692, 25.588, 37.004, 19.0
2019-02-16 12:25:13,827 : Epoch 2 finished
2019-02-16 12:25:14,262 : Image to text: 26.9, 58.0, 74.6, 4.0
2019-02-16 12:25:14,599 : Text to Image: 21.56, 53.52, 70.62, 5.0
2019-02-16 12:25:15,037 : Image to text: 25.6, 58.7, 74.0, 4.0
2019-02-16 12:25:15,376 : Text to Image: 21.92, 53.34, 71.16, 5.0
2019-02-16 12:25:15,821 : Image to text: 25.6, 58.1, 74.0, 4.0
2019-02-16 12:25:16,159 : Text to Image: 21.6, 53.64, 70.62, 5.0
2019-02-16 12:25:16,627 : Image to text: 26.7, 59.0, 75.4, 4.0
2019-02-16 12:25:16,968 : Text to Image: 22.08, 54.88, 71.34, 5.0
2019-02-16 12:25:17,423 : Image to text: 26.4, 59.1, 74.3, 4.0
2019-02-16 12:25:17,760 : Text to Image: 22.1, 53.46, 70.14, 5.0
2019-02-16 12:25:17,760 : Dev mean Text to Image: 21.851999999999997, 53.768, 70.77600000000001, 5.0
2019-02-16 12:25:17,760 : Dev mean Image to text: 26.240000000000002, 58.580000000000005, 74.46, 4.0
2019-02-16 12:25:17,761 : start epoch
2019-02-16 12:26:00,467 : samples : 64000
2019-02-16 12:26:10,973 : Image to text: 9.88, 29.22, 41.74, 16.0
2019-02-16 12:26:18,483 : Text to Image: 8.972, 26.132, 37.844, 19.0
2019-02-16 12:27:01,332 : samples : 128000
2019-02-16 12:27:11,876 : Image to text: 7.96, 25.5, 38.44, 17.0
2019-02-16 12:27:19,429 : Text to Image: 7.284, 22.772, 34.396, 22.0
2019-02-16 12:28:02,271 : samples : 192000
2019-02-16 12:28:12,841 : Image to text: 10.14, 28.96, 41.2, 15.0
2019-02-16 12:28:20,410 : Text to Image: 8.512, 25.124, 36.908, 20.0
2019-02-16 12:29:02,808 : samples : 256000
2019-02-16 12:29:13,307 : Image to text: 9.46, 29.04, 40.78, 16.0
2019-02-16 12:29:20,928 : Text to Image: 7.968, 24.36, 36.08, 20.0
2019-02-16 12:30:03,255 : samples : 320000
2019-02-16 12:30:13,823 : Image to text: 10.14, 29.16, 41.88, 15.0
2019-02-16 12:30:21,404 : Text to Image: 8.776, 25.092, 36.828, 19.0
2019-02-16 12:31:04,245 : samples : 384000
2019-02-16 12:31:14,725 : Image to text: 10.48, 29.66, 42.4, 15.0
2019-02-16 12:31:22,285 : Text to Image: 9.148, 25.988, 37.356, 19.0
2019-02-16 12:32:04,926 : samples : 448000
2019-02-16 12:32:15,197 : Image to text: 10.04, 29.98, 43.28, 15.0
2019-02-16 12:32:22,368 : Text to Image: 8.936, 26.32, 38.068, 19.0
2019-02-16 12:33:14,704 : samples : 512000
2019-02-16 12:33:25,048 : Image to text: 10.46, 29.68, 42.12, 15.0
2019-02-16 12:33:32,588 : Text to Image: 9.32, 26.692, 38.64, 18.0
2019-02-16 12:34:09,102 : Epoch 3 finished
2019-02-16 12:34:09,530 : Image to text: 24.3, 58.0, 73.8, 4.0
2019-02-16 12:34:09,870 : Text to Image: 20.82, 54.16, 71.46, 5.0
2019-02-16 12:34:10,300 : Image to text: 27.3, 59.1, 73.9, 4.0
2019-02-16 12:34:10,636 : Text to Image: 21.96, 53.28, 71.1, 5.0
2019-02-16 12:34:11,095 : Image to text: 25.6, 59.7, 75.6, 4.0
2019-02-16 12:34:11,433 : Text to Image: 20.86, 54.08, 71.4, 5.0
2019-02-16 12:34:11,881 : Image to text: 26.1, 60.3, 73.5, 4.0
2019-02-16 12:34:12,216 : Text to Image: 21.58, 54.54, 71.74, 5.0
2019-02-16 12:34:12,660 : Image to text: 26.2, 59.4, 74.5, 4.0
2019-02-16 12:34:12,999 : Text to Image: 21.86, 54.2, 70.1, 5.0
2019-02-16 12:34:13,000 : Dev mean Text to Image: 21.416, 54.05200000000001, 71.16, 5.0
2019-02-16 12:34:13,000 : Dev mean Image to text: 25.900000000000006, 59.3, 74.26, 4.0
2019-02-16 12:34:13,000 : start epoch
2019-02-16 12:34:55,857 : samples : 64000
2019-02-16 12:35:06,335 : Image to text: 10.24, 30.06, 42.48, 15.0
2019-02-16 12:35:13,861 : Text to Image: 8.996, 26.064, 37.752, 19.0
2019-02-16 12:35:56,221 : samples : 128000
2019-02-16 12:36:06,787 : Image to text: 10.28, 30.5, 42.66, 15.0
2019-02-16 12:36:14,332 : Text to Image: 8.968, 26.032, 37.864, 19.0
2019-02-16 12:36:56,655 : samples : 192000
2019-02-16 12:37:07,223 : Image to text: 10.08, 28.86, 42.14, 15.0
2019-02-16 12:37:14,779 : Text to Image: 8.524, 24.828, 36.856, 19.0
2019-02-16 12:37:57,045 : samples : 256000
2019-02-16 12:38:07,582 : Image to text: 10.3, 29.32, 41.56, 15.0
2019-02-16 12:38:15,143 : Text to Image: 9.184, 25.9, 38.144, 18.0
2019-02-16 12:38:57,908 : samples : 320000
2019-02-16 12:39:08,434 : Image to text: 10.5, 30.78, 43.82, 14.0
2019-02-16 12:39:16,001 : Text to Image: 9.604, 27.052, 39.084, 18.0
2019-02-16 12:39:58,236 : samples : 384000
2019-02-16 12:40:08,771 : Image to text: 10.08, 29.22, 41.96, 15.0
2019-02-16 12:40:16,259 : Text to Image: 8.636, 25.464, 37.32, 19.0
2019-02-16 12:40:59,655 : samples : 448000
2019-02-16 12:41:09,975 : Image to text: 10.32, 30.72, 44.08, 14.0
2019-02-16 12:41:17,462 : Text to Image: 9.752, 27.336, 39.08, 17.0
2019-02-16 12:42:00,216 : samples : 512000
2019-02-16 12:42:10,552 : Image to text: 10.72, 30.78, 43.96, 14.0
2019-02-16 12:42:18,137 : Text to Image: 9.284, 26.164, 38.016, 18.0
2019-02-16 12:42:54,603 : Epoch 4 finished
2019-02-16 12:42:55,041 : Image to text: 27.4, 59.7, 76.2, 4.0
2019-02-16 12:42:55,376 : Text to Image: 21.9, 55.52, 72.76, 4.0
2019-02-16 12:42:55,825 : Image to text: 28.1, 59.9, 74.8, 4.0
2019-02-16 12:42:56,164 : Text to Image: 22.38, 54.48, 71.56, 5.0
2019-02-16 12:42:56,604 : Image to text: 25.9, 60.5, 77.2, 4.0
2019-02-16 12:42:56,944 : Text to Image: 22.26, 55.4, 72.12, 5.0
2019-02-16 12:42:57,387 : Image to text: 29.1, 62.9, 77.2, 3.0
2019-02-16 12:42:57,724 : Text to Image: 22.8, 55.46, 72.36, 4.0
2019-02-16 12:42:58,176 : Image to text: 28.2, 59.3, 74.2, 4.0
2019-02-16 12:42:58,509 : Text to Image: 22.02, 55.62, 71.24, 4.0
2019-02-16 12:42:58,510 : Dev mean Text to Image: 22.272000000000002, 55.29599999999999, 72.00800000000001, 4.3999999999999995
2019-02-16 12:42:58,510 : Dev mean Image to text: 27.740000000000002, 60.46, 75.92, 3.8000000000000007
2019-02-16 12:42:58,510 : start epoch
2019-02-16 12:43:41,334 : samples : 64000
2019-02-16 12:43:51,853 : Image to text: 9.98, 29.92, 43.28, 14.0
2019-02-16 12:43:59,387 : Text to Image: 9.132, 26.872, 38.98, 17.0
2019-02-16 12:44:42,138 : samples : 128000
2019-02-16 12:44:52,749 : Image to text: 10.74, 30.96, 43.56, 14.0
2019-02-16 12:45:00,277 : Text to Image: 9.528, 27.196, 39.232, 17.0
2019-02-16 12:45:42,792 : samples : 192000
2019-02-16 12:45:53,307 : Image to text: 9.94, 29.72, 42.78, 14.0
2019-02-16 12:46:00,849 : Text to Image: 9.172, 26.54, 38.68, 18.0
2019-02-16 12:46:43,156 : samples : 256000
2019-02-16 12:46:53,677 : Image to text: 9.64, 29.36, 42.22, 16.0
2019-02-16 12:47:01,254 : Text to Image: 8.884, 26.324, 37.996, 18.0
2019-02-16 12:47:43,831 : samples : 320000
2019-02-16 12:47:54,351 : Image to text: 10.88, 30.36, 42.9, 15.0
2019-02-16 12:48:01,894 : Text to Image: 8.924, 26.376, 38.756, 18.0
2019-02-16 12:48:45,127 : samples : 384000
2019-02-16 12:48:55,707 : Image to text: 10.84, 30.64, 43.68, 14.0
2019-02-16 12:49:03,239 : Text to Image: 9.756, 27.82, 40.112, 17.0
2019-02-16 12:49:53,715 : samples : 448000
2019-02-16 12:50:06,746 : Image to text: 10.78, 30.46, 42.58, 15.0
2019-02-16 12:50:16,800 : Text to Image: 8.956, 26.516, 38.388, 18.0
2019-02-16 12:51:01,722 : samples : 512000
2019-02-16 12:51:11,950 : Image to text: 10.68, 31.14, 43.5, 14.0
2019-02-16 12:51:19,360 : Text to Image: 9.76, 27.648, 39.544, 17.0
2019-02-16 12:51:55,392 : Epoch 5 finished
2019-02-16 12:51:55,884 : Image to text: 28.4, 62.5, 77.3, 3.0
2019-02-16 12:51:56,255 : Text to Image: 23.48, 56.9, 73.68, 4.0
2019-02-16 12:51:56,709 : Image to text: 28.9, 61.8, 75.8, 3.0
2019-02-16 12:51:57,073 : Text to Image: 23.42, 55.18, 73.28, 5.0
2019-02-16 12:51:57,527 : Image to text: 27.7, 61.9, 77.2, 3.0
2019-02-16 12:51:57,892 : Text to Image: 23.58, 56.84, 72.48, 4.0
2019-02-16 12:51:58,344 : Image to text: 30.9, 64.8, 77.9, 3.0
2019-02-16 12:51:58,710 : Text to Image: 24.34, 57.76, 73.86, 4.0
2019-02-16 12:51:59,161 : Image to text: 28.9, 62.7, 76.9, 3.0
2019-02-16 12:51:59,441 : Text to Image: 23.58, 57.04, 72.92, 4.0
2019-02-16 12:51:59,441 : Dev mean Text to Image: 23.68, 56.744, 73.244, 4.2
2019-02-16 12:51:59,441 : Dev mean Image to text: 28.96, 62.739999999999995, 77.02, 3.0
2019-02-16 12:51:59,442 : start epoch
2019-02-16 12:52:43,326 : samples : 64000
2019-02-16 12:52:55,951 : Image to text: 11.44, 30.44, 44.32, 14.0
2019-02-16 12:53:06,050 : Text to Image: 9.484, 27.332, 39.264, 17.0
2019-02-16 12:53:49,420 : samples : 128000
2019-02-16 12:53:59,701 : Image to text: 10.8, 30.6, 42.96, 14.0
2019-02-16 12:54:06,934 : Text to Image: 9.42, 26.412, 38.228, 18.0
2019-02-16 12:54:48,635 : samples : 192000
2019-02-16 12:54:59,403 : Image to text: 10.34, 30.8, 44.1, 14.0
2019-02-16 12:55:09,478 : Text to Image: 9.056, 26.512, 38.38, 18.0
2019-02-16 12:55:54,011 : samples : 256000
2019-02-16 12:56:06,711 : Image to text: 10.9, 30.94, 43.78, 14.0
2019-02-16 12:56:16,770 : Text to Image: 9.728, 27.708, 39.776, 17.0
2019-02-16 12:56:59,355 : samples : 320000
2019-02-16 12:57:09,599 : Image to text: 10.42, 31.16, 44.46, 14.0
2019-02-16 12:57:16,890 : Text to Image: 9.596, 27.152, 39.332, 17.0
2019-02-16 12:58:00,661 : samples : 384000
2019-02-16 12:58:13,326 : Image to text: 11.08, 31.1, 44.5, 13.0
2019-02-16 12:58:23,368 : Text to Image: 10.052, 28.256, 40.42, 17.0
2019-02-16 12:59:06,640 : samples : 448000
2019-02-16 12:59:16,938 : Image to text: 11.02, 31.12, 44.54, 13.0
2019-02-16 12:59:24,294 : Text to Image: 9.716, 27.624, 39.788, 17.0
2019-02-16 13:00:07,897 : samples : 512000
2019-02-16 13:00:20,508 : Image to text: 11.24, 30.46, 43.12, 14.0
2019-02-16 13:00:30,590 : Text to Image: 9.68, 27.58, 39.608, 17.0
2019-02-16 13:01:08,715 : Epoch 6 finished
2019-02-16 13:01:09,192 : Image to text: 28.6, 61.6, 77.5, 3.0
2019-02-16 13:01:09,570 : Text to Image: 23.9, 57.78, 75.64, 4.0
2019-02-16 13:01:10,027 : Image to text: 28.4, 62.7, 75.5, 3.0
2019-02-16 13:01:10,388 : Text to Image: 23.76, 56.96, 74.26, 4.0
2019-02-16 13:01:10,850 : Image to text: 27.9, 60.7, 76.8, 3.0
2019-02-16 13:01:11,219 : Text to Image: 24.22, 58.5, 74.68, 4.0
2019-02-16 13:01:11,684 : Image to text: 30.1, 64.7, 78.8, 3.0
2019-02-16 13:01:12,052 : Text to Image: 24.36, 58.3, 74.84, 4.0
2019-02-16 13:01:12,507 : Image to text: 29.8, 62.6, 76.3, 3.0
2019-02-16 13:01:12,875 : Text to Image: 24.38, 57.96, 73.48, 4.0
2019-02-16 13:01:12,875 : Dev mean Text to Image: 24.123999999999995, 57.89999999999999, 74.58, 4.0
2019-02-16 13:01:12,875 : Dev mean Image to text: 28.96, 62.459999999999994, 76.98, 3.0
2019-02-16 13:01:12,876 : start epoch
2019-02-16 13:01:55,315 : samples : 64000
2019-02-16 13:02:06,094 : Image to text: 11.0, 31.72, 44.06, 14.0
2019-02-16 13:02:16,110 : Text to Image: 9.892, 27.552, 40.232, 17.0
2019-02-16 13:03:01,062 : samples : 128000
2019-02-16 13:03:13,692 : Image to text: 10.7, 31.32, 44.38, 14.0
2019-02-16 13:03:23,759 : Text to Image: 9.804, 27.708, 39.784, 17.0
2019-02-16 13:04:06,282 : samples : 192000
2019-02-16 13:04:16,623 : Image to text: 10.94, 31.88, 44.92, 13.0
2019-02-16 13:04:23,727 : Text to Image: 9.856, 27.712, 40.104, 16.0
2019-02-16 13:05:08,195 : samples : 256000
2019-02-16 13:05:20,815 : Image to text: 10.94, 31.56, 44.86, 14.0
2019-02-16 13:05:30,873 : Text to Image: 9.96, 28.316, 40.736, 16.0
2019-02-16 13:06:14,056 : samples : 320000
2019-02-16 13:06:25,020 : Image to text: 10.78, 31.4, 43.96, 14.0
2019-02-16 13:06:33,639 : Text to Image: 9.88, 27.872, 40.404, 16.0
2019-02-16 13:07:22,839 : samples : 384000
2019-02-16 13:07:35,478 : Image to text: 10.86, 30.9, 43.84, 14.0
2019-02-16 13:07:45,493 : Text to Image: 10.024, 28.14, 40.656, 16.0
2019-02-16 13:08:28,924 : samples : 448000
2019-02-16 13:08:39,123 : Image to text: 11.58, 31.26, 44.64, 13.0
2019-02-16 13:08:46,528 : Text to Image: 10.088, 28.292, 40.748, 16.0
2019-02-16 13:09:29,764 : samples : 512000
2019-02-16 13:09:42,348 : Image to text: 11.46, 32.18, 45.28, 13.0
2019-02-16 13:09:52,406 : Text to Image: 10.364, 28.468, 40.792, 16.0
2019-02-16 13:10:31,251 : Epoch 7 finished
2019-02-16 13:10:32,258 : Image to text: 27.0, 61.3, 75.1, 4.0
2019-02-16 13:10:33,062 : Text to Image: 23.04, 57.36, 75.16, 4.0
2019-02-16 13:10:34,039 : Image to text: 28.0, 62.3, 76.1, 4.0
2019-02-16 13:10:34,846 : Text to Image: 23.32, 56.68, 73.78, 4.0
2019-02-16 13:10:35,725 : Image to text: 26.2, 60.6, 77.1, 4.0
2019-02-16 13:10:36,521 : Text to Image: 23.58, 57.28, 74.0, 4.0
2019-02-16 13:10:37,467 : Image to text: 28.8, 63.6, 77.6, 3.0
2019-02-16 13:10:38,231 : Text to Image: 24.74, 59.5, 75.22, 4.0
2019-02-16 13:10:39,181 : Image to text: 29.6, 59.9, 75.6, 3.0
2019-02-16 13:10:39,970 : Text to Image: 24.14, 57.5, 73.28, 4.0
2019-02-16 13:10:39,970 : Dev mean Text to Image: 23.763999999999996, 57.663999999999994, 74.28800000000001, 4.0
2019-02-16 13:10:39,970 : Dev mean Image to text: 27.92, 61.540000000000006, 76.3, 3.6000000000000005
2019-02-16 13:10:39,971 : start epoch
2019-02-16 13:11:23,200 : samples : 64000
2019-02-16 13:11:33,471 : Image to text: 11.6, 31.94, 45.34, 13.0
2019-02-16 13:11:40,613 : Text to Image: 9.992, 27.808, 40.18, 17.0
2019-02-16 13:12:24,963 : samples : 128000
2019-02-16 13:12:37,598 : Image to text: 11.74, 32.02, 45.2, 13.0
2019-02-16 13:12:47,672 : Text to Image: 9.688, 27.72, 40.172, 17.0
2019-02-16 13:13:32,324 : samples : 192000
2019-02-16 13:13:42,547 : Image to text: 11.42, 32.0, 45.16, 13.0
2019-02-16 13:13:49,990 : Text to Image: 9.972, 28.124, 40.536, 16.0
2019-02-16 13:14:32,669 : samples : 256000
2019-02-16 13:14:43,913 : Image to text: 11.66, 32.34, 45.1, 13.0
2019-02-16 13:14:52,575 : Text to Image: 9.808, 27.872, 40.312, 16.0
2019-02-16 13:15:35,989 : samples : 320000
2019-02-16 13:15:46,313 : Image to text: 11.56, 32.34, 46.02, 13.0
2019-02-16 13:15:53,647 : Text to Image: 10.352, 29.232, 41.228, 16.0
2019-02-16 13:16:36,333 : samples : 384000
2019-02-16 13:16:46,623 : Image to text: 11.66, 32.92, 45.6, 13.0
2019-02-16 13:16:54,738 : Text to Image: 10.24, 28.32, 40.344, 16.0
2019-02-16 13:17:37,258 : samples : 448000
2019-02-16 13:17:47,508 : Image to text: 11.7, 32.5, 46.58, 12.0
2019-02-16 13:17:54,857 : Text to Image: 10.416, 28.536, 40.88, 16.0
2019-02-16 13:18:37,435 : samples : 512000
2019-02-16 13:18:47,665 : Image to text: 10.78, 31.6, 45.06, 13.0
2019-02-16 13:18:55,103 : Text to Image: 9.984, 27.652, 39.584, 17.0
2019-02-16 13:19:32,236 : Epoch 8 finished
2019-02-16 13:19:32,700 : Image to text: 29.9, 63.6, 80.1, 3.0
2019-02-16 13:19:33,061 : Text to Image: 23.88, 58.4, 74.72, 4.0
2019-02-16 13:19:33,523 : Image to text: 30.7, 61.9, 77.2, 3.0
2019-02-16 13:19:33,885 : Text to Image: 24.22, 57.06, 73.88, 4.0
2019-02-16 13:19:34,357 : Image to text: 27.5, 62.6, 79.0, 3.0
2019-02-16 13:19:34,728 : Text to Image: 24.08, 58.02, 74.6, 4.0
2019-02-16 13:19:35,194 : Image to text: 29.8, 63.7, 79.5, 3.0
2019-02-16 13:19:35,565 : Text to Image: 25.14, 59.68, 75.6, 4.0
2019-02-16 13:19:36,030 : Image to text: 30.2, 62.5, 77.0, 3.0
2019-02-16 13:19:36,406 : Text to Image: 24.48, 58.64, 74.5, 4.0
2019-02-16 13:19:36,406 : Dev mean Text to Image: 24.36, 58.36, 74.66, 4.0
2019-02-16 13:19:36,406 : Dev mean Image to text: 29.619999999999997, 62.86000000000001, 78.56, 3.0
2019-02-16 13:19:36,406 : start epoch
2019-02-16 13:20:18,993 : samples : 64000
2019-02-16 13:20:29,217 : Image to text: 11.06, 31.36, 44.9, 13.0
2019-02-16 13:20:36,665 : Text to Image: 10.328, 28.42, 40.664, 16.0
2019-02-16 13:21:20,260 : samples : 128000
2019-02-16 13:21:32,994 : Image to text: 11.66, 33.02, 46.42, 12.0
2019-02-16 13:21:42,232 : Text to Image: 9.824, 27.72, 40.148, 16.0
2019-02-16 13:22:25,286 : samples : 192000
2019-02-16 13:22:36,770 : Image to text: 11.76, 32.5, 45.9, 13.0
2019-02-16 13:22:44,685 : Text to Image: 10.68, 28.892, 41.292, 16.0
2019-02-16 13:23:32,509 : samples : 256000
2019-02-16 13:23:44,556 : Image to text: 11.62, 32.0, 44.44, 14.0
2019-02-16 13:23:54,664 : Text to Image: 9.808, 27.756, 40.04, 16.0
2019-02-16 13:24:37,807 : samples : 320000
2019-02-16 13:24:48,033 : Image to text: 11.52, 31.64, 45.08, 13.0
2019-02-16 13:24:55,388 : Text to Image: 10.052, 27.964, 40.2, 17.0
2019-02-16 13:25:39,479 : samples : 384000
2019-02-16 13:25:52,059 : Image to text: 11.54, 32.24, 45.06, 13.0
2019-02-16 13:26:02,012 : Text to Image: 10.184, 28.28, 40.844, 16.0
2019-02-16 13:26:46,948 : samples : 448000
2019-02-16 13:26:59,554 : Image to text: 12.04, 32.46, 45.78, 13.0
2019-02-16 13:27:09,501 : Text to Image: 10.064, 28.216, 40.32, 16.0
2019-02-16 13:27:54,384 : samples : 512000
2019-02-16 13:28:07,017 : Image to text: 11.84, 31.52, 45.16, 13.0
2019-02-16 13:28:17,015 : Text to Image: 9.912, 28.3, 40.652, 16.0
2019-02-16 13:28:55,195 : Epoch 9 finished
2019-02-16 13:28:56,131 : Image to text: 31.0, 65.0, 78.8, 3.0
2019-02-16 13:28:56,857 : Text to Image: 25.36, 60.1, 76.46, 4.0
2019-02-16 13:28:57,767 : Image to text: 30.2, 64.3, 77.9, 3.0
2019-02-16 13:28:58,533 : Text to Image: 25.48, 58.0, 75.52, 4.0
2019-02-16 13:28:59,437 : Image to text: 28.1, 64.8, 78.0, 3.0
2019-02-16 13:29:00,189 : Text to Image: 25.66, 60.1, 75.92, 4.0
2019-02-16 13:29:01,120 : Image to text: 30.7, 64.5, 78.3, 3.0
2019-02-16 13:29:01,903 : Text to Image: 25.72, 60.42, 76.18, 4.0
2019-02-16 13:29:02,798 : Image to text: 29.8, 62.8, 77.5, 3.0
2019-02-16 13:29:03,547 : Text to Image: 26.16, 59.34, 74.6, 4.0
2019-02-16 13:29:03,547 : Dev mean Text to Image: 25.676, 59.592000000000006, 75.736, 4.0
2019-02-16 13:29:03,547 : Dev mean Image to text: 29.96, 64.28, 78.10000000000001, 3.0
2019-02-16 13:29:03,548 : start epoch
2019-02-16 13:29:48,315 : samples : 64000
2019-02-16 13:30:01,002 : Image to text: 11.24, 32.12, 44.74, 13.0
2019-02-16 13:30:11,128 : Text to Image: 9.82, 27.624, 40.1, 17.0
2019-02-16 13:30:55,733 : samples : 128000
2019-02-16 13:31:08,360 : Image to text: 11.82, 33.22, 46.72, 12.0
2019-02-16 13:31:18,389 : Text to Image: 10.36, 29.076, 41.44, 16.0
2019-02-16 13:32:03,366 : samples : 192000
2019-02-16 13:32:15,976 : Image to text: 11.86, 33.5, 46.18, 12.0
2019-02-16 13:32:25,998 : Text to Image: 10.172, 28.82, 41.12, 16.0
2019-02-16 13:33:10,790 : samples : 256000
2019-02-16 13:33:23,396 : Image to text: 11.2, 32.2, 45.74, 12.0
2019-02-16 13:33:33,441 : Text to Image: 9.988, 28.764, 41.044, 16.0
2019-02-16 13:34:17,858 : samples : 320000
2019-02-16 13:34:30,500 : Image to text: 11.5, 31.94, 45.72, 13.0
2019-02-16 13:34:40,529 : Text to Image: 10.196, 28.812, 41.348, 16.0
2019-02-16 13:35:24,728 : samples : 384000
2019-02-16 13:35:37,358 : Image to text: 11.74, 33.24, 46.68, 12.0
2019-02-16 13:35:47,446 : Text to Image: 10.472, 29.116, 41.852, 15.0
2019-02-16 13:36:32,869 : samples : 448000
2019-02-16 13:36:45,557 : Image to text: 11.98, 33.76, 46.9, 12.0
2019-02-16 13:36:53,747 : Text to Image: 10.216, 29.016, 41.612, 15.0
2019-02-16 13:37:36,134 : samples : 512000
2019-02-16 13:37:46,404 : Image to text: 11.8, 31.96, 45.22, 13.0
2019-02-16 13:37:53,835 : Text to Image: 10.108, 28.676, 41.036, 16.0
2019-02-16 13:38:29,776 : Epoch 10 finished
2019-02-16 13:38:30,172 : Image to text: 30.1, 62.2, 79.0, 3.0
2019-02-16 13:38:30,455 : Text to Image: 25.72, 60.0, 76.86, 4.0
2019-02-16 13:38:30,847 : Image to text: 30.5, 64.1, 77.8, 3.0
2019-02-16 13:38:31,138 : Text to Image: 25.26, 58.58, 75.74, 4.0
2019-02-16 13:38:31,882 : Image to text: 28.0, 63.2, 79.0, 3.0
2019-02-16 13:38:32,763 : Text to Image: 25.22, 59.18, 75.98, 4.0
2019-02-16 13:38:33,907 : Image to text: 30.2, 64.7, 79.1, 3.0
2019-02-16 13:38:34,817 : Text to Image: 26.12, 60.06, 76.7, 4.0
2019-02-16 13:38:35,887 : Image to text: 29.5, 63.1, 78.9, 3.0
2019-02-16 13:38:36,792 : Text to Image: 26.02, 59.22, 75.12, 4.0
2019-02-16 13:38:36,792 : Dev mean Text to Image: 25.668000000000003, 59.408, 76.08, 4.0
2019-02-16 13:38:36,792 : Dev mean Image to text: 29.659999999999997, 63.46000000000001, 78.75999999999999, 3.0
2019-02-16 13:38:36,792 : start epoch
2019-02-16 13:39:22,581 : samples : 64000
2019-02-16 13:39:35,421 : Image to text: 12.26, 32.68, 46.44, 12.0
2019-02-16 13:39:45,843 : Text to Image: 10.32, 29.36, 41.644, 15.0
2019-02-16 13:40:37,545 : samples : 128000
2019-02-16 13:40:50,690 : Image to text: 11.74, 32.02, 45.14, 13.0
2019-02-16 13:41:01,190 : Text to Image: 9.688, 27.98, 40.588, 16.0
2019-02-16 13:41:46,747 : samples : 192000
2019-02-16 13:41:59,662 : Image to text: 12.02, 32.26, 46.12, 12.0
2019-02-16 13:42:10,155 : Text to Image: 10.236, 28.98, 41.344, 16.0
2019-02-16 13:42:55,772 : samples : 256000
2019-02-16 13:43:08,716 : Image to text: 12.5, 32.62, 46.84, 12.0
2019-02-16 13:43:19,230 : Text to Image: 10.604, 29.304, 41.8, 15.0
2019-02-16 13:44:04,920 : samples : 320000
2019-02-16 13:44:17,792 : Image to text: 11.74, 32.6, 45.68, 13.0
2019-02-16 13:44:28,194 : Text to Image: 10.032, 28.612, 40.988, 16.0
2019-02-16 13:45:14,301 : samples : 384000
2019-02-16 13:45:27,217 : Image to text: 12.02, 33.54, 46.46, 13.0
2019-02-16 13:45:37,685 : Text to Image: 10.716, 29.628, 42.484, 15.0
2019-02-16 13:46:23,464 : samples : 448000
2019-02-16 13:46:36,379 : Image to text: 11.82, 32.18, 45.2, 13.0
2019-02-16 13:46:45,064 : Text to Image: 10.268, 29.156, 41.628, 15.0
2019-02-16 13:47:27,329 : samples : 512000
2019-02-16 13:47:37,587 : Image to text: 11.42, 32.62, 46.14, 12.0
2019-02-16 13:47:45,058 : Text to Image: 10.376, 29.024, 41.336, 16.0
2019-02-16 13:48:21,340 : Epoch 11 finished
2019-02-16 13:48:21,785 : Image to text: 29.8, 63.3, 78.9, 3.0
2019-02-16 13:48:22,134 : Text to Image: 24.62, 60.04, 76.58, 4.0
2019-02-16 13:48:22,593 : Image to text: 29.5, 63.9, 76.7, 3.0
2019-02-16 13:48:22,936 : Text to Image: 25.74, 57.78, 75.2, 4.0
2019-02-16 13:48:23,372 : Image to text: 27.9, 63.6, 79.9, 3.0
2019-02-16 13:48:23,717 : Text to Image: 25.28, 59.56, 75.24, 4.0
2019-02-16 13:48:24,233 : Image to text: 31.1, 63.2, 78.8, 3.0
2019-02-16 13:48:24,591 : Text to Image: 25.1, 60.56, 76.06, 4.0
2019-02-16 13:48:25,065 : Image to text: 29.7, 63.6, 78.6, 3.0
2019-02-16 13:48:25,418 : Text to Image: 25.12, 58.96, 74.92, 4.0
2019-02-16 13:48:25,418 : Dev mean Text to Image: 25.172, 59.38, 75.6, 4.0
2019-02-16 13:48:25,418 : Dev mean Image to text: 29.599999999999994, 63.519999999999996, 78.58, 3.0
2019-02-16 13:48:25,418 : start epoch
2019-02-16 13:49:08,146 : samples : 64000
2019-02-16 13:49:18,638 : Image to text: 11.46, 33.1, 46.52, 12.0
2019-02-16 13:49:26,211 : Text to Image: 10.54, 29.336, 41.86, 15.0
2019-02-16 13:50:08,686 : samples : 128000
2019-02-16 13:50:19,262 : Image to text: 11.6, 33.06, 46.86, 13.0
2019-02-16 13:50:26,836 : Text to Image: 9.968, 28.656, 41.18, 16.0
2019-02-16 13:51:09,066 : samples : 192000
2019-02-16 13:51:19,512 : Image to text: 12.12, 33.28, 46.82, 12.0
2019-02-16 13:51:27,120 : Text to Image: 10.488, 29.396, 42.016, 15.0
2019-02-16 13:52:09,756 : samples : 256000
2019-02-16 13:52:20,335 : Image to text: 12.22, 32.82, 46.16, 13.0
2019-02-16 13:52:27,949 : Text to Image: 10.292, 29.06, 41.62, 15.0
2019-02-16 13:53:10,277 : samples : 320000
2019-02-16 13:53:20,812 : Image to text: 12.46, 32.16, 45.44, 13.0
2019-02-16 13:53:28,437 : Text to Image: 10.204, 28.6, 41.408, 16.0
2019-02-16 13:54:10,528 : samples : 384000
2019-02-16 13:54:21,046 : Image to text: 11.32, 33.34, 46.16, 13.0
2019-02-16 13:54:28,614 : Text to Image: 10.408, 29.456, 41.76, 15.0
2019-02-16 13:55:10,897 : samples : 448000
2019-02-16 13:55:21,412 : Image to text: 12.54, 33.12, 46.14, 12.0
2019-02-16 13:55:28,900 : Text to Image: 10.78, 29.176, 41.58, 15.0
2019-02-16 13:56:11,275 : samples : 512000
2019-02-16 13:56:21,494 : Image to text: 12.06, 32.7, 46.42, 12.0
2019-02-16 13:56:28,961 : Text to Image: 10.616, 29.652, 42.412, 15.0
2019-02-16 13:57:07,027 : Epoch 12 finished
2019-02-16 13:57:07,579 : Image to text: 30.3, 63.0, 79.2, 3.0
2019-02-16 13:57:08,016 : Text to Image: 25.2, 59.54, 76.6, 4.0
2019-02-16 13:57:08,580 : Image to text: 30.4, 63.7, 78.2, 3.0
2019-02-16 13:57:09,002 : Text to Image: 25.42, 58.32, 75.4, 4.0
2019-02-16 13:57:09,570 : Image to text: 28.6, 63.1, 78.0, 3.0
2019-02-16 13:57:10,008 : Text to Image: 24.86, 59.8, 76.52, 4.0
2019-02-16 13:57:10,557 : Image to text: 30.7, 65.0, 78.5, 3.0
2019-02-16 13:57:10,999 : Text to Image: 25.44, 60.92, 76.76, 4.0
2019-02-16 13:57:11,563 : Image to text: 30.1, 63.5, 78.5, 3.0
2019-02-16 13:57:11,985 : Text to Image: 25.6, 59.88, 75.34, 4.0
2019-02-16 13:57:11,985 : Dev mean Text to Image: 25.304000000000002, 59.69199999999999, 76.124, 4.0
2019-02-16 13:57:11,985 : Dev mean Image to text: 30.02, 63.66, 78.48, 3.0
2019-02-16 13:57:17,095 : 
Test scores | Image to text:             29.38, 62.78, 78.32000000000001, 3.1999999999999997
2019-02-16 13:57:17,096 : Test scores | Text to image:             25.112000000000002, 58.980000000000004, 75.156, 4.0

2019-02-16 13:57:17,251 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 13:57:17,593 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 13:57:18,371 : loading BERT model bert-base-uncased
2019-02-16 13:57:18,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:57:18,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:57:18,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_flfkuo2
2019-02-16 13:57:21,653 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:57:23,979 : Computing embeddings for train/dev/test
2019-02-16 13:59:00,617 : Computed embeddings
2019-02-16 13:59:00,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:59:46,868 : [('reg:1e-05', 63.96), ('reg:0.0001', 59.53), ('reg:0.001', 62.64), ('reg:0.01', 54.03)]
2019-02-16 13:59:46,868 : Validation : best param found is reg = 1e-05 with score             63.96
2019-02-16 13:59:46,868 : Evaluating...
2019-02-16 13:59:59,078 : 
Dev acc : 64.0 Test acc : 64.3 for LENGTH classification

2019-02-16 13:59:59,078 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 13:59:59,471 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 13:59:59,522 : loading BERT model bert-base-uncased
2019-02-16 13:59:59,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:59:59,557 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:59:59,558 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiiifj2__
2019-02-16 14:00:02,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:00:03,539 : Computing embeddings for train/dev/test
2019-02-16 14:01:32,528 : Computed embeddings
2019-02-16 14:01:32,529 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:02:30,580 : [('reg:1e-05', 18.34), ('reg:0.0001', 8.3), ('reg:0.001', 0.57), ('reg:0.01', 0.19)]
2019-02-16 14:02:30,581 : Validation : best param found is reg = 1e-05 with score             18.34
2019-02-16 14:02:30,581 : Evaluating...
2019-02-16 14:02:47,936 : 
Dev acc : 18.3 Test acc : 18.3 for WORDCONTENT classification

2019-02-16 14:02:47,938 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 14:02:48,254 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 14:02:48,324 : loading BERT model bert-base-uncased
2019-02-16 14:02:48,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:02:48,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:02:48,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcwdw7ewg
2019-02-16 14:02:50,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:02:52,338 : Computing embeddings for train/dev/test
2019-02-16 14:04:15,402 : Computed embeddings
2019-02-16 14:04:15,402 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:04:57,616 : [('reg:1e-05', 29.5), ('reg:0.0001', 29.02), ('reg:0.001', 26.77), ('reg:0.01', 27.0)]
2019-02-16 14:04:57,616 : Validation : best param found is reg = 1e-05 with score             29.5
2019-02-16 14:04:57,616 : Evaluating...
2019-02-16 14:05:11,688 : 
Dev acc : 29.5 Test acc : 29.8 for DEPTH classification

2019-02-16 14:05:11,689 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 14:05:12,125 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 14:05:12,193 : loading BERT model bert-base-uncased
2019-02-16 14:05:12,193 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:05:12,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:05:12,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptyqx7zcq
2019-02-16 14:05:14,729 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:05:16,211 : Computing embeddings for train/dev/test
2019-02-16 14:06:34,696 : Computed embeddings
2019-02-16 14:06:34,696 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:07:24,610 : [('reg:1e-05', 61.71), ('reg:0.0001', 59.04), ('reg:0.001', 55.3), ('reg:0.01', 39.39)]
2019-02-16 14:07:24,610 : Validation : best param found is reg = 1e-05 with score             61.71
2019-02-16 14:07:24,610 : Evaluating...
2019-02-16 14:07:38,043 : 
Dev acc : 61.7 Test acc : 61.6 for TOPCONSTITUENTS classification

2019-02-16 14:07:38,044 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 14:07:38,402 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 14:07:38,468 : loading BERT model bert-base-uncased
2019-02-16 14:07:38,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:07:38,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:07:38,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmj7yxkcw
2019-02-16 14:07:41,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:07:42,443 : Computing embeddings for train/dev/test
2019-02-16 14:09:06,685 : Computed embeddings
2019-02-16 14:09:06,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:10:07,501 : [('reg:1e-05', 86.1), ('reg:0.0001', 86.07), ('reg:0.001', 85.94), ('reg:0.01', 79.4)]
2019-02-16 14:10:07,502 : Validation : best param found is reg = 1e-05 with score             86.1
2019-02-16 14:10:07,502 : Evaluating...
2019-02-16 14:10:24,174 : 
Dev acc : 86.1 Test acc : 85.0 for BIGRAMSHIFT classification

2019-02-16 14:10:24,175 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 14:10:24,637 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 14:10:24,710 : loading BERT model bert-base-uncased
2019-02-16 14:10:24,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:10:24,745 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:10:24,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkprug5r_
2019-02-16 14:10:27,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:10:28,760 : Computing embeddings for train/dev/test
2019-02-16 14:11:51,772 : Computed embeddings
2019-02-16 14:11:51,772 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:12:33,984 : [('reg:1e-05', 87.61), ('reg:0.0001', 87.71), ('reg:0.001', 87.5), ('reg:0.01', 86.99)]
2019-02-16 14:12:33,984 : Validation : best param found is reg = 0.0001 with score             87.71
2019-02-16 14:12:33,984 : Evaluating...
2019-02-16 14:12:44,002 : 
Dev acc : 87.7 Test acc : 86.4 for TENSE classification

2019-02-16 14:12:44,003 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 14:12:44,455 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 14:12:44,521 : loading BERT model bert-base-uncased
2019-02-16 14:12:44,522 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:12:44,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:12:44,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphoz7k_qx
2019-02-16 14:12:47,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:12:48,574 : Computing embeddings for train/dev/test
2019-02-16 14:14:18,266 : Computed embeddings
2019-02-16 14:14:18,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:14:58,440 : [('reg:1e-05', 81.25), ('reg:0.0001', 81.31), ('reg:0.001', 81.43), ('reg:0.01', 81.25)]
2019-02-16 14:14:58,440 : Validation : best param found is reg = 0.001 with score             81.43
2019-02-16 14:14:58,440 : Evaluating...
2019-02-16 14:15:13,034 : 
Dev acc : 81.4 Test acc : 80.6 for SUBJNUMBER classification

2019-02-16 14:15:13,035 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 14:15:13,691 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 14:15:13,773 : loading BERT model bert-base-uncased
2019-02-16 14:15:13,773 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:15:13,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:15:13,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzfr75z27
2019-02-16 14:15:16,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:15:17,794 : Computing embeddings for train/dev/test
2019-02-16 14:16:43,777 : Computed embeddings
2019-02-16 14:16:43,777 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:17:40,627 : [('reg:1e-05', 78.7), ('reg:0.0001', 78.8), ('reg:0.001', 78.8), ('reg:0.01', 77.66)]
2019-02-16 14:17:40,627 : Validation : best param found is reg = 0.0001 with score             78.8
2019-02-16 14:17:40,627 : Evaluating...
2019-02-16 14:17:57,174 : 
Dev acc : 78.8 Test acc : 79.4 for OBJNUMBER classification

2019-02-16 14:17:57,176 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 14:17:57,595 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 14:17:57,670 : loading BERT model bert-base-uncased
2019-02-16 14:17:57,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:17:57,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:17:57,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnyoeut6m
2019-02-16 14:18:00,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:18:01,813 : Computing embeddings for train/dev/test
2019-02-16 14:19:41,234 : Computed embeddings
2019-02-16 14:19:41,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:20:26,143 : [('reg:1e-05', 60.02), ('reg:0.0001', 59.98), ('reg:0.001', 60.01), ('reg:0.01', 60.47)]
2019-02-16 14:20:26,143 : Validation : best param found is reg = 0.01 with score             60.47
2019-02-16 14:20:26,143 : Evaluating...
2019-02-16 14:20:36,706 : 
Dev acc : 60.5 Test acc : 59.1 for ODDMANOUT classification

2019-02-16 14:20:36,707 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 14:20:37,352 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 14:20:37,435 : loading BERT model bert-base-uncased
2019-02-16 14:20:37,435 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:20:37,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:20:37,475 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu0w3u_47
2019-02-16 14:20:39,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:20:41,445 : Computing embeddings for train/dev/test
2019-02-16 14:22:18,682 : Computed embeddings
2019-02-16 14:22:18,682 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:22:59,467 : [('reg:1e-05', 55.73), ('reg:0.0001', 55.68), ('reg:0.001', 55.29), ('reg:0.01', 51.64)]
2019-02-16 14:22:59,468 : Validation : best param found is reg = 1e-05 with score             55.73
2019-02-16 14:22:59,468 : Evaluating...
2019-02-16 14:23:09,929 : 
Dev acc : 55.7 Test acc : 55.4 for COORDINATIONINVERSION classification

2019-02-16 14:23:09,931 : total results: {'STS12': {'MSRpar': {'pearson': (0.2981410492071199, 7.319850661186964e-17), 'spearman': SpearmanrResult(correlation=0.3331825448570781, pvalue=6.6828614201996355e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6186565673076134, 2.035232469419895e-80), 'spearman': SpearmanrResult(correlation=0.6304397991394215, pvalue=2.401999734591585e-84), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47107265805056026, 9.90345978754508e-27), 'spearman': SpearmanrResult(correlation=0.5950121401201401, pvalue=2.6765751582712125e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5413159011007462, 2.5562644865014283e-58), 'spearman': SpearmanrResult(correlation=0.5686482153884025, pvalue=1.8296772185126702e-65), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.564127802804599, 6.674611077818537e-35), 'spearman': SpearmanrResult(correlation=0.4660537254008056, pvalue=6.591969228250448e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49866279569412775, 'wmean': 0.4938527932998236}, 'spearman': {'mean': 0.5186672849811697, 'wmean': 0.517461045137948}}}, 'STS13': {'FNWN': {'pearson': (0.156778583628512, 0.031209504070672363), 'spearman': SpearmanrResult(correlation=0.16860299612689955, pvalue=0.020386424368231997), 'nsamples': 189}, 'headlines': {'pearson': (0.630826140380286, 1.773786736844811e-84), 'spearman': SpearmanrResult(correlation=0.6260985444682912, pvalue=7.037366521598613e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.47390270950806895, 9.434464281811659e-33), 'spearman': SpearmanrResult(correlation=0.46535678937868863, pvalue=1.7083446010799967e-31), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42050247783895567, 'wmean': 0.5124067850833532}, 'spearman': {'mean': 0.42001944332462643, 'wmean': 0.5083366889737645}}}, 'STS14': {'deft-forum': {'pearson': (0.37235916681612463, 3.0156804205097445e-16), 'spearman': SpearmanrResult(correlation=0.379466399961024, pvalue=7.34590925466866e-17), 'nsamples': 450}, 'deft-news': {'pearson': (0.7229884222384468, 8.755601030699382e-50), 'spearman': SpearmanrResult(correlation=0.700885708308919, pvalue=1.2236743630558444e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5802143265312536, 1.0770386836215897e-68), 'spearman': SpearmanrResult(correlation=0.553589769670222, pvalue=1.9086185397824357e-61), 'nsamples': 750}, 'images': {'pearson': (0.5511582499416293, 8.137837674132223e-61), 'spearman': SpearmanrResult(correlation=0.5464585749422874, pvalue=1.2977804957757058e-59), 'nsamples': 750}, 'OnWN': {'pearson': (0.6084401156092961, 3.815939536603471e-77), 'spearman': SpearmanrResult(correlation=0.6376657207376274, pvalue=7.701601335405377e-87), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5872107455632243, 1.0344527079680136e-70), 'spearman': SpearmanrResult(correlation=0.559111519513651, pvalue=6.777977788484154e-63), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5703951711166625, 'wmean': 0.5679268613260914}, 'spearman': {'mean': 0.5628629488556218, 'wmean': 0.560971941632794}}}, 'STS15': {'answers-forums': {'pearson': (0.5035991412131355, 1.6529581800386946e-25), 'spearman': SpearmanrResult(correlation=0.48827223724801727, pvalue=7.320517356638968e-24), 'nsamples': 375}, 'answers-students': {'pearson': (0.6577261125554303, 3.977507725688212e-94), 'spearman': SpearmanrResult(correlation=0.665964534848632, pvalue=2.7674846787225974e-97), 'nsamples': 750}, 'belief': {'pearson': (0.5305216635415728, 1.309326068349684e-28), 'spearman': SpearmanrResult(correlation=0.5718477504609795, pvalue=6.022633443655633e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.6244384950764078, 2.5246285745667938e-82), 'spearman': SpearmanrResult(correlation=0.6322770371077504, pvalue=5.65900562690981e-85), 'nsamples': 750}, 'images': {'pearson': (0.6921014200382868, 5.24796803367259e-108), 'spearman': SpearmanrResult(correlation=0.7004131641223772, pvalue=1.1634765281438695e-111), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6016773664849666, 'wmean': 0.6228316075118697}, 'spearman': {'mean': 0.6117549447575513, 'wmean': 0.6321786824833145}}}, 'STS16': {'answer-answer': {'pearson': (0.4950797907235934, 4.1190297024495717e-17), 'spearman': SpearmanrResult(correlation=0.5139691106167098, pvalue=1.5835207172114458e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6284704361803324, 8.979086375885866e-29), 'spearman': SpearmanrResult(correlation=0.6356169093484715, pvalue=1.3852764866977421e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6729305835354212, 1.0907097938712454e-31), 'spearman': SpearmanrResult(correlation=0.678539147707006, pvalue=2.2055982331423917e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7648531745441184, 4.0599783613769126e-48), 'spearman': SpearmanrResult(correlation=0.813402629165494, pvalue=7.173707270166841e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.23004956521313488, 0.0008054960668942018), 'spearman': SpearmanrResult(correlation=0.24676626826956574, pvalue=0.0003158681742155237), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.55827671003932, 'wmean': 0.5663726588399262}, 'spearman': {'mean': 0.5776588130214494, 'wmean': 0.5859402698834273}}}, 'MR': {'devacc': 64.4, 'acc': 65.46, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 74.5, 'acc': 73.72, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.6, 'acc': 86.78, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.86, 'acc': 91.94, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.42, 'acc': 79.35, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.15, 'acc': 42.04, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 73.22, 'acc': 88.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.03, 'acc': 69.62, 'f1': 75.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.0, 'acc': 75.08, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7874576173065023, 'pearson': 0.7788641232113926, 'spearman': 0.7137650912235657, 'mse': 0.4040303642313609, 'yhat': array([2.64468767, 4.0588832 , 3.05230148, ..., 2.98883684, 4.85290666,        4.65678326]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.646405449255224, 'pearson': 0.6474794577567471, 'spearman': 0.6458808812620485, 'mse': 1.4307438215016695, 'yhat': array([1.02788539, 1.46765863, 2.16271286, ..., 3.96495983, 3.99179306,        4.7884861 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.33, 'acc': 65.34, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 333.34400000000005, 'acc': [(29.38, 62.78, 78.32000000000001, 3.1999999999999997), (25.112000000000002, 58.980000000000004, 75.156, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 63.96, 'acc': 64.34, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 18.34, 'acc': 18.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.5, 'acc': 29.78, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.71, 'acc': 61.64, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.1, 'acc': 85.04, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.71, 'acc': 86.38, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.43, 'acc': 80.62, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.8, 'acc': 79.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.47, 'acc': 59.13, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.73, 'acc': 55.38, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 14:23:09,931 : STS12 p=0.4939, STS12 s=0.5175, STS13 p=0.5124, STS13 s=0.5083, STS14 p=0.5679, STS14 s=0.5610, STS15 p=0.6228, STS15 s=0.6322, STS 16 p=0.5664, STS16 s=0.5859, STS B p=0.6475, STS B s=0.6459, STS B m=1.4307, SICK-R p=0.7789, SICK-R s=0.7138, SICK-P m=0.4040
2019-02-16 14:23:09,931 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 14:23:09,931 : 0.4939,0.5175,0.5124,0.5083,0.5679,0.5610,0.6228,0.6322,0.5664,0.5859,0.6475,0.6459,1.4307,0.7789,0.7138,0.4040
2019-02-16 14:23:09,931 : MR=65.46, CR=73.72, SUBJ=91.94, MPQA=86.78, SST-B=79.35, SST-F=42.04, TREC=88.80, SICK-E=75.08, SNLI=65.34, MRPC=69.62, MRPC f=75.24
2019-02-16 14:23:09,931 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 14:23:09,931 : 65.46,73.72,91.94,86.78,79.35,42.04,88.80,75.08,65.34,69.62,75.24
2019-02-16 14:23:09,931 : COCO r1i2t=29.38, COCO r5i2t=62.78, COCO r10i2t=78.32, COCO medr_i2t=3.20, COCO r1t2i=25.11, COCO r5t2i=58.98, COCO r10t2i=75.16, COCO medr_t2i=4.00
2019-02-16 14:23:09,931 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 14:23:09,931 : 29.38,62.78,78.32,3.20,25.11,58.98,75.16,4.00
2019-02-16 14:23:09,932 : SentLen=64.34, WC=18.31, TreeDepth=29.78, TopConst=61.64, BShift=85.04, Tense=86.38, SubjNum=80.62, ObjNum=79.42, SOMO=59.13, CoordInv=55.38, average=62.00
2019-02-16 14:23:09,932 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 14:23:09,932 : 64.34,18.31,29.78,61.64,85.04,86.38,80.62,79.42,59.13,55.38,62.00
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:09,932 : layer 8
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:09,932 : ********************************************************************************
2019-02-16 14:23:10,035 : ***** Transfer task : STS12 *****


2019-02-16 14:23:10,049 : loading BERT model bert-base-uncased
2019-02-16 14:23:10,049 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:10,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:10,074 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsn4s12es
2019-02-16 14:23:12,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:15,903 : MSRpar : pearson = 0.2988, spearman = 0.3272
2019-02-16 14:23:16,666 : MSRvid : pearson = 0.5666, spearman = 0.5779
2019-02-16 14:23:17,328 : SMTeuroparl : pearson = 0.4743, spearman = 0.5940
2019-02-16 14:23:18,503 : surprise.OnWN : pearson = 0.5380, spearman = 0.5650
2019-02-16 14:23:19,146 : surprise.SMTnews : pearson = 0.5826, spearman = 0.4821
2019-02-16 14:23:19,147 : ALL (weighted average) : Pearson = 0.4835,             Spearman = 0.5044
2019-02-16 14:23:19,147 : ALL (average) : Pearson = 0.4921,             Spearman = 0.5092

2019-02-16 14:23:19,147 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 14:23:19,155 : loading BERT model bert-base-uncased
2019-02-16 14:23:19,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:19,178 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:19,178 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsvzfi3ql
2019-02-16 14:23:21,685 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:23,735 : FNWN : pearson = 0.1544, spearman = 0.1756
2019-02-16 14:23:24,664 : headlines : pearson = 0.6260, spearman = 0.6193
2019-02-16 14:23:25,351 : OnWN : pearson = 0.4485, spearman = 0.4440
2019-02-16 14:23:25,351 : ALL (weighted average) : Pearson = 0.5002,             Spearman = 0.4979
2019-02-16 14:23:25,351 : ALL (average) : Pearson = 0.4096,             Spearman = 0.4130

2019-02-16 14:23:25,351 : ***** Transfer task : STS14 *****


2019-02-16 14:23:25,368 : loading BERT model bert-base-uncased
2019-02-16 14:23:25,368 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:25,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:25,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1uvn7e_3
2019-02-16 14:23:27,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:30,075 : deft-forum : pearson = 0.3461, spearman = 0.3502
2019-02-16 14:23:30,800 : deft-news : pearson = 0.7273, spearman = 0.7059
2019-02-16 14:23:31,779 : headlines : pearson = 0.5715, spearman = 0.5435
2019-02-16 14:23:32,731 : images : pearson = 0.5352, spearman = 0.5248
2019-02-16 14:23:33,691 : OnWN : pearson = 0.5879, spearman = 0.6196
2019-02-16 14:23:34,975 : tweet-news : pearson = 0.5850, spearman = 0.5513
2019-02-16 14:23:34,975 : ALL (weighted average) : Pearson = 0.5556,             Spearman = 0.5463
2019-02-16 14:23:34,975 : ALL (average) : Pearson = 0.5588,             Spearman = 0.5492

2019-02-16 14:23:34,975 : ***** Transfer task : STS15 *****


2019-02-16 14:23:35,044 : loading BERT model bert-base-uncased
2019-02-16 14:23:35,045 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:35,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:35,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjpaaivl8
2019-02-16 14:23:37,539 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:39,888 : answers-forums : pearson = 0.4956, spearman = 0.4776
2019-02-16 14:23:40,809 : answers-students : pearson = 0.6637, spearman = 0.6720
2019-02-16 14:23:41,674 : belief : pearson = 0.5417, spearman = 0.5800
2019-02-16 14:23:42,711 : headlines : pearson = 0.6166, spearman = 0.6242
2019-02-16 14:23:43,663 : images : pearson = 0.6747, spearman = 0.6839
2019-02-16 14:23:43,664 : ALL (weighted average) : Pearson = 0.6184,             Spearman = 0.6272
2019-02-16 14:23:43,664 : ALL (average) : Pearson = 0.5985,             Spearman = 0.6076

2019-02-16 14:23:43,664 : ***** Transfer task : STS16 *****


2019-02-16 14:23:43,711 : loading BERT model bert-base-uncased
2019-02-16 14:23:43,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:43,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:43,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc659j4jp
2019-02-16 14:23:46,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:48,164 : answer-answer : pearson = 0.5111, spearman = 0.5206
2019-02-16 14:23:48,472 : headlines : pearson = 0.6275, spearman = 0.6322
2019-02-16 14:23:48,852 : plagiarism : pearson = 0.6815, spearman = 0.6888
2019-02-16 14:23:49,470 : postediting : pearson = 0.7623, spearman = 0.8117
2019-02-16 14:23:49,773 : question-question : pearson = 0.2197, spearman = 0.2355
2019-02-16 14:23:49,773 : ALL (weighted average) : Pearson = 0.5689,             Spearman = 0.5863
2019-02-16 14:23:49,773 : ALL (average) : Pearson = 0.5604,             Spearman = 0.5778

2019-02-16 14:23:49,773 : ***** Transfer task : MR *****


2019-02-16 14:23:49,793 : loading BERT model bert-base-uncased
2019-02-16 14:23:49,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:23:49,815 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:23:49,816 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7hwl_gnr
2019-02-16 14:23:52,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:23:53,833 : Generating sentence embeddings
2019-02-16 14:24:07,355 : Generated sentence embeddings
2019-02-16 14:24:07,356 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 14:24:25,787 : Best param found at split 1: l2reg = 0.0001                 with score 68.93
2019-02-16 14:24:46,399 : Best param found at split 2: l2reg = 1e-05                 with score 64.63
2019-02-16 14:25:10,649 : Best param found at split 3: l2reg = 0.001                 with score 68.83
2019-02-16 14:25:28,233 : Best param found at split 4: l2reg = 1e-05                 with score 65.31
2019-02-16 14:25:47,845 : Best param found at split 5: l2reg = 0.0001                 with score 65.38
2019-02-16 14:25:49,199 : Dev acc : 66.62 Test acc : 68.34

2019-02-16 14:25:49,200 : ***** Transfer task : CR *****


2019-02-16 14:25:49,210 : loading BERT model bert-base-uncased
2019-02-16 14:25:49,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:25:49,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:25:49,272 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuf4bhqsv
2019-02-16 14:25:51,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:25:53,277 : Generating sentence embeddings
2019-02-16 14:25:56,981 : Generated sentence embeddings
2019-02-16 14:25:56,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 14:26:03,160 : Best param found at split 1: l2reg = 1e-05                 with score 74.16
2019-02-16 14:26:08,707 : Best param found at split 2: l2reg = 0.0001                 with score 75.62
2019-02-16 14:26:15,621 : Best param found at split 3: l2reg = 1e-05                 with score 78.61
2019-02-16 14:26:21,390 : Best param found at split 4: l2reg = 0.0001                 with score 76.7
2019-02-16 14:26:25,918 : Best param found at split 5: l2reg = 0.001                 with score 74.48
2019-02-16 14:26:26,223 : Dev acc : 75.91 Test acc : 65.91

2019-02-16 14:26:26,223 : ***** Transfer task : MPQA *****


2019-02-16 14:26:26,230 : loading BERT model bert-base-uncased
2019-02-16 14:26:26,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:26:26,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:26:26,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6b7o6wm6
2019-02-16 14:26:28,794 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:26:30,343 : Generating sentence embeddings
2019-02-16 14:26:34,160 : Generated sentence embeddings
2019-02-16 14:26:34,160 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 14:26:51,335 : Best param found at split 1: l2reg = 0.01                 with score 87.42
2019-02-16 14:27:09,985 : Best param found at split 2: l2reg = 0.01                 with score 88.25
2019-02-16 14:27:28,582 : Best param found at split 3: l2reg = 0.01                 with score 86.89
2019-02-16 14:27:46,430 : Best param found at split 4: l2reg = 0.0001                 with score 87.96
2019-02-16 14:28:04,505 : Best param found at split 5: l2reg = 0.001                 with score 88.02
2019-02-16 14:28:05,442 : Dev acc : 87.71 Test acc : 87.15

2019-02-16 14:28:05,443 : ***** Transfer task : SUBJ *****


2019-02-16 14:28:05,468 : loading BERT model bert-base-uncased
2019-02-16 14:28:05,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:28:05,494 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:28:05,494 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt5pf2m9h
2019-02-16 14:28:08,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:28:09,564 : Generating sentence embeddings
2019-02-16 14:28:24,025 : Generated sentence embeddings
2019-02-16 14:28:24,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 14:28:42,745 : Best param found at split 1: l2reg = 0.01                 with score 92.56
2019-02-16 14:28:58,930 : Best param found at split 2: l2reg = 0.001                 with score 92.4
2019-02-16 14:29:18,365 : Best param found at split 3: l2reg = 0.001                 with score 92.54
2019-02-16 14:29:37,210 : Best param found at split 4: l2reg = 0.0001                 with score 92.84
2019-02-16 14:29:57,545 : Best param found at split 5: l2reg = 0.001                 with score 92.69
2019-02-16 14:29:59,179 : Dev acc : 92.61 Test acc : 92.68

2019-02-16 14:29:59,180 : ***** Transfer task : SST Binary classification *****


2019-02-16 14:29:59,344 : loading BERT model bert-base-uncased
2019-02-16 14:29:59,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:29:59,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:29:59,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3lbqkxak
2019-02-16 14:30:01,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:30:03,379 : Computing embedding for train
2019-02-16 14:30:49,225 : Computed train embeddings
2019-02-16 14:30:49,226 : Computing embedding for dev
2019-02-16 14:30:50,186 : Computed dev embeddings
2019-02-16 14:30:50,186 : Computing embedding for test
2019-02-16 14:30:52,181 : Computed test embeddings
2019-02-16 14:30:52,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:31:11,081 : [('reg:1e-05', 81.54), ('reg:0.0001', 81.54), ('reg:0.001', 81.42), ('reg:0.01', 81.77)]
2019-02-16 14:31:11,082 : Validation : best param found is reg = 0.01 with score             81.77
2019-02-16 14:31:11,082 : Evaluating...
2019-02-16 14:31:15,705 : 
Dev acc : 81.77 Test acc : 81.16 for             SST Binary classification

2019-02-16 14:31:15,705 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 14:31:15,795 : loading BERT model bert-base-uncased
2019-02-16 14:31:15,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:31:15,886 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:31:15,886 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq0ybyup8
2019-02-16 14:31:19,074 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:31:21,336 : Computing embedding for train
2019-02-16 14:31:32,828 : Computed train embeddings
2019-02-16 14:31:32,829 : Computing embedding for dev
2019-02-16 14:31:34,368 : Computed dev embeddings
2019-02-16 14:31:34,368 : Computing embedding for test
2019-02-16 14:31:37,423 : Computed test embeddings
2019-02-16 14:31:37,423 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:31:39,864 : [('reg:1e-05', 31.79), ('reg:0.0001', 34.24), ('reg:0.001', 35.79), ('reg:0.01', 39.6)]
2019-02-16 14:31:39,864 : Validation : best param found is reg = 0.01 with score             39.6
2019-02-16 14:31:39,864 : Evaluating...
2019-02-16 14:31:40,468 : 
Dev acc : 39.6 Test acc : 39.95 for             SST Fine-Grained classification

2019-02-16 14:31:40,468 : ***** Transfer task : TREC *****


2019-02-16 14:31:40,487 : loading BERT model bert-base-uncased
2019-02-16 14:31:40,488 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:31:40,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:31:40,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5bjsk6gg
2019-02-16 14:31:43,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:31:48,087 : Computed train embeddings
2019-02-16 14:31:48,352 : Computed test embeddings
2019-02-16 14:31:48,352 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 14:31:59,825 : [('reg:1e-05', 63.13), ('reg:0.0001', 69.99), ('reg:0.001', 65.74), ('reg:0.01', 61.52)]
2019-02-16 14:31:59,825 : Cross-validation : best param found is reg = 0.0001             with score 69.99
2019-02-16 14:31:59,825 : Evaluating...
2019-02-16 14:32:00,532 : 
Dev acc : 69.99 Test acc : 86.2             for TREC

2019-02-16 14:32:00,533 : ***** Transfer task : MRPC *****


2019-02-16 14:32:00,589 : loading BERT model bert-base-uncased
2019-02-16 14:32:00,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:32:00,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:32:00,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpckhr1zzz
2019-02-16 14:32:03,100 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:32:04,580 : Computing embedding for train
2019-02-16 14:32:14,308 : Computed train embeddings
2019-02-16 14:32:14,308 : Computing embedding for test
2019-02-16 14:32:18,530 : Computed test embeddings
2019-02-16 14:32:18,738 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 14:32:25,831 : [('reg:1e-05', 71.39), ('reg:0.0001', 71.52), ('reg:0.001', 71.74), ('reg:0.01', 71.69)]
2019-02-16 14:32:25,831 : Cross-validation : best param found is reg = 0.001             with score 71.74
2019-02-16 14:32:25,831 : Evaluating...
2019-02-16 14:32:26,164 : Dev acc : 71.74 Test acc 68.41; Test F1 73.81 for MRPC.

2019-02-16 14:32:26,164 : ***** Transfer task : SICK-Entailment*****


2019-02-16 14:32:26,194 : loading BERT model bert-base-uncased
2019-02-16 14:32:26,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:32:26,266 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:32:26,266 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkz9a8tnj
2019-02-16 14:32:28,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:32:30,276 : Computing embedding for train
2019-02-16 14:32:35,488 : Computed train embeddings
2019-02-16 14:32:35,488 : Computing embedding for dev
2019-02-16 14:32:36,173 : Computed dev embeddings
2019-02-16 14:32:36,174 : Computing embedding for test
2019-02-16 14:32:41,727 : Computed test embeddings
2019-02-16 14:32:41,756 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:32:43,652 : [('reg:1e-05', 76.8), ('reg:0.0001', 77.4), ('reg:0.001', 77.6), ('reg:0.01', 78.2)]
2019-02-16 14:32:43,652 : Validation : best param found is reg = 0.01 with score             78.2
2019-02-16 14:32:43,652 : Evaluating...
2019-02-16 14:32:44,061 : 
Dev acc : 78.2 Test acc : 76.17 for                        SICK entailment

2019-02-16 14:32:44,062 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 14:32:44,091 : loading BERT model bert-base-uncased
2019-02-16 14:32:44,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:32:44,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:32:44,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkwheotxw
2019-02-16 14:32:46,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:32:48,112 : Computing embedding for train
2019-02-16 14:32:53,305 : Computed train embeddings
2019-02-16 14:32:53,305 : Computing embedding for dev
2019-02-16 14:32:53,990 : Computed dev embeddings
2019-02-16 14:32:53,990 : Computing embedding for test
2019-02-16 14:32:59,483 : Computed test embeddings
2019-02-16 14:33:22,001 : Dev : Pearson 0.7877903470144794
2019-02-16 14:33:22,001 : Test : Pearson 0.7830765028664892 Spearman 0.7235250441660269 MSE 0.39576590142065426                        for SICK Relatedness

2019-02-16 14:33:22,002 : 

***** Transfer task : STSBenchmark*****


2019-02-16 14:33:22,104 : loading BERT model bert-base-uncased
2019-02-16 14:33:22,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:33:22,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:33:22,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprex2ji76
2019-02-16 14:33:24,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:33:26,098 : Computing embedding for train
2019-02-16 14:33:34,453 : Computed train embeddings
2019-02-16 14:33:34,453 : Computing embedding for dev
2019-02-16 14:33:36,900 : Computed dev embeddings
2019-02-16 14:33:36,900 : Computing embedding for test
2019-02-16 14:33:38,855 : Computed test embeddings
2019-02-16 14:34:06,848 : Dev : Pearson 0.6383269239371329
2019-02-16 14:34:06,849 : Test : Pearson 0.6075716365189994 Spearman 0.6043524616674351 MSE 1.5038752906319992                        for SICK Relatedness

2019-02-16 14:34:06,849 : ***** Transfer task : SNLI Entailment*****


2019-02-16 14:34:11,993 : loading BERT model bert-base-uncased
2019-02-16 14:34:11,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:34:12,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:34:12,120 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2f3y760s
2019-02-16 14:34:14,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:34:16,280 : PROGRESS (encoding): 0.00%
2019-02-16 14:35:35,525 : PROGRESS (encoding): 14.56%
2019-02-16 14:37:02,898 : PROGRESS (encoding): 29.12%
2019-02-16 14:38:30,337 : PROGRESS (encoding): 43.69%
2019-02-16 14:40:06,380 : PROGRESS (encoding): 58.25%
2019-02-16 14:41:50,500 : PROGRESS (encoding): 72.81%
2019-02-16 14:43:34,865 : PROGRESS (encoding): 87.37%
2019-02-16 14:45:25,675 : PROGRESS (encoding): 0.00%
2019-02-16 14:45:39,161 : PROGRESS (encoding): 0.00%
2019-02-16 14:45:52,312 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 14:46:26,630 : [('reg:1e-09', 61.38)]
2019-02-16 14:46:26,630 : Validation : best param found is reg = 1e-09 with score             61.38
2019-02-16 14:46:26,630 : Evaluating...
2019-02-16 14:47:02,490 : Dev acc : 61.38 Test acc : 60.84 for SNLI

2019-02-16 14:47:02,490 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 14:47:11,727 : loading BERT model bert-base-uncased
2019-02-16 14:47:11,727 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 14:47:11,773 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 14:47:11,773 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp4_2jbda
2019-02-16 14:47:14,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 14:47:15,726 : Computing embedding for train
2019-02-16 14:54:48,181 : Computed train embeddings
2019-02-16 14:54:48,182 : Computing embedding for dev
2019-02-16 14:55:08,672 : Computed dev embeddings
2019-02-16 14:55:08,673 : Computing embedding for test
2019-02-16 14:55:28,446 : Computed test embeddings
2019-02-16 14:55:28,463 : prepare data
2019-02-16 14:55:28,530 : start epoch
2019-02-16 14:56:11,754 : samples : 64000
2019-02-16 14:56:22,205 : Image to text: 6.54, 20.58, 30.82, 27.0
2019-02-16 14:56:29,764 : Text to Image: 5.044, 17.092, 26.92, 32.0
2019-02-16 14:57:13,124 : samples : 128000
2019-02-16 14:57:23,531 : Image to text: 7.76, 22.76, 34.34, 22.0
2019-02-16 14:57:31,096 : Text to Image: 6.568, 20.46, 30.5, 26.0
2019-02-16 14:58:14,155 : samples : 192000
2019-02-16 14:58:24,543 : Image to text: 7.18, 22.16, 32.88, 24.0
2019-02-16 14:58:32,111 : Text to Image: 5.548, 18.196, 28.02, 29.0
2019-02-16 14:59:15,778 : samples : 256000
2019-02-16 14:59:26,233 : Image to text: 8.34, 24.66, 36.3, 20.0
2019-02-16 14:59:33,786 : Text to Image: 6.976, 21.204, 31.5, 25.0
2019-02-16 15:00:17,210 : samples : 320000
2019-02-16 15:00:27,576 : Image to text: 8.24, 25.06, 36.66, 20.0
2019-02-16 15:00:35,141 : Text to Image: 7.236, 22.316, 33.54, 23.0
2019-02-16 15:01:18,629 : samples : 384000
2019-02-16 15:01:29,243 : Image to text: 7.86, 24.76, 36.28, 20.0
2019-02-16 15:01:36,799 : Text to Image: 6.856, 21.184, 31.812, 24.0
2019-02-16 15:02:20,211 : samples : 448000
2019-02-16 15:02:30,793 : Image to text: 8.2, 24.7, 36.1, 20.0
2019-02-16 15:02:38,380 : Text to Image: 6.924, 21.708, 32.512, 24.0
2019-02-16 15:03:20,773 : samples : 512000
2019-02-16 15:03:31,386 : Image to text: 9.14, 26.2, 38.42, 17.0
2019-02-16 15:03:38,925 : Text to Image: 7.24, 23.412, 34.444, 22.0
2019-02-16 15:04:15,383 : Epoch 1 finished
2019-02-16 15:04:15,824 : Image to text: 23.8, 56.9, 72.5, 4.0
2019-02-16 15:04:16,164 : Text to Image: 19.4, 49.58, 67.34, 6.0
2019-02-16 15:04:16,602 : Image to text: 25.2, 56.6, 71.5, 4.0
2019-02-16 15:04:16,946 : Text to Image: 19.64, 49.66, 68.34, 6.0
2019-02-16 15:04:17,402 : Image to text: 24.5, 56.9, 71.4, 4.0
2019-02-16 15:04:17,736 : Text to Image: 19.08, 49.1, 66.3, 6.0
2019-02-16 15:04:18,179 : Image to text: 26.6, 58.1, 73.5, 4.0
2019-02-16 15:04:18,533 : Text to Image: 19.68, 50.06, 67.22, 5.0
2019-02-16 15:04:18,963 : Image to text: 23.0, 56.6, 72.0, 4.0
2019-02-16 15:04:19,295 : Text to Image: 19.62, 50.82, 67.88, 5.0
2019-02-16 15:04:19,295 : Dev mean Text to Image: 19.483999999999998, 49.844, 67.416, 5.6
2019-02-16 15:04:19,295 : Dev mean Image to text: 24.620000000000005, 57.02, 72.18, 4.0
2019-02-16 15:04:19,295 : start epoch
2019-02-16 15:05:03,935 : samples : 64000
2019-02-16 15:05:16,225 : Image to text: 8.96, 26.46, 38.62, 18.0
2019-02-16 15:05:25,365 : Text to Image: 7.356, 22.872, 34.024, 22.0
2019-02-16 15:06:11,182 : samples : 128000
2019-02-16 15:06:21,582 : Image to text: 8.42, 26.82, 37.92, 19.0
2019-02-16 15:06:29,141 : Text to Image: 7.612, 23.028, 34.348, 22.0
2019-02-16 15:07:11,965 : samples : 192000
2019-02-16 15:07:22,335 : Image to text: 9.28, 26.5, 38.38, 18.0
2019-02-16 15:07:29,858 : Text to Image: 7.632, 23.496, 35.064, 21.0
2019-02-16 15:08:12,393 : samples : 256000
2019-02-16 15:08:22,849 : Image to text: 8.78, 27.54, 39.6, 17.0
2019-02-16 15:08:30,426 : Text to Image: 8.2, 24.14, 35.536, 21.0
2019-02-16 15:09:13,090 : samples : 320000
2019-02-16 15:09:23,499 : Image to text: 9.66, 28.78, 41.08, 16.0
2019-02-16 15:09:31,060 : Text to Image: 8.276, 24.396, 35.992, 20.0
2019-02-16 15:10:13,660 : samples : 384000
2019-02-16 15:10:24,167 : Image to text: 10.32, 27.44, 38.92, 18.0
2019-02-16 15:10:31,693 : Text to Image: 7.712, 24.068, 35.712, 20.0
2019-02-16 15:11:14,963 : samples : 448000
2019-02-16 15:11:25,486 : Image to text: 10.04, 27.94, 39.68, 17.0
2019-02-16 15:11:33,002 : Text to Image: 7.788, 24.348, 35.652, 21.0
2019-02-16 15:12:15,468 : samples : 512000
2019-02-16 15:12:25,992 : Image to text: 10.22, 28.76, 40.92, 16.0
2019-02-16 15:12:33,593 : Text to Image: 8.604, 25.668, 37.416, 19.0
2019-02-16 15:13:10,357 : Epoch 2 finished
2019-02-16 15:13:10,794 : Image to text: 24.2, 57.1, 74.4, 4.0
2019-02-16 15:13:11,121 : Text to Image: 22.4, 55.06, 72.38, 5.0
2019-02-16 15:13:11,544 : Image to text: 25.4, 57.6, 73.8, 4.0
2019-02-16 15:13:11,872 : Text to Image: 22.0, 53.92, 71.26, 5.0
2019-02-16 15:13:12,307 : Image to text: 25.9, 59.1, 73.9, 4.0
2019-02-16 15:13:12,642 : Text to Image: 21.8, 53.84, 70.52, 5.0
2019-02-16 15:13:13,087 : Image to text: 27.1, 60.3, 74.2, 4.0
2019-02-16 15:13:13,428 : Text to Image: 22.3, 54.92, 70.8, 5.0
2019-02-16 15:13:13,875 : Image to text: 27.2, 58.6, 73.7, 4.0
2019-02-16 15:13:14,235 : Text to Image: 21.7, 54.06, 70.48, 5.0
2019-02-16 15:13:14,235 : Dev mean Text to Image: 22.04, 54.36, 71.08800000000001, 5.0
2019-02-16 15:13:14,235 : Dev mean Image to text: 25.96, 58.53999999999999, 74.0, 4.0
2019-02-16 15:13:14,235 : start epoch
2019-02-16 15:13:57,092 : samples : 64000
2019-02-16 15:14:07,594 : Image to text: 10.26, 28.76, 40.78, 16.0
2019-02-16 15:14:15,100 : Text to Image: 8.456, 25.524, 37.208, 19.0
2019-02-16 15:14:57,686 : samples : 128000
2019-02-16 15:15:08,209 : Image to text: 9.12, 26.58, 38.88, 18.0
2019-02-16 15:15:15,840 : Text to Image: 7.504, 23.404, 34.84, 21.0
2019-02-16 15:15:58,929 : samples : 192000
2019-02-16 15:16:09,379 : Image to text: 10.58, 29.68, 41.82, 16.0
2019-02-16 15:16:16,941 : Text to Image: 8.492, 25.648, 37.376, 19.0
2019-02-16 15:16:59,984 : samples : 256000
2019-02-16 15:17:10,396 : Image to text: 9.84, 27.82, 40.16, 17.0
2019-02-16 15:17:17,966 : Text to Image: 7.7, 23.592, 35.02, 21.0
2019-02-16 15:18:01,082 : samples : 320000
2019-02-16 15:18:11,471 : Image to text: 9.96, 28.3, 41.72, 15.0
2019-02-16 15:18:19,028 : Text to Image: 8.76, 26.032, 37.72, 18.0
2019-02-16 15:19:02,370 : samples : 384000
2019-02-16 15:19:12,888 : Image to text: 10.98, 29.86, 42.62, 15.0
2019-02-16 15:19:20,480 : Text to Image: 9.192, 26.42, 38.328, 18.0
2019-02-16 15:20:03,055 : samples : 448000
2019-02-16 15:20:13,591 : Image to text: 10.6, 29.82, 41.94, 15.0
2019-02-16 15:20:21,139 : Text to Image: 8.832, 26.032, 38.136, 18.0
2019-02-16 15:21:04,249 : samples : 512000
2019-02-16 15:21:14,813 : Image to text: 10.72, 29.64, 41.96, 15.0
2019-02-16 15:21:22,368 : Text to Image: 9.076, 26.652, 38.724, 18.0
2019-02-16 15:21:59,482 : Epoch 3 finished
2019-02-16 15:22:00,034 : Image to text: 25.3, 59.6, 74.6, 4.0
2019-02-16 15:22:00,475 : Text to Image: 21.94, 55.18, 71.92, 5.0
2019-02-16 15:22:01,041 : Image to text: 27.8, 58.4, 72.7, 4.0
2019-02-16 15:22:01,457 : Text to Image: 22.12, 54.04, 71.42, 5.0
2019-02-16 15:22:01,990 : Image to text: 25.3, 57.9, 72.9, 4.0
2019-02-16 15:22:02,427 : Text to Image: 20.82, 53.76, 71.24, 5.0
2019-02-16 15:22:02,979 : Image to text: 28.3, 60.2, 75.0, 4.0
2019-02-16 15:22:03,415 : Text to Image: 22.16, 54.68, 71.48, 5.0
2019-02-16 15:22:03,964 : Image to text: 25.6, 59.8, 74.6, 4.0
2019-02-16 15:22:04,337 : Text to Image: 22.12, 55.32, 71.48, 4.0
2019-02-16 15:22:04,337 : Dev mean Text to Image: 21.832, 54.596000000000004, 71.508, 4.8
2019-02-16 15:22:04,337 : Dev mean Image to text: 26.460000000000004, 59.18, 73.96000000000001, 4.0
2019-02-16 15:22:04,337 : start epoch
2019-02-16 15:22:56,215 : samples : 64000
2019-02-16 15:23:06,715 : Image to text: 10.5, 29.1, 41.64, 15.0
2019-02-16 15:23:14,298 : Text to Image: 9.044, 26.448, 38.276, 18.0
2019-02-16 15:23:57,195 : samples : 128000
2019-02-16 15:24:07,688 : Image to text: 11.18, 30.72, 43.32, 15.0
2019-02-16 15:24:15,253 : Text to Image: 8.952, 26.676, 38.756, 18.0
2019-02-16 15:24:58,192 : samples : 192000
2019-02-16 15:25:08,618 : Image to text: 10.72, 29.44, 42.44, 15.0
2019-02-16 15:25:16,193 : Text to Image: 8.552, 25.568, 37.424, 19.0
2019-02-16 15:25:58,282 : samples : 256000
2019-02-16 15:26:08,734 : Image to text: 10.16, 28.4, 41.6, 15.0
2019-02-16 15:26:16,222 : Text to Image: 8.932, 25.968, 38.096, 18.0
2019-02-16 15:26:59,352 : samples : 320000
2019-02-16 15:27:09,808 : Image to text: 10.5, 30.14, 42.72, 15.0
2019-02-16 15:27:17,361 : Text to Image: 9.328, 27.376, 39.416, 17.0
2019-02-16 15:28:00,283 : samples : 384000
2019-02-16 15:28:10,773 : Image to text: 10.36, 29.44, 42.46, 15.0
2019-02-16 15:28:18,391 : Text to Image: 8.848, 26.12, 38.104, 18.0
2019-02-16 15:29:01,120 : samples : 448000
2019-02-16 15:29:11,636 : Image to text: 10.44, 29.14, 42.22, 15.0
2019-02-16 15:29:19,170 : Text to Image: 9.124, 26.412, 38.664, 18.0
2019-02-16 15:30:02,024 : samples : 512000
2019-02-16 15:30:12,646 : Image to text: 10.42, 31.38, 43.72, 14.0
2019-02-16 15:30:20,251 : Text to Image: 8.972, 26.66, 38.616, 18.0
2019-02-16 15:30:56,507 : Epoch 4 finished
2019-02-16 15:30:56,951 : Image to text: 26.6, 60.8, 76.9, 4.0
2019-02-16 15:30:57,291 : Text to Image: 23.1, 56.62, 73.08, 4.0
2019-02-16 15:30:57,722 : Image to text: 29.1, 58.4, 75.6, 4.0
2019-02-16 15:30:58,059 : Text to Image: 22.96, 55.28, 72.36, 5.0
2019-02-16 15:30:58,497 : Image to text: 27.8, 60.0, 76.8, 4.0
2019-02-16 15:30:58,838 : Text to Image: 22.84, 55.68, 72.4, 4.0
2019-02-16 15:30:59,301 : Image to text: 28.8, 61.5, 77.8, 3.0
2019-02-16 15:30:59,640 : Text to Image: 22.16, 55.68, 72.12, 5.0
2019-02-16 15:31:00,091 : Image to text: 27.8, 62.0, 75.7, 4.0
2019-02-16 15:31:00,430 : Text to Image: 22.66, 55.7, 72.38, 4.0
2019-02-16 15:31:00,430 : Dev mean Text to Image: 22.744, 55.792, 72.46799999999999, 4.4
2019-02-16 15:31:00,430 : Dev mean Image to text: 28.020000000000003, 60.54, 76.56, 3.8000000000000007
2019-02-16 15:31:00,430 : start epoch
2019-02-16 15:31:43,380 : samples : 64000
2019-02-16 15:31:53,809 : Image to text: 10.2, 28.9, 42.7, 15.0
2019-02-16 15:32:01,346 : Text to Image: 8.812, 26.572, 38.728, 18.0
2019-02-16 15:32:44,591 : samples : 128000
2019-02-16 15:32:55,072 : Image to text: 10.9, 30.6, 43.06, 14.0
2019-02-16 15:33:02,628 : Text to Image: 9.396, 27.224, 39.34, 17.0
2019-02-16 15:33:45,393 : samples : 192000
2019-02-16 15:33:55,806 : Image to text: 9.96, 29.4, 42.46, 15.0
2019-02-16 15:34:03,353 : Text to Image: 8.988, 26.944, 39.196, 17.0
2019-02-16 15:34:46,963 : samples : 256000
2019-02-16 15:34:57,458 : Image to text: 9.56, 27.92, 39.9, 16.0
2019-02-16 15:35:04,987 : Text to Image: 8.164, 25.252, 36.988, 19.0
2019-02-16 15:35:48,220 : samples : 320000
2019-02-16 15:35:58,610 : Image to text: 11.32, 30.22, 42.84, 14.0
2019-02-16 15:36:06,194 : Text to Image: 8.74, 26.412, 38.532, 18.0
2019-02-16 15:36:49,708 : samples : 384000
2019-02-16 15:37:00,243 : Image to text: 10.8, 30.58, 43.8, 14.0
2019-02-16 15:37:07,803 : Text to Image: 9.748, 27.872, 40.008, 17.0
2019-02-16 15:37:50,539 : samples : 448000
2019-02-16 15:38:01,028 : Image to text: 11.14, 31.3, 44.38, 14.0
2019-02-16 15:38:08,587 : Text to Image: 9.3, 27.392, 39.596, 17.0
2019-02-16 15:38:51,853 : samples : 512000
2019-02-16 15:39:03,915 : Image to text: 10.78, 30.66, 43.32, 14.0
2019-02-16 15:39:12,690 : Text to Image: 9.644, 27.596, 39.816, 17.0
2019-02-16 15:39:53,521 : Epoch 5 finished
2019-02-16 15:39:53,957 : Image to text: 27.4, 61.5, 77.5, 4.0
2019-02-16 15:39:54,310 : Text to Image: 23.3, 57.72, 74.16, 4.0
2019-02-16 15:39:54,746 : Image to text: 29.6, 60.3, 74.3, 3.0
2019-02-16 15:39:55,086 : Text to Image: 23.78, 56.54, 73.64, 4.0
2019-02-16 15:39:55,529 : Image to text: 27.8, 59.6, 76.8, 4.0
2019-02-16 15:39:55,886 : Text to Image: 22.92, 56.9, 73.36, 4.0
2019-02-16 15:39:56,331 : Image to text: 29.9, 64.1, 77.6, 3.0
2019-02-16 15:39:56,677 : Text to Image: 24.3, 58.0, 74.08, 4.0
2019-02-16 15:39:57,124 : Image to text: 28.8, 62.7, 75.8, 3.0
2019-02-16 15:39:57,466 : Text to Image: 24.6, 57.44, 73.68, 4.0
2019-02-16 15:39:57,466 : Dev mean Text to Image: 23.78, 57.32, 73.784, 4.0
2019-02-16 15:39:57,467 : Dev mean Image to text: 28.700000000000003, 61.64, 76.39999999999999, 3.4000000000000004
2019-02-16 15:39:57,467 : start epoch
2019-02-16 15:40:41,195 : samples : 64000
2019-02-16 15:40:51,573 : Image to text: 11.04, 30.32, 43.18, 14.0
2019-02-16 15:40:59,174 : Text to Image: 9.212, 27.116, 38.848, 18.0
2019-02-16 15:41:42,412 : samples : 128000
2019-02-16 15:41:52,770 : Image to text: 10.7, 30.38, 43.28, 15.0
2019-02-16 15:42:00,331 : Text to Image: 9.22, 26.892, 39.156, 17.0
2019-02-16 15:42:43,114 : samples : 192000
2019-02-16 15:42:53,506 : Image to text: 10.98, 30.32, 42.98, 15.0
2019-02-16 15:43:01,049 : Text to Image: 9.136, 26.96, 39.084, 17.0
2019-02-16 15:43:44,250 : samples : 256000
2019-02-16 15:43:54,647 : Image to text: 11.0, 31.2, 43.84, 14.0
2019-02-16 15:44:02,194 : Text to Image: 9.752, 27.728, 39.884, 17.0
2019-02-16 15:44:45,394 : samples : 320000
2019-02-16 15:44:55,729 : Image to text: 10.6, 30.92, 43.48, 15.0
2019-02-16 15:45:03,290 : Text to Image: 9.216, 26.84, 39.116, 17.0
2019-02-16 15:45:46,771 : samples : 384000
2019-02-16 15:45:57,308 : Image to text: 11.48, 31.2, 44.6, 13.0
2019-02-16 15:46:04,955 : Text to Image: 10.1, 28.668, 40.948, 16.0
2019-02-16 15:46:47,958 : samples : 448000
2019-02-16 15:46:58,471 : Image to text: 11.34, 30.78, 44.28, 13.0
2019-02-16 15:47:06,031 : Text to Image: 9.688, 27.936, 40.304, 16.0
2019-02-16 15:47:48,304 : samples : 512000
2019-02-16 15:47:58,827 : Image to text: 11.12, 30.48, 43.3, 14.0
2019-02-16 15:48:06,382 : Text to Image: 9.304, 27.58, 39.844, 17.0
2019-02-16 15:48:43,017 : Epoch 6 finished
2019-02-16 15:48:43,480 : Image to text: 28.1, 63.6, 78.1, 3.0
2019-02-16 15:48:43,813 : Text to Image: 24.12, 58.92, 75.54, 4.0
2019-02-16 15:48:44,263 : Image to text: 29.8, 60.9, 77.0, 3.0
2019-02-16 15:48:44,613 : Text to Image: 24.5, 57.84, 74.78, 4.0
2019-02-16 15:48:45,077 : Image to text: 28.0, 62.6, 77.7, 4.0
2019-02-16 15:48:45,414 : Text to Image: 24.24, 58.16, 74.56, 4.0
2019-02-16 15:48:45,850 : Image to text: 29.2, 65.6, 79.3, 3.0
2019-02-16 15:48:46,181 : Text to Image: 24.5, 58.74, 74.64, 4.0
2019-02-16 15:48:46,633 : Image to text: 29.5, 63.8, 77.1, 3.0
2019-02-16 15:48:46,966 : Text to Image: 24.98, 58.88, 74.52, 4.0
2019-02-16 15:48:46,966 : Dev mean Text to Image: 24.468000000000004, 58.507999999999996, 74.80799999999999, 4.0
2019-02-16 15:48:46,966 : Dev mean Image to text: 28.92, 63.3, 77.84, 3.2
2019-02-16 15:48:46,967 : start epoch
2019-02-16 15:49:30,107 : samples : 64000
2019-02-16 15:49:40,588 : Image to text: 11.42, 31.32, 43.82, 14.0
2019-02-16 15:49:48,203 : Text to Image: 9.788, 28.168, 40.32, 16.0
2019-02-16 15:50:31,930 : samples : 128000
2019-02-16 15:50:42,350 : Image to text: 10.76, 30.74, 43.42, 14.0
2019-02-16 15:50:49,889 : Text to Image: 9.58, 27.88, 40.532, 16.0
2019-02-16 15:51:32,101 : samples : 192000
2019-02-16 15:51:42,496 : Image to text: 10.86, 31.1, 43.8, 14.0
2019-02-16 15:51:50,038 : Text to Image: 9.776, 28.356, 40.708, 16.0
2019-02-16 15:52:32,553 : samples : 256000
2019-02-16 15:52:42,888 : Image to text: 10.96, 31.44, 44.52, 14.0
2019-02-16 15:52:50,485 : Text to Image: 9.892, 28.668, 40.72, 16.0
2019-02-16 15:53:32,778 : samples : 320000
2019-02-16 15:53:43,151 : Image to text: 10.94, 31.42, 43.76, 14.0
2019-02-16 15:53:50,685 : Text to Image: 9.868, 28.42, 40.88, 16.0
2019-02-16 15:54:33,419 : samples : 384000
2019-02-16 15:54:43,942 : Image to text: 11.54, 32.16, 44.82, 14.0
2019-02-16 15:54:51,520 : Text to Image: 9.832, 28.572, 41.108, 16.0
2019-02-16 15:55:33,506 : samples : 448000
2019-02-16 15:55:44,319 : Image to text: 10.74, 30.84, 43.72, 14.0
2019-02-16 15:55:53,411 : Text to Image: 9.756, 28.616, 41.284, 16.0
2019-02-16 15:56:43,109 : samples : 512000
2019-02-16 15:56:53,634 : Image to text: 11.56, 31.76, 44.74, 13.0
2019-02-16 15:57:01,203 : Text to Image: 10.108, 29.024, 41.536, 16.0
2019-02-16 15:57:37,317 : Epoch 7 finished
2019-02-16 15:57:37,764 : Image to text: 26.9, 61.5, 77.8, 4.0
2019-02-16 15:57:38,119 : Text to Image: 23.92, 58.84, 75.44, 4.0
2019-02-16 15:57:38,557 : Image to text: 28.5, 60.6, 74.5, 3.0
2019-02-16 15:57:38,889 : Text to Image: 24.44, 57.72, 74.92, 4.0
2019-02-16 15:57:39,324 : Image to text: 26.8, 61.2, 76.1, 4.0
2019-02-16 15:57:39,668 : Text to Image: 24.56, 58.48, 74.28, 4.0
2019-02-16 15:57:40,105 : Image to text: 29.2, 64.5, 78.7, 3.0
2019-02-16 15:57:40,436 : Text to Image: 24.96, 59.08, 75.32, 4.0
2019-02-16 15:57:40,880 : Image to text: 27.4, 63.0, 75.3, 3.0
2019-02-16 15:57:41,213 : Text to Image: 24.5, 57.58, 73.88, 4.0
2019-02-16 15:57:41,213 : Dev mean Text to Image: 24.476, 58.339999999999996, 74.768, 4.0
2019-02-16 15:57:41,214 : Dev mean Image to text: 27.76, 62.160000000000004, 76.48, 3.4000000000000004
2019-02-16 15:57:41,214 : start epoch
2019-02-16 15:58:24,239 : samples : 64000
2019-02-16 15:58:34,659 : Image to text: 11.0, 31.1, 44.12, 14.0
2019-02-16 15:58:42,150 : Text to Image: 9.82, 27.912, 40.468, 16.0
2019-02-16 15:59:25,933 : samples : 128000
2019-02-16 15:59:36,287 : Image to text: 11.94, 31.84, 45.26, 13.0
2019-02-16 15:59:43,859 : Text to Image: 9.92, 28.544, 41.004, 16.0
2019-02-16 16:00:26,744 : samples : 192000
2019-02-16 16:00:37,044 : Image to text: 11.28, 32.2, 45.22, 13.0
2019-02-16 16:00:44,580 : Text to Image: 9.852, 27.904, 40.628, 16.0
2019-02-16 16:01:27,406 : samples : 256000
2019-02-16 16:01:37,762 : Image to text: 11.54, 31.54, 45.2, 13.0
2019-02-16 16:01:45,257 : Text to Image: 9.74, 28.356, 40.864, 16.0
2019-02-16 16:02:28,362 : samples : 320000
2019-02-16 16:02:38,705 : Image to text: 10.98, 32.02, 45.7, 13.0
2019-02-16 16:02:46,220 : Text to Image: 10.284, 29.332, 41.852, 15.0
2019-02-16 16:03:29,404 : samples : 384000
2019-02-16 16:03:40,043 : Image to text: 11.76, 32.58, 45.7, 13.0
2019-02-16 16:03:47,687 : Text to Image: 10.156, 28.72, 41.172, 16.0
2019-02-16 16:04:30,562 : samples : 448000
2019-02-16 16:04:41,082 : Image to text: 11.24, 32.14, 45.16, 13.0
2019-02-16 16:04:48,620 : Text to Image: 10.092, 28.824, 41.184, 16.0
2019-02-16 16:05:31,060 : samples : 512000
2019-02-16 16:05:41,573 : Image to text: 10.84, 31.54, 44.68, 13.0
2019-02-16 16:05:49,132 : Text to Image: 10.012, 27.54, 40.02, 17.0
2019-02-16 16:06:25,471 : Epoch 8 finished
2019-02-16 16:06:25,919 : Image to text: 29.3, 64.1, 79.8, 3.0
2019-02-16 16:06:26,261 : Text to Image: 24.12, 58.78, 75.54, 4.0
2019-02-16 16:06:26,697 : Image to text: 32.1, 60.9, 76.8, 3.0
2019-02-16 16:06:27,027 : Text to Image: 24.58, 58.08, 74.8, 4.0
2019-02-16 16:06:27,471 : Image to text: 28.0, 62.6, 78.1, 4.0
2019-02-16 16:06:27,810 : Text to Image: 24.12, 57.56, 75.08, 4.0
2019-02-16 16:06:28,262 : Image to text: 28.9, 65.7, 80.6, 3.0
2019-02-16 16:06:28,597 : Text to Image: 24.62, 58.84, 75.22, 4.0
2019-02-16 16:06:29,040 : Image to text: 29.6, 63.1, 78.3, 3.0
2019-02-16 16:06:29,386 : Text to Image: 24.92, 57.7, 74.48, 4.0
2019-02-16 16:06:29,386 : Dev mean Text to Image: 24.472, 58.192, 75.024, 4.0
2019-02-16 16:06:29,386 : Dev mean Image to text: 29.580000000000005, 63.28, 78.72, 3.2
2019-02-16 16:06:29,386 : start epoch
2019-02-16 16:07:12,485 : samples : 64000
2019-02-16 16:07:22,782 : Image to text: 11.28, 32.02, 45.06, 13.0
2019-02-16 16:07:30,332 : Text to Image: 9.984, 28.904, 41.04, 16.0
2019-02-16 16:08:12,910 : samples : 128000
2019-02-16 16:08:23,281 : Image to text: 11.88, 32.92, 46.1, 12.0
2019-02-16 16:08:30,810 : Text to Image: 9.932, 28.228, 40.572, 16.0
2019-02-16 16:09:13,508 : samples : 192000
2019-02-16 16:09:23,855 : Image to text: 11.48, 31.84, 45.26, 13.0
2019-02-16 16:09:31,445 : Text to Image: 10.144, 28.732, 41.228, 16.0
2019-02-16 16:10:15,122 : samples : 256000
2019-02-16 16:10:25,495 : Image to text: 12.1, 31.24, 44.74, 13.0
2019-02-16 16:10:33,051 : Text to Image: 9.824, 28.5, 41.08, 16.0
2019-02-16 16:11:15,907 : samples : 320000
2019-02-16 16:11:26,180 : Image to text: 11.84, 32.08, 45.58, 13.0
2019-02-16 16:11:33,693 : Text to Image: 9.984, 28.564, 41.232, 16.0
2019-02-16 16:12:16,321 : samples : 384000
2019-02-16 16:12:28,636 : Image to text: 11.6, 30.94, 44.26, 14.0
2019-02-16 16:12:38,672 : Text to Image: 9.784, 27.86, 40.72, 16.0
2019-02-16 16:13:31,141 : samples : 448000
2019-02-16 16:13:43,763 : Image to text: 11.96, 31.9, 44.7, 13.0
2019-02-16 16:13:53,840 : Text to Image: 9.988, 28.316, 40.7, 16.0
2019-02-16 16:14:36,575 : samples : 512000
2019-02-16 16:14:46,791 : Image to text: 11.38, 32.22, 44.76, 13.0
2019-02-16 16:14:54,234 : Text to Image: 9.896, 28.684, 41.12, 16.0
2019-02-16 16:15:31,354 : Epoch 9 finished
2019-02-16 16:15:32,295 : Image to text: 29.4, 64.0, 78.4, 3.0
2019-02-16 16:15:33,050 : Text to Image: 26.36, 60.3, 76.16, 4.0
2019-02-16 16:15:33,973 : Image to text: 31.3, 62.6, 78.2, 3.0
2019-02-16 16:15:34,754 : Text to Image: 25.78, 58.64, 75.1, 4.0
2019-02-16 16:15:35,667 : Image to text: 28.4, 62.5, 77.2, 3.0
2019-02-16 16:15:36,444 : Text to Image: 25.0, 59.8, 75.34, 4.0
2019-02-16 16:15:37,370 : Image to text: 28.9, 64.8, 77.9, 3.0
2019-02-16 16:15:38,133 : Text to Image: 25.98, 59.08, 76.02, 4.0
2019-02-16 16:15:39,079 : Image to text: 29.9, 62.2, 77.5, 3.0
2019-02-16 16:15:39,880 : Text to Image: 26.32, 59.32, 74.7, 4.0
2019-02-16 16:15:39,880 : Dev mean Text to Image: 25.888, 59.428, 75.464, 4.0
2019-02-16 16:15:39,880 : Dev mean Image to text: 29.580000000000002, 63.22, 77.84, 3.0
2019-02-16 16:15:39,881 : start epoch
2019-02-16 16:16:25,799 : samples : 64000
2019-02-16 16:16:38,293 : Image to text: 11.42, 31.24, 44.24, 14.0
2019-02-16 16:16:45,521 : Text to Image: 9.64, 27.984, 40.484, 16.0
2019-02-16 16:17:27,378 : samples : 128000
2019-02-16 16:17:37,676 : Image to text: 12.14, 31.86, 45.12, 13.0
2019-02-16 16:17:44,879 : Text to Image: 10.3, 29.288, 41.58, 15.0
2019-02-16 16:18:28,713 : samples : 192000
2019-02-16 16:18:41,394 : Image to text: 12.34, 33.08, 45.08, 13.0
2019-02-16 16:18:51,514 : Text to Image: 10.28, 29.24, 41.58, 15.0
2019-02-16 16:19:35,328 : samples : 256000
2019-02-16 16:19:45,545 : Image to text: 11.98, 31.76, 45.06, 13.0
2019-02-16 16:19:52,910 : Text to Image: 10.048, 28.516, 41.012, 16.0
2019-02-16 16:20:37,156 : samples : 320000
2019-02-16 16:20:49,758 : Image to text: 11.06, 32.34, 44.92, 13.0
2019-02-16 16:20:59,846 : Text to Image: 10.204, 28.996, 41.236, 16.0
2019-02-16 16:21:43,574 : samples : 384000
2019-02-16 16:21:53,833 : Image to text: 12.38, 32.3, 46.24, 13.0
2019-02-16 16:22:01,167 : Text to Image: 10.316, 29.044, 41.476, 15.0
2019-02-16 16:22:44,931 : samples : 448000
2019-02-16 16:22:57,497 : Image to text: 12.22, 32.52, 46.48, 12.0
2019-02-16 16:23:07,489 : Text to Image: 10.048, 29.192, 41.528, 15.0
2019-02-16 16:23:52,768 : samples : 512000
2019-02-16 16:24:03,047 : Image to text: 12.02, 32.98, 45.86, 13.0
2019-02-16 16:24:10,450 : Text to Image: 10.048, 29.044, 41.616, 15.0
2019-02-16 16:24:46,692 : Epoch 10 finished
2019-02-16 16:24:47,071 : Image to text: 27.6, 63.6, 80.1, 3.0
2019-02-16 16:24:47,349 : Text to Image: 25.52, 61.16, 76.74, 4.0
2019-02-16 16:24:47,729 : Image to text: 29.4, 63.1, 76.3, 3.0
2019-02-16 16:24:48,007 : Text to Image: 25.14, 58.28, 75.94, 4.0
2019-02-16 16:24:48,389 : Image to text: 28.8, 62.7, 78.2, 4.0
2019-02-16 16:24:48,683 : Text to Image: 24.94, 59.54, 75.9, 4.0
2019-02-16 16:24:49,070 : Image to text: 31.4, 66.4, 78.9, 3.0
2019-02-16 16:24:49,360 : Text to Image: 25.52, 60.1, 76.02, 4.0
2019-02-16 16:24:49,771 : Image to text: 30.4, 64.6, 78.0, 3.0
2019-02-16 16:24:50,143 : Text to Image: 26.18, 59.8, 75.02, 4.0
2019-02-16 16:24:50,143 : Dev mean Text to Image: 25.46, 59.776, 75.924, 4.0
2019-02-16 16:24:50,143 : Dev mean Image to text: 29.519999999999996, 64.08, 78.3, 3.2
2019-02-16 16:24:50,144 : start epoch
2019-02-16 16:25:34,817 : samples : 64000
2019-02-16 16:25:47,475 : Image to text: 11.84, 32.68, 45.64, 13.0
2019-02-16 16:25:57,558 : Text to Image: 10.124, 29.836, 42.252, 15.0
2019-02-16 16:26:41,057 : samples : 128000
2019-02-16 16:26:51,323 : Image to text: 11.36, 32.22, 45.54, 13.0
2019-02-16 16:26:58,706 : Text to Image: 9.812, 28.56, 41.34, 16.0
2019-02-16 16:27:42,316 : samples : 192000
2019-02-16 16:27:54,947 : Image to text: 12.26, 33.0, 46.24, 13.0
2019-02-16 16:28:05,021 : Text to Image: 10.232, 29.04, 41.084, 16.0
2019-02-16 16:28:49,580 : samples : 256000
2019-02-16 16:28:59,855 : Image to text: 12.26, 33.18, 46.1, 13.0
2019-02-16 16:29:07,173 : Text to Image: 10.58, 29.612, 42.336, 15.0
2019-02-16 16:29:55,469 : samples : 320000
2019-02-16 16:30:08,653 : Image to text: 11.74, 31.86, 45.42, 13.0
2019-02-16 16:30:18,701 : Text to Image: 10.172, 28.872, 41.508, 15.0
2019-02-16 16:31:04,479 : samples : 384000
2019-02-16 16:31:15,947 : Image to text: 12.4, 32.96, 45.74, 13.0
2019-02-16 16:31:23,390 : Text to Image: 10.536, 29.728, 42.372, 15.0
2019-02-16 16:32:05,917 : samples : 448000
2019-02-16 16:32:16,109 : Image to text: 11.82, 32.24, 45.06, 13.0
2019-02-16 16:32:25,372 : Text to Image: 10.184, 29.32, 41.972, 15.0
2019-02-16 16:33:10,493 : samples : 512000
2019-02-16 16:33:23,181 : Image to text: 11.84, 32.3, 45.56, 13.0
2019-02-16 16:33:33,248 : Text to Image: 10.476, 29.456, 42.408, 15.0
2019-02-16 16:34:10,887 : Epoch 11 finished
2019-02-16 16:34:11,370 : Image to text: 27.8, 63.8, 78.6, 3.0
2019-02-16 16:34:11,764 : Text to Image: 24.92, 59.48, 76.62, 4.0
2019-02-16 16:34:12,231 : Image to text: 31.4, 62.8, 76.8, 3.0
2019-02-16 16:34:12,600 : Text to Image: 25.04, 57.98, 74.86, 4.0
2019-02-16 16:34:13,067 : Image to text: 28.5, 61.8, 78.5, 3.0
2019-02-16 16:34:13,438 : Text to Image: 24.7, 59.46, 75.3, 4.0
2019-02-16 16:34:13,894 : Image to text: 30.1, 65.6, 79.4, 3.0
2019-02-16 16:34:14,256 : Text to Image: 24.26, 59.08, 75.6, 4.0
2019-02-16 16:34:14,706 : Image to text: 29.8, 63.4, 77.3, 3.0
2019-02-16 16:34:15,069 : Text to Image: 24.68, 58.76, 74.98, 4.0
2019-02-16 16:34:15,069 : Dev mean Text to Image: 24.72, 58.952, 75.472, 4.0
2019-02-16 16:34:15,069 : Dev mean Image to text: 29.52, 63.48, 78.12, 3.0
2019-02-16 16:34:15,069 : start epoch
2019-02-16 16:34:58,008 : samples : 64000
2019-02-16 16:35:10,582 : Image to text: 11.86, 32.92, 45.32, 13.0
2019-02-16 16:35:20,580 : Text to Image: 10.412, 29.34, 41.736, 15.0
2019-02-16 16:36:06,752 : samples : 128000
2019-02-16 16:36:19,347 : Image to text: 11.7, 31.92, 45.46, 13.0
2019-02-16 16:36:29,174 : Text to Image: 10.108, 29.028, 41.592, 15.0
2019-02-16 16:37:11,589 : samples : 192000
2019-02-16 16:37:21,905 : Image to text: 11.82, 32.88, 45.98, 13.0
2019-02-16 16:37:29,053 : Text to Image: 10.26, 29.408, 41.848, 15.0
2019-02-16 16:38:12,467 : samples : 256000
2019-02-16 16:38:25,076 : Image to text: 11.74, 31.88, 45.68, 13.0
2019-02-16 16:38:32,551 : Text to Image: 10.248, 29.132, 42.148, 15.0
2019-02-16 16:39:15,517 : samples : 320000
2019-02-16 16:39:25,775 : Image to text: 11.4, 31.44, 44.12, 14.0
2019-02-16 16:39:33,136 : Text to Image: 9.7, 27.56, 40.02, 17.0
2019-02-16 16:40:15,873 : samples : 384000
2019-02-16 16:40:26,147 : Image to text: 11.66, 32.92, 45.48, 13.0
2019-02-16 16:40:33,294 : Text to Image: 10.16, 28.936, 41.512, 16.0
2019-02-16 16:41:15,924 : samples : 448000
2019-02-16 16:41:26,278 : Image to text: 12.18, 33.02, 46.0, 13.0
2019-02-16 16:41:33,699 : Text to Image: 10.76, 29.412, 42.176, 15.0
2019-02-16 16:42:16,533 : samples : 512000
2019-02-16 16:42:28,632 : Image to text: 11.56, 32.62, 45.8, 13.0
2019-02-16 16:42:36,086 : Text to Image: 10.068, 29.748, 42.348, 15.0
2019-02-16 16:43:12,246 : Epoch 12 finished
2019-02-16 16:43:12,727 : Image to text: 28.6, 64.1, 79.1, 3.0
2019-02-16 16:43:13,100 : Text to Image: 25.42, 60.76, 76.64, 4.0
2019-02-16 16:43:13,559 : Image to text: 30.3, 63.4, 76.7, 3.0
2019-02-16 16:43:13,935 : Text to Image: 24.84, 59.2, 75.7, 4.0
2019-02-16 16:43:14,383 : Image to text: 29.7, 62.9, 77.9, 3.0
2019-02-16 16:43:14,754 : Text to Image: 25.0, 60.38, 75.86, 4.0
2019-02-16 16:43:15,213 : Image to text: 29.1, 65.9, 78.7, 3.0
2019-02-16 16:43:15,598 : Text to Image: 25.64, 60.24, 75.98, 4.0
2019-02-16 16:43:16,055 : Image to text: 30.2, 64.0, 79.2, 3.0
2019-02-16 16:43:16,429 : Text to Image: 25.38, 60.2, 75.96, 4.0
2019-02-16 16:43:16,429 : Dev mean Text to Image: 25.256, 60.156, 76.02799999999999, 4.0
2019-02-16 16:43:16,429 : Dev mean Image to text: 29.58, 64.06, 78.32, 3.0
2019-02-16 16:43:16,429 : start epoch
2019-02-16 16:43:59,377 : samples : 64000
2019-02-16 16:44:12,056 : Image to text: 12.4, 32.68, 45.26, 13.0
2019-02-16 16:44:22,241 : Text to Image: 10.596, 29.432, 42.084, 15.0
2019-02-16 16:45:06,013 : samples : 128000
2019-02-16 16:45:16,491 : Image to text: 12.34, 33.1, 46.6, 13.0
2019-02-16 16:45:26,741 : Text to Image: 10.316, 29.16, 41.596, 15.0
2019-02-16 16:46:09,934 : samples : 192000
2019-02-16 16:46:20,649 : Image to text: 12.08, 32.08, 45.5, 13.0
2019-02-16 16:46:30,960 : Text to Image: 10.076, 29.2, 42.0, 15.0
2019-02-16 16:47:19,364 : samples : 256000
2019-02-16 16:47:30,292 : Image to text: 12.38, 33.58, 47.0, 12.0
2019-02-16 16:47:40,147 : Text to Image: 10.396, 29.488, 42.036, 15.0
2019-02-16 16:48:22,780 : samples : 320000
2019-02-16 16:48:35,288 : Image to text: 12.2, 32.5, 45.84, 13.0
2019-02-16 16:48:45,305 : Text to Image: 10.204, 29.616, 42.364, 15.0
2019-02-16 16:49:30,451 : samples : 384000
2019-02-16 16:49:43,014 : Image to text: 12.4, 33.08, 45.18, 13.0
2019-02-16 16:49:53,019 : Text to Image: 10.22, 29.228, 42.032, 15.0
2019-02-16 16:50:37,977 : samples : 448000
2019-02-16 16:50:50,541 : Image to text: 12.22, 32.7, 46.06, 13.0
2019-02-16 16:51:00,564 : Text to Image: 10.236, 29.872, 42.812, 15.0
2019-02-16 16:51:45,490 : samples : 512000
2019-02-16 16:51:58,103 : Image to text: 12.38, 32.84, 45.96, 13.0
2019-02-16 16:52:08,069 : Text to Image: 10.484, 29.788, 42.74, 15.0
2019-02-16 16:52:46,288 : Epoch 13 finished
2019-02-16 16:52:47,198 : Image to text: 29.5, 62.2, 78.3, 3.0
2019-02-16 16:52:48,013 : Text to Image: 25.56, 61.64, 77.98, 4.0
2019-02-16 16:52:48,906 : Image to text: 30.8, 64.5, 77.3, 3.0
2019-02-16 16:52:49,654 : Text to Image: 26.1, 60.16, 76.0, 4.0
2019-02-16 16:52:50,598 : Image to text: 29.1, 62.9, 79.2, 3.0
2019-02-16 16:52:51,380 : Text to Image: 25.66, 61.24, 76.38, 4.0
2019-02-16 16:52:52,359 : Image to text: 30.2, 65.8, 79.0, 3.0
2019-02-16 16:52:53,066 : Text to Image: 26.42, 60.82, 77.56, 4.0
2019-02-16 16:52:54,070 : Image to text: 29.7, 64.0, 78.6, 3.0
2019-02-16 16:52:54,806 : Text to Image: 26.4, 60.54, 75.82, 4.0
2019-02-16 16:52:54,806 : Dev mean Text to Image: 26.028, 60.88000000000001, 76.748, 4.0
2019-02-16 16:52:54,806 : Dev mean Image to text: 29.86, 63.879999999999995, 78.47999999999999, 3.0
2019-02-16 16:52:54,806 : start epoch
2019-02-16 16:53:39,906 : samples : 64000
2019-02-16 16:53:52,492 : Image to text: 12.3, 32.56, 45.88, 13.0
2019-02-16 16:54:02,481 : Text to Image: 10.672, 29.748, 42.576, 15.0
2019-02-16 16:54:47,494 : samples : 128000
2019-02-16 16:55:00,061 : Image to text: 12.48, 32.36, 45.78, 13.0
2019-02-16 16:55:10,042 : Text to Image: 10.512, 29.584, 42.18, 15.0
2019-02-16 16:55:54,497 : samples : 192000
2019-02-16 16:56:07,051 : Image to text: 13.08, 33.64, 46.66, 12.0
2019-02-16 16:56:17,039 : Text to Image: 10.584, 29.784, 42.08, 15.0
2019-02-16 16:57:01,873 : samples : 256000
2019-02-16 16:57:14,432 : Image to text: 12.36, 32.7, 46.34, 12.0
2019-02-16 16:57:24,459 : Text to Image: 10.528, 29.568, 42.576, 15.0
2019-02-16 16:58:08,891 : samples : 320000
2019-02-16 16:58:21,551 : Image to text: 12.28, 33.4, 46.98, 12.0
2019-02-16 16:58:31,608 : Text to Image: 10.452, 29.448, 42.492, 15.0
2019-02-16 16:59:16,838 : samples : 384000
2019-02-16 16:59:28,875 : Image to text: 12.2, 32.98, 46.56, 12.0
2019-02-16 16:59:39,110 : Text to Image: 10.424, 29.98, 42.612, 15.0
2019-02-16 17:00:22,263 : samples : 448000
2019-02-16 17:00:32,515 : Image to text: 11.82, 32.62, 45.96, 13.0
2019-02-16 17:00:39,976 : Text to Image: 10.168, 29.404, 41.932, 15.0
2019-02-16 17:01:22,405 : samples : 512000
2019-02-16 17:01:32,529 : Image to text: 11.82, 32.98, 46.48, 12.0
2019-02-16 17:01:39,431 : Text to Image: 10.76, 29.996, 42.856, 15.0
2019-02-16 17:02:17,955 : Epoch 14 finished
2019-02-16 17:02:19,021 : Image to text: 29.6, 64.9, 78.9, 3.0
2019-02-16 17:02:19,892 : Text to Image: 25.78, 60.58, 77.18, 4.0
2019-02-16 17:02:20,973 : Image to text: 30.1, 62.9, 77.5, 3.0
2019-02-16 17:02:21,805 : Text to Image: 25.02, 59.56, 75.72, 4.0
2019-02-16 17:02:22,941 : Image to text: 29.8, 64.0, 79.0, 3.0
2019-02-16 17:02:23,780 : Text to Image: 25.52, 60.16, 75.92, 4.0
2019-02-16 17:02:24,843 : Image to text: 29.8, 65.0, 78.2, 3.0
2019-02-16 17:02:25,727 : Text to Image: 26.24, 61.12, 76.82, 4.0
2019-02-16 17:02:26,812 : Image to text: 30.7, 63.0, 78.6, 3.0
2019-02-16 17:02:27,713 : Text to Image: 26.32, 60.4, 76.2, 4.0
2019-02-16 17:02:27,713 : Dev mean Text to Image: 25.776, 60.364000000000004, 76.368, 4.0
2019-02-16 17:02:27,713 : Dev mean Image to text: 30.000000000000004, 63.96, 78.44, 3.0
2019-02-16 17:02:27,713 : start epoch
2019-02-16 17:03:14,041 : samples : 64000
2019-02-16 17:03:27,439 : Image to text: 12.38, 33.12, 45.9, 13.0
2019-02-16 17:03:38,180 : Text to Image: 10.256, 29.032, 41.888, 15.0
2019-02-16 17:04:26,190 : samples : 128000
2019-02-16 17:04:39,062 : Image to text: 12.62, 32.66, 46.5, 12.0
2019-02-16 17:04:49,583 : Text to Image: 10.376, 29.416, 42.256, 15.0
2019-02-16 17:05:36,050 : samples : 192000
2019-02-16 17:05:49,004 : Image to text: 11.9, 32.76, 45.5, 13.0
2019-02-16 17:05:59,497 : Text to Image: 10.08, 28.792, 41.664, 15.0
2019-02-16 17:06:45,680 : samples : 256000
2019-02-16 17:06:58,588 : Image to text: 12.56, 33.0, 46.7, 12.0
2019-02-16 17:07:09,089 : Text to Image: 10.456, 29.924, 42.644, 15.0
2019-02-16 17:07:54,820 : samples : 320000
2019-02-16 17:08:07,659 : Image to text: 11.92, 31.94, 45.06, 13.0
2019-02-16 17:08:18,121 : Text to Image: 10.488, 29.652, 42.04, 15.0
2019-02-16 17:09:04,081 : samples : 384000
2019-02-16 17:09:16,152 : Image to text: 12.22, 32.46, 45.78, 13.0
2019-02-16 17:09:26,601 : Text to Image: 10.132, 29.488, 42.032, 15.0
2019-02-16 17:10:11,175 : samples : 448000
2019-02-16 17:10:21,682 : Image to text: 12.1, 33.04, 46.66, 12.0
2019-02-16 17:10:29,249 : Text to Image: 10.652, 30.208, 43.096, 14.0
2019-02-16 17:11:11,888 : samples : 512000
2019-02-16 17:11:22,361 : Image to text: 12.66, 33.72, 46.72, 12.0
2019-02-16 17:11:29,907 : Text to Image: 10.784, 29.812, 42.492, 15.0
2019-02-16 17:12:05,766 : Epoch 15 finished
2019-02-16 17:12:06,200 : Image to text: 29.3, 65.9, 81.3, 3.0
2019-02-16 17:12:06,518 : Text to Image: 26.44, 60.92, 77.18, 4.0
2019-02-16 17:12:06,958 : Image to text: 31.4, 63.7, 77.4, 3.0
2019-02-16 17:12:07,291 : Text to Image: 25.26, 59.58, 75.64, 4.0
2019-02-16 17:12:07,752 : Image to text: 29.6, 64.4, 79.0, 3.0
2019-02-16 17:12:08,098 : Text to Image: 25.5, 60.72, 76.02, 4.0
2019-02-16 17:12:08,545 : Image to text: 31.1, 66.7, 80.1, 3.0
2019-02-16 17:12:08,885 : Text to Image: 25.64, 59.52, 75.78, 4.0
2019-02-16 17:12:09,327 : Image to text: 31.1, 64.1, 80.0, 3.0
2019-02-16 17:12:09,670 : Text to Image: 27.04, 59.62, 74.74, 4.0
2019-02-16 17:12:09,670 : Dev mean Text to Image: 25.976, 60.071999999999996, 75.872, 4.0
2019-02-16 17:12:09,670 : Dev mean Image to text: 30.5, 64.96, 79.56, 3.0
2019-02-16 17:12:09,670 : start epoch
2019-02-16 17:12:51,969 : samples : 64000
2019-02-16 17:13:02,481 : Image to text: 11.92, 32.64, 46.6, 12.0
2019-02-16 17:13:10,043 : Text to Image: 10.4, 29.92, 42.584, 15.0
2019-02-16 17:13:52,849 : samples : 128000
2019-02-16 17:14:03,326 : Image to text: 11.8, 32.72, 46.14, 13.0
2019-02-16 17:14:10,891 : Text to Image: 10.208, 29.256, 41.932, 15.0
2019-02-16 17:14:54,041 : samples : 192000
2019-02-16 17:15:04,583 : Image to text: 12.2, 32.6, 46.16, 12.0
2019-02-16 17:15:12,132 : Text to Image: 10.508, 30.004, 42.532, 15.0
2019-02-16 17:15:55,023 : samples : 256000
2019-02-16 17:16:05,521 : Image to text: 12.1, 32.78, 45.92, 12.0
2019-02-16 17:16:13,050 : Text to Image: 10.656, 30.212, 43.04, 15.0
2019-02-16 17:16:56,049 : samples : 320000
2019-02-16 17:17:06,520 : Image to text: 12.76, 34.24, 46.56, 12.0
2019-02-16 17:17:14,090 : Text to Image: 10.42, 29.884, 42.344, 15.0
2019-02-16 17:17:57,461 : samples : 384000
2019-02-16 17:18:07,942 : Image to text: 11.8, 33.16, 45.9, 13.0
2019-02-16 17:18:15,478 : Text to Image: 10.944, 30.068, 42.476, 15.0
2019-02-16 17:18:58,985 : samples : 448000
2019-02-16 17:19:09,570 : Image to text: 12.32, 33.36, 46.96, 12.0
2019-02-16 17:19:17,106 : Text to Image: 10.808, 29.9, 42.752, 15.0
2019-02-16 17:19:58,589 : samples : 512000
2019-02-16 17:20:10,380 : Image to text: 13.08, 33.66, 47.22, 12.0
2019-02-16 17:20:19,562 : Text to Image: 11.084, 30.16, 42.896, 15.0
2019-02-16 17:21:01,457 : Epoch 16 finished
2019-02-16 17:21:01,888 : Image to text: 30.0, 64.8, 79.4, 3.0
2019-02-16 17:21:02,221 : Text to Image: 25.7, 60.58, 77.52, 4.0
2019-02-16 17:21:02,656 : Image to text: 31.3, 63.1, 78.0, 3.0
2019-02-16 17:21:02,971 : Text to Image: 26.1, 60.44, 76.6, 4.0
2019-02-16 17:21:03,399 : Image to text: 30.5, 64.2, 80.2, 3.0
2019-02-16 17:21:03,730 : Text to Image: 26.52, 61.26, 75.76, 4.0
2019-02-16 17:21:04,163 : Image to text: 31.5, 66.5, 79.4, 3.0
2019-02-16 17:21:04,495 : Text to Image: 26.1, 60.44, 76.9, 4.0
2019-02-16 17:21:04,935 : Image to text: 30.3, 64.8, 79.0, 3.0
2019-02-16 17:21:05,275 : Text to Image: 26.38, 59.7, 76.14, 4.0
2019-02-16 17:21:05,275 : Dev mean Text to Image: 26.16, 60.48400000000001, 76.584, 4.0
2019-02-16 17:21:05,275 : Dev mean Image to text: 30.72, 64.67999999999999, 79.2, 3.0
2019-02-16 17:21:05,275 : start epoch
2019-02-16 17:21:47,890 : samples : 64000
2019-02-16 17:21:58,405 : Image to text: 12.84, 33.28, 46.16, 12.0
2019-02-16 17:22:06,002 : Text to Image: 10.888, 30.728, 43.188, 14.0
2019-02-16 17:22:48,853 : samples : 128000
2019-02-16 17:22:59,317 : Image to text: 12.52, 33.8, 46.34, 12.0
2019-02-16 17:23:06,887 : Text to Image: 10.508, 29.808, 42.644, 15.0
2019-02-16 17:23:49,993 : samples : 192000
2019-02-16 17:24:00,462 : Image to text: 12.5, 33.84, 46.62, 12.0
2019-02-16 17:24:08,056 : Text to Image: 10.572, 29.764, 42.244, 15.0
2019-02-16 17:24:50,805 : samples : 256000
2019-02-16 17:25:01,243 : Image to text: 12.42, 33.68, 46.86, 12.0
2019-02-16 17:25:08,827 : Text to Image: 10.556, 29.964, 42.636, 15.0
2019-02-16 17:25:51,473 : samples : 320000
2019-02-16 17:26:02,061 : Image to text: 11.96, 33.4, 47.72, 12.0
2019-02-16 17:26:09,644 : Text to Image: 10.512, 29.604, 42.2, 15.0
2019-02-16 17:26:52,537 : samples : 384000
2019-02-16 17:27:02,983 : Image to text: 11.96, 32.86, 46.14, 12.0
2019-02-16 17:27:10,532 : Text to Image: 10.492, 29.84, 42.42, 15.0
2019-02-16 17:27:53,620 : samples : 448000
2019-02-16 17:28:04,146 : Image to text: 11.9, 33.2, 45.94, 12.0
2019-02-16 17:28:11,819 : Text to Image: 10.336, 29.76, 42.388, 15.0
2019-02-16 17:28:54,318 : samples : 512000
2019-02-16 17:29:04,847 : Image to text: 12.14, 32.62, 45.9, 13.0
2019-02-16 17:29:12,414 : Text to Image: 10.464, 29.48, 42.064, 15.0
2019-02-16 17:29:49,183 : Epoch 17 finished
2019-02-16 17:29:49,621 : Image to text: 28.0, 64.4, 80.6, 3.0
2019-02-16 17:29:49,951 : Text to Image: 25.88, 60.26, 76.62, 4.0
2019-02-16 17:29:50,383 : Image to text: 30.7, 63.3, 76.9, 3.0
2019-02-16 17:29:50,714 : Text to Image: 24.56, 59.06, 75.78, 4.0
2019-02-16 17:29:51,155 : Image to text: 28.8, 63.9, 78.6, 3.0
2019-02-16 17:29:51,494 : Text to Image: 25.76, 59.76, 75.76, 4.0
2019-02-16 17:29:51,957 : Image to text: 31.1, 65.9, 78.1, 3.0
2019-02-16 17:29:52,298 : Text to Image: 25.64, 60.56, 76.72, 4.0
2019-02-16 17:29:52,742 : Image to text: 30.3, 64.2, 78.1, 3.0
2019-02-16 17:29:53,087 : Text to Image: 25.78, 59.98, 75.3, 4.0
2019-02-16 17:29:53,087 : Dev mean Text to Image: 25.524, 59.92400000000001, 76.036, 4.0
2019-02-16 17:29:53,087 : Dev mean Image to text: 29.78, 64.34, 78.46, 3.0
2019-02-16 17:29:56,940 : 
Test scores | Image to text:             30.700000000000003, 64.26, 79.50000000000001, 3.0
2019-02-16 17:29:56,940 : Test scores | Text to image:             25.543999999999997, 59.524, 76.048, 4.0

2019-02-16 17:29:57,051 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 17:29:57,290 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 17:29:57,986 : loading BERT model bert-base-uncased
2019-02-16 17:29:57,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:29:58,023 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:29:58,023 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphzl2fj0j
2019-02-16 17:30:00,527 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:30:02,003 : Computing embeddings for train/dev/test
2019-02-16 17:31:37,694 : Computed embeddings
2019-02-16 17:31:37,694 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:32:37,885 : [('reg:1e-05', 67.5), ('reg:0.0001', 69.16), ('reg:0.001', 63.35), ('reg:0.01', 59.72)]
2019-02-16 17:32:37,885 : Validation : best param found is reg = 0.0001 with score             69.16
2019-02-16 17:32:37,885 : Evaluating...
2019-02-16 17:32:56,939 : 
Dev acc : 69.2 Test acc : 68.5 for LENGTH classification

2019-02-16 17:32:56,940 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 17:32:57,321 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 17:32:57,374 : loading BERT model bert-base-uncased
2019-02-16 17:32:57,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:32:57,409 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:32:57,409 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpao20gfi9
2019-02-16 17:32:59,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:33:01,401 : Computing embeddings for train/dev/test
2019-02-16 17:34:29,748 : Computed embeddings
2019-02-16 17:34:29,748 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:35:31,456 : [('reg:1e-05', 12.59), ('reg:0.0001', 6.39), ('reg:0.001', 0.37), ('reg:0.01', 0.14)]
2019-02-16 17:35:31,456 : Validation : best param found is reg = 1e-05 with score             12.59
2019-02-16 17:35:31,457 : Evaluating...
2019-02-16 17:35:51,799 : 
Dev acc : 12.6 Test acc : 13.0 for WORDCONTENT classification

2019-02-16 17:35:51,801 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 17:35:52,172 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 17:35:52,240 : loading BERT model bert-base-uncased
2019-02-16 17:35:52,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:35:52,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:35:52,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6im353n1
2019-02-16 17:35:54,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:35:56,273 : Computing embeddings for train/dev/test
2019-02-16 17:37:22,037 : Computed embeddings
2019-02-16 17:37:22,037 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:37:58,990 : [('reg:1e-05', 30.25), ('reg:0.0001', 27.37), ('reg:0.001', 30.72), ('reg:0.01', 24.6)]
2019-02-16 17:37:58,991 : Validation : best param found is reg = 0.001 with score             30.72
2019-02-16 17:37:58,991 : Evaluating...
2019-02-16 17:38:09,619 : 
Dev acc : 30.7 Test acc : 30.2 for DEPTH classification

2019-02-16 17:38:09,620 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 17:38:10,269 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 17:38:10,340 : loading BERT model bert-base-uncased
2019-02-16 17:38:10,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:38:10,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:38:10,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprnrt7tg_
2019-02-16 17:38:12,875 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:38:14,381 : Computing embeddings for train/dev/test
2019-02-16 17:39:32,656 : Computed embeddings
2019-02-16 17:39:32,656 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:40:21,033 : [('reg:1e-05', 53.8), ('reg:0.0001', 62.93), ('reg:0.001', 59.54), ('reg:0.01', 38.73)]
2019-02-16 17:40:21,033 : Validation : best param found is reg = 0.0001 with score             62.93
2019-02-16 17:40:21,033 : Evaluating...
2019-02-16 17:40:34,731 : 
Dev acc : 62.9 Test acc : 63.0 for TOPCONSTITUENTS classification

2019-02-16 17:40:34,732 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 17:40:35,115 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 17:40:35,188 : loading BERT model bert-base-uncased
2019-02-16 17:40:35,188 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:40:35,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:40:35,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd3rfbu0d
2019-02-16 17:40:37,843 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:40:39,280 : Computing embeddings for train/dev/test
2019-02-16 17:42:04,435 : Computed embeddings
2019-02-16 17:42:04,435 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:43:05,469 : [('reg:1e-05', 88.03), ('reg:0.0001', 88.11), ('reg:0.001', 88.06), ('reg:0.01', 87.55)]
2019-02-16 17:43:05,469 : Validation : best param found is reg = 0.0001 with score             88.11
2019-02-16 17:43:05,469 : Evaluating...
2019-02-16 17:43:20,589 : 
Dev acc : 88.1 Test acc : 87.4 for BIGRAMSHIFT classification

2019-02-16 17:43:20,590 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 17:43:21,222 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 17:43:21,301 : loading BERT model bert-base-uncased
2019-02-16 17:43:21,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:43:21,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:43:21,340 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxwehxo03
2019-02-16 17:43:23,815 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:43:25,314 : Computing embeddings for train/dev/test
2019-02-16 17:44:47,662 : Computed embeddings
2019-02-16 17:44:47,662 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:45:30,558 : [('reg:1e-05', 88.19), ('reg:0.0001', 88.2), ('reg:0.001', 88.27), ('reg:0.01', 87.9)]
2019-02-16 17:45:30,558 : Validation : best param found is reg = 0.001 with score             88.27
2019-02-16 17:45:30,558 : Evaluating...
2019-02-16 17:45:40,115 : 
Dev acc : 88.3 Test acc : 86.6 for TENSE classification

2019-02-16 17:45:40,116 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 17:45:40,757 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 17:45:40,834 : loading BERT model bert-base-uncased
2019-02-16 17:45:40,834 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:45:40,870 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:45:40,870 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp673kh0ci
2019-02-16 17:45:43,412 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:45:44,919 : Computing embeddings for train/dev/test
2019-02-16 17:47:12,367 : Computed embeddings
2019-02-16 17:47:12,368 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:47:54,450 : [('reg:1e-05', 80.07), ('reg:0.0001', 80.47), ('reg:0.001', 81.86), ('reg:0.01', 81.37)]
2019-02-16 17:47:54,451 : Validation : best param found is reg = 0.001 with score             81.86
2019-02-16 17:47:54,451 : Evaluating...
2019-02-16 17:48:03,434 : 
Dev acc : 81.9 Test acc : 80.8 for SUBJNUMBER classification

2019-02-16 17:48:03,435 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 17:48:03,878 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 17:48:03,954 : loading BERT model bert-base-uncased
2019-02-16 17:48:03,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:48:03,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:48:03,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ubc_hvr
2019-02-16 17:48:06,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:48:07,959 : Computing embeddings for train/dev/test
2019-02-16 17:49:33,129 : Computed embeddings
2019-02-16 17:49:33,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:50:28,255 : [('reg:1e-05', 76.05), ('reg:0.0001', 76.17), ('reg:0.001', 78.76), ('reg:0.01', 75.56)]
2019-02-16 17:50:28,255 : Validation : best param found is reg = 0.001 with score             78.76
2019-02-16 17:50:28,255 : Evaluating...
2019-02-16 17:50:44,077 : 
Dev acc : 78.8 Test acc : 79.9 for OBJNUMBER classification

2019-02-16 17:50:44,078 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 17:50:44,523 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 17:50:44,599 : loading BERT model bert-base-uncased
2019-02-16 17:50:44,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:50:44,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:50:44,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm__jv6bg
2019-02-16 17:50:47,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:50:48,784 : Computing embeddings for train/dev/test
2019-02-16 17:52:28,127 : Computed embeddings
2019-02-16 17:52:28,127 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:53:18,225 : [('reg:1e-05', 60.95), ('reg:0.0001', 60.95), ('reg:0.001', 60.33), ('reg:0.01', 63.21)]
2019-02-16 17:53:18,225 : Validation : best param found is reg = 0.01 with score             63.21
2019-02-16 17:53:18,226 : Evaluating...
2019-02-16 17:53:28,718 : 
Dev acc : 63.2 Test acc : 62.8 for ODDMANOUT classification

2019-02-16 17:53:28,719 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 17:53:29,113 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 17:53:29,189 : loading BERT model bert-base-uncased
2019-02-16 17:53:29,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:53:29,315 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:53:29,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4luznecs
2019-02-16 17:53:31,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:53:33,156 : Computing embeddings for train/dev/test
2019-02-16 17:55:14,549 : Computed embeddings
2019-02-16 17:55:14,549 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:55:58,106 : [('reg:1e-05', 69.04), ('reg:0.0001', 68.95), ('reg:0.001', 59.81), ('reg:0.01', 53.3)]
2019-02-16 17:55:58,107 : Validation : best param found is reg = 1e-05 with score             69.04
2019-02-16 17:55:58,107 : Evaluating...
2019-02-16 17:56:10,820 : 
Dev acc : 69.0 Test acc : 68.9 for COORDINATIONINVERSION classification

2019-02-16 17:56:10,822 : total results: {'STS12': {'MSRpar': {'pearson': (0.29879998470238733, 6.214876824522798e-17), 'spearman': SpearmanrResult(correlation=0.32719096674972736, pvalue=3.5807684497938974e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5665534721064202, 6.820056168243729e-65), 'spearman': SpearmanrResult(correlation=0.5778848349208165, pvalue=4.933960571356812e-68), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4743052478319482, 4.0026360924153866e-27), 'spearman': SpearmanrResult(correlation=0.5939576528059204, pvalue=4.176022687440376e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5380492578759628, 1.6529663523151526e-57), 'spearman': SpearmanrResult(correlation=0.5650143642135739, pvalue=1.782508584390095e-64), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.582627666634786, 1.217351035089455e-37), 'spearman': SpearmanrResult(correlation=0.4820683055276228, pvalue=1.3063788175602618e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.492067125830301, 'wmean': 0.48350276182616525}, 'spearman': {'mean': 0.5092232248435321, 'wmean': 0.5043563194840822}}}, 'STS13': {'FNWN': {'pearson': (0.15441431832086935, 0.03387941899864418), 'spearman': SpearmanrResult(correlation=0.17558606190746254, pvalue=0.0156618709554831), 'nsamples': 189}, 'headlines': {'pearson': (0.6259600844155241, 7.830901797830514e-83), 'spearman': SpearmanrResult(correlation=0.6193499514457952, pvalue=1.2080686392560593e-80), 'nsamples': 750}, 'OnWN': {'pearson': (0.44851826203656, 4.068566933379204e-29), 'spearman': SpearmanrResult(correlation=0.44403185337995793, pvalue=1.662212620222331e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40963088825765115, 'wmean': 0.5001820763178649}, 'spearman': {'mean': 0.41298928891107184, 'wmean': 0.49786673268734216}}}, 'STS14': {'deft-forum': {'pearson': (0.3460675038622933, 4.1727087283636444e-14), 'spearman': SpearmanrResult(correlation=0.3501926542356272, pvalue=1.98381418415068e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7272921992354662, 1.2243945460274939e-50), 'spearman': SpearmanrResult(correlation=0.7059289898634915, pvalue=1.4990718655110774e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.5715095514994751, 2.986693318557726e-66), 'spearman': SpearmanrResult(correlation=0.5434667098325954, pvalue=7.395629539864906e-59), 'nsamples': 750}, 'images': {'pearson': (0.5351771711334926, 8.388842171643619e-57), 'spearman': SpearmanrResult(correlation=0.5248000785170979, pvalue=2.6105097057299523e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.5878804333560607, 6.591910579909776e-71), 'spearman': SpearmanrResult(correlation=0.619603212073914, pvalue=9.98187333149154e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5849523567115926, 4.6915330355169724e-70), 'spearman': SpearmanrResult(correlation=0.5512942717721375, pvalue=7.506103734515621e-61), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5588132026330634, 'wmean': 0.5556153789424366}, 'spearman': {'mean': 0.5492143193824772, 'wmean': 0.5463302921365036}}}, 'STS15': {'answers-forums': {'pearson': (0.49564245932164214, 1.211309727341745e-24), 'spearman': SpearmanrResult(correlation=0.47763997892936205, pvalue=9.094984043156116e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.6636850349988911, 2.1174872223170806e-96), 'spearman': SpearmanrResult(correlation=0.6719748161725907, pvalue=1.1857643625088108e-99), 'nsamples': 750}, 'belief': {'pearson': (0.541739792985768, 5.5047946156417556e-30), 'spearman': SpearmanrResult(correlation=0.5800216594320853, pvalue=4.2910915535074715e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6166133126591805, 9.393801802802987e-80), 'spearman': SpearmanrResult(correlation=0.6241700963309899, pvalue=3.101571238278989e-82), 'nsamples': 750}, 'images': {'pearson': (0.674719547808599, 9.41566633220472e-101), 'spearman': SpearmanrResult(correlation=0.683943980747165, pvalue=1.5398029626179588e-104), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.598480029554816, 'wmean': 0.6184272554050938}, 'spearman': {'mean': 0.6075501063224386, 'wmean': 0.6272299281078674}}}, 'STS16': {'answer-answer': {'pearson': (0.5110746951480559, 2.643622500828177e-18), 'spearman': SpearmanrResult(correlation=0.5205900980662171, pvalue=4.813296738592338e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.627517224112993, 1.1479079226079992e-28), 'spearman': SpearmanrResult(correlation=0.6322003516780754, pvalue=3.405928206613921e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6815027029036544, 9.342357318811391e-33), 'spearman': SpearmanrResult(correlation=0.6888458672715829, pvalue=1.063615775717041e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.7622612939709552, 1.284693677134475e-47), 'spearman': SpearmanrResult(correlation=0.8116750175160692, pvalue=1.953712106434047e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.2197035648413153, 0.0013919598502565458), 'spearman': SpearmanrResult(correlation=0.23549262804069673, pvalue=0.000598130614532572), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5604118961953948, 'wmean': 0.5689040335753204}, 'spearman': {'mean': 0.5777607925145283, 'wmean': 0.5862976268832636}}}, 'MR': {'devacc': 66.62, 'acc': 68.34, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.91, 'acc': 65.91, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.71, 'acc': 87.15, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.61, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.77, 'acc': 81.16, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.6, 'acc': 39.95, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 69.99, 'acc': 86.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.74, 'acc': 68.41, 'f1': 73.81, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 76.17, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7877903470144794, 'pearson': 0.7830765028664892, 'spearman': 0.7235250441660269, 'mse': 0.39576590142065426, 'yhat': array([2.01878814, 3.74509233, 2.76615837, ..., 3.24776367, 4.16729955,        3.64642097]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6383269239371329, 'pearson': 0.6075716365189994, 'spearman': 0.6043524616674351, 'mse': 1.5038752906319992, 'yhat': array([1.06439409, 1.64141715, 1.64131424, ..., 3.55992977, 3.955018  ,        4.14952057]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.38, 'acc': 60.84, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 337.828, 'acc': [(30.700000000000003, 64.26, 79.50000000000001, 3.0), (25.543999999999997, 59.524, 76.048, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 69.16, 'acc': 68.52, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.59, 'acc': 12.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.72, 'acc': 30.16, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.93, 'acc': 63.03, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.11, 'acc': 87.43, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.27, 'acc': 86.64, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.86, 'acc': 80.75, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.76, 'acc': 79.93, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.21, 'acc': 62.84, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.04, 'acc': 68.86, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 17:56:10,822 : STS12 p=0.4835, STS12 s=0.5044, STS13 p=0.5002, STS13 s=0.4979, STS14 p=0.5556, STS14 s=0.5463, STS15 p=0.6184, STS15 s=0.6272, STS 16 p=0.5689, STS16 s=0.5863, STS B p=0.6076, STS B s=0.6044, STS B m=1.5039, SICK-R p=0.7831, SICK-R s=0.7235, SICK-P m=0.3958
2019-02-16 17:56:10,822 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 17:56:10,822 : 0.4835,0.5044,0.5002,0.4979,0.5556,0.5463,0.6184,0.6272,0.5689,0.5863,0.6076,0.6044,1.5039,0.7831,0.7235,0.3958
2019-02-16 17:56:10,822 : MR=68.34, CR=65.91, SUBJ=92.68, MPQA=87.15, SST-B=81.16, SST-F=39.95, TREC=86.20, SICK-E=76.17, SNLI=60.84, MRPC=68.41, MRPC f=73.81
2019-02-16 17:56:10,822 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 17:56:10,822 : 68.34,65.91,92.68,87.15,81.16,39.95,86.20,76.17,60.84,68.41,73.81
2019-02-16 17:56:10,822 : COCO r1i2t=30.70, COCO r5i2t=64.26, COCO r10i2t=79.50, COCO medr_i2t=3.00, COCO r1t2i=25.54, COCO r5t2i=59.52, COCO r10t2i=76.05, COCO medr_t2i=4.00
2019-02-16 17:56:10,822 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 17:56:10,822 : 30.70,64.26,79.50,3.00,25.54,59.52,76.05,4.00
2019-02-16 17:56:10,822 : SentLen=68.52, WC=12.98, TreeDepth=30.16, TopConst=63.03, BShift=87.43, Tense=86.64, SubjNum=80.75, ObjNum=79.93, SOMO=62.84, CoordInv=68.86, average=64.11
2019-02-16 17:56:10,822 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 17:56:10,822 : 68.52,12.98,30.16,63.03,87.43,86.64,80.75,79.93,62.84,68.86,64.11
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,822 : layer 9
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,822 : ********************************************************************************
2019-02-16 17:56:10,920 : ***** Transfer task : STS12 *****


2019-02-16 17:56:10,962 : loading BERT model bert-base-uncased
2019-02-16 17:56:10,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:10,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:10,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphzyr31ku
2019-02-16 17:56:13,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:16,883 : MSRpar : pearson = 0.2979, spearman = 0.3253
2019-02-16 17:56:17,727 : MSRvid : pearson = 0.5219, spearman = 0.5373
2019-02-16 17:56:18,442 : SMTeuroparl : pearson = 0.4636, spearman = 0.5761
2019-02-16 17:56:19,732 : surprise.OnWN : pearson = 0.5090, spearman = 0.5390
2019-02-16 17:56:20,444 : surprise.SMTnews : pearson = 0.5896, spearman = 0.4906
2019-02-16 17:56:20,444 : ALL (weighted average) : Pearson = 0.4648,             Spearman = 0.4863
2019-02-16 17:56:20,444 : ALL (average) : Pearson = 0.4764,             Spearman = 0.4937

2019-02-16 17:56:20,444 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 17:56:20,453 : loading BERT model bert-base-uncased
2019-02-16 17:56:20,453 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:20,470 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:20,470 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpld8u42ob
2019-02-16 17:56:22,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:25,073 : FNWN : pearson = 0.1677, spearman = 0.1828
2019-02-16 17:56:26,001 : headlines : pearson = 0.6147, spearman = 0.6040
2019-02-16 17:56:26,647 : OnWN : pearson = 0.4403, spearman = 0.4337
2019-02-16 17:56:26,648 : ALL (weighted average) : Pearson = 0.4932,             Spearman = 0.4873
2019-02-16 17:56:26,648 : ALL (average) : Pearson = 0.4076,             Spearman = 0.4068

2019-02-16 17:56:26,648 : ***** Transfer task : STS14 *****


2019-02-16 17:56:26,663 : loading BERT model bert-base-uncased
2019-02-16 17:56:26,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:26,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:26,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp50l1p4v8
2019-02-16 17:56:29,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:31,348 : deft-forum : pearson = 0.3193, spearman = 0.3244
2019-02-16 17:56:32,048 : deft-news : pearson = 0.7171, spearman = 0.6943
2019-02-16 17:56:33,022 : headlines : pearson = 0.5549, spearman = 0.5291
2019-02-16 17:56:33,924 : images : pearson = 0.4585, spearman = 0.4626
2019-02-16 17:56:34,895 : OnWN : pearson = 0.5856, spearman = 0.6133
2019-02-16 17:56:36,122 : tweet-news : pearson = 0.5823, spearman = 0.5397
2019-02-16 17:56:36,122 : ALL (weighted average) : Pearson = 0.5319,             Spearman = 0.5234
2019-02-16 17:56:36,123 : ALL (average) : Pearson = 0.5363,             Spearman = 0.5272

2019-02-16 17:56:36,123 : ***** Transfer task : STS15 *****


2019-02-16 17:56:36,158 : loading BERT model bert-base-uncased
2019-02-16 17:56:36,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:36,178 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:36,178 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzmfzbf2u
2019-02-16 17:56:38,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:41,020 : answers-forums : pearson = 0.5003, spearman = 0.4787
2019-02-16 17:56:41,996 : answers-students : pearson = 0.6544, spearman = 0.6623
2019-02-16 17:56:42,836 : belief : pearson = 0.5441, spearman = 0.5796
2019-02-16 17:56:43,825 : headlines : pearson = 0.6068, spearman = 0.6147
2019-02-16 17:56:44,768 : images : pearson = 0.6332, spearman = 0.6419
2019-02-16 17:56:44,768 : ALL (weighted average) : Pearson = 0.6041,             Spearman = 0.6120
2019-02-16 17:56:44,768 : ALL (average) : Pearson = 0.5877,             Spearman = 0.5954

2019-02-16 17:56:44,768 : ***** Transfer task : STS16 *****


2019-02-16 17:56:44,846 : loading BERT model bert-base-uncased
2019-02-16 17:56:44,846 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:44,865 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:44,865 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfihzi4h5
2019-02-16 17:56:47,399 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:49,307 : answer-answer : pearson = 0.4932, spearman = 0.4916
2019-02-16 17:56:49,604 : headlines : pearson = 0.6283, spearman = 0.6325
2019-02-16 17:56:49,993 : plagiarism : pearson = 0.6631, spearman = 0.6751
2019-02-16 17:56:50,619 : postediting : pearson = 0.7541, spearman = 0.7930
2019-02-16 17:56:50,906 : question-question : pearson = 0.2320, spearman = 0.2528
2019-02-16 17:56:50,907 : ALL (weighted average) : Pearson = 0.5621,             Spearman = 0.5767
2019-02-16 17:56:50,907 : ALL (average) : Pearson = 0.5541,             Spearman = 0.5690

2019-02-16 17:56:50,907 : ***** Transfer task : MR *****


2019-02-16 17:56:50,925 : loading BERT model bert-base-uncased
2019-02-16 17:56:50,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:56:50,953 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:56:50,953 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpom74rks4
2019-02-16 17:56:53,507 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:56:55,046 : Generating sentence embeddings
2019-02-16 17:57:08,566 : Generated sentence embeddings
2019-02-16 17:57:08,567 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 17:57:28,019 : Best param found at split 1: l2reg = 1e-05                 with score 75.52
2019-02-16 17:57:49,159 : Best param found at split 2: l2reg = 1e-05                 with score 72.42
2019-02-16 17:58:11,496 : Best param found at split 3: l2reg = 0.001                 with score 74.03
2019-02-16 17:58:31,626 : Best param found at split 4: l2reg = 0.0001                 with score 73.51
2019-02-16 17:58:52,935 : Best param found at split 5: l2reg = 1e-05                 with score 72.53
2019-02-16 17:58:54,174 : Dev acc : 73.6 Test acc : 73.01

2019-02-16 17:58:54,175 : ***** Transfer task : CR *****


2019-02-16 17:58:54,185 : loading BERT model bert-base-uncased
2019-02-16 17:58:54,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:58:54,212 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:58:54,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdfikybhe
2019-02-16 17:58:56,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:58:58,231 : Generating sentence embeddings
2019-02-16 17:59:01,991 : Generated sentence embeddings
2019-02-16 17:59:01,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 17:59:07,381 : Best param found at split 1: l2reg = 0.001                 with score 74.49
2019-02-16 17:59:12,079 : Best param found at split 2: l2reg = 0.0001                 with score 80.29
2019-02-16 17:59:18,740 : Best param found at split 3: l2reg = 0.0001                 with score 80.82
2019-02-16 17:59:24,055 : Best param found at split 4: l2reg = 0.001                 with score 82.03
2019-02-16 17:59:30,121 : Best param found at split 5: l2reg = 0.01                 with score 78.68
2019-02-16 17:59:30,363 : Dev acc : 79.26 Test acc : 73.25

2019-02-16 17:59:30,364 : ***** Transfer task : MPQA *****


2019-02-16 17:59:30,371 : loading BERT model bert-base-uncased
2019-02-16 17:59:30,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:59:30,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:59:30,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8v2z8wm6
2019-02-16 17:59:32,918 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:59:34,403 : Generating sentence embeddings
2019-02-16 17:59:38,142 : Generated sentence embeddings
2019-02-16 17:59:38,142 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 17:59:55,459 : Best param found at split 1: l2reg = 1e-05                 with score 86.8
2019-02-16 18:00:16,144 : Best param found at split 2: l2reg = 1e-05                 with score 88.31
2019-02-16 18:00:34,357 : Best param found at split 3: l2reg = 0.0001                 with score 87.53
2019-02-16 18:00:51,308 : Best param found at split 4: l2reg = 0.001                 with score 87.53
2019-02-16 18:01:08,252 : Best param found at split 5: l2reg = 0.0001                 with score 87.76
2019-02-16 18:01:09,269 : Dev acc : 87.59 Test acc : 87.68

2019-02-16 18:01:09,270 : ***** Transfer task : SUBJ *****


2019-02-16 18:01:09,292 : loading BERT model bert-base-uncased
2019-02-16 18:01:09,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:01:09,315 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:01:09,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4fu1pz5i
2019-02-16 18:01:11,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:01:13,279 : Generating sentence embeddings
2019-02-16 18:01:27,297 : Generated sentence embeddings
2019-02-16 18:01:27,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 18:01:45,284 : Best param found at split 1: l2reg = 1e-05                 with score 93.45
2019-02-16 18:02:04,328 : Best param found at split 2: l2reg = 1e-05                 with score 93.22
2019-02-16 18:02:23,417 : Best param found at split 3: l2reg = 1e-05                 with score 93.21
2019-02-16 18:02:40,839 : Best param found at split 4: l2reg = 1e-05                 with score 93.34
2019-02-16 18:02:58,254 : Best param found at split 5: l2reg = 0.001                 with score 93.14
2019-02-16 18:02:58,980 : Dev acc : 93.27 Test acc : 92.91

2019-02-16 18:02:58,981 : ***** Transfer task : SST Binary classification *****


2019-02-16 18:02:59,120 : loading BERT model bert-base-uncased
2019-02-16 18:02:59,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:02:59,145 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:02:59,145 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfgz63q5h
2019-02-16 18:03:01,629 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:03:03,126 : Computing embedding for train
2019-02-16 18:03:48,793 : Computed train embeddings
2019-02-16 18:03:48,793 : Computing embedding for dev
2019-02-16 18:03:49,730 : Computed dev embeddings
2019-02-16 18:03:49,730 : Computing embedding for test
2019-02-16 18:03:51,813 : Computed test embeddings
2019-02-16 18:03:51,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 18:04:20,554 : [('reg:1e-05', 83.83), ('reg:0.0001', 83.94), ('reg:0.001', 84.06), ('reg:0.01', 76.72)]
2019-02-16 18:04:20,554 : Validation : best param found is reg = 0.001 with score             84.06
2019-02-16 18:04:20,554 : Evaluating...
2019-02-16 18:04:29,239 : 
Dev acc : 84.06 Test acc : 83.2 for             SST Binary classification

2019-02-16 18:04:29,240 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 18:04:29,293 : loading BERT model bert-base-uncased
2019-02-16 18:04:29,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:04:29,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:04:29,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzql80x8y
2019-02-16 18:04:31,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:04:33,335 : Computing embedding for train
2019-02-16 18:04:42,762 : Computed train embeddings
2019-02-16 18:04:42,762 : Computing embedding for dev
2019-02-16 18:04:43,985 : Computed dev embeddings
2019-02-16 18:04:43,985 : Computing embedding for test
2019-02-16 18:04:46,413 : Computed test embeddings
2019-02-16 18:04:46,413 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 18:04:51,197 : [('reg:1e-05', 37.33), ('reg:0.0001', 42.51), ('reg:0.001', 38.87), ('reg:0.01', 32.97)]
2019-02-16 18:04:51,198 : Validation : best param found is reg = 0.0001 with score             42.51
2019-02-16 18:04:51,198 : Evaluating...
2019-02-16 18:04:52,044 : 
Dev acc : 42.51 Test acc : 43.26 for             SST Fine-Grained classification

2019-02-16 18:04:52,045 : ***** Transfer task : TREC *****


2019-02-16 18:04:52,059 : loading BERT model bert-base-uncased
2019-02-16 18:04:52,059 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:04:52,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:04:52,081 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvt05lns4
2019-02-16 18:04:54,547 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:04:59,514 : Computed train embeddings
2019-02-16 18:04:59,774 : Computed test embeddings
2019-02-16 18:04:59,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 18:05:11,741 : [('reg:1e-05', 74.42), ('reg:0.0001', 68.02), ('reg:0.001', 72.63), ('reg:0.01', 63.14)]
2019-02-16 18:05:11,741 : Cross-validation : best param found is reg = 1e-05             with score 74.42
2019-02-16 18:05:11,742 : Evaluating...
2019-02-16 18:05:12,226 : 
Dev acc : 74.42 Test acc : 89.2             for TREC

2019-02-16 18:05:12,226 : ***** Transfer task : MRPC *****


2019-02-16 18:05:12,292 : loading BERT model bert-base-uncased
2019-02-16 18:05:12,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:05:12,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:05:12,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqfax4y3d
2019-02-16 18:05:14,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:05:16,351 : Computing embedding for train
2019-02-16 18:05:26,324 : Computed train embeddings
2019-02-16 18:05:26,324 : Computing embedding for test
2019-02-16 18:05:30,436 : Computed test embeddings
2019-02-16 18:05:30,452 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 18:05:38,859 : [('reg:1e-05', 71.74), ('reg:0.0001', 72.86), ('reg:0.001', 72.28), ('reg:0.01', 72.84)]
2019-02-16 18:05:38,859 : Cross-validation : best param found is reg = 0.0001             with score 72.86
2019-02-16 18:05:38,859 : Evaluating...
2019-02-16 18:05:39,435 : Dev acc : 72.86 Test acc 69.91; Test F1 75.06 for MRPC.

2019-02-16 18:05:39,435 : ***** Transfer task : SICK-Entailment*****


2019-02-16 18:05:39,461 : loading BERT model bert-base-uncased
2019-02-16 18:05:39,462 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:05:39,483 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:05:39,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptf8tq8r0
2019-02-16 18:05:41,997 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:05:43,506 : Computing embedding for train
2019-02-16 18:05:48,634 : Computed train embeddings
2019-02-16 18:05:48,634 : Computing embedding for dev
2019-02-16 18:05:49,324 : Computed dev embeddings
2019-02-16 18:05:49,324 : Computing embedding for test
2019-02-16 18:05:54,888 : Computed test embeddings
2019-02-16 18:05:54,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 18:05:56,741 : [('reg:1e-05', 76.4), ('reg:0.0001', 68.4), ('reg:0.001', 74.6), ('reg:0.01', 74.6)]
2019-02-16 18:05:56,741 : Validation : best param found is reg = 1e-05 with score             76.4
2019-02-16 18:05:56,741 : Evaluating...
2019-02-16 18:05:57,386 : 
Dev acc : 76.4 Test acc : 73.03 for                        SICK entailment

2019-02-16 18:05:57,387 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 18:05:57,421 : loading BERT model bert-base-uncased
2019-02-16 18:05:57,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:05:57,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:05:57,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgord6i2r
2019-02-16 18:05:59,983 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:06:01,496 : Computing embedding for train
2019-02-16 18:06:06,652 : Computed train embeddings
2019-02-16 18:06:06,653 : Computing embedding for dev
2019-02-16 18:06:07,349 : Computed dev embeddings
2019-02-16 18:06:07,349 : Computing embedding for test
2019-02-16 18:06:12,967 : Computed test embeddings
2019-02-16 18:06:36,455 : Dev : Pearson 0.7849114722360065
2019-02-16 18:06:36,456 : Test : Pearson 0.7791974566428523 Spearman 0.7155194844797105 MSE 0.40100860132932276                        for SICK Relatedness

2019-02-16 18:06:36,457 : 

***** Transfer task : STSBenchmark*****


2019-02-16 18:06:36,549 : loading BERT model bert-base-uncased
2019-02-16 18:06:36,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:06:36,575 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:06:36,575 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy4ah29bg
2019-02-16 18:06:39,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:06:40,605 : Computing embedding for train
2019-02-16 18:06:48,953 : Computed train embeddings
2019-02-16 18:06:48,953 : Computing embedding for dev
2019-02-16 18:06:51,400 : Computed dev embeddings
2019-02-16 18:06:51,400 : Computing embedding for test
2019-02-16 18:06:53,398 : Computed test embeddings
2019-02-16 18:07:24,078 : Dev : Pearson 0.6350037223493994
2019-02-16 18:07:24,079 : Test : Pearson 0.625147973339803 Spearman 0.6210754290394137 MSE 1.4506891891495912                        for SICK Relatedness

2019-02-16 18:07:24,079 : ***** Transfer task : SNLI Entailment*****


2019-02-16 18:07:29,144 : loading BERT model bert-base-uncased
2019-02-16 18:07:29,144 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:07:29,290 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:07:29,290 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4ir67tat
2019-02-16 18:07:31,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:07:33,463 : PROGRESS (encoding): 0.00%
2019-02-16 18:08:51,925 : PROGRESS (encoding): 14.56%
2019-02-16 18:10:19,092 : PROGRESS (encoding): 29.12%
2019-02-16 18:11:51,560 : PROGRESS (encoding): 43.69%
2019-02-16 18:13:26,116 : PROGRESS (encoding): 58.25%
2019-02-16 18:15:10,806 : PROGRESS (encoding): 72.81%
2019-02-16 18:16:54,842 : PROGRESS (encoding): 87.37%
2019-02-16 18:18:45,440 : PROGRESS (encoding): 0.00%
2019-02-16 18:18:58,815 : PROGRESS (encoding): 0.00%
2019-02-16 18:19:11,829 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 18:20:08,538 : [('reg:1e-09', 66.1)]
2019-02-16 18:20:08,539 : Validation : best param found is reg = 1e-09 with score             66.1
2019-02-16 18:20:08,539 : Evaluating...
2019-02-16 18:21:05,098 : Dev acc : 66.1 Test acc : 65.87 for SNLI

2019-02-16 18:21:05,098 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 18:21:14,391 : loading BERT model bert-base-uncased
2019-02-16 18:21:14,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 18:21:14,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 18:21:14,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn1idj621
2019-02-16 18:21:16,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 18:21:18,446 : Computing embedding for train
2019-02-16 18:28:50,014 : Computed train embeddings
2019-02-16 18:28:50,014 : Computing embedding for dev
2019-02-16 18:29:10,330 : Computed dev embeddings
2019-02-16 18:29:10,330 : Computing embedding for test
2019-02-16 18:29:30,158 : Computed test embeddings
2019-02-16 18:29:30,174 : prepare data
2019-02-16 18:29:30,239 : start epoch
2019-02-16 18:30:13,154 : samples : 64000
2019-02-16 18:30:23,443 : Image to text: 6.92, 20.8, 30.68, 27.0
2019-02-16 18:30:31,007 : Text to Image: 5.436, 17.976, 27.804, 31.0
2019-02-16 18:31:13,889 : samples : 128000
2019-02-16 18:31:24,217 : Image to text: 8.06, 23.78, 34.04, 23.0
2019-02-16 18:31:31,731 : Text to Image: 6.236, 19.832, 29.88, 28.0
2019-02-16 18:32:14,567 : samples : 192000
2019-02-16 18:32:24,890 : Image to text: 7.6, 22.28, 33.08, 24.0
2019-02-16 18:32:32,433 : Text to Image: 5.88, 18.264, 28.296, 29.0
2019-02-16 18:33:15,361 : samples : 256000
2019-02-16 18:33:25,814 : Image to text: 7.42, 23.58, 34.72, 21.0
2019-02-16 18:33:33,425 : Text to Image: 6.412, 20.188, 30.828, 26.0
2019-02-16 18:34:15,999 : samples : 320000
2019-02-16 18:34:26,517 : Image to text: 8.42, 25.54, 37.14, 19.0
2019-02-16 18:34:34,070 : Text to Image: 7.236, 22.3, 33.1, 23.0
2019-02-16 18:35:16,415 : samples : 384000
2019-02-16 18:35:26,968 : Image to text: 8.26, 25.5, 37.02, 19.0
2019-02-16 18:35:34,602 : Text to Image: 6.928, 21.408, 32.424, 24.0
2019-02-16 18:36:17,097 : samples : 448000
2019-02-16 18:36:27,595 : Image to text: 8.18, 24.88, 36.92, 19.0
2019-02-16 18:36:35,127 : Text to Image: 7.104, 21.62, 32.512, 23.0
2019-02-16 18:37:17,788 : samples : 512000
2019-02-16 18:37:28,299 : Image to text: 8.92, 26.46, 38.84, 17.0
2019-02-16 18:37:35,816 : Text to Image: 7.46, 23.136, 34.176, 22.0
2019-02-16 18:38:12,256 : Epoch 1 finished
2019-02-16 18:38:12,687 : Image to text: 24.0, 57.1, 71.6, 4.0
2019-02-16 18:38:13,017 : Text to Image: 19.66, 50.9, 67.88, 5.0
2019-02-16 18:38:13,461 : Image to text: 27.3, 55.3, 71.9, 4.0
2019-02-16 18:38:13,793 : Text to Image: 19.72, 49.98, 67.9, 6.0
2019-02-16 18:38:14,225 : Image to text: 24.1, 56.5, 70.9, 4.0
2019-02-16 18:38:14,563 : Text to Image: 19.56, 50.2, 67.28, 5.0
2019-02-16 18:38:15,008 : Image to text: 24.0, 56.6, 72.9, 4.0
2019-02-16 18:38:15,356 : Text to Image: 20.72, 50.9, 67.82, 5.0
2019-02-16 18:38:15,804 : Image to text: 24.6, 55.5, 71.4, 4.0
2019-02-16 18:38:16,150 : Text to Image: 20.26, 51.74, 68.04, 5.0
2019-02-16 18:38:16,150 : Dev mean Text to Image: 19.984, 50.744, 67.78399999999999, 5.2
2019-02-16 18:38:16,150 : Dev mean Image to text: 24.799999999999997, 56.2, 71.74000000000001, 4.0
2019-02-16 18:38:16,150 : start epoch
2019-02-16 18:38:59,007 : samples : 64000
2019-02-16 18:39:09,332 : Image to text: 9.38, 26.86, 39.2, 17.0
2019-02-16 18:39:16,812 : Text to Image: 7.692, 23.344, 34.884, 21.0
2019-02-16 18:39:59,789 : samples : 128000
2019-02-16 18:40:10,155 : Image to text: 8.4, 26.06, 37.74, 19.0
2019-02-16 18:40:17,711 : Text to Image: 7.316, 22.504, 33.916, 22.0
2019-02-16 18:41:00,392 : samples : 192000
2019-02-16 18:41:10,745 : Image to text: 9.3, 26.98, 38.82, 18.0
2019-02-16 18:41:18,237 : Text to Image: 7.788, 23.976, 35.452, 21.0
2019-02-16 18:42:01,831 : samples : 256000
2019-02-16 18:42:12,276 : Image to text: 8.98, 27.16, 39.94, 17.0
2019-02-16 18:42:19,888 : Text to Image: 8.272, 24.408, 35.792, 20.0
2019-02-16 18:43:02,382 : samples : 320000
2019-02-16 18:43:12,908 : Image to text: 9.68, 28.76, 40.76, 16.0
2019-02-16 18:43:20,493 : Text to Image: 8.192, 24.524, 36.18, 20.0
2019-02-16 18:44:03,193 : samples : 384000
2019-02-16 18:44:13,557 : Image to text: 10.04, 27.54, 39.98, 17.0
2019-02-16 18:44:21,782 : Text to Image: 7.852, 24.292, 35.792, 20.0
2019-02-16 18:45:12,739 : samples : 448000
2019-02-16 18:45:23,261 : Image to text: 9.68, 27.34, 39.12, 17.0
2019-02-16 18:45:30,835 : Text to Image: 7.688, 23.632, 35.396, 21.0
2019-02-16 18:46:13,667 : samples : 512000
2019-02-16 18:46:24,162 : Image to text: 9.56, 28.86, 41.48, 16.0
2019-02-16 18:46:31,632 : Text to Image: 8.456, 25.312, 37.084, 19.0
2019-02-16 18:47:08,092 : Epoch 2 finished
2019-02-16 18:47:08,519 : Image to text: 26.2, 59.2, 76.0, 4.0
2019-02-16 18:47:08,854 : Text to Image: 22.44, 54.5, 71.3, 5.0
2019-02-16 18:47:09,286 : Image to text: 25.9, 58.5, 73.7, 4.0
2019-02-16 18:47:09,629 : Text to Image: 21.58, 53.36, 70.86, 5.0
2019-02-16 18:47:10,057 : Image to text: 27.0, 58.1, 73.7, 4.0
2019-02-16 18:47:10,390 : Text to Image: 21.36, 53.8, 70.26, 5.0
2019-02-16 18:47:10,828 : Image to text: 27.4, 60.8, 75.5, 4.0
2019-02-16 18:47:11,166 : Text to Image: 21.86, 53.52, 70.74, 5.0
2019-02-16 18:47:11,631 : Image to text: 25.9, 59.0, 73.8, 4.0
2019-02-16 18:47:11,973 : Text to Image: 22.0, 54.58, 70.86, 5.0
2019-02-16 18:47:11,973 : Dev mean Text to Image: 21.848, 53.952, 70.804, 5.0
2019-02-16 18:47:11,973 : Dev mean Image to text: 26.48, 59.11999999999999, 74.54, 4.0
2019-02-16 18:47:11,973 : start epoch
2019-02-16 18:47:54,860 : samples : 64000
2019-02-16 18:48:05,273 : Image to text: 9.24, 28.5, 41.1, 16.0
2019-02-16 18:48:12,770 : Text to Image: 8.384, 25.34, 37.108, 19.0
2019-02-16 18:48:55,672 : samples : 128000
2019-02-16 18:49:06,023 : Image to text: 8.64, 26.66, 39.1, 17.0
2019-02-16 18:49:13,591 : Text to Image: 7.588, 23.54, 35.152, 21.0
2019-02-16 18:49:56,468 : samples : 192000
2019-02-16 18:50:06,885 : Image to text: 10.58, 29.54, 42.24, 16.0
2019-02-16 18:50:14,480 : Text to Image: 8.48, 25.16, 37.012, 19.0
2019-02-16 18:50:57,465 : samples : 256000
2019-02-16 18:51:08,008 : Image to text: 9.74, 28.34, 40.04, 16.0
2019-02-16 18:51:15,587 : Text to Image: 7.832, 24.084, 35.716, 20.0
2019-02-16 18:51:57,970 : samples : 320000
2019-02-16 18:52:08,479 : Image to text: 10.1, 29.64, 41.7, 15.0
2019-02-16 18:52:16,032 : Text to Image: 9.036, 26.024, 38.08, 18.0
2019-02-16 18:52:59,546 : samples : 384000
2019-02-16 18:53:10,043 : Image to text: 10.3, 30.08, 41.96, 15.0
2019-02-16 18:53:17,619 : Text to Image: 8.74, 25.636, 37.892, 18.0
2019-02-16 18:54:00,170 : samples : 448000
2019-02-16 18:54:10,663 : Image to text: 11.0, 30.06, 42.68, 15.0
2019-02-16 18:54:18,206 : Text to Image: 8.98, 26.052, 38.072, 18.0
2019-02-16 18:55:00,454 : samples : 512000
2019-02-16 18:55:10,937 : Image to text: 10.54, 29.46, 42.36, 15.0
2019-02-16 18:55:18,544 : Text to Image: 8.524, 25.304, 37.168, 19.0
2019-02-16 18:55:54,957 : Epoch 3 finished
2019-02-16 18:55:55,396 : Image to text: 26.8, 57.8, 74.5, 4.0
2019-02-16 18:55:55,733 : Text to Image: 21.54, 53.8, 71.16, 5.0
2019-02-16 18:55:56,170 : Image to text: 27.3, 58.1, 74.6, 4.0
2019-02-16 18:55:56,509 : Text to Image: 20.76, 53.38, 70.72, 5.0
2019-02-16 18:55:56,968 : Image to text: 26.1, 59.3, 75.5, 4.0
2019-02-16 18:55:57,306 : Text to Image: 20.6, 53.88, 71.04, 5.0
2019-02-16 18:55:57,753 : Image to text: 26.9, 60.0, 75.6, 4.0
2019-02-16 18:55:58,097 : Text to Image: 21.86, 53.7, 70.9, 5.0
2019-02-16 18:55:58,543 : Image to text: 28.4, 60.1, 74.0, 4.0
2019-02-16 18:55:58,879 : Text to Image: 22.24, 55.1, 71.54, 5.0
2019-02-16 18:55:58,880 : Dev mean Text to Image: 21.400000000000002, 53.97200000000001, 71.072, 5.0
2019-02-16 18:55:58,880 : Dev mean Image to text: 27.099999999999998, 59.06, 74.84, 4.0
2019-02-16 18:55:58,880 : start epoch
2019-02-16 18:56:41,663 : samples : 64000
2019-02-16 18:56:52,070 : Image to text: 9.56, 28.74, 40.98, 16.0
2019-02-16 18:56:59,618 : Text to Image: 8.416, 25.204, 37.124, 19.0
2019-02-16 18:57:42,478 : samples : 128000
2019-02-16 18:57:52,789 : Image to text: 10.44, 30.8, 43.02, 14.0
2019-02-16 18:58:00,366 : Text to Image: 9.096, 26.076, 38.3, 18.0
2019-02-16 18:58:43,094 : samples : 192000
2019-02-16 18:58:53,408 : Image to text: 10.26, 28.98, 42.32, 15.0
2019-02-16 18:59:00,999 : Text to Image: 8.724, 25.092, 36.764, 19.0
2019-02-16 18:59:44,277 : samples : 256000
2019-02-16 18:59:54,785 : Image to text: 10.54, 29.82, 42.2, 15.0
2019-02-16 19:00:02,390 : Text to Image: 8.896, 26.132, 38.328, 18.0
2019-02-16 19:00:45,259 : samples : 320000
2019-02-16 19:00:55,465 : Image to text: 10.78, 30.08, 42.76, 15.0
2019-02-16 19:01:02,843 : Text to Image: 9.116, 26.704, 38.86, 18.0
2019-02-16 19:01:56,273 : samples : 384000
2019-02-16 19:02:06,742 : Image to text: 10.24, 29.7, 42.68, 15.0
2019-02-16 19:02:14,290 : Text to Image: 8.756, 25.784, 38.192, 18.0
2019-02-16 19:02:56,782 : samples : 448000
2019-02-16 19:03:07,290 : Image to text: 10.32, 29.76, 43.42, 15.0
2019-02-16 19:03:14,879 : Text to Image: 9.18, 26.84, 39.048, 18.0
2019-02-16 19:03:58,189 : samples : 512000
2019-02-16 19:04:08,749 : Image to text: 11.3, 31.02, 43.76, 14.0
2019-02-16 19:04:16,316 : Text to Image: 9.108, 26.724, 39.08, 17.0
2019-02-16 19:04:52,895 : Epoch 4 finished
2019-02-16 19:04:53,336 : Image to text: 25.7, 60.8, 76.2, 4.0
2019-02-16 19:04:53,669 : Text to Image: 22.88, 56.24, 73.14, 4.0
2019-02-16 19:04:54,113 : Image to text: 28.3, 59.9, 75.5, 4.0
2019-02-16 19:04:54,448 : Text to Image: 22.44, 55.56, 72.64, 4.0
2019-02-16 19:04:54,898 : Image to text: 29.0, 61.2, 77.2, 4.0
2019-02-16 19:04:55,230 : Text to Image: 22.74, 56.8, 73.4, 4.0
2019-02-16 19:04:55,660 : Image to text: 28.3, 62.0, 75.8, 4.0
2019-02-16 19:04:55,999 : Text to Image: 22.94, 55.88, 71.62, 4.0
2019-02-16 19:04:56,434 : Image to text: 29.2, 62.8, 76.4, 4.0
2019-02-16 19:04:56,775 : Text to Image: 22.48, 56.1, 72.78, 4.0
2019-02-16 19:04:56,775 : Dev mean Text to Image: 22.695999999999998, 56.116, 72.716, 4.0
2019-02-16 19:04:56,775 : Dev mean Image to text: 28.1, 61.34, 76.22, 4.0
2019-02-16 19:04:56,776 : start epoch
2019-02-16 19:05:39,505 : samples : 64000
2019-02-16 19:05:50,022 : Image to text: 10.48, 28.94, 42.02, 15.0
2019-02-16 19:05:57,580 : Text to Image: 9.068, 26.932, 38.6, 18.0
2019-02-16 19:06:40,304 : samples : 128000
2019-02-16 19:06:50,795 : Image to text: 10.46, 30.5, 43.3, 15.0
2019-02-16 19:06:58,366 : Text to Image: 9.38, 27.02, 39.196, 17.0
2019-02-16 19:07:40,847 : samples : 192000
2019-02-16 19:07:51,228 : Image to text: 10.28, 29.6, 42.74, 15.0
2019-02-16 19:07:58,824 : Text to Image: 9.08, 26.588, 38.692, 18.0
2019-02-16 19:08:41,161 : samples : 256000
2019-02-16 19:08:51,691 : Image to text: 9.86, 28.3, 40.82, 16.0
2019-02-16 19:08:59,250 : Text to Image: 8.756, 25.656, 37.944, 18.0
2019-02-16 19:09:42,346 : samples : 320000
2019-02-16 19:09:52,867 : Image to text: 10.56, 30.72, 43.3, 14.0
2019-02-16 19:10:00,445 : Text to Image: 8.728, 26.144, 38.4, 18.0
2019-02-16 19:10:43,122 : samples : 384000
2019-02-16 19:10:53,604 : Image to text: 10.96, 30.78, 44.42, 13.0
2019-02-16 19:11:01,181 : Text to Image: 9.612, 27.276, 39.752, 17.0
2019-02-16 19:11:43,932 : samples : 448000
2019-02-16 19:11:54,520 : Image to text: 10.6, 31.0, 44.2, 14.0
2019-02-16 19:12:02,086 : Text to Image: 9.356, 27.208, 39.548, 17.0
2019-02-16 19:12:44,635 : samples : 512000
2019-02-16 19:12:55,146 : Image to text: 10.66, 30.88, 42.78, 14.0
2019-02-16 19:13:02,716 : Text to Image: 9.324, 27.176, 39.288, 17.0
2019-02-16 19:13:38,939 : Epoch 5 finished
2019-02-16 19:13:39,383 : Image to text: 28.2, 61.1, 78.3, 3.0
2019-02-16 19:13:39,713 : Text to Image: 24.14, 57.02, 73.86, 4.0
2019-02-16 19:13:40,149 : Image to text: 29.3, 62.0, 75.9, 3.0
2019-02-16 19:13:40,482 : Text to Image: 24.42, 56.8, 73.74, 4.0
2019-02-16 19:13:40,925 : Image to text: 28.9, 62.4, 78.2, 3.0
2019-02-16 19:13:41,272 : Text to Image: 23.7, 58.02, 74.04, 4.0
2019-02-16 19:13:41,733 : Image to text: 29.8, 61.8, 77.6, 4.0
2019-02-16 19:13:42,078 : Text to Image: 23.8, 57.14, 73.88, 4.0
2019-02-16 19:13:42,521 : Image to text: 29.5, 62.1, 76.3, 3.0
2019-02-16 19:13:42,860 : Text to Image: 24.6, 58.3, 73.98, 4.0
2019-02-16 19:13:42,860 : Dev mean Text to Image: 24.131999999999998, 57.456, 73.9, 4.0
2019-02-16 19:13:42,860 : Dev mean Image to text: 29.14, 61.88, 77.26, 3.1999999999999997
2019-02-16 19:13:42,860 : start epoch
2019-02-16 19:14:24,883 : samples : 64000
2019-02-16 19:14:35,332 : Image to text: 10.68, 30.28, 43.54, 14.0
2019-02-16 19:14:42,868 : Text to Image: 9.216, 26.808, 39.128, 17.0
2019-02-16 19:15:26,203 : samples : 128000
2019-02-16 19:15:36,649 : Image to text: 10.42, 30.38, 43.34, 14.0
2019-02-16 19:15:44,176 : Text to Image: 9.04, 26.692, 39.072, 17.0
2019-02-16 19:16:26,962 : samples : 192000
2019-02-16 19:16:37,455 : Image to text: 10.58, 30.54, 43.3, 14.0
2019-02-16 19:16:44,989 : Text to Image: 9.06, 26.64, 38.852, 18.0
2019-02-16 19:17:27,934 : samples : 256000
2019-02-16 19:17:38,455 : Image to text: 10.94, 31.2, 43.9, 14.0
2019-02-16 19:17:45,870 : Text to Image: 9.336, 27.8, 39.776, 17.0
2019-02-16 19:18:39,265 : samples : 320000
2019-02-16 19:18:49,756 : Image to text: 10.54, 30.76, 43.58, 14.0
2019-02-16 19:18:57,327 : Text to Image: 9.564, 27.224, 39.312, 17.0
2019-02-16 19:19:40,231 : samples : 384000
2019-02-16 19:19:50,709 : Image to text: 11.52, 32.02, 45.18, 13.0
2019-02-16 19:19:58,271 : Text to Image: 10.112, 28.344, 40.66, 16.0
2019-02-16 19:20:40,669 : samples : 448000
2019-02-16 19:20:51,128 : Image to text: 10.78, 31.62, 44.34, 13.0
2019-02-16 19:20:58,714 : Text to Image: 9.496, 27.972, 40.284, 17.0
2019-02-16 19:21:41,323 : samples : 512000
2019-02-16 19:21:51,787 : Image to text: 10.64, 30.16, 43.24, 14.0
2019-02-16 19:21:59,310 : Text to Image: 9.096, 27.004, 39.308, 17.0
2019-02-16 19:22:35,357 : Epoch 6 finished
2019-02-16 19:22:35,810 : Image to text: 28.8, 64.5, 78.2, 3.0
2019-02-16 19:22:36,143 : Text to Image: 23.34, 57.78, 75.1, 4.0
2019-02-16 19:22:36,612 : Image to text: 29.8, 62.6, 76.9, 3.0
2019-02-16 19:22:36,949 : Text to Image: 23.9, 57.54, 74.54, 4.0
2019-02-16 19:22:37,409 : Image to text: 28.5, 63.2, 79.1, 3.0
2019-02-16 19:22:37,752 : Text to Image: 24.04, 58.76, 75.04, 4.0
2019-02-16 19:22:38,205 : Image to text: 29.4, 64.9, 78.7, 3.0
2019-02-16 19:22:38,552 : Text to Image: 24.48, 57.46, 74.02, 4.0
2019-02-16 19:22:39,019 : Image to text: 31.0, 64.5, 77.7, 3.0
2019-02-16 19:22:39,366 : Text to Image: 24.1, 58.32, 74.24, 4.0
2019-02-16 19:22:39,366 : Dev mean Text to Image: 23.972, 57.97200000000001, 74.588, 4.0
2019-02-16 19:22:39,366 : Dev mean Image to text: 29.499999999999996, 63.940000000000005, 78.12, 3.0
2019-02-16 19:22:39,367 : start epoch
2019-02-16 19:23:21,980 : samples : 64000
2019-02-16 19:23:32,397 : Image to text: 10.66, 31.78, 44.5, 13.0
2019-02-16 19:23:39,942 : Text to Image: 9.732, 27.852, 40.072, 16.0
2019-02-16 19:24:22,873 : samples : 128000
2019-02-16 19:24:33,263 : Image to text: 11.1, 31.12, 44.52, 13.0
2019-02-16 19:24:40,825 : Text to Image: 9.472, 27.904, 40.464, 16.0
2019-02-16 19:25:23,467 : samples : 192000
2019-02-16 19:25:33,928 : Image to text: 10.9, 30.12, 44.02, 14.0
2019-02-16 19:25:41,506 : Text to Image: 9.704, 28.268, 40.36, 16.0
2019-02-16 19:26:23,947 : samples : 256000
2019-02-16 19:26:34,368 : Image to text: 10.6, 31.14, 43.96, 14.0
2019-02-16 19:26:41,929 : Text to Image: 9.408, 28.028, 40.56, 16.0
2019-02-16 19:27:24,408 : samples : 320000
2019-02-16 19:27:34,985 : Image to text: 10.86, 30.92, 43.48, 14.0
2019-02-16 19:27:42,511 : Text to Image: 9.752, 27.684, 40.092, 17.0
2019-02-16 19:28:24,759 : samples : 384000
2019-02-16 19:28:35,283 : Image to text: 10.76, 31.42, 43.72, 14.0
2019-02-16 19:28:42,856 : Text to Image: 9.708, 28.332, 40.78, 16.0
2019-02-16 19:29:25,715 : samples : 448000
2019-02-16 19:29:36,225 : Image to text: 11.14, 31.08, 44.4, 14.0
2019-02-16 19:29:43,873 : Text to Image: 9.888, 28.42, 41.08, 16.0
2019-02-16 19:30:26,754 : samples : 512000
2019-02-16 19:30:37,236 : Image to text: 11.14, 32.2, 45.9, 13.0
2019-02-16 19:30:44,836 : Text to Image: 9.984, 28.716, 41.076, 16.0
2019-02-16 19:31:21,253 : Epoch 7 finished
2019-02-16 19:31:21,720 : Image to text: 28.3, 59.9, 78.5, 4.0
2019-02-16 19:31:22,074 : Text to Image: 23.7, 58.02, 74.84, 4.0
2019-02-16 19:31:22,534 : Image to text: 29.9, 62.3, 77.8, 3.0
2019-02-16 19:31:22,896 : Text to Image: 24.6, 57.56, 74.54, 4.0
2019-02-16 19:31:23,382 : Image to text: 27.2, 61.9, 77.5, 4.0
2019-02-16 19:31:23,738 : Text to Image: 24.12, 58.56, 75.16, 4.0
2019-02-16 19:31:24,197 : Image to text: 27.6, 64.1, 78.3, 3.0
2019-02-16 19:31:24,556 : Text to Image: 24.58, 58.22, 75.34, 4.0
2019-02-16 19:31:25,045 : Image to text: 28.8, 61.8, 76.9, 3.0
2019-02-16 19:31:25,397 : Text to Image: 24.46, 58.76, 74.44, 4.0
2019-02-16 19:31:25,397 : Dev mean Text to Image: 24.291999999999998, 58.224000000000004, 74.864, 4.0
2019-02-16 19:31:25,397 : Dev mean Image to text: 28.36, 61.99999999999999, 77.8, 3.4000000000000004
2019-02-16 19:31:25,397 : start epoch
2019-02-16 19:32:07,798 : samples : 64000
2019-02-16 19:32:18,286 : Image to text: 11.44, 31.98, 44.8, 13.0
2019-02-16 19:32:25,924 : Text to Image: 9.816, 27.56, 39.976, 17.0
2019-02-16 19:33:08,484 : samples : 128000
2019-02-16 19:33:19,013 : Image to text: 10.82, 31.18, 43.88, 14.0
2019-02-16 19:33:26,603 : Text to Image: 9.44, 27.86, 39.744, 17.0
2019-02-16 19:34:09,388 : samples : 192000
2019-02-16 19:34:19,820 : Image to text: 10.88, 31.38, 45.68, 13.0
2019-02-16 19:34:27,387 : Text to Image: 9.608, 27.648, 40.54, 16.0
2019-02-16 19:35:16,864 : samples : 256000
2019-02-16 19:35:29,179 : Image to text: 11.1, 31.52, 45.0, 13.0
2019-02-16 19:35:36,718 : Text to Image: 9.836, 28.524, 41.132, 16.0
2019-02-16 19:36:19,518 : samples : 320000
2019-02-16 19:36:30,084 : Image to text: 11.34, 32.54, 45.98, 13.0
2019-02-16 19:36:37,644 : Text to Image: 10.368, 28.752, 41.728, 15.0
2019-02-16 19:37:20,700 : samples : 384000
2019-02-16 19:37:31,248 : Image to text: 10.98, 32.1, 44.88, 13.0
2019-02-16 19:37:38,800 : Text to Image: 10.0, 28.56, 40.82, 16.0
2019-02-16 19:38:21,716 : samples : 448000
2019-02-16 19:38:32,241 : Image to text: 11.16, 31.44, 45.68, 13.0
2019-02-16 19:38:39,828 : Text to Image: 9.88, 28.272, 40.744, 16.0
2019-02-16 19:39:22,638 : samples : 512000
2019-02-16 19:39:33,126 : Image to text: 10.52, 32.24, 45.68, 13.0
2019-02-16 19:39:40,716 : Text to Image: 10.016, 27.752, 40.408, 16.0
2019-02-16 19:40:17,344 : Epoch 8 finished
2019-02-16 19:40:17,785 : Image to text: 28.6, 65.2, 79.3, 3.0
2019-02-16 19:40:18,115 : Text to Image: 24.4, 58.96, 74.96, 4.0
2019-02-16 19:40:18,544 : Image to text: 31.0, 64.9, 79.0, 3.0
2019-02-16 19:40:18,885 : Text to Image: 24.92, 57.44, 74.42, 4.0
2019-02-16 19:40:19,315 : Image to text: 28.2, 64.3, 79.0, 3.0
2019-02-16 19:40:19,646 : Text to Image: 24.68, 59.06, 75.5, 4.0
2019-02-16 19:40:20,087 : Image to text: 31.2, 64.3, 78.6, 3.0
2019-02-16 19:40:20,416 : Text to Image: 24.74, 57.4, 74.96, 4.0
2019-02-16 19:40:20,845 : Image to text: 31.1, 64.3, 78.8, 3.0
2019-02-16 19:40:21,184 : Text to Image: 25.14, 58.4, 74.82, 4.0
2019-02-16 19:40:21,184 : Dev mean Text to Image: 24.776000000000003, 58.252, 74.93199999999999, 4.0
2019-02-16 19:40:21,184 : Dev mean Image to text: 30.020000000000003, 64.6, 78.94, 3.0
2019-02-16 19:40:21,185 : start epoch
2019-02-16 19:41:03,980 : samples : 64000
2019-02-16 19:41:14,528 : Image to text: 10.92, 31.62, 44.66, 13.0
2019-02-16 19:41:22,070 : Text to Image: 9.868, 28.092, 40.236, 16.0
2019-02-16 19:42:05,319 : samples : 128000
2019-02-16 19:42:15,818 : Image to text: 11.84, 32.88, 45.96, 13.0
2019-02-16 19:42:23,352 : Text to Image: 9.756, 27.764, 40.412, 16.0
2019-02-16 19:43:06,315 : samples : 192000
2019-02-16 19:43:16,750 : Image to text: 11.58, 32.32, 45.62, 13.0
2019-02-16 19:43:24,337 : Text to Image: 10.024, 28.288, 40.764, 16.0
2019-02-16 19:44:07,587 : samples : 256000
2019-02-16 19:44:20,169 : Image to text: 11.12, 31.46, 45.16, 13.0
2019-02-16 19:44:30,193 : Text to Image: 9.736, 28.496, 40.944, 16.0
2019-02-16 19:45:16,264 : samples : 320000
2019-02-16 19:45:28,953 : Image to text: 11.1, 32.76, 45.52, 13.0
2019-02-16 19:45:39,061 : Text to Image: 10.012, 28.712, 40.984, 16.0
2019-02-16 19:46:21,641 : samples : 384000
2019-02-16 19:46:31,965 : Image to text: 11.42, 31.18, 44.82, 13.0
2019-02-16 19:46:39,188 : Text to Image: 9.76, 28.492, 40.768, 16.0
2019-02-16 19:47:24,213 : samples : 448000
2019-02-16 19:47:36,896 : Image to text: 11.14, 32.2, 45.1, 13.0
2019-02-16 19:47:46,966 : Text to Image: 9.66, 27.892, 40.608, 16.0
2019-02-16 19:48:30,621 : samples : 512000
2019-02-16 19:48:40,914 : Image to text: 11.44, 31.9, 45.24, 13.0
2019-02-16 19:48:48,147 : Text to Image: 9.864, 28.132, 40.896, 16.0
2019-02-16 19:49:24,658 : Epoch 9 finished
2019-02-16 19:49:25,609 : Image to text: 27.4, 62.3, 78.9, 4.0
2019-02-16 19:49:26,397 : Text to Image: 25.38, 59.48, 75.38, 4.0
2019-02-16 19:49:27,347 : Image to text: 29.1, 64.1, 78.5, 3.0
2019-02-16 19:49:28,145 : Text to Image: 24.3, 58.38, 75.16, 4.0
2019-02-16 19:49:29,084 : Image to text: 29.1, 64.3, 79.6, 3.0
2019-02-16 19:49:29,859 : Text to Image: 25.44, 59.52, 75.88, 4.0
2019-02-16 19:49:30,834 : Image to text: 27.7, 64.6, 77.5, 3.0
2019-02-16 19:49:31,609 : Text to Image: 24.44, 58.28, 75.16, 4.0
2019-02-16 19:49:32,568 : Image to text: 29.6, 64.3, 78.9, 3.0
2019-02-16 19:49:33,345 : Text to Image: 26.14, 59.42, 75.04, 4.0
2019-02-16 19:49:33,345 : Dev mean Text to Image: 25.14, 59.016, 75.32399999999998, 4.0
2019-02-16 19:49:33,345 : Dev mean Image to text: 28.58, 63.92, 78.68, 3.2
2019-02-16 19:49:33,345 : start epoch
2019-02-16 19:50:19,053 : samples : 64000
2019-02-16 19:50:31,707 : Image to text: 11.5, 31.5, 44.86, 13.0
2019-02-16 19:50:41,350 : Text to Image: 9.556, 27.404, 39.528, 17.0
2019-02-16 19:51:24,113 : samples : 128000
2019-02-16 19:51:34,413 : Image to text: 11.46, 32.62, 45.66, 13.0
2019-02-16 19:51:41,723 : Text to Image: 10.3, 29.004, 41.532, 15.0
2019-02-16 19:52:35,741 : samples : 192000
2019-02-16 19:52:48,401 : Image to text: 11.88, 32.9, 45.98, 13.0
2019-02-16 19:52:58,431 : Text to Image: 10.232, 28.5, 41.052, 16.0
2019-02-16 19:53:41,830 : samples : 256000
2019-02-16 19:53:52,144 : Image to text: 11.36, 31.76, 44.48, 13.0
2019-02-16 19:53:59,638 : Text to Image: 9.824, 28.116, 40.98, 16.0
2019-02-16 19:54:44,362 : samples : 320000
2019-02-16 19:54:57,001 : Image to text: 11.5, 31.7, 44.92, 13.0
2019-02-16 19:55:07,049 : Text to Image: 9.788, 28.392, 40.928, 16.0
2019-02-16 19:55:51,206 : samples : 384000
2019-02-16 19:56:01,451 : Image to text: 11.92, 32.56, 45.46, 13.0
2019-02-16 19:56:08,910 : Text to Image: 10.112, 28.896, 41.492, 16.0
2019-02-16 19:56:52,457 : samples : 448000
2019-02-16 19:57:05,040 : Image to text: 11.62, 32.66, 45.78, 13.0
2019-02-16 19:57:15,065 : Text to Image: 9.668, 28.104, 40.828, 16.0
2019-02-16 19:58:00,760 : samples : 512000
2019-02-16 19:58:11,067 : Image to text: 11.32, 32.48, 45.7, 13.0
2019-02-16 19:58:18,593 : Text to Image: 10.228, 28.992, 41.84, 15.0
2019-02-16 19:58:54,925 : Epoch 10 finished
2019-02-16 19:58:55,815 : Image to text: 31.4, 63.1, 79.1, 3.0
2019-02-16 19:58:56,535 : Text to Image: 25.06, 60.06, 76.3, 4.0
2019-02-16 19:58:57,433 : Image to text: 29.0, 64.0, 78.7, 3.0
2019-02-16 19:58:58,171 : Text to Image: 25.68, 58.3, 75.62, 4.0
2019-02-16 19:58:59,090 : Image to text: 27.9, 63.1, 80.2, 3.0
2019-02-16 19:58:59,810 : Text to Image: 24.56, 59.86, 76.18, 4.0
2019-02-16 19:59:00,750 : Image to text: 29.4, 65.3, 79.8, 3.0
2019-02-16 19:59:01,526 : Text to Image: 25.2, 58.96, 75.52, 4.0
2019-02-16 19:59:02,439 : Image to text: 31.5, 64.2, 77.7, 3.0
2019-02-16 19:59:03,193 : Text to Image: 26.0, 59.38, 75.8, 4.0
2019-02-16 19:59:03,194 : Dev mean Text to Image: 25.299999999999997, 59.312, 75.884, 4.0
2019-02-16 19:59:03,194 : Dev mean Image to text: 29.839999999999996, 63.94000000000001, 79.1, 3.0
2019-02-16 19:59:03,194 : start epoch
2019-02-16 19:59:48,209 : samples : 64000
2019-02-16 20:00:00,842 : Image to text: 11.14, 32.42, 46.08, 13.0
2019-02-16 20:00:10,865 : Text to Image: 10.084, 29.08, 41.808, 15.0
2019-02-16 20:00:54,634 : samples : 128000
2019-02-16 20:01:04,912 : Image to text: 11.68, 32.92, 46.48, 13.0
2019-02-16 20:01:12,313 : Text to Image: 9.94, 28.64, 41.376, 15.0
2019-02-16 20:01:56,394 : samples : 192000
2019-02-16 20:02:09,025 : Image to text: 12.02, 32.92, 45.48, 13.0
2019-02-16 20:02:19,092 : Text to Image: 10.068, 28.496, 40.888, 16.0
2019-02-16 20:03:03,273 : samples : 256000
2019-02-16 20:03:13,574 : Image to text: 11.52, 33.22, 46.0, 13.0
2019-02-16 20:03:21,002 : Text to Image: 10.372, 29.252, 41.988, 15.0
2019-02-16 20:04:04,768 : samples : 320000
2019-02-16 20:04:17,381 : Image to text: 11.24, 32.18, 45.42, 13.0
2019-02-16 20:04:27,455 : Text to Image: 9.976, 28.54, 41.244, 16.0
2019-02-16 20:05:13,521 : samples : 384000
2019-02-16 20:05:25,999 : Image to text: 11.56, 32.8, 46.28, 13.0
2019-02-16 20:05:33,402 : Text to Image: 10.488, 29.268, 41.988, 15.0
2019-02-16 20:06:16,555 : samples : 448000
2019-02-16 20:06:27,040 : Image to text: 11.24, 32.46, 45.92, 12.0
2019-02-16 20:06:37,027 : Text to Image: 10.144, 29.04, 41.54, 15.0
2019-02-16 20:07:22,075 : samples : 512000
2019-02-16 20:07:34,741 : Image to text: 11.24, 32.5, 45.6, 13.0
2019-02-16 20:07:44,858 : Text to Image: 10.336, 28.876, 41.824, 15.0
2019-02-16 20:08:22,169 : Epoch 11 finished
2019-02-16 20:08:22,637 : Image to text: 28.0, 63.4, 79.7, 3.0
2019-02-16 20:08:23,014 : Text to Image: 24.76, 59.78, 76.44, 4.0
2019-02-16 20:08:23,479 : Image to text: 30.1, 63.0, 79.2, 3.0
2019-02-16 20:08:23,852 : Text to Image: 25.08, 57.92, 75.24, 4.0
2019-02-16 20:08:24,319 : Image to text: 29.5, 62.8, 78.5, 3.0
2019-02-16 20:08:24,693 : Text to Image: 25.26, 59.88, 76.3, 4.0
2019-02-16 20:08:25,151 : Image to text: 28.1, 64.6, 77.4, 3.0
2019-02-16 20:08:25,515 : Text to Image: 24.9, 58.16, 75.04, 4.0
2019-02-16 20:08:25,973 : Image to text: 29.9, 61.6, 76.4, 3.0
2019-02-16 20:08:26,336 : Text to Image: 25.54, 59.48, 75.5, 4.0
2019-02-16 20:08:26,336 : Dev mean Text to Image: 25.108, 59.044, 75.704, 4.0
2019-02-16 20:08:26,336 : Dev mean Image to text: 29.120000000000005, 63.080000000000005, 78.24000000000001, 3.0
2019-02-16 20:08:26,336 : start epoch
2019-02-16 20:09:10,890 : samples : 64000
2019-02-16 20:09:23,229 : Image to text: 11.58, 32.08, 45.28, 13.0
2019-02-16 20:09:32,399 : Text to Image: 10.184, 28.892, 41.276, 16.0
2019-02-16 20:10:18,332 : samples : 128000
2019-02-16 20:10:28,618 : Image to text: 10.86, 31.96, 45.62, 13.0
2019-02-16 20:10:36,052 : Text to Image: 9.98, 28.468, 41.32, 16.0
2019-02-16 20:11:18,841 : samples : 192000
2019-02-16 20:11:29,711 : Image to text: 11.6, 32.86, 46.56, 12.0
2019-02-16 20:11:37,148 : Text to Image: 10.46, 29.428, 41.896, 15.0
2019-02-16 20:12:20,027 : samples : 256000
2019-02-16 20:12:30,281 : Image to text: 11.6, 32.56, 45.78, 13.0
2019-02-16 20:12:37,619 : Text to Image: 10.22, 29.452, 42.244, 15.0
2019-02-16 20:13:20,687 : samples : 320000
2019-02-16 20:13:30,887 : Image to text: 11.66, 31.9, 44.7, 13.0
2019-02-16 20:13:40,518 : Text to Image: 9.892, 27.7, 40.164, 17.0
2019-02-16 20:14:23,745 : samples : 384000
2019-02-16 20:14:34,002 : Image to text: 11.08, 32.22, 45.76, 13.0
2019-02-16 20:14:41,435 : Text to Image: 10.136, 28.916, 41.656, 15.0
2019-02-16 20:15:24,980 : samples : 448000
2019-02-16 20:15:37,662 : Image to text: 11.18, 32.9, 46.0, 13.0
2019-02-16 20:15:47,824 : Text to Image: 10.384, 29.392, 42.076, 15.0
2019-02-16 20:16:31,366 : samples : 512000
2019-02-16 20:16:43,699 : Image to text: 11.2, 32.98, 46.96, 12.0
2019-02-16 20:16:52,071 : Text to Image: 10.364, 29.412, 42.012, 15.0
2019-02-16 20:17:29,553 : Epoch 12 finished
2019-02-16 20:17:30,017 : Image to text: 29.3, 64.5, 79.3, 3.0
2019-02-16 20:17:30,393 : Text to Image: 24.58, 58.98, 76.24, 4.0
2019-02-16 20:17:30,859 : Image to text: 31.0, 64.4, 78.1, 3.0
2019-02-16 20:17:31,225 : Text to Image: 25.82, 59.2, 76.12, 4.0
2019-02-16 20:17:31,683 : Image to text: 30.0, 62.0, 78.7, 3.0
2019-02-16 20:17:32,049 : Text to Image: 25.1, 60.1, 76.46, 4.0
2019-02-16 20:17:32,515 : Image to text: 28.6, 65.2, 79.2, 3.0
2019-02-16 20:17:32,882 : Text to Image: 25.66, 59.38, 76.76, 4.0
2019-02-16 20:17:33,356 : Image to text: 32.5, 64.5, 79.0, 3.0
2019-02-16 20:17:33,731 : Text to Image: 25.92, 60.58, 76.74, 4.0
2019-02-16 20:17:33,731 : Dev mean Text to Image: 25.416, 59.647999999999996, 76.464, 4.0
2019-02-16 20:17:33,731 : Dev mean Image to text: 30.28, 64.12, 78.86, 3.0
2019-02-16 20:17:33,732 : start epoch
2019-02-16 20:18:17,724 : samples : 64000
2019-02-16 20:18:27,977 : Image to text: 11.14, 33.16, 46.76, 12.0
2019-02-16 20:18:35,231 : Text to Image: 10.604, 28.96, 41.96, 15.0
2019-02-16 20:19:18,782 : samples : 128000
2019-02-16 20:19:29,265 : Image to text: 11.74, 32.62, 46.52, 12.0
2019-02-16 20:19:39,276 : Text to Image: 10.116, 28.564, 41.084, 16.0
2019-02-16 20:20:24,704 : samples : 192000
2019-02-16 20:20:37,284 : Image to text: 11.2, 32.66, 45.84, 13.0
2019-02-16 20:20:47,186 : Text to Image: 10.016, 28.956, 41.864, 15.0
2019-02-16 20:21:32,642 : samples : 256000
2019-02-16 20:21:45,295 : Image to text: 12.16, 33.5, 47.06, 12.0
2019-02-16 20:21:55,263 : Text to Image: 10.468, 29.44, 42.296, 15.0
2019-02-16 20:22:40,722 : samples : 320000
2019-02-16 20:22:53,290 : Image to text: 12.0, 32.68, 45.88, 13.0
2019-02-16 20:23:03,202 : Text to Image: 10.304, 29.556, 42.236, 15.0
2019-02-16 20:23:48,213 : samples : 384000
2019-02-16 20:24:00,778 : Image to text: 11.92, 32.54, 45.4, 13.0
2019-02-16 20:24:10,855 : Text to Image: 10.172, 28.928, 41.288, 15.0
2019-02-16 20:24:56,137 : samples : 448000
2019-02-16 20:25:08,701 : Image to text: 11.56, 33.1, 46.56, 12.0
2019-02-16 20:25:18,693 : Text to Image: 10.492, 29.2, 41.876, 15.0
2019-02-16 20:26:03,920 : samples : 512000
2019-02-16 20:26:16,856 : Image to text: 12.12, 32.96, 45.98, 12.0
2019-02-16 20:26:27,068 : Text to Image: 10.396, 29.216, 42.172, 15.0
2019-02-16 20:27:10,096 : Epoch 13 finished
2019-02-16 20:27:11,032 : Image to text: 28.8, 63.8, 80.4, 3.0
2019-02-16 20:27:11,775 : Text to Image: 24.94, 60.92, 77.38, 4.0
2019-02-16 20:27:12,710 : Image to text: 31.2, 63.9, 78.2, 3.0
2019-02-16 20:27:13,472 : Text to Image: 25.88, 59.94, 75.94, 4.0
2019-02-16 20:27:14,377 : Image to text: 28.5, 64.4, 78.7, 3.0
2019-02-16 20:27:15,148 : Text to Image: 25.8, 61.16, 76.64, 4.0
2019-02-16 20:27:16,072 : Image to text: 29.5, 65.8, 79.2, 3.0
2019-02-16 20:27:16,830 : Text to Image: 25.28, 60.3, 76.74, 4.0
2019-02-16 20:27:17,733 : Image to text: 31.8, 64.9, 77.4, 3.0
2019-02-16 20:27:18,445 : Text to Image: 26.0, 60.0, 76.3, 4.0
2019-02-16 20:27:18,445 : Dev mean Text to Image: 25.580000000000002, 60.464, 76.6, 4.0
2019-02-16 20:27:18,445 : Dev mean Image to text: 29.96, 64.56, 78.78, 3.0
2019-02-16 20:27:18,445 : start epoch
2019-02-16 20:28:02,858 : samples : 64000
2019-02-16 20:28:15,420 : Image to text: 11.92, 32.64, 46.62, 12.0
2019-02-16 20:28:25,486 : Text to Image: 10.412, 29.6, 42.264, 15.0
2019-02-16 20:29:10,044 : samples : 128000
2019-02-16 20:29:22,645 : Image to text: 11.82, 32.6, 46.9, 12.0
2019-02-16 20:29:32,702 : Text to Image: 10.324, 29.332, 41.932, 15.0
2019-02-16 20:30:17,722 : samples : 192000
2019-02-16 20:30:30,388 : Image to text: 12.16, 33.34, 46.26, 12.0
2019-02-16 20:30:39,538 : Text to Image: 10.248, 29.12, 41.692, 15.0
2019-02-16 20:31:23,829 : samples : 256000
2019-02-16 20:31:34,149 : Image to text: 11.52, 33.1, 46.58, 12.0
2019-02-16 20:31:41,607 : Text to Image: 10.332, 29.448, 42.316, 15.0
2019-02-16 20:32:23,958 : samples : 320000
2019-02-16 20:32:34,262 : Image to text: 11.16, 33.56, 46.88, 12.0
2019-02-16 20:32:41,725 : Text to Image: 10.188, 29.28, 41.992, 15.0
2019-02-16 20:33:25,779 : samples : 384000
2019-02-16 20:33:38,742 : Image to text: 11.84, 33.06, 46.16, 12.0
2019-02-16 20:33:49,132 : Text to Image: 10.152, 29.176, 41.764, 15.0
2019-02-16 20:34:34,792 : samples : 448000
2019-02-16 20:34:47,646 : Image to text: 11.62, 32.86, 46.76, 12.0
2019-02-16 20:34:58,094 : Text to Image: 10.308, 29.02, 42.016, 15.0
2019-02-16 20:35:44,392 : samples : 512000
2019-02-16 20:35:57,303 : Image to text: 11.88, 33.72, 47.44, 12.0
2019-02-16 20:36:07,761 : Text to Image: 10.684, 29.94, 42.22, 15.0
2019-02-16 20:36:47,246 : Epoch 14 finished
2019-02-16 20:36:48,261 : Image to text: 29.3, 63.9, 78.6, 3.0
2019-02-16 20:36:49,146 : Text to Image: 25.56, 59.9, 77.3, 4.0
2019-02-16 20:36:50,193 : Image to text: 29.2, 64.0, 77.8, 3.0
2019-02-16 20:36:51,051 : Text to Image: 25.28, 59.04, 76.6, 4.0
2019-02-16 20:36:52,090 : Image to text: 28.8, 63.4, 78.8, 3.0
2019-02-16 20:36:52,974 : Text to Image: 26.0, 60.36, 76.7, 4.0
2019-02-16 20:36:54,088 : Image to text: 29.3, 64.7, 79.4, 3.0
2019-02-16 20:36:54,995 : Text to Image: 25.46, 59.68, 76.78, 4.0
2019-02-16 20:36:56,166 : Image to text: 31.2, 63.9, 76.9, 3.0
2019-02-16 20:36:57,073 : Text to Image: 25.06, 60.68, 76.42, 4.0
2019-02-16 20:36:57,073 : Dev mean Text to Image: 25.472, 59.932, 76.76, 4.0
2019-02-16 20:36:57,074 : Dev mean Image to text: 29.560000000000002, 63.980000000000004, 78.3, 3.0
2019-02-16 20:37:07,010 : 
Test scores | Image to text:             30.0, 63.7, 78.17999999999999, 3.0
2019-02-16 20:37:07,010 : Test scores | Text to image:             25.772, 60.135999999999996, 76.23599999999999, 4.0

2019-02-16 20:37:07,110 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 20:37:07,342 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 20:37:08,003 : loading BERT model bert-base-uncased
2019-02-16 20:37:08,004 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:37:08,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:37:08,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpme7ibiuj
2019-02-16 20:37:10,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:37:12,073 : Computing embeddings for train/dev/test
2019-02-16 20:39:38,868 : Computed embeddings
2019-02-16 20:39:38,869 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:41:02,877 : [('reg:1e-05', 64.73), ('reg:0.0001', 58.14), ('reg:0.001', 53.06), ('reg:0.01', 48.69)]
2019-02-16 20:41:02,877 : Validation : best param found is reg = 1e-05 with score             64.73
2019-02-16 20:41:02,877 : Evaluating...
2019-02-16 20:41:26,979 : 
Dev acc : 64.7 Test acc : 64.6 for LENGTH classification

2019-02-16 20:41:26,980 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 20:41:27,343 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 20:41:27,395 : loading BERT model bert-base-uncased
2019-02-16 20:41:27,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:41:27,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:41:27,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmjhsg5od
2019-02-16 20:41:29,906 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:41:31,472 : Computing embeddings for train/dev/test
2019-02-16 20:43:24,730 : Computed embeddings
2019-02-16 20:43:24,730 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:44:02,090 : [('reg:1e-05', 12.36), ('reg:0.0001', 5.18), ('reg:0.001', 0.89), ('reg:0.01', 0.1)]
2019-02-16 20:44:02,090 : Validation : best param found is reg = 1e-05 with score             12.36
2019-02-16 20:44:02,090 : Evaluating...
2019-02-16 20:44:12,617 : 
Dev acc : 12.4 Test acc : 12.8 for WORDCONTENT classification

2019-02-16 20:44:12,618 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 20:44:13,005 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 20:44:13,076 : loading BERT model bert-base-uncased
2019-02-16 20:44:13,076 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:44:13,184 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:44:13,184 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6m3fcrpk
2019-02-16 20:44:15,696 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:44:17,186 : Computing embeddings for train/dev/test
2019-02-16 20:45:41,185 : Computed embeddings
2019-02-16 20:45:41,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:46:24,134 : [('reg:1e-05', 28.78), ('reg:0.0001', 29.16), ('reg:0.001', 32.28), ('reg:0.01', 26.92)]
2019-02-16 20:46:24,134 : Validation : best param found is reg = 0.001 with score             32.28
2019-02-16 20:46:24,134 : Evaluating...
2019-02-16 20:46:34,311 : 
Dev acc : 32.3 Test acc : 30.9 for DEPTH classification

2019-02-16 20:46:34,312 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 20:46:34,715 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 20:46:34,780 : loading BERT model bert-base-uncased
2019-02-16 20:46:34,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:46:34,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:46:34,897 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk4_6n9gr
2019-02-16 20:46:37,339 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:46:38,780 : Computing embeddings for train/dev/test
2019-02-16 20:47:57,345 : Computed embeddings
2019-02-16 20:47:57,345 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:48:40,754 : [('reg:1e-05', 56.63), ('reg:0.0001', 55.01), ('reg:0.001', 54.09), ('reg:0.01', 43.84)]
2019-02-16 20:48:40,754 : Validation : best param found is reg = 1e-05 with score             56.63
2019-02-16 20:48:40,754 : Evaluating...
2019-02-16 20:48:50,756 : 
Dev acc : 56.6 Test acc : 56.9 for TOPCONSTITUENTS classification

2019-02-16 20:48:50,757 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 20:48:51,312 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 20:48:51,380 : loading BERT model bert-base-uncased
2019-02-16 20:48:51,380 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:48:51,411 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:48:51,411 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprerm7r4q
2019-02-16 20:48:53,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:48:55,467 : Computing embeddings for train/dev/test
2019-02-16 20:50:19,682 : Computed embeddings
2019-02-16 20:50:19,683 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:51:09,315 : [('reg:1e-05', 88.29), ('reg:0.0001', 88.33), ('reg:0.001', 88.31), ('reg:0.01', 87.86)]
2019-02-16 20:51:09,315 : Validation : best param found is reg = 0.0001 with score             88.33
2019-02-16 20:51:09,315 : Evaluating...
2019-02-16 20:51:22,648 : 
Dev acc : 88.3 Test acc : 87.8 for BIGRAMSHIFT classification

2019-02-16 20:51:22,649 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 20:51:23,263 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 20:51:23,334 : loading BERT model bert-base-uncased
2019-02-16 20:51:23,334 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:51:23,366 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:51:23,367 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz3jc3m3z
2019-02-16 20:51:25,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:51:27,353 : Computing embeddings for train/dev/test
2019-02-16 20:52:50,427 : Computed embeddings
2019-02-16 20:52:50,427 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:53:30,630 : [('reg:1e-05', 89.48), ('reg:0.0001', 89.46), ('reg:0.001', 89.58), ('reg:0.01', 89.58)]
2019-02-16 20:53:30,630 : Validation : best param found is reg = 0.001 with score             89.58
2019-02-16 20:53:30,630 : Evaluating...
2019-02-16 20:53:41,061 : 
Dev acc : 89.6 Test acc : 88.0 for TENSE classification

2019-02-16 20:53:41,062 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 20:53:41,570 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 20:53:41,645 : loading BERT model bert-base-uncased
2019-02-16 20:53:41,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:53:41,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:53:41,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5q58sqlm
2019-02-16 20:53:44,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:53:45,646 : Computing embeddings for train/dev/test
2019-02-16 20:55:13,706 : Computed embeddings
2019-02-16 20:55:13,706 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:55:58,370 : [('reg:1e-05', 82.4), ('reg:0.0001', 82.46), ('reg:0.001', 81.86), ('reg:0.01', 81.2)]
2019-02-16 20:55:58,370 : Validation : best param found is reg = 0.0001 with score             82.46
2019-02-16 20:55:58,370 : Evaluating...
2019-02-16 20:56:08,520 : 
Dev acc : 82.5 Test acc : 81.5 for SUBJNUMBER classification

2019-02-16 20:56:08,521 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 20:56:08,962 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 20:56:09,034 : loading BERT model bert-base-uncased
2019-02-16 20:56:09,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:56:09,162 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:56:09,162 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp78op1de0
2019-02-16 20:56:11,636 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:56:13,119 : Computing embeddings for train/dev/test
2019-02-16 20:57:39,030 : Computed embeddings
2019-02-16 20:57:39,030 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:58:32,680 : [('reg:1e-05', 76.91), ('reg:0.0001', 76.99), ('reg:0.001', 76.91), ('reg:0.01', 76.89)]
2019-02-16 20:58:32,680 : Validation : best param found is reg = 0.0001 with score             76.99
2019-02-16 20:58:32,680 : Evaluating...
2019-02-16 20:58:45,004 : 
Dev acc : 77.0 Test acc : 78.2 for OBJNUMBER classification

2019-02-16 20:58:45,006 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 20:58:45,642 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 20:58:45,716 : loading BERT model bert-base-uncased
2019-02-16 20:58:45,716 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:58:45,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:58:45,751 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw5431o6m
2019-02-16 20:58:48,268 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:58:49,790 : Computing embeddings for train/dev/test
2019-02-16 21:00:29,234 : Computed embeddings
2019-02-16 21:00:29,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:01:15,519 : [('reg:1e-05', 64.6), ('reg:0.0001', 64.61), ('reg:0.001', 64.68), ('reg:0.01', 64.2)]
2019-02-16 21:01:15,520 : Validation : best param found is reg = 0.001 with score             64.68
2019-02-16 21:01:15,520 : Evaluating...
2019-02-16 21:01:27,354 : 
Dev acc : 64.7 Test acc : 64.3 for ODDMANOUT classification

2019-02-16 21:01:27,355 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 21:01:27,789 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 21:01:27,871 : loading BERT model bert-base-uncased
2019-02-16 21:01:27,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:01:28,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:01:28,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqmnfosvb
2019-02-16 21:01:30,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:01:31,981 : Computing embeddings for train/dev/test
2019-02-16 21:03:09,035 : Computed embeddings
2019-02-16 21:03:09,035 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:03:56,819 : [('reg:1e-05', 68.33), ('reg:0.0001', 68.29), ('reg:0.001', 57.04), ('reg:0.01', 66.9)]
2019-02-16 21:03:56,819 : Validation : best param found is reg = 1e-05 with score             68.33
2019-02-16 21:03:56,820 : Evaluating...
2019-02-16 21:04:08,834 : 
Dev acc : 68.3 Test acc : 67.9 for COORDINATIONINVERSION classification

2019-02-16 21:04:08,837 : total results: {'STS12': {'MSRpar': {'pearson': (0.29793203523206657, 7.709163093254985e-17), 'spearman': SpearmanrResult(correlation=0.3253386555372184, pvalue=5.971644488826531e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.521904618729797, 1.2504356479926868e-53), 'spearman': SpearmanrResult(correlation=0.5373335379988915, pvalue=2.481386485908897e-57), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46360777732975544, 7.735605908649822e-26), 'spearman': SpearmanrResult(correlation=0.5761071524590785, pvalue=6.105407663029169e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5090432998861961, 1.0991047250407898e-50), 'spearman': SpearmanrResult(correlation=0.5389988746703394, pvalue=9.627666999652771e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.589615569832882, 1.0105445002872701e-38), 'spearman': SpearmanrResult(correlation=0.49063834435164594, pvalue=1.4697765722897426e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4764206602021394, 'wmean': 0.4648367270089197}, 'spearman': {'mean': 0.4936833130034347, 'wmean': 0.4863099046106373}}}, 'STS13': {'FNWN': {'pearson': (0.16774053660300872, 0.021047899617708664), 'spearman': SpearmanrResult(correlation=0.18279542249642852, pvalue=0.011815766249091246), 'nsamples': 189}, 'headlines': {'pearson': (0.6147169548123854, 3.845547368223589e-79), 'spearman': SpearmanrResult(correlation=0.6040281001921605, pvalue=9.089408833249743e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.44033828631054356, 5.21346182951842e-28), 'spearman': SpearmanrResult(correlation=0.43370540215032527, pvalue=3.921655992003065e-27), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40759859257531256, 'wmean': 0.49318030409831515}, 'spearman': {'mean': 0.4068429749463047, 'wmean': 0.48725209373485184}}}, 'STS14': {'deft-forum': {'pearson': (0.319319192600371, 3.994210355953463e-12), 'spearman': SpearmanrResult(correlation=0.32435819432805185, pvalue=1.749454956063282e-12), 'nsamples': 450}, 'deft-news': {'pearson': (0.7170746422738099, 1.2309551447739739e-48), 'spearman': SpearmanrResult(correlation=0.6943467362098951, pvalue=1.745089066094122e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.554885131418249, 8.771688022575813e-62), 'spearman': SpearmanrResult(correlation=0.5290768678795359, pvalue=2.510280408278949e-55), 'nsamples': 750}, 'images': {'pearson': (0.4584824390511578, 2.9615188926409804e-40), 'spearman': SpearmanrResult(correlation=0.46259436536981136, pvalue=4.863041391059824e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.5855907561832608, 3.063578908698617e-70), 'spearman': SpearmanrResult(correlation=0.6132835279796067, pvalue=1.1088891857475936e-78), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5823367544430249, 2.66295636847571e-69), 'spearman': SpearmanrResult(correlation=0.539745974379931, pvalue=6.285112059096976e-58), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5362814859949788, 'wmean': 0.5319432907130878}, 'spearman': {'mean': 0.5272342776911386, 'wmean': 0.5234108693379348}}}, 'STS15': {'answers-forums': {'pearson': (0.5003471228155241, 3.754378276194958e-25), 'spearman': SpearmanrResult(correlation=0.47872627894145076, pvalue=7.059043450956342e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.65436837519356, 7.2134155003563335e-93), 'spearman': SpearmanrResult(correlation=0.6622915603854383, pvalue=7.2815793567328606e-96), 'nsamples': 750}, 'belief': {'pearson': (0.544065325524441, 2.811368776120776e-30), 'spearman': SpearmanrResult(correlation=0.5796309699112007, pvalue=4.87683433795372e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6067518702626525, 1.2913210925210671e-76), 'spearman': SpearmanrResult(correlation=0.6147196359224287, pvalue=3.837917932905326e-79), 'nsamples': 750}, 'images': {'pearson': (0.6331594848360754, 2.8162651318915163e-85), 'spearman': SpearmanrResult(correlation=0.6418795334566697, pvalue=2.518310181911213e-88), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5877384357264506, 'wmean': 0.6041214886155677}, 'spearman': {'mean': 0.5954495957234376, 'wmean': 0.6120173385477157}}}, 'STS16': {'answer-answer': {'pearson': (0.49315371271871794, 5.678555071137419e-17), 'spearman': SpearmanrResult(correlation=0.49162448591271696, pvalue=7.316884410908604e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.628267538483566, 9.461709459931456e-29), 'spearman': SpearmanrResult(correlation=0.632497580763512, pvalue=3.150938126260474e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.663062964663489, 1.6699968345508015e-30), 'spearman': SpearmanrResult(correlation=0.6750648318473675, pvalue=5.961487072830946e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7541349628582156, 4.332399365935241e-46), 'spearman': SpearmanrResult(correlation=0.7929888280800257, pvalue=5.338358796023229e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.23204096780166983, 0.0007229609337872613), 'spearman': SpearmanrResult(correlation=0.252789676308243, pvalue=0.00022181779152539248), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5541320293051316, 'wmean': 0.562149776723034}, 'spearman': {'mean': 0.5689930805823731, 'wmean': 0.5766878117679496}}}, 'MR': {'devacc': 73.6, 'acc': 73.01, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.26, 'acc': 73.25, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.59, 'acc': 87.68, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.27, 'acc': 92.91, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.06, 'acc': 83.2, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 43.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 74.42, 'acc': 89.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.86, 'acc': 69.91, 'f1': 75.06, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 73.03, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7849114722360065, 'pearson': 0.7791974566428523, 'spearman': 0.7155194844797105, 'mse': 0.40100860132932276, 'yhat': array([1.69784252, 3.7252335 , 1.81158338, ..., 3.09173285, 4.16478261,        4.08256874]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6350037223493994, 'pearson': 0.625147973339803, 'spearman': 0.6210754290394137, 'mse': 1.4506891891495912, 'yhat': array([1.33419468, 1.39959447, 2.11318258, ..., 3.81336286, 3.78110322,        4.02130302]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.1, 'acc': 65.87, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 335.94399999999996, 'acc': [(30.0, 63.7, 78.17999999999999, 3.0), (25.772, 60.135999999999996, 76.23599999999999, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 64.73, 'acc': 64.58, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.36, 'acc': 12.8, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.28, 'acc': 30.92, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 56.63, 'acc': 56.9, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.33, 'acc': 87.83, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.58, 'acc': 87.99, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.46, 'acc': 81.46, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.99, 'acc': 78.24, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.68, 'acc': 64.32, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.33, 'acc': 67.89, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 21:04:08,837 : STS12 p=0.4648, STS12 s=0.4863, STS13 p=0.4932, STS13 s=0.4873, STS14 p=0.5319, STS14 s=0.5234, STS15 p=0.6041, STS15 s=0.6120, STS 16 p=0.5621, STS16 s=0.5767, STS B p=0.6251, STS B s=0.6211, STS B m=1.4507, SICK-R p=0.7792, SICK-R s=0.7155, SICK-P m=0.4010
2019-02-16 21:04:08,837 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 21:04:08,837 : 0.4648,0.4863,0.4932,0.4873,0.5319,0.5234,0.6041,0.6120,0.5621,0.5767,0.6251,0.6211,1.4507,0.7792,0.7155,0.4010
2019-02-16 21:04:08,837 : MR=73.01, CR=73.25, SUBJ=92.91, MPQA=87.68, SST-B=83.20, SST-F=43.26, TREC=89.20, SICK-E=73.03, SNLI=65.87, MRPC=69.91, MRPC f=75.06
2019-02-16 21:04:08,837 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 21:04:08,837 : 73.01,73.25,92.91,87.68,83.20,43.26,89.20,73.03,65.87,69.91,75.06
2019-02-16 21:04:08,837 : COCO r1i2t=30.00, COCO r5i2t=63.70, COCO r10i2t=78.18, COCO medr_i2t=3.00, COCO r1t2i=25.77, COCO r5t2i=60.14, COCO r10t2i=76.24, COCO medr_t2i=4.00
2019-02-16 21:04:08,837 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 21:04:08,837 : 30.00,63.70,78.18,3.00,25.77,60.14,76.24,4.00
2019-02-16 21:04:08,837 : SentLen=64.58, WC=12.80, TreeDepth=30.92, TopConst=56.90, BShift=87.83, Tense=87.99, SubjNum=81.46, ObjNum=78.24, SOMO=64.32, CoordInv=67.89, average=63.29
2019-02-16 21:04:08,837 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 21:04:08,837 : 64.58,12.80,30.92,56.90,87.83,87.99,81.46,78.24,64.32,67.89,63.29
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,837 : layer 10
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,837 : ********************************************************************************
2019-02-16 21:04:08,944 : ***** Transfer task : STS12 *****


2019-02-16 21:04:08,986 : loading BERT model bert-base-uncased
2019-02-16 21:04:08,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:09,004 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:09,004 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd0u1vgim
2019-02-16 21:04:11,501 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:14,753 : MSRpar : pearson = 0.3177, spearman = 0.3499
2019-02-16 21:04:15,512 : MSRvid : pearson = 0.5318, spearman = 0.5517
2019-02-16 21:04:16,154 : SMTeuroparl : pearson = 0.4933, spearman = 0.5934
2019-02-16 21:04:17,345 : surprise.OnWN : pearson = 0.5113, spearman = 0.5411
2019-02-16 21:04:18,060 : surprise.SMTnews : pearson = 0.5777, spearman = 0.4969
2019-02-16 21:04:18,060 : ALL (weighted average) : Pearson = 0.4754,             Spearman = 0.4996
2019-02-16 21:04:18,060 : ALL (average) : Pearson = 0.4864,             Spearman = 0.5066

2019-02-16 21:04:18,060 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 21:04:18,071 : loading BERT model bert-base-uncased
2019-02-16 21:04:18,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:18,090 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:18,090 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu2h1bfzu
2019-02-16 21:04:20,573 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:22,727 : FNWN : pearson = 0.2312, spearman = 0.2522
2019-02-16 21:04:23,701 : headlines : pearson = 0.6394, spearman = 0.6270
2019-02-16 21:04:24,444 : OnWN : pearson = 0.5518, spearman = 0.5427
2019-02-16 21:04:24,444 : ALL (weighted average) : Pearson = 0.5552,             Spearman = 0.5483
2019-02-16 21:04:24,444 : ALL (average) : Pearson = 0.4741,             Spearman = 0.4740

2019-02-16 21:04:24,444 : ***** Transfer task : STS14 *****


2019-02-16 21:04:24,462 : loading BERT model bert-base-uncased
2019-02-16 21:04:24,463 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:24,483 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:24,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3ce1nq0f
2019-02-16 21:04:27,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:29,109 : deft-forum : pearson = 0.3375, spearman = 0.3397
2019-02-16 21:04:29,809 : deft-news : pearson = 0.7413, spearman = 0.7210
2019-02-16 21:04:30,786 : headlines : pearson = 0.5794, spearman = 0.5454
2019-02-16 21:04:31,711 : images : pearson = 0.4654, spearman = 0.4666
2019-02-16 21:04:32,646 : OnWN : pearson = 0.6501, spearman = 0.6751
2019-02-16 21:04:33,890 : tweet-news : pearson = 0.6091, spearman = 0.5568
2019-02-16 21:04:33,890 : ALL (weighted average) : Pearson = 0.5606,             Spearman = 0.5472
2019-02-16 21:04:33,890 : ALL (average) : Pearson = 0.5638,             Spearman = 0.5508

2019-02-16 21:04:33,890 : ***** Transfer task : STS15 *****


2019-02-16 21:04:33,958 : loading BERT model bert-base-uncased
2019-02-16 21:04:33,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:33,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:33,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpinreihue
2019-02-16 21:04:36,487 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:38,830 : answers-forums : pearson = 0.5325, spearman = 0.5137
2019-02-16 21:04:39,780 : answers-students : pearson = 0.6244, spearman = 0.6396
2019-02-16 21:04:40,673 : belief : pearson = 0.5829, spearman = 0.6120
2019-02-16 21:04:41,694 : headlines : pearson = 0.6406, spearman = 0.6457
2019-02-16 21:04:42,683 : images : pearson = 0.6460, spearman = 0.6576
2019-02-16 21:04:42,683 : ALL (weighted average) : Pearson = 0.6172,             Spearman = 0.6265
2019-02-16 21:04:42,683 : ALL (average) : Pearson = 0.6053,             Spearman = 0.6137

2019-02-16 21:04:42,683 : ***** Transfer task : STS16 *****


2019-02-16 21:04:42,726 : loading BERT model bert-base-uncased
2019-02-16 21:04:42,726 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:42,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:42,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq3ul0hs9
2019-02-16 21:04:45,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:47,158 : answer-answer : pearson = 0.5145, spearman = 0.5004
2019-02-16 21:04:47,451 : headlines : pearson = 0.6537, spearman = 0.6568
2019-02-16 21:04:47,845 : plagiarism : pearson = 0.7186, spearman = 0.7258
2019-02-16 21:04:48,476 : postediting : pearson = 0.7791, spearman = 0.8085
2019-02-16 21:04:48,777 : question-question : pearson = 0.3818, spearman = 0.3936
2019-02-16 21:04:48,777 : ALL (weighted average) : Pearson = 0.6144,             Spearman = 0.6215
2019-02-16 21:04:48,777 : ALL (average) : Pearson = 0.6095,             Spearman = 0.6170

2019-02-16 21:04:48,777 : ***** Transfer task : MR *****


2019-02-16 21:04:48,836 : loading BERT model bert-base-uncased
2019-02-16 21:04:48,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:04:48,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:04:48,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph0zes20q
2019-02-16 21:04:51,412 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:04:52,903 : Generating sentence embeddings
2019-02-16 21:05:06,408 : Generated sentence embeddings
2019-02-16 21:05:06,408 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 21:05:25,389 : Best param found at split 1: l2reg = 0.01                 with score 74.28
2019-02-16 21:05:45,320 : Best param found at split 2: l2reg = 1e-05                 with score 75.04
2019-02-16 21:06:07,553 : Best param found at split 3: l2reg = 1e-05                 with score 71.04
2019-02-16 21:06:26,927 : Best param found at split 4: l2reg = 0.01                 with score 75.1
2019-02-16 21:06:47,496 : Best param found at split 5: l2reg = 0.001                 with score 76.18
2019-02-16 21:06:48,765 : Dev acc : 74.33 Test acc : 74.48

2019-02-16 21:06:48,766 : ***** Transfer task : CR *****


2019-02-16 21:06:48,774 : loading BERT model bert-base-uncased
2019-02-16 21:06:48,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:06:48,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:06:48,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi8whhr5x
2019-02-16 21:06:51,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:06:52,837 : Generating sentence embeddings
2019-02-16 21:06:56,487 : Generated sentence embeddings
2019-02-16 21:06:56,487 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 21:07:01,437 : Best param found at split 1: l2reg = 1e-05                 with score 77.31
2019-02-16 21:07:07,628 : Best param found at split 2: l2reg = 0.001                 with score 79.86
2019-02-16 21:07:14,412 : Best param found at split 3: l2reg = 1e-05                 with score 80.43
2019-02-16 21:07:20,627 : Best param found at split 4: l2reg = 0.001                 with score 81.53
2019-02-16 21:07:26,861 : Best param found at split 5: l2reg = 0.0001                 with score 75.34
2019-02-16 21:07:27,271 : Dev acc : 78.89 Test acc : 80.71

2019-02-16 21:07:27,271 : ***** Transfer task : MPQA *****


2019-02-16 21:07:27,310 : loading BERT model bert-base-uncased
2019-02-16 21:07:27,310 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:07:27,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:07:27,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfciv1h9b
2019-02-16 21:07:29,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:07:31,286 : Generating sentence embeddings
2019-02-16 21:07:35,148 : Generated sentence embeddings
2019-02-16 21:07:35,148 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 21:07:51,152 : Best param found at split 1: l2reg = 0.0001                 with score 87.38
2019-02-16 21:08:06,955 : Best param found at split 2: l2reg = 0.001                 with score 87.54
2019-02-16 21:08:25,749 : Best param found at split 3: l2reg = 0.001                 with score 86.88
2019-02-16 21:08:42,358 : Best param found at split 4: l2reg = 0.01                 with score 87.11
2019-02-16 21:08:57,815 : Best param found at split 5: l2reg = 1e-05                 with score 86.72
2019-02-16 21:08:59,597 : Dev acc : 87.13 Test acc : 87.62

2019-02-16 21:08:59,598 : ***** Transfer task : SUBJ *****


2019-02-16 21:08:59,618 : loading BERT model bert-base-uncased
2019-02-16 21:08:59,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:08:59,648 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:08:59,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt8yrz_yt
2019-02-16 21:09:02,189 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:09:03,780 : Generating sentence embeddings
2019-02-16 21:09:16,984 : Generated sentence embeddings
2019-02-16 21:09:16,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 21:09:34,892 : Best param found at split 1: l2reg = 0.01                 with score 94.06
2019-02-16 21:09:53,861 : Best param found at split 2: l2reg = 1e-05                 with score 94.26
2019-02-16 21:10:12,798 : Best param found at split 3: l2reg = 1e-05                 with score 93.72
2019-02-16 21:10:32,778 : Best param found at split 4: l2reg = 1e-05                 with score 94.52
2019-02-16 21:10:51,703 : Best param found at split 5: l2reg = 0.001                 with score 94.01
2019-02-16 21:10:52,345 : Dev acc : 94.11 Test acc : 93.65

2019-02-16 21:10:52,346 : ***** Transfer task : SST Binary classification *****


2019-02-16 21:10:52,442 : loading BERT model bert-base-uncased
2019-02-16 21:10:52,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:10:52,526 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:10:52,526 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa76r36xr
2019-02-16 21:10:55,057 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:10:56,564 : Computing embedding for train
2019-02-16 21:11:42,479 : Computed train embeddings
2019-02-16 21:11:42,479 : Computing embedding for dev
2019-02-16 21:11:43,416 : Computed dev embeddings
2019-02-16 21:11:43,416 : Computing embedding for test
2019-02-16 21:11:45,414 : Computed test embeddings
2019-02-16 21:11:45,414 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:12:17,708 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.63), ('reg:0.001', 84.4), ('reg:0.01', 85.32)]
2019-02-16 21:12:17,709 : Validation : best param found is reg = 0.01 with score             85.32
2019-02-16 21:12:17,709 : Evaluating...
2019-02-16 21:12:25,619 : 
Dev acc : 85.32 Test acc : 83.31 for             SST Binary classification

2019-02-16 21:12:25,619 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 21:12:25,674 : loading BERT model bert-base-uncased
2019-02-16 21:12:25,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:12:25,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:12:25,695 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpneioetys
2019-02-16 21:12:28,181 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:12:29,693 : Computing embedding for train
2019-02-16 21:12:39,157 : Computed train embeddings
2019-02-16 21:12:39,157 : Computing embedding for dev
2019-02-16 21:12:40,380 : Computed dev embeddings
2019-02-16 21:12:40,381 : Computing embedding for test
2019-02-16 21:12:42,837 : Computed test embeddings
2019-02-16 21:12:42,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:12:46,949 : [('reg:1e-05', 42.05), ('reg:0.0001', 37.6), ('reg:0.001', 37.24), ('reg:0.01', 40.42)]
2019-02-16 21:12:46,949 : Validation : best param found is reg = 1e-05 with score             42.05
2019-02-16 21:12:46,949 : Evaluating...
2019-02-16 21:12:47,856 : 
Dev acc : 42.05 Test acc : 42.26 for             SST Fine-Grained classification

2019-02-16 21:12:47,857 : ***** Transfer task : TREC *****


2019-02-16 21:12:47,870 : loading BERT model bert-base-uncased
2019-02-16 21:12:47,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:12:47,892 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:12:47,892 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo09h1jhf
2019-02-16 21:12:50,385 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:12:55,260 : Computed train embeddings
2019-02-16 21:12:55,535 : Computed test embeddings
2019-02-16 21:12:55,535 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 21:13:06,951 : [('reg:1e-05', 75.63), ('reg:0.0001', 73.7), ('reg:0.001', 67.18), ('reg:0.01', 74.28)]
2019-02-16 21:13:06,951 : Cross-validation : best param found is reg = 1e-05             with score 75.63
2019-02-16 21:13:06,952 : Evaluating...
2019-02-16 21:13:07,678 : 
Dev acc : 75.63 Test acc : 74.8             for TREC

2019-02-16 21:13:07,678 : ***** Transfer task : MRPC *****


2019-02-16 21:13:07,701 : loading BERT model bert-base-uncased
2019-02-16 21:13:07,701 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:13:07,728 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:13:07,728 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjhoiyrvp
2019-02-16 21:13:10,244 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:13:11,685 : Computing embedding for train
2019-02-16 21:13:22,245 : Computed train embeddings
2019-02-16 21:13:22,245 : Computing embedding for test
2019-02-16 21:13:26,824 : Computed test embeddings
2019-02-16 21:13:26,840 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 21:13:32,816 : [('reg:1e-05', 72.08), ('reg:0.0001', 71.79), ('reg:0.001', 72.25), ('reg:0.01', 71.05)]
2019-02-16 21:13:32,816 : Cross-validation : best param found is reg = 0.001             with score 72.25
2019-02-16 21:13:32,817 : Evaluating...
2019-02-16 21:13:33,278 : Dev acc : 72.25 Test acc 68.29; Test F1 74.64 for MRPC.

2019-02-16 21:13:33,278 : ***** Transfer task : SICK-Entailment*****


2019-02-16 21:13:33,341 : loading BERT model bert-base-uncased
2019-02-16 21:13:33,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:13:33,363 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:13:33,363 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppuvqgwww
2019-02-16 21:13:35,889 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:13:37,390 : Computing embedding for train
2019-02-16 21:13:42,558 : Computed train embeddings
2019-02-16 21:13:42,558 : Computing embedding for dev
2019-02-16 21:13:43,233 : Computed dev embeddings
2019-02-16 21:13:43,233 : Computing embedding for test
2019-02-16 21:13:48,807 : Computed test embeddings
2019-02-16 21:13:48,835 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:13:50,749 : [('reg:1e-05', 76.8), ('reg:0.0001', 77.0), ('reg:0.001', 77.2), ('reg:0.01', 73.2)]
2019-02-16 21:13:50,750 : Validation : best param found is reg = 0.001 with score             77.2
2019-02-16 21:13:50,750 : Evaluating...
2019-02-16 21:13:51,090 : 
Dev acc : 77.2 Test acc : 76.46 for                        SICK entailment

2019-02-16 21:13:51,090 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 21:13:51,119 : loading BERT model bert-base-uncased
2019-02-16 21:13:51,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:13:51,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:13:51,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo1fm7ecr
2019-02-16 21:13:53,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:13:55,183 : Computing embedding for train
2019-02-16 21:14:00,395 : Computed train embeddings
2019-02-16 21:14:00,395 : Computing embedding for dev
2019-02-16 21:14:01,071 : Computed dev embeddings
2019-02-16 21:14:01,071 : Computing embedding for test
2019-02-16 21:14:06,584 : Computed test embeddings
2019-02-16 21:14:29,574 : Dev : Pearson 0.7721522474429817
2019-02-16 21:14:29,574 : Test : Pearson 0.7852996423354045 Spearman 0.7144851283534263 MSE 0.3907123645302785                        for SICK Relatedness

2019-02-16 21:14:29,575 : 

***** Transfer task : STSBenchmark*****


2019-02-16 21:14:29,616 : loading BERT model bert-base-uncased
2019-02-16 21:14:29,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:14:29,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:14:29,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl7v9zaxc
2019-02-16 21:14:32,129 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:14:33,616 : Computing embedding for train
2019-02-16 21:14:41,884 : Computed train embeddings
2019-02-16 21:14:41,884 : Computing embedding for dev
2019-02-16 21:14:44,363 : Computed dev embeddings
2019-02-16 21:14:44,363 : Computing embedding for test
2019-02-16 21:14:46,392 : Computed test embeddings
2019-02-16 21:15:15,245 : Dev : Pearson 0.6290811178773263
2019-02-16 21:15:15,245 : Test : Pearson 0.6357128194002151 Spearman 0.6331347975190704 MSE 1.4463465968284188                        for SICK Relatedness

2019-02-16 21:15:15,246 : ***** Transfer task : SNLI Entailment*****


2019-02-16 21:15:20,146 : loading BERT model bert-base-uncased
2019-02-16 21:15:20,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:15:20,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:15:20,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp36iug7qz
2019-02-16 21:15:22,801 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:15:24,499 : PROGRESS (encoding): 0.00%
2019-02-16 21:16:43,673 : PROGRESS (encoding): 14.56%
2019-02-16 21:18:13,565 : PROGRESS (encoding): 29.12%
2019-02-16 21:19:43,335 : PROGRESS (encoding): 43.69%
2019-02-16 21:21:18,140 : PROGRESS (encoding): 58.25%
2019-02-16 21:23:02,874 : PROGRESS (encoding): 72.81%
2019-02-16 21:24:47,419 : PROGRESS (encoding): 87.37%
2019-02-16 21:26:38,408 : PROGRESS (encoding): 0.00%
2019-02-16 21:26:51,878 : PROGRESS (encoding): 0.00%
2019-02-16 21:27:04,871 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 21:27:46,642 : [('reg:1e-09', 67.2)]
2019-02-16 21:27:46,642 : Validation : best param found is reg = 1e-09 with score             67.2
2019-02-16 21:27:46,642 : Evaluating...
2019-02-16 21:28:29,072 : Dev acc : 67.2 Test acc : 67.0 for SNLI

2019-02-16 21:28:29,072 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 21:28:39,675 : loading BERT model bert-base-uncased
2019-02-16 21:28:39,675 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 21:28:39,725 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 21:28:39,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxxdypf47
2019-02-16 21:28:42,173 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 21:28:43,631 : Computing embedding for train
2019-02-16 21:36:20,047 : Computed train embeddings
2019-02-16 21:36:20,047 : Computing embedding for dev
2019-02-16 21:36:39,431 : Computed dev embeddings
2019-02-16 21:36:39,431 : Computing embedding for test
2019-02-16 21:36:59,360 : Computed test embeddings
2019-02-16 21:36:59,376 : prepare data
2019-02-16 21:36:59,442 : start epoch
2019-02-16 21:37:42,287 : samples : 64000
2019-02-16 21:37:52,754 : Image to text: 7.76, 21.76, 33.02, 24.0
2019-02-16 21:38:00,324 : Text to Image: 6.148, 19.188, 29.28, 28.0
2019-02-16 21:38:43,221 : samples : 128000
2019-02-16 21:38:53,658 : Image to text: 8.14, 23.98, 35.3, 21.0
2019-02-16 21:39:01,225 : Text to Image: 6.888, 21.084, 31.964, 25.0
2019-02-16 21:39:44,630 : samples : 192000
2019-02-16 21:39:55,101 : Image to text: 7.74, 23.64, 35.0, 22.0
2019-02-16 21:40:02,743 : Text to Image: 6.136, 19.604, 30.056, 26.0
2019-02-16 21:40:45,778 : samples : 256000
2019-02-16 21:40:56,278 : Image to text: 8.42, 24.1, 35.28, 22.0
2019-02-16 21:41:03,905 : Text to Image: 6.444, 20.16, 30.516, 26.0
2019-02-16 21:41:46,442 : samples : 320000
2019-02-16 21:41:56,940 : Image to text: 8.64, 24.74, 36.68, 19.0
2019-02-16 21:42:04,547 : Text to Image: 6.792, 21.328, 32.332, 24.0
2019-02-16 21:42:47,592 : samples : 384000
2019-02-16 21:42:58,089 : Image to text: 9.82, 26.38, 38.26, 19.0
2019-02-16 21:43:05,731 : Text to Image: 7.484, 22.18, 33.48, 22.0
2019-02-16 21:43:48,770 : samples : 448000
2019-02-16 21:43:59,129 : Image to text: 9.24, 25.7, 37.7, 19.0
2019-02-16 21:44:06,790 : Text to Image: 7.424, 22.916, 34.212, 22.0
2019-02-16 21:44:49,431 : samples : 512000
2019-02-16 21:44:59,891 : Image to text: 9.12, 27.7, 39.92, 17.0
2019-02-16 21:45:07,568 : Text to Image: 7.904, 24.228, 35.356, 21.0
2019-02-16 21:45:43,704 : Epoch 1 finished
2019-02-16 21:45:44,134 : Image to text: 25.6, 53.7, 73.5, 5.0
2019-02-16 21:45:44,463 : Text to Image: 20.24, 51.58, 67.72, 5.0
2019-02-16 21:45:44,890 : Image to text: 24.0, 56.5, 71.3, 4.0
2019-02-16 21:45:45,248 : Text to Image: 19.58, 50.7, 67.94, 5.0
2019-02-16 21:45:45,672 : Image to text: 24.6, 55.8, 71.0, 4.0
2019-02-16 21:45:46,016 : Text to Image: 19.08, 50.2, 68.2, 5.0
2019-02-16 21:45:46,461 : Image to text: 24.7, 57.3, 72.3, 4.0
2019-02-16 21:45:46,807 : Text to Image: 20.76, 51.08, 68.52, 5.0
2019-02-16 21:45:47,255 : Image to text: 25.0, 56.8, 75.2, 4.0
2019-02-16 21:45:47,596 : Text to Image: 20.1, 51.56, 68.34, 5.0
2019-02-16 21:45:47,597 : Dev mean Text to Image: 19.951999999999998, 51.024, 68.144, 5.0
2019-02-16 21:45:47,597 : Dev mean Image to text: 24.78, 56.02, 72.66, 4.2
2019-02-16 21:45:47,597 : start epoch
2019-02-16 21:46:30,502 : samples : 64000
2019-02-16 21:46:40,974 : Image to text: 9.62, 27.34, 39.58, 17.0
2019-02-16 21:46:48,524 : Text to Image: 7.62, 23.536, 35.164, 21.0
2019-02-16 21:47:31,040 : samples : 128000
2019-02-16 21:47:41,611 : Image to text: 9.76, 27.28, 39.3, 17.0
2019-02-16 21:47:49,153 : Text to Image: 8.288, 24.536, 36.02, 20.0
2019-02-16 21:48:31,686 : samples : 192000
2019-02-16 21:48:42,216 : Image to text: 10.4, 27.48, 40.36, 16.0
2019-02-16 21:48:49,873 : Text to Image: 8.208, 24.436, 36.124, 20.0
2019-02-16 21:49:32,693 : samples : 256000
2019-02-16 21:49:43,141 : Image to text: 9.42, 27.7, 39.8, 16.0
2019-02-16 21:49:50,829 : Text to Image: 8.62, 25.28, 37.096, 20.0
2019-02-16 21:50:33,564 : samples : 320000
2019-02-16 21:50:44,121 : Image to text: 9.82, 28.16, 41.04, 16.0
2019-02-16 21:50:52,092 : Text to Image: 8.836, 25.316, 37.056, 19.0
2019-02-16 21:51:34,957 : samples : 384000
2019-02-16 21:51:45,437 : Image to text: 9.88, 27.64, 39.98, 16.0
2019-02-16 21:51:53,197 : Text to Image: 8.328, 24.604, 36.328, 20.0
2019-02-16 21:52:40,827 : samples : 448000
2019-02-16 21:52:53,242 : Image to text: 9.9, 27.74, 40.76, 16.0
2019-02-16 21:53:02,124 : Text to Image: 8.112, 24.796, 36.34, 19.0
2019-02-16 21:53:44,672 : samples : 512000
2019-02-16 21:53:55,122 : Image to text: 10.48, 29.38, 41.7, 16.0
2019-02-16 21:54:02,740 : Text to Image: 8.728, 25.364, 37.324, 19.0
2019-02-16 21:54:39,211 : Epoch 2 finished
2019-02-16 21:54:39,656 : Image to text: 26.3, 60.3, 76.1, 4.0
2019-02-16 21:54:39,995 : Text to Image: 22.2, 54.64, 72.22, 5.0
2019-02-16 21:54:40,435 : Image to text: 28.0, 61.6, 75.0, 3.0
2019-02-16 21:54:40,768 : Text to Image: 21.96, 53.38, 71.24, 5.0
2019-02-16 21:54:41,206 : Image to text: 26.4, 60.6, 74.2, 3.0
2019-02-16 21:54:41,542 : Text to Image: 21.52, 53.8, 71.36, 5.0
2019-02-16 21:54:42,000 : Image to text: 27.4, 60.7, 76.1, 4.0
2019-02-16 21:54:42,340 : Text to Image: 21.9, 54.44, 71.24, 5.0
2019-02-16 21:54:42,775 : Image to text: 27.9, 60.6, 74.9, 4.0
2019-02-16 21:54:43,107 : Text to Image: 21.52, 54.56, 71.42, 5.0
2019-02-16 21:54:43,108 : Dev mean Text to Image: 21.82, 54.163999999999994, 71.496, 5.0
2019-02-16 21:54:43,108 : Dev mean Image to text: 27.200000000000003, 60.760000000000005, 75.26, 3.5999999999999996
2019-02-16 21:54:43,108 : start epoch
2019-02-16 21:55:26,175 : samples : 64000
2019-02-16 21:55:36,685 : Image to text: 10.38, 29.1, 42.28, 16.0
2019-02-16 21:55:44,217 : Text to Image: 8.808, 25.68, 37.368, 19.0
2019-02-16 21:56:26,541 : samples : 128000
2019-02-16 21:56:36,987 : Image to text: 9.38, 28.3, 40.7, 16.0
2019-02-16 21:56:44,574 : Text to Image: 8.4, 25.004, 36.716, 19.0
2019-02-16 21:57:27,179 : samples : 192000
2019-02-16 21:57:37,781 : Image to text: 11.34, 30.88, 42.24, 15.0
2019-02-16 21:57:45,427 : Text to Image: 9.116, 25.868, 37.932, 18.0
2019-02-16 21:58:28,143 : samples : 256000
2019-02-16 21:58:38,666 : Image to text: 10.52, 29.48, 41.02, 16.0
2019-02-16 21:58:46,265 : Text to Image: 8.064, 24.544, 35.936, 20.0
2019-02-16 21:59:28,837 : samples : 320000
2019-02-16 21:59:39,314 : Image to text: 11.02, 29.96, 43.0, 14.0
2019-02-16 21:59:46,903 : Text to Image: 9.072, 26.476, 38.452, 18.0
2019-02-16 22:00:29,850 : samples : 384000
2019-02-16 22:00:40,349 : Image to text: 10.5, 29.02, 41.94, 15.0
2019-02-16 22:00:47,859 : Text to Image: 8.692, 26.088, 38.012, 18.0
2019-02-16 22:01:30,693 : samples : 448000
2019-02-16 22:01:41,217 : Image to text: 10.82, 29.76, 43.32, 14.0
2019-02-16 22:01:48,808 : Text to Image: 9.148, 25.996, 38.3, 18.0
2019-02-16 22:02:31,861 : samples : 512000
2019-02-16 22:02:42,438 : Image to text: 10.84, 29.64, 42.04, 15.0
2019-02-16 22:02:50,047 : Text to Image: 8.732, 25.4, 37.54, 19.0
2019-02-16 22:03:26,317 : Epoch 3 finished
2019-02-16 22:03:26,764 : Image to text: 26.7, 58.7, 75.7, 4.0
2019-02-16 22:03:27,108 : Text to Image: 21.74, 55.9, 73.1, 4.0
2019-02-16 22:03:27,559 : Image to text: 27.1, 59.5, 76.5, 4.0
2019-02-16 22:03:27,915 : Text to Image: 22.48, 54.94, 72.84, 5.0
2019-02-16 22:03:28,396 : Image to text: 26.9, 62.3, 75.4, 4.0
2019-02-16 22:03:28,742 : Text to Image: 22.74, 55.96, 73.34, 4.0
2019-02-16 22:03:29,191 : Image to text: 27.0, 61.1, 75.4, 4.0
2019-02-16 22:03:29,524 : Text to Image: 22.96, 55.84, 72.86, 4.0
2019-02-16 22:03:29,955 : Image to text: 28.5, 60.5, 76.2, 4.0
2019-02-16 22:03:30,289 : Text to Image: 23.52, 56.72, 73.02, 4.0
2019-02-16 22:03:30,289 : Dev mean Text to Image: 22.688000000000002, 55.872, 73.032, 4.2
2019-02-16 22:03:30,289 : Dev mean Image to text: 27.24, 60.42, 75.84, 4.0
2019-02-16 22:03:30,290 : start epoch
2019-02-16 22:04:13,360 : samples : 64000
2019-02-16 22:04:23,889 : Image to text: 11.24, 30.44, 42.36, 15.0
2019-02-16 22:04:31,451 : Text to Image: 8.972, 26.528, 38.712, 18.0
2019-02-16 22:05:14,485 : samples : 128000
2019-02-16 22:05:24,980 : Image to text: 11.34, 30.5, 42.96, 14.0
2019-02-16 22:05:32,531 : Text to Image: 9.308, 26.816, 38.868, 17.0
2019-02-16 22:06:15,392 : samples : 192000
2019-02-16 22:06:25,867 : Image to text: 10.98, 30.02, 41.98, 15.0
2019-02-16 22:06:33,492 : Text to Image: 8.684, 25.848, 37.684, 18.0
2019-02-16 22:07:16,026 : samples : 256000
2019-02-16 22:07:26,622 : Image to text: 10.68, 30.6, 42.2, 15.0
2019-02-16 22:07:34,196 : Text to Image: 9.176, 27.036, 39.332, 17.0
2019-02-16 22:08:16,979 : samples : 320000
2019-02-16 22:08:27,417 : Image to text: 11.0, 30.04, 43.46, 14.0
2019-02-16 22:08:35,054 : Text to Image: 9.312, 26.46, 38.82, 18.0
2019-02-16 22:09:18,059 : samples : 384000
2019-02-16 22:09:28,708 : Image to text: 11.0, 31.42, 43.86, 14.0
2019-02-16 22:09:37,246 : Text to Image: 9.276, 26.712, 38.92, 17.0
2019-02-16 22:10:27,410 : samples : 448000
2019-02-16 22:10:37,978 : Image to text: 11.44, 31.08, 44.24, 14.0
2019-02-16 22:10:45,703 : Text to Image: 9.624, 27.54, 39.728, 17.0
2019-02-16 22:11:28,611 : samples : 512000
2019-02-16 22:11:39,188 : Image to text: 12.06, 31.82, 44.66, 13.0
2019-02-16 22:11:46,971 : Text to Image: 9.176, 27.068, 39.112, 17.0
2019-02-16 22:12:23,823 : Epoch 4 finished
2019-02-16 22:12:24,263 : Image to text: 27.5, 62.9, 80.6, 3.0
2019-02-16 22:12:24,593 : Text to Image: 23.06, 57.24, 73.8, 4.0
2019-02-16 22:12:25,033 : Image to text: 28.5, 60.9, 75.5, 4.0
2019-02-16 22:12:25,362 : Text to Image: 22.86, 55.58, 73.26, 4.0
2019-02-16 22:12:25,789 : Image to text: 28.2, 62.9, 76.1, 3.0
2019-02-16 22:12:26,119 : Text to Image: 23.72, 57.2, 73.92, 4.0
2019-02-16 22:12:26,550 : Image to text: 30.3, 63.5, 76.8, 3.0
2019-02-16 22:12:26,883 : Text to Image: 23.4, 56.72, 72.74, 4.0
2019-02-16 22:12:27,329 : Image to text: 31.2, 64.9, 78.4, 3.0
2019-02-16 22:12:27,669 : Text to Image: 23.58, 56.84, 73.66, 4.0
2019-02-16 22:12:27,669 : Dev mean Text to Image: 23.323999999999998, 56.71600000000001, 73.476, 4.0
2019-02-16 22:12:27,669 : Dev mean Image to text: 29.14, 63.019999999999996, 77.48, 3.2
2019-02-16 22:12:27,669 : start epoch
2019-02-16 22:13:10,381 : samples : 64000
2019-02-16 22:13:20,957 : Image to text: 11.4, 30.26, 43.82, 14.0
2019-02-16 22:13:28,493 : Text to Image: 9.572, 27.296, 39.62, 17.0
2019-02-16 22:14:11,034 : samples : 128000
2019-02-16 22:14:21,548 : Image to text: 11.82, 31.5, 43.74, 14.0
2019-02-16 22:14:29,069 : Text to Image: 9.548, 27.496, 39.668, 17.0
2019-02-16 22:15:11,802 : samples : 192000
2019-02-16 22:15:22,412 : Image to text: 10.82, 30.4, 43.6, 14.0
2019-02-16 22:15:30,040 : Text to Image: 9.516, 27.14, 39.428, 17.0
2019-02-16 22:16:12,887 : samples : 256000
2019-02-16 22:16:23,403 : Image to text: 11.22, 30.76, 42.78, 15.0
2019-02-16 22:16:31,182 : Text to Image: 9.52, 27.116, 39.388, 17.0
2019-02-16 22:17:13,471 : samples : 320000
2019-02-16 22:17:23,974 : Image to text: 11.66, 31.42, 44.06, 14.0
2019-02-16 22:17:31,621 : Text to Image: 8.976, 26.816, 38.896, 17.0
2019-02-16 22:18:14,332 : samples : 384000
2019-02-16 22:18:24,825 : Image to text: 11.3, 31.6, 45.24, 13.0
2019-02-16 22:18:32,463 : Text to Image: 9.68, 27.676, 40.088, 17.0
2019-02-16 22:19:15,743 : samples : 448000
2019-02-16 22:19:26,291 : Image to text: 11.26, 31.14, 44.38, 13.0
2019-02-16 22:19:33,901 : Text to Image: 9.152, 27.044, 39.5, 17.0
2019-02-16 22:20:16,775 : samples : 512000
2019-02-16 22:20:27,290 : Image to text: 10.98, 30.4, 43.74, 14.0
2019-02-16 22:20:35,090 : Text to Image: 9.552, 27.076, 38.96, 17.0
2019-02-16 22:21:11,868 : Epoch 5 finished
2019-02-16 22:21:12,303 : Image to text: 28.8, 62.1, 78.3, 3.0
2019-02-16 22:21:12,625 : Text to Image: 23.68, 57.98, 74.36, 4.0
2019-02-16 22:21:13,058 : Image to text: 30.4, 61.9, 76.2, 3.0
2019-02-16 22:21:13,395 : Text to Image: 23.66, 56.94, 74.1, 4.0
2019-02-16 22:21:13,831 : Image to text: 30.4, 63.5, 76.4, 3.0
2019-02-16 22:21:14,169 : Text to Image: 24.06, 58.02, 74.94, 4.0
2019-02-16 22:21:14,615 : Image to text: 28.9, 64.7, 76.9, 3.0
2019-02-16 22:21:14,956 : Text to Image: 24.72, 58.26, 74.58, 4.0
2019-02-16 22:21:15,398 : Image to text: 31.3, 63.5, 77.2, 3.0
2019-02-16 22:21:15,740 : Text to Image: 24.36, 59.4, 75.0, 4.0
2019-02-16 22:21:15,740 : Dev mean Text to Image: 24.096, 58.120000000000005, 74.596, 4.0
2019-02-16 22:21:15,741 : Dev mean Image to text: 29.96, 63.14, 77.0, 3.0
2019-02-16 22:21:15,742 : start epoch
2019-02-16 22:21:58,370 : samples : 64000
2019-02-16 22:22:08,915 : Image to text: 12.26, 32.26, 45.1, 13.0
2019-02-16 22:22:16,531 : Text to Image: 9.612, 27.752, 39.9, 17.0
2019-02-16 22:22:58,718 : samples : 128000
2019-02-16 22:23:09,236 : Image to text: 11.76, 31.44, 44.82, 13.0
2019-02-16 22:23:16,776 : Text to Image: 9.772, 28.316, 40.416, 16.0
2019-02-16 22:24:00,579 : samples : 192000
2019-02-16 22:24:13,187 : Image to text: 11.54, 31.36, 44.16, 14.0
2019-02-16 22:24:23,360 : Text to Image: 9.316, 27.016, 39.432, 17.0
2019-02-16 22:25:09,285 : samples : 256000
2019-02-16 22:25:20,833 : Image to text: 11.82, 31.8, 45.38, 13.0
2019-02-16 22:25:28,297 : Text to Image: 9.844, 28.412, 40.68, 16.0
2019-02-16 22:26:11,071 : samples : 320000
2019-02-16 22:26:21,227 : Image to text: 10.84, 31.42, 44.16, 14.0
2019-02-16 22:26:28,195 : Text to Image: 9.524, 27.244, 39.764, 17.0
2019-02-16 22:27:22,547 : samples : 384000
2019-02-16 22:27:35,203 : Image to text: 12.42, 32.56, 45.68, 12.0
2019-02-16 22:27:45,289 : Text to Image: 10.192, 28.628, 41.448, 16.0
2019-02-16 22:28:28,624 : samples : 448000
2019-02-16 22:28:38,888 : Image to text: 11.58, 31.52, 44.74, 13.0
2019-02-16 22:28:46,101 : Text to Image: 10.004, 28.0, 40.22, 16.0
2019-02-16 22:29:29,213 : samples : 512000
2019-02-16 22:29:41,823 : Image to text: 11.62, 31.36, 44.28, 13.0
2019-02-16 22:29:51,896 : Text to Image: 9.496, 27.316, 39.676, 17.0
2019-02-16 22:30:31,086 : Epoch 6 finished
2019-02-16 22:30:31,985 : Image to text: 28.8, 63.6, 79.0, 3.0
2019-02-16 22:30:32,354 : Text to Image: 24.12, 59.54, 76.28, 4.0
2019-02-16 22:30:32,806 : Image to text: 29.8, 61.4, 77.2, 3.0
2019-02-16 22:30:33,170 : Text to Image: 24.88, 58.76, 74.98, 4.0
2019-02-16 22:30:33,624 : Image to text: 29.8, 62.7, 78.6, 3.0
2019-02-16 22:30:33,988 : Text to Image: 25.6, 59.86, 76.36, 4.0
2019-02-16 22:30:34,439 : Image to text: 31.3, 65.3, 77.6, 3.0
2019-02-16 22:30:34,804 : Text to Image: 25.72, 59.68, 76.08, 4.0
2019-02-16 22:30:35,255 : Image to text: 31.1, 64.5, 77.5, 3.0
2019-02-16 22:30:35,620 : Text to Image: 25.34, 59.78, 75.78, 4.0
2019-02-16 22:30:35,620 : Dev mean Text to Image: 25.131999999999998, 59.524, 75.896, 4.0
2019-02-16 22:30:35,620 : Dev mean Image to text: 30.159999999999997, 63.49999999999999, 77.98, 3.0
2019-02-16 22:30:35,621 : start epoch
2019-02-16 22:31:18,110 : samples : 64000
2019-02-16 22:31:28,629 : Image to text: 11.66, 32.3, 45.14, 13.0
2019-02-16 22:31:38,599 : Text to Image: 10.116, 28.224, 40.776, 16.0
2019-02-16 22:32:23,571 : samples : 128000
2019-02-16 22:32:36,214 : Image to text: 12.1, 32.02, 45.52, 13.0
2019-02-16 22:32:45,756 : Text to Image: 10.072, 28.692, 41.416, 16.0
2019-02-16 22:33:28,859 : samples : 192000
2019-02-16 22:33:39,133 : Image to text: 12.1, 32.0, 44.86, 13.0
2019-02-16 22:33:46,372 : Text to Image: 9.868, 28.372, 41.028, 16.0
2019-02-16 22:34:31,155 : samples : 256000
2019-02-16 22:34:43,764 : Image to text: 11.58, 31.92, 45.3, 13.0
2019-02-16 22:34:53,811 : Text to Image: 9.776, 28.852, 41.1, 16.0
2019-02-16 22:35:37,412 : samples : 320000
2019-02-16 22:35:47,687 : Image to text: 11.56, 30.98, 44.26, 13.0
2019-02-16 22:35:55,099 : Text to Image: 9.992, 28.28, 40.936, 16.0
2019-02-16 22:36:39,096 : samples : 384000
2019-02-16 22:36:51,696 : Image to text: 11.54, 32.02, 45.58, 13.0
2019-02-16 22:37:01,778 : Text to Image: 9.9, 28.876, 41.552, 16.0
2019-02-16 22:37:46,661 : samples : 448000
2019-02-16 22:37:56,996 : Image to text: 11.8, 32.9, 46.26, 13.0
2019-02-16 22:38:04,412 : Text to Image: 10.324, 29.216, 41.756, 15.0
2019-02-16 22:38:47,905 : samples : 512000
2019-02-16 22:39:00,451 : Image to text: 12.3, 32.08, 45.38, 13.0
2019-02-16 22:39:10,466 : Text to Image: 10.244, 29.152, 41.296, 15.0
2019-02-16 22:39:49,587 : Epoch 7 finished
2019-02-16 22:39:50,502 : Image to text: 28.2, 62.1, 78.8, 4.0
2019-02-16 22:39:51,292 : Text to Image: 24.64, 59.56, 76.26, 4.0
2019-02-16 22:39:52,262 : Image to text: 29.5, 63.6, 77.6, 3.0
2019-02-16 22:39:53,033 : Text to Image: 24.66, 59.0, 76.16, 4.0
2019-02-16 22:39:54,015 : Image to text: 29.1, 64.4, 78.2, 3.0
2019-02-16 22:39:54,808 : Text to Image: 25.66, 59.72, 76.3, 4.0
2019-02-16 22:39:55,731 : Image to text: 31.2, 63.8, 79.2, 3.0
2019-02-16 22:39:56,449 : Text to Image: 25.44, 59.4, 76.6, 4.0
2019-02-16 22:39:57,402 : Image to text: 29.4, 63.7, 78.4, 3.0
2019-02-16 22:39:58,171 : Text to Image: 25.46, 60.18, 75.62, 4.0
2019-02-16 22:39:58,171 : Dev mean Text to Image: 25.171999999999997, 59.572, 76.188, 4.0
2019-02-16 22:39:58,171 : Dev mean Image to text: 29.48, 63.52, 78.44000000000001, 3.2
2019-02-16 22:39:58,171 : start epoch
2019-02-16 22:40:41,010 : samples : 64000
2019-02-16 22:40:51,284 : Image to text: 12.62, 32.48, 45.68, 13.0
2019-02-16 22:40:58,407 : Text to Image: 9.96, 28.464, 40.968, 16.0
2019-02-16 22:41:43,342 : samples : 128000
2019-02-16 22:41:55,939 : Image to text: 11.8, 32.36, 45.18, 13.0
2019-02-16 22:42:06,053 : Text to Image: 9.916, 28.548, 41.048, 16.0
2019-02-16 22:42:49,953 : samples : 192000
2019-02-16 22:43:00,255 : Image to text: 12.36, 32.32, 45.42, 13.0
2019-02-16 22:43:07,630 : Text to Image: 9.824, 28.9, 41.328, 16.0
2019-02-16 22:43:52,465 : samples : 256000
2019-02-16 22:44:05,737 : Image to text: 11.8, 32.62, 45.56, 13.0
2019-02-16 22:44:16,258 : Text to Image: 10.18, 28.788, 41.68, 15.0
2019-02-16 22:45:04,820 : samples : 320000
2019-02-16 22:45:15,151 : Image to text: 11.86, 32.3, 46.22, 12.0
2019-02-16 22:45:22,567 : Text to Image: 10.252, 29.26, 42.12, 15.0
2019-02-16 22:46:05,487 : samples : 384000
2019-02-16 22:46:15,771 : Image to text: 11.76, 32.12, 45.62, 13.0
2019-02-16 22:46:25,751 : Text to Image: 10.224, 28.864, 41.348, 16.0
2019-02-16 22:47:11,057 : samples : 448000
2019-02-16 22:47:23,675 : Image to text: 12.12, 32.24, 46.0, 12.0
2019-02-16 22:47:33,757 : Text to Image: 10.124, 28.592, 41.116, 16.0
2019-02-16 22:48:17,367 : samples : 512000
2019-02-16 22:48:27,687 : Image to text: 11.92, 32.62, 46.42, 12.0
2019-02-16 22:48:35,127 : Text to Image: 10.168, 28.052, 41.036, 16.0
2019-02-16 22:49:11,891 : Epoch 8 finished
2019-02-16 22:49:12,271 : Image to text: 32.3, 63.7, 80.3, 3.0
2019-02-16 22:49:12,550 : Text to Image: 24.12, 59.12, 75.2, 4.0
2019-02-16 22:49:13,282 : Image to text: 29.7, 64.3, 79.0, 3.0
2019-02-16 22:49:14,128 : Text to Image: 24.26, 58.14, 74.74, 4.0
2019-02-16 22:49:15,060 : Image to text: 29.4, 64.3, 79.5, 3.0
2019-02-16 22:49:15,842 : Text to Image: 25.44, 59.66, 75.9, 4.0
2019-02-16 22:49:16,786 : Image to text: 32.7, 63.6, 78.2, 3.0
2019-02-16 22:49:17,593 : Text to Image: 24.64, 58.44, 75.04, 4.0
2019-02-16 22:49:18,488 : Image to text: 32.0, 65.4, 77.7, 3.0
2019-02-16 22:49:18,767 : Text to Image: 25.36, 58.76, 75.28, 4.0
2019-02-16 22:49:18,768 : Dev mean Text to Image: 24.764, 58.824, 75.232, 4.0
2019-02-16 22:49:18,768 : Dev mean Image to text: 31.22, 64.26, 78.94, 3.0
2019-02-16 22:49:18,768 : start epoch
2019-02-16 22:50:02,711 : samples : 64000
2019-02-16 22:50:12,976 : Image to text: 11.92, 32.38, 45.84, 13.0
2019-02-16 22:50:20,458 : Text to Image: 10.16, 28.696, 40.856, 16.0
2019-02-16 22:51:03,548 : samples : 128000
2019-02-16 22:51:13,794 : Image to text: 12.26, 32.88, 46.72, 12.0
2019-02-16 22:51:20,935 : Text to Image: 9.664, 27.548, 40.012, 17.0
2019-02-16 22:52:04,058 : samples : 192000
2019-02-16 22:52:14,469 : Image to text: 11.94, 33.26, 46.88, 12.0
2019-02-16 22:52:21,899 : Text to Image: 10.212, 28.752, 41.356, 16.0
2019-02-16 22:53:04,668 : samples : 256000
2019-02-16 22:53:14,994 : Image to text: 12.16, 32.02, 45.42, 13.0
2019-02-16 22:53:22,488 : Text to Image: 10.016, 28.656, 41.604, 15.0
2019-02-16 22:54:06,233 : samples : 320000
2019-02-16 22:54:16,552 : Image to text: 12.0, 32.58, 46.08, 13.0
2019-02-16 22:54:24,009 : Text to Image: 10.22, 28.788, 41.192, 15.0
2019-02-16 22:55:06,917 : samples : 384000
2019-02-16 22:55:18,619 : Image to text: 11.74, 32.24, 45.26, 13.0
2019-02-16 22:55:28,804 : Text to Image: 10.32, 29.272, 41.952, 15.0
2019-02-16 22:56:13,913 : samples : 448000
2019-02-16 22:56:24,198 : Image to text: 12.62, 32.98, 46.26, 12.0
2019-02-16 22:56:31,663 : Text to Image: 10.228, 28.896, 41.452, 16.0
2019-02-16 22:57:15,406 : samples : 512000
2019-02-16 22:57:26,882 : Image to text: 12.12, 32.22, 45.5, 13.0
2019-02-16 22:57:36,420 : Text to Image: 10.132, 29.004, 41.688, 15.0
2019-02-16 22:58:13,911 : Epoch 9 finished
2019-02-16 22:58:14,867 : Image to text: 30.0, 64.7, 79.6, 3.0
2019-02-16 22:58:15,671 : Text to Image: 25.52, 60.22, 76.68, 4.0
2019-02-16 22:58:16,428 : Image to text: 30.7, 64.5, 78.6, 3.0
2019-02-16 22:58:16,793 : Text to Image: 25.6, 58.96, 76.28, 4.0
2019-02-16 22:58:17,243 : Image to text: 30.8, 64.5, 79.2, 3.0
2019-02-16 22:58:17,606 : Text to Image: 25.74, 60.6, 77.18, 4.0
2019-02-16 22:58:18,057 : Image to text: 30.4, 64.6, 79.2, 3.0
2019-02-16 22:58:18,447 : Text to Image: 25.66, 59.36, 76.24, 4.0
2019-02-16 22:58:18,914 : Image to text: 33.1, 64.1, 78.4, 3.0
2019-02-16 22:58:19,293 : Text to Image: 26.6, 60.3, 76.24, 4.0
2019-02-16 22:58:19,293 : Dev mean Text to Image: 25.823999999999998, 59.888000000000005, 76.524, 4.0
2019-02-16 22:58:19,293 : Dev mean Image to text: 31.000000000000004, 64.47999999999999, 79.0, 3.0
2019-02-16 22:58:19,293 : start epoch
2019-02-16 22:59:02,798 : samples : 64000
2019-02-16 22:59:13,163 : Image to text: 11.76, 31.92, 45.58, 13.0
2019-02-16 22:59:20,635 : Text to Image: 10.172, 28.896, 41.38, 16.0
2019-02-16 23:00:05,343 : samples : 128000
2019-02-16 23:00:17,948 : Image to text: 12.04, 31.9, 45.82, 13.0
2019-02-16 23:00:27,881 : Text to Image: 10.464, 29.916, 42.236, 15.0
2019-02-16 23:01:17,055 : samples : 192000
2019-02-16 23:01:30,131 : Image to text: 11.94, 33.38, 46.68, 12.0
2019-02-16 23:01:40,363 : Text to Image: 10.492, 29.4, 42.176, 15.0
2019-02-16 23:02:25,617 : samples : 256000
2019-02-16 23:02:38,255 : Image to text: 11.74, 32.5, 46.22, 12.0
2019-02-16 23:02:48,263 : Text to Image: 10.32, 29.036, 41.652, 15.0
2019-02-16 23:03:33,368 : samples : 320000
2019-02-16 23:03:45,961 : Image to text: 12.06, 32.24, 46.56, 13.0
2019-02-16 23:03:55,983 : Text to Image: 10.196, 28.976, 41.428, 15.0
2019-02-16 23:04:41,384 : samples : 384000
2019-02-16 23:04:53,972 : Image to text: 12.88, 33.9, 47.08, 12.0
2019-02-16 23:05:03,985 : Text to Image: 10.548, 29.32, 41.988, 15.0
2019-02-16 23:05:48,693 : samples : 448000
2019-02-16 23:06:01,345 : Image to text: 12.54, 33.6, 46.2, 13.0
2019-02-16 23:06:11,378 : Text to Image: 10.604, 29.432, 42.244, 15.0
2019-02-16 23:06:55,906 : samples : 512000
2019-02-16 23:07:08,533 : Image to text: 12.36, 32.52, 45.78, 12.0
2019-02-16 23:07:18,617 : Text to Image: 10.328, 29.292, 42.392, 15.0
2019-02-16 23:07:56,551 : Epoch 10 finished
2019-02-16 23:07:57,460 : Image to text: 30.0, 62.9, 80.1, 3.0
2019-02-16 23:07:58,253 : Text to Image: 25.88, 60.96, 77.46, 4.0
2019-02-16 23:07:59,237 : Image to text: 31.1, 64.4, 78.8, 3.0
2019-02-16 23:08:00,012 : Text to Image: 24.9, 59.6, 76.72, 4.0
2019-02-16 23:08:00,913 : Image to text: 29.8, 63.7, 79.2, 3.0
2019-02-16 23:08:01,726 : Text to Image: 26.44, 61.04, 77.38, 4.0
2019-02-16 23:08:02,649 : Image to text: 32.1, 64.8, 79.2, 3.0
2019-02-16 23:08:03,447 : Text to Image: 25.84, 60.18, 76.9, 4.0
2019-02-16 23:08:04,430 : Image to text: 31.7, 64.8, 78.8, 3.0
2019-02-16 23:08:05,168 : Text to Image: 26.42, 60.62, 76.52, 4.0
2019-02-16 23:08:05,168 : Dev mean Text to Image: 25.896, 60.480000000000004, 76.996, 4.0
2019-02-16 23:08:05,168 : Dev mean Image to text: 30.94, 64.12, 79.22000000000001, 3.0
2019-02-16 23:08:05,168 : start epoch
2019-02-16 23:08:49,879 : samples : 64000
2019-02-16 23:09:02,457 : Image to text: 12.12, 31.98, 46.1, 13.0
2019-02-16 23:09:12,464 : Text to Image: 10.22, 29.16, 42.128, 15.0
2019-02-16 23:09:57,666 : samples : 128000
2019-02-16 23:10:10,346 : Image to text: 12.6, 32.9, 46.38, 12.0
2019-02-16 23:10:20,384 : Text to Image: 10.188, 29.032, 41.812, 15.0
2019-02-16 23:11:05,908 : samples : 192000
2019-02-16 23:11:17,159 : Image to text: 11.92, 32.78, 46.22, 12.0
2019-02-16 23:11:24,526 : Text to Image: 10.268, 29.12, 41.616, 15.0
2019-02-16 23:12:07,235 : samples : 256000
2019-02-16 23:12:17,497 : Image to text: 12.5, 33.94, 46.8, 12.0
2019-02-16 23:12:24,862 : Text to Image: 10.916, 29.884, 42.704, 15.0
2019-02-16 23:13:08,563 : samples : 320000
2019-02-16 23:13:21,508 : Image to text: 12.42, 33.22, 46.82, 12.0
2019-02-16 23:13:31,901 : Text to Image: 10.8, 29.62, 42.32, 15.0
2019-02-16 23:14:17,345 : samples : 384000
2019-02-16 23:14:30,216 : Image to text: 12.4, 33.68, 46.5, 12.0
2019-02-16 23:14:40,540 : Text to Image: 10.664, 29.784, 42.528, 15.0
2019-02-16 23:15:26,761 : samples : 448000
2019-02-16 23:15:39,595 : Image to text: 11.78, 32.72, 46.9, 12.0
2019-02-16 23:15:50,033 : Text to Image: 10.364, 29.444, 42.128, 15.0
2019-02-16 23:16:36,479 : samples : 512000
2019-02-16 23:16:49,431 : Image to text: 12.8, 33.64, 47.1, 12.0
2019-02-16 23:16:59,945 : Text to Image: 10.588, 29.872, 42.508, 15.0
2019-02-16 23:17:39,404 : Epoch 11 finished
2019-02-16 23:17:40,466 : Image to text: 28.4, 64.6, 78.0, 3.0
2019-02-16 23:17:41,395 : Text to Image: 24.8, 59.88, 76.5, 4.0
2019-02-16 23:17:42,531 : Image to text: 30.1, 63.8, 79.1, 3.0
2019-02-16 23:17:43,434 : Text to Image: 24.78, 59.04, 75.9, 4.0
2019-02-16 23:17:44,492 : Image to text: 30.6, 64.7, 77.8, 3.0
2019-02-16 23:17:45,336 : Text to Image: 25.9, 60.5, 76.6, 4.0
2019-02-16 23:17:46,367 : Image to text: 30.4, 64.7, 78.5, 3.0
2019-02-16 23:17:47,253 : Text to Image: 25.24, 59.54, 76.48, 4.0
2019-02-16 23:17:48,317 : Image to text: 31.8, 63.0, 78.5, 3.0
2019-02-16 23:17:49,205 : Text to Image: 26.08, 59.96, 76.18, 4.0
2019-02-16 23:17:49,206 : Dev mean Text to Image: 25.36, 59.784000000000006, 76.33200000000001, 4.0
2019-02-16 23:17:49,206 : Dev mean Image to text: 30.259999999999998, 64.16, 78.38, 3.0
2019-02-16 23:17:49,206 : start epoch
2019-02-16 23:18:42,914 : samples : 64000
2019-02-16 23:18:55,930 : Image to text: 12.78, 34.1, 47.22, 12.0
2019-02-16 23:19:06,248 : Text to Image: 10.5, 29.548, 41.944, 15.0
2019-02-16 23:19:52,578 : samples : 128000
2019-02-16 23:20:05,475 : Image to text: 12.04, 32.9, 46.38, 12.0
2019-02-16 23:20:15,962 : Text to Image: 10.352, 29.14, 41.66, 15.0
2019-02-16 23:21:01,914 : samples : 192000
2019-02-16 23:21:12,492 : Image to text: 12.1, 33.48, 47.22, 12.0
2019-02-16 23:21:20,023 : Text to Image: 10.712, 29.66, 42.42, 15.0
2019-02-16 23:22:03,375 : samples : 256000
2019-02-16 23:22:13,886 : Image to text: 12.14, 33.18, 46.86, 12.0
2019-02-16 23:22:21,457 : Text to Image: 10.32, 29.572, 42.332, 15.0
2019-02-16 23:23:03,981 : samples : 320000
2019-02-16 23:23:14,487 : Image to text: 13.14, 32.9, 46.58, 12.0
2019-02-16 23:23:22,080 : Text to Image: 10.38, 29.2, 41.952, 15.0
2019-02-16 23:24:04,866 : samples : 384000
2019-02-16 23:24:15,409 : Image to text: 11.96, 33.08, 46.18, 12.0
2019-02-16 23:24:22,962 : Text to Image: 10.392, 29.176, 41.888, 15.0
2019-02-16 23:25:05,798 : samples : 448000
2019-02-16 23:25:16,293 : Image to text: 12.5, 33.5, 47.26, 12.0
2019-02-16 23:25:23,856 : Text to Image: 10.908, 29.788, 42.8, 15.0
2019-02-16 23:26:06,726 : samples : 512000
2019-02-16 23:26:17,174 : Image to text: 12.68, 33.3, 46.46, 12.0
2019-02-16 23:26:24,742 : Text to Image: 10.776, 30.116, 42.912, 14.0
2019-02-16 23:27:01,718 : Epoch 12 finished
2019-02-16 23:27:02,171 : Image to text: 28.8, 64.2, 79.5, 3.0
2019-02-16 23:27:02,505 : Text to Image: 25.08, 61.06, 77.26, 4.0
2019-02-16 23:27:02,950 : Image to text: 29.7, 63.5, 78.4, 3.0
2019-02-16 23:27:03,287 : Text to Image: 26.04, 60.52, 76.7, 4.0
2019-02-16 23:27:03,731 : Image to text: 29.8, 63.5, 78.4, 3.0
2019-02-16 23:27:04,061 : Text to Image: 26.78, 61.16, 77.34, 4.0
2019-02-16 23:27:04,490 : Image to text: 30.6, 64.7, 78.5, 3.0
2019-02-16 23:27:04,807 : Text to Image: 26.26, 60.9, 77.44, 4.0
2019-02-16 23:27:05,233 : Image to text: 32.0, 64.1, 78.7, 3.0
2019-02-16 23:27:05,549 : Text to Image: 26.32, 60.76, 77.02, 4.0
2019-02-16 23:27:05,550 : Dev mean Text to Image: 26.096, 60.88, 77.152, 4.0
2019-02-16 23:27:05,550 : Dev mean Image to text: 30.18, 63.99999999999999, 78.7, 3.0
2019-02-16 23:27:05,550 : start epoch
2019-02-16 23:27:48,852 : samples : 64000
2019-02-16 23:27:59,142 : Image to text: 12.58, 33.48, 47.38, 12.0
2019-02-16 23:28:06,624 : Text to Image: 10.788, 29.872, 42.64, 15.0
2019-02-16 23:28:49,482 : samples : 128000
2019-02-16 23:28:59,789 : Image to text: 12.34, 33.84, 46.7, 12.0
2019-02-16 23:29:07,270 : Text to Image: 10.312, 29.248, 42.004, 15.0
2019-02-16 23:29:49,883 : samples : 192000
2019-02-16 23:30:00,412 : Image to text: 12.66, 33.14, 47.48, 12.0
2019-02-16 23:30:07,964 : Text to Image: 10.512, 29.352, 42.396, 15.0
2019-02-16 23:30:51,050 : samples : 256000
2019-02-16 23:31:01,544 : Image to text: 13.0, 35.34, 48.12, 12.0
2019-02-16 23:31:09,193 : Text to Image: 10.692, 29.828, 42.696, 15.0
2019-02-16 23:31:52,514 : samples : 320000
2019-02-16 23:32:03,006 : Image to text: 12.38, 33.22, 46.98, 12.0
2019-02-16 23:32:10,571 : Text to Image: 10.732, 29.912, 42.644, 15.0
2019-02-16 23:32:53,276 : samples : 384000
2019-02-16 23:33:03,736 : Image to text: 12.24, 33.74, 47.22, 12.0
2019-02-16 23:33:11,317 : Text to Image: 10.468, 29.508, 42.424, 15.0
2019-02-16 23:33:54,929 : samples : 448000
2019-02-16 23:34:05,456 : Image to text: 13.28, 33.84, 47.18, 12.0
2019-02-16 23:34:13,041 : Text to Image: 10.72, 29.64, 42.204, 15.0
2019-02-16 23:34:55,919 : samples : 512000
2019-02-16 23:35:06,292 : Image to text: 13.04, 33.68, 47.62, 12.0
2019-02-16 23:35:13,702 : Text to Image: 10.56, 29.868, 42.448, 15.0
2019-02-16 23:36:01,980 : Epoch 13 finished
2019-02-16 23:36:02,521 : Image to text: 30.5, 63.6, 80.6, 3.0
2019-02-16 23:36:02,946 : Text to Image: 25.4, 61.7, 78.12, 4.0
2019-02-16 23:36:03,525 : Image to text: 30.5, 62.4, 78.0, 3.0
2019-02-16 23:36:03,966 : Text to Image: 26.08, 59.86, 77.22, 4.0
2019-02-16 23:36:04,528 : Image to text: 29.3, 65.1, 79.5, 3.0
2019-02-16 23:36:04,967 : Text to Image: 26.98, 61.34, 77.26, 4.0
2019-02-16 23:36:05,477 : Image to text: 31.5, 65.4, 78.3, 3.0
2019-02-16 23:36:05,856 : Text to Image: 26.5, 61.14, 77.72, 4.0
2019-02-16 23:36:06,368 : Image to text: 31.5, 64.9, 78.0, 3.0
2019-02-16 23:36:06,709 : Text to Image: 26.26, 61.22, 76.96, 4.0
2019-02-16 23:36:06,709 : Dev mean Text to Image: 26.244, 61.052, 77.45599999999999, 4.0
2019-02-16 23:36:06,709 : Dev mean Image to text: 30.66, 64.28, 78.88, 3.0
2019-02-16 23:36:06,710 : start epoch
2019-02-16 23:36:50,210 : samples : 64000
2019-02-16 23:37:00,544 : Image to text: 12.48, 34.14, 47.46, 12.0
2019-02-16 23:37:08,027 : Text to Image: 10.836, 30.144, 42.896, 15.0
2019-02-16 23:37:51,310 : samples : 128000
2019-02-16 23:38:01,562 : Image to text: 12.58, 33.52, 47.32, 12.0
2019-02-16 23:38:09,054 : Text to Image: 10.476, 29.656, 42.488, 15.0
2019-02-16 23:38:52,510 : samples : 192000
2019-02-16 23:39:03,098 : Image to text: 12.18, 33.88, 47.16, 12.0
2019-02-16 23:39:10,651 : Text to Image: 10.688, 29.488, 42.156, 15.0
2019-02-16 23:39:54,473 : samples : 256000
2019-02-16 23:40:05,024 : Image to text: 12.74, 33.96, 48.1, 11.0
2019-02-16 23:40:12,712 : Text to Image: 10.748, 30.076, 42.764, 15.0
2019-02-16 23:40:55,873 : samples : 320000
2019-02-16 23:41:06,375 : Image to text: 13.08, 33.68, 47.62, 12.0
2019-02-16 23:41:14,039 : Text to Image: 10.568, 29.924, 42.628, 15.0
2019-02-16 23:41:57,023 : samples : 384000
2019-02-16 23:42:07,619 : Image to text: 12.36, 33.76, 47.26, 12.0
2019-02-16 23:42:15,175 : Text to Image: 10.532, 29.78, 42.772, 15.0
2019-02-16 23:42:58,198 : samples : 448000
2019-02-16 23:43:08,754 : Image to text: 12.44, 34.0, 47.32, 12.0
2019-02-16 23:43:16,370 : Text to Image: 10.784, 29.764, 42.304, 15.0
2019-02-16 23:43:58,759 : samples : 512000
2019-02-16 23:44:09,270 : Image to text: 13.0, 34.68, 47.42, 12.0
2019-02-16 23:44:16,828 : Text to Image: 11.132, 30.2, 43.264, 15.0
2019-02-16 23:44:53,345 : Epoch 14 finished
2019-02-16 23:44:53,799 : Image to text: 29.1, 64.3, 78.8, 3.0
2019-02-16 23:44:54,129 : Text to Image: 25.48, 60.9, 77.02, 4.0
2019-02-16 23:44:54,592 : Image to text: 31.6, 63.6, 77.5, 3.0
2019-02-16 23:44:54,939 : Text to Image: 25.68, 59.5, 76.78, 4.0
2019-02-16 23:44:55,416 : Image to text: 30.0, 62.3, 79.4, 3.0
2019-02-16 23:44:55,759 : Text to Image: 26.84, 61.16, 77.12, 4.0
2019-02-16 23:44:56,232 : Image to text: 31.9, 63.4, 77.7, 3.0
2019-02-16 23:44:56,569 : Text to Image: 25.9, 60.82, 76.9, 4.0
2019-02-16 23:44:57,021 : Image to text: 32.3, 64.4, 78.5, 3.0
2019-02-16 23:44:57,366 : Text to Image: 26.22, 61.44, 76.54, 4.0
2019-02-16 23:44:57,366 : Dev mean Text to Image: 26.024, 60.763999999999996, 76.872, 4.0
2019-02-16 23:44:57,367 : Dev mean Image to text: 30.979999999999997, 63.6, 78.38, 3.0
2019-02-16 23:44:57,367 : start epoch
2019-02-16 23:45:39,490 : samples : 64000
2019-02-16 23:45:49,711 : Image to text: 12.5, 33.54, 47.76, 12.0
2019-02-16 23:45:57,242 : Text to Image: 10.688, 29.612, 42.34, 15.0
2019-02-16 23:46:40,200 : samples : 128000
2019-02-16 23:46:50,447 : Image to text: 12.2, 33.18, 45.94, 13.0
2019-02-16 23:46:57,995 : Text to Image: 10.38, 29.324, 41.996, 15.0
2019-02-16 23:47:41,631 : samples : 192000
2019-02-16 23:47:52,196 : Image to text: 12.52, 33.96, 47.68, 12.0
2019-02-16 23:47:59,758 : Text to Image: 10.532, 29.788, 42.536, 15.0
2019-02-16 23:48:42,909 : samples : 256000
2019-02-16 23:48:53,411 : Image to text: 12.58, 33.36, 47.84, 12.0
2019-02-16 23:49:00,987 : Text to Image: 10.84, 30.08, 43.112, 14.0
2019-02-16 23:49:43,661 : samples : 320000
2019-02-16 23:49:54,218 : Image to text: 12.2, 33.28, 47.1, 12.0
2019-02-16 23:50:01,785 : Text to Image: 10.848, 30.096, 43.152, 14.0
2019-02-16 23:50:44,534 : samples : 384000
2019-02-16 23:50:55,068 : Image to text: 12.26, 33.36, 46.88, 12.0
2019-02-16 23:51:02,594 : Text to Image: 10.508, 29.932, 42.78, 15.0
2019-02-16 23:51:45,875 : samples : 448000
2019-02-16 23:51:56,414 : Image to text: 12.22, 33.38, 46.94, 12.0
2019-02-16 23:52:03,967 : Text to Image: 10.728, 30.172, 42.784, 15.0
2019-02-16 23:52:50,214 : samples : 512000
2019-02-16 23:53:02,715 : Image to text: 12.9, 34.64, 48.12, 11.0
2019-02-16 23:53:11,207 : Text to Image: 10.888, 30.136, 42.552, 15.0
2019-02-16 23:53:49,679 : Epoch 15 finished
2019-02-16 23:53:50,129 : Image to text: 32.1, 65.8, 81.5, 3.0
2019-02-16 23:53:50,467 : Text to Image: 25.32, 61.04, 77.44, 4.0
2019-02-16 23:53:50,910 : Image to text: 31.9, 66.0, 79.6, 3.0
2019-02-16 23:53:51,242 : Text to Image: 25.74, 59.74, 76.44, 4.0
2019-02-16 23:53:51,706 : Image to text: 32.4, 66.1, 79.4, 3.0
2019-02-16 23:53:52,059 : Text to Image: 26.58, 61.0, 77.46, 4.0
2019-02-16 23:53:52,530 : Image to text: 31.8, 65.9, 79.7, 3.0
2019-02-16 23:53:52,879 : Text to Image: 25.94, 60.56, 76.86, 4.0
2019-02-16 23:53:53,333 : Image to text: 33.5, 66.4, 80.2, 3.0
2019-02-16 23:53:53,677 : Text to Image: 26.34, 60.92, 77.04, 4.0
2019-02-16 23:53:53,677 : Dev mean Text to Image: 25.984, 60.652, 77.048, 4.0
2019-02-16 23:53:53,677 : Dev mean Image to text: 32.34, 66.03999999999999, 80.08000000000001, 3.0
2019-02-16 23:53:53,678 : start epoch
2019-02-16 23:54:37,154 : samples : 64000
2019-02-16 23:54:47,434 : Image to text: 12.48, 33.8, 47.14, 12.0
2019-02-16 23:54:54,920 : Text to Image: 10.708, 29.656, 42.488, 15.0
2019-02-16 23:55:37,164 : samples : 128000
2019-02-16 23:55:47,496 : Image to text: 12.86, 33.44, 47.02, 12.0
2019-02-16 23:55:55,008 : Text to Image: 10.736, 29.632, 42.592, 15.0
2019-02-16 23:56:38,741 : samples : 192000
2019-02-16 23:56:49,281 : Image to text: 12.9, 34.34, 48.4, 11.0
2019-02-16 23:56:56,856 : Text to Image: 11.004, 30.108, 42.852, 14.0
2019-02-16 23:57:39,675 : samples : 256000
2019-02-16 23:57:50,254 : Image to text: 12.24, 33.82, 47.86, 11.0
2019-02-16 23:57:57,768 : Text to Image: 10.928, 30.352, 43.456, 14.0
2019-02-16 23:58:41,228 : samples : 320000
2019-02-16 23:58:51,787 : Image to text: 13.44, 34.98, 47.78, 12.0
2019-02-16 23:58:59,325 : Text to Image: 10.832, 30.292, 43.192, 14.0
2019-02-16 23:59:42,535 : samples : 384000
2019-02-16 23:59:53,101 : Image to text: 13.02, 34.52, 47.8, 12.0
2019-02-17 00:00:00,685 : Text to Image: 10.956, 30.064, 42.972, 15.0
2019-02-17 00:00:43,214 : samples : 448000
2019-02-17 00:00:53,711 : Image to text: 12.8, 34.1, 47.8, 12.0
2019-02-17 00:01:01,274 : Text to Image: 10.892, 29.96, 43.264, 14.0
2019-02-17 00:01:44,289 : samples : 512000
2019-02-17 00:01:54,883 : Image to text: 13.0, 34.64, 48.18, 11.0
2019-02-17 00:02:02,475 : Text to Image: 11.08, 30.408, 43.376, 14.0
2019-02-17 00:02:39,497 : Epoch 16 finished
2019-02-17 00:02:39,960 : Image to text: 31.2, 65.6, 81.7, 3.0
2019-02-17 00:02:40,294 : Text to Image: 25.06, 62.14, 77.86, 4.0
2019-02-17 00:02:40,729 : Image to text: 33.0, 66.4, 79.1, 3.0
2019-02-17 00:02:41,061 : Text to Image: 25.5, 60.5, 77.46, 4.0
2019-02-17 00:02:41,495 : Image to text: 30.5, 66.8, 80.8, 3.0
2019-02-17 00:02:41,827 : Text to Image: 26.82, 61.86, 77.02, 4.0
2019-02-17 00:02:42,271 : Image to text: 32.1, 66.4, 79.9, 3.0
2019-02-17 00:02:42,618 : Text to Image: 26.82, 62.0, 77.8, 3.0
2019-02-17 00:02:43,091 : Image to text: 33.7, 65.9, 80.6, 3.0
2019-02-17 00:02:43,442 : Text to Image: 26.28, 60.94, 77.6, 4.0
2019-02-17 00:02:43,442 : Dev mean Text to Image: 26.096, 61.488, 77.548, 3.8000000000000007
2019-02-17 00:02:43,442 : Dev mean Image to text: 32.1, 66.22, 80.41999999999999, 3.0
2019-02-17 00:02:43,443 : start epoch
2019-02-17 00:03:26,312 : samples : 64000
2019-02-17 00:03:36,588 : Image to text: 12.86, 34.1, 47.62, 12.0
2019-02-17 00:03:44,125 : Text to Image: 11.036, 30.588, 43.776, 14.0
2019-02-17 00:04:26,792 : samples : 128000
2019-02-17 00:04:37,035 : Image to text: 13.42, 34.82, 48.7, 11.0
2019-02-17 00:04:44,502 : Text to Image: 10.948, 30.404, 43.072, 15.0
2019-02-17 00:05:27,713 : samples : 192000
2019-02-17 00:05:38,330 : Image to text: 13.2, 34.7, 48.36, 11.0
2019-02-17 00:05:45,899 : Text to Image: 10.92, 30.0, 42.772, 15.0
2019-02-17 00:06:28,350 : samples : 256000
2019-02-17 00:06:38,832 : Image to text: 12.88, 34.32, 47.92, 12.0
2019-02-17 00:06:46,369 : Text to Image: 10.82, 30.02, 42.64, 15.0
2019-02-17 00:07:29,402 : samples : 320000
2019-02-17 00:07:39,893 : Image to text: 12.94, 33.9, 48.3, 11.0
2019-02-17 00:07:47,520 : Text to Image: 10.696, 29.496, 42.584, 15.0
2019-02-17 00:08:30,270 : samples : 384000
2019-02-17 00:08:40,786 : Image to text: 11.84, 32.24, 47.14, 12.0
2019-02-17 00:08:48,339 : Text to Image: 10.604, 29.48, 42.332, 15.0
2019-02-17 00:09:31,564 : samples : 448000
2019-02-17 00:09:41,972 : Image to text: 12.48, 33.38, 47.12, 12.0
2019-02-17 00:09:49,475 : Text to Image: 10.708, 29.68, 42.368, 15.0
2019-02-17 00:10:42,522 : samples : 512000
2019-02-17 00:10:53,001 : Image to text: 12.4, 33.76, 47.22, 12.0
2019-02-17 00:11:00,556 : Text to Image: 10.636, 29.724, 42.688, 15.0
2019-02-17 00:11:37,067 : Epoch 17 finished
2019-02-17 00:11:37,517 : Image to text: 29.2, 64.5, 79.7, 3.0
2019-02-17 00:11:37,864 : Text to Image: 25.32, 61.24, 77.7, 4.0
2019-02-17 00:11:38,322 : Image to text: 29.8, 64.2, 78.6, 3.0
2019-02-17 00:11:38,657 : Text to Image: 25.54, 60.2, 76.74, 4.0
2019-02-17 00:11:39,109 : Image to text: 30.2, 64.7, 79.7, 3.0
2019-02-17 00:11:39,452 : Text to Image: 26.3, 61.44, 77.74, 4.0
2019-02-17 00:11:39,925 : Image to text: 32.0, 64.7, 78.5, 3.0
2019-02-17 00:11:40,271 : Text to Image: 26.5, 60.82, 77.38, 4.0
2019-02-17 00:11:40,724 : Image to text: 30.9, 65.3, 78.7, 3.0
2019-02-17 00:11:41,074 : Text to Image: 26.4, 61.54, 76.9, 4.0
2019-02-17 00:11:41,074 : Dev mean Text to Image: 26.012, 61.04800000000001, 77.29199999999999, 4.0
2019-02-17 00:11:41,075 : Dev mean Image to text: 30.42, 64.68, 79.03999999999999, 3.0
2019-02-17 00:11:45,090 : 
Test scores | Image to text:             31.86, 65.97999999999999, 80.19999999999999, 3.0
2019-02-17 00:11:45,091 : Test scores | Text to image:             26.124000000000002, 60.796, 76.952, 4.0

2019-02-17 00:11:45,226 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-17 00:11:45,444 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-17 00:11:46,161 : loading BERT model bert-base-uncased
2019-02-17 00:11:46,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:11:46,196 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:11:46,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1cov1qba
2019-02-17 00:11:48,695 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:11:50,187 : Computing embeddings for train/dev/test
2019-02-17 00:13:25,565 : Computed embeddings
2019-02-17 00:13:25,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:14:10,551 : [('reg:1e-05', 60.17), ('reg:0.0001', 59.76), ('reg:0.001', 57.93), ('reg:0.01', 53.06)]
2019-02-17 00:14:10,551 : Validation : best param found is reg = 1e-05 with score             60.17
2019-02-17 00:14:10,552 : Evaluating...
2019-02-17 00:14:20,782 : 
Dev acc : 60.2 Test acc : 60.4 for LENGTH classification

2019-02-17 00:14:20,783 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-17 00:14:21,179 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-17 00:14:21,230 : loading BERT model bert-base-uncased
2019-02-17 00:14:21,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:14:21,266 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:14:21,266 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplpdx8lav
2019-02-17 00:14:23,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:14:25,291 : Computing embeddings for train/dev/test
2019-02-17 00:15:54,254 : Computed embeddings
2019-02-17 00:15:54,254 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:16:58,058 : [('reg:1e-05', 20.98), ('reg:0.0001', 7.45), ('reg:0.001', 0.83), ('reg:0.01', 0.17)]
2019-02-17 00:16:58,058 : Validation : best param found is reg = 1e-05 with score             20.98
2019-02-17 00:16:58,058 : Evaluating...
2019-02-17 00:17:18,769 : 
Dev acc : 21.0 Test acc : 21.0 for WORDCONTENT classification

2019-02-17 00:17:18,770 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-17 00:17:19,115 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-17 00:17:19,195 : loading BERT model bert-base-uncased
2019-02-17 00:17:19,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:17:19,224 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:17:19,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv2hj_39p
2019-02-17 00:17:21,706 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:17:23,172 : Computing embeddings for train/dev/test
2019-02-17 00:18:46,299 : Computed embeddings
2019-02-17 00:18:46,300 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:19:33,170 : [('reg:1e-05', 28.41), ('reg:0.0001', 28.82), ('reg:0.001', 27.29), ('reg:0.01', 22.25)]
2019-02-17 00:19:33,170 : Validation : best param found is reg = 0.0001 with score             28.82
2019-02-17 00:19:33,170 : Evaluating...
2019-02-17 00:19:42,561 : 
Dev acc : 28.8 Test acc : 28.6 for DEPTH classification

2019-02-17 00:19:42,562 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-17 00:19:42,978 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-17 00:19:43,050 : loading BERT model bert-base-uncased
2019-02-17 00:19:43,050 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:19:43,084 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:19:43,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkl592rw8
2019-02-17 00:19:45,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:19:46,990 : Computing embeddings for train/dev/test
2019-02-17 00:21:05,388 : Computed embeddings
2019-02-17 00:21:05,389 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:21:56,143 : [('reg:1e-05', 51.4), ('reg:0.0001', 52.49), ('reg:0.001', 50.24), ('reg:0.01', 43.19)]
2019-02-17 00:21:56,144 : Validation : best param found is reg = 0.0001 with score             52.49
2019-02-17 00:21:56,144 : Evaluating...
2019-02-17 00:22:10,051 : 
Dev acc : 52.5 Test acc : 52.7 for TOPCONSTITUENTS classification

2019-02-17 00:22:10,052 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-17 00:22:10,474 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-17 00:22:10,544 : loading BERT model bert-base-uncased
2019-02-17 00:22:10,544 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:22:10,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:22:10,579 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8h5ye1qj
2019-02-17 00:22:13,125 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:22:14,607 : Computing embeddings for train/dev/test
2019-02-17 00:23:39,426 : Computed embeddings
2019-02-17 00:23:39,426 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:24:39,059 : [('reg:1e-05', 87.58), ('reg:0.0001', 87.79), ('reg:0.001', 87.83), ('reg:0.01', 87.18)]
2019-02-17 00:24:39,059 : Validation : best param found is reg = 0.001 with score             87.83
2019-02-17 00:24:39,059 : Evaluating...
2019-02-17 00:24:58,502 : 
Dev acc : 87.8 Test acc : 86.9 for BIGRAMSHIFT classification

2019-02-17 00:24:58,503 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-17 00:24:58,943 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-17 00:24:59,017 : loading BERT model bert-base-uncased
2019-02-17 00:24:59,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:24:59,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:24:59,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp43y9u6xk
2019-02-17 00:25:01,623 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:25:03,080 : Computing embeddings for train/dev/test
2019-02-17 00:26:25,999 : Computed embeddings
2019-02-17 00:26:25,999 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:27:03,119 : [('reg:1e-05', 89.6), ('reg:0.0001', 89.66), ('reg:0.001', 89.65), ('reg:0.01', 89.82)]
2019-02-17 00:27:03,119 : Validation : best param found is reg = 0.01 with score             89.82
2019-02-17 00:27:03,119 : Evaluating...
2019-02-17 00:27:10,092 : 
Dev acc : 89.8 Test acc : 88.3 for TENSE classification

2019-02-17 00:27:10,093 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-17 00:27:10,643 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-17 00:27:10,729 : loading BERT model bert-base-uncased
2019-02-17 00:27:10,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:27:10,898 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:27:10,898 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7bq9iy11
2019-02-17 00:27:13,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:27:15,954 : Computing embeddings for train/dev/test
2019-02-17 00:28:46,883 : Computed embeddings
2019-02-17 00:28:46,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:29:27,478 : [('reg:1e-05', 81.76), ('reg:0.0001', 73.3), ('reg:0.001', 79.39), ('reg:0.01', 80.6)]
2019-02-17 00:29:27,478 : Validation : best param found is reg = 1e-05 with score             81.76
2019-02-17 00:29:27,478 : Evaluating...
2019-02-17 00:29:37,349 : 
Dev acc : 81.8 Test acc : 81.2 for SUBJNUMBER classification

2019-02-17 00:29:37,350 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-17 00:29:38,023 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-17 00:29:38,096 : loading BERT model bert-base-uncased
2019-02-17 00:29:38,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:29:38,129 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:29:38,129 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf_ufsedj
2019-02-17 00:29:40,609 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:29:42,077 : Computing embeddings for train/dev/test
2019-02-17 00:31:09,401 : Computed embeddings
2019-02-17 00:31:09,401 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:32:03,015 : [('reg:1e-05', 76.56), ('reg:0.0001', 77.19), ('reg:0.001', 76.0), ('reg:0.01', 74.48)]
2019-02-17 00:32:03,015 : Validation : best param found is reg = 0.0001 with score             77.19
2019-02-17 00:32:03,015 : Evaluating...
2019-02-17 00:32:14,428 : 
Dev acc : 77.2 Test acc : 78.0 for OBJNUMBER classification

2019-02-17 00:32:14,429 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-17 00:32:14,850 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-17 00:32:14,921 : loading BERT model bert-base-uncased
2019-02-17 00:32:14,922 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:32:14,953 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:32:14,953 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyybg72xo
2019-02-17 00:32:17,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:32:18,893 : Computing embeddings for train/dev/test
2019-02-17 00:33:57,149 : Computed embeddings
2019-02-17 00:33:57,149 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:34:29,356 : [('reg:1e-05', 61.84), ('reg:0.0001', 61.83), ('reg:0.001', 61.75), ('reg:0.01', 61.12)]
2019-02-17 00:34:29,356 : Validation : best param found is reg = 1e-05 with score             61.84
2019-02-17 00:34:29,356 : Evaluating...
2019-02-17 00:34:37,886 : 
Dev acc : 61.8 Test acc : 60.8 for ODDMANOUT classification

2019-02-17 00:34:37,888 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-17 00:34:38,348 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-17 00:34:38,433 : loading BERT model bert-base-uncased
2019-02-17 00:34:38,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:34:38,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:34:38,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbv3txgy0
2019-02-17 00:34:40,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:34:42,501 : Computing embeddings for train/dev/test
2019-02-17 00:36:21,952 : Computed embeddings
2019-02-17 00:36:21,952 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:37:06,480 : [('reg:1e-05', 67.12), ('reg:0.0001', 67.09), ('reg:0.001', 67.09), ('reg:0.01', 57.33)]
2019-02-17 00:37:06,480 : Validation : best param found is reg = 1e-05 with score             67.12
2019-02-17 00:37:06,480 : Evaluating...
2019-02-17 00:37:18,673 : 
Dev acc : 67.1 Test acc : 67.4 for COORDINATIONINVERSION classification

2019-02-17 00:37:18,675 : total results: {'STS12': {'MSRpar': {'pearson': (0.3177114267772954, 4.727847183135348e-19), 'spearman': SpearmanrResult(correlation=0.3499168752297693, pvalue=5.035205656223861e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5318149239190572, 5.507467473705454e-56), 'spearman': SpearmanrResult(correlation=0.5516689876322041, pvalue=6.0069226364554184e-61), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4933395755640929, 1.5804102241274114e-29), 'spearman': SpearmanrResult(correlation=0.5934293939153934, pvalue=5.21522285902493e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5112644090069811, 3.4799335307210674e-51), 'spearman': SpearmanrResult(correlation=0.5411381758857834, pvalue=2.8309993326193958e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5777264487368039, 6.731033207179055e-37), 'spearman': SpearmanrResult(correlation=0.49693317366691375, pvalue=2.838515326067615e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48637135680084603, 'wmean': 0.47540179794318005}, 'spearman': {'mean': 0.5066173212660128, 'wmean': 0.4995828369244149}}}, 'STS13': {'FNWN': {'pearson': (0.23118197837895474, 0.0013713584019167083), 'spearman': SpearmanrResult(correlation=0.2521752725341372, pvalue=0.00046423869216876047), 'nsamples': 189}, 'headlines': {'pearson': (0.6394082101351356, 1.8842223676951607e-87), 'spearman': SpearmanrResult(correlation=0.6270260023297658, pvalue=3.435498168009744e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.551772468466727, 5.185677579213662e-46), 'spearman': SpearmanrResult(correlation=0.5426976734286421, pvalue=2.773261025593889e-44), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4741208856602725, 'wmean': 0.555195937549872}, 'spearman': {'mean': 0.473966316097515, 'wmean': 0.5482560153664963}}}, 'STS14': {'deft-forum': {'pearson': (0.3374661416425509, 1.899211939761301e-13), 'spearman': SpearmanrResult(correlation=0.33970121496729017, pvalue=1.2867853489854492e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7412917859372268, 1.5555471944315878e-53), 'spearman': SpearmanrResult(correlation=0.7209626829922081, pvalue=2.1820003028036598e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5793712820748428, 1.8708947850123745e-68), 'spearman': SpearmanrResult(correlation=0.5454422646471138, pvalue=2.3484748937665537e-59), 'nsamples': 750}, 'images': {'pearson': (0.46540785808934, 1.39273068011811e-41), 'spearman': SpearmanrResult(correlation=0.46659944632112677, pvalue=8.172324620310519e-42), 'nsamples': 750}, 'OnWN': {'pearson': (0.6500834612461802, 2.7606413424763127e-91), 'spearman': SpearmanrResult(correlation=0.6750857328186214, pvalue=6.701524753287493e-101), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6091385646641065, 2.2995102772238432e-77), 'spearman': SpearmanrResult(correlation=0.5567529364339914, pvalue=2.8419843294685394e-62), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5637931822757078, 'wmean': 0.5605995130869782}, 'spearman': {'mean': 0.5507573796967252, 'wmean': 0.5472172364796222}}}, 'STS15': {'answers-forums': {'pearson': (0.5324864628689328, 7.58050754536243e-29), 'spearman': SpearmanrResult(correlation=0.5137325363553714, pvalue=1.2113726990362248e-26), 'nsamples': 375}, 'answers-students': {'pearson': (0.6243591684234505, 2.68302404740675e-82), 'spearman': SpearmanrResult(correlation=0.6396216793985848, pvalue=1.5847187041905987e-87), 'nsamples': 750}, 'belief': {'pearson': (0.5829396487372465, 1.641147566853056e-35), 'spearman': SpearmanrResult(correlation=0.6119691275953213, pvalue=6.648769163360828e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6406077613937924, 7.110437054299213e-88), 'spearman': SpearmanrResult(correlation=0.6457355513727421, pvalue=1.049915692172674e-89), 'nsamples': 750}, 'images': {'pearson': (0.6459502560923823, 8.78477081736151e-90), 'spearman': SpearmanrResult(correlation=0.6576285600760874, pvalue=4.329157726919352e-94), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6052686595031609, 'wmean': 0.6171575604281787}, 'spearman': {'mean': 0.6137374909596215, 'wmean': 0.6264591557056901}}}, 'STS16': {'answer-answer': {'pearson': (0.5145166361674754, 1.4364194809809388e-18), 'spearman': SpearmanrResult(correlation=0.5004123284666717, pvalue=1.6754005001631563e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6537489191754112, 9.621269748742019e-32), 'spearman': SpearmanrResult(correlation=0.6567974382085588, pvalue=4.033786866631412e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7185760083970633, 7.95567389677719e-38), 'spearman': SpearmanrResult(correlation=0.7257582814804022, pvalue=6.652142249914582e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.779131680989704, 5.396994562214571e-51), 'spearman': SpearmanrResult(correlation=0.8084694684517145, pvalue=1.2202097911184729e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.3817748968108354, 1.1725437139989676e-08), 'spearman': SpearmanrResult(correlation=0.3935699364502679, pvalue=3.739870912110002e-09), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6095496283080978, 'wmean': 0.614369537932119}, 'spearman': {'mean': 0.617001490611523, 'wmean': 0.6214960921629702}}}, 'MR': {'devacc': 74.33, 'acc': 74.48, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.89, 'acc': 80.71, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.13, 'acc': 87.62, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.11, 'acc': 93.65, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.32, 'acc': 83.31, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.05, 'acc': 42.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.63, 'acc': 74.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.25, 'acc': 68.29, 'f1': 74.64, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.2, 'acc': 76.46, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7721522474429817, 'pearson': 0.7852996423354045, 'spearman': 0.7144851283534263, 'mse': 0.3907123645302785, 'yhat': array([2.7546782 , 3.58835174, 1.92281702, ..., 3.18413261, 4.05649257,        4.71688018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6290811178773263, 'pearson': 0.6357128194002151, 'spearman': 0.6331347975190704, 'mse': 1.4463465968284188, 'yhat': array([1.87483528, 1.15069896, 1.86073579, ..., 3.88181083, 4.11178007,        3.9461092 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.2, 'acc': 67.0, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 343.87199999999996, 'acc': [(31.86, 65.97999999999999, 80.19999999999999, 3.0), (26.124000000000002, 60.796, 76.952, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 60.17, 'acc': 60.38, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 20.98, 'acc': 21.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.82, 'acc': 28.55, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.49, 'acc': 52.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.83, 'acc': 86.91, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.82, 'acc': 88.28, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.76, 'acc': 81.25, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.19, 'acc': 78.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.84, 'acc': 60.77, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 67.12, 'acc': 67.43, 'ndev': 10002, 'ntest': 10002}}
2019-02-17 00:37:18,676 : STS12 p=0.4754, STS12 s=0.4996, STS13 p=0.5552, STS13 s=0.5483, STS14 p=0.5606, STS14 s=0.5472, STS15 p=0.6172, STS15 s=0.6265, STS 16 p=0.6144, STS16 s=0.6215, STS B p=0.6357, STS B s=0.6331, STS B m=1.4463, SICK-R p=0.7853, SICK-R s=0.7145, SICK-P m=0.3907
2019-02-17 00:37:18,676 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-17 00:37:18,676 : 0.4754,0.4996,0.5552,0.5483,0.5606,0.5472,0.6172,0.6265,0.6144,0.6215,0.6357,0.6331,1.4463,0.7853,0.7145,0.3907
2019-02-17 00:37:18,676 : MR=74.48, CR=80.71, SUBJ=93.65, MPQA=87.62, SST-B=83.31, SST-F=42.26, TREC=74.80, SICK-E=76.46, SNLI=67.00, MRPC=68.29, MRPC f=74.64
2019-02-17 00:37:18,676 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-17 00:37:18,676 : 74.48,80.71,93.65,87.62,83.31,42.26,74.80,76.46,67.00,68.29,74.64
2019-02-17 00:37:18,676 : COCO r1i2t=31.86, COCO r5i2t=65.98, COCO r10i2t=80.20, COCO medr_i2t=3.00, COCO r1t2i=26.12, COCO r5t2i=60.80, COCO r10t2i=76.95, COCO medr_t2i=4.00
2019-02-17 00:37:18,676 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-17 00:37:18,676 : 31.86,65.98,80.20,3.00,26.12,60.80,76.95,4.00
2019-02-17 00:37:18,676 : SentLen=60.38, WC=21.03, TreeDepth=28.55, TopConst=52.70, BShift=86.91, Tense=88.28, SubjNum=81.25, ObjNum=78.05, SOMO=60.77, CoordInv=67.43, average=62.54
2019-02-17 00:37:18,676 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-17 00:37:18,676 : 60.38,21.03,28.55,52.70,86.91,88.28,81.25,78.05,60.77,67.43,62.54
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,676 : layer 11
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,676 : ********************************************************************************
2019-02-17 00:37:18,775 : ***** Transfer task : STS12 *****


2019-02-17 00:37:18,789 : loading BERT model bert-base-uncased
2019-02-17 00:37:18,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:18,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:18,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpygmd0ejs
2019-02-17 00:37:21,273 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:37:24,502 : MSRpar : pearson = 0.3160, spearman = 0.3507
2019-02-17 00:37:25,240 : MSRvid : pearson = 0.5576, spearman = 0.5726
2019-02-17 00:37:25,898 : SMTeuroparl : pearson = 0.4934, spearman = 0.5902
2019-02-17 00:37:27,057 : surprise.OnWN : pearson = 0.5098, spearman = 0.5336
2019-02-17 00:37:27,718 : surprise.SMTnews : pearson = 0.5582, spearman = 0.4885
2019-02-17 00:37:27,718 : ALL (weighted average) : Pearson = 0.4783,             Spearman = 0.5014
2019-02-17 00:37:27,718 : ALL (average) : Pearson = 0.4870,             Spearman = 0.5071

2019-02-17 00:37:27,719 : ***** Transfer task : STS13 (-SMT) *****


2019-02-17 00:37:27,740 : loading BERT model bert-base-uncased
2019-02-17 00:37:27,741 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:27,763 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:27,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmperhe33nw
2019-02-17 00:37:30,237 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:37:32,318 : FNWN : pearson = 0.2562, spearman = 0.2735
2019-02-17 00:37:33,226 : headlines : pearson = 0.6487, spearman = 0.6361
2019-02-17 00:37:33,887 : OnWN : pearson = 0.5590, spearman = 0.5502
2019-02-17 00:37:33,887 : ALL (weighted average) : Pearson = 0.5657,             Spearman = 0.5583
2019-02-17 00:37:33,887 : ALL (average) : Pearson = 0.4880,             Spearman = 0.4866

2019-02-17 00:37:33,887 : ***** Transfer task : STS14 *****


2019-02-17 00:37:33,906 : loading BERT model bert-base-uncased
2019-02-17 00:37:33,906 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:33,927 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:33,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphyw_b8q4
2019-02-17 00:37:36,419 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:37:38,624 : deft-forum : pearson = 0.3459, spearman = 0.3487
2019-02-17 00:37:39,311 : deft-news : pearson = 0.7386, spearman = 0.7196
2019-02-17 00:37:40,318 : headlines : pearson = 0.5839, spearman = 0.5494
2019-02-17 00:37:41,264 : images : pearson = 0.4736, spearman = 0.4719
2019-02-17 00:37:42,234 : OnWN : pearson = 0.6657, spearman = 0.6876
2019-02-17 00:37:43,513 : tweet-news : pearson = 0.6038, spearman = 0.5487
2019-02-17 00:37:43,513 : ALL (weighted average) : Pearson = 0.5660,             Spearman = 0.5509
2019-02-17 00:37:43,514 : ALL (average) : Pearson = 0.5686,             Spearman = 0.5543

2019-02-17 00:37:43,514 : ***** Transfer task : STS15 *****


2019-02-17 00:37:43,560 : loading BERT model bert-base-uncased
2019-02-17 00:37:43,560 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:43,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:43,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuditrbju
2019-02-17 00:37:46,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:37:48,418 : answers-forums : pearson = 0.5246, spearman = 0.5148
2019-02-17 00:37:49,340 : answers-students : pearson = 0.6059, spearman = 0.6241
2019-02-17 00:37:50,208 : belief : pearson = 0.5875, spearman = 0.6152
2019-02-17 00:37:51,220 : headlines : pearson = 0.6512, spearman = 0.6539
2019-02-17 00:37:52,210 : images : pearson = 0.6498, spearman = 0.6601
2019-02-17 00:37:52,210 : ALL (weighted average) : Pearson = 0.6157,             Spearman = 0.6258
2019-02-17 00:37:52,210 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6136

2019-02-17 00:37:52,210 : ***** Transfer task : STS16 *****


2019-02-17 00:37:52,289 : loading BERT model bert-base-uncased
2019-02-17 00:37:52,289 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:52,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:52,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_9_obcyx
2019-02-17 00:37:54,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:37:56,654 : answer-answer : pearson = 0.5254, spearman = 0.5052
2019-02-17 00:37:56,966 : headlines : pearson = 0.6546, spearman = 0.6575
2019-02-17 00:37:57,366 : plagiarism : pearson = 0.7183, spearman = 0.7299
2019-02-17 00:37:58,001 : postediting : pearson = 0.7808, spearman = 0.8120
2019-02-17 00:37:58,294 : question-question : pearson = 0.4230, spearman = 0.4299
2019-02-17 00:37:58,294 : ALL (weighted average) : Pearson = 0.6244,             Spearman = 0.6306
2019-02-17 00:37:58,294 : ALL (average) : Pearson = 0.6204,             Spearman = 0.6269

2019-02-17 00:37:58,294 : ***** Transfer task : MR *****


2019-02-17 00:37:58,314 : loading BERT model bert-base-uncased
2019-02-17 00:37:58,314 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:37:58,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:37:58,337 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphtbs5fyj
2019-02-17 00:38:00,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:38:02,404 : Generating sentence embeddings
2019-02-17 00:38:16,695 : Generated sentence embeddings
2019-02-17 00:38:16,695 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 00:38:34,488 : Best param found at split 1: l2reg = 0.0001                 with score 75.9
2019-02-17 00:38:55,325 : Best param found at split 2: l2reg = 0.0001                 with score 73.09
2019-02-17 00:39:15,652 : Best param found at split 3: l2reg = 0.001                 with score 73.66
2019-02-17 00:39:35,463 : Best param found at split 4: l2reg = 0.001                 with score 73.46
2019-02-17 00:39:57,733 : Best param found at split 5: l2reg = 1e-05                 with score 75.19
2019-02-17 00:39:58,861 : Dev acc : 74.26 Test acc : 75.56

2019-02-17 00:39:58,862 : ***** Transfer task : CR *****


2019-02-17 00:39:58,875 : loading BERT model bert-base-uncased
2019-02-17 00:39:58,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:39:58,904 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:39:58,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp74azxf7w
2019-02-17 00:40:01,437 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:40:02,942 : Generating sentence embeddings
2019-02-17 00:40:06,679 : Generated sentence embeddings
2019-02-17 00:40:06,679 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 00:40:12,124 : Best param found at split 1: l2reg = 0.001                 with score 80.79
2019-02-17 00:40:18,212 : Best param found at split 2: l2reg = 0.0001                 with score 78.8
2019-02-17 00:40:23,255 : Best param found at split 3: l2reg = 1e-05                 with score 77.95
2019-02-17 00:40:29,859 : Best param found at split 4: l2reg = 0.001                 with score 79.31
2019-02-17 00:40:35,446 : Best param found at split 5: l2reg = 1e-05                 with score 77.56
2019-02-17 00:40:35,874 : Dev acc : 78.88 Test acc : 75.44

2019-02-17 00:40:35,875 : ***** Transfer task : MPQA *****


2019-02-17 00:40:35,883 : loading BERT model bert-base-uncased
2019-02-17 00:40:35,883 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:40:35,913 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:40:35,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4106azaq
2019-02-17 00:40:38,434 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:40:39,917 : Generating sentence embeddings
2019-02-17 00:40:43,720 : Generated sentence embeddings
2019-02-17 00:40:43,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 00:40:59,660 : Best param found at split 1: l2reg = 1e-05                 with score 87.36
2019-02-17 00:41:17,240 : Best param found at split 2: l2reg = 0.001                 with score 87.49
2019-02-17 00:41:34,931 : Best param found at split 3: l2reg = 0.001                 with score 87.17
2019-02-17 00:41:51,834 : Best param found at split 4: l2reg = 0.001                 with score 87.63
2019-02-17 00:42:08,383 : Best param found at split 5: l2reg = 0.0001                 with score 87.24
2019-02-17 00:42:08,955 : Dev acc : 87.38 Test acc : 86.58

2019-02-17 00:42:08,956 : ***** Transfer task : SUBJ *****


2019-02-17 00:42:08,975 : loading BERT model bert-base-uncased
2019-02-17 00:42:08,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:42:08,997 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:42:08,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0_giersh
2019-02-17 00:42:11,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:42:12,957 : Generating sentence embeddings
2019-02-17 00:42:26,240 : Generated sentence embeddings
2019-02-17 00:42:26,241 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 00:42:43,403 : Best param found at split 1: l2reg = 1e-05                 with score 93.96
2019-02-17 00:43:00,501 : Best param found at split 2: l2reg = 1e-05                 with score 93.8
2019-02-17 00:43:19,817 : Best param found at split 3: l2reg = 1e-05                 with score 93.94
2019-02-17 00:43:39,797 : Best param found at split 4: l2reg = 0.001                 with score 94.19
2019-02-17 00:43:57,498 : Best param found at split 5: l2reg = 0.0001                 with score 93.95
2019-02-17 00:43:58,068 : Dev acc : 93.97 Test acc : 93.7

2019-02-17 00:43:58,069 : ***** Transfer task : SST Binary classification *****


2019-02-17 00:43:58,202 : loading BERT model bert-base-uncased
2019-02-17 00:43:58,202 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:43:58,225 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:43:58,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp24sy53cj
2019-02-17 00:44:00,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:44:02,189 : Computing embedding for train
2019-02-17 00:44:49,545 : Computed train embeddings
2019-02-17 00:44:49,546 : Computing embedding for dev
2019-02-17 00:44:50,617 : Computed dev embeddings
2019-02-17 00:44:50,618 : Computing embedding for test
2019-02-17 00:44:52,930 : Computed test embeddings
2019-02-17 00:44:52,931 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:45:12,480 : [('reg:1e-05', 83.94), ('reg:0.0001', 84.06), ('reg:0.001', 84.52), ('reg:0.01', 84.17)]
2019-02-17 00:45:12,480 : Validation : best param found is reg = 0.001 with score             84.52
2019-02-17 00:45:12,481 : Evaluating...
2019-02-17 00:45:17,019 : 
Dev acc : 84.52 Test acc : 83.75 for             SST Binary classification

2019-02-17 00:45:17,019 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-17 00:45:17,069 : loading BERT model bert-base-uncased
2019-02-17 00:45:17,069 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:45:17,093 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:45:17,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvqgjdtud
2019-02-17 00:45:19,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:45:21,001 : Computing embedding for train
2019-02-17 00:45:30,416 : Computed train embeddings
2019-02-17 00:45:30,416 : Computing embedding for dev
2019-02-17 00:45:31,649 : Computed dev embeddings
2019-02-17 00:45:31,649 : Computing embedding for test
2019-02-17 00:45:34,124 : Computed test embeddings
2019-02-17 00:45:34,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:45:39,050 : [('reg:1e-05', 43.87), ('reg:0.0001', 42.23), ('reg:0.001', 43.51), ('reg:0.01', 35.51)]
2019-02-17 00:45:39,051 : Validation : best param found is reg = 1e-05 with score             43.87
2019-02-17 00:45:39,051 : Evaluating...
2019-02-17 00:45:40,335 : 
Dev acc : 43.87 Test acc : 45.7 for             SST Fine-Grained classification

2019-02-17 00:45:40,335 : ***** Transfer task : TREC *****


2019-02-17 00:45:40,349 : loading BERT model bert-base-uncased
2019-02-17 00:45:40,349 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:45:40,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:45:40,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpic9nnja_
2019-02-17 00:45:42,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:45:47,791 : Computed train embeddings
2019-02-17 00:45:48,048 : Computed test embeddings
2019-02-17 00:45:48,048 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 00:45:59,375 : [('reg:1e-05', 75.7), ('reg:0.0001', 69.62), ('reg:0.001', 74.94), ('reg:0.01', 71.02)]
2019-02-17 00:45:59,375 : Cross-validation : best param found is reg = 1e-05             with score 75.7
2019-02-17 00:45:59,376 : Evaluating...
2019-02-17 00:46:00,230 : 
Dev acc : 75.7 Test acc : 88.4             for TREC

2019-02-17 00:46:00,231 : ***** Transfer task : MRPC *****


2019-02-17 00:46:00,287 : loading BERT model bert-base-uncased
2019-02-17 00:46:00,287 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:46:00,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:46:00,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9bq_3u57
2019-02-17 00:46:02,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:46:04,286 : Computing embedding for train
2019-02-17 00:46:14,197 : Computed train embeddings
2019-02-17 00:46:14,197 : Computing embedding for test
2019-02-17 00:46:18,863 : Computed test embeddings
2019-02-17 00:46:18,879 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 00:46:26,153 : [('reg:1e-05', 73.01), ('reg:0.0001', 71.86), ('reg:0.001', 71.64), ('reg:0.01', 72.64)]
2019-02-17 00:46:26,153 : Cross-validation : best param found is reg = 1e-05             with score 73.01
2019-02-17 00:46:26,154 : Evaluating...
2019-02-17 00:46:26,351 : Dev acc : 73.01 Test acc 60.35; Test F1 61.62 for MRPC.

2019-02-17 00:46:26,351 : ***** Transfer task : SICK-Entailment*****


2019-02-17 00:46:26,376 : loading BERT model bert-base-uncased
2019-02-17 00:46:26,376 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:46:26,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:46:26,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf2yswdan
2019-02-17 00:46:28,922 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:46:30,379 : Computing embedding for train
2019-02-17 00:46:35,533 : Computed train embeddings
2019-02-17 00:46:35,534 : Computing embedding for dev
2019-02-17 00:46:36,214 : Computed dev embeddings
2019-02-17 00:46:36,214 : Computing embedding for test
2019-02-17 00:46:41,804 : Computed test embeddings
2019-02-17 00:46:41,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 00:46:43,710 : [('reg:1e-05', 70.0), ('reg:0.0001', 74.2), ('reg:0.001', 76.6), ('reg:0.01', 73.4)]
2019-02-17 00:46:43,711 : Validation : best param found is reg = 0.001 with score             76.6
2019-02-17 00:46:43,711 : Evaluating...
2019-02-17 00:46:44,327 : 
Dev acc : 76.6 Test acc : 75.34 for                        SICK entailment

2019-02-17 00:46:44,328 : ***** Transfer task : SICK-Relatedness*****


2019-02-17 00:46:44,359 : loading BERT model bert-base-uncased
2019-02-17 00:46:44,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:46:44,381 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:46:44,381 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0pudgvbp
2019-02-17 00:46:46,895 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:46:48,358 : Computing embedding for train
2019-02-17 00:46:53,456 : Computed train embeddings
2019-02-17 00:46:53,456 : Computing embedding for dev
2019-02-17 00:46:54,147 : Computed dev embeddings
2019-02-17 00:46:54,147 : Computing embedding for test
2019-02-17 00:46:59,749 : Computed test embeddings
2019-02-17 00:47:21,426 : Dev : Pearson 0.789778892712242
2019-02-17 00:47:21,428 : Test : Pearson 0.7838590893186084 Spearman 0.7140007001331513 MSE 0.3956690813288107                        for SICK Relatedness

2019-02-17 00:47:21,429 : 

***** Transfer task : STSBenchmark*****


2019-02-17 00:47:21,542 : loading BERT model bert-base-uncased
2019-02-17 00:47:21,542 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:47:21,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:47:21,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsljymcbe
2019-02-17 00:47:24,086 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:47:25,554 : Computing embedding for train
2019-02-17 00:47:33,808 : Computed train embeddings
2019-02-17 00:47:33,809 : Computing embedding for dev
2019-02-17 00:47:36,270 : Computed dev embeddings
2019-02-17 00:47:36,270 : Computing embedding for test
2019-02-17 00:47:38,280 : Computed test embeddings
2019-02-17 00:48:06,024 : Dev : Pearson 0.6229046588663187
2019-02-17 00:48:06,024 : Test : Pearson 0.6397960231710759 Spearman 0.6345147084797617 MSE 1.4430591637292234                        for SICK Relatedness

2019-02-17 00:48:06,024 : ***** Transfer task : SNLI Entailment*****


2019-02-17 00:48:11,326 : loading BERT model bert-base-uncased
2019-02-17 00:48:11,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 00:48:11,459 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 00:48:11,460 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps903m7jm
2019-02-17 00:48:13,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 00:48:15,675 : PROGRESS (encoding): 0.00%
2019-02-17 00:49:35,566 : PROGRESS (encoding): 14.56%
2019-02-17 00:51:03,147 : PROGRESS (encoding): 29.12%
2019-02-17 00:52:30,378 : PROGRESS (encoding): 43.69%
2019-02-17 00:54:05,837 : PROGRESS (encoding): 58.25%
2019-02-17 00:55:50,446 : PROGRESS (encoding): 72.81%
2019-02-17 00:57:35,946 : PROGRESS (encoding): 87.37%
2019-02-17 00:59:26,543 : PROGRESS (encoding): 0.00%
2019-02-17 00:59:39,917 : PROGRESS (encoding): 0.00%
2019-02-17 00:59:52,791 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:00:33,697 : [('reg:1e-09', 65.24)]
2019-02-17 01:00:33,697 : Validation : best param found is reg = 1e-09 with score             65.24
2019-02-17 01:00:33,697 : Evaluating...
2019-02-17 01:01:15,826 : Dev acc : 65.24 Test acc : 65.73 for SNLI

2019-02-17 01:01:15,827 : ***** Transfer task: Image Caption Retrieval *****


2019-02-17 01:01:25,264 : loading BERT model bert-base-uncased
2019-02-17 01:01:25,264 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:01:25,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:01:25,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcxhuxe2c
2019-02-17 01:01:27,770 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:01:29,241 : Computing embedding for train
2019-02-17 01:09:05,970 : Computed train embeddings
2019-02-17 01:09:05,970 : Computing embedding for dev
2019-02-17 01:09:25,798 : Computed dev embeddings
2019-02-17 01:09:25,798 : Computing embedding for test
2019-02-17 01:09:45,778 : Computed test embeddings
2019-02-17 01:09:45,794 : prepare data
2019-02-17 01:09:45,864 : start epoch
2019-02-17 01:10:29,051 : samples : 64000
2019-02-17 01:10:39,465 : Image to text: 7.08, 21.14, 32.2, 24.0
2019-02-17 01:10:46,994 : Text to Image: 5.748, 18.708, 28.936, 28.0
2019-02-17 01:11:30,226 : samples : 128000
2019-02-17 01:11:40,617 : Image to text: 7.54, 23.3, 34.54, 23.0
2019-02-17 01:11:48,180 : Text to Image: 6.668, 20.788, 31.172, 25.0
2019-02-17 01:12:30,875 : samples : 192000
2019-02-17 01:12:41,435 : Image to text: 8.26, 23.8, 35.6, 22.0
2019-02-17 01:12:48,910 : Text to Image: 6.548, 20.096, 30.668, 26.0
2019-02-17 01:13:32,446 : samples : 256000
2019-02-17 01:13:42,999 : Image to text: 7.92, 24.04, 35.72, 21.0
2019-02-17 01:13:50,624 : Text to Image: 6.236, 20.072, 30.22, 26.0
2019-02-17 01:14:34,005 : samples : 320000
2019-02-17 01:14:44,512 : Image to text: 8.14, 24.82, 36.92, 19.0
2019-02-17 01:14:52,066 : Text to Image: 6.908, 21.136, 32.048, 24.0
2019-02-17 01:15:35,447 : samples : 384000
2019-02-17 01:15:45,912 : Image to text: 8.98, 25.98, 37.84, 18.0
2019-02-17 01:15:53,496 : Text to Image: 7.54, 22.208, 33.588, 22.0
2019-02-17 01:16:36,583 : samples : 448000
2019-02-17 01:16:47,098 : Image to text: 8.0, 24.74, 36.42, 20.0
2019-02-17 01:16:54,667 : Text to Image: 7.14, 22.608, 33.888, 22.0
2019-02-17 01:17:37,829 : samples : 512000
2019-02-17 01:17:48,291 : Image to text: 9.5, 27.08, 38.56, 18.0
2019-02-17 01:17:55,921 : Text to Image: 7.732, 23.704, 35.068, 21.0
2019-02-17 01:18:32,890 : Epoch 1 finished
2019-02-17 01:18:33,327 : Image to text: 25.5, 57.8, 70.9, 4.0
2019-02-17 01:18:33,661 : Text to Image: 20.56, 51.26, 68.88, 5.0
2019-02-17 01:18:34,105 : Image to text: 25.2, 57.0, 72.1, 4.0
2019-02-17 01:18:34,443 : Text to Image: 19.48, 50.28, 68.2, 5.0
2019-02-17 01:18:34,887 : Image to text: 26.3, 56.9, 71.5, 4.0
2019-02-17 01:18:35,227 : Text to Image: 19.54, 50.48, 68.0, 5.0
2019-02-17 01:18:35,675 : Image to text: 26.3, 59.0, 72.8, 4.0
2019-02-17 01:18:36,008 : Text to Image: 20.22, 52.24, 69.46, 5.0
2019-02-17 01:18:36,475 : Image to text: 25.5, 54.6, 71.9, 5.0
2019-02-17 01:18:36,822 : Text to Image: 20.4, 52.18, 68.42, 5.0
2019-02-17 01:18:36,822 : Dev mean Text to Image: 20.04, 51.288000000000004, 68.592, 5.0
2019-02-17 01:18:36,822 : Dev mean Image to text: 25.759999999999998, 57.06, 71.84, 4.2
2019-02-17 01:18:36,823 : start epoch
2019-02-17 01:19:27,426 : samples : 64000
2019-02-17 01:19:40,374 : Image to text: 9.18, 26.24, 38.42, 18.0
2019-02-17 01:19:48,257 : Text to Image: 7.704, 23.56, 35.188, 21.0
2019-02-17 01:20:32,150 : samples : 128000
2019-02-17 01:20:42,610 : Image to text: 9.14, 27.48, 39.3, 17.0
2019-02-17 01:20:50,199 : Text to Image: 7.964, 24.096, 35.476, 20.0
2019-02-17 01:21:33,768 : samples : 192000
2019-02-17 01:21:44,244 : Image to text: 9.42, 27.66, 40.38, 16.0
2019-02-17 01:21:51,856 : Text to Image: 8.452, 24.568, 35.992, 20.0
2019-02-17 01:22:34,670 : samples : 256000
2019-02-17 01:22:45,246 : Image to text: 9.04, 27.54, 39.84, 16.0
2019-02-17 01:22:52,936 : Text to Image: 8.544, 25.284, 36.42, 20.0
2019-02-17 01:23:36,253 : samples : 320000
2019-02-17 01:23:46,752 : Image to text: 9.42, 28.1, 40.16, 16.0
2019-02-17 01:23:54,386 : Text to Image: 8.416, 25.076, 36.72, 19.0
2019-02-17 01:24:37,590 : samples : 384000
2019-02-17 01:24:48,076 : Image to text: 9.62, 27.5, 39.78, 17.0
2019-02-17 01:24:55,806 : Text to Image: 8.16, 24.612, 36.148, 20.0
2019-02-17 01:25:38,594 : samples : 448000
2019-02-17 01:25:49,144 : Image to text: 9.66, 27.64, 40.28, 16.0
2019-02-17 01:25:56,796 : Text to Image: 8.252, 25.028, 36.488, 20.0
2019-02-17 01:26:39,779 : samples : 512000
2019-02-17 01:26:50,226 : Image to text: 9.58, 28.18, 41.1, 15.0
2019-02-17 01:26:57,980 : Text to Image: 8.6, 25.7, 37.164, 19.0
2019-02-17 01:27:34,466 : Epoch 2 finished
2019-02-17 01:27:34,902 : Image to text: 26.1, 61.2, 76.9, 3.0
2019-02-17 01:27:35,237 : Text to Image: 21.52, 54.9, 72.44, 5.0
2019-02-17 01:27:35,669 : Image to text: 26.8, 59.2, 73.4, 4.0
2019-02-17 01:27:36,009 : Text to Image: 22.18, 52.68, 70.3, 5.0
2019-02-17 01:27:36,447 : Image to text: 28.3, 59.5, 73.7, 4.0
2019-02-17 01:27:36,791 : Text to Image: 21.66, 53.18, 70.38, 5.0
2019-02-17 01:27:37,255 : Image to text: 26.5, 61.0, 75.8, 4.0
2019-02-17 01:27:37,594 : Text to Image: 22.0, 53.5, 70.6, 5.0
2019-02-17 01:27:38,043 : Image to text: 26.0, 60.5, 73.8, 4.0
2019-02-17 01:27:38,380 : Text to Image: 21.7, 54.34, 70.6, 5.0
2019-02-17 01:27:38,380 : Dev mean Text to Image: 21.812, 53.720000000000006, 70.86399999999999, 5.0
2019-02-17 01:27:38,380 : Dev mean Image to text: 26.740000000000002, 60.279999999999994, 74.72000000000001, 3.8
2019-02-17 01:27:38,381 : start epoch
2019-02-17 01:28:20,975 : samples : 64000
2019-02-17 01:28:31,433 : Image to text: 10.34, 29.42, 42.0, 15.0
2019-02-17 01:28:38,978 : Text to Image: 8.808, 25.44, 36.86, 19.0
2019-02-17 01:29:21,518 : samples : 128000
2019-02-17 01:29:32,037 : Image to text: 9.46, 28.06, 41.06, 16.0
2019-02-17 01:29:39,724 : Text to Image: 8.168, 24.864, 36.444, 20.0
2019-02-17 01:30:22,614 : samples : 192000
2019-02-17 01:30:33,109 : Image to text: 11.22, 29.96, 42.94, 14.0
2019-02-17 01:30:40,869 : Text to Image: 8.968, 25.948, 37.848, 19.0
2019-02-17 01:31:23,223 : samples : 256000
2019-02-17 01:31:33,754 : Image to text: 10.66, 29.46, 41.98, 15.0
2019-02-17 01:31:41,432 : Text to Image: 8.436, 25.216, 37.096, 19.0
2019-02-17 01:32:24,218 : samples : 320000
2019-02-17 01:32:34,820 : Image to text: 10.46, 28.58, 41.72, 16.0
2019-02-17 01:32:42,514 : Text to Image: 8.764, 25.748, 37.536, 19.0
2019-02-17 01:33:25,278 : samples : 384000
2019-02-17 01:33:35,819 : Image to text: 9.98, 28.34, 41.7, 15.0
2019-02-17 01:33:43,616 : Text to Image: 8.456, 25.392, 37.3, 19.0
2019-02-17 01:34:26,453 : samples : 448000
2019-02-17 01:34:36,957 : Image to text: 10.12, 29.76, 42.38, 15.0
2019-02-17 01:34:44,752 : Text to Image: 9.184, 26.244, 38.568, 18.0
2019-02-17 01:35:27,579 : samples : 512000
2019-02-17 01:35:38,156 : Image to text: 10.52, 29.48, 41.56, 16.0
2019-02-17 01:35:45,897 : Text to Image: 8.668, 25.464, 37.624, 19.0
2019-02-17 01:36:24,804 : Epoch 3 finished
2019-02-17 01:36:25,333 : Image to text: 26.5, 59.3, 76.3, 4.0
2019-02-17 01:36:25,734 : Text to Image: 22.64, 55.88, 72.92, 4.0
2019-02-17 01:36:26,258 : Image to text: 28.4, 58.8, 72.7, 4.0
2019-02-17 01:36:26,674 : Text to Image: 22.1, 54.36, 72.06, 5.0
2019-02-17 01:36:27,190 : Image to text: 26.9, 59.8, 73.8, 4.0
2019-02-17 01:36:27,598 : Text to Image: 22.54, 54.94, 72.92, 5.0
2019-02-17 01:36:28,139 : Image to text: 26.4, 63.1, 76.5, 4.0
2019-02-17 01:36:28,551 : Text to Image: 22.98, 55.96, 72.86, 4.0
2019-02-17 01:36:29,088 : Image to text: 28.8, 61.0, 75.7, 4.0
2019-02-17 01:36:29,500 : Text to Image: 23.06, 56.38, 72.68, 4.0
2019-02-17 01:36:29,500 : Dev mean Text to Image: 22.664, 55.504000000000005, 72.688, 4.3999999999999995
2019-02-17 01:36:29,500 : Dev mean Image to text: 27.4, 60.400000000000006, 75.0, 4.0
2019-02-17 01:36:29,501 : start epoch
2019-02-17 01:37:20,340 : samples : 64000
2019-02-17 01:37:30,836 : Image to text: 10.34, 29.5, 41.44, 15.0
2019-02-17 01:37:38,420 : Text to Image: 8.772, 26.488, 38.288, 18.0
2019-02-17 01:38:21,334 : samples : 128000
2019-02-17 01:38:31,811 : Image to text: 10.3, 30.06, 42.7, 14.0
2019-02-17 01:38:39,546 : Text to Image: 9.308, 26.588, 38.588, 18.0
2019-02-17 01:39:22,352 : samples : 192000
2019-02-17 01:39:32,850 : Image to text: 10.08, 29.22, 42.4, 15.0
2019-02-17 01:39:40,574 : Text to Image: 8.404, 25.788, 37.432, 18.0
2019-02-17 01:40:23,676 : samples : 256000
2019-02-17 01:40:34,159 : Image to text: 10.34, 29.98, 41.78, 16.0
2019-02-17 01:40:41,901 : Text to Image: 9.376, 26.828, 38.972, 17.0
2019-02-17 01:41:25,139 : samples : 320000
2019-02-17 01:41:35,571 : Image to text: 10.7, 30.14, 43.76, 14.0
2019-02-17 01:41:43,321 : Text to Image: 9.312, 26.504, 38.1, 18.0
2019-02-17 01:42:26,506 : samples : 384000
2019-02-17 01:42:36,911 : Image to text: 11.1, 30.38, 42.94, 15.0
2019-02-17 01:42:44,695 : Text to Image: 9.188, 26.772, 38.748, 18.0
2019-02-17 01:43:27,525 : samples : 448000
2019-02-17 01:43:37,966 : Image to text: 11.04, 30.82, 44.2, 14.0
2019-02-17 01:43:45,701 : Text to Image: 9.5, 27.052, 39.244, 17.0
2019-02-17 01:44:28,241 : samples : 512000
2019-02-17 01:44:38,647 : Image to text: 11.12, 31.56, 44.66, 13.0
2019-02-17 01:44:46,311 : Text to Image: 9.356, 27.1, 39.428, 17.0
2019-02-17 01:45:22,921 : Epoch 4 finished
2019-02-17 01:45:23,358 : Image to text: 29.5, 62.5, 78.5, 3.0
2019-02-17 01:45:23,697 : Text to Image: 22.82, 56.68, 73.8, 4.0
2019-02-17 01:45:24,144 : Image to text: 26.6, 61.1, 75.5, 4.0
2019-02-17 01:45:24,481 : Text to Image: 22.9, 55.24, 72.88, 5.0
2019-02-17 01:45:24,925 : Image to text: 29.3, 61.2, 75.4, 4.0
2019-02-17 01:45:25,260 : Text to Image: 23.76, 56.06, 73.36, 4.0
2019-02-17 01:45:25,703 : Image to text: 29.0, 63.1, 77.1, 3.0
2019-02-17 01:45:26,045 : Text to Image: 23.46, 56.24, 72.84, 4.0
2019-02-17 01:45:26,523 : Image to text: 29.5, 62.6, 77.1, 3.0
2019-02-17 01:45:26,857 : Text to Image: 23.68, 56.54, 72.78, 4.0
2019-02-17 01:45:26,857 : Dev mean Text to Image: 23.324, 56.15200000000001, 73.13199999999999, 4.2
2019-02-17 01:45:26,857 : Dev mean Image to text: 28.78, 62.099999999999994, 76.72, 3.4000000000000004
2019-02-17 01:45:26,857 : start epoch
2019-02-17 01:46:09,830 : samples : 64000
2019-02-17 01:46:20,349 : Image to text: 10.94, 30.34, 44.1, 14.0
2019-02-17 01:46:27,943 : Text to Image: 9.756, 27.472, 39.608, 17.0
2019-02-17 01:47:12,033 : samples : 128000
2019-02-17 01:47:24,686 : Image to text: 11.58, 31.24, 44.54, 14.0
2019-02-17 01:47:34,838 : Text to Image: 9.96, 27.612, 39.828, 17.0
2019-02-17 01:48:20,522 : samples : 192000
2019-02-17 01:48:30,687 : Image to text: 11.26, 30.42, 43.52, 14.0
2019-02-17 01:48:38,183 : Text to Image: 9.436, 27.464, 39.748, 17.0
2019-02-17 01:49:20,796 : samples : 256000
2019-02-17 01:49:32,817 : Image to text: 10.38, 29.98, 43.52, 14.0
2019-02-17 01:49:42,827 : Text to Image: 9.464, 27.232, 39.548, 17.0
2019-02-17 01:50:28,631 : samples : 320000
2019-02-17 01:50:41,259 : Image to text: 11.52, 30.64, 44.1, 14.0
2019-02-17 01:50:51,339 : Text to Image: 9.216, 26.868, 39.06, 17.0
2019-02-17 01:51:33,221 : samples : 384000
2019-02-17 01:51:43,431 : Image to text: 11.26, 30.9, 43.98, 13.0
2019-02-17 01:51:50,703 : Text to Image: 9.66, 27.588, 40.112, 17.0
2019-02-17 01:52:34,233 : samples : 448000
2019-02-17 01:52:46,825 : Image to text: 10.7, 31.18, 44.56, 14.0
2019-02-17 01:52:56,926 : Text to Image: 9.276, 27.3, 39.44, 17.0
2019-02-17 01:53:44,005 : samples : 512000
2019-02-17 01:53:55,793 : Image to text: 11.34, 29.96, 43.34, 14.0
2019-02-17 01:54:04,037 : Text to Image: 9.836, 27.516, 39.68, 17.0
2019-02-17 01:54:44,449 : Epoch 5 finished
2019-02-17 01:54:45,396 : Image to text: 29.4, 63.8, 78.6, 3.0
2019-02-17 01:54:46,165 : Text to Image: 23.4, 57.76, 74.4, 4.0
2019-02-17 01:54:47,106 : Image to text: 28.3, 60.8, 75.6, 4.0
2019-02-17 01:54:47,865 : Text to Image: 23.08, 57.12, 74.02, 4.0
2019-02-17 01:54:48,795 : Image to text: 29.3, 62.0, 75.9, 3.0
2019-02-17 01:54:49,580 : Text to Image: 23.7, 56.38, 73.24, 4.0
2019-02-17 01:54:50,520 : Image to text: 30.5, 63.3, 78.2, 3.0
2019-02-17 01:54:51,298 : Text to Image: 24.06, 58.3, 74.16, 4.0
2019-02-17 01:54:52,212 : Image to text: 30.2, 63.1, 77.5, 3.0
2019-02-17 01:54:52,960 : Text to Image: 24.22, 58.72, 74.42, 4.0
2019-02-17 01:54:52,960 : Dev mean Text to Image: 23.692, 57.65599999999999, 74.048, 4.0
2019-02-17 01:54:52,960 : Dev mean Image to text: 29.54, 62.60000000000001, 77.16, 3.2
2019-02-17 01:54:52,960 : start epoch
2019-02-17 01:55:37,730 : samples : 64000
2019-02-17 01:55:48,065 : Image to text: 12.04, 32.08, 44.94, 13.0
2019-02-17 01:55:55,541 : Text to Image: 9.792, 28.12, 40.04, 17.0
2019-02-17 01:56:39,190 : samples : 128000
2019-02-17 01:56:51,731 : Image to text: 11.6, 31.68, 45.44, 13.0
2019-02-17 01:57:01,740 : Text to Image: 9.656, 27.564, 39.792, 17.0
2019-02-17 01:57:47,759 : samples : 192000
2019-02-17 01:57:58,750 : Image to text: 10.88, 31.6, 43.82, 14.0
2019-02-17 01:58:06,246 : Text to Image: 9.656, 27.232, 39.544, 17.0
2019-02-17 01:58:48,929 : samples : 256000
2019-02-17 01:59:00,037 : Image to text: 10.54, 31.28, 44.8, 13.0
2019-02-17 01:59:10,038 : Text to Image: 9.74, 27.716, 39.772, 17.0
2019-02-17 01:59:55,060 : samples : 320000
2019-02-17 02:00:07,675 : Image to text: 11.04, 31.48, 44.88, 13.0
2019-02-17 02:00:17,461 : Text to Image: 10.044, 27.96, 40.412, 17.0
2019-02-17 02:01:00,528 : samples : 384000
2019-02-17 02:01:10,797 : Image to text: 11.78, 32.22, 45.8, 13.0
2019-02-17 02:01:17,860 : Text to Image: 10.224, 28.664, 40.968, 16.0
2019-02-17 02:02:02,906 : samples : 448000
2019-02-17 02:02:15,551 : Image to text: 11.36, 31.14, 45.16, 13.0
2019-02-17 02:02:25,623 : Text to Image: 10.0, 28.132, 40.668, 16.0
2019-02-17 02:03:09,460 : samples : 512000
2019-02-17 02:03:19,742 : Image to text: 10.98, 31.62, 45.14, 13.0
2019-02-17 02:03:27,170 : Text to Image: 9.652, 27.932, 40.228, 16.0
2019-02-17 02:04:04,851 : Epoch 6 finished
2019-02-17 02:04:05,790 : Image to text: 29.6, 64.0, 78.3, 3.0
2019-02-17 02:04:06,510 : Text to Image: 24.36, 58.86, 75.8, 4.0
2019-02-17 02:04:07,431 : Image to text: 27.8, 61.3, 77.6, 3.0
2019-02-17 02:04:08,193 : Text to Image: 24.26, 57.36, 74.82, 4.0
2019-02-17 02:04:09,137 : Image to text: 29.1, 63.3, 78.0, 3.0
2019-02-17 02:04:09,906 : Text to Image: 25.1, 58.14, 75.22, 4.0
2019-02-17 02:04:10,810 : Image to text: 32.7, 62.6, 77.3, 3.0
2019-02-17 02:04:11,574 : Text to Image: 25.38, 58.56, 75.08, 4.0
2019-02-17 02:04:12,516 : Image to text: 30.3, 63.6, 77.1, 3.0
2019-02-17 02:04:13,285 : Text to Image: 25.1, 59.38, 74.96, 4.0
2019-02-17 02:04:13,285 : Dev mean Text to Image: 24.84, 58.46000000000001, 75.176, 4.0
2019-02-17 02:04:13,285 : Dev mean Image to text: 29.900000000000006, 62.959999999999994, 77.66, 3.0
2019-02-17 02:04:13,286 : start epoch
2019-02-17 02:04:59,389 : samples : 64000
2019-02-17 02:05:10,740 : Image to text: 11.56, 31.42, 44.8, 14.0
2019-02-17 02:05:18,156 : Text to Image: 9.996, 28.352, 40.548, 16.0
2019-02-17 02:06:01,609 : samples : 128000
2019-02-17 02:06:14,283 : Image to text: 11.52, 31.44, 44.44, 14.0
2019-02-17 02:06:24,365 : Text to Image: 9.868, 28.088, 40.636, 16.0
2019-02-17 02:07:10,725 : samples : 192000
2019-02-17 02:07:23,347 : Image to text: 10.34, 31.3, 44.38, 13.0
2019-02-17 02:07:32,846 : Text to Image: 9.72, 28.14, 40.568, 16.0
2019-02-17 02:08:15,460 : samples : 256000
2019-02-17 02:08:25,721 : Image to text: 10.9, 31.02, 43.38, 14.0
2019-02-17 02:08:35,011 : Text to Image: 9.616, 28.208, 40.98, 16.0
2019-02-17 02:09:20,383 : samples : 320000
2019-02-17 02:09:33,047 : Image to text: 10.8, 30.8, 44.4, 14.0
2019-02-17 02:09:43,096 : Text to Image: 9.748, 27.932, 39.996, 17.0
2019-02-17 02:10:27,122 : samples : 384000
2019-02-17 02:10:37,310 : Image to text: 11.24, 31.72, 44.96, 13.0
2019-02-17 02:10:44,593 : Text to Image: 10.18, 28.96, 41.36, 16.0
2019-02-17 02:11:38,110 : samples : 448000
2019-02-17 02:11:49,675 : Image to text: 11.2, 31.24, 44.52, 13.0
2019-02-17 02:11:59,568 : Text to Image: 10.096, 28.624, 41.064, 16.0
2019-02-17 02:12:42,267 : samples : 512000
2019-02-17 02:12:52,643 : Image to text: 11.72, 32.52, 45.82, 13.0
2019-02-17 02:13:00,030 : Text to Image: 10.252, 28.68, 41.212, 16.0
2019-02-17 02:13:36,650 : Epoch 7 finished
2019-02-17 02:13:37,128 : Image to text: 29.6, 62.2, 78.5, 3.0
2019-02-17 02:13:37,511 : Text to Image: 24.36, 59.44, 76.46, 4.0
2019-02-17 02:13:37,960 : Image to text: 28.2, 60.9, 77.1, 3.0
2019-02-17 02:13:38,320 : Text to Image: 24.1, 58.36, 75.84, 4.0
2019-02-17 02:13:38,769 : Image to text: 28.0, 62.3, 77.4, 3.0
2019-02-17 02:13:39,139 : Text to Image: 25.38, 58.6, 75.6, 4.0
2019-02-17 02:13:39,587 : Image to text: 30.0, 65.2, 79.7, 3.0
2019-02-17 02:13:39,958 : Text to Image: 25.18, 58.86, 75.78, 4.0
2019-02-17 02:13:40,413 : Image to text: 28.4, 64.1, 77.5, 3.0
2019-02-17 02:13:40,787 : Text to Image: 25.34, 59.28, 75.18, 4.0
2019-02-17 02:13:40,787 : Dev mean Text to Image: 24.872, 58.908, 75.77199999999999, 4.0
2019-02-17 02:13:40,787 : Dev mean Image to text: 28.839999999999996, 62.94, 78.03999999999999, 3.0
2019-02-17 02:13:40,787 : start epoch
2019-02-17 02:14:23,739 : samples : 64000
2019-02-17 02:14:34,070 : Image to text: 12.12, 33.26, 46.08, 13.0
2019-02-17 02:14:41,458 : Text to Image: 10.02, 28.456, 41.136, 16.0
2019-02-17 02:15:24,470 : samples : 128000
2019-02-17 02:15:34,884 : Image to text: 11.7, 31.58, 45.32, 13.0
2019-02-17 02:15:42,466 : Text to Image: 9.696, 28.176, 40.36, 16.0
2019-02-17 02:16:25,922 : samples : 192000
2019-02-17 02:16:36,277 : Image to text: 11.22, 31.2, 45.24, 13.0
2019-02-17 02:16:43,741 : Text to Image: 10.072, 28.596, 40.856, 16.0
2019-02-17 02:17:26,719 : samples : 256000
2019-02-17 02:17:39,429 : Image to text: 11.34, 32.2, 45.46, 13.0
2019-02-17 02:17:49,693 : Text to Image: 10.068, 28.592, 41.136, 16.0
2019-02-17 02:18:33,634 : samples : 320000
2019-02-17 02:18:43,897 : Image to text: 11.86, 32.74, 46.46, 12.0
2019-02-17 02:18:53,023 : Text to Image: 10.372, 29.216, 41.804, 15.0
2019-02-17 02:19:36,898 : samples : 384000
2019-02-17 02:19:48,325 : Image to text: 11.16, 32.16, 46.48, 12.0
2019-02-17 02:19:55,398 : Text to Image: 9.976, 28.576, 40.916, 16.0
2019-02-17 02:20:39,336 : samples : 448000
2019-02-17 02:20:49,512 : Image to text: 11.6, 31.82, 45.66, 13.0
2019-02-17 02:20:58,795 : Text to Image: 9.792, 28.204, 40.432, 16.0
2019-02-17 02:21:41,762 : samples : 512000
2019-02-17 02:21:54,035 : Image to text: 11.92, 32.5, 45.5, 13.0
2019-02-17 02:22:03,966 : Text to Image: 10.084, 28.36, 40.888, 16.0
2019-02-17 02:22:43,020 : Epoch 8 finished
2019-02-17 02:22:43,928 : Image to text: 31.0, 64.7, 79.2, 3.0
2019-02-17 02:22:44,675 : Text to Image: 23.98, 59.06, 75.94, 4.0
2019-02-17 02:22:45,586 : Image to text: 30.2, 61.5, 78.1, 3.0
2019-02-17 02:22:46,322 : Text to Image: 24.64, 57.64, 74.86, 4.0
2019-02-17 02:22:47,232 : Image to text: 29.5, 63.8, 79.7, 3.0
2019-02-17 02:22:47,955 : Text to Image: 24.4, 58.42, 75.84, 4.0
2019-02-17 02:22:48,855 : Image to text: 31.9, 66.6, 78.8, 3.0
2019-02-17 02:22:49,584 : Text to Image: 24.96, 59.0, 74.96, 4.0
2019-02-17 02:22:50,468 : Image to text: 30.8, 64.9, 78.7, 3.0
2019-02-17 02:22:51,203 : Text to Image: 25.4, 58.16, 74.5, 4.0
2019-02-17 02:22:51,203 : Dev mean Text to Image: 24.676000000000002, 58.455999999999996, 75.22, 4.0
2019-02-17 02:22:51,203 : Dev mean Image to text: 30.68, 64.3, 78.9, 3.0
2019-02-17 02:22:51,204 : start epoch
2019-02-17 02:23:36,513 : samples : 64000
2019-02-17 02:23:49,084 : Image to text: 10.6, 31.94, 45.14, 13.0
2019-02-17 02:23:59,054 : Text to Image: 9.764, 27.924, 40.372, 16.0
2019-02-17 02:24:44,789 : samples : 128000
2019-02-17 02:24:57,353 : Image to text: 11.6, 32.68, 46.26, 12.0
2019-02-17 02:25:07,326 : Text to Image: 9.852, 27.652, 40.224, 16.0
2019-02-17 02:25:52,564 : samples : 192000
2019-02-17 02:26:05,151 : Image to text: 11.96, 33.74, 46.54, 12.0
2019-02-17 02:26:15,150 : Text to Image: 10.084, 28.496, 41.228, 16.0
2019-02-17 02:27:01,042 : samples : 256000
2019-02-17 02:27:13,590 : Image to text: 11.2, 32.16, 45.44, 13.0
2019-02-17 02:27:23,610 : Text to Image: 10.136, 28.268, 40.708, 16.0
2019-02-17 02:28:09,182 : samples : 320000
2019-02-17 02:28:22,493 : Image to text: 11.74, 31.54, 45.24, 13.0
2019-02-17 02:28:33,027 : Text to Image: 10.216, 28.344, 40.896, 16.0
2019-02-17 02:29:20,807 : samples : 384000
2019-02-17 02:29:33,400 : Image to text: 11.66, 32.24, 45.52, 13.0
2019-02-17 02:29:43,410 : Text to Image: 10.376, 29.14, 41.58, 15.0
2019-02-17 02:30:28,866 : samples : 448000
2019-02-17 02:30:41,433 : Image to text: 11.86, 32.5, 45.28, 13.0
2019-02-17 02:30:51,486 : Text to Image: 9.876, 28.18, 40.628, 16.0
2019-02-17 02:31:36,149 : samples : 512000
2019-02-17 02:31:48,804 : Image to text: 11.64, 32.42, 44.94, 13.0
2019-02-17 02:31:58,851 : Text to Image: 9.94, 28.78, 41.144, 16.0
2019-02-17 02:32:37,723 : Epoch 9 finished
2019-02-17 02:32:38,638 : Image to text: 30.1, 65.1, 79.6, 3.0
2019-02-17 02:32:39,448 : Text to Image: 25.62, 60.16, 76.76, 4.0
2019-02-17 02:32:40,457 : Image to text: 29.9, 62.6, 79.5, 3.0
2019-02-17 02:32:41,236 : Text to Image: 24.88, 58.2, 75.28, 4.0
2019-02-17 02:32:42,205 : Image to text: 28.0, 62.7, 77.9, 3.0
2019-02-17 02:32:42,984 : Text to Image: 25.78, 59.9, 76.06, 4.0
2019-02-17 02:32:44,006 : Image to text: 29.9, 64.0, 79.5, 3.0
2019-02-17 02:32:44,761 : Text to Image: 26.7, 59.3, 75.82, 4.0
2019-02-17 02:32:45,784 : Image to text: 30.9, 64.0, 78.4, 3.0
2019-02-17 02:32:46,578 : Text to Image: 26.74, 59.44, 75.24, 4.0
2019-02-17 02:32:46,578 : Dev mean Text to Image: 25.944000000000003, 59.4, 75.83200000000001, 4.0
2019-02-17 02:32:46,579 : Dev mean Image to text: 29.76, 63.67999999999999, 78.98, 3.0
2019-02-17 02:32:46,579 : start epoch
2019-02-17 02:33:31,704 : samples : 64000
2019-02-17 02:33:41,995 : Image to text: 11.86, 32.3, 45.52, 13.0
2019-02-17 02:33:49,364 : Text to Image: 10.316, 28.904, 41.26, 16.0
2019-02-17 02:34:32,136 : samples : 128000
2019-02-17 02:34:42,477 : Image to text: 11.38, 31.58, 45.52, 13.0
2019-02-17 02:34:49,816 : Text to Image: 10.308, 29.244, 41.584, 15.0
2019-02-17 02:35:33,636 : samples : 192000
2019-02-17 02:35:46,599 : Image to text: 12.56, 32.82, 46.0, 13.0
2019-02-17 02:35:57,034 : Text to Image: 10.528, 29.46, 41.628, 15.0
2019-02-17 02:36:43,187 : samples : 256000
2019-02-17 02:36:56,108 : Image to text: 10.82, 32.54, 46.26, 12.0
2019-02-17 02:37:06,455 : Text to Image: 10.252, 28.8, 41.212, 16.0
2019-02-17 02:37:53,232 : samples : 320000
2019-02-17 02:38:06,099 : Image to text: 11.96, 32.98, 46.44, 12.0
2019-02-17 02:38:16,612 : Text to Image: 10.468, 29.224, 41.276, 16.0
2019-02-17 02:39:02,700 : samples : 384000
2019-02-17 02:39:15,617 : Image to text: 12.0, 32.76, 45.78, 12.0
2019-02-17 02:39:26,144 : Text to Image: 10.344, 28.816, 41.116, 16.0
2019-02-17 02:40:12,345 : samples : 448000
2019-02-17 02:40:25,295 : Image to text: 11.72, 32.62, 46.48, 12.0
2019-02-17 02:40:35,782 : Text to Image: 10.452, 29.1, 41.44, 16.0
2019-02-17 02:41:21,923 : samples : 512000
2019-02-17 02:41:34,775 : Image to text: 11.78, 31.88, 45.96, 13.0
2019-02-17 02:41:45,238 : Text to Image: 10.392, 29.096, 41.836, 15.0
2019-02-17 02:42:24,732 : Epoch 10 finished
2019-02-17 02:42:25,766 : Image to text: 31.0, 64.5, 79.2, 3.0
2019-02-17 02:42:26,625 : Text to Image: 25.68, 61.12, 77.56, 4.0
2019-02-17 02:42:27,674 : Image to text: 28.4, 61.7, 78.8, 3.0
2019-02-17 02:42:28,559 : Text to Image: 24.64, 59.08, 76.14, 4.0
2019-02-17 02:42:29,673 : Image to text: 29.8, 63.8, 78.5, 3.0
2019-02-17 02:42:30,544 : Text to Image: 25.94, 60.2, 76.54, 4.0
2019-02-17 02:42:31,657 : Image to text: 30.7, 65.4, 79.6, 3.0
2019-02-17 02:42:32,585 : Text to Image: 26.14, 60.12, 76.82, 4.0
2019-02-17 02:42:33,674 : Image to text: 30.3, 64.7, 78.7, 3.0
2019-02-17 02:42:34,506 : Text to Image: 26.42, 59.98, 76.16, 4.0
2019-02-17 02:42:34,507 : Dev mean Text to Image: 25.764000000000003, 60.099999999999994, 76.644, 4.0
2019-02-17 02:42:34,507 : Dev mean Image to text: 30.04, 64.02, 78.96, 3.0
2019-02-17 02:42:34,507 : start epoch
2019-02-17 02:43:20,868 : samples : 64000
2019-02-17 02:43:31,597 : Image to text: 11.18, 32.14, 46.32, 13.0
2019-02-17 02:43:39,198 : Text to Image: 10.108, 29.4, 41.552, 16.0
2019-02-17 02:44:22,762 : samples : 128000
2019-02-17 02:44:33,180 : Image to text: 12.2, 31.94, 45.74, 13.0
2019-02-17 02:44:40,729 : Text to Image: 10.268, 28.768, 41.356, 16.0
2019-02-17 02:45:23,478 : samples : 192000
2019-02-17 02:45:35,476 : Image to text: 11.44, 33.18, 46.92, 12.0
2019-02-17 02:45:44,021 : Text to Image: 10.128, 28.896, 41.528, 16.0
2019-02-17 02:46:31,579 : samples : 256000
2019-02-17 02:46:42,120 : Image to text: 12.18, 33.2, 47.08, 12.0
2019-02-17 02:46:49,673 : Text to Image: 10.416, 29.28, 42.008, 15.0
2019-02-17 02:47:32,838 : samples : 320000
2019-02-17 02:47:43,363 : Image to text: 12.34, 32.88, 46.24, 13.0
2019-02-17 02:47:50,902 : Text to Image: 10.56, 29.312, 42.124, 15.0
2019-02-17 02:48:33,779 : samples : 384000
2019-02-17 02:48:44,398 : Image to text: 11.98, 33.18, 47.16, 12.0
2019-02-17 02:48:51,986 : Text to Image: 10.608, 29.868, 42.4, 15.0
2019-02-17 02:49:35,202 : samples : 448000
2019-02-17 02:49:45,655 : Image to text: 11.52, 32.72, 46.62, 12.0
2019-02-17 02:49:53,247 : Text to Image: 10.276, 29.156, 41.7, 15.0
2019-02-17 02:50:35,979 : samples : 512000
2019-02-17 02:50:46,492 : Image to text: 11.88, 33.38, 46.44, 12.0
2019-02-17 02:50:54,031 : Text to Image: 10.54, 29.544, 42.396, 15.0
2019-02-17 02:51:30,690 : Epoch 11 finished
2019-02-17 02:51:31,168 : Image to text: 29.3, 65.5, 80.0, 3.0
2019-02-17 02:51:31,518 : Text to Image: 25.04, 60.1, 76.7, 4.0
2019-02-17 02:51:32,000 : Image to text: 28.1, 63.2, 78.0, 3.0
2019-02-17 02:51:32,349 : Text to Image: 24.62, 58.42, 75.38, 4.0
2019-02-17 02:51:32,829 : Image to text: 30.4, 62.6, 77.1, 3.0
2019-02-17 02:51:33,172 : Text to Image: 25.18, 59.22, 76.14, 4.0
2019-02-17 02:51:33,614 : Image to text: 30.9, 67.0, 79.3, 3.0
2019-02-17 02:51:33,915 : Text to Image: 25.82, 59.2, 76.3, 4.0
2019-02-17 02:51:34,314 : Image to text: 31.3, 62.9, 77.2, 3.0
2019-02-17 02:51:34,614 : Text to Image: 26.1, 60.16, 75.58, 4.0
2019-02-17 02:51:34,614 : Dev mean Text to Image: 25.351999999999997, 59.42, 76.02, 4.0
2019-02-17 02:51:34,614 : Dev mean Image to text: 30.0, 64.24000000000001, 78.32, 3.0
2019-02-17 02:51:34,614 : start epoch
2019-02-17 02:52:17,451 : samples : 64000
2019-02-17 02:52:27,917 : Image to text: 11.56, 32.8, 46.82, 12.0
2019-02-17 02:52:35,487 : Text to Image: 10.532, 29.324, 41.856, 15.0
2019-02-17 02:53:18,115 : samples : 128000
2019-02-17 02:53:28,699 : Image to text: 11.86, 33.3, 46.98, 12.0
2019-02-17 02:53:36,282 : Text to Image: 10.22, 28.856, 41.08, 16.0
2019-02-17 02:54:18,994 : samples : 192000
2019-02-17 02:54:29,530 : Image to text: 12.06, 33.78, 46.7, 12.0
2019-02-17 02:54:37,144 : Text to Image: 10.596, 29.372, 42.068, 15.0
2019-02-17 02:55:20,167 : samples : 256000
2019-02-17 02:55:30,685 : Image to text: 12.04, 33.76, 46.44, 13.0
2019-02-17 02:55:38,198 : Text to Image: 10.524, 29.228, 42.012, 15.0
2019-02-17 02:56:21,092 : samples : 320000
2019-02-17 02:56:31,586 : Image to text: 12.24, 32.7, 47.0, 12.0
2019-02-17 02:56:39,210 : Text to Image: 10.372, 29.28, 41.992, 15.0
2019-02-17 02:57:22,245 : samples : 384000
2019-02-17 02:57:32,763 : Image to text: 11.32, 32.34, 45.96, 13.0
2019-02-17 02:57:40,309 : Text to Image: 10.452, 29.044, 41.636, 15.0
2019-02-17 02:58:23,571 : samples : 448000
2019-02-17 02:58:34,113 : Image to text: 12.44, 33.98, 47.58, 12.0
2019-02-17 02:58:41,692 : Text to Image: 10.796, 29.6, 42.196, 15.0
2019-02-17 02:59:25,033 : samples : 512000
2019-02-17 02:59:35,537 : Image to text: 11.84, 33.38, 46.86, 12.0
2019-02-17 02:59:43,109 : Text to Image: 10.612, 29.548, 42.196, 15.0
2019-02-17 03:00:20,087 : Epoch 12 finished
2019-02-17 03:00:20,572 : Image to text: 29.3, 63.6, 79.4, 3.0
2019-02-17 03:00:20,930 : Text to Image: 24.98, 61.06, 77.4, 4.0
2019-02-17 03:00:21,419 : Image to text: 28.3, 63.1, 77.9, 3.0
2019-02-17 03:00:21,777 : Text to Image: 25.38, 59.8, 76.62, 4.0
2019-02-17 03:00:22,267 : Image to text: 29.4, 64.0, 77.4, 3.0
2019-02-17 03:00:22,627 : Text to Image: 26.1, 60.38, 77.28, 4.0
2019-02-17 03:00:23,095 : Image to text: 31.2, 63.6, 78.8, 3.0
2019-02-17 03:00:23,437 : Text to Image: 26.92, 60.5, 76.74, 4.0
2019-02-17 03:00:23,929 : Image to text: 31.2, 64.7, 78.7, 3.0
2019-02-17 03:00:24,285 : Text to Image: 26.04, 59.66, 76.18, 4.0
2019-02-17 03:00:24,286 : Dev mean Text to Image: 25.884, 60.28, 76.84400000000001, 4.0
2019-02-17 03:00:24,286 : Dev mean Image to text: 29.880000000000003, 63.8, 78.44, 3.0
2019-02-17 03:00:24,286 : start epoch
2019-02-17 03:01:06,857 : samples : 64000
2019-02-17 03:01:17,292 : Image to text: 11.94, 33.5, 46.64, 12.0
2019-02-17 03:01:24,867 : Text to Image: 10.708, 29.824, 42.544, 15.0
2019-02-17 03:02:07,724 : samples : 128000
2019-02-17 03:02:18,098 : Image to text: 11.48, 32.94, 46.72, 12.0
2019-02-17 03:02:25,519 : Text to Image: 10.236, 28.924, 41.24, 16.0
2019-02-17 03:03:20,253 : samples : 192000
2019-02-17 03:03:30,779 : Image to text: 11.6, 33.3, 46.16, 12.0
2019-02-17 03:03:38,408 : Text to Image: 10.324, 29.256, 42.216, 15.0
2019-02-17 03:04:21,594 : samples : 256000
2019-02-17 03:04:32,110 : Image to text: 12.7, 34.46, 47.88, 11.0
2019-02-17 03:04:39,681 : Text to Image: 10.66, 30.036, 42.676, 15.0
2019-02-17 03:05:22,101 : samples : 320000
2019-02-17 03:05:32,594 : Image to text: 12.02, 33.26, 46.64, 12.0
2019-02-17 03:05:40,195 : Text to Image: 10.48, 29.64, 42.144, 15.0
2019-02-17 03:06:22,656 : samples : 384000
2019-02-17 03:06:33,136 : Image to text: 11.9, 33.36, 46.86, 12.0
2019-02-17 03:06:40,672 : Text to Image: 10.324, 29.416, 42.048, 15.0
2019-02-17 03:07:23,645 : samples : 448000
2019-02-17 03:07:34,154 : Image to text: 12.68, 33.86, 47.4, 12.0
2019-02-17 03:07:41,766 : Text to Image: 10.864, 29.904, 42.184, 15.0
2019-02-17 03:08:24,566 : samples : 512000
2019-02-17 03:08:35,086 : Image to text: 12.12, 33.56, 47.76, 12.0
2019-02-17 03:08:42,719 : Text to Image: 10.736, 29.508, 42.42, 15.0
2019-02-17 03:09:19,001 : Epoch 13 finished
2019-02-17 03:09:19,479 : Image to text: 31.6, 66.1, 80.6, 3.0
2019-02-17 03:09:19,836 : Text to Image: 25.08, 61.8, 78.44, 4.0
2019-02-17 03:09:20,305 : Image to text: 29.1, 63.1, 78.0, 3.0
2019-02-17 03:09:20,660 : Text to Image: 25.42, 59.72, 76.64, 4.0
2019-02-17 03:09:21,134 : Image to text: 30.8, 63.8, 78.6, 3.0
2019-02-17 03:09:21,487 : Text to Image: 26.14, 60.64, 77.2, 4.0
2019-02-17 03:09:21,973 : Image to text: 30.1, 64.6, 77.8, 3.0
2019-02-17 03:09:22,284 : Text to Image: 26.6, 61.3, 77.52, 4.0
2019-02-17 03:09:22,686 : Image to text: 30.8, 65.1, 79.0, 3.0
2019-02-17 03:09:22,986 : Text to Image: 26.82, 60.74, 76.52, 4.0
2019-02-17 03:09:22,986 : Dev mean Text to Image: 26.012000000000004, 60.84, 77.264, 4.0
2019-02-17 03:09:22,986 : Dev mean Image to text: 30.48, 64.53999999999999, 78.8, 3.0
2019-02-17 03:09:22,987 : start epoch
2019-02-17 03:10:05,937 : samples : 64000
2019-02-17 03:10:16,312 : Image to text: 11.94, 33.96, 47.1, 12.0
2019-02-17 03:10:23,869 : Text to Image: 10.888, 29.76, 42.704, 15.0
2019-02-17 03:11:06,385 : samples : 128000
2019-02-17 03:11:16,879 : Image to text: 11.44, 32.46, 46.46, 12.0
2019-02-17 03:11:24,416 : Text to Image: 10.48, 29.28, 42.176, 15.0
2019-02-17 03:12:07,190 : samples : 192000
2019-02-17 03:12:17,674 : Image to text: 12.5, 33.48, 47.34, 12.0
2019-02-17 03:12:25,254 : Text to Image: 10.668, 29.584, 42.04, 15.0
2019-02-17 03:13:08,699 : samples : 256000
2019-02-17 03:13:19,219 : Image to text: 11.84, 34.04, 47.42, 12.0
2019-02-17 03:13:26,745 : Text to Image: 10.668, 29.584, 42.444, 15.0
2019-02-17 03:14:09,653 : samples : 320000
2019-02-17 03:14:20,117 : Image to text: 12.52, 34.66, 47.84, 11.0
2019-02-17 03:14:27,694 : Text to Image: 10.52, 29.684, 42.228, 15.0
2019-02-17 03:15:09,987 : samples : 384000
2019-02-17 03:15:20,493 : Image to text: 11.86, 33.86, 47.66, 12.0
2019-02-17 03:15:28,010 : Text to Image: 10.748, 29.86, 42.548, 15.0
2019-02-17 03:16:10,558 : samples : 448000
2019-02-17 03:16:21,117 : Image to text: 12.26, 33.9, 47.52, 12.0
2019-02-17 03:16:28,843 : Text to Image: 10.784, 29.456, 42.224, 15.0
2019-02-17 03:17:11,584 : samples : 512000
2019-02-17 03:17:22,172 : Image to text: 12.68, 34.46, 47.72, 12.0
2019-02-17 03:17:29,747 : Text to Image: 10.916, 29.784, 42.6, 15.0
2019-02-17 03:18:06,008 : Epoch 14 finished
2019-02-17 03:18:06,483 : Image to text: 28.9, 65.5, 80.4, 3.0
2019-02-17 03:18:06,824 : Text to Image: 24.76, 60.66, 77.52, 4.0
2019-02-17 03:18:07,302 : Image to text: 29.2, 61.8, 77.2, 3.0
2019-02-17 03:18:07,655 : Text to Image: 25.82, 58.96, 76.46, 4.0
2019-02-17 03:18:08,133 : Image to text: 30.1, 62.8, 77.4, 3.0
2019-02-17 03:18:08,489 : Text to Image: 25.8, 60.12, 77.2, 4.0
2019-02-17 03:18:08,967 : Image to text: 31.4, 64.9, 79.0, 3.0
2019-02-17 03:18:09,309 : Text to Image: 26.96, 61.02, 76.84, 4.0
2019-02-17 03:18:09,721 : Image to text: 31.5, 63.4, 78.2, 3.0
2019-02-17 03:18:10,041 : Text to Image: 26.12, 60.92, 76.1, 4.0
2019-02-17 03:18:10,041 : Dev mean Text to Image: 25.892, 60.336, 76.82400000000001, 4.0
2019-02-17 03:18:10,041 : Dev mean Image to text: 30.220000000000002, 63.68, 78.44, 3.0
2019-02-17 03:18:10,041 : start epoch
2019-02-17 03:18:52,319 : samples : 64000
2019-02-17 03:19:02,739 : Image to text: 11.78, 33.76, 47.1, 12.0
2019-02-17 03:19:10,299 : Text to Image: 10.852, 29.956, 42.672, 15.0
2019-02-17 03:19:53,434 : samples : 128000
2019-02-17 03:20:05,599 : Image to text: 11.84, 32.94, 46.0, 13.0
2019-02-17 03:20:14,391 : Text to Image: 10.376, 29.14, 42.144, 15.0
2019-02-17 03:21:01,262 : samples : 192000
2019-02-17 03:21:11,808 : Image to text: 12.2, 33.98, 47.4, 12.0
2019-02-17 03:21:19,381 : Text to Image: 10.552, 29.184, 42.0, 15.0
2019-02-17 03:22:02,212 : samples : 256000
2019-02-17 03:22:12,683 : Image to text: 12.06, 34.14, 47.76, 12.0
2019-02-17 03:22:20,241 : Text to Image: 10.956, 30.012, 42.696, 15.0
2019-02-17 03:23:02,691 : samples : 320000
2019-02-17 03:23:13,226 : Image to text: 11.92, 33.0, 46.3, 12.0
2019-02-17 03:23:20,823 : Text to Image: 10.712, 29.696, 42.1, 15.0
2019-02-17 03:24:03,592 : samples : 384000
2019-02-17 03:24:14,057 : Image to text: 12.08, 33.76, 47.32, 12.0
2019-02-17 03:24:21,613 : Text to Image: 10.696, 29.92, 42.416, 15.0
2019-02-17 03:25:04,926 : samples : 448000
2019-02-17 03:25:15,456 : Image to text: 12.3, 33.82, 47.24, 12.0
2019-02-17 03:25:22,996 : Text to Image: 10.776, 29.996, 42.528, 15.0
2019-02-17 03:26:06,177 : samples : 512000
2019-02-17 03:26:16,696 : Image to text: 12.54, 33.96, 47.8, 11.0
2019-02-17 03:26:24,267 : Text to Image: 10.78, 29.752, 42.224, 15.0
2019-02-17 03:27:00,966 : Epoch 15 finished
2019-02-17 03:27:01,444 : Image to text: 32.2, 67.9, 80.2, 3.0
2019-02-17 03:27:01,799 : Text to Image: 25.7, 61.28, 78.1, 4.0
2019-02-17 03:27:02,285 : Image to text: 30.5, 63.9, 77.9, 3.0
2019-02-17 03:27:02,634 : Text to Image: 25.36, 59.06, 76.12, 4.0
2019-02-17 03:27:03,126 : Image to text: 29.7, 65.4, 79.5, 3.0
2019-02-17 03:27:03,468 : Text to Image: 25.66, 59.72, 76.82, 4.0
2019-02-17 03:27:03,956 : Image to text: 31.8, 67.0, 80.7, 3.0
2019-02-17 03:27:04,297 : Text to Image: 26.68, 60.98, 77.2, 4.0
2019-02-17 03:27:04,785 : Image to text: 31.6, 65.7, 79.5, 3.0
2019-02-17 03:27:05,126 : Text to Image: 25.88, 60.46, 76.6, 4.0
2019-02-17 03:27:05,126 : Dev mean Text to Image: 25.856, 60.3, 76.96799999999999, 4.0
2019-02-17 03:27:05,126 : Dev mean Image to text: 31.159999999999997, 65.97999999999999, 79.56, 3.0
2019-02-17 03:27:05,127 : start epoch
2019-02-17 03:27:48,013 : samples : 64000
2019-02-17 03:27:58,425 : Image to text: 11.82, 33.86, 47.9, 12.0
2019-02-17 03:28:05,976 : Text to Image: 10.508, 29.596, 41.892, 15.0
2019-02-17 03:28:48,460 : samples : 128000
2019-02-17 03:28:58,988 : Image to text: 12.42, 33.14, 46.3, 12.0
2019-02-17 03:29:06,603 : Text to Image: 10.788, 29.512, 42.32, 15.0
2019-02-17 03:29:49,544 : samples : 192000
2019-02-17 03:30:00,016 : Image to text: 12.5, 34.24, 48.32, 11.0
2019-02-17 03:30:07,580 : Text to Image: 11.024, 30.044, 42.908, 15.0
2019-02-17 03:30:51,025 : samples : 256000
2019-02-17 03:31:01,490 : Image to text: 12.44, 34.46, 48.6, 11.0
2019-02-17 03:31:09,033 : Text to Image: 11.068, 30.536, 43.296, 14.0
2019-02-17 03:31:51,431 : samples : 320000
2019-02-17 03:32:01,859 : Image to text: 12.2, 34.44, 47.04, 12.0
2019-02-17 03:32:09,419 : Text to Image: 10.424, 29.616, 42.516, 15.0
2019-02-17 03:32:52,170 : samples : 384000
2019-02-17 03:33:02,630 : Image to text: 11.86, 34.74, 47.92, 12.0
2019-02-17 03:33:10,211 : Text to Image: 10.788, 30.06, 42.524, 15.0
2019-02-17 03:33:52,644 : samples : 448000
2019-02-17 03:34:03,180 : Image to text: 12.62, 33.46, 47.0, 12.0
2019-02-17 03:34:10,761 : Text to Image: 10.568, 29.88, 42.928, 14.0
2019-02-17 03:34:53,303 : samples : 512000
2019-02-17 03:35:03,796 : Image to text: 11.96, 34.14, 48.44, 11.0
2019-02-17 03:35:11,328 : Text to Image: 11.288, 30.392, 43.424, 14.0
2019-02-17 03:35:48,090 : Epoch 16 finished
2019-02-17 03:35:48,573 : Image to text: 32.1, 66.3, 80.7, 3.0
2019-02-17 03:35:48,916 : Text to Image: 26.2, 61.8, 78.34, 4.0
2019-02-17 03:35:49,378 : Image to text: 30.0, 65.2, 78.4, 3.0
2019-02-17 03:35:49,754 : Text to Image: 25.64, 60.02, 77.08, 4.0
2019-02-17 03:35:50,232 : Image to text: 31.2, 65.3, 79.2, 3.0
2019-02-17 03:35:50,585 : Text to Image: 26.6, 60.4, 76.68, 4.0
2019-02-17 03:35:51,051 : Image to text: 31.9, 66.0, 80.2, 3.0
2019-02-17 03:35:51,413 : Text to Image: 26.8, 62.0, 77.72, 4.0
2019-02-17 03:35:51,883 : Image to text: 32.0, 64.9, 80.2, 3.0
2019-02-17 03:35:52,227 : Text to Image: 26.4, 60.78, 76.74, 4.0
2019-02-17 03:35:52,227 : Dev mean Text to Image: 26.328000000000003, 61.0, 77.312, 4.0
2019-02-17 03:35:52,227 : Dev mean Image to text: 31.439999999999998, 65.54, 79.74, 3.0
2019-02-17 03:35:52,227 : start epoch
2019-02-17 03:36:34,750 : samples : 64000
2019-02-17 03:36:45,175 : Image to text: 11.78, 34.12, 47.46, 12.0
2019-02-17 03:36:52,631 : Text to Image: 10.964, 30.2, 43.324, 14.0
2019-02-17 03:37:44,227 : samples : 128000
2019-02-17 03:37:55,548 : Image to text: 12.56, 34.86, 48.32, 11.0
2019-02-17 03:38:03,110 : Text to Image: 10.772, 30.42, 42.884, 14.0
2019-02-17 03:38:46,361 : samples : 192000
2019-02-17 03:38:56,887 : Image to text: 12.32, 34.62, 47.5, 12.0
2019-02-17 03:39:04,473 : Text to Image: 10.672, 29.8, 42.38, 15.0
2019-02-17 03:39:46,809 : samples : 256000
2019-02-17 03:39:57,289 : Image to text: 12.04, 34.5, 47.76, 12.0
2019-02-17 03:40:04,863 : Text to Image: 10.896, 29.92, 42.692, 15.0
2019-02-17 03:40:47,860 : samples : 320000
2019-02-17 03:40:58,312 : Image to text: 12.68, 34.48, 48.3, 12.0
2019-02-17 03:41:05,869 : Text to Image: 10.724, 29.768, 42.46, 15.0
2019-02-17 03:41:49,039 : samples : 384000
2019-02-17 03:41:59,542 : Image to text: 12.02, 33.48, 47.32, 12.0
2019-02-17 03:42:07,125 : Text to Image: 10.564, 29.428, 42.352, 15.0
2019-02-17 03:42:49,414 : samples : 448000
2019-02-17 03:42:59,890 : Image to text: 12.04, 33.64, 46.8, 12.0
2019-02-17 03:43:07,527 : Text to Image: 10.676, 29.68, 42.444, 15.0
2019-02-17 03:43:50,125 : samples : 512000
2019-02-17 03:44:00,709 : Image to text: 11.52, 33.64, 47.54, 12.0
2019-02-17 03:44:08,301 : Text to Image: 10.652, 29.92, 42.592, 15.0
2019-02-17 03:44:44,985 : Epoch 17 finished
2019-02-17 03:44:45,469 : Image to text: 30.9, 66.5, 80.2, 3.0
2019-02-17 03:44:45,824 : Text to Image: 25.5, 61.74, 77.74, 4.0
2019-02-17 03:44:46,300 : Image to text: 28.1, 64.1, 79.0, 3.0
2019-02-17 03:44:46,652 : Text to Image: 25.3, 59.46, 76.64, 4.0
2019-02-17 03:44:47,134 : Image to text: 29.2, 64.6, 79.5, 3.0
2019-02-17 03:44:47,476 : Text to Image: 26.28, 60.56, 76.94, 4.0
2019-02-17 03:44:47,962 : Image to text: 31.6, 66.4, 79.6, 3.0
2019-02-17 03:44:48,302 : Text to Image: 26.8, 61.6, 77.34, 4.0
2019-02-17 03:44:48,784 : Image to text: 31.2, 66.1, 78.5, 3.0
2019-02-17 03:44:49,135 : Text to Image: 26.18, 61.1, 76.36, 4.0
2019-02-17 03:44:49,135 : Dev mean Text to Image: 26.012, 60.892, 77.004, 4.0
2019-02-17 03:44:49,135 : Dev mean Image to text: 30.200000000000003, 65.53999999999999, 79.36, 3.0
2019-02-17 03:44:52,938 : 
Test scores | Image to text:             31.32, 64.46, 79.44, 3.0
2019-02-17 03:44:52,938 : Test scores | Text to image:             25.912, 60.66, 76.868, 4.0

2019-02-17 03:44:53,054 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-17 03:44:53,279 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-17 03:44:53,992 : loading BERT model bert-base-uncased
2019-02-17 03:44:53,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:44:54,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:44:54,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplk82mf8v
2019-02-17 03:44:56,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:44:57,966 : Computing embeddings for train/dev/test
2019-02-17 03:46:32,953 : Computed embeddings
2019-02-17 03:46:32,953 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:47:16,351 : [('reg:1e-05', 51.57), ('reg:0.0001', 59.27), ('reg:0.001', 54.96), ('reg:0.01', 46.43)]
2019-02-17 03:47:16,351 : Validation : best param found is reg = 0.0001 with score             59.27
2019-02-17 03:47:16,351 : Evaluating...
2019-02-17 03:47:28,005 : 
Dev acc : 59.3 Test acc : 58.8 for LENGTH classification

2019-02-17 03:47:28,005 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-17 03:47:28,378 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-17 03:47:28,432 : loading BERT model bert-base-uncased
2019-02-17 03:47:28,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:47:28,548 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:47:28,548 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuz85u4xz
2019-02-17 03:47:31,062 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:47:32,537 : Computing embeddings for train/dev/test
2019-02-17 03:49:01,887 : Computed embeddings
2019-02-17 03:49:01,888 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:49:58,022 : [('reg:1e-05', 24.82), ('reg:0.0001', 6.58), ('reg:0.001', 0.81), ('reg:0.01', 0.2)]
2019-02-17 03:49:58,023 : Validation : best param found is reg = 1e-05 with score             24.82
2019-02-17 03:49:58,023 : Evaluating...
2019-02-17 03:50:18,265 : 
Dev acc : 24.8 Test acc : 25.3 for WORDCONTENT classification

2019-02-17 03:50:18,267 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-17 03:50:18,886 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-17 03:50:18,958 : loading BERT model bert-base-uncased
2019-02-17 03:50:18,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:50:18,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:50:18,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsb2cygv9
2019-02-17 03:50:21,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:50:22,969 : Computing embeddings for train/dev/test
2019-02-17 03:51:46,677 : Computed embeddings
2019-02-17 03:51:46,677 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:52:31,898 : [('reg:1e-05', 25.86), ('reg:0.0001', 27.48), ('reg:0.001', 26.92), ('reg:0.01', 23.95)]
2019-02-17 03:52:31,899 : Validation : best param found is reg = 0.0001 with score             27.48
2019-02-17 03:52:31,899 : Evaluating...
2019-02-17 03:52:41,875 : 
Dev acc : 27.5 Test acc : 27.1 for DEPTH classification

2019-02-17 03:52:41,876 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-17 03:52:42,491 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-17 03:52:42,564 : loading BERT model bert-base-uncased
2019-02-17 03:52:42,564 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:52:42,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:52:42,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppigzocqd
2019-02-17 03:52:45,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:52:46,573 : Computing embeddings for train/dev/test
2019-02-17 03:54:05,401 : Computed embeddings
2019-02-17 03:54:07,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:54:42,509 : [('reg:1e-05', 54.37), ('reg:0.0001', 51.54), ('reg:0.001', 51.58), ('reg:0.01', 40.38)]
2019-02-17 03:54:42,509 : Validation : best param found is reg = 1e-05 with score             54.37
2019-02-17 03:54:42,510 : Evaluating...
2019-02-17 03:54:50,687 : 
Dev acc : 54.4 Test acc : 54.2 for TOPCONSTITUENTS classification

2019-02-17 03:54:50,689 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-17 03:54:51,245 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-17 03:54:51,334 : loading BERT model bert-base-uncased
2019-02-17 03:54:51,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:54:51,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:54:51,385 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa2n525rt
2019-02-17 03:54:54,317 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:54:56,580 : Computing embeddings for train/dev/test
2019-02-17 03:56:22,371 : Computed embeddings
2019-02-17 03:56:22,372 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:57:19,170 : [('reg:1e-05', 87.57), ('reg:0.0001', 87.47), ('reg:0.001', 87.59), ('reg:0.01', 85.63)]
2019-02-17 03:57:19,170 : Validation : best param found is reg = 0.001 with score             87.59
2019-02-17 03:57:19,170 : Evaluating...
2019-02-17 03:57:33,598 : 
Dev acc : 87.6 Test acc : 86.7 for BIGRAMSHIFT classification

2019-02-17 03:57:33,599 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-17 03:57:34,042 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-17 03:57:34,114 : loading BERT model bert-base-uncased
2019-02-17 03:57:34,114 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:57:34,253 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:57:34,253 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl_z7hcos
2019-02-17 03:57:36,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:57:38,217 : Computing embeddings for train/dev/test
2019-02-17 03:59:01,170 : Computed embeddings
2019-02-17 03:59:01,170 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 03:59:42,998 : [('reg:1e-05', 89.5), ('reg:0.0001', 89.56), ('reg:0.001', 89.58), ('reg:0.01', 89.76)]
2019-02-17 03:59:42,998 : Validation : best param found is reg = 0.01 with score             89.76
2019-02-17 03:59:42,998 : Evaluating...
2019-02-17 03:59:54,462 : 
Dev acc : 89.8 Test acc : 87.6 for TENSE classification

2019-02-17 03:59:54,463 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-17 03:59:55,135 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-17 03:59:55,204 : loading BERT model bert-base-uncased
2019-02-17 03:59:55,204 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 03:59:55,237 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 03:59:55,238 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplo9b5_j7
2019-02-17 03:59:57,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 03:59:59,251 : Computing embeddings for train/dev/test
2019-02-17 04:01:28,285 : Computed embeddings
2019-02-17 04:01:28,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:02:11,473 : [('reg:1e-05', 81.42), ('reg:0.0001', 81.5), ('reg:0.001', 81.51), ('reg:0.01', 79.55)]
2019-02-17 04:02:11,473 : Validation : best param found is reg = 0.001 with score             81.51
2019-02-17 04:02:11,474 : Evaluating...
2019-02-17 04:02:23,324 : 
Dev acc : 81.5 Test acc : 81.0 for SUBJNUMBER classification

2019-02-17 04:02:23,326 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-17 04:02:23,773 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-17 04:02:23,848 : loading BERT model bert-base-uncased
2019-02-17 04:02:23,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:02:23,980 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:02:23,980 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5r_wp0c4
2019-02-17 04:02:26,490 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:02:27,986 : Computing embeddings for train/dev/test
2019-02-17 04:03:54,024 : Computed embeddings
2019-02-17 04:03:54,024 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:04:52,576 : [('reg:1e-05', 75.88), ('reg:0.0001', 75.9), ('reg:0.001', 75.17), ('reg:0.01', 75.87)]
2019-02-17 04:04:52,576 : Validation : best param found is reg = 0.0001 with score             75.9
2019-02-17 04:04:52,576 : Evaluating...
2019-02-17 04:05:07,669 : 
Dev acc : 75.9 Test acc : 75.9 for OBJNUMBER classification

2019-02-17 04:05:07,670 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-17 04:05:08,325 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-17 04:05:08,407 : loading BERT model bert-base-uncased
2019-02-17 04:05:08,407 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:05:08,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:05:08,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp24494969
2019-02-17 04:05:10,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:05:12,460 : Computing embeddings for train/dev/test
2019-02-17 04:06:52,286 : Computed embeddings
2019-02-17 04:06:52,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:07:24,991 : [('reg:1e-05', 62.74), ('reg:0.0001', 62.78), ('reg:0.001', 62.75), ('reg:0.01', 61.74)]
2019-02-17 04:07:24,991 : Validation : best param found is reg = 0.0001 with score             62.78
2019-02-17 04:07:24,992 : Evaluating...
2019-02-17 04:07:32,513 : 
Dev acc : 62.8 Test acc : 61.2 for ODDMANOUT classification

2019-02-17 04:07:32,514 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-17 04:07:33,162 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-17 04:07:33,244 : loading BERT model bert-base-uncased
2019-02-17 04:07:33,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:07:33,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:07:33,281 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc99900qs
2019-02-17 04:07:35,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:07:37,210 : Computing embeddings for train/dev/test
2019-02-17 04:09:15,102 : Computed embeddings
2019-02-17 04:09:15,102 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:09:58,587 : [('reg:1e-05', 62.31), ('reg:0.0001', 62.29), ('reg:0.001', 61.79), ('reg:0.01', 58.07)]
2019-02-17 04:09:58,587 : Validation : best param found is reg = 1e-05 with score             62.31
2019-02-17 04:09:58,587 : Evaluating...
2019-02-17 04:10:10,911 : 
Dev acc : 62.3 Test acc : 61.2 for COORDINATIONINVERSION classification

2019-02-17 04:10:10,913 : total results: {'STS12': {'MSRpar': {'pearson': (0.31595073731379864, 7.558803380977722e-19), 'spearman': SpearmanrResult(correlation=0.3507040638809848, pvalue=3.9714507140897083e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5575935603843336, 1.7073410073756885e-62), 'spearman': SpearmanrResult(correlation=0.5725552750595269, pvalue=1.53308792654326e-66), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49336720517585514, 1.5673651713872784e-29), 'spearman': SpearmanrResult(correlation=0.5902238890023571, pvalue=1.9911188933723535e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5097771563156831, 7.52352844503072e-51), 'spearman': SpearmanrResult(correlation=0.5336071085313794, pvalue=2.0252427364855171e-56), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5582401104401848, 4.575928216265622e-34), 'spearman': SpearmanrResult(correlation=0.4884534453217214, pvalue=2.580443141559188e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48698575392597104, 'wmean': 0.478341197474811}, 'spearman': {'mean': 0.507108756359194, 'wmean': 0.501433566711508}}}, 'STS13': {'FNWN': {'pearson': (0.2562418521418283, 0.0003722975169509068), 'spearman': SpearmanrResult(correlation=0.2734701485595284, pvalue=0.00014038489993970096), 'nsamples': 189}, 'headlines': {'pearson': (0.6486541597373371, 9.189989167488396e-91), 'spearman': SpearmanrResult(correlation=0.636128120523308, pvalue=2.6476590201321118e-86), 'nsamples': 750}, 'OnWN': {'pearson': (0.5589889484116807, 2.0054232116690815e-47), 'spearman': SpearmanrResult(correlation=0.5501994661327523, pvalue=1.0427125011227551e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.487961653430282, 'wmean': 0.5656754199445075}, 'spearman': {'mean': 0.48659924507186286, 'wmean': 0.5582958993138039}}}, 'STS14': {'deft-forum': {'pearson': (0.34592294519944955, 4.28201422817043e-14), 'spearman': SpearmanrResult(correlation=0.3487129913932867, pvalue=2.5934379125233717e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7385992305902515, 5.798374670524147e-53), 'spearman': SpearmanrResult(correlation=0.7196459411378964, pvalue=3.933222005575042e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5839210339835171, 9.320762298251137e-70), 'spearman': SpearmanrResult(correlation=0.5494007346998849, pvalue=2.3041472469995847e-60), 'nsamples': 750}, 'images': {'pearson': (0.47364153869089815, 3.3518920910544224e-43), 'spearman': SpearmanrResult(correlation=0.47191058458384033, pvalue=7.399996796750927e-43), 'nsamples': 750}, 'OnWN': {'pearson': (0.6656985461655095, 3.512442447297624e-97), 'spearman': SpearmanrResult(correlation=0.6875563420399124, pvalue=4.635648706071364e-106), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6037703115280189, 1.0922525070519205e-75), 'spearman': SpearmanrResult(correlation=0.548651511870599, pvalue=3.5841178970132267e-60), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5685922676929408, 'wmean': 0.5660049779447428}, 'spearman': {'mean': 0.5543130176209033, 'wmean': 0.5509210688970735}}}, 'STS15': {'answers-forums': {'pearson': (0.5245719182638092, 6.7049828787197625e-28), 'spearman': SpearmanrResult(correlation=0.5147618067574177, pvalue=9.243928217784293e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6058567957143661, 2.4572071474216956e-76), 'spearman': SpearmanrResult(correlation=0.6241363429727577, pvalue=3.1828516395414385e-82), 'nsamples': 750}, 'belief': {'pearson': (0.5874884897138652, 3.597296101081283e-36), 'spearman': SpearmanrResult(correlation=0.6151998962123019, pvalue=2.0204993361955903e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6511688841158828, 1.1027526400754407e-91), 'spearman': SpearmanrResult(correlation=0.6538806726095209, pvalue=1.0954748136179296e-92), 'nsamples': 750}, 'images': {'pearson': (0.6498204534175535, 3.446131844872639e-91), 'spearman': SpearmanrResult(correlation=0.6601115411538103, pvalue=4.961797588513171e-95), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6037813082450953, 'wmean': 0.61571908430916}, 'spearman': {'mean': 0.6136180519411617, 'wmean': 0.6257773520552372}}}, 'STS16': {'answer-answer': {'pearson': (0.5253662947487908, 2.0058294772970947e-19), 'spearman': SpearmanrResult(correlation=0.5052147186561342, pvalue=7.351861596622249e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6545604703691251, 7.641143469634103e-32), 'spearman': SpearmanrResult(correlation=0.6575242208239908, pvalue=3.273881386382618e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7183139656308635, 8.696842494834758e-38), 'spearman': SpearmanrResult(correlation=0.7299194447868416, pvalue=1.5232442712024837e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7807919548123804, 2.41961672242791e-51), 'spearman': SpearmanrResult(correlation=0.8119908805576954, pvalue=1.6279586530901575e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.42304662788595127, 1.7547804757258025e-10), 'spearman': SpearmanrResult(correlation=0.4298998295006182, pvalue=8.249168668870685e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6204158626894222, 'wmean': 0.6244273105274775}, 'spearman': {'mean': 0.6269098188650559, 'wmean': 0.6306107765990829}}}, 'MR': {'devacc': 74.26, 'acc': 75.56, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.88, 'acc': 75.44, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.38, 'acc': 86.58, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.97, 'acc': 93.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.52, 'acc': 83.75, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.87, 'acc': 45.7, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.7, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.01, 'acc': 60.35, 'f1': 61.62, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.6, 'acc': 75.34, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.789778892712242, 'pearson': 0.7838590893186084, 'spearman': 0.7140007001331513, 'mse': 0.3956690813288107, 'yhat': array([3.08877657, 3.73863326, 1.56991219, ..., 3.32772398, 4.39358179,        4.97569228]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6229046588663187, 'pearson': 0.6397960231710759, 'spearman': 0.6345147084797617, 'mse': 1.4430591637292234, 'yhat': array([2.88225204, 1.1489056 , 1.24742531, ..., 3.89198362, 4.01768007,        3.73548216]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.24, 'acc': 65.73, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 341.36, 'acc': [(31.32, 64.46, 79.44, 3.0), (25.912, 60.66, 76.868, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 59.27, 'acc': 58.76, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 24.82, 'acc': 25.3, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.48, 'acc': 27.13, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.37, 'acc': 54.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.59, 'acc': 86.7, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.76, 'acc': 87.61, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.51, 'acc': 81.01, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.9, 'acc': 75.86, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.78, 'acc': 61.23, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 62.31, 'acc': 61.19, 'ndev': 10002, 'ntest': 10002}}
2019-02-17 04:10:10,913 : STS12 p=0.4783, STS12 s=0.5014, STS13 p=0.5657, STS13 s=0.5583, STS14 p=0.5660, STS14 s=0.5509, STS15 p=0.6157, STS15 s=0.6258, STS 16 p=0.6244, STS16 s=0.6306, STS B p=0.6398, STS B s=0.6345, STS B m=1.4431, SICK-R p=0.7839, SICK-R s=0.7140, SICK-P m=0.3957
2019-02-17 04:10:10,913 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-17 04:10:10,913 : 0.4783,0.5014,0.5657,0.5583,0.5660,0.5509,0.6157,0.6258,0.6244,0.6306,0.6398,0.6345,1.4431,0.7839,0.7140,0.3957
2019-02-17 04:10:10,913 : MR=75.56, CR=75.44, SUBJ=93.70, MPQA=86.58, SST-B=83.75, SST-F=45.70, TREC=88.40, SICK-E=75.34, SNLI=65.73, MRPC=60.35, MRPC f=61.62
2019-02-17 04:10:10,913 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-17 04:10:10,913 : 75.56,75.44,93.70,86.58,83.75,45.70,88.40,75.34,65.73,60.35,61.62
2019-02-17 04:10:10,913 : COCO r1i2t=31.32, COCO r5i2t=64.46, COCO r10i2t=79.44, COCO medr_i2t=3.00, COCO r1t2i=25.91, COCO r5t2i=60.66, COCO r10t2i=76.87, COCO medr_t2i=4.00
2019-02-17 04:10:10,913 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-17 04:10:10,913 : 31.32,64.46,79.44,3.00,25.91,60.66,76.87,4.00
2019-02-17 04:10:10,913 : SentLen=58.76, WC=25.30, TreeDepth=27.13, TopConst=54.19, BShift=86.70, Tense=87.61, SubjNum=81.01, ObjNum=75.86, SOMO=61.23, CoordInv=61.19, average=61.90
2019-02-17 04:10:10,913 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-17 04:10:10,913 : 58.76,25.30,27.13,54.19,86.70,87.61,81.01,75.86,61.23,61.19,61.90
2019-02-17 04:10:10,913 : ********************************************************************************
2019-02-17 04:10:10,913 : ********************************************************************************
2019-02-17 04:10:10,914 : ********************************************************************************
2019-02-17 04:10:10,914 : layer 12
2019-02-17 04:10:10,914 : ********************************************************************************
2019-02-17 04:10:10,914 : ********************************************************************************
2019-02-17 04:10:10,914 : ********************************************************************************
2019-02-17 04:10:11,010 : ***** Transfer task : STS12 *****


2019-02-17 04:10:11,023 : loading BERT model bert-base-uncased
2019-02-17 04:10:11,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:11,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:11,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp23c1fq6i
2019-02-17 04:10:13,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:16,827 : MSRpar : pearson = 0.3293, spearman = 0.3646
2019-02-17 04:10:17,577 : MSRvid : pearson = 0.4814, spearman = 0.4985
2019-02-17 04:10:18,215 : SMTeuroparl : pearson = 0.4833, spearman = 0.5695
2019-02-17 04:10:19,398 : surprise.OnWN : pearson = 0.5282, spearman = 0.5469
2019-02-17 04:10:20,059 : surprise.SMTnews : pearson = 0.5668, spearman = 0.5071
2019-02-17 04:10:20,059 : ALL (weighted average) : Pearson = 0.4672,             Spearman = 0.4895
2019-02-17 04:10:20,059 : ALL (average) : Pearson = 0.4778,             Spearman = 0.4973

2019-02-17 04:10:20,059 : ***** Transfer task : STS13 (-SMT) *****


2019-02-17 04:10:20,070 : loading BERT model bert-base-uncased
2019-02-17 04:10:20,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:20,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:20,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp55t5n2vm
2019-02-17 04:10:22,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:24,791 : FNWN : pearson = 0.2547, spearman = 0.2773
2019-02-17 04:10:25,687 : headlines : pearson = 0.6675, spearman = 0.6471
2019-02-17 04:10:26,341 : OnWN : pearson = 0.4801, spearman = 0.4808
2019-02-17 04:10:26,342 : ALL (weighted average) : Pearson = 0.5454,             Spearman = 0.5383
2019-02-17 04:10:26,342 : ALL (average) : Pearson = 0.4674,             Spearman = 0.4684

2019-02-17 04:10:26,342 : ***** Transfer task : STS14 *****


2019-02-17 04:10:26,360 : loading BERT model bert-base-uncased
2019-02-17 04:10:26,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:26,381 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:26,381 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2zmr2376
2019-02-17 04:10:28,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:31,089 : deft-forum : pearson = 0.3504, spearman = 0.3455
2019-02-17 04:10:31,820 : deft-news : pearson = 0.7528, spearman = 0.7214
2019-02-17 04:10:32,795 : headlines : pearson = 0.6161, spearman = 0.5727
2019-02-17 04:10:33,767 : images : pearson = 0.4399, spearman = 0.4380
2019-02-17 04:10:34,720 : OnWN : pearson = 0.6272, spearman = 0.6495
2019-02-17 04:10:35,944 : tweet-news : pearson = 0.6140, spearman = 0.5610
2019-02-17 04:10:35,944 : ALL (weighted average) : Pearson = 0.5617,             Spearman = 0.5434
2019-02-17 04:10:35,944 : ALL (average) : Pearson = 0.5667,             Spearman = 0.5480

2019-02-17 04:10:35,944 : ***** Transfer task : STS15 *****


2019-02-17 04:10:35,980 : loading BERT model bert-base-uncased
2019-02-17 04:10:35,980 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:36,000 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:36,000 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsv9ncxrd
2019-02-17 04:10:38,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:40,833 : answers-forums : pearson = 0.5328, spearman = 0.5148
2019-02-17 04:10:41,821 : answers-students : pearson = 0.5998, spearman = 0.6083
2019-02-17 04:10:42,715 : belief : pearson = 0.6006, spearman = 0.6129
2019-02-17 04:10:43,748 : headlines : pearson = 0.6795, spearman = 0.6763
2019-02-17 04:10:44,815 : images : pearson = 0.6061, spearman = 0.6144
2019-02-17 04:10:44,815 : ALL (weighted average) : Pearson = 0.6130,             Spearman = 0.6157
2019-02-17 04:10:44,815 : ALL (average) : Pearson = 0.6038,             Spearman = 0.6053

2019-02-17 04:10:44,815 : ***** Transfer task : STS16 *****


2019-02-17 04:10:44,886 : loading BERT model bert-base-uncased
2019-02-17 04:10:44,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:44,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:44,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb2p0taud
2019-02-17 04:10:47,435 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:49,401 : answer-answer : pearson = 0.5267, spearman = 0.5141
2019-02-17 04:10:49,747 : headlines : pearson = 0.6548, spearman = 0.6601
2019-02-17 04:10:50,184 : plagiarism : pearson = 0.7288, spearman = 0.7331
2019-02-17 04:10:50,892 : postediting : pearson = 0.7815, spearman = 0.8027
2019-02-17 04:10:51,208 : question-question : pearson = 0.4896, spearman = 0.4826
2019-02-17 04:10:51,209 : ALL (weighted average) : Pearson = 0.6387,             Spearman = 0.6410
2019-02-17 04:10:51,209 : ALL (average) : Pearson = 0.6363,             Spearman = 0.6385

2019-02-17 04:10:51,209 : ***** Transfer task : MR *****


2019-02-17 04:10:51,228 : loading BERT model bert-base-uncased
2019-02-17 04:10:51,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:10:51,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:10:51,249 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwszbiwxi
2019-02-17 04:10:53,714 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:10:55,222 : Generating sentence embeddings
2019-02-17 04:11:08,594 : Generated sentence embeddings
2019-02-17 04:11:08,595 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 04:11:25,975 : Best param found at split 1: l2reg = 1e-05                 with score 76.2
2019-02-17 04:11:42,192 : Best param found at split 2: l2reg = 1e-05                 with score 74.0
2019-02-17 04:11:55,663 : Best param found at split 3: l2reg = 1e-05                 with score 71.07
2019-02-17 04:12:08,814 : Best param found at split 4: l2reg = 1e-05                 with score 70.94
2019-02-17 04:12:24,824 : Best param found at split 5: l2reg = 0.001                 with score 75.96
2019-02-17 04:12:25,664 : Dev acc : 73.63 Test acc : 74.68

2019-02-17 04:12:25,665 : ***** Transfer task : CR *****


2019-02-17 04:12:25,675 : loading BERT model bert-base-uncased
2019-02-17 04:12:25,675 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:12:25,707 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:12:25,707 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7r7m8jnd
2019-02-17 04:12:28,323 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:12:29,777 : Generating sentence embeddings
2019-02-17 04:12:33,475 : Generated sentence embeddings
2019-02-17 04:12:33,476 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 04:12:38,666 : Best param found at split 1: l2reg = 0.0001                 with score 77.05
2019-02-17 04:12:43,944 : Best param found at split 2: l2reg = 1e-05                 with score 75.22
2019-02-17 04:12:49,445 : Best param found at split 3: l2reg = 1e-05                 with score 81.26
2019-02-17 04:12:55,218 : Best param found at split 4: l2reg = 0.0001                 with score 78.42
2019-02-17 04:13:00,929 : Best param found at split 5: l2reg = 0.0001                 with score 75.91
2019-02-17 04:13:01,661 : Dev acc : 77.57 Test acc : 73.32

2019-02-17 04:13:01,662 : ***** Transfer task : MPQA *****


2019-02-17 04:13:01,674 : loading BERT model bert-base-uncased
2019-02-17 04:13:01,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:13:01,701 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:13:01,702 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzjvvks45
2019-02-17 04:13:04,218 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:13:06,139 : Generating sentence embeddings
2019-02-17 04:13:09,914 : Generated sentence embeddings
2019-02-17 04:13:09,914 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 04:13:25,893 : Best param found at split 1: l2reg = 0.001                 with score 86.5
2019-02-17 04:13:44,722 : Best param found at split 2: l2reg = 1e-05                 with score 86.34
2019-02-17 04:14:03,241 : Best param found at split 3: l2reg = 1e-05                 with score 86.33
2019-02-17 04:14:22,190 : Best param found at split 4: l2reg = 0.0001                 with score 86.31
2019-02-17 04:14:38,537 : Best param found at split 5: l2reg = 0.01                 with score 85.96
2019-02-17 04:14:39,099 : Dev acc : 86.29 Test acc : 86.33

2019-02-17 04:14:39,100 : ***** Transfer task : SUBJ *****


2019-02-17 04:14:39,118 : loading BERT model bert-base-uncased
2019-02-17 04:14:39,118 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:14:39,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:14:39,138 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ydv2qex
2019-02-17 04:14:41,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:14:43,100 : Generating sentence embeddings
2019-02-17 04:14:57,096 : Generated sentence embeddings
2019-02-17 04:14:57,096 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 04:15:14,862 : Best param found at split 1: l2reg = 0.001                 with score 93.69
2019-02-17 04:15:33,239 : Best param found at split 2: l2reg = 0.01                 with score 93.46
2019-02-17 04:15:50,823 : Best param found at split 3: l2reg = 0.0001                 with score 93.65
2019-02-17 04:16:09,718 : Best param found at split 4: l2reg = 0.0001                 with score 93.72
2019-02-17 04:16:27,237 : Best param found at split 5: l2reg = 0.0001                 with score 93.65
2019-02-17 04:16:28,356 : Dev acc : 93.63 Test acc : 93.42

2019-02-17 04:16:28,357 : ***** Transfer task : SST Binary classification *****


2019-02-17 04:16:28,507 : loading BERT model bert-base-uncased
2019-02-17 04:16:28,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:16:28,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:16:28,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4y03ms2q
2019-02-17 04:16:31,043 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:16:32,540 : Computing embedding for train
2019-02-17 04:17:18,512 : Computed train embeddings
2019-02-17 04:17:18,512 : Computing embedding for dev
2019-02-17 04:17:19,475 : Computed dev embeddings
2019-02-17 04:17:19,475 : Computing embedding for test
2019-02-17 04:17:21,545 : Computed test embeddings
2019-02-17 04:17:21,545 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:17:49,784 : [('reg:1e-05', 83.03), ('reg:0.0001', 82.68), ('reg:0.001', 81.77), ('reg:0.01', 77.18)]
2019-02-17 04:17:49,784 : Validation : best param found is reg = 1e-05 with score             83.03
2019-02-17 04:17:49,785 : Evaluating...
2019-02-17 04:17:56,157 : 
Dev acc : 83.03 Test acc : 81.71 for             SST Binary classification

2019-02-17 04:17:56,157 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-17 04:17:56,210 : loading BERT model bert-base-uncased
2019-02-17 04:17:56,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:17:56,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:17:56,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4n7xo2m1
2019-02-17 04:17:58,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:18:00,167 : Computing embedding for train
2019-02-17 04:18:09,576 : Computed train embeddings
2019-02-17 04:18:09,576 : Computing embedding for dev
2019-02-17 04:18:10,800 : Computed dev embeddings
2019-02-17 04:18:10,800 : Computing embedding for test
2019-02-17 04:18:13,270 : Computed test embeddings
2019-02-17 04:18:13,270 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:18:18,041 : [('reg:1e-05', 36.88), ('reg:0.0001', 41.78), ('reg:0.001', 40.78), ('reg:0.01', 32.24)]
2019-02-17 04:18:18,042 : Validation : best param found is reg = 0.0001 with score             41.78
2019-02-17 04:18:18,042 : Evaluating...
2019-02-17 04:18:19,015 : 
Dev acc : 41.78 Test acc : 42.62 for             SST Fine-Grained classification

2019-02-17 04:18:19,015 : ***** Transfer task : TREC *****


2019-02-17 04:18:19,029 : loading BERT model bert-base-uncased
2019-02-17 04:18:19,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:18:19,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:18:19,052 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo6otn8qr
2019-02-17 04:18:21,647 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:18:26,558 : Computed train embeddings
2019-02-17 04:18:26,829 : Computed test embeddings
2019-02-17 04:18:26,830 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 04:18:39,376 : [('reg:1e-05', 67.31), ('reg:0.0001', 70.61), ('reg:0.001', 68.87), ('reg:0.01', 63.4)]
2019-02-17 04:18:39,376 : Cross-validation : best param found is reg = 0.0001             with score 70.61
2019-02-17 04:18:39,376 : Evaluating...
2019-02-17 04:18:40,260 : 
Dev acc : 70.61 Test acc : 81.4             for TREC

2019-02-17 04:18:40,261 : ***** Transfer task : MRPC *****


2019-02-17 04:18:40,285 : loading BERT model bert-base-uncased
2019-02-17 04:18:40,285 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:18:40,307 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:18:40,307 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiynmcozg
2019-02-17 04:18:42,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:18:44,237 : Computing embedding for train
2019-02-17 04:18:54,036 : Computed train embeddings
2019-02-17 04:18:54,036 : Computing embedding for test
2019-02-17 04:18:58,675 : Computed test embeddings
2019-02-17 04:18:58,691 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 04:19:06,422 : [('reg:1e-05', 72.2), ('reg:0.0001', 72.35), ('reg:0.001', 71.52), ('reg:0.01', 73.01)]
2019-02-17 04:19:06,422 : Cross-validation : best param found is reg = 0.01             with score 73.01
2019-02-17 04:19:06,422 : Evaluating...
2019-02-17 04:19:06,776 : Dev acc : 73.01 Test acc 72.12; Test F1 81.92 for MRPC.

2019-02-17 04:19:06,776 : ***** Transfer task : SICK-Entailment*****


2019-02-17 04:19:06,803 : loading BERT model bert-base-uncased
2019-02-17 04:19:06,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:19:06,865 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:19:06,865 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7rfe60n5
2019-02-17 04:19:09,379 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:19:10,811 : Computing embedding for train
2019-02-17 04:19:15,937 : Computed train embeddings
2019-02-17 04:19:15,937 : Computing embedding for dev
2019-02-17 04:19:16,631 : Computed dev embeddings
2019-02-17 04:19:16,631 : Computing embedding for test
2019-02-17 04:19:22,182 : Computed test embeddings
2019-02-17 04:19:22,211 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:19:24,939 : [('reg:1e-05', 75.6), ('reg:0.0001', 75.8), ('reg:0.001', 71.4), ('reg:0.01', 76.4)]
2019-02-17 04:19:24,940 : Validation : best param found is reg = 0.01 with score             76.4
2019-02-17 04:19:24,940 : Evaluating...
2019-02-17 04:19:25,954 : 
Dev acc : 76.4 Test acc : 75.42 for                        SICK entailment

2019-02-17 04:19:25,955 : ***** Transfer task : SICK-Relatedness*****


2019-02-17 04:19:26,003 : loading BERT model bert-base-uncased
2019-02-17 04:19:26,003 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:19:26,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:19:26,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz2qkkxp_
2019-02-17 04:19:28,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:19:30,025 : Computing embedding for train
2019-02-17 04:19:35,211 : Computed train embeddings
2019-02-17 04:19:35,211 : Computing embedding for dev
2019-02-17 04:19:35,906 : Computed dev embeddings
2019-02-17 04:19:35,906 : Computing embedding for test
2019-02-17 04:19:41,509 : Computed test embeddings
2019-02-17 04:20:04,779 : Dev : Pearson 0.791600232068805
2019-02-17 04:20:04,780 : Test : Pearson 0.7878980212060073 Spearman 0.7127124050973181 MSE 0.3864142098810773                        for SICK Relatedness

2019-02-17 04:20:04,780 : 

***** Transfer task : STSBenchmark*****


2019-02-17 04:20:04,824 : loading BERT model bert-base-uncased
2019-02-17 04:20:04,824 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:20:04,855 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:20:04,856 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ge8uwu1
2019-02-17 04:20:07,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:20:08,752 : Computing embedding for train
2019-02-17 04:20:16,954 : Computed train embeddings
2019-02-17 04:20:16,954 : Computing embedding for dev
2019-02-17 04:20:19,397 : Computed dev embeddings
2019-02-17 04:20:19,397 : Computing embedding for test
2019-02-17 04:20:21,355 : Computed test embeddings
2019-02-17 04:20:49,159 : Dev : Pearson 0.6560547998901856
2019-02-17 04:20:49,159 : Test : Pearson 0.6257516252582351 Spearman 0.6219055568350936 MSE 1.5263952258928792                        for SICK Relatedness

2019-02-17 04:20:49,160 : ***** Transfer task : SNLI Entailment*****


2019-02-17 04:20:54,221 : loading BERT model bert-base-uncased
2019-02-17 04:20:54,221 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:20:54,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:20:54,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2rnmc86a
2019-02-17 04:20:56,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:20:58,600 : PROGRESS (encoding): 0.00%
2019-02-17 04:22:18,466 : PROGRESS (encoding): 14.56%
2019-02-17 04:23:45,111 : PROGRESS (encoding): 29.12%
2019-02-17 04:25:12,900 : PROGRESS (encoding): 43.69%
2019-02-17 04:26:48,214 : PROGRESS (encoding): 58.25%
2019-02-17 04:28:32,688 : PROGRESS (encoding): 72.81%
2019-02-17 04:30:22,453 : PROGRESS (encoding): 87.37%
2019-02-17 04:32:13,604 : PROGRESS (encoding): 0.00%
2019-02-17 04:32:27,218 : PROGRESS (encoding): 0.00%
2019-02-17 04:32:40,319 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 04:33:14,982 : [('reg:1e-09', 61.47)]
2019-02-17 04:33:14,982 : Validation : best param found is reg = 1e-09 with score             61.47
2019-02-17 04:33:14,982 : Evaluating...
2019-02-17 04:33:50,158 : Dev acc : 61.47 Test acc : 61.55 for SNLI

2019-02-17 04:33:50,158 : ***** Transfer task: Image Caption Retrieval *****


2019-02-17 04:33:59,185 : loading BERT model bert-base-uncased
2019-02-17 04:33:59,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 04:33:59,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 04:33:59,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0wzh51i4
2019-02-17 04:34:01,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 04:34:03,222 : Computing embedding for train
2019-02-17 04:41:36,589 : Computed train embeddings
2019-02-17 04:41:36,589 : Computing embedding for dev
2019-02-17 04:41:56,590 : Computed dev embeddings
2019-02-17 04:41:56,590 : Computing embedding for test
2019-02-17 04:42:17,060 : Computed test embeddings
2019-02-17 04:42:17,077 : prepare data
2019-02-17 04:42:17,144 : start epoch
2019-02-17 04:43:00,202 : samples : 64000
2019-02-17 04:43:10,693 : Image to text: 6.8, 21.16, 30.9, 26.0
2019-02-17 04:43:18,301 : Text to Image: 5.484, 17.88, 27.748, 30.0
2019-02-17 04:44:01,245 : samples : 128000
2019-02-17 04:44:11,735 : Image to text: 7.02, 21.54, 33.06, 24.0
2019-02-17 04:44:19,285 : Text to Image: 5.716, 18.804, 29.196, 28.0
2019-02-17 04:45:02,375 : samples : 192000
2019-02-17 04:45:12,891 : Image to text: 6.56, 20.98, 32.42, 24.0
2019-02-17 04:45:20,450 : Text to Image: 5.512, 18.008, 28.176, 29.0
2019-02-17 04:46:03,508 : samples : 256000
2019-02-17 04:46:14,700 : Image to text: 8.34, 24.38, 34.88, 21.0
2019-02-17 04:46:23,277 : Text to Image: 6.036, 19.564, 29.824, 27.0
2019-02-17 04:47:13,640 : samples : 320000
2019-02-17 04:47:24,154 : Image to text: 8.2, 24.04, 36.12, 20.0
2019-02-17 04:47:31,749 : Text to Image: 6.728, 20.616, 31.632, 24.0
2019-02-17 04:48:14,513 : samples : 384000
2019-02-17 04:48:25,105 : Image to text: 8.26, 24.76, 36.84, 19.0
2019-02-17 04:48:32,709 : Text to Image: 6.588, 21.404, 31.98, 24.0
2019-02-17 04:49:15,475 : samples : 448000
2019-02-17 04:49:25,983 : Image to text: 7.76, 23.98, 35.52, 20.0
2019-02-17 04:49:33,586 : Text to Image: 6.784, 21.592, 32.596, 23.0
2019-02-17 04:50:16,725 : samples : 512000
2019-02-17 04:50:27,327 : Image to text: 9.1, 26.34, 38.26, 18.0
2019-02-17 04:50:34,876 : Text to Image: 7.42, 23.024, 34.38, 21.0
2019-02-17 04:51:11,363 : Epoch 1 finished
2019-02-17 04:51:11,795 : Image to text: 24.6, 57.0, 72.3, 4.0
2019-02-17 04:51:12,132 : Text to Image: 19.32, 49.36, 66.98, 6.0
2019-02-17 04:51:12,570 : Image to text: 22.0, 53.6, 70.0, 5.0
2019-02-17 04:51:12,899 : Text to Image: 18.88, 49.2, 67.04, 6.0
2019-02-17 04:51:13,338 : Image to text: 26.6, 55.1, 71.1, 4.0
2019-02-17 04:51:13,670 : Text to Image: 18.58, 49.02, 66.24, 6.0
2019-02-17 04:51:14,115 : Image to text: 22.9, 55.5, 72.1, 4.0
2019-02-17 04:51:14,458 : Text to Image: 19.64, 50.32, 67.64, 5.0
2019-02-17 04:51:14,913 : Image to text: 25.0, 55.8, 72.9, 4.0
2019-02-17 04:51:15,261 : Text to Image: 19.54, 50.96, 66.86, 5.0
2019-02-17 04:51:15,261 : Dev mean Text to Image: 19.192, 49.772, 66.952, 5.6
2019-02-17 04:51:15,261 : Dev mean Image to text: 24.22, 55.400000000000006, 71.67999999999999, 4.2
2019-02-17 04:51:15,261 : start epoch
2019-02-17 04:51:58,205 : samples : 64000
2019-02-17 04:52:08,719 : Image to text: 9.3, 25.9, 37.46, 19.0
2019-02-17 04:52:16,224 : Text to Image: 7.16, 22.364, 34.08, 22.0
2019-02-17 04:52:59,495 : samples : 128000
2019-02-17 04:53:09,924 : Image to text: 8.72, 26.06, 38.08, 18.0
2019-02-17 04:53:17,482 : Text to Image: 7.344, 23.1, 34.076, 22.0
2019-02-17 04:54:00,256 : samples : 192000
2019-02-17 04:54:10,749 : Image to text: 9.54, 26.58, 38.24, 18.0
2019-02-17 04:54:18,305 : Text to Image: 7.492, 23.212, 34.296, 21.0
2019-02-17 04:55:01,055 : samples : 256000
2019-02-17 04:55:11,581 : Image to text: 9.12, 27.12, 38.74, 17.0
2019-02-17 04:55:19,145 : Text to Image: 7.86, 24.092, 35.728, 20.0
2019-02-17 04:56:02,350 : samples : 320000
2019-02-17 04:56:12,888 : Image to text: 9.76, 28.3, 40.48, 16.0
2019-02-17 04:56:20,533 : Text to Image: 8.236, 24.36, 35.716, 20.0
2019-02-17 04:57:03,687 : samples : 384000
2019-02-17 04:57:14,174 : Image to text: 9.14, 26.54, 38.74, 18.0
2019-02-17 04:57:21,761 : Text to Image: 7.728, 23.6, 35.288, 21.0
2019-02-17 04:58:04,869 : samples : 448000
2019-02-17 04:58:15,316 : Image to text: 10.04, 27.76, 40.16, 17.0
2019-02-17 04:58:22,823 : Text to Image: 7.672, 23.996, 35.688, 20.0
2019-02-17 04:59:06,054 : samples : 512000
2019-02-17 04:59:16,557 : Image to text: 10.42, 27.8, 40.38, 16.0
2019-02-17 04:59:24,127 : Text to Image: 8.284, 24.032, 35.856, 20.0
2019-02-17 05:00:00,674 : Epoch 2 finished
2019-02-17 05:00:01,115 : Image to text: 27.0, 60.5, 76.2, 4.0
2019-02-17 05:00:01,461 : Text to Image: 20.86, 53.42, 70.34, 5.0
2019-02-17 05:00:01,902 : Image to text: 24.5, 57.2, 72.2, 4.0
2019-02-17 05:00:02,251 : Text to Image: 19.46, 52.32, 69.94, 5.0
2019-02-17 05:00:02,698 : Image to text: 25.8, 58.1, 74.1, 4.0
2019-02-17 05:00:03,043 : Text to Image: 20.58, 51.66, 69.76, 5.0
2019-02-17 05:00:03,496 : Image to text: 28.1, 61.3, 74.8, 4.0
2019-02-17 05:00:03,836 : Text to Image: 21.36, 52.06, 68.38, 5.0
2019-02-17 05:00:04,280 : Image to text: 26.2, 59.8, 75.1, 4.0
2019-02-17 05:00:04,623 : Text to Image: 20.5, 52.72, 69.58, 5.0
2019-02-17 05:00:04,623 : Dev mean Text to Image: 20.552, 52.43600000000001, 69.6, 5.0
2019-02-17 05:00:04,623 : Dev mean Image to text: 26.32, 59.379999999999995, 74.48, 4.0
2019-02-17 05:00:04,623 : start epoch
2019-02-17 05:00:46,992 : samples : 64000
2019-02-17 05:00:57,451 : Image to text: 10.22, 28.42, 41.22, 16.0
2019-02-17 05:01:05,002 : Text to Image: 8.132, 24.548, 36.144, 20.0
2019-02-17 05:01:47,648 : samples : 128000
2019-02-17 05:01:58,173 : Image to text: 8.98, 26.74, 38.66, 17.0
2019-02-17 05:02:05,729 : Text to Image: 7.708, 23.788, 35.352, 20.0
2019-02-17 05:02:48,342 : samples : 192000
2019-02-17 05:02:58,882 : Image to text: 10.92, 29.56, 42.34, 16.0
2019-02-17 05:03:06,503 : Text to Image: 8.38, 24.948, 36.672, 19.0
2019-02-17 05:03:54,598 : samples : 256000
2019-02-17 05:04:06,742 : Image to text: 9.64, 27.28, 40.56, 17.0
2019-02-17 05:04:15,308 : Text to Image: 7.668, 23.332, 34.956, 21.0
2019-02-17 05:04:58,109 : samples : 320000
2019-02-17 05:05:08,607 : Image to text: 9.52, 28.08, 40.36, 16.0
2019-02-17 05:05:16,209 : Text to Image: 8.476, 25.24, 36.876, 19.0
2019-02-17 05:05:59,490 : samples : 384000
2019-02-17 05:06:09,938 : Image to text: 9.96, 28.46, 40.56, 16.0
2019-02-17 05:06:17,478 : Text to Image: 7.812, 23.944, 35.5, 20.0
2019-02-17 05:07:00,443 : samples : 448000
2019-02-17 05:07:10,947 : Image to text: 10.44, 29.64, 42.28, 15.0
2019-02-17 05:07:18,534 : Text to Image: 8.584, 25.268, 37.468, 18.0
2019-02-17 05:08:01,174 : samples : 512000
2019-02-17 05:08:11,756 : Image to text: 9.82, 28.6, 41.4, 16.0
2019-02-17 05:08:19,301 : Text to Image: 8.308, 24.636, 36.72, 19.0
2019-02-17 05:08:55,325 : Epoch 3 finished
2019-02-17 05:08:55,773 : Image to text: 24.4, 58.7, 75.4, 4.0
2019-02-17 05:08:56,106 : Text to Image: 21.76, 55.46, 71.82, 4.0
2019-02-17 05:08:56,551 : Image to text: 23.7, 57.0, 73.2, 4.0
2019-02-17 05:08:56,894 : Text to Image: 20.98, 54.56, 71.48, 5.0
2019-02-17 05:08:57,341 : Image to text: 25.1, 59.8, 74.0, 4.0
2019-02-17 05:08:57,684 : Text to Image: 21.4, 53.64, 71.46, 5.0
2019-02-17 05:08:58,122 : Image to text: 25.2, 59.7, 74.4, 4.0
2019-02-17 05:08:58,464 : Text to Image: 22.32, 54.22, 71.54, 5.0
2019-02-17 05:08:58,915 : Image to text: 27.6, 58.3, 76.1, 4.0
2019-02-17 05:08:59,259 : Text to Image: 21.38, 54.74, 71.66, 5.0
2019-02-17 05:08:59,259 : Dev mean Text to Image: 21.568, 54.524, 71.592, 4.8
2019-02-17 05:08:59,260 : Dev mean Image to text: 25.2, 58.7, 74.62, 4.0
2019-02-17 05:08:59,260 : start epoch
2019-02-17 05:09:42,747 : samples : 64000
2019-02-17 05:09:53,290 : Image to text: 10.72, 28.7, 41.56, 16.0
2019-02-17 05:10:00,811 : Text to Image: 8.02, 24.912, 36.928, 19.0
2019-02-17 05:10:43,730 : samples : 128000
2019-02-17 05:10:54,241 : Image to text: 10.72, 29.48, 42.12, 15.0
2019-02-17 05:11:01,790 : Text to Image: 8.676, 25.432, 37.5, 18.0
2019-02-17 05:11:44,837 : samples : 192000
2019-02-17 05:11:55,311 : Image to text: 10.76, 29.3, 41.92, 15.0
2019-02-17 05:12:02,918 : Text to Image: 8.328, 25.112, 36.928, 19.0
2019-02-17 05:12:45,456 : samples : 256000
2019-02-17 05:12:55,948 : Image to text: 10.88, 29.32, 41.8, 16.0
2019-02-17 05:13:03,523 : Text to Image: 8.444, 25.336, 37.616, 19.0
2019-02-17 05:13:46,144 : samples : 320000
2019-02-17 05:13:56,637 : Image to text: 9.92, 28.5, 41.42, 16.0
2019-02-17 05:14:04,154 : Text to Image: 8.368, 25.232, 37.24, 19.0
2019-02-17 05:14:46,969 : samples : 384000
2019-02-17 05:14:57,469 : Image to text: 10.46, 29.18, 42.46, 15.0
2019-02-17 05:15:05,018 : Text to Image: 8.416, 24.932, 37.044, 19.0
2019-02-17 05:15:47,269 : samples : 448000
2019-02-17 05:15:57,759 : Image to text: 10.48, 29.82, 42.58, 15.0
2019-02-17 05:16:05,315 : Text to Image: 8.808, 26.0, 38.16, 18.0
2019-02-17 05:16:47,776 : samples : 512000
2019-02-17 05:16:58,317 : Image to text: 11.34, 29.56, 42.44, 15.0
2019-02-17 05:17:05,901 : Text to Image: 8.628, 25.584, 37.828, 18.0
2019-02-17 05:17:42,348 : Epoch 4 finished
2019-02-17 05:17:42,789 : Image to text: 27.9, 59.4, 76.7, 3.0
2019-02-17 05:17:43,120 : Text to Image: 22.24, 54.98, 72.56, 5.0
2019-02-17 05:17:43,556 : Image to text: 25.1, 58.8, 75.1, 4.0
2019-02-17 05:17:43,892 : Text to Image: 20.96, 54.26, 71.76, 5.0
2019-02-17 05:17:44,329 : Image to text: 28.7, 60.7, 75.1, 4.0
2019-02-17 05:17:44,669 : Text to Image: 22.58, 55.74, 72.12, 4.0
2019-02-17 05:17:45,104 : Image to text: 28.3, 60.0, 74.6, 4.0
2019-02-17 05:17:45,447 : Text to Image: 22.42, 55.14, 71.8, 5.0
2019-02-17 05:17:45,884 : Image to text: 28.3, 60.5, 76.4, 4.0
2019-02-17 05:17:46,231 : Text to Image: 21.9, 55.58, 72.34, 4.0
2019-02-17 05:17:46,231 : Dev mean Text to Image: 22.02, 55.13999999999999, 72.116, 4.6
2019-02-17 05:17:46,231 : Dev mean Image to text: 27.660000000000004, 59.88, 75.58, 3.8
2019-02-17 05:17:46,231 : start epoch
2019-02-17 05:18:29,350 : samples : 64000
2019-02-17 05:18:39,894 : Image to text: 11.12, 29.0, 42.46, 15.0
2019-02-17 05:18:47,485 : Text to Image: 8.888, 26.068, 37.968, 18.0
2019-02-17 05:19:30,915 : samples : 128000
2019-02-17 05:19:41,515 : Image to text: 11.32, 30.78, 43.02, 15.0
2019-02-17 05:19:49,138 : Text to Image: 8.956, 26.536, 38.368, 17.0
2019-02-17 05:20:31,306 : samples : 192000
2019-02-17 05:20:41,818 : Image to text: 10.76, 29.32, 41.82, 15.0
2019-02-17 05:20:50,189 : Text to Image: 8.644, 25.84, 38.0, 18.0
2019-02-17 05:21:41,207 : samples : 256000
2019-02-17 05:21:51,759 : Image to text: 10.36, 29.08, 41.72, 15.0
2019-02-17 05:21:59,333 : Text to Image: 8.2, 24.764, 36.96, 19.0
2019-02-17 05:22:42,262 : samples : 320000
2019-02-17 05:22:52,778 : Image to text: 11.02, 29.76, 43.14, 14.0
2019-02-17 05:23:00,401 : Text to Image: 8.952, 26.104, 38.468, 18.0
2019-02-17 05:23:43,193 : samples : 384000
2019-02-17 05:23:53,724 : Image to text: 10.3, 29.38, 42.04, 15.0
2019-02-17 05:24:01,313 : Text to Image: 8.996, 26.372, 38.728, 18.0
2019-02-17 05:24:44,393 : samples : 448000
2019-02-17 05:24:54,905 : Image to text: 11.0, 30.64, 43.34, 14.0
2019-02-17 05:25:02,505 : Text to Image: 8.764, 26.5, 38.496, 17.0
2019-02-17 05:25:45,747 : samples : 512000
2019-02-17 05:25:56,335 : Image to text: 10.66, 30.38, 42.68, 15.0
2019-02-17 05:26:03,934 : Text to Image: 8.892, 26.656, 38.476, 18.0
2019-02-17 05:26:40,325 : Epoch 5 finished
2019-02-17 05:26:40,762 : Image to text: 28.2, 61.8, 77.8, 3.0
2019-02-17 05:26:41,103 : Text to Image: 22.2, 56.66, 73.42, 4.0
2019-02-17 05:26:41,561 : Image to text: 27.2, 59.9, 74.7, 4.0
2019-02-17 05:26:41,892 : Text to Image: 21.58, 55.72, 73.14, 5.0
2019-02-17 05:26:42,318 : Image to text: 29.1, 61.6, 77.2, 4.0
2019-02-17 05:26:42,660 : Text to Image: 21.92, 55.56, 72.54, 4.0
2019-02-17 05:26:43,095 : Image to text: 28.3, 63.1, 76.0, 3.0
2019-02-17 05:26:43,436 : Text to Image: 23.66, 56.52, 73.24, 4.0
2019-02-17 05:26:43,876 : Image to text: 29.0, 61.5, 76.4, 3.0
2019-02-17 05:26:44,216 : Text to Image: 23.4, 57.24, 72.94, 4.0
2019-02-17 05:26:44,216 : Dev mean Text to Image: 22.552, 56.34, 73.056, 4.2
2019-02-17 05:26:44,216 : Dev mean Image to text: 28.36, 61.58, 76.42, 3.4000000000000004
2019-02-17 05:26:44,217 : start epoch
2019-02-17 05:27:27,143 : samples : 64000
2019-02-17 05:27:39,717 : Image to text: 11.6, 30.5, 43.32, 14.0
2019-02-17 05:27:49,792 : Text to Image: 8.996, 26.744, 38.716, 17.0
2019-02-17 05:28:35,813 : samples : 128000
2019-02-17 05:28:48,488 : Image to text: 10.16, 30.14, 42.72, 15.0
2019-02-17 05:28:58,544 : Text to Image: 8.868, 26.272, 38.212, 18.0
2019-02-17 05:29:41,350 : samples : 192000
2019-02-17 05:29:51,700 : Image to text: 10.62, 29.24, 42.38, 15.0
2019-02-17 05:29:59,156 : Text to Image: 8.596, 25.56, 37.58, 19.0
2019-02-17 05:30:43,182 : samples : 256000
2019-02-17 05:30:55,880 : Image to text: 10.56, 29.9, 42.72, 14.0
2019-02-17 05:31:05,977 : Text to Image: 9.088, 26.884, 39.224, 17.0
2019-02-17 05:31:50,216 : samples : 320000
2019-02-17 05:32:00,505 : Image to text: 10.92, 30.36, 42.8, 15.0
2019-02-17 05:32:07,758 : Text to Image: 9.012, 26.092, 38.032, 18.0
2019-02-17 05:32:49,873 : samples : 384000
2019-02-17 05:33:00,316 : Image to text: 11.56, 30.94, 44.44, 14.0
2019-02-17 05:33:10,393 : Text to Image: 9.656, 27.26, 39.648, 17.0
2019-02-17 05:33:55,556 : samples : 448000
2019-02-17 05:34:08,235 : Image to text: 11.32, 31.06, 44.4, 14.0
2019-02-17 05:34:18,310 : Text to Image: 9.304, 27.276, 39.872, 17.0
2019-02-17 05:35:01,263 : samples : 512000
2019-02-17 05:35:12,738 : Image to text: 11.18, 30.64, 43.66, 14.0
2019-02-17 05:35:22,763 : Text to Image: 8.852, 26.196, 38.648, 18.0
2019-02-17 05:36:01,323 : Epoch 6 finished
2019-02-17 05:36:02,275 : Image to text: 27.6, 63.5, 78.1, 4.0
2019-02-17 05:36:03,053 : Text to Image: 23.7, 58.24, 75.08, 4.0
2019-02-17 05:36:03,997 : Image to text: 27.2, 59.2, 76.2, 4.0
2019-02-17 05:36:04,788 : Text to Image: 22.88, 56.98, 74.34, 4.0
2019-02-17 05:36:05,718 : Image to text: 27.3, 60.6, 75.8, 4.0
2019-02-17 05:36:06,511 : Text to Image: 24.12, 57.26, 73.8, 4.0
2019-02-17 05:36:07,404 : Image to text: 28.6, 63.2, 76.8, 3.0
2019-02-17 05:36:08,207 : Text to Image: 23.84, 57.14, 73.84, 4.0
2019-02-17 05:36:09,198 : Image to text: 30.9, 62.8, 78.0, 3.0
2019-02-17 05:36:10,004 : Text to Image: 23.62, 57.7, 73.82, 4.0
2019-02-17 05:36:10,005 : Dev mean Text to Image: 23.631999999999998, 57.46399999999999, 74.176, 4.0
2019-02-17 05:36:10,005 : Dev mean Image to text: 28.32, 61.86, 76.97999999999999, 3.6000000000000005
2019-02-17 05:36:10,005 : start epoch
2019-02-17 05:36:53,268 : samples : 64000
2019-02-17 05:37:03,582 : Image to text: 11.3, 30.72, 44.06, 14.0
2019-02-17 05:37:10,967 : Text to Image: 9.208, 27.08, 39.388, 17.0
2019-02-17 05:37:53,777 : samples : 128000
2019-02-17 05:38:06,532 : Image to text: 11.62, 31.14, 44.1, 14.0
2019-02-17 05:38:16,752 : Text to Image: 9.612, 27.536, 40.048, 17.0
2019-02-17 05:39:06,548 : samples : 192000
2019-02-17 05:39:16,847 : Image to text: 11.02, 30.32, 43.38, 14.0
2019-02-17 05:39:24,280 : Text to Image: 9.192, 27.3, 39.648, 17.0
2019-02-17 05:40:07,559 : samples : 256000
2019-02-17 05:40:20,152 : Image to text: 10.82, 31.16, 43.94, 14.0
2019-02-17 05:40:30,190 : Text to Image: 9.22, 27.164, 39.496, 17.0
2019-02-17 05:41:15,734 : samples : 320000
2019-02-17 05:41:27,026 : Image to text: 10.68, 30.02, 42.84, 14.0
2019-02-17 05:41:34,508 : Text to Image: 9.308, 26.652, 39.052, 17.0
2019-02-17 05:42:17,139 : samples : 384000
2019-02-17 05:42:29,493 : Image to text: 11.14, 30.72, 43.5, 14.0
2019-02-17 05:42:39,548 : Text to Image: 9.6, 27.484, 39.844, 17.0
2019-02-17 05:43:24,538 : samples : 448000
2019-02-17 05:43:37,129 : Image to text: 11.4, 31.56, 44.9, 13.0
2019-02-17 05:43:46,664 : Text to Image: 9.644, 27.912, 40.252, 16.0
2019-02-17 05:44:29,505 : samples : 512000
2019-02-17 05:44:40,257 : Image to text: 11.12, 31.4, 44.4, 13.0
2019-02-17 05:44:50,272 : Text to Image: 9.516, 27.66, 39.86, 17.0
2019-02-17 05:45:28,518 : Epoch 7 finished
2019-02-17 05:45:29,505 : Image to text: 28.0, 63.1, 78.6, 3.0
2019-02-17 05:45:30,279 : Text to Image: 22.54, 57.88, 75.22, 4.0
2019-02-17 05:45:31,263 : Image to text: 26.5, 63.0, 78.2, 3.0
2019-02-17 05:45:32,040 : Text to Image: 22.8, 57.32, 75.18, 4.0
2019-02-17 05:45:33,023 : Image to text: 28.0, 61.4, 76.1, 4.0
2019-02-17 05:45:33,799 : Text to Image: 23.92, 57.52, 74.44, 4.0
2019-02-17 05:45:34,792 : Image to text: 29.5, 63.3, 77.6, 3.0
2019-02-17 05:45:35,500 : Text to Image: 23.78, 58.04, 74.7, 4.0
2019-02-17 05:45:36,441 : Image to text: 28.7, 64.1, 78.5, 3.0
2019-02-17 05:45:37,226 : Text to Image: 23.9, 58.24, 74.52, 4.0
2019-02-17 05:45:37,226 : Dev mean Text to Image: 23.388000000000005, 57.8, 74.812, 4.0
2019-02-17 05:45:37,226 : Dev mean Image to text: 28.14, 62.98, 77.8, 3.2
2019-02-17 05:45:37,226 : start epoch
2019-02-17 05:46:21,621 : samples : 64000
2019-02-17 05:46:31,997 : Image to text: 11.68, 31.76, 44.58, 13.0
2019-02-17 05:46:39,421 : Text to Image: 9.368, 27.4, 39.44, 17.0
2019-02-17 05:47:22,985 : samples : 128000
2019-02-17 05:47:35,597 : Image to text: 11.4, 30.98, 44.5, 14.0
2019-02-17 05:47:45,706 : Text to Image: 9.296, 27.096, 39.52, 17.0
2019-02-17 05:48:31,619 : samples : 192000
2019-02-17 05:48:42,895 : Image to text: 11.2, 30.92, 44.46, 13.0
2019-02-17 05:48:50,311 : Text to Image: 9.128, 27.484, 39.772, 17.0
2019-02-17 05:49:33,230 : samples : 256000
2019-02-17 05:49:43,474 : Image to text: 11.62, 31.34, 44.56, 14.0
2019-02-17 05:49:52,030 : Text to Image: 9.408, 27.42, 39.692, 17.0
2019-02-17 05:50:36,886 : samples : 320000
2019-02-17 05:50:49,534 : Image to text: 12.14, 31.98, 44.96, 13.0
2019-02-17 05:50:59,587 : Text to Image: 9.36, 27.608, 40.124, 16.0
2019-02-17 05:51:43,876 : samples : 384000
2019-02-17 05:51:54,151 : Image to text: 11.06, 30.36, 43.7, 14.0
2019-02-17 05:52:01,625 : Text to Image: 9.376, 27.268, 39.968, 16.0
2019-02-17 05:52:43,774 : samples : 448000
2019-02-17 05:52:53,771 : Image to text: 11.38, 30.52, 43.94, 14.0
2019-02-17 05:53:00,649 : Text to Image: 9.444, 26.816, 39.032, 17.0
2019-02-17 05:53:42,221 : samples : 512000
2019-02-17 05:53:52,272 : Image to text: 11.28, 31.26, 44.44, 14.0
2019-02-17 05:53:59,193 : Text to Image: 9.408, 27.264, 39.564, 17.0
2019-02-17 05:54:34,359 : Epoch 8 finished
2019-02-17 05:54:34,753 : Image to text: 30.1, 64.2, 78.7, 3.0
2019-02-17 05:54:35,040 : Text to Image: 23.82, 58.0, 74.92, 4.0
2019-02-17 05:54:35,429 : Image to text: 27.4, 60.7, 78.7, 4.0
2019-02-17 05:54:35,720 : Text to Image: 22.84, 56.72, 74.38, 4.0
2019-02-17 05:54:36,105 : Image to text: 27.6, 63.2, 76.7, 3.0
2019-02-17 05:54:36,398 : Text to Image: 23.86, 57.7, 74.74, 4.0
2019-02-17 05:54:36,780 : Image to text: 28.7, 62.1, 77.9, 3.0
2019-02-17 05:54:37,059 : Text to Image: 24.0, 57.48, 73.74, 4.0
2019-02-17 05:54:37,438 : Image to text: 30.0, 62.5, 78.2, 3.0
2019-02-17 05:54:37,715 : Text to Image: 24.04, 58.0, 73.84, 4.0
2019-02-17 05:54:37,715 : Dev mean Text to Image: 23.712, 57.580000000000005, 74.324, 4.0
2019-02-17 05:54:37,715 : Dev mean Image to text: 28.759999999999998, 62.540000000000006, 78.04, 3.2
2019-02-17 05:54:37,716 : start epoch
2019-02-17 05:55:19,544 : samples : 64000
2019-02-17 05:55:31,048 : Image to text: 10.74, 30.5, 43.66, 14.0
2019-02-17 05:55:39,381 : Text to Image: 9.456, 26.976, 39.296, 17.0
2019-02-17 05:56:24,752 : samples : 128000
2019-02-17 05:56:34,800 : Image to text: 11.74, 31.82, 44.7, 13.0
2019-02-17 05:56:41,768 : Text to Image: 9.46, 27.372, 39.304, 17.0
2019-02-17 05:57:23,292 : samples : 192000
2019-02-17 05:57:33,287 : Image to text: 11.26, 31.18, 45.18, 13.0
2019-02-17 05:57:40,155 : Text to Image: 9.304, 27.148, 39.448, 17.0
2019-02-17 05:58:21,646 : samples : 256000
2019-02-17 05:58:31,685 : Image to text: 10.8, 30.16, 43.22, 14.0
2019-02-17 05:58:38,552 : Text to Image: 9.188, 27.016, 39.376, 17.0
2019-02-17 05:59:20,181 : samples : 320000
2019-02-17 05:59:30,157 : Image to text: 11.22, 31.82, 44.62, 13.0
2019-02-17 05:59:37,007 : Text to Image: 9.644, 27.872, 40.248, 16.0
2019-02-17 06:00:18,659 : samples : 384000
2019-02-17 06:00:28,649 : Image to text: 11.44, 31.9, 44.52, 13.0
2019-02-17 06:00:35,524 : Text to Image: 9.516, 27.872, 40.34, 16.0
2019-02-17 06:01:17,008 : samples : 448000
2019-02-17 06:01:26,963 : Image to text: 11.54, 31.86, 44.68, 13.0
2019-02-17 06:01:33,894 : Text to Image: 9.34, 27.12, 39.676, 17.0
2019-02-17 06:02:15,328 : samples : 512000
2019-02-17 06:02:25,424 : Image to text: 11.72, 31.46, 45.22, 13.0
2019-02-17 06:02:32,284 : Text to Image: 9.452, 27.776, 40.148, 17.0
2019-02-17 06:03:07,766 : Epoch 9 finished
2019-02-17 06:03:08,161 : Image to text: 30.6, 64.2, 79.3, 3.0
2019-02-17 06:03:08,454 : Text to Image: 24.56, 58.9, 76.08, 4.0
2019-02-17 06:03:08,845 : Image to text: 27.2, 61.4, 76.5, 4.0
2019-02-17 06:03:09,127 : Text to Image: 23.62, 57.36, 74.86, 4.0
2019-02-17 06:03:09,519 : Image to text: 29.3, 62.7, 75.3, 3.0
2019-02-17 06:03:09,805 : Text to Image: 24.64, 58.66, 75.06, 4.0
2019-02-17 06:03:10,186 : Image to text: 29.8, 63.9, 77.3, 3.0
2019-02-17 06:03:10,465 : Text to Image: 24.58, 58.28, 74.86, 4.0
2019-02-17 06:03:10,848 : Image to text: 30.2, 63.9, 78.6, 3.0
2019-02-17 06:03:11,126 : Text to Image: 25.04, 58.7, 74.6, 4.0
2019-02-17 06:03:11,126 : Dev mean Text to Image: 24.488, 58.379999999999995, 75.092, 4.0
2019-02-17 06:03:11,126 : Dev mean Image to text: 29.419999999999998, 63.22, 77.4, 3.2
2019-02-17 06:03:11,126 : start epoch
2019-02-17 06:03:52,713 : samples : 64000
2019-02-17 06:04:02,692 : Image to text: 12.06, 31.64, 45.32, 13.0
2019-02-17 06:04:09,589 : Text to Image: 9.524, 27.792, 40.288, 16.0
2019-02-17 06:04:51,232 : samples : 128000
2019-02-17 06:05:01,265 : Image to text: 11.24, 31.72, 44.58, 14.0
2019-02-17 06:05:08,175 : Text to Image: 9.868, 28.296, 40.848, 16.0
2019-02-17 06:05:49,517 : samples : 192000
2019-02-17 06:05:59,542 : Image to text: 12.0, 32.88, 45.42, 13.0
2019-02-17 06:06:06,411 : Text to Image: 9.864, 28.204, 40.888, 16.0
2019-02-17 06:06:48,065 : samples : 256000
2019-02-17 06:06:58,073 : Image to text: 11.84, 32.4, 45.54, 13.0
2019-02-17 06:07:04,977 : Text to Image: 9.696, 27.58, 39.704, 17.0
2019-02-17 06:07:46,597 : samples : 320000
2019-02-17 06:07:56,540 : Image to text: 11.76, 31.68, 44.58, 13.0
2019-02-17 06:08:03,439 : Text to Image: 9.46, 27.7, 40.256, 17.0
2019-02-17 06:08:45,248 : samples : 384000
2019-02-17 06:08:55,279 : Image to text: 12.1, 32.1, 46.18, 13.0
2019-02-17 06:09:02,185 : Text to Image: 9.848, 28.244, 40.784, 16.0
2019-02-17 06:09:43,920 : samples : 448000
2019-02-17 06:09:53,847 : Image to text: 11.86, 31.82, 45.24, 13.0
2019-02-17 06:10:00,783 : Text to Image: 9.484, 27.452, 39.488, 17.0
2019-02-17 06:10:42,149 : samples : 512000
2019-02-17 06:10:52,164 : Image to text: 11.44, 31.74, 45.18, 13.0
2019-02-17 06:10:59,042 : Text to Image: 9.628, 28.188, 40.708, 16.0
2019-02-17 06:11:34,369 : Epoch 10 finished
2019-02-17 06:11:34,761 : Image to text: 29.2, 63.9, 80.5, 3.0
2019-02-17 06:11:35,042 : Text to Image: 24.94, 59.54, 76.7, 4.0
2019-02-17 06:11:35,438 : Image to text: 28.0, 61.7, 77.2, 4.0
2019-02-17 06:11:35,725 : Text to Image: 23.92, 58.38, 75.66, 4.0
2019-02-17 06:11:36,114 : Image to text: 29.1, 62.8, 77.6, 3.0
2019-02-17 06:11:36,406 : Text to Image: 24.86, 59.14, 75.7, 4.0
2019-02-17 06:11:36,789 : Image to text: 30.4, 63.4, 77.4, 3.0
2019-02-17 06:11:37,086 : Text to Image: 24.72, 58.96, 75.92, 4.0
2019-02-17 06:11:37,474 : Image to text: 31.0, 62.8, 79.4, 3.0
2019-02-17 06:11:37,752 : Text to Image: 25.12, 59.3, 75.24, 4.0
2019-02-17 06:11:37,752 : Dev mean Text to Image: 24.712000000000003, 59.064, 75.84400000000001, 4.0
2019-02-17 06:11:37,752 : Dev mean Image to text: 29.539999999999996, 62.91999999999999, 78.42, 3.2
2019-02-17 06:11:37,752 : start epoch
2019-02-17 06:12:18,969 : samples : 64000
2019-02-17 06:12:29,865 : Image to text: 11.2, 31.36, 44.08, 14.0
2019-02-17 06:12:38,000 : Text to Image: 9.616, 27.664, 40.064, 16.0
2019-02-17 06:13:26,179 : samples : 128000
2019-02-17 06:13:36,245 : Image to text: 11.76, 32.12, 44.86, 13.0
2019-02-17 06:13:43,172 : Text to Image: 9.52, 27.74, 40.108, 16.0
2019-02-17 06:14:24,553 : samples : 192000
2019-02-17 06:14:34,599 : Image to text: 11.54, 31.52, 44.7, 14.0
2019-02-17 06:14:41,540 : Text to Image: 9.648, 27.768, 40.196, 16.0
2019-02-17 06:15:23,229 : samples : 256000
2019-02-17 06:15:33,229 : Image to text: 11.8, 32.42, 45.08, 13.0
2019-02-17 06:15:40,120 : Text to Image: 9.908, 28.052, 40.356, 16.0
2019-02-17 06:16:21,793 : samples : 320000
2019-02-17 06:16:31,772 : Image to text: 11.9, 32.14, 45.16, 13.0
2019-02-17 06:16:38,659 : Text to Image: 9.56, 27.752, 40.28, 16.0
2019-02-17 06:17:20,071 : samples : 384000
2019-02-17 06:17:30,088 : Image to text: 12.12, 32.3, 45.88, 13.0
2019-02-17 06:17:36,974 : Text to Image: 9.952, 28.688, 41.06, 16.0
2019-02-17 06:18:18,519 : samples : 448000
2019-02-17 06:18:28,562 : Image to text: 11.26, 31.16, 45.08, 13.0
2019-02-17 06:18:35,430 : Text to Image: 9.34, 27.768, 40.184, 16.0
2019-02-17 06:19:17,014 : samples : 512000
2019-02-17 06:19:26,967 : Image to text: 11.36, 32.42, 45.1, 13.0
2019-02-17 06:19:33,894 : Text to Image: 9.916, 28.636, 40.984, 16.0
2019-02-17 06:20:09,279 : Epoch 11 finished
2019-02-17 06:20:09,658 : Image to text: 28.6, 63.2, 78.5, 3.0
2019-02-17 06:20:09,947 : Text to Image: 24.54, 58.2, 76.34, 4.0
2019-02-17 06:20:10,329 : Image to text: 27.4, 60.6, 77.0, 4.0
2019-02-17 06:20:10,622 : Text to Image: 23.2, 57.5, 74.8, 4.0
2019-02-17 06:20:11,018 : Image to text: 29.6, 61.2, 76.6, 3.0
2019-02-17 06:20:11,304 : Text to Image: 24.14, 57.64, 75.26, 4.0
2019-02-17 06:20:11,698 : Image to text: 30.4, 62.8, 77.1, 3.0
2019-02-17 06:20:11,988 : Text to Image: 24.56, 57.76, 74.98, 4.0
2019-02-17 06:20:12,373 : Image to text: 30.2, 63.7, 78.1, 3.0
2019-02-17 06:20:12,691 : Text to Image: 24.58, 58.32, 74.88, 4.0
2019-02-17 06:20:12,691 : Dev mean Text to Image: 24.203999999999997, 57.884, 75.25200000000001, 4.0
2019-02-17 06:20:12,691 : Dev mean Image to text: 29.239999999999995, 62.300000000000004, 77.46000000000001, 3.2
2019-02-17 06:20:12,691 : start epoch
2019-02-17 06:20:54,384 : samples : 64000
2019-02-17 06:21:04,370 : Image to text: 11.04, 32.0, 45.32, 13.0
2019-02-17 06:21:11,217 : Text to Image: 9.824, 28.236, 40.248, 16.0
2019-02-17 06:21:52,909 : samples : 128000
2019-02-17 06:22:02,926 : Image to text: 11.62, 31.7, 45.04, 13.0
2019-02-17 06:22:09,814 : Text to Image: 9.492, 28.0, 40.36, 16.0
2019-02-17 06:22:51,584 : samples : 192000
2019-02-17 06:23:01,555 : Image to text: 11.94, 32.3, 45.8, 13.0
2019-02-17 06:23:08,404 : Text to Image: 9.912, 28.472, 41.196, 16.0
2019-02-17 06:23:49,861 : samples : 256000
2019-02-17 06:23:59,881 : Image to text: 12.6, 32.38, 45.46, 13.0
2019-02-17 06:24:06,741 : Text to Image: 9.896, 28.428, 41.06, 16.0
2019-02-17 06:24:48,478 : samples : 320000
2019-02-17 06:24:58,531 : Image to text: 11.96, 32.26, 46.1, 13.0
2019-02-17 06:25:05,465 : Text to Image: 9.496, 27.864, 40.16, 16.0
2019-02-17 06:25:46,930 : samples : 384000
2019-02-17 06:25:56,962 : Image to text: 11.56, 31.84, 46.12, 13.0
2019-02-17 06:26:03,794 : Text to Image: 10.036, 28.432, 40.7, 16.0
2019-02-17 06:26:45,898 : samples : 448000
2019-02-17 06:26:55,900 : Image to text: 12.38, 32.86, 45.52, 13.0
2019-02-17 06:27:02,804 : Text to Image: 10.032, 28.476, 40.832, 16.0
2019-02-17 06:27:44,217 : samples : 512000
2019-02-17 06:27:54,228 : Image to text: 11.64, 32.08, 45.04, 13.0
2019-02-17 06:28:01,124 : Text to Image: 9.812, 28.652, 41.1, 16.0
2019-02-17 06:28:36,497 : Epoch 12 finished
2019-02-17 06:28:36,894 : Image to text: 29.2, 64.0, 78.5, 3.0
2019-02-17 06:28:37,182 : Text to Image: 24.4, 59.44, 76.98, 4.0
2019-02-17 06:28:37,578 : Image to text: 27.8, 61.6, 79.0, 4.0
2019-02-17 06:28:37,864 : Text to Image: 25.02, 59.68, 76.14, 4.0
2019-02-17 06:28:38,257 : Image to text: 29.8, 63.5, 75.9, 3.0
2019-02-17 06:28:38,553 : Text to Image: 24.88, 59.46, 75.84, 4.0
2019-02-17 06:28:38,937 : Image to text: 30.7, 64.2, 77.7, 3.0
2019-02-17 06:28:39,230 : Text to Image: 25.16, 59.34, 76.08, 4.0
2019-02-17 06:28:39,621 : Image to text: 30.2, 65.7, 79.5, 3.0
2019-02-17 06:28:39,903 : Text to Image: 24.82, 60.24, 76.02, 4.0
2019-02-17 06:28:39,903 : Dev mean Text to Image: 24.856, 59.632, 76.212, 4.0
2019-02-17 06:28:39,903 : Dev mean Image to text: 29.54, 63.8, 78.12, 3.2
2019-02-17 06:28:39,903 : start epoch
2019-02-17 06:29:21,414 : samples : 64000
2019-02-17 06:29:31,127 : Image to text: 11.78, 32.16, 45.1, 13.0
2019-02-17 06:29:38,894 : Text to Image: 9.972, 28.264, 40.764, 16.0
2019-02-17 06:30:28,357 : samples : 128000
2019-02-17 06:30:38,348 : Image to text: 11.74, 32.32, 46.02, 12.0
2019-02-17 06:30:45,232 : Text to Image: 9.588, 28.244, 40.312, 16.0
2019-02-17 06:31:26,931 : samples : 192000
2019-02-17 06:31:36,958 : Image to text: 12.3, 31.5, 45.48, 13.0
2019-02-17 06:31:43,840 : Text to Image: 9.4, 28.016, 40.764, 16.0
2019-02-17 06:32:25,312 : samples : 256000
2019-02-17 06:32:35,380 : Image to text: 12.16, 32.64, 46.28, 13.0
2019-02-17 06:32:42,359 : Text to Image: 10.056, 28.556, 41.076, 16.0
2019-02-17 06:33:24,070 : samples : 320000
2019-02-17 06:33:34,052 : Image to text: 12.04, 31.68, 44.98, 13.0
2019-02-17 06:33:40,962 : Text to Image: 9.78, 28.412, 41.404, 16.0
2019-02-17 06:34:22,709 : samples : 384000
2019-02-17 06:34:32,752 : Image to text: 11.9, 32.16, 45.72, 13.0
2019-02-17 06:34:39,636 : Text to Image: 9.616, 27.664, 40.588, 16.0
2019-02-17 06:35:21,307 : samples : 448000
2019-02-17 06:35:31,254 : Image to text: 12.06, 32.32, 45.9, 12.0
2019-02-17 06:35:38,111 : Text to Image: 9.972, 28.304, 40.828, 16.0
2019-02-17 06:36:19,910 : samples : 512000
2019-02-17 06:36:29,847 : Image to text: 12.14, 32.94, 45.92, 13.0
2019-02-17 06:36:36,780 : Text to Image: 9.884, 28.448, 40.932, 16.0
2019-02-17 06:37:12,081 : Epoch 13 finished
2019-02-17 06:37:12,473 : Image to text: 30.5, 63.5, 79.4, 3.0
2019-02-17 06:37:12,766 : Text to Image: 24.84, 60.74, 77.4, 4.0
2019-02-17 06:37:13,159 : Image to text: 26.5, 60.5, 77.5, 4.0
2019-02-17 06:37:13,451 : Text to Image: 24.12, 59.24, 76.48, 4.0
2019-02-17 06:37:13,856 : Image to text: 29.1, 63.1, 76.3, 3.0
2019-02-17 06:37:14,156 : Text to Image: 25.14, 60.22, 76.42, 4.0
2019-02-17 06:37:14,547 : Image to text: 30.7, 63.5, 78.4, 3.0
2019-02-17 06:37:14,837 : Text to Image: 25.42, 59.96, 76.34, 4.0
2019-02-17 06:37:15,217 : Image to text: 30.3, 64.2, 78.2, 3.0
2019-02-17 06:37:15,496 : Text to Image: 25.74, 59.66, 75.88, 4.0
2019-02-17 06:37:15,496 : Dev mean Text to Image: 25.052, 59.96400000000001, 76.504, 4.0
2019-02-17 06:37:15,496 : Dev mean Image to text: 29.42, 62.96000000000001, 77.96000000000001, 3.2
2019-02-17 06:37:15,496 : start epoch
2019-02-17 06:37:56,956 : samples : 64000
2019-02-17 06:38:06,997 : Image to text: 12.22, 32.78, 44.82, 13.0
2019-02-17 06:38:13,909 : Text to Image: 10.004, 28.712, 41.224, 16.0
2019-02-17 06:38:55,371 : samples : 128000
2019-02-17 06:39:05,442 : Image to text: 11.92, 32.64, 45.82, 13.0
2019-02-17 06:39:12,348 : Text to Image: 9.64, 28.152, 40.7, 16.0
2019-02-17 06:39:53,719 : samples : 192000
2019-02-17 06:40:03,720 : Image to text: 11.88, 32.38, 44.74, 13.0
2019-02-17 06:40:10,636 : Text to Image: 9.52, 28.108, 40.552, 16.0
2019-02-17 06:40:52,289 : samples : 256000
2019-02-17 06:41:02,289 : Image to text: 12.0, 32.56, 45.84, 13.0
2019-02-17 06:41:09,158 : Text to Image: 9.824, 28.56, 41.04, 16.0
2019-02-17 06:41:50,666 : samples : 320000
2019-02-17 06:42:00,657 : Image to text: 12.22, 33.14, 46.48, 12.0
2019-02-17 06:42:07,577 : Text to Image: 10.148, 28.472, 41.08, 16.0
2019-02-17 06:42:49,055 : samples : 384000
2019-02-17 06:42:59,052 : Image to text: 11.8, 32.52, 45.86, 13.0
2019-02-17 06:43:05,990 : Text to Image: 9.8, 28.6, 40.96, 16.0
2019-02-17 06:43:47,683 : samples : 448000
2019-02-17 06:43:57,645 : Image to text: 11.98, 32.84, 46.36, 12.0
2019-02-17 06:44:04,504 : Text to Image: 9.636, 28.088, 40.58, 16.0
2019-02-17 06:44:46,256 : samples : 512000
2019-02-17 06:44:56,257 : Image to text: 11.76, 33.0, 46.14, 13.0
2019-02-17 06:45:03,116 : Text to Image: 10.016, 28.3, 41.24, 16.0
2019-02-17 06:45:38,532 : Epoch 14 finished
2019-02-17 06:45:38,927 : Image to text: 28.3, 64.3, 78.8, 3.0
2019-02-17 06:45:39,206 : Text to Image: 24.74, 59.16, 77.24, 4.0
2019-02-17 06:45:39,585 : Image to text: 28.0, 61.7, 77.8, 4.0
2019-02-17 06:45:39,863 : Text to Image: 23.56, 58.8, 76.16, 4.0
2019-02-17 06:45:40,246 : Image to text: 30.1, 61.9, 74.5, 3.0
2019-02-17 06:45:40,539 : Text to Image: 25.18, 59.16, 75.72, 4.0
2019-02-17 06:45:40,932 : Image to text: 29.6, 63.0, 76.4, 3.0
2019-02-17 06:45:41,223 : Text to Image: 24.64, 59.84, 75.94, 4.0
2019-02-17 06:45:41,608 : Image to text: 28.8, 64.0, 78.8, 3.0
2019-02-17 06:45:41,897 : Text to Image: 24.88, 59.8, 75.66, 4.0
2019-02-17 06:45:41,897 : Dev mean Text to Image: 24.599999999999998, 59.352, 76.144, 4.0
2019-02-17 06:45:41,897 : Dev mean Image to text: 28.96, 62.980000000000004, 77.26, 3.2
2019-02-17 06:45:41,897 : start epoch
2019-02-17 06:46:23,700 : samples : 64000
2019-02-17 06:46:33,486 : Image to text: 11.46, 32.44, 45.46, 13.0
2019-02-17 06:46:40,176 : Text to Image: 9.764, 28.36, 41.22, 16.0
2019-02-17 06:47:31,045 : samples : 128000
2019-02-17 06:47:41,032 : Image to text: 12.54, 32.12, 45.38, 13.0
2019-02-17 06:47:47,941 : Text to Image: 9.836, 28.296, 40.876, 16.0
2019-02-17 06:48:29,454 : samples : 192000
2019-02-17 06:48:39,529 : Image to text: 11.74, 32.44, 45.82, 13.0
2019-02-17 06:48:46,382 : Text to Image: 9.892, 28.528, 40.944, 16.0
2019-02-17 06:49:28,205 : samples : 256000
2019-02-17 06:49:38,233 : Image to text: 12.08, 32.4, 45.42, 13.0
2019-02-17 06:49:45,121 : Text to Image: 9.896, 28.384, 41.196, 16.0
2019-02-17 06:50:26,939 : samples : 320000
2019-02-17 06:50:37,010 : Image to text: 11.7, 32.06, 45.64, 13.0
2019-02-17 06:50:43,912 : Text to Image: 10.04, 28.644, 41.34, 16.0
2019-02-17 06:51:25,372 : samples : 384000
2019-02-17 06:51:35,371 : Image to text: 12.24, 32.2, 46.26, 13.0
2019-02-17 06:51:42,323 : Text to Image: 9.924, 28.392, 41.192, 16.0
2019-02-17 06:52:23,841 : samples : 448000
2019-02-17 06:52:33,900 : Image to text: 12.24, 32.84, 46.4, 12.0
2019-02-17 06:52:40,818 : Text to Image: 10.068, 28.752, 41.152, 16.0
2019-02-17 06:53:22,332 : samples : 512000
2019-02-17 06:53:32,362 : Image to text: 11.98, 32.96, 46.92, 12.0
2019-02-17 06:53:39,238 : Text to Image: 10.18, 28.34, 40.828, 16.0
2019-02-17 06:54:14,724 : Epoch 15 finished
2019-02-17 06:54:15,104 : Image to text: 29.9, 65.8, 81.2, 3.0
2019-02-17 06:54:15,385 : Text to Image: 24.96, 59.6, 77.04, 4.0
2019-02-17 06:54:15,767 : Image to text: 29.2, 62.5, 78.4, 3.0
2019-02-17 06:54:16,049 : Text to Image: 24.4, 59.24, 76.16, 4.0
2019-02-17 06:54:16,452 : Image to text: 30.1, 64.6, 76.9, 3.0
2019-02-17 06:54:16,752 : Text to Image: 24.42, 59.2, 75.5, 4.0
2019-02-17 06:54:17,171 : Image to text: 31.6, 66.0, 78.8, 3.0
2019-02-17 06:54:17,462 : Text to Image: 25.82, 58.76, 75.94, 4.0
2019-02-17 06:54:17,850 : Image to text: 29.5, 65.7, 79.5, 3.0
2019-02-17 06:54:18,135 : Text to Image: 25.4, 60.1, 75.66, 4.0
2019-02-17 06:54:18,135 : Dev mean Text to Image: 25.0, 59.379999999999995, 76.06, 4.0
2019-02-17 06:54:18,135 : Dev mean Image to text: 30.060000000000002, 64.92, 78.96000000000001, 3.0
2019-02-17 06:54:18,136 : start epoch
2019-02-17 06:54:59,845 : samples : 64000
2019-02-17 06:55:09,935 : Image to text: 11.8, 32.2, 45.96, 12.0
2019-02-17 06:55:16,842 : Text to Image: 9.7, 28.488, 41.096, 16.0
2019-02-17 06:55:58,507 : samples : 128000
2019-02-17 06:56:08,507 : Image to text: 11.76, 32.22, 45.0, 13.0
2019-02-17 06:56:15,381 : Text to Image: 9.8, 27.7, 40.428, 16.0
2019-02-17 06:56:57,017 : samples : 192000
2019-02-17 06:57:07,039 : Image to text: 12.36, 33.38, 46.6, 12.0
2019-02-17 06:57:13,877 : Text to Image: 10.072, 29.012, 41.464, 15.0
2019-02-17 06:57:55,587 : samples : 256000
2019-02-17 06:58:05,547 : Image to text: 12.26, 33.4, 46.02, 13.0
2019-02-17 06:58:12,396 : Text to Image: 10.208, 29.116, 41.852, 15.0
2019-02-17 06:58:54,024 : samples : 320000
2019-02-17 06:59:04,019 : Image to text: 12.32, 33.12, 46.46, 12.0
2019-02-17 06:59:10,913 : Text to Image: 9.924, 28.284, 41.188, 16.0
2019-02-17 06:59:52,460 : samples : 384000
2019-02-17 07:00:02,419 : Image to text: 12.04, 32.12, 45.36, 13.0
2019-02-17 07:00:09,284 : Text to Image: 10.028, 28.648, 41.2, 16.0
2019-02-17 07:00:51,045 : samples : 448000
2019-02-17 07:01:01,072 : Image to text: 12.0, 32.74, 45.94, 13.0
2019-02-17 07:01:07,944 : Text to Image: 10.252, 29.008, 41.872, 15.0
2019-02-17 07:01:49,668 : samples : 512000
2019-02-17 07:01:59,611 : Image to text: 12.3, 33.8, 46.42, 12.0
2019-02-17 07:02:06,553 : Text to Image: 10.344, 29.272, 41.7, 15.0
2019-02-17 07:02:41,875 : Epoch 16 finished
2019-02-17 07:02:42,260 : Image to text: 31.4, 65.9, 79.9, 3.0
2019-02-17 07:02:42,553 : Text to Image: 25.08, 60.1, 77.48, 4.0
2019-02-17 07:02:42,945 : Image to text: 29.7, 61.6, 80.0, 3.0
2019-02-17 07:02:43,228 : Text to Image: 23.82, 59.88, 76.72, 4.0
2019-02-17 07:02:43,622 : Image to text: 30.8, 64.2, 77.9, 3.0
2019-02-17 07:02:43,911 : Text to Image: 25.58, 59.98, 75.86, 4.0
2019-02-17 07:02:44,303 : Image to text: 31.7, 65.9, 79.9, 3.0
2019-02-17 07:02:44,595 : Text to Image: 25.64, 60.18, 76.7, 4.0
2019-02-17 07:02:44,986 : Image to text: 32.7, 65.0, 79.8, 3.0
2019-02-17 07:02:45,274 : Text to Image: 25.52, 60.58, 76.28, 4.0
2019-02-17 07:02:45,274 : Dev mean Text to Image: 25.128, 60.144000000000005, 76.608, 4.0
2019-02-17 07:02:45,274 : Dev mean Image to text: 31.259999999999998, 64.52000000000001, 79.5, 3.0
2019-02-17 07:02:45,274 : start epoch
2019-02-17 07:03:27,025 : samples : 64000
2019-02-17 07:03:36,864 : Image to text: 12.12, 32.72, 45.5, 13.0
2019-02-17 07:03:43,506 : Text to Image: 10.104, 29.028, 41.816, 15.0
2019-02-17 07:04:34,388 : samples : 128000
2019-02-17 07:04:44,413 : Image to text: 12.48, 33.88, 46.82, 12.0
2019-02-17 07:04:51,305 : Text to Image: 9.952, 28.728, 41.464, 15.0
2019-02-17 07:05:32,778 : samples : 192000
2019-02-17 07:05:42,796 : Image to text: 12.52, 33.6, 47.22, 12.0
2019-02-17 07:05:49,631 : Text to Image: 10.352, 28.692, 41.324, 15.0
2019-02-17 07:06:31,080 : samples : 256000
2019-02-17 07:06:41,079 : Image to text: 11.88, 32.06, 45.34, 13.0
2019-02-17 07:06:48,000 : Text to Image: 10.096, 28.74, 41.22, 16.0
2019-02-17 07:07:29,814 : samples : 320000
2019-02-17 07:07:39,809 : Image to text: 12.22, 33.26, 46.3, 12.0
2019-02-17 07:07:46,712 : Text to Image: 9.688, 28.088, 41.056, 16.0
2019-02-17 07:08:28,206 : samples : 384000
2019-02-17 07:08:38,202 : Image to text: 11.48, 32.16, 45.66, 13.0
2019-02-17 07:08:45,056 : Text to Image: 9.828, 28.244, 41.068, 16.0
2019-02-17 07:09:26,610 : samples : 448000
2019-02-17 07:09:36,596 : Image to text: 11.48, 31.48, 44.24, 13.0
2019-02-17 07:09:43,467 : Text to Image: 9.724, 28.116, 40.716, 16.0
2019-02-17 07:10:25,465 : samples : 512000
2019-02-17 07:10:35,407 : Image to text: 11.76, 32.26, 46.04, 13.0
2019-02-17 07:10:42,293 : Text to Image: 10.044, 28.636, 41.272, 16.0
2019-02-17 07:11:17,662 : Epoch 17 finished
2019-02-17 07:11:18,046 : Image to text: 28.5, 64.8, 79.3, 3.0
2019-02-17 07:11:18,336 : Text to Image: 24.38, 59.26, 76.82, 4.0
2019-02-17 07:11:18,724 : Image to text: 28.9, 61.9, 77.3, 4.0
2019-02-17 07:11:19,007 : Text to Image: 23.62, 58.78, 75.52, 4.0
2019-02-17 07:11:19,401 : Image to text: 28.9, 62.1, 76.8, 3.0
2019-02-17 07:11:19,693 : Text to Image: 24.6, 59.8, 75.12, 4.0
2019-02-17 07:11:20,083 : Image to text: 29.5, 64.2, 76.5, 3.0
2019-02-17 07:11:20,378 : Text to Image: 25.7, 59.74, 76.2, 4.0
2019-02-17 07:11:20,758 : Image to text: 31.3, 63.7, 78.8, 3.0
2019-02-17 07:11:21,036 : Text to Image: 25.46, 60.06, 75.7, 4.0
2019-02-17 07:11:21,037 : Dev mean Text to Image: 24.752000000000002, 59.528, 75.872, 4.0
2019-02-17 07:11:21,037 : Dev mean Image to text: 29.419999999999995, 63.339999999999996, 77.74000000000001, 3.2
2019-02-17 07:11:24,388 : 
Test scores | Image to text:             30.019999999999996, 64.36, 79.24, 3.0
2019-02-17 07:11:24,388 : Test scores | Text to image:             25.52, 59.816, 76.252, 4.0

2019-02-17 07:11:24,505 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-17 07:11:24,735 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-17 07:11:25,437 : loading BERT model bert-base-uncased
2019-02-17 07:11:25,437 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:11:25,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:11:25,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq04u_ono
2019-02-17 07:11:27,988 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:11:29,478 : Computing embeddings for train/dev/test
2019-02-17 07:12:51,326 : Computed embeddings
2019-02-17 07:12:51,327 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:13:35,991 : [('reg:1e-05', 61.21), ('reg:0.0001', 61.57), ('reg:0.001', 53.61), ('reg:0.01', 52.57)]
2019-02-17 07:13:35,991 : Validation : best param found is reg = 0.0001 with score             61.57
2019-02-17 07:13:35,991 : Evaluating...
2019-02-17 07:13:48,152 : 
Dev acc : 61.6 Test acc : 60.9 for LENGTH classification

2019-02-17 07:13:48,152 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-17 07:13:48,549 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-17 07:13:48,602 : loading BERT model bert-base-uncased
2019-02-17 07:13:48,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:13:48,636 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:13:48,636 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo856rswv
2019-02-17 07:13:51,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:13:52,530 : Computing embeddings for train/dev/test
2019-02-17 07:15:08,891 : Computed embeddings
2019-02-17 07:15:08,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:16:03,845 : [('reg:1e-05', 12.63), ('reg:0.0001', 4.32), ('reg:0.001', 0.53), ('reg:0.01', 0.19)]
2019-02-17 07:16:03,846 : Validation : best param found is reg = 1e-05 with score             12.63
2019-02-17 07:16:03,846 : Evaluating...
2019-02-17 07:16:21,817 : 
Dev acc : 12.6 Test acc : 12.5 for WORDCONTENT classification

2019-02-17 07:16:21,819 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-17 07:16:22,186 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-17 07:16:22,257 : loading BERT model bert-base-uncased
2019-02-17 07:16:22,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:16:22,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:16:22,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpynmb1cxq
2019-02-17 07:16:24,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:16:26,314 : Computing embeddings for train/dev/test
2019-02-17 07:17:37,859 : Computed embeddings
2019-02-17 07:17:37,860 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:18:17,006 : [('reg:1e-05', 28.33), ('reg:0.0001', 27.12), ('reg:0.001', 23.05), ('reg:0.01', 22.28)]
2019-02-17 07:18:17,006 : Validation : best param found is reg = 1e-05 with score             28.33
2019-02-17 07:18:17,007 : Evaluating...
2019-02-17 07:18:25,250 : 
Dev acc : 28.3 Test acc : 27.9 for DEPTH classification

2019-02-17 07:18:25,251 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-17 07:18:25,639 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-17 07:18:25,703 : loading BERT model bert-base-uncased
2019-02-17 07:18:25,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:18:25,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:18:25,820 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp40vx4co_
2019-02-17 07:18:28,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:18:29,848 : Computing embeddings for train/dev/test
2019-02-17 07:19:37,050 : Computed embeddings
2019-02-17 07:19:37,050 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:20:18,083 : [('reg:1e-05', 54.79), ('reg:0.0001', 53.72), ('reg:0.001', 45.32), ('reg:0.01', 34.56)]
2019-02-17 07:20:18,083 : Validation : best param found is reg = 1e-05 with score             54.79
2019-02-17 07:20:18,083 : Evaluating...
2019-02-17 07:20:28,308 : 
Dev acc : 54.8 Test acc : 54.8 for TOPCONSTITUENTS classification

2019-02-17 07:20:28,309 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-17 07:20:28,869 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-17 07:20:28,936 : loading BERT model bert-base-uncased
2019-02-17 07:20:28,936 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:20:28,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:20:28,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2fbhlsao
2019-02-17 07:20:31,491 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:20:32,926 : Computing embeddings for train/dev/test
2019-02-17 07:21:50,714 : Computed embeddings
2019-02-17 07:21:50,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:22:33,228 : [('reg:1e-05', 85.2), ('reg:0.0001', 85.16), ('reg:0.001', 85.08), ('reg:0.01', 84.38)]
2019-02-17 07:22:33,228 : Validation : best param found is reg = 1e-05 with score             85.2
2019-02-17 07:22:33,228 : Evaluating...
2019-02-17 07:22:43,598 : 
Dev acc : 85.2 Test acc : 84.5 for BIGRAMSHIFT classification

2019-02-17 07:22:43,599 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-17 07:22:44,024 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-17 07:22:44,101 : loading BERT model bert-base-uncased
2019-02-17 07:22:44,101 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:22:44,136 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:22:44,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyqlwxz89
2019-02-17 07:22:46,637 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:22:48,116 : Computing embeddings for train/dev/test
2019-02-17 07:23:58,994 : Computed embeddings
2019-02-17 07:23:58,995 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:24:35,295 : [('reg:1e-05', 89.52), ('reg:0.0001', 89.55), ('reg:0.001', 89.59), ('reg:0.01', 89.62)]
2019-02-17 07:24:35,295 : Validation : best param found is reg = 0.01 with score             89.62
2019-02-17 07:24:35,295 : Evaluating...
2019-02-17 07:24:44,051 : 
Dev acc : 89.6 Test acc : 88.3 for TENSE classification

2019-02-17 07:24:44,052 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-17 07:24:44,472 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-17 07:24:44,535 : loading BERT model bert-base-uncased
2019-02-17 07:24:44,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:24:44,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:24:44,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2d3kw2h9
2019-02-17 07:24:47,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:24:48,505 : Computing embeddings for train/dev/test
2019-02-17 07:26:03,685 : Computed embeddings
2019-02-17 07:26:03,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:26:39,099 : [('reg:1e-05', 76.74), ('reg:0.0001', 76.74), ('reg:0.001', 77.69), ('reg:0.01', 78.0)]
2019-02-17 07:26:39,099 : Validation : best param found is reg = 0.01 with score             78.0
2019-02-17 07:26:39,099 : Evaluating...
2019-02-17 07:26:46,737 : 
Dev acc : 78.0 Test acc : 77.4 for SUBJNUMBER classification

2019-02-17 07:26:46,738 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-17 07:26:47,176 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-17 07:26:47,247 : loading BERT model bert-base-uncased
2019-02-17 07:26:47,247 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:26:47,373 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:26:47,373 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpna7_6q8u
2019-02-17 07:26:49,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:26:51,324 : Computing embeddings for train/dev/test
2019-02-17 07:28:05,288 : Computed embeddings
2019-02-17 07:28:05,289 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:28:45,791 : [('reg:1e-05', 73.68), ('reg:0.0001', 73.7), ('reg:0.001', 74.17), ('reg:0.01', 75.4)]
2019-02-17 07:28:45,791 : Validation : best param found is reg = 0.01 with score             75.4
2019-02-17 07:28:45,792 : Evaluating...
2019-02-17 07:28:55,494 : 
Dev acc : 75.4 Test acc : 76.5 for OBJNUMBER classification

2019-02-17 07:28:55,496 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-17 07:28:55,908 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-17 07:28:55,982 : loading BERT model bert-base-uncased
2019-02-17 07:28:55,982 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:28:56,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:28:56,114 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdcot9lqd
2019-02-17 07:28:58,630 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:29:00,117 : Computing embeddings for train/dev/test
2019-02-17 07:30:24,675 : Computed embeddings
2019-02-17 07:30:24,675 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:30:53,807 : [('reg:1e-05', 61.77), ('reg:0.0001', 61.74), ('reg:0.001', 61.59), ('reg:0.01', 60.5)]
2019-02-17 07:30:53,807 : Validation : best param found is reg = 1e-05 with score             61.77
2019-02-17 07:30:53,807 : Evaluating...
2019-02-17 07:31:01,430 : 
Dev acc : 61.8 Test acc : 61.0 for ODDMANOUT classification

2019-02-17 07:31:01,431 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-17 07:31:02,077 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-17 07:31:02,159 : loading BERT model bert-base-uncased
2019-02-17 07:31:02,159 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 07:31:02,196 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 07:31:02,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxmmxm274
2019-02-17 07:31:04,690 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 07:31:06,133 : Computing embeddings for train/dev/test
2019-02-17 07:32:30,585 : Computed embeddings
2019-02-17 07:32:30,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 07:33:06,495 : [('reg:1e-05', 57.73), ('reg:0.0001', 57.72), ('reg:0.001', 57.35), ('reg:0.01', 54.74)]
2019-02-17 07:33:06,495 : Validation : best param found is reg = 1e-05 with score             57.73
2019-02-17 07:33:06,495 : Evaluating...
2019-02-17 07:33:14,621 : 
Dev acc : 57.7 Test acc : 57.2 for COORDINATIONINVERSION classification

2019-02-17 07:33:14,623 : total results: {'STS12': {'MSRpar': {'pearson': (0.3292701396650404, 2.0082899770610228e-20), 'spearman': SpearmanrResult(correlation=0.3645991496615272, pvalue=5.384063356572047e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4813987758111209, 9.109705035724574e-45), 'spearman': SpearmanrResult(correlation=0.4985232629312539, pvalue=2.277015508998401e-48), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48329305308622844, 3.063134918867712e-28), 'spearman': SpearmanrResult(correlation=0.5694600433425483, pvalue=8.228919429330057e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5281831031648189, 4.1063695769363556e-55), 'spearman': SpearmanrResult(correlation=0.5468906077520795, pvalue=1.0079396415939168e-59), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5667964669073319, 2.7537967741220236e-35), 'spearman': SpearmanrResult(correlation=0.5071083002786961, pvalue=1.8489724027715693e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4777883077269081, 'wmean': 0.4672208222790667}, 'spearman': {'mean': 0.497316272793221, 'wmean': 0.48945564252383367}}}, 'STS13': {'FNWN': {'pearson': (0.2547090729651854, 0.0004047635384451121), 'spearman': SpearmanrResult(correlation=0.2773233967734777, pvalue=0.000111853258612407), 'nsamples': 189}, 'headlines': {'pearson': (0.6675186688813312, 6.840119875134273e-98), 'spearman': SpearmanrResult(correlation=0.647096319780582, pvalue=3.383716129959606e-90), 'nsamples': 750}, 'OnWN': {'pearson': (0.480093688367714, 1.0991639033884098e-33), 'spearman': SpearmanrResult(correlation=0.480755410257724, pvalue=8.712364702157595e-34), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46744047673807687, 'wmean': 0.545407717083804}, 'spearman': {'mean': 0.4683917089372613, 'wmean': 0.538293431320138}}}, 'STS14': {'deft-forum': {'pearson': (0.35035529508364155, 1.926069375099467e-14), 'spearman': SpearmanrResult(correlation=0.34548276624588536, pvalue=4.632417035518106e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7528417437911562, 4.545910599647106e-56), 'spearman': SpearmanrResult(correlation=0.7213971454836918, pvalue=1.795140763506984e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.6161442294743605, 1.3324271949755748e-79), 'spearman': SpearmanrResult(correlation=0.572742414916247, pvalue=1.360275788455499e-66), 'nsamples': 750}, 'images': {'pearson': (0.43993237471562596, 7.60251458257844e-37), 'spearman': SpearmanrResult(correlation=0.4380484553158856, pvalue=1.642820461183141e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.6271625534438446, 3.0906491341853625e-83), 'spearman': SpearmanrResult(correlation=0.649525712451253, pvalue=4.4173498249072936e-91), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6140234168920401, 6.423658931609656e-79), 'spearman': SpearmanrResult(correlation=0.5609741339349115, pvalue=2.1674248283443142e-63), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5667432689001115, 'wmean': 0.5617224898185038}, 'spearman': {'mean': 0.5480284380579791, 'wmean': 0.543427846911861}}}, 'STS15': {'answers-forums': {'pearson': (0.5327909656289264, 6.96265572121149e-29), 'spearman': SpearmanrResult(correlation=0.5148010431291269, pvalue=9.148976077560888e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.5997667450283266, 1.8541957111838487e-74), 'spearman': SpearmanrResult(correlation=0.6083388479983969, pvalue=4.106282026000495e-77), 'nsamples': 750}, 'belief': {'pearson': (0.6005899199295938, 3.9634138212959947e-38), 'spearman': SpearmanrResult(correlation=0.6128790328719657, pvalue=4.760569352510402e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6795381727297876, 1.0311233246974753e-102), 'spearman': SpearmanrResult(correlation=0.6762720984904763, pvalue=2.2195080252420848e-101), 'nsamples': 750}, 'images': {'pearson': (0.6061257288995658, 2.025744543180625e-76), 'spearman': SpearmanrResult(correlation=0.6144108422285541, pvalue=4.823656557651322e-79), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.60376230644324, 'wmean': 0.613030272359235}, 'spearman': {'mean': 0.605340372943704, 'wmean': 0.6157154566794933}}}, 'STS16': {'answer-answer': {'pearson': (0.5266905972550545, 1.5697263075627333e-19), 'spearman': SpearmanrResult(correlation=0.5140943645052517, pvalue=1.548617135385278e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6548133333694409, 7.110732956322236e-32), 'spearman': SpearmanrResult(correlation=0.6600701117726094, pvalue=1.5686656550484838e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7288117716779764, 2.2612265035682826e-39), 'spearman': SpearmanrResult(correlation=0.7331262358203738, pvalue=4.7997697084648476e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.781548413835633, 1.674918238970018e-51), 'spearman': SpearmanrResult(correlation=0.8026926099052625, pvalue=3.0403669935606535e-56), 'nsamples': 244}, 'question-question': {'pearson': (0.48956001899413354, 5.383213758533612e-14), 'spearman': SpearmanrResult(correlation=0.4825648195680615, pvalue=1.371784143823528e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6362848270264477, 'wmean': 0.6386766409303353}, 'spearman': {'mean': 0.6385096283143118, 'wmean': 0.6410366819232787}}}, 'MR': {'devacc': 73.63, 'acc': 74.68, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.57, 'acc': 73.32, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.29, 'acc': 86.33, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.63, 'acc': 93.42, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.03, 'acc': 81.71, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.78, 'acc': 42.62, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.61, 'acc': 81.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.01, 'acc': 72.12, 'f1': 81.92, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 75.42, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791600232068805, 'pearson': 0.7878980212060073, 'spearman': 0.7127124050973181, 'mse': 0.3864142098810773, 'yhat': array([2.5979958 , 4.19721743, 2.47941798, ..., 3.33426063, 4.26706675,        4.87567802]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6560547998901856, 'pearson': 0.6257516252582351, 'spearman': 0.6219055568350936, 'mse': 1.5263952258928792, 'yhat': array([2.08333733, 1.79652376, 1.90633487, ..., 3.95513819, 3.66242626,        3.20695483]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.47, 'acc': 61.55, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 337.16, 'acc': [(30.019999999999996, 64.36, 79.24, 3.0), (25.52, 59.816, 76.252, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 61.57, 'acc': 60.94, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.63, 'acc': 12.5, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.33, 'acc': 27.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.79, 'acc': 54.76, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.2, 'acc': 84.55, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.62, 'acc': 88.34, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.0, 'acc': 77.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.4, 'acc': 76.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.77, 'acc': 60.98, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.73, 'acc': 57.19, 'ndev': 10002, 'ntest': 10002}}
2019-02-17 07:33:14,623 : STS12 p=0.4672, STS12 s=0.4895, STS13 p=0.5454, STS13 s=0.5383, STS14 p=0.5617, STS14 s=0.5434, STS15 p=0.6130, STS15 s=0.6157, STS 16 p=0.6387, STS16 s=0.6410, STS B p=0.6258, STS B s=0.6219, STS B m=1.5264, SICK-R p=0.7879, SICK-R s=0.7127, SICK-P m=0.3864
2019-02-17 07:33:14,624 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-17 07:33:14,624 : 0.4672,0.4895,0.5454,0.5383,0.5617,0.5434,0.6130,0.6157,0.6387,0.6410,0.6258,0.6219,1.5264,0.7879,0.7127,0.3864
2019-02-17 07:33:14,624 : MR=74.68, CR=73.32, SUBJ=93.42, MPQA=86.33, SST-B=81.71, SST-F=42.62, TREC=81.40, SICK-E=75.42, SNLI=61.55, MRPC=72.12, MRPC f=81.92
2019-02-17 07:33:14,624 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-17 07:33:14,624 : 74.68,73.32,93.42,86.33,81.71,42.62,81.40,75.42,61.55,72.12,81.92
2019-02-17 07:33:14,624 : COCO r1i2t=30.02, COCO r5i2t=64.36, COCO r10i2t=79.24, COCO medr_i2t=3.00, COCO r1t2i=25.52, COCO r5t2i=59.82, COCO r10t2i=76.25, COCO medr_t2i=4.00
2019-02-17 07:33:14,624 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-17 07:33:14,624 : 30.02,64.36,79.24,3.00,25.52,59.82,76.25,4.00
2019-02-17 07:33:14,624 : SentLen=60.94, WC=12.50, TreeDepth=27.88, TopConst=54.76, BShift=84.55, Tense=88.34, SubjNum=77.42, ObjNum=76.55, SOMO=60.98, CoordInv=57.19, average=60.11
2019-02-17 07:33:14,624 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-17 07:33:14,624 : 60.94,12.50,27.88,54.76,84.55,88.34,77.42,76.55,60.98,57.19,60.11
