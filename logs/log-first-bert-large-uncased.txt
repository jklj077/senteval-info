2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : layer 0
2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : ********************************************************************************
2019-03-12 11:29:44,417 : ***** Transfer task : STS12 *****


2019-03-12 11:29:44,452 : loading BERT model bert-large-uncased
2019-03-12 11:29:44,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:29:44,470 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:29:44,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1_la86ux
2019-03-12 11:29:51,950 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:30:09,354 : MSRpar : pearson = 0.0000, spearman = nan
2019-03-12 11:30:12,350 : MSRvid : pearson = 0.0000, spearman = nan
2019-03-12 11:30:15,229 : SMTeuroparl : pearson = -0.0000, spearman = nan
2019-03-12 11:30:20,871 : surprise.OnWN : pearson = 0.0000, spearman = nan
2019-03-12 11:30:23,346 : surprise.SMTnews : pearson = -0.0000, spearman = nan
2019-03-12 11:30:23,346 : ALL (weighted average) : Pearson = 0.0000,             Spearman = nan
2019-03-12 11:30:23,346 : ALL (average) : Pearson = -0.0000,             Spearman = nan

2019-03-12 11:30:23,346 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 11:30:23,355 : loading BERT model bert-large-uncased
2019-03-12 11:30:23,356 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:30:23,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:30:23,374 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7zvf80gc
2019-03-12 11:30:30,826 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:30:39,274 : FNWN : pearson = -0.0000, spearman = nan
2019-03-12 11:30:43,523 : headlines : pearson = 0.0000, spearman = nan
2019-03-12 11:30:47,584 : OnWN : pearson = nan, spearman = nan
2019-03-12 11:30:47,584 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-03-12 11:30:47,584 : ALL (average) : Pearson = nan,             Spearman = nan

2019-03-12 11:30:47,584 : ***** Transfer task : STS14 *****


2019-03-12 11:30:47,636 : loading BERT model bert-large-uncased
2019-03-12 11:30:47,636 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:30:47,654 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:30:47,654 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpah_mkp88
2019-03-12 11:30:55,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:31:04,232 : deft-forum : pearson = -0.0000, spearman = nan
2019-03-12 11:31:07,360 : deft-news : pearson = nan, spearman = nan
2019-03-12 11:31:12,967 : headlines : pearson = -0.0000, spearman = nan
2019-03-12 11:31:18,481 : images : pearson = -0.0000, spearman = nan
2019-03-12 11:31:24,123 : OnWN : pearson = -0.0000, spearman = nan
2019-03-12 11:31:30,607 : tweet-news : pearson = -0.0000, spearman = nan
2019-03-12 11:31:30,607 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-03-12 11:31:30,607 : ALL (average) : Pearson = nan,             Spearman = nan

2019-03-12 11:31:30,607 : ***** Transfer task : STS15 *****


2019-03-12 11:31:30,639 : loading BERT model bert-large-uncased
2019-03-12 11:31:30,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:31:30,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:31:30,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuaqy94iy
2019-03-12 11:31:38,125 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:31:47,600 : answers-forums : pearson = 0.0000, spearman = nan
2019-03-12 11:31:53,113 : answers-students : pearson = -0.0000, spearman = nan
2019-03-12 11:31:57,469 : belief : pearson = 0.0000, spearman = nan
2019-03-12 11:32:02,218 : headlines : pearson = -0.0000, spearman = nan
2019-03-12 11:32:06,965 : images : pearson = -0.0000, spearman = nan
2019-03-12 11:32:06,966 : ALL (weighted average) : Pearson = -0.0000,             Spearman = nan
2019-03-12 11:32:06,966 : ALL (average) : Pearson = -0.0000,             Spearman = nan

2019-03-12 11:32:06,966 : ***** Transfer task : STS16 *****


2019-03-12 11:32:07,033 : loading BERT model bert-large-uncased
2019-03-12 11:32:07,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:07,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:07,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqv17_gcz
2019-03-12 11:32:14,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:32:20,647 : answer-answer : pearson = nan, spearman = nan
2019-03-12 11:32:21,309 : headlines : pearson = nan, spearman = nan
2019-03-12 11:32:22,194 : plagiarism : pearson = 0.0000, spearman = nan
2019-03-12 11:32:23,695 : postediting : pearson = -0.0000, spearman = nan
2019-03-12 11:32:24,303 : question-question : pearson = 0.0000, spearman = nan
2019-03-12 11:32:24,303 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-03-12 11:32:24,303 : ALL (average) : Pearson = nan,             Spearman = nan

2019-03-12 11:32:24,303 : ***** Transfer task : MR *****


2019-03-12 11:32:24,319 : loading BERT model bert-large-uncased
2019-03-12 11:32:24,319 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:24,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:24,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyf1vyihk
2019-03-12 11:32:31,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:32:37,135 : Generating sentence embeddings
2019-03-12 11:33:08,591 : Generated sentence embeddings
2019-03-12 11:33:08,592 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:33:16,229 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-03-12 11:33:24,378 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-03-12 11:33:32,402 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-03-12 11:33:40,579 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-03-12 11:33:47,320 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-03-12 11:33:47,659 : Dev acc : 50.0 Test acc : 50.0

2019-03-12 11:33:47,660 : ***** Transfer task : CR *****


2019-03-12 11:33:47,668 : loading BERT model bert-large-uncased
2019-03-12 11:33:47,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:33:47,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:33:47,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplcrnthj_
2019-03-12 11:33:55,139 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:34:00,432 : Generating sentence embeddings
2019-03-12 11:34:08,747 : Generated sentence embeddings
2019-03-12 11:34:08,747 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:34:11,527 : Best param found at split 1: l2reg = 1e-05                 with score 63.76
2019-03-12 11:34:14,509 : Best param found at split 2: l2reg = 1e-05                 with score 63.76
2019-03-12 11:34:17,668 : Best param found at split 3: l2reg = 1e-05                 with score 63.77
2019-03-12 11:34:20,260 : Best param found at split 4: l2reg = 0.0001                 with score 63.75
2019-03-12 11:34:23,078 : Best param found at split 5: l2reg = 0.0001                 with score 63.75
2019-03-12 11:34:23,235 : Dev acc : 63.76 Test acc : 63.76

2019-03-12 11:34:23,236 : ***** Transfer task : MPQA *****


2019-03-12 11:34:23,282 : loading BERT model bert-large-uncased
2019-03-12 11:34:23,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:34:23,300 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:34:23,300 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp24k0zz5s
2019-03-12 11:34:30,745 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:34:35,952 : Generating sentence embeddings
2019-03-12 11:34:43,516 : Generated sentence embeddings
2019-03-12 11:34:43,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:34:50,616 : Best param found at split 1: l2reg = 0.0001                 with score 61.27
2019-03-12 11:34:59,415 : Best param found at split 2: l2reg = 0.0001                 with score 68.78
2019-03-12 11:35:08,307 : Best param found at split 3: l2reg = 1e-05                 with score 68.77
2019-03-12 11:35:16,276 : Best param found at split 4: l2reg = 1e-05                 with score 68.77
2019-03-12 11:35:25,997 : Best param found at split 5: l2reg = 1e-05                 with score 68.77
2019-03-12 11:35:26,431 : Dev acc : 67.27 Test acc : 68.77

2019-03-12 11:35:26,432 : ***** Transfer task : SUBJ *****


2019-03-12 11:35:26,485 : loading BERT model bert-large-uncased
2019-03-12 11:35:26,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:35:26,542 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:35:26,542 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd0iwjg7u
2019-03-12 11:35:33,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:35:39,312 : Generating sentence embeddings
2019-03-12 11:36:10,100 : Generated sentence embeddings
2019-03-12 11:36:10,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:36:17,372 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-03-12 11:36:24,871 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-03-12 11:36:32,493 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-03-12 11:36:38,953 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-03-12 11:36:46,500 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-03-12 11:36:46,953 : Dev acc : 50.0 Test acc : 50.0

2019-03-12 11:36:46,954 : ***** Transfer task : SST Binary classification *****


2019-03-12 11:36:47,130 : loading BERT model bert-large-uncased
2019-03-12 11:36:47,130 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:36:47,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:36:47,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprxhtl463
2019-03-12 11:36:54,648 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:36:59,932 : Computing embedding for train
2019-03-12 11:38:46,416 : Computed train embeddings
2019-03-12 11:38:46,416 : Computing embedding for dev
2019-03-12 11:38:50,790 : Computed dev embeddings
2019-03-12 11:38:50,791 : Computing embedding for test
2019-03-12 11:39:00,278 : Computed test embeddings
2019-03-12 11:39:00,278 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:39:38,455 : [('reg:1e-05', 50.92), ('reg:0.0001', 50.92), ('reg:0.001', 50.92), ('reg:0.01', 50.92)]
2019-03-12 11:39:38,455 : Validation : best param found is reg = 1e-05 with score             50.92
2019-03-12 11:39:38,455 : Evaluating...
2019-03-12 11:39:45,056 : 
Dev acc : 50.92 Test acc : 49.92 for             SST Binary classification

2019-03-12 11:39:45,057 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 11:39:45,149 : loading BERT model bert-large-uncased
2019-03-12 11:39:45,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:39:45,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:39:45,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy96wdbt9
2019-03-12 11:39:52,641 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:39:58,192 : Computing embedding for train
2019-03-12 11:40:38,795 : Computed train embeddings
2019-03-12 11:40:38,795 : Computing embedding for dev
2019-03-12 11:40:44,489 : Computed dev embeddings
2019-03-12 11:40:44,490 : Computing embedding for test
2019-03-12 11:40:56,185 : Computed test embeddings
2019-03-12 11:40:56,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:41:00,272 : [('reg:1e-05', 26.25), ('reg:0.0001', 26.25), ('reg:0.001', 26.25), ('reg:0.01', 26.25)]
2019-03-12 11:41:00,272 : Validation : best param found is reg = 1e-05 with score             26.25
2019-03-12 11:41:00,273 : Evaluating...
2019-03-12 11:41:01,354 : 
Dev acc : 26.25 Test acc : 28.64 for             SST Fine-Grained classification

2019-03-12 11:41:01,355 : ***** Transfer task : TREC *****


2019-03-12 11:41:01,412 : loading BERT model bert-large-uncased
2019-03-12 11:41:01,412 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:41:01,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:41:01,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr197r0o4
2019-03-12 11:41:08,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:41:29,271 : Computed train embeddings
2019-03-12 11:41:30,328 : Computed test embeddings
2019-03-12 11:41:30,328 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:41:39,907 : [('reg:1e-05', 22.74), ('reg:0.0001', 22.74), ('reg:0.001', 22.74), ('reg:0.01', 22.74)]
2019-03-12 11:41:39,907 : Cross-validation : best param found is reg = 1e-05             with score 22.74
2019-03-12 11:41:39,907 : Evaluating...
2019-03-12 11:41:40,603 : 
Dev acc : 22.74 Test acc : 18.8             for TREC

2019-03-12 11:41:40,604 : ***** Transfer task : MRPC *****


2019-03-12 11:41:40,690 : loading BERT model bert-large-uncased
2019-03-12 11:41:40,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:41:40,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:41:40,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4_vxec76
2019-03-12 11:41:48,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:41:53,742 : Computing embedding for train
2019-03-12 11:42:34,009 : Computed train embeddings
2019-03-12 11:42:34,009 : Computing embedding for test
2019-03-12 11:42:51,467 : Computed test embeddings
2019-03-12 11:42:51,488 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:42:59,560 : [('reg:1e-05', 67.54), ('reg:0.0001', 67.54), ('reg:0.001', 67.54), ('reg:0.01', 67.54)]
2019-03-12 11:42:59,560 : Cross-validation : best param found is reg = 1e-05             with score 67.54
2019-03-12 11:42:59,560 : Evaluating...
2019-03-12 11:43:00,013 : Dev acc : 67.54 Test acc 66.49; Test F1 79.87 for MRPC.

2019-03-12 11:43:00,013 : ***** Transfer task : SICK-Entailment*****


2019-03-12 11:43:00,047 : loading BERT model bert-large-uncased
2019-03-12 11:43:00,047 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:43:00,109 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:43:00,109 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpje8_a7h0
2019-03-12 11:43:07,582 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:43:13,208 : Computing embedding for train
2019-03-12 11:43:35,472 : Computed train embeddings
2019-03-12 11:43:35,472 : Computing embedding for dev
2019-03-12 11:43:38,715 : Computed dev embeddings
2019-03-12 11:43:38,716 : Computing embedding for test
2019-03-12 11:44:02,835 : Computed test embeddings
2019-03-12 11:44:02,872 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:44:05,361 : [('reg:1e-05', 56.4), ('reg:0.0001', 56.4), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-03-12 11:44:05,361 : Validation : best param found is reg = 1e-05 with score             56.4
2019-03-12 11:44:05,361 : Evaluating...
2019-03-12 11:44:06,099 : 
Dev acc : 56.4 Test acc : 56.69 for                        SICK entailment

2019-03-12 11:44:06,099 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 11:44:06,127 : loading BERT model bert-large-uncased
2019-03-12 11:44:06,128 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:44:06,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:44:06,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpft17k7os
2019-03-12 11:44:13,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:44:19,022 : Computing embedding for train
2019-03-12 11:44:39,512 : Computed train embeddings
2019-03-12 11:44:39,512 : Computing embedding for dev
2019-03-12 11:44:42,396 : Computed dev embeddings
2019-03-12 11:44:42,396 : Computing embedding for test
2019-03-12 11:45:04,854 : Computed test embeddings
2019-03-12 11:45:45,065 : Dev : Pearson 0
2019-03-12 11:45:45,065 : Test : Pearson 0 Spearman 0 MSE 1.018703588669647                        for SICK Relatedness

2019-03-12 11:45:45,066 : 

***** Transfer task : STSBenchmark*****


2019-03-12 11:45:45,108 : loading BERT model bert-large-uncased
2019-03-12 11:45:45,108 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:45:45,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:45:45,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2qr_70zn
2019-03-12 11:45:52,669 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:45:58,244 : Computing embedding for train
2019-03-12 11:46:36,790 : Computed train embeddings
2019-03-12 11:46:36,791 : Computing embedding for dev
2019-03-12 11:46:46,489 : Computed dev embeddings
2019-03-12 11:46:46,489 : Computing embedding for test
2019-03-12 11:46:54,659 : Computed test embeddings
2019-03-12 11:47:28,639 : Dev : Pearson 0
2019-03-12 11:47:28,639 : Test : Pearson 0 Spearman 0 MSE 2.495342069274611                        for SICK Relatedness

2019-03-12 11:47:28,639 : ***** Transfer task : SNLI Entailment*****


2019-03-12 11:47:34,074 : loading BERT model bert-large-uncased
2019-03-12 11:47:34,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:47:34,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:47:34,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3ulsc2p1
2019-03-12 11:47:41,699 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:47:48,363 : PROGRESS (encoding): 0.00%
2019-03-12 11:53:43,400 : PROGRESS (encoding): 14.56%
2019-03-12 12:00:04,178 : PROGRESS (encoding): 29.12%
2019-03-12 12:06:34,523 : PROGRESS (encoding): 43.69%
2019-03-12 12:13:23,751 : PROGRESS (encoding): 58.25%
2019-03-12 12:20:47,251 : PROGRESS (encoding): 72.81%
2019-03-12 12:28:06,763 : PROGRESS (encoding): 87.37%
2019-03-12 12:35:35,984 : PROGRESS (encoding): 0.00%
2019-03-12 12:36:31,260 : PROGRESS (encoding): 0.00%
2019-03-12 12:37:26,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:38:35,918 : [('reg:1e-09', 33.82)]
2019-03-12 12:38:35,918 : Validation : best param found is reg = 1e-09 with score             33.82
2019-03-12 12:38:35,918 : Evaluating...
2019-03-12 12:39:43,373 : Dev acc : 33.82 Test acc : 34.28 for SNLI

2019-03-12 12:39:43,373 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 12:39:43,618 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 12:39:44,665 : loading BERT model bert-large-uncased
2019-03-12 12:39:44,665 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:39:44,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:39:44,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr33l9z_6
2019-03-12 12:39:52,199 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:39:57,479 : Computing embeddings for train/dev/test
2019-03-12 12:46:49,522 : Computed embeddings
2019-03-12 12:46:49,523 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:47:28,410 : [('reg:1e-05', 16.67), ('reg:0.0001', 16.67), ('reg:0.001', 16.67), ('reg:0.01', 16.67)]
2019-03-12 12:47:28,410 : Validation : best param found is reg = 1e-05 with score             16.67
2019-03-12 12:47:28,410 : Evaluating...
2019-03-12 12:47:36,507 : 
Dev acc : 16.7 Test acc : 16.7 for LENGTH classification

2019-03-12 12:47:36,507 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 12:47:36,966 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 12:47:37,012 : loading BERT model bert-large-uncased
2019-03-12 12:47:37,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:47:37,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:47:37,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxdq6n622
2019-03-12 12:47:44,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:47:49,940 : Computing embeddings for train/dev/test
2019-03-12 12:54:03,035 : Computed embeddings
2019-03-12 12:54:03,035 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:54:47,488 : [('reg:1e-05', 0.1), ('reg:0.0001', 0.1), ('reg:0.001', 0.1), ('reg:0.01', 0.1)]
2019-03-12 12:54:47,488 : Validation : best param found is reg = 1e-05 with score             0.1
2019-03-12 12:54:47,488 : Evaluating...
2019-03-12 12:54:58,212 : 
Dev acc : 0.1 Test acc : 0.1 for WORDCONTENT classification

2019-03-12 12:54:58,213 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 12:54:58,596 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 12:54:58,664 : loading BERT model bert-large-uncased
2019-03-12 12:54:58,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:54:58,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:54:58,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3kl8w8m0
2019-03-12 12:55:06,182 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:55:11,744 : Computing embeddings for train/dev/test
2019-03-12 13:01:19,708 : Computed embeddings
2019-03-12 13:01:19,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:02:00,290 : [('reg:1e-05', 18.07), ('reg:0.0001', 18.07), ('reg:0.001', 18.07), ('reg:0.01', 18.07)]
2019-03-12 13:02:00,290 : Validation : best param found is reg = 1e-05 with score             18.07
2019-03-12 13:02:00,290 : Evaluating...
2019-03-12 13:02:10,924 : 
Dev acc : 18.1 Test acc : 17.9 for DEPTH classification

2019-03-12 13:02:10,925 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 13:02:11,332 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 13:02:11,394 : loading BERT model bert-large-uncased
2019-03-12 13:02:11,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:02:11,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:02:11,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9ya59tok
2019-03-12 13:02:19,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:02:24,474 : Computing embeddings for train/dev/test
2019-03-12 13:08:05,153 : Computed embeddings
2019-03-12 13:08:05,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:08:47,729 : [('reg:1e-05', 5.0), ('reg:0.0001', 5.0), ('reg:0.001', 5.0), ('reg:0.01', 5.0)]
2019-03-12 13:08:47,729 : Validation : best param found is reg = 1e-05 with score             5.0
2019-03-12 13:08:47,730 : Evaluating...
2019-03-12 13:08:57,329 : 
Dev acc : 5.0 Test acc : 5.0 for TOPCONSTITUENTS classification

2019-03-12 13:08:57,330 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 13:08:57,776 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 13:08:57,862 : loading BERT model bert-large-uncased
2019-03-12 13:08:57,862 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:08:57,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:08:57,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp14xux1v7
2019-03-12 13:09:05,455 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:09:11,030 : Computing embeddings for train/dev/test
2019-03-12 13:12:54,877 : Computed embeddings
2019-03-12 13:12:54,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:13:13,341 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:13:13,341 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:13:13,341 : Evaluating...
2019-03-12 13:13:19,043 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-03-12 13:13:19,044 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 13:13:19,641 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 13:13:19,706 : loading BERT model bert-large-uncased
2019-03-12 13:13:19,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:13:19,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:13:19,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzxsmira5
2019-03-12 13:13:27,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:13:32,479 : Computing embeddings for train/dev/test
2019-03-12 13:16:31,285 : Computed embeddings
2019-03-12 13:16:31,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:16:52,495 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:16:52,495 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:16:52,495 : Evaluating...
2019-03-12 13:16:57,820 : 
Dev acc : 50.0 Test acc : 50.0 for TENSE classification

2019-03-12 13:16:57,821 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 13:16:58,329 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 13:16:58,392 : loading BERT model bert-large-uncased
2019-03-12 13:16:58,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:16:58,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:16:58,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpavrx394f
2019-03-12 13:17:05,855 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:17:11,188 : Computing embeddings for train/dev/test
2019-03-12 13:21:12,135 : Computed embeddings
2019-03-12 13:21:12,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:21:50,237 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:21:50,237 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:21:50,237 : Evaluating...
2019-03-12 13:21:59,494 : 
Dev acc : 50.0 Test acc : 50.0 for SUBJNUMBER classification

2019-03-12 13:21:59,495 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 13:21:59,920 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 13:21:59,987 : loading BERT model bert-large-uncased
2019-03-12 13:21:59,987 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:22:00,104 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:22:00,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa3s5umef
2019-03-12 13:22:07,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:22:12,977 : Computing embeddings for train/dev/test
2019-03-12 13:28:12,540 : Computed embeddings
2019-03-12 13:28:12,540 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:28:53,134 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:28:53,134 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:28:53,134 : Evaluating...
2019-03-12 13:29:01,484 : 
Dev acc : 50.0 Test acc : 50.0 for OBJNUMBER classification

2019-03-12 13:29:01,485 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 13:29:02,117 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 13:29:02,192 : loading BERT model bert-large-uncased
2019-03-12 13:29:02,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:29:02,218 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:29:02,219 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgybbqsri
2019-03-12 13:29:09,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:29:15,246 : Computing embeddings for train/dev/test
2019-03-12 13:36:09,367 : Computed embeddings
2019-03-12 13:36:09,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:36:47,260 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-03-12 13:36:47,260 : Validation : best param found is reg = 1e-05 with score             50.19
2019-03-12 13:36:47,260 : Evaluating...
2019-03-12 13:36:57,512 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-03-12 13:36:57,513 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 13:36:57,961 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 13:36:58,048 : loading BERT model bert-large-uncased
2019-03-12 13:36:58,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:36:58,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:36:58,178 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkv8sg0dd
2019-03-12 13:37:05,733 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:37:11,105 : Computing embeddings for train/dev/test
2019-03-12 13:44:02,060 : Computed embeddings
2019-03-12 13:44:02,060 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:44:43,759 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:44:43,759 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:44:43,759 : Evaluating...
2019-03-12 13:44:54,305 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-03-12 13:44:54,307 : total results: {'STS12': {'MSRpar': {'pearson': (2.364285406174688e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'MSRvid': {'pearson': (9.226551884700229e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (-3.4156817837760074e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (1.7941820861222277e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (-3.91724895414939e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 399}, 'all': {'pearson': {'mean': -4.503616114316917e-17, 'wmean': 2.1881008337078834e-17}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS13': {'FNWN': {'pearson': (-3.7929699211982904e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 189}, 'headlines': {'pearson': (9.357832684713867e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 561}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS14': {'deft-forum': {'pearson': (-4.056028757513065e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 450}, 'deft-news': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 300}, 'headlines': {'pearson': (-1.6593598400261966e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (-6.318803999578205e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (-1.76756178354105e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'tweet-news': {'pearson': (-1.3467361374227783e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS15': {'answers-forums': {'pearson': (1.03728452482575e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'answers-students': {'pearson': (-1.208487472700249e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'belief': {'pearson': (1.3716155386547843e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'headlines': {'pearson': (-4.689970649056253e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (-6.887503421918227e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': -2.3835776022129373e-17, 'wmean': -4.447529601130207e-17}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS16': {'answer-answer': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 254}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 249}, 'plagiarism': {'pearson': (1.7606849631727422e-16, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 230}, 'postediting': {'pearson': (-8.446974863002646e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 244}, 'question-question': {'pearson': (7.938926377237772e-17, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 209}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'MR': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 63.76, 'acc': 63.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 67.27, 'acc': 68.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 50.92, 'acc': 49.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 26.25, 'acc': 28.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 22.74, 'acc': 18.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.54, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 56.4, 'acc': 56.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 1.018703588669647, 'yhat': array([3.56311223, 3.56311223, 3.56311223, ..., 3.56311223, 3.56311223,        3.56311223]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 2.495342069274611, 'yhat': array([3.02001667, 3.02001667, 3.02001667, ..., 3.02001667, 3.02001667,        3.02001667]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 33.82, 'acc': 34.28, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 16.67, 'acc': 16.67, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.1, 'acc': 0.1, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 18.07, 'acc': 17.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 5.0, 'acc': 5.0, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 13:44:54,307 : STS12 p=0.0000, STS12 s=nan, STS13 p=nan, STS13 s=nan, STS14 p=nan, STS14 s=nan, STS15 p=-0.0000, STS15 s=nan, STS 16 p=nan, STS16 s=nan, STS B p=0.0000, STS B s=0.0000, STS B m=2.4953, SICK-R p=0.0000, SICK-R s=0.0000, SICK-P m=1.0187
2019-03-12 13:44:54,307 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 13:44:54,308 : 0.0000,nan,nan,nan,nan,nan,-0.0000,nan,nan,nan,0.0000,0.0000,2.4953,0.0000,0.0000,1.0187
2019-03-12 13:44:54,308 : MR=50.00, CR=63.76, SUBJ=50.00, MPQA=68.77, SST-B=49.92, SST-F=28.64, TREC=18.80, SICK-E=56.69, SNLI=34.28, MRPC=66.49, MRPC f=79.87
2019-03-12 13:44:54,308 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 13:44:54,308 : 50.00,63.76,50.00,68.77,49.92,28.64,18.80,56.69,34.28,66.49,79.87
2019-03-12 13:44:54,308 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 13:44:54,308 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 13:44:54,308 : na,na,na,na,na,na,na,na,na,na
2019-03-12 13:44:54,308 : SentLen=16.67, WC=0.10, TreeDepth=17.88, TopConst=5.00, BShift=50.00, Tense=50.00, SubjNum=50.00, ObjNum=50.00, SOMO=49.87, CoordInv=50.00, average=33.95
2019-03-12 13:44:54,308 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 13:44:54,308 : 16.67,0.10,17.88,5.00,50.00,50.00,50.00,50.00,49.87,50.00,33.95
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,308 : layer 1
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,308 : ********************************************************************************
2019-03-12 13:44:54,396 : ***** Transfer task : STS12 *****


2019-03-12 13:44:54,408 : loading BERT model bert-large-uncased
2019-03-12 13:44:54,408 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:44:54,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:44:54,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7now5okh
2019-03-12 13:45:01,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:45:15,170 : MSRpar : pearson = 0.1360, spearman = 0.1603
2019-03-12 13:45:18,745 : MSRvid : pearson = 0.2080, spearman = 0.2265
2019-03-12 13:45:21,496 : SMTeuroparl : pearson = 0.3491, spearman = 0.5217
2019-03-12 13:45:26,244 : surprise.OnWN : pearson = 0.1357, spearman = 0.2267
2019-03-12 13:45:28,579 : surprise.SMTnews : pearson = 0.1883, spearman = 0.3167
2019-03-12 13:45:28,579 : ALL (weighted average) : Pearson = 0.1915,             Spearman = 0.2657
2019-03-12 13:45:28,579 : ALL (average) : Pearson = 0.2034,             Spearman = 0.2904

2019-03-12 13:45:28,579 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 13:45:28,587 : loading BERT model bert-large-uncased
2019-03-12 13:45:28,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:45:28,606 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:45:28,606 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqd8ti2vd
2019-03-12 13:45:36,086 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:45:43,877 : FNWN : pearson = 0.0508, spearman = 0.0810
2019-03-12 13:45:48,088 : headlines : pearson = 0.2477, spearman = 0.2929
2019-03-12 13:45:50,905 : OnWN : pearson = -0.0990, spearman = -0.1407
2019-03-12 13:45:50,905 : ALL (weighted average) : Pearson = 0.0932,             Spearman = 0.1041
2019-03-12 13:45:50,905 : ALL (average) : Pearson = 0.0665,             Spearman = 0.0778

2019-03-12 13:45:50,906 : ***** Transfer task : STS14 *****


2019-03-12 13:45:50,922 : loading BERT model bert-large-uncased
2019-03-12 13:45:50,923 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:45:50,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:45:50,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu3q8de8o
2019-03-12 13:45:58,432 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:46:06,623 : deft-forum : pearson = 0.0778, spearman = 0.1213
2019-03-12 13:46:09,137 : deft-news : pearson = 0.2677, spearman = 0.4161
2019-03-12 13:46:13,282 : headlines : pearson = 0.2444, spearman = 0.2618
2019-03-12 13:46:17,174 : images : pearson = 0.0879, spearman = 0.1577
2019-03-12 13:46:21,191 : OnWN : pearson = -0.0991, spearman = -0.0335
2019-03-12 13:46:26,710 : tweet-news : pearson = 0.1928, spearman = 0.2197
2019-03-12 13:46:26,710 : ALL (weighted average) : Pearson = 0.1159,             Spearman = 0.1690
2019-03-12 13:46:26,710 : ALL (average) : Pearson = 0.1286,             Spearman = 0.1905

2019-03-12 13:46:26,710 : ***** Transfer task : STS15 *****


2019-03-12 13:46:26,744 : loading BERT model bert-large-uncased
2019-03-12 13:46:26,744 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:46:26,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:46:26,762 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy6z59xd7
2019-03-12 13:46:34,230 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:46:43,117 : answers-forums : pearson = 0.1644, spearman = 0.2078
2019-03-12 13:46:47,093 : answers-students : pearson = 0.2594, spearman = 0.4233
2019-03-12 13:46:50,598 : belief : pearson = 0.1322, spearman = 0.2229
2019-03-12 13:46:54,793 : headlines : pearson = 0.3225, spearman = 0.3801
2019-03-12 13:46:58,802 : images : pearson = 0.0431, spearman = 0.2236
2019-03-12 13:46:58,802 : ALL (weighted average) : Pearson = 0.1933,             Spearman = 0.3106
2019-03-12 13:46:58,802 : ALL (average) : Pearson = 0.1843,             Spearman = 0.2916

2019-03-12 13:46:58,802 : ***** Transfer task : STS16 *****


2019-03-12 13:46:58,876 : loading BERT model bert-large-uncased
2019-03-12 13:46:58,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:46:58,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:46:58,897 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdtbq8olc
2019-03-12 13:47:06,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:47:13,616 : answer-answer : pearson = 0.0926, spearman = 0.1317
2019-03-12 13:47:14,805 : headlines : pearson = 0.3034, spearman = 0.3676
2019-03-12 13:47:16,639 : plagiarism : pearson = 0.2054, spearman = 0.2645
2019-03-12 13:47:19,256 : postediting : pearson = 0.1949, spearman = 0.6143
2019-03-12 13:47:20,428 : question-question : pearson = -0.2114, spearman = -0.2291
2019-03-12 13:47:20,428 : ALL (weighted average) : Pearson = 0.1262,             Spearman = 0.2427
2019-03-12 13:47:20,428 : ALL (average) : Pearson = 0.1170,             Spearman = 0.2298

2019-03-12 13:47:20,429 : ***** Transfer task : MR *****


2019-03-12 13:47:20,448 : loading BERT model bert-large-uncased
2019-03-12 13:47:20,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:47:20,466 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:47:20,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmponcpalb3
2019-03-12 13:47:27,956 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:47:33,425 : Generating sentence embeddings
2019-03-12 13:48:35,585 : Generated sentence embeddings
2019-03-12 13:48:35,585 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 13:49:01,447 : Best param found at split 1: l2reg = 1e-05                 with score 66.83
2019-03-12 13:49:25,887 : Best param found at split 2: l2reg = 1e-05                 with score 63.99
2019-03-12 13:49:48,178 : Best param found at split 3: l2reg = 0.0001                 with score 64.24
2019-03-12 13:50:11,874 : Best param found at split 4: l2reg = 0.001                 with score 64.91
2019-03-12 13:50:35,107 : Best param found at split 5: l2reg = 1e-05                 with score 65.69
2019-03-12 13:50:36,388 : Dev acc : 65.13 Test acc : 62.33

2019-03-12 13:50:36,389 : ***** Transfer task : CR *****


2019-03-12 13:50:36,396 : loading BERT model bert-large-uncased
2019-03-12 13:50:36,396 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:50:36,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:50:36,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7wg20z1j
2019-03-12 13:50:43,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:50:49,315 : Generating sentence embeddings
2019-03-12 13:51:04,603 : Generated sentence embeddings
2019-03-12 13:51:04,604 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 13:51:11,010 : Best param found at split 1: l2reg = 0.0001                 with score 69.43
2019-03-12 13:51:19,645 : Best param found at split 2: l2reg = 1e-05                 with score 70.92
2019-03-12 13:51:27,209 : Best param found at split 3: l2reg = 1e-05                 with score 69.3
2019-03-12 13:51:33,848 : Best param found at split 4: l2reg = 1e-05                 with score 70.18
2019-03-12 13:51:41,559 : Best param found at split 5: l2reg = 0.0001                 with score 70.41
2019-03-12 13:51:42,029 : Dev acc : 70.05 Test acc : 67.5

2019-03-12 13:51:42,030 : ***** Transfer task : MPQA *****


2019-03-12 13:51:42,041 : loading BERT model bert-large-uncased
2019-03-12 13:51:42,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:51:42,098 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:51:42,098 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8h6hq_va
2019-03-12 13:51:49,634 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:51:54,961 : Generating sentence embeddings
2019-03-12 13:52:10,259 : Generated sentence embeddings
2019-03-12 13:52:10,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 13:52:31,975 : Best param found at split 1: l2reg = 1e-05                 with score 82.47
2019-03-12 13:52:52,691 : Best param found at split 2: l2reg = 1e-05                 with score 83.63
2019-03-12 13:53:16,638 : Best param found at split 3: l2reg = 1e-05                 with score 83.22
2019-03-12 13:53:41,625 : Best param found at split 4: l2reg = 1e-05                 with score 83.71
2019-03-12 13:54:04,285 : Best param found at split 5: l2reg = 1e-05                 with score 83.84
2019-03-12 13:54:05,420 : Dev acc : 83.37 Test acc : 84.68

2019-03-12 13:54:05,421 : ***** Transfer task : SUBJ *****


2019-03-12 13:54:05,436 : loading BERT model bert-large-uncased
2019-03-12 13:54:05,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:54:05,457 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:54:05,457 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_ksdb02
2019-03-12 13:54:12,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:54:18,524 : Generating sentence embeddings
2019-03-12 13:55:15,264 : Generated sentence embeddings
2019-03-12 13:55:15,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 13:55:34,533 : Best param found at split 1: l2reg = 1e-05                 with score 86.46
2019-03-12 13:55:55,386 : Best param found at split 2: l2reg = 1e-05                 with score 86.71
2019-03-12 13:56:21,640 : Best param found at split 3: l2reg = 1e-05                 with score 87.2
2019-03-12 13:56:46,267 : Best param found at split 4: l2reg = 1e-05                 with score 87.29
2019-03-12 13:57:06,692 : Best param found at split 5: l2reg = 1e-05                 with score 87.18
2019-03-12 13:57:07,930 : Dev acc : 86.97 Test acc : 87.0

2019-03-12 13:57:07,931 : ***** Transfer task : SST Binary classification *****


2019-03-12 13:57:08,023 : loading BERT model bert-large-uncased
2019-03-12 13:57:08,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:57:08,105 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:57:08,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf06yp4h_
2019-03-12 13:57:15,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:57:21,217 : Computing embedding for train
2019-03-12 14:00:51,054 : Computed train embeddings
2019-03-12 14:00:51,054 : Computing embedding for dev
2019-03-12 14:00:55,751 : Computed dev embeddings
2019-03-12 14:00:55,751 : Computing embedding for test
2019-03-12 14:01:05,106 : Computed test embeddings
2019-03-12 14:01:05,106 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:01:42,482 : [('reg:1e-05', 74.2), ('reg:0.0001', 74.31), ('reg:0.001', 69.72), ('reg:0.01', 65.37)]
2019-03-12 14:01:42,482 : Validation : best param found is reg = 0.0001 with score             74.31
2019-03-12 14:01:42,482 : Evaluating...
2019-03-12 14:01:52,045 : 
Dev acc : 74.31 Test acc : 75.07 for             SST Binary classification

2019-03-12 14:01:52,045 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 14:01:52,101 : loading BERT model bert-large-uncased
2019-03-12 14:01:52,101 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:01:52,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:01:52,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp90_bofd1
2019-03-12 14:01:59,588 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:02:05,166 : Computing embedding for train
2019-03-12 14:02:48,676 : Computed train embeddings
2019-03-12 14:02:48,676 : Computing embedding for dev
2019-03-12 14:02:54,027 : Computed dev embeddings
2019-03-12 14:02:54,027 : Computing embedding for test
2019-03-12 14:03:05,474 : Computed test embeddings
2019-03-12 14:03:05,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:03:10,630 : [('reg:1e-05', 35.51), ('reg:0.0001', 33.51), ('reg:0.001', 30.88), ('reg:0.01', 31.88)]
2019-03-12 14:03:10,631 : Validation : best param found is reg = 1e-05 with score             35.51
2019-03-12 14:03:10,631 : Evaluating...
2019-03-12 14:03:12,001 : 
Dev acc : 35.51 Test acc : 36.43 for             SST Fine-Grained classification

2019-03-12 14:03:12,002 : ***** Transfer task : TREC *****


2019-03-12 14:03:12,016 : loading BERT model bert-large-uncased
2019-03-12 14:03:12,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:03:12,038 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:03:12,038 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4hg7h0yc
2019-03-12 14:03:19,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:03:41,116 : Computed train embeddings
2019-03-12 14:03:42,177 : Computed test embeddings
2019-03-12 14:03:42,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:03:55,700 : [('reg:1e-05', 58.21), ('reg:0.0001', 57.88), ('reg:0.001', 53.47), ('reg:0.01', 39.69)]
2019-03-12 14:03:55,700 : Cross-validation : best param found is reg = 1e-05             with score 58.21
2019-03-12 14:03:55,701 : Evaluating...
2019-03-12 14:03:56,551 : 
Dev acc : 58.21 Test acc : 75.8             for TREC

2019-03-12 14:03:56,552 : ***** Transfer task : MRPC *****


2019-03-12 14:03:56,574 : loading BERT model bert-large-uncased
2019-03-12 14:03:56,574 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:03:56,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:03:56,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1vzstmvk
2019-03-12 14:04:04,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:04:09,477 : Computing embedding for train
2019-03-12 14:04:49,143 : Computed train embeddings
2019-03-12 14:04:49,143 : Computing embedding for test
2019-03-12 14:05:05,755 : Computed test embeddings
2019-03-12 14:05:05,778 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:05:12,883 : [('reg:1e-05', 67.98), ('reg:0.0001', 68.48), ('reg:0.001', 67.93), ('reg:0.01', 67.64)]
2019-03-12 14:05:12,883 : Cross-validation : best param found is reg = 0.0001             with score 68.48
2019-03-12 14:05:12,883 : Evaluating...
2019-03-12 14:05:13,120 : Dev acc : 68.48 Test acc 67.3; Test F1 77.86 for MRPC.

2019-03-12 14:05:13,121 : ***** Transfer task : SICK-Entailment*****


2019-03-12 14:05:13,182 : loading BERT model bert-large-uncased
2019-03-12 14:05:13,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:05:13,202 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:05:13,202 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps51rt68m
2019-03-12 14:05:20,658 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:05:26,033 : Computing embedding for train
2019-03-12 14:05:51,678 : Computed train embeddings
2019-03-12 14:05:51,678 : Computing embedding for dev
2019-03-12 14:05:54,980 : Computed dev embeddings
2019-03-12 14:05:54,980 : Computing embedding for test
2019-03-12 14:06:27,001 : Computed test embeddings
2019-03-12 14:06:27,038 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:06:31,280 : [('reg:1e-05', 73.4), ('reg:0.0001', 70.0), ('reg:0.001', 64.4), ('reg:0.01', 61.2)]
2019-03-12 14:06:31,280 : Validation : best param found is reg = 1e-05 with score             73.4
2019-03-12 14:06:31,280 : Evaluating...
2019-03-12 14:06:32,946 : 
Dev acc : 73.4 Test acc : 73.19 for                        SICK entailment

2019-03-12 14:06:32,947 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 14:06:32,975 : loading BERT model bert-large-uncased
2019-03-12 14:06:32,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:06:33,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:06:33,035 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn70fjk1y
2019-03-12 14:06:40,588 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:06:46,146 : Computing embedding for train
2019-03-12 14:07:15,836 : Computed train embeddings
2019-03-12 14:07:15,836 : Computing embedding for dev
2019-03-12 14:07:19,763 : Computed dev embeddings
2019-03-12 14:07:19,763 : Computing embedding for test
2019-03-12 14:07:46,445 : Computed test embeddings
2019-03-12 14:08:46,606 : Dev : Pearson 0.679357112297888
2019-03-12 14:08:46,606 : Test : Pearson 0.7164877958419206 Spearman 0.6598833104605 MSE 0.5016616354030128                        for SICK Relatedness

2019-03-12 14:08:46,607 : 

***** Transfer task : STSBenchmark*****


2019-03-12 14:08:46,646 : loading BERT model bert-large-uncased
2019-03-12 14:08:46,647 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:08:46,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:08:46,675 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv06wds3v
2019-03-12 14:08:54,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:08:59,793 : Computing embedding for train
2019-03-12 14:09:33,371 : Computed train embeddings
2019-03-12 14:09:33,372 : Computing embedding for dev
2019-03-12 14:09:44,267 : Computed dev embeddings
2019-03-12 14:09:44,267 : Computing embedding for test
2019-03-12 14:09:52,761 : Computed test embeddings
2019-03-12 14:10:34,359 : Dev : Pearson 0.5295383483957088
2019-03-12 14:10:34,359 : Test : Pearson 0.4613368206747292 Spearman 0.4688047853459425 MSE 2.012822807738176                        for SICK Relatedness

2019-03-12 14:10:34,360 : ***** Transfer task : SNLI Entailment*****


2019-03-12 14:10:39,513 : loading BERT model bert-large-uncased
2019-03-12 14:10:39,514 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:10:39,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:10:39,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp81j4s1iu
2019-03-12 14:10:47,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:10:53,066 : PROGRESS (encoding): 0.00%
2019-03-12 14:16:51,207 : PROGRESS (encoding): 14.56%
2019-03-12 14:23:16,147 : PROGRESS (encoding): 29.12%
2019-03-12 14:29:34,368 : PROGRESS (encoding): 43.69%
2019-03-12 14:36:22,817 : PROGRESS (encoding): 58.25%
2019-03-12 14:43:41,176 : PROGRESS (encoding): 72.81%
2019-03-12 14:50:24,997 : PROGRESS (encoding): 87.37%
2019-03-12 14:54:24,189 : PROGRESS (encoding): 0.00%
2019-03-12 14:54:54,355 : PROGRESS (encoding): 0.00%
2019-03-12 14:55:23,303 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:55:58,595 : [('reg:1e-09', 60.81)]
2019-03-12 14:55:58,595 : Validation : best param found is reg = 1e-09 with score             60.81
2019-03-12 14:55:58,595 : Evaluating...
2019-03-12 14:56:35,382 : Dev acc : 60.81 Test acc : 61.04 for SNLI

2019-03-12 14:56:35,382 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 14:56:35,588 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 14:56:36,608 : loading BERT model bert-large-uncased
2019-03-12 14:56:36,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:56:36,635 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:56:36,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi775z7d5
2019-03-12 14:56:44,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:56:49,405 : Computing embeddings for train/dev/test
2019-03-12 15:00:51,976 : Computed embeddings
2019-03-12 15:00:51,976 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:01:42,449 : [('reg:1e-05', 88.67), ('reg:0.0001', 87.53), ('reg:0.001', 78.03), ('reg:0.01', 53.65)]
2019-03-12 15:01:42,449 : Validation : best param found is reg = 1e-05 with score             88.67
2019-03-12 15:01:42,449 : Evaluating...
2019-03-12 15:01:53,522 : 
Dev acc : 88.7 Test acc : 89.9 for LENGTH classification

2019-03-12 15:01:53,522 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 15:01:53,779 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 15:01:53,826 : loading BERT model bert-large-uncased
2019-03-12 15:01:53,826 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:01:53,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:01:53,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3gnytsy9
2019-03-12 15:02:01,316 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:02:06,681 : Computing embeddings for train/dev/test
2019-03-12 15:08:12,778 : Computed embeddings
2019-03-12 15:08:12,778 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:09:26,374 : [('reg:1e-05', 2.02), ('reg:0.0001', 0.39), ('reg:0.001', 0.2), ('reg:0.01', 0.14)]
2019-03-12 15:09:26,375 : Validation : best param found is reg = 1e-05 with score             2.02
2019-03-12 15:09:26,375 : Evaluating...
2019-03-12 15:09:43,675 : 
Dev acc : 2.0 Test acc : 1.7 for WORDCONTENT classification

2019-03-12 15:09:43,677 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 15:09:44,211 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 15:09:44,281 : loading BERT model bert-large-uncased
2019-03-12 15:09:44,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:09:44,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:09:44,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsuat8hr2
2019-03-12 15:09:51,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:09:57,235 : Computing embeddings for train/dev/test
2019-03-12 15:15:34,744 : Computed embeddings
2019-03-12 15:15:34,745 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:16:19,766 : [('reg:1e-05', 30.45), ('reg:0.0001', 29.32), ('reg:0.001', 25.05), ('reg:0.01', 19.66)]
2019-03-12 15:16:19,766 : Validation : best param found is reg = 1e-05 with score             30.45
2019-03-12 15:16:19,766 : Evaluating...
2019-03-12 15:16:33,093 : 
Dev acc : 30.4 Test acc : 30.5 for DEPTH classification

2019-03-12 15:16:33,094 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 15:16:33,487 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 15:16:33,553 : loading BERT model bert-large-uncased
2019-03-12 15:16:33,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:16:33,666 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:16:33,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeixas941
2019-03-12 15:16:41,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:16:46,843 : Computing embeddings for train/dev/test
2019-03-12 15:22:14,880 : Computed embeddings
2019-03-12 15:22:14,880 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:23:19,276 : [('reg:1e-05', 57.05), ('reg:0.0001', 47.89), ('reg:0.001', 31.75), ('reg:0.01', 15.97)]
2019-03-12 15:23:19,277 : Validation : best param found is reg = 1e-05 with score             57.05
2019-03-12 15:23:19,277 : Evaluating...
2019-03-12 15:23:38,105 : 
Dev acc : 57.0 Test acc : 57.8 for TOPCONSTITUENTS classification

2019-03-12 15:23:38,106 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 15:23:38,509 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 15:23:38,576 : loading BERT model bert-large-uncased
2019-03-12 15:23:38,576 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:23:38,607 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:23:38,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9aadarvq
2019-03-12 15:23:46,184 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:23:51,600 : Computing embeddings for train/dev/test
2019-03-12 15:29:57,514 : Computed embeddings
2019-03-12 15:29:57,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:30:54,252 : [('reg:1e-05', 52.34), ('reg:0.0001', 51.85), ('reg:0.001', 51.34), ('reg:0.01', 50.75)]
2019-03-12 15:30:54,252 : Validation : best param found is reg = 1e-05 with score             52.34
2019-03-12 15:30:54,252 : Evaluating...
2019-03-12 15:31:09,403 : 
Dev acc : 52.3 Test acc : 51.3 for BIGRAMSHIFT classification

2019-03-12 15:31:09,404 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 15:31:09,831 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 15:31:09,899 : loading BERT model bert-large-uncased
2019-03-12 15:31:09,899 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:31:09,931 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:31:09,931 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyw7x2w2x
2019-03-12 15:31:17,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:31:23,011 : Computing embeddings for train/dev/test
2019-03-12 15:37:18,042 : Computed embeddings
2019-03-12 15:37:18,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:38:27,767 : [('reg:1e-05', 82.01), ('reg:0.0001', 82.04), ('reg:0.001', 78.74), ('reg:0.01', 70.19)]
2019-03-12 15:38:27,767 : Validation : best param found is reg = 0.0001 with score             82.04
2019-03-12 15:38:27,767 : Evaluating...
2019-03-12 15:38:44,331 : 
Dev acc : 82.0 Test acc : 79.4 for TENSE classification

2019-03-12 15:38:44,333 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 15:38:44,760 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 15:38:44,824 : loading BERT model bert-large-uncased
2019-03-12 15:38:44,824 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:38:44,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:38:44,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpil6iz_a6
2019-03-12 15:38:52,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:38:58,034 : Computing embeddings for train/dev/test
2019-03-12 15:45:10,668 : Computed embeddings
2019-03-12 15:45:10,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:46:06,685 : [('reg:1e-05', 76.64), ('reg:0.0001', 76.23), ('reg:0.001', 74.25), ('reg:0.01', 69.67)]
2019-03-12 15:46:06,685 : Validation : best param found is reg = 1e-05 with score             76.64
2019-03-12 15:46:06,685 : Evaluating...
2019-03-12 15:46:21,369 : 
Dev acc : 76.6 Test acc : 74.9 for SUBJNUMBER classification

2019-03-12 15:46:21,370 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 15:46:21,783 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 15:46:21,849 : loading BERT model bert-large-uncased
2019-03-12 15:46:21,849 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:46:21,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:46:21,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_wag4e15
2019-03-12 15:46:29,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:46:34,971 : Computing embeddings for train/dev/test
2019-03-12 15:52:40,075 : Computed embeddings
2019-03-12 15:52:40,076 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:53:50,838 : [('reg:1e-05', 75.16), ('reg:0.0001', 74.13), ('reg:0.001', 69.86), ('reg:0.01', 65.19)]
2019-03-12 15:53:50,838 : Validation : best param found is reg = 1e-05 with score             75.16
2019-03-12 15:53:50,838 : Evaluating...
2019-03-12 15:54:10,114 : 
Dev acc : 75.2 Test acc : 75.6 for OBJNUMBER classification

2019-03-12 15:54:10,115 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 15:54:10,680 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 15:54:10,756 : loading BERT model bert-large-uncased
2019-03-12 15:54:10,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:54:10,787 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:54:10,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7_knz_i_
2019-03-12 15:54:18,312 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:54:23,826 : Computing embeddings for train/dev/test
2019-03-12 16:01:27,349 : Computed embeddings
2019-03-12 16:01:27,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:02:29,476 : [('reg:1e-05', 50.7), ('reg:0.0001', 50.28), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-03-12 16:02:29,476 : Validation : best param found is reg = 1e-05 with score             50.7
2019-03-12 16:02:29,476 : Evaluating...
2019-03-12 16:02:45,656 : 
Dev acc : 50.7 Test acc : 49.3 for ODDMANOUT classification

2019-03-12 16:02:45,657 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 16:02:46,049 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 16:02:46,143 : loading BERT model bert-large-uncased
2019-03-12 16:02:46,144 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:02:46,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:02:46,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkqxcra1n
2019-03-12 16:02:53,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:02:59,255 : Computing embeddings for train/dev/test
2019-03-12 16:09:56,174 : Computed embeddings
2019-03-12 16:09:56,174 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:10:49,010 : [('reg:1e-05', 53.24), ('reg:0.0001', 52.91), ('reg:0.001', 51.53), ('reg:0.01', 51.14)]
2019-03-12 16:10:49,010 : Validation : best param found is reg = 1e-05 with score             53.24
2019-03-12 16:10:49,011 : Evaluating...
2019-03-12 16:11:03,431 : 
Dev acc : 53.2 Test acc : 53.6 for COORDINATIONINVERSION classification

2019-03-12 16:11:03,433 : total results: {'STS12': {'MSRpar': {'pearson': (0.1359751654586618, 0.00018772836418160164), 'spearman': SpearmanrResult(correlation=0.1602588749950304, pvalue=1.0333060339531297e-05), 'nsamples': 750}, 'MSRvid': {'pearson': (0.20801549459181837, 8.911060011180792e-09), 'spearman': SpearmanrResult(correlation=0.22654639212470534, pvalue=3.481752416993998e-10), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3490730527824325, 1.3411605186612492e-14), 'spearman': SpearmanrResult(correlation=0.5217066510451362, pvalue=2.111543970063332e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.1357280956784546, 0.0001929041540481866), 'spearman': SpearmanrResult(correlation=0.22666491271016972, pvalue=3.407095734687583e-10), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.18830047735126376, 0.00015470281663875735), 'spearman': SpearmanrResult(correlation=0.3166935872123844, pvalue=9.577553814769748e-11), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.2034184571725262, 'wmean': 0.19148825240894204}, 'spearman': {'mean': 0.2903740836174852, 'wmean': 0.2657421907979048}}}, 'STS13': {'FNWN': {'pearson': (0.05084187391380137, 0.4871976767895939), 'spearman': SpearmanrResult(correlation=0.08103848729267767, pvalue=0.26763367302190444), 'nsamples': 189}, 'headlines': {'pearson': (0.24774148895081344, 5.947108794245975e-12), 'spearman': SpearmanrResult(correlation=0.29289406925939443, pvalue=2.6540158125458614e-16), 'nsamples': 750}, 'OnWN': {'pearson': (-0.09904263921314913, 0.018955362980746217), 'spearman': SpearmanrResult(correlation=-0.14065809400091672, pvalue=0.0008355527166229153), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.06651357455048855, 'wmean': 0.09323487352282792}, 'spearman': {'mean': 0.07775815418371845, 'wmean': 0.10405175687223175}}}, 'STS14': {'deft-forum': {'pearson': (0.07784907756626129, 0.09907820660270017), 'spearman': SpearmanrResult(correlation=0.12125470805428362, pvalue=0.010036984402521217), 'nsamples': 450}, 'deft-news': {'pearson': (0.267710013518432, 2.555527958189841e-06), 'spearman': SpearmanrResult(correlation=0.4160705330844989, pvalue=5.4699860445128444e-14), 'nsamples': 300}, 'headlines': {'pearson': (0.24438826360737145, 1.1622574521981647e-11), 'spearman': SpearmanrResult(correlation=0.2618097454777227, pvalue=3.206075311885014e-13), 'nsamples': 750}, 'images': {'pearson': (0.08788394525419438, 0.01606453276994813), 'spearman': SpearmanrResult(correlation=0.15767591182985194, pvalue=1.4372332380233973e-05), 'nsamples': 750}, 'OnWN': {'pearson': (-0.09912573992992758, 0.006590924004200108), 'spearman': SpearmanrResult(correlation=-0.03354009844521001, pvalue=0.3590067064173249), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19279238006219568, 1.0318308665203426e-07), 'spearman': SpearmanrResult(correlation=0.21966756918819733, pvalue=1.2001270259208724e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.12858299001308787, 'wmean': 0.1159464601881927}, 'spearman': {'mean': 0.19048972819822407, 'wmean': 0.1689588332233863}}}, 'STS15': {'answers-forums': {'pearson': (0.16444186377025263, 0.0013956334225564487), 'spearman': SpearmanrResult(correlation=0.20784007954936784, pvalue=4.997506300777782e-05), 'nsamples': 375}, 'answers-students': {'pearson': (0.2593787799132833, 5.378549365773599e-13), 'spearman': SpearmanrResult(correlation=0.4233491038131116, pvalue=5.6945542202846095e-34), 'nsamples': 750}, 'belief': {'pearson': (0.1321909838491527, 0.010389872446898589), 'spearman': SpearmanrResult(correlation=0.22285272305322684, pvalue=1.3244987023698846e-05), 'nsamples': 375}, 'headlines': {'pearson': (0.3224900095665124, 1.3022822165199464e-19), 'spearman': SpearmanrResult(correlation=0.3801464311609892, pvalue=3.385853808138572e-27), 'nsamples': 750}, 'images': {'pearson': (0.04314523301811853, 0.23793672980713218), 'spearman': SpearmanrResult(correlation=0.22360360490804057, pvalue=5.940956325398677e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.18432937402346392, 'wmean': 0.19333261157690423}, 'spearman': {'mean': 0.2915583884969472, 'wmean': 0.3106113852958597}}}, 'STS16': {'answer-answer': {'pearson': (0.09262606077749928, 0.14099343633925218), 'spearman': SpearmanrResult(correlation=0.1317288084002373, pvalue=0.035886747441691756), 'nsamples': 254}, 'headlines': {'pearson': (0.3034117779837708, 1.0653251011819337e-06), 'spearman': SpearmanrResult(correlation=0.3676474182359114, pvalue=2.1901603020160368e-09), 'nsamples': 249}, 'plagiarism': {'pearson': (0.20539259825275685, 0.0017396609323025425), 'spearman': SpearmanrResult(correlation=0.264504400681929, pvalue=4.862602431860848e-05), 'nsamples': 230}, 'postediting': {'pearson': (0.194900954961978, 0.0022267251762834116), 'spearman': SpearmanrResult(correlation=0.6142622921657171, pvalue=1.0581569501866672e-26), 'nsamples': 244}, 'question-question': {'pearson': (-0.21143715231819663, 0.002117769080406259), 'spearman': SpearmanrResult(correlation=-0.22908028912066014, pvalue=0.00084873311613933), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.11697884793156166, 'wmean': 0.12620768796778867}, 'spearman': {'mean': 0.22981252607262698, 'wmean': 0.24269945657121664}}}, 'MR': {'devacc': 65.13, 'acc': 62.33, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.05, 'acc': 67.5, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.37, 'acc': 84.68, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.97, 'acc': 87.0, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 74.31, 'acc': 75.07, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.51, 'acc': 36.43, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 58.21, 'acc': 75.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.48, 'acc': 67.3, 'f1': 77.86, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.4, 'acc': 73.19, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.679357112297888, 'pearson': 0.7164877958419206, 'spearman': 0.6598833104605, 'mse': 0.5016616354030128, 'yhat': array([2.98946878, 4.9353144 , 1.42660906, ..., 3.2833661 , 4.00130709,        4.23173875]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5295383483957088, 'pearson': 0.4613368206747292, 'spearman': 0.4688047853459425, 'mse': 2.012822807738176, 'yhat': array([1.53634953, 1.82507309, 2.02292417, ..., 3.50887959, 3.44846272,        3.39839225]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.81, 'acc': 61.04, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 88.67, 'acc': 89.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 2.02, 'acc': 1.71, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.45, 'acc': 30.47, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.05, 'acc': 57.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 52.34, 'acc': 51.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.04, 'acc': 79.39, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.64, 'acc': 74.91, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.16, 'acc': 75.61, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.7, 'acc': 49.31, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.24, 'acc': 53.59, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 16:11:03,433 : STS12 p=0.1915, STS12 s=0.2657, STS13 p=0.0932, STS13 s=0.1041, STS14 p=0.1159, STS14 s=0.1690, STS15 p=0.1933, STS15 s=0.3106, STS 16 p=0.1262, STS16 s=0.2427, STS B p=0.4613, STS B s=0.4688, STS B m=2.0128, SICK-R p=0.7165, SICK-R s=0.6599, SICK-P m=0.5017
2019-03-12 16:11:03,433 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 16:11:03,433 : 0.1915,0.2657,0.0932,0.1041,0.1159,0.1690,0.1933,0.3106,0.1262,0.2427,0.4613,0.4688,2.0128,0.7165,0.6599,0.5017
2019-03-12 16:11:03,433 : MR=62.33, CR=67.50, SUBJ=87.00, MPQA=84.68, SST-B=75.07, SST-F=36.43, TREC=75.80, SICK-E=73.19, SNLI=61.04, MRPC=67.30, MRPC f=77.86
2019-03-12 16:11:03,433 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 16:11:03,433 : 62.33,67.50,87.00,84.68,75.07,36.43,75.80,73.19,61.04,67.30,77.86
2019-03-12 16:11:03,433 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 16:11:03,433 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 16:11:03,433 : na,na,na,na,na,na,na,na,na,na
2019-03-12 16:11:03,433 : SentLen=89.86, WC=1.71, TreeDepth=30.47, TopConst=57.79, BShift=51.34, Tense=79.39, SubjNum=74.91, ObjNum=75.61, SOMO=49.31, CoordInv=53.59, average=56.40
2019-03-12 16:11:03,433 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 16:11:03,433 : 89.86,1.71,30.47,57.79,51.34,79.39,74.91,75.61,49.31,53.59,56.40
2019-03-12 16:11:03,433 : ********************************************************************************
2019-03-12 16:11:03,433 : ********************************************************************************
2019-03-12 16:11:03,434 : ********************************************************************************
2019-03-12 16:11:03,434 : layer 2
2019-03-12 16:11:03,434 : ********************************************************************************
2019-03-12 16:11:03,434 : ********************************************************************************
2019-03-12 16:11:03,434 : ********************************************************************************
2019-03-12 16:11:03,528 : ***** Transfer task : STS12 *****


2019-03-12 16:11:03,540 : loading BERT model bert-large-uncased
2019-03-12 16:11:03,540 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:11:03,558 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:11:03,558 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg6nftob5
2019-03-12 16:11:11,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:11:23,730 : MSRpar : pearson = 0.2832, spearman = 0.3210
2019-03-12 16:11:27,200 : MSRvid : pearson = 0.5283, spearman = 0.5368
2019-03-12 16:11:30,336 : SMTeuroparl : pearson = 0.4788, spearman = 0.5681
2019-03-12 16:11:35,579 : surprise.OnWN : pearson = 0.3051, spearman = 0.3928
2019-03-12 16:11:38,080 : surprise.SMTnews : pearson = 0.4915, spearman = 0.4346
2019-03-12 16:11:38,080 : ALL (weighted average) : Pearson = 0.4032,             Spearman = 0.4415
2019-03-12 16:11:38,080 : ALL (average) : Pearson = 0.4174,             Spearman = 0.4506

2019-03-12 16:11:38,080 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 16:11:38,089 : loading BERT model bert-large-uncased
2019-03-12 16:11:38,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:11:38,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:11:38,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4cw11ud8
2019-03-12 16:11:45,637 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:11:53,048 : FNWN : pearson = 0.2121, spearman = 0.2272
2019-03-12 16:11:56,865 : headlines : pearson = 0.3727, spearman = 0.3988
2019-03-12 16:11:59,975 : OnWN : pearson = 0.0352, spearman = 0.0556
2019-03-12 16:11:59,975 : ALL (weighted average) : Pearson = 0.2262,             Spearman = 0.2488
2019-03-12 16:11:59,975 : ALL (average) : Pearson = 0.2067,             Spearman = 0.2272

2019-03-12 16:11:59,976 : ***** Transfer task : STS14 *****


2019-03-12 16:11:59,990 : loading BERT model bert-large-uncased
2019-03-12 16:11:59,991 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:12:00,008 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:12:00,008 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf3pihomm
2019-03-12 16:12:07,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:12:15,704 : deft-forum : pearson = 0.2212, spearman = 0.2803
2019-03-12 16:12:18,586 : deft-news : pearson = 0.5192, spearman = 0.5681
2019-03-12 16:12:23,077 : headlines : pearson = 0.3316, spearman = 0.3344
2019-03-12 16:12:27,466 : images : pearson = 0.5391, spearman = 0.5339
2019-03-12 16:12:31,168 : OnWN : pearson = 0.1245, spearman = 0.2447
2019-03-12 16:12:36,547 : tweet-news : pearson = 0.3747, spearman = 0.3902
2019-03-12 16:12:36,547 : ALL (weighted average) : Pearson = 0.3420,             Spearman = 0.3797
2019-03-12 16:12:36,547 : ALL (average) : Pearson = 0.3517,             Spearman = 0.3919

2019-03-12 16:12:36,547 : ***** Transfer task : STS15 *****


2019-03-12 16:12:36,583 : loading BERT model bert-large-uncased
2019-03-12 16:12:36,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:12:36,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:12:36,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb7oichs_
2019-03-12 16:12:44,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:12:53,025 : answers-forums : pearson = 0.4520, spearman = 0.4679
2019-03-12 16:12:57,349 : answers-students : pearson = 0.4494, spearman = 0.5780
2019-03-12 16:13:01,312 : belief : pearson = 0.4522, spearman = 0.4924
2019-03-12 16:13:05,419 : headlines : pearson = 0.4452, spearman = 0.4825
2019-03-12 16:13:09,503 : images : pearson = 0.3982, spearman = 0.5572
2019-03-12 16:13:09,504 : ALL (weighted average) : Pearson = 0.4362,             Spearman = 0.5245
2019-03-12 16:13:09,504 : ALL (average) : Pearson = 0.4394,             Spearman = 0.5156

2019-03-12 16:13:09,504 : ***** Transfer task : STS16 *****


2019-03-12 16:13:09,575 : loading BERT model bert-large-uncased
2019-03-12 16:13:09,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:13:09,593 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:13:09,593 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpghpuft_h
2019-03-12 16:13:17,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:13:24,336 : answer-answer : pearson = 0.3488, spearman = 0.4325
2019-03-12 16:13:25,695 : headlines : pearson = 0.4190, spearman = 0.4728
2019-03-12 16:13:27,923 : plagiarism : pearson = 0.6322, spearman = 0.6514
2019-03-12 16:13:30,784 : postediting : pearson = 0.6266, spearman = 0.7935
2019-03-12 16:13:32,179 : question-question : pearson = 0.2620, spearman = 0.3033
2019-03-12 16:13:32,180 : ALL (weighted average) : Pearson = 0.4604,             Spearman = 0.5349
2019-03-12 16:13:32,180 : ALL (average) : Pearson = 0.4577,             Spearman = 0.5307

2019-03-12 16:13:32,180 : ***** Transfer task : MR *****


2019-03-12 16:13:32,195 : loading BERT model bert-large-uncased
2019-03-12 16:13:32,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:13:32,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:13:32,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_k861_xd
2019-03-12 16:13:39,693 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:13:45,241 : Generating sentence embeddings
2019-03-12 16:14:46,008 : Generated sentence embeddings
2019-03-12 16:14:46,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:15:07,149 : Best param found at split 1: l2reg = 0.0001                 with score 71.12
2019-03-12 16:15:28,703 : Best param found at split 2: l2reg = 1e-05                 with score 71.58
2019-03-12 16:15:51,878 : Best param found at split 3: l2reg = 0.0001                 with score 71.2
2019-03-12 16:16:10,868 : Best param found at split 4: l2reg = 1e-05                 with score 70.54
2019-03-12 16:16:30,671 : Best param found at split 5: l2reg = 1e-05                 with score 70.52
2019-03-12 16:16:31,833 : Dev acc : 70.99 Test acc : 70.31

2019-03-12 16:16:31,834 : ***** Transfer task : CR *****


2019-03-12 16:16:31,842 : loading BERT model bert-large-uncased
2019-03-12 16:16:31,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:16:31,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:16:31,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpemvqx4vv
2019-03-12 16:16:39,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:16:44,867 : Generating sentence embeddings
2019-03-12 16:16:59,919 : Generated sentence embeddings
2019-03-12 16:16:59,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:17:06,797 : Best param found at split 1: l2reg = 1e-05                 with score 75.92
2019-03-12 16:17:13,756 : Best param found at split 2: l2reg = 0.0001                 with score 75.32
2019-03-12 16:17:20,894 : Best param found at split 3: l2reg = 0.0001                 with score 76.03
2019-03-12 16:17:29,259 : Best param found at split 4: l2reg = 1e-05                 with score 76.23
2019-03-12 16:17:38,232 : Best param found at split 5: l2reg = 0.0001                 with score 77.29
2019-03-12 16:17:38,700 : Dev acc : 76.16 Test acc : 73.03

2019-03-12 16:17:38,700 : ***** Transfer task : MPQA *****


2019-03-12 16:17:38,706 : loading BERT model bert-large-uncased
2019-03-12 16:17:38,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:17:38,756 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:17:38,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2z8h91fk
2019-03-12 16:17:46,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:17:51,953 : Generating sentence embeddings
2019-03-12 16:18:07,324 : Generated sentence embeddings
2019-03-12 16:18:07,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:18:26,457 : Best param found at split 1: l2reg = 1e-05                 with score 84.54
2019-03-12 16:18:47,665 : Best param found at split 2: l2reg = 1e-05                 with score 84.82
2019-03-12 16:19:13,381 : Best param found at split 3: l2reg = 1e-05                 with score 84.3
2019-03-12 16:19:32,104 : Best param found at split 4: l2reg = 0.0001                 with score 85.27
2019-03-12 16:19:53,113 : Best param found at split 5: l2reg = 0.001                 with score 84.83
2019-03-12 16:19:54,231 : Dev acc : 84.75 Test acc : 85.24

2019-03-12 16:19:54,232 : ***** Transfer task : SUBJ *****


2019-03-12 16:19:54,248 : loading BERT model bert-large-uncased
2019-03-12 16:19:54,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:19:54,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:19:54,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy8ttgpmc
2019-03-12 16:20:01,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:20:07,320 : Generating sentence embeddings
2019-03-12 16:21:03,918 : Generated sentence embeddings
2019-03-12 16:21:03,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:21:28,366 : Best param found at split 1: l2reg = 1e-05                 with score 90.59
2019-03-12 16:21:54,468 : Best param found at split 2: l2reg = 1e-05                 with score 90.64
2019-03-12 16:22:16,499 : Best param found at split 3: l2reg = 0.0001                 with score 90.69
2019-03-12 16:22:39,256 : Best param found at split 4: l2reg = 1e-05                 with score 90.82
2019-03-12 16:23:03,307 : Best param found at split 5: l2reg = 1e-05                 with score 90.93
2019-03-12 16:23:04,858 : Dev acc : 90.73 Test acc : 90.7

2019-03-12 16:23:04,859 : ***** Transfer task : SST Binary classification *****


2019-03-12 16:23:04,954 : loading BERT model bert-large-uncased
2019-03-12 16:23:04,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:23:05,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:23:05,036 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpul_53gqy
2019-03-12 16:23:12,502 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:23:18,053 : Computing embedding for train
2019-03-12 16:26:29,485 : Computed train embeddings
2019-03-12 16:26:29,485 : Computing embedding for dev
2019-03-12 16:26:33,699 : Computed dev embeddings
2019-03-12 16:26:33,699 : Computing embedding for test
2019-03-12 16:26:42,231 : Computed test embeddings
2019-03-12 16:26:42,232 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:27:18,568 : [('reg:1e-05', 77.06), ('reg:0.0001', 77.87), ('reg:0.001', 76.03), ('reg:0.01', 72.25)]
2019-03-12 16:27:18,569 : Validation : best param found is reg = 0.0001 with score             77.87
2019-03-12 16:27:18,569 : Evaluating...
2019-03-12 16:27:26,630 : 
Dev acc : 77.87 Test acc : 78.69 for             SST Binary classification

2019-03-12 16:27:26,630 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 16:27:26,687 : loading BERT model bert-large-uncased
2019-03-12 16:27:26,687 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:27:26,711 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:27:26,711 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpslq1bf6e
2019-03-12 16:27:34,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:27:39,946 : Computing embedding for train
2019-03-12 16:28:24,390 : Computed train embeddings
2019-03-12 16:28:24,390 : Computing embedding for dev
2019-03-12 16:28:29,926 : Computed dev embeddings
2019-03-12 16:28:29,926 : Computing embedding for test
2019-03-12 16:28:41,321 : Computed test embeddings
2019-03-12 16:28:41,321 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:28:48,926 : [('reg:1e-05', 38.6), ('reg:0.0001', 37.69), ('reg:0.001', 36.51), ('reg:0.01', 34.88)]
2019-03-12 16:28:48,926 : Validation : best param found is reg = 1e-05 with score             38.6
2019-03-12 16:28:48,926 : Evaluating...
2019-03-12 16:28:50,790 : 
Dev acc : 38.6 Test acc : 39.82 for             SST Fine-Grained classification

2019-03-12 16:28:50,790 : ***** Transfer task : TREC *****


2019-03-12 16:28:50,805 : loading BERT model bert-large-uncased
2019-03-12 16:28:50,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:28:50,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:28:50,827 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp8qvsbrj
2019-03-12 16:28:58,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:29:21,925 : Computed train embeddings
2019-03-12 16:29:23,212 : Computed test embeddings
2019-03-12 16:29:23,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:29:40,539 : [('reg:1e-05', 66.87), ('reg:0.0001', 66.27), ('reg:0.001', 61.35), ('reg:0.01', 46.66)]
2019-03-12 16:29:40,539 : Cross-validation : best param found is reg = 1e-05             with score 66.87
2019-03-12 16:29:40,539 : Evaluating...
2019-03-12 16:29:41,926 : 
Dev acc : 66.87 Test acc : 79.4             for TREC

2019-03-12 16:29:41,927 : ***** Transfer task : MRPC *****


2019-03-12 16:29:41,948 : loading BERT model bert-large-uncased
2019-03-12 16:29:41,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:29:41,971 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:29:41,971 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9yryytdy
2019-03-12 16:29:49,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:29:54,785 : Computing embedding for train
2019-03-12 16:30:27,769 : Computed train embeddings
2019-03-12 16:30:27,769 : Computing embedding for test
2019-03-12 16:30:37,498 : Computed test embeddings
2019-03-12 16:30:37,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:30:41,171 : [('reg:1e-05', 69.77), ('reg:0.0001', 69.16), ('reg:0.001', 68.89), ('reg:0.01', 68.23)]
2019-03-12 16:30:41,171 : Cross-validation : best param found is reg = 1e-05             with score 69.77
2019-03-12 16:30:41,171 : Evaluating...
2019-03-12 16:30:41,406 : Dev acc : 69.77 Test acc 69.74; Test F1 77.3 for MRPC.

2019-03-12 16:30:41,406 : ***** Transfer task : SICK-Entailment*****


2019-03-12 16:30:41,469 : loading BERT model bert-large-uncased
2019-03-12 16:30:41,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:30:41,489 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:30:41,489 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwqp0xzts
2019-03-12 16:30:48,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:30:54,296 : Computing embedding for train
2019-03-12 16:31:05,553 : Computed train embeddings
2019-03-12 16:31:05,553 : Computing embedding for dev
2019-03-12 16:31:07,093 : Computed dev embeddings
2019-03-12 16:31:07,093 : Computing embedding for test
2019-03-12 16:31:19,165 : Computed test embeddings
2019-03-12 16:31:19,203 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:31:20,304 : [('reg:1e-05', 71.4), ('reg:0.0001', 69.4), ('reg:0.001', 71.4), ('reg:0.01', 70.6)]
2019-03-12 16:31:20,304 : Validation : best param found is reg = 1e-05 with score             71.4
2019-03-12 16:31:20,304 : Evaluating...
2019-03-12 16:31:20,533 : 
Dev acc : 71.4 Test acc : 69.84 for                        SICK entailment

2019-03-12 16:31:20,534 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 16:31:20,561 : loading BERT model bert-large-uncased
2019-03-12 16:31:20,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:31:20,619 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:31:20,619 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi8v49xu1
2019-03-12 16:31:28,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:31:33,476 : Computing embedding for train
2019-03-12 16:31:44,721 : Computed train embeddings
2019-03-12 16:31:44,721 : Computing embedding for dev
2019-03-12 16:31:46,259 : Computed dev embeddings
2019-03-12 16:31:46,259 : Computing embedding for test
2019-03-12 16:31:58,340 : Computed test embeddings
2019-03-12 16:32:25,611 : Dev : Pearson 0.7842645445325276
2019-03-12 16:32:25,611 : Test : Pearson 0.7850995588072642 Spearman 0.7084637788178647 MSE 0.39414558386744963                        for SICK Relatedness

2019-03-12 16:32:25,613 : 

***** Transfer task : STSBenchmark*****


2019-03-12 16:32:25,662 : loading BERT model bert-large-uncased
2019-03-12 16:32:25,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:32:25,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:32:25,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5jxf8xyt
2019-03-12 16:32:33,229 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:32:38,462 : Computing embedding for train
2019-03-12 16:32:56,936 : Computed train embeddings
2019-03-12 16:32:56,937 : Computing embedding for dev
2019-03-12 16:33:02,533 : Computed dev embeddings
2019-03-12 16:33:02,533 : Computing embedding for test
2019-03-12 16:33:07,097 : Computed test embeddings
2019-03-12 16:33:26,369 : Dev : Pearson 0.7053238290310561
2019-03-12 16:33:26,369 : Test : Pearson 0.6290711890634992 Spearman 0.6242004120535143 MSE 1.7689002627237                        for SICK Relatedness

2019-03-12 16:33:26,369 : ***** Transfer task : SNLI Entailment*****


2019-03-12 16:33:31,429 : loading BERT model bert-large-uncased
2019-03-12 16:33:31,429 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:33:31,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:33:31,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8um_lajy
2019-03-12 16:33:38,991 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:33:44,769 : PROGRESS (encoding): 0.00%
2019-03-12 16:36:29,104 : PROGRESS (encoding): 14.56%
2019-03-12 16:39:35,196 : PROGRESS (encoding): 29.12%
2019-03-12 16:42:41,822 : PROGRESS (encoding): 43.69%
2019-03-12 16:46:00,980 : PROGRESS (encoding): 58.25%
2019-03-12 16:49:42,781 : PROGRESS (encoding): 72.81%
2019-03-12 16:53:23,783 : PROGRESS (encoding): 87.37%
2019-03-12 16:57:22,868 : PROGRESS (encoding): 0.00%
2019-03-12 16:57:52,957 : PROGRESS (encoding): 0.00%
2019-03-12 16:58:21,841 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:58:53,407 : [('reg:1e-09', 56.09)]
2019-03-12 16:58:53,408 : Validation : best param found is reg = 1e-09 with score             56.09
2019-03-12 16:58:53,408 : Evaluating...
2019-03-12 16:59:24,473 : Dev acc : 56.09 Test acc : 56.3 for SNLI

2019-03-12 16:59:24,473 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 16:59:24,671 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 16:59:25,743 : loading BERT model bert-large-uncased
2019-03-12 16:59:25,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:59:25,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:59:25,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmnc_d9l6
2019-03-12 16:59:33,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:59:38,714 : Computing embeddings for train/dev/test
2019-03-12 17:03:07,864 : Computed embeddings
2019-03-12 17:03:07,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:03:33,886 : [('reg:1e-05', 89.84), ('reg:0.0001', 85.98), ('reg:0.001', 83.32), ('reg:0.01', 59.51)]
2019-03-12 17:03:33,887 : Validation : best param found is reg = 1e-05 with score             89.84
2019-03-12 17:03:33,887 : Evaluating...
2019-03-12 17:03:40,484 : 
Dev acc : 89.8 Test acc : 90.5 for LENGTH classification

2019-03-12 17:03:40,485 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 17:03:40,862 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 17:03:40,907 : loading BERT model bert-large-uncased
2019-03-12 17:03:40,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:03:40,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:03:40,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa5ld2_vu
2019-03-12 17:03:48,468 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:03:53,840 : Computing embeddings for train/dev/test
2019-03-12 17:07:06,960 : Computed embeddings
2019-03-12 17:07:06,960 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:07:30,701 : [('reg:1e-05', 24.17), ('reg:0.0001', 3.07), ('reg:0.001', 0.65), ('reg:0.01', 0.27)]
2019-03-12 17:07:30,701 : Validation : best param found is reg = 1e-05 with score             24.17
2019-03-12 17:07:30,701 : Evaluating...
2019-03-12 17:07:37,307 : 
Dev acc : 24.2 Test acc : 24.6 for WORDCONTENT classification

2019-03-12 17:07:37,309 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 17:07:37,692 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 17:07:37,759 : loading BERT model bert-large-uncased
2019-03-12 17:07:37,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:07:37,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:07:37,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphx2wt6xd
2019-03-12 17:07:45,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:07:50,622 : Computing embeddings for train/dev/test
2019-03-12 17:10:52,233 : Computed embeddings
2019-03-12 17:10:52,233 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:11:11,972 : [('reg:1e-05', 30.49), ('reg:0.0001', 30.39), ('reg:0.001', 27.68), ('reg:0.01', 23.42)]
2019-03-12 17:11:11,973 : Validation : best param found is reg = 1e-05 with score             30.49
2019-03-12 17:11:11,973 : Evaluating...
2019-03-12 17:11:17,484 : 
Dev acc : 30.5 Test acc : 30.5 for DEPTH classification

2019-03-12 17:11:17,485 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 17:11:17,869 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 17:11:17,932 : loading BERT model bert-large-uncased
2019-03-12 17:11:17,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:11:18,041 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:11:18,041 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9xpp3x26
2019-03-12 17:11:25,532 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:11:30,877 : Computing embeddings for train/dev/test
2019-03-12 17:14:18,875 : Computed embeddings
2019-03-12 17:14:18,875 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:14:48,194 : [('reg:1e-05', 57.29), ('reg:0.0001', 48.73), ('reg:0.001', 34.59), ('reg:0.01', 17.81)]
2019-03-12 17:14:48,194 : Validation : best param found is reg = 1e-05 with score             57.29
2019-03-12 17:14:48,194 : Evaluating...
2019-03-12 17:14:59,316 : 
Dev acc : 57.3 Test acc : 58.3 for TOPCONSTITUENTS classification

2019-03-12 17:14:59,317 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 17:14:59,672 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 17:14:59,740 : loading BERT model bert-large-uncased
2019-03-12 17:14:59,740 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:14:59,868 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:14:59,868 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpag350gp4
2019-03-12 17:15:07,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:15:12,685 : Computing embeddings for train/dev/test
2019-03-12 17:18:14,856 : Computed embeddings
2019-03-12 17:18:14,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:18:41,034 : [('reg:1e-05', 54.73), ('reg:0.0001', 54.21), ('reg:0.001', 51.51), ('reg:0.01', 50.13)]
2019-03-12 17:18:41,034 : Validation : best param found is reg = 1e-05 with score             54.73
2019-03-12 17:18:41,034 : Evaluating...
2019-03-12 17:18:46,664 : 
Dev acc : 54.7 Test acc : 54.0 for BIGRAMSHIFT classification

2019-03-12 17:18:46,665 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 17:18:47,222 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 17:18:47,287 : loading BERT model bert-large-uncased
2019-03-12 17:18:47,288 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:18:47,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:18:47,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq8om3lg4
2019-03-12 17:18:54,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:19:00,270 : Computing embeddings for train/dev/test
2019-03-12 17:21:58,402 : Computed embeddings
2019-03-12 17:21:58,402 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:22:23,374 : [('reg:1e-05', 83.1), ('reg:0.0001', 82.88), ('reg:0.001', 82.49), ('reg:0.01', 77.13)]
2019-03-12 17:22:23,374 : Validation : best param found is reg = 1e-05 with score             83.1
2019-03-12 17:22:23,374 : Evaluating...
2019-03-12 17:22:28,875 : 
Dev acc : 83.1 Test acc : 81.9 for TENSE classification

2019-03-12 17:22:28,876 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 17:22:29,295 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 17:22:29,358 : loading BERT model bert-large-uncased
2019-03-12 17:22:29,358 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:22:29,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:22:29,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0xexgs90
2019-03-12 17:22:36,855 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:22:42,184 : Computing embeddings for train/dev/test
2019-03-12 17:25:50,999 : Computed embeddings
2019-03-12 17:25:50,999 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:26:18,018 : [('reg:1e-05', 78.86), ('reg:0.0001', 78.79), ('reg:0.001', 76.74), ('reg:0.01', 71.72)]
2019-03-12 17:26:18,018 : Validation : best param found is reg = 1e-05 with score             78.86
2019-03-12 17:26:18,018 : Evaluating...
2019-03-12 17:26:24,510 : 
Dev acc : 78.9 Test acc : 77.1 for SUBJNUMBER classification

2019-03-12 17:26:24,511 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 17:26:24,918 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 17:26:24,984 : loading BERT model bert-large-uncased
2019-03-12 17:26:24,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:26:25,101 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:26:25,101 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwl6gvhmm
2019-03-12 17:26:32,550 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:26:37,889 : Computing embeddings for train/dev/test
2019-03-12 17:29:43,025 : Computed embeddings
2019-03-12 17:29:43,025 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:30:09,574 : [('reg:1e-05', 75.77), ('reg:0.0001', 74.83), ('reg:0.001', 72.86), ('reg:0.01', 68.99)]
2019-03-12 17:30:09,574 : Validation : best param found is reg = 1e-05 with score             75.77
2019-03-12 17:30:09,574 : Evaluating...
2019-03-12 17:30:16,338 : 
Dev acc : 75.8 Test acc : 78.5 for OBJNUMBER classification

2019-03-12 17:30:16,339 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 17:30:16,924 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 17:30:16,992 : loading BERT model bert-large-uncased
2019-03-12 17:30:16,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:30:17,019 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:30:17,019 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphz5nxklu
2019-03-12 17:30:24,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:30:29,906 : Computing embeddings for train/dev/test
2019-03-12 17:34:04,818 : Computed embeddings
2019-03-12 17:34:04,818 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:34:36,825 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.45), ('reg:0.001', 50.45), ('reg:0.01', 50.19)]
2019-03-12 17:34:36,825 : Validation : best param found is reg = 0.0001 with score             50.45
2019-03-12 17:34:36,825 : Evaluating...
2019-03-12 17:34:45,047 : 
Dev acc : 50.5 Test acc : 51.3 for ODDMANOUT classification

2019-03-12 17:34:45,048 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 17:34:45,436 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 17:34:45,511 : loading BERT model bert-large-uncased
2019-03-12 17:34:45,511 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:34:45,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:34:45,633 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphcrf0xr_
2019-03-12 17:34:53,063 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:34:58,390 : Computing embeddings for train/dev/test
2019-03-12 17:38:31,333 : Computed embeddings
2019-03-12 17:38:31,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:39:00,143 : [('reg:1e-05', 54.01), ('reg:0.0001', 53.59), ('reg:0.001', 51.77), ('reg:0.01', 50.56)]
2019-03-12 17:39:00,143 : Validation : best param found is reg = 1e-05 with score             54.01
2019-03-12 17:39:00,143 : Evaluating...
2019-03-12 17:39:05,854 : 
Dev acc : 54.0 Test acc : 53.2 for COORDINATIONINVERSION classification

2019-03-12 17:39:05,857 : total results: {'STS12': {'MSRpar': {'pearson': (0.28317064849852114, 2.693165264633522e-15), 'spearman': SpearmanrResult(correlation=0.32098288162540384, pvalue=1.9606432445678968e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5283261589638142, 3.795687187438128e-55), 'spearman': SpearmanrResult(correlation=0.5367704045925169, pvalue=3.4136074732510507e-57), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47876181680849017, 1.1298506678872973e-27), 'spearman': SpearmanrResult(correlation=0.5681158550910658, pvalue=1.382514240480933e-40), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3050597190106839, 1.2856172819997215e-17), 'spearman': SpearmanrResult(correlation=0.3927881324822144, pvalue=4.465116789437246e-29), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.49149005910224214, 1.1789242907058897e-25), 'spearman': SpearmanrResult(correlation=0.4345807602060545, pvalue=8.224669031314689e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.41736168047675026, 'wmean': 0.40324118479783017}, 'spearman': {'mean': 0.45064760679945104, 'wmean': 0.4414636308990078}}}, 'STS13': {'FNWN': {'pearson': (0.2120986437817548, 0.0033902765271754992), 'spearman': SpearmanrResult(correlation=0.22720978780551568, pvalue=0.0016658642053413914), 'nsamples': 189}, 'headlines': {'pearson': (0.3726675800200213, 4.0142632445577594e-26), 'spearman': SpearmanrResult(correlation=0.39878964034601827, pvalue=5.350382591333304e-30), 'nsamples': 750}, 'OnWN': {'pearson': (0.035222452175231025, 0.40503829417307624), 'spearman': SpearmanrResult(correlation=0.0556131169966363, pvalue=0.1884087166752369), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2066628919923357, 'wmean': 0.22623141624004814}, 'spearman': {'mean': 0.22720418171605675, 'wmean': 0.24882255919324608}}}, 'STS14': {'deft-forum': {'pearson': (0.22116685487329946, 2.1647714205506257e-06), 'spearman': SpearmanrResult(correlation=0.2803104764346357, pvalue=1.4351699704399556e-09), 'nsamples': 450}, 'deft-news': {'pearson': (0.5191550889234374, 4.195605353021388e-22), 'spearman': SpearmanrResult(correlation=0.5681270509119181, pvalue=4.862792575694065e-27), 'nsamples': 300}, 'headlines': {'pearson': (0.3315774027418454, 1.0516015808520455e-20), 'spearman': SpearmanrResult(correlation=0.3343764193393315, pvalue=4.7617170318618054e-21), 'nsamples': 750}, 'images': {'pearson': (0.5390683486913234, 9.253751951632287e-58), 'spearman': SpearmanrResult(correlation=0.5339092163313143, pvalue=1.7099394368453194e-56), 'nsamples': 750}, 'OnWN': {'pearson': (0.12447693715786148, 0.0006341963756972166), 'spearman': SpearmanrResult(correlation=0.24472967830661538, pvalue=1.0861036982774981e-11), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3746932385602275, 2.067705985838627e-26), 'spearman': SpearmanrResult(correlation=0.39024632304895535, pvalue=1.0824604926885886e-28), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3516896451579991, 'wmean': 0.3420356151289225}, 'spearman': {'mean': 0.391949860728795, 'wmean': 0.379739748650353}}}, 'STS15': {'answers-forums': {'pearson': (0.452032054198768, 2.770925106517976e-20), 'spearman': SpearmanrResult(correlation=0.4678559790304783, pvalue=8.56148539892803e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.44940922440835807, 1.4635429414310458e-38), 'spearman': SpearmanrResult(correlation=0.5780145899480303, pvalue=4.534391513089712e-68), 'nsamples': 750}, 'belief': {'pearson': (0.45219232440402046, 2.6774475890861538e-20), 'spearman': SpearmanrResult(correlation=0.49242287988263367, pvalue=2.672263888883537e-24), 'nsamples': 375}, 'headlines': {'pearson': (0.4451915445690267, 8.621801132255598e-38), 'spearman': SpearmanrResult(correlation=0.4825445598575016, pvalue=5.306472258294663e-45), 'nsamples': 750}, 'images': {'pearson': (0.39821793530648225, 6.561147338936419e-30), 'spearman': SpearmanrResult(correlation=0.5571714956265953, pvalue=2.2055218758093076e-62), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4394086165773311, 'wmean': 0.43623272339631525}, 'spearman': {'mean': 0.5156019008690478, 'wmean': 0.5244675187221709}}}, 'STS16': {'answer-answer': {'pearson': (0.3488457390304353, 1.1117842093387675e-08), 'spearman': SpearmanrResult(correlation=0.4325258358967853, pvalue=5.293911884174603e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.41904661933134674, 5.225032475263525e-12), 'spearman': SpearmanrResult(correlation=0.472814119255502, pvalue=2.8405089761643274e-15), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6322068722005759, 4.508480461727065e-27), 'spearman': SpearmanrResult(correlation=0.6513543169223539, pvalue=3.7303773522042867e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.6266394290260436, 5.054108092160134e-28), 'spearman': SpearmanrResult(correlation=0.7934789603859973, pvalue=4.139411968647882e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.26198914868162804, 0.0001271228636131773), 'spearman': SpearmanrResult(correlation=0.30326433617980364, pvalue=8.06263921238729e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.45774556165400593, 'wmean': 0.46038175319577}, 'spearman': {'mean': 0.5306875137280884, 'wmean': 0.5349029371840701}}}, 'MR': {'devacc': 70.99, 'acc': 70.31, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.16, 'acc': 73.03, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.75, 'acc': 85.24, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.73, 'acc': 90.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.87, 'acc': 78.69, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.6, 'acc': 39.82, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.87, 'acc': 79.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.77, 'acc': 69.74, 'f1': 77.3, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.4, 'acc': 69.84, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7842645445325276, 'pearson': 0.7850995588072642, 'spearman': 0.7084637788178647, 'mse': 0.39414558386744963, 'yhat': array([2.84618367, 4.82544811, 1.31365356, ..., 3.43523456, 4.18008893,        4.46673097]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7053238290310561, 'pearson': 0.6290711890634992, 'spearman': 0.6242004120535143, 'mse': 1.7689002627237, 'yhat': array([2.78107185, 1.46352233, 2.02208302, ..., 3.678101  , 3.70938753,        3.24285923]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 56.09, 'acc': 56.3, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 89.84, 'acc': 90.45, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 24.17, 'acc': 24.6, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.49, 'acc': 30.52, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.29, 'acc': 58.28, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 54.73, 'acc': 53.98, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 83.1, 'acc': 81.87, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.86, 'acc': 77.06, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.77, 'acc': 78.53, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.45, 'acc': 51.29, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 54.01, 'acc': 53.21, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 17:39:05,857 : STS12 p=0.4032, STS12 s=0.4415, STS13 p=0.2262, STS13 s=0.2488, STS14 p=0.3420, STS14 s=0.3797, STS15 p=0.4362, STS15 s=0.5245, STS 16 p=0.4604, STS16 s=0.5349, STS B p=0.6291, STS B s=0.6242, STS B m=1.7689, SICK-R p=0.7851, SICK-R s=0.7085, SICK-P m=0.3941
2019-03-12 17:39:05,857 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 17:39:05,857 : 0.4032,0.4415,0.2262,0.2488,0.3420,0.3797,0.4362,0.5245,0.4604,0.5349,0.6291,0.6242,1.7689,0.7851,0.7085,0.3941
2019-03-12 17:39:05,857 : MR=70.31, CR=73.03, SUBJ=90.70, MPQA=85.24, SST-B=78.69, SST-F=39.82, TREC=79.40, SICK-E=69.84, SNLI=56.30, MRPC=69.74, MRPC f=77.30
2019-03-12 17:39:05,857 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 17:39:05,857 : 70.31,73.03,90.70,85.24,78.69,39.82,79.40,69.84,56.30,69.74,77.30
2019-03-12 17:39:05,857 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 17:39:05,857 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 17:39:05,857 : na,na,na,na,na,na,na,na,na,na
2019-03-12 17:39:05,857 : SentLen=90.45, WC=24.60, TreeDepth=30.52, TopConst=58.28, BShift=53.98, Tense=81.87, SubjNum=77.06, ObjNum=78.53, SOMO=51.29, CoordInv=53.21, average=59.98
2019-03-12 17:39:05,857 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 17:39:05,857 : 90.45,24.60,30.52,58.28,53.98,81.87,77.06,78.53,51.29,53.21,59.98
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,857 : layer 3
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,857 : ********************************************************************************
2019-03-12 17:39:05,947 : ***** Transfer task : STS12 *****


2019-03-12 17:39:05,959 : loading BERT model bert-large-uncased
2019-03-12 17:39:05,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:39:05,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:39:05,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_tq35ap0
2019-03-12 17:39:13,463 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:39:22,923 : MSRpar : pearson = 0.3054, spearman = 0.3379
2019-03-12 17:39:24,557 : MSRvid : pearson = 0.4666, spearman = 0.4844
2019-03-12 17:39:25,962 : SMTeuroparl : pearson = 0.4944, spearman = 0.5845
2019-03-12 17:39:28,652 : surprise.OnWN : pearson = 0.4404, spearman = 0.4730
2019-03-12 17:39:30,076 : surprise.SMTnews : pearson = 0.5410, spearman = 0.4822
2019-03-12 17:39:30,076 : ALL (weighted average) : Pearson = 0.4350,             Spearman = 0.4608
2019-03-12 17:39:30,076 : ALL (average) : Pearson = 0.4496,             Spearman = 0.4724

2019-03-12 17:39:30,076 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 17:39:30,084 : loading BERT model bert-large-uncased
2019-03-12 17:39:30,084 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:39:30,102 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:39:30,102 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgvcfro3b
2019-03-12 17:39:37,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:39:44,205 : FNWN : pearson = 0.2113, spearman = 0.2358
2019-03-12 17:39:46,089 : headlines : pearson = 0.4406, spearman = 0.4509
2019-03-12 17:39:47,549 : OnWN : pearson = 0.1980, spearman = 0.2272
2019-03-12 17:39:47,549 : ALL (weighted average) : Pearson = 0.3210,             Spearman = 0.3402
2019-03-12 17:39:47,549 : ALL (average) : Pearson = 0.2833,             Spearman = 0.3047

2019-03-12 17:39:47,549 : ***** Transfer task : STS14 *****


2019-03-12 17:39:47,566 : loading BERT model bert-large-uncased
2019-03-12 17:39:47,566 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:39:47,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:39:47,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkecnxyuy
2019-03-12 17:39:55,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:40:01,844 : deft-forum : pearson = 0.2229, spearman = 0.2969
2019-03-12 17:40:03,474 : deft-news : pearson = 0.5697, spearman = 0.5843
2019-03-12 17:40:05,635 : headlines : pearson = 0.4027, spearman = 0.3851
2019-03-12 17:40:07,701 : images : pearson = 0.5522, spearman = 0.5432
2019-03-12 17:40:09,822 : OnWN : pearson = 0.3479, spearman = 0.4206
2019-03-12 17:40:12,672 : tweet-news : pearson = 0.4905, spearman = 0.4841
2019-03-12 17:40:12,672 : ALL (weighted average) : Pearson = 0.4310,             Spearman = 0.4490
2019-03-12 17:40:12,672 : ALL (average) : Pearson = 0.4310,             Spearman = 0.4524

2019-03-12 17:40:12,673 : ***** Transfer task : STS15 *****


2019-03-12 17:40:12,706 : loading BERT model bert-large-uncased
2019-03-12 17:40:12,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:40:12,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:40:12,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy7z6ect5
2019-03-12 17:40:20,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:40:27,596 : answers-forums : pearson = 0.4532, spearman = 0.4506
2019-03-12 17:40:29,672 : answers-students : pearson = 0.5489, spearman = 0.6127
2019-03-12 17:40:31,711 : belief : pearson = 0.5022, spearman = 0.5240
2019-03-12 17:40:33,957 : headlines : pearson = 0.4997, spearman = 0.5256
2019-03-12 17:40:36,083 : images : pearson = 0.5238, spearman = 0.6027
2019-03-12 17:40:36,083 : ALL (weighted average) : Pearson = 0.5125,             Spearman = 0.5571
2019-03-12 17:40:36,083 : ALL (average) : Pearson = 0.5056,             Spearman = 0.5431

2019-03-12 17:40:36,083 : ***** Transfer task : STS16 *****


2019-03-12 17:40:36,152 : loading BERT model bert-large-uncased
2019-03-12 17:40:36,152 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:40:36,170 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:40:36,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp97n2eb2e
2019-03-12 17:40:43,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:40:49,961 : answer-answer : pearson = 0.3856, spearman = 0.4568
2019-03-12 17:40:50,617 : headlines : pearson = 0.5143, spearman = 0.5511
2019-03-12 17:40:51,510 : plagiarism : pearson = 0.6731, spearman = 0.6903
2019-03-12 17:40:53,003 : postediting : pearson = 0.7209, spearman = 0.8061
2019-03-12 17:40:53,606 : question-question : pearson = 0.3233, spearman = 0.3289
2019-03-12 17:40:53,606 : ALL (weighted average) : Pearson = 0.5264,             Spearman = 0.5712
2019-03-12 17:40:53,606 : ALL (average) : Pearson = 0.5234,             Spearman = 0.5667

2019-03-12 17:40:53,607 : ***** Transfer task : MR *****


2019-03-12 17:40:53,626 : loading BERT model bert-large-uncased
2019-03-12 17:40:53,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:40:53,644 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:40:53,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy3n8ruhx
2019-03-12 17:41:01,268 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:41:06,670 : Generating sentence embeddings
2019-03-12 17:41:38,062 : Generated sentence embeddings
2019-03-12 17:41:38,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:41:48,668 : Best param found at split 1: l2reg = 0.0001                 with score 70.31
2019-03-12 17:42:02,156 : Best param found at split 2: l2reg = 0.001                 with score 71.1
2019-03-12 17:42:12,572 : Best param found at split 3: l2reg = 0.001                 with score 71.18
2019-03-12 17:42:23,086 : Best param found at split 4: l2reg = 0.001                 with score 70.54
2019-03-12 17:42:33,231 : Best param found at split 5: l2reg = 0.001                 with score 70.68
2019-03-12 17:42:33,939 : Dev acc : 70.76 Test acc : 68.7

2019-03-12 17:42:33,940 : ***** Transfer task : CR *****


2019-03-12 17:42:33,948 : loading BERT model bert-large-uncased
2019-03-12 17:42:33,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:42:33,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:42:33,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprmy9mlt7
2019-03-12 17:42:41,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:42:46,781 : Generating sentence embeddings
2019-03-12 17:42:55,111 : Generated sentence embeddings
2019-03-12 17:42:55,112 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:42:58,771 : Best param found at split 1: l2reg = 1e-05                 with score 74.66
2019-03-12 17:43:02,740 : Best param found at split 2: l2reg = 1e-05                 with score 75.16
2019-03-12 17:43:07,210 : Best param found at split 3: l2reg = 1e-05                 with score 75.43
2019-03-12 17:43:11,028 : Best param found at split 4: l2reg = 0.0001                 with score 75.87
2019-03-12 17:43:14,506 : Best param found at split 5: l2reg = 0.0001                 with score 76.3
2019-03-12 17:43:14,786 : Dev acc : 75.48 Test acc : 73.17

2019-03-12 17:43:14,787 : ***** Transfer task : MPQA *****


2019-03-12 17:43:14,794 : loading BERT model bert-large-uncased
2019-03-12 17:43:14,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:43:14,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:43:14,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4t87fpzm
2019-03-12 17:43:22,349 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:43:27,891 : Generating sentence embeddings
2019-03-12 17:43:35,466 : Generated sentence embeddings
2019-03-12 17:43:35,466 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:43:45,477 : Best param found at split 1: l2reg = 0.0001                 with score 82.9
2019-03-12 17:43:56,670 : Best param found at split 2: l2reg = 0.001                 with score 83.75
2019-03-12 17:44:07,959 : Best param found at split 3: l2reg = 1e-05                 with score 85.24
2019-03-12 17:44:18,946 : Best param found at split 4: l2reg = 1e-05                 with score 84.75
2019-03-12 17:44:30,128 : Best param found at split 5: l2reg = 1e-05                 with score 85.07
2019-03-12 17:44:30,741 : Dev acc : 84.34 Test acc : 84.83

2019-03-12 17:44:30,742 : ***** Transfer task : SUBJ *****


2019-03-12 17:44:30,757 : loading BERT model bert-large-uncased
2019-03-12 17:44:30,757 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:44:30,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:44:30,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvznkfx_k
2019-03-12 17:44:38,276 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:44:43,736 : Generating sentence embeddings
2019-03-12 17:45:14,473 : Generated sentence embeddings
2019-03-12 17:45:14,473 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:45:25,144 : Best param found at split 1: l2reg = 1e-05                 with score 90.94
2019-03-12 17:45:36,466 : Best param found at split 2: l2reg = 1e-05                 with score 91.21
2019-03-12 17:45:47,238 : Best param found at split 3: l2reg = 0.0001                 with score 90.52
2019-03-12 17:45:56,612 : Best param found at split 4: l2reg = 1e-05                 with score 90.76
2019-03-12 17:46:06,295 : Best param found at split 5: l2reg = 1e-05                 with score 90.95
2019-03-12 17:46:07,096 : Dev acc : 90.88 Test acc : 90.7

2019-03-12 17:46:07,097 : ***** Transfer task : SST Binary classification *****


2019-03-12 17:46:07,190 : loading BERT model bert-large-uncased
2019-03-12 17:46:07,190 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:46:07,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:46:07,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkxcmoc3d
2019-03-12 17:46:14,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:46:20,169 : Computing embedding for train
2019-03-12 17:47:59,692 : Computed train embeddings
2019-03-12 17:47:59,693 : Computing embedding for dev
2019-03-12 17:48:01,858 : Computed dev embeddings
2019-03-12 17:48:01,858 : Computing embedding for test
2019-03-12 17:48:06,402 : Computed test embeddings
2019-03-12 17:48:06,402 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:48:21,454 : [('reg:1e-05', 76.26), ('reg:0.0001', 74.89), ('reg:0.001', 75.0), ('reg:0.01', 73.51)]
2019-03-12 17:48:21,454 : Validation : best param found is reg = 1e-05 with score             76.26
2019-03-12 17:48:21,454 : Evaluating...
2019-03-12 17:48:26,449 : 
Dev acc : 76.26 Test acc : 79.35 for             SST Binary classification

2019-03-12 17:48:26,450 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 17:48:26,502 : loading BERT model bert-large-uncased
2019-03-12 17:48:26,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:48:26,532 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:48:26,532 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt7g_ly8j
2019-03-12 17:48:34,076 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:48:39,418 : Computing embedding for train
2019-03-12 17:49:01,198 : Computed train embeddings
2019-03-12 17:49:01,198 : Computing embedding for dev
2019-03-12 17:49:04,044 : Computed dev embeddings
2019-03-12 17:49:04,044 : Computing embedding for test
2019-03-12 17:49:09,644 : Computed test embeddings
2019-03-12 17:49:09,644 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:49:12,065 : [('reg:1e-05', 36.78), ('reg:0.0001', 36.97), ('reg:0.001', 36.69), ('reg:0.01', 32.79)]
2019-03-12 17:49:12,066 : Validation : best param found is reg = 0.0001 with score             36.97
2019-03-12 17:49:12,066 : Evaluating...
2019-03-12 17:49:12,784 : 
Dev acc : 36.97 Test acc : 40.81 for             SST Fine-Grained classification

2019-03-12 17:49:12,784 : ***** Transfer task : TREC *****


2019-03-12 17:49:12,797 : loading BERT model bert-large-uncased
2019-03-12 17:49:12,797 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:49:12,816 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:49:12,816 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgj5byxco
2019-03-12 17:49:20,324 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:49:33,229 : Computed train embeddings
2019-03-12 17:49:33,812 : Computed test embeddings
2019-03-12 17:49:33,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 17:49:39,795 : [('reg:1e-05', 66.82), ('reg:0.0001', 66.43), ('reg:0.001', 63.24), ('reg:0.01', 46.31)]
2019-03-12 17:49:39,795 : Cross-validation : best param found is reg = 1e-05             with score 66.82
2019-03-12 17:49:39,796 : Evaluating...
2019-03-12 17:49:40,171 : 
Dev acc : 66.82 Test acc : 81.2             for TREC

2019-03-12 17:49:40,172 : ***** Transfer task : MRPC *****


2019-03-12 17:49:40,194 : loading BERT model bert-large-uncased
2019-03-12 17:49:40,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:49:40,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:49:40,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk6ztt3yj
2019-03-12 17:49:47,680 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:49:53,245 : Computing embedding for train
2019-03-12 17:50:15,387 : Computed train embeddings
2019-03-12 17:50:15,387 : Computing embedding for test
2019-03-12 17:50:25,079 : Computed test embeddings
2019-03-12 17:50:25,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 17:50:28,610 : [('reg:1e-05', 68.72), ('reg:0.0001', 69.01), ('reg:0.001', 69.8), ('reg:0.01', 68.74)]
2019-03-12 17:50:28,610 : Cross-validation : best param found is reg = 0.001             with score 69.8
2019-03-12 17:50:28,610 : Evaluating...
2019-03-12 17:50:28,814 : Dev acc : 69.8 Test acc 71.3; Test F1 79.77 for MRPC.

2019-03-12 17:50:28,814 : ***** Transfer task : SICK-Entailment*****


2019-03-12 17:50:28,876 : loading BERT model bert-large-uncased
2019-03-12 17:50:28,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:50:28,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:50:28,896 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphubcophf
2019-03-12 17:50:36,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:50:41,962 : Computing embedding for train
2019-03-12 17:50:53,230 : Computed train embeddings
2019-03-12 17:50:53,230 : Computing embedding for dev
2019-03-12 17:50:54,762 : Computed dev embeddings
2019-03-12 17:50:54,762 : Computing embedding for test
2019-03-12 17:51:06,817 : Computed test embeddings
2019-03-12 17:51:06,854 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:51:08,210 : [('reg:1e-05', 76.0), ('reg:0.0001', 75.2), ('reg:0.001', 71.4), ('reg:0.01', 60.8)]
2019-03-12 17:51:08,210 : Validation : best param found is reg = 1e-05 with score             76.0
2019-03-12 17:51:08,210 : Evaluating...
2019-03-12 17:51:08,586 : 
Dev acc : 76.0 Test acc : 75.89 for                        SICK entailment

2019-03-12 17:51:08,587 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 17:51:08,614 : loading BERT model bert-large-uncased
2019-03-12 17:51:08,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:08,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:08,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm843rkuo
2019-03-12 17:51:16,165 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:51:21,519 : Computing embedding for train
2019-03-12 17:51:32,761 : Computed train embeddings
2019-03-12 17:51:32,762 : Computing embedding for dev
2019-03-12 17:51:34,295 : Computed dev embeddings
2019-03-12 17:51:34,295 : Computing embedding for test
2019-03-12 17:51:46,356 : Computed test embeddings
2019-03-12 17:52:18,458 : Dev : Pearson 0.8147315331188402
2019-03-12 17:52:18,458 : Test : Pearson 0.7947961838054917 Spearman 0.7184299933352662 MSE 0.3755501564858059                        for SICK Relatedness

2019-03-12 17:52:18,459 : 

***** Transfer task : STSBenchmark*****


2019-03-12 17:52:18,528 : loading BERT model bert-large-uncased
2019-03-12 17:52:18,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:52:18,547 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:52:18,547 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvibqy3_x
2019-03-12 17:52:26,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:52:31,434 : Computing embedding for train
2019-03-12 17:52:49,892 : Computed train embeddings
2019-03-12 17:52:49,892 : Computing embedding for dev
2019-03-12 17:52:55,496 : Computed dev embeddings
2019-03-12 17:52:55,496 : Computing embedding for test
2019-03-12 17:53:00,070 : Computed test embeddings
2019-03-12 17:53:15,014 : Dev : Pearson 0.7187621388556031
2019-03-12 17:53:15,014 : Test : Pearson 0.6489057052493414 Spearman 0.6402372230783987 MSE 1.5917530720370714                        for SICK Relatedness

2019-03-12 17:53:15,014 : ***** Transfer task : SNLI Entailment*****


2019-03-12 17:53:20,100 : loading BERT model bert-large-uncased
2019-03-12 17:53:20,100 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:53:20,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:53:20,193 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ffbrhe3
2019-03-12 17:53:27,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:53:33,537 : PROGRESS (encoding): 0.00%
2019-03-12 17:56:17,794 : PROGRESS (encoding): 14.56%
2019-03-12 17:59:23,882 : PROGRESS (encoding): 29.12%
2019-03-12 18:02:30,594 : PROGRESS (encoding): 43.69%
2019-03-12 18:05:49,692 : PROGRESS (encoding): 58.25%
2019-03-12 18:09:31,363 : PROGRESS (encoding): 72.81%
2019-03-12 18:13:11,900 : PROGRESS (encoding): 87.37%
2019-03-12 18:17:10,516 : PROGRESS (encoding): 0.00%
2019-03-12 18:17:40,596 : PROGRESS (encoding): 0.00%
2019-03-12 18:18:09,477 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:18:35,669 : [('reg:1e-09', 60.78)]
2019-03-12 18:18:35,669 : Validation : best param found is reg = 1e-09 with score             60.78
2019-03-12 18:18:35,669 : Evaluating...
2019-03-12 18:19:02,263 : Dev acc : 60.78 Test acc : 60.78 for SNLI

2019-03-12 18:19:02,264 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 18:19:02,468 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 18:19:03,520 : loading BERT model bert-large-uncased
2019-03-12 18:19:03,520 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:19:03,546 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:19:03,546 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpixs8rocc
2019-03-12 18:19:10,968 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:19:16,358 : Computing embeddings for train/dev/test
2019-03-12 18:22:45,570 : Computed embeddings
2019-03-12 18:22:45,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:23:13,424 : [('reg:1e-05', 91.81), ('reg:0.0001', 88.33), ('reg:0.001', 86.4), ('reg:0.01', 58.85)]
2019-03-12 18:23:13,425 : Validation : best param found is reg = 1e-05 with score             91.81
2019-03-12 18:23:13,425 : Evaluating...
2019-03-12 18:23:22,143 : 
Dev acc : 91.8 Test acc : 91.7 for LENGTH classification

2019-03-12 18:23:22,144 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 18:23:22,397 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 18:23:22,442 : loading BERT model bert-large-uncased
2019-03-12 18:23:22,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:23:22,472 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:23:22,472 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjmxgl8js
2019-03-12 18:23:29,951 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:23:35,080 : Computing embeddings for train/dev/test
2019-03-12 18:26:47,668 : Computed embeddings
2019-03-12 18:26:47,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:27:22,044 : [('reg:1e-05', 19.51), ('reg:0.0001', 2.13), ('reg:0.001', 0.55), ('reg:0.01', 0.25)]
2019-03-12 18:27:22,044 : Validation : best param found is reg = 1e-05 with score             19.51
2019-03-12 18:27:22,044 : Evaluating...
2019-03-12 18:27:30,978 : 
Dev acc : 19.5 Test acc : 19.4 for WORDCONTENT classification

2019-03-12 18:27:30,980 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 18:27:31,518 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 18:27:31,585 : loading BERT model bert-large-uncased
2019-03-12 18:27:31,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:27:31,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:27:31,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj1hcayn5
2019-03-12 18:27:39,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:27:44,472 : Computing embeddings for train/dev/test
2019-03-12 18:30:45,380 : Computed embeddings
2019-03-12 18:30:45,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:31:06,710 : [('reg:1e-05', 30.44), ('reg:0.0001', 30.02), ('reg:0.001', 28.17), ('reg:0.01', 25.44)]
2019-03-12 18:31:06,710 : Validation : best param found is reg = 1e-05 with score             30.44
2019-03-12 18:31:06,710 : Evaluating...
2019-03-12 18:31:10,771 : 
Dev acc : 30.4 Test acc : 30.2 for DEPTH classification

2019-03-12 18:31:10,772 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 18:31:11,152 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 18:31:11,217 : loading BERT model bert-large-uncased
2019-03-12 18:31:11,218 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:31:11,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:31:11,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjnc4v6wg
2019-03-12 18:31:18,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:31:24,154 : Computing embeddings for train/dev/test
2019-03-12 18:34:12,033 : Computed embeddings
2019-03-12 18:34:12,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:34:47,248 : [('reg:1e-05', 53.48), ('reg:0.0001', 46.91), ('reg:0.001', 33.44), ('reg:0.01', 16.75)]
2019-03-12 18:34:47,248 : Validation : best param found is reg = 1e-05 with score             53.48
2019-03-12 18:34:47,248 : Evaluating...
2019-03-12 18:34:57,138 : 
Dev acc : 53.5 Test acc : 53.8 for TOPCONSTITUENTS classification

2019-03-12 18:34:57,139 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 18:34:57,523 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 18:34:57,590 : loading BERT model bert-large-uncased
2019-03-12 18:34:57,590 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:34:57,620 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:34:57,620 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_qfghdw5
2019-03-12 18:35:05,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:35:10,476 : Computing embeddings for train/dev/test
2019-03-12 18:38:12,786 : Computed embeddings
2019-03-12 18:38:12,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:38:40,127 : [('reg:1e-05', 54.45), ('reg:0.0001', 54.59), ('reg:0.001', 54.31), ('reg:0.01', 50.0)]
2019-03-12 18:38:40,127 : Validation : best param found is reg = 0.0001 with score             54.59
2019-03-12 18:38:40,128 : Evaluating...
2019-03-12 18:38:47,847 : 
Dev acc : 54.6 Test acc : 54.2 for BIGRAMSHIFT classification

2019-03-12 18:38:47,848 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 18:38:48,241 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 18:38:48,307 : loading BERT model bert-large-uncased
2019-03-12 18:38:48,308 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:38:48,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:38:48,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp14fuun9i
2019-03-12 18:38:55,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:39:01,304 : Computing embeddings for train/dev/test
2019-03-12 18:41:59,599 : Computed embeddings
2019-03-12 18:41:59,599 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:42:29,435 : [('reg:1e-05', 84.82), ('reg:0.0001', 84.84), ('reg:0.001', 83.61), ('reg:0.01', 78.45)]
2019-03-12 18:42:29,435 : Validation : best param found is reg = 0.0001 with score             84.84
2019-03-12 18:42:29,435 : Evaluating...
2019-03-12 18:42:36,596 : 
Dev acc : 84.8 Test acc : 83.0 for TENSE classification

2019-03-12 18:42:36,597 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 18:42:36,999 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 18:42:37,061 : loading BERT model bert-large-uncased
2019-03-12 18:42:37,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:42:37,175 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:42:37,175 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp956lyex2
2019-03-12 18:42:44,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:42:50,069 : Computing embeddings for train/dev/test
2019-03-12 18:45:58,878 : Computed embeddings
2019-03-12 18:45:58,879 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:46:35,945 : [('reg:1e-05', 80.24), ('reg:0.0001', 79.6), ('reg:0.001', 76.17), ('reg:0.01', 73.0)]
2019-03-12 18:46:35,945 : Validation : best param found is reg = 1e-05 with score             80.24
2019-03-12 18:46:35,945 : Evaluating...
2019-03-12 18:46:46,763 : 
Dev acc : 80.2 Test acc : 79.2 for SUBJNUMBER classification

2019-03-12 18:46:46,764 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 18:46:47,163 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 18:46:47,229 : loading BERT model bert-large-uncased
2019-03-12 18:46:47,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:46:47,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:46:47,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpszjzq63u
2019-03-12 18:46:54,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:47:00,150 : Computing embeddings for train/dev/test
2019-03-12 18:50:05,478 : Computed embeddings
2019-03-12 18:50:05,478 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:50:37,987 : [('reg:1e-05', 76.41), ('reg:0.0001', 76.82), ('reg:0.001', 73.48), ('reg:0.01', 68.95)]
2019-03-12 18:50:37,987 : Validation : best param found is reg = 0.0001 with score             76.82
2019-03-12 18:50:37,987 : Evaluating...
2019-03-12 18:50:44,960 : 
Dev acc : 76.8 Test acc : 78.4 for OBJNUMBER classification

2019-03-12 18:50:44,961 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 18:50:45,530 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 18:50:45,601 : loading BERT model bert-large-uncased
2019-03-12 18:50:45,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:50:45,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:50:45,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8g80ksn1
2019-03-12 18:50:53,158 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:50:58,488 : Computing embeddings for train/dev/test
2019-03-12 18:54:33,293 : Computed embeddings
2019-03-12 18:54:33,293 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:55:03,760 : [('reg:1e-05', 52.26), ('reg:0.0001', 51.71), ('reg:0.001', 51.15), ('reg:0.01', 50.34)]
2019-03-12 18:55:03,760 : Validation : best param found is reg = 1e-05 with score             52.26
2019-03-12 18:55:03,760 : Evaluating...
2019-03-12 18:55:11,320 : 
Dev acc : 52.3 Test acc : 52.5 for ODDMANOUT classification

2019-03-12 18:55:11,321 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 18:55:11,716 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 18:55:11,800 : loading BERT model bert-large-uncased
2019-03-12 18:55:11,801 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:55:11,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:55:11,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9t7x6w9g
2019-03-12 18:55:19,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:55:24,613 : Computing embeddings for train/dev/test
2019-03-12 18:58:57,396 : Computed embeddings
2019-03-12 18:58:57,396 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:59:23,036 : [('reg:1e-05', 54.84), ('reg:0.0001', 54.62), ('reg:0.001', 50.03), ('reg:0.01', 50.0)]
2019-03-12 18:59:23,036 : Validation : best param found is reg = 1e-05 with score             54.84
2019-03-12 18:59:23,036 : Evaluating...
2019-03-12 18:59:29,083 : 
Dev acc : 54.8 Test acc : 54.6 for COORDINATIONINVERSION classification

2019-03-12 18:59:29,085 : total results: {'STS12': {'MSRpar': {'pearson': (0.30543427230452197, 1.1685200826815754e-17), 'spearman': SpearmanrResult(correlation=0.33785817856052974, pvalue=1.757019377995991e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4666102488807727, 8.132845620061068e-42), 'spearman': SpearmanrResult(correlation=0.48435016981778606, pvalue=2.2549726240979002e-45), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49443344243114185, 1.1376483629489631e-29), 'spearman': SpearmanrResult(correlation=0.5844690955602403, pvalue=2.125595922561313e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.44036360642842765, 6.368882803773173e-37), 'spearman': SpearmanrResult(correlation=0.4729860642720282, pvalue=4.5265247113411425e-43), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5410109081453389, 1.031993113786805e-31), 'spearman': SpearmanrResult(correlation=0.4821647943049747, pvalue=1.2750718671192479e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4495704956380406, 'wmean': 0.4350432426435573}, 'spearman': {'mean': 0.47236566050311185, 'wmean': 0.4607628305268961}}}, 'STS13': {'FNWN': {'pearson': (0.2113255355414027, 0.003511429717721722), 'spearman': SpearmanrResult(correlation=0.2358080996003885, pvalue=0.0010888277418154568), 'nsamples': 189}, 'headlines': {'pearson': (0.440608367151172, 5.759306178564065e-37), 'spearman': SpearmanrResult(correlation=0.45094481104603595, pvalue=7.626044540502888e-39), 'nsamples': 750}, 'OnWN': {'pearson': (0.1980100988408973, 2.285674365340596e-06), 'spearman': SpearmanrResult(correlation=0.22719752010937208, pvalue=5.310827005047663e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.283314667177824, 'wmean': 0.3209869780202983}, 'spearman': {'mean': 0.30465014358526554, 'wmean': 0.3401560985935721}}}, 'STS14': {'deft-forum': {'pearson': (0.2228892638010636, 1.7948689102150422e-06), 'spearman': SpearmanrResult(correlation=0.2968836232017289, pvalue=1.311971776500778e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.569730229856183, 3.2445080160542695e-27), 'spearman': SpearmanrResult(correlation=0.5842848340000473, pvalue=7.4273560797424e-29), 'nsamples': 300}, 'headlines': {'pearson': (0.40272720185169647, 1.2986100308399589e-30), 'spearman': SpearmanrResult(correlation=0.38510965888733695, pvalue=6.329939179389649e-28), 'nsamples': 750}, 'images': {'pearson': (0.5522056251394554, 4.363779859339633e-61), 'spearman': SpearmanrResult(correlation=0.5432114053354784, pvalue=8.572659123860484e-59), 'nsamples': 750}, 'OnWN': {'pearson': (0.347863926212521, 9.320607831992733e-23), 'spearman': SpearmanrResult(correlation=0.4205862689720898, pvalue=1.6558727179665604e-33), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4905029866142323, 1.1745024869154754e-46), 'spearman': SpearmanrResult(correlation=0.48410501120020194, pvalue=2.533580699581311e-45), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4309865389125253, 'wmean': 0.4309850780082033}, 'spearman': {'mean': 0.4523634669328139, 'wmean': 0.44897129038323264}}}, 'STS15': {'answers-forums': {'pearson': (0.4532254696026465, 2.1451514535139514e-20), 'spearman': SpearmanrResult(correlation=0.4505825085631589, pvalue=3.77627681258971e-20), 'nsamples': 375}, 'answers-students': {'pearson': (0.5488558180897205, 3.1776690622538754e-60), 'spearman': SpearmanrResult(correlation=0.6127490046948676, pvalue=1.6435812659644216e-78), 'nsamples': 750}, 'belief': {'pearson': (0.5022069305087934, 2.351003847455421e-25), 'spearman': SpearmanrResult(correlation=0.5240448100584307, pvalue=7.73692169181931e-28), 'nsamples': 375}, 'headlines': {'pearson': (0.4997190294529499, 1.2535128185642508e-48), 'spearman': SpearmanrResult(correlation=0.5255552364891963, pvalue=1.7306359513733085e-54), 'nsamples': 750}, 'images': {'pearson': (0.5237709626297853, 4.563370902012209e-54), 'spearman': SpearmanrResult(correlation=0.6026734636300534, pvalue=2.382273601054835e-75), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5055556420567792, 'wmean': 0.5125155025570439}, 'spearman': {'mean': 0.5431210046871413, 'wmean': 0.5570728410312281}}}, 'STS16': {'answer-answer': {'pearson': (0.38558228537411143, 1.9838021120778657e-10), 'spearman': SpearmanrResult(correlation=0.45681542676914405, pvalue=1.6871562505412243e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.5142622854976384, 3.271190683915943e-18), 'spearman': SpearmanrResult(correlation=0.5511435360207356, pvalue=3.473533928008895e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.673086247409317, 1.0438744425648903e-31), 'spearman': SpearmanrResult(correlation=0.6903397980420866, pvalue=6.781976084267442e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7208972349878767, 2.0601018039514744e-40), 'spearman': SpearmanrResult(correlation=0.806059022905936, pvalue=4.7298380852208045e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.32331082113545834, 1.7981219769540523e-06), 'spearman': SpearmanrResult(correlation=0.3289029811988454, pvalue=1.1600653818202254e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5234277748808804, 'wmean': 0.5263658797912578}, 'spearman': {'mean': 0.5666521529873496, 'wmean': 0.5712168103522872}}}, 'MR': {'devacc': 70.76, 'acc': 68.7, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.48, 'acc': 73.17, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.34, 'acc': 84.83, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.88, 'acc': 90.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.26, 'acc': 79.35, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.97, 'acc': 40.81, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.82, 'acc': 81.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.8, 'acc': 71.3, 'f1': 79.77, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.0, 'acc': 75.89, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8147315331188402, 'pearson': 0.7947961838054917, 'spearman': 0.7184299933352662, 'mse': 0.3755501564858059, 'yhat': array([2.62023041, 4.53487078, 1.54614667, ..., 3.26201327, 4.50058472,        4.71564213]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7187621388556031, 'pearson': 0.6489057052493414, 'spearman': 0.6402372230783987, 'mse': 1.5917530720370714, 'yhat': array([2.26076125, 2.09264671, 1.94977393, ..., 3.81972534, 3.83797117,        3.28573663]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.78, 'acc': 60.78, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.81, 'acc': 91.67, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 19.51, 'acc': 19.42, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.44, 'acc': 30.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.48, 'acc': 53.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 54.59, 'acc': 54.24, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.84, 'acc': 83.05, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.24, 'acc': 79.18, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.82, 'acc': 78.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 52.26, 'acc': 52.48, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 54.84, 'acc': 54.6, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 18:59:29,085 : STS12 p=0.4350, STS12 s=0.4608, STS13 p=0.3210, STS13 s=0.3402, STS14 p=0.4310, STS14 s=0.4490, STS15 p=0.5125, STS15 s=0.5571, STS 16 p=0.5264, STS16 s=0.5712, STS B p=0.6489, STS B s=0.6402, STS B m=1.5918, SICK-R p=0.7948, SICK-R s=0.7184, SICK-P m=0.3756
2019-03-12 18:59:29,085 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 18:59:29,085 : 0.4350,0.4608,0.3210,0.3402,0.4310,0.4490,0.5125,0.5571,0.5264,0.5712,0.6489,0.6402,1.5918,0.7948,0.7184,0.3756
2019-03-12 18:59:29,085 : MR=68.70, CR=73.17, SUBJ=90.70, MPQA=84.83, SST-B=79.35, SST-F=40.81, TREC=81.20, SICK-E=75.89, SNLI=60.78, MRPC=71.30, MRPC f=79.77
2019-03-12 18:59:29,085 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 18:59:29,085 : 68.70,73.17,90.70,84.83,79.35,40.81,81.20,75.89,60.78,71.30,79.77
2019-03-12 18:59:29,085 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 18:59:29,085 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 18:59:29,086 : na,na,na,na,na,na,na,na,na,na
2019-03-12 18:59:29,086 : SentLen=91.67, WC=19.42, TreeDepth=30.25, TopConst=53.79, BShift=54.24, Tense=83.05, SubjNum=79.18, ObjNum=78.42, SOMO=52.48, CoordInv=54.60, average=59.71
2019-03-12 18:59:29,086 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 18:59:29,086 : 91.67,19.42,30.25,53.79,54.24,83.05,79.18,78.42,52.48,54.60,59.71
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,086 : layer 4
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,086 : ********************************************************************************
2019-03-12 18:59:29,177 : ***** Transfer task : STS12 *****


2019-03-12 18:59:29,190 : loading BERT model bert-large-uncased
2019-03-12 18:59:29,190 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:59:29,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:59:29,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsp6i45mf
2019-03-12 18:59:36,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:59:46,022 : MSRpar : pearson = 0.2995, spearman = 0.3301
2019-03-12 18:59:47,657 : MSRvid : pearson = 0.4512, spearman = 0.4684
2019-03-12 18:59:49,062 : SMTeuroparl : pearson = 0.4649, spearman = 0.5818
2019-03-12 18:59:51,748 : surprise.OnWN : pearson = 0.4986, spearman = 0.5157
2019-03-12 18:59:53,168 : surprise.SMTnews : pearson = 0.5861, spearman = 0.5025
2019-03-12 18:59:53,168 : ALL (weighted average) : Pearson = 0.4454,             Spearman = 0.4676
2019-03-12 18:59:53,168 : ALL (average) : Pearson = 0.4601,             Spearman = 0.4797

2019-03-12 18:59:53,168 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 18:59:53,177 : loading BERT model bert-large-uncased
2019-03-12 18:59:53,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:59:53,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:59:53,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqvta12vg
2019-03-12 19:00:00,638 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:00:07,263 : FNWN : pearson = 0.2308, spearman = 0.2497
2019-03-12 19:00:09,149 : headlines : pearson = 0.4874, spearman = 0.4837
2019-03-12 19:00:10,610 : OnWN : pearson = 0.2133, spearman = 0.2581
2019-03-12 19:00:10,611 : ALL (weighted average) : Pearson = 0.3525,             Spearman = 0.3698
2019-03-12 19:00:10,611 : ALL (average) : Pearson = 0.3105,             Spearman = 0.3305

2019-03-12 19:00:10,611 : ***** Transfer task : STS14 *****


2019-03-12 19:00:10,626 : loading BERT model bert-large-uncased
2019-03-12 19:00:10,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:00:10,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:00:10,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf36a6lyw
2019-03-12 19:00:18,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:00:24,652 : deft-forum : pearson = 0.2225, spearman = 0.2844
2019-03-12 19:00:26,286 : deft-news : pearson = 0.5451, spearman = 0.5410
2019-03-12 19:00:28,451 : headlines : pearson = 0.4471, spearman = 0.4187
2019-03-12 19:00:30,521 : images : pearson = 0.5731, spearman = 0.5665
2019-03-12 19:00:32,642 : OnWN : pearson = 0.3727, spearman = 0.4425
2019-03-12 19:00:35,490 : tweet-news : pearson = 0.5502, spearman = 0.5291
2019-03-12 19:00:35,490 : ALL (weighted average) : Pearson = 0.4590,             Spearman = 0.4688
2019-03-12 19:00:35,490 : ALL (average) : Pearson = 0.4518,             Spearman = 0.4637

2019-03-12 19:00:35,490 : ***** Transfer task : STS15 *****


2019-03-12 19:00:35,522 : loading BERT model bert-large-uncased
2019-03-12 19:00:35,522 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:00:35,548 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:00:35,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmk5npz9d
2019-03-12 19:00:43,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:00:50,047 : answers-forums : pearson = 0.4477, spearman = 0.4403
2019-03-12 19:00:52,130 : answers-students : pearson = 0.6071, spearman = 0.6316
2019-03-12 19:00:54,174 : belief : pearson = 0.4611, spearman = 0.4654
2019-03-12 19:00:56,416 : headlines : pearson = 0.5411, spearman = 0.5537
2019-03-12 19:00:58,542 : images : pearson = 0.5365, spearman = 0.5888
2019-03-12 19:00:58,543 : ALL (weighted average) : Pearson = 0.5348,             Spearman = 0.5567
2019-03-12 19:00:58,543 : ALL (average) : Pearson = 0.5187,             Spearman = 0.5360

2019-03-12 19:00:58,543 : ***** Transfer task : STS16 *****


2019-03-12 19:00:58,613 : loading BERT model bert-large-uncased
2019-03-12 19:00:58,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:00:58,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:00:58,631 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_q62_bn
2019-03-12 19:01:06,087 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:01:12,281 : answer-answer : pearson = 0.4122, spearman = 0.4906
2019-03-12 19:01:12,938 : headlines : pearson = 0.5685, spearman = 0.5849
2019-03-12 19:01:13,817 : plagiarism : pearson = 0.7052, spearman = 0.7242
2019-03-12 19:01:15,308 : postediting : pearson = 0.7386, spearman = 0.8037
2019-03-12 19:01:15,912 : question-question : pearson = 0.3094, spearman = 0.2963
2019-03-12 19:01:15,912 : ALL (weighted average) : Pearson = 0.5509,             Spearman = 0.5859
2019-03-12 19:01:15,912 : ALL (average) : Pearson = 0.5468,             Spearman = 0.5800

2019-03-12 19:01:15,912 : ***** Transfer task : MR *****


2019-03-12 19:01:15,927 : loading BERT model bert-large-uncased
2019-03-12 19:01:15,927 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:01:15,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:01:15,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0guhb1el
2019-03-12 19:01:23,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:01:28,677 : Generating sentence embeddings
2019-03-12 19:02:00,051 : Generated sentence embeddings
2019-03-12 19:02:00,052 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:02:08,265 : Best param found at split 1: l2reg = 0.0001                 with score 71.62
2019-03-12 19:02:17,063 : Best param found at split 2: l2reg = 1e-05                 with score 71.07
2019-03-12 19:02:26,419 : Best param found at split 3: l2reg = 1e-05                 with score 70.18
2019-03-12 19:02:37,578 : Best param found at split 4: l2reg = 1e-05                 with score 70.42
2019-03-12 19:02:48,238 : Best param found at split 5: l2reg = 1e-05                 with score 71.22
2019-03-12 19:02:48,958 : Dev acc : 70.9 Test acc : 71.14

2019-03-12 19:02:48,959 : ***** Transfer task : CR *****


2019-03-12 19:02:48,967 : loading BERT model bert-large-uncased
2019-03-12 19:02:48,967 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:02:48,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:02:48,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp04_m97aw
2019-03-12 19:02:56,480 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:03:01,714 : Generating sentence embeddings
2019-03-12 19:03:10,012 : Generated sentence embeddings
2019-03-12 19:03:10,012 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:03:13,502 : Best param found at split 1: l2reg = 1e-05                 with score 75.65
2019-03-12 19:03:17,241 : Best param found at split 2: l2reg = 1e-05                 with score 75.36
2019-03-12 19:03:21,134 : Best param found at split 3: l2reg = 1e-05                 with score 76.49
2019-03-12 19:03:25,245 : Best param found at split 4: l2reg = 1e-05                 with score 75.87
2019-03-12 19:03:29,003 : Best param found at split 5: l2reg = 0.0001                 with score 75.14
2019-03-12 19:03:29,159 : Dev acc : 75.7 Test acc : 73.32

2019-03-12 19:03:29,160 : ***** Transfer task : MPQA *****


2019-03-12 19:03:29,166 : loading BERT model bert-large-uncased
2019-03-12 19:03:29,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:03:29,215 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:03:29,216 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt5h_hoj8
2019-03-12 19:03:36,721 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:03:42,079 : Generating sentence embeddings
2019-03-12 19:03:49,643 : Generated sentence embeddings
2019-03-12 19:03:49,644 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:03:58,216 : Best param found at split 1: l2reg = 0.0001                 with score 79.68
2019-03-12 19:04:08,004 : Best param found at split 2: l2reg = 0.001                 with score 84.87
2019-03-12 19:04:18,673 : Best param found at split 3: l2reg = 0.0001                 with score 83.92
2019-03-12 19:04:29,325 : Best param found at split 4: l2reg = 1e-05                 with score 85.35
2019-03-12 19:04:40,841 : Best param found at split 5: l2reg = 1e-05                 with score 85.38
2019-03-12 19:04:41,552 : Dev acc : 83.84 Test acc : 85.17

2019-03-12 19:04:41,553 : ***** Transfer task : SUBJ *****


2019-03-12 19:04:41,571 : loading BERT model bert-large-uncased
2019-03-12 19:04:41,572 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:04:41,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:04:41,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6_w94x_y
2019-03-12 19:04:49,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:04:54,469 : Generating sentence embeddings
2019-03-12 19:05:25,260 : Generated sentence embeddings
2019-03-12 19:05:25,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:05:34,274 : Best param found at split 1: l2reg = 0.0001                 with score 91.24
2019-03-12 19:05:45,945 : Best param found at split 2: l2reg = 1e-05                 with score 91.85
2019-03-12 19:05:53,884 : Best param found at split 3: l2reg = 1e-05                 with score 91.18
2019-03-12 19:06:01,999 : Best param found at split 4: l2reg = 1e-05                 with score 91.34
2019-03-12 19:06:10,826 : Best param found at split 5: l2reg = 1e-05                 with score 91.7
2019-03-12 19:06:11,432 : Dev acc : 91.46 Test acc : 91.49

2019-03-12 19:06:11,433 : ***** Transfer task : SST Binary classification *****


2019-03-12 19:06:11,527 : loading BERT model bert-large-uncased
2019-03-12 19:06:11,527 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:06:11,604 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:06:11,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9noi7rap
2019-03-12 19:06:19,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:06:24,504 : Computing embedding for train
2019-03-12 19:08:04,081 : Computed train embeddings
2019-03-12 19:08:04,082 : Computing embedding for dev
2019-03-12 19:08:06,245 : Computed dev embeddings
2019-03-12 19:08:06,245 : Computing embedding for test
2019-03-12 19:08:10,806 : Computed test embeddings
2019-03-12 19:08:10,806 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:08:28,935 : [('reg:1e-05', 77.41), ('reg:0.0001', 76.83), ('reg:0.001', 74.43), ('reg:0.01', 74.2)]
2019-03-12 19:08:28,935 : Validation : best param found is reg = 1e-05 with score             77.41
2019-03-12 19:08:28,936 : Evaluating...
2019-03-12 19:08:32,596 : 
Dev acc : 77.41 Test acc : 77.87 for             SST Binary classification

2019-03-12 19:08:32,596 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 19:08:32,651 : loading BERT model bert-large-uncased
2019-03-12 19:08:32,651 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:08:32,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:08:32,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxqgdb33f
2019-03-12 19:08:40,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:08:45,567 : Computing embedding for train
2019-03-12 19:09:07,339 : Computed train embeddings
2019-03-12 19:09:07,339 : Computing embedding for dev
2019-03-12 19:09:10,182 : Computed dev embeddings
2019-03-12 19:09:10,182 : Computing embedding for test
2019-03-12 19:09:15,780 : Computed test embeddings
2019-03-12 19:09:15,780 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:09:17,558 : [('reg:1e-05', 36.88), ('reg:0.0001', 35.42), ('reg:0.001', 35.69), ('reg:0.01', 33.42)]
2019-03-12 19:09:17,558 : Validation : best param found is reg = 1e-05 with score             36.88
2019-03-12 19:09:17,558 : Evaluating...
2019-03-12 19:09:18,090 : 
Dev acc : 36.88 Test acc : 37.6 for             SST Fine-Grained classification

2019-03-12 19:09:18,090 : ***** Transfer task : TREC *****


2019-03-12 19:09:18,104 : loading BERT model bert-large-uncased
2019-03-12 19:09:18,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:09:18,123 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:09:18,124 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzw9a5br1
2019-03-12 19:09:25,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:09:38,609 : Computed train embeddings
2019-03-12 19:09:39,194 : Computed test embeddings
2019-03-12 19:09:39,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:09:47,235 : [('reg:1e-05', 67.88), ('reg:0.0001', 67.5), ('reg:0.001', 63.94), ('reg:0.01', 46.84)]
2019-03-12 19:09:47,235 : Cross-validation : best param found is reg = 1e-05             with score 67.88
2019-03-12 19:09:47,235 : Evaluating...
2019-03-12 19:09:47,740 : 
Dev acc : 67.88 Test acc : 81.4             for TREC

2019-03-12 19:09:47,741 : ***** Transfer task : MRPC *****


2019-03-12 19:09:47,762 : loading BERT model bert-large-uncased
2019-03-12 19:09:47,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:09:47,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:09:47,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfg89krzt
2019-03-12 19:09:55,252 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:10:00,360 : Computing embedding for train
2019-03-12 19:10:22,508 : Computed train embeddings
2019-03-12 19:10:22,508 : Computing embedding for test
2019-03-12 19:10:32,195 : Computed test embeddings
2019-03-12 19:10:32,215 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:10:36,920 : [('reg:1e-05', 69.97), ('reg:0.0001', 70.66), ('reg:0.001', 70.49), ('reg:0.01', 69.92)]
2019-03-12 19:10:36,921 : Cross-validation : best param found is reg = 0.0001             with score 70.66
2019-03-12 19:10:36,921 : Evaluating...
2019-03-12 19:10:37,187 : Dev acc : 70.66 Test acc 65.28; Test F1 69.79 for MRPC.

2019-03-12 19:10:37,188 : ***** Transfer task : SICK-Entailment*****


2019-03-12 19:10:37,249 : loading BERT model bert-large-uncased
2019-03-12 19:10:37,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:10:37,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:10:37,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwxsyp3d6
2019-03-12 19:10:44,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:10:49,985 : Computing embedding for train
2019-03-12 19:11:01,224 : Computed train embeddings
2019-03-12 19:11:01,224 : Computing embedding for dev
2019-03-12 19:11:02,757 : Computed dev embeddings
2019-03-12 19:11:02,757 : Computing embedding for test
2019-03-12 19:11:14,807 : Computed test embeddings
2019-03-12 19:11:14,843 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:11:16,293 : [('reg:1e-05', 71.8), ('reg:0.0001', 72.0), ('reg:0.001', 66.6), ('reg:0.01', 67.0)]
2019-03-12 19:11:16,293 : Validation : best param found is reg = 0.0001 with score             72.0
2019-03-12 19:11:16,293 : Evaluating...
2019-03-12 19:11:16,770 : 
Dev acc : 72.0 Test acc : 71.46 for                        SICK entailment

2019-03-12 19:11:16,771 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 19:11:16,798 : loading BERT model bert-large-uncased
2019-03-12 19:11:16,798 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:11:16,856 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:11:16,856 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3nqiz7lv
2019-03-12 19:11:24,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:11:29,836 : Computing embedding for train
2019-03-12 19:11:41,102 : Computed train embeddings
2019-03-12 19:11:41,102 : Computing embedding for dev
2019-03-12 19:11:42,637 : Computed dev embeddings
2019-03-12 19:11:42,637 : Computing embedding for test
2019-03-12 19:11:54,705 : Computed test embeddings
2019-03-12 19:12:16,488 : Dev : Pearson 0.799897962638977
2019-03-12 19:12:16,488 : Test : Pearson 0.796918629260612 Spearman 0.7268088497723539 MSE 0.371380917902789                        for SICK Relatedness

2019-03-12 19:12:16,489 : 

***** Transfer task : STSBenchmark*****


2019-03-12 19:12:16,556 : loading BERT model bert-large-uncased
2019-03-12 19:12:16,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:12:16,576 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:12:16,576 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3rjbk95v
2019-03-12 19:12:24,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:12:29,532 : Computing embedding for train
2019-03-12 19:12:48,039 : Computed train embeddings
2019-03-12 19:12:48,039 : Computing embedding for dev
2019-03-12 19:12:53,640 : Computed dev embeddings
2019-03-12 19:12:53,640 : Computing embedding for test
2019-03-12 19:12:58,209 : Computed test embeddings
2019-03-12 19:13:14,903 : Dev : Pearson 0.7126730754618269
2019-03-12 19:13:14,904 : Test : Pearson 0.6483130464703377 Spearman 0.6439963856517935 MSE 1.674633190031507                        for SICK Relatedness

2019-03-12 19:13:14,904 : ***** Transfer task : SNLI Entailment*****


2019-03-12 19:13:20,021 : loading BERT model bert-large-uncased
2019-03-12 19:13:20,021 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:20,143 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:20,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3ez23vxx
2019-03-12 19:13:27,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:33,401 : PROGRESS (encoding): 0.00%
2019-03-12 19:16:17,868 : PROGRESS (encoding): 14.56%
2019-03-12 19:19:24,394 : PROGRESS (encoding): 29.12%
2019-03-12 19:22:31,340 : PROGRESS (encoding): 43.69%
2019-03-12 19:25:50,749 : PROGRESS (encoding): 58.25%
2019-03-12 19:29:32,719 : PROGRESS (encoding): 72.81%
2019-03-12 19:33:13,705 : PROGRESS (encoding): 87.37%
2019-03-12 19:37:12,845 : PROGRESS (encoding): 0.00%
2019-03-12 19:37:42,964 : PROGRESS (encoding): 0.00%
2019-03-12 19:38:11,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:38:49,527 : [('reg:1e-09', 60.73)]
2019-03-12 19:38:49,527 : Validation : best param found is reg = 1e-09 with score             60.73
2019-03-12 19:38:49,527 : Evaluating...
2019-03-12 19:39:24,247 : Dev acc : 60.73 Test acc : 61.22 for SNLI

2019-03-12 19:39:24,247 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 19:39:24,447 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 19:39:25,513 : loading BERT model bert-large-uncased
2019-03-12 19:39:25,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:39:25,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:39:25,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfs9xd1e0
2019-03-12 19:39:33,026 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:39:38,326 : Computing embeddings for train/dev/test
2019-03-12 19:43:07,380 : Computed embeddings
2019-03-12 19:43:07,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:43:33,360 : [('reg:1e-05', 91.03), ('reg:0.0001', 86.45), ('reg:0.001', 84.62), ('reg:0.01', 60.01)]
2019-03-12 19:43:33,360 : Validation : best param found is reg = 1e-05 with score             91.03
2019-03-12 19:43:33,360 : Evaluating...
2019-03-12 19:43:40,886 : 
Dev acc : 91.0 Test acc : 91.7 for LENGTH classification

2019-03-12 19:43:40,887 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 19:43:41,264 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 19:43:41,308 : loading BERT model bert-large-uncased
2019-03-12 19:43:41,308 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:43:41,339 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:43:41,339 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzpcqh5nd
2019-03-12 19:43:48,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:43:54,247 : Computing embeddings for train/dev/test
2019-03-12 19:47:07,259 : Computed embeddings
2019-03-12 19:47:07,259 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:47:40,064 : [('reg:1e-05', 13.62), ('reg:0.0001', 1.83), ('reg:0.001', 0.48), ('reg:0.01', 0.22)]
2019-03-12 19:47:40,064 : Validation : best param found is reg = 1e-05 with score             13.62
2019-03-12 19:47:40,064 : Evaluating...
2019-03-12 19:47:47,885 : 
Dev acc : 13.6 Test acc : 13.4 for WORDCONTENT classification

2019-03-12 19:47:47,886 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 19:47:48,270 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 19:47:48,336 : loading BERT model bert-large-uncased
2019-03-12 19:47:48,336 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:47:48,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:47:48,361 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfe8a3aqj
2019-03-12 19:47:55,801 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:48:01,292 : Computing embeddings for train/dev/test
2019-03-12 19:51:02,319 : Computed embeddings
2019-03-12 19:51:02,319 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:51:20,679 : [('reg:1e-05', 30.19), ('reg:0.0001', 29.91), ('reg:0.001', 28.36), ('reg:0.01', 24.32)]
2019-03-12 19:51:20,679 : Validation : best param found is reg = 1e-05 with score             30.19
2019-03-12 19:51:20,679 : Evaluating...
2019-03-12 19:51:24,552 : 
Dev acc : 30.2 Test acc : 30.7 for DEPTH classification

2019-03-12 19:51:24,553 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 19:51:24,947 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 19:51:25,012 : loading BERT model bert-large-uncased
2019-03-12 19:51:25,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:51:25,128 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:51:25,128 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp5ee0uop
2019-03-12 19:51:32,607 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:51:37,974 : Computing embeddings for train/dev/test
2019-03-12 19:54:25,907 : Computed embeddings
2019-03-12 19:54:25,907 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:54:59,921 : [('reg:1e-05', 61.5), ('reg:0.0001', 52.96), ('reg:0.001', 38.92), ('reg:0.01', 17.9)]
2019-03-12 19:54:59,921 : Validation : best param found is reg = 1e-05 with score             61.5
2019-03-12 19:54:59,921 : Evaluating...
2019-03-12 19:55:08,213 : 
Dev acc : 61.5 Test acc : 62.3 for TOPCONSTITUENTS classification

2019-03-12 19:55:08,214 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 19:55:08,565 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 19:55:08,632 : loading BERT model bert-large-uncased
2019-03-12 19:55:08,633 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:55:08,755 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:55:08,755 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi9cr7g63
2019-03-12 19:55:16,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:55:21,675 : Computing embeddings for train/dev/test
2019-03-12 19:58:24,003 : Computed embeddings
2019-03-12 19:58:24,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:58:54,863 : [('reg:1e-05', 60.31), ('reg:0.0001', 60.89), ('reg:0.001', 54.92), ('reg:0.01', 50.0)]
2019-03-12 19:58:54,864 : Validation : best param found is reg = 0.0001 with score             60.89
2019-03-12 19:58:54,864 : Evaluating...
2019-03-12 19:59:03,766 : 
Dev acc : 60.9 Test acc : 60.3 for BIGRAMSHIFT classification

2019-03-12 19:59:03,767 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 19:59:04,323 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 19:59:04,388 : loading BERT model bert-large-uncased
2019-03-12 19:59:04,388 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:59:04,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:59:04,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp724dik7g
2019-03-12 19:59:11,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:59:17,275 : Computing embeddings for train/dev/test
2019-03-12 20:02:15,645 : Computed embeddings
2019-03-12 20:02:15,645 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:02:45,768 : [('reg:1e-05', 84.66), ('reg:0.0001', 84.85), ('reg:0.001', 84.44), ('reg:0.01', 81.33)]
2019-03-12 20:02:45,768 : Validation : best param found is reg = 0.0001 with score             84.85
2019-03-12 20:02:45,768 : Evaluating...
2019-03-12 20:02:53,136 : 
Dev acc : 84.8 Test acc : 83.0 for TENSE classification

2019-03-12 20:02:53,137 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 20:02:53,560 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 20:02:53,624 : loading BERT model bert-large-uncased
2019-03-12 20:02:53,625 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:02:53,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:02:53,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmparhmqukq
2019-03-12 20:03:01,155 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:03:06,615 : Computing embeddings for train/dev/test
2019-03-12 20:06:15,451 : Computed embeddings
2019-03-12 20:06:15,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:06:52,110 : [('reg:1e-05', 80.0), ('reg:0.0001', 80.89), ('reg:0.001', 77.84), ('reg:0.01', 73.76)]
2019-03-12 20:06:52,111 : Validation : best param found is reg = 0.0001 with score             80.89
2019-03-12 20:06:52,111 : Evaluating...
2019-03-12 20:07:01,822 : 
Dev acc : 80.9 Test acc : 79.3 for SUBJNUMBER classification

2019-03-12 20:07:01,823 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 20:07:02,257 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 20:07:02,326 : loading BERT model bert-large-uncased
2019-03-12 20:07:02,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:07:02,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:07:02,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmbx15fu2
2019-03-12 20:07:09,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:07:15,298 : Computing embeddings for train/dev/test
2019-03-12 20:10:20,564 : Computed embeddings
2019-03-12 20:10:20,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:10:53,570 : [('reg:1e-05', 75.88), ('reg:0.0001', 75.54), ('reg:0.001', 74.02), ('reg:0.01', 68.8)]
2019-03-12 20:10:53,570 : Validation : best param found is reg = 1e-05 with score             75.88
2019-03-12 20:10:53,570 : Evaluating...
2019-03-12 20:11:02,395 : 
Dev acc : 75.9 Test acc : 78.1 for OBJNUMBER classification

2019-03-12 20:11:02,396 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 20:11:02,987 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 20:11:03,056 : loading BERT model bert-large-uncased
2019-03-12 20:11:03,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:11:03,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:11:03,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnavlbxo5
2019-03-12 20:11:10,535 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:11:16,039 : Computing embeddings for train/dev/test
2019-03-12 20:14:50,992 : Computed embeddings
2019-03-12 20:14:50,992 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:15:19,446 : [('reg:1e-05', 52.97), ('reg:0.0001', 53.08), ('reg:0.001', 52.66), ('reg:0.01', 51.19)]
2019-03-12 20:15:19,446 : Validation : best param found is reg = 0.0001 with score             53.08
2019-03-12 20:15:19,446 : Evaluating...
2019-03-12 20:15:26,231 : 
Dev acc : 53.1 Test acc : 53.7 for ODDMANOUT classification

2019-03-12 20:15:26,232 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 20:15:26,624 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 20:15:26,700 : loading BERT model bert-large-uncased
2019-03-12 20:15:26,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:15:26,823 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:15:26,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm3jq22de
2019-03-12 20:15:34,316 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:15:39,611 : Computing embeddings for train/dev/test
2019-03-12 20:19:12,493 : Computed embeddings
2019-03-12 20:19:12,494 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:19:38,466 : [('reg:1e-05', 58.55), ('reg:0.0001', 58.1), ('reg:0.001', 50.86), ('reg:0.01', 50.0)]
2019-03-12 20:19:38,466 : Validation : best param found is reg = 1e-05 with score             58.55
2019-03-12 20:19:38,466 : Evaluating...
2019-03-12 20:19:46,617 : 
Dev acc : 58.5 Test acc : 58.5 for COORDINATIONINVERSION classification

2019-03-12 20:19:46,619 : total results: {'STS12': {'MSRpar': {'pearson': (0.29948989903049245, 5.233896910193009e-17), 'spearman': SpearmanrResult(correlation=0.33010840278794346, pvalue=1.5886241314985606e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.45119204349656566, 6.864099650607138e-39), 'spearman': SpearmanrResult(correlation=0.4684048325225677, pvalue=3.629280632454747e-42), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46487668030267754, 5.473863366189019e-26), 'spearman': SpearmanrResult(correlation=0.5817887989196266, pvalue=6.30214994311872e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4985622422960918, 2.2332204795472588e-48), 'spearman': SpearmanrResult(correlation=0.5157089130254192, pvalue=3.3961638720473624e-52), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5861350379306259, 3.517091731012637e-38), 'spearman': SpearmanrResult(correlation=0.5025178504882395, pvalue=6.4119201366202055e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4600511806112907, 'wmean': 0.44536017213983625}, 'spearman': {'mean': 0.4797057595487593, 'wmean': 0.4675715226193256}}}, 'STS13': {'FNWN': {'pearson': (0.23081360935010845, 0.001396515898345007), 'spearman': SpearmanrResult(correlation=0.24971899384607749, pvalue=0.0005295229349486186), 'nsamples': 189}, 'headlines': {'pearson': (0.4874067890841447, 5.234671168501112e-46), 'spearman': SpearmanrResult(correlation=0.48369531382028186, pvalue=3.07746149746051e-45), 'nsamples': 750}, 'OnWN': {'pearson': (0.21326376998865457, 3.419017397826697e-07), 'spearman': SpearmanrResult(correlation=0.2580640498155012, pvalue=5.500457214622402e-10), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3104947228076359, 'wmean': 0.3525465592959428}, 'spearman': {'mean': 0.3304927858272868, 'wmean': 0.36982820476574413}}}, 'STS14': {'deft-forum': {'pearson': (0.22250657016490172, 1.8714150667161244e-06), 'spearman': SpearmanrResult(correlation=0.2844214205690097, pvalue=8.044118022039704e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.5451308219547133, 1.275030678526731e-24), 'spearman': SpearmanrResult(correlation=0.5409856544268677, pvalue=3.326240864890825e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.4471406006117726, 3.810852052780776e-38), 'spearman': SpearmanrResult(correlation=0.41873526081075063, pvalue=3.366666014615701e-33), 'nsamples': 750}, 'images': {'pearson': (0.5731232377760318, 1.0661726351039648e-66), 'spearman': SpearmanrResult(correlation=0.5664938326951613, pvalue=7.079405334386724e-65), 'nsamples': 750}, 'OnWN': {'pearson': (0.3727156079095636, 3.951830005692311e-26), 'spearman': SpearmanrResult(correlation=0.44254013231541495, pvalue=2.595796937086422e-37), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5502488511341377, 1.3954850984758138e-60), 'spearman': SpearmanrResult(correlation=0.5291225910049016, pvalue=2.4477710270993902e-55), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.45181094825852014, 'wmean': 0.4589569136624664}, 'spearman': {'mean': 0.463716481970351, 'wmean': 0.4687877861876763}}}, 'STS15': {'answers-forums': {'pearson': (0.4476922222808006, 6.969734815547136e-20), 'spearman': SpearmanrResult(correlation=0.44031136173529256, pvalue=3.2469965726760174e-19), 'nsamples': 375}, 'answers-students': {'pearson': (0.6070668487705431, 1.0291968457594802e-76), 'spearman': SpearmanrResult(correlation=0.6315503932646642, pvalue=1.0035989355526391e-84), 'nsamples': 750}, 'belief': {'pearson': (0.4611373601736826, 3.8310468799752434e-21), 'spearman': SpearmanrResult(correlation=0.46538700055122073, pvalue=1.4906205212729211e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.5410830221171433, 2.9220834985126925e-58), 'spearman': SpearmanrResult(correlation=0.553725417885302, pvalue=1.7596725827243338e-61), 'nsamples': 750}, 'images': {'pearson': (0.5365295181409334, 3.9118915976950866e-57), 'spearman': SpearmanrResult(correlation=0.588822116090986, pvalue=3.4918412957734404e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5187017942966207, 'wmean': 0.5347735450639653}, 'spearman': {'mean': 0.5359592579054931, 'wmean': 0.5567367770960523}}}, 'STS16': {'answer-answer': {'pearson': (0.4121627325506314, 7.755769377120619e-12), 'spearman': SpearmanrResult(correlation=0.4906046144879136, pvalue=8.658467643244239e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.5684652688772212, 1.029116942639768e-22), 'spearman': SpearmanrResult(correlation=0.584882137612108, pvalue=3.0101458307324895e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7051924883051557, 6.635341866383972e-36), 'spearman': SpearmanrResult(correlation=0.7242112465309338, pvalue=1.1428165001869893e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.7386138727185745, 2.48577939746372e-43), 'spearman': SpearmanrResult(correlation=0.8037382254913352, pvalue=1.7124423416173578e-56), 'nsamples': 244}, 'question-question': {'pearson': (0.30942985575189313, 5.141435746507027e-06), 'spearman': SpearmanrResult(correlation=0.29633855494306716, pvalue=1.3204885049370027e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5467728436406951, 'wmean': 0.5508634764957437}, 'spearman': {'mean': 0.5799549558130715, 'wmean': 0.5858892884067847}}}, 'MR': {'devacc': 70.9, 'acc': 71.14, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.7, 'acc': 73.32, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.84, 'acc': 85.17, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.46, 'acc': 91.49, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.41, 'acc': 77.87, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.88, 'acc': 37.6, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.88, 'acc': 81.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.66, 'acc': 65.28, 'f1': 69.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.0, 'acc': 71.46, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.799897962638977, 'pearson': 0.796918629260612, 'spearman': 0.7268088497723539, 'mse': 0.371380917902789, 'yhat': array([2.06613119, 4.64148553, 1.54517474, ..., 3.10637847, 4.50269981,        4.51155702]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7126730754618269, 'pearson': 0.6483130464703377, 'spearman': 0.6439963856517935, 'mse': 1.674633190031507, 'yhat': array([2.38311587, 1.88734592, 2.36494353, ..., 3.69821258, 3.71835662,        3.19485489]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.73, 'acc': 61.22, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.03, 'acc': 91.68, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 13.62, 'acc': 13.43, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.19, 'acc': 30.66, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.5, 'acc': 62.3, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 60.89, 'acc': 60.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.85, 'acc': 82.97, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.89, 'acc': 79.32, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.88, 'acc': 78.14, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.08, 'acc': 53.72, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.55, 'acc': 58.51, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 20:19:46,619 : STS12 p=0.4454, STS12 s=0.4676, STS13 p=0.3525, STS13 s=0.3698, STS14 p=0.4590, STS14 s=0.4688, STS15 p=0.5348, STS15 s=0.5567, STS 16 p=0.5509, STS16 s=0.5859, STS B p=0.6483, STS B s=0.6440, STS B m=1.6746, SICK-R p=0.7969, SICK-R s=0.7268, SICK-P m=0.3714
2019-03-12 20:19:46,619 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 20:19:46,619 : 0.4454,0.4676,0.3525,0.3698,0.4590,0.4688,0.5348,0.5567,0.5509,0.5859,0.6483,0.6440,1.6746,0.7969,0.7268,0.3714
2019-03-12 20:19:46,619 : MR=71.14, CR=73.32, SUBJ=91.49, MPQA=85.17, SST-B=77.87, SST-F=37.60, TREC=81.40, SICK-E=71.46, SNLI=61.22, MRPC=65.28, MRPC f=69.79
2019-03-12 20:19:46,619 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 20:19:46,619 : 71.14,73.32,91.49,85.17,77.87,37.60,81.40,71.46,61.22,65.28,69.79
2019-03-12 20:19:46,619 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 20:19:46,619 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 20:19:46,619 : na,na,na,na,na,na,na,na,na,na
2019-03-12 20:19:46,619 : SentLen=91.68, WC=13.43, TreeDepth=30.66, TopConst=62.30, BShift=60.34, Tense=82.97, SubjNum=79.32, ObjNum=78.14, SOMO=53.72, CoordInv=58.51, average=61.11
2019-03-12 20:19:46,619 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 20:19:46,619 : 91.68,13.43,30.66,62.30,60.34,82.97,79.32,78.14,53.72,58.51,61.11
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,619 : layer 5
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,619 : ********************************************************************************
2019-03-12 20:19:46,708 : ***** Transfer task : STS12 *****


2019-03-12 20:19:46,720 : loading BERT model bert-large-uncased
2019-03-12 20:19:46,720 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:19:46,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:19:46,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp55misa0x
2019-03-12 20:19:54,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:20:03,532 : MSRpar : pearson = 0.3123, spearman = 0.3460
2019-03-12 20:20:05,168 : MSRvid : pearson = 0.6090, spearman = 0.6235
2019-03-12 20:20:06,573 : SMTeuroparl : pearson = 0.4825, spearman = 0.5994
2019-03-12 20:20:09,259 : surprise.OnWN : pearson = 0.6024, spearman = 0.6225
2019-03-12 20:20:10,679 : surprise.SMTnews : pearson = 0.6073, spearman = 0.5628
2019-03-12 20:20:10,679 : ALL (weighted average) : Pearson = 0.5169,             Spearman = 0.5450
2019-03-12 20:20:10,679 : ALL (average) : Pearson = 0.5227,             Spearman = 0.5509

2019-03-12 20:20:10,679 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 20:20:10,687 : loading BERT model bert-large-uncased
2019-03-12 20:20:10,687 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:20:10,705 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:20:10,705 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm3pjg9uq
2019-03-12 20:20:18,188 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:20:24,916 : FNWN : pearson = 0.2925, spearman = 0.2967
2019-03-12 20:20:26,802 : headlines : pearson = 0.5410, spearman = 0.5241
2019-03-12 20:20:28,263 : OnWN : pearson = 0.4481, spearman = 0.4796
2019-03-12 20:20:28,263 : ALL (weighted average) : Pearson = 0.4749,             Spearman = 0.4788
2019-03-12 20:20:28,263 : ALL (average) : Pearson = 0.4272,             Spearman = 0.4335

2019-03-12 20:20:28,263 : ***** Transfer task : STS14 *****


2019-03-12 20:20:28,280 : loading BERT model bert-large-uncased
2019-03-12 20:20:28,280 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:20:28,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:20:28,298 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpht1n5oe5
2019-03-12 20:20:35,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:20:42,577 : deft-forum : pearson = 0.3288, spearman = 0.3405
2019-03-12 20:20:44,208 : deft-news : pearson = 0.6160, spearman = 0.6206
2019-03-12 20:20:46,369 : headlines : pearson = 0.5072, spearman = 0.4667
2019-03-12 20:20:48,439 : images : pearson = 0.6192, spearman = 0.6233
2019-03-12 20:20:50,564 : OnWN : pearson = 0.5730, spearman = 0.6213
2019-03-12 20:20:53,416 : tweet-news : pearson = 0.6573, spearman = 0.6053
2019-03-12 20:20:53,416 : ALL (weighted average) : Pearson = 0.5601,             Spearman = 0.5538
2019-03-12 20:20:53,416 : ALL (average) : Pearson = 0.5502,             Spearman = 0.5463

2019-03-12 20:20:53,416 : ***** Transfer task : STS15 *****


2019-03-12 20:20:53,449 : loading BERT model bert-large-uncased
2019-03-12 20:20:53,449 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:20:53,466 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:20:53,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqg91i7_x
2019-03-12 20:21:00,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:21:08,261 : answers-forums : pearson = 0.4372, spearman = 0.4264
2019-03-12 20:21:10,338 : answers-students : pearson = 0.6186, spearman = 0.6349
2019-03-12 20:21:12,379 : belief : pearson = 0.5717, spearman = 0.5875
2019-03-12 20:21:14,618 : headlines : pearson = 0.5892, spearman = 0.5943
2019-03-12 20:21:16,743 : images : pearson = 0.6574, spearman = 0.6940
2019-03-12 20:21:16,744 : ALL (weighted average) : Pearson = 0.5924,             Spearman = 0.6075
2019-03-12 20:21:16,744 : ALL (average) : Pearson = 0.5748,             Spearman = 0.5874

2019-03-12 20:21:16,744 : ***** Transfer task : STS16 *****


2019-03-12 20:21:16,813 : loading BERT model bert-large-uncased
2019-03-12 20:21:16,813 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:21:16,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:21:16,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjav8y4qx
2019-03-12 20:21:24,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:21:30,540 : answer-answer : pearson = 0.4767, spearman = 0.5003
2019-03-12 20:21:31,199 : headlines : pearson = 0.6277, spearman = 0.6421
2019-03-12 20:21:32,079 : plagiarism : pearson = 0.7484, spearman = 0.7628
2019-03-12 20:21:33,568 : postediting : pearson = 0.7976, spearman = 0.8252
2019-03-12 20:21:34,172 : question-question : pearson = 0.5667, spearman = 0.5681
2019-03-12 20:21:34,172 : ALL (weighted average) : Pearson = 0.6430,             Spearman = 0.6598
2019-03-12 20:21:34,172 : ALL (average) : Pearson = 0.6434,             Spearman = 0.6597

2019-03-12 20:21:34,172 : ***** Transfer task : MR *****


2019-03-12 20:21:34,191 : loading BERT model bert-large-uncased
2019-03-12 20:21:34,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:21:34,210 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:21:34,210 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq5jw0z5i
2019-03-12 20:21:41,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:21:47,228 : Generating sentence embeddings
2019-03-12 20:22:18,568 : Generated sentence embeddings
2019-03-12 20:22:18,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:22:28,321 : Best param found at split 1: l2reg = 1e-05                 with score 71.95
2019-03-12 20:22:40,084 : Best param found at split 2: l2reg = 0.001                 with score 72.7
2019-03-12 20:22:50,686 : Best param found at split 3: l2reg = 1e-05                 with score 71.42
2019-03-12 20:23:02,056 : Best param found at split 4: l2reg = 0.001                 with score 72.0
2019-03-12 20:23:12,320 : Best param found at split 5: l2reg = 1e-05                 with score 71.38
2019-03-12 20:23:12,922 : Dev acc : 71.89 Test acc : 72.0

2019-03-12 20:23:12,923 : ***** Transfer task : CR *****


2019-03-12 20:23:12,931 : loading BERT model bert-large-uncased
2019-03-12 20:23:12,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:23:12,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:23:12,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmtakh6xh
2019-03-12 20:23:20,392 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:23:25,861 : Generating sentence embeddings
2019-03-12 20:23:34,143 : Generated sentence embeddings
2019-03-12 20:23:34,143 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:23:37,749 : Best param found at split 1: l2reg = 0.0001                 with score 76.81
2019-03-12 20:23:41,330 : Best param found at split 2: l2reg = 1e-05                 with score 77.31
2019-03-12 20:23:45,164 : Best param found at split 3: l2reg = 1e-05                 with score 76.49
2019-03-12 20:23:48,762 : Best param found at split 4: l2reg = 1e-05                 with score 75.87
2019-03-12 20:23:52,226 : Best param found at split 5: l2reg = 1e-05                 with score 75.74
2019-03-12 20:23:52,417 : Dev acc : 76.44 Test acc : 76.85

2019-03-12 20:23:52,418 : ***** Transfer task : MPQA *****


2019-03-12 20:23:52,424 : loading BERT model bert-large-uncased
2019-03-12 20:23:52,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:23:52,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:23:52,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwmbtl3qj
2019-03-12 20:23:59,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:24:05,356 : Generating sentence embeddings
2019-03-12 20:24:12,912 : Generated sentence embeddings
2019-03-12 20:24:12,912 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:24:22,902 : Best param found at split 1: l2reg = 1e-05                 with score 82.13
2019-03-12 20:24:33,756 : Best param found at split 2: l2reg = 1e-05                 with score 84.92
2019-03-12 20:24:44,400 : Best param found at split 3: l2reg = 0.0001                 with score 82.88
2019-03-12 20:24:55,333 : Best param found at split 4: l2reg = 0.0001                 with score 85.65
2019-03-12 20:25:06,571 : Best param found at split 5: l2reg = 0.0001                 with score 85.47
2019-03-12 20:25:07,199 : Dev acc : 84.21 Test acc : 85.43

2019-03-12 20:25:07,200 : ***** Transfer task : SUBJ *****


2019-03-12 20:25:07,230 : loading BERT model bert-large-uncased
2019-03-12 20:25:07,231 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:25:07,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:25:07,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo1lrj_kt
2019-03-12 20:25:14,744 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:25:20,122 : Generating sentence embeddings
2019-03-12 20:25:50,895 : Generated sentence embeddings
2019-03-12 20:25:50,896 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:26:01,080 : Best param found at split 1: l2reg = 1e-05                 with score 92.61
2019-03-12 20:26:12,461 : Best param found at split 2: l2reg = 0.0001                 with score 92.79
2019-03-12 20:26:21,788 : Best param found at split 3: l2reg = 0.0001                 with score 92.55
2019-03-12 20:26:32,664 : Best param found at split 4: l2reg = 1e-05                 with score 92.68
2019-03-12 20:26:43,875 : Best param found at split 5: l2reg = 0.0001                 with score 92.51
2019-03-12 20:26:44,538 : Dev acc : 92.63 Test acc : 92.03

2019-03-12 20:26:44,539 : ***** Transfer task : SST Binary classification *****


2019-03-12 20:26:44,630 : loading BERT model bert-large-uncased
2019-03-12 20:26:44,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:26:44,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:26:44,704 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw9_z4o9f
2019-03-12 20:26:52,210 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:26:57,595 : Computing embedding for train
2019-03-12 20:28:37,142 : Computed train embeddings
2019-03-12 20:28:37,142 : Computing embedding for dev
2019-03-12 20:28:39,305 : Computed dev embeddings
2019-03-12 20:28:39,305 : Computing embedding for test
2019-03-12 20:28:43,855 : Computed test embeddings
2019-03-12 20:28:43,855 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:29:02,921 : [('reg:1e-05', 77.41), ('reg:0.0001', 78.56), ('reg:0.001', 78.1), ('reg:0.01', 76.26)]
2019-03-12 20:29:02,921 : Validation : best param found is reg = 0.0001 with score             78.56
2019-03-12 20:29:02,921 : Evaluating...
2019-03-12 20:29:08,736 : 
Dev acc : 78.56 Test acc : 79.57 for             SST Binary classification

2019-03-12 20:29:08,737 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 20:29:08,789 : loading BERT model bert-large-uncased
2019-03-12 20:29:08,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:29:08,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:29:08,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcy3u0_ny
2019-03-12 20:29:16,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:29:21,604 : Computing embedding for train
2019-03-12 20:29:43,363 : Computed train embeddings
2019-03-12 20:29:43,363 : Computing embedding for dev
2019-03-12 20:29:46,203 : Computed dev embeddings
2019-03-12 20:29:46,203 : Computing embedding for test
2019-03-12 20:29:51,816 : Computed test embeddings
2019-03-12 20:29:51,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:29:54,403 : [('reg:1e-05', 35.42), ('reg:0.0001', 35.15), ('reg:0.001', 38.51), ('reg:0.01', 33.24)]
2019-03-12 20:29:54,403 : Validation : best param found is reg = 0.001 with score             38.51
2019-03-12 20:29:54,403 : Evaluating...
2019-03-12 20:29:55,146 : 
Dev acc : 38.51 Test acc : 40.59 for             SST Fine-Grained classification

2019-03-12 20:29:55,146 : ***** Transfer task : TREC *****


2019-03-12 20:29:55,159 : loading BERT model bert-large-uncased
2019-03-12 20:29:55,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:29:55,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:29:55,179 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4kgg_4em
2019-03-12 20:30:02,632 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:30:15,600 : Computed train embeddings
2019-03-12 20:30:16,185 : Computed test embeddings
2019-03-12 20:30:16,186 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:30:22,560 : [('reg:1e-05', 65.89), ('reg:0.0001', 63.91), ('reg:0.001', 62.82), ('reg:0.01', 50.95)]
2019-03-12 20:30:22,560 : Cross-validation : best param found is reg = 1e-05             with score 65.89
2019-03-12 20:30:22,560 : Evaluating...
2019-03-12 20:30:23,137 : 
Dev acc : 65.89 Test acc : 80.6             for TREC

2019-03-12 20:30:23,138 : ***** Transfer task : MRPC *****


2019-03-12 20:30:23,161 : loading BERT model bert-large-uncased
2019-03-12 20:30:23,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:30:23,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:30:23,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt4_rg8uv
2019-03-12 20:30:30,691 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:30:36,161 : Computing embedding for train
2019-03-12 20:30:58,285 : Computed train embeddings
2019-03-12 20:30:58,285 : Computing embedding for test
2019-03-12 20:31:07,959 : Computed test embeddings
2019-03-12 20:31:07,980 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:31:12,572 : [('reg:1e-05', 71.71), ('reg:0.0001', 71.3), ('reg:0.001', 71.47), ('reg:0.01', 69.63)]
2019-03-12 20:31:12,572 : Cross-validation : best param found is reg = 1e-05             with score 71.71
2019-03-12 20:31:12,572 : Evaluating...
2019-03-12 20:31:12,742 : Dev acc : 71.71 Test acc 70.26; Test F1 80.53 for MRPC.

2019-03-12 20:31:12,742 : ***** Transfer task : SICK-Entailment*****


2019-03-12 20:31:12,803 : loading BERT model bert-large-uncased
2019-03-12 20:31:12,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:31:12,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:31:12,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuqh7c3v0
2019-03-12 20:31:20,295 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:31:25,748 : Computing embedding for train
2019-03-12 20:31:36,993 : Computed train embeddings
2019-03-12 20:31:36,994 : Computing embedding for dev
2019-03-12 20:31:38,524 : Computed dev embeddings
2019-03-12 20:31:38,525 : Computing embedding for test
2019-03-12 20:31:50,546 : Computed test embeddings
2019-03-12 20:31:50,582 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:31:51,803 : [('reg:1e-05', 76.4), ('reg:0.0001', 77.4), ('reg:0.001', 76.6), ('reg:0.01', 65.0)]
2019-03-12 20:31:51,803 : Validation : best param found is reg = 0.0001 with score             77.4
2019-03-12 20:31:51,803 : Evaluating...
2019-03-12 20:31:52,204 : 
Dev acc : 77.4 Test acc : 77.39 for                        SICK entailment

2019-03-12 20:31:52,204 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 20:31:52,231 : loading BERT model bert-large-uncased
2019-03-12 20:31:52,231 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:31:52,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:31:52,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm8v6no4_
2019-03-12 20:31:59,958 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:32:05,351 : Computing embedding for train
2019-03-12 20:32:16,593 : Computed train embeddings
2019-03-12 20:32:16,593 : Computing embedding for dev
2019-03-12 20:32:18,126 : Computed dev embeddings
2019-03-12 20:32:18,126 : Computing embedding for test
2019-03-12 20:32:30,176 : Computed test embeddings
2019-03-12 20:32:47,732 : Dev : Pearson 0.7853237480092348
2019-03-12 20:32:47,732 : Test : Pearson 0.8007500893307419 Spearman 0.728730275142403 MSE 0.3651200900125144                        for SICK Relatedness

2019-03-12 20:32:47,733 : 

***** Transfer task : STSBenchmark*****


2019-03-12 20:32:47,800 : loading BERT model bert-large-uncased
2019-03-12 20:32:47,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:32:47,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:32:47,820 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc8d1gsd4
2019-03-12 20:32:55,299 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:33:00,705 : Computing embedding for train
2019-03-12 20:33:19,173 : Computed train embeddings
2019-03-12 20:33:19,173 : Computing embedding for dev
2019-03-12 20:33:24,777 : Computed dev embeddings
2019-03-12 20:33:24,777 : Computing embedding for test
2019-03-12 20:33:29,348 : Computed test embeddings
2019-03-12 20:33:47,702 : Dev : Pearson 0.7332903761268001
2019-03-12 20:33:47,702 : Test : Pearson 0.6712920276888341 Spearman 0.6618226806881939 MSE 1.48504759952621                        for SICK Relatedness

2019-03-12 20:33:47,702 : ***** Transfer task : SNLI Entailment*****


2019-03-12 20:33:52,788 : loading BERT model bert-large-uncased
2019-03-12 20:33:52,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:33:52,873 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:33:52,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpibqx1gpv
2019-03-12 20:34:00,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:06,266 : PROGRESS (encoding): 0.00%
2019-03-12 20:36:50,320 : PROGRESS (encoding): 14.56%
2019-03-12 20:39:56,525 : PROGRESS (encoding): 29.12%
2019-03-12 20:43:03,591 : PROGRESS (encoding): 43.69%
2019-03-12 20:46:23,174 : PROGRESS (encoding): 58.25%
2019-03-12 20:50:05,132 : PROGRESS (encoding): 72.81%
2019-03-12 20:53:45,956 : PROGRESS (encoding): 87.37%
2019-03-12 20:57:44,916 : PROGRESS (encoding): 0.00%
2019-03-12 20:58:15,006 : PROGRESS (encoding): 0.00%
2019-03-12 20:58:43,945 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:59:30,430 : [('reg:1e-09', 62.94)]
2019-03-12 20:59:30,430 : Validation : best param found is reg = 1e-09 with score             62.94
2019-03-12 20:59:30,430 : Evaluating...
2019-03-12 21:00:17,513 : Dev acc : 62.94 Test acc : 63.09 for SNLI

2019-03-12 21:00:17,513 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 21:00:17,720 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 21:00:18,774 : loading BERT model bert-large-uncased
2019-03-12 21:00:18,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:00:18,801 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:00:18,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpokh2rb34
2019-03-12 21:00:26,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:00:31,603 : Computing embeddings for train/dev/test
2019-03-12 21:04:00,398 : Computed embeddings
2019-03-12 21:04:00,399 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:04:29,601 : [('reg:1e-05', 91.98), ('reg:0.0001', 89.77), ('reg:0.001', 82.18), ('reg:0.01', 69.51)]
2019-03-12 21:04:29,601 : Validation : best param found is reg = 1e-05 with score             91.98
2019-03-12 21:04:29,601 : Evaluating...
2019-03-12 21:04:37,416 : 
Dev acc : 92.0 Test acc : 92.1 for LENGTH classification

2019-03-12 21:04:37,417 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 21:04:37,668 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 21:04:37,713 : loading BERT model bert-large-uncased
2019-03-12 21:04:37,713 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:04:37,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:04:37,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuupzfi4t
2019-03-12 21:04:45,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:04:50,406 : Computing embeddings for train/dev/test
2019-03-12 21:08:03,003 : Computed embeddings
2019-03-12 21:08:03,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:08:26,981 : [('reg:1e-05', 23.79), ('reg:0.0001', 2.56), ('reg:0.001', 0.43), ('reg:0.01', 0.2)]
2019-03-12 21:08:26,981 : Validation : best param found is reg = 1e-05 with score             23.79
2019-03-12 21:08:26,981 : Evaluating...
2019-03-12 21:08:35,163 : 
Dev acc : 23.8 Test acc : 24.3 for WORDCONTENT classification

2019-03-12 21:08:35,164 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 21:08:35,696 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 21:08:35,761 : loading BERT model bert-large-uncased
2019-03-12 21:08:35,761 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:08:35,785 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:08:35,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp75do1p4i
2019-03-12 21:08:43,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:08:48,577 : Computing embeddings for train/dev/test
2019-03-12 21:11:49,617 : Computed embeddings
2019-03-12 21:11:49,617 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:12:13,631 : [('reg:1e-05', 30.45), ('reg:0.0001', 30.08), ('reg:0.001', 28.06), ('reg:0.01', 25.39)]
2019-03-12 21:12:13,632 : Validation : best param found is reg = 1e-05 with score             30.45
2019-03-12 21:12:13,632 : Evaluating...
2019-03-12 21:12:20,137 : 
Dev acc : 30.4 Test acc : 30.4 for DEPTH classification

2019-03-12 21:12:20,138 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 21:12:20,506 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 21:12:20,569 : loading BERT model bert-large-uncased
2019-03-12 21:12:20,569 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:12:20,677 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:12:20,677 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp89koejrh
2019-03-12 21:12:28,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:12:33,660 : Computing embeddings for train/dev/test
2019-03-12 21:15:21,578 : Computed embeddings
2019-03-12 21:15:21,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:15:53,885 : [('reg:1e-05', 57.36), ('reg:0.0001', 51.94), ('reg:0.001', 39.73), ('reg:0.01', 16.94)]
2019-03-12 21:15:53,886 : Validation : best param found is reg = 1e-05 with score             57.36
2019-03-12 21:15:53,886 : Evaluating...
2019-03-12 21:16:02,837 : 
Dev acc : 57.4 Test acc : 57.2 for TOPCONSTITUENTS classification

2019-03-12 21:16:02,838 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 21:16:03,220 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 21:16:03,287 : loading BERT model bert-large-uncased
2019-03-12 21:16:03,288 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:16:03,317 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:16:03,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj4jmjqst
2019-03-12 21:16:10,826 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:16:16,285 : Computing embeddings for train/dev/test
2019-03-12 21:19:18,488 : Computed embeddings
2019-03-12 21:19:18,488 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:19:47,824 : [('reg:1e-05', 66.75), ('reg:0.0001', 67.81), ('reg:0.001', 64.81), ('reg:0.01', 56.84)]
2019-03-12 21:19:47,824 : Validation : best param found is reg = 0.0001 with score             67.81
2019-03-12 21:19:47,824 : Evaluating...
2019-03-12 21:19:55,771 : 
Dev acc : 67.8 Test acc : 68.0 for BIGRAMSHIFT classification

2019-03-12 21:19:55,772 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 21:19:56,161 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 21:19:56,227 : loading BERT model bert-large-uncased
2019-03-12 21:19:56,227 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:19:56,257 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:19:56,257 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdew29uew
2019-03-12 21:20:03,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:20:09,585 : Computing embeddings for train/dev/test
2019-03-12 21:23:07,991 : Computed embeddings
2019-03-12 21:23:07,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:23:31,015 : [('reg:1e-05', 84.76), ('reg:0.0001', 84.71), ('reg:0.001', 84.34), ('reg:0.01', 81.9)]
2019-03-12 21:23:31,015 : Validation : best param found is reg = 1e-05 with score             84.76
2019-03-12 21:23:31,015 : Evaluating...
2019-03-12 21:23:37,431 : 
Dev acc : 84.8 Test acc : 83.7 for TENSE classification

2019-03-12 21:23:37,432 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 21:23:37,886 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 21:23:37,953 : loading BERT model bert-large-uncased
2019-03-12 21:23:37,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:23:38,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:23:38,085 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0rnk4fss
2019-03-12 21:23:45,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:23:51,040 : Computing embeddings for train/dev/test
2019-03-12 21:26:59,758 : Computed embeddings
2019-03-12 21:26:59,758 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:27:35,627 : [('reg:1e-05', 80.27), ('reg:0.0001', 80.26), ('reg:0.001', 78.07), ('reg:0.01', 73.06)]
2019-03-12 21:27:35,627 : Validation : best param found is reg = 1e-05 with score             80.27
2019-03-12 21:27:35,627 : Evaluating...
2019-03-12 21:27:45,540 : 
Dev acc : 80.3 Test acc : 79.6 for SUBJNUMBER classification

2019-03-12 21:27:45,541 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 21:27:45,946 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 21:27:46,013 : loading BERT model bert-large-uncased
2019-03-12 21:27:46,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:27:46,130 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:27:46,130 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7rd2plqq
2019-03-12 21:27:53,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:27:58,948 : Computing embeddings for train/dev/test
2019-03-12 21:31:04,215 : Computed embeddings
2019-03-12 21:31:04,215 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:31:36,410 : [('reg:1e-05', 73.05), ('reg:0.0001', 75.76), ('reg:0.001', 74.91), ('reg:0.01', 68.88)]
2019-03-12 21:31:36,411 : Validation : best param found is reg = 0.0001 with score             75.76
2019-03-12 21:31:36,411 : Evaluating...
2019-03-12 21:31:42,868 : 
Dev acc : 75.8 Test acc : 77.3 for OBJNUMBER classification

2019-03-12 21:31:42,869 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 21:31:43,440 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 21:31:43,508 : loading BERT model bert-large-uncased
2019-03-12 21:31:43,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:31:43,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:31:43,537 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbj041mc9
2019-03-12 21:31:51,068 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:31:56,406 : Computing embeddings for train/dev/test
2019-03-12 21:35:31,210 : Computed embeddings
2019-03-12 21:35:31,210 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:35:55,431 : [('reg:1e-05', 53.96), ('reg:0.0001', 54.03), ('reg:0.001', 53.96), ('reg:0.01', 52.81)]
2019-03-12 21:35:55,432 : Validation : best param found is reg = 0.0001 with score             54.03
2019-03-12 21:35:55,432 : Evaluating...
2019-03-12 21:36:01,369 : 
Dev acc : 54.0 Test acc : 56.1 for ODDMANOUT classification

2019-03-12 21:36:01,370 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 21:36:01,745 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 21:36:01,825 : loading BERT model bert-large-uncased
2019-03-12 21:36:01,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:36:01,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:36:01,854 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7jaikjsq
2019-03-12 21:36:09,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:36:14,711 : Computing embeddings for train/dev/test
2019-03-12 21:39:47,521 : Computed embeddings
2019-03-12 21:39:47,521 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:40:12,475 : [('reg:1e-05', 56.78), ('reg:0.0001', 56.81), ('reg:0.001', 55.51), ('reg:0.01', 50.0)]
2019-03-12 21:40:12,475 : Validation : best param found is reg = 0.0001 with score             56.81
2019-03-12 21:40:12,475 : Evaluating...
2019-03-12 21:40:18,954 : 
Dev acc : 56.8 Test acc : 56.6 for COORDINATIONINVERSION classification

2019-03-12 21:40:18,956 : total results: {'STS12': {'MSRpar': {'pearson': (0.3123443100076109, 1.9572814492041822e-18), 'spearman': SpearmanrResult(correlation=0.34601292841694253, pvalue=1.617607171919657e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6089503503625481, 2.6361163725971005e-77), 'spearman': SpearmanrResult(correlation=0.6234871732278064, pvalue=5.2314403644562665e-82), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48247497536530426, 3.8826996296605097e-28), 'spearman': SpearmanrResult(correlation=0.5993800002516098, pvalue=4.166105555309672e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6023665547990826, 2.9615505410226793e-75), 'spearman': SpearmanrResult(correlation=0.6225342101013607, pvalue=1.0826708287228717e-81), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6072696348406758, 1.4236904290672778e-41), 'spearman': SpearmanrResult(correlation=0.5628471523914375, pvalue=1.0178950152853938e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5226811650750444, 'wmean': 0.51689269928283}, 'spearman': {'mean': 0.5508522928778313, 'wmean': 0.5449540436709313}}}, 'STS13': {'FNWN': {'pearson': (0.2924603130957109, 4.4336253008682524e-05), 'spearman': SpearmanrResult(correlation=0.29674911331843756, pvalue=3.378215072213952e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.5410459719334577, 2.98490030025753e-58), 'spearman': SpearmanrResult(correlation=0.5240759734558517, pvalue=3.867961509671951e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.44806547227054405, 4.6940165023810647e-29), 'spearman': SpearmanrResult(correlation=0.47963134828199117, pvalue=1.2925529063456066e-33), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42719058576657093, 'wmean': 0.4749494720459719}, 'spearman': {'mean': 0.43348547835209345, 'wmean': 0.47881049926351366}}}, 'STS14': {'deft-forum': {'pearson': (0.32884966536677895, 8.273024267077439e-13), 'spearman': SpearmanrResult(correlation=0.3404513878322925, pvalue=1.1283776790055303e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.6160237413505947, 9.83101841893106e-33), 'spearman': SpearmanrResult(correlation=0.620607189211457, pvalue=2.4867459581595505e-33), 'nsamples': 300}, 'headlines': {'pearson': (0.5071808454908383, 2.8643530348414635e-50), 'spearman': SpearmanrResult(correlation=0.4666624250513605, pvalue=7.944812032107214e-42), 'nsamples': 750}, 'images': {'pearson': (0.6191547990779108, 1.3992764327936907e-80), 'spearman': SpearmanrResult(correlation=0.623307417415817, pvalue=6.001888805513973e-82), 'nsamples': 750}, 'OnWN': {'pearson': (0.5729642695346736, 1.180344026560217e-66), 'spearman': SpearmanrResult(correlation=0.6212923729266984, pvalue=2.782839923463812e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6572986574871835, 5.76402647751942e-94), 'spearman': SpearmanrResult(correlation=0.605284003256089, pvalue=3.7049262361964023e-76), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5502453297179967, 'wmean': 0.5600635734701822}, 'spearman': {'mean': 0.5462674659489524, 'wmean': 0.5538119854067847}}}, 'STS15': {'answers-forums': {'pearson': (0.4372499303059794, 6.080236061588616e-19), 'spearman': SpearmanrResult(correlation=0.4263533646774014, pvalue=5.389008758489888e-18), 'nsamples': 375}, 'answers-students': {'pearson': (0.6186267102623135, 2.0813989083469925e-80), 'spearman': SpearmanrResult(correlation=0.6349483611882775, pvalue=6.796304369379183e-86), 'nsamples': 750}, 'belief': {'pearson': (0.5716816045094371, 6.349996243494456e-34), 'spearman': SpearmanrResult(correlation=0.587457419644763, pvalue=3.635079215123742e-36), 'nsamples': 375}, 'headlines': {'pearson': (0.5892219733707204, 2.6644240471989996e-71), 'spearman': SpearmanrResult(correlation=0.5942610314536696, pvalue=8.537157097278702e-73), 'nsamples': 750}, 'images': {'pearson': (0.6573893554955411, 5.327977803859129e-94), 'spearman': SpearmanrResult(correlation=0.6940105959514733, pvalue=7.792483378038059e-109), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5748339147887983, 'wmean': 0.5924259516340709}, 'spearman': {'mean': 0.587406154583117, 'wmean': 0.6075313451886256}}}, 'STS16': {'answer-answer': {'pearson': (0.47671798829263135, 8.111844350405371e-16), 'spearman': SpearmanrResult(correlation=0.5002926496770265, pvalue=1.7098642607758126e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6276967046162363, 1.0960981449914044e-28), 'spearman': SpearmanrResult(correlation=0.6420794738400454, pvalue=2.4487084117143747e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7484084584357448, 1.5400178262035778e-42), 'spearman': SpearmanrResult(correlation=0.7628249359987475, pvalue=4.617865838485237e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7975982246517442, 4.748190549716933e-55), 'spearman': SpearmanrResult(correlation=0.8251529885067053, pvalue=5.903029499420087e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.5666516591316044, 3.7854467955469994e-19), 'spearman': SpearmanrResult(correlation=0.56805999416994, pvalue=2.9589651423434665e-19), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6434146070255922, 'wmean': 0.6429687668545728}, 'spearman': {'mean': 0.6596820084384929, 'wmean': 0.6597501899333909}}}, 'MR': {'devacc': 71.89, 'acc': 72.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.44, 'acc': 76.85, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.21, 'acc': 85.43, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.63, 'acc': 92.03, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.56, 'acc': 79.57, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.51, 'acc': 40.59, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 65.89, 'acc': 80.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.71, 'acc': 70.26, 'f1': 80.53, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.4, 'acc': 77.39, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7853237480092348, 'pearson': 0.8007500893307419, 'spearman': 0.728730275142403, 'mse': 0.3651200900125144, 'yhat': array([2.00598535, 4.46908024, 2.33874343, ..., 3.53487533, 4.45630587,        4.62737595]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7332903761268001, 'pearson': 0.6712920276888341, 'spearman': 0.6618226806881939, 'mse': 1.48504759952621, 'yhat': array([2.18946613, 1.48743683, 1.58338863, ..., 4.11186107, 3.87906618,        3.50095403]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.94, 'acc': 63.09, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.98, 'acc': 92.09, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 23.79, 'acc': 24.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.45, 'acc': 30.39, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.36, 'acc': 57.23, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 67.81, 'acc': 68.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.76, 'acc': 83.72, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.27, 'acc': 79.57, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.76, 'acc': 77.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.03, 'acc': 56.06, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.81, 'acc': 56.64, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 21:40:18,956 : STS12 p=0.5169, STS12 s=0.5450, STS13 p=0.4749, STS13 s=0.4788, STS14 p=0.5601, STS14 s=0.5538, STS15 p=0.5924, STS15 s=0.6075, STS 16 p=0.6430, STS16 s=0.6598, STS B p=0.6713, STS B s=0.6618, STS B m=1.4850, SICK-R p=0.8008, SICK-R s=0.7287, SICK-P m=0.3651
2019-03-12 21:40:18,956 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 21:40:18,956 : 0.5169,0.5450,0.4749,0.4788,0.5601,0.5538,0.5924,0.6075,0.6430,0.6598,0.6713,0.6618,1.4850,0.8008,0.7287,0.3651
2019-03-12 21:40:18,956 : MR=72.00, CR=76.85, SUBJ=92.03, MPQA=85.43, SST-B=79.57, SST-F=40.59, TREC=80.60, SICK-E=77.39, SNLI=63.09, MRPC=70.26, MRPC f=80.53
2019-03-12 21:40:18,956 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 21:40:18,956 : 72.00,76.85,92.03,85.43,79.57,40.59,80.60,77.39,63.09,70.26,80.53
2019-03-12 21:40:18,956 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 21:40:18,956 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 21:40:18,956 : na,na,na,na,na,na,na,na,na,na
2019-03-12 21:40:18,956 : SentLen=92.09, WC=24.31, TreeDepth=30.39, TopConst=57.23, BShift=68.01, Tense=83.72, SubjNum=79.57, ObjNum=77.35, SOMO=56.06, CoordInv=56.64, average=62.54
2019-03-12 21:40:18,956 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 21:40:18,956 : 92.09,24.31,30.39,57.23,68.01,83.72,79.57,77.35,56.06,56.64,62.54
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:18,956 : layer 6
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:18,956 : ********************************************************************************
2019-03-12 21:40:19,048 : ***** Transfer task : STS12 *****


2019-03-12 21:40:19,061 : loading BERT model bert-large-uncased
2019-03-12 21:40:19,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:40:19,078 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:40:19,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph8paussx
2019-03-12 21:40:26,514 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:40:35,881 : MSRpar : pearson = 0.1974, spearman = 0.3112
2019-03-12 21:40:37,511 : MSRvid : pearson = 0.5603, spearman = 0.5877
2019-03-12 21:40:38,916 : SMTeuroparl : pearson = 0.3788, spearman = 0.5439
2019-03-12 21:40:41,603 : surprise.OnWN : pearson = 0.5587, spearman = 0.6195
2019-03-12 21:40:43,023 : surprise.SMTnews : pearson = 0.5384, spearman = 0.5140
2019-03-12 21:40:43,024 : ALL (weighted average) : Pearson = 0.4427,             Spearman = 0.5127
2019-03-12 21:40:43,024 : ALL (average) : Pearson = 0.4467,             Spearman = 0.5153

2019-03-12 21:40:43,024 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 21:40:43,032 : loading BERT model bert-large-uncased
2019-03-12 21:40:43,032 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:40:43,050 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:40:43,050 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7nzogzjj
2019-03-12 21:40:50,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:40:57,305 : FNWN : pearson = 0.3291, spearman = 0.3086
2019-03-12 21:40:59,189 : headlines : pearson = 0.5852, spearman = 0.5965
2019-03-12 21:41:00,650 : OnWN : pearson = 0.5536, spearman = 0.5908
2019-03-12 21:41:00,650 : ALL (weighted average) : Pearson = 0.5411,             Spearman = 0.5581
2019-03-12 21:41:00,650 : ALL (average) : Pearson = 0.4893,             Spearman = 0.4986

2019-03-12 21:41:00,650 : ***** Transfer task : STS14 *****


2019-03-12 21:41:00,666 : loading BERT model bert-large-uncased
2019-03-12 21:41:00,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:41:00,683 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:41:00,684 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmporx9qwfo
2019-03-12 21:41:08,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:41:14,826 : deft-forum : pearson = 0.2793, spearman = 0.3294
2019-03-12 21:41:16,456 : deft-news : pearson = 0.6564, spearman = 0.6483
2019-03-12 21:41:18,613 : headlines : pearson = 0.5379, spearman = 0.5247
2019-03-12 21:41:20,677 : images : pearson = 0.6194, spearman = 0.6076
2019-03-12 21:41:22,799 : OnWN : pearson = 0.6319, spearman = 0.6917
2019-03-12 21:41:25,649 : tweet-news : pearson = 0.6265, spearman = 0.5811
2019-03-12 21:41:25,649 : ALL (weighted average) : Pearson = 0.5692,             Spearman = 0.5724
2019-03-12 21:41:25,649 : ALL (average) : Pearson = 0.5586,             Spearman = 0.5638

2019-03-12 21:41:25,649 : ***** Transfer task : STS15 *****


2019-03-12 21:41:25,681 : loading BERT model bert-large-uncased
2019-03-12 21:41:25,681 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:41:25,698 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:41:25,698 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzkp4t4ee
2019-03-12 21:41:33,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:41:40,180 : answers-forums : pearson = 0.3745, spearman = 0.3465
2019-03-12 21:41:42,254 : answers-students : pearson = 0.5760, spearman = 0.6303
2019-03-12 21:41:44,296 : belief : pearson = 0.5502, spearman = 0.5679
2019-03-12 21:41:46,537 : headlines : pearson = 0.6107, spearman = 0.6359
2019-03-12 21:41:48,658 : images : pearson = 0.6866, spearman = 0.7081
2019-03-12 21:41:48,659 : ALL (weighted average) : Pearson = 0.5839,             Spearman = 0.6079
2019-03-12 21:41:48,659 : ALL (average) : Pearson = 0.5596,             Spearman = 0.5777

2019-03-12 21:41:48,659 : ***** Transfer task : STS16 *****


2019-03-12 21:41:48,729 : loading BERT model bert-large-uncased
2019-03-12 21:41:48,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:41:48,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:41:48,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbygh9or0
2019-03-12 21:41:56,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:42:02,389 : answer-answer : pearson = 0.4527, spearman = 0.5020
2019-03-12 21:42:03,042 : headlines : pearson = 0.6376, spearman = 0.6800
2019-03-12 21:42:03,919 : plagiarism : pearson = 0.6130, spearman = 0.7243
2019-03-12 21:42:05,404 : postediting : pearson = 0.7406, spearman = 0.7841
2019-03-12 21:42:06,006 : question-question : pearson = 0.5584, spearman = 0.5660
2019-03-12 21:42:06,007 : ALL (weighted average) : Pearson = 0.6005,             Spearman = 0.6518
2019-03-12 21:42:06,007 : ALL (average) : Pearson = 0.6005,             Spearman = 0.6513

2019-03-12 21:42:06,007 : ***** Transfer task : MR *****


2019-03-12 21:42:06,022 : loading BERT model bert-large-uncased
2019-03-12 21:42:06,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:42:06,041 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:42:06,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcpoyu230
2019-03-12 21:42:13,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:42:18,823 : Generating sentence embeddings
2019-03-12 21:42:50,166 : Generated sentence embeddings
2019-03-12 21:42:50,167 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:42:58,583 : Best param found at split 1: l2reg = 1e-05                 with score 72.68
2019-03-12 21:43:08,340 : Best param found at split 2: l2reg = 1e-05                 with score 71.17
2019-03-12 21:43:18,431 : Best param found at split 3: l2reg = 1e-05                 with score 72.98
2019-03-12 21:43:29,274 : Best param found at split 4: l2reg = 0.001                 with score 72.19
2019-03-12 21:43:37,306 : Best param found at split 5: l2reg = 0.0001                 with score 71.58
2019-03-12 21:43:37,818 : Dev acc : 72.12 Test acc : 71.69

2019-03-12 21:43:37,819 : ***** Transfer task : CR *****


2019-03-12 21:43:37,826 : loading BERT model bert-large-uncased
2019-03-12 21:43:37,826 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:43:37,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:43:37,847 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy9ppq6o9
2019-03-12 21:43:45,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:43:50,770 : Generating sentence embeddings
2019-03-12 21:43:59,067 : Generated sentence embeddings
2019-03-12 21:43:59,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:44:02,304 : Best param found at split 1: l2reg = 1e-05                 with score 76.08
2019-03-12 21:44:05,636 : Best param found at split 2: l2reg = 1e-05                 with score 76.75
2019-03-12 21:44:09,766 : Best param found at split 3: l2reg = 1e-05                 with score 75.93
2019-03-12 21:44:13,475 : Best param found at split 4: l2reg = 0.001                 with score 74.78
2019-03-12 21:44:16,914 : Best param found at split 5: l2reg = 0.0001                 with score 75.77
2019-03-12 21:44:17,133 : Dev acc : 75.86 Test acc : 75.47

2019-03-12 21:44:17,134 : ***** Transfer task : MPQA *****


2019-03-12 21:44:17,140 : loading BERT model bert-large-uncased
2019-03-12 21:44:17,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:44:17,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:44:17,191 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5dvauwy2
2019-03-12 21:44:24,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:44:29,962 : Generating sentence embeddings
2019-03-12 21:44:37,517 : Generated sentence embeddings
2019-03-12 21:44:37,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:44:47,023 : Best param found at split 1: l2reg = 1e-05                 with score 83.65
2019-03-12 21:44:58,546 : Best param found at split 2: l2reg = 0.0001                 with score 85.01
2019-03-12 21:45:09,289 : Best param found at split 3: l2reg = 1e-05                 with score 84.4
2019-03-12 21:45:19,800 : Best param found at split 4: l2reg = 0.0001                 with score 84.75
2019-03-12 21:45:28,406 : Best param found at split 5: l2reg = 0.0001                 with score 85.48
2019-03-12 21:45:29,042 : Dev acc : 84.66 Test acc : 85.58

2019-03-12 21:45:29,043 : ***** Transfer task : SUBJ *****


2019-03-12 21:45:29,058 : loading BERT model bert-large-uncased
2019-03-12 21:45:29,059 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:45:29,078 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:45:29,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5x9fucf_
2019-03-12 21:45:36,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:45:41,948 : Generating sentence embeddings
2019-03-12 21:46:12,675 : Generated sentence embeddings
2019-03-12 21:46:12,675 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:46:20,948 : Best param found at split 1: l2reg = 0.0001                 with score 92.56
2019-03-12 21:46:29,165 : Best param found at split 2: l2reg = 0.001                 with score 92.8
2019-03-12 21:46:40,122 : Best param found at split 3: l2reg = 0.0001                 with score 92.31
2019-03-12 21:46:50,990 : Best param found at split 4: l2reg = 1e-05                 with score 92.94
2019-03-12 21:47:01,620 : Best param found at split 5: l2reg = 1e-05                 with score 92.59
2019-03-12 21:47:02,035 : Dev acc : 92.64 Test acc : 92.3

2019-03-12 21:47:02,036 : ***** Transfer task : SST Binary classification *****


2019-03-12 21:47:02,129 : loading BERT model bert-large-uncased
2019-03-12 21:47:02,129 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:47:02,202 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:47:02,202 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp033985d_
2019-03-12 21:47:09,639 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:47:14,965 : Computing embedding for train
2019-03-12 21:48:54,385 : Computed train embeddings
2019-03-12 21:48:54,386 : Computing embedding for dev
2019-03-12 21:48:56,573 : Computed dev embeddings
2019-03-12 21:48:56,573 : Computing embedding for test
2019-03-12 21:49:01,120 : Computed test embeddings
2019-03-12 21:49:01,120 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:49:16,897 : [('reg:1e-05', 78.21), ('reg:0.0001', 78.21), ('reg:0.001', 77.98), ('reg:0.01', 76.49)]
2019-03-12 21:49:16,897 : Validation : best param found is reg = 1e-05 with score             78.21
2019-03-12 21:49:16,897 : Evaluating...
2019-03-12 21:49:21,211 : 
Dev acc : 78.21 Test acc : 79.96 for             SST Binary classification

2019-03-12 21:49:21,211 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 21:49:21,267 : loading BERT model bert-large-uncased
2019-03-12 21:49:21,267 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:49:21,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:49:21,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpma3aoivu
2019-03-12 21:49:28,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:49:34,261 : Computing embedding for train
2019-03-12 21:49:56,034 : Computed train embeddings
2019-03-12 21:49:56,034 : Computing embedding for dev
2019-03-12 21:49:58,876 : Computed dev embeddings
2019-03-12 21:49:58,877 : Computing embedding for test
2019-03-12 21:50:04,477 : Computed test embeddings
2019-03-12 21:50:04,477 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:50:06,943 : [('reg:1e-05', 35.24), ('reg:0.0001', 35.51), ('reg:0.001', 37.87), ('reg:0.01', 32.79)]
2019-03-12 21:50:06,943 : Validation : best param found is reg = 0.001 with score             37.87
2019-03-12 21:50:06,943 : Evaluating...
2019-03-12 21:50:07,581 : 
Dev acc : 37.87 Test acc : 41.36 for             SST Fine-Grained classification

2019-03-12 21:50:07,582 : ***** Transfer task : TREC *****


2019-03-12 21:50:07,595 : loading BERT model bert-large-uncased
2019-03-12 21:50:07,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:50:07,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:50:07,614 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg89x50gv
2019-03-12 21:50:15,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:50:27,957 : Computed train embeddings
2019-03-12 21:50:28,543 : Computed test embeddings
2019-03-12 21:50:28,544 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 21:50:34,510 : [('reg:1e-05', 64.51), ('reg:0.0001', 65.64), ('reg:0.001', 60.42), ('reg:0.01', 50.59)]
2019-03-12 21:50:34,510 : Cross-validation : best param found is reg = 0.0001             with score 65.64
2019-03-12 21:50:34,510 : Evaluating...
2019-03-12 21:50:34,836 : 
Dev acc : 65.64 Test acc : 82.0             for TREC

2019-03-12 21:50:34,837 : ***** Transfer task : MRPC *****


2019-03-12 21:50:34,857 : loading BERT model bert-large-uncased
2019-03-12 21:50:34,857 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:50:34,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:50:34,880 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoh75677n
2019-03-12 21:50:42,379 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:50:47,479 : Computing embedding for train
2019-03-12 21:51:09,599 : Computed train embeddings
2019-03-12 21:51:09,599 : Computing embedding for test
2019-03-12 21:51:19,274 : Computed test embeddings
2019-03-12 21:51:19,295 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 21:51:23,976 : [('reg:1e-05', 70.12), ('reg:0.0001', 70.73), ('reg:0.001', 70.19), ('reg:0.01', 70.24)]
2019-03-12 21:51:23,976 : Cross-validation : best param found is reg = 0.0001             with score 70.73
2019-03-12 21:51:23,976 : Evaluating...
2019-03-12 21:51:24,283 : Dev acc : 70.73 Test acc 71.88; Test F1 79.03 for MRPC.

2019-03-12 21:51:24,284 : ***** Transfer task : SICK-Entailment*****


2019-03-12 21:51:24,346 : loading BERT model bert-large-uncased
2019-03-12 21:51:24,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:51:24,366 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:51:24,366 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmixbqkf_
2019-03-12 21:51:31,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:51:37,019 : Computing embedding for train
2019-03-12 21:51:48,264 : Computed train embeddings
2019-03-12 21:51:48,264 : Computing embedding for dev
2019-03-12 21:51:49,798 : Computed dev embeddings
2019-03-12 21:51:49,798 : Computing embedding for test
2019-03-12 21:52:01,845 : Computed test embeddings
2019-03-12 21:52:01,881 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:52:03,459 : [('reg:1e-05', 76.8), ('reg:0.0001', 77.0), ('reg:0.001', 77.0), ('reg:0.01', 72.6)]
2019-03-12 21:52:03,459 : Validation : best param found is reg = 0.0001 with score             77.0
2019-03-12 21:52:03,459 : Evaluating...
2019-03-12 21:52:03,912 : 
Dev acc : 77.0 Test acc : 74.43 for                        SICK entailment

2019-03-12 21:52:03,912 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 21:52:03,939 : loading BERT model bert-large-uncased
2019-03-12 21:52:03,939 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:52:03,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:52:03,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxczqq1qm
2019-03-12 21:52:11,490 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:52:16,727 : Computing embedding for train
2019-03-12 21:52:27,954 : Computed train embeddings
2019-03-12 21:52:27,954 : Computing embedding for dev
2019-03-12 21:52:29,486 : Computed dev embeddings
2019-03-12 21:52:29,486 : Computing embedding for test
2019-03-12 21:52:41,522 : Computed test embeddings
2019-03-12 21:52:55,333 : Dev : Pearson 0.7959411962144851
2019-03-12 21:52:55,333 : Test : Pearson 0.7853623532428059 Spearman 0.7083514083937164 MSE 0.3902000899036207                        for SICK Relatedness

2019-03-12 21:52:55,334 : 

***** Transfer task : STSBenchmark*****


2019-03-12 21:52:55,385 : loading BERT model bert-large-uncased
2019-03-12 21:52:55,385 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:52:55,412 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:52:55,412 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfxx3uqit
2019-03-12 21:53:02,892 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:53:08,194 : Computing embedding for train
2019-03-12 21:53:26,717 : Computed train embeddings
2019-03-12 21:53:26,717 : Computing embedding for dev
2019-03-12 21:53:32,325 : Computed dev embeddings
2019-03-12 21:53:32,325 : Computing embedding for test
2019-03-12 21:53:36,896 : Computed test embeddings
2019-03-12 21:53:55,912 : Dev : Pearson 0.7238890293060682
2019-03-12 21:53:55,913 : Test : Pearson 0.6775177204031786 Spearman 0.6683992377315126 MSE 1.4652224069135638                        for SICK Relatedness

2019-03-12 21:53:55,913 : ***** Transfer task : SNLI Entailment*****


2019-03-12 21:54:01,129 : loading BERT model bert-large-uncased
2019-03-12 21:54:01,129 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:54:01,263 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:54:01,263 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsv9_npff
2019-03-12 21:54:08,720 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:54:14,527 : PROGRESS (encoding): 0.00%
2019-03-12 21:56:58,873 : PROGRESS (encoding): 14.56%
2019-03-12 22:00:05,131 : PROGRESS (encoding): 29.12%
2019-03-12 22:03:12,134 : PROGRESS (encoding): 43.69%
2019-03-12 22:06:31,431 : PROGRESS (encoding): 58.25%
2019-03-12 22:10:13,334 : PROGRESS (encoding): 72.81%
2019-03-12 22:13:54,095 : PROGRESS (encoding): 87.37%
2019-03-12 22:17:53,064 : PROGRESS (encoding): 0.00%
2019-03-12 22:18:23,161 : PROGRESS (encoding): 0.00%
2019-03-12 22:18:52,080 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:19:18,695 : [('reg:1e-09', 66.31)]
2019-03-12 22:19:18,695 : Validation : best param found is reg = 1e-09 with score             66.31
2019-03-12 22:19:18,695 : Evaluating...
2019-03-12 22:19:44,678 : Dev acc : 66.31 Test acc : 66.51 for SNLI

2019-03-12 22:19:44,679 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 22:19:44,877 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 22:19:45,941 : loading BERT model bert-large-uncased
2019-03-12 22:19:45,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:19:45,969 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:19:45,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp36i08swl
2019-03-12 22:19:53,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:19:58,920 : Computing embeddings for train/dev/test
2019-03-12 22:23:28,002 : Computed embeddings
2019-03-12 22:23:28,002 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:23:58,544 : [('reg:1e-05', 87.25), ('reg:0.0001', 86.48), ('reg:0.001', 79.45), ('reg:0.01', 70.25)]
2019-03-12 22:23:58,545 : Validation : best param found is reg = 1e-05 with score             87.25
2019-03-12 22:23:58,545 : Evaluating...
2019-03-12 22:24:07,342 : 
Dev acc : 87.2 Test acc : 86.5 for LENGTH classification

2019-03-12 22:24:07,343 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 22:24:07,714 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 22:24:07,758 : loading BERT model bert-large-uncased
2019-03-12 22:24:07,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:24:07,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:24:07,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpipusz103
2019-03-12 22:24:15,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:24:20,688 : Computing embeddings for train/dev/test
2019-03-12 22:27:33,398 : Computed embeddings
2019-03-12 22:27:33,398 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:28:08,417 : [('reg:1e-05', 39.79), ('reg:0.0001', 4.86), ('reg:0.001', 1.03), ('reg:0.01', 0.37)]
2019-03-12 22:28:08,417 : Validation : best param found is reg = 1e-05 with score             39.79
2019-03-12 22:28:08,417 : Evaluating...
2019-03-12 22:28:20,157 : 
Dev acc : 39.8 Test acc : 40.5 for WORDCONTENT classification

2019-03-12 22:28:20,158 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 22:28:20,542 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 22:28:20,610 : loading BERT model bert-large-uncased
2019-03-12 22:28:20,611 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:28:20,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:28:20,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp65vyf81s
2019-03-12 22:28:28,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:28:33,544 : Computing embeddings for train/dev/test
2019-03-12 22:31:34,181 : Computed embeddings
2019-03-12 22:31:34,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:31:56,900 : [('reg:1e-05', 29.42), ('reg:0.0001', 27.17), ('reg:0.001', 26.89), ('reg:0.01', 23.32)]
2019-03-12 22:31:56,901 : Validation : best param found is reg = 1e-05 with score             29.42
2019-03-12 22:31:56,901 : Evaluating...
2019-03-12 22:32:03,617 : 
Dev acc : 29.4 Test acc : 30.0 for DEPTH classification

2019-03-12 22:32:03,618 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 22:32:04,000 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 22:32:04,062 : loading BERT model bert-large-uncased
2019-03-12 22:32:04,062 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:32:04,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:32:04,169 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn1d89e6c
2019-03-12 22:32:11,614 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:32:17,041 : Computing embeddings for train/dev/test
2019-03-12 22:35:04,781 : Computed embeddings
2019-03-12 22:35:04,781 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:35:37,234 : [('reg:1e-05', 57.15), ('reg:0.0001', 53.56), ('reg:0.001', 41.47), ('reg:0.01', 24.31)]
2019-03-12 22:35:37,234 : Validation : best param found is reg = 1e-05 with score             57.15
2019-03-12 22:35:37,234 : Evaluating...
2019-03-12 22:35:47,119 : 
Dev acc : 57.1 Test acc : 56.2 for TOPCONSTITUENTS classification

2019-03-12 22:35:47,120 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 22:35:47,463 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 22:35:47,529 : loading BERT model bert-large-uncased
2019-03-12 22:35:47,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:35:47,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:35:47,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx4yt1kbu
2019-03-12 22:35:55,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:36:00,438 : Computing embeddings for train/dev/test
2019-03-12 22:39:02,578 : Computed embeddings
2019-03-12 22:39:02,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:39:37,484 : [('reg:1e-05', 70.02), ('reg:0.0001', 69.15), ('reg:0.001', 67.74), ('reg:0.01', 56.89)]
2019-03-12 22:39:37,485 : Validation : best param found is reg = 1e-05 with score             70.02
2019-03-12 22:39:37,485 : Evaluating...
2019-03-12 22:39:45,008 : 
Dev acc : 70.0 Test acc : 69.1 for BIGRAMSHIFT classification

2019-03-12 22:39:45,009 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 22:39:45,563 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 22:39:45,628 : loading BERT model bert-large-uncased
2019-03-12 22:39:45,628 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:39:45,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:39:45,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqizxhv7f
2019-03-12 22:39:53,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:39:58,382 : Computing embeddings for train/dev/test
2019-03-12 22:42:56,647 : Computed embeddings
2019-03-12 22:42:56,647 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:43:19,866 : [('reg:1e-05', 87.28), ('reg:0.0001', 87.36), ('reg:0.001', 87.02), ('reg:0.01', 85.6)]
2019-03-12 22:43:19,866 : Validation : best param found is reg = 0.0001 with score             87.36
2019-03-12 22:43:19,866 : Evaluating...
2019-03-12 22:43:26,306 : 
Dev acc : 87.4 Test acc : 85.9 for TENSE classification

2019-03-12 22:43:26,307 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 22:43:26,727 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 22:43:26,790 : loading BERT model bert-large-uncased
2019-03-12 22:43:26,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:43:26,815 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:43:26,815 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvfyio0am
2019-03-12 22:43:34,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:43:39,603 : Computing embeddings for train/dev/test
2019-03-12 22:46:48,696 : Computed embeddings
2019-03-12 22:46:48,696 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:47:22,959 : [('reg:1e-05', 78.69), ('reg:0.0001', 77.89), ('reg:0.001', 78.37), ('reg:0.01', 71.06)]
2019-03-12 22:47:22,959 : Validation : best param found is reg = 1e-05 with score             78.69
2019-03-12 22:47:22,959 : Evaluating...
2019-03-12 22:47:32,597 : 
Dev acc : 78.7 Test acc : 77.3 for SUBJNUMBER classification

2019-03-12 22:47:32,598 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 22:47:33,005 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 22:47:33,072 : loading BERT model bert-large-uncased
2019-03-12 22:47:33,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:47:33,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:47:33,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_3dji5vx
2019-03-12 22:47:40,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:47:46,127 : Computing embeddings for train/dev/test
2019-03-12 22:50:51,391 : Computed embeddings
2019-03-12 22:50:51,391 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:51:21,489 : [('reg:1e-05', 76.68), ('reg:0.0001', 76.75), ('reg:0.001', 75.54), ('reg:0.01', 70.89)]
2019-03-12 22:51:21,489 : Validation : best param found is reg = 0.0001 with score             76.75
2019-03-12 22:51:21,489 : Evaluating...
2019-03-12 22:51:27,607 : 
Dev acc : 76.8 Test acc : 77.8 for OBJNUMBER classification

2019-03-12 22:51:27,608 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 22:51:28,194 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 22:51:28,262 : loading BERT model bert-large-uncased
2019-03-12 22:51:28,262 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:51:28,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:51:28,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdm4t79wg
2019-03-12 22:51:35,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:51:41,033 : Computing embeddings for train/dev/test
2019-03-12 22:55:15,958 : Computed embeddings
2019-03-12 22:55:15,958 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:55:37,474 : [('reg:1e-05', 53.98), ('reg:0.0001', 54.0), ('reg:0.001', 53.56), ('reg:0.01', 52.26)]
2019-03-12 22:55:37,474 : Validation : best param found is reg = 0.0001 with score             54.0
2019-03-12 22:55:37,474 : Evaluating...
2019-03-12 22:55:42,859 : 
Dev acc : 54.0 Test acc : 54.0 for ODDMANOUT classification

2019-03-12 22:55:42,860 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 22:55:43,254 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 22:55:43,331 : loading BERT model bert-large-uncased
2019-03-12 22:55:43,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:55:43,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:55:43,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgez68q0u
2019-03-12 22:55:50,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:55:56,348 : Computing embeddings for train/dev/test
2019-03-12 22:59:29,304 : Computed embeddings
2019-03-12 22:59:29,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:00:01,394 : [('reg:1e-05', 53.22), ('reg:0.0001', 53.03), ('reg:0.001', 51.46), ('reg:0.01', 50.17)]
2019-03-12 23:00:01,394 : Validation : best param found is reg = 1e-05 with score             53.22
2019-03-12 23:00:01,394 : Evaluating...
2019-03-12 23:00:10,053 : 
Dev acc : 53.2 Test acc : 52.8 for COORDINATIONINVERSION classification

2019-03-12 23:00:10,055 : total results: {'STS12': {'MSRpar': {'pearson': (0.1974255908575968, 4.996154540210247e-08), 'spearman': SpearmanrResult(correlation=0.311188091468108, pvalue=2.6480671152759653e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5602579230518037, 3.362888654924678e-63), 'spearman': SpearmanrResult(correlation=0.5877320974279182, pvalue=7.28447250359564e-71), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3787863205875223, 4.1543341462727395e-17), 'spearman': SpearmanrResult(correlation=0.5438803995812517, pvalue=1.0757904962896864e-36), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5587159629408627, 8.626832709656045e-63), 'spearman': SpearmanrResult(correlation=0.6195038263248208, pvalue=1.075833565047647e-80), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.538390002804741, 2.2901595300878685e-31), 'spearman': SpearmanrResult(correlation=0.5139630373735555, pvalue=2.786734110514543e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4467151600485053, 'wmean': 0.4427220527369568}, 'spearman': {'mean': 0.5152534904351308, 'wmean': 0.5127189082160485}}}, 'STS13': {'FNWN': {'pearson': (0.3290529017980477, 3.78013435486706e-06), 'spearman': SpearmanrResult(correlation=0.30864651832400497, pvalue=1.5531756164021173e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.5851745202170835, 4.0453061772156966e-70), 'spearman': SpearmanrResult(correlation=0.596498869762466, pvalue=1.8163230119972685e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.5536081353839668, 2.2842638617392296e-46), 'spearman': SpearmanrResult(correlation=0.5907998384084072, pvalue=4.393044767041688e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.48927851913303266, 'wmean': 0.5410973683686994}, 'spearman': {'mean': 0.4986484088316261, 'wmean': 0.5580980357548019}}}, 'STS14': {'deft-forum': {'pearson': (0.27930148064270843, 1.6519086680383823e-09), 'spearman': SpearmanrResult(correlation=0.32943811142929313, pvalue=7.49298425117325e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.6563700497027153, 2.3847074323619675e-38), 'spearman': SpearmanrResult(correlation=0.6483023924595507, pvalue=3.7021556885558095e-37), 'nsamples': 300}, 'headlines': {'pearson': (0.5378997990925679, 1.7994581660541353e-57), 'spearman': SpearmanrResult(correlation=0.5246513027739446, pvalue=2.830368262450076e-54), 'nsamples': 750}, 'images': {'pearson': (0.6194194101193997, 1.1464861251798045e-80), 'spearman': SpearmanrResult(correlation=0.6076115349446356, pvalue=6.947708176687644e-77), 'nsamples': 750}, 'OnWN': {'pearson': (0.6318898515778872, 7.680762588894753e-85), 'spearman': SpearmanrResult(correlation=0.6916628340505292, pvalue=8.116382287769759e-108), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6264526206219939, 5.353568025386013e-83), 'spearman': SpearmanrResult(correlation=0.5811386812869302, pvalue=5.867837190367701e-69), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5585555352928788, 'wmean': 0.5691581179357119}, 'spearman': {'mean': 0.5638008094908139, 'wmean': 0.5724096353794871}}}, 'STS15': {'answers-forums': {'pearson': (0.37452689464942385, 6.222600110371451e-14), 'spearman': SpearmanrResult(correlation=0.3464648041600679, pvalue=5.135192973256052e-12), 'nsamples': 375}, 'answers-students': {'pearson': (0.5760061788640441, 1.6687155335410155e-67), 'spearman': SpearmanrResult(correlation=0.6303125373966212, pvalue=2.6540251693903866e-84), 'nsamples': 750}, 'belief': {'pearson': (0.5502460684952909, 4.593681619497664e-31), 'spearman': SpearmanrResult(correlation=0.5678788636500811, pvalue=2.1151858131064617e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6106851862839209, 7.457521291430042e-78), 'spearman': SpearmanrResult(correlation=0.6359008050699286, pvalue=3.1760405366049715e-86), 'nsamples': 750}, 'images': {'pearson': (0.6865509021789077, 1.2353410885442744e-105), 'spearman': SpearmanrResult(correlation=0.7081330554415567, pvalue=3.6010332130202503e-115), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5596030460943175, 'wmean': 0.5839071872248075}, 'spearman': {'mean': 0.5777380131436511, 'wmean': 0.6078795579532953}}}, 'STS16': {'answer-answer': {'pearson': (0.4527072087840074, 3.081185092070135e-14), 'spearman': SpearmanrResult(correlation=0.5019581175540468, pvalue=1.2870316353060642e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6375706197640778, 8.239621659224931e-30), 'spearman': SpearmanrResult(correlation=0.6799612025180244, pvalue=3.8694124012646677e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6129922441388638, 4.0001917684706998e-25), 'spearman': SpearmanrResult(correlation=0.7242793801945115, pvalue=1.1159887692697414e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.7405758455274524, 1.1420041035342096e-43), 'spearman': SpearmanrResult(correlation=0.7840994199324032, pvalue=4.7920426223703734e-52), 'nsamples': 244}, 'question-question': {'pearson': (0.5584170455178359, 1.5614745520183253e-18), 'spearman': SpearmanrResult(correlation=0.5659680681729554, pvalue=4.2644129976215343e-19), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6004525927464475, 'wmean': 0.6004558181503018}, 'spearman': {'mean': 0.6512532376743884, 'wmean': 0.6517702727167856}}}, 'MR': {'devacc': 72.12, 'acc': 71.69, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.86, 'acc': 75.47, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.66, 'acc': 85.58, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.64, 'acc': 92.3, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.21, 'acc': 79.96, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.87, 'acc': 41.36, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 65.64, 'acc': 82.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.73, 'acc': 71.88, 'f1': 79.03, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.0, 'acc': 74.43, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7959411962144851, 'pearson': 0.7853623532428059, 'spearman': 0.7083514083937164, 'mse': 0.3902000899036207, 'yhat': array([2.95392759, 4.19159559, 2.31289816, ..., 3.10780366, 4.52958337,        4.60106356]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7238890293060682, 'pearson': 0.6775177204031786, 'spearman': 0.6683992377315126, 'mse': 1.4652224069135638, 'yhat': array([2.43450141, 1.25816321, 1.75221343, ..., 4.24459289, 4.17085756,        3.48197571]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.31, 'acc': 66.51, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 87.25, 'acc': 86.55, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 39.79, 'acc': 40.5, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.42, 'acc': 29.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.15, 'acc': 56.17, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 70.02, 'acc': 69.12, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.36, 'acc': 85.94, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.69, 'acc': 77.35, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.75, 'acc': 77.84, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.0, 'acc': 53.97, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.22, 'acc': 52.83, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 23:00:10,055 : STS12 p=0.4427, STS12 s=0.5127, STS13 p=0.5411, STS13 s=0.5581, STS14 p=0.5692, STS14 s=0.5724, STS15 p=0.5839, STS15 s=0.6079, STS 16 p=0.6005, STS16 s=0.6518, STS B p=0.6775, STS B s=0.6684, STS B m=1.4652, SICK-R p=0.7854, SICK-R s=0.7084, SICK-P m=0.3902
2019-03-12 23:00:10,055 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 23:00:10,055 : 0.4427,0.5127,0.5411,0.5581,0.5692,0.5724,0.5839,0.6079,0.6005,0.6518,0.6775,0.6684,1.4652,0.7854,0.7084,0.3902
2019-03-12 23:00:10,056 : MR=71.69, CR=75.47, SUBJ=92.30, MPQA=85.58, SST-B=79.96, SST-F=41.36, TREC=82.00, SICK-E=74.43, SNLI=66.51, MRPC=71.88, MRPC f=79.03
2019-03-12 23:00:10,056 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 23:00:10,056 : 71.69,75.47,92.30,85.58,79.96,41.36,82.00,74.43,66.51,71.88,79.03
2019-03-12 23:00:10,056 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 23:00:10,056 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 23:00:10,056 : na,na,na,na,na,na,na,na,na,na
2019-03-12 23:00:10,056 : SentLen=86.55, WC=40.50, TreeDepth=29.96, TopConst=56.17, BShift=69.12, Tense=85.94, SubjNum=77.35, ObjNum=77.84, SOMO=53.97, CoordInv=52.83, average=63.02
2019-03-12 23:00:10,056 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 23:00:10,056 : 86.55,40.50,29.96,56.17,69.12,85.94,77.35,77.84,53.97,52.83,63.02
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,056 : layer 7
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,056 : ********************************************************************************
2019-03-12 23:00:10,144 : ***** Transfer task : STS12 *****


2019-03-12 23:00:10,156 : loading BERT model bert-large-uncased
2019-03-12 23:00:10,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:00:10,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:00:10,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxshflse1
2019-03-12 23:00:17,625 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:00:27,146 : MSRpar : pearson = 0.2331, spearman = 0.3195
2019-03-12 23:00:28,780 : MSRvid : pearson = 0.6505, spearman = 0.6642
2019-03-12 23:00:30,184 : SMTeuroparl : pearson = 0.4661, spearman = 0.5803
2019-03-12 23:00:32,866 : surprise.OnWN : pearson = 0.6091, spearman = 0.6326
2019-03-12 23:00:34,285 : surprise.SMTnews : pearson = 0.6526, spearman = 0.5618
2019-03-12 23:00:34,286 : ALL (weighted average) : Pearson = 0.5128,             Spearman = 0.5479
2019-03-12 23:00:34,286 : ALL (average) : Pearson = 0.5223,             Spearman = 0.5517

2019-03-12 23:00:34,286 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 23:00:34,293 : loading BERT model bert-large-uncased
2019-03-12 23:00:34,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:00:34,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:00:34,311 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9au49lb3
2019-03-12 23:00:41,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:00:48,469 : FNWN : pearson = 0.3870, spearman = 0.3632
2019-03-12 23:00:50,353 : headlines : pearson = 0.6493, spearman = 0.6516
2019-03-12 23:00:51,813 : OnWN : pearson = 0.6590, spearman = 0.6727
2019-03-12 23:00:51,813 : ALL (weighted average) : Pearson = 0.6199,             Spearman = 0.6231
2019-03-12 23:00:51,813 : ALL (average) : Pearson = 0.5651,             Spearman = 0.5625

2019-03-12 23:00:51,813 : ***** Transfer task : STS14 *****


2019-03-12 23:00:51,830 : loading BERT model bert-large-uncased
2019-03-12 23:00:51,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:00:51,847 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:00:51,848 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz5kbfr0s
2019-03-12 23:00:59,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:01:06,042 : deft-forum : pearson = 0.3367, spearman = 0.3662
2019-03-12 23:01:07,673 : deft-news : pearson = 0.6634, spearman = 0.6497
2019-03-12 23:01:09,835 : headlines : pearson = 0.5954, spearman = 0.5515
2019-03-12 23:01:11,903 : images : pearson = 0.6631, spearman = 0.6492
2019-03-12 23:01:14,025 : OnWN : pearson = 0.7048, spearman = 0.7372
2019-03-12 23:01:16,872 : tweet-news : pearson = 0.6658, spearman = 0.5971
2019-03-12 23:01:16,872 : ALL (weighted average) : Pearson = 0.6193,             Spearman = 0.6029
2019-03-12 23:01:16,873 : ALL (average) : Pearson = 0.6049,             Spearman = 0.5918

2019-03-12 23:01:16,873 : ***** Transfer task : STS15 *****


2019-03-12 23:01:16,906 : loading BERT model bert-large-uncased
2019-03-12 23:01:16,906 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:01:16,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:01:16,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp16heu5w9
2019-03-12 23:01:24,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:01:31,643 : answers-forums : pearson = 0.5214, spearman = 0.5025
2019-03-12 23:01:33,718 : answers-students : pearson = 0.5913, spearman = 0.6292
2019-03-12 23:01:35,755 : belief : pearson = 0.6319, spearman = 0.6510
2019-03-12 23:01:37,994 : headlines : pearson = 0.6641, spearman = 0.6733
2019-03-12 23:01:40,113 : images : pearson = 0.7230, spearman = 0.7427
2019-03-12 23:01:40,114 : ALL (weighted average) : Pearson = 0.6388,             Spearman = 0.6555
2019-03-12 23:01:40,114 : ALL (average) : Pearson = 0.6263,             Spearman = 0.6397

2019-03-12 23:01:40,114 : ***** Transfer task : STS16 *****


2019-03-12 23:01:40,183 : loading BERT model bert-large-uncased
2019-03-12 23:01:40,183 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:01:40,201 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:01:40,201 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp31b7sz0o
2019-03-12 23:01:47,736 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:01:53,964 : answer-answer : pearson = 0.5470, spearman = 0.5633
2019-03-12 23:01:54,622 : headlines : pearson = 0.6833, spearman = 0.7098
2019-03-12 23:01:55,502 : plagiarism : pearson = 0.7116, spearman = 0.7225
2019-03-12 23:01:56,992 : postediting : pearson = 0.7745, spearman = 0.8044
2019-03-12 23:01:57,597 : question-question : pearson = 0.6032, spearman = 0.6086
2019-03-12 23:01:57,598 : ALL (weighted average) : Pearson = 0.6642,             Spearman = 0.6825
2019-03-12 23:01:57,598 : ALL (average) : Pearson = 0.6639,             Spearman = 0.6817

2019-03-12 23:01:57,598 : ***** Transfer task : MR *****


2019-03-12 23:01:57,616 : loading BERT model bert-large-uncased
2019-03-12 23:01:57,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:01:57,635 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:01:57,636 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9l31cq40
2019-03-12 23:02:05,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:02:10,496 : Generating sentence embeddings
2019-03-12 23:02:41,849 : Generated sentence embeddings
2019-03-12 23:02:41,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:02:51,339 : Best param found at split 1: l2reg = 1e-05                 with score 72.1
2019-03-12 23:03:01,410 : Best param found at split 2: l2reg = 0.001                 with score 72.61
2019-03-12 23:03:08,783 : Best param found at split 3: l2reg = 1e-05                 with score 72.68
2019-03-12 23:03:18,567 : Best param found at split 4: l2reg = 0.001                 with score 73.13
2019-03-12 23:03:28,622 : Best param found at split 5: l2reg = 1e-05                 with score 70.46
2019-03-12 23:03:29,152 : Dev acc : 72.2 Test acc : 70.42

2019-03-12 23:03:29,153 : ***** Transfer task : CR *****


2019-03-12 23:03:29,161 : loading BERT model bert-large-uncased
2019-03-12 23:03:29,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:03:29,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:03:29,180 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpahq8iz2a
2019-03-12 23:03:36,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:03:42,001 : Generating sentence embeddings
2019-03-12 23:03:50,300 : Generated sentence embeddings
2019-03-12 23:03:50,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:03:52,933 : Best param found at split 1: l2reg = 0.0001                 with score 76.45
2019-03-12 23:03:55,754 : Best param found at split 2: l2reg = 0.001                 with score 75.99
2019-03-12 23:03:58,547 : Best param found at split 3: l2reg = 1e-05                 with score 75.96
2019-03-12 23:04:01,788 : Best param found at split 4: l2reg = 1e-05                 with score 76.2
2019-03-12 23:04:05,423 : Best param found at split 5: l2reg = 0.001                 with score 77.16
2019-03-12 23:04:05,611 : Dev acc : 76.35 Test acc : 74.15

2019-03-12 23:04:05,612 : ***** Transfer task : MPQA *****


2019-03-12 23:04:05,618 : loading BERT model bert-large-uncased
2019-03-12 23:04:05,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:04:05,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:04:05,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph2y8wxqm
2019-03-12 23:04:13,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:04:18,579 : Generating sentence embeddings
2019-03-12 23:04:26,122 : Generated sentence embeddings
2019-03-12 23:04:26,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:04:36,540 : Best param found at split 1: l2reg = 0.001                 with score 84.85
2019-03-12 23:04:48,491 : Best param found at split 2: l2reg = 0.0001                 with score 83.96
2019-03-12 23:05:00,719 : Best param found at split 3: l2reg = 0.0001                 with score 85.32
2019-03-12 23:05:11,616 : Best param found at split 4: l2reg = 0.001                 with score 84.41
2019-03-12 23:05:22,307 : Best param found at split 5: l2reg = 0.01                 with score 85.94
2019-03-12 23:05:22,826 : Dev acc : 84.9 Test acc : 84.67

2019-03-12 23:05:22,827 : ***** Transfer task : SUBJ *****


2019-03-12 23:05:22,842 : loading BERT model bert-large-uncased
2019-03-12 23:05:22,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:05:22,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:05:22,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkss1vh_q
2019-03-12 23:05:30,308 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:05:35,706 : Generating sentence embeddings
2019-03-12 23:06:06,475 : Generated sentence embeddings
2019-03-12 23:06:06,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:06:15,167 : Best param found at split 1: l2reg = 1e-05                 with score 93.05
2019-03-12 23:06:24,916 : Best param found at split 2: l2reg = 0.0001                 with score 93.22
2019-03-12 23:06:35,398 : Best param found at split 3: l2reg = 1e-05                 with score 92.9
2019-03-12 23:06:44,610 : Best param found at split 4: l2reg = 0.001                 with score 93.65
2019-03-12 23:06:52,558 : Best param found at split 5: l2reg = 0.001                 with score 93.17
2019-03-12 23:06:52,969 : Dev acc : 93.2 Test acc : 92.37

2019-03-12 23:06:52,970 : ***** Transfer task : SST Binary classification *****


2019-03-12 23:06:53,061 : loading BERT model bert-large-uncased
2019-03-12 23:06:53,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:06:53,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:06:53,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp03d59drg
2019-03-12 23:07:00,557 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:07:05,924 : Computing embedding for train
2019-03-12 23:08:45,311 : Computed train embeddings
2019-03-12 23:08:45,312 : Computing embedding for dev
2019-03-12 23:08:47,481 : Computed dev embeddings
2019-03-12 23:08:47,481 : Computing embedding for test
2019-03-12 23:08:52,028 : Computed test embeddings
2019-03-12 23:08:52,028 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:09:06,437 : [('reg:1e-05', 79.7), ('reg:0.0001', 76.72), ('reg:0.001', 79.01), ('reg:0.01', 76.95)]
2019-03-12 23:09:06,438 : Validation : best param found is reg = 1e-05 with score             79.7
2019-03-12 23:09:06,438 : Evaluating...
2019-03-12 23:09:10,404 : 
Dev acc : 79.7 Test acc : 79.68 for             SST Binary classification

2019-03-12 23:09:10,405 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 23:09:10,457 : loading BERT model bert-large-uncased
2019-03-12 23:09:10,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:09:10,479 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:09:10,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprptg1dvl
2019-03-12 23:09:17,941 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:09:23,341 : Computing embedding for train
2019-03-12 23:09:45,124 : Computed train embeddings
2019-03-12 23:09:45,124 : Computing embedding for dev
2019-03-12 23:09:47,964 : Computed dev embeddings
2019-03-12 23:09:47,965 : Computing embedding for test
2019-03-12 23:09:53,580 : Computed test embeddings
2019-03-12 23:09:53,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:09:55,980 : [('reg:1e-05', 34.33), ('reg:0.0001', 34.88), ('reg:0.001', 35.33), ('reg:0.01', 34.97)]
2019-03-12 23:09:55,980 : Validation : best param found is reg = 0.001 with score             35.33
2019-03-12 23:09:55,980 : Evaluating...
2019-03-12 23:09:56,642 : 
Dev acc : 35.33 Test acc : 38.37 for             SST Fine-Grained classification

2019-03-12 23:09:56,642 : ***** Transfer task : TREC *****


2019-03-12 23:09:56,655 : loading BERT model bert-large-uncased
2019-03-12 23:09:56,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:09:56,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:09:56,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxgmub3wg
2019-03-12 23:10:04,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:16,898 : Computed train embeddings
2019-03-12 23:10:17,485 : Computed test embeddings
2019-03-12 23:10:17,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:10:24,605 : [('reg:1e-05', 66.47), ('reg:0.0001', 65.99), ('reg:0.001', 62.82), ('reg:0.01', 57.54)]
2019-03-12 23:10:24,606 : Cross-validation : best param found is reg = 1e-05             with score 66.47
2019-03-12 23:10:24,606 : Evaluating...
2019-03-12 23:10:24,999 : 
Dev acc : 66.47 Test acc : 74.6             for TREC

2019-03-12 23:10:25,000 : ***** Transfer task : MRPC *****


2019-03-12 23:10:25,022 : loading BERT model bert-large-uncased
2019-03-12 23:10:25,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:10:25,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:10:25,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbhpohesw
2019-03-12 23:10:32,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:37,986 : Computing embedding for train
2019-03-12 23:11:00,114 : Computed train embeddings
2019-03-12 23:11:00,114 : Computing embedding for test
2019-03-12 23:11:09,785 : Computed test embeddings
2019-03-12 23:11:09,806 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:11:14,547 : [('reg:1e-05', 70.9), ('reg:0.0001', 70.34), ('reg:0.001', 70.46), ('reg:0.01', 69.09)]
2019-03-12 23:11:14,547 : Cross-validation : best param found is reg = 1e-05             with score 70.9
2019-03-12 23:11:14,548 : Evaluating...
2019-03-12 23:11:14,774 : Dev acc : 70.9 Test acc 69.68; Test F1 78.78 for MRPC.

2019-03-12 23:11:14,774 : ***** Transfer task : SICK-Entailment*****


2019-03-12 23:11:14,835 : loading BERT model bert-large-uncased
2019-03-12 23:11:14,835 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:11:14,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:11:14,855 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsg1hme4i
2019-03-12 23:11:22,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:11:27,646 : Computing embedding for train
2019-03-12 23:11:38,868 : Computed train embeddings
2019-03-12 23:11:38,869 : Computing embedding for dev
2019-03-12 23:11:40,397 : Computed dev embeddings
2019-03-12 23:11:40,397 : Computing embedding for test
2019-03-12 23:11:52,417 : Computed test embeddings
2019-03-12 23:11:52,455 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:11:54,276 : [('reg:1e-05', 72.4), ('reg:0.0001', 77.8), ('reg:0.001', 76.0), ('reg:0.01', 70.8)]
2019-03-12 23:11:54,276 : Validation : best param found is reg = 0.0001 with score             77.8
2019-03-12 23:11:54,276 : Evaluating...
2019-03-12 23:11:54,823 : 
Dev acc : 77.8 Test acc : 76.21 for                        SICK entailment

2019-03-12 23:11:54,824 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 23:11:54,851 : loading BERT model bert-large-uncased
2019-03-12 23:11:54,851 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:11:54,909 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:11:54,909 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzx85abti
2019-03-12 23:12:02,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:12:07,722 : Computing embedding for train
2019-03-12 23:12:18,950 : Computed train embeddings
2019-03-12 23:12:18,951 : Computing embedding for dev
2019-03-12 23:12:20,482 : Computed dev embeddings
2019-03-12 23:12:20,482 : Computing embedding for test
2019-03-12 23:12:32,531 : Computed test embeddings
2019-03-12 23:12:45,870 : Dev : Pearson 0.79411441664565
2019-03-12 23:12:45,870 : Test : Pearson 0.7944811523502232 Spearman 0.7081585764106392 MSE 0.37721038926131767                        for SICK Relatedness

2019-03-12 23:12:45,871 : 

***** Transfer task : STSBenchmark*****


2019-03-12 23:12:45,911 : loading BERT model bert-large-uncased
2019-03-12 23:12:45,911 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:12:45,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:12:45,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwh4948c9
2019-03-12 23:12:53,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:12:58,977 : Computing embedding for train
2019-03-12 23:13:17,475 : Computed train embeddings
2019-03-12 23:13:17,475 : Computing embedding for dev
2019-03-12 23:13:23,084 : Computed dev embeddings
2019-03-12 23:13:23,084 : Computing embedding for test
2019-03-12 23:13:27,669 : Computed test embeddings
2019-03-12 23:13:47,098 : Dev : Pearson 0.7147718218593627
2019-03-12 23:13:47,098 : Test : Pearson 0.6711305489136026 Spearman 0.6640605333091725 MSE 1.4516629798080725                        for SICK Relatedness

2019-03-12 23:13:47,098 : ***** Transfer task : SNLI Entailment*****


2019-03-12 23:13:52,187 : loading BERT model bert-large-uncased
2019-03-12 23:13:52,187 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:13:52,256 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:13:52,256 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvh_54ho4
2019-03-12 23:13:59,729 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:14:05,540 : PROGRESS (encoding): 0.00%
2019-03-12 23:16:49,775 : PROGRESS (encoding): 14.56%
2019-03-12 23:19:55,894 : PROGRESS (encoding): 29.12%
2019-03-12 23:23:02,651 : PROGRESS (encoding): 43.69%
2019-03-12 23:26:21,813 : PROGRESS (encoding): 58.25%
2019-03-12 23:30:03,573 : PROGRESS (encoding): 72.81%
2019-03-12 23:33:44,107 : PROGRESS (encoding): 87.37%
2019-03-12 23:37:42,855 : PROGRESS (encoding): 0.00%
2019-03-12 23:38:12,924 : PROGRESS (encoding): 0.00%
2019-03-12 23:38:41,810 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:39:11,367 : [('reg:1e-09', 62.37)]
2019-03-12 23:39:11,368 : Validation : best param found is reg = 1e-09 with score             62.37
2019-03-12 23:39:11,368 : Evaluating...
2019-03-12 23:39:43,071 : Dev acc : 62.37 Test acc : 63.12 for SNLI

2019-03-12 23:39:43,071 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 23:39:43,280 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 23:39:44,330 : loading BERT model bert-large-uncased
2019-03-12 23:39:44,330 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:39:44,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:39:44,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp80vgqqii
2019-03-12 23:39:51,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:39:57,078 : Computing embeddings for train/dev/test
2019-03-12 23:43:25,900 : Computed embeddings
2019-03-12 23:43:25,900 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:44:00,609 : [('reg:1e-05', 85.4), ('reg:0.0001', 85.96), ('reg:0.001', 76.63), ('reg:0.01', 64.99)]
2019-03-12 23:44:00,609 : Validation : best param found is reg = 0.0001 with score             85.96
2019-03-12 23:44:00,609 : Evaluating...
2019-03-12 23:44:07,500 : 
Dev acc : 86.0 Test acc : 87.1 for LENGTH classification

2019-03-12 23:44:07,500 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 23:44:07,749 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 23:44:07,794 : loading BERT model bert-large-uncased
2019-03-12 23:44:07,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:44:07,825 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:44:07,825 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm9uqlfnq
2019-03-12 23:44:15,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:44:20,388 : Computing embeddings for train/dev/test
2019-03-12 23:47:32,813 : Computed embeddings
2019-03-12 23:47:32,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:48:09,082 : [('reg:1e-05', 50.33), ('reg:0.0001', 9.11), ('reg:0.001', 1.32), ('reg:0.01', 0.35)]
2019-03-12 23:48:09,082 : Validation : best param found is reg = 1e-05 with score             50.33
2019-03-12 23:48:09,082 : Evaluating...
2019-03-12 23:48:17,405 : 
Dev acc : 50.3 Test acc : 51.0 for WORDCONTENT classification

2019-03-12 23:48:17,406 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 23:48:17,949 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 23:48:18,017 : loading BERT model bert-large-uncased
2019-03-12 23:48:18,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:48:18,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:48:18,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgxgp0xpa
2019-03-12 23:48:25,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:48:30,808 : Computing embeddings for train/dev/test
2019-03-12 23:51:31,596 : Computed embeddings
2019-03-12 23:51:31,596 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:51:59,575 : [('reg:1e-05', 29.22), ('reg:0.0001', 29.02), ('reg:0.001', 28.19), ('reg:0.01', 24.35)]
2019-03-12 23:51:59,575 : Validation : best param found is reg = 1e-05 with score             29.22
2019-03-12 23:51:59,575 : Evaluating...
2019-03-12 23:52:05,370 : 
Dev acc : 29.2 Test acc : 29.9 for DEPTH classification

2019-03-12 23:52:05,371 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 23:52:05,735 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 23:52:05,797 : loading BERT model bert-large-uncased
2019-03-12 23:52:05,797 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:52:05,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:52:05,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdnq62lc2
2019-03-12 23:52:13,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:52:18,797 : Computing embeddings for train/dev/test
2019-03-12 23:55:06,437 : Computed embeddings
2019-03-12 23:55:06,437 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:55:40,025 : [('reg:1e-05', 52.57), ('reg:0.0001', 52.11), ('reg:0.001', 41.16), ('reg:0.01', 24.38)]
2019-03-12 23:55:40,026 : Validation : best param found is reg = 1e-05 with score             52.57
2019-03-12 23:55:40,026 : Evaluating...
2019-03-12 23:55:46,409 : 
Dev acc : 52.6 Test acc : 52.2 for TOPCONSTITUENTS classification

2019-03-12 23:55:46,410 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 23:55:46,786 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 23:55:46,851 : loading BERT model bert-large-uncased
2019-03-12 23:55:46,851 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:55:46,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:55:46,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpplimhu0n
2019-03-12 23:55:54,367 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:55:59,755 : Computing embeddings for train/dev/test
2019-03-12 23:59:01,775 : Computed embeddings
2019-03-12 23:59:01,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:59:30,849 : [('reg:1e-05', 64.41), ('reg:0.0001', 65.11), ('reg:0.001', 68.3), ('reg:0.01', 60.58)]
2019-03-12 23:59:30,849 : Validation : best param found is reg = 0.001 with score             68.3
2019-03-12 23:59:30,849 : Evaluating...
2019-03-12 23:59:36,276 : 
Dev acc : 68.3 Test acc : 67.8 for BIGRAMSHIFT classification

2019-03-12 23:59:36,277 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 23:59:36,672 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 23:59:36,738 : loading BERT model bert-large-uncased
2019-03-12 23:59:36,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:59:36,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:59:36,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyflnisun
2019-03-12 23:59:44,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:59:49,685 : Computing embeddings for train/dev/test
2019-03-13 00:02:48,112 : Computed embeddings
2019-03-13 00:02:48,112 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:03:15,428 : [('reg:1e-05', 87.43), ('reg:0.0001', 87.36), ('reg:0.001', 88.11), ('reg:0.01', 87.31)]
2019-03-13 00:03:15,429 : Validation : best param found is reg = 0.001 with score             88.11
2019-03-13 00:03:15,429 : Evaluating...
2019-03-13 00:03:22,975 : 
Dev acc : 88.1 Test acc : 86.5 for TENSE classification

2019-03-13 00:03:22,976 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 00:03:23,400 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 00:03:23,466 : loading BERT model bert-large-uncased
2019-03-13 00:03:23,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:03:23,592 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:03:23,593 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa_5hu010
2019-03-13 00:03:31,058 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:03:36,425 : Computing embeddings for train/dev/test
2019-03-13 00:06:44,953 : Computed embeddings
2019-03-13 00:06:44,954 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:07:11,793 : [('reg:1e-05', 75.28), ('reg:0.0001', 76.94), ('reg:0.001', 79.48), ('reg:0.01', 77.67)]
2019-03-13 00:07:11,793 : Validation : best param found is reg = 0.001 with score             79.48
2019-03-13 00:07:11,793 : Evaluating...
2019-03-13 00:07:17,242 : 
Dev acc : 79.5 Test acc : 77.3 for SUBJNUMBER classification

2019-03-13 00:07:17,243 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 00:07:17,640 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 00:07:17,706 : loading BERT model bert-large-uncased
2019-03-13 00:07:17,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:07:17,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:07:17,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpndlntum_
2019-03-13 00:07:25,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:07:30,668 : Computing embeddings for train/dev/test
2019-03-13 00:10:35,816 : Computed embeddings
2019-03-13 00:10:35,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:11:08,069 : [('reg:1e-05', 78.2), ('reg:0.0001', 78.17), ('reg:0.001', 72.33), ('reg:0.01', 74.22)]
2019-03-13 00:11:08,069 : Validation : best param found is reg = 1e-05 with score             78.2
2019-03-13 00:11:08,069 : Evaluating...
2019-03-13 00:11:14,924 : 
Dev acc : 78.2 Test acc : 79.9 for OBJNUMBER classification

2019-03-13 00:11:14,926 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 00:11:15,513 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 00:11:15,582 : loading BERT model bert-large-uncased
2019-03-13 00:11:15,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:11:15,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:11:15,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzqrv99ej
2019-03-13 00:11:23,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:11:28,500 : Computing embeddings for train/dev/test
2019-03-13 00:15:03,164 : Computed embeddings
2019-03-13 00:15:03,164 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:15:28,040 : [('reg:1e-05', 57.05), ('reg:0.0001', 56.94), ('reg:0.001', 56.53), ('reg:0.01', 55.25)]
2019-03-13 00:15:28,040 : Validation : best param found is reg = 1e-05 with score             57.05
2019-03-13 00:15:28,040 : Evaluating...
2019-03-13 00:15:34,566 : 
Dev acc : 57.0 Test acc : 57.7 for ODDMANOUT classification

2019-03-13 00:15:34,567 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 00:15:34,945 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 00:15:35,025 : loading BERT model bert-large-uncased
2019-03-13 00:15:35,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:15:35,055 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:15:35,056 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps5vhge9c
2019-03-13 00:15:42,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:15:47,894 : Computing embeddings for train/dev/test
2019-03-13 00:19:20,414 : Computed embeddings
2019-03-13 00:19:20,414 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:19:46,308 : [('reg:1e-05', 50.94), ('reg:0.0001', 50.79), ('reg:0.001', 50.43), ('reg:0.01', 50.01)]
2019-03-13 00:19:46,309 : Validation : best param found is reg = 1e-05 with score             50.94
2019-03-13 00:19:46,309 : Evaluating...
2019-03-13 00:19:52,849 : 
Dev acc : 50.9 Test acc : 50.9 for COORDINATIONINVERSION classification

2019-03-13 00:19:52,851 : total results: {'STS12': {'MSRpar': {'pearson': (0.23307724658009168, 1.03576923677613e-10), 'spearman': SpearmanrResult(correlation=0.31951728478527663, pvalue=2.9122842219894397e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6505405166764049, 1.8766946217501485e-91), 'spearman': SpearmanrResult(correlation=0.6642031909782284, pvalue=1.335435790932866e-96), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46612336355974704, 3.891443975677721e-26), 'spearman': SpearmanrResult(correlation=0.580289212017545, pvalue=1.1525728230980874e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6091298594797641, 2.3140902688916873e-77), 'spearman': SpearmanrResult(correlation=0.6325894802665888, pvalue=4.421275930645219e-85), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6526028524177467, 8.84350322271283e-50), 'spearman': SpearmanrResult(correlation=0.5617647219403857, pvalue=1.452083837549543e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.522294767742751, 'wmean': 0.5128377989191765}, 'spearman': {'mean': 0.551672777997605, 'wmean': 0.5478537128033583}}}, 'STS13': {'FNWN': {'pearson': (0.38695968151091353, 3.7911372504378925e-08), 'spearman': SpearmanrResult(correlation=0.36323702261568847, pvalue=2.7899754886395216e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6493465850566231, 5.136133119168259e-91), 'spearman': SpearmanrResult(correlation=0.6515634112544957, pvalue=7.892495188249667e-92), 'nsamples': 750}, 'OnWN': {'pearson': (0.6589980134876454, 3.636040748706236e-71), 'spearman': SpearmanrResult(correlation=0.6726558318325349, pvalue=3.8480863431356087e-75), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5651014266850607, 'wmean': 0.6198954694430661}, 'spearman': {'mean': 0.5624854219009063, 'wmean': 0.6231228515821926}}}, 'STS14': {'deft-forum': {'pearson': (0.33667541007929386, 2.1780115712305294e-13), 'spearman': SpearmanrResult(correlation=0.36616426304332145, pvalue=1.0041484126273705e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.6634173761347754, 2.0263015838145764e-39), 'spearman': SpearmanrResult(correlation=0.6497137278378791, pvalue=2.3052068574724474e-37), 'nsamples': 300}, 'headlines': {'pearson': (0.5953728003222426, 3.963336438009995e-73), 'spearman': SpearmanrResult(correlation=0.5515317410942058, pvalue=6.51789437488844e-61), 'nsamples': 750}, 'images': {'pearson': (0.6630740462162547, 3.6422591114951168e-96), 'spearman': SpearmanrResult(correlation=0.649216109222398, pvalue=5.731915861667678e-91), 'nsamples': 750}, 'OnWN': {'pearson': (0.7047933527617083, 1.2263180266893491e-113), 'spearman': SpearmanrResult(correlation=0.7372339256784122, pvalue=1.6698376306907964e-129), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6658212908495488, 3.146659329148539e-97), 'spearman': SpearmanrResult(correlation=0.597126407986451, pvalue=1.1742626506616508e-73), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6048590460606372, 'wmean': 0.6192867373302482}, 'spearman': {'mean': 0.5918310291437779, 'wmean': 0.6029384465885222}}}, 'STS15': {'answers-forums': {'pearson': (0.521402878818328, 1.5795826938089738e-27), 'spearman': SpearmanrResult(correlation=0.50250112495361, pvalue=2.182641721836871e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.5912712971725333, 6.623225646879393e-72), 'spearman': SpearmanrResult(correlation=0.6292055164538871, pvalue=6.309419608960485e-84), 'nsamples': 750}, 'belief': {'pearson': (0.6319294182254526, 3.3756572934901135e-43), 'spearman': SpearmanrResult(correlation=0.6510112912145791, pvalue=1.3974948336688087e-46), 'nsamples': 375}, 'headlines': {'pearson': (0.6640940427642859, 1.4717280157636112e-96), 'spearman': SpearmanrResult(correlation=0.673328674221525, pvalue=3.4105934146062417e-100), 'nsamples': 750}, 'images': {'pearson': (0.7230398379235886, 2.777640690661446e-122), 'spearman': SpearmanrResult(correlation=0.7426530379010279, pvalue=2.19077965840839e-132), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6263474949808377, 'wmean': 0.6387678315955745}, 'spearman': {'mean': 0.6397399289489258, 'wmean': 0.6554858591651337}}}, 'STS16': {'answer-answer': {'pearson': (0.5469679049007519, 3.204969956463341e-21), 'spearman': SpearmanrResult(correlation=0.5633291778802002, pvalue=1.138212169146836e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6833247986662075, 1.3369953589101994e-35), 'spearman': SpearmanrResult(correlation=0.70977976446975, pvalue=1.8505808864934443e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7116086280772347, 8.21335842807974e-37), 'spearman': SpearmanrResult(correlation=0.7225114118212362, pvalue=2.0623592858525636e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.7744827685596591, 4.918425229076507e-50), 'spearman': SpearmanrResult(correlation=0.8044428430457321, pvalue=1.1608110020398278e-56), 'nsamples': 244}, 'question-question': {'pearson': (0.6031645687346728, 4.274129702917417e-22), 'spearman': SpearmanrResult(correlation=0.6085851192916829, pvalue=1.4473305442768044e-22), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6639097337877051, 'wmean': 0.6642351581488567}, 'spearman': {'mean': 0.6817296633017202, 'wmean': 0.682526762975163}}}, 'MR': {'devacc': 72.2, 'acc': 70.42, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.35, 'acc': 74.15, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.9, 'acc': 84.67, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.2, 'acc': 92.37, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.7, 'acc': 79.68, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.33, 'acc': 38.37, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.47, 'acc': 74.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.9, 'acc': 69.68, 'f1': 78.78, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.8, 'acc': 76.21, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.79411441664565, 'pearson': 0.7944811523502232, 'spearman': 0.7081585764106392, 'mse': 0.37721038926131767, 'yhat': array([2.69464344, 4.33115058, 1.54652595, ..., 3.22047866, 4.57061042,        4.3795885 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7147718218593627, 'pearson': 0.6711305489136026, 'spearman': 0.6640605333091725, 'mse': 1.4516629798080725, 'yhat': array([2.44489528, 1.13189664, 2.37574351, ..., 4.29250408, 4.217628  ,        3.11804476]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.37, 'acc': 63.12, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 85.96, 'acc': 87.14, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 50.33, 'acc': 51.04, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.22, 'acc': 29.89, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.57, 'acc': 52.17, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 68.3, 'acc': 67.75, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.11, 'acc': 86.53, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.48, 'acc': 77.28, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.2, 'acc': 79.93, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.05, 'acc': 57.66, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.94, 'acc': 50.88, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 00:19:52,851 : STS12 p=0.5128, STS12 s=0.5479, STS13 p=0.6199, STS13 s=0.6231, STS14 p=0.6193, STS14 s=0.6029, STS15 p=0.6388, STS15 s=0.6555, STS 16 p=0.6642, STS16 s=0.6825, STS B p=0.6711, STS B s=0.6641, STS B m=1.4517, SICK-R p=0.7945, SICK-R s=0.7082, SICK-P m=0.3772
2019-03-13 00:19:52,851 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 00:19:52,851 : 0.5128,0.5479,0.6199,0.6231,0.6193,0.6029,0.6388,0.6555,0.6642,0.6825,0.6711,0.6641,1.4517,0.7945,0.7082,0.3772
2019-03-13 00:19:52,851 : MR=70.42, CR=74.15, SUBJ=92.37, MPQA=84.67, SST-B=79.68, SST-F=38.37, TREC=74.60, SICK-E=76.21, SNLI=63.12, MRPC=69.68, MRPC f=78.78
2019-03-13 00:19:52,851 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 00:19:52,851 : 70.42,74.15,92.37,84.67,79.68,38.37,74.60,76.21,63.12,69.68,78.78
2019-03-13 00:19:52,851 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 00:19:52,851 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 00:19:52,851 : na,na,na,na,na,na,na,na,na,na
2019-03-13 00:19:52,851 : SentLen=87.14, WC=51.04, TreeDepth=29.89, TopConst=52.17, BShift=67.75, Tense=86.53, SubjNum=77.28, ObjNum=79.93, SOMO=57.66, CoordInv=50.88, average=64.03
2019-03-13 00:19:52,851 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 00:19:52,851 : 87.14,51.04,29.89,52.17,67.75,86.53,77.28,79.93,57.66,50.88,64.03
2019-03-13 00:19:52,851 : ********************************************************************************
2019-03-13 00:19:52,851 : ********************************************************************************
2019-03-13 00:19:52,851 : ********************************************************************************
2019-03-13 00:19:52,851 : layer 8
2019-03-13 00:19:52,851 : ********************************************************************************
2019-03-13 00:19:52,852 : ********************************************************************************
2019-03-13 00:19:52,852 : ********************************************************************************
2019-03-13 00:19:52,942 : ***** Transfer task : STS12 *****


2019-03-13 00:19:52,955 : loading BERT model bert-large-uncased
2019-03-13 00:19:52,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:19:52,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:19:52,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp08o15m3_
2019-03-13 00:20:00,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:20:09,798 : MSRpar : pearson = 0.2223, spearman = 0.3087
2019-03-13 00:20:11,429 : MSRvid : pearson = 0.6175, spearman = 0.6316
2019-03-13 00:20:12,831 : SMTeuroparl : pearson = 0.4484, spearman = 0.5706
2019-03-13 00:20:15,511 : surprise.OnWN : pearson = 0.5712, spearman = 0.6128
2019-03-13 00:20:16,931 : surprise.SMTnews : pearson = 0.6348, spearman = 0.5320
2019-03-13 00:20:16,931 : ALL (weighted average) : Pearson = 0.4882,             Spearman = 0.5274
2019-03-13 00:20:16,931 : ALL (average) : Pearson = 0.4989,             Spearman = 0.5312

2019-03-13 00:20:16,931 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 00:20:16,940 : loading BERT model bert-large-uncased
2019-03-13 00:20:16,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:20:16,957 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:20:16,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_n1ywxv
2019-03-13 00:20:24,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:20:31,251 : FNWN : pearson = 0.4005, spearman = 0.3726
2019-03-13 00:20:33,131 : headlines : pearson = 0.6702, spearman = 0.6699
2019-03-13 00:20:34,590 : OnWN : pearson = 0.6556, spearman = 0.6729
2019-03-13 00:20:34,591 : ALL (weighted average) : Pearson = 0.6307,             Spearman = 0.6336
2019-03-13 00:20:34,591 : ALL (average) : Pearson = 0.5754,             Spearman = 0.5718

2019-03-13 00:20:34,591 : ***** Transfer task : STS14 *****


2019-03-13 00:20:34,606 : loading BERT model bert-large-uncased
2019-03-13 00:20:34,606 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:20:34,623 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:20:34,623 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdo9smog4
2019-03-13 00:20:42,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:20:48,607 : deft-forum : pearson = 0.3264, spearman = 0.3494
2019-03-13 00:20:50,236 : deft-news : pearson = 0.6785, spearman = 0.6696
2019-03-13 00:20:52,394 : headlines : pearson = 0.6010, spearman = 0.5606
2019-03-13 00:20:54,459 : images : pearson = 0.6617, spearman = 0.6423
2019-03-13 00:20:56,580 : OnWN : pearson = 0.6810, spearman = 0.7142
2019-03-13 00:20:59,427 : tweet-news : pearson = 0.6598, spearman = 0.5920
2019-03-13 00:20:59,427 : ALL (weighted average) : Pearson = 0.6141,             Spearman = 0.5973
2019-03-13 00:20:59,427 : ALL (average) : Pearson = 0.6014,             Spearman = 0.5880

2019-03-13 00:20:59,427 : ***** Transfer task : STS15 *****


2019-03-13 00:20:59,458 : loading BERT model bert-large-uncased
2019-03-13 00:20:59,459 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:20:59,476 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:20:59,476 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprtpxhhgr
2019-03-13 00:21:06,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:21:13,875 : answers-forums : pearson = 0.5587, spearman = 0.5447
2019-03-13 00:21:15,948 : answers-students : pearson = 0.5911, spearman = 0.6236
2019-03-13 00:21:17,989 : belief : pearson = 0.6432, spearman = 0.6608
2019-03-13 00:21:20,228 : headlines : pearson = 0.6781, spearman = 0.6792
2019-03-13 00:21:22,351 : images : pearson = 0.7059, spearman = 0.7245
2019-03-13 00:21:22,352 : ALL (weighted average) : Pearson = 0.6440,             Spearman = 0.6575
2019-03-13 00:21:22,352 : ALL (average) : Pearson = 0.6354,             Spearman = 0.6466

2019-03-13 00:21:22,352 : ***** Transfer task : STS16 *****


2019-03-13 00:21:22,420 : loading BERT model bert-large-uncased
2019-03-13 00:21:22,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:21:22,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:21:22,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyxsus2ng
2019-03-13 00:21:29,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:21:36,142 : answer-answer : pearson = 0.5391, spearman = 0.5631
2019-03-13 00:21:36,800 : headlines : pearson = 0.6812, spearman = 0.7017
2019-03-13 00:21:37,678 : plagiarism : pearson = 0.7128, spearman = 0.7293
2019-03-13 00:21:39,164 : postediting : pearson = 0.7805, spearman = 0.8052
2019-03-13 00:21:39,768 : question-question : pearson = 0.5890, spearman = 0.5857
2019-03-13 00:21:39,769 : ALL (weighted average) : Pearson = 0.6611,             Spearman = 0.6782
2019-03-13 00:21:39,769 : ALL (average) : Pearson = 0.6605,             Spearman = 0.6770

2019-03-13 00:21:39,769 : ***** Transfer task : MR *****


2019-03-13 00:21:39,784 : loading BERT model bert-large-uncased
2019-03-13 00:21:39,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:21:39,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:21:39,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8viwc7p6
2019-03-13 00:21:47,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:21:52,546 : Generating sentence embeddings
2019-03-13 00:22:23,863 : Generated sentence embeddings
2019-03-13 00:22:23,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:22:33,934 : Best param found at split 1: l2reg = 1e-05                 with score 73.37
2019-03-13 00:22:44,801 : Best param found at split 2: l2reg = 0.001                 with score 72.27
2019-03-13 00:22:55,924 : Best param found at split 3: l2reg = 1e-05                 with score 72.46
2019-03-13 00:23:07,119 : Best param found at split 4: l2reg = 1e-05                 with score 72.59
2019-03-13 00:23:16,966 : Best param found at split 5: l2reg = 1e-05                 with score 71.17
2019-03-13 00:23:17,474 : Dev acc : 72.37 Test acc : 71.24

2019-03-13 00:23:17,476 : ***** Transfer task : CR *****


2019-03-13 00:23:17,483 : loading BERT model bert-large-uncased
2019-03-13 00:23:17,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:23:17,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:23:17,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpycx_nszx
2019-03-13 00:23:24,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:23:30,388 : Generating sentence embeddings
2019-03-13 00:23:38,670 : Generated sentence embeddings
2019-03-13 00:23:38,670 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:23:42,296 : Best param found at split 1: l2reg = 0.0001                 with score 76.35
2019-03-13 00:23:46,498 : Best param found at split 2: l2reg = 1e-05                 with score 77.08
2019-03-13 00:23:49,945 : Best param found at split 3: l2reg = 1e-05                 with score 76.56
2019-03-13 00:23:53,960 : Best param found at split 4: l2reg = 0.01                 with score 76.13
2019-03-13 00:23:57,538 : Best param found at split 5: l2reg = 0.001                 with score 76.33
2019-03-13 00:23:57,726 : Dev acc : 76.49 Test acc : 74.73

2019-03-13 00:23:57,727 : ***** Transfer task : MPQA *****


2019-03-13 00:23:57,733 : loading BERT model bert-large-uncased
2019-03-13 00:23:57,733 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:23:57,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:23:57,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeig4d3c3
2019-03-13 00:24:05,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:24:10,534 : Generating sentence embeddings
2019-03-13 00:24:18,062 : Generated sentence embeddings
2019-03-13 00:24:18,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:24:28,304 : Best param found at split 1: l2reg = 0.0001                 with score 81.93
2019-03-13 00:24:39,327 : Best param found at split 2: l2reg = 1e-05                 with score 85.01
2019-03-13 00:24:49,488 : Best param found at split 3: l2reg = 1e-05                 with score 83.88
2019-03-13 00:24:57,739 : Best param found at split 4: l2reg = 1e-05                 with score 84.17
2019-03-13 00:25:09,317 : Best param found at split 5: l2reg = 0.0001                 with score 85.63
2019-03-13 00:25:09,862 : Dev acc : 84.12 Test acc : 84.9

2019-03-13 00:25:09,863 : ***** Transfer task : SUBJ *****


2019-03-13 00:25:09,880 : loading BERT model bert-large-uncased
2019-03-13 00:25:09,880 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:25:09,898 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:25:09,899 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyjuvel6c
2019-03-13 00:25:17,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:25:22,851 : Generating sentence embeddings
2019-03-13 00:25:53,727 : Generated sentence embeddings
2019-03-13 00:25:53,728 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:26:03,712 : Best param found at split 1: l2reg = 0.001                 with score 93.44
2019-03-13 00:26:13,576 : Best param found at split 2: l2reg = 0.001                 with score 93.45
2019-03-13 00:26:24,564 : Best param found at split 3: l2reg = 0.001                 with score 92.91
2019-03-13 00:26:35,456 : Best param found at split 4: l2reg = 0.001                 with score 93.58
2019-03-13 00:26:45,409 : Best param found at split 5: l2reg = 1e-05                 with score 93.14
2019-03-13 00:26:45,997 : Dev acc : 93.3 Test acc : 92.84

2019-03-13 00:26:45,998 : ***** Transfer task : SST Binary classification *****


2019-03-13 00:26:46,091 : loading BERT model bert-large-uncased
2019-03-13 00:26:46,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:26:46,167 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:26:46,167 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpblclv69o
2019-03-13 00:26:53,586 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:26:58,806 : Computing embedding for train
2019-03-13 00:28:38,045 : Computed train embeddings
2019-03-13 00:28:38,045 : Computing embedding for dev
2019-03-13 00:28:40,205 : Computed dev embeddings
2019-03-13 00:28:40,205 : Computing embedding for test
2019-03-13 00:28:44,745 : Computed test embeddings
2019-03-13 00:28:44,745 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:29:08,698 : [('reg:1e-05', 77.06), ('reg:0.0001', 77.18), ('reg:0.001', 77.29), ('reg:0.01', 76.83)]
2019-03-13 00:29:08,698 : Validation : best param found is reg = 0.001 with score             77.29
2019-03-13 00:29:08,698 : Evaluating...
2019-03-13 00:29:14,266 : 
Dev acc : 77.29 Test acc : 77.98 for             SST Binary classification

2019-03-13 00:29:14,266 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 00:29:14,321 : loading BERT model bert-large-uncased
2019-03-13 00:29:14,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:29:14,341 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:29:14,341 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp64y_3et1
2019-03-13 00:29:21,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:29:27,157 : Computing embedding for train
2019-03-13 00:29:48,880 : Computed train embeddings
2019-03-13 00:29:48,881 : Computing embedding for dev
2019-03-13 00:29:51,718 : Computed dev embeddings
2019-03-13 00:29:51,718 : Computing embedding for test
2019-03-13 00:29:57,301 : Computed test embeddings
2019-03-13 00:29:57,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:29:58,931 : [('reg:1e-05', 36.6), ('reg:0.0001', 35.88), ('reg:0.001', 37.87), ('reg:0.01', 37.06)]
2019-03-13 00:29:58,931 : Validation : best param found is reg = 0.001 with score             37.87
2019-03-13 00:29:58,931 : Evaluating...
2019-03-13 00:29:59,325 : 
Dev acc : 37.87 Test acc : 36.02 for             SST Fine-Grained classification

2019-03-13 00:29:59,325 : ***** Transfer task : TREC *****


2019-03-13 00:29:59,338 : loading BERT model bert-large-uncased
2019-03-13 00:29:59,338 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:29:59,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:29:59,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa9dj5557
2019-03-13 00:30:06,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:30:19,608 : Computed train embeddings
2019-03-13 00:30:20,192 : Computed test embeddings
2019-03-13 00:30:20,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:30:25,151 : [('reg:1e-05', 66.49), ('reg:0.0001', 66.49), ('reg:0.001', 63.9), ('reg:0.01', 59.17)]
2019-03-13 00:30:25,152 : Cross-validation : best param found is reg = 1e-05             with score 66.49
2019-03-13 00:30:25,152 : Evaluating...
2019-03-13 00:30:25,441 : 
Dev acc : 66.49 Test acc : 80.6             for TREC

2019-03-13 00:30:25,442 : ***** Transfer task : MRPC *****


2019-03-13 00:30:25,462 : loading BERT model bert-large-uncased
2019-03-13 00:30:25,462 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:30:25,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:30:25,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg6x_bh33
2019-03-13 00:30:33,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:30:38,207 : Computing embedding for train
2019-03-13 00:31:00,329 : Computed train embeddings
2019-03-13 00:31:00,329 : Computing embedding for test
2019-03-13 00:31:09,995 : Computed test embeddings
2019-03-13 00:31:10,016 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:31:14,945 : [('reg:1e-05', 70.19), ('reg:0.0001', 70.04), ('reg:0.001', 69.46), ('reg:0.01', 69.48)]
2019-03-13 00:31:14,945 : Cross-validation : best param found is reg = 1e-05             with score 70.19
2019-03-13 00:31:14,945 : Evaluating...
2019-03-13 00:31:15,306 : Dev acc : 70.19 Test acc 70.96; Test F1 79.66 for MRPC.

2019-03-13 00:31:15,306 : ***** Transfer task : SICK-Entailment*****


2019-03-13 00:31:15,371 : loading BERT model bert-large-uncased
2019-03-13 00:31:15,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:31:15,391 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:31:15,391 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzh5p6a_n
2019-03-13 00:31:22,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:31:28,179 : Computing embedding for train
2019-03-13 00:31:39,385 : Computed train embeddings
2019-03-13 00:31:39,385 : Computing embedding for dev
2019-03-13 00:31:40,914 : Computed dev embeddings
2019-03-13 00:31:40,914 : Computing embedding for test
2019-03-13 00:31:52,912 : Computed test embeddings
2019-03-13 00:31:52,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:31:54,622 : [('reg:1e-05', 68.2), ('reg:0.0001', 74.4), ('reg:0.001', 75.0), ('reg:0.01', 75.4)]
2019-03-13 00:31:54,622 : Validation : best param found is reg = 0.01 with score             75.4
2019-03-13 00:31:54,622 : Evaluating...
2019-03-13 00:31:55,168 : 
Dev acc : 75.4 Test acc : 72.72 for                        SICK entailment

2019-03-13 00:31:55,169 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 00:31:55,196 : loading BERT model bert-large-uncased
2019-03-13 00:31:55,196 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:31:55,253 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:31:55,253 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzzj89e5j
2019-03-13 00:32:02,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:32:08,187 : Computing embedding for train
2019-03-13 00:32:19,417 : Computed train embeddings
2019-03-13 00:32:19,417 : Computing embedding for dev
2019-03-13 00:32:20,948 : Computed dev embeddings
2019-03-13 00:32:20,948 : Computing embedding for test
2019-03-13 00:32:32,965 : Computed test embeddings
2019-03-13 00:32:42,406 : Dev : Pearson 0.7872540284386392
2019-03-13 00:32:42,406 : Test : Pearson 0.7819054823827486 Spearman 0.7037242377585656 MSE 0.39836852952706                        for SICK Relatedness

2019-03-13 00:32:42,407 : 

***** Transfer task : STSBenchmark*****


2019-03-13 00:32:42,445 : loading BERT model bert-large-uncased
2019-03-13 00:32:42,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:32:42,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:32:42,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6fg4rruk
2019-03-13 00:32:49,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:32:55,374 : Computing embedding for train
2019-03-13 00:33:13,885 : Computed train embeddings
2019-03-13 00:33:13,885 : Computing embedding for dev
2019-03-13 00:33:19,485 : Computed dev embeddings
2019-03-13 00:33:19,485 : Computing embedding for test
2019-03-13 00:33:24,047 : Computed test embeddings
2019-03-13 00:33:43,391 : Dev : Pearson 0.7010282786974694
2019-03-13 00:33:43,392 : Test : Pearson 0.6624157799696242 Spearman 0.6555902509565382 MSE 1.4661832445887648                        for SICK Relatedness

2019-03-13 00:33:43,392 : ***** Transfer task : SNLI Entailment*****


2019-03-13 00:33:48,608 : loading BERT model bert-large-uncased
2019-03-13 00:33:48,608 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:33:48,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:33:48,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcaaf9f41
2019-03-13 00:33:56,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:34:01,996 : PROGRESS (encoding): 0.00%
2019-03-13 00:36:46,143 : PROGRESS (encoding): 14.56%
2019-03-13 00:39:52,283 : PROGRESS (encoding): 29.12%
2019-03-13 00:42:59,351 : PROGRESS (encoding): 43.69%
2019-03-13 00:46:18,812 : PROGRESS (encoding): 58.25%
2019-03-13 00:50:00,989 : PROGRESS (encoding): 72.81%
2019-03-13 00:53:41,792 : PROGRESS (encoding): 87.37%
2019-03-13 00:57:40,932 : PROGRESS (encoding): 0.00%
2019-03-13 00:58:11,001 : PROGRESS (encoding): 0.00%
2019-03-13 00:58:39,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:59:17,465 : [('reg:1e-09', 65.41)]
2019-03-13 00:59:17,465 : Validation : best param found is reg = 1e-09 with score             65.41
2019-03-13 00:59:17,465 : Evaluating...
2019-03-13 00:59:54,834 : Dev acc : 65.41 Test acc : 65.73 for SNLI

2019-03-13 00:59:54,834 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 00:59:55,037 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 00:59:56,120 : loading BERT model bert-large-uncased
2019-03-13 00:59:56,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:59:56,147 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:59:56,147 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppewbn52l
2019-03-13 01:00:03,614 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:00:09,055 : Computing embeddings for train/dev/test
2019-03-13 01:03:37,812 : Computed embeddings
2019-03-13 01:03:37,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:04:08,888 : [('reg:1e-05', 82.27), ('reg:0.0001', 81.44), ('reg:0.001', 76.04), ('reg:0.01', 62.53)]
2019-03-13 01:04:08,888 : Validation : best param found is reg = 1e-05 with score             82.27
2019-03-13 01:04:08,888 : Evaluating...
2019-03-13 01:04:18,636 : 
Dev acc : 82.3 Test acc : 82.0 for LENGTH classification

2019-03-13 01:04:18,637 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 01:04:19,011 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 01:04:19,055 : loading BERT model bert-large-uncased
2019-03-13 01:04:19,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:04:19,084 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:04:19,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoqfo3jua
2019-03-13 01:04:26,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:04:31,894 : Computing embeddings for train/dev/test
2019-03-13 01:07:44,322 : Computed embeddings
2019-03-13 01:07:44,322 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:08:17,555 : [('reg:1e-05', 54.5), ('reg:0.0001', 14.25), ('reg:0.001', 1.43), ('reg:0.01', 0.48)]
2019-03-13 01:08:17,555 : Validation : best param found is reg = 1e-05 with score             54.5
2019-03-13 01:08:17,555 : Evaluating...
2019-03-13 01:08:26,332 : 
Dev acc : 54.5 Test acc : 53.7 for WORDCONTENT classification

2019-03-13 01:08:26,334 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 01:08:26,718 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 01:08:26,786 : loading BERT model bert-large-uncased
2019-03-13 01:08:26,786 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:08:26,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:08:26,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3idencjk
2019-03-13 01:08:34,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:08:39,552 : Computing embeddings for train/dev/test
2019-03-13 01:11:40,219 : Computed embeddings
2019-03-13 01:11:40,220 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:12:03,658 : [('reg:1e-05', 27.61), ('reg:0.0001', 27.68), ('reg:0.001', 26.24), ('reg:0.01', 23.19)]
2019-03-13 01:12:03,658 : Validation : best param found is reg = 0.0001 with score             27.68
2019-03-13 01:12:03,658 : Evaluating...
2019-03-13 01:12:10,116 : 
Dev acc : 27.7 Test acc : 28.0 for DEPTH classification

2019-03-13 01:12:10,117 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 01:12:10,497 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 01:12:10,560 : loading BERT model bert-large-uncased
2019-03-13 01:12:10,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:12:10,669 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:12:10,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7xu4o19f
2019-03-13 01:12:18,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:12:23,529 : Computing embeddings for train/dev/test
2019-03-13 01:15:11,068 : Computed embeddings
2019-03-13 01:15:11,069 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:15:40,961 : [('reg:1e-05', 48.04), ('reg:0.0001', 49.33), ('reg:0.001', 40.47), ('reg:0.01', 23.57)]
2019-03-13 01:15:40,962 : Validation : best param found is reg = 0.0001 with score             49.33
2019-03-13 01:15:40,962 : Evaluating...
2019-03-13 01:15:48,124 : 
Dev acc : 49.3 Test acc : 49.7 for TOPCONSTITUENTS classification

2019-03-13 01:15:48,125 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 01:15:48,471 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 01:15:48,536 : loading BERT model bert-large-uncased
2019-03-13 01:15:48,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:15:48,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:15:48,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkex4el1u
2019-03-13 01:15:56,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:16:01,440 : Computing embeddings for train/dev/test
2019-03-13 01:19:03,266 : Computed embeddings
2019-03-13 01:19:03,267 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:19:23,588 : [('reg:1e-05', 70.48), ('reg:0.0001', 70.51), ('reg:0.001', 65.99), ('reg:0.01', 56.99)]
2019-03-13 01:19:23,588 : Validation : best param found is reg = 0.0001 with score             70.51
2019-03-13 01:19:23,588 : Evaluating...
2019-03-13 01:19:29,479 : 
Dev acc : 70.5 Test acc : 69.3 for BIGRAMSHIFT classification

2019-03-13 01:19:29,480 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 01:19:30,039 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 01:19:30,105 : loading BERT model bert-large-uncased
2019-03-13 01:19:30,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:19:30,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:19:30,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1_qtn5n5
2019-03-13 01:19:37,630 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:19:42,879 : Computing embeddings for train/dev/test
2019-03-13 01:22:40,924 : Computed embeddings
2019-03-13 01:22:40,924 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:23:08,813 : [('reg:1e-05', 88.05), ('reg:0.0001', 87.99), ('reg:0.001', 87.81), ('reg:0.01', 87.54)]
2019-03-13 01:23:08,813 : Validation : best param found is reg = 1e-05 with score             88.05
2019-03-13 01:23:08,813 : Evaluating...
2019-03-13 01:23:17,138 : 
Dev acc : 88.0 Test acc : 86.6 for TENSE classification

2019-03-13 01:23:17,140 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 01:23:17,573 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 01:23:17,638 : loading BERT model bert-large-uncased
2019-03-13 01:23:17,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:23:17,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:23:17,666 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpegppq6hg
2019-03-13 01:23:25,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:23:30,514 : Computing embeddings for train/dev/test
2019-03-13 01:26:39,156 : Computed embeddings
2019-03-13 01:26:39,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:27:05,633 : [('reg:1e-05', 79.94), ('reg:0.0001', 79.8), ('reg:0.001', 77.31), ('reg:0.01', 77.0)]
2019-03-13 01:27:05,633 : Validation : best param found is reg = 1e-05 with score             79.94
2019-03-13 01:27:05,633 : Evaluating...
2019-03-13 01:27:12,312 : 
Dev acc : 79.9 Test acc : 78.7 for SUBJNUMBER classification

2019-03-13 01:27:12,313 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 01:27:12,721 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 01:27:12,787 : loading BERT model bert-large-uncased
2019-03-13 01:27:12,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:27:12,904 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:27:12,904 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc7n1gsa2
2019-03-13 01:27:20,363 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:27:25,708 : Computing embeddings for train/dev/test
2019-03-13 01:30:30,812 : Computed embeddings
2019-03-13 01:30:30,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:30:55,398 : [('reg:1e-05', 76.73), ('reg:0.0001', 76.69), ('reg:0.001', 76.73), ('reg:0.01', 72.8)]
2019-03-13 01:30:55,399 : Validation : best param found is reg = 1e-05 with score             76.73
2019-03-13 01:30:55,399 : Evaluating...
2019-03-13 01:31:01,655 : 
Dev acc : 76.7 Test acc : 77.9 for OBJNUMBER classification

2019-03-13 01:31:01,656 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 01:31:02,252 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 01:31:02,321 : loading BERT model bert-large-uncased
2019-03-13 01:31:02,322 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:31:02,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:31:02,350 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaiu3slrn
2019-03-13 01:31:09,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:31:15,274 : Computing embeddings for train/dev/test
2019-03-13 01:34:49,843 : Computed embeddings
2019-03-13 01:34:49,843 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:35:10,459 : [('reg:1e-05', 58.12), ('reg:0.0001', 58.14), ('reg:0.001', 57.82), ('reg:0.01', 57.21)]
2019-03-13 01:35:10,459 : Validation : best param found is reg = 0.0001 with score             58.14
2019-03-13 01:35:10,459 : Evaluating...
2019-03-13 01:35:15,963 : 
Dev acc : 58.1 Test acc : 58.8 for ODDMANOUT classification

2019-03-13 01:35:15,964 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 01:35:16,355 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 01:35:16,432 : loading BERT model bert-large-uncased
2019-03-13 01:35:16,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:35:16,555 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:35:16,555 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpardtzzb3
2019-03-13 01:35:24,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:35:29,386 : Computing embeddings for train/dev/test
2019-03-13 01:39:01,992 : Computed embeddings
2019-03-13 01:39:01,992 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:39:35,759 : [('reg:1e-05', 53.47), ('reg:0.0001', 53.33), ('reg:0.001', 52.82), ('reg:0.01', 50.58)]
2019-03-13 01:39:35,759 : Validation : best param found is reg = 1e-05 with score             53.47
2019-03-13 01:39:35,759 : Evaluating...
2019-03-13 01:39:43,830 : 
Dev acc : 53.5 Test acc : 53.2 for COORDINATIONINVERSION classification

2019-03-13 01:39:43,832 : total results: {'STS12': {'MSRpar': {'pearson': (0.22231269664773534, 7.492729430093294e-10), 'spearman': SpearmanrResult(correlation=0.3087428968485333, pvalue=4.9963186781357374e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6175019634153414, 4.836789869616642e-80), 'spearman': SpearmanrResult(correlation=0.6315927766273302, pvalue=9.706560533055973e-85), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4484171333790967, 4.351517667513544e-24), 'spearman': SpearmanrResult(correlation=0.570616659244857, pvalue=5.255673714142689e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5712220722706187, 3.5861527817586747e-66), 'spearman': SpearmanrResult(correlation=0.6128142123040854, pvalue=1.566606389123984e-78), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.634831660188954, 2.1246975399617765e-46), 'spearman': SpearmanrResult(correlation=0.5320226273649328, pvalue=1.5431148471601011e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49885710518034926, 'wmean': 0.4882235668876029}, 'spearman': {'mean': 0.5311578344779477, 'wmean': 0.527365665780875}}}, 'STS13': {'FNWN': {'pearson': (0.4004647672032313, 1.1316150410877546e-08), 'spearman': SpearmanrResult(correlation=0.37260687497779366, pvalue=1.2926272243642008e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6701566333720704, 6.2553903011162445e-99), 'spearman': SpearmanrResult(correlation=0.6699495031643616, pvalue=7.554552841386814e-99), 'nsamples': 750}, 'OnWN': {'pearson': (0.6556340761278554, 3.221311680878406e-70), 'spearman': SpearmanrResult(correlation=0.672938311303478, pvalue=3.167874293825013e-75), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5754184922343858, 'wmean': 0.6307440218254603}, 'spearman': {'mean': 0.5718315631485443, 'wmean': 0.6336021462568836}}}, 'STS14': {'deft-forum': {'pearson': (0.3263854331846597, 1.2495975580720764e-12), 'spearman': SpearmanrResult(correlation=0.349376444403808, pvalue=2.3002306234786814e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.6784850285351003, 8.259021860776897e-42), 'spearman': SpearmanrResult(correlation=0.6696195438859476, pvalue=2.187886009819048e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.6010088337051966, 7.735108267453303e-75), 'spearman': SpearmanrResult(correlation=0.5605555351720286, pvalue=2.802190224306485e-63), 'nsamples': 750}, 'images': {'pearson': (0.6616573934936925, 1.274641033834253e-95), 'spearman': SpearmanrResult(correlation=0.6423416889453331, pvalue=1.7249045995937206e-88), 'nsamples': 750}, 'OnWN': {'pearson': (0.6810079331836396, 2.5573630310052544e-103), 'spearman': SpearmanrResult(correlation=0.7142494828166521, pvalue=4.934844307378689e-118), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6598451401346223, 6.26620249566546e-95), 'spearman': SpearmanrResult(correlation=0.5919621662375835, pvalue=4.1332430041636436e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6013982937061518, 'wmean': 0.6141489143683972}, 'spearman': {'mean': 0.5880174769102254, 'wmean': 0.5973165114736523}}}, 'STS15': {'answers-forums': {'pearson': (0.5586865198536336, 3.6389349913189475e-32), 'spearman': SpearmanrResult(correlation=0.5447158110176771, pvalue=2.3274630334033033e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.5910707303353286, 7.593227434404466e-72), 'spearman': SpearmanrResult(correlation=0.6235526474930487, pvalue=4.975979783670226e-82), 'nsamples': 750}, 'belief': {'pearson': (0.6431832122197797, 3.650187391470635e-45), 'spearman': SpearmanrResult(correlation=0.6607718498439289, pvalue=2.0835693368372124e-48), 'nsamples': 375}, 'headlines': {'pearson': (0.6780927469709291, 4.030431330045688e-102), 'spearman': SpearmanrResult(correlation=0.6792267976729054, pvalue=1.384002565890502e-102), 'nsamples': 750}, 'images': {'pearson': (0.7059366791858681, 3.6855751728936075e-114), 'spearman': SpearmanrResult(correlation=0.7245147029282321, pvalue=5.1821086560270015e-123), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6353939777131078, 'wmean': 0.644008755632208}, 'spearman': {'mean': 0.6465563617911585, 'wmean': 0.6575094946312473}}}, 'STS16': {'answer-answer': {'pearson': (0.5391250092735183, 1.4894580211019576e-20), 'spearman': SpearmanrResult(correlation=0.5631191824884313, pvalue=1.189431521864729e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6812065608098891, 2.6151227885970853e-35), 'spearman': SpearmanrResult(correlation=0.7017194966595143, pvalue=3.0764127052465276e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7128109068009638, 5.51736873171281e-37), 'spearman': SpearmanrResult(correlation=0.7293342968525868, pvalue=1.877208759549352e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7804693358144904, 2.829342787673514e-51), 'spearman': SpearmanrResult(correlation=0.8052237034032801, pvalue=7.530609200453303e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.5890338800340081, 6.5413492834722076e-21), 'spearman': SpearmanrResult(correlation=0.5856783622706231, pvalue=1.2262630826120183e-20), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6605291385465739, 'wmean': 0.6610854076114681}, 'spearman': {'mean': 0.677015008334887, 'wmean': 0.6782366582136055}}}, 'MR': {'devacc': 72.37, 'acc': 71.24, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.49, 'acc': 74.73, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.12, 'acc': 84.9, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.3, 'acc': 92.84, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.29, 'acc': 77.98, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.87, 'acc': 36.02, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.49, 'acc': 80.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.19, 'acc': 70.96, 'f1': 79.66, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.4, 'acc': 72.72, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7872540284386392, 'pearson': 0.7819054823827486, 'spearman': 0.7037242377585656, 'mse': 0.39836852952706, 'yhat': array([3.04495834, 4.50383662, 2.17633288, ..., 3.17084441, 4.44355693,        4.54840881]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7010282786974694, 'pearson': 0.6624157799696242, 'spearman': 0.6555902509565382, 'mse': 1.4661832445887648, 'yhat': array([2.62017341, 2.17698736, 2.85037847, ..., 4.08582411, 4.01226313,        3.23500481]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.41, 'acc': 65.73, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 82.27, 'acc': 81.99, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 54.5, 'acc': 53.72, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.68, 'acc': 28.03, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.33, 'acc': 49.71, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 70.51, 'acc': 69.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.05, 'acc': 86.64, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.94, 'acc': 78.74, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.73, 'acc': 77.87, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.14, 'acc': 58.81, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.47, 'acc': 53.17, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 01:39:43,832 : STS12 p=0.4882, STS12 s=0.5274, STS13 p=0.6307, STS13 s=0.6336, STS14 p=0.6141, STS14 s=0.5973, STS15 p=0.6440, STS15 s=0.6575, STS 16 p=0.6611, STS16 s=0.6782, STS B p=0.6624, STS B s=0.6556, STS B m=1.4662, SICK-R p=0.7819, SICK-R s=0.7037, SICK-P m=0.3984
2019-03-13 01:39:43,832 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 01:39:43,832 : 0.4882,0.5274,0.6307,0.6336,0.6141,0.5973,0.6440,0.6575,0.6611,0.6782,0.6624,0.6556,1.4662,0.7819,0.7037,0.3984
2019-03-13 01:39:43,832 : MR=71.24, CR=74.73, SUBJ=92.84, MPQA=84.90, SST-B=77.98, SST-F=36.02, TREC=80.60, SICK-E=72.72, SNLI=65.73, MRPC=70.96, MRPC f=79.66
2019-03-13 01:39:43,832 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 01:39:43,832 : 71.24,74.73,92.84,84.90,77.98,36.02,80.60,72.72,65.73,70.96,79.66
2019-03-13 01:39:43,832 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 01:39:43,832 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 01:39:43,832 : na,na,na,na,na,na,na,na,na,na
2019-03-13 01:39:43,832 : SentLen=81.99, WC=53.72, TreeDepth=28.03, TopConst=49.71, BShift=69.34, Tense=86.64, SubjNum=78.74, ObjNum=77.87, SOMO=58.81, CoordInv=53.17, average=63.80
2019-03-13 01:39:43,832 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 01:39:43,832 : 81.99,53.72,28.03,49.71,69.34,86.64,78.74,77.87,58.81,53.17,63.80
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,832 : layer 9
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,832 : ********************************************************************************
2019-03-13 01:39:43,921 : ***** Transfer task : STS12 *****


2019-03-13 01:39:43,933 : loading BERT model bert-large-uncased
2019-03-13 01:39:43,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:39:43,951 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:39:43,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg9rh34r9
2019-03-13 01:39:51,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:40:00,716 : MSRpar : pearson = 0.2439, spearman = 0.3324
2019-03-13 01:40:02,350 : MSRvid : pearson = 0.5564, spearman = 0.5708
2019-03-13 01:40:03,753 : SMTeuroparl : pearson = 0.4736, spearman = 0.5759
2019-03-13 01:40:06,432 : surprise.OnWN : pearson = 0.5630, spearman = 0.6026
2019-03-13 01:40:07,849 : surprise.SMTnews : pearson = 0.6410, spearman = 0.5599
2019-03-13 01:40:07,849 : ALL (weighted average) : Pearson = 0.4812,             Spearman = 0.5203
2019-03-13 01:40:07,849 : ALL (average) : Pearson = 0.4956,             Spearman = 0.5283

2019-03-13 01:40:07,849 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 01:40:07,857 : loading BERT model bert-large-uncased
2019-03-13 01:40:07,858 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:40:07,876 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:40:07,876 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps9pnrhln
2019-03-13 01:40:15,347 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:40:22,095 : FNWN : pearson = 0.3912, spearman = 0.3611
2019-03-13 01:40:23,975 : headlines : pearson = 0.6735, spearman = 0.6706
2019-03-13 01:40:25,433 : OnWN : pearson = 0.6415, spearman = 0.6562
2019-03-13 01:40:25,433 : ALL (weighted average) : Pearson = 0.6260,             Spearman = 0.6262
2019-03-13 01:40:25,433 : ALL (average) : Pearson = 0.5687,             Spearman = 0.5626

2019-03-13 01:40:25,433 : ***** Transfer task : STS14 *****


2019-03-13 01:40:25,450 : loading BERT model bert-large-uncased
2019-03-13 01:40:25,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:40:25,468 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:40:25,468 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn8n5aiph
2019-03-13 01:40:32,912 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:40:39,582 : deft-forum : pearson = 0.2934, spearman = 0.3174
2019-03-13 01:40:41,209 : deft-news : pearson = 0.6400, spearman = 0.6570
2019-03-13 01:40:43,364 : headlines : pearson = 0.6053, spearman = 0.5697
2019-03-13 01:40:45,426 : images : pearson = 0.6301, spearman = 0.6194
2019-03-13 01:40:47,541 : OnWN : pearson = 0.6598, spearman = 0.6984
2019-03-13 01:40:50,378 : tweet-news : pearson = 0.6214, spearman = 0.5693
2019-03-13 01:40:50,379 : ALL (weighted average) : Pearson = 0.5897,             Spearman = 0.5820
2019-03-13 01:40:50,379 : ALL (average) : Pearson = 0.5750,             Spearman = 0.5719

2019-03-13 01:40:50,379 : ***** Transfer task : STS15 *****


2019-03-13 01:40:50,412 : loading BERT model bert-large-uncased
2019-03-13 01:40:50,412 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:40:50,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:40:50,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpupw3ne9d
2019-03-13 01:40:57,883 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:41:05,177 : answers-forums : pearson = 0.5526, spearman = 0.5430
2019-03-13 01:41:07,247 : answers-students : pearson = 0.5971, spearman = 0.6261
2019-03-13 01:41:09,282 : belief : pearson = 0.6293, spearman = 0.6629
2019-03-13 01:41:11,517 : headlines : pearson = 0.6674, spearman = 0.6733
2019-03-13 01:41:13,635 : images : pearson = 0.6740, spearman = 0.7030
2019-03-13 01:41:13,635 : ALL (weighted average) : Pearson = 0.6324,             Spearman = 0.6513
2019-03-13 01:41:13,635 : ALL (average) : Pearson = 0.6241,             Spearman = 0.6417

2019-03-13 01:41:13,636 : ***** Transfer task : STS16 *****


2019-03-13 01:41:13,704 : loading BERT model bert-large-uncased
2019-03-13 01:41:13,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:41:13,721 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:41:13,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpockfm9cj
2019-03-13 01:41:21,198 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:41:27,486 : answer-answer : pearson = 0.5453, spearman = 0.5626
2019-03-13 01:41:28,142 : headlines : pearson = 0.6754, spearman = 0.7011
2019-03-13 01:41:29,019 : plagiarism : pearson = 0.7151, spearman = 0.7312
2019-03-13 01:41:30,503 : postediting : pearson = 0.7934, spearman = 0.8148
2019-03-13 01:41:31,104 : question-question : pearson = 0.5781, spearman = 0.6011
2019-03-13 01:41:31,104 : ALL (weighted average) : Pearson = 0.6624,             Spearman = 0.6831
2019-03-13 01:41:31,104 : ALL (average) : Pearson = 0.6615,             Spearman = 0.6822

2019-03-13 01:41:31,104 : ***** Transfer task : MR *****


2019-03-13 01:41:31,123 : loading BERT model bert-large-uncased
2019-03-13 01:41:31,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:41:31,141 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:41:31,141 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpenqe2hfd
2019-03-13 01:41:38,598 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:41:44,029 : Generating sentence embeddings
2019-03-13 01:42:15,285 : Generated sentence embeddings
2019-03-13 01:42:15,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:42:24,364 : Best param found at split 1: l2reg = 0.001                 with score 72.42
2019-03-13 01:42:33,692 : Best param found at split 2: l2reg = 0.0001                 with score 72.18
2019-03-13 01:42:41,261 : Best param found at split 3: l2reg = 0.001                 with score 72.16
2019-03-13 01:42:49,004 : Best param found at split 4: l2reg = 1e-05                 with score 71.74
2019-03-13 01:42:57,102 : Best param found at split 5: l2reg = 0.0001                 with score 71.69
2019-03-13 01:42:57,605 : Dev acc : 72.04 Test acc : 70.15

2019-03-13 01:42:57,606 : ***** Transfer task : CR *****


2019-03-13 01:42:57,614 : loading BERT model bert-large-uncased
2019-03-13 01:42:57,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:42:57,633 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:42:57,633 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps7jxnmmp
2019-03-13 01:43:05,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:43:10,542 : Generating sentence embeddings
2019-03-13 01:43:18,835 : Generated sentence embeddings
2019-03-13 01:43:18,835 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:43:22,436 : Best param found at split 1: l2reg = 1e-05                 with score 75.62
2019-03-13 01:43:26,008 : Best param found at split 2: l2reg = 1e-05                 with score 75.95
2019-03-13 01:43:29,490 : Best param found at split 3: l2reg = 1e-05                 with score 75.66
2019-03-13 01:43:33,162 : Best param found at split 4: l2reg = 1e-05                 with score 76.2
2019-03-13 01:43:35,980 : Best param found at split 5: l2reg = 0.0001                 with score 76.0
2019-03-13 01:43:36,176 : Dev acc : 75.89 Test acc : 76.51

2019-03-13 01:43:36,176 : ***** Transfer task : MPQA *****


2019-03-13 01:43:36,182 : loading BERT model bert-large-uncased
2019-03-13 01:43:36,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:43:36,232 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:43:36,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0dg8a983
2019-03-13 01:43:43,680 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:43:48,975 : Generating sentence embeddings
2019-03-13 01:43:56,498 : Generated sentence embeddings
2019-03-13 01:43:56,498 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:44:06,750 : Best param found at split 1: l2reg = 1e-05                 with score 81.66
2019-03-13 01:44:17,507 : Best param found at split 2: l2reg = 0.001                 with score 84.57
2019-03-13 01:44:26,151 : Best param found at split 3: l2reg = 0.001                 with score 84.25
2019-03-13 01:44:34,914 : Best param found at split 4: l2reg = 0.001                 with score 83.77
2019-03-13 01:44:45,389 : Best param found at split 5: l2reg = 0.001                 with score 85.6
2019-03-13 01:44:45,805 : Dev acc : 83.97 Test acc : 83.57

2019-03-13 01:44:45,806 : ***** Transfer task : SUBJ *****


2019-03-13 01:44:45,820 : loading BERT model bert-large-uncased
2019-03-13 01:44:45,821 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:44:45,841 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:44:45,841 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvmu6iz0s
2019-03-13 01:44:53,257 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:44:58,540 : Generating sentence embeddings
2019-03-13 01:45:29,238 : Generated sentence embeddings
2019-03-13 01:45:29,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:45:38,656 : Best param found at split 1: l2reg = 0.001                 with score 93.3
2019-03-13 01:45:48,562 : Best param found at split 2: l2reg = 0.001                 with score 93.56
2019-03-13 01:45:58,738 : Best param found at split 3: l2reg = 1e-05                 with score 93.45
2019-03-13 01:46:09,423 : Best param found at split 4: l2reg = 0.001                 with score 93.75
2019-03-13 01:46:19,777 : Best param found at split 5: l2reg = 0.001                 with score 93.37
2019-03-13 01:46:20,282 : Dev acc : 93.49 Test acc : 93.17

2019-03-13 01:46:20,283 : ***** Transfer task : SST Binary classification *****


2019-03-13 01:46:20,378 : loading BERT model bert-large-uncased
2019-03-13 01:46:20,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:46:20,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:46:20,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpshfzkj3m
2019-03-13 01:46:27,916 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:46:33,235 : Computing embedding for train
2019-03-13 01:48:12,571 : Computed train embeddings
2019-03-13 01:48:12,571 : Computing embedding for dev
2019-03-13 01:48:14,731 : Computed dev embeddings
2019-03-13 01:48:14,731 : Computing embedding for test
2019-03-13 01:48:19,273 : Computed test embeddings
2019-03-13 01:48:19,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:48:35,937 : [('reg:1e-05', 77.52), ('reg:0.0001', 77.87), ('reg:0.001', 77.98), ('reg:0.01', 76.95)]
2019-03-13 01:48:35,938 : Validation : best param found is reg = 0.001 with score             77.98
2019-03-13 01:48:35,938 : Evaluating...
2019-03-13 01:48:40,264 : 
Dev acc : 77.98 Test acc : 78.58 for             SST Binary classification

2019-03-13 01:48:40,264 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 01:48:40,314 : loading BERT model bert-large-uncased
2019-03-13 01:48:40,314 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:48:40,336 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:48:40,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3k6qkmun
2019-03-13 01:48:47,824 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:48:53,147 : Computing embedding for train
2019-03-13 01:49:14,893 : Computed train embeddings
2019-03-13 01:49:14,893 : Computing embedding for dev
2019-03-13 01:49:17,730 : Computed dev embeddings
2019-03-13 01:49:17,730 : Computing embedding for test
2019-03-13 01:49:23,321 : Computed test embeddings
2019-03-13 01:49:23,322 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:49:25,634 : [('reg:1e-05', 37.33), ('reg:0.0001', 35.33), ('reg:0.001', 37.15), ('reg:0.01', 35.24)]
2019-03-13 01:49:25,634 : Validation : best param found is reg = 1e-05 with score             37.33
2019-03-13 01:49:25,634 : Evaluating...
2019-03-13 01:49:26,286 : 
Dev acc : 37.33 Test acc : 37.06 for             SST Fine-Grained classification

2019-03-13 01:49:26,287 : ***** Transfer task : TREC *****


2019-03-13 01:49:26,300 : loading BERT model bert-large-uncased
2019-03-13 01:49:26,300 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:49:26,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:49:26,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppg3m9qof
2019-03-13 01:49:33,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:49:46,584 : Computed train embeddings
2019-03-13 01:49:47,168 : Computed test embeddings
2019-03-13 01:49:47,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 01:49:53,891 : [('reg:1e-05', 66.17), ('reg:0.0001', 66.01), ('reg:0.001', 65.53), ('reg:0.01', 55.08)]
2019-03-13 01:49:53,891 : Cross-validation : best param found is reg = 1e-05             with score 66.17
2019-03-13 01:49:53,891 : Evaluating...
2019-03-13 01:49:54,342 : 
Dev acc : 66.17 Test acc : 77.6             for TREC

2019-03-13 01:49:54,343 : ***** Transfer task : MRPC *****


2019-03-13 01:49:54,365 : loading BERT model bert-large-uncased
2019-03-13 01:49:54,365 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:49:54,386 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:49:54,386 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_v_jm4sk
2019-03-13 01:50:01,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:50:07,141 : Computing embedding for train
2019-03-13 01:50:29,249 : Computed train embeddings
2019-03-13 01:50:29,249 : Computing embedding for test
2019-03-13 01:50:38,918 : Computed test embeddings
2019-03-13 01:50:38,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 01:50:42,589 : [('reg:1e-05', 69.82), ('reg:0.0001', 69.53), ('reg:0.001', 70.0), ('reg:0.01', 68.99)]
2019-03-13 01:50:42,589 : Cross-validation : best param found is reg = 0.001             with score 70.0
2019-03-13 01:50:42,589 : Evaluating...
2019-03-13 01:50:42,847 : Dev acc : 70.0 Test acc 62.67; Test F1 66.84 for MRPC.

2019-03-13 01:50:42,847 : ***** Transfer task : SICK-Entailment*****


2019-03-13 01:50:42,908 : loading BERT model bert-large-uncased
2019-03-13 01:50:42,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:50:42,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:50:42,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfd8pqu4a
2019-03-13 01:50:50,428 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:50:55,832 : Computing embedding for train
2019-03-13 01:51:07,061 : Computed train embeddings
2019-03-13 01:51:07,062 : Computing embedding for dev
2019-03-13 01:51:08,591 : Computed dev embeddings
2019-03-13 01:51:08,591 : Computing embedding for test
2019-03-13 01:51:20,618 : Computed test embeddings
2019-03-13 01:51:20,654 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:51:21,773 : [('reg:1e-05', 71.0), ('reg:0.0001', 77.0), ('reg:0.001', 71.8), ('reg:0.01', 74.6)]
2019-03-13 01:51:21,773 : Validation : best param found is reg = 0.0001 with score             77.0
2019-03-13 01:51:21,773 : Evaluating...
2019-03-13 01:51:22,068 : 
Dev acc : 77.0 Test acc : 74.79 for                        SICK entailment

2019-03-13 01:51:22,069 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 01:51:22,096 : loading BERT model bert-large-uncased
2019-03-13 01:51:22,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:51:22,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:51:22,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp17thb7es
2019-03-13 01:51:29,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:51:34,941 : Computing embedding for train
2019-03-13 01:51:46,178 : Computed train embeddings
2019-03-13 01:51:46,179 : Computing embedding for dev
2019-03-13 01:51:47,708 : Computed dev embeddings
2019-03-13 01:51:47,708 : Computing embedding for test
2019-03-13 01:51:59,730 : Computed test embeddings
2019-03-13 01:52:14,881 : Dev : Pearson 0.783465512595046
2019-03-13 01:52:14,881 : Test : Pearson 0.7780628789023348 Spearman 0.6980190228735401 MSE 0.4044349144822709                        for SICK Relatedness

2019-03-13 01:52:14,882 : 

***** Transfer task : STSBenchmark*****


2019-03-13 01:52:14,952 : loading BERT model bert-large-uncased
2019-03-13 01:52:14,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:52:14,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:52:14,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsq_mrgg8
2019-03-13 01:52:22,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:52:27,966 : Computing embedding for train
2019-03-13 01:52:46,415 : Computed train embeddings
2019-03-13 01:52:46,415 : Computing embedding for dev
2019-03-13 01:52:52,017 : Computed dev embeddings
2019-03-13 01:52:52,018 : Computing embedding for test
2019-03-13 01:52:56,580 : Computed test embeddings
2019-03-13 01:53:15,571 : Dev : Pearson 0.6882971619980379
2019-03-13 01:53:15,572 : Test : Pearson 0.6507396161819536 Spearman 0.6479370026690551 MSE 1.4861786259527527                        for SICK Relatedness

2019-03-13 01:53:15,572 : ***** Transfer task : SNLI Entailment*****


2019-03-13 01:53:20,694 : loading BERT model bert-large-uncased
2019-03-13 01:53:20,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:53:20,763 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:53:20,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmgucxleg
2019-03-13 01:53:28,248 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:53:34,032 : PROGRESS (encoding): 0.00%
2019-03-13 01:56:17,900 : PROGRESS (encoding): 14.56%
2019-03-13 01:59:23,608 : PROGRESS (encoding): 29.12%
2019-03-13 02:02:30,064 : PROGRESS (encoding): 43.69%
2019-03-13 02:05:48,879 : PROGRESS (encoding): 58.25%
2019-03-13 02:09:30,385 : PROGRESS (encoding): 72.81%
2019-03-13 02:13:10,682 : PROGRESS (encoding): 87.37%
2019-03-13 02:17:09,159 : PROGRESS (encoding): 0.00%
2019-03-13 02:17:39,191 : PROGRESS (encoding): 0.00%
2019-03-13 02:18:08,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:18:50,084 : [('reg:1e-09', 63.87)]
2019-03-13 02:18:50,084 : Validation : best param found is reg = 1e-09 with score             63.87
2019-03-13 02:18:50,084 : Evaluating...
2019-03-13 02:19:32,791 : Dev acc : 63.87 Test acc : 65.37 for SNLI

2019-03-13 02:19:32,791 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 02:19:32,997 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 02:19:34,054 : loading BERT model bert-large-uncased
2019-03-13 02:19:34,054 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:19:34,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:19:34,081 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq6i9c20n
2019-03-13 02:19:41,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:19:46,915 : Computing embeddings for train/dev/test
2019-03-13 02:23:15,382 : Computed embeddings
2019-03-13 02:23:15,382 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:23:45,413 : [('reg:1e-05', 79.62), ('reg:0.0001', 78.81), ('reg:0.001', 66.94), ('reg:0.01', 59.13)]
2019-03-13 02:23:45,413 : Validation : best param found is reg = 1e-05 with score             79.62
2019-03-13 02:23:45,413 : Evaluating...
2019-03-13 02:23:52,851 : 
Dev acc : 79.6 Test acc : 80.7 for LENGTH classification

2019-03-13 02:23:52,852 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 02:23:53,102 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 02:23:53,147 : loading BERT model bert-large-uncased
2019-03-13 02:23:53,147 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:23:53,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:23:53,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsjm6gypm
2019-03-13 02:24:00,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:24:05,689 : Computing embeddings for train/dev/test
2019-03-13 02:27:18,026 : Computed embeddings
2019-03-13 02:27:18,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:27:49,159 : [('reg:1e-05', 51.33), ('reg:0.0001', 9.08), ('reg:0.001', 1.6), ('reg:0.01', 0.52)]
2019-03-13 02:27:49,159 : Validation : best param found is reg = 1e-05 with score             51.33
2019-03-13 02:27:49,159 : Evaluating...
2019-03-13 02:27:56,845 : 
Dev acc : 51.3 Test acc : 50.4 for WORDCONTENT classification

2019-03-13 02:27:56,846 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 02:27:57,376 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 02:27:57,441 : loading BERT model bert-large-uncased
2019-03-13 02:27:57,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:27:57,465 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:27:57,465 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqyfthpk2
2019-03-13 02:28:04,949 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:28:10,248 : Computing embeddings for train/dev/test
2019-03-13 02:31:10,890 : Computed embeddings
2019-03-13 02:31:10,890 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:31:37,826 : [('reg:1e-05', 28.52), ('reg:0.0001', 25.49), ('reg:0.001', 27.07), ('reg:0.01', 23.84)]
2019-03-13 02:31:37,826 : Validation : best param found is reg = 1e-05 with score             28.52
2019-03-13 02:31:37,826 : Evaluating...
2019-03-13 02:31:45,345 : 
Dev acc : 28.5 Test acc : 28.5 for DEPTH classification

2019-03-13 02:31:45,346 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 02:31:45,725 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 02:31:45,789 : loading BERT model bert-large-uncased
2019-03-13 02:31:45,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:31:45,906 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:31:45,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpig6oilwk
2019-03-13 02:31:53,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:31:58,810 : Computing embeddings for train/dev/test
2019-03-13 02:34:46,412 : Computed embeddings
2019-03-13 02:34:46,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:35:15,072 : [('reg:1e-05', 52.66), ('reg:0.0001', 51.49), ('reg:0.001', 47.61), ('reg:0.01', 26.0)]
2019-03-13 02:35:15,073 : Validation : best param found is reg = 1e-05 with score             52.66
2019-03-13 02:35:15,073 : Evaluating...
2019-03-13 02:35:23,715 : 
Dev acc : 52.7 Test acc : 52.9 for TOPCONSTITUENTS classification

2019-03-13 02:35:23,716 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 02:35:24,109 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 02:35:24,180 : loading BERT model bert-large-uncased
2019-03-13 02:35:24,180 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:35:24,213 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:35:24,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvnt932dt
2019-03-13 02:35:31,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:35:37,115 : Computing embeddings for train/dev/test
2019-03-13 02:38:39,064 : Computed embeddings
2019-03-13 02:38:39,064 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:39:00,398 : [('reg:1e-05', 63.37), ('reg:0.0001', 63.48), ('reg:0.001', 64.15), ('reg:0.01', 56.89)]
2019-03-13 02:39:00,398 : Validation : best param found is reg = 0.001 with score             64.15
2019-03-13 02:39:00,398 : Evaluating...
2019-03-13 02:39:04,317 : 
Dev acc : 64.2 Test acc : 63.0 for BIGRAMSHIFT classification

2019-03-13 02:39:04,318 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 02:39:04,706 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 02:39:04,771 : loading BERT model bert-large-uncased
2019-03-13 02:39:04,771 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:39:04,801 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:39:04,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq61jaaqx
2019-03-13 02:39:12,244 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:39:17,625 : Computing embeddings for train/dev/test
2019-03-13 02:42:15,454 : Computed embeddings
2019-03-13 02:42:15,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:42:41,428 : [('reg:1e-05', 88.2), ('reg:0.0001', 88.15), ('reg:0.001', 88.27), ('reg:0.01', 87.67)]
2019-03-13 02:42:41,428 : Validation : best param found is reg = 0.001 with score             88.27
2019-03-13 02:42:41,428 : Evaluating...
2019-03-13 02:42:48,045 : 
Dev acc : 88.3 Test acc : 86.8 for TENSE classification

2019-03-13 02:42:48,046 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 02:42:48,444 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 02:42:48,506 : loading BERT model bert-large-uncased
2019-03-13 02:42:48,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:42:48,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:42:48,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0q5nk0rs
2019-03-13 02:42:56,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:43:01,488 : Computing embeddings for train/dev/test
2019-03-13 02:46:09,938 : Computed embeddings
2019-03-13 02:46:09,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:46:40,984 : [('reg:1e-05', 78.9), ('reg:0.0001', 78.72), ('reg:0.001', 78.99), ('reg:0.01', 75.39)]
2019-03-13 02:46:40,984 : Validation : best param found is reg = 0.001 with score             78.99
2019-03-13 02:46:40,985 : Evaluating...
2019-03-13 02:46:47,503 : 
Dev acc : 79.0 Test acc : 77.5 for SUBJNUMBER classification

2019-03-13 02:46:47,504 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 02:46:47,901 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 02:46:47,968 : loading BERT model bert-large-uncased
2019-03-13 02:46:47,968 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:46:48,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:46:48,081 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpadgd1m91
2019-03-13 02:46:55,543 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:47:00,967 : Computing embeddings for train/dev/test
2019-03-13 02:50:05,824 : Computed embeddings
2019-03-13 02:50:05,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:50:34,929 : [('reg:1e-05', 75.99), ('reg:0.0001', 76.15), ('reg:0.001', 75.51), ('reg:0.01', 72.32)]
2019-03-13 02:50:34,929 : Validation : best param found is reg = 0.0001 with score             76.15
2019-03-13 02:50:34,929 : Evaluating...
2019-03-13 02:50:42,399 : 
Dev acc : 76.2 Test acc : 77.8 for OBJNUMBER classification

2019-03-13 02:50:42,400 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 02:50:42,954 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 02:50:43,022 : loading BERT model bert-large-uncased
2019-03-13 02:50:43,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:50:43,050 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:50:43,050 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7z7uvr8t
2019-03-13 02:50:50,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:50:55,851 : Computing embeddings for train/dev/test
2019-03-13 02:54:30,406 : Computed embeddings
2019-03-13 02:54:30,406 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:54:50,160 : [('reg:1e-05', 58.61), ('reg:0.0001', 58.59), ('reg:0.001', 58.5), ('reg:0.01', 57.96)]
2019-03-13 02:54:50,160 : Validation : best param found is reg = 1e-05 with score             58.61
2019-03-13 02:54:50,160 : Evaluating...
2019-03-13 02:54:54,987 : 
Dev acc : 58.6 Test acc : 58.7 for ODDMANOUT classification

2019-03-13 02:54:54,988 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 02:54:55,361 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 02:54:55,441 : loading BERT model bert-large-uncased
2019-03-13 02:54:55,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:54:55,470 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:54:55,470 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_tn3ffuj
2019-03-13 02:55:02,917 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:55:08,204 : Computing embeddings for train/dev/test
2019-03-13 02:58:40,655 : Computed embeddings
2019-03-13 02:58:40,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:59:06,933 : [('reg:1e-05', 55.07), ('reg:0.0001', 54.98), ('reg:0.001', 51.36), ('reg:0.01', 50.5)]
2019-03-13 02:59:06,933 : Validation : best param found is reg = 1e-05 with score             55.07
2019-03-13 02:59:06,933 : Evaluating...
2019-03-13 02:59:14,575 : 
Dev acc : 55.1 Test acc : 54.7 for COORDINATIONINVERSION classification

2019-03-13 02:59:14,577 : total results: {'STS12': {'MSRpar': {'pearson': (0.2438931675629412, 1.2820410264121104e-11), 'spearman': SpearmanrResult(correlation=0.3323808888626109, pvalue=8.383787247018593e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5564274137024073, 3.4605793375259887e-62), 'spearman': SpearmanrResult(correlation=0.5707752816245011, pvalue=4.763580032760818e-66), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4736480410831471, 4.81587012252361e-27), 'spearman': SpearmanrResult(correlation=0.5759366326737809, pvalue=6.53155258157429e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.563020439317559, 6.142188858445623e-64), 'spearman': SpearmanrResult(correlation=0.602597422081958, pvalue=2.514327837886153e-75), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6409570672306837, 1.5390402397098104e-47), 'spearman': SpearmanrResult(correlation=0.5598979613596123, pvalue=2.6714697380028035e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4955892257793476, 'wmean': 0.48122653993545306}, 'spearman': {'mean': 0.5283176373204926, 'wmean': 0.5202925982646568}}}, 'STS13': {'FNWN': {'pearson': (0.3912274424808305, 2.6024301438124905e-08), 'spearman': SpearmanrResult(correlation=0.3611478047979518, pvalue=3.301144491163358e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6734910332547726, 2.935873655607663e-100), 'spearman': SpearmanrResult(correlation=0.6705690641199135, pvalue=4.294093624616245e-99), 'nsamples': 750}, 'OnWN': {'pearson': (0.6414922156701033, 2.2980348490751994e-66), 'spearman': SpearmanrResult(correlation=0.6561772044719695, pvalue=2.2692991260995077e-70), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5687368971352355, 'wmean': 0.6259582630405895}, 'spearman': {'mean': 0.5626313577966116, 'wmean': 0.6261994299370153}}}, 'STS14': {'deft-forum': {'pearson': (0.29343683479552335, 2.1857206627460969e-10), 'spearman': SpearmanrResult(correlation=0.31740825534023787, pvalue=5.440655012091105e-12), 'nsamples': 450}, 'deft-news': {'pearson': (0.6400074122729219, 5.701464077550501e-36), 'spearman': SpearmanrResult(correlation=0.6569542500256099, pvalue=1.9488020420109496e-38), 'nsamples': 300}, 'headlines': {'pearson': (0.6052685488230506, 3.7461593621273005e-76), 'spearman': SpearmanrResult(correlation=0.569679769848396, pvalue=9.538338657471084e-66), 'nsamples': 750}, 'images': {'pearson': (0.6300522577986283, 3.2543643936680374e-84), 'spearman': SpearmanrResult(correlation=0.6193972552281813, pvalue=1.1657816220652714e-80), 'nsamples': 750}, 'OnWN': {'pearson': (0.6597741757336557, 6.667896045558067e-95), 'spearman': SpearmanrResult(correlation=0.6983994377806553, pvalue=9.176061152717054e-111), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6213528680174248, 2.6580217870616546e-81), 'spearman': SpearmanrResult(correlation=0.5692783569781997, pvalue=1.2293577403540464e-65), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5749820162402007, 'wmean': 0.5897025832318484}, 'spearman': {'mean': 0.5718528875335466, 'wmean': 0.5819962946099638}}}, 'STS15': {'answers-forums': {'pearson': (0.5526408519963646, 2.2537387742264415e-31), 'spearman': SpearmanrResult(correlation=0.5429508586460832, pvalue=3.881970077630753e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.5971195138490406, 1.1799091664292216e-73), 'spearman': SpearmanrResult(correlation=0.6261092580800109, pvalue=6.979411119132934e-83), 'nsamples': 750}, 'belief': {'pearson': (0.6292886916619284, 9.506351678191846e-43), 'spearman': SpearmanrResult(correlation=0.662927814429805, pvalue=8.055196239419004e-49), 'nsamples': 375}, 'headlines': {'pearson': (0.667411320997643, 7.535470565641927e-98), 'spearman': SpearmanrResult(correlation=0.6732914095158741, pvalue=3.529914197471316e-100), 'nsamples': 750}, 'images': {'pearson': (0.6740382113991449, 1.7703411694523722e-100), 'spearman': SpearmanrResult(correlation=0.703031711814068, pvalue=7.729471652667184e-113), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6240997179808243, 'wmean': 0.6323834545187438}, 'spearman': {'mean': 0.6416622104971682, 'wmean': 0.6513429289869743}}}, 'STS16': {'answer-answer': {'pearson': (0.5453150459053318, 4.445189089517129e-21), 'spearman': SpearmanrResult(correlation=0.5625977249049767, pvalue=1.3266302302647067e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6753791375167109, 1.6088141926337554e-34), 'spearman': SpearmanrResult(correlation=0.701114531368967, pvalue=3.7845485174734914e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.715149962931129, 2.529349728509335e-37), 'spearman': SpearmanrResult(correlation=0.7311593772673903, pvalue=9.765565730779223e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7933692648070713, 4.3821629064510005e-54), 'spearman': SpearmanrResult(correlation=0.8148353235983342, pvalue=3.1006781267584852e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.5780931546529247, 4.94343649871474e-20), 'spearman': SpearmanrResult(correlation=0.601134515660823, pvalue=6.377899819035155e-22), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6614613131626335, 'wmean': 0.6623671908188546}, 'spearman': {'mean': 0.6821682945600982, 'wmean': 0.6830530606571182}}}, 'MR': {'devacc': 72.04, 'acc': 70.15, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.89, 'acc': 76.51, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.97, 'acc': 83.57, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.49, 'acc': 93.17, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.98, 'acc': 78.58, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.33, 'acc': 37.06, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.17, 'acc': 77.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.0, 'acc': 62.67, 'f1': 66.84, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.0, 'acc': 74.79, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.783465512595046, 'pearson': 0.7780628789023348, 'spearman': 0.6980190228735401, 'mse': 0.4044349144822709, 'yhat': array([2.99447365, 4.38227745, 1.96507845, ..., 3.03371467, 4.12933266,        4.41274354]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6882971619980379, 'pearson': 0.6507396161819536, 'spearman': 0.6479370026690551, 'mse': 1.4861786259527527, 'yhat': array([3.16654652, 1.87281206, 2.59046993, ..., 4.11954266, 3.93712318,        3.43638754]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.87, 'acc': 65.37, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 79.62, 'acc': 80.71, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 51.33, 'acc': 50.37, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.52, 'acc': 28.5, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.66, 'acc': 52.91, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 64.15, 'acc': 63.02, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.27, 'acc': 86.84, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.99, 'acc': 77.47, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.15, 'acc': 77.82, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.61, 'acc': 58.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.07, 'acc': 54.68, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 02:59:14,577 : STS12 p=0.4812, STS12 s=0.5203, STS13 p=0.6260, STS13 s=0.6262, STS14 p=0.5897, STS14 s=0.5820, STS15 p=0.6324, STS15 s=0.6513, STS 16 p=0.6624, STS16 s=0.6831, STS B p=0.6507, STS B s=0.6479, STS B m=1.4862, SICK-R p=0.7781, SICK-R s=0.6980, SICK-P m=0.4044
2019-03-13 02:59:14,577 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 02:59:14,577 : 0.4812,0.5203,0.6260,0.6262,0.5897,0.5820,0.6324,0.6513,0.6624,0.6831,0.6507,0.6479,1.4862,0.7781,0.6980,0.4044
2019-03-13 02:59:14,577 : MR=70.15, CR=76.51, SUBJ=93.17, MPQA=83.57, SST-B=78.58, SST-F=37.06, TREC=77.60, SICK-E=74.79, SNLI=65.37, MRPC=62.67, MRPC f=66.84
2019-03-13 02:59:14,577 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 02:59:14,577 : 70.15,76.51,93.17,83.57,78.58,37.06,77.60,74.79,65.37,62.67,66.84
2019-03-13 02:59:14,577 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 02:59:14,577 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 02:59:14,577 : na,na,na,na,na,na,na,na,na,na
2019-03-13 02:59:14,577 : SentLen=80.71, WC=50.37, TreeDepth=28.50, TopConst=52.91, BShift=63.02, Tense=86.84, SubjNum=77.47, ObjNum=77.82, SOMO=58.69, CoordInv=54.68, average=63.10
2019-03-13 02:59:14,577 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 02:59:14,577 : 80.71,50.37,28.50,52.91,63.02,86.84,77.47,77.82,58.69,54.68,63.10
2019-03-13 02:59:14,577 : ********************************************************************************
2019-03-13 02:59:14,577 : ********************************************************************************
2019-03-13 02:59:14,577 : ********************************************************************************
2019-03-13 02:59:14,577 : layer 10
2019-03-13 02:59:14,577 : ********************************************************************************
2019-03-13 02:59:14,578 : ********************************************************************************
2019-03-13 02:59:14,578 : ********************************************************************************
2019-03-13 02:59:14,670 : ***** Transfer task : STS12 *****


2019-03-13 02:59:14,683 : loading BERT model bert-large-uncased
2019-03-13 02:59:14,683 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:59:14,700 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:59:14,700 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2an48z4w
2019-03-13 02:59:22,177 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:59:31,471 : MSRpar : pearson = 0.2580, spearman = 0.3467
2019-03-13 02:59:33,103 : MSRvid : pearson = 0.4732, spearman = 0.5036
2019-03-13 02:59:34,508 : SMTeuroparl : pearson = 0.4571, spearman = 0.5814
2019-03-13 02:59:37,190 : surprise.OnWN : pearson = 0.5583, spearman = 0.5983
2019-03-13 02:59:38,608 : surprise.SMTnews : pearson = 0.6032, spearman = 0.5444
2019-03-13 02:59:38,608 : ALL (weighted average) : Pearson = 0.4561,             Spearman = 0.5053
2019-03-13 02:59:38,608 : ALL (average) : Pearson = 0.4699,             Spearman = 0.5149

2019-03-13 02:59:38,608 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 02:59:38,617 : loading BERT model bert-large-uncased
2019-03-13 02:59:38,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:59:38,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:59:38,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1isza_uf
2019-03-13 02:59:46,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:59:52,743 : FNWN : pearson = 0.3610, spearman = 0.3353
2019-03-13 02:59:54,625 : headlines : pearson = 0.6688, spearman = 0.6675
2019-03-13 02:59:56,085 : OnWN : pearson = 0.6221, spearman = 0.6526
2019-03-13 02:59:56,085 : ALL (weighted average) : Pearson = 0.6125,             Spearman = 0.6200
2019-03-13 02:59:56,085 : ALL (average) : Pearson = 0.5506,             Spearman = 0.5518

2019-03-13 02:59:56,085 : ***** Transfer task : STS14 *****


2019-03-13 02:59:56,100 : loading BERT model bert-large-uncased
2019-03-13 02:59:56,100 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:59:56,118 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:59:56,118 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaxungrcx
2019-03-13 03:00:03,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:00:10,162 : deft-forum : pearson = 0.2636, spearman = 0.2860
2019-03-13 03:00:11,792 : deft-news : pearson = 0.6336, spearman = 0.6582
2019-03-13 03:00:13,948 : headlines : pearson = 0.5965, spearman = 0.5601
2019-03-13 03:00:16,010 : images : pearson = 0.5989, spearman = 0.5962
2019-03-13 03:00:18,128 : OnWN : pearson = 0.6497, spearman = 0.6927
2019-03-13 03:00:20,972 : tweet-news : pearson = 0.5707, spearman = 0.5269
2019-03-13 03:00:20,972 : ALL (weighted average) : Pearson = 0.5655,             Spearman = 0.5621
2019-03-13 03:00:20,972 : ALL (average) : Pearson = 0.5522,             Spearman = 0.5533

2019-03-13 03:00:20,972 : ***** Transfer task : STS15 *****


2019-03-13 03:00:21,004 : loading BERT model bert-large-uncased
2019-03-13 03:00:21,004 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:00:21,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:00:21,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjaki97fl
2019-03-13 03:00:28,513 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:00:35,561 : answers-forums : pearson = 0.5484, spearman = 0.5490
2019-03-13 03:00:37,631 : answers-students : pearson = 0.5812, spearman = 0.6143
2019-03-13 03:00:39,667 : belief : pearson = 0.6115, spearman = 0.6496
2019-03-13 03:00:41,905 : headlines : pearson = 0.6524, spearman = 0.6613
2019-03-13 03:00:44,024 : images : pearson = 0.6421, spearman = 0.6752
2019-03-13 03:00:44,024 : ALL (weighted average) : Pearson = 0.6139,             Spearman = 0.6375
2019-03-13 03:00:44,024 : ALL (average) : Pearson = 0.6071,             Spearman = 0.6299

2019-03-13 03:00:44,024 : ***** Transfer task : STS16 *****


2019-03-13 03:00:44,093 : loading BERT model bert-large-uncased
2019-03-13 03:00:44,093 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:00:44,111 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:00:44,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp00horl1t
2019-03-13 03:00:51,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:00:57,754 : answer-answer : pearson = 0.5101, spearman = 0.5386
2019-03-13 03:00:58,408 : headlines : pearson = 0.6678, spearman = 0.7042
2019-03-13 03:00:59,285 : plagiarism : pearson = 0.7142, spearman = 0.7336
2019-03-13 03:01:00,767 : postediting : pearson = 0.7846, spearman = 0.8110
2019-03-13 03:01:01,368 : question-question : pearson = 0.5431, spearman = 0.5826
2019-03-13 03:01:01,369 : ALL (weighted average) : Pearson = 0.6450,             Spearman = 0.6750
2019-03-13 03:01:01,369 : ALL (average) : Pearson = 0.6439,             Spearman = 0.6740

2019-03-13 03:01:01,369 : ***** Transfer task : MR *****


2019-03-13 03:01:01,384 : loading BERT model bert-large-uncased
2019-03-13 03:01:01,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:01:01,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:01:01,405 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk8e9bshz
2019-03-13 03:01:08,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:01:14,273 : Generating sentence embeddings
2019-03-13 03:01:45,509 : Generated sentence embeddings
2019-03-13 03:01:45,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:01:54,531 : Best param found at split 1: l2reg = 0.001                 with score 72.63
2019-03-13 03:02:03,547 : Best param found at split 2: l2reg = 0.0001                 with score 72.3
2019-03-13 03:02:11,750 : Best param found at split 3: l2reg = 1e-05                 with score 72.36
2019-03-13 03:02:22,425 : Best param found at split 4: l2reg = 1e-05                 with score 71.83
2019-03-13 03:02:33,752 : Best param found at split 5: l2reg = 0.001                 with score 70.75
2019-03-13 03:02:34,368 : Dev acc : 71.97 Test acc : 70.03

2019-03-13 03:02:34,370 : ***** Transfer task : CR *****


2019-03-13 03:02:34,377 : loading BERT model bert-large-uncased
2019-03-13 03:02:34,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:02:34,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:02:34,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgvuf3_nf
2019-03-13 03:02:41,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:02:47,255 : Generating sentence embeddings
2019-03-13 03:02:55,517 : Generated sentence embeddings
2019-03-13 03:02:55,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:02:59,367 : Best param found at split 1: l2reg = 1e-05                 with score 75.09
2019-03-13 03:03:02,611 : Best param found at split 2: l2reg = 1e-05                 with score 77.44
2019-03-13 03:03:06,470 : Best param found at split 3: l2reg = 1e-05                 with score 76.39
2019-03-13 03:03:10,518 : Best param found at split 4: l2reg = 1e-05                 with score 75.7
2019-03-13 03:03:13,722 : Best param found at split 5: l2reg = 0.0001                 with score 77.43
2019-03-13 03:03:13,864 : Dev acc : 76.41 Test acc : 77.38

2019-03-13 03:03:13,864 : ***** Transfer task : MPQA *****


2019-03-13 03:03:13,869 : loading BERT model bert-large-uncased
2019-03-13 03:03:13,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:03:13,919 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:03:13,919 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7z7i5irb
2019-03-13 03:03:21,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:03:26,608 : Generating sentence embeddings
2019-03-13 03:03:34,149 : Generated sentence embeddings
2019-03-13 03:03:34,150 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:03:40,803 : Best param found at split 1: l2reg = 0.01                 with score 82.73
2019-03-13 03:03:49,918 : Best param found at split 2: l2reg = 1e-05                 with score 85.21
2019-03-13 03:04:01,564 : Best param found at split 3: l2reg = 0.0001                 with score 85.41
2019-03-13 03:04:12,261 : Best param found at split 4: l2reg = 0.001                 with score 84.03
2019-03-13 03:04:24,215 : Best param found at split 5: l2reg = 1e-05                 with score 85.15
2019-03-13 03:04:24,734 : Dev acc : 84.51 Test acc : 84.77

2019-03-13 03:04:24,735 : ***** Transfer task : SUBJ *****


2019-03-13 03:04:24,752 : loading BERT model bert-large-uncased
2019-03-13 03:04:24,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:04:24,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:04:24,771 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgm0u33pn
2019-03-13 03:04:32,243 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:04:37,694 : Generating sentence embeddings
2019-03-13 03:05:08,413 : Generated sentence embeddings
2019-03-13 03:05:08,413 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:05:19,114 : Best param found at split 1: l2reg = 0.001                 with score 94.08
2019-03-13 03:05:29,957 : Best param found at split 2: l2reg = 1e-05                 with score 93.96
2019-03-13 03:05:40,712 : Best param found at split 3: l2reg = 1e-05                 with score 93.75
2019-03-13 03:05:51,234 : Best param found at split 4: l2reg = 0.001                 with score 94.44
2019-03-13 03:06:01,982 : Best param found at split 5: l2reg = 0.0001                 with score 93.99
2019-03-13 03:06:02,397 : Dev acc : 94.04 Test acc : 93.51

2019-03-13 03:06:02,399 : ***** Transfer task : SST Binary classification *****


2019-03-13 03:06:02,490 : loading BERT model bert-large-uncased
2019-03-13 03:06:02,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:06:02,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:06:02,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprjk3gaml
2019-03-13 03:06:10,007 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:06:15,308 : Computing embedding for train
2019-03-13 03:07:54,566 : Computed train embeddings
2019-03-13 03:07:54,566 : Computing embedding for dev
2019-03-13 03:07:56,724 : Computed dev embeddings
2019-03-13 03:07:56,724 : Computing embedding for test
2019-03-13 03:08:01,263 : Computed test embeddings
2019-03-13 03:08:01,263 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:08:15,119 : [('reg:1e-05', 77.52), ('reg:0.0001', 79.13), ('reg:0.001', 76.15), ('reg:0.01', 77.18)]
2019-03-13 03:08:15,120 : Validation : best param found is reg = 0.0001 with score             79.13
2019-03-13 03:08:15,120 : Evaluating...
2019-03-13 03:08:20,716 : 
Dev acc : 79.13 Test acc : 78.58 for             SST Binary classification

2019-03-13 03:08:20,717 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 03:08:20,772 : loading BERT model bert-large-uncased
2019-03-13 03:08:20,772 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:08:20,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:08:20,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk0wfg4je
2019-03-13 03:08:28,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:08:33,664 : Computing embedding for train
2019-03-13 03:08:55,384 : Computed train embeddings
2019-03-13 03:08:55,384 : Computing embedding for dev
2019-03-13 03:08:58,218 : Computed dev embeddings
2019-03-13 03:08:58,218 : Computing embedding for test
2019-03-13 03:09:03,803 : Computed test embeddings
2019-03-13 03:09:03,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:09:06,082 : [('reg:1e-05', 36.06), ('reg:0.0001', 37.78), ('reg:0.001', 36.15), ('reg:0.01', 35.79)]
2019-03-13 03:09:06,083 : Validation : best param found is reg = 0.0001 with score             37.78
2019-03-13 03:09:06,083 : Evaluating...
2019-03-13 03:09:06,736 : 
Dev acc : 37.78 Test acc : 36.2 for             SST Fine-Grained classification

2019-03-13 03:09:06,736 : ***** Transfer task : TREC *****


2019-03-13 03:09:06,750 : loading BERT model bert-large-uncased
2019-03-13 03:09:06,750 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:09:06,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:09:06,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplyh9sefa
2019-03-13 03:09:14,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:09:27,254 : Computed train embeddings
2019-03-13 03:09:27,838 : Computed test embeddings
2019-03-13 03:09:27,838 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:09:35,111 : [('reg:1e-05', 68.31), ('reg:0.0001', 69.06), ('reg:0.001', 62.02), ('reg:0.01', 59.58)]
2019-03-13 03:09:35,111 : Cross-validation : best param found is reg = 0.0001             with score 69.06
2019-03-13 03:09:35,111 : Evaluating...
2019-03-13 03:09:35,494 : 
Dev acc : 69.06 Test acc : 79.4             for TREC

2019-03-13 03:09:35,495 : ***** Transfer task : MRPC *****


2019-03-13 03:09:35,515 : loading BERT model bert-large-uncased
2019-03-13 03:09:35,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:09:35,538 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:09:35,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptoenerir
2019-03-13 03:09:43,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:09:48,222 : Computing embedding for train
2019-03-13 03:10:10,308 : Computed train embeddings
2019-03-13 03:10:10,309 : Computing embedding for test
2019-03-13 03:10:19,962 : Computed test embeddings
2019-03-13 03:10:19,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:10:25,179 : [('reg:1e-05', 69.38), ('reg:0.0001', 69.65), ('reg:0.001', 69.53), ('reg:0.01', 68.72)]
2019-03-13 03:10:25,179 : Cross-validation : best param found is reg = 0.0001             with score 69.65
2019-03-13 03:10:25,180 : Evaluating...
2019-03-13 03:10:25,492 : Dev acc : 69.65 Test acc 71.25; Test F1 79.66 for MRPC.

2019-03-13 03:10:25,493 : ***** Transfer task : SICK-Entailment*****


2019-03-13 03:10:25,554 : loading BERT model bert-large-uncased
2019-03-13 03:10:25,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:10:25,573 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:10:25,573 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptracadge
2019-03-13 03:10:33,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:10:38,394 : Computing embedding for train
2019-03-13 03:10:49,597 : Computed train embeddings
2019-03-13 03:10:49,597 : Computing embedding for dev
2019-03-13 03:10:51,125 : Computed dev embeddings
2019-03-13 03:10:51,126 : Computing embedding for test
2019-03-13 03:11:03,126 : Computed test embeddings
2019-03-13 03:11:03,164 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:11:04,761 : [('reg:1e-05', 76.4), ('reg:0.0001', 74.8), ('reg:0.001', 76.2), ('reg:0.01', 76.4)]
2019-03-13 03:11:04,762 : Validation : best param found is reg = 1e-05 with score             76.4
2019-03-13 03:11:04,762 : Evaluating...
2019-03-13 03:11:05,164 : 
Dev acc : 76.4 Test acc : 74.53 for                        SICK entailment

2019-03-13 03:11:05,164 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 03:11:05,191 : loading BERT model bert-large-uncased
2019-03-13 03:11:05,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:11:05,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:11:05,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsayp4u7p
2019-03-13 03:11:12,726 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:11:18,043 : Computing embedding for train
2019-03-13 03:11:29,264 : Computed train embeddings
2019-03-13 03:11:29,264 : Computing embedding for dev
2019-03-13 03:11:30,795 : Computed dev embeddings
2019-03-13 03:11:30,795 : Computing embedding for test
2019-03-13 03:11:42,826 : Computed test embeddings
2019-03-13 03:12:00,985 : Dev : Pearson 0.7805362427467539
2019-03-13 03:12:00,985 : Test : Pearson 0.7782882233478061 Spearman 0.700950425956238 MSE 0.4014285761253279                        for SICK Relatedness

2019-03-13 03:12:00,986 : 

***** Transfer task : STSBenchmark*****


2019-03-13 03:12:01,024 : loading BERT model bert-large-uncased
2019-03-13 03:12:01,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:12:01,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:12:01,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsxh2xxzl
2019-03-13 03:12:08,537 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:12:13,931 : Computing embedding for train
2019-03-13 03:12:32,381 : Computed train embeddings
2019-03-13 03:12:32,381 : Computing embedding for dev
2019-03-13 03:12:37,981 : Computed dev embeddings
2019-03-13 03:12:37,981 : Computing embedding for test
2019-03-13 03:12:42,545 : Computed test embeddings
2019-03-13 03:13:01,713 : Dev : Pearson 0.6791230432983683
2019-03-13 03:13:01,713 : Test : Pearson 0.6523531741938207 Spearman 0.6490943866560729 MSE 1.5258546115219729                        for SICK Relatedness

2019-03-13 03:13:01,714 : ***** Transfer task : SNLI Entailment*****


2019-03-13 03:13:06,853 : loading BERT model bert-large-uncased
2019-03-13 03:13:06,854 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:13:06,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:13:06,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2dgbumgz
2019-03-13 03:13:14,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:13:20,405 : PROGRESS (encoding): 0.00%
2019-03-13 03:16:04,207 : PROGRESS (encoding): 14.56%
2019-03-13 03:19:09,925 : PROGRESS (encoding): 29.12%
2019-03-13 03:22:16,295 : PROGRESS (encoding): 43.69%
2019-03-13 03:25:34,974 : PROGRESS (encoding): 58.25%
2019-03-13 03:29:16,292 : PROGRESS (encoding): 72.81%
2019-03-13 03:32:56,514 : PROGRESS (encoding): 87.37%
2019-03-13 03:36:54,777 : PROGRESS (encoding): 0.00%
2019-03-13 03:37:24,811 : PROGRESS (encoding): 0.00%
2019-03-13 03:37:53,693 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:38:20,354 : [('reg:1e-09', 60.92)]
2019-03-13 03:38:20,354 : Validation : best param found is reg = 1e-09 with score             60.92
2019-03-13 03:38:20,355 : Evaluating...
2019-03-13 03:38:45,942 : Dev acc : 60.92 Test acc : 60.91 for SNLI

2019-03-13 03:38:45,942 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 03:38:46,163 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 03:38:47,239 : loading BERT model bert-large-uncased
2019-03-13 03:38:47,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:38:47,266 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:38:47,266 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp925_xa5t
2019-03-13 03:38:54,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:39:00,188 : Computing embeddings for train/dev/test
2019-03-13 03:42:28,916 : Computed embeddings
2019-03-13 03:42:28,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:43:05,251 : [('reg:1e-05', 80.55), ('reg:0.0001', 80.1), ('reg:0.001', 71.84), ('reg:0.01', 56.95)]
2019-03-13 03:43:05,252 : Validation : best param found is reg = 1e-05 with score             80.55
2019-03-13 03:43:05,252 : Evaluating...
2019-03-13 03:43:16,113 : 
Dev acc : 80.5 Test acc : 80.5 for LENGTH classification

2019-03-13 03:43:16,114 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 03:43:16,490 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 03:43:16,535 : loading BERT model bert-large-uncased
2019-03-13 03:43:16,535 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:43:16,565 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:43:16,565 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl6teyrk_
2019-03-13 03:43:24,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:43:29,445 : Computing embeddings for train/dev/test
2019-03-13 03:46:41,574 : Computed embeddings
2019-03-13 03:46:41,574 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:47:15,964 : [('reg:1e-05', 42.67), ('reg:0.0001', 7.36), ('reg:0.001', 1.32), ('reg:0.01', 0.44)]
2019-03-13 03:47:15,964 : Validation : best param found is reg = 1e-05 with score             42.67
2019-03-13 03:47:15,964 : Evaluating...
2019-03-13 03:47:25,646 : 
Dev acc : 42.7 Test acc : 42.2 for WORDCONTENT classification

2019-03-13 03:47:25,647 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 03:47:26,027 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 03:47:26,093 : loading BERT model bert-large-uncased
2019-03-13 03:47:26,093 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:47:26,118 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:47:26,118 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpct06wqy1
2019-03-13 03:47:33,566 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:47:38,937 : Computing embeddings for train/dev/test
2019-03-13 03:50:39,425 : Computed embeddings
2019-03-13 03:50:39,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:51:02,505 : [('reg:1e-05', 27.01), ('reg:0.0001', 26.79), ('reg:0.001', 25.3), ('reg:0.01', 23.44)]
2019-03-13 03:51:02,505 : Validation : best param found is reg = 1e-05 with score             27.01
2019-03-13 03:51:02,505 : Evaluating...
2019-03-13 03:51:09,045 : 
Dev acc : 27.0 Test acc : 27.2 for DEPTH classification

2019-03-13 03:51:09,046 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 03:51:09,427 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 03:51:09,490 : loading BERT model bert-large-uncased
2019-03-13 03:51:09,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:51:09,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:51:09,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj30hc_sm
2019-03-13 03:51:17,045 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:51:22,413 : Computing embeddings for train/dev/test
2019-03-13 03:54:10,097 : Computed embeddings
2019-03-13 03:54:10,097 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:54:40,890 : [('reg:1e-05', 53.86), ('reg:0.0001', 53.17), ('reg:0.001', 40.18), ('reg:0.01', 24.06)]
2019-03-13 03:54:40,891 : Validation : best param found is reg = 1e-05 with score             53.86
2019-03-13 03:54:40,891 : Evaluating...
2019-03-13 03:54:49,823 : 
Dev acc : 53.9 Test acc : 54.3 for TOPCONSTITUENTS classification

2019-03-13 03:54:49,825 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 03:54:50,170 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 03:54:50,236 : loading BERT model bert-large-uncased
2019-03-13 03:54:50,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:54:50,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:54:50,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeiyvdl7o
2019-03-13 03:54:57,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:55:03,164 : Computing embeddings for train/dev/test
2019-03-13 03:58:05,121 : Computed embeddings
2019-03-13 03:58:05,121 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:58:33,193 : [('reg:1e-05', 62.37), ('reg:0.0001', 62.47), ('reg:0.001', 60.66), ('reg:0.01', 59.17)]
2019-03-13 03:58:33,193 : Validation : best param found is reg = 0.0001 with score             62.47
2019-03-13 03:58:33,193 : Evaluating...
2019-03-13 03:58:41,811 : 
Dev acc : 62.5 Test acc : 61.2 for BIGRAMSHIFT classification

2019-03-13 03:58:41,812 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 03:58:42,374 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 03:58:42,439 : loading BERT model bert-large-uncased
2019-03-13 03:58:42,439 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:58:42,468 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:58:42,468 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpty9gsgtg
2019-03-13 03:58:49,949 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:58:55,388 : Computing embeddings for train/dev/test
2019-03-13 04:01:53,263 : Computed embeddings
2019-03-13 04:01:53,263 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:02:21,273 : [('reg:1e-05', 88.68), ('reg:0.0001', 88.94), ('reg:0.001', 88.66), ('reg:0.01', 88.88)]
2019-03-13 04:02:21,273 : Validation : best param found is reg = 0.0001 with score             88.94
2019-03-13 04:02:21,273 : Evaluating...
2019-03-13 04:02:29,823 : 
Dev acc : 88.9 Test acc : 87.8 for TENSE classification

2019-03-13 04:02:29,824 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 04:02:30,249 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 04:02:30,312 : loading BERT model bert-large-uncased
2019-03-13 04:02:30,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:02:30,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:02:30,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqytbuvoh
2019-03-13 04:02:37,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:02:43,278 : Computing embeddings for train/dev/test
2019-03-13 04:05:51,714 : Computed embeddings
2019-03-13 04:05:51,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:06:22,569 : [('reg:1e-05', 81.72), ('reg:0.0001', 81.36), ('reg:0.001', 80.74), ('reg:0.01', 76.67)]
2019-03-13 04:06:22,569 : Validation : best param found is reg = 1e-05 with score             81.72
2019-03-13 04:06:22,569 : Evaluating...
2019-03-13 04:06:27,937 : 
Dev acc : 81.7 Test acc : 80.7 for SUBJNUMBER classification

2019-03-13 04:06:27,938 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 04:06:28,359 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 04:06:28,428 : loading BERT model bert-large-uncased
2019-03-13 04:06:28,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:06:28,551 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:06:28,551 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplmkwpji_
2019-03-13 04:06:36,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:06:41,302 : Computing embeddings for train/dev/test
2019-03-13 04:09:46,445 : Computed embeddings
2019-03-13 04:09:46,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:10:18,764 : [('reg:1e-05', 75.91), ('reg:0.0001', 75.91), ('reg:0.001', 76.09), ('reg:0.01', 72.88)]
2019-03-13 04:10:18,764 : Validation : best param found is reg = 0.001 with score             76.09
2019-03-13 04:10:18,764 : Evaluating...
2019-03-13 04:10:23,519 : 
Dev acc : 76.1 Test acc : 76.7 for OBJNUMBER classification

2019-03-13 04:10:23,521 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 04:10:24,108 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 04:10:24,177 : loading BERT model bert-large-uncased
2019-03-13 04:10:24,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:10:24,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:10:24,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplp4yuc61
2019-03-13 04:10:31,680 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:10:36,997 : Computing embeddings for train/dev/test
2019-03-13 04:14:11,483 : Computed embeddings
2019-03-13 04:14:11,483 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:14:28,804 : [('reg:1e-05', 58.56), ('reg:0.0001', 58.5), ('reg:0.001', 58.15), ('reg:0.01', 57.49)]
2019-03-13 04:14:28,804 : Validation : best param found is reg = 1e-05 with score             58.56
2019-03-13 04:14:28,804 : Evaluating...
2019-03-13 04:14:33,536 : 
Dev acc : 58.6 Test acc : 58.4 for ODDMANOUT classification

2019-03-13 04:14:33,537 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 04:14:33,925 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 04:14:34,000 : loading BERT model bert-large-uncased
2019-03-13 04:14:34,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:14:34,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:14:34,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxfrqe0kt
2019-03-13 04:14:41,584 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:14:46,856 : Computing embeddings for train/dev/test
2019-03-13 04:18:19,542 : Computed embeddings
2019-03-13 04:18:19,542 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:18:44,149 : [('reg:1e-05', 54.52), ('reg:0.0001', 54.48), ('reg:0.001', 51.43), ('reg:0.01', 50.28)]
2019-03-13 04:18:44,149 : Validation : best param found is reg = 1e-05 with score             54.52
2019-03-13 04:18:44,149 : Evaluating...
2019-03-13 04:18:52,135 : 
Dev acc : 54.5 Test acc : 54.9 for COORDINATIONINVERSION classification

2019-03-13 04:18:52,137 : total results: {'STS12': {'MSRpar': {'pearson': (0.257966769671396, 7.246186463457775e-13), 'spearman': SpearmanrResult(correlation=0.34665090343482, pvalue=1.3382303381795897e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.47319099728718916, 4.120999248314065e-43), 'spearman': SpearmanrResult(correlation=0.5036342264124501, pvalue=1.7462590645943983e-49), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.45709078940519926, 4.467747367419508e-25), 'spearman': SpearmanrResult(correlation=0.5813702651704412, pvalue=7.46104347880626e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5582994940549469, 1.111702280896759e-62), 'spearman': SpearmanrResult(correlation=0.5982865385037168, pvalue=5.2295750383327965e-74), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6031786505034403, 6.760317135641221e-41), 'spearman': SpearmanrResult(correlation=0.5444192885016932, pvalue=3.6216321705182923e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4699453401844343, 'wmean': 0.456102284314031}, 'spearman': {'mean': 0.5148722444046243, 'wmean': 0.5053092017659744}}}, 'STS13': {'FNWN': {'pearson': (0.36100459977529176, 3.339286609924396e-07), 'spearman': SpearmanrResult(correlation=0.33525575864909735, pvalue=2.4097504898747403e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.6687578566699689, 2.2305635704066874e-98), 'spearman': SpearmanrResult(correlation=0.6674840052294242, pvalue=7.057370268072553e-98), 'nsamples': 750}, 'OnWN': {'pearson': (0.622052548624845, 2.1788238288760675e-61), 'spearman': SpearmanrResult(correlation=0.6525658906906473, pvalue=2.2993700738656396e-69), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5506050016900352, 'wmean': 0.6125131610923633}, 'spearman': {'mean': 0.5517685515230563, 'wmean': 0.6200438713228005}}}, 'STS14': {'deft-forum': {'pearson': (0.2636143890620015, 1.3685324910206437e-08), 'spearman': SpearmanrResult(correlation=0.28596694230064373, pvalue=6.454881879673968e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.6336021654574456, 4.449376330017379e-35), 'spearman': SpearmanrResult(correlation=0.6582184646524678, pvalue=1.2571439557543367e-38), 'nsamples': 300}, 'headlines': {'pearson': (0.596471237725975, 1.8515031241100092e-73), 'spearman': SpearmanrResult(correlation=0.5600791434911646, pvalue=3.7519700037030974e-63), 'nsamples': 750}, 'images': {'pearson': (0.5989202026050527, 3.357219305823283e-74), 'spearman': SpearmanrResult(correlation=0.5961647055574234, pvalue=2.2903033766132675e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.6497040611505341, 3.801263018669683e-91), 'spearman': SpearmanrResult(correlation=0.6927256513166787, pvalue=2.8175745392770555e-108), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5706633198236138, 5.114496940094008e-66), 'spearman': SpearmanrResult(correlation=0.5268692790997734, pvalue=8.442888802267012e-55), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5521625626374372, 'wmean': 0.5654736641850711}, 'spearman': {'mean': 0.5533373644030253, 'wmean': 0.5621412661412827}}}, 'STS15': {'answers-forums': {'pearson': (0.5483895382124534, 7.946945850619116e-31), 'spearman': SpearmanrResult(correlation=0.5489514001555303, pvalue=6.734673568852794e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.5811715833087232, 5.74214699897126e-69), 'spearman': SpearmanrResult(correlation=0.614317074329813, pvalue=5.170136940705285e-79), 'nsamples': 750}, 'belief': {'pearson': (0.6114837348070978, 7.942250385472051e-40), 'spearman': SpearmanrResult(correlation=0.6496474025007081, pvalue=2.484485908172063e-46), 'nsamples': 375}, 'headlines': {'pearson': (0.6524074642921648, 3.852173711657037e-92), 'spearman': SpearmanrResult(correlation=0.6613218523754861, pvalue=1.7131918584728603e-95), 'nsamples': 750}, 'images': {'pearson': (0.6420505169832249, 2.1894745745023557e-88), 'spearman': SpearmanrResult(correlation=0.6752366679702336, pvalue=5.824267216352801e-101), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6071005675207328, 'wmean': 0.6138915502734722}, 'spearman': {'mean': 0.6298948794663544, 'wmean': 0.6375437490009129}}}, 'STS16': {'answer-answer': {'pearson': (0.5100900742091061, 3.1436487916082655e-18), 'spearman': SpearmanrResult(correlation=0.5386407816001103, pvalue=1.635510818109362e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6677562277216929, 1.6281192762400807e-33), 'spearman': SpearmanrResult(correlation=0.7042295237205846, pvalue=1.295154125848685e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7141685476518661, 3.511911248168277e-37), 'spearman': SpearmanrResult(correlation=0.733568603650808, pvalue=4.087550841485165e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.784563035519505, 3.810280570380497e-52), 'spearman': SpearmanrResult(correlation=0.8110114537787537, pvalue=2.862818615982308e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.543074169858039, 1.9760671590223474e-17), 'spearman': SpearmanrResult(correlation=0.5826177908599948, pvalue=2.161596618098791e-20), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6439304109920418, 'wmean': 0.6450496017528103}, 'spearman': {'mean': 0.6740136307220503, 'wmean': 0.6749937620440929}}}, 'MR': {'devacc': 71.97, 'acc': 70.03, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.41, 'acc': 77.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.51, 'acc': 84.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.04, 'acc': 93.51, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.13, 'acc': 78.58, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.78, 'acc': 36.2, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 69.06, 'acc': 79.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.65, 'acc': 71.25, 'f1': 79.66, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 74.53, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7805362427467539, 'pearson': 0.7782882233478061, 'spearman': 0.700950425956238, 'mse': 0.4014285761253279, 'yhat': array([3.14097422, 4.3242277 , 1.66476497, ..., 3.12684169, 4.10763482,        4.34873128]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6791230432983683, 'pearson': 0.6523531741938207, 'spearman': 0.6490943866560729, 'mse': 1.5258546115219729, 'yhat': array([2.90477499, 2.03087166, 2.31354208, ..., 3.88582333, 3.89563486,        3.73240313]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.92, 'acc': 60.91, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 80.55, 'acc': 80.54, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 42.67, 'acc': 42.16, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.01, 'acc': 27.23, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.86, 'acc': 54.3, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 62.47, 'acc': 61.21, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.94, 'acc': 87.75, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.72, 'acc': 80.71, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.09, 'acc': 76.7, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.56, 'acc': 58.43, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 54.52, 'acc': 54.85, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 04:18:52,137 : STS12 p=0.4561, STS12 s=0.5053, STS13 p=0.6125, STS13 s=0.6200, STS14 p=0.5655, STS14 s=0.5621, STS15 p=0.6139, STS15 s=0.6375, STS 16 p=0.6450, STS16 s=0.6750, STS B p=0.6524, STS B s=0.6491, STS B m=1.5259, SICK-R p=0.7783, SICK-R s=0.7010, SICK-P m=0.4014
2019-03-13 04:18:52,137 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 04:18:52,137 : 0.4561,0.5053,0.6125,0.6200,0.5655,0.5621,0.6139,0.6375,0.6450,0.6750,0.6524,0.6491,1.5259,0.7783,0.7010,0.4014
2019-03-13 04:18:52,137 : MR=70.03, CR=77.38, SUBJ=93.51, MPQA=84.77, SST-B=78.58, SST-F=36.20, TREC=79.40, SICK-E=74.53, SNLI=60.91, MRPC=71.25, MRPC f=79.66
2019-03-13 04:18:52,137 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 04:18:52,137 : 70.03,77.38,93.51,84.77,78.58,36.20,79.40,74.53,60.91,71.25,79.66
2019-03-13 04:18:52,137 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 04:18:52,137 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 04:18:52,137 : na,na,na,na,na,na,na,na,na,na
2019-03-13 04:18:52,137 : SentLen=80.54, WC=42.16, TreeDepth=27.23, TopConst=54.30, BShift=61.21, Tense=87.75, SubjNum=80.71, ObjNum=76.70, SOMO=58.43, CoordInv=54.85, average=62.39
2019-03-13 04:18:52,137 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 04:18:52,137 : 80.54,42.16,27.23,54.30,61.21,87.75,80.71,76.70,58.43,54.85,62.39
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,137 : layer 11
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,137 : ********************************************************************************
2019-03-13 04:18:52,224 : ***** Transfer task : STS12 *****


2019-03-13 04:18:52,236 : loading BERT model bert-large-uncased
2019-03-13 04:18:52,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:18:52,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:18:52,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprxzqnaks
2019-03-13 04:18:59,701 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:19:09,043 : MSRpar : pearson = 0.2971, spearman = 0.3772
2019-03-13 04:19:10,678 : MSRvid : pearson = 0.5020, spearman = 0.5382
2019-03-13 04:19:12,083 : SMTeuroparl : pearson = 0.4251, spearman = 0.5941
2019-03-13 04:19:14,765 : surprise.OnWN : pearson = 0.5341, spearman = 0.5816
2019-03-13 04:19:16,185 : surprise.SMTnews : pearson = 0.5709, spearman = 0.5500
2019-03-13 04:19:16,185 : ALL (weighted average) : Pearson = 0.4578,             Spearman = 0.5196
2019-03-13 04:19:16,185 : ALL (average) : Pearson = 0.4658,             Spearman = 0.5283

2019-03-13 04:19:16,185 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 04:19:16,193 : loading BERT model bert-large-uncased
2019-03-13 04:19:16,193 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:19:16,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:19:16,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkdgphc7p
2019-03-13 04:19:23,647 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:19:30,396 : FNWN : pearson = 0.3607, spearman = 0.3395
2019-03-13 04:19:32,278 : headlines : pearson = 0.6500, spearman = 0.6584
2019-03-13 04:19:33,739 : OnWN : pearson = 0.6021, spearman = 0.6293
2019-03-13 04:19:33,739 : ALL (weighted average) : Pearson = 0.5956,             Spearman = 0.6073
2019-03-13 04:19:33,739 : ALL (average) : Pearson = 0.5376,             Spearman = 0.5424

2019-03-13 04:19:33,739 : ***** Transfer task : STS14 *****


2019-03-13 04:19:33,756 : loading BERT model bert-large-uncased
2019-03-13 04:19:33,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:19:33,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:19:33,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprjt1b93s
2019-03-13 04:19:41,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:19:48,032 : deft-forum : pearson = 0.2701, spearman = 0.2912
2019-03-13 04:19:49,659 : deft-news : pearson = 0.6313, spearman = 0.6123
2019-03-13 04:19:51,814 : headlines : pearson = 0.5757, spearman = 0.5520
2019-03-13 04:19:53,881 : images : pearson = 0.6051, spearman = 0.6056
2019-03-13 04:19:56,001 : OnWN : pearson = 0.6352, spearman = 0.6751
2019-03-13 04:19:58,845 : tweet-news : pearson = 0.5296, spearman = 0.4972
2019-03-13 04:19:58,845 : ALL (weighted average) : Pearson = 0.5520,             Spearman = 0.5499
2019-03-13 04:19:58,846 : ALL (average) : Pearson = 0.5412,             Spearman = 0.5389

2019-03-13 04:19:58,846 : ***** Transfer task : STS15 *****


2019-03-13 04:19:58,879 : loading BERT model bert-large-uncased
2019-03-13 04:19:58,879 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:19:58,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:19:58,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdjy9j9pr
2019-03-13 04:20:06,447 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:20:13,660 : answers-forums : pearson = 0.5074, spearman = 0.5183
2019-03-13 04:20:15,731 : answers-students : pearson = 0.5620, spearman = 0.5875
2019-03-13 04:20:17,768 : belief : pearson = 0.5671, spearman = 0.6099
2019-03-13 04:20:20,006 : headlines : pearson = 0.6328, spearman = 0.6482
2019-03-13 04:20:22,126 : images : pearson = 0.6344, spearman = 0.6715
2019-03-13 04:20:22,126 : ALL (weighted average) : Pearson = 0.5916,             Spearman = 0.6178
2019-03-13 04:20:22,126 : ALL (average) : Pearson = 0.5807,             Spearman = 0.6071

2019-03-13 04:20:22,126 : ***** Transfer task : STS16 *****


2019-03-13 04:20:22,194 : loading BERT model bert-large-uncased
2019-03-13 04:20:22,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:20:22,213 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:20:22,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdq7tzkm1
2019-03-13 04:20:29,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:20:35,933 : answer-answer : pearson = 0.5137, spearman = 0.5487
2019-03-13 04:20:36,589 : headlines : pearson = 0.6514, spearman = 0.6893
2019-03-13 04:20:37,464 : plagiarism : pearson = 0.7225, spearman = 0.7425
2019-03-13 04:20:38,946 : postediting : pearson = 0.7446, spearman = 0.7962
2019-03-13 04:20:39,548 : question-question : pearson = 0.5387, spearman = 0.5632
2019-03-13 04:20:39,548 : ALL (weighted average) : Pearson = 0.6350,             Spearman = 0.6693
2019-03-13 04:20:39,549 : ALL (average) : Pearson = 0.6342,             Spearman = 0.6680

2019-03-13 04:20:39,549 : ***** Transfer task : MR *****


2019-03-13 04:20:39,567 : loading BERT model bert-large-uncased
2019-03-13 04:20:39,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:20:39,586 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:20:39,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb1l525ot
2019-03-13 04:20:47,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:20:52,628 : Generating sentence embeddings
2019-03-13 04:21:23,882 : Generated sentence embeddings
2019-03-13 04:21:23,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:21:33,020 : Best param found at split 1: l2reg = 0.001                 with score 71.49
2019-03-13 04:21:45,084 : Best param found at split 2: l2reg = 0.0001                 with score 73.56
2019-03-13 04:21:55,996 : Best param found at split 3: l2reg = 0.0001                 with score 71.68
2019-03-13 04:22:06,239 : Best param found at split 4: l2reg = 1e-05                 with score 70.53
2019-03-13 04:22:15,997 : Best param found at split 5: l2reg = 0.0001                 with score 70.67
2019-03-13 04:22:16,708 : Dev acc : 71.59 Test acc : 68.56

2019-03-13 04:22:16,709 : ***** Transfer task : CR *****


2019-03-13 04:22:16,717 : loading BERT model bert-large-uncased
2019-03-13 04:22:16,717 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:22:16,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:22:16,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4dwj0v9j
2019-03-13 04:22:24,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:22:29,607 : Generating sentence embeddings
2019-03-13 04:22:37,887 : Generated sentence embeddings
2019-03-13 04:22:37,887 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:22:41,783 : Best param found at split 1: l2reg = 0.0001                 with score 76.28
2019-03-13 04:22:45,239 : Best param found at split 2: l2reg = 0.001                 with score 75.39
2019-03-13 04:22:49,212 : Best param found at split 3: l2reg = 0.001                 with score 75.86
2019-03-13 04:22:52,937 : Best param found at split 4: l2reg = 0.001                 with score 75.84
2019-03-13 04:22:56,698 : Best param found at split 5: l2reg = 1e-05                 with score 75.84
2019-03-13 04:22:56,889 : Dev acc : 75.84 Test acc : 76.11

2019-03-13 04:22:56,889 : ***** Transfer task : MPQA *****


2019-03-13 04:22:56,896 : loading BERT model bert-large-uncased
2019-03-13 04:22:56,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:22:56,946 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:22:56,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq8ctbupd
2019-03-13 04:23:04,485 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:23:09,813 : Generating sentence embeddings
2019-03-13 04:23:17,361 : Generated sentence embeddings
2019-03-13 04:23:17,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:23:26,843 : Best param found at split 1: l2reg = 0.001                 with score 81.72
2019-03-13 04:23:36,568 : Best param found at split 2: l2reg = 1e-05                 with score 83.24
2019-03-13 04:23:48,224 : Best param found at split 3: l2reg = 0.001                 with score 84.8
2019-03-13 04:23:59,308 : Best param found at split 4: l2reg = 0.0001                 with score 84.63
2019-03-13 04:24:09,834 : Best param found at split 5: l2reg = 0.0001                 with score 85.48
2019-03-13 04:24:10,214 : Dev acc : 83.97 Test acc : 84.9

2019-03-13 04:24:10,215 : ***** Transfer task : SUBJ *****


2019-03-13 04:24:10,230 : loading BERT model bert-large-uncased
2019-03-13 04:24:10,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:24:10,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:24:10,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeivede5y
2019-03-13 04:24:17,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:24:23,152 : Generating sentence embeddings
2019-03-13 04:24:53,845 : Generated sentence embeddings
2019-03-13 04:24:53,846 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:25:04,110 : Best param found at split 1: l2reg = 1e-05                 with score 93.62
2019-03-13 04:25:14,698 : Best param found at split 2: l2reg = 0.0001                 with score 94.01
2019-03-13 04:25:24,457 : Best param found at split 3: l2reg = 0.0001                 with score 93.36
2019-03-13 04:25:34,521 : Best param found at split 4: l2reg = 0.0001                 with score 94.05
2019-03-13 04:25:43,084 : Best param found at split 5: l2reg = 0.0001                 with score 93.89
2019-03-13 04:25:43,661 : Dev acc : 93.79 Test acc : 93.17

2019-03-13 04:25:43,662 : ***** Transfer task : SST Binary classification *****


2019-03-13 04:25:43,755 : loading BERT model bert-large-uncased
2019-03-13 04:25:43,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:25:43,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:25:43,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptmt9f5bg
2019-03-13 04:25:51,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:25:56,668 : Computing embedding for train
2019-03-13 04:27:35,868 : Computed train embeddings
2019-03-13 04:27:35,868 : Computing embedding for dev
2019-03-13 04:27:38,027 : Computed dev embeddings
2019-03-13 04:27:38,027 : Computing embedding for test
2019-03-13 04:27:42,570 : Computed test embeddings
2019-03-13 04:27:42,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:27:58,701 : [('reg:1e-05', 77.75), ('reg:0.0001', 77.75), ('reg:0.001', 77.52), ('reg:0.01', 75.0)]
2019-03-13 04:27:58,702 : Validation : best param found is reg = 1e-05 with score             77.75
2019-03-13 04:27:58,702 : Evaluating...
2019-03-13 04:28:03,141 : 
Dev acc : 77.75 Test acc : 77.38 for             SST Binary classification

2019-03-13 04:28:03,141 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 04:28:03,191 : loading BERT model bert-large-uncased
2019-03-13 04:28:03,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:28:03,213 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:28:03,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0gxj3211
2019-03-13 04:28:10,724 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:28:16,097 : Computing embedding for train
2019-03-13 04:28:37,849 : Computed train embeddings
2019-03-13 04:28:37,849 : Computing embedding for dev
2019-03-13 04:28:40,686 : Computed dev embeddings
2019-03-13 04:28:40,686 : Computing embedding for test
2019-03-13 04:28:46,274 : Computed test embeddings
2019-03-13 04:28:46,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:28:48,919 : [('reg:1e-05', 37.06), ('reg:0.0001', 36.97), ('reg:0.001', 38.51), ('reg:0.01', 36.6)]
2019-03-13 04:28:48,920 : Validation : best param found is reg = 0.001 with score             38.51
2019-03-13 04:28:48,920 : Evaluating...
2019-03-13 04:28:49,464 : 
Dev acc : 38.51 Test acc : 39.14 for             SST Fine-Grained classification

2019-03-13 04:28:49,464 : ***** Transfer task : TREC *****


2019-03-13 04:28:49,477 : loading BERT model bert-large-uncased
2019-03-13 04:28:49,477 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:28:49,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:28:49,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6ia6o9c_
2019-03-13 04:28:56,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:29:09,838 : Computed train embeddings
2019-03-13 04:29:10,422 : Computed test embeddings
2019-03-13 04:29:10,423 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:29:17,376 : [('reg:1e-05', 64.85), ('reg:0.0001', 67.37), ('reg:0.001', 63.59), ('reg:0.01', 55.27)]
2019-03-13 04:29:17,376 : Cross-validation : best param found is reg = 0.0001             with score 67.37
2019-03-13 04:29:17,376 : Evaluating...
2019-03-13 04:29:17,887 : 
Dev acc : 67.37 Test acc : 82.0             for TREC

2019-03-13 04:29:17,888 : ***** Transfer task : MRPC *****


2019-03-13 04:29:17,911 : loading BERT model bert-large-uncased
2019-03-13 04:29:17,911 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:29:17,932 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:29:17,932 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxhl0507s
2019-03-13 04:29:25,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:29:30,852 : Computing embedding for train
2019-03-13 04:29:52,925 : Computed train embeddings
2019-03-13 04:29:52,925 : Computing embedding for test
2019-03-13 04:30:02,580 : Computed test embeddings
2019-03-13 04:30:02,600 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:30:07,574 : [('reg:1e-05', 69.77), ('reg:0.0001', 69.85), ('reg:0.001', 69.65), ('reg:0.01', 68.4)]
2019-03-13 04:30:07,574 : Cross-validation : best param found is reg = 0.0001             with score 69.85
2019-03-13 04:30:07,574 : Evaluating...
2019-03-13 04:30:07,878 : Dev acc : 69.85 Test acc 66.49; Test F1 79.87 for MRPC.

2019-03-13 04:30:07,878 : ***** Transfer task : SICK-Entailment*****


2019-03-13 04:30:07,939 : loading BERT model bert-large-uncased
2019-03-13 04:30:07,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:30:07,959 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:30:07,959 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4s2blcdj
2019-03-13 04:30:15,407 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:30:20,736 : Computing embedding for train
2019-03-13 04:30:31,946 : Computed train embeddings
2019-03-13 04:30:31,946 : Computing embedding for dev
2019-03-13 04:30:33,471 : Computed dev embeddings
2019-03-13 04:30:33,471 : Computing embedding for test
2019-03-13 04:30:45,463 : Computed test embeddings
2019-03-13 04:30:45,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:30:46,837 : [('reg:1e-05', 65.4), ('reg:0.0001', 72.0), ('reg:0.001', 75.4), ('reg:0.01', 69.0)]
2019-03-13 04:30:46,837 : Validation : best param found is reg = 0.001 with score             75.4
2019-03-13 04:30:46,837 : Evaluating...
2019-03-13 04:30:47,143 : 
Dev acc : 75.4 Test acc : 73.11 for                        SICK entailment

2019-03-13 04:30:47,144 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 04:30:47,171 : loading BERT model bert-large-uncased
2019-03-13 04:30:47,172 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:30:47,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:30:47,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkq7ni7t6
2019-03-13 04:30:54,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:31:00,115 : Computing embedding for train
2019-03-13 04:31:11,328 : Computed train embeddings
2019-03-13 04:31:11,329 : Computing embedding for dev
2019-03-13 04:31:12,857 : Computed dev embeddings
2019-03-13 04:31:12,857 : Computing embedding for test
2019-03-13 04:31:24,850 : Computed test embeddings
2019-03-13 04:31:46,351 : Dev : Pearson 0.7574227065903789
2019-03-13 04:31:46,351 : Test : Pearson 0.7539678203350436 Spearman 0.6559862259729876 MSE 0.4549364200372925                        for SICK Relatedness

2019-03-13 04:31:46,352 : 

***** Transfer task : STSBenchmark*****


2019-03-13 04:31:46,424 : loading BERT model bert-large-uncased
2019-03-13 04:31:46,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:31:46,454 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:31:46,454 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzm0qtlk6
2019-03-13 04:31:53,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:31:59,243 : Computing embedding for train
2019-03-13 04:32:17,654 : Computed train embeddings
2019-03-13 04:32:17,655 : Computing embedding for dev
2019-03-13 04:32:23,237 : Computed dev embeddings
2019-03-13 04:32:23,237 : Computing embedding for test
2019-03-13 04:32:27,796 : Computed test embeddings
2019-03-13 04:32:46,906 : Dev : Pearson 0.6660354227901123
2019-03-13 04:32:46,906 : Test : Pearson 0.6523286432058494 Spearman 0.646772542661976 MSE 1.4710442770038468                        for SICK Relatedness

2019-03-13 04:32:46,906 : ***** Transfer task : SNLI Entailment*****


2019-03-13 04:32:51,954 : loading BERT model bert-large-uncased
2019-03-13 04:32:51,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:32:52,023 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:32:52,023 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkumnh9y4
2019-03-13 04:32:59,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:33:05,242 : PROGRESS (encoding): 0.00%
2019-03-13 04:35:49,030 : PROGRESS (encoding): 14.56%
2019-03-13 04:38:54,964 : PROGRESS (encoding): 29.12%
2019-03-13 04:42:01,533 : PROGRESS (encoding): 43.69%
2019-03-13 04:45:20,356 : PROGRESS (encoding): 58.25%
2019-03-13 04:49:01,807 : PROGRESS (encoding): 72.81%
2019-03-13 04:52:42,229 : PROGRESS (encoding): 87.37%
2019-03-13 04:56:40,817 : PROGRESS (encoding): 0.00%
2019-03-13 04:57:10,869 : PROGRESS (encoding): 0.00%
2019-03-13 04:57:39,751 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:58:06,697 : [('reg:1e-09', 63.34)]
2019-03-13 04:58:06,697 : Validation : best param found is reg = 1e-09 with score             63.34
2019-03-13 04:58:06,697 : Evaluating...
2019-03-13 04:58:33,939 : Dev acc : 63.34 Test acc : 63.21 for SNLI

2019-03-13 04:58:33,940 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 04:58:34,147 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 04:58:35,199 : loading BERT model bert-large-uncased
2019-03-13 04:58:35,200 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:58:35,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:58:35,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp17i5wk0q
2019-03-13 04:58:42,672 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:58:48,125 : Computing embeddings for train/dev/test
2019-03-13 05:02:16,797 : Computed embeddings
2019-03-13 05:02:16,797 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:02:46,176 : [('reg:1e-05', 82.71), ('reg:0.0001', 79.46), ('reg:0.001', 73.88), ('reg:0.01', 60.43)]
2019-03-13 05:02:46,176 : Validation : best param found is reg = 1e-05 with score             82.71
2019-03-13 05:02:46,176 : Evaluating...
2019-03-13 05:02:57,018 : 
Dev acc : 82.7 Test acc : 83.7 for LENGTH classification

2019-03-13 05:02:57,018 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 05:02:57,274 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 05:02:57,321 : loading BERT model bert-large-uncased
2019-03-13 05:02:57,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:02:57,352 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:02:57,352 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyimhrf0e
2019-03-13 05:03:04,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:03:09,929 : Computing embeddings for train/dev/test
2019-03-13 05:06:22,046 : Computed embeddings
2019-03-13 05:06:22,046 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:06:54,957 : [('reg:1e-05', 38.32), ('reg:0.0001', 9.94), ('reg:0.001', 1.22), ('reg:0.01', 0.34)]
2019-03-13 05:06:54,957 : Validation : best param found is reg = 1e-05 with score             38.32
2019-03-13 05:06:54,958 : Evaluating...
2019-03-13 05:07:04,812 : 
Dev acc : 38.3 Test acc : 37.7 for WORDCONTENT classification

2019-03-13 05:07:04,814 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 05:07:05,352 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 05:07:05,419 : loading BERT model bert-large-uncased
2019-03-13 05:07:05,419 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:07:05,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:07:05,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpii14halw
2019-03-13 05:07:12,905 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:07:18,115 : Computing embeddings for train/dev/test
2019-03-13 05:10:18,598 : Computed embeddings
2019-03-13 05:10:18,598 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:10:38,837 : [('reg:1e-05', 27.02), ('reg:0.0001', 27.4), ('reg:0.001', 22.28), ('reg:0.01', 23.27)]
2019-03-13 05:10:38,837 : Validation : best param found is reg = 0.0001 with score             27.4
2019-03-13 05:10:38,837 : Evaluating...
2019-03-13 05:10:43,969 : 
Dev acc : 27.4 Test acc : 27.6 for DEPTH classification

2019-03-13 05:10:43,970 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 05:10:44,341 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 05:10:44,405 : loading BERT model bert-large-uncased
2019-03-13 05:10:44,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:10:44,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:10:44,518 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpytnw_626
2019-03-13 05:10:51,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:10:57,291 : Computing embeddings for train/dev/test
2019-03-13 05:13:44,983 : Computed embeddings
2019-03-13 05:13:44,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:14:19,590 : [('reg:1e-05', 51.41), ('reg:0.0001', 49.41), ('reg:0.001', 42.57), ('reg:0.01', 23.16)]
2019-03-13 05:14:19,590 : Validation : best param found is reg = 1e-05 with score             51.41
2019-03-13 05:14:19,590 : Evaluating...
2019-03-13 05:14:28,445 : 
Dev acc : 51.4 Test acc : 52.2 for TOPCONSTITUENTS classification

2019-03-13 05:14:28,446 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 05:14:28,836 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 05:14:28,904 : loading BERT model bert-large-uncased
2019-03-13 05:14:28,905 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:14:28,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:14:28,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwagg_cst
2019-03-13 05:14:36,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:14:41,716 : Computing embeddings for train/dev/test
2019-03-13 05:17:43,531 : Computed embeddings
2019-03-13 05:17:43,532 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:18:19,779 : [('reg:1e-05', 70.5), ('reg:0.0001', 70.66), ('reg:0.001', 68.25), ('reg:0.01', 60.31)]
2019-03-13 05:18:19,779 : Validation : best param found is reg = 0.0001 with score             70.66
2019-03-13 05:18:19,779 : Evaluating...
2019-03-13 05:18:27,172 : 
Dev acc : 70.7 Test acc : 69.4 for BIGRAMSHIFT classification

2019-03-13 05:18:27,173 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 05:18:27,573 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 05:18:27,639 : loading BERT model bert-large-uncased
2019-03-13 05:18:27,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:18:27,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:18:27,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6c18_7xa
2019-03-13 05:18:35,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:18:40,767 : Computing embeddings for train/dev/test
2019-03-13 05:21:38,448 : Computed embeddings
2019-03-13 05:21:38,449 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:22:02,685 : [('reg:1e-05', 88.54), ('reg:0.0001', 88.55), ('reg:0.001', 88.89), ('reg:0.01', 88.36)]
2019-03-13 05:22:02,685 : Validation : best param found is reg = 0.001 with score             88.89
2019-03-13 05:22:02,685 : Evaluating...
2019-03-13 05:22:09,003 : 
Dev acc : 88.9 Test acc : 87.6 for TENSE classification

2019-03-13 05:22:09,004 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 05:22:09,409 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 05:22:09,471 : loading BERT model bert-large-uncased
2019-03-13 05:22:09,472 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:22:09,587 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:22:09,587 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplb6xw15f
2019-03-13 05:22:17,050 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:22:22,413 : Computing embeddings for train/dev/test
2019-03-13 05:25:30,708 : Computed embeddings
2019-03-13 05:25:30,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:26:00,352 : [('reg:1e-05', 82.25), ('reg:0.0001', 81.48), ('reg:0.001', 82.29), ('reg:0.01', 77.18)]
2019-03-13 05:26:00,352 : Validation : best param found is reg = 0.001 with score             82.29
2019-03-13 05:26:00,352 : Evaluating...
2019-03-13 05:26:09,193 : 
Dev acc : 82.3 Test acc : 81.4 for SUBJNUMBER classification

2019-03-13 05:26:09,194 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 05:26:09,595 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 05:26:09,661 : loading BERT model bert-large-uncased
2019-03-13 05:26:09,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:26:09,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:26:09,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp057insqq
2019-03-13 05:26:17,280 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:26:22,540 : Computing embeddings for train/dev/test
2019-03-13 05:29:27,569 : Computed embeddings
2019-03-13 05:29:27,569 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:29:58,048 : [('reg:1e-05', 78.81), ('reg:0.0001', 78.58), ('reg:0.001', 77.24), ('reg:0.01', 70.84)]
2019-03-13 05:29:58,048 : Validation : best param found is reg = 1e-05 with score             78.81
2019-03-13 05:29:58,049 : Evaluating...
2019-03-13 05:30:06,092 : 
Dev acc : 78.8 Test acc : 79.5 for OBJNUMBER classification

2019-03-13 05:30:06,093 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 05:30:06,661 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 05:30:06,730 : loading BERT model bert-large-uncased
2019-03-13 05:30:06,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:30:06,758 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:30:06,758 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkuf6n74v
2019-03-13 05:30:14,258 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:30:19,685 : Computing embeddings for train/dev/test
2019-03-13 05:33:54,185 : Computed embeddings
2019-03-13 05:33:54,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:34:15,892 : [('reg:1e-05', 59.14), ('reg:0.0001', 59.13), ('reg:0.001', 59.03), ('reg:0.01', 57.98)]
2019-03-13 05:34:15,893 : Validation : best param found is reg = 1e-05 with score             59.14
2019-03-13 05:34:15,893 : Evaluating...
2019-03-13 05:34:21,324 : 
Dev acc : 59.1 Test acc : 59.6 for ODDMANOUT classification

2019-03-13 05:34:21,325 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 05:34:21,699 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 05:34:21,780 : loading BERT model bert-large-uncased
2019-03-13 05:34:21,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:34:21,809 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:34:21,809 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppoc15ect
2019-03-13 05:34:29,257 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:34:34,587 : Computing embeddings for train/dev/test
2019-03-13 05:38:06,856 : Computed embeddings
2019-03-13 05:38:06,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:38:35,907 : [('reg:1e-05', 56.88), ('reg:0.0001', 56.5), ('reg:0.001', 54.6), ('reg:0.01', 50.16)]
2019-03-13 05:38:35,907 : Validation : best param found is reg = 1e-05 with score             56.88
2019-03-13 05:38:35,907 : Evaluating...
2019-03-13 05:38:43,415 : 
Dev acc : 56.9 Test acc : 57.0 for COORDINATIONINVERSION classification

2019-03-13 05:38:43,417 : total results: {'STS12': {'MSRpar': {'pearson': (0.29713681338782494, 9.385607087077367e-17), 'spearman': SpearmanrResult(correlation=0.37724305500511646, pvalue=8.910663478910238e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5019891383977574, 4.010001602653517e-49), 'spearman': SpearmanrResult(correlation=0.5382429615265559, pvalue=1.4806134074225797e-57), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4250969602243938, 1.4450504044837272e-21), 'spearman': SpearmanrResult(correlation=0.59412010584938, pvalue=3.8998480780280477e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5340725699005673, 1.5603043454599774e-56), 'spearman': SpearmanrResult(correlation=0.5816364971574984, pvalue=4.227487855193409e-69), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5708937768739355, 6.96228988546385e-36), 'spearman': SpearmanrResult(correlation=0.5500160861162856, pvalue=6.320631713835546e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.46583785175689574, 'wmean': 0.45778797071438526}, 'spearman': {'mean': 0.5282517411309673, 'wmean': 0.5196265869408435}}}, 'STS13': {'FNWN': {'pearson': (0.36067443125082277, 3.428840339897676e-07), 'spearman': SpearmanrResult(correlation=0.3394956680711054, pvalue=1.7613608797875901e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.6500214640313247, 2.908867785806382e-91), 'spearman': SpearmanrResult(correlation=0.6583564686912788, pvalue=2.2989922132001928e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.6021132882537776, 1.2346073047115647e-56), 'spearman': SpearmanrResult(correlation=0.6292567440658108, pvalue=3.4283722793214384e-63), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5376030611786417, 'wmean': 0.5956460801601788}, 'spearman': {'mean': 0.5423696269427317, 'wmean': 0.607296710803212}}}, 'STS14': {'deft-forum': {'pearson': (0.2700981237000352, 5.805339007679371e-09), 'spearman': SpearmanrResult(correlation=0.2912287043077977, pvalue=3.0203038143754843e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.6312885781321395, 9.235746930740993e-35), 'spearman': SpearmanrResult(correlation=0.6123072195547398, pvalue=2.94787039496993e-32), 'nsamples': 300}, 'headlines': {'pearson': (0.5757331500614425, 1.9906969934645937e-67), 'spearman': SpearmanrResult(correlation=0.5520358945280338, pvalue=4.828220390661358e-61), 'nsamples': 750}, 'images': {'pearson': (0.6050523401628881, 4.373251422192515e-76), 'spearman': SpearmanrResult(correlation=0.6055803600528001, pvalue=2.996089377122582e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.6351910647028458, 5.6000692101617645e-86), 'spearman': SpearmanrResult(correlation=0.6751058439168856, pvalue=6.577440077477917e-101), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5296119238591682, 1.8683687905736728e-55), 'spearman': SpearmanrResult(correlation=0.497180833098448, pvalue=4.437911896242412e-48), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5411625301030866, 'wmean': 0.5520325568518444}, 'spearman': {'mean': 0.5389064759097842, 'wmean': 0.5499126084005485}}}, 'STS15': {'answers-forums': {'pearson': (0.507425393365475, 6.225258985836596e-26), 'spearman': SpearmanrResult(correlation=0.5182796507078404, pvalue=3.6433716992228546e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.5619504364711565, 1.1889063557084769e-63), 'spearman': SpearmanrResult(correlation=0.5875277939969558, pvalue=8.358303848746972e-71), 'nsamples': 750}, 'belief': {'pearson': (0.5670666626042269, 2.729516522881723e-33), 'spearman': SpearmanrResult(correlation=0.6099161015629447, pvalue=1.4072370635729045e-39), 'nsamples': 375}, 'headlines': {'pearson': (0.6327915000357179, 3.7685420870205135e-85), 'spearman': SpearmanrResult(correlation=0.6482213989453558, pvalue=1.3209992037173035e-90), 'nsamples': 750}, 'images': {'pearson': (0.6343648692008323, 1.081690714726274e-85), 'spearman': SpearmanrResult(correlation=0.6715425311459333, pvalue=1.7627341337018996e-99), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5807197723354818, 'wmean': 0.5915882084231395}, 'spearman': {'mean': 0.607097495271806, 'wmean': 0.6178474000559093}}}, 'STS16': {'answer-answer': {'pearson': (0.5137366510382053, 1.6503565978998329e-18), 'spearman': SpearmanrResult(correlation=0.5486700128416901, pvalue=2.2840303380518813e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6514081504759446, 1.862785307065225e-31), 'spearman': SpearmanrResult(correlation=0.6893152462425918, pvalue=1.943626210605945e-36), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7225253037078854, 2.0524696581471062e-38), 'spearman': SpearmanrResult(correlation=0.7425321882738292, pvalue=1.4698922823932887e-41), 'nsamples': 230}, 'postediting': {'pearson': (0.7446253855831052, 2.2423140680904325e-44), 'spearman': SpearmanrResult(correlation=0.7961642151563205, pvalue=1.014756406790609e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.5387365021944494, 3.9567881257617926e-17), 'spearman': SpearmanrResult(correlation=0.5632468908360782, pvalue=6.833583336587813e-19), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.634206398599918, 'wmean': 0.63503801157331}, 'spearman': {'mean': 0.6679857106701019, 'wmean': 0.6692805662411955}}}, 'MR': {'devacc': 71.59, 'acc': 68.56, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.84, 'acc': 76.11, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.97, 'acc': 84.9, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.79, 'acc': 93.17, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.75, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.51, 'acc': 39.14, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.37, 'acc': 82.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.85, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.4, 'acc': 73.11, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7574227065903789, 'pearson': 0.7539678203350436, 'spearman': 0.6559862259729876, 'mse': 0.4549364200372925, 'yhat': array([3.76867586, 4.83568651, 1.46783503, ..., 3.10669692, 4.17676564,        4.34203993]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6660354227901123, 'pearson': 0.6523286432058494, 'spearman': 0.646772542661976, 'mse': 1.4710442770038468, 'yhat': array([2.20515603, 1.88903738, 2.28900208, ..., 3.85911352, 3.8395741 ,        3.65780499]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.34, 'acc': 63.21, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 82.71, 'acc': 83.71, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 38.32, 'acc': 37.67, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.4, 'acc': 27.55, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 51.41, 'acc': 52.25, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 70.66, 'acc': 69.41, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.89, 'acc': 87.63, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.29, 'acc': 81.39, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.81, 'acc': 79.47, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.14, 'acc': 59.65, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.88, 'acc': 56.99, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 05:38:43,417 : STS12 p=0.4578, STS12 s=0.5196, STS13 p=0.5956, STS13 s=0.6073, STS14 p=0.5520, STS14 s=0.5499, STS15 p=0.5916, STS15 s=0.6178, STS 16 p=0.6350, STS16 s=0.6693, STS B p=0.6523, STS B s=0.6468, STS B m=1.4710, SICK-R p=0.7540, SICK-R s=0.6560, SICK-P m=0.4549
2019-03-13 05:38:43,417 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 05:38:43,417 : 0.4578,0.5196,0.5956,0.6073,0.5520,0.5499,0.5916,0.6178,0.6350,0.6693,0.6523,0.6468,1.4710,0.7540,0.6560,0.4549
2019-03-13 05:38:43,417 : MR=68.56, CR=76.11, SUBJ=93.17, MPQA=84.90, SST-B=77.38, SST-F=39.14, TREC=82.00, SICK-E=73.11, SNLI=63.21, MRPC=66.49, MRPC f=79.87
2019-03-13 05:38:43,417 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 05:38:43,417 : 68.56,76.11,93.17,84.90,77.38,39.14,82.00,73.11,63.21,66.49,79.87
2019-03-13 05:38:43,417 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 05:38:43,417 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 05:38:43,417 : na,na,na,na,na,na,na,na,na,na
2019-03-13 05:38:43,417 : SentLen=83.71, WC=37.67, TreeDepth=27.55, TopConst=52.25, BShift=69.41, Tense=87.63, SubjNum=81.39, ObjNum=79.47, SOMO=59.65, CoordInv=56.99, average=63.57
2019-03-13 05:38:43,417 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 05:38:43,417 : 83.71,37.67,27.55,52.25,69.41,87.63,81.39,79.47,59.65,56.99,63.57
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,417 : layer 12
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,417 : ********************************************************************************
2019-03-13 05:38:43,509 : ***** Transfer task : STS12 *****


2019-03-13 05:38:43,521 : loading BERT model bert-large-uncased
2019-03-13 05:38:43,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:38:43,538 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:38:43,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_fdi3vdz
2019-03-13 05:38:50,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:39:00,311 : MSRpar : pearson = 0.2767, spearman = 0.3534
2019-03-13 05:39:01,940 : MSRvid : pearson = 0.4906, spearman = 0.5337
2019-03-13 05:39:03,342 : SMTeuroparl : pearson = 0.4344, spearman = 0.5935
2019-03-13 05:39:06,019 : surprise.OnWN : pearson = 0.5312, spearman = 0.5765
2019-03-13 05:39:07,436 : surprise.SMTnews : pearson = 0.5678, spearman = 0.5190
2019-03-13 05:39:07,437 : ALL (weighted average) : Pearson = 0.4504,             Spearman = 0.5075
2019-03-13 05:39:07,437 : ALL (average) : Pearson = 0.4601,             Spearman = 0.5152

2019-03-13 05:39:07,437 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 05:39:07,445 : loading BERT model bert-large-uncased
2019-03-13 05:39:07,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:39:07,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:39:07,463 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8sa_t5wm
2019-03-13 05:39:14,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:39:21,634 : FNWN : pearson = 0.3672, spearman = 0.3362
2019-03-13 05:39:23,514 : headlines : pearson = 0.6385, spearman = 0.6384
2019-03-13 05:39:24,970 : OnWN : pearson = 0.5797, spearman = 0.5946
2019-03-13 05:39:24,970 : ALL (weighted average) : Pearson = 0.5823,             Spearman = 0.5839
2019-03-13 05:39:24,970 : ALL (average) : Pearson = 0.5285,             Spearman = 0.5231

2019-03-13 05:39:24,971 : ***** Transfer task : STS14 *****


2019-03-13 05:39:24,985 : loading BERT model bert-large-uncased
2019-03-13 05:39:24,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:39:25,003 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:39:25,003 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2b7ej3c8
2019-03-13 05:39:32,479 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:39:38,999 : deft-forum : pearson = 0.2479, spearman = 0.2486
2019-03-13 05:39:40,623 : deft-news : pearson = 0.6282, spearman = 0.6093
2019-03-13 05:39:42,776 : headlines : pearson = 0.5651, spearman = 0.5383
2019-03-13 05:39:44,834 : images : pearson = 0.5745, spearman = 0.5597
2019-03-13 05:39:46,948 : OnWN : pearson = 0.6252, spearman = 0.6603
2019-03-13 05:39:49,785 : tweet-news : pearson = 0.5292, spearman = 0.4912
2019-03-13 05:39:49,785 : ALL (weighted average) : Pearson = 0.5388,             Spearman = 0.5285
2019-03-13 05:39:49,785 : ALL (average) : Pearson = 0.5283,             Spearman = 0.5179

2019-03-13 05:39:49,785 : ***** Transfer task : STS15 *****


2019-03-13 05:39:49,816 : loading BERT model bert-large-uncased
2019-03-13 05:39:49,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:39:49,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:39:49,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxccaabjg
2019-03-13 05:39:57,275 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:40:04,257 : answers-forums : pearson = 0.4829, spearman = 0.4917
2019-03-13 05:40:06,327 : answers-students : pearson = 0.5625, spearman = 0.5849
2019-03-13 05:40:08,359 : belief : pearson = 0.5617, spearman = 0.6065
2019-03-13 05:40:10,596 : headlines : pearson = 0.6276, spearman = 0.6369
2019-03-13 05:40:12,713 : images : pearson = 0.6274, spearman = 0.6597
2019-03-13 05:40:12,714 : ALL (weighted average) : Pearson = 0.5849,             Spearman = 0.6076
2019-03-13 05:40:12,714 : ALL (average) : Pearson = 0.5724,             Spearman = 0.5959

2019-03-13 05:40:12,714 : ***** Transfer task : STS16 *****


2019-03-13 05:40:12,784 : loading BERT model bert-large-uncased
2019-03-13 05:40:12,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:40:12,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:40:12,802 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp06_f57zt
2019-03-13 05:40:20,261 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:40:26,456 : answer-answer : pearson = 0.5073, spearman = 0.5369
2019-03-13 05:40:27,113 : headlines : pearson = 0.6474, spearman = 0.6831
2019-03-13 05:40:27,990 : plagiarism : pearson = 0.7209, spearman = 0.7519
2019-03-13 05:40:29,473 : postediting : pearson = 0.7652, spearman = 0.8117
2019-03-13 05:40:30,075 : question-question : pearson = 0.5327, spearman = 0.5517
2019-03-13 05:40:30,075 : ALL (weighted average) : Pearson = 0.6357,             Spearman = 0.6684
2019-03-13 05:40:30,075 : ALL (average) : Pearson = 0.6347,             Spearman = 0.6671

2019-03-13 05:40:30,075 : ***** Transfer task : MR *****


2019-03-13 05:40:30,090 : loading BERT model bert-large-uncased
2019-03-13 05:40:30,090 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:40:30,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:40:30,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphneeo4cu
2019-03-13 05:40:37,639 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:40:43,139 : Generating sentence embeddings
2019-03-13 05:41:14,367 : Generated sentence embeddings
2019-03-13 05:41:14,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:41:23,620 : Best param found at split 1: l2reg = 0.001                 with score 72.29
2019-03-13 05:41:35,086 : Best param found at split 2: l2reg = 0.0001                 with score 74.01
2019-03-13 05:41:45,459 : Best param found at split 3: l2reg = 0.001                 with score 72.83
2019-03-13 05:41:56,219 : Best param found at split 4: l2reg = 0.001                 with score 71.52
2019-03-13 05:42:06,386 : Best param found at split 5: l2reg = 1e-05                 with score 71.52
2019-03-13 05:42:07,097 : Dev acc : 72.43 Test acc : 70.49

2019-03-13 05:42:07,098 : ***** Transfer task : CR *****


2019-03-13 05:42:07,106 : loading BERT model bert-large-uncased
2019-03-13 05:42:07,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:42:07,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:42:07,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzqq8mtga
2019-03-13 05:42:14,660 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:42:20,039 : Generating sentence embeddings
2019-03-13 05:42:28,311 : Generated sentence embeddings
2019-03-13 05:42:28,311 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:42:32,425 : Best param found at split 1: l2reg = 1e-05                 with score 75.65
2019-03-13 05:42:36,345 : Best param found at split 2: l2reg = 1e-05                 with score 75.55
2019-03-13 05:42:40,057 : Best param found at split 3: l2reg = 1e-05                 with score 76.42
2019-03-13 05:42:43,802 : Best param found at split 4: l2reg = 0.001                 with score 76.53
2019-03-13 05:42:47,402 : Best param found at split 5: l2reg = 1e-05                 with score 76.1
2019-03-13 05:42:47,593 : Dev acc : 76.05 Test acc : 75.41

2019-03-13 05:42:47,594 : ***** Transfer task : MPQA *****


2019-03-13 05:42:47,600 : loading BERT model bert-large-uncased
2019-03-13 05:42:47,600 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:42:47,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:42:47,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpppxq9vme
2019-03-13 05:42:55,164 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:43:00,586 : Generating sentence embeddings
2019-03-13 05:43:08,129 : Generated sentence embeddings
2019-03-13 05:43:08,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:43:18,428 : Best param found at split 1: l2reg = 1e-05                 with score 82.3
2019-03-13 05:43:30,229 : Best param found at split 2: l2reg = 1e-05                 with score 84.1
2019-03-13 05:43:40,341 : Best param found at split 3: l2reg = 1e-05                 with score 84.08
2019-03-13 05:43:52,132 : Best param found at split 4: l2reg = 0.0001                 with score 83.03
2019-03-13 05:44:04,037 : Best param found at split 5: l2reg = 0.0001                 with score 84.96
2019-03-13 05:44:04,728 : Dev acc : 83.69 Test acc : 85.21

2019-03-13 05:44:04,728 : ***** Transfer task : SUBJ *****


2019-03-13 05:44:04,745 : loading BERT model bert-large-uncased
2019-03-13 05:44:04,745 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:44:04,765 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:44:04,765 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsj2yy3u5
2019-03-13 05:44:12,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:44:17,771 : Generating sentence embeddings
2019-03-13 05:44:48,449 : Generated sentence embeddings
2019-03-13 05:44:48,449 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:44:59,483 : Best param found at split 1: l2reg = 1e-05                 with score 94.36
2019-03-13 05:45:09,896 : Best param found at split 2: l2reg = 1e-05                 with score 94.35
2019-03-13 05:45:20,100 : Best param found at split 3: l2reg = 0.0001                 with score 93.82
2019-03-13 05:45:31,632 : Best param found at split 4: l2reg = 1e-05                 with score 94.55
2019-03-13 05:45:42,771 : Best param found at split 5: l2reg = 0.0001                 with score 94.39
2019-03-13 05:45:43,188 : Dev acc : 94.29 Test acc : 93.37

2019-03-13 05:45:43,189 : ***** Transfer task : SST Binary classification *****


2019-03-13 05:45:43,282 : loading BERT model bert-large-uncased
2019-03-13 05:45:43,283 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:45:43,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:45:43,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmuh_09mn
2019-03-13 05:45:50,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:45:56,287 : Computing embedding for train
2019-03-13 05:47:35,372 : Computed train embeddings
2019-03-13 05:47:35,372 : Computing embedding for dev
2019-03-13 05:47:37,529 : Computed dev embeddings
2019-03-13 05:47:37,529 : Computing embedding for test
2019-03-13 05:47:42,062 : Computed test embeddings
2019-03-13 05:47:42,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:47:57,203 : [('reg:1e-05', 77.87), ('reg:0.0001', 77.64), ('reg:0.001', 78.1), ('reg:0.01', 77.06)]
2019-03-13 05:47:57,203 : Validation : best param found is reg = 0.001 with score             78.1
2019-03-13 05:47:57,203 : Evaluating...
2019-03-13 05:48:00,912 : 
Dev acc : 78.1 Test acc : 77.32 for             SST Binary classification

2019-03-13 05:48:00,913 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 05:48:00,967 : loading BERT model bert-large-uncased
2019-03-13 05:48:00,968 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:48:00,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:48:00,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9q2ufshn
2019-03-13 05:48:08,478 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:48:13,727 : Computing embedding for train
2019-03-13 05:48:35,456 : Computed train embeddings
2019-03-13 05:48:35,456 : Computing embedding for dev
2019-03-13 05:48:38,291 : Computed dev embeddings
2019-03-13 05:48:38,291 : Computing embedding for test
2019-03-13 05:48:43,870 : Computed test embeddings
2019-03-13 05:48:43,870 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:48:46,313 : [('reg:1e-05', 37.15), ('reg:0.0001', 35.24), ('reg:0.001', 38.6), ('reg:0.01', 34.24)]
2019-03-13 05:48:46,313 : Validation : best param found is reg = 0.001 with score             38.6
2019-03-13 05:48:46,313 : Evaluating...
2019-03-13 05:48:46,862 : 
Dev acc : 38.6 Test acc : 41.86 for             SST Fine-Grained classification

2019-03-13 05:48:46,862 : ***** Transfer task : TREC *****


2019-03-13 05:48:46,876 : loading BERT model bert-large-uncased
2019-03-13 05:48:46,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:48:46,895 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:48:46,895 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmw1vvsrc
2019-03-13 05:48:54,363 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:49:07,323 : Computed train embeddings
2019-03-13 05:49:07,908 : Computed test embeddings
2019-03-13 05:49:07,908 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 05:49:14,265 : [('reg:1e-05', 68.03), ('reg:0.0001', 67.91), ('reg:0.001', 68.32), ('reg:0.01', 56.01)]
2019-03-13 05:49:14,265 : Cross-validation : best param found is reg = 0.001             with score 68.32
2019-03-13 05:49:14,265 : Evaluating...
2019-03-13 05:49:14,747 : 
Dev acc : 68.32 Test acc : 88.2             for TREC

2019-03-13 05:49:14,748 : ***** Transfer task : MRPC *****


2019-03-13 05:49:14,770 : loading BERT model bert-large-uncased
2019-03-13 05:49:14,770 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:49:14,793 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:49:14,793 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprtq5a95w
2019-03-13 05:49:22,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:49:27,472 : Computing embedding for train
2019-03-13 05:49:49,550 : Computed train embeddings
2019-03-13 05:49:49,550 : Computing embedding for test
2019-03-13 05:49:59,199 : Computed test embeddings
2019-03-13 05:49:59,221 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 05:50:03,977 : [('reg:1e-05', 69.97), ('reg:0.0001', 69.55), ('reg:0.001', 68.99), ('reg:0.01', 69.43)]
2019-03-13 05:50:03,977 : Cross-validation : best param found is reg = 1e-05             with score 69.97
2019-03-13 05:50:03,977 : Evaluating...
2019-03-13 05:50:04,262 : Dev acc : 69.97 Test acc 68.7; Test F1 80.67 for MRPC.

2019-03-13 05:50:04,262 : ***** Transfer task : SICK-Entailment*****


2019-03-13 05:50:04,325 : loading BERT model bert-large-uncased
2019-03-13 05:50:04,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:50:04,344 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:50:04,345 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo0x9tslt
2019-03-13 05:50:11,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:50:17,221 : Computing embedding for train
2019-03-13 05:50:28,438 : Computed train embeddings
2019-03-13 05:50:28,438 : Computing embedding for dev
2019-03-13 05:50:29,966 : Computed dev embeddings
2019-03-13 05:50:29,966 : Computing embedding for test
2019-03-13 05:50:41,979 : Computed test embeddings
2019-03-13 05:50:42,016 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:50:43,696 : [('reg:1e-05', 75.2), ('reg:0.0001', 75.4), ('reg:0.001', 75.2), ('reg:0.01', 73.8)]
2019-03-13 05:50:43,697 : Validation : best param found is reg = 0.0001 with score             75.4
2019-03-13 05:50:43,697 : Evaluating...
2019-03-13 05:50:44,093 : 
Dev acc : 75.4 Test acc : 73.03 for                        SICK entailment

2019-03-13 05:50:44,094 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 05:50:44,120 : loading BERT model bert-large-uncased
2019-03-13 05:50:44,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:50:44,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:50:44,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgz08a3r2
2019-03-13 05:50:51,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:50:56,950 : Computing embedding for train
2019-03-13 05:51:08,152 : Computed train embeddings
2019-03-13 05:51:08,152 : Computing embedding for dev
2019-03-13 05:51:09,680 : Computed dev embeddings
2019-03-13 05:51:09,681 : Computing embedding for test
2019-03-13 05:51:21,690 : Computed test embeddings
2019-03-13 05:51:37,205 : Dev : Pearson 0.7377278104210847
2019-03-13 05:51:37,205 : Test : Pearson 0.761975917856173 Spearman 0.6798149754629706 MSE 0.4301519133438932                        for SICK Relatedness

2019-03-13 05:51:37,206 : 

***** Transfer task : STSBenchmark*****


2019-03-13 05:51:37,275 : loading BERT model bert-large-uncased
2019-03-13 05:51:37,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:51:37,296 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:51:37,296 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpghnprn0m
2019-03-13 05:51:44,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:51:50,071 : Computing embedding for train
2019-03-13 05:52:08,545 : Computed train embeddings
2019-03-13 05:52:08,545 : Computing embedding for dev
2019-03-13 05:52:14,137 : Computed dev embeddings
2019-03-13 05:52:14,137 : Computing embedding for test
2019-03-13 05:52:18,694 : Computed test embeddings
2019-03-13 05:52:36,678 : Dev : Pearson 0.6613741006510734
2019-03-13 05:52:36,679 : Test : Pearson 0.6287377219792188 Spearman 0.6221295709154164 MSE 1.5566513328531442                        for SICK Relatedness

2019-03-13 05:52:36,679 : ***** Transfer task : SNLI Entailment*****


2019-03-13 05:52:41,789 : loading BERT model bert-large-uncased
2019-03-13 05:52:41,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:52:41,908 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:52:41,909 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8sfbi4rd
2019-03-13 05:52:49,354 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:52:55,246 : PROGRESS (encoding): 0.00%
2019-03-13 05:55:39,042 : PROGRESS (encoding): 14.56%
2019-03-13 05:58:44,844 : PROGRESS (encoding): 29.12%
2019-03-13 06:01:51,464 : PROGRESS (encoding): 43.69%
2019-03-13 06:05:10,259 : PROGRESS (encoding): 58.25%
2019-03-13 06:08:51,711 : PROGRESS (encoding): 72.81%
2019-03-13 06:12:32,172 : PROGRESS (encoding): 87.37%
2019-03-13 06:16:30,644 : PROGRESS (encoding): 0.00%
2019-03-13 06:17:00,679 : PROGRESS (encoding): 0.00%
2019-03-13 06:17:29,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:18:05,878 : [('reg:1e-09', 66.98)]
2019-03-13 06:18:05,879 : Validation : best param found is reg = 1e-09 with score             66.98
2019-03-13 06:18:05,879 : Evaluating...
2019-03-13 06:18:42,838 : Dev acc : 66.98 Test acc : 67.02 for SNLI

2019-03-13 06:18:42,838 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 06:18:43,039 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 06:18:44,105 : loading BERT model bert-large-uncased
2019-03-13 06:18:44,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:18:44,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:18:44,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeb2j0drp
2019-03-13 06:18:51,607 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:18:57,067 : Computing embeddings for train/dev/test
2019-03-13 06:22:25,730 : Computed embeddings
2019-03-13 06:22:25,731 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:22:58,492 : [('reg:1e-05', 80.78), ('reg:0.0001', 78.27), ('reg:0.001', 74.03), ('reg:0.01', 55.99)]
2019-03-13 06:22:58,492 : Validation : best param found is reg = 1e-05 with score             80.78
2019-03-13 06:22:58,492 : Evaluating...
2019-03-13 06:23:09,345 : 
Dev acc : 80.8 Test acc : 81.0 for LENGTH classification

2019-03-13 06:23:09,346 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 06:23:09,724 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 06:23:09,769 : loading BERT model bert-large-uncased
2019-03-13 06:23:09,769 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:23:09,798 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:23:09,798 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4mqfpegd
2019-03-13 06:23:17,307 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:23:22,699 : Computing embeddings for train/dev/test
2019-03-13 06:26:34,841 : Computed embeddings
2019-03-13 06:26:34,841 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:27:02,876 : [('reg:1e-05', 36.35), ('reg:0.0001', 8.64), ('reg:0.001', 1.35), ('reg:0.01', 0.42)]
2019-03-13 06:27:02,876 : Validation : best param found is reg = 1e-05 with score             36.35
2019-03-13 06:27:02,876 : Evaluating...
2019-03-13 06:27:12,658 : 
Dev acc : 36.4 Test acc : 35.7 for WORDCONTENT classification

2019-03-13 06:27:12,660 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 06:27:13,036 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 06:27:13,101 : loading BERT model bert-large-uncased
2019-03-13 06:27:13,102 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:27:13,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:27:13,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuak9s4z3
2019-03-13 06:27:20,577 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:27:25,901 : Computing embeddings for train/dev/test
2019-03-13 06:30:26,486 : Computed embeddings
2019-03-13 06:30:26,486 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:30:51,079 : [('reg:1e-05', 28.58), ('reg:0.0001', 29.13), ('reg:0.001', 27.28), ('reg:0.01', 20.24)]
2019-03-13 06:30:51,079 : Validation : best param found is reg = 0.0001 with score             29.13
2019-03-13 06:30:51,079 : Evaluating...
2019-03-13 06:30:57,588 : 
Dev acc : 29.1 Test acc : 30.0 for DEPTH classification

2019-03-13 06:30:57,589 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 06:30:57,970 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 06:30:58,034 : loading BERT model bert-large-uncased
2019-03-13 06:30:58,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:30:58,141 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:30:58,141 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm8ji9j12
2019-03-13 06:31:05,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:31:10,962 : Computing embeddings for train/dev/test
2019-03-13 06:33:58,275 : Computed embeddings
2019-03-13 06:33:58,275 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:34:25,385 : [('reg:1e-05', 68.15), ('reg:0.0001', 65.59), ('reg:0.001', 63.38), ('reg:0.01', 55.25)]
2019-03-13 06:34:25,385 : Validation : best param found is reg = 1e-05 with score             68.15
2019-03-13 06:34:25,385 : Evaluating...
2019-03-13 06:34:32,171 : 
Dev acc : 68.2 Test acc : 68.3 for TOPCONSTITUENTS classification

2019-03-13 06:34:32,172 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 06:34:32,532 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 06:34:32,602 : loading BERT model bert-large-uncased
2019-03-13 06:34:32,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:34:32,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:34:32,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_nm34rqe
2019-03-13 06:34:40,258 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:34:45,560 : Computing embeddings for train/dev/test
2019-03-13 06:37:47,598 : Computed embeddings
2019-03-13 06:37:47,598 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:38:19,396 : [('reg:1e-05', 79.98), ('reg:0.0001', 79.89), ('reg:0.001', 77.63), ('reg:0.01', 67.85)]
2019-03-13 06:38:19,397 : Validation : best param found is reg = 1e-05 with score             79.98
2019-03-13 06:38:19,397 : Evaluating...
2019-03-13 06:38:26,922 : 
Dev acc : 80.0 Test acc : 78.8 for BIGRAMSHIFT classification

2019-03-13 06:38:26,923 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 06:38:27,489 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 06:38:27,555 : loading BERT model bert-large-uncased
2019-03-13 06:38:27,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:38:27,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:38:27,585 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0db350h_
2019-03-13 06:38:35,044 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:38:40,337 : Computing embeddings for train/dev/test
2019-03-13 06:41:38,299 : Computed embeddings
2019-03-13 06:41:38,299 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:42:03,268 : [('reg:1e-05', 88.99), ('reg:0.0001', 89.06), ('reg:0.001', 88.8), ('reg:0.01', 88.25)]
2019-03-13 06:42:03,268 : Validation : best param found is reg = 0.0001 with score             89.06
2019-03-13 06:42:03,268 : Evaluating...
2019-03-13 06:42:09,589 : 
Dev acc : 89.1 Test acc : 88.1 for TENSE classification

2019-03-13 06:42:09,590 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 06:42:10,007 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 06:42:10,070 : loading BERT model bert-large-uncased
2019-03-13 06:42:10,070 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:42:10,094 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:42:10,094 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8rqxm9ym
2019-03-13 06:42:17,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:42:22,945 : Computing embeddings for train/dev/test
2019-03-13 06:45:31,416 : Computed embeddings
2019-03-13 06:45:31,416 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:46:02,720 : [('reg:1e-05', 85.92), ('reg:0.0001', 86.08), ('reg:0.001', 85.57), ('reg:0.01', 72.1)]
2019-03-13 06:46:02,721 : Validation : best param found is reg = 0.0001 with score             86.08
2019-03-13 06:46:02,721 : Evaluating...
2019-03-13 06:46:11,264 : 
Dev acc : 86.1 Test acc : 85.0 for SUBJNUMBER classification

2019-03-13 06:46:11,265 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 06:46:11,671 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 06:46:11,738 : loading BERT model bert-large-uncased
2019-03-13 06:46:11,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:46:11,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:46:11,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpczx22bep
2019-03-13 06:46:19,313 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:46:24,665 : Computing embeddings for train/dev/test
2019-03-13 06:49:29,735 : Computed embeddings
2019-03-13 06:49:29,735 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:49:57,682 : [('reg:1e-05', 76.69), ('reg:0.0001', 78.69), ('reg:0.001', 78.76), ('reg:0.01', 69.96)]
2019-03-13 06:49:57,682 : Validation : best param found is reg = 0.001 with score             78.76
2019-03-13 06:49:57,682 : Evaluating...
2019-03-13 06:50:06,425 : 
Dev acc : 78.8 Test acc : 78.9 for OBJNUMBER classification

2019-03-13 06:50:06,426 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 06:50:07,067 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 06:50:07,142 : loading BERT model bert-large-uncased
2019-03-13 06:50:07,142 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:50:07,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:50:07,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpigdb_g02
2019-03-13 06:50:14,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:50:20,170 : Computing embeddings for train/dev/test
2019-03-13 06:53:54,810 : Computed embeddings
2019-03-13 06:53:54,810 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:54:16,523 : [('reg:1e-05', 58.58), ('reg:0.0001', 58.48), ('reg:0.001', 58.46), ('reg:0.01', 58.06)]
2019-03-13 06:54:16,523 : Validation : best param found is reg = 1e-05 with score             58.58
2019-03-13 06:54:16,523 : Evaluating...
2019-03-13 06:54:21,912 : 
Dev acc : 58.6 Test acc : 59.2 for ODDMANOUT classification

2019-03-13 06:54:21,913 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 06:54:22,302 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 06:54:22,378 : loading BERT model bert-large-uncased
2019-03-13 06:54:22,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:54:22,501 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:54:22,501 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp80s8gdj4
2019-03-13 06:54:29,996 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:54:36,157 : Computing embeddings for train/dev/test
2019-03-13 06:58:08,732 : Computed embeddings
2019-03-13 06:58:08,732 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:58:35,921 : [('reg:1e-05', 55.09), ('reg:0.0001', 54.94), ('reg:0.001', 54.49), ('reg:0.01', 56.19)]
2019-03-13 06:58:35,921 : Validation : best param found is reg = 0.01 with score             56.19
2019-03-13 06:58:35,922 : Evaluating...
2019-03-13 06:58:43,545 : 
Dev acc : 56.2 Test acc : 56.5 for COORDINATIONINVERSION classification

2019-03-13 06:58:43,547 : total results: {'STS12': {'MSRpar': {'pearson': (0.2766689434515695, 1.2064791496920992e-14), 'spearman': SpearmanrResult(correlation=0.35337517083926157, pvalue=1.7661916228430485e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4906013644826721, 1.1197536067049554e-46), 'spearman': SpearmanrResult(correlation=0.5337301111984035, pvalue=1.890436719246909e-56), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.43435606901588436, 1.5210544701738959e-22), 'spearman': SpearmanrResult(correlation=0.5934656465142891, pvalue=5.1363595373923525e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5312478456794141, 7.548931199491531e-56), 'spearman': SpearmanrResult(correlation=0.5765040233685728, pvalue=1.2091533987330817e-67), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5678126158703654, 1.961582701349845e-35), 'spearman': SpearmanrResult(correlation=0.5190025901508274, pvalue=6.74366911795313e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.46013736769998104, 'wmean': 0.4503910182177633}, 'spearman': {'mean': 0.5152155084142709, 'wmean': 0.5074613076817623}}}, 'STS13': {'FNWN': {'pearson': (0.36716545504528336, 2.026761788631012e-07), 'spearman': SpearmanrResult(correlation=0.3361707380515774, pvalue=2.2530291570484076e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.6384545528072875, 4.076294216083298e-87), 'spearman': SpearmanrResult(correlation=0.6384487790460556, pvalue=4.095349023244859e-87), 'nsamples': 750}, 'OnWN': {'pearson': (0.5797431728350076, 1.0979732151984558e-51), 'spearman': SpearmanrResult(correlation=0.5945561650937895, pvalue=6.410670363780586e-55), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5284543935625261, 'wmean': 0.5823140703796422}, 'spearman': {'mean': 0.5230585607304742, 'wmean': 0.5839459082626038}}}, 'STS14': {'deft-forum': {'pearson': (0.24788885729220378, 9.97041827539346e-08), 'spearman': SpearmanrResult(correlation=0.2486283132728474, pvalue=9.108295545302933e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.6281551290505817, 2.4591553749062732e-34), 'spearman': SpearmanrResult(correlation=0.6092920231550842, pvalue=7.108975227499388e-32), 'nsamples': 300}, 'headlines': {'pearson': (0.5651194767288049, 1.6695751032725332e-64), 'spearman': SpearmanrResult(correlation=0.5382736247967232, pvalue=1.4550186346944612e-57), 'nsamples': 750}, 'images': {'pearson': (0.574501792037723, 4.402301103620764e-67), 'spearman': SpearmanrResult(correlation=0.5597429063001943, pvalue=4.608972996500294e-63), 'nsamples': 750}, 'OnWN': {'pearson': (0.6252009915987753, 1.4053617506412844e-82), 'spearman': SpearmanrResult(correlation=0.6603233154022432, pvalue=4.120832765841787e-95), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5291985914455102, 2.3472736387241165e-55), 'spearman': SpearmanrResult(correlation=0.49124845752886065, pvalue=8.176929840223158e-47), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5283441396922665, 'wmean': 0.5388032435612736}, 'spearman': {'mean': 0.5179181067426589, 'wmean': 0.5284964202507526}}}, 'STS15': {'answers-forums': {'pearson': (0.4829210219862239, 2.630629878156648e-23), 'spearman': SpearmanrResult(correlation=0.491711921148118, pvalue=3.1788533363732745e-24), 'nsamples': 375}, 'answers-students': {'pearson': (0.5624506552654874, 8.733522250853178e-64), 'spearman': SpearmanrResult(correlation=0.5848517506253162, pvalue=5.0170127404138625e-70), 'nsamples': 750}, 'belief': {'pearson': (0.5616901844816033, 1.450302384433806e-32), 'spearman': SpearmanrResult(correlation=0.6065380263953579, pvalue=4.7753802588526597e-39), 'nsamples': 375}, 'headlines': {'pearson': (0.6275856286703954, 2.2262226018757164e-83), 'spearman': SpearmanrResult(correlation=0.6368843285042851, pvalue=1.4437598852395816e-86), 'nsamples': 750}, 'images': {'pearson': (0.6274001124016657, 2.5708273781215057e-83), 'spearman': SpearmanrResult(correlation=0.6596578437463961, pvalue=7.382584716590685e-95), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5724095205610752, 'wmean': 0.5849354998928655}, 'spearman': {'mean': 0.5959287740838947, 'wmean': 0.6076297241619338}}}, 'STS16': {'answer-answer': {'pearson': (0.507349545431866, 5.076462889254544e-18), 'spearman': SpearmanrResult(correlation=0.5369452081890486, pvalue=2.2666902123088385e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6473857285665054, 5.719515745835441e-31), 'spearman': SpearmanrResult(correlation=0.6830733027096948, pvalue=1.4482787628293637e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.720890828698756, 3.605998000619877e-38), 'spearman': SpearmanrResult(correlation=0.7518504687337103, pvalue=3.989521879154753e-43), 'nsamples': 230}, 'postediting': {'pearson': (0.7651962279761904, 3.4819712116305697e-48), 'spearman': SpearmanrResult(correlation=0.8117370171581921, pvalue=1.8850509825026632e-58), 'nsamples': 244}, 'question-question': {'pearson': (0.5326915412928579, 1.0242415670858729e-16), 'spearman': SpearmanrResult(correlation=0.5517302524833994, pvalue=4.796095705186815e-18), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6347027743932351, 'wmean': 0.6356754918295661}, 'spearman': {'mean': 0.6670672498548089, 'wmean': 0.6684404705051561}}}, 'MR': {'devacc': 72.43, 'acc': 70.49, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.05, 'acc': 75.41, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.69, 'acc': 85.21, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.29, 'acc': 93.37, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.1, 'acc': 77.32, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.6, 'acc': 41.86, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.32, 'acc': 88.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.97, 'acc': 68.7, 'f1': 80.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.4, 'acc': 73.03, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7377278104210847, 'pearson': 0.761975917856173, 'spearman': 0.6798149754629706, 'mse': 0.4301519133438932, 'yhat': array([3.5469571 , 4.6341958 , 1.3699249 , ..., 3.26691131, 4.54880847,        4.43157446]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6613741006510734, 'pearson': 0.6287377219792188, 'spearman': 0.6221295709154164, 'mse': 1.5566513328531442, 'yhat': array([1.88185194, 1.80882771, 1.90250716, ..., 3.6871187 , 3.74652895,        3.81879078]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.98, 'acc': 67.02, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 80.78, 'acc': 81.03, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 36.35, 'acc': 35.69, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.13, 'acc': 29.97, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.15, 'acc': 68.27, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 79.98, 'acc': 78.79, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.06, 'acc': 88.13, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.08, 'acc': 84.96, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.76, 'acc': 78.94, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.58, 'acc': 59.19, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.19, 'acc': 56.49, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 06:58:43,548 : STS12 p=0.4504, STS12 s=0.5075, STS13 p=0.5823, STS13 s=0.5839, STS14 p=0.5388, STS14 s=0.5285, STS15 p=0.5849, STS15 s=0.6076, STS 16 p=0.6357, STS16 s=0.6684, STS B p=0.6287, STS B s=0.6221, STS B m=1.5567, SICK-R p=0.7620, SICK-R s=0.6798, SICK-P m=0.4302
2019-03-13 06:58:43,548 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 06:58:43,548 : 0.4504,0.5075,0.5823,0.5839,0.5388,0.5285,0.5849,0.6076,0.6357,0.6684,0.6287,0.6221,1.5567,0.7620,0.6798,0.4302
2019-03-13 06:58:43,548 : MR=70.49, CR=75.41, SUBJ=93.37, MPQA=85.21, SST-B=77.32, SST-F=41.86, TREC=88.20, SICK-E=73.03, SNLI=67.02, MRPC=68.70, MRPC f=80.67
2019-03-13 06:58:43,548 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 06:58:43,548 : 70.49,75.41,93.37,85.21,77.32,41.86,88.20,73.03,67.02,68.70,80.67
2019-03-13 06:58:43,548 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 06:58:43,548 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 06:58:43,548 : na,na,na,na,na,na,na,na,na,na
2019-03-13 06:58:43,548 : SentLen=81.03, WC=35.69, TreeDepth=29.97, TopConst=68.27, BShift=78.79, Tense=88.13, SubjNum=84.96, ObjNum=78.94, SOMO=59.19, CoordInv=56.49, average=66.15
2019-03-13 06:58:43,548 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 06:58:43,548 : 81.03,35.69,29.97,68.27,78.79,88.13,84.96,78.94,59.19,56.49,66.15
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,548 : layer 13
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,548 : ********************************************************************************
2019-03-13 06:58:43,637 : ***** Transfer task : STS12 *****


2019-03-13 06:58:43,649 : loading BERT model bert-large-uncased
2019-03-13 06:58:43,650 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:58:43,667 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:58:43,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptcx7w3k9
2019-03-13 06:58:51,111 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:59:00,392 : MSRpar : pearson = 0.2830, spearman = 0.3596
2019-03-13 06:59:02,022 : MSRvid : pearson = 0.4851, spearman = 0.5237
2019-03-13 06:59:03,424 : SMTeuroparl : pearson = 0.4272, spearman = 0.6091
2019-03-13 06:59:06,103 : surprise.OnWN : pearson = 0.5054, spearman = 0.5565
2019-03-13 06:59:07,520 : surprise.SMTnews : pearson = 0.5973, spearman = 0.5362
2019-03-13 06:59:07,520 : ALL (weighted average) : Pearson = 0.4471,             Spearman = 0.5063
2019-03-13 06:59:07,520 : ALL (average) : Pearson = 0.4596,             Spearman = 0.5170

2019-03-13 06:59:07,521 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 06:59:07,528 : loading BERT model bert-large-uncased
2019-03-13 06:59:07,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:59:07,546 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:59:07,546 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk2_t5wca
2019-03-13 06:59:15,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:59:21,628 : FNWN : pearson = 0.3396, spearman = 0.3177
2019-03-13 06:59:23,508 : headlines : pearson = 0.6075, spearman = 0.6073
2019-03-13 06:59:24,965 : OnWN : pearson = 0.5339, spearman = 0.5468
2019-03-13 06:59:24,965 : ALL (weighted average) : Pearson = 0.5462,             Spearman = 0.5482
2019-03-13 06:59:24,965 : ALL (average) : Pearson = 0.4937,             Spearman = 0.4906

2019-03-13 06:59:24,965 : ***** Transfer task : STS14 *****


2019-03-13 06:59:24,982 : loading BERT model bert-large-uncased
2019-03-13 06:59:24,982 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:59:24,999 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:59:24,999 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpivott0fh
2019-03-13 06:59:32,492 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:59:39,173 : deft-forum : pearson = 0.2241, spearman = 0.2240
2019-03-13 06:59:40,799 : deft-news : pearson = 0.6187, spearman = 0.5849
2019-03-13 06:59:42,955 : headlines : pearson = 0.5353, spearman = 0.5130
2019-03-13 06:59:45,019 : images : pearson = 0.5337, spearman = 0.5233
2019-03-13 06:59:47,136 : OnWN : pearson = 0.5865, spearman = 0.6248
2019-03-13 06:59:49,977 : tweet-news : pearson = 0.5433, spearman = 0.4954
2019-03-13 06:59:49,977 : ALL (weighted average) : Pearson = 0.5161,             Spearman = 0.5050
2019-03-13 06:59:49,977 : ALL (average) : Pearson = 0.5069,             Spearman = 0.4942

2019-03-13 06:59:49,977 : ***** Transfer task : STS15 *****


2019-03-13 06:59:50,011 : loading BERT model bert-large-uncased
2019-03-13 06:59:50,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:59:50,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:59:50,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsh2ye_zk
2019-03-13 06:59:57,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:00:04,765 : answers-forums : pearson = 0.4653, spearman = 0.4654
2019-03-13 07:00:06,838 : answers-students : pearson = 0.5377, spearman = 0.5543
2019-03-13 07:00:08,873 : belief : pearson = 0.5638, spearman = 0.6071
2019-03-13 07:00:11,108 : headlines : pearson = 0.6147, spearman = 0.6227
2019-03-13 07:00:13,224 : images : pearson = 0.6008, spearman = 0.6275
2019-03-13 07:00:13,225 : ALL (weighted average) : Pearson = 0.5669,             Spearman = 0.5852
2019-03-13 07:00:13,225 : ALL (average) : Pearson = 0.5564,             Spearman = 0.5754

2019-03-13 07:00:13,225 : ***** Transfer task : STS16 *****


2019-03-13 07:00:13,294 : loading BERT model bert-large-uncased
2019-03-13 07:00:13,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:00:13,312 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:00:13,312 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_27erjb0
2019-03-13 07:00:20,807 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:00:27,038 : answer-answer : pearson = 0.5008, spearman = 0.5290
2019-03-13 07:00:27,694 : headlines : pearson = 0.6320, spearman = 0.6565
2019-03-13 07:00:28,569 : plagiarism : pearson = 0.7054, spearman = 0.7455
2019-03-13 07:00:30,055 : postediting : pearson = 0.7725, spearman = 0.8135
2019-03-13 07:00:30,658 : question-question : pearson = 0.5040, spearman = 0.5273
2019-03-13 07:00:30,658 : ALL (weighted average) : Pearson = 0.6245,             Spearman = 0.6560
2019-03-13 07:00:30,658 : ALL (average) : Pearson = 0.6230,             Spearman = 0.6544

2019-03-13 07:00:30,658 : ***** Transfer task : MR *****


2019-03-13 07:00:30,677 : loading BERT model bert-large-uncased
2019-03-13 07:00:30,677 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:00:30,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:00:30,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb1fdii8w
2019-03-13 07:00:38,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:00:43,636 : Generating sentence embeddings
2019-03-13 07:01:14,880 : Generated sentence embeddings
2019-03-13 07:01:14,881 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:01:24,328 : Best param found at split 1: l2reg = 1e-05                 with score 73.07
2019-03-13 07:01:35,695 : Best param found at split 2: l2reg = 0.0001                 with score 72.7
2019-03-13 07:01:45,359 : Best param found at split 3: l2reg = 1e-05                 with score 73.32
2019-03-13 07:01:56,419 : Best param found at split 4: l2reg = 0.001                 with score 72.53
2019-03-13 07:02:06,343 : Best param found at split 5: l2reg = 0.001                 with score 72.12
2019-03-13 07:02:07,134 : Dev acc : 72.75 Test acc : 71.02

2019-03-13 07:02:07,135 : ***** Transfer task : CR *****


2019-03-13 07:02:07,143 : loading BERT model bert-large-uncased
2019-03-13 07:02:07,144 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:02:07,163 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:02:07,163 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp06eeenh7
2019-03-13 07:02:14,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:02:19,983 : Generating sentence embeddings
2019-03-13 07:02:28,269 : Generated sentence embeddings
2019-03-13 07:02:28,269 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:02:31,408 : Best param found at split 1: l2reg = 1e-05                 with score 76.95
2019-03-13 07:02:34,757 : Best param found at split 2: l2reg = 1e-05                 with score 76.48
2019-03-13 07:02:38,426 : Best param found at split 3: l2reg = 1e-05                 with score 75.93
2019-03-13 07:02:43,013 : Best param found at split 4: l2reg = 0.001                 with score 76.66
2019-03-13 07:02:46,631 : Best param found at split 5: l2reg = 1e-05                 with score 77.66
2019-03-13 07:02:46,853 : Dev acc : 76.74 Test acc : 72.82

2019-03-13 07:02:46,853 : ***** Transfer task : MPQA *****


2019-03-13 07:02:46,860 : loading BERT model bert-large-uncased
2019-03-13 07:02:46,860 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:02:46,915 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:02:46,915 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph9wvtc6u
2019-03-13 07:02:54,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:02:59,753 : Generating sentence embeddings
2019-03-13 07:03:07,309 : Generated sentence embeddings
2019-03-13 07:03:07,310 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:03:18,239 : Best param found at split 1: l2reg = 1e-05                 with score 82.22
2019-03-13 07:03:28,881 : Best param found at split 2: l2reg = 1e-05                 with score 82.47
2019-03-13 07:03:39,640 : Best param found at split 3: l2reg = 1e-05                 with score 83.37
2019-03-13 07:03:51,319 : Best param found at split 4: l2reg = 1e-05                 with score 83.98
2019-03-13 07:04:01,771 : Best param found at split 5: l2reg = 0.001                 with score 84.43
2019-03-13 07:04:02,288 : Dev acc : 83.29 Test acc : 83.48

2019-03-13 07:04:02,289 : ***** Transfer task : SUBJ *****


2019-03-13 07:04:02,304 : loading BERT model bert-large-uncased
2019-03-13 07:04:02,304 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:04:02,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:04:02,325 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp620wgkhe
2019-03-13 07:04:09,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:04:15,174 : Generating sentence embeddings
2019-03-13 07:04:45,838 : Generated sentence embeddings
2019-03-13 07:04:45,838 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:04:55,744 : Best param found at split 1: l2reg = 0.001                 with score 94.44
2019-03-13 07:05:05,491 : Best param found at split 2: l2reg = 1e-05                 with score 94.41
2019-03-13 07:05:16,605 : Best param found at split 3: l2reg = 0.001                 with score 94.02
2019-03-13 07:05:28,033 : Best param found at split 4: l2reg = 0.0001                 with score 94.72
2019-03-13 07:05:38,268 : Best param found at split 5: l2reg = 0.0001                 with score 94.34
2019-03-13 07:05:38,695 : Dev acc : 94.39 Test acc : 93.85

2019-03-13 07:05:38,696 : ***** Transfer task : SST Binary classification *****


2019-03-13 07:05:38,790 : loading BERT model bert-large-uncased
2019-03-13 07:05:38,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:05:38,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:05:38,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppglz57sr
2019-03-13 07:05:46,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:05:51,667 : Computing embedding for train
2019-03-13 07:07:30,753 : Computed train embeddings
2019-03-13 07:07:30,753 : Computing embedding for dev
2019-03-13 07:07:32,909 : Computed dev embeddings
2019-03-13 07:07:32,909 : Computing embedding for test
2019-03-13 07:07:37,437 : Computed test embeddings
2019-03-13 07:07:37,437 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:07:52,593 : [('reg:1e-05', 77.87), ('reg:0.0001', 77.87), ('reg:0.001', 77.64), ('reg:0.01', 75.69)]
2019-03-13 07:07:52,593 : Validation : best param found is reg = 1e-05 with score             77.87
2019-03-13 07:07:52,593 : Evaluating...
2019-03-13 07:07:56,182 : 
Dev acc : 77.87 Test acc : 78.64 for             SST Binary classification

2019-03-13 07:07:56,182 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 07:07:56,233 : loading BERT model bert-large-uncased
2019-03-13 07:07:56,233 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:07:56,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:07:56,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5kgmd1g6
2019-03-13 07:08:03,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:08:09,030 : Computing embedding for train
2019-03-13 07:08:30,771 : Computed train embeddings
2019-03-13 07:08:30,771 : Computing embedding for dev
2019-03-13 07:08:33,605 : Computed dev embeddings
2019-03-13 07:08:33,605 : Computing embedding for test
2019-03-13 07:08:39,188 : Computed test embeddings
2019-03-13 07:08:39,189 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:08:41,627 : [('reg:1e-05', 39.15), ('reg:0.0001', 34.51), ('reg:0.001', 38.42), ('reg:0.01', 32.79)]
2019-03-13 07:08:41,627 : Validation : best param found is reg = 1e-05 with score             39.15
2019-03-13 07:08:41,627 : Evaluating...
2019-03-13 07:08:42,365 : 
Dev acc : 39.15 Test acc : 40.27 for             SST Fine-Grained classification

2019-03-13 07:08:42,366 : ***** Transfer task : TREC *****


2019-03-13 07:08:42,379 : loading BERT model bert-large-uncased
2019-03-13 07:08:42,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:08:42,398 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:08:42,398 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0exdwzkf
2019-03-13 07:08:49,864 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:09:02,746 : Computed train embeddings
2019-03-13 07:09:03,329 : Computed test embeddings
2019-03-13 07:09:03,329 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:09:10,056 : [('reg:1e-05', 76.25), ('reg:0.0001', 74.42), ('reg:0.001', 72.29), ('reg:0.01', 60.22)]
2019-03-13 07:09:10,056 : Cross-validation : best param found is reg = 1e-05             with score 76.25
2019-03-13 07:09:10,056 : Evaluating...
2019-03-13 07:09:10,503 : 
Dev acc : 76.25 Test acc : 84.2             for TREC

2019-03-13 07:09:10,504 : ***** Transfer task : MRPC *****


2019-03-13 07:09:10,527 : loading BERT model bert-large-uncased
2019-03-13 07:09:10,527 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:09:10,548 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:09:10,548 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6en4j6ga
2019-03-13 07:09:18,012 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:09:23,257 : Computing embedding for train
2019-03-13 07:09:45,345 : Computed train embeddings
2019-03-13 07:09:45,345 : Computing embedding for test
2019-03-13 07:09:55,000 : Computed test embeddings
2019-03-13 07:09:55,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:10:00,025 : [('reg:1e-05', 70.22), ('reg:0.0001', 70.04), ('reg:0.001', 69.21), ('reg:0.01', 69.63)]
2019-03-13 07:10:00,026 : Cross-validation : best param found is reg = 1e-05             with score 70.22
2019-03-13 07:10:00,026 : Evaluating...
2019-03-13 07:10:00,372 : Dev acc : 70.22 Test acc 69.33; Test F1 80.84 for MRPC.

2019-03-13 07:10:00,373 : ***** Transfer task : SICK-Entailment*****


2019-03-13 07:10:00,433 : loading BERT model bert-large-uncased
2019-03-13 07:10:00,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:10:00,453 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:10:00,453 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiv9mbm4_
2019-03-13 07:10:07,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:10:13,209 : Computing embedding for train
2019-03-13 07:10:24,408 : Computed train embeddings
2019-03-13 07:10:24,408 : Computing embedding for dev
2019-03-13 07:10:25,933 : Computed dev embeddings
2019-03-13 07:10:25,933 : Computing embedding for test
2019-03-13 07:10:37,914 : Computed test embeddings
2019-03-13 07:10:37,950 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:10:39,126 : [('reg:1e-05', 72.2), ('reg:0.0001', 71.6), ('reg:0.001', 68.8), ('reg:0.01', 73.2)]
2019-03-13 07:10:39,127 : Validation : best param found is reg = 0.01 with score             73.2
2019-03-13 07:10:39,127 : Evaluating...
2019-03-13 07:10:39,454 : 
Dev acc : 73.2 Test acc : 72.56 for                        SICK entailment

2019-03-13 07:10:39,455 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 07:10:39,482 : loading BERT model bert-large-uncased
2019-03-13 07:10:39,482 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:10:39,538 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:10:39,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz03zp8kh
2019-03-13 07:10:47,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:10:52,358 : Computing embedding for train
2019-03-13 07:11:03,553 : Computed train embeddings
2019-03-13 07:11:03,553 : Computing embedding for dev
2019-03-13 07:11:05,078 : Computed dev embeddings
2019-03-13 07:11:05,078 : Computing embedding for test
2019-03-13 07:11:17,089 : Computed test embeddings
2019-03-13 07:11:35,479 : Dev : Pearson 0.7462806046455219
2019-03-13 07:11:35,482 : Test : Pearson 0.766239966659787 Spearman 0.6857442138431141 MSE 0.42204140148669045                        for SICK Relatedness

2019-03-13 07:11:35,482 : 

***** Transfer task : STSBenchmark*****


2019-03-13 07:11:35,555 : loading BERT model bert-large-uncased
2019-03-13 07:11:35,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:11:35,586 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:11:35,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa1rb2cv3
2019-03-13 07:11:43,111 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:11:48,475 : Computing embedding for train
2019-03-13 07:12:06,916 : Computed train embeddings
2019-03-13 07:12:06,916 : Computing embedding for dev
2019-03-13 07:12:12,498 : Computed dev embeddings
2019-03-13 07:12:12,498 : Computing embedding for test
2019-03-13 07:12:17,052 : Computed test embeddings
2019-03-13 07:12:36,944 : Dev : Pearson 0.6629221969276327
2019-03-13 07:12:36,944 : Test : Pearson 0.6291724947234911 Spearman 0.6230030275931044 MSE 1.5544634087286535                        for SICK Relatedness

2019-03-13 07:12:36,945 : ***** Transfer task : SNLI Entailment*****


2019-03-13 07:12:42,006 : loading BERT model bert-large-uncased
2019-03-13 07:12:42,006 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:12:42,075 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:12:42,076 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7zsjo4ay
2019-03-13 07:12:49,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:12:55,277 : PROGRESS (encoding): 0.00%
2019-03-13 07:15:38,999 : PROGRESS (encoding): 14.56%
2019-03-13 07:18:44,834 : PROGRESS (encoding): 29.12%
2019-03-13 07:21:51,297 : PROGRESS (encoding): 43.69%
2019-03-13 07:25:10,037 : PROGRESS (encoding): 58.25%
2019-03-13 07:28:51,386 : PROGRESS (encoding): 72.81%
2019-03-13 07:32:31,677 : PROGRESS (encoding): 87.37%
2019-03-13 07:36:30,110 : PROGRESS (encoding): 0.00%
2019-03-13 07:37:00,148 : PROGRESS (encoding): 0.00%
2019-03-13 07:37:28,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:38:03,826 : [('reg:1e-09', 59.08)]
2019-03-13 07:38:03,826 : Validation : best param found is reg = 1e-09 with score             59.08
2019-03-13 07:38:03,826 : Evaluating...
2019-03-13 07:38:38,205 : Dev acc : 59.08 Test acc : 59.34 for SNLI

2019-03-13 07:38:38,205 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 07:38:38,412 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 07:38:39,466 : loading BERT model bert-large-uncased
2019-03-13 07:38:39,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:38:39,492 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:38:39,492 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4pujgy1m
2019-03-13 07:38:46,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:38:52,417 : Computing embeddings for train/dev/test
2019-03-13 07:42:20,833 : Computed embeddings
2019-03-13 07:42:20,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:42:49,218 : [('reg:1e-05', 82.13), ('reg:0.0001', 82.26), ('reg:0.001', 70.16), ('reg:0.01', 59.85)]
2019-03-13 07:42:49,218 : Validation : best param found is reg = 0.0001 with score             82.26
2019-03-13 07:42:49,218 : Evaluating...
2019-03-13 07:42:58,255 : 
Dev acc : 82.3 Test acc : 82.5 for LENGTH classification

2019-03-13 07:42:58,256 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 07:42:58,507 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 07:42:58,554 : loading BERT model bert-large-uncased
2019-03-13 07:42:58,554 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:42:58,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:42:58,585 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaqgjlfuk
2019-03-13 07:43:06,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:43:11,251 : Computing embeddings for train/dev/test
2019-03-13 07:46:23,442 : Computed embeddings
2019-03-13 07:46:23,442 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:46:58,690 : [('reg:1e-05', 36.44), ('reg:0.0001', 11.93), ('reg:0.001', 1.26), ('reg:0.01', 0.61)]
2019-03-13 07:46:58,690 : Validation : best param found is reg = 1e-05 with score             36.44
2019-03-13 07:46:58,690 : Evaluating...
2019-03-13 07:47:08,409 : 
Dev acc : 36.4 Test acc : 35.4 for WORDCONTENT classification

2019-03-13 07:47:08,411 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 07:47:08,938 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 07:47:09,004 : loading BERT model bert-large-uncased
2019-03-13 07:47:09,004 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:47:09,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:47:09,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgs8x_tgd
2019-03-13 07:47:16,451 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:47:21,753 : Computing embeddings for train/dev/test
2019-03-13 07:50:22,292 : Computed embeddings
2019-03-13 07:50:22,292 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:50:46,079 : [('reg:1e-05', 30.18), ('reg:0.0001', 29.74), ('reg:0.001', 27.14), ('reg:0.01', 24.05)]
2019-03-13 07:50:46,079 : Validation : best param found is reg = 1e-05 with score             30.18
2019-03-13 07:50:46,079 : Evaluating...
2019-03-13 07:50:50,688 : 
Dev acc : 30.2 Test acc : 29.9 for DEPTH classification

2019-03-13 07:50:50,689 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 07:50:51,054 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 07:50:51,117 : loading BERT model bert-large-uncased
2019-03-13 07:50:51,118 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:50:51,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:50:51,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkvqqzluo
2019-03-13 07:50:58,675 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:51:03,999 : Computing embeddings for train/dev/test
2019-03-13 07:53:51,526 : Computed embeddings
2019-03-13 07:53:51,526 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:54:22,271 : [('reg:1e-05', 68.13), ('reg:0.0001', 70.55), ('reg:0.001', 66.93), ('reg:0.01', 51.36)]
2019-03-13 07:54:22,271 : Validation : best param found is reg = 0.0001 with score             70.55
2019-03-13 07:54:22,271 : Evaluating...
2019-03-13 07:54:30,031 : 
Dev acc : 70.5 Test acc : 70.4 for TOPCONSTITUENTS classification

2019-03-13 07:54:30,032 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 07:54:30,412 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 07:54:30,478 : loading BERT model bert-large-uncased
2019-03-13 07:54:30,478 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:54:30,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:54:30,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjmj1h2xn
2019-03-13 07:54:37,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:54:43,218 : Computing embeddings for train/dev/test
2019-03-13 07:57:45,264 : Computed embeddings
2019-03-13 07:57:45,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:58:23,610 : [('reg:1e-05', 84.71), ('reg:0.0001', 84.62), ('reg:0.001', 84.21), ('reg:0.01', 81.08)]
2019-03-13 07:58:23,610 : Validation : best param found is reg = 1e-05 with score             84.71
2019-03-13 07:58:23,610 : Evaluating...
2019-03-13 07:58:31,883 : 
Dev acc : 84.7 Test acc : 83.3 for BIGRAMSHIFT classification

2019-03-13 07:58:31,884 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 07:58:32,275 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 07:58:32,340 : loading BERT model bert-large-uncased
2019-03-13 07:58:32,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:58:32,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:58:32,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf70d74lh
2019-03-13 07:58:39,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:58:45,206 : Computing embeddings for train/dev/test
2019-03-13 08:01:42,963 : Computed embeddings
2019-03-13 08:01:42,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:02:09,798 : [('reg:1e-05', 88.87), ('reg:0.0001', 88.85), ('reg:0.001', 89.77), ('reg:0.01', 89.68)]
2019-03-13 08:02:09,798 : Validation : best param found is reg = 0.001 with score             89.77
2019-03-13 08:02:09,798 : Evaluating...
2019-03-13 08:02:18,490 : 
Dev acc : 89.8 Test acc : 88.3 for TENSE classification

2019-03-13 08:02:18,491 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 08:02:18,891 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 08:02:18,954 : loading BERT model bert-large-uncased
2019-03-13 08:02:18,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:02:19,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:02:19,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6wty1f0y
2019-03-13 08:02:26,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:02:31,998 : Computing embeddings for train/dev/test
2019-03-13 08:05:40,268 : Computed embeddings
2019-03-13 08:05:40,268 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:06:07,261 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.44), ('reg:0.001', 87.15), ('reg:0.01', 78.88)]
2019-03-13 08:06:07,261 : Validation : best param found is reg = 0.001 with score             87.15
2019-03-13 08:06:07,261 : Evaluating...
2019-03-13 08:06:12,616 : 
Dev acc : 87.2 Test acc : 86.9 for SUBJNUMBER classification

2019-03-13 08:06:12,617 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 08:06:13,016 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 08:06:13,082 : loading BERT model bert-large-uncased
2019-03-13 08:06:13,083 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:06:13,196 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:06:13,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqajrp4ed
2019-03-13 08:06:20,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:06:26,081 : Computing embeddings for train/dev/test
2019-03-13 08:09:31,041 : Computed embeddings
2019-03-13 08:09:31,041 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:09:57,630 : [('reg:1e-05', 80.9), ('reg:0.0001', 80.8), ('reg:0.001', 80.49), ('reg:0.01', 74.33)]
2019-03-13 08:09:57,630 : Validation : best param found is reg = 1e-05 with score             80.9
2019-03-13 08:09:57,630 : Evaluating...
2019-03-13 08:10:05,146 : 
Dev acc : 80.9 Test acc : 81.4 for OBJNUMBER classification

2019-03-13 08:10:05,147 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 08:10:05,722 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 08:10:05,792 : loading BERT model bert-large-uncased
2019-03-13 08:10:05,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:10:05,820 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:10:05,820 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9o9qli5j
2019-03-13 08:10:13,260 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:10:18,633 : Computing embeddings for train/dev/test
2019-03-13 08:13:53,099 : Computed embeddings
2019-03-13 08:13:53,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:14:11,327 : [('reg:1e-05', 60.44), ('reg:0.0001', 60.5), ('reg:0.001', 60.29), ('reg:0.01', 59.13)]
2019-03-13 08:14:11,327 : Validation : best param found is reg = 0.0001 with score             60.5
2019-03-13 08:14:11,327 : Evaluating...
2019-03-13 08:14:16,652 : 
Dev acc : 60.5 Test acc : 60.9 for ODDMANOUT classification

2019-03-13 08:14:16,653 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 08:14:17,027 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 08:14:17,107 : loading BERT model bert-large-uncased
2019-03-13 08:14:17,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:14:17,136 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:14:17,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcvg37p7j
2019-03-13 08:14:24,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:14:30,131 : Computing embeddings for train/dev/test
2019-03-13 08:18:02,522 : Computed embeddings
2019-03-13 08:18:02,522 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:18:25,057 : [('reg:1e-05', 57.99), ('reg:0.0001', 57.95), ('reg:0.001', 64.27), ('reg:0.01', 56.5)]
2019-03-13 08:18:25,058 : Validation : best param found is reg = 0.001 with score             64.27
2019-03-13 08:18:25,058 : Evaluating...
2019-03-13 08:18:32,703 : 
Dev acc : 64.3 Test acc : 64.1 for COORDINATIONINVERSION classification

2019-03-13 08:18:32,705 : total results: {'STS12': {'MSRpar': {'pearson': (0.2829572514260573, 2.8308196237704007e-15), 'spearman': SpearmanrResult(correlation=0.3596201932555748, pvalue=2.576576577436698e-24), 'nsamples': 750}, 'MSRvid': {'pearson': (0.485120140396446, 1.5630583175633662e-45), 'spearman': SpearmanrResult(correlation=0.5237121551961611, pvalue=4.711090301263225e-54), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.42716568515221615, 8.79101083806576e-22), 'spearman': SpearmanrResult(correlation=0.6091113782257811, pvalue=5.949772737931784e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5053616823776648, 7.258904090607154e-50), 'spearman': SpearmanrResult(correlation=0.5565141622277577, pvalue=3.2837185576130827e-62), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5972553959494713, 6.1998713300389e-40), 'spearman': SpearmanrResult(correlation=0.5362486091006954, pvalue=4.369928893209829e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.45957203106037114, 'wmean': 0.4470570328567672}, 'spearman': {'mean': 0.517041299601194, 'wmean': 0.5062516733096625}}}, 'STS13': {'FNWN': {'pearson': (0.3396330789179295, 1.7434236469342504e-06), 'spearman': SpearmanrResult(correlation=0.31765554628688486, pvalue=8.429379425202501e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.6075262215322975, 7.389127643937049e-77), 'spearman': SpearmanrResult(correlation=0.6073444943913655, pvalue=8.424613781748969e-77), 'nsamples': 750}, 'OnWN': {'pearson': (0.5338509285250977, 1.1961303138158914e-42), 'spearman': SpearmanrResult(correlation=0.5468494441365631, pvalue=4.559196739027475e-45), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4936700763251083, 'wmean': 0.5462171259781944}, 'spearman': {'mean': 0.49061649493827114, 'wmean': 0.548218538134905}}}, 'STS14': {'deft-forum': {'pearson': (0.2240527493682786, 1.5801316848651928e-06), 'spearman': SpearmanrResult(correlation=0.22401223774401782, pvalue=1.5871762190423345e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.6186889316516061, 4.432714604534122e-33), 'spearman': SpearmanrResult(correlation=0.5848590723340779, pvalue=6.374099990978517e-29), 'nsamples': 300}, 'headlines': {'pearson': (0.5352931802237545, 7.858497084455972e-57), 'spearman': SpearmanrResult(correlation=0.5129819275330139, pvalue=1.421717596991649e-51), 'nsamples': 750}, 'images': {'pearson': (0.5337011075480727, 1.9213969861821398e-56), 'spearman': SpearmanrResult(correlation=0.5232672456605322, pvalue=5.993875309368636e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.5864504464109961, 1.7232026830648107e-70), 'spearman': SpearmanrResult(correlation=0.624844095720994, pvalue=1.8490789682834274e-82), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5433175511060977, 8.062229192292053e-59), 'spearman': SpearmanrResult(correlation=0.495417395311904, pvalue=1.0615225017884461e-47), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5069173277181342, 'wmean': 0.5161339015141061}, 'spearman': {'mean': 0.49423032905075653, 'wmean': 0.5049723271612971}}}, 'STS15': {'answers-forums': {'pearson': (0.46528187389852343, 1.526080992477555e-21), 'spearman': SpearmanrResult(correlation=0.465389334336922, pvalue=1.4898425909007722e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.5376682646571465, 2.0522699520285352e-57), 'spearman': SpearmanrResult(correlation=0.5543010480595383, pvalue=1.2459710856899046e-61), 'nsamples': 750}, 'belief': {'pearson': (0.563793521993856, 7.572898425717067e-33), 'spearman': SpearmanrResult(correlation=0.6071006256804323, pvalue=3.9000827773514445e-39), 'nsamples': 375}, 'headlines': {'pearson': (0.6147229200684122, 3.828593006433841e-79), 'spearman': SpearmanrResult(correlation=0.6227080297975244, pvalue=9.483366065612437e-82), 'nsamples': 750}, 'images': {'pearson': (0.600778828379934, 9.097069696641914e-75), 'spearman': SpearmanrResult(correlation=0.6274557954595871, pvalue=2.4621617084436378e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5564490817995744, 'wmean': 0.5669269277629206}, 'spearman': {'mean': 0.5753909666668008, 'wmean': 0.5851774633313318}}}, 'STS16': {'answer-answer': {'pearson': (0.5008404538917051, 1.557601386533176e-17), 'spearman': SpearmanrResult(correlation=0.5290315411420682, pvalue=1.0150207768957443e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6319951487176287, 3.5937354109514063e-29), 'spearman': SpearmanrResult(correlation=0.6565080931837439, pvalue=4.3826076400854335e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7053835626120917, 6.240163254892946e-36), 'spearman': SpearmanrResult(correlation=0.7455310714368855, pvalue=4.684268724679832e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.772532277988319, 1.223653304964519e-49), 'spearman': SpearmanrResult(correlation=0.8134872502986619, pvalue=6.828333344726546e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.5040240669287351, 7.263047829284226e-15), 'spearman': SpearmanrResult(correlation=0.5272731272674543, pvalue=2.3642840587186753e-16), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.622955102027696, 'wmean': 0.624500330975733}, 'spearman': {'mean': 0.6543662166657628, 'wmean': 0.6559929559486448}}}, 'MR': {'devacc': 72.75, 'acc': 71.02, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.74, 'acc': 72.82, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.29, 'acc': 83.48, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.39, 'acc': 93.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.87, 'acc': 78.64, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.15, 'acc': 40.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.25, 'acc': 84.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.22, 'acc': 69.33, 'f1': 80.84, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 72.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7462806046455219, 'pearson': 0.766239966659787, 'spearman': 0.6857442138431141, 'mse': 0.42204140148669045, 'yhat': array([3.59602783, 4.45216022, 1.51217161, ..., 3.18852104, 4.66719482,        4.60137542]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6629221969276327, 'pearson': 0.6291724947234911, 'spearman': 0.6230030275931044, 'mse': 1.5544634087286535, 'yhat': array([2.34668365, 2.0237622 , 2.12516327, ..., 3.88973464, 3.78289627,        3.84014919]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.08, 'acc': 59.34, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 82.26, 'acc': 82.51, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 36.44, 'acc': 35.43, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.18, 'acc': 29.87, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.55, 'acc': 70.4, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.71, 'acc': 83.28, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.77, 'acc': 88.33, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.15, 'acc': 86.92, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.9, 'acc': 81.39, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.5, 'acc': 60.85, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 64.27, 'acc': 64.1, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 08:18:32,705 : STS12 p=0.4471, STS12 s=0.5063, STS13 p=0.5462, STS13 s=0.5482, STS14 p=0.5161, STS14 s=0.5050, STS15 p=0.5669, STS15 s=0.5852, STS 16 p=0.6245, STS16 s=0.6560, STS B p=0.6292, STS B s=0.6230, STS B m=1.5545, SICK-R p=0.7662, SICK-R s=0.6857, SICK-P m=0.4220
2019-03-13 08:18:32,706 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 08:18:32,706 : 0.4471,0.5063,0.5462,0.5482,0.5161,0.5050,0.5669,0.5852,0.6245,0.6560,0.6292,0.6230,1.5545,0.7662,0.6857,0.4220
2019-03-13 08:18:32,706 : MR=71.02, CR=72.82, SUBJ=93.85, MPQA=83.48, SST-B=78.64, SST-F=40.27, TREC=84.20, SICK-E=72.56, SNLI=59.34, MRPC=69.33, MRPC f=80.84
2019-03-13 08:18:32,706 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 08:18:32,706 : 71.02,72.82,93.85,83.48,78.64,40.27,84.20,72.56,59.34,69.33,80.84
2019-03-13 08:18:32,706 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 08:18:32,706 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 08:18:32,706 : na,na,na,na,na,na,na,na,na,na
2019-03-13 08:18:32,706 : SentLen=82.51, WC=35.43, TreeDepth=29.87, TopConst=70.40, BShift=83.28, Tense=88.33, SubjNum=86.92, ObjNum=81.39, SOMO=60.85, CoordInv=64.10, average=68.31
2019-03-13 08:18:32,706 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 08:18:32,706 : 82.51,35.43,29.87,70.40,83.28,88.33,86.92,81.39,60.85,64.10,68.31
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,706 : layer 14
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,706 : ********************************************************************************
2019-03-13 08:18:32,797 : ***** Transfer task : STS12 *****


2019-03-13 08:18:32,809 : loading BERT model bert-large-uncased
2019-03-13 08:18:32,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:18:32,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:18:32,827 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoh0qbk1c
2019-03-13 08:18:40,295 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:18:49,765 : MSRpar : pearson = 0.2627, spearman = 0.3281
2019-03-13 08:18:51,392 : MSRvid : pearson = 0.3746, spearman = 0.4086
2019-03-13 08:18:52,795 : SMTeuroparl : pearson = 0.4153, spearman = 0.5515
2019-03-13 08:18:55,474 : surprise.OnWN : pearson = 0.4551, spearman = 0.4867
2019-03-13 08:18:56,892 : surprise.SMTnews : pearson = 0.6355, spearman = 0.5159
2019-03-13 08:18:56,892 : ALL (weighted average) : Pearson = 0.4065,             Spearman = 0.4429
2019-03-13 08:18:56,892 : ALL (average) : Pearson = 0.4286,             Spearman = 0.4582

2019-03-13 08:18:56,892 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 08:18:56,901 : loading BERT model bert-large-uncased
2019-03-13 08:18:56,901 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:18:56,918 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:18:56,918 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9g6beu6d
2019-03-13 08:19:04,381 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:19:11,098 : FNWN : pearson = 0.2456, spearman = 0.2316
2019-03-13 08:19:12,976 : headlines : pearson = 0.5260, spearman = 0.5106
2019-03-13 08:19:14,431 : OnWN : pearson = 0.3925, spearman = 0.3896
2019-03-13 08:19:14,431 : ALL (weighted average) : Pearson = 0.4407,             Spearman = 0.4302
2019-03-13 08:19:14,431 : ALL (average) : Pearson = 0.3880,             Spearman = 0.3773

2019-03-13 08:19:14,432 : ***** Transfer task : STS14 *****


2019-03-13 08:19:14,446 : loading BERT model bert-large-uncased
2019-03-13 08:19:14,447 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:19:14,464 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:19:14,464 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0dcqsdyp
2019-03-13 08:19:21,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:19:28,479 : deft-forum : pearson = 0.0460, spearman = 0.0516
2019-03-13 08:19:30,103 : deft-news : pearson = 0.5836, spearman = 0.5425
2019-03-13 08:19:32,259 : headlines : pearson = 0.4567, spearman = 0.4388
2019-03-13 08:19:34,320 : images : pearson = 0.3486, spearman = 0.3428
2019-03-13 08:19:36,435 : OnWN : pearson = 0.4967, spearman = 0.5206
2019-03-13 08:19:39,276 : tweet-news : pearson = 0.5007, spearman = 0.4614
2019-03-13 08:19:39,276 : ALL (weighted average) : Pearson = 0.4128,             Spearman = 0.4023
2019-03-13 08:19:39,276 : ALL (average) : Pearson = 0.4054,             Spearman = 0.3930

2019-03-13 08:19:39,277 : ***** Transfer task : STS15 *****


2019-03-13 08:19:39,308 : loading BERT model bert-large-uncased
2019-03-13 08:19:39,308 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:19:39,326 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:19:39,326 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcuuggj5d
2019-03-13 08:19:46,753 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:19:53,705 : answers-forums : pearson = 0.4129, spearman = 0.4126
2019-03-13 08:19:55,774 : answers-students : pearson = 0.5453, spearman = 0.5667
2019-03-13 08:19:57,808 : belief : pearson = 0.4903, spearman = 0.5534
2019-03-13 08:20:00,043 : headlines : pearson = 0.5359, spearman = 0.5352
2019-03-13 08:20:02,163 : images : pearson = 0.3683, spearman = 0.3769
2019-03-13 08:20:02,163 : ALL (weighted average) : Pearson = 0.4753,             Spearman = 0.4905
2019-03-13 08:20:02,163 : ALL (average) : Pearson = 0.4705,             Spearman = 0.4890

2019-03-13 08:20:02,163 : ***** Transfer task : STS16 *****


2019-03-13 08:20:02,234 : loading BERT model bert-large-uncased
2019-03-13 08:20:02,234 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:20:02,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:20:02,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpym4f_15y
2019-03-13 08:20:09,742 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:20:16,031 : answer-answer : pearson = 0.4338, spearman = 0.4910
2019-03-13 08:20:16,687 : headlines : pearson = 0.6136, spearman = 0.6212
2019-03-13 08:20:17,564 : plagiarism : pearson = 0.6633, spearman = 0.7001
2019-03-13 08:20:19,047 : postediting : pearson = 0.7605, spearman = 0.8057
2019-03-13 08:20:19,649 : question-question : pearson = 0.3783, spearman = 0.4237
2019-03-13 08:20:19,649 : ALL (weighted average) : Pearson = 0.5735,             Spearman = 0.6118
2019-03-13 08:20:19,649 : ALL (average) : Pearson = 0.5699,             Spearman = 0.6084

2019-03-13 08:20:19,649 : ***** Transfer task : MR *****


2019-03-13 08:20:19,664 : loading BERT model bert-large-uncased
2019-03-13 08:20:19,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:20:19,684 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:20:19,684 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0njtmp54
2019-03-13 08:20:27,185 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:20:32,537 : Generating sentence embeddings
2019-03-13 08:21:03,774 : Generated sentence embeddings
2019-03-13 08:21:03,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:21:13,282 : Best param found at split 1: l2reg = 0.001                 with score 74.89
2019-03-13 08:21:23,304 : Best param found at split 2: l2reg = 1e-05                 with score 74.06
2019-03-13 08:21:31,235 : Best param found at split 3: l2reg = 1e-05                 with score 74.4
2019-03-13 08:21:41,591 : Best param found at split 4: l2reg = 1e-05                 with score 73.85
2019-03-13 08:21:51,857 : Best param found at split 5: l2reg = 0.001                 with score 74.27
2019-03-13 08:21:52,304 : Dev acc : 74.29 Test acc : 72.81

2019-03-13 08:21:52,305 : ***** Transfer task : CR *****


2019-03-13 08:21:52,312 : loading BERT model bert-large-uncased
2019-03-13 08:21:52,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:21:52,332 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:21:52,332 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxjvwwbzl
2019-03-13 08:21:59,796 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:22:05,255 : Generating sentence embeddings
2019-03-13 08:22:13,536 : Generated sentence embeddings
2019-03-13 08:22:13,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:22:16,999 : Best param found at split 1: l2reg = 0.0001                 with score 78.84
2019-03-13 08:22:21,352 : Best param found at split 2: l2reg = 0.0001                 with score 80.19
2019-03-13 08:22:24,583 : Best param found at split 3: l2reg = 1e-05                 with score 79.27
2019-03-13 08:22:27,847 : Best param found at split 4: l2reg = 0.001                 with score 79.68
2019-03-13 08:22:31,398 : Best param found at split 5: l2reg = 1e-05                 with score 79.54
2019-03-13 08:22:31,586 : Dev acc : 79.5 Test acc : 79.63

2019-03-13 08:22:31,586 : ***** Transfer task : MPQA *****


2019-03-13 08:22:31,592 : loading BERT model bert-large-uncased
2019-03-13 08:22:31,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:22:31,640 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:22:31,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpotbjt0_j
2019-03-13 08:22:39,111 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:22:44,537 : Generating sentence embeddings
2019-03-13 08:22:52,067 : Generated sentence embeddings
2019-03-13 08:22:52,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:23:02,251 : Best param found at split 1: l2reg = 0.0001                 with score 83.86
2019-03-13 08:23:12,686 : Best param found at split 2: l2reg = 0.001                 with score 84.17
2019-03-13 08:23:21,631 : Best param found at split 3: l2reg = 0.01                 with score 84.71
2019-03-13 08:23:31,276 : Best param found at split 4: l2reg = 0.01                 with score 85.55
2019-03-13 08:23:41,235 : Best param found at split 5: l2reg = 1e-05                 with score 83.29
2019-03-13 08:23:41,765 : Dev acc : 84.32 Test acc : 85.46

2019-03-13 08:23:41,766 : ***** Transfer task : SUBJ *****


2019-03-13 08:23:41,783 : loading BERT model bert-large-uncased
2019-03-13 08:23:41,783 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:23:41,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:23:41,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpncsjq6_u
2019-03-13 08:23:49,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:23:54,746 : Generating sentence embeddings
2019-03-13 08:24:25,432 : Generated sentence embeddings
2019-03-13 08:24:25,433 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:24:33,761 : Best param found at split 1: l2reg = 0.001                 with score 94.99
2019-03-13 08:24:44,129 : Best param found at split 2: l2reg = 0.0001                 with score 95.3
2019-03-13 08:24:52,291 : Best param found at split 3: l2reg = 1e-05                 with score 94.66
2019-03-13 08:25:00,435 : Best param found at split 4: l2reg = 0.0001                 with score 95.24
2019-03-13 08:25:09,127 : Best param found at split 5: l2reg = 0.001                 with score 95.14
2019-03-13 08:25:09,606 : Dev acc : 95.07 Test acc : 94.97

2019-03-13 08:25:09,607 : ***** Transfer task : SST Binary classification *****


2019-03-13 08:25:09,700 : loading BERT model bert-large-uncased
2019-03-13 08:25:09,701 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:25:09,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:25:09,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4qjf28xm
2019-03-13 08:25:17,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:25:22,733 : Computing embedding for train
2019-03-13 08:27:01,836 : Computed train embeddings
2019-03-13 08:27:01,836 : Computing embedding for dev
2019-03-13 08:27:03,991 : Computed dev embeddings
2019-03-13 08:27:03,991 : Computing embedding for test
2019-03-13 08:27:08,525 : Computed test embeddings
2019-03-13 08:27:08,525 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:27:23,826 : [('reg:1e-05', 79.93), ('reg:0.0001', 79.82), ('reg:0.001', 80.28), ('reg:0.01', 80.73)]
2019-03-13 08:27:23,826 : Validation : best param found is reg = 0.01 with score             80.73
2019-03-13 08:27:23,826 : Evaluating...
2019-03-13 08:27:28,149 : 
Dev acc : 80.73 Test acc : 80.34 for             SST Binary classification

2019-03-13 08:27:28,149 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 08:27:28,206 : loading BERT model bert-large-uncased
2019-03-13 08:27:28,206 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:27:28,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:27:28,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp91o03ng8
2019-03-13 08:27:35,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:27:40,970 : Computing embedding for train
2019-03-13 08:28:02,676 : Computed train embeddings
2019-03-13 08:28:02,676 : Computing embedding for dev
2019-03-13 08:28:05,507 : Computed dev embeddings
2019-03-13 08:28:05,507 : Computing embedding for test
2019-03-13 08:28:11,085 : Computed test embeddings
2019-03-13 08:28:11,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:28:13,745 : [('reg:1e-05', 38.96), ('reg:0.0001', 37.24), ('reg:0.001', 38.15), ('reg:0.01', 38.6)]
2019-03-13 08:28:13,745 : Validation : best param found is reg = 1e-05 with score             38.96
2019-03-13 08:28:13,745 : Evaluating...
2019-03-13 08:28:14,317 : 
Dev acc : 38.96 Test acc : 39.64 for             SST Fine-Grained classification

2019-03-13 08:28:14,318 : ***** Transfer task : TREC *****


2019-03-13 08:28:14,332 : loading BERT model bert-large-uncased
2019-03-13 08:28:14,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:28:14,350 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:28:14,350 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp51cbaymb
2019-03-13 08:28:21,827 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:28:34,744 : Computed train embeddings
2019-03-13 08:28:35,329 : Computed test embeddings
2019-03-13 08:28:35,330 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:28:41,801 : [('reg:1e-05', 78.92), ('reg:0.0001', 79.2), ('reg:0.001', 78.93), ('reg:0.01', 66.07)]
2019-03-13 08:28:41,801 : Cross-validation : best param found is reg = 0.0001             with score 79.2
2019-03-13 08:28:41,801 : Evaluating...
2019-03-13 08:28:42,105 : 
Dev acc : 79.2 Test acc : 88.8             for TREC

2019-03-13 08:28:42,106 : ***** Transfer task : MRPC *****


2019-03-13 08:28:42,126 : loading BERT model bert-large-uncased
2019-03-13 08:28:42,126 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:28:42,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:28:42,149 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyiovzyvq
2019-03-13 08:28:49,641 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:28:54,920 : Computing embedding for train
2019-03-13 08:29:17,001 : Computed train embeddings
2019-03-13 08:29:17,001 : Computing embedding for test
2019-03-13 08:29:26,655 : Computed test embeddings
2019-03-13 08:29:26,677 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:29:31,741 : [('reg:1e-05', 70.14), ('reg:0.0001', 70.46), ('reg:0.001', 70.9), ('reg:0.01', 69.82)]
2019-03-13 08:29:31,741 : Cross-validation : best param found is reg = 0.001             with score 70.9
2019-03-13 08:29:31,741 : Evaluating...
2019-03-13 08:29:32,052 : Dev acc : 70.9 Test acc 71.42; Test F1 81.06 for MRPC.

2019-03-13 08:29:32,053 : ***** Transfer task : SICK-Entailment*****


2019-03-13 08:29:32,116 : loading BERT model bert-large-uncased
2019-03-13 08:29:32,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:29:32,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:29:32,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl3td8lcj
2019-03-13 08:29:39,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:29:45,019 : Computing embedding for train
2019-03-13 08:29:56,228 : Computed train embeddings
2019-03-13 08:29:56,228 : Computing embedding for dev
2019-03-13 08:29:57,755 : Computed dev embeddings
2019-03-13 08:29:57,755 : Computing embedding for test
2019-03-13 08:30:09,737 : Computed test embeddings
2019-03-13 08:30:09,773 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:30:11,416 : [('reg:1e-05', 76.8), ('reg:0.0001', 70.6), ('reg:0.001', 76.0), ('reg:0.01', 71.8)]
2019-03-13 08:30:11,417 : Validation : best param found is reg = 1e-05 with score             76.8
2019-03-13 08:30:11,417 : Evaluating...
2019-03-13 08:30:11,863 : 
Dev acc : 76.8 Test acc : 75.36 for                        SICK entailment

2019-03-13 08:30:11,864 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 08:30:11,891 : loading BERT model bert-large-uncased
2019-03-13 08:30:11,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:30:11,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:30:11,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4xeu7qok
2019-03-13 08:30:19,407 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:30:24,838 : Computing embedding for train
2019-03-13 08:30:36,044 : Computed train embeddings
2019-03-13 08:30:36,044 : Computing embedding for dev
2019-03-13 08:30:37,570 : Computed dev embeddings
2019-03-13 08:30:37,570 : Computing embedding for test
2019-03-13 08:30:49,560 : Computed test embeddings
2019-03-13 08:31:04,617 : Dev : Pearson 0.7284517434817513
2019-03-13 08:31:04,617 : Test : Pearson 0.7714579657534245 Spearman 0.6998095998515821 MSE 0.4146528650263769                        for SICK Relatedness

2019-03-13 08:31:04,618 : 

***** Transfer task : STSBenchmark*****


2019-03-13 08:31:04,667 : loading BERT model bert-large-uncased
2019-03-13 08:31:04,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:31:04,692 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:31:04,692 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwzdgpiza
2019-03-13 08:31:12,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:31:17,627 : Computing embedding for train
2019-03-13 08:31:36,061 : Computed train embeddings
2019-03-13 08:31:36,061 : Computing embedding for dev
2019-03-13 08:31:41,647 : Computed dev embeddings
2019-03-13 08:31:41,647 : Computing embedding for test
2019-03-13 08:31:46,202 : Computed test embeddings
2019-03-13 08:32:05,476 : Dev : Pearson 0.6122148078953283
2019-03-13 08:32:05,476 : Test : Pearson 0.5842587185558689 Spearman 0.5760347429569336 MSE 1.651338866698064                        for SICK Relatedness

2019-03-13 08:32:05,476 : ***** Transfer task : SNLI Entailment*****


2019-03-13 08:32:10,561 : loading BERT model bert-large-uncased
2019-03-13 08:32:10,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:32:10,681 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:32:10,681 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqu1owupb
2019-03-13 08:32:18,144 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:32:23,900 : PROGRESS (encoding): 0.00%
2019-03-13 08:35:07,624 : PROGRESS (encoding): 14.56%
2019-03-13 08:38:13,365 : PROGRESS (encoding): 29.12%
2019-03-13 08:41:19,865 : PROGRESS (encoding): 43.69%
2019-03-13 08:44:38,704 : PROGRESS (encoding): 58.25%
2019-03-13 08:48:20,034 : PROGRESS (encoding): 72.81%
2019-03-13 08:52:00,315 : PROGRESS (encoding): 87.37%
2019-03-13 08:55:58,648 : PROGRESS (encoding): 0.00%
2019-03-13 08:56:28,635 : PROGRESS (encoding): 0.00%
2019-03-13 08:56:57,520 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:57:39,437 : [('reg:1e-09', 58.58)]
2019-03-13 08:57:39,437 : Validation : best param found is reg = 1e-09 with score             58.58
2019-03-13 08:57:39,438 : Evaluating...
2019-03-13 08:58:22,566 : Dev acc : 58.58 Test acc : 59.22 for SNLI

2019-03-13 08:58:22,566 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 08:58:22,769 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 08:58:23,831 : loading BERT model bert-large-uncased
2019-03-13 08:58:23,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:58:23,857 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:58:23,857 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn362kqk6
2019-03-13 08:58:31,350 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:58:36,716 : Computing embeddings for train/dev/test
2019-03-13 09:02:04,917 : Computed embeddings
2019-03-13 09:02:04,917 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:02:34,004 : [('reg:1e-05', 74.37), ('reg:0.0001', 71.41), ('reg:0.001', 66.98), ('reg:0.01', 59.47)]
2019-03-13 09:02:34,004 : Validation : best param found is reg = 1e-05 with score             74.37
2019-03-13 09:02:34,004 : Evaluating...
2019-03-13 09:02:41,754 : 
Dev acc : 74.4 Test acc : 74.3 for LENGTH classification

2019-03-13 09:02:41,755 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 09:02:42,129 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 09:02:42,173 : loading BERT model bert-large-uncased
2019-03-13 09:02:42,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:02:42,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:02:42,203 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwugnzkvc
2019-03-13 09:02:49,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:02:54,993 : Computing embeddings for train/dev/test
2019-03-13 09:06:07,242 : Computed embeddings
2019-03-13 09:06:07,242 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:06:41,710 : [('reg:1e-05', 39.78), ('reg:0.0001', 12.07), ('reg:0.001', 1.23), ('reg:0.01', 0.52)]
2019-03-13 09:06:41,710 : Validation : best param found is reg = 1e-05 with score             39.78
2019-03-13 09:06:41,710 : Evaluating...
2019-03-13 09:06:52,674 : 
Dev acc : 39.8 Test acc : 39.1 for WORDCONTENT classification

2019-03-13 09:06:52,675 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 09:06:53,054 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 09:06:53,121 : loading BERT model bert-large-uncased
2019-03-13 09:06:53,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:06:53,147 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:06:53,147 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk1zlyzaz
2019-03-13 09:07:00,640 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:07:06,001 : Computing embeddings for train/dev/test
2019-03-13 09:10:06,415 : Computed embeddings
2019-03-13 09:10:06,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:10:30,816 : [('reg:1e-05', 31.57), ('reg:0.0001', 29.43), ('reg:0.001', 24.79), ('reg:0.01', 21.56)]
2019-03-13 09:10:30,816 : Validation : best param found is reg = 1e-05 with score             31.57
2019-03-13 09:10:30,816 : Evaluating...
2019-03-13 09:10:39,727 : 
Dev acc : 31.6 Test acc : 31.7 for DEPTH classification

2019-03-13 09:10:39,728 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 09:10:40,108 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 09:10:40,171 : loading BERT model bert-large-uncased
2019-03-13 09:10:40,172 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:10:40,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:10:40,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbcubrl80
2019-03-13 09:10:47,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:10:53,032 : Computing embeddings for train/dev/test
2019-03-13 09:13:40,474 : Computed embeddings
2019-03-13 09:13:40,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:14:12,249 : [('reg:1e-05', 77.15), ('reg:0.0001', 72.93), ('reg:0.001', 70.75), ('reg:0.01', 58.36)]
2019-03-13 09:14:12,250 : Validation : best param found is reg = 1e-05 with score             77.15
2019-03-13 09:14:12,250 : Evaluating...
2019-03-13 09:14:22,244 : 
Dev acc : 77.2 Test acc : 77.4 for TOPCONSTITUENTS classification

2019-03-13 09:14:22,245 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 09:14:22,588 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 09:14:22,655 : loading BERT model bert-large-uncased
2019-03-13 09:14:22,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:14:22,773 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:14:22,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpso5qtp58
2019-03-13 09:14:30,192 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:14:35,579 : Computing embeddings for train/dev/test
2019-03-13 09:17:37,491 : Computed embeddings
2019-03-13 09:17:37,491 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:18:07,288 : [('reg:1e-05', 85.98), ('reg:0.0001', 86.04), ('reg:0.001', 85.62), ('reg:0.01', 83.08)]
2019-03-13 09:18:07,289 : Validation : best param found is reg = 0.0001 with score             86.04
2019-03-13 09:18:07,289 : Evaluating...
2019-03-13 09:18:13,577 : 
Dev acc : 86.0 Test acc : 85.1 for BIGRAMSHIFT classification

2019-03-13 09:18:13,578 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 09:18:14,134 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 09:18:14,199 : loading BERT model bert-large-uncased
2019-03-13 09:18:14,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:18:14,229 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:18:14,229 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkylh5ne8
2019-03-13 09:18:21,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:18:27,026 : Computing embeddings for train/dev/test
2019-03-13 09:21:25,009 : Computed embeddings
2019-03-13 09:21:25,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:21:55,930 : [('reg:1e-05', 89.64), ('reg:0.0001', 89.92), ('reg:0.001', 90.31), ('reg:0.01', 90.08)]
2019-03-13 09:21:55,931 : Validation : best param found is reg = 0.001 with score             90.31
2019-03-13 09:21:55,931 : Evaluating...
2019-03-13 09:22:03,171 : 
Dev acc : 90.3 Test acc : 88.9 for TENSE classification

2019-03-13 09:22:03,173 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 09:22:03,599 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 09:22:03,663 : loading BERT model bert-large-uncased
2019-03-13 09:22:03,663 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:22:03,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:22:03,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbg6t9rey
2019-03-13 09:22:11,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:22:16,526 : Computing embeddings for train/dev/test
2019-03-13 09:25:24,734 : Computed embeddings
2019-03-13 09:25:24,734 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:25:52,531 : [('reg:1e-05', 89.31), ('reg:0.0001', 89.18), ('reg:0.001', 89.08), ('reg:0.01', 86.96)]
2019-03-13 09:25:52,531 : Validation : best param found is reg = 1e-05 with score             89.31
2019-03-13 09:25:52,531 : Evaluating...
2019-03-13 09:26:00,154 : 
Dev acc : 89.3 Test acc : 89.4 for SUBJNUMBER classification

2019-03-13 09:26:00,155 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 09:26:00,568 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 09:26:00,635 : loading BERT model bert-large-uncased
2019-03-13 09:26:00,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:26:00,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:26:00,752 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmceov7xn
2019-03-13 09:26:08,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:26:13,651 : Computing embeddings for train/dev/test
2019-03-13 09:29:18,551 : Computed embeddings
2019-03-13 09:29:18,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:29:49,893 : [('reg:1e-05', 80.85), ('reg:0.0001', 80.75), ('reg:0.001', 80.54), ('reg:0.01', 77.4)]
2019-03-13 09:29:49,894 : Validation : best param found is reg = 1e-05 with score             80.85
2019-03-13 09:29:49,894 : Evaluating...
2019-03-13 09:29:56,327 : 
Dev acc : 80.8 Test acc : 81.8 for OBJNUMBER classification

2019-03-13 09:29:56,328 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 09:29:56,923 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 09:29:56,993 : loading BERT model bert-large-uncased
2019-03-13 09:29:56,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:29:57,019 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:29:57,019 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxjsjnbln
2019-03-13 09:30:04,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:30:09,801 : Computing embeddings for train/dev/test
2019-03-13 09:33:44,194 : Computed embeddings
2019-03-13 09:33:44,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:34:05,708 : [('reg:1e-05', 60.99), ('reg:0.0001', 60.96), ('reg:0.001', 60.94), ('reg:0.01', 59.53)]
2019-03-13 09:34:05,709 : Validation : best param found is reg = 1e-05 with score             60.99
2019-03-13 09:34:05,709 : Evaluating...
2019-03-13 09:34:11,168 : 
Dev acc : 61.0 Test acc : 61.7 for ODDMANOUT classification

2019-03-13 09:34:11,169 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 09:34:11,586 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 09:34:11,670 : loading BERT model bert-large-uncased
2019-03-13 09:34:11,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:34:11,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:34:11,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeo314_5i
2019-03-13 09:34:19,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:34:24,616 : Computing embeddings for train/dev/test
2019-03-13 09:37:57,165 : Computed embeddings
2019-03-13 09:37:57,166 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:38:25,280 : [('reg:1e-05', 61.25), ('reg:0.0001', 68.25), ('reg:0.001', 67.45), ('reg:0.01', 65.31)]
2019-03-13 09:38:25,281 : Validation : best param found is reg = 0.0001 with score             68.25
2019-03-13 09:38:25,281 : Evaluating...
2019-03-13 09:38:34,157 : 
Dev acc : 68.2 Test acc : 67.3 for COORDINATIONINVERSION classification

2019-03-13 09:38:34,159 : total results: {'STS12': {'MSRpar': {'pearson': (0.2626656610969523, 2.6687514278484026e-13), 'spearman': SpearmanrResult(correlation=0.32811026013824024, pvalue=2.7744113845763934e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.37461537171443454, 2.121298639853547e-26), 'spearman': SpearmanrResult(correlation=0.4085845721108454, pvalue=1.525403481560098e-31), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4152550287736429, 1.4674920355443334e-20), 'spearman': SpearmanrResult(correlation=0.5515357415402162, pvalue=6.869277223859046e-38), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4551435716766533, 1.2612193876750439e-39), 'spearman': SpearmanrResult(correlation=0.486738812328792, pvalue=7.211794725659576e-46), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6355233584653714, 1.5843403983840548e-46), 'spearman': SpearmanrResult(correlation=0.515893643440386, pvalue=1.6227399195238308e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.42864059834541096, 'wmean': 0.4065297077222701}, 'spearman': {'mean': 0.458172605911696, 'wmean': 0.44291238820240714}}}, 'STS13': {'FNWN': {'pearson': (0.24556543331086705, 0.000659529119590159), 'spearman': SpearmanrResult(correlation=0.231639464133783, pvalue=0.0013406921556162236), 'nsamples': 189}, 'headlines': {'pearson': (0.5259751994972738, 1.3763583243072569e-54), 'spearman': SpearmanrResult(correlation=0.5106416039245505, pvalue=4.8083376034192233e-51), 'nsamples': 750}, 'OnWN': {'pearson': (0.3925493265021154, 4.134125321800394e-22), 'spearman': SpearmanrResult(correlation=0.3895795008671864, pvalue=8.96325574172722e-22), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3880299864367521, 'wmean': 0.44074229245759733}, 'spearman': {'mean': 0.3772868563085066, 'wmean': 0.4302101077674596}}}, 'STS14': {'deft-forum': {'pearson': (0.04598911055972275, 0.3303654489573946), 'spearman': SpearmanrResult(correlation=0.05156247653181869, pvalue=0.2750576612500559), 'nsamples': 450}, 'deft-news': {'pearson': (0.5836264726140981, 8.847432527477027e-29), 'spearman': SpearmanrResult(correlation=0.5425368168240403, pvalue=2.3269942661110596e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.45666860023380806, 6.519779924498645e-40), 'spearman': SpearmanrResult(correlation=0.4387579482034301, pvalue=1.2297378419488629e-36), 'nsamples': 750}, 'images': {'pearson': (0.3486196256423909, 7.43417969886185e-23), 'spearman': SpearmanrResult(correlation=0.34284719398863167, pvalue=4.1178237873754892e-22), 'nsamples': 750}, 'OnWN': {'pearson': (0.49672250625088277, 5.569686002464198e-48), 'spearman': SpearmanrResult(correlation=0.5206183575338442, pvalue=2.4956858607545053e-53), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5007133002844719, 7.617137946250982e-49), 'spearman': SpearmanrResult(correlation=0.46142707926063226, pvalue=8.141948041919746e-41), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.40538993593089573, 'wmean': 0.4127536175586053}, 'spearman': {'mean': 0.3929583120570663, 'wmean': 0.4023205583270491}}}, 'STS15': {'answers-forums': {'pearson': (0.41291462345721763, 7.142915669581129e-17), 'spearman': SpearmanrResult(correlation=0.4126109630186176, pvalue=7.562393693452181e-17), 'nsamples': 375}, 'answers-students': {'pearson': (0.5452586499718959, 2.6135401433749626e-59), 'spearman': SpearmanrResult(correlation=0.5667135363979311, pvalue=6.1697740326050106e-65), 'nsamples': 750}, 'belief': {'pearson': (0.4902879633865409, 4.495109507702169e-24), 'spearman': SpearmanrResult(correlation=0.5534245311847905, pvalue=1.7830267347667052e-31), 'nsamples': 375}, 'headlines': {'pearson': (0.5359324266482124, 5.480940809466017e-57), 'spearman': SpearmanrResult(correlation=0.5351930736236816, pvalue=8.314090534293213e-57), 'nsamples': 750}, 'images': {'pearson': (0.3683028790227701, 1.650098863696273e-25), 'spearman': SpearmanrResult(correlation=0.3769163025796783, pvalue=9.929782861984223e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4705393084973274, 'wmean': 0.4752738122661895}, 'spearman': {'mean': 0.4889716813609398, 'wmean': 0.4904601649257488}}}, 'STS16': {'answer-answer': {'pearson': (0.4338120073421633, 4.4409961098926046e-13), 'spearman': SpearmanrResult(correlation=0.49101308340039856, pvalue=8.094444465656119e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6136202146672923, 3.751044158716202e-27), 'spearman': SpearmanrResult(correlation=0.6212190378411975, pvalue=5.695861922670975e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.663283341409713, 1.5730463935771168e-30), 'spearman': SpearmanrResult(correlation=0.700138521026754, pvalue=3.3077952259537086e-35), 'nsamples': 230}, 'postediting': {'pearson': (0.7605029599997707, 2.78330503518957e-47), 'spearman': SpearmanrResult(correlation=0.8057322680352883, pvalue=5.675105102005342e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.37832028899666664, 1.6248075558907036e-08), 'spearman': SpearmanrResult(correlation=0.4237365134512627, pvalue=1.627628794890685e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5699077624831211, 'wmean': 0.5734962179439685}, 'spearman': {'mean': 0.6083678847509801, 'wmean': 0.6117979158130161}}}, 'MR': {'devacc': 74.29, 'acc': 72.81, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.5, 'acc': 79.63, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.32, 'acc': 85.46, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.07, 'acc': 94.97, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.73, 'acc': 80.34, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.96, 'acc': 39.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.2, 'acc': 88.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.9, 'acc': 71.42, 'f1': 81.06, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.8, 'acc': 75.36, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7284517434817513, 'pearson': 0.7714579657534245, 'spearman': 0.6998095998515821, 'mse': 0.4146528650263769, 'yhat': array([3.73076514, 4.38143144, 1.29917577, ..., 3.15387759, 4.48190213,        4.33185384]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6122148078953283, 'pearson': 0.5842587185558689, 'spearman': 0.5760347429569336, 'mse': 1.651338866698064, 'yhat': array([2.39407485, 1.22896769, 2.5100694 , ..., 3.98594582, 3.85624206,        3.3945577 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.58, 'acc': 59.22, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 74.37, 'acc': 74.27, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 39.78, 'acc': 39.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.57, 'acc': 31.7, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 77.15, 'acc': 77.42, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.04, 'acc': 85.11, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.31, 'acc': 88.93, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 89.31, 'acc': 89.37, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.85, 'acc': 81.76, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.99, 'acc': 61.68, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.25, 'acc': 67.34, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 09:38:34,159 : STS12 p=0.4065, STS12 s=0.4429, STS13 p=0.4407, STS13 s=0.4302, STS14 p=0.4128, STS14 s=0.4023, STS15 p=0.4753, STS15 s=0.4905, STS 16 p=0.5735, STS16 s=0.6118, STS B p=0.5843, STS B s=0.5760, STS B m=1.6513, SICK-R p=0.7715, SICK-R s=0.6998, SICK-P m=0.4147
2019-03-13 09:38:34,159 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 09:38:34,159 : 0.4065,0.4429,0.4407,0.4302,0.4128,0.4023,0.4753,0.4905,0.5735,0.6118,0.5843,0.5760,1.6513,0.7715,0.6998,0.4147
2019-03-13 09:38:34,159 : MR=72.81, CR=79.63, SUBJ=94.97, MPQA=85.46, SST-B=80.34, SST-F=39.64, TREC=88.80, SICK-E=75.36, SNLI=59.22, MRPC=71.42, MRPC f=81.06
2019-03-13 09:38:34,160 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 09:38:34,160 : 72.81,79.63,94.97,85.46,80.34,39.64,88.80,75.36,59.22,71.42,81.06
2019-03-13 09:38:34,160 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 09:38:34,160 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 09:38:34,160 : na,na,na,na,na,na,na,na,na,na
2019-03-13 09:38:34,160 : SentLen=74.27, WC=39.06, TreeDepth=31.70, TopConst=77.42, BShift=85.11, Tense=88.93, SubjNum=89.37, ObjNum=81.76, SOMO=61.68, CoordInv=67.34, average=69.66
2019-03-13 09:38:34,160 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 09:38:34,160 : 74.27,39.06,31.70,77.42,85.11,88.93,89.37,81.76,61.68,67.34,69.66
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,160 : layer 15
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,160 : ********************************************************************************
2019-03-13 09:38:34,246 : ***** Transfer task : STS12 *****


2019-03-13 09:38:34,258 : loading BERT model bert-large-uncased
2019-03-13 09:38:34,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:38:34,276 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:38:34,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpms_xlkhc
2019-03-13 09:38:41,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:38:51,081 : MSRpar : pearson = 0.2737, spearman = 0.3357
2019-03-13 09:38:52,710 : MSRvid : pearson = 0.2729, spearman = 0.3092
2019-03-13 09:38:54,111 : SMTeuroparl : pearson = 0.4095, spearman = 0.5282
2019-03-13 09:38:56,790 : surprise.OnWN : pearson = 0.4263, spearman = 0.4392
2019-03-13 09:38:58,207 : surprise.SMTnews : pearson = 0.6178, spearman = 0.5188
2019-03-13 09:38:58,207 : ALL (weighted average) : Pearson = 0.3746,             Spearman = 0.4062
2019-03-13 09:38:58,207 : ALL (average) : Pearson = 0.4001,             Spearman = 0.4262

2019-03-13 09:38:58,207 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 09:38:58,215 : loading BERT model bert-large-uncased
2019-03-13 09:38:58,215 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:38:58,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:38:58,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp40cgddkh
2019-03-13 09:39:05,727 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:39:12,456 : FNWN : pearson = 0.2364, spearman = 0.2228
2019-03-13 09:39:14,334 : headlines : pearson = 0.4733, spearman = 0.4581
2019-03-13 09:39:15,790 : OnWN : pearson = 0.2719, spearman = 0.2651
2019-03-13 09:39:15,790 : ALL (weighted average) : Pearson = 0.3681,             Spearman = 0.3563
2019-03-13 09:39:15,790 : ALL (average) : Pearson = 0.3272,             Spearman = 0.3153

2019-03-13 09:39:15,790 : ***** Transfer task : STS14 *****


2019-03-13 09:39:15,807 : loading BERT model bert-large-uncased
2019-03-13 09:39:15,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:39:15,825 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:39:15,825 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqxxr2qbi
2019-03-13 09:39:23,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:39:30,037 : deft-forum : pearson = 0.0173, spearman = 0.0259
2019-03-13 09:39:31,661 : deft-news : pearson = 0.5447, spearman = 0.5075
2019-03-13 09:39:33,815 : headlines : pearson = 0.4288, spearman = 0.4104
2019-03-13 09:39:35,875 : images : pearson = 0.2133, spearman = 0.2217
2019-03-13 09:39:37,988 : OnWN : pearson = 0.4311, spearman = 0.4379
2019-03-13 09:39:40,829 : tweet-news : pearson = 0.5014, spearman = 0.4708
2019-03-13 09:39:40,829 : ALL (weighted average) : Pearson = 0.3606,             Spearman = 0.3519
2019-03-13 09:39:40,829 : ALL (average) : Pearson = 0.3561,             Spearman = 0.3457

2019-03-13 09:39:40,829 : ***** Transfer task : STS15 *****


2019-03-13 09:39:40,862 : loading BERT model bert-large-uncased
2019-03-13 09:39:40,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:39:40,880 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:39:40,880 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbw2_ht_h
2019-03-13 09:39:48,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:39:55,677 : answers-forums : pearson = 0.3623, spearman = 0.3986
2019-03-13 09:39:57,745 : answers-students : pearson = 0.5195, spearman = 0.5430
2019-03-13 09:39:59,777 : belief : pearson = 0.4505, spearman = 0.5085
2019-03-13 09:40:02,014 : headlines : pearson = 0.4900, spearman = 0.4828
2019-03-13 09:40:04,132 : images : pearson = 0.2121, spearman = 0.2342
2019-03-13 09:40:04,132 : ALL (weighted average) : Pearson = 0.4070,             Spearman = 0.4284
2019-03-13 09:40:04,132 : ALL (average) : Pearson = 0.4069,             Spearman = 0.4334

2019-03-13 09:40:04,132 : ***** Transfer task : STS16 *****


2019-03-13 09:40:04,200 : loading BERT model bert-large-uncased
2019-03-13 09:40:04,200 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:40:04,218 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:40:04,218 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgr44iaz5
2019-03-13 09:40:11,701 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:40:17,950 : answer-answer : pearson = 0.4064, spearman = 0.4615
2019-03-13 09:40:18,606 : headlines : pearson = 0.5910, spearman = 0.6005
2019-03-13 09:40:19,480 : plagiarism : pearson = 0.6223, spearman = 0.6995
2019-03-13 09:40:20,962 : postediting : pearson = 0.7468, spearman = 0.7929
2019-03-13 09:40:21,564 : question-question : pearson = 0.2568, spearman = 0.3261
2019-03-13 09:40:21,564 : ALL (weighted average) : Pearson = 0.5307,             Spearman = 0.5812
2019-03-13 09:40:21,564 : ALL (average) : Pearson = 0.5247,             Spearman = 0.5761

2019-03-13 09:40:21,564 : ***** Transfer task : MR *****


2019-03-13 09:40:21,583 : loading BERT model bert-large-uncased
2019-03-13 09:40:21,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:40:21,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:40:21,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9_fr173s
2019-03-13 09:40:29,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:40:34,463 : Generating sentence embeddings
2019-03-13 09:41:05,684 : Generated sentence embeddings
2019-03-13 09:41:05,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 09:41:13,750 : Best param found at split 1: l2reg = 0.001                 with score 75.98
2019-03-13 09:41:22,174 : Best param found at split 2: l2reg = 0.001                 with score 74.92
2019-03-13 09:41:30,066 : Best param found at split 3: l2reg = 0.01                 with score 75.8
2019-03-13 09:41:41,098 : Best param found at split 4: l2reg = 0.001                 with score 75.96
2019-03-13 09:41:51,182 : Best param found at split 5: l2reg = 1e-05                 with score 74.97
2019-03-13 09:41:51,863 : Dev acc : 75.53 Test acc : 74.46

2019-03-13 09:41:51,864 : ***** Transfer task : CR *****


2019-03-13 09:41:51,873 : loading BERT model bert-large-uncased
2019-03-13 09:41:51,873 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:41:51,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:41:51,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt8pv6shj
2019-03-13 09:41:59,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:42:04,929 : Generating sentence embeddings
2019-03-13 09:42:13,205 : Generated sentence embeddings
2019-03-13 09:42:13,205 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 09:42:16,670 : Best param found at split 1: l2reg = 1e-05                 with score 80.59
2019-03-13 09:42:20,329 : Best param found at split 2: l2reg = 0.001                 with score 81.58
2019-03-13 09:42:23,762 : Best param found at split 3: l2reg = 0.001                 with score 81.49
2019-03-13 09:42:26,590 : Best param found at split 4: l2reg = 0.001                 with score 80.24
2019-03-13 09:42:29,263 : Best param found at split 5: l2reg = 1e-05                 with score 81.17
2019-03-13 09:42:29,423 : Dev acc : 81.01 Test acc : 81.17

2019-03-13 09:42:29,423 : ***** Transfer task : MPQA *****


2019-03-13 09:42:29,429 : loading BERT model bert-large-uncased
2019-03-13 09:42:29,429 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:42:29,479 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:42:29,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppw6kjdb3
2019-03-13 09:42:36,941 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:42:42,317 : Generating sentence embeddings
2019-03-13 09:42:49,846 : Generated sentence embeddings
2019-03-13 09:42:49,847 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 09:43:00,007 : Best param found at split 1: l2reg = 1e-05                 with score 84.58
2019-03-13 09:43:11,724 : Best param found at split 2: l2reg = 0.0001                 with score 85.25
2019-03-13 09:43:21,621 : Best param found at split 3: l2reg = 0.001                 with score 83.82
2019-03-13 09:43:33,803 : Best param found at split 4: l2reg = 0.01                 with score 85.35
2019-03-13 09:43:44,097 : Best param found at split 5: l2reg = 0.001                 with score 83.15
2019-03-13 09:43:44,525 : Dev acc : 84.43 Test acc : 84.34

2019-03-13 09:43:44,526 : ***** Transfer task : SUBJ *****


2019-03-13 09:43:44,541 : loading BERT model bert-large-uncased
2019-03-13 09:43:44,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:43:44,562 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:43:44,562 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptl8oxi0d
2019-03-13 09:43:52,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:43:57,500 : Generating sentence embeddings
2019-03-13 09:44:28,184 : Generated sentence embeddings
2019-03-13 09:44:28,184 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 09:44:37,213 : Best param found at split 1: l2reg = 0.001                 with score 95.14
2019-03-13 09:44:48,165 : Best param found at split 2: l2reg = 1e-05                 with score 95.34
2019-03-13 09:44:59,140 : Best param found at split 3: l2reg = 0.001                 with score 94.94
2019-03-13 09:45:09,989 : Best param found at split 4: l2reg = 0.001                 with score 95.19
2019-03-13 09:45:19,910 : Best param found at split 5: l2reg = 0.0001                 with score 95.5
2019-03-13 09:45:20,212 : Dev acc : 95.22 Test acc : 94.68

2019-03-13 09:45:20,213 : ***** Transfer task : SST Binary classification *****


2019-03-13 09:45:20,304 : loading BERT model bert-large-uncased
2019-03-13 09:45:20,305 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:45:20,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:45:20,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps_5rz8_z
2019-03-13 09:45:27,881 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:45:33,210 : Computing embedding for train
2019-03-13 09:47:12,309 : Computed train embeddings
2019-03-13 09:47:12,309 : Computing embedding for dev
2019-03-13 09:47:14,466 : Computed dev embeddings
2019-03-13 09:47:14,466 : Computing embedding for test
2019-03-13 09:47:18,998 : Computed test embeddings
2019-03-13 09:47:18,998 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:47:34,736 : [('reg:1e-05', 81.54), ('reg:0.0001', 81.65), ('reg:0.001', 80.16), ('reg:0.01', 80.62)]
2019-03-13 09:47:34,736 : Validation : best param found is reg = 0.0001 with score             81.65
2019-03-13 09:47:34,736 : Evaluating...
2019-03-13 09:47:38,330 : 
Dev acc : 81.65 Test acc : 80.89 for             SST Binary classification

2019-03-13 09:47:38,331 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 09:47:38,381 : loading BERT model bert-large-uncased
2019-03-13 09:47:38,381 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:47:38,403 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:47:38,403 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzr3v70b8
2019-03-13 09:47:45,863 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:47:51,289 : Computing embedding for train
2019-03-13 09:48:12,998 : Computed train embeddings
2019-03-13 09:48:12,999 : Computing embedding for dev
2019-03-13 09:48:15,839 : Computed dev embeddings
2019-03-13 09:48:15,840 : Computing embedding for test
2019-03-13 09:48:21,418 : Computed test embeddings
2019-03-13 09:48:21,418 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:48:23,976 : [('reg:1e-05', 41.78), ('reg:0.0001', 41.69), ('reg:0.001', 41.42), ('reg:0.01', 40.05)]
2019-03-13 09:48:23,977 : Validation : best param found is reg = 1e-05 with score             41.78
2019-03-13 09:48:23,977 : Evaluating...
2019-03-13 09:48:24,616 : 
Dev acc : 41.78 Test acc : 43.21 for             SST Fine-Grained classification

2019-03-13 09:48:24,617 : ***** Transfer task : TREC *****


2019-03-13 09:48:24,630 : loading BERT model bert-large-uncased
2019-03-13 09:48:24,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:48:24,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:48:24,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpihd8qmrt
2019-03-13 09:48:32,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:48:44,953 : Computed train embeddings
2019-03-13 09:48:45,537 : Computed test embeddings
2019-03-13 09:48:45,537 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 09:48:52,215 : [('reg:1e-05', 82.55), ('reg:0.0001', 84.04), ('reg:0.001', 77.29), ('reg:0.01', 67.57)]
2019-03-13 09:48:52,215 : Cross-validation : best param found is reg = 0.0001             with score 84.04
2019-03-13 09:48:52,215 : Evaluating...
2019-03-13 09:48:52,726 : 
Dev acc : 84.04 Test acc : 91.8             for TREC

2019-03-13 09:48:52,727 : ***** Transfer task : MRPC *****


2019-03-13 09:48:52,750 : loading BERT model bert-large-uncased
2019-03-13 09:48:52,750 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:48:52,771 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:48:52,771 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgkvltd9g
2019-03-13 09:49:00,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:49:05,678 : Computing embedding for train
2019-03-13 09:49:27,724 : Computed train embeddings
2019-03-13 09:49:27,724 : Computing embedding for test
2019-03-13 09:49:37,371 : Computed test embeddings
2019-03-13 09:49:37,392 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 09:49:41,096 : [('reg:1e-05', 70.34), ('reg:0.0001', 70.63), ('reg:0.001', 70.98), ('reg:0.01', 70.68)]
2019-03-13 09:49:41,096 : Cross-validation : best param found is reg = 0.001             with score 70.98
2019-03-13 09:49:41,096 : Evaluating...
2019-03-13 09:49:41,269 : Dev acc : 70.98 Test acc 69.68; Test F1 77.89 for MRPC.

2019-03-13 09:49:41,270 : ***** Transfer task : SICK-Entailment*****


2019-03-13 09:49:41,332 : loading BERT model bert-large-uncased
2019-03-13 09:49:41,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:49:41,353 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:49:41,353 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdqajz436
2019-03-13 09:49:48,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:49:54,435 : Computing embedding for train
2019-03-13 09:50:05,646 : Computed train embeddings
2019-03-13 09:50:05,646 : Computing embedding for dev
2019-03-13 09:50:07,172 : Computed dev embeddings
2019-03-13 09:50:07,172 : Computing embedding for test
2019-03-13 09:50:19,153 : Computed test embeddings
2019-03-13 09:50:19,190 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:50:20,363 : [('reg:1e-05', 74.2), ('reg:0.0001', 76.0), ('reg:0.001', 73.0), ('reg:0.01', 72.8)]
2019-03-13 09:50:20,364 : Validation : best param found is reg = 0.0001 with score             76.0
2019-03-13 09:50:20,364 : Evaluating...
2019-03-13 09:50:20,701 : 
Dev acc : 76.0 Test acc : 72.78 for                        SICK entailment

2019-03-13 09:50:20,701 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 09:50:20,729 : loading BERT model bert-large-uncased
2019-03-13 09:50:20,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:50:20,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:50:20,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeq0pgska
2019-03-13 09:50:28,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:50:33,624 : Computing embedding for train
2019-03-13 09:50:44,825 : Computed train embeddings
2019-03-13 09:50:44,825 : Computing embedding for dev
2019-03-13 09:50:46,352 : Computed dev embeddings
2019-03-13 09:50:46,352 : Computing embedding for test
2019-03-13 09:50:58,345 : Computed test embeddings
2019-03-13 09:51:10,904 : Dev : Pearson 0.7194936635770355
2019-03-13 09:51:10,906 : Test : Pearson 0.7550951192457072 Spearman 0.6805378394923466 MSE 0.43933985384926855                        for SICK Relatedness

2019-03-13 09:51:10,907 : 

***** Transfer task : STSBenchmark*****


2019-03-13 09:51:10,976 : loading BERT model bert-large-uncased
2019-03-13 09:51:10,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:51:10,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:51:10,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8xsc6lv2
2019-03-13 09:51:18,453 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:51:23,828 : Computing embedding for train
2019-03-13 09:51:42,245 : Computed train embeddings
2019-03-13 09:51:42,245 : Computing embedding for dev
2019-03-13 09:51:47,831 : Computed dev embeddings
2019-03-13 09:51:47,831 : Computing embedding for test
2019-03-13 09:51:52,390 : Computed test embeddings
2019-03-13 09:52:08,319 : Dev : Pearson 0.5851745502998742
2019-03-13 09:52:08,319 : Test : Pearson 0.58971057813778 Spearman 0.5896403168708482 MSE 1.625605005081301                        for SICK Relatedness

2019-03-13 09:52:08,319 : ***** Transfer task : SNLI Entailment*****


2019-03-13 09:52:13,382 : loading BERT model bert-large-uncased
2019-03-13 09:52:13,382 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:52:13,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:52:13,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwvht9ddz
2019-03-13 09:52:20,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:52:26,845 : PROGRESS (encoding): 0.00%
2019-03-13 09:55:10,662 : PROGRESS (encoding): 14.56%
2019-03-13 09:58:16,391 : PROGRESS (encoding): 29.12%
2019-03-13 10:01:22,992 : PROGRESS (encoding): 43.69%
2019-03-13 10:04:41,824 : PROGRESS (encoding): 58.25%
2019-03-13 10:08:23,111 : PROGRESS (encoding): 72.81%
2019-03-13 10:12:03,425 : PROGRESS (encoding): 87.37%
2019-03-13 10:16:01,781 : PROGRESS (encoding): 0.00%
2019-03-13 10:16:31,769 : PROGRESS (encoding): 0.00%
2019-03-13 10:17:00,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:17:31,067 : [('reg:1e-09', 60.85)]
2019-03-13 10:17:31,067 : Validation : best param found is reg = 1e-09 with score             60.85
2019-03-13 10:17:31,067 : Evaluating...
2019-03-13 10:18:02,810 : Dev acc : 60.85 Test acc : 61.22 for SNLI

2019-03-13 10:18:02,811 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 10:18:03,021 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 10:18:04,094 : loading BERT model bert-large-uncased
2019-03-13 10:18:04,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:18:04,121 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:18:04,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfbiq0vqt
2019-03-13 10:18:11,614 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:18:16,962 : Computing embeddings for train/dev/test
2019-03-13 10:21:45,391 : Computed embeddings
2019-03-13 10:21:45,391 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:22:17,422 : [('reg:1e-05', 77.11), ('reg:0.0001', 73.01), ('reg:0.001', 66.46), ('reg:0.01', 55.03)]
2019-03-13 10:22:17,422 : Validation : best param found is reg = 1e-05 with score             77.11
2019-03-13 10:22:17,422 : Evaluating...
2019-03-13 10:22:24,310 : 
Dev acc : 77.1 Test acc : 79.3 for LENGTH classification

2019-03-13 10:22:24,311 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 10:22:24,566 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 10:22:24,614 : loading BERT model bert-large-uncased
2019-03-13 10:22:24,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:22:24,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:22:24,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm5f9e94k
2019-03-13 10:22:32,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:22:37,357 : Computing embeddings for train/dev/test
2019-03-13 10:25:49,499 : Computed embeddings
2019-03-13 10:25:49,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:26:23,410 : [('reg:1e-05', 37.95), ('reg:0.0001', 11.78), ('reg:0.001', 1.3), ('reg:0.01', 0.46)]
2019-03-13 10:26:23,410 : Validation : best param found is reg = 1e-05 with score             37.95
2019-03-13 10:26:23,410 : Evaluating...
2019-03-13 10:26:31,216 : 
Dev acc : 38.0 Test acc : 37.7 for WORDCONTENT classification

2019-03-13 10:26:31,218 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 10:26:31,769 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 10:26:31,835 : loading BERT model bert-large-uncased
2019-03-13 10:26:31,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:26:31,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:26:31,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvwb_ydq_
2019-03-13 10:26:39,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:26:44,521 : Computing embeddings for train/dev/test
2019-03-13 10:29:45,149 : Computed embeddings
2019-03-13 10:29:45,149 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:30:11,441 : [('reg:1e-05', 29.72), ('reg:0.0001', 25.94), ('reg:0.001', 29.2), ('reg:0.01', 23.43)]
2019-03-13 10:30:11,442 : Validation : best param found is reg = 1e-05 with score             29.72
2019-03-13 10:30:11,442 : Evaluating...
2019-03-13 10:30:16,923 : 
Dev acc : 29.7 Test acc : 29.8 for DEPTH classification

2019-03-13 10:30:16,924 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 10:30:17,292 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 10:30:17,355 : loading BERT model bert-large-uncased
2019-03-13 10:30:17,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:30:17,466 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:30:17,466 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc972a44_
2019-03-13 10:30:24,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:30:30,211 : Computing embeddings for train/dev/test
2019-03-13 10:33:17,565 : Computed embeddings
2019-03-13 10:33:17,565 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:33:48,553 : [('reg:1e-05', 72.77), ('reg:0.0001', 72.85), ('reg:0.001', 71.23), ('reg:0.01', 63.2)]
2019-03-13 10:33:48,554 : Validation : best param found is reg = 0.0001 with score             72.85
2019-03-13 10:33:48,554 : Evaluating...
2019-03-13 10:33:56,242 : 
Dev acc : 72.8 Test acc : 72.9 for TOPCONSTITUENTS classification

2019-03-13 10:33:56,243 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 10:33:56,623 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 10:33:56,689 : loading BERT model bert-large-uncased
2019-03-13 10:33:56,689 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:33:56,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:33:56,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiexg17dv
2019-03-13 10:34:04,181 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:34:09,560 : Computing embeddings for train/dev/test
2019-03-13 10:37:11,318 : Computed embeddings
2019-03-13 10:37:11,318 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:37:40,720 : [('reg:1e-05', 86.65), ('reg:0.0001', 86.65), ('reg:0.001', 86.33), ('reg:0.01', 85.16)]
2019-03-13 10:37:40,720 : Validation : best param found is reg = 1e-05 with score             86.65
2019-03-13 10:37:40,720 : Evaluating...
2019-03-13 10:37:49,472 : 
Dev acc : 86.7 Test acc : 86.0 for BIGRAMSHIFT classification

2019-03-13 10:37:49,473 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 10:37:49,898 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 10:37:49,969 : loading BERT model bert-large-uncased
2019-03-13 10:37:49,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:37:50,004 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:37:50,004 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpedeykvb8
2019-03-13 10:37:57,535 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:38:02,831 : Computing embeddings for train/dev/test
2019-03-13 10:41:00,721 : Computed embeddings
2019-03-13 10:41:00,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:41:26,605 : [('reg:1e-05', 90.06), ('reg:0.0001', 89.65), ('reg:0.001', 89.48), ('reg:0.01', 89.79)]
2019-03-13 10:41:26,605 : Validation : best param found is reg = 1e-05 with score             90.06
2019-03-13 10:41:26,605 : Evaluating...
2019-03-13 10:41:33,093 : 
Dev acc : 90.1 Test acc : 89.0 for TENSE classification

2019-03-13 10:41:33,095 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 10:41:33,497 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 10:41:33,560 : loading BERT model bert-large-uncased
2019-03-13 10:41:33,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:41:33,675 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:41:33,675 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8u373iu6
2019-03-13 10:41:41,126 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:41:46,514 : Computing embeddings for train/dev/test
2019-03-13 10:44:54,759 : Computed embeddings
2019-03-13 10:44:54,759 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:45:23,335 : [('reg:1e-05', 88.98), ('reg:0.0001', 89.13), ('reg:0.001', 88.69), ('reg:0.01', 88.21)]
2019-03-13 10:45:23,335 : Validation : best param found is reg = 0.0001 with score             89.13
2019-03-13 10:45:23,335 : Evaluating...
2019-03-13 10:45:28,777 : 
Dev acc : 89.1 Test acc : 89.5 for SUBJNUMBER classification

2019-03-13 10:45:28,778 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 10:45:29,185 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 10:45:29,253 : loading BERT model bert-large-uncased
2019-03-13 10:45:29,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:45:29,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:45:29,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9rh2dlse
2019-03-13 10:45:36,860 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:45:42,207 : Computing embeddings for train/dev/test
2019-03-13 10:48:47,121 : Computed embeddings
2019-03-13 10:48:47,122 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:49:19,315 : [('reg:1e-05', 80.17), ('reg:0.0001', 80.29), ('reg:0.001', 80.3), ('reg:0.01', 78.74)]
2019-03-13 10:49:19,315 : Validation : best param found is reg = 0.001 with score             80.3
2019-03-13 10:49:19,315 : Evaluating...
2019-03-13 10:49:27,876 : 
Dev acc : 80.3 Test acc : 80.7 for OBJNUMBER classification

2019-03-13 10:49:27,877 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 10:49:28,446 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 10:49:28,516 : loading BERT model bert-large-uncased
2019-03-13 10:49:28,516 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:49:28,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:49:28,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5zf6ii9q
2019-03-13 10:49:35,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:49:41,382 : Computing embeddings for train/dev/test
2019-03-13 10:53:15,660 : Computed embeddings
2019-03-13 10:53:15,660 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:53:35,399 : [('reg:1e-05', 60.92), ('reg:0.0001', 60.95), ('reg:0.001', 61.07), ('reg:0.01', 59.94)]
2019-03-13 10:53:35,400 : Validation : best param found is reg = 0.001 with score             61.07
2019-03-13 10:53:35,400 : Evaluating...
2019-03-13 10:53:39,221 : 
Dev acc : 61.1 Test acc : 61.2 for ODDMANOUT classification

2019-03-13 10:53:39,222 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 10:53:39,614 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 10:53:39,694 : loading BERT model bert-large-uncased
2019-03-13 10:53:39,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:53:39,724 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:53:39,725 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjngj25v4
2019-03-13 10:53:47,230 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:53:52,586 : Computing embeddings for train/dev/test
2019-03-13 10:57:25,011 : Computed embeddings
2019-03-13 10:57:25,011 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:57:49,571 : [('reg:1e-05', 69.78), ('reg:0.0001', 72.19), ('reg:0.001', 70.86), ('reg:0.01', 67.72)]
2019-03-13 10:57:49,571 : Validation : best param found is reg = 0.0001 with score             72.19
2019-03-13 10:57:49,571 : Evaluating...
2019-03-13 10:57:54,922 : 
Dev acc : 72.2 Test acc : 71.3 for COORDINATIONINVERSION classification

2019-03-13 10:57:54,924 : total results: {'STS12': {'MSRpar': {'pearson': (0.27372073575829203, 2.3506064677631348e-14), 'spearman': SpearmanrResult(correlation=0.33573003849424116, pvalue=3.2365757223157186e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.2728546486345707, 2.855002253832485e-14), 'spearman': SpearmanrResult(correlation=0.3091654600186113, pvalue=4.479051731093953e-18), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4095411033558428, 5.444728751348028e-20), 'spearman': SpearmanrResult(correlation=0.5281590988338131, pvalue=2.4625040618472064e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.42633424184639485, 1.7771879376858116e-34), 'spearman': SpearmanrResult(correlation=0.4391745494508825, pvalue=1.03709714726476e-36), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6178257664742901, 2.2987946900019166e-43), 'spearman': SpearmanrResult(correlation=0.518831036045512, pvalue=7.080144007020566e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.40005529921387806, 'wmean': 0.3745733806122962}, 'spearman': {'mean': 0.426212036568612, 'wmean': 0.40620693234223965}}}, 'STS13': {'FNWN': {'pearson': (0.23644564922275615, 0.0010543844828763665), 'spearman': SpearmanrResult(correlation=0.22281129383273698, pvalue=0.0020584879022811755), 'nsamples': 189}, 'headlines': {'pearson': (0.4732618215798183, 3.989410695579139e-43), 'spearman': SpearmanrResult(correlation=0.45807337196575637, pvalue=3.539862458631565e-40), 'nsamples': 750}, 'OnWN': {'pearson': (0.2718770586857163, 5.7954762083869115e-11), 'spearman': SpearmanrResult(correlation=0.2651290282501319, pvalue=1.7683477122626354e-10), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3271948431627636, 'wmean': 0.3681050825404343}, 'spearman': {'mean': 0.31533789801620843, 'wmean': 0.3562691655713524}}}, 'STS14': {'deft-forum': {'pearson': (0.017310419010783958, 0.7142042404012461), 'spearman': SpearmanrResult(correlation=0.025926392431381593, pvalue=0.5833181729690078), 'nsamples': 450}, 'deft-news': {'pearson': (0.5446531713041105, 1.4249551395018296e-24), 'spearman': SpearmanrResult(correlation=0.5074602026806804, pvalue=4.868296611346795e-21), 'nsamples': 300}, 'headlines': {'pearson': (0.42877892521179645, 6.788586751461631e-35), 'spearman': SpearmanrResult(correlation=0.41035337540864336, pvalue=7.922490440075255e-32), 'nsamples': 750}, 'images': {'pearson': (0.21326258067145626, 3.6641269641353707e-09), 'spearman': SpearmanrResult(correlation=0.22171310759110988, pvalue=8.341449654966269e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.4311284972206003, 2.672066532028456e-35), 'spearman': SpearmanrResult(correlation=0.4378675831346141, pvalue=1.7685086720126616e-36), 'nsamples': 750}, 'tweet-news': {'pearson': (0.501441080141731, 5.2842896468806865e-49), 'spearman': SpearmanrResult(correlation=0.4708303663433784, pvalue=1.2102510798546191e-42), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3560957789267465, 'wmean': 0.36057172063473975}, 'spearman': {'mean': 0.34569183793163466, 'wmean': 0.3518608698017694}}}, 'STS15': {'answers-forums': {'pearson': (0.36231467509287457, 4.479134348047203e-13), 'spearman': SpearmanrResult(correlation=0.3985631435257391, pvalue=9.954433200196356e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.5195078908127394, 4.521342990666106e-53), 'spearman': SpearmanrResult(correlation=0.5430198425762451, pvalue=9.576479454778952e-59), 'nsamples': 750}, 'belief': {'pearson': (0.45052350240753886, 3.824043281439632e-20), 'spearman': SpearmanrResult(correlation=0.5084793594539695, pvalue=4.746703242957327e-26), 'nsamples': 375}, 'headlines': {'pearson': (0.4899719865769264, 1.519276384690567e-46), 'spearman': SpearmanrResult(correlation=0.48275231486367726, pvalue=4.810107215909e-45), 'nsamples': 750}, 'images': {'pearson': (0.21214172406172105, 4.438717722987966e-09), 'spearman': SpearmanrResult(correlation=0.2342304119381582, pvalue=8.329726219193524e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4068919557903601, 'wmean': 0.4070101725503984}, 'spearman': {'mean': 0.4334090144715578, 'wmean': 0.4283809552169837}}}, 'STS16': {'answer-answer': {'pearson': (0.40635547902072006, 1.6142324657834258e-11), 'spearman': SpearmanrResult(correlation=0.4614594841499455, pvalue=8.45705805313772e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.5910401885807691, 7.594941150049274e-25), 'spearman': SpearmanrResult(correlation=0.6005398185362992, pvalue=8.559693181971044e-26), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6223447217873301, 4.685320986025637e-26), 'spearman': SpearmanrResult(correlation=0.6994667030645657, pvalue=4.084877435506328e-35), 'nsamples': 230}, 'postediting': {'pearson': (0.7467818164932748, 9.305703319122304e-45), 'spearman': SpearmanrResult(correlation=0.7929226392728944, pvalue=5.524626394793433e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.25682665692903844, 0.00017417914160116957), 'spearman': SpearmanrResult(correlation=0.3260746972509224, pvalue=1.449509092504977e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5246697725622266, 'wmean': 0.530703304520648}, 'spearman': {'mean': 0.5760926684549255, 'wmean': 0.5811510971353321}}}, 'MR': {'devacc': 75.53, 'acc': 74.46, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.01, 'acc': 81.17, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.43, 'acc': 84.34, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.22, 'acc': 94.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.65, 'acc': 80.89, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.78, 'acc': 43.21, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.04, 'acc': 91.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.98, 'acc': 69.68, 'f1': 77.89, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.0, 'acc': 72.78, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7194936635770355, 'pearson': 0.7550951192457072, 'spearman': 0.6805378394923466, 'mse': 0.43933985384926855, 'yhat': array([4.11568072, 4.63854463, 1.65970075, ..., 3.32874126, 4.62627333,        4.03879677]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5851745502998742, 'pearson': 0.58971057813778, 'spearman': 0.5896403168708482, 'mse': 1.625605005081301, 'yhat': array([2.81705261, 2.36299614, 2.68439884, ..., 3.85704083, 3.75694719,        3.49616461]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.85, 'acc': 61.22, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 77.11, 'acc': 79.32, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 37.95, 'acc': 37.72, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.72, 'acc': 29.78, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.85, 'acc': 72.86, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.65, 'acc': 86.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.06, 'acc': 88.96, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 89.13, 'acc': 89.48, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.3, 'acc': 80.66, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.07, 'acc': 61.16, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.19, 'acc': 71.27, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 10:57:54,924 : STS12 p=0.3746, STS12 s=0.4062, STS13 p=0.3681, STS13 s=0.3563, STS14 p=0.3606, STS14 s=0.3519, STS15 p=0.4070, STS15 s=0.4284, STS 16 p=0.5307, STS16 s=0.5812, STS B p=0.5897, STS B s=0.5896, STS B m=1.6256, SICK-R p=0.7551, SICK-R s=0.6805, SICK-P m=0.4393
2019-03-13 10:57:54,924 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 10:57:54,924 : 0.3746,0.4062,0.3681,0.3563,0.3606,0.3519,0.4070,0.4284,0.5307,0.5812,0.5897,0.5896,1.6256,0.7551,0.6805,0.4393
2019-03-13 10:57:54,924 : MR=74.46, CR=81.17, SUBJ=94.68, MPQA=84.34, SST-B=80.89, SST-F=43.21, TREC=91.80, SICK-E=72.78, SNLI=61.22, MRPC=69.68, MRPC f=77.89
2019-03-13 10:57:54,924 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 10:57:54,924 : 74.46,81.17,94.68,84.34,80.89,43.21,91.80,72.78,61.22,69.68,77.89
2019-03-13 10:57:54,924 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 10:57:54,924 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 10:57:54,924 : na,na,na,na,na,na,na,na,na,na
2019-03-13 10:57:54,924 : SentLen=79.32, WC=37.72, TreeDepth=29.78, TopConst=72.86, BShift=86.00, Tense=88.96, SubjNum=89.48, ObjNum=80.66, SOMO=61.16, CoordInv=71.27, average=69.72
2019-03-13 10:57:54,924 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 10:57:54,924 : 79.32,37.72,29.78,72.86,86.00,88.96,89.48,80.66,61.16,71.27,69.72
2019-03-13 10:57:54,924 : ********************************************************************************
2019-03-13 10:57:54,924 : ********************************************************************************
2019-03-13 10:57:54,925 : ********************************************************************************
2019-03-13 10:57:54,925 : layer 16
2019-03-13 10:57:54,925 : ********************************************************************************
2019-03-13 10:57:54,925 : ********************************************************************************
2019-03-13 10:57:54,925 : ********************************************************************************
2019-03-13 10:57:55,016 : ***** Transfer task : STS12 *****


2019-03-13 10:57:55,028 : loading BERT model bert-large-uncased
2019-03-13 10:57:55,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:57:55,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:57:55,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps_n7_n9n
2019-03-13 10:58:02,492 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:58:11,839 : MSRpar : pearson = 0.2943, spearman = 0.3406
2019-03-13 10:58:13,471 : MSRvid : pearson = 0.1837, spearman = 0.2169
2019-03-13 10:58:14,879 : SMTeuroparl : pearson = 0.4433, spearman = 0.5254
2019-03-13 10:58:17,564 : surprise.OnWN : pearson = 0.3918, spearman = 0.4213
2019-03-13 10:58:18,985 : surprise.SMTnews : pearson = 0.6120, spearman = 0.5152
2019-03-13 10:58:18,985 : ALL (weighted average) : Pearson = 0.3539,             Spearman = 0.3799
2019-03-13 10:58:18,985 : ALL (average) : Pearson = 0.3850,             Spearman = 0.4039

2019-03-13 10:58:18,985 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 10:58:18,993 : loading BERT model bert-large-uncased
2019-03-13 10:58:18,994 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:58:19,011 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:58:19,011 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjb9m7flc
2019-03-13 10:58:26,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:58:33,257 : FNWN : pearson = 0.2672, spearman = 0.2718
2019-03-13 10:58:35,137 : headlines : pearson = 0.4409, spearman = 0.4305
2019-03-13 10:58:36,595 : OnWN : pearson = 0.2391, spearman = 0.2361
2019-03-13 10:58:36,595 : ALL (weighted average) : Pearson = 0.3435,             Spearman = 0.3378
2019-03-13 10:58:36,595 : ALL (average) : Pearson = 0.3157,             Spearman = 0.3128

2019-03-13 10:58:36,595 : ***** Transfer task : STS14 *****


2019-03-13 10:58:36,610 : loading BERT model bert-large-uncased
2019-03-13 10:58:36,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:58:36,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:58:36,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkofq3z34
2019-03-13 10:58:44,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:58:50,672 : deft-forum : pearson = -0.0005, spearman = 0.0088
2019-03-13 10:58:52,300 : deft-news : pearson = 0.4924, spearman = 0.4730
2019-03-13 10:58:54,453 : headlines : pearson = 0.4029, spearman = 0.3853
2019-03-13 10:58:56,513 : images : pearson = 0.1839, spearman = 0.1955
2019-03-13 10:58:58,651 : OnWN : pearson = 0.4099, spearman = 0.4138
2019-03-13 10:59:01,519 : tweet-news : pearson = 0.4889, spearman = 0.4608
2019-03-13 10:59:01,519 : ALL (weighted average) : Pearson = 0.3365,             Spearman = 0.3300
2019-03-13 10:59:01,519 : ALL (average) : Pearson = 0.3296,             Spearman = 0.3228

2019-03-13 10:59:01,520 : ***** Transfer task : STS15 *****


2019-03-13 10:59:01,568 : loading BERT model bert-large-uncased
2019-03-13 10:59:01,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:59:01,585 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:59:01,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa62r1kgu
2019-03-13 10:59:09,035 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:59:16,078 : answers-forums : pearson = 0.3706, spearman = 0.3966
2019-03-13 10:59:18,146 : answers-students : pearson = 0.5200, spearman = 0.5311
2019-03-13 10:59:20,178 : belief : pearson = 0.4354, spearman = 0.4765
2019-03-13 10:59:22,411 : headlines : pearson = 0.4687, spearman = 0.4629
2019-03-13 10:59:24,532 : images : pearson = 0.2035, spearman = 0.2161
2019-03-13 10:59:24,532 : ALL (weighted average) : Pearson = 0.3988,             Spearman = 0.4117
2019-03-13 10:59:24,532 : ALL (average) : Pearson = 0.3996,             Spearman = 0.4166

2019-03-13 10:59:24,532 : ***** Transfer task : STS16 *****


2019-03-13 10:59:24,602 : loading BERT model bert-large-uncased
2019-03-13 10:59:24,603 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:59:24,620 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:59:24,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpatcxnfw7
2019-03-13 10:59:32,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:59:38,248 : answer-answer : pearson = 0.3907, spearman = 0.4187
2019-03-13 10:59:38,904 : headlines : pearson = 0.5758, spearman = 0.5821
2019-03-13 10:59:39,780 : plagiarism : pearson = 0.6205, spearman = 0.6540
2019-03-13 10:59:41,263 : postediting : pearson = 0.7391, spearman = 0.7793
2019-03-13 10:59:41,866 : question-question : pearson = 0.1237, spearman = 0.2021
2019-03-13 10:59:41,866 : ALL (weighted average) : Pearson = 0.4987,             Spearman = 0.5347
2019-03-13 10:59:41,866 : ALL (average) : Pearson = 0.4900,             Spearman = 0.5272

2019-03-13 10:59:41,866 : ***** Transfer task : MR *****


2019-03-13 10:59:41,881 : loading BERT model bert-large-uncased
2019-03-13 10:59:41,881 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:59:41,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:59:41,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0h52tz8u
2019-03-13 10:59:49,357 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:59:54,703 : Generating sentence embeddings
2019-03-13 11:00:25,946 : Generated sentence embeddings
2019-03-13 11:00:25,947 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:00:35,242 : Best param found at split 1: l2reg = 0.01                 with score 78.33
2019-03-13 11:00:42,432 : Best param found at split 2: l2reg = 0.001                 with score 76.92
2019-03-13 11:00:51,108 : Best param found at split 3: l2reg = 0.001                 with score 77.5
2019-03-13 11:01:02,304 : Best param found at split 4: l2reg = 0.001                 with score 78.44
2019-03-13 11:01:12,290 : Best param found at split 5: l2reg = 0.0001                 with score 77.19
2019-03-13 11:01:13,188 : Dev acc : 77.68 Test acc : 77.0

2019-03-13 11:01:13,189 : ***** Transfer task : CR *****


2019-03-13 11:01:13,197 : loading BERT model bert-large-uncased
2019-03-13 11:01:13,197 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:01:13,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:01:13,217 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc__1f9jd
2019-03-13 11:01:20,677 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:01:26,004 : Generating sentence embeddings
2019-03-13 11:01:34,282 : Generated sentence embeddings
2019-03-13 11:01:34,283 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:01:37,931 : Best param found at split 1: l2reg = 0.001                 with score 82.91
2019-03-13 11:01:41,593 : Best param found at split 2: l2reg = 0.01                 with score 83.34
2019-03-13 11:01:44,719 : Best param found at split 3: l2reg = 0.001                 with score 83.44
2019-03-13 11:01:48,509 : Best param found at split 4: l2reg = 0.001                 with score 83.45
2019-03-13 11:01:51,279 : Best param found at split 5: l2reg = 0.001                 with score 82.65
2019-03-13 11:01:51,443 : Dev acc : 83.16 Test acc : 83.87

2019-03-13 11:01:51,444 : ***** Transfer task : MPQA *****


2019-03-13 11:01:51,449 : loading BERT model bert-large-uncased
2019-03-13 11:01:51,449 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:01:51,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:01:51,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpspv8ulbs
2019-03-13 11:01:59,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:02:04,469 : Generating sentence embeddings
2019-03-13 11:02:11,995 : Generated sentence embeddings
2019-03-13 11:02:11,995 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:02:21,805 : Best param found at split 1: l2reg = 0.01                 with score 81.75
2019-03-13 11:02:31,822 : Best param found at split 2: l2reg = 0.0001                 with score 84.9
2019-03-13 11:02:40,344 : Best param found at split 3: l2reg = 1e-05                 with score 84.3
2019-03-13 11:02:51,194 : Best param found at split 4: l2reg = 1e-05                 with score 85.56
2019-03-13 11:03:00,828 : Best param found at split 5: l2reg = 0.01                 with score 82.16
2019-03-13 11:03:01,449 : Dev acc : 83.73 Test acc : 83.26

2019-03-13 11:03:01,450 : ***** Transfer task : SUBJ *****


2019-03-13 11:03:01,466 : loading BERT model bert-large-uncased
2019-03-13 11:03:01,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:03:01,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:03:01,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkdqs96c6
2019-03-13 11:03:08,938 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:03:14,319 : Generating sentence embeddings
2019-03-13 11:03:45,025 : Generated sentence embeddings
2019-03-13 11:03:45,025 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:03:54,687 : Best param found at split 1: l2reg = 1e-05                 with score 95.32
2019-03-13 11:04:05,440 : Best param found at split 2: l2reg = 0.001                 with score 95.51
2019-03-13 11:04:16,436 : Best param found at split 3: l2reg = 0.001                 with score 95.29
2019-03-13 11:04:27,038 : Best param found at split 4: l2reg = 0.001                 with score 95.57
2019-03-13 11:04:38,697 : Best param found at split 5: l2reg = 0.001                 with score 95.36
2019-03-13 11:04:39,201 : Dev acc : 95.41 Test acc : 94.9

2019-03-13 11:04:39,202 : ***** Transfer task : SST Binary classification *****


2019-03-13 11:04:39,295 : loading BERT model bert-large-uncased
2019-03-13 11:04:39,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:04:39,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:04:39,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2a42kx_c
2019-03-13 11:04:46,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:04:52,174 : Computing embedding for train
2019-03-13 11:06:31,393 : Computed train embeddings
2019-03-13 11:06:31,393 : Computing embedding for dev
2019-03-13 11:06:33,555 : Computed dev embeddings
2019-03-13 11:06:33,555 : Computing embedding for test
2019-03-13 11:06:38,093 : Computed test embeddings
2019-03-13 11:06:38,093 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:06:58,178 : [('reg:1e-05', 82.8), ('reg:0.0001', 82.8), ('reg:0.001', 82.45), ('reg:0.01', 81.19)]
2019-03-13 11:06:58,179 : Validation : best param found is reg = 1e-05 with score             82.8
2019-03-13 11:06:58,179 : Evaluating...
2019-03-13 11:07:02,405 : 
Dev acc : 82.8 Test acc : 83.25 for             SST Binary classification

2019-03-13 11:07:02,406 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 11:07:02,460 : loading BERT model bert-large-uncased
2019-03-13 11:07:02,461 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:07:02,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:07:02,481 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1w8z_nbl
2019-03-13 11:07:09,941 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:07:15,335 : Computing embedding for train
2019-03-13 11:07:37,047 : Computed train embeddings
2019-03-13 11:07:37,047 : Computing embedding for dev
2019-03-13 11:07:39,877 : Computed dev embeddings
2019-03-13 11:07:39,877 : Computing embedding for test
2019-03-13 11:07:45,455 : Computed test embeddings
2019-03-13 11:07:45,455 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:07:47,199 : [('reg:1e-05', 37.87), ('reg:0.0001', 40.15), ('reg:0.001', 41.42), ('reg:0.01', 40.05)]
2019-03-13 11:07:47,199 : Validation : best param found is reg = 0.001 with score             41.42
2019-03-13 11:07:47,199 : Evaluating...
2019-03-13 11:07:47,719 : 
Dev acc : 41.42 Test acc : 44.07 for             SST Fine-Grained classification

2019-03-13 11:07:47,720 : ***** Transfer task : TREC *****


2019-03-13 11:07:47,734 : loading BERT model bert-large-uncased
2019-03-13 11:07:47,734 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:07:47,753 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:07:47,753 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpertc5p1c
2019-03-13 11:07:55,207 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:08:07,960 : Computed train embeddings
2019-03-13 11:08:08,544 : Computed test embeddings
2019-03-13 11:08:08,544 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:08:16,383 : [('reg:1e-05', 80.63), ('reg:0.0001', 80.89), ('reg:0.001', 82.81), ('reg:0.01', 72.82)]
2019-03-13 11:08:16,383 : Cross-validation : best param found is reg = 0.001             with score 82.81
2019-03-13 11:08:16,383 : Evaluating...
2019-03-13 11:08:16,935 : 
Dev acc : 82.81 Test acc : 91.0             for TREC

2019-03-13 11:08:16,936 : ***** Transfer task : MRPC *****


2019-03-13 11:08:16,957 : loading BERT model bert-large-uncased
2019-03-13 11:08:16,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:08:16,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:08:16,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_qwlbsnt
2019-03-13 11:08:24,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:08:29,620 : Computing embedding for train
2019-03-13 11:08:51,691 : Computed train embeddings
2019-03-13 11:08:51,691 : Computing embedding for test
2019-03-13 11:09:01,341 : Computed test embeddings
2019-03-13 11:09:01,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:09:06,095 : [('reg:1e-05', 70.78), ('reg:0.0001', 70.41), ('reg:0.001', 70.66), ('reg:0.01', 70.07)]
2019-03-13 11:09:06,096 : Cross-validation : best param found is reg = 1e-05             with score 70.78
2019-03-13 11:09:06,096 : Evaluating...
2019-03-13 11:09:06,322 : Dev acc : 70.78 Test acc 69.74; Test F1 81.05 for MRPC.

2019-03-13 11:09:06,323 : ***** Transfer task : SICK-Entailment*****


2019-03-13 11:09:06,385 : loading BERT model bert-large-uncased
2019-03-13 11:09:06,385 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:09:06,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:09:06,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb4nl53ok
2019-03-13 11:09:13,883 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:09:19,192 : Computing embedding for train
2019-03-13 11:09:30,393 : Computed train embeddings
2019-03-13 11:09:30,393 : Computing embedding for dev
2019-03-13 11:09:31,921 : Computed dev embeddings
2019-03-13 11:09:31,922 : Computing embedding for test
2019-03-13 11:09:43,929 : Computed test embeddings
2019-03-13 11:09:43,966 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:09:45,923 : [('reg:1e-05', 76.0), ('reg:0.0001', 74.6), ('reg:0.001', 77.0), ('reg:0.01', 75.2)]
2019-03-13 11:09:45,923 : Validation : best param found is reg = 0.001 with score             77.0
2019-03-13 11:09:45,924 : Evaluating...
2019-03-13 11:09:46,372 : 
Dev acc : 77.0 Test acc : 73.98 for                        SICK entailment

2019-03-13 11:09:46,373 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 11:09:46,400 : loading BERT model bert-large-uncased
2019-03-13 11:09:46,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:09:46,456 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:09:46,457 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpypx1czsy
2019-03-13 11:09:53,943 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:09:59,196 : Computing embedding for train
2019-03-13 11:10:10,411 : Computed train embeddings
2019-03-13 11:10:10,412 : Computing embedding for dev
2019-03-13 11:10:11,941 : Computed dev embeddings
2019-03-13 11:10:11,941 : Computing embedding for test
2019-03-13 11:10:23,960 : Computed test embeddings
2019-03-13 11:10:39,211 : Dev : Pearson 0.7062157706946142
2019-03-13 11:10:39,211 : Test : Pearson 0.7361368557067095 Spearman 0.6735725022975877 MSE 0.4677327722705792                        for SICK Relatedness

2019-03-13 11:10:39,212 : 

***** Transfer task : STSBenchmark*****


2019-03-13 11:10:39,281 : loading BERT model bert-large-uncased
2019-03-13 11:10:39,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:10:39,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:10:39,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa4oezd5w
2019-03-13 11:10:46,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:10:52,181 : Computing embedding for train
2019-03-13 11:11:10,630 : Computed train embeddings
2019-03-13 11:11:10,630 : Computing embedding for dev
2019-03-13 11:11:16,219 : Computed dev embeddings
2019-03-13 11:11:16,220 : Computing embedding for test
2019-03-13 11:11:20,797 : Computed test embeddings
2019-03-13 11:11:39,867 : Dev : Pearson 0.5533585694454035
2019-03-13 11:11:39,868 : Test : Pearson 0.5400507827481144 Spearman 0.5381971814295873 MSE 1.7131447661323893                        for SICK Relatedness

2019-03-13 11:11:39,868 : ***** Transfer task : SNLI Entailment*****


2019-03-13 11:11:44,579 : loading BERT model bert-large-uncased
2019-03-13 11:11:44,579 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:11:45,570 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:11:45,570 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7_cvcjxm
2019-03-13 11:11:53,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:11:58,741 : PROGRESS (encoding): 0.00%
2019-03-13 11:14:42,538 : PROGRESS (encoding): 14.56%
2019-03-13 11:17:48,122 : PROGRESS (encoding): 29.12%
2019-03-13 11:20:54,831 : PROGRESS (encoding): 43.69%
2019-03-13 11:24:13,854 : PROGRESS (encoding): 58.25%
2019-03-13 11:27:55,509 : PROGRESS (encoding): 72.81%
2019-03-13 11:31:36,032 : PROGRESS (encoding): 87.37%
2019-03-13 11:35:34,962 : PROGRESS (encoding): 0.00%
2019-03-13 11:36:05,002 : PROGRESS (encoding): 0.00%
2019-03-13 11:36:33,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:37:04,550 : [('reg:1e-09', 57.65)]
2019-03-13 11:37:04,550 : Validation : best param found is reg = 1e-09 with score             57.65
2019-03-13 11:37:04,550 : Evaluating...
2019-03-13 11:37:35,968 : Dev acc : 57.65 Test acc : 57.51 for SNLI

2019-03-13 11:37:35,968 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 11:37:36,174 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 11:37:37,152 : loading BERT model bert-large-uncased
2019-03-13 11:37:37,153 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:37:37,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:37:37,179 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3oma772k
2019-03-13 11:37:44,687 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:37:50,179 : Computing embeddings for train/dev/test
2019-03-13 11:41:19,305 : Computed embeddings
2019-03-13 11:41:19,305 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:41:48,065 : [('reg:1e-05', 70.42), ('reg:0.0001', 69.31), ('reg:0.001', 63.81), ('reg:0.01', 55.82)]
2019-03-13 11:41:48,066 : Validation : best param found is reg = 1e-05 with score             70.42
2019-03-13 11:41:48,066 : Evaluating...
2019-03-13 11:41:53,563 : 
Dev acc : 70.4 Test acc : 71.9 for LENGTH classification

2019-03-13 11:41:53,564 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 11:41:53,935 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 11:41:53,981 : loading BERT model bert-large-uncased
2019-03-13 11:41:53,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:41:54,010 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:41:54,010 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ezggexk
2019-03-13 11:42:01,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:42:06,854 : Computing embeddings for train/dev/test
2019-03-13 11:45:19,568 : Computed embeddings
2019-03-13 11:45:19,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:45:56,100 : [('reg:1e-05', 36.69), ('reg:0.0001', 10.1), ('reg:0.001', 1.29), ('reg:0.01', 0.48)]
2019-03-13 11:45:56,100 : Validation : best param found is reg = 1e-05 with score             36.69
2019-03-13 11:45:56,100 : Evaluating...
2019-03-13 11:46:07,360 : 
Dev acc : 36.7 Test acc : 36.2 for WORDCONTENT classification

2019-03-13 11:46:07,361 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 11:46:07,724 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 11:46:07,792 : loading BERT model bert-large-uncased
2019-03-13 11:46:07,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:46:07,817 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:46:07,817 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbxykcf6k
2019-03-13 11:46:15,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:46:20,710 : Computing embeddings for train/dev/test
2019-03-13 11:49:21,753 : Computed embeddings
2019-03-13 11:49:21,753 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:49:46,877 : [('reg:1e-05', 29.47), ('reg:0.0001', 28.71), ('reg:0.001', 24.14), ('reg:0.01', 27.71)]
2019-03-13 11:49:46,878 : Validation : best param found is reg = 1e-05 with score             29.47
2019-03-13 11:49:46,878 : Evaluating...
2019-03-13 11:49:53,442 : 
Dev acc : 29.5 Test acc : 29.9 for DEPTH classification

2019-03-13 11:49:53,443 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 11:49:53,846 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 11:49:53,912 : loading BERT model bert-large-uncased
2019-03-13 11:49:53,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:49:54,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:49:54,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxq1yadv1
2019-03-13 11:50:01,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:50:06,925 : Computing embeddings for train/dev/test
2019-03-13 11:52:54,802 : Computed embeddings
2019-03-13 11:52:54,803 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:53:25,319 : [('reg:1e-05', 74.68), ('reg:0.0001', 71.17), ('reg:0.001', 69.62), ('reg:0.01', 60.93)]
2019-03-13 11:53:25,319 : Validation : best param found is reg = 1e-05 with score             74.68
2019-03-13 11:53:25,319 : Evaluating...
2019-03-13 11:53:34,352 : 
Dev acc : 74.7 Test acc : 74.8 for TOPCONSTITUENTS classification

2019-03-13 11:53:34,353 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 11:53:34,688 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 11:53:34,754 : loading BERT model bert-large-uncased
2019-03-13 11:53:34,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:53:34,873 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:53:34,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm93pqyrh
2019-03-13 11:53:42,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:53:47,605 : Computing embeddings for train/dev/test
2019-03-13 11:56:50,088 : Computed embeddings
2019-03-13 11:56:50,088 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:57:19,073 : [('reg:1e-05', 87.03), ('reg:0.0001', 86.37), ('reg:0.001', 86.19), ('reg:0.01', 84.51)]
2019-03-13 11:57:19,074 : Validation : best param found is reg = 1e-05 with score             87.03
2019-03-13 11:57:19,074 : Evaluating...
2019-03-13 11:57:26,617 : 
Dev acc : 87.0 Test acc : 86.5 for BIGRAMSHIFT classification

2019-03-13 11:57:26,618 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 11:57:27,026 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 11:57:27,091 : loading BERT model bert-large-uncased
2019-03-13 11:57:27,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:57:27,119 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:57:27,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfus3lf99
2019-03-13 11:57:34,538 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:57:39,800 : Computing embeddings for train/dev/test
2019-03-13 12:00:38,333 : Computed embeddings
2019-03-13 12:00:38,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:01:03,867 : [('reg:1e-05', 89.81), ('reg:0.0001', 90.0), ('reg:0.001', 89.82), ('reg:0.01', 88.7)]
2019-03-13 12:01:03,867 : Validation : best param found is reg = 0.0001 with score             90.0
2019-03-13 12:01:03,867 : Evaluating...
2019-03-13 12:01:10,346 : 
Dev acc : 90.0 Test acc : 88.9 for TENSE classification

2019-03-13 12:01:10,347 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 12:01:10,753 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 12:01:10,816 : loading BERT model bert-large-uncased
2019-03-13 12:01:10,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:01:10,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:01:10,844 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppmd7wwx6
2019-03-13 12:01:18,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:01:23,759 : Computing embeddings for train/dev/test
2019-03-13 12:04:32,812 : Computed embeddings
2019-03-13 12:04:32,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:05:01,770 : [('reg:1e-05', 87.31), ('reg:0.0001', 87.47), ('reg:0.001', 87.62), ('reg:0.01', 87.57)]
2019-03-13 12:05:01,770 : Validation : best param found is reg = 0.001 with score             87.62
2019-03-13 12:05:01,770 : Evaluating...
2019-03-13 12:05:08,336 : 
Dev acc : 87.6 Test acc : 88.2 for SUBJNUMBER classification

2019-03-13 12:05:08,337 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 12:05:08,744 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 12:05:08,812 : loading BERT model bert-large-uncased
2019-03-13 12:05:08,812 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:05:08,927 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:05:08,927 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdl1uxoa3
2019-03-13 12:05:16,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:05:21,818 : Computing embeddings for train/dev/test
2019-03-13 12:08:27,212 : Computed embeddings
2019-03-13 12:08:27,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:08:55,414 : [('reg:1e-05', 79.16), ('reg:0.0001', 79.62), ('reg:0.001', 79.16), ('reg:0.01', 78.01)]
2019-03-13 12:08:55,414 : Validation : best param found is reg = 0.0001 with score             79.62
2019-03-13 12:08:55,414 : Evaluating...
2019-03-13 12:09:03,021 : 
Dev acc : 79.6 Test acc : 80.5 for OBJNUMBER classification

2019-03-13 12:09:03,022 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 12:09:03,402 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 12:09:03,470 : loading BERT model bert-large-uncased
2019-03-13 12:09:03,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:09:03,590 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:09:03,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcdjsq2j9
2019-03-13 12:09:11,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:09:16,313 : Computing embeddings for train/dev/test
2019-03-13 12:12:51,188 : Computed embeddings
2019-03-13 12:12:51,188 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:13:13,878 : [('reg:1e-05', 63.94), ('reg:0.0001', 61.4), ('reg:0.001', 61.4), ('reg:0.01', 60.72)]
2019-03-13 12:13:13,879 : Validation : best param found is reg = 1e-05 with score             63.94
2019-03-13 12:13:13,879 : Evaluating...
2019-03-13 12:13:20,340 : 
Dev acc : 63.9 Test acc : 63.7 for ODDMANOUT classification

2019-03-13 12:13:20,341 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 12:13:20,929 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 12:13:21,005 : loading BERT model bert-large-uncased
2019-03-13 12:13:21,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:13:21,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:13:21,036 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmjrvwf7p
2019-03-13 12:13:28,529 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:13:33,842 : Computing embeddings for train/dev/test
2019-03-13 12:17:07,029 : Computed embeddings
2019-03-13 12:17:07,029 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:17:32,565 : [('reg:1e-05', 73.35), ('reg:0.0001', 73.26), ('reg:0.001', 72.45), ('reg:0.01', 69.4)]
2019-03-13 12:17:32,566 : Validation : best param found is reg = 1e-05 with score             73.35
2019-03-13 12:17:32,566 : Evaluating...
2019-03-13 12:17:39,350 : 
Dev acc : 73.3 Test acc : 72.6 for COORDINATIONINVERSION classification

2019-03-13 12:17:39,352 : total results: {'STS12': {'MSRpar': {'pearson': (0.29426462030411465, 1.9006278404860514e-16), 'spearman': SpearmanrResult(correlation=0.34060474760950366, pvalue=7.930569907511008e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.18368575683862354, 4.079508491116451e-07), 'spearman': SpearmanrResult(correlation=0.21685955662178294, pvalue=1.966048412680938e-09), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.443284599343453, 1.623290265433683e-23), 'spearman': SpearmanrResult(correlation=0.5253904073289684, pvalue=6.2268577562458194e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.391779546120354, 6.350901455676556e-29), 'spearman': SpearmanrResult(correlation=0.42130694153578474, pvalue=1.2546496545168757e-33), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6120237289392373, 2.2633841793428605e-42), 'spearman': SpearmanrResult(correlation=0.5152204456012374, pvalue=1.960242305610083e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.38500765030915646, 'wmean': 0.35391330160640916}, 'spearman': {'mean': 0.40387641973945543, 'wmean': 0.3799245782124175}}}, 'STS13': {'FNWN': {'pearson': (0.26721232178348747, 0.00020159442359327312), 'spearman': SpearmanrResult(correlation=0.2717533371587699, pvalue=0.00015517269352968458), 'nsamples': 189}, 'headlines': {'pearson': (0.4408764136109697, 5.157976419983533e-37), 'spearman': SpearmanrResult(correlation=0.4304855357261879, pvalue=3.4512297644169377e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.2390699728613631, 9.856026735846012e-09), 'spearman': SpearmanrResult(correlation=0.23608910363948754, pvalue=1.517270235188298e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.31571956941860674, 'wmean': 0.34351912920035405}, 'spearman': {'mean': 0.3127759921748151, 'wmean': 0.3377810131062673}}}, 'STS14': {'deft-forum': {'pearson': (-0.0005161050937785387, 0.9912890351076008), 'spearman': SpearmanrResult(correlation=0.008753420800102555, pvalue=0.8530916333895904), 'nsamples': 450}, 'deft-news': {'pearson': (0.4924009818466579, 9.983332974972547e-20), 'spearman': SpearmanrResult(correlation=0.47296472663427924, pvalue=3.976710593943767e-18), 'nsamples': 300}, 'headlines': {'pearson': (0.4029347578234241, 1.2045821517717845e-30), 'spearman': SpearmanrResult(correlation=0.38525423081613364, pvalue=6.025568012783166e-28), 'nsamples': 750}, 'images': {'pearson': (0.1839145015948785, 3.9442842345558606e-07), 'spearman': SpearmanrResult(correlation=0.1955367757139073, pvalue=6.72915348756101e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.4098927143122741, 9.399964572324281e-32), 'spearman': SpearmanrResult(correlation=0.41377209182245195, pvalue=2.2084515984877243e-32), 'nsamples': 750}, 'tweet-news': {'pearson': (0.48894676602964965, 2.4940497402250137e-46), 'spearman': SpearmanrResult(correlation=0.46080168658991233, pvalue=1.0722304178550286e-40), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3295956027521843, 'wmean': 0.33646789388852444}, 'spearman': {'mean': 0.3228471553961311, 'wmean': 0.3299605456152357}}}, 'STS15': {'answers-forums': {'pearson': (0.37055010257602927, 1.194294622303731e-13), 'spearman': SpearmanrResult(correlation=0.39660808878989734, pvalue=1.411375738932152e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.5200232864965358, 3.432462603970349e-53), 'spearman': SpearmanrResult(correlation=0.5310704378280964, pvalue=8.330502133724331e-56), 'nsamples': 750}, 'belief': {'pearson': (0.43540539829593145, 8.845853103179852e-19), 'spearman': SpearmanrResult(correlation=0.4764918546118096, pvalue=1.1876694693807147e-22), 'nsamples': 375}, 'headlines': {'pearson': (0.46868502881694996, 3.1983022753961e-42), 'spearman': SpearmanrResult(correlation=0.46293351290884543, pvalue=4.1851975560463257e-41), 'nsamples': 750}, 'images': {'pearson': (0.20348954440622014, 1.8829594836396368e-08), 'spearman': SpearmanrResult(correlation=0.21608671693911766, pvalue=2.2494996581825306e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3996306721183333, 'wmean': 0.3987939025389215}, 'spearman': {'mean': 0.41663812221555324, 'wmean': 0.41166015984422827}}}, 'STS16': {'answer-answer': {'pearson': (0.3906538423088187, 1.0926022558131772e-10), 'spearman': SpearmanrResult(correlation=0.41868732541584297, pvalue=3.3463498206265754e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.5758043153523561, 2.1746641087968175e-23), 'spearman': SpearmanrResult(correlation=0.5820959089641052, pvalue=5.559457502328872e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6204558243307465, 7.267167430987217e-26), 'spearman': SpearmanrResult(correlation=0.6540205474926489, pvalue=1.8612807548706393e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.7391168831477077, 2.03774039308033e-43), 'spearman': SpearmanrResult(correlation=0.7792542316837981, pvalue=5.087899112526206e-51), 'nsamples': 244}, 'question-question': {'pearson': (0.12374959677497414, 0.07423444395821485), 'spearman': SpearmanrResult(correlation=0.20214096819373323, pvalue=0.0033338361435255623), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4899560923829206, 'wmean': 0.49874736532821123}, 'spearman': {'mean': 0.5272397963500257, 'wmean': 0.5346531895399095}}}, 'MR': {'devacc': 77.68, 'acc': 77.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.16, 'acc': 83.87, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.73, 'acc': 83.26, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.41, 'acc': 94.9, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.8, 'acc': 83.25, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.42, 'acc': 44.07, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.81, 'acc': 91.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.78, 'acc': 69.74, 'f1': 81.05, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.0, 'acc': 73.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7062157706946142, 'pearson': 0.7361368557067095, 'spearman': 0.6735725022975877, 'mse': 0.4677327722705792, 'yhat': array([3.66841421, 4.46488876, 1.56901473, ..., 3.10150084, 4.35665398,        4.13905345]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5533585694454035, 'pearson': 0.5400507827481144, 'spearman': 0.5381971814295873, 'mse': 1.7131447661323893, 'yhat': array([3.08029003, 1.71683914, 2.37934233, ..., 3.89080306, 3.37756581,        3.41184295]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.65, 'acc': 57.51, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 70.42, 'acc': 71.87, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 36.69, 'acc': 36.22, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.47, 'acc': 29.85, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.68, 'acc': 74.84, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.03, 'acc': 86.49, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.0, 'acc': 88.87, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.62, 'acc': 88.16, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.62, 'acc': 80.45, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.94, 'acc': 63.67, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.35, 'acc': 72.58, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 12:17:39,352 : STS12 p=0.3539, STS12 s=0.3799, STS13 p=0.3435, STS13 s=0.3378, STS14 p=0.3365, STS14 s=0.3300, STS15 p=0.3988, STS15 s=0.4117, STS 16 p=0.4987, STS16 s=0.5347, STS B p=0.5401, STS B s=0.5382, STS B m=1.7131, SICK-R p=0.7361, SICK-R s=0.6736, SICK-P m=0.4677
2019-03-13 12:17:39,352 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 12:17:39,352 : 0.3539,0.3799,0.3435,0.3378,0.3365,0.3300,0.3988,0.4117,0.4987,0.5347,0.5401,0.5382,1.7131,0.7361,0.6736,0.4677
2019-03-13 12:17:39,353 : MR=77.00, CR=83.87, SUBJ=94.90, MPQA=83.26, SST-B=83.25, SST-F=44.07, TREC=91.00, SICK-E=73.98, SNLI=57.51, MRPC=69.74, MRPC f=81.05
2019-03-13 12:17:39,353 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 12:17:39,353 : 77.00,83.87,94.90,83.26,83.25,44.07,91.00,73.98,57.51,69.74,81.05
2019-03-13 12:17:39,353 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 12:17:39,353 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 12:17:39,353 : na,na,na,na,na,na,na,na,na,na
2019-03-13 12:17:39,353 : SentLen=71.87, WC=36.22, TreeDepth=29.85, TopConst=74.84, BShift=86.49, Tense=88.87, SubjNum=88.16, ObjNum=80.45, SOMO=63.67, CoordInv=72.58, average=69.30
2019-03-13 12:17:39,353 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 12:17:39,353 : 71.87,36.22,29.85,74.84,86.49,88.87,88.16,80.45,63.67,72.58,69.30
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,353 : layer 17
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,353 : ********************************************************************************
2019-03-13 12:17:39,444 : ***** Transfer task : STS12 *****


2019-03-13 12:17:39,457 : loading BERT model bert-large-uncased
2019-03-13 12:17:39,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:17:39,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:17:39,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi_xpb6dn
2019-03-13 12:17:46,919 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:17:56,203 : MSRpar : pearson = 0.2997, spearman = 0.3447
2019-03-13 12:17:57,842 : MSRvid : pearson = 0.0987, spearman = 0.1346
2019-03-13 12:17:59,251 : SMTeuroparl : pearson = 0.4439, spearman = 0.5233
2019-03-13 12:18:01,943 : surprise.OnWN : pearson = 0.3716, spearman = 0.4037
2019-03-13 12:18:03,369 : surprise.SMTnews : pearson = 0.6045, spearman = 0.5163
2019-03-13 12:18:03,369 : ALL (weighted average) : Pearson = 0.3290,             Spearman = 0.3566
2019-03-13 12:18:03,369 : ALL (average) : Pearson = 0.3637,             Spearman = 0.3845

2019-03-13 12:18:03,369 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 12:18:03,379 : loading BERT model bert-large-uncased
2019-03-13 12:18:03,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:18:03,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:18:03,398 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2yov89dd
2019-03-13 12:18:10,825 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:18:17,455 : FNWN : pearson = 0.2797, spearman = 0.2826
2019-03-13 12:18:19,340 : headlines : pearson = 0.4049, spearman = 0.3970
2019-03-13 12:18:20,807 : OnWN : pearson = 0.2171, spearman = 0.2173
2019-03-13 12:18:20,807 : ALL (weighted average) : Pearson = 0.3189,             Spearman = 0.3154
2019-03-13 12:18:20,807 : ALL (average) : Pearson = 0.3005,             Spearman = 0.2989

2019-03-13 12:18:20,807 : ***** Transfer task : STS14 *****


2019-03-13 12:18:20,823 : loading BERT model bert-large-uncased
2019-03-13 12:18:20,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:18:20,841 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:18:20,841 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpikc054dm
2019-03-13 12:18:28,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:18:35,153 : deft-forum : pearson = -0.0130, spearman = -0.0045
2019-03-13 12:18:36,788 : deft-news : pearson = 0.4473, spearman = 0.4522
2019-03-13 12:18:38,954 : headlines : pearson = 0.3783, spearman = 0.3627
2019-03-13 12:18:41,029 : images : pearson = 0.1537, spearman = 0.1644
2019-03-13 12:18:43,156 : OnWN : pearson = 0.3764, spearman = 0.3817
2019-03-13 12:18:46,012 : tweet-news : pearson = 0.4740, spearman = 0.4387
2019-03-13 12:18:46,012 : ALL (weighted average) : Pearson = 0.3107,             Spearman = 0.3051
2019-03-13 12:18:46,012 : ALL (average) : Pearson = 0.3028,             Spearman = 0.2992

2019-03-13 12:18:46,012 : ***** Transfer task : STS15 *****


2019-03-13 12:18:46,046 : loading BERT model bert-large-uncased
2019-03-13 12:18:46,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:18:46,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:18:46,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8nr6incw
2019-03-13 12:18:53,638 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:19:00,911 : answers-forums : pearson = 0.3842, spearman = 0.3984
2019-03-13 12:19:02,990 : answers-students : pearson = 0.5125, spearman = 0.5244
2019-03-13 12:19:05,037 : belief : pearson = 0.4619, spearman = 0.4983
2019-03-13 12:19:07,285 : headlines : pearson = 0.4382, spearman = 0.4376
2019-03-13 12:19:09,416 : images : pearson = 0.1882, spearman = 0.1940
2019-03-13 12:19:09,416 : ALL (weighted average) : Pearson = 0.3905,             Spearman = 0.4011
2019-03-13 12:19:09,416 : ALL (average) : Pearson = 0.3970,             Spearman = 0.4105

2019-03-13 12:19:09,416 : ***** Transfer task : STS16 *****


2019-03-13 12:19:09,488 : loading BERT model bert-large-uncased
2019-03-13 12:19:09,489 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:19:09,508 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:19:09,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcb971ww7
2019-03-13 12:19:17,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:19:23,119 : answer-answer : pearson = 0.3625, spearman = 0.3776
2019-03-13 12:19:23,777 : headlines : pearson = 0.5534, spearman = 0.5672
2019-03-13 12:19:24,658 : plagiarism : pearson = 0.6155, spearman = 0.6497
2019-03-13 12:19:26,146 : postediting : pearson = 0.7398, spearman = 0.7726
2019-03-13 12:19:26,749 : question-question : pearson = 0.0489, spearman = 0.1322
2019-03-13 12:19:26,750 : ALL (weighted average) : Pearson = 0.4740,             Spearman = 0.5082
2019-03-13 12:19:26,750 : ALL (average) : Pearson = 0.4640,             Spearman = 0.4999

2019-03-13 12:19:26,750 : ***** Transfer task : MR *****


2019-03-13 12:19:26,769 : loading BERT model bert-large-uncased
2019-03-13 12:19:26,769 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:19:26,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:19:26,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7opdiglx
2019-03-13 12:19:34,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:19:39,638 : Generating sentence embeddings
2019-03-13 12:20:10,999 : Generated sentence embeddings
2019-03-13 12:20:11,000 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:20:21,044 : Best param found at split 1: l2reg = 0.01                 with score 81.25
2019-03-13 12:20:31,042 : Best param found at split 2: l2reg = 0.0001                 with score 80.67
2019-03-13 12:20:40,723 : Best param found at split 3: l2reg = 0.001                 with score 81.29
2019-03-13 12:20:51,692 : Best param found at split 4: l2reg = 0.001                 with score 80.86
2019-03-13 12:21:01,597 : Best param found at split 5: l2reg = 0.001                 with score 80.43
2019-03-13 12:21:02,215 : Dev acc : 80.9 Test acc : 79.8

2019-03-13 12:21:02,216 : ***** Transfer task : CR *****


2019-03-13 12:21:02,223 : loading BERT model bert-large-uncased
2019-03-13 12:21:02,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:21:02,243 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:21:02,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphcuz_676
2019-03-13 12:21:09,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:21:15,058 : Generating sentence embeddings
2019-03-13 12:21:23,378 : Generated sentence embeddings
2019-03-13 12:21:23,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:21:26,864 : Best param found at split 1: l2reg = 0.01                 with score 87.71
2019-03-13 12:21:30,931 : Best param found at split 2: l2reg = 0.01                 with score 88.11
2019-03-13 12:21:34,495 : Best param found at split 3: l2reg = 1e-05                 with score 89.14
2019-03-13 12:21:38,019 : Best param found at split 4: l2reg = 0.01                 with score 88.91
2019-03-13 12:21:41,328 : Best param found at split 5: l2reg = 1e-05                 with score 88.48
2019-03-13 12:21:41,514 : Dev acc : 88.47 Test acc : 86.41

2019-03-13 12:21:41,515 : ***** Transfer task : MPQA *****


2019-03-13 12:21:41,520 : loading BERT model bert-large-uncased
2019-03-13 12:21:41,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:21:41,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:21:41,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw3ph61i5
2019-03-13 12:21:48,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:21:54,379 : Generating sentence embeddings
2019-03-13 12:22:01,932 : Generated sentence embeddings
2019-03-13 12:22:01,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:22:11,309 : Best param found at split 1: l2reg = 0.0001                 with score 84.61
2019-03-13 12:22:21,315 : Best param found at split 2: l2reg = 0.001                 with score 85.89
2019-03-13 12:22:29,393 : Best param found at split 3: l2reg = 0.001                 with score 82.91
2019-03-13 12:22:39,217 : Best param found at split 4: l2reg = 0.0001                 with score 84.71
2019-03-13 12:22:49,857 : Best param found at split 5: l2reg = 0.001                 with score 83.82
2019-03-13 12:22:50,490 : Dev acc : 84.39 Test acc : 84.45

2019-03-13 12:22:50,491 : ***** Transfer task : SUBJ *****


2019-03-13 12:22:50,507 : loading BERT model bert-large-uncased
2019-03-13 12:22:50,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:22:50,527 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:22:50,527 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt_ibk3oq
2019-03-13 12:22:58,031 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:23:03,474 : Generating sentence embeddings
2019-03-13 12:23:34,313 : Generated sentence embeddings
2019-03-13 12:23:34,313 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:23:43,898 : Best param found at split 1: l2reg = 0.0001                 with score 95.22
2019-03-13 12:23:53,954 : Best param found at split 2: l2reg = 1e-05                 with score 95.34
2019-03-13 12:24:05,487 : Best param found at split 3: l2reg = 0.001                 with score 95.21
2019-03-13 12:24:15,396 : Best param found at split 4: l2reg = 0.0001                 with score 95.6
2019-03-13 12:24:26,392 : Best param found at split 5: l2reg = 0.001                 with score 95.28
2019-03-13 12:24:26,889 : Dev acc : 95.33 Test acc : 94.71

2019-03-13 12:24:26,890 : ***** Transfer task : SST Binary classification *****


2019-03-13 12:24:27,020 : loading BERT model bert-large-uncased
2019-03-13 12:24:27,020 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:24:27,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:24:27,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbsdglylj
2019-03-13 12:24:34,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:24:39,978 : Computing embedding for train
2019-03-13 12:26:19,551 : Computed train embeddings
2019-03-13 12:26:19,552 : Computing embedding for dev
2019-03-13 12:26:21,719 : Computed dev embeddings
2019-03-13 12:26:21,719 : Computing embedding for test
2019-03-13 12:26:26,274 : Computed test embeddings
2019-03-13 12:26:26,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:26:42,807 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.35), ('reg:0.001', 85.21), ('reg:0.01', 84.63)]
2019-03-13 12:26:42,807 : Validation : best param found is reg = 0.0001 with score             86.35
2019-03-13 12:26:42,807 : Evaluating...
2019-03-13 12:26:47,330 : 
Dev acc : 86.35 Test acc : 85.67 for             SST Binary classification

2019-03-13 12:26:47,331 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 12:26:47,386 : loading BERT model bert-large-uncased
2019-03-13 12:26:47,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:26:47,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:26:47,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxxta6xpf
2019-03-13 12:26:54,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:27:00,299 : Computing embedding for train
2019-03-13 12:27:22,081 : Computed train embeddings
2019-03-13 12:27:22,081 : Computing embedding for dev
2019-03-13 12:27:24,925 : Computed dev embeddings
2019-03-13 12:27:24,925 : Computing embedding for test
2019-03-13 12:27:30,528 : Computed test embeddings
2019-03-13 12:27:30,528 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:27:32,775 : [('reg:1e-05', 42.87), ('reg:0.0001', 45.05), ('reg:0.001', 44.05), ('reg:0.01', 37.78)]
2019-03-13 12:27:32,775 : Validation : best param found is reg = 0.0001 with score             45.05
2019-03-13 12:27:32,775 : Evaluating...
2019-03-13 12:27:33,252 : 
Dev acc : 45.05 Test acc : 44.84 for             SST Fine-Grained classification

2019-03-13 12:27:33,253 : ***** Transfer task : TREC *****


2019-03-13 12:27:33,266 : loading BERT model bert-large-uncased
2019-03-13 12:27:33,266 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:27:33,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:27:33,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpro4beean
2019-03-13 12:27:40,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:27:53,792 : Computed train embeddings
2019-03-13 12:27:54,379 : Computed test embeddings
2019-03-13 12:27:54,379 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:28:01,487 : [('reg:1e-05', 81.95), ('reg:0.0001', 82.8), ('reg:0.001', 80.52), ('reg:0.01', 76.34)]
2019-03-13 12:28:01,487 : Cross-validation : best param found is reg = 0.0001             with score 82.8
2019-03-13 12:28:01,487 : Evaluating...
2019-03-13 12:28:01,936 : 
Dev acc : 82.8 Test acc : 90.0             for TREC

2019-03-13 12:28:01,937 : ***** Transfer task : MRPC *****


2019-03-13 12:28:01,958 : loading BERT model bert-large-uncased
2019-03-13 12:28:01,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:28:01,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:28:01,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnvw3n13l
2019-03-13 12:28:09,428 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:28:14,556 : Computing embedding for train
2019-03-13 12:28:36,769 : Computed train embeddings
2019-03-13 12:28:36,769 : Computing embedding for test
2019-03-13 12:28:46,468 : Computed test embeddings
2019-03-13 12:28:46,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:28:50,989 : [('reg:1e-05', 70.51), ('reg:0.0001', 70.9), ('reg:0.001', 70.56), ('reg:0.01', 71.0)]
2019-03-13 12:28:50,989 : Cross-validation : best param found is reg = 0.01             with score 71.0
2019-03-13 12:28:50,990 : Evaluating...
2019-03-13 12:28:51,258 : Dev acc : 71.0 Test acc 71.59; Test F1 80.42 for MRPC.

2019-03-13 12:28:51,258 : ***** Transfer task : SICK-Entailment*****


2019-03-13 12:28:51,283 : loading BERT model bert-large-uncased
2019-03-13 12:28:51,283 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:28:51,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:28:51,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdbmhatho
2019-03-13 12:28:58,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:29:04,123 : Computing embedding for train
2019-03-13 12:29:15,361 : Computed train embeddings
2019-03-13 12:29:15,361 : Computing embedding for dev
2019-03-13 12:29:16,894 : Computed dev embeddings
2019-03-13 12:29:16,894 : Computing embedding for test
2019-03-13 12:29:28,925 : Computed test embeddings
2019-03-13 12:29:28,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:29:30,782 : [('reg:1e-05', 73.2), ('reg:0.0001', 74.2), ('reg:0.001', 73.6), ('reg:0.01', 72.2)]
2019-03-13 12:29:30,782 : Validation : best param found is reg = 0.0001 with score             74.2
2019-03-13 12:29:30,782 : Evaluating...
2019-03-13 12:29:31,181 : 
Dev acc : 74.2 Test acc : 72.32 for                        SICK entailment

2019-03-13 12:29:31,182 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 12:29:31,250 : loading BERT model bert-large-uncased
2019-03-13 12:29:31,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:29:31,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:29:31,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp99b6kyo_
2019-03-13 12:29:38,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:29:44,130 : Computing embedding for train
2019-03-13 12:29:55,390 : Computed train embeddings
2019-03-13 12:29:55,390 : Computing embedding for dev
2019-03-13 12:29:56,925 : Computed dev embeddings
2019-03-13 12:29:56,925 : Computing embedding for test
2019-03-13 12:30:08,980 : Computed test embeddings
2019-03-13 12:30:24,297 : Dev : Pearson 0.6937914286647974
2019-03-13 12:30:24,297 : Test : Pearson 0.7036382945580539 Spearman 0.6502487860593172 MSE 0.5143434026032572                        for SICK Relatedness

2019-03-13 12:30:24,298 : 

***** Transfer task : STSBenchmark*****


2019-03-13 12:30:24,370 : loading BERT model bert-large-uncased
2019-03-13 12:30:24,370 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:30:24,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:30:24,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpym3ytx8v
2019-03-13 12:30:31,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:30:37,348 : Computing embedding for train
2019-03-13 12:30:55,892 : Computed train embeddings
2019-03-13 12:30:55,892 : Computing embedding for dev
2019-03-13 12:31:01,505 : Computed dev embeddings
2019-03-13 12:31:01,505 : Computing embedding for test
2019-03-13 12:31:06,081 : Computed test embeddings
2019-03-13 12:31:25,473 : Dev : Pearson 0.5038035726713287
2019-03-13 12:31:25,473 : Test : Pearson 0.47362998471645334 Spearman 0.4746078090107275 MSE 1.8724002874029526                        for SICK Relatedness

2019-03-13 12:31:25,473 : ***** Transfer task : SNLI Entailment*****


2019-03-13 12:31:30,196 : loading BERT model bert-large-uncased
2019-03-13 12:31:30,196 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:31:30,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:31:30,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_fvr7j2u
2019-03-13 12:31:37,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:31:44,616 : PROGRESS (encoding): 0.00%
2019-03-13 12:34:28,854 : PROGRESS (encoding): 14.56%
2019-03-13 12:37:35,175 : PROGRESS (encoding): 29.12%
2019-03-13 12:40:42,255 : PROGRESS (encoding): 43.69%
2019-03-13 12:44:01,756 : PROGRESS (encoding): 58.25%
2019-03-13 12:47:43,764 : PROGRESS (encoding): 72.81%
2019-03-13 12:51:24,574 : PROGRESS (encoding): 87.37%
2019-03-13 12:55:23,641 : PROGRESS (encoding): 0.00%
2019-03-13 12:55:53,707 : PROGRESS (encoding): 0.00%
2019-03-13 12:56:22,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:57:10,808 : [('reg:1e-09', 65.01)]
2019-03-13 12:57:10,809 : Validation : best param found is reg = 1e-09 with score             65.01
2019-03-13 12:57:10,809 : Evaluating...
2019-03-13 12:57:56,847 : Dev acc : 65.01 Test acc : 65.39 for SNLI

2019-03-13 12:57:56,847 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 12:57:57,067 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 12:57:58,033 : loading BERT model bert-large-uncased
2019-03-13 12:57:58,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:57:58,060 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:57:58,060 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6eo_bi43
2019-03-13 12:58:05,535 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:58:10,896 : Computing embeddings for train/dev/test
2019-03-13 13:01:39,575 : Computed embeddings
2019-03-13 13:01:39,575 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:02:13,750 : [('reg:1e-05', 69.73), ('reg:0.0001', 68.59), ('reg:0.001', 64.4), ('reg:0.01', 48.29)]
2019-03-13 13:02:13,750 : Validation : best param found is reg = 1e-05 with score             69.73
2019-03-13 13:02:13,750 : Evaluating...
2019-03-13 13:02:24,554 : 
Dev acc : 69.7 Test acc : 70.7 for LENGTH classification

2019-03-13 13:02:24,555 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 13:02:24,916 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 13:02:24,960 : loading BERT model bert-large-uncased
2019-03-13 13:02:24,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:02:24,990 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:02:24,990 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl3rhiuou
2019-03-13 13:02:32,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:02:37,829 : Computing embeddings for train/dev/test
2019-03-13 13:05:50,512 : Computed embeddings
2019-03-13 13:05:50,512 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:06:23,382 : [('reg:1e-05', 32.91), ('reg:0.0001', 9.94), ('reg:0.001', 1.69), ('reg:0.01', 0.55)]
2019-03-13 13:06:23,382 : Validation : best param found is reg = 1e-05 with score             32.91
2019-03-13 13:06:23,382 : Evaluating...
2019-03-13 13:06:33,479 : 
Dev acc : 32.9 Test acc : 32.1 for WORDCONTENT classification

2019-03-13 13:06:33,481 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 13:06:33,840 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 13:06:33,908 : loading BERT model bert-large-uncased
2019-03-13 13:06:33,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:06:33,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:06:33,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptozxy4pj
2019-03-13 13:06:41,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:06:46,855 : Computing embeddings for train/dev/test
2019-03-13 13:09:47,721 : Computed embeddings
2019-03-13 13:09:47,722 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:10:09,392 : [('reg:1e-05', 29.46), ('reg:0.0001', 29.39), ('reg:0.001', 27.82), ('reg:0.01', 25.07)]
2019-03-13 13:10:09,392 : Validation : best param found is reg = 1e-05 with score             29.46
2019-03-13 13:10:09,392 : Evaluating...
2019-03-13 13:10:14,234 : 
Dev acc : 29.5 Test acc : 29.3 for DEPTH classification

2019-03-13 13:10:14,235 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 13:10:14,615 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 13:10:14,679 : loading BERT model bert-large-uncased
2019-03-13 13:10:14,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:10:14,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:10:14,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpln0n72nm
2019-03-13 13:10:22,312 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:10:27,869 : Computing embeddings for train/dev/test
2019-03-13 13:13:15,711 : Computed embeddings
2019-03-13 13:13:15,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:13:45,099 : [('reg:1e-05', 74.8), ('reg:0.0001', 70.46), ('reg:0.001', 68.58), ('reg:0.01', 58.92)]
2019-03-13 13:13:45,100 : Validation : best param found is reg = 1e-05 with score             74.8
2019-03-13 13:13:45,100 : Evaluating...
2019-03-13 13:13:51,387 : 
Dev acc : 74.8 Test acc : 75.1 for TOPCONSTITUENTS classification

2019-03-13 13:13:51,388 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 13:13:51,734 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 13:13:51,801 : loading BERT model bert-large-uncased
2019-03-13 13:13:51,801 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:13:51,921 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:13:51,921 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz1fxq1d3
2019-03-13 13:13:59,385 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:14:04,787 : Computing embeddings for train/dev/test
2019-03-13 13:17:07,098 : Computed embeddings
2019-03-13 13:17:07,098 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:17:43,813 : [('reg:1e-05', 86.82), ('reg:0.0001', 86.84), ('reg:0.001', 86.57), ('reg:0.01', 85.19)]
2019-03-13 13:17:43,813 : Validation : best param found is reg = 0.0001 with score             86.84
2019-03-13 13:17:43,813 : Evaluating...
2019-03-13 13:17:53,576 : 
Dev acc : 86.8 Test acc : 86.4 for BIGRAMSHIFT classification

2019-03-13 13:17:53,577 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 13:17:54,129 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 13:17:54,193 : loading BERT model bert-large-uncased
2019-03-13 13:17:54,193 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:17:54,222 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:17:54,222 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprcdfeih4
2019-03-13 13:18:01,682 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:18:07,104 : Computing embeddings for train/dev/test
2019-03-13 13:21:05,775 : Computed embeddings
2019-03-13 13:21:05,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:21:32,956 : [('reg:1e-05', 90.01), ('reg:0.0001', 89.88), ('reg:0.001', 89.02), ('reg:0.01', 88.21)]
2019-03-13 13:21:32,957 : Validation : best param found is reg = 1e-05 with score             90.01
2019-03-13 13:21:32,957 : Evaluating...
2019-03-13 13:21:39,342 : 
Dev acc : 90.0 Test acc : 88.8 for TENSE classification

2019-03-13 13:21:39,343 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 13:21:39,720 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 13:21:39,787 : loading BERT model bert-large-uncased
2019-03-13 13:21:39,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:21:39,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:21:39,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu49iewuu
2019-03-13 13:21:47,265 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:21:52,627 : Computing embeddings for train/dev/test
2019-03-13 13:25:01,587 : Computed embeddings
2019-03-13 13:25:01,587 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:25:27,556 : [('reg:1e-05', 87.21), ('reg:0.0001', 87.21), ('reg:0.001', 87.31), ('reg:0.01', 86.78)]
2019-03-13 13:25:27,556 : Validation : best param found is reg = 0.001 with score             87.31
2019-03-13 13:25:27,556 : Evaluating...
2019-03-13 13:25:34,236 : 
Dev acc : 87.3 Test acc : 88.3 for SUBJNUMBER classification

2019-03-13 13:25:34,237 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 13:25:34,651 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 13:25:34,717 : loading BERT model bert-large-uncased
2019-03-13 13:25:34,718 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:25:34,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:25:34,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp78whk3qt
2019-03-13 13:25:42,243 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:25:47,638 : Computing embeddings for train/dev/test
2019-03-13 13:28:53,244 : Computed embeddings
2019-03-13 13:28:53,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:29:19,873 : [('reg:1e-05', 78.33), ('reg:0.0001', 78.22), ('reg:0.001', 77.99), ('reg:0.01', 74.95)]
2019-03-13 13:29:19,873 : Validation : best param found is reg = 1e-05 with score             78.33
2019-03-13 13:29:19,873 : Evaluating...
2019-03-13 13:29:26,330 : 
Dev acc : 78.3 Test acc : 79.4 for OBJNUMBER classification

2019-03-13 13:29:26,331 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 13:29:26,722 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 13:29:26,791 : loading BERT model bert-large-uncased
2019-03-13 13:29:26,791 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:29:26,913 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:29:26,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzsco6tt2
2019-03-13 13:29:34,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:29:39,757 : Computing embeddings for train/dev/test
2019-03-13 13:33:14,816 : Computed embeddings
2019-03-13 13:33:14,817 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:33:34,589 : [('reg:1e-05', 62.96), ('reg:0.0001', 62.98), ('reg:0.001', 62.7), ('reg:0.01', 63.18)]
2019-03-13 13:33:34,589 : Validation : best param found is reg = 0.01 with score             63.18
2019-03-13 13:33:34,589 : Evaluating...
2019-03-13 13:33:41,015 : 
Dev acc : 63.2 Test acc : 63.3 for ODDMANOUT classification

2019-03-13 13:33:41,016 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 13:33:41,411 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 13:33:41,487 : loading BERT model bert-large-uncased
2019-03-13 13:33:41,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:33:41,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:33:41,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaw198x_z
2019-03-13 13:33:49,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:33:54,427 : Computing embeddings for train/dev/test
2019-03-13 13:37:27,301 : Computed embeddings
2019-03-13 13:37:27,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:37:48,621 : [('reg:1e-05', 71.76), ('reg:0.0001', 71.76), ('reg:0.001', 72.46), ('reg:0.01', 69.05)]
2019-03-13 13:37:48,621 : Validation : best param found is reg = 0.001 with score             72.46
2019-03-13 13:37:48,621 : Evaluating...
2019-03-13 13:37:53,945 : 
Dev acc : 72.5 Test acc : 72.3 for COORDINATIONINVERSION classification

2019-03-13 13:37:53,947 : total results: {'STS12': {'MSRpar': {'pearson': (0.29966920532750885, 5.004976501781894e-17), 'spearman': SpearmanrResult(correlation=0.34466191182335787, pvalue=2.4133502077359973e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.09870343360752588, 0.006826234077316637), 'spearman': SpearmanrResult(correlation=0.1345689493201658, pvalue=0.0002190236764873891), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4438711774456704, 1.3981148404367362e-23), 'spearman': SpearmanrResult(correlation=0.5233199835631235, pvalue=1.2391840641438422e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.37162820660923035, 5.631825798052957e-26), 'spearman': SpearmanrResult(correlation=0.4037046420676826, pvalue=9.110770310007957e-31), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.604514784581021, 4.074540092421464e-41), 'spearman': SpearmanrResult(correlation=0.5162633717646803, pvalue=1.4625216292208592e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.36367736151419133, 'wmean': 0.32897004622058845}, 'spearman': {'mean': 0.38450377170780203, 'wmean': 0.3566263144137663}}}, 'STS13': {'FNWN': {'pearson': (0.27965605155232703, 9.732067630332545e-05), 'spearman': SpearmanrResult(correlation=0.28259856039770753, pvalue=8.150615174216469e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.4048754910256202, 5.949754544085074e-31), 'spearman': SpearmanrResult(correlation=0.3969794566990374, pvalue=1.019317507646424e-29), 'nsamples': 750}, 'OnWN': {'pearson': (0.21706616579906676, 2.0819376177245045e-07), 'spearman': SpearmanrResult(correlation=0.21726719382181753, pvalue=2.0275361389432154e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3005325694590047, 'wmean': 0.31885715401725423}, 'spearman': {'mean': 0.29894840363952085, 'wmean': 0.3153550774489896}}}, 'STS14': {'deft-forum': {'pearson': (-0.012954982869615304, 0.7840355682169683), 'spearman': SpearmanrResult(correlation=-0.004533100335066399, pvalue=0.9236042716910259), 'nsamples': 450}, 'deft-news': {'pearson': (0.4472733972619008, 3.667473266614021e-16), 'spearman': SpearmanrResult(correlation=0.4521690637414613, pvalue=1.5944136035753724e-16), 'nsamples': 300}, 'headlines': {'pearson': (0.37827901175325995, 6.316111974719046e-27), 'spearman': SpearmanrResult(correlation=0.36268529238877234, pvalue=9.860254573545027e-25), 'nsamples': 750}, 'images': {'pearson': (0.15368941221528407, 2.367478265871984e-05), 'spearman': SpearmanrResult(correlation=0.16438079066078104, pvalue=6.03782230188827e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.37639881703142347, 1.1784532576384231e-26), 'spearman': SpearmanrResult(correlation=0.38166032414096246, pvalue=2.0363687779444978e-27), 'nsamples': 750}, 'tweet-news': {'pearson': (0.47395185188414096, 2.906847013058493e-43), 'spearman': SpearmanrResult(correlation=0.4386903548997778, pvalue=1.26417786957781e-36), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30277291787939903, 'wmean': 0.31069109241341986}, 'spearman': {'mean': 0.2991754542494481, 'wmean': 0.3051129054771677}}}, 'STS15': {'answers-forums': {'pearson': (0.38424808868373395, 1.2170535194126787e-14), 'spearman': SpearmanrResult(correlation=0.3983719005692904, pvalue=1.0301318373283976e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.5125155704949866, 1.8138107136472626e-51), 'spearman': SpearmanrResult(correlation=0.5244269737798132, pvalue=3.1971391403988224e-54), 'nsamples': 750}, 'belief': {'pearson': (0.46187041036536597, 3.2584448280550023e-21), 'spearman': SpearmanrResult(correlation=0.4983366874352377, pvalue=6.20713736721994e-25), 'nsamples': 375}, 'headlines': {'pearson': (0.4381854808743819, 1.5535372025738246e-36), 'spearman': SpearmanrResult(correlation=0.43755403698073037, pvalue=2.0093880310167706e-36), 'nsamples': 750}, 'images': {'pearson': (0.18821833682662215, 2.075439608712268e-07), 'spearman': SpearmanrResult(correlation=0.1939665009554917, pvalue=8.60023533789936e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.39700757744901816, 'wmean': 0.39049465943013517}, 'spearman': {'mean': 0.41053121994411274, 'wmean': 0.4010754514295748}}}, 'STS16': {'answer-answer': {'pearson': (0.3624878984283703, 2.6443553443201378e-09), 'spearman': SpearmanrResult(correlation=0.3776285804092711, pvalue=4.953495155399783e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5533787843624163, 2.2311467945889147e-21), 'spearman': SpearmanrResult(correlation=0.5672080979434282, pvalue=1.337895043219471e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6155298281567588, 2.2514107206719097e-25), 'spearman': SpearmanrResult(correlation=0.6496569871267583, pvalue=5.786212328291686e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.7398498691730189, 1.5241058690616775e-43), 'spearman': SpearmanrResult(correlation=0.772610215980627, pvalue=1.1800961249273802e-49), 'nsamples': 244}, 'question-question': {'pearson': (0.0488949034234325, 0.4820266314821079), 'spearman': SpearmanrResult(correlation=0.1321769216097866, pvalue=0.0564198872972219), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4640282567087993, 'wmean': 0.4740113885976528}, 'spearman': {'mean': 0.49985616061397425, 'wmean': 0.508191780916308}}}, 'MR': {'devacc': 80.9, 'acc': 79.8, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.47, 'acc': 86.41, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.39, 'acc': 84.45, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.33, 'acc': 94.71, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.35, 'acc': 85.67, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.05, 'acc': 44.84, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.8, 'acc': 90.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.0, 'acc': 71.59, 'f1': 80.42, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.2, 'acc': 72.32, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6937914286647974, 'pearson': 0.7036382945580539, 'spearman': 0.6502487860593172, 'mse': 0.5143434026032572, 'yhat': array([3.70379019, 4.56587425, 1.41884669, ..., 3.26028703, 4.03032198,        4.23440756]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5038035726713287, 'pearson': 0.47362998471645334, 'spearman': 0.4746078090107275, 'mse': 1.8724002874029526, 'yhat': array([2.86986786, 1.62767033, 3.08133223, ..., 3.93986699, 3.45243011,        3.38381695]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.01, 'acc': 65.39, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 69.73, 'acc': 70.65, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 32.91, 'acc': 32.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.46, 'acc': 29.32, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.8, 'acc': 75.14, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.84, 'acc': 86.39, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.01, 'acc': 88.81, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.31, 'acc': 88.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.33, 'acc': 79.44, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.18, 'acc': 63.32, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.46, 'acc': 72.27, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 13:37:53,947 : STS12 p=0.3290, STS12 s=0.3566, STS13 p=0.3189, STS13 s=0.3154, STS14 p=0.3107, STS14 s=0.3051, STS15 p=0.3905, STS15 s=0.4011, STS 16 p=0.4740, STS16 s=0.5082, STS B p=0.4736, STS B s=0.4746, STS B m=1.8724, SICK-R p=0.7036, SICK-R s=0.6502, SICK-P m=0.5143
2019-03-13 13:37:53,947 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 13:37:53,947 : 0.3290,0.3566,0.3189,0.3154,0.3107,0.3051,0.3905,0.4011,0.4740,0.5082,0.4736,0.4746,1.8724,0.7036,0.6502,0.5143
2019-03-13 13:37:53,947 : MR=79.80, CR=86.41, SUBJ=94.71, MPQA=84.45, SST-B=85.67, SST-F=44.84, TREC=90.00, SICK-E=72.32, SNLI=65.39, MRPC=71.59, MRPC f=80.42
2019-03-13 13:37:53,948 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 13:37:53,948 : 79.80,86.41,94.71,84.45,85.67,44.84,90.00,72.32,65.39,71.59,80.42
2019-03-13 13:37:53,948 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 13:37:53,948 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 13:37:53,948 : na,na,na,na,na,na,na,na,na,na
2019-03-13 13:37:53,948 : SentLen=70.65, WC=32.06, TreeDepth=29.32, TopConst=75.14, BShift=86.39, Tense=88.81, SubjNum=88.30, ObjNum=79.44, SOMO=63.32, CoordInv=72.27, average=68.57
2019-03-13 13:37:53,948 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 13:37:53,948 : 70.65,32.06,29.32,75.14,86.39,88.81,88.30,79.44,63.32,72.27,68.57
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:53,948 : layer 18
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:53,948 : ********************************************************************************
2019-03-13 13:37:54,039 : ***** Transfer task : STS12 *****


2019-03-13 13:37:54,075 : loading BERT model bert-large-uncased
2019-03-13 13:37:54,076 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:37:54,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:37:54,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc459hyjb
2019-03-13 13:38:01,533 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:38:10,860 : MSRpar : pearson = 0.2882, spearman = 0.3332
2019-03-13 13:38:12,493 : MSRvid : pearson = 0.0605, spearman = 0.1054
2019-03-13 13:38:13,900 : SMTeuroparl : pearson = 0.4519, spearman = 0.5291
2019-03-13 13:38:16,586 : surprise.OnWN : pearson = 0.3719, spearman = 0.4027
2019-03-13 13:38:18,006 : surprise.SMTnews : pearson = 0.5882, spearman = 0.5149
2019-03-13 13:38:18,006 : ALL (weighted average) : Pearson = 0.3161,             Spearman = 0.3473
2019-03-13 13:38:18,006 : ALL (average) : Pearson = 0.3521,             Spearman = 0.3771

2019-03-13 13:38:18,006 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 13:38:18,016 : loading BERT model bert-large-uncased
2019-03-13 13:38:18,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:38:18,032 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:38:18,033 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo9tg_ex4
2019-03-13 13:38:25,524 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:38:32,232 : FNWN : pearson = 0.2207, spearman = 0.2225
2019-03-13 13:38:34,120 : headlines : pearson = 0.3984, spearman = 0.3973
2019-03-13 13:38:35,582 : OnWN : pearson = 0.2102, spearman = 0.2096
2019-03-13 13:38:35,582 : ALL (weighted average) : Pearson = 0.3056,             Spearman = 0.3051
2019-03-13 13:38:35,582 : ALL (average) : Pearson = 0.2764,             Spearman = 0.2765

2019-03-13 13:38:35,582 : ***** Transfer task : STS14 *****


2019-03-13 13:38:35,624 : loading BERT model bert-large-uncased
2019-03-13 13:38:35,624 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:38:35,642 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:38:35,642 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdb9ump5p
2019-03-13 13:38:43,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:38:49,804 : deft-forum : pearson = -0.0331, spearman = -0.0314
2019-03-13 13:38:51,440 : deft-news : pearson = 0.4321, spearman = 0.4370
2019-03-13 13:38:53,608 : headlines : pearson = 0.3852, spearman = 0.3674
2019-03-13 13:38:55,679 : images : pearson = 0.1539, spearman = 0.1605
2019-03-13 13:38:57,803 : OnWN : pearson = 0.3620, spearman = 0.3691
2019-03-13 13:39:00,652 : tweet-news : pearson = 0.4632, spearman = 0.4214
2019-03-13 13:39:00,652 : ALL (weighted average) : Pearson = 0.3034,             Spearman = 0.2949
2019-03-13 13:39:00,652 : ALL (average) : Pearson = 0.2939,             Spearman = 0.2873

2019-03-13 13:39:00,652 : ***** Transfer task : STS15 *****


2019-03-13 13:39:00,684 : loading BERT model bert-large-uncased
2019-03-13 13:39:00,684 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:39:00,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:39:00,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd7aavut4
2019-03-13 13:39:08,187 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:39:15,470 : answers-forums : pearson = 0.4107, spearman = 0.4081
2019-03-13 13:39:17,547 : answers-students : pearson = 0.5286, spearman = 0.5347
2019-03-13 13:39:19,590 : belief : pearson = 0.4620, spearman = 0.4927
2019-03-13 13:39:21,832 : headlines : pearson = 0.4251, spearman = 0.4277
2019-03-13 13:39:23,957 : images : pearson = 0.1964, spearman = 0.1970
2019-03-13 13:39:23,957 : ALL (weighted average) : Pearson = 0.3966,             Spearman = 0.4025
2019-03-13 13:39:23,957 : ALL (average) : Pearson = 0.4046,             Spearman = 0.4120

2019-03-13 13:39:23,957 : ***** Transfer task : STS16 *****


2019-03-13 13:39:23,996 : loading BERT model bert-large-uncased
2019-03-13 13:39:23,996 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:39:24,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:39:24,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprhc4rr6b
2019-03-13 13:39:31,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:39:37,719 : answer-answer : pearson = 0.3429, spearman = 0.3408
2019-03-13 13:39:38,376 : headlines : pearson = 0.5627, spearman = 0.5748
2019-03-13 13:39:39,257 : plagiarism : pearson = 0.6148, spearman = 0.6349
2019-03-13 13:39:40,747 : postediting : pearson = 0.7505, spearman = 0.7752
2019-03-13 13:39:41,351 : question-question : pearson = 0.0027, spearman = 0.0862
2019-03-13 13:39:41,351 : ALL (weighted average) : Pearson = 0.4657,             Spearman = 0.4915
2019-03-13 13:39:41,351 : ALL (average) : Pearson = 0.4547,             Spearman = 0.4824

2019-03-13 13:39:41,351 : ***** Transfer task : MR *****


2019-03-13 13:39:41,396 : loading BERT model bert-large-uncased
2019-03-13 13:39:41,396 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:39:41,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:39:41,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_fwhd_jt
2019-03-13 13:39:48,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:39:54,148 : Generating sentence embeddings
2019-03-13 13:40:25,579 : Generated sentence embeddings
2019-03-13 13:40:25,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:40:35,850 : Best param found at split 1: l2reg = 0.01                 with score 82.16
2019-03-13 13:40:46,597 : Best param found at split 2: l2reg = 0.001                 with score 82.17
2019-03-13 13:40:56,757 : Best param found at split 3: l2reg = 0.01                 with score 82.45
2019-03-13 13:41:08,105 : Best param found at split 4: l2reg = 0.0001                 with score 82.23
2019-03-13 13:41:18,418 : Best param found at split 5: l2reg = 0.01                 with score 81.97
2019-03-13 13:41:19,038 : Dev acc : 82.2 Test acc : 80.73

2019-03-13 13:41:19,039 : ***** Transfer task : CR *****


2019-03-13 13:41:19,047 : loading BERT model bert-large-uncased
2019-03-13 13:41:19,047 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:41:19,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:41:19,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt1ih0pmv
2019-03-13 13:41:26,558 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:41:31,942 : Generating sentence embeddings
2019-03-13 13:41:40,223 : Generated sentence embeddings
2019-03-13 13:41:40,224 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:41:44,096 : Best param found at split 1: l2reg = 0.001                 with score 88.14
2019-03-13 13:41:48,156 : Best param found at split 2: l2reg = 0.001                 with score 88.44
2019-03-13 13:41:51,946 : Best param found at split 3: l2reg = 0.01                 with score 89.37
2019-03-13 13:41:55,413 : Best param found at split 4: l2reg = 1e-05                 with score 88.41
2019-03-13 13:41:58,969 : Best param found at split 5: l2reg = 1e-05                 with score 88.58
2019-03-13 13:41:59,130 : Dev acc : 88.59 Test acc : 87.02

2019-03-13 13:41:59,130 : ***** Transfer task : MPQA *****


2019-03-13 13:41:59,136 : loading BERT model bert-large-uncased
2019-03-13 13:41:59,136 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:41:59,155 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:41:59,155 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3xseadvr
2019-03-13 13:42:06,630 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:42:11,972 : Generating sentence embeddings
2019-03-13 13:42:19,525 : Generated sentence embeddings
2019-03-13 13:42:19,525 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:42:28,395 : Best param found at split 1: l2reg = 1e-05                 with score 83.22
2019-03-13 13:42:39,559 : Best param found at split 2: l2reg = 0.01                 with score 84.81
2019-03-13 13:42:50,191 : Best param found at split 3: l2reg = 0.001                 with score 83.19
2019-03-13 13:43:00,915 : Best param found at split 4: l2reg = 1e-05                 with score 85.62
2019-03-13 13:43:10,515 : Best param found at split 5: l2reg = 0.0001                 with score 84.11
2019-03-13 13:43:11,087 : Dev acc : 84.19 Test acc : 85.65

2019-03-13 13:43:11,088 : ***** Transfer task : SUBJ *****


2019-03-13 13:43:11,103 : loading BERT model bert-large-uncased
2019-03-13 13:43:11,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:43:11,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:43:11,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp903vj0nm
2019-03-13 13:43:18,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:43:23,999 : Generating sentence embeddings
2019-03-13 13:43:54,795 : Generated sentence embeddings
2019-03-13 13:43:54,796 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:44:04,656 : Best param found at split 1: l2reg = 1e-05                 with score 95.41
2019-03-13 13:44:15,196 : Best param found at split 2: l2reg = 1e-05                 with score 95.59
2019-03-13 13:44:26,089 : Best param found at split 3: l2reg = 0.001                 with score 95.28
2019-03-13 13:44:37,290 : Best param found at split 4: l2reg = 1e-05                 with score 95.8
2019-03-13 13:44:47,091 : Best param found at split 5: l2reg = 0.001                 with score 95.44
2019-03-13 13:44:47,508 : Dev acc : 95.5 Test acc : 94.87

2019-03-13 13:44:47,509 : ***** Transfer task : SST Binary classification *****


2019-03-13 13:44:47,649 : loading BERT model bert-large-uncased
2019-03-13 13:44:47,649 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:44:47,673 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:44:47,673 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmputcd1jfc
2019-03-13 13:44:55,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:45:00,493 : Computing embedding for train
2019-03-13 13:46:40,054 : Computed train embeddings
2019-03-13 13:46:40,054 : Computing embedding for dev
2019-03-13 13:46:42,219 : Computed dev embeddings
2019-03-13 13:46:42,219 : Computing embedding for test
2019-03-13 13:46:46,771 : Computed test embeddings
2019-03-13 13:46:46,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:47:02,338 : [('reg:1e-05', 86.12), ('reg:0.0001', 86.01), ('reg:0.001', 85.78), ('reg:0.01', 86.47)]
2019-03-13 13:47:02,338 : Validation : best param found is reg = 0.01 with score             86.47
2019-03-13 13:47:02,338 : Evaluating...
2019-03-13 13:47:06,687 : 
Dev acc : 86.47 Test acc : 86.55 for             SST Binary classification

2019-03-13 13:47:06,687 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 13:47:06,736 : loading BERT model bert-large-uncased
2019-03-13 13:47:06,736 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:47:06,757 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:47:06,758 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsjer20cl
2019-03-13 13:47:14,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:47:19,659 : Computing embedding for train
2019-03-13 13:47:41,527 : Computed train embeddings
2019-03-13 13:47:41,527 : Computing embedding for dev
2019-03-13 13:47:44,372 : Computed dev embeddings
2019-03-13 13:47:44,373 : Computing embedding for test
2019-03-13 13:47:49,982 : Computed test embeddings
2019-03-13 13:47:49,982 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:47:52,593 : [('reg:1e-05', 44.32), ('reg:0.0001', 44.32), ('reg:0.001', 44.05), ('reg:0.01', 44.32)]
2019-03-13 13:47:52,593 : Validation : best param found is reg = 1e-05 with score             44.32
2019-03-13 13:47:52,593 : Evaluating...
2019-03-13 13:47:53,061 : 
Dev acc : 44.32 Test acc : 44.43 for             SST Fine-Grained classification

2019-03-13 13:47:53,062 : ***** Transfer task : TREC *****


2019-03-13 13:47:53,074 : loading BERT model bert-large-uncased
2019-03-13 13:47:53,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:47:53,093 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:47:53,094 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpna1uk3u1
2019-03-13 13:48:00,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:48:13,355 : Computed train embeddings
2019-03-13 13:48:13,941 : Computed test embeddings
2019-03-13 13:48:13,941 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 13:48:20,488 : [('reg:1e-05', 82.19), ('reg:0.0001', 79.58), ('reg:0.001', 78.65), ('reg:0.01', 72.8)]
2019-03-13 13:48:20,488 : Cross-validation : best param found is reg = 1e-05             with score 82.19
2019-03-13 13:48:20,488 : Evaluating...
2019-03-13 13:48:20,843 : 
Dev acc : 82.19 Test acc : 88.4             for TREC

2019-03-13 13:48:20,844 : ***** Transfer task : MRPC *****


2019-03-13 13:48:20,865 : loading BERT model bert-large-uncased
2019-03-13 13:48:20,865 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:48:20,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:48:20,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxoc76foi
2019-03-13 13:48:28,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:48:33,824 : Computing embedding for train
2019-03-13 13:48:55,984 : Computed train embeddings
2019-03-13 13:48:55,984 : Computing embedding for test
2019-03-13 13:49:05,679 : Computed test embeddings
2019-03-13 13:49:05,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 13:49:10,712 : [('reg:1e-05', 71.47), ('reg:0.0001', 70.98), ('reg:0.001', 71.2), ('reg:0.01', 70.29)]
2019-03-13 13:49:10,713 : Cross-validation : best param found is reg = 1e-05             with score 71.47
2019-03-13 13:49:10,713 : Evaluating...
2019-03-13 13:49:11,064 : Dev acc : 71.47 Test acc 71.65; Test F1 81.75 for MRPC.

2019-03-13 13:49:11,064 : ***** Transfer task : SICK-Entailment*****


2019-03-13 13:49:11,088 : loading BERT model bert-large-uncased
2019-03-13 13:49:11,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:49:11,108 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:49:11,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdjox44yr
2019-03-13 13:49:18,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:49:23,935 : Computing embedding for train
2019-03-13 13:49:35,165 : Computed train embeddings
2019-03-13 13:49:35,165 : Computing embedding for dev
2019-03-13 13:49:36,700 : Computed dev embeddings
2019-03-13 13:49:36,700 : Computing embedding for test
2019-03-13 13:49:48,778 : Computed test embeddings
2019-03-13 13:49:48,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:49:50,444 : [('reg:1e-05', 71.8), ('reg:0.0001', 71.4), ('reg:0.001', 71.0), ('reg:0.01', 71.8)]
2019-03-13 13:49:50,444 : Validation : best param found is reg = 1e-05 with score             71.8
2019-03-13 13:49:50,444 : Evaluating...
2019-03-13 13:49:50,895 : 
Dev acc : 71.8 Test acc : 70.83 for                        SICK entailment

2019-03-13 13:49:50,896 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 13:49:50,923 : loading BERT model bert-large-uncased
2019-03-13 13:49:50,923 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:49:50,943 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:49:50,943 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfr49s1wk
2019-03-13 13:49:58,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:50:03,850 : Computing embedding for train
2019-03-13 13:50:15,079 : Computed train embeddings
2019-03-13 13:50:15,079 : Computing embedding for dev
2019-03-13 13:50:16,613 : Computed dev embeddings
2019-03-13 13:50:16,613 : Computing embedding for test
2019-03-13 13:50:28,658 : Computed test embeddings
2019-03-13 13:50:44,158 : Dev : Pearson 0.6583995474951411
2019-03-13 13:50:44,159 : Test : Pearson 0.6862032401353443 Spearman 0.6351844666760209 MSE 0.540003390658666                        for SICK Relatedness

2019-03-13 13:50:44,159 : 

***** Transfer task : STSBenchmark*****


2019-03-13 13:50:44,199 : loading BERT model bert-large-uncased
2019-03-13 13:50:44,200 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:50:44,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:50:44,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2zz85wln
2019-03-13 13:50:51,752 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:50:57,138 : Computing embedding for train
2019-03-13 13:51:15,627 : Computed train embeddings
2019-03-13 13:51:15,628 : Computing embedding for dev
2019-03-13 13:51:21,240 : Computed dev embeddings
2019-03-13 13:51:21,240 : Computing embedding for test
2019-03-13 13:51:25,817 : Computed test embeddings
2019-03-13 13:51:42,071 : Dev : Pearson 0.5086694522308384
2019-03-13 13:51:42,071 : Test : Pearson 0.4808239799554322 Spearman 0.48399673240353164 MSE 1.8759219945057304                        for SICK Relatedness

2019-03-13 13:51:42,071 : ***** Transfer task : SNLI Entailment*****


2019-03-13 13:51:46,977 : loading BERT model bert-large-uncased
2019-03-13 13:51:46,977 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:51:47,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:51:47,096 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnr0x95bm
2019-03-13 13:51:54,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:52:00,358 : PROGRESS (encoding): 0.00%
2019-03-13 13:54:44,741 : PROGRESS (encoding): 14.56%
2019-03-13 13:57:50,940 : PROGRESS (encoding): 29.12%
2019-03-13 14:00:57,759 : PROGRESS (encoding): 43.69%
2019-03-13 14:04:17,100 : PROGRESS (encoding): 58.25%
2019-03-13 14:07:58,969 : PROGRESS (encoding): 72.81%
2019-03-13 14:11:39,772 : PROGRESS (encoding): 87.37%
2019-03-13 14:15:38,887 : PROGRESS (encoding): 0.00%
2019-03-13 14:16:09,010 : PROGRESS (encoding): 0.00%
2019-03-13 14:16:37,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:17:25,159 : [('reg:1e-09', 62.64)]
2019-03-13 14:17:25,159 : Validation : best param found is reg = 1e-09 with score             62.64
2019-03-13 14:17:25,159 : Evaluating...
2019-03-13 14:18:12,080 : Dev acc : 62.64 Test acc : 62.55 for SNLI

2019-03-13 14:18:12,081 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 14:18:12,290 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 14:18:13,334 : loading BERT model bert-large-uncased
2019-03-13 14:18:13,334 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:18:13,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:18:13,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwb35mo0o
2019-03-13 14:18:20,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:18:26,132 : Computing embeddings for train/dev/test
2019-03-13 14:21:55,116 : Computed embeddings
2019-03-13 14:21:55,116 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:22:23,203 : [('reg:1e-05', 64.28), ('reg:0.0001', 63.39), ('reg:0.001', 61.79), ('reg:0.01', 52.8)]
2019-03-13 14:22:23,203 : Validation : best param found is reg = 1e-05 with score             64.28
2019-03-13 14:22:23,204 : Evaluating...
2019-03-13 14:22:30,042 : 
Dev acc : 64.3 Test acc : 65.0 for LENGTH classification

2019-03-13 14:22:30,043 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 14:22:30,413 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 14:22:30,466 : loading BERT model bert-large-uncased
2019-03-13 14:22:30,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:22:30,500 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:22:30,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2au7ruj9
2019-03-13 14:22:37,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:22:43,360 : Computing embeddings for train/dev/test
2019-03-13 14:25:56,156 : Computed embeddings
2019-03-13 14:25:56,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:26:30,815 : [('reg:1e-05', 29.86), ('reg:0.0001', 12.09), ('reg:0.001', 1.57), ('reg:0.01', 0.53)]
2019-03-13 14:26:30,816 : Validation : best param found is reg = 1e-05 with score             29.86
2019-03-13 14:26:30,816 : Evaluating...
2019-03-13 14:26:40,521 : 
Dev acc : 29.9 Test acc : 29.2 for WORDCONTENT classification

2019-03-13 14:26:40,522 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 14:26:40,884 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 14:26:40,950 : loading BERT model bert-large-uncased
2019-03-13 14:26:40,950 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:26:40,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:26:40,975 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp94prp7u6
2019-03-13 14:26:48,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:26:53,819 : Computing embeddings for train/dev/test
2019-03-13 14:29:54,949 : Computed embeddings
2019-03-13 14:29:54,949 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:30:15,626 : [('reg:1e-05', 28.65), ('reg:0.0001', 28.07), ('reg:0.001', 26.48), ('reg:0.01', 24.31)]
2019-03-13 14:30:15,627 : Validation : best param found is reg = 1e-05 with score             28.65
2019-03-13 14:30:15,627 : Evaluating...
2019-03-13 14:30:22,165 : 
Dev acc : 28.6 Test acc : 28.3 for DEPTH classification

2019-03-13 14:30:22,166 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 14:30:22,550 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 14:30:22,613 : loading BERT model bert-large-uncased
2019-03-13 14:30:22,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:30:22,640 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:30:22,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1ts_mb7w
2019-03-13 14:30:30,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:30:35,450 : Computing embeddings for train/dev/test
2019-03-13 14:33:23,390 : Computed embeddings
2019-03-13 14:33:23,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:33:53,482 : [('reg:1e-05', 74.38), ('reg:0.0001', 71.93), ('reg:0.001', 68.16), ('reg:0.01', 61.72)]
2019-03-13 14:33:53,482 : Validation : best param found is reg = 1e-05 with score             74.38
2019-03-13 14:33:53,482 : Evaluating...
2019-03-13 14:34:01,798 : 
Dev acc : 74.4 Test acc : 74.6 for TOPCONSTITUENTS classification

2019-03-13 14:34:01,799 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 14:34:02,156 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 14:34:02,222 : loading BERT model bert-large-uncased
2019-03-13 14:34:02,222 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:34:02,342 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:34:02,342 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0oehzxdi
2019-03-13 14:34:09,825 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:34:15,280 : Computing embeddings for train/dev/test
2019-03-13 14:37:17,781 : Computed embeddings
2019-03-13 14:37:17,781 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:37:43,608 : [('reg:1e-05', 86.93), ('reg:0.0001', 86.85), ('reg:0.001', 84.26), ('reg:0.01', 83.98)]
2019-03-13 14:37:43,608 : Validation : best param found is reg = 1e-05 with score             86.93
2019-03-13 14:37:43,608 : Evaluating...
2019-03-13 14:37:50,160 : 
Dev acc : 86.9 Test acc : 86.9 for BIGRAMSHIFT classification

2019-03-13 14:37:50,161 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 14:37:50,536 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 14:37:50,601 : loading BERT model bert-large-uncased
2019-03-13 14:37:50,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:37:50,717 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:37:50,717 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4fw9lt6e
2019-03-13 14:37:58,173 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:38:03,481 : Computing embeddings for train/dev/test
2019-03-13 14:41:02,342 : Computed embeddings
2019-03-13 14:41:02,342 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:41:25,183 : [('reg:1e-05', 89.62), ('reg:0.0001', 89.91), ('reg:0.001', 90.08), ('reg:0.01', 89.41)]
2019-03-13 14:41:25,183 : Validation : best param found is reg = 0.001 with score             90.08
2019-03-13 14:41:25,183 : Evaluating...
2019-03-13 14:41:29,775 : 
Dev acc : 90.1 Test acc : 88.8 for TENSE classification

2019-03-13 14:41:29,776 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 14:41:30,358 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 14:41:30,423 : loading BERT model bert-large-uncased
2019-03-13 14:41:30,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:41:30,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:41:30,452 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp39iv1dn0
2019-03-13 14:41:37,989 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:41:43,403 : Computing embeddings for train/dev/test
2019-03-13 14:44:52,715 : Computed embeddings
2019-03-13 14:44:52,715 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:45:16,076 : [('reg:1e-05', 86.11), ('reg:0.0001', 85.97), ('reg:0.001', 87.14), ('reg:0.01', 83.24)]
2019-03-13 14:45:16,076 : Validation : best param found is reg = 0.001 with score             87.14
2019-03-13 14:45:16,076 : Evaluating...
2019-03-13 14:45:22,484 : 
Dev acc : 87.1 Test acc : 87.3 for SUBJNUMBER classification

2019-03-13 14:45:22,485 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 14:45:22,913 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 14:45:22,981 : loading BERT model bert-large-uncased
2019-03-13 14:45:22,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:45:23,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:45:23,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvjtho9vw
2019-03-13 14:45:30,471 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:45:35,919 : Computing embeddings for train/dev/test
2019-03-13 14:48:41,713 : Computed embeddings
2019-03-13 14:48:41,713 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:49:15,227 : [('reg:1e-05', 77.81), ('reg:0.0001', 78.1), ('reg:0.001', 77.75), ('reg:0.01', 75.69)]
2019-03-13 14:49:15,227 : Validation : best param found is reg = 0.0001 with score             78.1
2019-03-13 14:49:15,227 : Evaluating...
2019-03-13 14:49:23,887 : 
Dev acc : 78.1 Test acc : 79.6 for OBJNUMBER classification

2019-03-13 14:49:23,888 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 14:49:24,273 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 14:49:24,341 : loading BERT model bert-large-uncased
2019-03-13 14:49:24,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:49:24,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:49:24,463 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5xah2rwt
2019-03-13 14:49:31,927 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:49:37,247 : Computing embeddings for train/dev/test
2019-03-13 14:53:12,220 : Computed embeddings
2019-03-13 14:53:12,220 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:53:33,710 : [('reg:1e-05', 63.86), ('reg:0.0001', 63.86), ('reg:0.001', 63.77), ('reg:0.01', 62.2)]
2019-03-13 14:53:33,711 : Validation : best param found is reg = 1e-05 with score             63.86
2019-03-13 14:53:33,711 : Evaluating...
2019-03-13 14:53:39,912 : 
Dev acc : 63.9 Test acc : 63.6 for ODDMANOUT classification

2019-03-13 14:53:39,913 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 14:53:40,531 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 14:53:40,608 : loading BERT model bert-large-uncased
2019-03-13 14:53:40,608 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:53:40,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:53:40,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5m7aaup9
2019-03-13 14:53:48,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:53:53,411 : Computing embeddings for train/dev/test
2019-03-13 14:57:26,417 : Computed embeddings
2019-03-13 14:57:26,417 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:57:58,964 : [('reg:1e-05', 73.99), ('reg:0.0001', 74.02), ('reg:0.001', 73.9), ('reg:0.01', 68.46)]
2019-03-13 14:57:58,965 : Validation : best param found is reg = 0.0001 with score             74.02
2019-03-13 14:57:58,965 : Evaluating...
2019-03-13 14:58:07,538 : 
Dev acc : 74.0 Test acc : 74.0 for COORDINATIONINVERSION classification

2019-03-13 14:58:07,540 : total results: {'STS12': {'MSRpar': {'pearson': (0.2881876360855634, 8.239393842798241e-16), 'spearman': SpearmanrResult(correlation=0.33317609672781784, pvalue=6.695079270709756e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.06048641783801918, 0.09787550057972219), 'spearman': SpearmanrResult(correlation=0.10539953814398037, pvalue=0.003855810341534408), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.45191250732787724, 1.752549220006676e-24), 'spearman': SpearmanrResult(correlation=0.5290745460280649, pvalue=1.8086095692773261e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.37194733462232793, 5.0764265439324827e-26), 'spearman': SpearmanrResult(correlation=0.40272098877947043, pvalue=1.3015340295247223e-30), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5881756832261886, 1.6959775100267364e-38), 'spearman': SpearmanrResult(correlation=0.5149454261205533, pvalue=2.117294527723528e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.35214191581999527, 'wmean': 0.31614413767058486}, 'spearman': {'mean': 0.3770633191599774, 'wmean': 0.3472589798543867}}}, 'STS13': {'FNWN': {'pearson': (0.22072928097697234, 0.0022722345573507334), 'spearman': SpearmanrResult(correlation=0.22254312557553496, pvalue=0.002084952327857428), 'nsamples': 189}, 'headlines': {'pearson': (0.39835494299608554, 6.2483304979027735e-30), 'spearman': SpearmanrResult(correlation=0.3973032549181349, pvalue=9.08583478281569e-30), 'nsamples': 750}, 'OnWN': {'pearson': (0.21021994217964624, 5.052824964349827e-07), 'spearman': SpearmanrResult(correlation=0.20961076429029707, pvalue=5.459886442724779e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.27643472205090136, 'wmean': 0.30561161927632896}, 'spearman': {'mean': 0.27648571492798896, 'wmean': 0.305086487126156}}}, 'STS14': {'deft-forum': {'pearson': (-0.03310702270111188, 0.48358718817361745), 'spearman': SpearmanrResult(correlation=-0.03136715496850055, pvalue=0.5068756316766607), 'nsamples': 450}, 'deft-news': {'pearson': (0.43206568442572113, 4.4845573720129105e-15), 'spearman': SpearmanrResult(correlation=0.4370280904278598, pvalue=2.008505813742392e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.3851655522633696, 6.210501790781778e-28), 'spearman': SpearmanrResult(correlation=0.36742769739806413, pvalue=2.1851315741200496e-25), 'nsamples': 750}, 'images': {'pearson': (0.15392213632769286, 2.300268654270423e-05), 'spearman': SpearmanrResult(correlation=0.16049205979578052, pvalue=1.002724138173445e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.36196224605113614, 1.2379478298793471e-24), 'spearman': SpearmanrResult(correlation=0.36908151778234805, pvalue=1.2843424657201572e-25), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4631626375166109, 3.7812156211961564e-41), 'spearman': SpearmanrResult(correlation=0.421400936764227, pvalue=1.2099958362423008e-33), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.29386187231390315, 'wmean': 0.3034349264616861}, 'spearman': {'mean': 0.2873438578666298, 'wmean': 0.2948786309860927}}}, 'STS15': {'answers-forums': {'pearson': (0.4107275594404785, 1.0760067285551956e-16), 'spearman': SpearmanrResult(correlation=0.40807742772475575, pvalue=1.7607168289548322e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.5285967348211577, 3.270552165169354e-55), 'spearman': SpearmanrResult(correlation=0.5347219448708491, pvalue=1.0836567106375868e-56), 'nsamples': 750}, 'belief': {'pearson': (0.46197886247200254, 3.181224341132629e-21), 'spearman': SpearmanrResult(correlation=0.4926804104466611, pvalue=2.5091515932588527e-24), 'nsamples': 375}, 'headlines': {'pearson': (0.42509903888066564, 2.8814409486218473e-34), 'spearman': SpearmanrResult(correlation=0.42769758704825017, pvalue=1.0400932263674227e-34), 'nsamples': 750}, 'images': {'pearson': (0.1963747023963098, 5.898550689248806e-08), 'spearman': SpearmanrResult(correlation=0.19704566751424438, pvalue=5.305797273719296e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.40455537960212284, 'wmean': 0.39660592176359344}, 'spearman': {'mean': 0.4120446075209522, 'wmean': 0.402461029629763}}}, 'STS16': {'answer-answer': {'pearson': (0.3429148415877755, 2.0326144379571298e-08), 'spearman': SpearmanrResult(correlation=0.34075859304028466, pvalue=2.5233312182446892e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5627042697750069, 3.393997831032737e-22), 'spearman': SpearmanrResult(correlation=0.5748338438747123, pvalue=2.676899992051705e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.614845806482378, 2.6300591368161954e-25), 'spearman': SpearmanrResult(correlation=0.6349371098970689, pvalue=2.3230024531495903e-27), 'nsamples': 230}, 'postediting': {'pearson': (0.7504893812660535, 2.0088143479942066e-45), 'spearman': SpearmanrResult(correlation=0.775234308941829, pvalue=3.453400044402529e-50), 'nsamples': 244}, 'question-question': {'pearson': (0.002724882534786781, 0.968765227478854), 'spearman': SpearmanrResult(correlation=0.0862482679607929, pvalue=0.2143421403193953), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4547358363292001, 'wmean': 0.4656974518607977}, 'spearman': {'mean': 0.48240242474293754, 'wmean': 0.4914881150244296}}}, 'MR': {'devacc': 82.2, 'acc': 80.73, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.59, 'acc': 87.02, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.19, 'acc': 85.65, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.5, 'acc': 94.87, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.47, 'acc': 86.55, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.32, 'acc': 44.43, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.19, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.47, 'acc': 71.65, 'f1': 81.75, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.8, 'acc': 70.83, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6583995474951411, 'pearson': 0.6862032401353443, 'spearman': 0.6351844666760209, 'mse': 0.540003390658666, 'yhat': array([4.05306153, 4.50077081, 2.14235   , ..., 3.4019466 , 4.28251356,        3.97036519]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5086694522308384, 'pearson': 0.4808239799554322, 'spearman': 0.48399673240353164, 'mse': 1.8759219945057304, 'yhat': array([1.49136096, 3.22236229, 2.99830236, ..., 4.18166753, 3.700077  ,        3.25105057]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.64, 'acc': 62.55, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 64.28, 'acc': 65.04, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 29.86, 'acc': 29.23, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.65, 'acc': 28.26, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.38, 'acc': 74.59, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.93, 'acc': 86.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.08, 'acc': 88.75, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.14, 'acc': 87.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.1, 'acc': 79.58, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.86, 'acc': 63.59, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 74.02, 'acc': 74.04, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 14:58:07,540 : STS12 p=0.3161, STS12 s=0.3473, STS13 p=0.3056, STS13 s=0.3051, STS14 p=0.3034, STS14 s=0.2949, STS15 p=0.3966, STS15 s=0.4025, STS 16 p=0.4657, STS16 s=0.4915, STS B p=0.4808, STS B s=0.4840, STS B m=1.8759, SICK-R p=0.6862, SICK-R s=0.6352, SICK-P m=0.5400
2019-03-13 14:58:07,540 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 14:58:07,540 : 0.3161,0.3473,0.3056,0.3051,0.3034,0.2949,0.3966,0.4025,0.4657,0.4915,0.4808,0.4840,1.8759,0.6862,0.6352,0.5400
2019-03-13 14:58:07,540 : MR=80.73, CR=87.02, SUBJ=94.87, MPQA=85.65, SST-B=86.55, SST-F=44.43, TREC=88.40, SICK-E=70.83, SNLI=62.55, MRPC=71.65, MRPC f=81.75
2019-03-13 14:58:07,540 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 14:58:07,540 : 80.73,87.02,94.87,85.65,86.55,44.43,88.40,70.83,62.55,71.65,81.75
2019-03-13 14:58:07,540 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 14:58:07,540 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 14:58:07,540 : na,na,na,na,na,na,na,na,na,na
2019-03-13 14:58:07,540 : SentLen=65.04, WC=29.23, TreeDepth=28.26, TopConst=74.59, BShift=86.89, Tense=88.75, SubjNum=87.30, ObjNum=79.58, SOMO=63.59, CoordInv=74.04, average=67.73
2019-03-13 14:58:07,540 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 14:58:07,540 : 65.04,29.23,28.26,74.59,86.89,88.75,87.30,79.58,63.59,74.04,67.73
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,540 : layer 19
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,540 : ********************************************************************************
2019-03-13 14:58:07,633 : ***** Transfer task : STS12 *****


2019-03-13 14:58:07,646 : loading BERT model bert-large-uncased
2019-03-13 14:58:07,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:58:07,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:58:07,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyrlx3hzh
2019-03-13 14:58:15,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:58:24,461 : MSRpar : pearson = 0.2690, spearman = 0.3132
2019-03-13 14:58:26,093 : MSRvid : pearson = 0.0480, spearman = 0.1045
2019-03-13 14:58:27,497 : SMTeuroparl : pearson = 0.4142, spearman = 0.5106
2019-03-13 14:58:30,183 : surprise.OnWN : pearson = 0.3752, spearman = 0.4040
2019-03-13 14:58:31,604 : surprise.SMTnews : pearson = 0.5482, spearman = 0.5166
2019-03-13 14:58:31,604 : ALL (weighted average) : Pearson = 0.2986,             Spearman = 0.3400
2019-03-13 14:58:31,604 : ALL (average) : Pearson = 0.3309,             Spearman = 0.3698

2019-03-13 14:58:31,604 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 14:58:31,613 : loading BERT model bert-large-uncased
2019-03-13 14:58:31,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:58:31,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:58:31,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnrdzdsqi
2019-03-13 14:58:39,133 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:58:45,840 : FNWN : pearson = 0.1620, spearman = 0.1582
2019-03-13 14:58:47,728 : headlines : pearson = 0.4080, spearman = 0.4048
2019-03-13 14:58:49,190 : OnWN : pearson = 0.2096, spearman = 0.2219
2019-03-13 14:58:49,191 : ALL (weighted average) : Pearson = 0.3028,             Spearman = 0.3053
2019-03-13 14:58:49,191 : ALL (average) : Pearson = 0.2599,             Spearman = 0.2616

2019-03-13 14:58:49,191 : ***** Transfer task : STS14 *****


2019-03-13 14:58:49,237 : loading BERT model bert-large-uncased
2019-03-13 14:58:49,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:58:49,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:58:49,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqxqxqn2_
2019-03-13 14:58:56,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:59:03,440 : deft-forum : pearson = -0.0481, spearman = -0.0475
2019-03-13 14:59:05,072 : deft-news : pearson = 0.4177, spearman = 0.4256
2019-03-13 14:59:07,231 : headlines : pearson = 0.3903, spearman = 0.3719
2019-03-13 14:59:09,299 : images : pearson = 0.1559, spearman = 0.1696
2019-03-13 14:59:11,421 : OnWN : pearson = 0.3621, spearman = 0.3693
2019-03-13 14:59:14,273 : tweet-news : pearson = 0.4546, spearman = 0.4161
2019-03-13 14:59:14,273 : ALL (weighted average) : Pearson = 0.3002,             Spearman = 0.2937
2019-03-13 14:59:14,273 : ALL (average) : Pearson = 0.2888,             Spearman = 0.2842

2019-03-13 14:59:14,273 : ***** Transfer task : STS15 *****


2019-03-13 14:59:14,305 : loading BERT model bert-large-uncased
2019-03-13 14:59:14,305 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:59:14,354 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:59:14,355 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjb5bq034
2019-03-13 14:59:21,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:59:29,071 : answers-forums : pearson = 0.4104, spearman = 0.3961
2019-03-13 14:59:31,147 : answers-students : pearson = 0.5416, spearman = 0.5559
2019-03-13 14:59:33,192 : belief : pearson = 0.4684, spearman = 0.4929
2019-03-13 14:59:35,434 : headlines : pearson = 0.4424, spearman = 0.4451
2019-03-13 14:59:37,558 : images : pearson = 0.2107, spearman = 0.2073
2019-03-13 14:59:37,558 : ALL (weighted average) : Pearson = 0.4085,             Spearman = 0.4132
2019-03-13 14:59:37,558 : ALL (average) : Pearson = 0.4147,             Spearman = 0.4195

2019-03-13 14:59:37,558 : ***** Transfer task : STS16 *****


2019-03-13 14:59:37,595 : loading BERT model bert-large-uncased
2019-03-13 14:59:37,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:59:37,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:59:37,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6vc5s4qa
2019-03-13 14:59:45,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:59:51,371 : answer-answer : pearson = 0.3442, spearman = 0.3357
2019-03-13 14:59:52,029 : headlines : pearson = 0.5636, spearman = 0.5738
2019-03-13 14:59:52,908 : plagiarism : pearson = 0.6380, spearman = 0.6439
2019-03-13 14:59:54,396 : postediting : pearson = 0.7509, spearman = 0.7683
2019-03-13 14:59:55,000 : question-question : pearson = -0.0098, spearman = 0.0580
2019-03-13 14:59:55,000 : ALL (weighted average) : Pearson = 0.4685,             Spearman = 0.4855
2019-03-13 14:59:55,000 : ALL (average) : Pearson = 0.4574,             Spearman = 0.4760

2019-03-13 14:59:55,000 : ***** Transfer task : MR *****


2019-03-13 14:59:55,046 : loading BERT model bert-large-uncased
2019-03-13 14:59:55,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:59:55,067 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:59:55,067 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnxsg_4ex
2019-03-13 15:00:02,532 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:00:08,003 : Generating sentence embeddings
2019-03-13 15:00:39,368 : Generated sentence embeddings
2019-03-13 15:00:39,369 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:00:49,713 : Best param found at split 1: l2reg = 0.001                 with score 82.53
2019-03-13 15:00:59,729 : Best param found at split 2: l2reg = 0.001                 with score 82.59
2019-03-13 15:01:11,542 : Best param found at split 3: l2reg = 0.01                 with score 82.65
2019-03-13 15:01:23,100 : Best param found at split 4: l2reg = 0.01                 with score 82.95
2019-03-13 15:01:32,560 : Best param found at split 5: l2reg = 0.01                 with score 82.32
2019-03-13 15:01:32,934 : Dev acc : 82.61 Test acc : 82.23

2019-03-13 15:01:32,935 : ***** Transfer task : CR *****


2019-03-13 15:01:32,943 : loading BERT model bert-large-uncased
2019-03-13 15:01:32,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:01:32,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:01:32,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl4q9623_
2019-03-13 15:01:40,436 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:01:45,723 : Generating sentence embeddings
2019-03-13 15:01:54,039 : Generated sentence embeddings
2019-03-13 15:01:54,039 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:01:57,432 : Best param found at split 1: l2reg = 0.001                 with score 87.28
2019-03-13 15:02:01,186 : Best param found at split 2: l2reg = 1e-05                 with score 88.31
2019-03-13 15:02:04,464 : Best param found at split 3: l2reg = 0.001                 with score 89.5
2019-03-13 15:02:08,166 : Best param found at split 4: l2reg = 0.01                 with score 88.88
2019-03-13 15:02:12,030 : Best param found at split 5: l2reg = 0.01                 with score 88.61
2019-03-13 15:02:12,155 : Dev acc : 88.52 Test acc : 87.13

2019-03-13 15:02:12,155 : ***** Transfer task : MPQA *****


2019-03-13 15:02:12,160 : loading BERT model bert-large-uncased
2019-03-13 15:02:12,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:02:12,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:02:12,180 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9t3sb6lm
2019-03-13 15:02:19,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:02:25,042 : Generating sentence embeddings
2019-03-13 15:02:32,635 : Generated sentence embeddings
2019-03-13 15:02:32,635 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:02:40,965 : Best param found at split 1: l2reg = 0.001                 with score 84.28
2019-03-13 15:02:51,184 : Best param found at split 2: l2reg = 1e-05                 with score 84.02
2019-03-13 15:03:01,656 : Best param found at split 3: l2reg = 0.0001                 with score 83.35
2019-03-13 15:03:13,359 : Best param found at split 4: l2reg = 0.0001                 with score 85.34
2019-03-13 15:03:24,326 : Best param found at split 5: l2reg = 0.0001                 with score 84.14
2019-03-13 15:03:24,933 : Dev acc : 84.23 Test acc : 85.62

2019-03-13 15:03:24,934 : ***** Transfer task : SUBJ *****


2019-03-13 15:03:24,951 : loading BERT model bert-large-uncased
2019-03-13 15:03:24,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:03:25,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:03:25,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa2ynyj61
2019-03-13 15:03:32,440 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:03:38,007 : Generating sentence embeddings
2019-03-13 15:04:08,770 : Generated sentence embeddings
2019-03-13 15:04:08,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:04:18,083 : Best param found at split 1: l2reg = 0.01                 with score 95.44
2019-03-13 15:04:28,346 : Best param found at split 2: l2reg = 0.001                 with score 95.81
2019-03-13 15:04:37,863 : Best param found at split 3: l2reg = 0.001                 with score 95.35
2019-03-13 15:04:48,503 : Best param found at split 4: l2reg = 0.001                 with score 95.85
2019-03-13 15:04:58,185 : Best param found at split 5: l2reg = 0.01                 with score 95.55
2019-03-13 15:04:58,604 : Dev acc : 95.6 Test acc : 95.19

2019-03-13 15:04:58,605 : ***** Transfer task : SST Binary classification *****


2019-03-13 15:04:58,743 : loading BERT model bert-large-uncased
2019-03-13 15:04:58,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:04:58,766 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:04:58,766 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3ovgkp9m
2019-03-13 15:05:06,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:05:11,515 : Computing embedding for train
2019-03-13 15:06:51,050 : Computed train embeddings
2019-03-13 15:06:51,050 : Computing embedding for dev
2019-03-13 15:06:53,217 : Computed dev embeddings
2019-03-13 15:06:53,217 : Computing embedding for test
2019-03-13 15:06:57,769 : Computed test embeddings
2019-03-13 15:06:57,769 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:07:15,605 : [('reg:1e-05', 85.89), ('reg:0.0001', 85.78), ('reg:0.001', 85.89), ('reg:0.01', 86.12)]
2019-03-13 15:07:15,606 : Validation : best param found is reg = 0.01 with score             86.12
2019-03-13 15:07:15,606 : Evaluating...
2019-03-13 15:07:19,996 : 
Dev acc : 86.12 Test acc : 87.7 for             SST Binary classification

2019-03-13 15:07:19,997 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 15:07:20,047 : loading BERT model bert-large-uncased
2019-03-13 15:07:20,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:07:20,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:07:20,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9uk6ev83
2019-03-13 15:07:27,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:07:33,016 : Computing embedding for train
2019-03-13 15:07:54,840 : Computed train embeddings
2019-03-13 15:07:54,840 : Computing embedding for dev
2019-03-13 15:07:57,682 : Computed dev embeddings
2019-03-13 15:07:57,682 : Computing embedding for test
2019-03-13 15:08:03,284 : Computed test embeddings
2019-03-13 15:08:03,284 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:08:05,667 : [('reg:1e-05', 45.87), ('reg:0.0001', 44.87), ('reg:0.001', 43.32), ('reg:0.01', 37.33)]
2019-03-13 15:08:05,668 : Validation : best param found is reg = 1e-05 with score             45.87
2019-03-13 15:08:05,668 : Evaluating...
2019-03-13 15:08:06,315 : 
Dev acc : 45.87 Test acc : 45.84 for             SST Fine-Grained classification

2019-03-13 15:08:06,315 : ***** Transfer task : TREC *****


2019-03-13 15:08:06,329 : loading BERT model bert-large-uncased
2019-03-13 15:08:06,329 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:08:06,347 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:08:06,348 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwtjz3j9u
2019-03-13 15:08:13,851 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:08:26,800 : Computed train embeddings
2019-03-13 15:08:27,386 : Computed test embeddings
2019-03-13 15:08:27,386 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:08:34,170 : [('reg:1e-05', 78.63), ('reg:0.0001', 78.8), ('reg:0.001', 78.96), ('reg:0.01', 73.69)]
2019-03-13 15:08:34,170 : Cross-validation : best param found is reg = 0.001             with score 78.96
2019-03-13 15:08:34,170 : Evaluating...
2019-03-13 15:08:34,618 : 
Dev acc : 78.96 Test acc : 86.8             for TREC

2019-03-13 15:08:34,619 : ***** Transfer task : MRPC *****


2019-03-13 15:08:34,640 : loading BERT model bert-large-uncased
2019-03-13 15:08:34,641 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:08:34,701 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:08:34,701 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphvzwdq83
2019-03-13 15:08:42,189 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:08:47,605 : Computing embedding for train
2019-03-13 15:09:09,755 : Computed train embeddings
2019-03-13 15:09:09,755 : Computing embedding for test
2019-03-13 15:09:19,437 : Computed test embeddings
2019-03-13 15:09:19,459 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:09:24,071 : [('reg:1e-05', 71.17), ('reg:0.0001', 71.03), ('reg:0.001', 71.25), ('reg:0.01', 70.36)]
2019-03-13 15:09:24,071 : Cross-validation : best param found is reg = 0.001             with score 71.25
2019-03-13 15:09:24,071 : Evaluating...
2019-03-13 15:09:24,379 : Dev acc : 71.25 Test acc 62.09; Test F1 66.67 for MRPC.

2019-03-13 15:09:24,379 : ***** Transfer task : SICK-Entailment*****


2019-03-13 15:09:24,403 : loading BERT model bert-large-uncased
2019-03-13 15:09:24,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:09:24,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:09:24,423 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpynxzyl3n
2019-03-13 15:09:31,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:09:37,344 : Computing embedding for train
2019-03-13 15:09:48,605 : Computed train embeddings
2019-03-13 15:09:48,605 : Computing embedding for dev
2019-03-13 15:09:50,139 : Computed dev embeddings
2019-03-13 15:09:50,139 : Computing embedding for test
2019-03-13 15:10:02,217 : Computed test embeddings
2019-03-13 15:10:02,253 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:10:03,784 : [('reg:1e-05', 74.0), ('reg:0.0001', 72.0), ('reg:0.001', 69.2), ('reg:0.01', 72.4)]
2019-03-13 15:10:03,785 : Validation : best param found is reg = 1e-05 with score             74.0
2019-03-13 15:10:03,785 : Evaluating...
2019-03-13 15:10:04,231 : 
Dev acc : 74.0 Test acc : 71.3 for                        SICK entailment

2019-03-13 15:10:04,232 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 15:10:04,259 : loading BERT model bert-large-uncased
2019-03-13 15:10:04,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:10:04,278 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:10:04,278 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpol8_03y4
2019-03-13 15:10:11,778 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:10:17,177 : Computing embedding for train
2019-03-13 15:10:28,434 : Computed train embeddings
2019-03-13 15:10:28,434 : Computing embedding for dev
2019-03-13 15:10:29,969 : Computed dev embeddings
2019-03-13 15:10:29,969 : Computing embedding for test
2019-03-13 15:10:42,040 : Computed test embeddings
2019-03-13 15:10:57,183 : Dev : Pearson 0.6509876874210582
2019-03-13 15:10:57,183 : Test : Pearson 0.67197093875499 Spearman 0.6246913978584104 MSE 0.5602436461745274                        for SICK Relatedness

2019-03-13 15:10:57,184 : 

***** Transfer task : STSBenchmark*****


2019-03-13 15:10:57,228 : loading BERT model bert-large-uncased
2019-03-13 15:10:57,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:10:57,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:10:57,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi7r5979r
2019-03-13 15:11:04,733 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:11:10,152 : Computing embedding for train
2019-03-13 15:11:28,656 : Computed train embeddings
2019-03-13 15:11:28,656 : Computing embedding for dev
2019-03-13 15:11:34,277 : Computed dev embeddings
2019-03-13 15:11:34,277 : Computing embedding for test
2019-03-13 15:11:38,861 : Computed test embeddings
2019-03-13 15:11:57,786 : Dev : Pearson 0.4720757078947915
2019-03-13 15:11:57,786 : Test : Pearson 0.43512196031835637 Spearman 0.4285375421034249 MSE 1.9642779249158955                        for SICK Relatedness

2019-03-13 15:11:57,786 : ***** Transfer task : SNLI Entailment*****


2019-03-13 15:12:02,665 : loading BERT model bert-large-uncased
2019-03-13 15:12:02,665 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:12:02,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:12:02,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnofm4wvs
2019-03-13 15:12:10,174 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:12:15,909 : PROGRESS (encoding): 0.00%
2019-03-13 15:15:00,085 : PROGRESS (encoding): 14.56%
2019-03-13 15:18:06,147 : PROGRESS (encoding): 29.12%
2019-03-13 15:21:12,959 : PROGRESS (encoding): 43.69%
2019-03-13 15:24:32,154 : PROGRESS (encoding): 58.25%
2019-03-13 15:28:13,935 : PROGRESS (encoding): 72.81%
2019-03-13 15:31:54,614 : PROGRESS (encoding): 87.37%
2019-03-13 15:35:53,400 : PROGRESS (encoding): 0.00%
2019-03-13 15:36:23,484 : PROGRESS (encoding): 0.00%
2019-03-13 15:36:52,384 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:37:30,209 : [('reg:1e-09', 60.94)]
2019-03-13 15:37:30,210 : Validation : best param found is reg = 1e-09 with score             60.94
2019-03-13 15:37:30,210 : Evaluating...
2019-03-13 15:38:08,240 : Dev acc : 60.94 Test acc : 61.11 for SNLI

2019-03-13 15:38:08,241 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 15:38:08,450 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 15:38:09,531 : loading BERT model bert-large-uncased
2019-03-13 15:38:09,531 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:38:09,557 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:38:09,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz25kvaz7
2019-03-13 15:38:17,020 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:38:22,438 : Computing embeddings for train/dev/test
2019-03-13 15:41:51,382 : Computed embeddings
2019-03-13 15:41:51,382 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:42:17,693 : [('reg:1e-05', 65.43), ('reg:0.0001', 62.11), ('reg:0.001', 57.07), ('reg:0.01', 47.27)]
2019-03-13 15:42:17,693 : Validation : best param found is reg = 1e-05 with score             65.43
2019-03-13 15:42:17,693 : Evaluating...
2019-03-13 15:42:25,182 : 
Dev acc : 65.4 Test acc : 64.9 for LENGTH classification

2019-03-13 15:42:25,183 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 15:42:25,547 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 15:42:25,592 : loading BERT model bert-large-uncased
2019-03-13 15:42:25,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:42:25,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:42:25,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0iy95xmy
2019-03-13 15:42:33,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:42:38,581 : Computing embeddings for train/dev/test
2019-03-13 15:45:51,456 : Computed embeddings
2019-03-13 15:45:51,457 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:46:32,526 : [('reg:1e-05', 28.09), ('reg:0.0001', 10.56), ('reg:0.001', 1.61), ('reg:0.01', 0.54)]
2019-03-13 15:46:32,526 : Validation : best param found is reg = 1e-05 with score             28.09
2019-03-13 15:46:32,527 : Evaluating...
2019-03-13 15:46:44,667 : 
Dev acc : 28.1 Test acc : 28.3 for WORDCONTENT classification

2019-03-13 15:46:44,668 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 15:46:45,028 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 15:46:45,094 : loading BERT model bert-large-uncased
2019-03-13 15:46:45,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:46:45,192 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:46:45,192 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc6jl8iey
2019-03-13 15:46:52,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:46:58,124 : Computing embeddings for train/dev/test
2019-03-13 15:49:59,294 : Computed embeddings
2019-03-13 15:49:59,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:50:20,729 : [('reg:1e-05', 26.64), ('reg:0.0001', 26.77), ('reg:0.001', 26.72), ('reg:0.01', 25.91)]
2019-03-13 15:50:20,729 : Validation : best param found is reg = 0.0001 with score             26.77
2019-03-13 15:50:20,729 : Evaluating...
2019-03-13 15:50:26,221 : 
Dev acc : 26.8 Test acc : 26.7 for DEPTH classification

2019-03-13 15:50:26,222 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 15:50:26,593 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 15:50:26,656 : loading BERT model bert-large-uncased
2019-03-13 15:50:26,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:50:26,767 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:50:26,767 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvw0p_3jl
2019-03-13 15:50:34,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:50:39,708 : Computing embeddings for train/dev/test
2019-03-13 15:53:27,697 : Computed embeddings
2019-03-13 15:53:27,697 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:53:50,348 : [('reg:1e-05', 72.56), ('reg:0.0001', 69.83), ('reg:0.001', 69.26), ('reg:0.01', 58.73)]
2019-03-13 15:53:50,348 : Validation : best param found is reg = 1e-05 with score             72.56
2019-03-13 15:53:50,348 : Evaluating...
2019-03-13 15:53:57,065 : 
Dev acc : 72.6 Test acc : 72.8 for TOPCONSTITUENTS classification

2019-03-13 15:53:57,066 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 15:53:57,440 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 15:53:57,505 : loading BERT model bert-large-uncased
2019-03-13 15:53:57,505 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:53:57,534 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:53:57,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4hskj0s0
2019-03-13 15:54:05,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:54:10,463 : Computing embeddings for train/dev/test
2019-03-13 15:57:13,078 : Computed embeddings
2019-03-13 15:57:13,079 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:57:43,196 : [('reg:1e-05', 86.52), ('reg:0.0001', 86.51), ('reg:0.001', 87.79), ('reg:0.01', 86.5)]
2019-03-13 15:57:43,196 : Validation : best param found is reg = 0.001 with score             87.79
2019-03-13 15:57:43,197 : Evaluating...
2019-03-13 15:57:50,293 : 
Dev acc : 87.8 Test acc : 87.0 for BIGRAMSHIFT classification

2019-03-13 15:57:50,294 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 15:57:50,683 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 15:57:50,748 : loading BERT model bert-large-uncased
2019-03-13 15:57:50,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:57:50,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:57:50,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvel76v2m
2019-03-13 15:57:58,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:58:03,552 : Computing embeddings for train/dev/test
2019-03-13 16:01:01,976 : Computed embeddings
2019-03-13 16:01:01,977 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:01:29,187 : [('reg:1e-05', 88.34), ('reg:0.0001', 88.4), ('reg:0.001', 88.47), ('reg:0.01', 87.78)]
2019-03-13 16:01:29,187 : Validation : best param found is reg = 0.001 with score             88.47
2019-03-13 16:01:29,187 : Evaluating...
2019-03-13 16:01:35,499 : 
Dev acc : 88.5 Test acc : 88.0 for TENSE classification

2019-03-13 16:01:35,501 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 16:01:35,903 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 16:01:35,966 : loading BERT model bert-large-uncased
2019-03-13 16:01:35,966 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:01:36,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:01:36,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6dw6gjg7
2019-03-13 16:01:43,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:01:48,823 : Computing embeddings for train/dev/test
2019-03-13 16:04:57,825 : Computed embeddings
2019-03-13 16:04:57,825 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:05:22,171 : [('reg:1e-05', 84.24), ('reg:0.0001', 83.85), ('reg:0.001', 84.29), ('reg:0.01', 85.09)]
2019-03-13 16:05:22,171 : Validation : best param found is reg = 0.01 with score             85.09
2019-03-13 16:05:22,171 : Evaluating...
2019-03-13 16:05:27,640 : 
Dev acc : 85.1 Test acc : 85.3 for SUBJNUMBER classification

2019-03-13 16:05:27,641 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 16:05:28,059 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 16:05:28,125 : loading BERT model bert-large-uncased
2019-03-13 16:05:28,126 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:05:28,247 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:05:28,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkehf8j26
2019-03-13 16:05:35,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:05:41,136 : Computing embeddings for train/dev/test
2019-03-13 16:08:46,590 : Computed embeddings
2019-03-13 16:08:46,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:09:11,105 : [('reg:1e-05', 75.83), ('reg:0.0001', 75.81), ('reg:0.001', 75.93), ('reg:0.01', 75.76)]
2019-03-13 16:09:11,105 : Validation : best param found is reg = 0.001 with score             75.93
2019-03-13 16:09:11,105 : Evaluating...
2019-03-13 16:09:17,571 : 
Dev acc : 75.9 Test acc : 77.5 for OBJNUMBER classification

2019-03-13 16:09:17,573 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 16:09:18,138 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 16:09:18,207 : loading BERT model bert-large-uncased
2019-03-13 16:09:18,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:09:18,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:09:18,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9hfgjdz8
2019-03-13 16:09:25,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:09:31,121 : Computing embeddings for train/dev/test
2019-03-13 16:13:06,153 : Computed embeddings
2019-03-13 16:13:06,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:13:28,875 : [('reg:1e-05', 63.84), ('reg:0.0001', 63.93), ('reg:0.001', 63.88), ('reg:0.01', 63.71)]
2019-03-13 16:13:28,875 : Validation : best param found is reg = 0.0001 with score             63.93
2019-03-13 16:13:28,875 : Evaluating...
2019-03-13 16:13:34,382 : 
Dev acc : 63.9 Test acc : 63.5 for ODDMANOUT classification

2019-03-13 16:13:34,383 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 16:13:34,757 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 16:13:34,837 : loading BERT model bert-large-uncased
2019-03-13 16:13:34,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:13:34,866 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:13:34,866 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjt0kfs96
2019-03-13 16:13:42,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:13:47,670 : Computing embeddings for train/dev/test
2019-03-13 16:17:20,761 : Computed embeddings
2019-03-13 16:17:20,762 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:17:45,448 : [('reg:1e-05', 73.17), ('reg:0.0001', 73.05), ('reg:0.001', 66.01), ('reg:0.01', 62.44)]
2019-03-13 16:17:45,448 : Validation : best param found is reg = 1e-05 with score             73.17
2019-03-13 16:17:45,448 : Evaluating...
2019-03-13 16:17:51,941 : 
Dev acc : 73.2 Test acc : 73.0 for COORDINATIONINVERSION classification

2019-03-13 16:17:51,943 : total results: {'STS12': {'MSRpar': {'pearson': (0.26902361760815285, 6.690289144461785e-14), 'spearman': SpearmanrResult(correlation=0.313187565801502, pvalue=1.5687201267854108e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.04800430626951665, 0.1891100247822366), 'spearman': SpearmanrResult(correlation=0.10449371835913082, pvalue=0.004173370751493595), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41419749169873715, 1.8740726657902334e-20), 'spearman': SpearmanrResult(correlation=0.510553236034979, pvalue=7.786491200757132e-32), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.37518412818645525, 1.7593823069330957e-26), 'spearman': SpearmanrResult(correlation=0.4040414577574213, pvalue=8.06112940537613e-31), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.548177257381977, 1.1257017457378576e-32), 'spearman': SpearmanrResult(correlation=0.5165678220341561, pvalue=1.3424098311508479e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3309173602289678, 'wmean': 0.29858378810592756}, 'spearman': {'mean': 0.36976875999743786, 'wmean': 0.340008543362363}}}, 'STS13': {'FNWN': {'pearson': (0.16200637229465747, 0.02593539653995688), 'spearman': SpearmanrResult(correlation=0.1582406539358141, pvalue=0.029649737431543647), 'nsamples': 189}, 'headlines': {'pearson': (0.4079950568040804, 1.8959791261605626e-31), 'spearman': SpearmanrResult(correlation=0.4048181097952776, pvalue=6.075548585830746e-31), 'nsamples': 750}, 'OnWN': {'pearson': (0.20957917099398693, 5.481835681975859e-07), 'spearman': SpearmanrResult(correlation=0.2218656817935981, pvalue=1.0987893159512808e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.25986020003090826, 'wmean': 0.3027929412629181}, 'spearman': {'mean': 0.2616414818415633, 'wmean': 0.3053251422843571}}}, 'STS14': {'deft-forum': {'pearson': (-0.04809318191871536, 0.30869599778937884), 'spearman': SpearmanrResult(correlation=-0.04752197353682071, pvalue=0.3144839694600311), 'nsamples': 450}, 'deft-news': {'pearson': (0.4177115869696078, 4.257658172999883e-14), 'spearman': SpearmanrResult(correlation=0.4256167728364865, pvalue=1.2491386949215565e-14), 'nsamples': 300}, 'headlines': {'pearson': (0.3902643462771328, 1.0757142624795007e-28), 'spearman': SpearmanrResult(correlation=0.3718668586442096, pvalue=5.211151398477745e-26), 'nsamples': 750}, 'images': {'pearson': (0.15591320413187815, 1.7948549262929662e-05), 'spearman': SpearmanrResult(correlation=0.16957372433065956, pvalue=3.0107670540339213e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.3621366387827857, 1.1719047757717019e-24), 'spearman': SpearmanrResult(correlation=0.3692573756245557, pvalue=1.213557112990564e-25), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4546187020485484, 1.5815409603452606e-39), 'spearman': SpearmanrResult(correlation=0.4161145699536278, pvalue=9.125224467387641e-33), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2887585493818729, 'wmean': 0.3002323233753918}, 'spearman': {'mean': 0.2841512213087864, 'wmean': 0.29370921071311096}}}, 'STS15': {'answers-forums': {'pearson': (0.4104081543261908, 1.142070395456566e-16), 'spearman': SpearmanrResult(correlation=0.39612687294035914, pvalue=1.5374829943706856e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.5416199274339307, 2.1463682785816953e-58), 'spearman': SpearmanrResult(correlation=0.5559396668661684, pvalue=4.646453888906624e-62), 'nsamples': 750}, 'belief': {'pearson': (0.46843406268474186, 7.514177847068693e-22), 'spearman': SpearmanrResult(correlation=0.49291214176947057, pvalue=2.3707993669712855e-24), 'nsamples': 375}, 'headlines': {'pearson': (0.4423632269558066, 2.7929181294772563e-37), 'spearman': SpearmanrResult(correlation=0.4450923189999012, pvalue=8.986444089779154e-38), 'nsamples': 750}, 'images': {'pearson': (0.21065514195451218, 5.714984813272574e-09), 'spearman': SpearmanrResult(correlation=0.20726504602582144, pvalue=1.0099831084683568e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.41469610267103646, 'wmean': 0.4085148512124289}, 'spearman': {'mean': 0.4194672093203442, 'wmean': 0.4132041348117015}}}, 'STS16': {'answer-answer': {'pearson': (0.34424197571327564, 1.7778438304427896e-08), 'spearman': SpearmanrResult(correlation=0.33571424355871327, pvalue=4.158276355819912e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5636481595783245, 2.79571209873739e-22), 'spearman': SpearmanrResult(correlation=0.5738290205437448, pvalue=3.317047160507904e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6379889115857077, 1.0982899783298258e-27), 'spearman': SpearmanrResult(correlation=0.6439362643003399, pvalue=2.489300997416528e-28), 'nsamples': 230}, 'postediting': {'pearson': (0.7508935019895024, 1.6969086145975608e-45), 'spearman': SpearmanrResult(correlation=0.7682752680300333, pvalue=8.670286577621004e-49), 'nsamples': 244}, 'question-question': {'pearson': (-0.009785246990741184, 0.8881712311580571), 'spearman': SpearmanrResult(correlation=0.05799689104855733, pvalue=0.4042104369317219), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4573974603752139, 'wmean': 0.46854654392517814}, 'spearman': {'mean': 0.4759503374962777, 'wmean': 0.4855317878557002}}}, 'MR': {'devacc': 82.61, 'acc': 82.23, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.52, 'acc': 87.13, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.23, 'acc': 85.62, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.6, 'acc': 95.19, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.12, 'acc': 87.7, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.87, 'acc': 45.84, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 78.96, 'acc': 86.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.25, 'acc': 62.09, 'f1': 66.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.0, 'acc': 71.3, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6509876874210582, 'pearson': 0.67197093875499, 'spearman': 0.6246913978584104, 'mse': 0.5602436461745274, 'yhat': array([3.38829897, 4.12273718, 1.4249472 , ..., 3.54017   , 4.334352  ,        4.07980118]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4720757078947915, 'pearson': 0.43512196031835637, 'spearman': 0.4285375421034249, 'mse': 1.9642779249158955, 'yhat': array([1.25188478, 3.42481626, 2.86113913, ..., 3.86152006, 3.64006348,        3.78827515]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.94, 'acc': 61.11, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 65.43, 'acc': 64.87, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.09, 'acc': 28.3, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.77, 'acc': 26.68, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.56, 'acc': 72.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.79, 'acc': 87.05, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.47, 'acc': 88.04, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.09, 'acc': 85.29, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.93, 'acc': 77.52, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.93, 'acc': 63.55, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.17, 'acc': 72.96, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 16:17:51,943 : STS12 p=0.2986, STS12 s=0.3400, STS13 p=0.3028, STS13 s=0.3053, STS14 p=0.3002, STS14 s=0.2937, STS15 p=0.4085, STS15 s=0.4132, STS 16 p=0.4685, STS16 s=0.4855, STS B p=0.4351, STS B s=0.4285, STS B m=1.9643, SICK-R p=0.6720, SICK-R s=0.6247, SICK-P m=0.5602
2019-03-13 16:17:51,944 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 16:17:51,944 : 0.2986,0.3400,0.3028,0.3053,0.3002,0.2937,0.4085,0.4132,0.4685,0.4855,0.4351,0.4285,1.9643,0.6720,0.6247,0.5602
2019-03-13 16:17:51,944 : MR=82.23, CR=87.13, SUBJ=95.19, MPQA=85.62, SST-B=87.70, SST-F=45.84, TREC=86.80, SICK-E=71.30, SNLI=61.11, MRPC=62.09, MRPC f=66.67
2019-03-13 16:17:51,944 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 16:17:51,944 : 82.23,87.13,95.19,85.62,87.70,45.84,86.80,71.30,61.11,62.09,66.67
2019-03-13 16:17:51,944 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 16:17:51,944 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 16:17:51,944 : na,na,na,na,na,na,na,na,na,na
2019-03-13 16:17:51,944 : SentLen=64.87, WC=28.30, TreeDepth=26.68, TopConst=72.79, BShift=87.05, Tense=88.04, SubjNum=85.29, ObjNum=77.52, SOMO=63.55, CoordInv=72.96, average=66.71
2019-03-13 16:17:51,944 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 16:17:51,944 : 64.87,28.30,26.68,72.79,87.05,88.04,85.29,77.52,63.55,72.96,66.71
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:51,944 : layer 20
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:51,944 : ********************************************************************************
2019-03-13 16:17:52,036 : ***** Transfer task : STS12 *****


2019-03-13 16:17:52,049 : loading BERT model bert-large-uncased
2019-03-13 16:17:52,049 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:17:52,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:17:52,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzd4_qg39
2019-03-13 16:17:59,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:18:08,849 : MSRpar : pearson = 0.2656, spearman = 0.3099
2019-03-13 16:18:10,487 : MSRvid : pearson = 0.0479, spearman = 0.1121
2019-03-13 16:18:11,895 : SMTeuroparl : pearson = 0.4142, spearman = 0.5028
2019-03-13 16:18:14,583 : surprise.OnWN : pearson = 0.3711, spearman = 0.4106
2019-03-13 16:18:16,004 : surprise.SMTnews : pearson = 0.5284, spearman = 0.5222
2019-03-13 16:18:16,004 : ALL (weighted average) : Pearson = 0.2942,             Spearman = 0.3422
2019-03-13 16:18:16,004 : ALL (average) : Pearson = 0.3255,             Spearman = 0.3715

2019-03-13 16:18:16,004 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 16:18:16,013 : loading BERT model bert-large-uncased
2019-03-13 16:18:16,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:18:16,030 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:18:16,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp16m9es5i
2019-03-13 16:18:23,460 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:18:30,178 : FNWN : pearson = 0.1401, spearman = 0.1423
2019-03-13 16:18:32,065 : headlines : pearson = 0.4326, spearman = 0.4282
2019-03-13 16:18:33,526 : OnWN : pearson = 0.1982, spearman = 0.2146
2019-03-13 16:18:33,526 : ALL (weighted average) : Pearson = 0.3081,             Spearman = 0.3123
2019-03-13 16:18:33,526 : ALL (average) : Pearson = 0.2569,             Spearman = 0.2617

2019-03-13 16:18:33,527 : ***** Transfer task : STS14 *****


2019-03-13 16:18:33,541 : loading BERT model bert-large-uncased
2019-03-13 16:18:33,542 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:18:33,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:18:33,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsj9bdfgh
2019-03-13 16:18:41,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:18:47,536 : deft-forum : pearson = -0.0469, spearman = -0.0480
2019-03-13 16:18:49,169 : deft-news : pearson = 0.4406, spearman = 0.4315
2019-03-13 16:18:51,334 : headlines : pearson = 0.4053, spearman = 0.3841
2019-03-13 16:18:53,403 : images : pearson = 0.1719, spearman = 0.1852
2019-03-13 16:18:55,528 : OnWN : pearson = 0.3621, spearman = 0.3671
2019-03-13 16:18:58,378 : tweet-news : pearson = 0.4693, spearman = 0.4297
2019-03-13 16:18:58,378 : ALL (weighted average) : Pearson = 0.3113,             Spearman = 0.3020
2019-03-13 16:18:58,378 : ALL (average) : Pearson = 0.3004,             Spearman = 0.2916

2019-03-13 16:18:58,378 : ***** Transfer task : STS15 *****


2019-03-13 16:18:58,410 : loading BERT model bert-large-uncased
2019-03-13 16:18:58,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:18:58,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:18:58,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptmliy2nj
2019-03-13 16:19:05,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:19:12,943 : answers-forums : pearson = 0.4382, spearman = 0.4209
2019-03-13 16:19:15,018 : answers-students : pearson = 0.5535, spearman = 0.5649
2019-03-13 16:19:17,060 : belief : pearson = 0.4940, spearman = 0.5181
2019-03-13 16:19:19,301 : headlines : pearson = 0.4594, spearman = 0.4592
2019-03-13 16:19:21,425 : images : pearson = 0.2401, spearman = 0.2379
2019-03-13 16:19:21,425 : ALL (weighted average) : Pearson = 0.4298,             Spearman = 0.4329
2019-03-13 16:19:21,425 : ALL (average) : Pearson = 0.4370,             Spearman = 0.4402

2019-03-13 16:19:21,425 : ***** Transfer task : STS16 *****


2019-03-13 16:19:21,493 : loading BERT model bert-large-uncased
2019-03-13 16:19:21,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:19:21,511 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:19:21,511 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv5rg_sjk
2019-03-13 16:19:28,971 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:19:35,129 : answer-answer : pearson = 0.3934, spearman = 0.3878
2019-03-13 16:19:35,786 : headlines : pearson = 0.5593, spearman = 0.5677
2019-03-13 16:19:36,666 : plagiarism : pearson = 0.6692, spearman = 0.6624
2019-03-13 16:19:38,156 : postediting : pearson = 0.7554, spearman = 0.7683
2019-03-13 16:19:38,760 : question-question : pearson = 0.0155, spearman = 0.0811
2019-03-13 16:19:38,760 : ALL (weighted average) : Pearson = 0.4896,             Spearman = 0.5030
2019-03-13 16:19:38,760 : ALL (average) : Pearson = 0.4786,             Spearman = 0.4934

2019-03-13 16:19:38,760 : ***** Transfer task : MR *****


2019-03-13 16:19:38,775 : loading BERT model bert-large-uncased
2019-03-13 16:19:38,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:19:38,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:19:38,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5d11jm4u
2019-03-13 16:19:46,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:19:51,443 : Generating sentence embeddings
2019-03-13 16:20:22,846 : Generated sentence embeddings
2019-03-13 16:20:22,846 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:20:33,426 : Best param found at split 1: l2reg = 1e-05                 with score 83.36
2019-03-13 16:20:42,798 : Best param found at split 2: l2reg = 0.001                 with score 83.4
2019-03-13 16:20:52,369 : Best param found at split 3: l2reg = 0.01                 with score 83.52
2019-03-13 16:21:03,130 : Best param found at split 4: l2reg = 0.0001                 with score 83.56
2019-03-13 16:21:13,312 : Best param found at split 5: l2reg = 1e-05                 with score 83.18
2019-03-13 16:21:13,915 : Dev acc : 83.4 Test acc : 81.94

2019-03-13 16:21:13,916 : ***** Transfer task : CR *****


2019-03-13 16:21:13,924 : loading BERT model bert-large-uncased
2019-03-13 16:21:13,924 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:21:13,944 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:21:13,944 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxxb3ufjx
2019-03-13 16:21:21,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:21:26,721 : Generating sentence embeddings
2019-03-13 16:21:35,027 : Generated sentence embeddings
2019-03-13 16:21:35,028 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:21:38,077 : Best param found at split 1: l2reg = 0.01                 with score 88.97
2019-03-13 16:21:41,565 : Best param found at split 2: l2reg = 0.01                 with score 88.67
2019-03-13 16:21:45,253 : Best param found at split 3: l2reg = 0.001                 with score 89.84
2019-03-13 16:21:48,917 : Best param found at split 4: l2reg = 0.001                 with score 89.24
2019-03-13 16:21:52,131 : Best param found at split 5: l2reg = 0.001                 with score 89.34
2019-03-13 16:21:52,249 : Dev acc : 89.21 Test acc : 88.87

2019-03-13 16:21:52,250 : ***** Transfer task : MPQA *****


2019-03-13 16:21:52,255 : loading BERT model bert-large-uncased
2019-03-13 16:21:52,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:21:52,304 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:21:52,305 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkoaekebf
2019-03-13 16:21:59,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:22:05,138 : Generating sentence embeddings
2019-03-13 16:22:12,697 : Generated sentence embeddings
2019-03-13 16:22:12,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:22:21,001 : Best param found at split 1: l2reg = 0.0001                 with score 82.19
2019-03-13 16:22:31,611 : Best param found at split 2: l2reg = 0.0001                 with score 85.31
2019-03-13 16:22:41,990 : Best param found at split 3: l2reg = 1e-05                 with score 82.96
2019-03-13 16:22:53,596 : Best param found at split 4: l2reg = 0.001                 with score 84.95
2019-03-13 16:23:02,938 : Best param found at split 5: l2reg = 0.001                 with score 84.98
2019-03-13 16:23:03,371 : Dev acc : 84.08 Test acc : 84.41

2019-03-13 16:23:03,372 : ***** Transfer task : SUBJ *****


2019-03-13 16:23:03,388 : loading BERT model bert-large-uncased
2019-03-13 16:23:03,388 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:23:03,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:23:03,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfnf9mk5q
2019-03-13 16:23:10,896 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:23:16,292 : Generating sentence embeddings
2019-03-13 16:23:47,104 : Generated sentence embeddings
2019-03-13 16:23:47,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:23:56,657 : Best param found at split 1: l2reg = 0.001                 with score 95.5
2019-03-13 16:24:05,271 : Best param found at split 2: l2reg = 1e-05                 with score 95.88
2019-03-13 16:24:13,460 : Best param found at split 3: l2reg = 0.001                 with score 95.26
2019-03-13 16:24:23,946 : Best param found at split 4: l2reg = 0.001                 with score 95.85
2019-03-13 16:24:34,516 : Best param found at split 5: l2reg = 0.01                 with score 95.65
2019-03-13 16:24:35,019 : Dev acc : 95.63 Test acc : 95.49

2019-03-13 16:24:35,020 : ***** Transfer task : SST Binary classification *****


2019-03-13 16:24:35,115 : loading BERT model bert-large-uncased
2019-03-13 16:24:35,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:24:35,198 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:24:35,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq2eorqwv
2019-03-13 16:24:42,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:24:48,250 : Computing embedding for train
2019-03-13 16:26:27,746 : Computed train embeddings
2019-03-13 16:26:27,746 : Computing embedding for dev
2019-03-13 16:26:29,908 : Computed dev embeddings
2019-03-13 16:26:29,908 : Computing embedding for test
2019-03-13 16:26:34,451 : Computed test embeddings
2019-03-13 16:26:34,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:26:48,908 : [('reg:1e-05', 86.58), ('reg:0.0001', 86.47), ('reg:0.001', 87.04), ('reg:0.01', 86.81)]
2019-03-13 16:26:48,908 : Validation : best param found is reg = 0.001 with score             87.04
2019-03-13 16:26:48,908 : Evaluating...
2019-03-13 16:26:52,523 : 
Dev acc : 87.04 Test acc : 87.86 for             SST Binary classification

2019-03-13 16:26:52,523 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 16:26:52,577 : loading BERT model bert-large-uncased
2019-03-13 16:26:52,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:26:52,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:26:52,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqfa3g_4s
2019-03-13 16:27:00,072 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:27:05,517 : Computing embedding for train
2019-03-13 16:27:27,312 : Computed train embeddings
2019-03-13 16:27:27,312 : Computing embedding for dev
2019-03-13 16:27:30,157 : Computed dev embeddings
2019-03-13 16:27:30,157 : Computing embedding for test
2019-03-13 16:27:35,767 : Computed test embeddings
2019-03-13 16:27:35,767 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:27:38,217 : [('reg:1e-05', 43.05), ('reg:0.0001', 43.6), ('reg:0.001', 42.14), ('reg:0.01', 40.51)]
2019-03-13 16:27:38,217 : Validation : best param found is reg = 0.0001 with score             43.6
2019-03-13 16:27:38,217 : Evaluating...
2019-03-13 16:27:38,765 : 
Dev acc : 43.6 Test acc : 46.52 for             SST Fine-Grained classification

2019-03-13 16:27:38,765 : ***** Transfer task : TREC *****


2019-03-13 16:27:38,779 : loading BERT model bert-large-uncased
2019-03-13 16:27:38,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:27:38,798 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:27:38,798 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkfqah9ga
2019-03-13 16:27:46,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:27:59,211 : Computed train embeddings
2019-03-13 16:27:59,796 : Computed test embeddings
2019-03-13 16:27:59,796 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:28:06,240 : [('reg:1e-05', 81.69), ('reg:0.0001', 81.57), ('reg:0.001', 79.36), ('reg:0.01', 72.05)]
2019-03-13 16:28:06,241 : Cross-validation : best param found is reg = 1e-05             with score 81.69
2019-03-13 16:28:06,241 : Evaluating...
2019-03-13 16:28:06,598 : 
Dev acc : 81.69 Test acc : 88.0             for TREC

2019-03-13 16:28:06,599 : ***** Transfer task : MRPC *****


2019-03-13 16:28:06,620 : loading BERT model bert-large-uncased
2019-03-13 16:28:06,620 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:28:06,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:28:06,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptee6hnxp
2019-03-13 16:28:14,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:28:19,261 : Computing embedding for train
2019-03-13 16:28:41,404 : Computed train embeddings
2019-03-13 16:28:41,404 : Computing embedding for test
2019-03-13 16:28:51,107 : Computed test embeddings
2019-03-13 16:28:51,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:28:55,875 : [('reg:1e-05', 71.59), ('reg:0.0001', 70.9), ('reg:0.001', 71.91), ('reg:0.01', 71.08)]
2019-03-13 16:28:55,875 : Cross-validation : best param found is reg = 0.001             with score 71.91
2019-03-13 16:28:55,875 : Evaluating...
2019-03-13 16:28:56,142 : Dev acc : 71.91 Test acc 71.71; Test F1 80.39 for MRPC.

2019-03-13 16:28:56,142 : ***** Transfer task : SICK-Entailment*****


2019-03-13 16:28:56,210 : loading BERT model bert-large-uncased
2019-03-13 16:28:56,210 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:28:56,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:28:56,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr06t1w17
2019-03-13 16:29:03,687 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:29:09,046 : Computing embedding for train
2019-03-13 16:29:20,281 : Computed train embeddings
2019-03-13 16:29:20,281 : Computing embedding for dev
2019-03-13 16:29:21,811 : Computed dev embeddings
2019-03-13 16:29:21,811 : Computing embedding for test
2019-03-13 16:29:33,866 : Computed test embeddings
2019-03-13 16:29:33,902 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:29:35,457 : [('reg:1e-05', 71.8), ('reg:0.0001', 73.4), ('reg:0.001', 73.0), ('reg:0.01', 74.2)]
2019-03-13 16:29:35,457 : Validation : best param found is reg = 0.01 with score             74.2
2019-03-13 16:29:35,457 : Evaluating...
2019-03-13 16:29:35,909 : 
Dev acc : 74.2 Test acc : 71.89 for                        SICK entailment

2019-03-13 16:29:35,910 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 16:29:35,937 : loading BERT model bert-large-uncased
2019-03-13 16:29:35,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:29:35,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:29:35,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4hugqbui
2019-03-13 16:29:43,457 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:29:48,797 : Computing embedding for train
2019-03-13 16:30:00,020 : Computed train embeddings
2019-03-13 16:30:00,020 : Computing embedding for dev
2019-03-13 16:30:01,555 : Computed dev embeddings
2019-03-13 16:30:01,555 : Computing embedding for test
2019-03-13 16:30:13,601 : Computed test embeddings
2019-03-13 16:30:28,897 : Dev : Pearson 0.6441991600909073
2019-03-13 16:30:28,898 : Test : Pearson 0.6668243173231732 Spearman 0.626214086690185 MSE 0.5716549031170776                        for SICK Relatedness

2019-03-13 16:30:28,898 : 

***** Transfer task : STSBenchmark*****


2019-03-13 16:30:28,967 : loading BERT model bert-large-uncased
2019-03-13 16:30:28,967 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:30:28,987 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:30:28,987 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5dpkr56l
2019-03-13 16:30:36,436 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:30:41,888 : Computing embedding for train
2019-03-13 16:31:00,397 : Computed train embeddings
2019-03-13 16:31:00,397 : Computing embedding for dev
2019-03-13 16:31:06,005 : Computed dev embeddings
2019-03-13 16:31:06,005 : Computing embedding for test
2019-03-13 16:31:10,598 : Computed test embeddings
2019-03-13 16:31:26,402 : Dev : Pearson 0.42950797406133384
2019-03-13 16:31:26,402 : Test : Pearson 0.43908772317276135 Spearman 0.4382647927163431 MSE 1.9534997727782697                        for SICK Relatedness

2019-03-13 16:31:26,402 : ***** Transfer task : SNLI Entailment*****


2019-03-13 16:31:31,180 : loading BERT model bert-large-uncased
2019-03-13 16:31:31,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:31:32,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:31:32,183 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo430420d
2019-03-13 16:31:39,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:31:45,451 : PROGRESS (encoding): 0.00%
2019-03-13 16:34:29,908 : PROGRESS (encoding): 14.56%
2019-03-13 16:37:36,105 : PROGRESS (encoding): 29.12%
2019-03-13 16:40:42,902 : PROGRESS (encoding): 43.69%
2019-03-13 16:44:02,149 : PROGRESS (encoding): 58.25%
2019-03-13 16:47:44,295 : PROGRESS (encoding): 72.81%
2019-03-13 16:51:25,110 : PROGRESS (encoding): 87.37%
2019-03-13 16:55:24,081 : PROGRESS (encoding): 0.00%
2019-03-13 16:55:54,222 : PROGRESS (encoding): 0.00%
2019-03-13 16:56:23,144 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:56:55,104 : [('reg:1e-09', 63.5)]
2019-03-13 16:56:55,105 : Validation : best param found is reg = 1e-09 with score             63.5
2019-03-13 16:56:55,105 : Evaluating...
2019-03-13 16:57:26,419 : Dev acc : 63.5 Test acc : 63.82 for SNLI

2019-03-13 16:57:26,419 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 16:57:26,628 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 16:57:27,621 : loading BERT model bert-large-uncased
2019-03-13 16:57:27,621 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:57:27,648 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:57:27,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsc1ksdga
2019-03-13 16:57:35,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:57:40,502 : Computing embeddings for train/dev/test
2019-03-13 17:01:09,844 : Computed embeddings
2019-03-13 17:01:09,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:01:41,901 : [('reg:1e-05', 60.95), ('reg:0.0001', 60.22), ('reg:0.001', 58.89), ('reg:0.01', 49.44)]
2019-03-13 17:01:41,901 : Validation : best param found is reg = 1e-05 with score             60.95
2019-03-13 17:01:41,902 : Evaluating...
2019-03-13 17:01:49,123 : 
Dev acc : 61.0 Test acc : 61.1 for LENGTH classification

2019-03-13 17:01:49,124 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 17:01:49,487 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 17:01:49,533 : loading BERT model bert-large-uncased
2019-03-13 17:01:49,533 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:01:49,561 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:01:49,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk30ag2iz
2019-03-13 17:01:57,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:02:02,322 : Computing embeddings for train/dev/test
2019-03-13 17:05:15,152 : Computed embeddings
2019-03-13 17:05:15,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:05:49,252 : [('reg:1e-05', 28.99), ('reg:0.0001', 13.84), ('reg:0.001', 1.9), ('reg:0.01', 0.67)]
2019-03-13 17:05:49,252 : Validation : best param found is reg = 1e-05 with score             28.99
2019-03-13 17:05:49,252 : Evaluating...
2019-03-13 17:06:00,343 : 
Dev acc : 29.0 Test acc : 29.0 for WORDCONTENT classification

2019-03-13 17:06:00,345 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 17:06:00,702 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 17:06:00,767 : loading BERT model bert-large-uncased
2019-03-13 17:06:00,767 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:06:00,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:06:00,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwaywwfey
2019-03-13 17:06:08,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:06:13,604 : Computing embeddings for train/dev/test
2019-03-13 17:09:14,984 : Computed embeddings
2019-03-13 17:09:14,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:09:32,386 : [('reg:1e-05', 25.23), ('reg:0.0001', 25.18), ('reg:0.001', 25.71), ('reg:0.01', 24.96)]
2019-03-13 17:09:32,387 : Validation : best param found is reg = 0.001 with score             25.71
2019-03-13 17:09:32,387 : Evaluating...
2019-03-13 17:09:37,850 : 
Dev acc : 25.7 Test acc : 25.2 for DEPTH classification

2019-03-13 17:09:37,851 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 17:09:38,231 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 17:09:38,293 : loading BERT model bert-large-uncased
2019-03-13 17:09:38,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:09:38,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:09:38,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_m_xug4p
2019-03-13 17:09:45,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:09:51,244 : Computing embeddings for train/dev/test
2019-03-13 17:12:39,401 : Computed embeddings
2019-03-13 17:12:39,401 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:13:08,170 : [('reg:1e-05', 72.51), ('reg:0.0001', 72.49), ('reg:0.001', 69.35), ('reg:0.01', 61.27)]
2019-03-13 17:13:08,170 : Validation : best param found is reg = 1e-05 with score             72.51
2019-03-13 17:13:08,170 : Evaluating...
2019-03-13 17:13:14,876 : 
Dev acc : 72.5 Test acc : 72.6 for TOPCONSTITUENTS classification

2019-03-13 17:13:14,877 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 17:13:15,213 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 17:13:15,279 : loading BERT model bert-large-uncased
2019-03-13 17:13:15,279 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:13:15,398 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:13:15,398 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj7dgunf5
2019-03-13 17:13:22,855 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:13:28,267 : Computing embeddings for train/dev/test
2019-03-13 17:16:30,973 : Computed embeddings
2019-03-13 17:16:30,973 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:17:04,404 : [('reg:1e-05', 86.93), ('reg:0.0001', 86.92), ('reg:0.001', 86.21), ('reg:0.01', 86.28)]
2019-03-13 17:17:04,405 : Validation : best param found is reg = 1e-05 with score             86.93
2019-03-13 17:17:04,405 : Evaluating...
2019-03-13 17:17:12,981 : 
Dev acc : 86.9 Test acc : 86.1 for BIGRAMSHIFT classification

2019-03-13 17:17:12,982 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 17:17:13,388 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 17:17:13,452 : loading BERT model bert-large-uncased
2019-03-13 17:17:13,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:17:13,481 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:17:13,481 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0vuh48sd
2019-03-13 17:17:21,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:17:26,457 : Computing embeddings for train/dev/test
2019-03-13 17:20:25,060 : Computed embeddings
2019-03-13 17:20:25,060 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:20:50,031 : [('reg:1e-05', 89.5), ('reg:0.0001', 89.47), ('reg:0.001', 89.61), ('reg:0.01', 89.67)]
2019-03-13 17:20:50,031 : Validation : best param found is reg = 0.01 with score             89.67
2019-03-13 17:20:50,031 : Evaluating...
2019-03-13 17:20:54,625 : 
Dev acc : 89.7 Test acc : 87.8 for TENSE classification

2019-03-13 17:20:54,626 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 17:20:55,055 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 17:20:55,121 : loading BERT model bert-large-uncased
2019-03-13 17:20:55,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:20:55,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:20:55,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcjoq8wtr
2019-03-13 17:21:02,661 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:21:07,956 : Computing embeddings for train/dev/test
2019-03-13 17:24:17,497 : Computed embeddings
2019-03-13 17:24:17,498 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:24:43,893 : [('reg:1e-05', 85.16), ('reg:0.0001', 85.19), ('reg:0.001', 85.35), ('reg:0.01', 84.96)]
2019-03-13 17:24:43,893 : Validation : best param found is reg = 0.001 with score             85.35
2019-03-13 17:24:43,893 : Evaluating...
2019-03-13 17:24:49,938 : 
Dev acc : 85.3 Test acc : 86.0 for SUBJNUMBER classification

2019-03-13 17:24:49,939 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 17:24:50,364 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 17:24:50,434 : loading BERT model bert-large-uncased
2019-03-13 17:24:50,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:24:50,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:24:50,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzk2gbnho
2019-03-13 17:24:58,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:25:03,422 : Computing embeddings for train/dev/test
2019-03-13 17:28:09,316 : Computed embeddings
2019-03-13 17:28:09,316 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:28:42,683 : [('reg:1e-05', 75.95), ('reg:0.0001', 75.85), ('reg:0.001', 75.75), ('reg:0.01', 73.38)]
2019-03-13 17:28:42,684 : Validation : best param found is reg = 1e-05 with score             75.95
2019-03-13 17:28:42,684 : Evaluating...
2019-03-13 17:28:51,320 : 
Dev acc : 76.0 Test acc : 77.2 for OBJNUMBER classification

2019-03-13 17:28:51,321 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 17:28:51,702 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 17:28:51,770 : loading BERT model bert-large-uncased
2019-03-13 17:28:51,771 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:28:51,891 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:28:51,891 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqwaa3ks7
2019-03-13 17:28:59,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:29:04,719 : Computing embeddings for train/dev/test
2019-03-13 17:32:39,886 : Computed embeddings
2019-03-13 17:32:39,886 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:33:10,948 : [('reg:1e-05', 64.59), ('reg:0.0001', 64.47), ('reg:0.001', 64.41), ('reg:0.01', 59.47)]
2019-03-13 17:33:10,948 : Validation : best param found is reg = 1e-05 with score             64.59
2019-03-13 17:33:10,948 : Evaluating...
2019-03-13 17:33:18,372 : 
Dev acc : 64.6 Test acc : 65.3 for ODDMANOUT classification

2019-03-13 17:33:18,373 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 17:33:18,959 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 17:33:19,035 : loading BERT model bert-large-uncased
2019-03-13 17:33:19,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:33:19,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:33:19,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd1hy37it
2019-03-13 17:33:26,538 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:33:31,866 : Computing embeddings for train/dev/test
2019-03-13 17:37:05,042 : Computed embeddings
2019-03-13 17:37:05,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:37:31,010 : [('reg:1e-05', 71.59), ('reg:0.0001', 73.48), ('reg:0.001', 67.25), ('reg:0.01', 71.01)]
2019-03-13 17:37:31,010 : Validation : best param found is reg = 0.0001 with score             73.48
2019-03-13 17:37:31,011 : Evaluating...
2019-03-13 17:37:37,710 : 
Dev acc : 73.5 Test acc : 73.0 for COORDINATIONINVERSION classification

2019-03-13 17:37:37,712 : total results: {'STS12': {'MSRpar': {'pearson': (0.2656109700806763, 1.412392622500835e-13), 'spearman': SpearmanrResult(correlation=0.30994337799341914, pvalue=3.66105153595337e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.0478857729118147, 0.1902059547142061), 'spearman': SpearmanrResult(correlation=0.11207418488877942, pvalue=0.0021131734513313316), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4142301064095101, 1.8600151911038724e-20), 'spearman': SpearmanrResult(correlation=0.5027590060110533, pvalue=8.961640113201037e-31), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.371128119755765, 6.625330180875383e-26), 'spearman': SpearmanrResult(correlation=0.41059417341974774, pvalue=7.244290440022404e-32), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5284234909138394, 4.456593683917012e-30), 'spearman': SpearmanrResult(correlation=0.5221822680350006, pvalue=2.721343914999804e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3254556920143211, 'wmean': 0.2942217627985132}, 'spearman': {'mean': 0.37151060206960007, 'wmean': 0.3422058915481012}}}, 'STS13': {'FNWN': {'pearson': (0.14006240406755444, 0.05457407168461811), 'spearman': SpearmanrResult(correlation=0.14226994237899196, pvalue=0.0508363953307517), 'nsamples': 189}, 'headlines': {'pearson': (0.4326280334525035, 1.4680582051165067e-35), 'spearman': SpearmanrResult(correlation=0.4282236857251125, pvalue=8.452913003539624e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.1981583650419044, 2.245394845983004e-06), 'spearman': SpearmanrResult(correlation=0.21461167570989576, pvalue=2.8706593625862845e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.25694960085398744, 'wmean': 0.30807310816443584}, 'spearman': {'mean': 0.26170176793800004, 'wmean': 0.31230262231781025}}}, 'STS14': {'deft-forum': {'pearson': (-0.046942814184582744, 0.32042464781165014), 'spearman': SpearmanrResult(correlation=-0.04802963274571655, pvalue=0.30933643938049105), 'nsamples': 450}, 'deft-news': {'pearson': (0.440560541496728, 1.1247386592566967e-15), 'spearman': SpearmanrResult(correlation=0.4314780101575558, pvalue=4.9278220591222496e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.4052834427445235, 5.12681735998806e-31), 'spearman': SpearmanrResult(correlation=0.3841347630422499, pvalue=8.819383885092432e-28), 'nsamples': 750}, 'images': {'pearson': (0.17186821275882316, 2.1989386558559805e-06), 'spearman': SpearmanrResult(correlation=0.18520897828272367, pvalue=3.256696938336901e-07), 'nsamples': 750}, 'OnWN': {'pearson': (0.3620885392717338, 1.1897641220697782e-24), 'spearman': SpearmanrResult(correlation=0.367109858846134, pvalue=2.419243154196071e-25), 'nsamples': 750}, 'tweet-news': {'pearson': (0.46927493150376, 2.450014882421971e-42), 'spearman': SpearmanrResult(correlation=0.42971021765916356, pvalue=4.695249786686236e-35), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3003554755984976, 'wmean': 0.3113147308733564}, 'spearman': {'mean': 0.2916020325403517, 'wmean': 0.30198744844917275}}}, 'STS15': {'answers-forums': {'pearson': (0.43818363195666, 5.024835388758839e-19), 'spearman': SpearmanrResult(correlation=0.4208698536125992, pvalue=1.5687587098373686e-17), 'nsamples': 375}, 'answers-students': {'pearson': (0.5535249857158735, 1.9841120451090008e-61), 'spearman': SpearmanrResult(correlation=0.5648675807556985, pvalue=1.9530319119160605e-64), 'nsamples': 750}, 'belief': {'pearson': (0.4940250723664606, 1.804400373883705e-24), 'spearman': SpearmanrResult(correlation=0.5181382557492326, pvalue=3.783091419395584e-27), 'nsamples': 375}, 'headlines': {'pearson': (0.45935974319278045, 2.0184130979302625e-40), 'spearman': SpearmanrResult(correlation=0.45917548291206783, pvalue=2.187868338082468e-40), 'nsamples': 750}, 'images': {'pearson': (0.24008432715117986, 2.707046708971598e-11), 'spearman': SpearmanrResult(correlation=0.23792960456117238, pvalue=4.1084544230999364e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.43703555207659084, 'wmean': 0.4297683520553486}, 'spearman': {'mean': 0.44019615551815405, 'wmean': 0.43286918072746366}}}, 'STS16': {'answer-answer': {'pearson': (0.3934384705902384, 7.840484784222813e-11), 'spearman': SpearmanrResult(correlation=0.3877730303613791, pvalue=1.5351330111925383e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5592815326074758, 6.8212567065134045e-22), 'spearman': SpearmanrResult(correlation=0.5676546667044933, pvalue=1.2189843504322904e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6691981072728314, 3.09950780885986e-31), 'spearman': SpearmanrResult(correlation=0.6624170195629504, pvalue=1.989397530467989e-30), 'nsamples': 230}, 'postediting': {'pearson': (0.7553843200101303, 2.5452107351595334e-46), 'spearman': SpearmanrResult(correlation=0.7682551600379934, pvalue=8.749976867460273e-49), 'nsamples': 244}, 'question-question': {'pearson': (0.015528802778185693, 0.8234075906174129), 'spearman': SpearmanrResult(correlation=0.08106229973722848, pvalue=0.24329238168836032), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.47856624665177233, 'wmean': 0.48960314644607583}, 'spearman': {'mean': 0.49343243528080893, 'wmean': 0.5030291365219551}}}, 'MR': {'devacc': 83.4, 'acc': 81.94, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.21, 'acc': 88.87, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.08, 'acc': 84.41, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.63, 'acc': 95.49, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.04, 'acc': 87.86, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.6, 'acc': 46.52, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.69, 'acc': 88.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.91, 'acc': 71.71, 'f1': 80.39, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.2, 'acc': 71.89, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6441991600909073, 'pearson': 0.6668243173231732, 'spearman': 0.626214086690185, 'mse': 0.5716549031170776, 'yhat': array([3.24812593, 4.67066663, 2.72971876, ..., 2.97152785, 4.43357619,        3.92274759]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.42950797406133384, 'pearson': 0.43908772317276135, 'spearman': 0.4382647927163431, 'mse': 1.9534997727782697, 'yhat': array([1.2556441 , 1.966066  , 3.02361379, ..., 3.77330351, 3.26712834,        3.54476322]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.5, 'acc': 63.82, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 60.95, 'acc': 61.09, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.99, 'acc': 28.96, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.71, 'acc': 25.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.51, 'acc': 72.58, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.93, 'acc': 86.13, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.67, 'acc': 87.83, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.35, 'acc': 85.95, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.95, 'acc': 77.17, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.59, 'acc': 65.33, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.48, 'acc': 72.98, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 17:37:37,712 : STS12 p=0.2942, STS12 s=0.3422, STS13 p=0.3081, STS13 s=0.3123, STS14 p=0.3113, STS14 s=0.3020, STS15 p=0.4298, STS15 s=0.4329, STS 16 p=0.4896, STS16 s=0.5030, STS B p=0.4391, STS B s=0.4383, STS B m=1.9535, SICK-R p=0.6668, SICK-R s=0.6262, SICK-P m=0.5717
2019-03-13 17:37:37,713 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 17:37:37,713 : 0.2942,0.3422,0.3081,0.3123,0.3113,0.3020,0.4298,0.4329,0.4896,0.5030,0.4391,0.4383,1.9535,0.6668,0.6262,0.5717
2019-03-13 17:37:37,713 : MR=81.94, CR=88.87, SUBJ=95.49, MPQA=84.41, SST-B=87.86, SST-F=46.52, TREC=88.00, SICK-E=71.89, SNLI=63.82, MRPC=71.71, MRPC f=80.39
2019-03-13 17:37:37,713 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 17:37:37,713 : 81.94,88.87,95.49,84.41,87.86,46.52,88.00,71.89,63.82,71.71,80.39
2019-03-13 17:37:37,713 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 17:37:37,713 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 17:37:37,713 : na,na,na,na,na,na,na,na,na,na
2019-03-13 17:37:37,713 : SentLen=61.09, WC=28.96, TreeDepth=25.25, TopConst=72.58, BShift=86.13, Tense=87.83, SubjNum=85.95, ObjNum=77.17, SOMO=65.33, CoordInv=72.98, average=66.33
2019-03-13 17:37:37,713 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 17:37:37,713 : 61.09,28.96,25.25,72.58,86.13,87.83,85.95,77.17,65.33,72.98,66.33
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,713 : layer 21
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,713 : ********************************************************************************
2019-03-13 17:37:37,809 : ***** Transfer task : STS12 *****


2019-03-13 17:37:37,822 : loading BERT model bert-large-uncased
2019-03-13 17:37:37,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:37:37,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:37:37,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvbmzjno7
2019-03-13 17:37:45,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:37:54,627 : MSRpar : pearson = 0.2641, spearman = 0.3091
2019-03-13 17:37:56,262 : MSRvid : pearson = 0.0399, spearman = 0.1077
2019-03-13 17:37:57,672 : SMTeuroparl : pearson = 0.4092, spearman = 0.4972
2019-03-13 17:38:00,364 : surprise.OnWN : pearson = 0.3717, spearman = 0.4079
2019-03-13 17:38:01,788 : surprise.SMTnews : pearson = 0.5341, spearman = 0.5476
2019-03-13 17:38:01,788 : ALL (weighted average) : Pearson = 0.2921,             Spearman = 0.3427
2019-03-13 17:38:01,788 : ALL (average) : Pearson = 0.3238,             Spearman = 0.3739

2019-03-13 17:38:01,788 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 17:38:01,798 : loading BERT model bert-large-uncased
2019-03-13 17:38:01,798 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:38:01,816 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:38:01,816 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5bn_dszn
2019-03-13 17:38:09,265 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:38:15,994 : FNWN : pearson = 0.1379, spearman = 0.1442
2019-03-13 17:38:17,887 : headlines : pearson = 0.4373, spearman = 0.4345
2019-03-13 17:38:19,354 : OnWN : pearson = 0.1894, spearman = 0.2022
2019-03-13 17:38:19,354 : ALL (weighted average) : Pearson = 0.3068,             Spearman = 0.3110
2019-03-13 17:38:19,354 : ALL (average) : Pearson = 0.2548,             Spearman = 0.2603

2019-03-13 17:38:19,354 : ***** Transfer task : STS14 *****


2019-03-13 17:38:19,370 : loading BERT model bert-large-uncased
2019-03-13 17:38:19,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:38:19,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:38:19,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpolajg6j5
2019-03-13 17:38:26,870 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:38:33,686 : deft-forum : pearson = -0.0412, spearman = -0.0405
2019-03-13 17:38:35,321 : deft-news : pearson = 0.4736, spearman = 0.4451
2019-03-13 17:38:37,486 : headlines : pearson = 0.4118, spearman = 0.3936
2019-03-13 17:38:39,557 : images : pearson = 0.1730, spearman = 0.1879
2019-03-13 17:38:41,681 : OnWN : pearson = 0.3548, spearman = 0.3602
2019-03-13 17:38:44,533 : tweet-news : pearson = 0.4665, spearman = 0.4351
2019-03-13 17:38:44,533 : ALL (weighted average) : Pearson = 0.3142,             Spearman = 0.3061
2019-03-13 17:38:44,533 : ALL (average) : Pearson = 0.3064,             Spearman = 0.2969

2019-03-13 17:38:44,533 : ***** Transfer task : STS15 *****


2019-03-13 17:38:44,567 : loading BERT model bert-large-uncased
2019-03-13 17:38:44,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:38:44,585 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:38:44,585 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3qafem99
2019-03-13 17:38:52,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:38:59,260 : answers-forums : pearson = 0.4536, spearman = 0.4371
2019-03-13 17:39:01,341 : answers-students : pearson = 0.5462, spearman = 0.5556
2019-03-13 17:39:03,386 : belief : pearson = 0.5028, spearman = 0.5330
2019-03-13 17:39:05,629 : headlines : pearson = 0.4638, spearman = 0.4639
2019-03-13 17:39:07,756 : images : pearson = 0.2297, spearman = 0.2325
2019-03-13 17:39:07,756 : ALL (weighted average) : Pearson = 0.4295,             Spearman = 0.4343
2019-03-13 17:39:07,756 : ALL (average) : Pearson = 0.4392,             Spearman = 0.4444

2019-03-13 17:39:07,756 : ***** Transfer task : STS16 *****


2019-03-13 17:39:07,825 : loading BERT model bert-large-uncased
2019-03-13 17:39:07,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:39:07,842 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:39:07,842 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0vvh7yzg
2019-03-13 17:39:15,287 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:39:21,392 : answer-answer : pearson = 0.3949, spearman = 0.3895
2019-03-13 17:39:22,048 : headlines : pearson = 0.5557, spearman = 0.5629
2019-03-13 17:39:22,930 : plagiarism : pearson = 0.6888, spearman = 0.6866
2019-03-13 17:39:24,422 : postediting : pearson = 0.7378, spearman = 0.7512
2019-03-13 17:39:25,028 : question-question : pearson = 0.0343, spearman = 0.1093
2019-03-13 17:39:25,028 : ALL (weighted average) : Pearson = 0.4927,             Spearman = 0.5086
2019-03-13 17:39:25,028 : ALL (average) : Pearson = 0.4823,             Spearman = 0.4999

2019-03-13 17:39:25,028 : ***** Transfer task : MR *****


2019-03-13 17:39:25,047 : loading BERT model bert-large-uncased
2019-03-13 17:39:25,047 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:39:25,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:39:25,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoup55_3k
2019-03-13 17:39:32,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:39:37,935 : Generating sentence embeddings
2019-03-13 17:40:09,290 : Generated sentence embeddings
2019-03-13 17:40:09,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:40:19,446 : Best param found at split 1: l2reg = 0.0001                 with score 83.22
2019-03-13 17:40:29,214 : Best param found at split 2: l2reg = 0.01                 with score 83.42
2019-03-13 17:40:38,934 : Best param found at split 3: l2reg = 0.01                 with score 83.25
2019-03-13 17:40:49,420 : Best param found at split 4: l2reg = 0.0001                 with score 83.24
2019-03-13 17:40:59,806 : Best param found at split 5: l2reg = 0.001                 with score 82.93
2019-03-13 17:41:00,250 : Dev acc : 83.21 Test acc : 82.54

2019-03-13 17:41:00,251 : ***** Transfer task : CR *****


2019-03-13 17:41:00,259 : loading BERT model bert-large-uncased
2019-03-13 17:41:00,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:41:00,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:41:00,279 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjpot52ts
2019-03-13 17:41:07,774 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:41:13,165 : Generating sentence embeddings
2019-03-13 17:41:21,484 : Generated sentence embeddings
2019-03-13 17:41:21,484 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:41:24,957 : Best param found at split 1: l2reg = 0.0001                 with score 89.23
2019-03-13 17:41:29,026 : Best param found at split 2: l2reg = 0.0001                 with score 89.1
2019-03-13 17:41:33,248 : Best param found at split 3: l2reg = 0.001                 with score 89.74
2019-03-13 17:41:36,978 : Best param found at split 4: l2reg = 0.01                 with score 89.71
2019-03-13 17:41:40,724 : Best param found at split 5: l2reg = 0.01                 with score 89.8
2019-03-13 17:41:40,947 : Dev acc : 89.52 Test acc : 88.42

2019-03-13 17:41:40,947 : ***** Transfer task : MPQA *****


2019-03-13 17:41:40,953 : loading BERT model bert-large-uncased
2019-03-13 17:41:40,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:41:40,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:41:40,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp31vt6vtj
2019-03-13 17:41:48,492 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:41:53,970 : Generating sentence embeddings
2019-03-13 17:42:01,534 : Generated sentence embeddings
2019-03-13 17:42:01,534 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:42:10,928 : Best param found at split 1: l2reg = 0.0001                 with score 81.02
2019-03-13 17:42:21,470 : Best param found at split 2: l2reg = 1e-05                 with score 83.98
2019-03-13 17:42:30,963 : Best param found at split 3: l2reg = 0.001                 with score 82.58
2019-03-13 17:42:40,290 : Best param found at split 4: l2reg = 1e-05                 with score 83.94
2019-03-13 17:42:49,240 : Best param found at split 5: l2reg = 0.001                 with score 85.38
2019-03-13 17:42:49,933 : Dev acc : 83.38 Test acc : 85.44

2019-03-13 17:42:49,934 : ***** Transfer task : SUBJ *****


2019-03-13 17:42:49,949 : loading BERT model bert-large-uncased
2019-03-13 17:42:49,950 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:42:49,969 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:42:49,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo152e2io
2019-03-13 17:42:57,434 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:43:02,845 : Generating sentence embeddings
2019-03-13 17:43:33,647 : Generated sentence embeddings
2019-03-13 17:43:33,648 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:43:42,505 : Best param found at split 1: l2reg = 0.001                 with score 95.93
2019-03-13 17:43:51,799 : Best param found at split 2: l2reg = 0.01                 with score 96.02
2019-03-13 17:44:00,954 : Best param found at split 3: l2reg = 0.01                 with score 95.71
2019-03-13 17:44:10,148 : Best param found at split 4: l2reg = 1e-05                 with score 96.15
2019-03-13 17:44:20,116 : Best param found at split 5: l2reg = 0.001                 with score 95.91
2019-03-13 17:44:20,693 : Dev acc : 95.94 Test acc : 95.02

2019-03-13 17:44:20,694 : ***** Transfer task : SST Binary classification *****


2019-03-13 17:44:20,824 : loading BERT model bert-large-uncased
2019-03-13 17:44:20,824 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:44:20,848 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:44:20,848 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfy1nzz_3
2019-03-13 17:44:28,266 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:44:33,670 : Computing embedding for train
2019-03-13 17:46:13,437 : Computed train embeddings
2019-03-13 17:46:13,437 : Computing embedding for dev
2019-03-13 17:46:15,604 : Computed dev embeddings
2019-03-13 17:46:15,604 : Computing embedding for test
2019-03-13 17:46:20,156 : Computed test embeddings
2019-03-13 17:46:20,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:46:34,214 : [('reg:1e-05', 87.27), ('reg:0.0001', 87.39), ('reg:0.001', 87.39), ('reg:0.01', 88.19)]
2019-03-13 17:46:34,214 : Validation : best param found is reg = 0.01 with score             88.19
2019-03-13 17:46:34,214 : Evaluating...
2019-03-13 17:46:37,733 : 
Dev acc : 88.19 Test acc : 88.63 for             SST Binary classification

2019-03-13 17:46:37,734 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 17:46:37,784 : loading BERT model bert-large-uncased
2019-03-13 17:46:37,785 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:46:37,807 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:46:37,807 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp512p2s7v
2019-03-13 17:46:45,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:46:50,706 : Computing embedding for train
2019-03-13 17:47:12,531 : Computed train embeddings
2019-03-13 17:47:12,531 : Computing embedding for dev
2019-03-13 17:47:15,377 : Computed dev embeddings
2019-03-13 17:47:15,377 : Computing embedding for test
2019-03-13 17:47:20,981 : Computed test embeddings
2019-03-13 17:47:20,982 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:47:23,307 : [('reg:1e-05', 43.42), ('reg:0.0001', 43.51), ('reg:0.001', 42.96), ('reg:0.01', 44.41)]
2019-03-13 17:47:23,308 : Validation : best param found is reg = 0.01 with score             44.41
2019-03-13 17:47:23,308 : Evaluating...
2019-03-13 17:47:23,934 : 
Dev acc : 44.41 Test acc : 45.88 for             SST Fine-Grained classification

2019-03-13 17:47:23,934 : ***** Transfer task : TREC *****


2019-03-13 17:47:23,948 : loading BERT model bert-large-uncased
2019-03-13 17:47:23,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:47:23,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:47:23,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8tw33tvo
2019-03-13 17:47:31,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:47:44,352 : Computed train embeddings
2019-03-13 17:47:44,938 : Computed test embeddings
2019-03-13 17:47:44,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 17:47:51,384 : [('reg:1e-05', 78.74), ('reg:0.0001', 82.15), ('reg:0.001', 81.07), ('reg:0.01', 78.35)]
2019-03-13 17:47:51,384 : Cross-validation : best param found is reg = 0.0001             with score 82.15
2019-03-13 17:47:51,384 : Evaluating...
2019-03-13 17:47:51,825 : 
Dev acc : 82.15 Test acc : 88.4             for TREC

2019-03-13 17:47:51,826 : ***** Transfer task : MRPC *****


2019-03-13 17:47:51,849 : loading BERT model bert-large-uncased
2019-03-13 17:47:51,849 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:47:51,869 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:47:51,869 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ivt_anl
2019-03-13 17:47:59,365 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:48:04,821 : Computing embedding for train
2019-03-13 17:48:26,977 : Computed train embeddings
2019-03-13 17:48:26,977 : Computing embedding for test
2019-03-13 17:48:36,671 : Computed test embeddings
2019-03-13 17:48:36,692 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 17:48:41,481 : [('reg:1e-05', 70.02), ('reg:0.0001', 70.12), ('reg:0.001', 70.09), ('reg:0.01', 70.41)]
2019-03-13 17:48:41,481 : Cross-validation : best param found is reg = 0.01             with score 70.41
2019-03-13 17:48:41,481 : Evaluating...
2019-03-13 17:48:41,747 : Dev acc : 70.41 Test acc 71.54; Test F1 81.66 for MRPC.

2019-03-13 17:48:41,748 : ***** Transfer task : SICK-Entailment*****


2019-03-13 17:48:41,808 : loading BERT model bert-large-uncased
2019-03-13 17:48:41,808 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:48:41,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:48:41,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptw7bqyi1
2019-03-13 17:48:49,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:48:54,616 : Computing embedding for train
2019-03-13 17:49:05,864 : Computed train embeddings
2019-03-13 17:49:05,864 : Computing embedding for dev
2019-03-13 17:49:07,397 : Computed dev embeddings
2019-03-13 17:49:07,397 : Computing embedding for test
2019-03-13 17:49:19,437 : Computed test embeddings
2019-03-13 17:49:19,473 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:49:21,220 : [('reg:1e-05', 70.2), ('reg:0.0001', 69.8), ('reg:0.001', 68.4), ('reg:0.01', 71.0)]
2019-03-13 17:49:21,220 : Validation : best param found is reg = 0.01 with score             71.0
2019-03-13 17:49:21,221 : Evaluating...
2019-03-13 17:49:21,566 : 
Dev acc : 71.0 Test acc : 69.21 for                        SICK entailment

2019-03-13 17:49:21,566 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 17:49:21,593 : loading BERT model bert-large-uncased
2019-03-13 17:49:21,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:49:21,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:49:21,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7i9wmilg
2019-03-13 17:49:29,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:49:34,622 : Computing embedding for train
2019-03-13 17:49:45,889 : Computed train embeddings
2019-03-13 17:49:45,889 : Computing embedding for dev
2019-03-13 17:49:47,422 : Computed dev embeddings
2019-03-13 17:49:47,422 : Computing embedding for test
2019-03-13 17:49:59,472 : Computed test embeddings
2019-03-13 17:50:14,902 : Dev : Pearson 0.6087861205418961
2019-03-13 17:50:14,903 : Test : Pearson 0.6432922351826651 Spearman 0.6021562911787861 MSE 0.6053150165596156                        for SICK Relatedness

2019-03-13 17:50:14,904 : 

***** Transfer task : STSBenchmark*****


2019-03-13 17:50:14,945 : loading BERT model bert-large-uncased
2019-03-13 17:50:14,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:50:14,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:50:14,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7ygwcfs3
2019-03-13 17:50:22,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:50:27,857 : Computing embedding for train
2019-03-13 17:50:46,363 : Computed train embeddings
2019-03-13 17:50:46,363 : Computing embedding for dev
2019-03-13 17:50:51,978 : Computed dev embeddings
2019-03-13 17:50:51,978 : Computing embedding for test
2019-03-13 17:50:56,550 : Computed test embeddings
2019-03-13 17:51:11,594 : Dev : Pearson 0.46720998254961627
2019-03-13 17:51:11,594 : Test : Pearson 0.43961336474215346 Spearman 0.44137381588097896 MSE 1.9854045977404327                        for SICK Relatedness

2019-03-13 17:51:11,595 : ***** Transfer task : SNLI Entailment*****


2019-03-13 17:51:16,635 : loading BERT model bert-large-uncased
2019-03-13 17:51:16,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:51:16,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:51:16,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp885fugbw
2019-03-13 17:51:24,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:51:29,843 : PROGRESS (encoding): 0.00%
2019-03-13 17:54:14,171 : PROGRESS (encoding): 14.56%
2019-03-13 17:57:20,496 : PROGRESS (encoding): 29.12%
2019-03-13 18:00:27,525 : PROGRESS (encoding): 43.69%
2019-03-13 18:03:47,059 : PROGRESS (encoding): 58.25%
2019-03-13 18:07:29,327 : PROGRESS (encoding): 72.81%
2019-03-13 18:11:10,374 : PROGRESS (encoding): 87.37%
2019-03-13 18:15:09,651 : PROGRESS (encoding): 0.00%
2019-03-13 18:15:39,764 : PROGRESS (encoding): 0.00%
2019-03-13 18:16:08,706 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:16:32,274 : [('reg:1e-09', 62.72)]
2019-03-13 18:16:32,274 : Validation : best param found is reg = 1e-09 with score             62.72
2019-03-13 18:16:32,274 : Evaluating...
2019-03-13 18:16:57,623 : Dev acc : 62.72 Test acc : 62.47 for SNLI

2019-03-13 18:16:57,623 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 18:16:57,832 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 18:16:58,929 : loading BERT model bert-large-uncased
2019-03-13 18:16:58,929 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:16:58,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:16:58,956 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoycxyud8
2019-03-13 18:17:06,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:17:11,806 : Computing embeddings for train/dev/test
2019-03-13 18:20:41,188 : Computed embeddings
2019-03-13 18:20:41,188 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:21:17,743 : [('reg:1e-05', 60.45), ('reg:0.0001', 59.66), ('reg:0.001', 55.77), ('reg:0.01', 51.73)]
2019-03-13 18:21:17,743 : Validation : best param found is reg = 1e-05 with score             60.45
2019-03-13 18:21:17,743 : Evaluating...
2019-03-13 18:21:26,034 : 
Dev acc : 60.5 Test acc : 59.2 for LENGTH classification

2019-03-13 18:21:26,035 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 18:21:26,286 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 18:21:26,332 : loading BERT model bert-large-uncased
2019-03-13 18:21:26,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:21:26,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:21:26,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg24wvdo0
2019-03-13 18:21:33,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:21:38,970 : Computing embeddings for train/dev/test
2019-03-13 18:24:51,754 : Computed embeddings
2019-03-13 18:24:51,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:25:29,693 : [('reg:1e-05', 28.32), ('reg:0.0001', 14.17), ('reg:0.001', 2.34), ('reg:0.01', 0.6)]
2019-03-13 18:25:29,693 : Validation : best param found is reg = 1e-05 with score             28.32
2019-03-13 18:25:29,693 : Evaluating...
2019-03-13 18:25:40,031 : 
Dev acc : 28.3 Test acc : 27.9 for WORDCONTENT classification

2019-03-13 18:25:40,032 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 18:25:40,572 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 18:25:40,640 : loading BERT model bert-large-uncased
2019-03-13 18:25:40,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:25:40,664 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:25:40,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpydhy_bge
2019-03-13 18:25:48,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:25:53,353 : Computing embeddings for train/dev/test
2019-03-13 18:28:54,617 : Computed embeddings
2019-03-13 18:28:54,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:29:14,459 : [('reg:1e-05', 25.84), ('reg:0.0001', 24.88), ('reg:0.001', 26.32), ('reg:0.01', 24.75)]
2019-03-13 18:29:14,459 : Validation : best param found is reg = 0.001 with score             26.32
2019-03-13 18:29:14,460 : Evaluating...
2019-03-13 18:29:19,591 : 
Dev acc : 26.3 Test acc : 26.0 for DEPTH classification

2019-03-13 18:29:19,592 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 18:29:19,961 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 18:29:20,024 : loading BERT model bert-large-uncased
2019-03-13 18:29:20,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:29:20,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:29:20,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppqpojydd
2019-03-13 18:29:27,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:29:32,987 : Computing embeddings for train/dev/test
2019-03-13 18:32:21,273 : Computed embeddings
2019-03-13 18:32:21,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:32:52,443 : [('reg:1e-05', 68.24), ('reg:0.0001', 68.38), ('reg:0.001', 65.21), ('reg:0.01', 55.12)]
2019-03-13 18:32:52,443 : Validation : best param found is reg = 0.0001 with score             68.38
2019-03-13 18:32:52,443 : Evaluating...
2019-03-13 18:33:00,348 : 
Dev acc : 68.4 Test acc : 68.5 for TOPCONSTITUENTS classification

2019-03-13 18:33:00,349 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 18:33:00,722 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 18:33:00,787 : loading BERT model bert-large-uncased
2019-03-13 18:33:00,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:33:00,817 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:33:00,817 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqvj1emly
2019-03-13 18:33:08,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:33:13,691 : Computing embeddings for train/dev/test
2019-03-13 18:36:16,341 : Computed embeddings
2019-03-13 18:36:16,341 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:36:41,772 : [('reg:1e-05', 86.84), ('reg:0.0001', 86.85), ('reg:0.001', 87.03), ('reg:0.01', 86.47)]
2019-03-13 18:36:41,772 : Validation : best param found is reg = 0.001 with score             87.03
2019-03-13 18:36:41,773 : Evaluating...
2019-03-13 18:36:47,521 : 
Dev acc : 87.0 Test acc : 86.7 for BIGRAMSHIFT classification

2019-03-13 18:36:47,522 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 18:36:47,911 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 18:36:47,976 : loading BERT model bert-large-uncased
2019-03-13 18:36:47,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:36:48,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:36:48,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_2ix14kd
2019-03-13 18:36:55,486 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:37:00,875 : Computing embeddings for train/dev/test
2019-03-13 18:39:59,825 : Computed embeddings
2019-03-13 18:39:59,825 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:40:20,687 : [('reg:1e-05', 89.2), ('reg:0.0001', 89.28), ('reg:0.001', 89.56), ('reg:0.01', 89.37)]
2019-03-13 18:40:20,687 : Validation : best param found is reg = 0.001 with score             89.56
2019-03-13 18:40:20,687 : Evaluating...
2019-03-13 18:40:27,177 : 
Dev acc : 89.6 Test acc : 88.5 for TENSE classification

2019-03-13 18:40:27,178 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 18:40:27,578 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 18:40:27,640 : loading BERT model bert-large-uncased
2019-03-13 18:40:27,641 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:40:27,753 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:40:27,753 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz39c4ttf
2019-03-13 18:40:35,214 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:40:40,544 : Computing embeddings for train/dev/test
2019-03-13 18:43:49,822 : Computed embeddings
2019-03-13 18:43:49,822 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:44:15,758 : [('reg:1e-05', 85.29), ('reg:0.0001', 85.15), ('reg:0.001', 85.41), ('reg:0.01', 84.96)]
2019-03-13 18:44:15,758 : Validation : best param found is reg = 0.001 with score             85.41
2019-03-13 18:44:15,758 : Evaluating...
2019-03-13 18:44:22,214 : 
Dev acc : 85.4 Test acc : 85.2 for SUBJNUMBER classification

2019-03-13 18:44:22,215 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 18:44:22,614 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 18:44:22,680 : loading BERT model bert-large-uncased
2019-03-13 18:44:22,680 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:44:22,793 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:44:22,793 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptt5dxlw4
2019-03-13 18:44:30,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:44:35,592 : Computing embeddings for train/dev/test
2019-03-13 18:47:41,586 : Computed embeddings
2019-03-13 18:47:41,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:48:14,025 : [('reg:1e-05', 72.99), ('reg:0.0001', 72.85), ('reg:0.001', 72.53), ('reg:0.01', 68.42)]
2019-03-13 18:48:14,025 : Validation : best param found is reg = 1e-05 with score             72.99
2019-03-13 18:48:14,025 : Evaluating...
2019-03-13 18:48:22,472 : 
Dev acc : 73.0 Test acc : 74.0 for OBJNUMBER classification

2019-03-13 18:48:22,473 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 18:48:23,031 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 18:48:23,099 : loading BERT model bert-large-uncased
2019-03-13 18:48:23,100 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:48:23,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:48:23,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc08hjod1
2019-03-13 18:48:30,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:48:36,013 : Computing embeddings for train/dev/test
2019-03-13 18:52:11,182 : Computed embeddings
2019-03-13 18:52:11,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:52:33,757 : [('reg:1e-05', 63.68), ('reg:0.0001', 63.61), ('reg:0.001', 63.45), ('reg:0.01', 63.28)]
2019-03-13 18:52:33,758 : Validation : best param found is reg = 1e-05 with score             63.68
2019-03-13 18:52:33,758 : Evaluating...
2019-03-13 18:52:39,459 : 
Dev acc : 63.7 Test acc : 63.5 for ODDMANOUT classification

2019-03-13 18:52:39,460 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 18:52:39,835 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 18:52:39,915 : loading BERT model bert-large-uncased
2019-03-13 18:52:39,915 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:52:39,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:52:39,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnfngltcq
2019-03-13 18:52:47,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:52:52,774 : Computing embeddings for train/dev/test
2019-03-13 18:56:26,075 : Computed embeddings
2019-03-13 18:56:26,075 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:56:51,923 : [('reg:1e-05', 73.33), ('reg:0.0001', 73.35), ('reg:0.001', 73.04), ('reg:0.01', 62.64)]
2019-03-13 18:56:51,923 : Validation : best param found is reg = 0.0001 with score             73.35
2019-03-13 18:56:51,923 : Evaluating...
2019-03-13 18:56:58,451 : 
Dev acc : 73.3 Test acc : 73.0 for COORDINATIONINVERSION classification

2019-03-13 18:56:58,453 : total results: {'STS12': {'MSRpar': {'pearson': (0.26406057018086293, 1.976308195483302e-13), 'spearman': SpearmanrResult(correlation=0.30911695948919354, pvalue=4.535631544904759e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.039884877712488456, 0.27531487162899426), 'spearman': SpearmanrResult(correlation=0.10770745428234531, pvalue=0.0031432931193309402), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4091937504133164, 5.891678544265816e-20), 'spearman': SpearmanrResult(correlation=0.4972310966160509, pvalue=4.881264008754825e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3717304306726473, 5.447672335685832e-26), 'spearman': SpearmanrResult(correlation=0.4078963469080189, pvalue=1.9662097522036384e-31), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5341247430976754, 8.256796380415848e-31), 'spearman': SpearmanrResult(correlation=0.5475899018589996, pvalue=1.3525818656726225e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3237988744153981, 'wmean': 0.2920503902381544}, 'spearman': {'mean': 0.37390835183092164, 'wmean': 0.34274710897624727}}}, 'STS13': {'FNWN': {'pearson': (0.1379162866776297, 0.05842293068418219), 'spearman': SpearmanrResult(correlation=0.14424868556585477, pvalue=0.04766926769445207), 'nsamples': 189}, 'headlines': {'pearson': (0.4372506304841203, 2.2733789185706207e-36), 'spearman': SpearmanrResult(correlation=0.4344890313171659, pvalue=6.95220503965897e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.1893809704458192, 6.286988357136665e-06), 'spearman': SpearmanrResult(correlation=0.20218125119530012, pvalue=1.3789829540175398e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2548492958691897, 'wmean': 0.3068312503101778}, 'spearman': {'mean': 0.2603063226927736, 'wmean': 0.3110356379869229}}}, 'STS14': {'deft-forum': {'pearson': (-0.041218627330439336, 0.3830349479990982), 'spearman': SpearmanrResult(correlation=-0.040470819336301686, pvalue=0.391732971850949), 'nsamples': 450}, 'deft-news': {'pearson': (0.4735867594055742, 3.5470851623735996e-18), 'spearman': SpearmanrResult(correlation=0.4450963615939858, pvalue=5.289088694670866e-16), 'nsamples': 300}, 'headlines': {'pearson': (0.4117806113272264, 4.6562492033444886e-32), 'spearman': SpearmanrResult(correlation=0.39361091708862733, pvalue=3.346751704674195e-29), 'nsamples': 750}, 'images': {'pearson': (0.17302033160059338, 1.8750367870279218e-06), 'spearman': SpearmanrResult(correlation=0.18790052204821675, pvalue=2.1773304530709443e-07), 'nsamples': 750}, 'OnWN': {'pearson': (0.3548011661942887, 1.142309236610161e-23), 'spearman': SpearmanrResult(correlation=0.36023427714291834, pvalue=2.1273067159326203e-24), 'nsamples': 750}, 'tweet-news': {'pearson': (0.46650374888302476, 8.530471647942957e-42), 'spearman': SpearmanrResult(correlation=0.43510526416659046, pvalue=5.4223196817691695e-36), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30641233168004467, 'wmean': 0.3141618770738198}, 'spearman': {'mean': 0.29691275378400617, 'wmean': 0.30612140669643323}}}, 'STS15': {'answers-forums': {'pearson': (0.4536014898907576, 1.9785225741147715e-20), 'spearman': SpearmanrResult(correlation=0.4370715095887806, pvalue=6.305409810279439e-19), 'nsamples': 375}, 'answers-students': {'pearson': (0.5461763916617501, 1.5304154872526736e-59), 'spearman': SpearmanrResult(correlation=0.5555733176847337, pvalue=5.79566813379816e-62), 'nsamples': 750}, 'belief': {'pearson': (0.5028396616962065, 2.0035898975916309e-25), 'spearman': SpearmanrResult(correlation=0.5330145394072063, pvalue=6.540952341660408e-29), 'nsamples': 375}, 'headlines': {'pearson': (0.46380280513985744, 2.8463158341104275e-41), 'spearman': SpearmanrResult(correlation=0.46387785544138005, pvalue=2.752993346501832e-41), 'nsamples': 750}, 'images': {'pearson': (0.2297243995124505, 1.9389700625866865e-10), 'spearman': SpearmanrResult(correlation=0.2325322902044599, pvalue=1.1476508337086956e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4392289495802045, 'wmean': 0.42948104302688506}, 'spearman': {'mean': 0.4444139024653121, 'wmean': 0.4342566219571418}}}, 'STS16': {'answer-answer': {'pearson': (0.39493361924338305, 6.55244706584189e-11), 'spearman': SpearmanrResult(correlation=0.3894712054852591, pvalue=1.2567905303505616e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5556545762613373, 1.4168050794959109e-21), 'spearman': SpearmanrResult(correlation=0.5628880269257902, pvalue=3.2684119432565614e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6888413551330389, 1.0650579427960044e-33), 'spearman': SpearmanrResult(correlation=0.6866275153427468, pvalue=2.06446778651377e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.7378485529049578, 3.360516251801028e-43), 'spearman': SpearmanrResult(correlation=0.7512303934442435, pvalue=1.4738741938413398e-45), 'nsamples': 244}, 'question-question': {'pearson': (0.03428955926312679, 0.6220900467340451), 'spearman': SpearmanrResult(correlation=0.10926146217657931, pvalue=0.11529805696102693), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.48231353256116877, 'wmean': 0.4926696502970442}, 'spearman': {'mean': 0.4998957206749238, 'wmean': 0.5085539587031279}}}, 'MR': {'devacc': 83.21, 'acc': 82.54, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.52, 'acc': 88.42, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.38, 'acc': 85.44, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.94, 'acc': 95.02, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 88.19, 'acc': 88.63, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.41, 'acc': 45.88, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.15, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.41, 'acc': 71.54, 'f1': 81.66, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.0, 'acc': 69.21, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6087861205418961, 'pearson': 0.6432922351826651, 'spearman': 0.6021562911787861, 'mse': 0.6053150165596156, 'yhat': array([3.19554149, 4.20407415, 2.28990115, ..., 2.89672408, 4.33590821,        4.01447919]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.46720998254961627, 'pearson': 0.43961336474215346, 'spearman': 0.44137381588097896, 'mse': 1.9854045977404327, 'yhat': array([1.68438799, 2.31189949, 3.0757877 , ..., 3.16313183, 2.73135652,        3.88554613]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.72, 'acc': 62.47, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 60.45, 'acc': 59.17, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.32, 'acc': 27.91, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.32, 'acc': 26.01, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.38, 'acc': 68.46, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.03, 'acc': 86.66, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.56, 'acc': 88.51, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.41, 'acc': 85.22, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.99, 'acc': 74.02, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.68, 'acc': 63.46, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.35, 'acc': 72.99, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 18:56:58,453 : STS12 p=0.2921, STS12 s=0.3427, STS13 p=0.3068, STS13 s=0.3110, STS14 p=0.3142, STS14 s=0.3061, STS15 p=0.4295, STS15 s=0.4343, STS 16 p=0.4927, STS16 s=0.5086, STS B p=0.4396, STS B s=0.4414, STS B m=1.9854, SICK-R p=0.6433, SICK-R s=0.6022, SICK-P m=0.6053
2019-03-13 18:56:58,453 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 18:56:58,453 : 0.2921,0.3427,0.3068,0.3110,0.3142,0.3061,0.4295,0.4343,0.4927,0.5086,0.4396,0.4414,1.9854,0.6433,0.6022,0.6053
2019-03-13 18:56:58,453 : MR=82.54, CR=88.42, SUBJ=95.02, MPQA=85.44, SST-B=88.63, SST-F=45.88, TREC=88.40, SICK-E=69.21, SNLI=62.47, MRPC=71.54, MRPC f=81.66
2019-03-13 18:56:58,453 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 18:56:58,453 : 82.54,88.42,95.02,85.44,88.63,45.88,88.40,69.21,62.47,71.54,81.66
2019-03-13 18:56:58,453 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 18:56:58,453 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 18:56:58,454 : na,na,na,na,na,na,na,na,na,na
2019-03-13 18:56:58,454 : SentLen=59.17, WC=27.91, TreeDepth=26.01, TopConst=68.46, BShift=86.66, Tense=88.51, SubjNum=85.22, ObjNum=74.02, SOMO=63.46, CoordInv=72.99, average=65.24
2019-03-13 18:56:58,454 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 18:56:58,454 : 59.17,27.91,26.01,68.46,86.66,88.51,85.22,74.02,63.46,72.99,65.24
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,454 : layer 22
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,454 : ********************************************************************************
2019-03-13 18:56:58,545 : ***** Transfer task : STS12 *****


2019-03-13 18:56:58,557 : loading BERT model bert-large-uncased
2019-03-13 18:56:58,557 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:56:58,574 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:56:58,574 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr_s8c8jj
2019-03-13 18:57:05,994 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:57:15,384 : MSRpar : pearson = 0.2557, spearman = 0.3043
2019-03-13 18:57:17,020 : MSRvid : pearson = 0.0515, spearman = 0.1217
2019-03-13 18:57:18,430 : SMTeuroparl : pearson = 0.3885, spearman = 0.4853
2019-03-13 18:57:21,121 : surprise.OnWN : pearson = 0.3604, spearman = 0.3978
2019-03-13 18:57:22,545 : surprise.SMTnews : pearson = 0.5168, spearman = 0.5280
2019-03-13 18:57:22,545 : ALL (weighted average) : Pearson = 0.2848,             Spearman = 0.3382
2019-03-13 18:57:22,546 : ALL (average) : Pearson = 0.3146,             Spearman = 0.3674

2019-03-13 18:57:22,546 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 18:57:22,554 : loading BERT model bert-large-uncased
2019-03-13 18:57:22,554 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:57:22,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:57:22,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx9_rznc8
2019-03-13 18:57:29,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:57:36,692 : FNWN : pearson = 0.1268, spearman = 0.1417
2019-03-13 18:57:38,582 : headlines : pearson = 0.4341, spearman = 0.4380
2019-03-13 18:57:40,045 : OnWN : pearson = 0.1883, spearman = 0.2110
2019-03-13 18:57:40,046 : ALL (weighted average) : Pearson = 0.3034,             Spearman = 0.3158
2019-03-13 18:57:40,046 : ALL (average) : Pearson = 0.2497,             Spearman = 0.2636

2019-03-13 18:57:40,046 : ***** Transfer task : STS14 *****


2019-03-13 18:57:40,061 : loading BERT model bert-large-uncased
2019-03-13 18:57:40,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:57:40,078 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:57:40,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph4zk_fsp
2019-03-13 18:57:47,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:57:54,080 : deft-forum : pearson = -0.0447, spearman = -0.0408
2019-03-13 18:57:55,713 : deft-news : pearson = 0.4856, spearman = 0.4542
2019-03-13 18:57:57,877 : headlines : pearson = 0.4100, spearman = 0.3939
2019-03-13 18:57:59,949 : images : pearson = 0.1693, spearman = 0.1875
2019-03-13 18:58:02,076 : OnWN : pearson = 0.3407, spearman = 0.3524
2019-03-13 18:58:04,930 : tweet-news : pearson = 0.4022, spearman = 0.3926
2019-03-13 18:58:04,930 : ALL (weighted average) : Pearson = 0.2979,             Spearman = 0.2967
2019-03-13 18:58:04,930 : ALL (average) : Pearson = 0.2938,             Spearman = 0.2900

2019-03-13 18:58:04,930 : ***** Transfer task : STS15 *****


2019-03-13 18:58:04,961 : loading BERT model bert-large-uncased
2019-03-13 18:58:04,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:58:04,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:58:04,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4a_h3_uw
2019-03-13 18:58:12,452 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:58:19,498 : answers-forums : pearson = 0.4187, spearman = 0.4101
2019-03-13 18:58:21,581 : answers-students : pearson = 0.5125, spearman = 0.5248
2019-03-13 18:58:23,628 : belief : pearson = 0.4673, spearman = 0.4965
2019-03-13 18:58:25,878 : headlines : pearson = 0.4724, spearman = 0.4702
2019-03-13 18:58:28,008 : images : pearson = 0.2359, spearman = 0.2397
2019-03-13 18:58:28,008 : ALL (weighted average) : Pearson = 0.4160,             Spearman = 0.4220
2019-03-13 18:58:28,008 : ALL (average) : Pearson = 0.4214,             Spearman = 0.4283

2019-03-13 18:58:28,008 : ***** Transfer task : STS16 *****


2019-03-13 18:58:28,079 : loading BERT model bert-large-uncased
2019-03-13 18:58:28,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:58:28,097 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:58:28,097 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdkaw4ktf
2019-03-13 18:58:35,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:58:41,711 : answer-answer : pearson = 0.3992, spearman = 0.3960
2019-03-13 18:58:42,369 : headlines : pearson = 0.5476, spearman = 0.5561
2019-03-13 18:58:43,249 : plagiarism : pearson = 0.6728, spearman = 0.6736
2019-03-13 18:58:44,742 : postediting : pearson = 0.7231, spearman = 0.7415
2019-03-13 18:58:45,349 : question-question : pearson = 0.0467, spearman = 0.1125
2019-03-13 18:58:45,350 : ALL (weighted average) : Pearson = 0.4879,             Spearman = 0.5046
2019-03-13 18:58:45,350 : ALL (average) : Pearson = 0.4779,             Spearman = 0.4960

2019-03-13 18:58:45,350 : ***** Transfer task : MR *****


2019-03-13 18:58:45,365 : loading BERT model bert-large-uncased
2019-03-13 18:58:45,365 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:58:45,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:58:45,386 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqt4zsn98
2019-03-13 18:58:52,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:58:58,299 : Generating sentence embeddings
2019-03-13 18:59:29,718 : Generated sentence embeddings
2019-03-13 18:59:29,719 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:59:39,495 : Best param found at split 1: l2reg = 0.01                 with score 83.54
2019-03-13 18:59:50,839 : Best param found at split 2: l2reg = 1e-05                 with score 83.59
2019-03-13 19:00:01,408 : Best param found at split 3: l2reg = 0.01                 with score 83.76
2019-03-13 19:00:12,563 : Best param found at split 4: l2reg = 0.01                 with score 83.68
2019-03-13 19:00:22,919 : Best param found at split 5: l2reg = 0.01                 with score 83.21
2019-03-13 19:00:23,363 : Dev acc : 83.56 Test acc : 81.96

2019-03-13 19:00:23,364 : ***** Transfer task : CR *****


2019-03-13 19:00:23,372 : loading BERT model bert-large-uncased
2019-03-13 19:00:23,373 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:00:23,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:00:23,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcave24dk
2019-03-13 19:00:30,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:00:36,144 : Generating sentence embeddings
2019-03-13 19:00:44,442 : Generated sentence embeddings
2019-03-13 19:00:44,443 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:00:47,307 : Best param found at split 1: l2reg = 0.01                 with score 90.03
2019-03-13 19:00:50,032 : Best param found at split 2: l2reg = 0.001                 with score 89.37
2019-03-13 19:00:52,916 : Best param found at split 3: l2reg = 0.0001                 with score 89.7
2019-03-13 19:00:56,202 : Best param found at split 4: l2reg = 0.01                 with score 90.37
2019-03-13 19:00:59,761 : Best param found at split 5: l2reg = 0.001                 with score 89.84
2019-03-13 19:00:59,920 : Dev acc : 89.86 Test acc : 88.5

2019-03-13 19:00:59,921 : ***** Transfer task : MPQA *****


2019-03-13 19:00:59,926 : loading BERT model bert-large-uncased
2019-03-13 19:00:59,927 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:00:59,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:00:59,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphq_g75jb
2019-03-13 19:01:07,479 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:01:12,858 : Generating sentence embeddings
2019-03-13 19:01:20,432 : Generated sentence embeddings
2019-03-13 19:01:20,432 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:01:30,330 : Best param found at split 1: l2reg = 0.01                 with score 83.32
2019-03-13 19:01:41,220 : Best param found at split 2: l2reg = 1e-05                 with score 84.06
2019-03-13 19:01:51,679 : Best param found at split 3: l2reg = 0.001                 with score 82.27
2019-03-13 19:02:03,726 : Best param found at split 4: l2reg = 0.001                 with score 84.31
2019-03-13 19:02:13,737 : Best param found at split 5: l2reg = 1e-05                 with score 82.74
2019-03-13 19:02:14,357 : Dev acc : 83.34 Test acc : 84.21

2019-03-13 19:02:14,358 : ***** Transfer task : SUBJ *****


2019-03-13 19:02:14,375 : loading BERT model bert-large-uncased
2019-03-13 19:02:14,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:02:14,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:02:14,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_hxhqln1
2019-03-13 19:02:21,848 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:02:27,297 : Generating sentence embeddings
2019-03-13 19:02:58,111 : Generated sentence embeddings
2019-03-13 19:02:58,111 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:03:06,900 : Best param found at split 1: l2reg = 1e-05                 with score 96.04
2019-03-13 19:03:17,480 : Best param found at split 2: l2reg = 0.01                 with score 96.0
2019-03-13 19:03:27,531 : Best param found at split 3: l2reg = 0.01                 with score 95.75
2019-03-13 19:03:37,855 : Best param found at split 4: l2reg = 0.01                 with score 96.26
2019-03-13 19:03:46,992 : Best param found at split 5: l2reg = 0.01                 with score 95.89
2019-03-13 19:03:47,490 : Dev acc : 95.99 Test acc : 95.49

2019-03-13 19:03:47,491 : ***** Transfer task : SST Binary classification *****


2019-03-13 19:03:47,584 : loading BERT model bert-large-uncased
2019-03-13 19:03:47,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:03:47,659 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:03:47,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqq3jq0rg
2019-03-13 19:03:55,150 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:04:00,559 : Computing embedding for train
2019-03-13 19:05:40,213 : Computed train embeddings
2019-03-13 19:05:40,213 : Computing embedding for dev
2019-03-13 19:05:42,383 : Computed dev embeddings
2019-03-13 19:05:42,383 : Computing embedding for test
2019-03-13 19:05:46,937 : Computed test embeddings
2019-03-13 19:05:46,937 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:06:04,279 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.24), ('reg:0.001', 86.7), ('reg:0.01', 86.81)]
2019-03-13 19:06:04,279 : Validation : best param found is reg = 0.01 with score             86.81
2019-03-13 19:06:04,279 : Evaluating...
2019-03-13 19:06:07,948 : 
Dev acc : 86.81 Test acc : 87.26 for             SST Binary classification

2019-03-13 19:06:07,948 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 19:06:08,002 : loading BERT model bert-large-uncased
2019-03-13 19:06:08,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:06:08,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:06:08,023 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbwjn9fpt
2019-03-13 19:06:15,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:06:20,746 : Computing embedding for train
2019-03-13 19:06:42,576 : Computed train embeddings
2019-03-13 19:06:42,576 : Computing embedding for dev
2019-03-13 19:06:45,427 : Computed dev embeddings
2019-03-13 19:06:45,427 : Computing embedding for test
2019-03-13 19:06:51,041 : Computed test embeddings
2019-03-13 19:06:51,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:06:53,822 : [('reg:1e-05', 40.87), ('reg:0.0001', 43.51), ('reg:0.001', 45.23), ('reg:0.01', 39.15)]
2019-03-13 19:06:53,822 : Validation : best param found is reg = 0.001 with score             45.23
2019-03-13 19:06:53,822 : Evaluating...
2019-03-13 19:06:54,629 : 
Dev acc : 45.23 Test acc : 44.39 for             SST Fine-Grained classification

2019-03-13 19:06:54,629 : ***** Transfer task : TREC *****


2019-03-13 19:06:54,645 : loading BERT model bert-large-uncased
2019-03-13 19:06:54,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:06:54,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:06:54,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp91o0ga6h
2019-03-13 19:07:02,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:07:15,027 : Computed train embeddings
2019-03-13 19:07:15,613 : Computed test embeddings
2019-03-13 19:07:15,614 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:07:22,030 : [('reg:1e-05', 79.42), ('reg:0.0001', 81.64), ('reg:0.001', 77.6), ('reg:0.01', 76.38)]
2019-03-13 19:07:22,030 : Cross-validation : best param found is reg = 0.0001             with score 81.64
2019-03-13 19:07:22,030 : Evaluating...
2019-03-13 19:07:22,540 : 
Dev acc : 81.64 Test acc : 91.4             for TREC

2019-03-13 19:07:22,541 : ***** Transfer task : MRPC *****


2019-03-13 19:07:22,563 : loading BERT model bert-large-uncased
2019-03-13 19:07:22,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:07:22,588 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:07:22,588 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpryobsmvu
2019-03-13 19:07:30,117 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:07:35,317 : Computing embedding for train
2019-03-13 19:07:57,488 : Computed train embeddings
2019-03-13 19:07:57,488 : Computing embedding for test
2019-03-13 19:08:07,184 : Computed test embeddings
2019-03-13 19:08:07,205 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:08:11,706 : [('reg:1e-05', 70.0), ('reg:0.0001', 70.88), ('reg:0.001', 70.98), ('reg:0.01', 70.66)]
2019-03-13 19:08:11,706 : Cross-validation : best param found is reg = 0.001             with score 70.98
2019-03-13 19:08:11,706 : Evaluating...
2019-03-13 19:08:11,931 : Dev acc : 70.98 Test acc 72.46; Test F1 81.82 for MRPC.

2019-03-13 19:08:11,931 : ***** Transfer task : SICK-Entailment*****


2019-03-13 19:08:11,994 : loading BERT model bert-large-uncased
2019-03-13 19:08:11,994 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:08:12,013 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:08:12,013 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmt0je60f
2019-03-13 19:08:19,458 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:08:24,803 : Computing embedding for train
2019-03-13 19:08:36,069 : Computed train embeddings
2019-03-13 19:08:36,069 : Computing embedding for dev
2019-03-13 19:08:37,603 : Computed dev embeddings
2019-03-13 19:08:37,603 : Computing embedding for test
2019-03-13 19:08:49,655 : Computed test embeddings
2019-03-13 19:08:49,691 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:08:51,191 : [('reg:1e-05', 73.2), ('reg:0.0001', 71.6), ('reg:0.001', 72.8), ('reg:0.01', 67.6)]
2019-03-13 19:08:51,191 : Validation : best param found is reg = 1e-05 with score             73.2
2019-03-13 19:08:51,191 : Evaluating...
2019-03-13 19:08:51,547 : 
Dev acc : 73.2 Test acc : 69.37 for                        SICK entailment

2019-03-13 19:08:51,548 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 19:08:51,575 : loading BERT model bert-large-uncased
2019-03-13 19:08:51,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:08:51,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:08:51,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpezpzc34o
2019-03-13 19:08:59,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:09:04,513 : Computing embedding for train
2019-03-13 19:09:15,746 : Computed train embeddings
2019-03-13 19:09:15,746 : Computing embedding for dev
2019-03-13 19:09:17,279 : Computed dev embeddings
2019-03-13 19:09:17,279 : Computing embedding for test
2019-03-13 19:09:29,328 : Computed test embeddings
2019-03-13 19:09:44,036 : Dev : Pearson 0.6479342729933432
2019-03-13 19:09:44,036 : Test : Pearson 0.6399659537694486 Spearman 0.5937935248450081 MSE 0.6140414649983424                        for SICK Relatedness

2019-03-13 19:09:44,037 : 

***** Transfer task : STSBenchmark*****


2019-03-13 19:09:44,104 : loading BERT model bert-large-uncased
2019-03-13 19:09:44,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:09:44,123 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:09:44,124 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5t801n1j
2019-03-13 19:09:51,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:09:57,009 : Computing embedding for train
2019-03-13 19:10:15,543 : Computed train embeddings
2019-03-13 19:10:15,543 : Computing embedding for dev
2019-03-13 19:10:21,160 : Computed dev embeddings
2019-03-13 19:10:21,160 : Computing embedding for test
2019-03-13 19:10:25,739 : Computed test embeddings
2019-03-13 19:10:38,555 : Dev : Pearson 0.48327643392745534
2019-03-13 19:10:38,555 : Test : Pearson 0.43484431690362896 Spearman 0.43201110514520286 MSE 1.9771553943375826                        for SICK Relatedness

2019-03-13 19:10:38,555 : ***** Transfer task : SNLI Entailment*****


2019-03-13 19:10:43,236 : loading BERT model bert-large-uncased
2019-03-13 19:10:43,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:10:44,219 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:10:44,219 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpysms21_j
2019-03-13 19:10:51,739 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:10:57,504 : PROGRESS (encoding): 0.00%
2019-03-13 19:13:41,961 : PROGRESS (encoding): 14.56%
2019-03-13 19:16:48,653 : PROGRESS (encoding): 29.12%
2019-03-13 19:19:55,816 : PROGRESS (encoding): 43.69%
2019-03-13 19:23:15,391 : PROGRESS (encoding): 58.25%
2019-03-13 19:26:57,784 : PROGRESS (encoding): 72.81%
2019-03-13 19:30:38,973 : PROGRESS (encoding): 87.37%
2019-03-13 19:34:38,319 : PROGRESS (encoding): 0.00%
2019-03-13 19:35:08,431 : PROGRESS (encoding): 0.00%
2019-03-13 19:35:37,362 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:36:13,446 : [('reg:1e-09', 59.37)]
2019-03-13 19:36:13,447 : Validation : best param found is reg = 1e-09 with score             59.37
2019-03-13 19:36:13,447 : Evaluating...
2019-03-13 19:36:48,505 : Dev acc : 59.37 Test acc : 59.64 for SNLI

2019-03-13 19:36:48,505 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 19:36:48,711 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 19:36:49,700 : loading BERT model bert-large-uncased
2019-03-13 19:36:49,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:36:49,727 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:36:49,727 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxo1gw1go
2019-03-13 19:36:57,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:37:02,594 : Computing embeddings for train/dev/test
2019-03-13 19:40:32,104 : Computed embeddings
2019-03-13 19:40:32,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:41:02,670 : [('reg:1e-05', 59.65), ('reg:0.0001', 59.21), ('reg:0.001', 55.48), ('reg:0.01', 47.53)]
2019-03-13 19:41:02,670 : Validation : best param found is reg = 1e-05 with score             59.65
2019-03-13 19:41:02,670 : Evaluating...
2019-03-13 19:41:12,270 : 
Dev acc : 59.6 Test acc : 60.2 for LENGTH classification

2019-03-13 19:41:12,270 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 19:41:12,633 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 19:41:12,679 : loading BERT model bert-large-uncased
2019-03-13 19:41:12,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:41:12,707 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:41:12,707 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiw4z6p9x
2019-03-13 19:41:20,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:41:25,536 : Computing embeddings for train/dev/test
2019-03-13 19:44:38,425 : Computed embeddings
2019-03-13 19:44:38,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:45:11,667 : [('reg:1e-05', 28.1), ('reg:0.0001', 14.94), ('reg:0.001', 1.97), ('reg:0.01', 0.61)]
2019-03-13 19:45:11,667 : Validation : best param found is reg = 1e-05 with score             28.1
2019-03-13 19:45:11,667 : Evaluating...
2019-03-13 19:45:21,660 : 
Dev acc : 28.1 Test acc : 28.6 for WORDCONTENT classification

2019-03-13 19:45:21,662 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 19:45:22,016 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 19:45:22,083 : loading BERT model bert-large-uncased
2019-03-13 19:45:22,083 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:45:22,108 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:45:22,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgdhojgof
2019-03-13 19:45:29,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:45:34,829 : Computing embeddings for train/dev/test
2019-03-13 19:48:36,056 : Computed embeddings
2019-03-13 19:48:36,056 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:48:58,898 : [('reg:1e-05', 25.03), ('reg:0.0001', 24.98), ('reg:0.001', 25.52), ('reg:0.01', 22.07)]
2019-03-13 19:48:58,898 : Validation : best param found is reg = 0.001 with score             25.52
2019-03-13 19:48:58,898 : Evaluating...
2019-03-13 19:49:04,353 : 
Dev acc : 25.5 Test acc : 24.6 for DEPTH classification

2019-03-13 19:49:04,354 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 19:49:04,749 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 19:49:04,815 : loading BERT model bert-large-uncased
2019-03-13 19:49:04,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:49:04,931 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:49:04,932 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprhq9zc8g
2019-03-13 19:49:12,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:49:17,834 : Computing embeddings for train/dev/test
2019-03-13 19:52:05,943 : Computed embeddings
2019-03-13 19:52:05,943 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:52:37,408 : [('reg:1e-05', 64.69), ('reg:0.0001', 65.8), ('reg:0.001', 62.94), ('reg:0.01', 49.71)]
2019-03-13 19:52:37,408 : Validation : best param found is reg = 0.0001 with score             65.8
2019-03-13 19:52:37,408 : Evaluating...
2019-03-13 19:52:45,175 : 
Dev acc : 65.8 Test acc : 66.0 for TOPCONSTITUENTS classification

2019-03-13 19:52:45,176 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 19:52:45,519 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 19:52:45,587 : loading BERT model bert-large-uncased
2019-03-13 19:52:45,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:52:45,711 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:52:45,711 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuhhjcm91
2019-03-13 19:52:53,158 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:52:58,552 : Computing embeddings for train/dev/test
2019-03-13 19:56:01,275 : Computed embeddings
2019-03-13 19:56:01,275 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:56:27,262 : [('reg:1e-05', 87.14), ('reg:0.0001', 87.21), ('reg:0.001', 87.26), ('reg:0.01', 86.69)]
2019-03-13 19:56:27,262 : Validation : best param found is reg = 0.001 with score             87.26
2019-03-13 19:56:27,262 : Evaluating...
2019-03-13 19:56:33,736 : 
Dev acc : 87.3 Test acc : 86.9 for BIGRAMSHIFT classification

2019-03-13 19:56:33,737 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 19:56:34,142 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 19:56:34,207 : loading BERT model bert-large-uncased
2019-03-13 19:56:34,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:56:34,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:56:34,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy5nuy1i8
2019-03-13 19:56:41,714 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:56:47,183 : Computing embeddings for train/dev/test
2019-03-13 19:59:46,054 : Computed embeddings
2019-03-13 19:59:46,055 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:00:10,631 : [('reg:1e-05', 88.36), ('reg:0.0001', 88.3), ('reg:0.001', 88.29), ('reg:0.01', 89.8)]
2019-03-13 20:00:10,632 : Validation : best param found is reg = 0.01 with score             89.8
2019-03-13 20:00:10,632 : Evaluating...
2019-03-13 20:00:16,613 : 
Dev acc : 89.8 Test acc : 88.0 for TENSE classification

2019-03-13 20:00:16,615 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 20:00:17,025 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 20:00:17,089 : loading BERT model bert-large-uncased
2019-03-13 20:00:17,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:00:17,117 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:00:17,118 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq0qh7ulp
2019-03-13 20:00:24,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:00:29,931 : Computing embeddings for train/dev/test
2019-03-13 20:03:39,340 : Computed embeddings
2019-03-13 20:03:39,340 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:03:57,482 : [('reg:1e-05', 84.1), ('reg:0.0001', 84.13), ('reg:0.001', 84.32), ('reg:0.01', 83.38)]
2019-03-13 20:03:57,483 : Validation : best param found is reg = 0.001 with score             84.32
2019-03-13 20:03:57,483 : Evaluating...
2019-03-13 20:04:02,829 : 
Dev acc : 84.3 Test acc : 84.4 for SUBJNUMBER classification

2019-03-13 20:04:02,830 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 20:04:03,236 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 20:04:03,302 : loading BERT model bert-large-uncased
2019-03-13 20:04:03,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:04:03,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:04:03,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgk5t560_
2019-03-13 20:04:10,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:04:16,312 : Computing embeddings for train/dev/test
2019-03-13 20:07:22,256 : Computed embeddings
2019-03-13 20:07:22,256 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:07:52,489 : [('reg:1e-05', 70.86), ('reg:0.0001', 72.79), ('reg:0.001', 71.95), ('reg:0.01', 69.05)]
2019-03-13 20:07:52,490 : Validation : best param found is reg = 0.0001 with score             72.79
2019-03-13 20:07:52,490 : Evaluating...
2019-03-13 20:08:00,579 : 
Dev acc : 72.8 Test acc : 73.9 for OBJNUMBER classification

2019-03-13 20:08:00,580 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 20:08:00,975 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 20:08:01,046 : loading BERT model bert-large-uncased
2019-03-13 20:08:01,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:08:01,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:08:01,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_alab8ax
2019-03-13 20:08:08,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:08:14,040 : Computing embeddings for train/dev/test
2019-03-13 20:11:49,303 : Computed embeddings
2019-03-13 20:11:49,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:12:15,707 : [('reg:1e-05', 63.96), ('reg:0.0001', 63.97), ('reg:0.001', 63.96), ('reg:0.01', 63.27)]
2019-03-13 20:12:15,707 : Validation : best param found is reg = 0.0001 with score             63.97
2019-03-13 20:12:15,707 : Evaluating...
2019-03-13 20:12:21,414 : 
Dev acc : 64.0 Test acc : 63.3 for ODDMANOUT classification

2019-03-13 20:12:21,415 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 20:12:22,004 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 20:12:22,081 : loading BERT model bert-large-uncased
2019-03-13 20:12:22,081 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:12:22,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:12:22,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptll9dsyv
2019-03-13 20:12:29,625 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:12:35,087 : Computing embeddings for train/dev/test
2019-03-13 20:16:08,179 : Computed embeddings
2019-03-13 20:16:08,180 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:16:32,986 : [('reg:1e-05', 71.8), ('reg:0.0001', 71.79), ('reg:0.001', 71.61), ('reg:0.01', 62.3)]
2019-03-13 20:16:32,986 : Validation : best param found is reg = 1e-05 with score             71.8
2019-03-13 20:16:32,986 : Evaluating...
2019-03-13 20:16:39,619 : 
Dev acc : 71.8 Test acc : 71.8 for COORDINATIONINVERSION classification

2019-03-13 20:16:39,621 : total results: {'STS12': {'MSRpar': {'pearson': (0.2557041428241655, 1.1638719042484041e-12), 'spearman': SpearmanrResult(correlation=0.304250051238582, pvalue=1.5796615507017866e-17), 'nsamples': 750}, 'MSRvid': {'pearson': (0.051522865271980585, 0.15865942412494546), 'spearman': SpearmanrResult(correlation=0.12173672789840263, pvalue=0.0008354653469809927), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.38853910915766804, 5.456516590017986e-18), 'spearman': SpearmanrResult(correlation=0.4852788467615257, pvalue=1.7181714493184249e-28), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.36044953867985424, 1.988921326182117e-24), 'spearman': SpearmanrResult(correlation=0.3977573372458935, pvalue=7.730919377527318e-30), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5168258785042323, 1.2482740093690356e-28), 'spearman': SpearmanrResult(correlation=0.5279621435517781, pvalue=5.10089985602297e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3146083068875801, 'wmean': 0.2848489017723805}, 'spearman': {'mean': 0.3673970213392364, 'wmean': 0.338226503612567}}}, 'STS13': {'FNWN': {'pearson': (0.12675083643385895, 0.08221211426860651), 'spearman': SpearmanrResult(correlation=0.1417148964512948, pvalue=0.05175555104760599), 'nsamples': 189}, 'headlines': {'pearson': (0.4340971838269258, 8.140327611132415e-36), 'spearman': SpearmanrResult(correlation=0.43802592719371813, pvalue=1.6579787974683918e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.18831145682532044, 7.104726340768095e-06), 'spearman': SpearmanrResult(correlation=0.21098472373701002, pvalue=4.582982284536834e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2497198256953684, 'wmean': 0.30344768215679896}, 'spearman': {'mean': 0.2635751824606743, 'wmean': 0.31577732722736396}}}, 'STS14': {'deft-forum': {'pearson': (-0.044712432086115354, 0.34398071549867726), 'spearman': SpearmanrResult(correlation=-0.0408489415452286, pvalue=0.38732006228120386), 'nsamples': 450}, 'deft-news': {'pearson': (0.48564857809372125, 3.6881123158763197e-19), 'spearman': SpearmanrResult(correlation=0.4541784527645726, pvalue=1.1283053050694571e-16), 'nsamples': 300}, 'headlines': {'pearson': (0.4100317250034693, 8.927464085209021e-32), 'spearman': SpearmanrResult(correlation=0.39387416969070904, pvalue=3.0513194621583106e-29), 'nsamples': 750}, 'images': {'pearson': (0.16930098992815765, 3.124481939269245e-06), 'spearman': SpearmanrResult(correlation=0.18751385394918105, pvalue=2.307810148820213e-07), 'nsamples': 750}, 'OnWN': {'pearson': (0.3406712533082073, 7.778504635503877e-22), 'spearman': SpearmanrResult(correlation=0.3524353132170579, pvalue=2.3509644819926172e-23), 'nsamples': 750}, 'tweet-news': {'pearson': (0.402151932540454, 1.5989352125716672e-30), 'spearman': SpearmanrResult(correlation=0.3926034813816718, pvalue=4.7630378326549385e-29), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.29384867446464896, 'wmean': 0.2979175745532215}, 'spearman': {'mean': 0.289959388242994, 'wmean': 0.2967177668834623}}}, 'STS15': {'answers-forums': {'pearson': (0.41874868433082174, 2.359400788498421e-17), 'spearman': SpearmanrResult(correlation=0.4101200002380364, pvalue=1.2050777815935318e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.5124601221196159, 1.8670579986983092e-51), 'spearman': SpearmanrResult(correlation=0.5247552926983791, pvalue=2.6748447066368777e-54), 'nsamples': 750}, 'belief': {'pearson': (0.4673067464534959, 9.689223605599222e-22), 'spearman': SpearmanrResult(correlation=0.4964935551336627, pvalue=9.8131493707642e-25), 'nsamples': 375}, 'headlines': {'pearson': (0.4724212253387199, 5.860999359219138e-43), 'spearman': SpearmanrResult(correlation=0.4702045494721234, pvalue=1.6080506046017248e-42), 'nsamples': 750}, 'images': {'pearson': (0.2359210201131368, 6.039326736028957e-11), 'spearman': SpearmanrResult(correlation=0.23970397628448975, pvalue=2.9147827729821684e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.421371559671158, 'wmean': 0.4159575207409078}, 'spearman': {'mean': 0.4282554747653383, 'wmean': 0.42199264903521044}}}, 'STS16': {'answer-answer': {'pearson': (0.399215118983045, 3.899825671014348e-11), 'spearman': SpearmanrResult(correlation=0.39604833549011387, pvalue=5.7284967159857364e-11), 'nsamples': 254}, 'headlines': {'pearson': (0.5475926882005128, 6.969772211755427e-21), 'spearman': SpearmanrResult(correlation=0.5561344706081102, pvalue=1.2868560304810722e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.672796432949857, 1.132730378941375e-31), 'spearman': SpearmanrResult(correlation=0.6736310190153147, pvalue=8.950548546925875e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.723079503018984, 9.261871402537405e-41), 'spearman': SpearmanrResult(correlation=0.7415073414599684, pvalue=7.874431032763676e-44), 'nsamples': 244}, 'question-question': {'pearson': (0.04667411126278098, 0.5021702945383877), 'spearman': SpearmanrResult(correlation=0.11250076774438984, pvalue=0.10484548377502244), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.477871570883036, 'wmean': 0.4879263803985173}, 'spearman': {'mean': 0.4959643868635794, 'wmean': 0.5045947272717036}}}, 'MR': {'devacc': 83.56, 'acc': 81.96, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.86, 'acc': 88.5, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.34, 'acc': 84.21, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.99, 'acc': 95.49, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.81, 'acc': 87.26, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.23, 'acc': 44.39, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.64, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.98, 'acc': 72.46, 'f1': 81.82, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 69.37, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6479342729933432, 'pearson': 0.6399659537694486, 'spearman': 0.5937935248450081, 'mse': 0.6140414649983424, 'yhat': array([3.46626846, 4.61044081, 1.37962951, ..., 3.00402667, 4.68130295,        4.40483163]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.48327643392745534, 'pearson': 0.43484431690362896, 'spearman': 0.43201110514520286, 'mse': 1.9771553943375826, 'yhat': array([1.22423136, 1.01439845, 2.72548484, ..., 3.85066668, 2.15069347,        3.65924055]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.37, 'acc': 59.64, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 59.65, 'acc': 60.2, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.1, 'acc': 28.58, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.52, 'acc': 24.61, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 65.8, 'acc': 66.0, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.26, 'acc': 86.94, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.8, 'acc': 87.99, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.32, 'acc': 84.4, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.79, 'acc': 73.88, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.97, 'acc': 63.28, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.8, 'acc': 71.84, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 20:16:39,621 : STS12 p=0.2848, STS12 s=0.3382, STS13 p=0.3034, STS13 s=0.3158, STS14 p=0.2979, STS14 s=0.2967, STS15 p=0.4160, STS15 s=0.4220, STS 16 p=0.4879, STS16 s=0.5046, STS B p=0.4348, STS B s=0.4320, STS B m=1.9772, SICK-R p=0.6400, SICK-R s=0.5938, SICK-P m=0.6140
2019-03-13 20:16:39,621 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 20:16:39,621 : 0.2848,0.3382,0.3034,0.3158,0.2979,0.2967,0.4160,0.4220,0.4879,0.5046,0.4348,0.4320,1.9772,0.6400,0.5938,0.6140
2019-03-13 20:16:39,621 : MR=81.96, CR=88.50, SUBJ=95.49, MPQA=84.21, SST-B=87.26, SST-F=44.39, TREC=91.40, SICK-E=69.37, SNLI=59.64, MRPC=72.46, MRPC f=81.82
2019-03-13 20:16:39,621 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 20:16:39,621 : 81.96,88.50,95.49,84.21,87.26,44.39,91.40,69.37,59.64,72.46,81.82
2019-03-13 20:16:39,621 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 20:16:39,622 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 20:16:39,622 : na,na,na,na,na,na,na,na,na,na
2019-03-13 20:16:39,622 : SentLen=60.20, WC=28.58, TreeDepth=24.61, TopConst=66.00, BShift=86.94, Tense=87.99, SubjNum=84.40, ObjNum=73.88, SOMO=63.28, CoordInv=71.84, average=64.77
2019-03-13 20:16:39,622 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 20:16:39,622 : 60.20,28.58,24.61,66.00,86.94,87.99,84.40,73.88,63.28,71.84,64.77
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,622 : layer 23
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,622 : ********************************************************************************
2019-03-13 20:16:39,714 : ***** Transfer task : STS12 *****


2019-03-13 20:16:39,726 : loading BERT model bert-large-uncased
2019-03-13 20:16:39,726 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:16:39,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:16:39,744 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7vqcckuu
2019-03-13 20:16:47,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:16:56,709 : MSRpar : pearson = 0.2198, spearman = 0.2720
2019-03-13 20:16:58,344 : MSRvid : pearson = 0.0523, spearman = 0.1441
2019-03-13 20:16:59,747 : SMTeuroparl : pearson = 0.3163, spearman = 0.4370
2019-03-13 20:17:02,439 : surprise.OnWN : pearson = 0.3155, spearman = 0.3649
2019-03-13 20:17:03,863 : surprise.SMTnews : pearson = 0.4633, spearman = 0.4879
2019-03-13 20:17:03,863 : ALL (weighted average) : Pearson = 0.2480,             Spearman = 0.3157
2019-03-13 20:17:03,863 : ALL (average) : Pearson = 0.2734,             Spearman = 0.3412

2019-03-13 20:17:03,863 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 20:17:03,873 : loading BERT model bert-large-uncased
2019-03-13 20:17:03,873 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:17:03,891 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:17:03,891 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmaam_58a
2019-03-13 20:17:11,393 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:17:18,070 : FNWN : pearson = 0.0313, spearman = 0.0465
2019-03-13 20:17:19,958 : headlines : pearson = 0.3865, spearman = 0.4193
2019-03-13 20:17:21,422 : OnWN : pearson = 0.1940, spearman = 0.2321
2019-03-13 20:17:21,422 : ALL (weighted average) : Pearson = 0.2698,             Spearman = 0.3023
2019-03-13 20:17:21,422 : ALL (average) : Pearson = 0.2039,             Spearman = 0.2327

2019-03-13 20:17:21,422 : ***** Transfer task : STS14 *****


2019-03-13 20:17:21,437 : loading BERT model bert-large-uncased
2019-03-13 20:17:21,438 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:17:21,455 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:17:21,455 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphz5gpd3z
2019-03-13 20:17:28,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:17:35,645 : deft-forum : pearson = -0.0457, spearman = -0.0339
2019-03-13 20:17:37,278 : deft-news : pearson = 0.4809, spearman = 0.4570
2019-03-13 20:17:39,442 : headlines : pearson = 0.3885, spearman = 0.3761
2019-03-13 20:17:41,511 : images : pearson = 0.1653, spearman = 0.1960
2019-03-13 20:17:43,633 : OnWN : pearson = 0.2961, spearman = 0.3088
2019-03-13 20:17:46,484 : tweet-news : pearson = 0.1936, spearman = 0.2962
2019-03-13 20:17:46,484 : ALL (weighted average) : Pearson = 0.2417,             Spearman = 0.2679
2019-03-13 20:17:46,484 : ALL (average) : Pearson = 0.2465,             Spearman = 0.2667

2019-03-13 20:17:46,484 : ***** Transfer task : STS15 *****


2019-03-13 20:17:46,517 : loading BERT model bert-large-uncased
2019-03-13 20:17:46,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:17:46,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:17:46,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2chow5g5
2019-03-13 20:17:54,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:18:01,220 : answers-forums : pearson = 0.3183, spearman = 0.3489
2019-03-13 20:18:03,300 : answers-students : pearson = 0.4058, spearman = 0.4315
2019-03-13 20:18:05,343 : belief : pearson = 0.3330, spearman = 0.3822
2019-03-13 20:18:07,589 : headlines : pearson = 0.4353, spearman = 0.4368
2019-03-13 20:18:09,718 : images : pearson = 0.2164, spearman = 0.2313
2019-03-13 20:18:09,718 : ALL (weighted average) : Pearson = 0.3458,             Spearman = 0.3663
2019-03-13 20:18:09,718 : ALL (average) : Pearson = 0.3418,             Spearman = 0.3662

2019-03-13 20:18:09,718 : ***** Transfer task : STS16 *****


2019-03-13 20:18:09,788 : loading BERT model bert-large-uncased
2019-03-13 20:18:09,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:18:09,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:18:09,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnplitfaq
2019-03-13 20:18:17,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:18:23,674 : answer-answer : pearson = 0.3754, spearman = 0.3774
2019-03-13 20:18:24,334 : headlines : pearson = 0.4935, spearman = 0.5092
2019-03-13 20:18:25,214 : plagiarism : pearson = 0.5645, spearman = 0.5849
2019-03-13 20:18:26,704 : postediting : pearson = 0.6702, spearman = 0.7187
2019-03-13 20:18:27,308 : question-question : pearson = 0.0532, spearman = 0.1256
2019-03-13 20:18:27,308 : ALL (weighted average) : Pearson = 0.4408,             Spearman = 0.4712
2019-03-13 20:18:27,308 : ALL (average) : Pearson = 0.4314,             Spearman = 0.4632

2019-03-13 20:18:27,308 : ***** Transfer task : MR *****


2019-03-13 20:18:27,327 : loading BERT model bert-large-uncased
2019-03-13 20:18:27,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:18:27,346 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:18:27,346 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1zu259ph
2019-03-13 20:18:34,841 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:18:40,234 : Generating sentence embeddings
2019-03-13 20:19:11,629 : Generated sentence embeddings
2019-03-13 20:19:11,630 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:19:20,546 : Best param found at split 1: l2reg = 0.01                 with score 83.81
2019-03-13 20:19:28,321 : Best param found at split 2: l2reg = 1e-05                 with score 83.09
2019-03-13 20:19:35,486 : Best param found at split 3: l2reg = 0.001                 with score 83.72
2019-03-13 20:19:44,033 : Best param found at split 4: l2reg = 0.01                 with score 83.97
2019-03-13 20:19:53,925 : Best param found at split 5: l2reg = 0.0001                 with score 83.6
2019-03-13 20:19:54,576 : Dev acc : 83.64 Test acc : 83.54

2019-03-13 20:19:54,577 : ***** Transfer task : CR *****


2019-03-13 20:19:54,585 : loading BERT model bert-large-uncased
2019-03-13 20:19:54,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:19:54,605 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:19:54,605 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0zegp4qh
2019-03-13 20:20:02,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:20:07,437 : Generating sentence embeddings
2019-03-13 20:20:15,752 : Generated sentence embeddings
2019-03-13 20:20:15,752 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:20:18,989 : Best param found at split 1: l2reg = 0.01                 with score 89.5
2019-03-13 20:20:22,612 : Best param found at split 2: l2reg = 0.0001                 with score 88.74
2019-03-13 20:20:26,356 : Best param found at split 3: l2reg = 1e-05                 with score 89.3
2019-03-13 20:20:29,928 : Best param found at split 4: l2reg = 0.0001                 with score 89.11
2019-03-13 20:20:33,665 : Best param found at split 5: l2reg = 0.01                 with score 89.28
2019-03-13 20:20:33,881 : Dev acc : 89.19 Test acc : 88.0

2019-03-13 20:20:33,882 : ***** Transfer task : MPQA *****


2019-03-13 20:20:33,888 : loading BERT model bert-large-uncased
2019-03-13 20:20:33,888 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:20:33,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:20:33,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4deh2vl5
2019-03-13 20:20:41,402 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:20:46,859 : Generating sentence embeddings
2019-03-13 20:20:54,409 : Generated sentence embeddings
2019-03-13 20:20:54,409 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:21:03,824 : Best param found at split 1: l2reg = 0.001                 with score 83.14
2019-03-13 20:21:14,985 : Best param found at split 2: l2reg = 0.001                 with score 82.84
2019-03-13 20:21:25,553 : Best param found at split 3: l2reg = 0.0001                 with score 83.75
2019-03-13 20:21:36,765 : Best param found at split 4: l2reg = 0.01                 with score 83.61
2019-03-13 20:21:45,865 : Best param found at split 5: l2reg = 0.01                 with score 82.55
2019-03-13 20:21:46,706 : Dev acc : 83.18 Test acc : 84.25

2019-03-13 20:21:46,707 : ***** Transfer task : SUBJ *****


2019-03-13 20:21:46,723 : loading BERT model bert-large-uncased
2019-03-13 20:21:46,723 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:21:46,742 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:21:46,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpovh4bmgs
2019-03-13 20:21:54,234 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:21:59,698 : Generating sentence embeddings
2019-03-13 20:22:30,585 : Generated sentence embeddings
2019-03-13 20:22:30,585 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:22:38,955 : Best param found at split 1: l2reg = 0.01                 with score 95.74
2019-03-13 20:22:49,721 : Best param found at split 2: l2reg = 0.01                 with score 95.75
2019-03-13 20:23:00,627 : Best param found at split 3: l2reg = 0.01                 with score 95.79
2019-03-13 20:23:10,709 : Best param found at split 4: l2reg = 0.001                 with score 95.9
2019-03-13 20:23:21,236 : Best param found at split 5: l2reg = 0.001                 with score 95.84
2019-03-13 20:23:21,877 : Dev acc : 95.8 Test acc : 95.36

2019-03-13 20:23:21,878 : ***** Transfer task : SST Binary classification *****


2019-03-13 20:23:22,006 : loading BERT model bert-large-uncased
2019-03-13 20:23:22,007 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:23:22,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:23:22,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwqcp3iht
2019-03-13 20:23:29,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:23:34,904 : Computing embedding for train
2019-03-13 20:25:14,687 : Computed train embeddings
2019-03-13 20:25:14,687 : Computing embedding for dev
2019-03-13 20:25:16,855 : Computed dev embeddings
2019-03-13 20:25:16,855 : Computing embedding for test
2019-03-13 20:25:21,415 : Computed test embeddings
2019-03-13 20:25:21,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:25:37,790 : [('reg:1e-05', 87.5), ('reg:0.0001', 87.39), ('reg:0.001', 87.96), ('reg:0.01', 87.5)]
2019-03-13 20:25:37,790 : Validation : best param found is reg = 0.001 with score             87.96
2019-03-13 20:25:37,791 : Evaluating...
2019-03-13 20:25:42,042 : 
Dev acc : 87.96 Test acc : 88.8 for             SST Binary classification

2019-03-13 20:25:42,042 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 20:25:42,097 : loading BERT model bert-large-uncased
2019-03-13 20:25:42,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:25:42,117 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:25:42,117 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnxhk0fn3
2019-03-13 20:25:49,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:25:54,953 : Computing embedding for train
2019-03-13 20:26:16,779 : Computed train embeddings
2019-03-13 20:26:16,779 : Computing embedding for dev
2019-03-13 20:26:19,625 : Computed dev embeddings
2019-03-13 20:26:19,625 : Computing embedding for test
2019-03-13 20:26:25,233 : Computed test embeddings
2019-03-13 20:26:25,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:26:27,671 : [('reg:1e-05', 43.23), ('reg:0.0001', 39.51), ('reg:0.001', 42.87), ('reg:0.01', 43.32)]
2019-03-13 20:26:27,671 : Validation : best param found is reg = 0.01 with score             43.32
2019-03-13 20:26:27,671 : Evaluating...
2019-03-13 20:26:28,219 : 
Dev acc : 43.32 Test acc : 43.8 for             SST Fine-Grained classification

2019-03-13 20:26:28,219 : ***** Transfer task : TREC *****


2019-03-13 20:26:28,233 : loading BERT model bert-large-uncased
2019-03-13 20:26:28,234 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:26:28,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:26:28,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiurzsdv5
2019-03-13 20:26:35,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:26:48,757 : Computed train embeddings
2019-03-13 20:26:49,344 : Computed test embeddings
2019-03-13 20:26:49,344 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 20:26:55,834 : [('reg:1e-05', 71.66), ('reg:0.0001', 74.02), ('reg:0.001', 75.26), ('reg:0.01', 76.63)]
2019-03-13 20:26:55,834 : Cross-validation : best param found is reg = 0.01             with score 76.63
2019-03-13 20:26:55,834 : Evaluating...
2019-03-13 20:26:56,238 : 
Dev acc : 76.63 Test acc : 82.2             for TREC

2019-03-13 20:26:56,238 : ***** Transfer task : MRPC *****


2019-03-13 20:26:56,260 : loading BERT model bert-large-uncased
2019-03-13 20:26:56,260 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:26:56,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:26:56,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp226m7_j3
2019-03-13 20:27:03,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:27:08,902 : Computing embedding for train
2019-03-13 20:27:31,141 : Computed train embeddings
2019-03-13 20:27:31,141 : Computing embedding for test
2019-03-13 20:27:40,853 : Computed test embeddings
2019-03-13 20:27:40,874 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 20:27:45,718 : [('reg:1e-05', 71.07), ('reg:0.0001', 70.24), ('reg:0.001', 69.9), ('reg:0.01', 70.19)]
2019-03-13 20:27:45,719 : Cross-validation : best param found is reg = 1e-05             with score 71.07
2019-03-13 20:27:45,719 : Evaluating...
2019-03-13 20:27:45,989 : Dev acc : 71.07 Test acc 68.58; Test F1 75.56 for MRPC.

2019-03-13 20:27:45,989 : ***** Transfer task : SICK-Entailment*****


2019-03-13 20:27:46,014 : loading BERT model bert-large-uncased
2019-03-13 20:27:46,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:27:46,033 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:27:46,033 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplxv9iobj
2019-03-13 20:27:53,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:27:58,722 : Computing embedding for train
2019-03-13 20:28:09,997 : Computed train embeddings
2019-03-13 20:28:09,997 : Computing embedding for dev
2019-03-13 20:28:11,533 : Computed dev embeddings
2019-03-13 20:28:11,533 : Computing embedding for test
2019-03-13 20:28:23,592 : Computed test embeddings
2019-03-13 20:28:23,630 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:28:25,123 : [('reg:1e-05', 71.4), ('reg:0.0001', 68.4), ('reg:0.001', 66.2), ('reg:0.01', 68.8)]
2019-03-13 20:28:25,123 : Validation : best param found is reg = 1e-05 with score             71.4
2019-03-13 20:28:25,123 : Evaluating...
2019-03-13 20:28:25,536 : 
Dev acc : 71.4 Test acc : 69.23 for                        SICK entailment

2019-03-13 20:28:25,536 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 20:28:25,607 : loading BERT model bert-large-uncased
2019-03-13 20:28:25,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:28:25,627 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:28:25,627 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9go74n1c
2019-03-13 20:28:33,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:28:38,406 : Computing embedding for train
2019-03-13 20:28:49,692 : Computed train embeddings
2019-03-13 20:28:49,693 : Computing embedding for dev
2019-03-13 20:28:51,229 : Computed dev embeddings
2019-03-13 20:28:51,229 : Computing embedding for test
2019-03-13 20:29:03,311 : Computed test embeddings
2019-03-13 20:29:18,100 : Dev : Pearson 0.5939061056153461
2019-03-13 20:29:18,100 : Test : Pearson 0.6301588699453607 Spearman 0.5924560534218137 MSE 0.6245202926293457                        for SICK Relatedness

2019-03-13 20:29:18,101 : 

***** Transfer task : STSBenchmark*****


2019-03-13 20:29:18,141 : loading BERT model bert-large-uncased
2019-03-13 20:29:18,141 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:29:18,172 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:29:18,172 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl6mcdoln
2019-03-13 20:29:25,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:29:30,924 : Computing embedding for train
2019-03-13 20:29:49,449 : Computed train embeddings
2019-03-13 20:29:49,449 : Computing embedding for dev
2019-03-13 20:29:55,057 : Computed dev embeddings
2019-03-13 20:29:55,057 : Computing embedding for test
2019-03-13 20:29:59,636 : Computed test embeddings
2019-03-13 20:30:14,583 : Dev : Pearson 0.3903191895552001
2019-03-13 20:30:14,583 : Test : Pearson 0.4360836891370011 Spearman 0.4333738930062394 MSE 1.9896772512016103                        for SICK Relatedness

2019-03-13 20:30:14,583 : ***** Transfer task : SNLI Entailment*****


2019-03-13 20:30:19,357 : loading BERT model bert-large-uncased
2019-03-13 20:30:19,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:30:19,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:30:19,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18w0kkfc
2019-03-13 20:30:26,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:30:33,688 : PROGRESS (encoding): 0.00%
2019-03-13 20:33:18,267 : PROGRESS (encoding): 14.56%
2019-03-13 20:36:24,670 : PROGRESS (encoding): 29.12%
2019-03-13 20:39:31,653 : PROGRESS (encoding): 43.69%
2019-03-13 20:42:51,108 : PROGRESS (encoding): 58.25%
2019-03-13 20:46:33,302 : PROGRESS (encoding): 72.81%
2019-03-13 20:50:14,363 : PROGRESS (encoding): 87.37%
2019-03-13 20:54:13,627 : PROGRESS (encoding): 0.00%
2019-03-13 20:54:43,758 : PROGRESS (encoding): 0.00%
2019-03-13 20:55:12,703 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:55:44,803 : [('reg:1e-09', 55.16)]
2019-03-13 20:55:44,803 : Validation : best param found is reg = 1e-09 with score             55.16
2019-03-13 20:55:44,803 : Evaluating...
2019-03-13 20:56:14,067 : Dev acc : 55.16 Test acc : 55.49 for SNLI

2019-03-13 20:56:14,067 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 20:56:14,276 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 20:56:15,258 : loading BERT model bert-large-uncased
2019-03-13 20:56:15,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:56:15,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:56:15,286 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprl60s9cb
2019-03-13 20:56:22,739 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:56:28,062 : Computing embeddings for train/dev/test
2019-03-13 20:59:57,440 : Computed embeddings
2019-03-13 20:59:57,440 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:00:24,744 : [('reg:1e-05', 57.26), ('reg:0.0001', 58.34), ('reg:0.001', 51.49), ('reg:0.01', 44.06)]
2019-03-13 21:00:24,745 : Validation : best param found is reg = 0.0001 with score             58.34
2019-03-13 21:00:24,745 : Evaluating...
2019-03-13 21:00:31,229 : 
Dev acc : 58.3 Test acc : 57.9 for LENGTH classification

2019-03-13 21:00:31,230 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 21:00:31,595 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 21:00:31,642 : loading BERT model bert-large-uncased
2019-03-13 21:00:31,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:00:31,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:00:31,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz2q1eapt
2019-03-13 21:00:39,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:00:44,558 : Computing embeddings for train/dev/test
2019-03-13 21:03:57,640 : Computed embeddings
2019-03-13 21:03:57,640 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:04:31,063 : [('reg:1e-05', 25.9), ('reg:0.0001', 11.4), ('reg:0.001', 1.85), ('reg:0.01', 0.39)]
2019-03-13 21:04:31,063 : Validation : best param found is reg = 1e-05 with score             25.9
2019-03-13 21:04:31,063 : Evaluating...
2019-03-13 21:04:41,134 : 
Dev acc : 25.9 Test acc : 26.0 for WORDCONTENT classification

2019-03-13 21:04:41,135 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 21:04:41,487 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 21:04:41,552 : loading BERT model bert-large-uncased
2019-03-13 21:04:41,552 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:04:41,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:04:41,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7fn0hslx
2019-03-13 21:04:49,040 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:04:54,408 : Computing embeddings for train/dev/test
2019-03-13 21:07:55,813 : Computed embeddings
2019-03-13 21:07:55,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:08:25,987 : [('reg:1e-05', 25.82), ('reg:0.0001', 25.0), ('reg:0.001', 25.94), ('reg:0.01', 23.33)]
2019-03-13 21:08:25,987 : Validation : best param found is reg = 0.001 with score             25.94
2019-03-13 21:08:25,987 : Evaluating...
2019-03-13 21:08:32,544 : 
Dev acc : 25.9 Test acc : 26.0 for DEPTH classification

2019-03-13 21:08:32,545 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 21:08:32,920 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 21:08:32,983 : loading BERT model bert-large-uncased
2019-03-13 21:08:32,983 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:08:33,090 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:08:33,090 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1tr2251e
2019-03-13 21:08:40,525 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:08:45,898 : Computing embeddings for train/dev/test
2019-03-13 21:11:34,025 : Computed embeddings
2019-03-13 21:11:34,025 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:12:04,788 : [('reg:1e-05', 60.37), ('reg:0.0001', 58.76), ('reg:0.001', 53.71), ('reg:0.01', 47.98)]
2019-03-13 21:12:04,789 : Validation : best param found is reg = 1e-05 with score             60.37
2019-03-13 21:12:04,789 : Evaluating...
2019-03-13 21:12:10,134 : 
Dev acc : 60.4 Test acc : 60.8 for TOPCONSTITUENTS classification

2019-03-13 21:12:10,135 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 21:12:10,474 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 21:12:10,540 : loading BERT model bert-large-uncased
2019-03-13 21:12:10,540 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:12:10,658 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:12:10,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppnpxmtx6
2019-03-13 21:12:18,119 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:12:23,570 : Computing embeddings for train/dev/test
2019-03-13 21:15:26,206 : Computed embeddings
2019-03-13 21:15:26,206 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:15:52,677 : [('reg:1e-05', 86.89), ('reg:0.0001', 86.74), ('reg:0.001', 86.9), ('reg:0.01', 85.99)]
2019-03-13 21:15:52,677 : Validation : best param found is reg = 0.001 with score             86.9
2019-03-13 21:15:52,677 : Evaluating...
2019-03-13 21:15:59,164 : 
Dev acc : 86.9 Test acc : 86.4 for BIGRAMSHIFT classification

2019-03-13 21:15:59,165 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 21:15:59,724 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 21:15:59,790 : loading BERT model bert-large-uncased
2019-03-13 21:15:59,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:15:59,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:15:59,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf_kf5a_d
2019-03-13 21:16:07,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:16:12,747 : Computing embeddings for train/dev/test
2019-03-13 21:19:11,506 : Computed embeddings
2019-03-13 21:19:11,506 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:19:33,251 : [('reg:1e-05', 89.6), ('reg:0.0001', 90.13), ('reg:0.001', 88.9), ('reg:0.01', 86.46)]
2019-03-13 21:19:33,251 : Validation : best param found is reg = 0.0001 with score             90.13
2019-03-13 21:19:33,251 : Evaluating...
2019-03-13 21:19:39,622 : 
Dev acc : 90.1 Test acc : 88.6 for TENSE classification

2019-03-13 21:19:39,623 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 21:19:40,007 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 21:19:40,074 : loading BERT model bert-large-uncased
2019-03-13 21:19:40,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:19:40,100 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:19:40,101 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi4sgiflp
2019-03-13 21:19:47,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:19:52,952 : Computing embeddings for train/dev/test
2019-03-13 21:23:02,230 : Computed embeddings
2019-03-13 21:23:02,231 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:23:29,026 : [('reg:1e-05', 82.27), ('reg:0.0001', 82.5), ('reg:0.001', 82.12), ('reg:0.01', 77.88)]
2019-03-13 21:23:29,026 : Validation : best param found is reg = 0.0001 with score             82.5
2019-03-13 21:23:29,026 : Evaluating...
2019-03-13 21:23:35,413 : 
Dev acc : 82.5 Test acc : 82.0 for SUBJNUMBER classification

2019-03-13 21:23:35,414 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 21:23:35,828 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 21:23:35,895 : loading BERT model bert-large-uncased
2019-03-13 21:23:35,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:23:35,923 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:23:35,923 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyahon8m9
2019-03-13 21:23:43,384 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:23:48,741 : Computing embeddings for train/dev/test
2019-03-13 21:26:54,687 : Computed embeddings
2019-03-13 21:26:54,687 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:27:23,808 : [('reg:1e-05', 73.33), ('reg:0.0001', 73.22), ('reg:0.001', 71.07), ('reg:0.01', 69.44)]
2019-03-13 21:27:23,809 : Validation : best param found is reg = 1e-05 with score             73.33
2019-03-13 21:27:23,809 : Evaluating...
2019-03-13 21:27:31,374 : 
Dev acc : 73.3 Test acc : 74.5 for OBJNUMBER classification

2019-03-13 21:27:31,375 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 21:27:31,765 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 21:27:31,833 : loading BERT model bert-large-uncased
2019-03-13 21:27:31,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:27:31,955 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:27:31,955 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjn7_y9q8
2019-03-13 21:27:39,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:27:44,799 : Computing embeddings for train/dev/test
2019-03-13 21:31:20,299 : Computed embeddings
2019-03-13 21:31:20,299 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:31:42,675 : [('reg:1e-05', 63.9), ('reg:0.0001', 63.9), ('reg:0.001', 64.02), ('reg:0.01', 62.58)]
2019-03-13 21:31:42,675 : Validation : best param found is reg = 0.001 with score             64.02
2019-03-13 21:31:42,675 : Evaluating...
2019-03-13 21:31:47,038 : 
Dev acc : 64.0 Test acc : 65.2 for ODDMANOUT classification

2019-03-13 21:31:47,039 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 21:31:47,433 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 21:31:47,509 : loading BERT model bert-large-uncased
2019-03-13 21:31:47,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:31:47,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:31:47,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi3jcgjwx
2019-03-13 21:31:55,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:32:00,559 : Computing embeddings for train/dev/test
2019-03-13 21:35:33,791 : Computed embeddings
2019-03-13 21:35:33,791 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:36:03,206 : [('reg:1e-05', 69.17), ('reg:0.0001', 69.07), ('reg:0.001', 68.14), ('reg:0.01', 62.37)]
2019-03-13 21:36:03,206 : Validation : best param found is reg = 1e-05 with score             69.17
2019-03-13 21:36:03,206 : Evaluating...
2019-03-13 21:36:10,743 : 
Dev acc : 69.2 Test acc : 69.0 for COORDINATIONINVERSION classification

2019-03-13 21:36:10,745 : total results: {'STS12': {'MSRpar': {'pearson': (0.21978939913151024, 1.1745214651565226e-09), 'spearman': SpearmanrResult(correlation=0.272024585247062, pvalue=3.437464824228471e-14), 'nsamples': 750}, 'MSRvid': {'pearson': (0.05233543901540699, 0.1521851018236267), 'spearman': SpearmanrResult(correlation=0.1441471819920707, pvalue=7.440027012259694e-05), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.31633182235133345, 3.986825594839178e-12), 'spearman': SpearmanrResult(correlation=0.43702678990855753, pvalue=7.843035425214172e-23), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3154822823711898, 8.559446629262395e-19), 'spearman': SpearmanrResult(correlation=0.3648732369348016, pvalue=4.935504366553306e-25), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4632747980077161, 1.2749582144652125e-22), 'spearman': SpearmanrResult(correlation=0.4878850109756858, pvalue=2.9853989470644993e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.2734427481754313, 'wmean': 0.24798851069913805}, 'spearman': {'mean': 0.34119136101163555, 'wmean': 0.31565159880237365}}}, 'STS13': {'FNWN': {'pearson': (0.031311471948128965, 0.6688616833620484), 'spearman': SpearmanrResult(correlation=0.04654634750006536, pvalue=0.524772375245596), 'nsamples': 189}, 'headlines': {'pearson': (0.38653947989903487, 3.883890611950693e-28), 'spearman': SpearmanrResult(correlation=0.4193488272444433, pvalue=2.6623214981703158e-33), 'nsamples': 750}, 'OnWN': {'pearson': (0.19397231402708484, 3.6903680805020607e-06), 'spearman': SpearmanrResult(correlation=0.23206718428774073, pvalue=2.6909507075982683e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.20394108862474955, 'wmean': 0.2697606308611114}, 'spearman': {'mean': 0.23265411967741645, 'wmean': 0.3023323803308449}}}, 'STS14': {'deft-forum': {'pearson': (-0.04571824356785362, 0.333224687475832), 'spearman': SpearmanrResult(correlation=-0.03394432441857627, pvalue=0.47258951794806203), 'nsamples': 450}, 'deft-news': {'pearson': (0.48094787155775043, 9.006287484821296e-19), 'spearman': SpearmanrResult(correlation=0.4569919980283243, pvalue=6.926006898767358e-17), 'nsamples': 300}, 'headlines': {'pearson': (0.3884524117062455, 2.012874834518373e-28), 'spearman': SpearmanrResult(correlation=0.3761382107712844, pvalue=1.2844456383342318e-26), 'nsamples': 750}, 'images': {'pearson': (0.16530963003090215, 5.339508478606907e-06), 'spearman': SpearmanrResult(correlation=0.19602134907994692, pvalue=6.23596013258363e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.2961241831164594, 1.2047453344430916e-16), 'spearman': SpearmanrResult(correlation=0.3088242966470564, pvalue=4.892298499732175e-18), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19359153288301356, 9.116418094420844e-08), 'spearman': SpearmanrResult(correlation=0.2961519601434251, pvalue=1.1965383579193062e-16), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2464512309544196, 'wmean': 0.24168519204380173}, 'spearman': {'mean': 0.2666972483752435, 'wmean': 0.26791320424037934}}}, 'STS15': {'answers-forums': {'pearson': (0.31829034443467585, 2.82880252072557e-10), 'spearman': SpearmanrResult(correlation=0.34888240338898774, pvalue=3.5708943108672986e-12), 'nsamples': 375}, 'answers-students': {'pearson': (0.40583849385270043, 4.1854352380121685e-31), 'spearman': SpearmanrResult(correlation=0.43154860011858265, pvalue=2.260004447892593e-35), 'nsamples': 750}, 'belief': {'pearson': (0.3329621277939128, 3.691037988296231e-11), 'spearman': SpearmanrResult(correlation=0.38223630934536923, pvalue=1.7136029905789856e-14), 'nsamples': 375}, 'headlines': {'pearson': (0.4353424277229971, 4.927028019327741e-36), 'spearman': SpearmanrResult(correlation=0.4367898686885356, pvalue=2.7414401021083192e-36), 'nsamples': 750}, 'images': {'pearson': (0.21638898950034355, 2.134198157907915e-09), 'spearman': SpearmanrResult(correlation=0.23134444809871296, pvalue=1.4339102173531985e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3417644766609259, 'wmean': 0.34579903679758384}, 'spearman': {'mean': 0.3661603259280376, 'wmean': 0.3663105683182525}}}, 'STS16': {'answer-answer': {'pearson': (0.37543995737356545, 6.34453954287198e-10), 'spearman': SpearmanrResult(correlation=0.3774289918687757, pvalue=5.066956509111115e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.49353700508795545, 1.0811640331112124e-16), 'spearman': SpearmanrResult(correlation=0.5092301956105691, pvalue=7.81817190351096e-18), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5644714543052938, 9.391005587943112e-21), 'spearman': SpearmanrResult(correlation=0.5849440342469863, pvalue=1.6503325812664798e-22), 'nsamples': 230}, 'postediting': {'pearson': (0.6701856668809595, 3.5186801885580614e-33), 'spearman': SpearmanrResult(correlation=0.7187325269758194, pvalue=4.5188819329426534e-40), 'nsamples': 244}, 'question-question': {'pearson': (0.05322631984196504, 0.44402633485403076), 'spearman': SpearmanrResult(correlation=0.12562601351170977, pvalue=0.0699168357674725), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4313720806979478, 'wmean': 0.4407508444316432}, 'spearman': {'mean': 0.46319235244277196, 'wmean': 0.471188013427112}}}, 'MR': {'devacc': 83.64, 'acc': 83.54, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 89.19, 'acc': 88.0, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.18, 'acc': 84.25, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.8, 'acc': 95.36, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.96, 'acc': 88.8, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.32, 'acc': 43.8, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.63, 'acc': 82.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.07, 'acc': 68.58, 'f1': 75.56, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.4, 'acc': 69.23, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.5939061056153461, 'pearson': 0.6301588699453607, 'spearman': 0.5924560534218137, 'mse': 0.6245202926293457, 'yhat': array([3.28864483, 4.56587353, 3.74165625, ..., 3.17635217, 4.13193818,        3.96811272]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.3903191895552001, 'pearson': 0.4360836891370011, 'spearman': 0.4333738930062394, 'mse': 1.9896772512016103, 'yhat': array([2.11040378, 2.1138225 , 1.80418386, ..., 3.62655457, 2.11064866,        3.04651299]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 55.16, 'acc': 55.49, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 58.34, 'acc': 57.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.9, 'acc': 26.01, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.94, 'acc': 25.97, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 60.37, 'acc': 60.8, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.9, 'acc': 86.38, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.13, 'acc': 88.62, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.5, 'acc': 82.01, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.33, 'acc': 74.45, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.02, 'acc': 65.15, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.17, 'acc': 69.0, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 21:36:10,745 : STS12 p=0.2480, STS12 s=0.3157, STS13 p=0.2698, STS13 s=0.3023, STS14 p=0.2417, STS14 s=0.2679, STS15 p=0.3458, STS15 s=0.3663, STS 16 p=0.4408, STS16 s=0.4712, STS B p=0.4361, STS B s=0.4334, STS B m=1.9897, SICK-R p=0.6302, SICK-R s=0.5925, SICK-P m=0.6245
2019-03-13 21:36:10,745 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 21:36:10,745 : 0.2480,0.3157,0.2698,0.3023,0.2417,0.2679,0.3458,0.3663,0.4408,0.4712,0.4361,0.4334,1.9897,0.6302,0.5925,0.6245
2019-03-13 21:36:10,745 : MR=83.54, CR=88.00, SUBJ=95.36, MPQA=84.25, SST-B=88.80, SST-F=43.80, TREC=82.20, SICK-E=69.23, SNLI=55.49, MRPC=68.58, MRPC f=75.56
2019-03-13 21:36:10,745 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 21:36:10,745 : 83.54,88.00,95.36,84.25,88.80,43.80,82.20,69.23,55.49,68.58,75.56
2019-03-13 21:36:10,745 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 21:36:10,745 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 21:36:10,745 : na,na,na,na,na,na,na,na,na,na
2019-03-13 21:36:10,745 : SentLen=57.86, WC=26.01, TreeDepth=25.97, TopConst=60.80, BShift=86.38, Tense=88.62, SubjNum=82.01, ObjNum=74.45, SOMO=65.15, CoordInv=69.00, average=63.62
2019-03-13 21:36:10,745 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 21:36:10,745 : 57.86,26.01,25.97,60.80,86.38,88.62,82.01,74.45,65.15,69.00,63.62
2019-03-13 21:36:10,745 : ********************************************************************************
2019-03-13 21:36:10,745 : ********************************************************************************
2019-03-13 21:36:10,745 : ********************************************************************************
2019-03-13 21:36:10,745 : layer 24
2019-03-13 21:36:10,746 : ********************************************************************************
2019-03-13 21:36:10,746 : ********************************************************************************
2019-03-13 21:36:10,746 : ********************************************************************************
2019-03-13 21:36:10,836 : ***** Transfer task : STS12 *****


2019-03-13 21:36:10,874 : loading BERT model bert-large-uncased
2019-03-13 21:36:10,874 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:36:10,890 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:36:10,890 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcq4wt17f
2019-03-13 21:36:18,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:36:27,672 : MSRpar : pearson = 0.1559, spearman = 0.2065
2019-03-13 21:36:29,306 : MSRvid : pearson = 0.0048, spearman = 0.1370
2019-03-13 21:36:30,713 : SMTeuroparl : pearson = 0.1982, spearman = 0.3875
2019-03-13 21:36:33,401 : surprise.OnWN : pearson = 0.2031, spearman = 0.2868
2019-03-13 21:36:34,822 : surprise.SMTnews : pearson = 0.2505, spearman = 0.3714
2019-03-13 21:36:34,823 : ALL (weighted average) : Pearson = 0.1492,             Spearman = 0.2570
2019-03-13 21:36:34,823 : ALL (average) : Pearson = 0.1625,             Spearman = 0.2778

2019-03-13 21:36:34,823 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 21:36:34,832 : loading BERT model bert-large-uncased
2019-03-13 21:36:34,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:36:34,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:36:34,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_sigrfah
2019-03-13 21:36:42,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:36:49,018 : FNWN : pearson = -0.0203, spearman = -0.0195
2019-03-13 21:36:50,906 : headlines : pearson = 0.3261, spearman = 0.3207
2019-03-13 21:36:52,370 : OnWN : pearson = 0.1156, spearman = 0.1676
2019-03-13 21:36:52,370 : ALL (weighted average) : Pearson = 0.2037,             Spearman = 0.2206
2019-03-13 21:36:52,370 : ALL (average) : Pearson = 0.1405,             Spearman = 0.1563

2019-03-13 21:36:52,370 : ***** Transfer task : STS14 *****


2019-03-13 21:36:52,413 : loading BERT model bert-large-uncased
2019-03-13 21:36:52,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:36:52,431 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:36:52,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaem_f11m
2019-03-13 21:36:59,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:37:06,806 : deft-forum : pearson = -0.0607, spearman = -0.0571
2019-03-13 21:37:08,437 : deft-news : pearson = 0.3780, spearman = 0.3852
2019-03-13 21:37:10,601 : headlines : pearson = 0.2779, spearman = 0.2775
2019-03-13 21:37:12,673 : images : pearson = 0.1336, spearman = 0.1758
2019-03-13 21:37:14,799 : OnWN : pearson = 0.1334, spearman = 0.1573
2019-03-13 21:37:17,652 : tweet-news : pearson = 0.1076, spearman = 0.1341
2019-03-13 21:37:17,652 : ALL (weighted average) : Pearson = 0.1535,             Spearman = 0.1729
2019-03-13 21:37:17,652 : ALL (average) : Pearson = 0.1616,             Spearman = 0.1788

2019-03-13 21:37:17,652 : ***** Transfer task : STS15 *****


2019-03-13 21:37:17,684 : loading BERT model bert-large-uncased
2019-03-13 21:37:17,684 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:37:17,736 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:37:17,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvp4kf0pw
2019-03-13 21:37:25,299 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:37:32,570 : answers-forums : pearson = 0.1786, spearman = 0.2352
2019-03-13 21:37:34,648 : answers-students : pearson = 0.2315, spearman = 0.2408
2019-03-13 21:37:36,694 : belief : pearson = 0.1966, spearman = 0.2494
2019-03-13 21:37:38,942 : headlines : pearson = 0.3452, spearman = 0.3406
2019-03-13 21:37:41,069 : images : pearson = 0.1245, spearman = 0.1782
2019-03-13 21:37:41,069 : ALL (weighted average) : Pearson = 0.2222,             Spearman = 0.2505
2019-03-13 21:37:41,069 : ALL (average) : Pearson = 0.2153,             Spearman = 0.2489

2019-03-13 21:37:41,069 : ***** Transfer task : STS16 *****


2019-03-13 21:37:41,107 : loading BERT model bert-large-uncased
2019-03-13 21:37:41,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:37:41,125 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:37:41,125 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiiz07xt0
2019-03-13 21:37:48,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:37:54,918 : answer-answer : pearson = 0.2150, spearman = 0.2518
2019-03-13 21:37:55,577 : headlines : pearson = 0.3164, spearman = 0.3444
2019-03-13 21:37:56,457 : plagiarism : pearson = 0.2486, spearman = 0.3912
2019-03-13 21:37:57,947 : postediting : pearson = 0.4489, spearman = 0.5763
2019-03-13 21:37:58,555 : question-question : pearson = 0.0479, spearman = 0.1042
2019-03-13 21:37:58,555 : ALL (weighted average) : Pearson = 0.2615,             Spearman = 0.3390
2019-03-13 21:37:58,555 : ALL (average) : Pearson = 0.2554,             Spearman = 0.3336

2019-03-13 21:37:58,555 : ***** Transfer task : MR *****


2019-03-13 21:37:58,601 : loading BERT model bert-large-uncased
2019-03-13 21:37:58,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:37:58,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:37:58,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9i62_43k
2019-03-13 21:38:06,128 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:38:11,750 : Generating sentence embeddings
2019-03-13 21:38:43,172 : Generated sentence embeddings
2019-03-13 21:38:43,173 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:38:53,769 : Best param found at split 1: l2reg = 1e-05                 with score 83.59
2019-03-13 21:39:04,462 : Best param found at split 2: l2reg = 0.001                 with score 83.14
2019-03-13 21:39:13,883 : Best param found at split 3: l2reg = 0.0001                 with score 83.13
2019-03-13 21:39:23,660 : Best param found at split 4: l2reg = 0.0001                 with score 83.22
2019-03-13 21:39:34,304 : Best param found at split 5: l2reg = 0.001                 with score 83.56
2019-03-13 21:39:34,832 : Dev acc : 83.33 Test acc : 82.95

2019-03-13 21:39:34,833 : ***** Transfer task : CR *****


2019-03-13 21:39:34,841 : loading BERT model bert-large-uncased
2019-03-13 21:39:34,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:39:34,890 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:39:34,890 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj8op6182
2019-03-13 21:39:42,360 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:39:47,789 : Generating sentence embeddings
2019-03-13 21:39:56,088 : Generated sentence embeddings
2019-03-13 21:39:56,088 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:39:59,521 : Best param found at split 1: l2reg = 0.001                 with score 88.94
2019-03-13 21:40:03,816 : Best param found at split 2: l2reg = 0.01                 with score 88.87
2019-03-13 21:40:07,607 : Best param found at split 3: l2reg = 0.0001                 with score 88.38
2019-03-13 21:40:11,045 : Best param found at split 4: l2reg = 1e-05                 with score 88.65
2019-03-13 21:40:15,204 : Best param found at split 5: l2reg = 0.0001                 with score 89.08
2019-03-13 21:40:15,485 : Dev acc : 88.78 Test acc : 88.29

2019-03-13 21:40:15,486 : ***** Transfer task : MPQA *****


2019-03-13 21:40:15,491 : loading BERT model bert-large-uncased
2019-03-13 21:40:15,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:40:15,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:40:15,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprjf49cj5
2019-03-13 21:40:22,932 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:40:28,254 : Generating sentence embeddings
2019-03-13 21:40:35,806 : Generated sentence embeddings
2019-03-13 21:40:35,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:40:46,711 : Best param found at split 1: l2reg = 1e-05                 with score 84.22
2019-03-13 21:40:57,291 : Best param found at split 2: l2reg = 1e-05                 with score 82.27
2019-03-13 21:41:07,577 : Best param found at split 3: l2reg = 0.01                 with score 83.37
2019-03-13 21:41:17,755 : Best param found at split 4: l2reg = 0.0001                 with score 84.47
2019-03-13 21:41:29,381 : Best param found at split 5: l2reg = 0.001                 with score 83.58
2019-03-13 21:41:30,077 : Dev acc : 83.58 Test acc : 84.27

2019-03-13 21:41:30,078 : ***** Transfer task : SUBJ *****


2019-03-13 21:41:30,094 : loading BERT model bert-large-uncased
2019-03-13 21:41:30,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:41:30,149 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:41:30,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe808s3_u
2019-03-13 21:41:37,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:41:43,081 : Generating sentence embeddings
2019-03-13 21:42:13,908 : Generated sentence embeddings
2019-03-13 21:42:13,909 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:42:23,974 : Best param found at split 1: l2reg = 0.001                 with score 95.44
2019-03-13 21:42:33,558 : Best param found at split 2: l2reg = 0.001                 with score 95.4
2019-03-13 21:42:41,902 : Best param found at split 3: l2reg = 1e-05                 with score 95.36
2019-03-13 21:42:51,727 : Best param found at split 4: l2reg = 0.001                 with score 95.81
2019-03-13 21:42:59,913 : Best param found at split 5: l2reg = 1e-05                 with score 95.52
2019-03-13 21:43:00,442 : Dev acc : 95.51 Test acc : 95.16

2019-03-13 21:43:00,443 : ***** Transfer task : SST Binary classification *****


2019-03-13 21:43:00,602 : loading BERT model bert-large-uncased
2019-03-13 21:43:00,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:43:00,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:43:00,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp559ilni_
2019-03-13 21:43:08,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:43:13,525 : Computing embedding for train
2019-03-13 21:44:53,245 : Computed train embeddings
2019-03-13 21:44:53,245 : Computing embedding for dev
2019-03-13 21:44:55,416 : Computed dev embeddings
2019-03-13 21:44:55,416 : Computing embedding for test
2019-03-13 21:44:59,974 : Computed test embeddings
2019-03-13 21:44:59,974 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:45:19,750 : [('reg:1e-05', 87.96), ('reg:0.0001', 87.84), ('reg:0.001', 87.96), ('reg:0.01', 85.89)]
2019-03-13 21:45:19,750 : Validation : best param found is reg = 1e-05 with score             87.96
2019-03-13 21:45:19,750 : Evaluating...
2019-03-13 21:45:24,838 : 
Dev acc : 87.96 Test acc : 88.63 for             SST Binary classification

2019-03-13 21:45:24,838 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 21:45:24,893 : loading BERT model bert-large-uncased
2019-03-13 21:45:24,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:45:24,918 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:45:24,918 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz9_dtiaz
2019-03-13 21:45:32,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:45:37,796 : Computing embedding for train
2019-03-13 21:45:59,617 : Computed train embeddings
2019-03-13 21:45:59,617 : Computing embedding for dev
2019-03-13 21:46:02,461 : Computed dev embeddings
2019-03-13 21:46:02,461 : Computing embedding for test
2019-03-13 21:46:08,063 : Computed test embeddings
2019-03-13 21:46:08,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:46:10,569 : [('reg:1e-05', 43.69), ('reg:0.0001', 40.15), ('reg:0.001', 43.78), ('reg:0.01', 42.51)]
2019-03-13 21:46:10,569 : Validation : best param found is reg = 0.001 with score             43.78
2019-03-13 21:46:10,569 : Evaluating...
2019-03-13 21:46:11,213 : 
Dev acc : 43.78 Test acc : 45.02 for             SST Fine-Grained classification

2019-03-13 21:46:11,214 : ***** Transfer task : TREC *****


2019-03-13 21:46:11,227 : loading BERT model bert-large-uncased
2019-03-13 21:46:11,227 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:46:11,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:46:11,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_ogfkzuf
2019-03-13 21:46:18,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:46:31,661 : Computed train embeddings
2019-03-13 21:46:32,247 : Computed test embeddings
2019-03-13 21:46:32,247 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 21:46:38,129 : [('reg:1e-05', 68.39), ('reg:0.0001', 70.4), ('reg:0.001', 67.31), ('reg:0.01', 66.3)]
2019-03-13 21:46:38,129 : Cross-validation : best param found is reg = 0.0001             with score 70.4
2019-03-13 21:46:38,129 : Evaluating...
2019-03-13 21:46:38,545 : 
Dev acc : 70.4 Test acc : 86.4             for TREC

2019-03-13 21:46:38,545 : ***** Transfer task : MRPC *****


2019-03-13 21:46:38,567 : loading BERT model bert-large-uncased
2019-03-13 21:46:38,567 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:46:38,627 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:46:38,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgp3e_esk
2019-03-13 21:46:46,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:46:51,490 : Computing embedding for train
2019-03-13 21:47:13,622 : Computed train embeddings
2019-03-13 21:47:13,622 : Computing embedding for test
2019-03-13 21:47:23,310 : Computed test embeddings
2019-03-13 21:47:23,331 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 21:47:28,121 : [('reg:1e-05', 69.77), ('reg:0.0001', 69.09), ('reg:0.001', 69.48), ('reg:0.01', 69.85)]
2019-03-13 21:47:28,121 : Cross-validation : best param found is reg = 0.01             with score 69.85
2019-03-13 21:47:28,121 : Evaluating...
2019-03-13 21:47:28,346 : Dev acc : 69.85 Test acc 67.94; Test F1 75.54 for MRPC.

2019-03-13 21:47:28,346 : ***** Transfer task : SICK-Entailment*****


2019-03-13 21:47:28,370 : loading BERT model bert-large-uncased
2019-03-13 21:47:28,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:47:28,389 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:47:28,389 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc67iob1k
2019-03-13 21:47:35,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:47:41,300 : Computing embedding for train
2019-03-13 21:47:52,555 : Computed train embeddings
2019-03-13 21:47:52,556 : Computing embedding for dev
2019-03-13 21:47:54,090 : Computed dev embeddings
2019-03-13 21:47:54,091 : Computing embedding for test
2019-03-13 21:48:06,162 : Computed test embeddings
2019-03-13 21:48:06,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:48:08,060 : [('reg:1e-05', 70.6), ('reg:0.0001', 70.8), ('reg:0.001', 69.4), ('reg:0.01', 71.2)]
2019-03-13 21:48:08,061 : Validation : best param found is reg = 0.01 with score             71.2
2019-03-13 21:48:08,061 : Evaluating...
2019-03-13 21:48:08,463 : 
Dev acc : 71.2 Test acc : 68.03 for                        SICK entailment

2019-03-13 21:48:08,464 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 21:48:08,491 : loading BERT model bert-large-uncased
2019-03-13 21:48:08,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:48:08,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:48:08,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp013j89za
2019-03-13 21:48:16,012 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:48:21,441 : Computing embedding for train
2019-03-13 21:48:32,690 : Computed train embeddings
2019-03-13 21:48:32,690 : Computing embedding for dev
2019-03-13 21:48:34,222 : Computed dev embeddings
2019-03-13 21:48:34,222 : Computing embedding for test
2019-03-13 21:48:46,283 : Computed test embeddings
2019-03-13 21:49:00,136 : Dev : Pearson 0.565705299868513
2019-03-13 21:49:00,136 : Test : Pearson 0.6141470515928664 Spearman 0.5721520572741355 MSE 0.644530754904246                        for SICK Relatedness

2019-03-13 21:49:00,137 : 

***** Transfer task : STSBenchmark*****


2019-03-13 21:49:00,177 : loading BERT model bert-large-uncased
2019-03-13 21:49:00,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:49:00,240 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:49:00,240 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjex9hzj4
2019-03-13 21:49:07,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:49:13,058 : Computing embedding for train
2019-03-13 21:49:31,601 : Computed train embeddings
2019-03-13 21:49:31,601 : Computing embedding for dev
2019-03-13 21:49:37,214 : Computed dev embeddings
2019-03-13 21:49:37,214 : Computing embedding for test
2019-03-13 21:49:41,811 : Computed test embeddings
2019-03-13 21:49:56,810 : Dev : Pearson 0.39763022183541236
2019-03-13 21:49:56,810 : Test : Pearson 0.43107600903600907 Spearman 0.4336807893195036 MSE 1.938931826735549                        for SICK Relatedness

2019-03-13 21:49:56,811 : ***** Transfer task : SNLI Entailment*****


2019-03-13 21:50:01,698 : loading BERT model bert-large-uncased
2019-03-13 21:50:01,698 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:50:01,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:50:01,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3p3cwfuh
2019-03-13 21:50:09,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:50:15,152 : PROGRESS (encoding): 0.00%
2019-03-13 21:52:59,656 : PROGRESS (encoding): 14.56%
2019-03-13 21:56:06,083 : PROGRESS (encoding): 29.12%
2019-03-13 21:59:13,300 : PROGRESS (encoding): 43.69%
2019-03-13 22:02:33,115 : PROGRESS (encoding): 58.25%
2019-03-13 22:06:15,449 : PROGRESS (encoding): 72.81%
2019-03-13 22:09:56,620 : PROGRESS (encoding): 87.37%
2019-03-13 22:13:56,073 : PROGRESS (encoding): 0.00%
2019-03-13 22:14:26,137 : PROGRESS (encoding): 0.00%
2019-03-13 22:14:55,077 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:15:37,930 : [('reg:1e-09', 61.65)]
2019-03-13 22:15:37,930 : Validation : best param found is reg = 1e-09 with score             61.65
2019-03-13 22:15:37,930 : Evaluating...
2019-03-13 22:16:18,589 : Dev acc : 61.65 Test acc : 61.69 for SNLI

2019-03-13 22:16:18,589 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 22:16:18,820 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 22:16:19,912 : loading BERT model bert-large-uncased
2019-03-13 22:16:19,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:16:19,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:16:19,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi7q02okp
2019-03-13 22:16:27,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:16:32,769 : Computing embeddings for train/dev/test
2019-03-13 22:20:01,732 : Computed embeddings
2019-03-13 22:20:01,732 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:20:23,063 : [('reg:1e-05', 50.72), ('reg:0.0001', 53.27), ('reg:0.001', 47.31), ('reg:0.01', 35.32)]
2019-03-13 22:20:23,063 : Validation : best param found is reg = 0.0001 with score             53.27
2019-03-13 22:20:23,063 : Evaluating...
2019-03-13 22:20:29,618 : 
Dev acc : 53.3 Test acc : 53.8 for LENGTH classification

2019-03-13 22:20:29,618 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 22:20:29,985 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 22:20:30,031 : loading BERT model bert-large-uncased
2019-03-13 22:20:30,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:20:30,060 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:20:30,060 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn8ex154k
2019-03-13 22:20:37,502 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:20:42,875 : Computing embeddings for train/dev/test
2019-03-13 22:23:55,888 : Computed embeddings
2019-03-13 22:23:55,889 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:24:28,524 : [('reg:1e-05', 15.58), ('reg:0.0001', 6.19), ('reg:0.001', 1.0), ('reg:0.01', 0.22)]
2019-03-13 22:24:28,525 : Validation : best param found is reg = 1e-05 with score             15.58
2019-03-13 22:24:28,525 : Evaluating...
2019-03-13 22:24:39,455 : 
Dev acc : 15.6 Test acc : 15.9 for WORDCONTENT classification

2019-03-13 22:24:39,456 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 22:24:39,814 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 22:24:39,881 : loading BERT model bert-large-uncased
2019-03-13 22:24:39,881 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:24:39,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:24:39,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwfz__s2_
2019-03-13 22:24:47,398 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:24:52,842 : Computing embeddings for train/dev/test
2019-03-13 22:27:53,921 : Computed embeddings
2019-03-13 22:27:53,922 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:28:17,440 : [('reg:1e-05', 25.02), ('reg:0.0001', 24.27), ('reg:0.001', 26.42), ('reg:0.01', 22.56)]
2019-03-13 22:28:17,440 : Validation : best param found is reg = 0.001 with score             26.42
2019-03-13 22:28:17,440 : Evaluating...
2019-03-13 22:28:23,955 : 
Dev acc : 26.4 Test acc : 26.3 for DEPTH classification

2019-03-13 22:28:23,956 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 22:28:24,326 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 22:28:24,388 : loading BERT model bert-large-uncased
2019-03-13 22:28:24,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:28:24,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:28:24,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuwvazexg
2019-03-13 22:28:31,922 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:28:37,194 : Computing embeddings for train/dev/test
2019-03-13 22:31:25,236 : Computed embeddings
2019-03-13 22:31:25,236 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:31:59,416 : [('reg:1e-05', 61.02), ('reg:0.0001', 52.99), ('reg:0.001', 52.92), ('reg:0.01', 45.88)]
2019-03-13 22:31:59,417 : Validation : best param found is reg = 1e-05 with score             61.02
2019-03-13 22:31:59,417 : Evaluating...
2019-03-13 22:32:09,595 : 
Dev acc : 61.0 Test acc : 61.5 for TOPCONSTITUENTS classification

2019-03-13 22:32:09,596 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 22:32:09,970 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 22:32:10,036 : loading BERT model bert-large-uncased
2019-03-13 22:32:10,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:32:10,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:32:10,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdn1d62ou
2019-03-13 22:32:17,563 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:32:22,893 : Computing embeddings for train/dev/test
2019-03-13 22:35:25,570 : Computed embeddings
2019-03-13 22:35:25,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:35:54,595 : [('reg:1e-05', 85.9), ('reg:0.0001', 85.87), ('reg:0.001', 86.4), ('reg:0.01', 83.22)]
2019-03-13 22:35:54,595 : Validation : best param found is reg = 0.001 with score             86.4
2019-03-13 22:35:54,595 : Evaluating...
2019-03-13 22:36:03,212 : 
Dev acc : 86.4 Test acc : 86.2 for BIGRAMSHIFT classification

2019-03-13 22:36:03,213 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 22:36:03,614 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 22:36:03,684 : loading BERT model bert-large-uncased
2019-03-13 22:36:03,684 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:36:03,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:36:03,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj93q8e52
2019-03-13 22:36:11,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:36:16,452 : Computing embeddings for train/dev/test
2019-03-13 22:39:15,149 : Computed embeddings
2019-03-13 22:39:15,149 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:39:35,276 : [('reg:1e-05', 89.67), ('reg:0.0001', 89.93), ('reg:0.001', 86.36), ('reg:0.01', 90.1)]
2019-03-13 22:39:35,276 : Validation : best param found is reg = 0.01 with score             90.1
2019-03-13 22:39:35,276 : Evaluating...
2019-03-13 22:39:41,759 : 
Dev acc : 90.1 Test acc : 88.3 for TENSE classification

2019-03-13 22:39:41,760 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 22:39:42,162 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 22:39:42,224 : loading BERT model bert-large-uncased
2019-03-13 22:39:42,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:39:42,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:39:42,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3vbtglz2
2019-03-13 22:39:49,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:39:55,228 : Computing embeddings for train/dev/test
2019-03-13 22:43:04,554 : Computed embeddings
2019-03-13 22:43:04,554 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:43:26,733 : [('reg:1e-05', 79.59), ('reg:0.0001', 79.4), ('reg:0.001', 77.35), ('reg:0.01', 75.95)]
2019-03-13 22:43:26,733 : Validation : best param found is reg = 1e-05 with score             79.59
2019-03-13 22:43:26,733 : Evaluating...
2019-03-13 22:43:32,636 : 
Dev acc : 79.6 Test acc : 79.7 for SUBJNUMBER classification

2019-03-13 22:43:32,637 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 22:43:33,030 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 22:43:33,096 : loading BERT model bert-large-uncased
2019-03-13 22:43:33,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:43:33,209 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:43:33,209 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp993tra7j
2019-03-13 22:43:40,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:43:46,080 : Computing embeddings for train/dev/test
2019-03-13 22:46:51,783 : Computed embeddings
2019-03-13 22:46:51,783 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:47:17,375 : [('reg:1e-05', 72.05), ('reg:0.0001', 72.28), ('reg:0.001', 71.12), ('reg:0.01', 67.28)]
2019-03-13 22:47:17,375 : Validation : best param found is reg = 0.0001 with score             72.28
2019-03-13 22:47:17,375 : Evaluating...
2019-03-13 22:47:24,355 : 
Dev acc : 72.3 Test acc : 73.2 for OBJNUMBER classification

2019-03-13 22:47:24,356 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 22:47:24,919 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 22:47:24,987 : loading BERT model bert-large-uncased
2019-03-13 22:47:24,988 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:47:25,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:47:25,015 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpboe6tfib
2019-03-13 22:47:32,411 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:47:37,737 : Computing embeddings for train/dev/test
2019-03-13 22:51:13,105 : Computed embeddings
2019-03-13 22:51:13,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:51:35,949 : [('reg:1e-05', 63.84), ('reg:0.0001', 63.77), ('reg:0.001', 63.86), ('reg:0.01', 56.97)]
2019-03-13 22:51:35,949 : Validation : best param found is reg = 0.001 with score             63.86
2019-03-13 22:51:35,949 : Evaluating...
2019-03-13 22:51:41,323 : 
Dev acc : 63.9 Test acc : 65.0 for ODDMANOUT classification

2019-03-13 22:51:41,324 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 22:51:41,698 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 22:51:41,778 : loading BERT model bert-large-uncased
2019-03-13 22:51:41,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:51:41,808 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:51:41,808 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpboxdm7ks
2019-03-13 22:51:49,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:51:54,625 : Computing embeddings for train/dev/test
2019-03-13 22:55:27,880 : Computed embeddings
2019-03-13 22:55:27,880 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:55:51,474 : [('reg:1e-05', 63.02), ('reg:0.0001', 62.97), ('reg:0.001', 62.34), ('reg:0.01', 67.92)]
2019-03-13 22:55:51,474 : Validation : best param found is reg = 0.01 with score             67.92
2019-03-13 22:55:51,474 : Evaluating...
2019-03-13 22:55:56,146 : 
Dev acc : 67.9 Test acc : 67.8 for COORDINATIONINVERSION classification

2019-03-13 22:55:56,148 : total results: {'STS12': {'MSRpar': {'pearson': (0.15592302015775972, 1.7926472930428452e-05), 'spearman': SpearmanrResult(correlation=0.20647756689906951, pvalue=1.1512341304422444e-08), 'nsamples': 750}, 'MSRvid': {'pearson': (0.004761500812992833, 0.89642219046773), 'spearman': SpearmanrResult(correlation=0.13703290790087194, pvalue=0.00016700785652021635), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.19824620637828752, 1.883790128633361e-05), 'spearman': SpearmanrResult(correlation=0.3875017696378895, pvalue=6.7934672768351016e-18), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.20309631332336248, 2.007808393358923e-08), 'spearman': SpearmanrResult(correlation=0.28676870796791504, pvalue=1.1545770307643917e-15), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.2505337411539183, 3.9826446750071826e-07), 'spearman': SpearmanrResult(correlation=0.37135969346255976, pvalue=1.7112339354538474e-14), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.1625121563652642, 'wmean': 0.149225739114747}, 'spearman': {'mean': 0.2778281291736612, 'wmean': 0.25699653057633365}}}, 'STS13': {'FNWN': {'pearson': (-0.020309822843584683, 0.7814804895473517), 'spearman': SpearmanrResult(correlation=-0.019497881424804867, pvalue=0.7900092317450306), 'nsamples': 189}, 'headlines': {'pearson': (0.32609410534017125, 4.849422140296339e-20), 'spearman': SpearmanrResult(correlation=0.32074388865088643, pvalue=2.091626550531312e-19), 'nsamples': 750}, 'OnWN': {'pearson': (0.11557786372241649, 0.006132783078462023), 'spearman': SpearmanrResult(correlation=0.1676267975564459, pvalue=6.615061539961611e-05), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1404540487396677, 'wmean': 0.2037141360239777}, 'spearman': {'mean': 0.15629093492750915, 'wmean': 0.2206076335520286}}}, 'STS14': {'deft-forum': {'pearson': (-0.06066262534805532, 0.198983628820141), 'spearman': SpearmanrResult(correlation=-0.05707593172396629, pvalue=0.2269007230037145), 'nsamples': 450}, 'deft-news': {'pearson': (0.3780332068474567, 1.2579239269881412e-11), 'spearman': SpearmanrResult(correlation=0.3851730334269208, pvalue=4.7769082006231375e-12), 'nsamples': 300}, 'headlines': {'pearson': (0.27794321498579383, 9.020588644681956e-15), 'spearman': SpearmanrResult(correlation=0.27751563688924463, pvalue=9.946786998849826e-15), 'nsamples': 750}, 'images': {'pearson': (0.13360120343378168, 0.00024333173764025292), 'spearman': SpearmanrResult(correlation=0.1758134070363559, pvalue=1.268656945003648e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.1334013214871792, 0.0002486574129160289), 'spearman': SpearmanrResult(correlation=0.15728525724418746, pvalue=1.5100953059037009e-05), 'nsamples': 750}, 'tweet-news': {'pearson': (0.10755390209538358, 0.0031866919266121656), 'spearman': SpearmanrResult(correlation=0.1340952731335714, pvalue=0.0002306223604706923), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1616450372502566, 'wmean': 0.15346306990645756}, 'spearman': {'mean': 0.17880111266771895, 'wmean': 0.1729066457279496}}}, 'STS15': {'answers-forums': {'pearson': (0.17858573893499322, 0.0005113758205595806), 'spearman': SpearmanrResult(correlation=0.23522877789069752, pvalue=4.129603428352552e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.23153346441693798, 1.3841016154087585e-10), 'spearman': SpearmanrResult(correlation=0.24081282143188557, pvalue=2.3487690162410146e-11), 'nsamples': 750}, 'belief': {'pearson': (0.19658014865772977, 0.00012736446530354248), 'spearman': SpearmanrResult(correlation=0.24941443100340022, pvalue=1.0018264938966167e-06), 'nsamples': 375}, 'headlines': {'pearson': (0.3452457006213943, 2.0307033105420992e-22), 'spearman': SpearmanrResult(correlation=0.3406401002520099, pvalue=7.849374209695131e-22), 'nsamples': 750}, 'images': {'pearson': (0.12451998452012945, 0.0006314282985630612), 'spearman': SpearmanrResult(correlation=0.17821639196789854, pvalue=9.020269206614944e-07), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.21529300743023697, 'wmean': 0.2222205233387058}, 'spearman': {'mean': 0.24886250450917835, 'wmean': 0.2504977295247107}}}, 'STS16': {'answer-answer': {'pearson': (0.21498355373738742, 0.0005610036957689257), 'spearman': SpearmanrResult(correlation=0.25184542914554264, pvalue=4.916029209644169e-05), 'nsamples': 254}, 'headlines': {'pearson': (0.316427708018512, 3.4026412728186905e-07), 'spearman': SpearmanrResult(correlation=0.34439390129350145, pvalue=2.4233333625149e-08), 'nsamples': 249}, 'plagiarism': {'pearson': (0.24857080610061127, 0.00013942707540663815), 'spearman': SpearmanrResult(correlation=0.39120195400144564, pvalue=7.881992062862039e-10), 'nsamples': 230}, 'postediting': {'pearson': (0.4488783619077598, 1.6844997940504278e-13), 'spearman': SpearmanrResult(correlation=0.5763176733534956, pvalue=5.397446858922159e-23), 'nsamples': 244}, 'question-question': {'pearson': (0.04792783952892463, 0.49074435583320686), 'spearman': SpearmanrResult(correlation=0.10416452632344655, pvalue=0.13337012835331274), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.25535765385863907, 'wmean': 0.2614762614806788}, 'spearman': {'mean': 0.3335846968234864, 'wmean': 0.33903133907692706}}}, 'MR': {'devacc': 83.33, 'acc': 82.95, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 88.78, 'acc': 88.29, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.58, 'acc': 84.27, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.51, 'acc': 95.16, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.96, 'acc': 88.63, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.78, 'acc': 45.02, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.4, 'acc': 86.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.85, 'acc': 67.94, 'f1': 75.54, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.2, 'acc': 68.03, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.565705299868513, 'pearson': 0.6141470515928664, 'spearman': 0.5721520572741355, 'mse': 0.644530754904246, 'yhat': array([3.50092643, 4.045782  , 3.72460386, ..., 3.63036276, 4.06691038,        4.22684103]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.39763022183541236, 'pearson': 0.43107600903600907, 'spearman': 0.4336807893195036, 'mse': 1.938931826735549, 'yhat': array([1.75221911, 2.27288791, 2.05394661, ..., 3.78998193, 2.82467594,        3.12792759]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.65, 'acc': 61.69, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 53.27, 'acc': 53.81, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 15.58, 'acc': 15.9, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.42, 'acc': 26.33, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.02, 'acc': 61.49, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.4, 'acc': 86.22, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.1, 'acc': 88.32, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.59, 'acc': 79.67, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.28, 'acc': 73.24, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.86, 'acc': 65.02, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 67.92, 'acc': 67.75, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 22:55:56,149 : STS12 p=0.1492, STS12 s=0.2570, STS13 p=0.2037, STS13 s=0.2206, STS14 p=0.1535, STS14 s=0.1729, STS15 p=0.2222, STS15 s=0.2505, STS 16 p=0.2615, STS16 s=0.3390, STS B p=0.4311, STS B s=0.4337, STS B m=1.9389, SICK-R p=0.6141, SICK-R s=0.5722, SICK-P m=0.6445
2019-03-13 22:55:56,149 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 22:55:56,149 : 0.1492,0.2570,0.2037,0.2206,0.1535,0.1729,0.2222,0.2505,0.2615,0.3390,0.4311,0.4337,1.9389,0.6141,0.5722,0.6445
2019-03-13 22:55:56,149 : MR=82.95, CR=88.29, SUBJ=95.16, MPQA=84.27, SST-B=88.63, SST-F=45.02, TREC=86.40, SICK-E=68.03, SNLI=61.69, MRPC=67.94, MRPC f=75.54
2019-03-13 22:55:56,149 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 22:55:56,149 : 82.95,88.29,95.16,84.27,88.63,45.02,86.40,68.03,61.69,67.94,75.54
2019-03-13 22:55:56,149 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 22:55:56,149 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 22:55:56,149 : na,na,na,na,na,na,na,na,na,na
2019-03-13 22:55:56,149 : SentLen=53.81, WC=15.90, TreeDepth=26.33, TopConst=61.49, BShift=86.22, Tense=88.32, SubjNum=79.67, ObjNum=73.24, SOMO=65.02, CoordInv=67.75, average=61.77
2019-03-13 22:55:56,149 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 22:55:56,149 : 53.81,15.90,26.33,61.49,86.22,88.32,79.67,73.24,65.02,67.75,61.77
