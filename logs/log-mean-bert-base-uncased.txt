2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : layer 0
2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : ********************************************************************************
2019-02-13 16:55:06,064 : ***** Transfer task : STS12 *****


2019-02-13 16:55:06,098 : loading BERT model bert-base-uncased
2019-02-13 16:55:06,099 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:06,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:06,130 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp39t08by7
2019-02-13 16:55:08,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:15,920 : MSRpar : pearson = 0.3761, spearman = 0.4191
2019-02-13 16:55:17,429 : MSRvid : pearson = 0.5699, spearman = 0.5753
2019-02-13 16:55:17,990 : SMTeuroparl : pearson = 0.4763, spearman = 0.5888
2019-02-13 16:55:18,985 : surprise.OnWN : pearson = 0.6554, spearman = 0.6771
2019-02-13 16:55:19,543 : surprise.SMTnews : pearson = 0.4940, spearman = 0.4405
2019-02-13 16:55:19,544 : ALL (weighted average) : Pearson = 0.5202,             Spearman = 0.5469
2019-02-13 16:55:19,544 : ALL (average) : Pearson = 0.5143,             Spearman = 0.5402

2019-02-13 16:55:19,544 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 16:55:19,553 : loading BERT model bert-base-uncased
2019-02-13 16:55:19,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:19,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:19,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgnwnvcvy
2019-02-13 16:55:22,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:24,332 : FNWN : pearson = 0.3676, spearman = 0.3646
2019-02-13 16:55:25,752 : headlines : pearson = 0.6577, spearman = 0.6415
2019-02-13 16:55:26,712 : OnWN : pearson = 0.4054, spearman = 0.4594
2019-02-13 16:55:26,712 : ALL (weighted average) : Pearson = 0.5268,             Spearman = 0.5385
2019-02-13 16:55:26,712 : ALL (average) : Pearson = 0.4769,             Spearman = 0.4885

2019-02-13 16:55:26,712 : ***** Transfer task : STS14 *****


2019-02-13 16:55:26,730 : loading BERT model bert-base-uncased
2019-02-13 16:55:26,731 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:26,781 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:26,781 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc0zv0p0d
2019-02-13 16:55:29,247 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:31,731 : deft-forum : pearson = 0.3369, spearman = 0.3553
2019-02-13 16:55:32,867 : deft-news : pearson = 0.6840, spearman = 0.6730
2019-02-13 16:55:33,910 : headlines : pearson = 0.6223, spearman = 0.5877
2019-02-13 16:55:34,725 : images : pearson = 0.5857, spearman = 0.5889
2019-02-13 16:55:35,549 : OnWN : pearson = 0.5576, spearman = 0.6217
2019-02-13 16:55:36,632 : tweet-news : pearson = 0.5937, spearman = 0.5864
2019-02-13 16:55:36,632 : ALL (weighted average) : Pearson = 0.5670,             Spearman = 0.5734
2019-02-13 16:55:36,633 : ALL (average) : Pearson = 0.5634,             Spearman = 0.5688

2019-02-13 16:55:36,633 : ***** Transfer task : STS15 *****


2019-02-13 16:55:36,669 : loading BERT model bert-base-uncased
2019-02-13 16:55:36,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:36,689 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:36,689 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxsw8j7o0
2019-02-13 16:55:39,121 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:41,780 : answers-forums : pearson = 0.4531, spearman = 0.4482
2019-02-13 16:55:43,865 : answers-students : pearson = 0.6952, spearman = 0.7051
2019-02-13 16:55:45,458 : belief : pearson = 0.5276, spearman = 0.5268
2019-02-13 16:55:48,229 : headlines : pearson = 0.6782, spearman = 0.6724
2019-02-13 16:55:51,120 : images : pearson = 0.6894, spearman = 0.7051
2019-02-13 16:55:51,121 : ALL (weighted average) : Pearson = 0.6383,             Spearman = 0.6425
2019-02-13 16:55:51,121 : ALL (average) : Pearson = 0.6087,             Spearman = 0.6115

2019-02-13 16:55:51,121 : ***** Transfer task : STS16 *****


2019-02-13 16:55:51,193 : loading BERT model bert-base-uncased
2019-02-13 16:55:51,193 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:51,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:51,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0ybsxu0v
2019-02-13 16:55:53,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:55,532 : answer-answer : pearson = 0.4588, spearman = 0.5082
2019-02-13 16:55:55,902 : headlines : pearson = 0.6884, spearman = 0.6927
2019-02-13 16:55:56,352 : plagiarism : pearson = 0.6704, spearman = 0.6758
2019-02-13 16:55:57,030 : postediting : pearson = 0.7656, spearman = 0.7897
2019-02-13 16:55:57,365 : question-question : pearson = 0.4425, spearman = 0.4461
2019-02-13 16:55:57,365 : ALL (weighted average) : Pearson = 0.6083,             Spearman = 0.6264
2019-02-13 16:55:57,365 : ALL (average) : Pearson = 0.6051,             Spearman = 0.6225

2019-02-13 16:55:57,365 : ***** Transfer task : MR *****


2019-02-13 16:55:57,386 : loading BERT model bert-base-uncased
2019-02-13 16:55:57,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:57,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:57,405 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_f6wya6k
2019-02-13 16:55:59,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:56:01,329 : Generating sentence embeddings
2019-02-13 16:56:15,779 : Generated sentence embeddings
2019-02-13 16:56:15,779 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:56:27,319 : Best param found at split 1: l2reg = 0.0001                 with score 75.26
2019-02-13 16:56:41,782 : Best param found at split 2: l2reg = 0.0001                 with score 75.05
2019-02-13 16:56:52,844 : Best param found at split 3: l2reg = 0.0001                 with score 75.13
2019-02-13 16:57:08,488 : Best param found at split 4: l2reg = 0.001                 with score 74.74
2019-02-13 16:57:22,226 : Best param found at split 5: l2reg = 1e-05                 with score 74.69
2019-02-13 16:57:23,849 : Dev acc : 74.97 Test acc : 74.47

2019-02-13 16:57:23,850 : ***** Transfer task : CR *****


2019-02-13 16:57:23,858 : loading BERT model bert-base-uncased
2019-02-13 16:57:23,858 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:57:23,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:57:23,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg3y6zotp
2019-02-13 16:57:26,315 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:57:27,752 : Generating sentence embeddings
2019-02-13 16:57:31,894 : Generated sentence embeddings
2019-02-13 16:57:31,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:57:35,678 : Best param found at split 1: l2reg = 0.001                 with score 78.3
2019-02-13 16:57:40,703 : Best param found at split 2: l2reg = 0.001                 with score 79.2
2019-02-13 16:57:45,609 : Best param found at split 3: l2reg = 0.0001                 with score 79.21
2019-02-13 16:57:50,170 : Best param found at split 4: l2reg = 0.001                 with score 78.55
2019-02-13 16:57:55,149 : Best param found at split 5: l2reg = 1e-05                 with score 79.05
2019-02-13 16:57:55,372 : Dev acc : 78.86 Test acc : 76.18

2019-02-13 16:57:55,372 : ***** Transfer task : MPQA *****


2019-02-13 16:57:55,378 : loading BERT model bert-base-uncased
2019-02-13 16:57:55,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:57:55,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:57:55,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk8qwlrqg
2019-02-13 16:57:57,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:57:59,336 : Generating sentence embeddings
2019-02-13 16:58:04,439 : Generated sentence embeddings
2019-02-13 16:58:04,439 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:58:16,774 : Best param found at split 1: l2reg = 0.001                 with score 87.38
2019-02-13 16:58:34,134 : Best param found at split 2: l2reg = 0.001                 with score 88.34
2019-02-13 16:58:49,535 : Best param found at split 3: l2reg = 0.001                 with score 88.46
2019-02-13 16:59:04,102 : Best param found at split 4: l2reg = 0.001                 with score 88.3
2019-02-13 16:59:18,320 : Best param found at split 5: l2reg = 0.001                 with score 87.92
2019-02-13 16:59:18,917 : Dev acc : 88.08 Test acc : 87.68

2019-02-13 16:59:18,918 : ***** Transfer task : SUBJ *****


2019-02-13 16:59:18,935 : loading BERT model bert-base-uncased
2019-02-13 16:59:18,935 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:59:18,955 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:59:18,955 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3z4asc3l
2019-02-13 16:59:21,392 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:59:22,809 : Generating sentence embeddings
2019-02-13 16:59:36,495 : Generated sentence embeddings
2019-02-13 16:59:36,496 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:59:57,430 : Best param found at split 1: l2reg = 1e-05                 with score 91.57
2019-02-13 17:00:19,331 : Best param found at split 2: l2reg = 0.001                 with score 91.82
2019-02-13 17:00:38,580 : Best param found at split 3: l2reg = 0.001                 with score 91.6
2019-02-13 17:00:52,581 : Best param found at split 4: l2reg = 1e-05                 with score 92.04
2019-02-13 17:01:06,177 : Best param found at split 5: l2reg = 0.0001                 with score 91.41
2019-02-13 17:01:07,290 : Dev acc : 91.69 Test acc : 91.65

2019-02-13 17:01:07,291 : ***** Transfer task : SST Binary classification *****


2019-02-13 17:01:07,421 : loading BERT model bert-base-uncased
2019-02-13 17:01:07,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:01:07,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:01:07,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpye91yhji
2019-02-13 17:01:09,877 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:01:11,367 : Computing embedding for train
2019-02-13 17:02:11,392 : Computed train embeddings
2019-02-13 17:02:11,392 : Computing embedding for dev
2019-02-13 17:02:12,533 : Computed dev embeddings
2019-02-13 17:02:12,533 : Computing embedding for test
2019-02-13 17:02:14,823 : Computed test embeddings
2019-02-13 17:02:14,823 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:02:42,419 : [('reg:1e-05', 79.47), ('reg:0.0001', 79.82), ('reg:0.001', 79.7), ('reg:0.01', 78.78)]
2019-02-13 17:02:42,419 : Validation : best param found is reg = 0.0001 with score             79.82
2019-02-13 17:02:42,419 : Evaluating...
2019-02-13 17:02:54,613 : 
Dev acc : 79.82 Test acc : 80.29 for             SST Binary classification

2019-02-13 17:02:54,619 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 17:02:54,666 : loading BERT model bert-base-uncased
2019-02-13 17:02:54,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:02:54,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:02:54,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_8vskemu
2019-02-13 17:02:57,127 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:02:58,551 : Computing embedding for train
2019-02-13 17:03:09,177 : Computed train embeddings
2019-02-13 17:03:09,177 : Computing embedding for dev
2019-02-13 17:03:10,556 : Computed dev embeddings
2019-02-13 17:03:10,556 : Computing embedding for test
2019-02-13 17:03:13,294 : Computed test embeddings
2019-02-13 17:03:13,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:03:16,241 : [('reg:1e-05', 40.24), ('reg:0.0001', 40.15), ('reg:0.001', 39.24), ('reg:0.01', 38.87)]
2019-02-13 17:03:16,242 : Validation : best param found is reg = 1e-05 with score             40.24
2019-02-13 17:03:16,242 : Evaluating...
2019-02-13 17:03:16,887 : 
Dev acc : 40.24 Test acc : 42.4 for             SST Fine-Grained classification

2019-02-13 17:03:16,887 : ***** Transfer task : TREC *****


2019-02-13 17:03:16,900 : loading BERT model bert-base-uncased
2019-02-13 17:03:16,900 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:03:16,920 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:03:16,920 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8m4_qffq
2019-02-13 17:03:19,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:03:24,596 : Computed train embeddings
2019-02-13 17:03:24,829 : Computed test embeddings
2019-02-13 17:03:24,829 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 17:03:35,936 : [('reg:1e-05', 76.76), ('reg:0.0001', 76.65), ('reg:0.001', 74.78), ('reg:0.01', 65.48)]
2019-02-13 17:03:35,937 : Cross-validation : best param found is reg = 1e-05             with score 76.76
2019-02-13 17:03:35,937 : Evaluating...
2019-02-13 17:03:36,909 : 
Dev acc : 76.76 Test acc : 83.2             for TREC

2019-02-13 17:03:36,909 : ***** Transfer task : MRPC *****


2019-02-13 17:03:36,930 : loading BERT model bert-base-uncased
2019-02-13 17:03:36,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:03:36,953 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:03:36,953 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoq2qs1uz
2019-02-13 17:03:39,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:03:40,923 : Computing embedding for train
2019-02-13 17:03:51,688 : Computed train embeddings
2019-02-13 17:03:51,688 : Computing embedding for test
2019-02-13 17:03:56,339 : Computed test embeddings
2019-02-13 17:03:56,355 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 17:04:03,231 : [('reg:1e-05', 74.41), ('reg:0.0001', 74.29), ('reg:0.001', 74.12), ('reg:0.01', 73.09)]
2019-02-13 17:04:03,231 : Cross-validation : best param found is reg = 1e-05             with score 74.41
2019-02-13 17:04:03,231 : Evaluating...
2019-02-13 17:04:03,513 : Dev acc : 74.41 Test acc 70.72; Test F1 77.24 for MRPC.

2019-02-13 17:04:03,514 : ***** Transfer task : SICK-Entailment*****


2019-02-13 17:04:03,538 : loading BERT model bert-base-uncased
2019-02-13 17:04:03,538 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:04:03,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:04:03,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvv8d3fz1
2019-02-13 17:04:06,034 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:04:07,459 : Computing embedding for train
2019-02-13 17:04:13,861 : Computed train embeddings
2019-02-13 17:04:13,861 : Computing embedding for dev
2019-02-13 17:04:14,680 : Computed dev embeddings
2019-02-13 17:04:14,680 : Computing embedding for test
2019-02-13 17:04:21,591 : Computed test embeddings
2019-02-13 17:04:21,620 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:04:23,562 : [('reg:1e-05', 80.0), ('reg:0.0001', 80.2), ('reg:0.001', 81.6), ('reg:0.01', 78.0)]
2019-02-13 17:04:23,562 : Validation : best param found is reg = 0.001 with score             81.6
2019-02-13 17:04:23,562 : Evaluating...
2019-02-13 17:04:24,015 : 
Dev acc : 81.6 Test acc : 81.49 for                        SICK entailment

2019-02-13 17:04:24,015 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 17:04:24,042 : loading BERT model bert-base-uncased
2019-02-13 17:04:24,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:04:24,062 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:04:24,062 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx0jvb0se
2019-02-13 17:04:26,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:04:27,923 : Computing embedding for train
2019-02-13 17:04:32,448 : Computed train embeddings
2019-02-13 17:04:32,448 : Computing embedding for dev
2019-02-13 17:04:33,065 : Computed dev embeddings
2019-02-13 17:04:33,065 : Computing embedding for test
2019-02-13 17:04:41,095 : Computed test embeddings
2019-02-13 17:05:22,577 : Dev : Pearson 0.807462851994446
2019-02-13 17:05:22,577 : Test : Pearson 0.8245061073607127 Spearman 0.7552315731337067 MSE 0.32676883589578126                        for SICK Relatedness

2019-02-13 17:05:22,578 : 

***** Transfer task : STSBenchmark*****


2019-02-13 17:05:22,655 : loading BERT model bert-base-uncased
2019-02-13 17:05:22,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:05:22,675 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:05:22,675 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpidezniax
2019-02-13 17:05:25,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:05:26,610 : Computing embedding for train
2019-02-13 17:05:37,374 : Computed train embeddings
2019-02-13 17:05:37,374 : Computing embedding for dev
2019-02-13 17:05:40,536 : Computed dev embeddings
2019-02-13 17:05:40,536 : Computing embedding for test
2019-02-13 17:05:43,253 : Computed test embeddings
2019-02-13 17:06:25,300 : Dev : Pearson 0.7550856779166432
2019-02-13 17:06:25,300 : Test : Pearson 0.7023578299620669 Spearman 0.7027351035990721 MSE 1.3971707294445477                        for SICK Relatedness

2019-02-13 17:06:25,300 : ***** Transfer task : SNLI Entailment*****


2019-02-13 17:06:30,112 : loading BERT model bert-base-uncased
2019-02-13 17:06:30,112 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:06:30,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:06:30,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjywdjt8f
2019-02-13 17:06:32,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:06:34,357 : PROGRESS (encoding): 0.00%
2019-02-13 17:08:48,723 : PROGRESS (encoding): 14.56%
2019-02-13 17:11:07,675 : PROGRESS (encoding): 29.12%
2019-02-13 17:13:30,623 : PROGRESS (encoding): 43.69%
2019-02-13 17:16:03,769 : PROGRESS (encoding): 58.25%
2019-02-13 17:18:47,907 : PROGRESS (encoding): 72.81%
2019-02-13 17:21:41,611 : PROGRESS (encoding): 87.37%
2019-02-13 17:23:39,429 : PROGRESS (encoding): 0.00%
2019-02-13 17:23:53,074 : PROGRESS (encoding): 0.00%
2019-02-13 17:24:04,556 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:25:13,710 : [('reg:1e-09', 68.86)]
2019-02-13 17:25:13,711 : Validation : best param found is reg = 1e-09 with score             68.86
2019-02-13 17:25:13,711 : Evaluating...
2019-02-13 17:26:31,086 : Dev acc : 68.86 Test acc : 68.88 for SNLI

2019-02-13 17:26:31,087 : ***** Transfer task: Image Caption Retrieval *****


2019-02-13 17:26:39,913 : loading BERT model bert-base-uncased
2019-02-13 17:26:39,914 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:26:39,961 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:26:39,961 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpopn28chn
2019-02-13 17:26:42,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:26:43,889 : Computing embedding for train
2019-02-13 17:37:52,459 : Computed train embeddings
2019-02-13 17:37:52,459 : Computing embedding for dev
2019-02-13 17:38:12,039 : Computed dev embeddings
2019-02-13 17:38:12,040 : Computing embedding for test
2019-02-13 17:38:32,467 : Computed test embeddings
2019-02-13 17:38:32,482 : prepare data
2019-02-13 17:38:32,548 : start epoch
2019-02-13 17:39:14,003 : samples : 64000
2019-02-13 17:39:24,302 : Image to text: 8.58, 24.16, 34.6, 22.0
2019-02-13 17:39:31,741 : Text to Image: 6.84, 21.012, 31.68, 25.0
2019-02-13 17:40:13,331 : samples : 128000
2019-02-13 17:40:23,781 : Image to text: 9.7, 27.14, 39.22, 18.0
2019-02-13 17:40:31,254 : Text to Image: 7.452, 22.748, 33.772, 22.0
2019-02-13 17:41:12,920 : samples : 192000
2019-02-13 17:41:23,249 : Image to text: 10.78, 28.86, 40.38, 16.0
2019-02-13 17:41:30,618 : Text to Image: 8.064, 24.268, 35.444, 21.0
2019-02-13 17:42:12,786 : samples : 256000
2019-02-13 17:42:23,025 : Image to text: 10.42, 27.78, 39.76, 17.0
2019-02-13 17:42:30,437 : Text to Image: 8.3, 24.0, 35.64, 21.0
2019-02-13 17:43:10,978 : samples : 320000
2019-02-13 17:43:21,241 : Image to text: 10.5, 29.52, 41.46, 16.0
2019-02-13 17:43:28,701 : Text to Image: 8.696, 24.944, 36.484, 20.0
2019-02-13 17:44:10,950 : samples : 384000
2019-02-13 17:44:21,181 : Image to text: 11.44, 30.0, 42.52, 15.0
2019-02-13 17:44:28,599 : Text to Image: 9.364, 26.524, 38.448, 18.0
2019-02-13 17:45:11,928 : samples : 448000
2019-02-13 17:45:22,268 : Image to text: 10.7, 29.68, 40.98, 15.0
2019-02-13 17:45:29,695 : Text to Image: 8.192, 24.744, 36.564, 20.0
2019-02-13 17:46:13,021 : samples : 512000
2019-02-13 17:46:23,347 : Image to text: 12.62, 32.34, 44.04, 14.0
2019-02-13 17:46:30,758 : Text to Image: 9.728, 27.352, 38.956, 17.0
2019-02-13 17:47:06,183 : Epoch 1 finished
2019-02-13 17:47:06,621 : Image to text: 30.8, 62.5, 75.0, 4.0
2019-02-13 17:47:06,951 : Text to Image: 23.52, 56.58, 73.86, 4.0
2019-02-13 17:47:07,378 : Image to text: 28.7, 61.0, 75.4, 3.0
2019-02-13 17:47:07,707 : Text to Image: 24.42, 56.36, 73.28, 4.0
2019-02-13 17:47:08,152 : Image to text: 30.3, 63.2, 76.1, 3.0
2019-02-13 17:47:08,482 : Text to Image: 24.38, 57.02, 73.72, 4.0
2019-02-13 17:47:08,910 : Image to text: 29.4, 62.8, 76.2, 3.0
2019-02-13 17:47:09,239 : Text to Image: 23.88, 56.82, 73.52, 4.0
2019-02-13 17:47:09,679 : Image to text: 30.3, 60.2, 74.2, 4.0
2019-02-13 17:47:10,008 : Text to Image: 24.22, 56.84, 72.28, 4.0
2019-02-13 17:47:10,008 : Dev mean Text to Image: 24.084000000000003, 56.72400000000001, 73.332, 4.0
2019-02-13 17:47:10,008 : Dev mean Image to text: 29.9, 61.940000000000005, 75.38, 3.4000000000000004
2019-02-13 17:47:10,008 : start epoch
2019-02-13 17:47:51,841 : samples : 64000
2019-02-13 17:48:02,136 : Image to text: 12.14, 32.24, 44.44, 14.0
2019-02-13 17:48:09,566 : Text to Image: 9.744, 27.488, 39.328, 17.0
2019-02-13 17:48:49,781 : samples : 128000
2019-02-13 17:49:00,066 : Image to text: 11.8, 30.6, 42.58, 15.0
2019-02-13 17:49:07,466 : Text to Image: 9.564, 27.444, 39.448, 17.0
2019-02-13 17:49:49,458 : samples : 192000
2019-02-13 17:49:59,803 : Image to text: 12.02, 32.54, 45.48, 13.0
2019-02-13 17:50:07,176 : Text to Image: 10.096, 27.824, 39.928, 17.0
2019-02-13 17:50:49,036 : samples : 256000
2019-02-13 17:50:59,397 : Image to text: 12.96, 32.8, 45.22, 13.0
2019-02-13 17:51:06,803 : Text to Image: 10.304, 28.608, 40.836, 16.0
2019-02-13 17:51:48,994 : samples : 320000
2019-02-13 17:51:59,362 : Image to text: 12.96, 33.0, 45.24, 13.0
2019-02-13 17:52:06,822 : Text to Image: 10.224, 28.54, 40.564, 16.0
2019-02-13 17:52:48,065 : samples : 384000
2019-02-13 17:52:58,374 : Image to text: 13.14, 33.76, 46.62, 12.0
2019-02-13 17:53:05,813 : Text to Image: 10.34, 28.692, 40.68, 16.0
2019-02-13 17:53:47,567 : samples : 448000
2019-02-13 17:53:57,975 : Image to text: 12.9, 34.04, 46.96, 12.0
2019-02-13 17:54:05,352 : Text to Image: 10.172, 28.616, 40.804, 16.0
2019-02-13 17:54:47,596 : samples : 512000
2019-02-13 17:54:58,308 : Image to text: 12.24, 32.32, 45.16, 13.0
2019-02-13 17:55:05,626 : Text to Image: 10.032, 28.076, 40.1, 17.0
2019-02-13 17:55:43,032 : Epoch 2 finished
2019-02-13 17:55:43,495 : Image to text: 31.4, 64.7, 78.1, 3.0
2019-02-13 17:55:43,829 : Text to Image: 25.76, 59.7, 76.58, 4.0
2019-02-13 17:55:44,263 : Image to text: 29.7, 61.9, 78.9, 3.0
2019-02-13 17:55:44,597 : Text to Image: 25.32, 58.12, 75.04, 4.0
2019-02-13 17:55:45,032 : Image to text: 31.3, 64.6, 80.1, 3.0
2019-02-13 17:55:45,361 : Text to Image: 26.54, 59.7, 75.96, 4.0
2019-02-13 17:55:45,787 : Image to text: 31.4, 63.4, 79.5, 3.0
2019-02-13 17:55:46,115 : Text to Image: 25.5, 59.28, 75.48, 4.0
2019-02-13 17:55:46,553 : Image to text: 32.0, 65.5, 76.4, 3.0
2019-02-13 17:55:46,881 : Text to Image: 24.9, 58.46, 74.3, 4.0
2019-02-13 17:55:46,881 : Dev mean Text to Image: 25.604000000000003, 59.05200000000001, 75.472, 4.0
2019-02-13 17:55:46,881 : Dev mean Image to text: 31.159999999999997, 64.02, 78.6, 3.0
2019-02-13 17:55:46,881 : start epoch
2019-02-13 17:56:27,284 : samples : 64000
2019-02-13 17:56:37,534 : Image to text: 13.44, 34.32, 47.38, 12.0
2019-02-13 17:56:44,948 : Text to Image: 10.108, 28.124, 40.116, 17.0
2019-02-13 17:57:26,243 : samples : 128000
2019-02-13 17:57:36,526 : Image to text: 13.54, 34.5, 47.1, 12.0
2019-02-13 17:57:43,975 : Text to Image: 10.8, 29.052, 41.524, 16.0
2019-02-13 17:58:26,797 : samples : 192000
2019-02-13 17:58:37,212 : Image to text: 13.82, 34.94, 47.94, 12.0
2019-02-13 17:58:44,599 : Text to Image: 11.092, 29.86, 42.592, 15.0
2019-02-13 17:59:26,998 : samples : 256000
2019-02-13 17:59:37,881 : Image to text: 14.64, 35.4, 47.52, 12.0
2019-02-13 17:59:45,165 : Text to Image: 11.124, 30.288, 42.808, 15.0
2019-02-13 18:00:28,120 : samples : 320000
2019-02-13 18:00:38,519 : Image to text: 13.78, 34.82, 46.78, 12.0
2019-02-13 18:00:45,973 : Text to Image: 10.536, 28.944, 41.168, 16.0
2019-02-13 18:01:28,801 : samples : 384000
2019-02-13 18:01:39,042 : Image to text: 13.82, 34.22, 47.1, 12.0
2019-02-13 18:01:46,441 : Text to Image: 10.612, 29.48, 41.872, 15.0
2019-02-13 18:02:29,197 : samples : 448000
2019-02-13 18:02:39,450 : Image to text: 14.64, 35.86, 48.6, 11.0
2019-02-13 18:02:46,839 : Text to Image: 11.036, 30.236, 42.932, 15.0
2019-02-13 18:03:29,834 : samples : 512000
2019-02-13 18:03:40,098 : Image to text: 14.22, 35.84, 48.44, 11.0
2019-02-13 18:03:47,502 : Text to Image: 10.996, 30.216, 42.744, 15.0
2019-02-13 18:04:22,806 : Epoch 3 finished
2019-02-13 18:04:23,225 : Image to text: 32.8, 66.0, 80.3, 3.0
2019-02-13 18:04:23,549 : Text to Image: 26.82, 60.82, 77.86, 4.0
2019-02-13 18:04:23,968 : Image to text: 31.5, 64.9, 80.1, 3.0
2019-02-13 18:04:24,292 : Text to Image: 26.8, 60.84, 76.38, 4.0
2019-02-13 18:04:24,731 : Image to text: 32.6, 66.8, 78.8, 3.0
2019-02-13 18:04:25,060 : Text to Image: 27.78, 61.5, 77.18, 4.0
2019-02-13 18:04:25,490 : Image to text: 34.4, 65.9, 80.5, 3.0
2019-02-13 18:04:25,819 : Text to Image: 25.88, 61.54, 77.34, 4.0
2019-02-13 18:04:26,261 : Image to text: 35.8, 66.5, 79.7, 3.0
2019-02-13 18:04:26,589 : Text to Image: 26.4, 60.26, 76.02, 4.0
2019-02-13 18:04:26,589 : Dev mean Text to Image: 26.736000000000004, 60.992000000000004, 76.95599999999999, 4.0
2019-02-13 18:04:26,590 : Dev mean Image to text: 33.419999999999995, 66.02, 79.88, 3.0
2019-02-13 18:04:26,590 : start epoch
2019-02-13 18:05:07,742 : samples : 64000
2019-02-13 18:05:18,017 : Image to text: 14.26, 35.9, 49.3, 11.0
2019-02-13 18:05:25,464 : Text to Image: 10.932, 30.436, 42.804, 15.0
2019-02-13 18:06:07,493 : samples : 128000
2019-02-13 18:06:17,856 : Image to text: 13.96, 35.64, 48.72, 11.0
2019-02-13 18:06:25,295 : Text to Image: 11.04, 30.632, 42.968, 14.0
2019-02-13 18:07:07,274 : samples : 192000
2019-02-13 18:07:17,639 : Image to text: 13.92, 34.42, 46.94, 12.0
2019-02-13 18:07:25,071 : Text to Image: 10.784, 29.588, 41.776, 15.0
2019-02-13 18:08:06,514 : samples : 256000
2019-02-13 18:08:16,828 : Image to text: 14.76, 36.2, 48.98, 11.0
2019-02-13 18:08:24,272 : Text to Image: 11.384, 30.64, 43.188, 14.0
2019-02-13 18:09:08,963 : samples : 320000
2019-02-13 18:09:19,784 : Image to text: 14.36, 35.08, 48.0, 11.0
2019-02-13 18:09:27,056 : Text to Image: 11.04, 29.852, 42.272, 15.0
2019-02-13 18:10:08,368 : samples : 384000
2019-02-13 18:10:19,141 : Image to text: 14.34, 35.4, 48.66, 11.0
2019-02-13 18:10:26,511 : Text to Image: 11.276, 30.46, 42.524, 15.0
2019-02-13 18:11:09,433 : samples : 448000
2019-02-13 18:11:19,922 : Image to text: 14.5, 35.94, 48.36, 11.0
2019-02-13 18:11:27,374 : Text to Image: 11.568, 31.172, 43.44, 14.0
2019-02-13 18:12:10,268 : samples : 512000
2019-02-13 18:12:20,519 : Image to text: 14.12, 36.1, 48.78, 11.0
2019-02-13 18:12:28,019 : Text to Image: 11.46, 30.44, 43.396, 14.0
2019-02-13 18:13:02,939 : Epoch 4 finished
2019-02-13 18:13:03,568 : Image to text: 35.1, 67.2, 81.3, 3.0
2019-02-13 18:13:03,950 : Text to Image: 28.06, 62.7, 78.28, 3.0
2019-02-13 18:13:04,346 : Image to text: 31.9, 64.9, 80.1, 3.0
2019-02-13 18:13:04,642 : Text to Image: 27.54, 60.66, 76.66, 4.0
2019-02-13 18:13:05,038 : Image to text: 34.0, 68.1, 81.9, 3.0
2019-02-13 18:13:05,336 : Text to Image: 28.62, 62.26, 77.56, 3.0
2019-02-13 18:13:05,732 : Image to text: 35.7, 68.8, 81.4, 3.0
2019-02-13 18:13:06,036 : Text to Image: 26.74, 62.16, 77.94, 3.0
2019-02-13 18:13:06,453 : Image to text: 34.0, 66.7, 79.8, 3.0
2019-02-13 18:13:06,757 : Text to Image: 27.24, 60.46, 76.38, 4.0
2019-02-13 18:13:06,757 : Dev mean Text to Image: 27.64, 61.647999999999996, 77.364, 3.4000000000000004
2019-02-13 18:13:06,757 : Dev mean Image to text: 34.14, 67.14, 80.89999999999999, 3.0
2019-02-13 18:13:06,757 : start epoch
2019-02-13 18:13:49,147 : samples : 64000
2019-02-13 18:14:00,071 : Image to text: 15.34, 36.92, 49.44, 11.0
2019-02-13 18:14:07,353 : Text to Image: 11.68, 31.124, 43.916, 14.0
2019-02-13 18:14:48,577 : samples : 128000
2019-02-13 18:14:59,633 : Image to text: 14.66, 36.6, 50.04, 10.0
2019-02-13 18:15:07,118 : Text to Image: 11.612, 31.06, 43.912, 14.0
2019-02-13 18:15:49,139 : samples : 192000
2019-02-13 18:15:59,738 : Image to text: 13.96, 35.52, 48.54, 11.0
2019-02-13 18:16:07,214 : Text to Image: 11.344, 30.724, 43.72, 14.0
2019-02-13 18:16:47,432 : samples : 256000
2019-02-13 18:16:57,832 : Image to text: 14.72, 36.84, 49.22, 11.0
2019-02-13 18:17:05,303 : Text to Image: 11.696, 31.24, 43.628, 14.0
2019-02-13 18:17:45,556 : samples : 320000
2019-02-13 18:17:55,982 : Image to text: 14.74, 36.06, 48.84, 11.0
2019-02-13 18:18:03,384 : Text to Image: 11.52, 31.572, 44.048, 14.0
2019-02-13 18:18:45,003 : samples : 384000
2019-02-13 18:18:55,379 : Image to text: 14.6, 36.28, 49.6, 11.0
2019-02-13 18:19:02,752 : Text to Image: 11.564, 30.836, 43.58, 14.0
2019-02-13 18:19:44,713 : samples : 448000
2019-02-13 18:19:55,033 : Image to text: 15.06, 36.82, 50.4, 10.0
2019-02-13 18:20:02,498 : Text to Image: 11.496, 31.16, 43.572, 14.0
2019-02-13 18:20:42,680 : samples : 512000
2019-02-13 18:20:52,974 : Image to text: 14.96, 36.56, 49.9, 11.0
2019-02-13 18:21:00,423 : Text to Image: 11.512, 31.172, 43.644, 14.0
2019-02-13 18:21:34,500 : Epoch 5 finished
2019-02-13 18:21:34,932 : Image to text: 32.9, 68.2, 81.3, 3.0
2019-02-13 18:21:35,258 : Text to Image: 27.48, 62.74, 79.22, 4.0
2019-02-13 18:21:35,679 : Image to text: 33.7, 66.9, 83.2, 3.0
2019-02-13 18:21:36,004 : Text to Image: 26.8, 61.24, 77.24, 4.0
2019-02-13 18:21:36,443 : Image to text: 35.2, 68.1, 81.5, 3.0
2019-02-13 18:21:36,773 : Text to Image: 28.4, 63.36, 77.98, 3.0
2019-02-13 18:21:37,205 : Image to text: 33.8, 67.5, 82.7, 3.0
2019-02-13 18:21:37,535 : Text to Image: 27.84, 62.12, 78.38, 3.0
2019-02-13 18:21:37,983 : Image to text: 35.5, 68.1, 80.9, 3.0
2019-02-13 18:21:38,314 : Text to Image: 27.3, 60.22, 76.84, 4.0
2019-02-13 18:21:38,314 : Dev mean Text to Image: 27.564, 61.93600000000001, 77.93199999999999, 3.6000000000000005
2019-02-13 18:21:38,314 : Dev mean Image to text: 34.22, 67.76, 81.92000000000002, 3.0
2019-02-13 18:21:38,315 : start epoch
2019-02-13 18:22:18,535 : samples : 64000
2019-02-13 18:22:28,814 : Image to text: 15.66, 38.08, 51.1, 10.0
2019-02-13 18:22:36,222 : Text to Image: 12.196, 32.144, 44.472, 14.0
2019-02-13 18:23:16,405 : samples : 128000
2019-02-13 18:23:26,632 : Image to text: 15.02, 37.16, 49.38, 11.0
2019-02-13 18:23:34,028 : Text to Image: 11.868, 31.548, 44.04, 14.0
2019-02-13 18:24:14,285 : samples : 192000
2019-02-13 18:24:24,563 : Image to text: 15.58, 37.4, 50.6, 10.0
2019-02-13 18:24:31,838 : Text to Image: 11.948, 31.412, 44.064, 14.0
2019-02-13 18:25:11,922 : samples : 256000
2019-02-13 18:25:22,486 : Image to text: 15.16, 37.9, 50.64, 10.0
2019-02-13 18:25:29,876 : Text to Image: 11.852, 31.532, 44.02, 14.0
2019-02-13 18:26:10,296 : samples : 320000
2019-02-13 18:26:20,635 : Image to text: 15.36, 36.94, 50.82, 10.0
2019-02-13 18:26:28,043 : Text to Image: 11.816, 31.512, 44.144, 14.0
2019-02-13 18:27:08,341 : samples : 384000
2019-02-13 18:27:18,658 : Image to text: 15.12, 37.7, 51.1, 10.0
2019-02-13 18:27:26,117 : Text to Image: 11.812, 31.64, 44.332, 14.0
2019-02-13 18:28:06,324 : samples : 448000
2019-02-13 18:28:16,653 : Image to text: 15.18, 37.8, 50.6, 10.0
2019-02-13 18:28:24,096 : Text to Image: 12.248, 32.276, 44.728, 13.0
2019-02-13 18:29:08,503 : samples : 512000
2019-02-13 18:29:18,883 : Image to text: 15.62, 37.2, 50.36, 10.0
2019-02-13 18:29:26,330 : Text to Image: 12.024, 31.708, 44.56, 14.0
2019-02-13 18:30:01,084 : Epoch 6 finished
2019-02-13 18:30:01,526 : Image to text: 35.0, 67.3, 81.1, 3.0
2019-02-13 18:30:01,856 : Text to Image: 27.88, 62.72, 78.96, 3.0
2019-02-13 18:30:02,285 : Image to text: 33.8, 66.3, 80.9, 3.0
2019-02-13 18:30:02,615 : Text to Image: 27.48, 61.12, 76.76, 4.0
2019-02-13 18:30:03,043 : Image to text: 34.6, 68.8, 82.1, 3.0
2019-02-13 18:30:03,374 : Text to Image: 29.56, 62.82, 78.04, 3.0
2019-02-13 18:30:03,819 : Image to text: 34.9, 69.0, 80.3, 3.0
2019-02-13 18:30:04,150 : Text to Image: 27.7, 62.28, 78.14, 3.0
2019-02-13 18:30:04,580 : Image to text: 36.0, 68.4, 80.0, 3.0
2019-02-13 18:30:04,910 : Text to Image: 27.08, 61.62, 76.76, 4.0
2019-02-13 18:30:04,910 : Dev mean Text to Image: 27.939999999999998, 62.111999999999995, 77.732, 3.4000000000000004
2019-02-13 18:30:04,910 : Dev mean Image to text: 34.86, 67.96000000000001, 80.88, 3.0
2019-02-13 18:30:04,910 : start epoch
2019-02-13 18:30:46,299 : samples : 64000
2019-02-13 18:30:56,661 : Image to text: 15.56, 37.86, 51.08, 10.0
2019-02-13 18:31:04,129 : Text to Image: 11.988, 31.812, 44.272, 14.0
2019-02-13 18:31:44,907 : samples : 128000
2019-02-13 18:31:55,254 : Image to text: 15.94, 37.82, 51.32, 10.0
2019-02-13 18:32:02,727 : Text to Image: 12.084, 32.176, 44.732, 13.0
2019-02-13 18:32:43,121 : samples : 192000
2019-02-13 18:32:53,411 : Image to text: 15.28, 37.84, 50.64, 10.0
2019-02-13 18:33:00,820 : Text to Image: 12.568, 32.264, 44.616, 13.0
2019-02-13 18:33:41,485 : samples : 256000
2019-02-13 18:33:51,828 : Image to text: 15.16, 36.76, 49.66, 11.0
2019-02-13 18:33:59,254 : Text to Image: 12.424, 32.324, 44.88, 13.0
2019-02-13 18:34:40,743 : samples : 320000
2019-02-13 18:34:51,046 : Image to text: 15.98, 38.46, 51.84, 10.0
2019-02-13 18:34:58,479 : Text to Image: 12.556, 32.368, 45.036, 13.0
2019-02-13 18:35:39,812 : samples : 384000
2019-02-13 18:35:50,110 : Image to text: 14.86, 37.74, 50.8, 10.0
2019-02-13 18:35:57,556 : Text to Image: 12.204, 32.408, 44.96, 13.0
2019-02-13 18:36:37,932 : samples : 448000
2019-02-13 18:36:48,206 : Image to text: 15.24, 37.7, 51.42, 10.0
2019-02-13 18:36:55,634 : Text to Image: 12.132, 32.148, 44.68, 13.0
2019-02-13 18:37:36,938 : samples : 512000
2019-02-13 18:37:47,660 : Image to text: 15.64, 38.5, 51.46, 10.0
2019-02-13 18:37:55,008 : Text to Image: 12.268, 32.272, 44.932, 13.0
2019-02-13 18:38:30,939 : Epoch 7 finished
2019-02-13 18:38:31,374 : Image to text: 35.4, 66.6, 81.9, 3.0
2019-02-13 18:38:31,703 : Text to Image: 28.18, 63.2, 79.2, 3.0
2019-02-13 18:38:32,132 : Image to text: 33.7, 66.8, 82.3, 3.0
2019-02-13 18:38:32,460 : Text to Image: 28.38, 62.0, 78.44, 4.0
2019-02-13 18:38:32,892 : Image to text: 34.1, 69.4, 82.3, 3.0
2019-02-13 18:38:33,216 : Text to Image: 29.48, 64.44, 79.56, 3.0
2019-02-13 18:38:33,656 : Image to text: 34.3, 67.8, 82.4, 3.0
2019-02-13 18:38:33,986 : Text to Image: 28.3, 63.82, 79.62, 3.0
2019-02-13 18:38:34,417 : Image to text: 35.2, 66.7, 80.2, 3.0
2019-02-13 18:38:34,747 : Text to Image: 28.04, 62.76, 77.74, 3.0
2019-02-13 18:38:34,747 : Dev mean Text to Image: 28.476000000000003, 63.24399999999999, 78.91199999999999, 3.2
2019-02-13 18:38:34,747 : Dev mean Image to text: 34.54, 67.46000000000001, 81.82, 3.0
2019-02-13 18:38:34,747 : start epoch
2019-02-13 18:39:19,294 : samples : 64000
2019-02-13 18:39:29,584 : Image to text: 15.8, 37.44, 50.94, 10.0
2019-02-13 18:39:36,987 : Text to Image: 12.052, 32.14, 44.852, 13.0
2019-02-13 18:40:19,101 : samples : 128000
2019-02-13 18:40:29,340 : Image to text: 15.78, 37.92, 51.52, 10.0
2019-02-13 18:40:36,678 : Text to Image: 12.328, 32.216, 45.116, 13.0
2019-02-13 18:41:21,465 : samples : 192000
2019-02-13 18:41:31,752 : Image to text: 15.76, 38.62, 51.44, 10.0
2019-02-13 18:41:39,137 : Text to Image: 12.396, 32.528, 45.176, 13.0
2019-02-13 18:42:25,826 : samples : 256000
2019-02-13 18:42:36,058 : Image to text: 15.68, 37.68, 51.04, 10.0
2019-02-13 18:42:43,439 : Text to Image: 12.396, 32.156, 44.804, 13.0
2019-02-13 18:43:25,278 : samples : 320000
2019-02-13 18:43:35,520 : Image to text: 15.98, 38.8, 51.98, 10.0
2019-02-13 18:43:42,895 : Text to Image: 12.548, 32.872, 45.656, 13.0
2019-02-13 18:44:23,838 : samples : 384000
2019-02-13 18:44:34,094 : Image to text: 15.52, 38.14, 51.88, 10.0
2019-02-13 18:44:41,474 : Text to Image: 12.552, 32.692, 45.424, 13.0
2019-02-13 18:45:22,027 : samples : 448000
2019-02-13 18:45:32,299 : Image to text: 15.12, 36.72, 50.88, 10.0
2019-02-13 18:45:39,683 : Text to Image: 12.252, 32.028, 44.828, 13.0
2019-02-13 18:46:20,289 : samples : 512000
2019-02-13 18:46:30,557 : Image to text: 15.6, 37.9, 51.56, 10.0
2019-02-13 18:46:37,939 : Text to Image: 12.712, 32.5, 45.444, 13.0
2019-02-13 18:47:12,209 : Epoch 8 finished
2019-02-13 18:47:12,628 : Image to text: 36.1, 65.8, 82.0, 3.0
2019-02-13 18:47:12,952 : Text to Image: 28.22, 64.06, 79.28, 3.0
2019-02-13 18:47:13,381 : Image to text: 33.8, 67.4, 80.3, 3.0
2019-02-13 18:47:13,705 : Text to Image: 28.2, 61.62, 77.68, 3.0
2019-02-13 18:47:14,126 : Image to text: 34.8, 69.0, 82.6, 3.0
2019-02-13 18:47:14,455 : Text to Image: 29.42, 63.82, 79.26, 3.0
2019-02-13 18:47:14,882 : Image to text: 34.1, 68.3, 81.6, 3.0
2019-02-13 18:47:15,213 : Text to Image: 27.92, 63.08, 79.22, 3.0
2019-02-13 18:47:15,656 : Image to text: 37.2, 67.1, 80.7, 3.0
2019-02-13 18:47:15,986 : Text to Image: 28.64, 61.92, 78.34, 4.0
2019-02-13 18:47:15,986 : Dev mean Text to Image: 28.479999999999997, 62.900000000000006, 78.756, 3.2
2019-02-13 18:47:15,986 : Dev mean Image to text: 35.199999999999996, 67.52, 81.43999999999998, 3.0
2019-02-13 18:47:15,986 : start epoch
2019-02-13 18:47:56,211 : samples : 64000
2019-02-13 18:48:06,440 : Image to text: 16.4, 38.28, 51.44, 10.0
2019-02-13 18:48:13,821 : Text to Image: 12.388, 32.512, 45.356, 13.0
2019-02-13 18:48:53,860 : samples : 128000
2019-02-13 18:49:04,150 : Image to text: 15.98, 38.88, 51.5, 10.0
2019-02-13 18:49:11,572 : Text to Image: 12.296, 32.508, 44.82, 13.0
2019-02-13 18:49:54,574 : samples : 192000
2019-02-13 18:50:04,820 : Image to text: 15.88, 37.68, 51.48, 10.0
2019-02-13 18:50:12,227 : Text to Image: 12.116, 32.248, 44.716, 13.0
2019-02-13 18:50:54,982 : samples : 256000
2019-02-13 18:51:05,239 : Image to text: 16.3, 39.02, 51.78, 10.0
2019-02-13 18:51:12,679 : Text to Image: 12.476, 32.672, 45.564, 13.0
2019-02-13 18:51:55,825 : samples : 320000
2019-02-13 18:52:06,071 : Image to text: 16.24, 38.84, 52.2, 9.0
2019-02-13 18:52:13,524 : Text to Image: 12.62, 33.348, 45.68, 13.0
2019-02-13 18:52:56,640 : samples : 384000
2019-02-13 18:53:07,479 : Image to text: 15.92, 38.76, 51.08, 10.0
2019-02-13 18:53:14,794 : Text to Image: 12.368, 32.296, 45.22, 13.0
2019-02-13 18:53:57,684 : samples : 448000
2019-02-13 18:54:08,472 : Image to text: 16.94, 39.24, 52.24, 9.0
2019-02-13 18:54:15,812 : Text to Image: 12.652, 32.372, 45.044, 13.0
2019-02-13 18:54:58,905 : samples : 512000
2019-02-13 18:55:09,513 : Image to text: 15.88, 37.4, 51.38, 10.0
2019-02-13 18:55:16,951 : Text to Image: 12.592, 32.864, 45.648, 13.0
2019-02-13 18:55:53,638 : Epoch 9 finished
2019-02-13 18:55:54,264 : Image to text: 37.3, 69.7, 81.5, 2.0
2019-02-13 18:55:54,685 : Text to Image: 29.4, 64.42, 79.8, 3.0
2019-02-13 18:55:55,312 : Image to text: 34.7, 67.9, 82.3, 3.0
2019-02-13 18:55:55,732 : Text to Image: 28.1, 62.34, 78.68, 3.0
2019-02-13 18:55:56,359 : Image to text: 36.7, 70.1, 83.1, 2.0
2019-02-13 18:55:56,780 : Text to Image: 29.26, 63.94, 80.14, 3.0
2019-02-13 18:55:57,406 : Image to text: 35.5, 69.8, 82.7, 3.0
2019-02-13 18:55:57,828 : Text to Image: 28.84, 63.9, 79.6, 3.0
2019-02-13 18:55:58,454 : Image to text: 37.6, 69.2, 82.0, 2.0
2019-02-13 18:55:58,825 : Text to Image: 29.52, 62.98, 78.24, 3.0
2019-02-13 18:55:58,825 : Dev mean Text to Image: 29.024, 63.516000000000005, 79.29199999999999, 3.0
2019-02-13 18:55:58,825 : Dev mean Image to text: 36.36, 69.34, 82.32, 2.4
2019-02-13 18:55:58,825 : start epoch
2019-02-13 18:56:40,024 : samples : 64000
2019-02-13 18:56:50,286 : Image to text: 16.0, 39.64, 52.54, 9.0
2019-02-13 18:56:57,674 : Text to Image: 12.516, 33.004, 45.728, 13.0
2019-02-13 18:57:39,977 : samples : 128000
2019-02-13 18:57:50,238 : Image to text: 15.84, 38.2, 51.54, 10.0
2019-02-13 18:57:57,618 : Text to Image: 12.908, 32.924, 45.648, 13.0
2019-02-13 18:58:41,212 : samples : 192000
2019-02-13 18:58:51,431 : Image to text: 15.92, 38.88, 52.36, 9.0
2019-02-13 18:58:58,793 : Text to Image: 12.692, 32.432, 45.244, 13.0
2019-02-13 18:59:39,874 : samples : 256000
2019-02-13 18:59:50,131 : Image to text: 15.54, 38.6, 52.64, 9.0
2019-02-13 18:59:57,556 : Text to Image: 12.268, 32.432, 45.396, 13.0
2019-02-13 19:00:38,954 : samples : 320000
2019-02-13 19:00:49,196 : Image to text: 15.86, 38.52, 51.46, 10.0
2019-02-13 19:00:56,640 : Text to Image: 12.728, 33.148, 45.84, 13.0
2019-02-13 19:01:37,937 : samples : 384000
2019-02-13 19:01:48,207 : Image to text: 16.5, 39.4, 52.14, 9.0
2019-02-13 19:01:55,625 : Text to Image: 12.532, 32.872, 45.496, 13.0
2019-02-13 19:02:37,141 : samples : 448000
2019-02-13 19:02:47,524 : Image to text: 16.28, 38.76, 52.02, 10.0
2019-02-13 19:02:54,956 : Text to Image: 12.804, 33.028, 45.736, 13.0
2019-02-13 19:03:35,998 : samples : 512000
2019-02-13 19:03:46,416 : Image to text: 15.98, 37.78, 51.14, 10.0
2019-02-13 19:03:53,894 : Text to Image: 12.26, 32.98, 45.628, 13.0
2019-02-13 19:04:29,685 : Epoch 10 finished
2019-02-13 19:04:30,119 : Image to text: 37.2, 68.6, 82.3, 3.0
2019-02-13 19:04:30,446 : Text to Image: 29.06, 64.36, 79.74, 3.0
2019-02-13 19:04:30,868 : Image to text: 34.2, 67.5, 82.3, 3.0
2019-02-13 19:04:31,194 : Text to Image: 28.64, 62.18, 78.4, 3.0
2019-02-13 19:04:31,634 : Image to text: 37.2, 69.9, 83.8, 2.0
2019-02-13 19:04:31,962 : Text to Image: 29.38, 64.14, 79.22, 3.0
2019-02-13 19:04:32,387 : Image to text: 34.5, 69.6, 84.0, 3.0
2019-02-13 19:04:32,714 : Text to Image: 28.78, 63.6, 79.6, 3.0
2019-02-13 19:04:33,151 : Image to text: 36.6, 68.2, 81.8, 3.0
2019-02-13 19:04:33,480 : Text to Image: 28.66, 62.1, 78.34, 3.0
2019-02-13 19:04:33,480 : Dev mean Text to Image: 28.903999999999996, 63.275999999999996, 79.06, 3.0
2019-02-13 19:04:33,480 : Dev mean Image to text: 35.940000000000005, 68.76, 82.84, 2.8000000000000003
2019-02-13 19:04:33,480 : start epoch
2019-02-13 19:05:15,475 : samples : 64000
2019-02-13 19:05:25,818 : Image to text: 16.9, 39.32, 52.5, 9.0
2019-02-13 19:05:33,261 : Text to Image: 12.532, 32.544, 45.6, 13.0
2019-02-13 19:06:14,347 : samples : 128000
2019-02-13 19:06:24,855 : Image to text: 16.56, 38.6, 51.88, 10.0
2019-02-13 19:06:32,202 : Text to Image: 12.468, 32.676, 45.684, 13.0
2019-02-13 19:07:13,679 : samples : 192000
2019-02-13 19:07:24,440 : Image to text: 16.3, 39.38, 52.74, 9.0
2019-02-13 19:07:31,739 : Text to Image: 12.712, 32.828, 45.296, 13.0
2019-02-13 19:08:11,484 : samples : 256000
2019-02-13 19:08:22,129 : Image to text: 16.54, 39.02, 52.5, 9.0
2019-02-13 19:08:29,486 : Text to Image: 12.684, 32.972, 45.908, 13.0
2019-02-13 19:09:09,489 : samples : 320000
2019-02-13 19:09:19,888 : Image to text: 16.38, 38.98, 51.54, 10.0
2019-02-13 19:09:27,342 : Text to Image: 12.412, 32.852, 45.376, 13.0
2019-02-13 19:10:07,509 : samples : 384000
2019-02-13 19:10:17,749 : Image to text: 16.86, 39.56, 52.0, 10.0
2019-02-13 19:10:25,145 : Text to Image: 12.868, 33.532, 46.288, 12.0
2019-02-13 19:11:07,437 : samples : 448000
2019-02-13 19:11:17,672 : Image to text: 15.9, 38.2, 52.02, 10.0
2019-02-13 19:11:25,086 : Text to Image: 12.652, 33.148, 46.068, 13.0
2019-02-13 19:12:06,483 : samples : 512000
2019-02-13 19:12:16,759 : Image to text: 15.72, 39.16, 51.92, 10.0
2019-02-13 19:12:24,194 : Text to Image: 12.636, 33.052, 45.984, 13.0
2019-02-13 19:12:59,566 : Epoch 11 finished
2019-02-13 19:13:00,514 : Image to text: 38.2, 68.9, 83.7, 3.0
2019-02-13 19:13:01,276 : Text to Image: 28.58, 63.7, 79.18, 3.0
2019-02-13 19:13:02,242 : Image to text: 35.0, 69.1, 82.6, 3.0
2019-02-13 19:13:03,032 : Text to Image: 28.74, 62.16, 78.72, 3.0
2019-02-13 19:13:03,950 : Image to text: 35.5, 70.8, 84.1, 2.0
2019-02-13 19:13:04,717 : Text to Image: 30.0, 64.34, 78.52, 3.0
2019-02-13 19:13:05,667 : Image to text: 38.2, 69.8, 82.0, 3.0
2019-02-13 19:13:06,428 : Text to Image: 28.9, 63.92, 79.4, 3.0
2019-02-13 19:13:07,372 : Image to text: 37.7, 66.9, 80.9, 2.0
2019-02-13 19:13:08,127 : Text to Image: 28.08, 62.26, 77.96, 4.0
2019-02-13 19:13:08,127 : Dev mean Text to Image: 28.86, 63.275999999999996, 78.756, 3.2
2019-02-13 19:13:08,127 : Dev mean Image to text: 36.92, 69.10000000000001, 82.66, 2.6
2019-02-13 19:13:08,127 : start epoch
2019-02-13 19:13:52,954 : samples : 64000
2019-02-13 19:14:05,591 : Image to text: 15.82, 39.0, 51.72, 10.0
2019-02-13 19:14:15,652 : Text to Image: 12.824, 33.172, 46.112, 13.0
2019-02-13 19:14:57,629 : samples : 128000
2019-02-13 19:15:08,846 : Image to text: 16.34, 38.52, 51.56, 10.0
2019-02-13 19:15:18,863 : Text to Image: 12.772, 33.048, 46.024, 13.0
2019-02-13 19:16:02,908 : samples : 192000
2019-02-13 19:16:15,568 : Image to text: 16.46, 39.66, 53.08, 9.0
2019-02-13 19:16:25,594 : Text to Image: 12.884, 33.44, 46.116, 13.0
2019-02-13 19:17:06,973 : samples : 256000
2019-02-13 19:17:16,993 : Image to text: 16.46, 38.74, 52.16, 9.0
2019-02-13 19:17:27,036 : Text to Image: 12.908, 32.988, 45.432, 13.0
2019-02-13 19:18:10,654 : samples : 320000
2019-02-13 19:18:23,411 : Image to text: 15.96, 38.44, 51.98, 9.0
2019-02-13 19:18:33,538 : Text to Image: 12.868, 32.96, 45.648, 13.0
2019-02-13 19:19:15,266 : samples : 384000
2019-02-13 19:19:26,344 : Image to text: 16.3, 39.8, 52.88, 9.0
2019-02-13 19:19:36,347 : Text to Image: 12.996, 33.72, 46.216, 13.0
2019-02-13 19:20:21,299 : samples : 448000
2019-02-13 19:20:34,009 : Image to text: 16.54, 40.02, 53.94, 9.0
2019-02-13 19:20:43,115 : Text to Image: 12.988, 33.308, 46.02, 13.0
2019-02-13 19:21:25,100 : samples : 512000
2019-02-13 19:21:37,685 : Image to text: 16.42, 39.42, 52.44, 9.0
2019-02-13 19:21:47,665 : Text to Image: 12.824, 33.368, 46.356, 12.0
2019-02-13 19:22:25,683 : Epoch 12 finished
2019-02-13 19:22:26,664 : Image to text: 38.4, 68.8, 83.3, 2.0
2019-02-13 19:22:27,446 : Text to Image: 29.14, 64.94, 80.4, 3.0
2019-02-13 19:22:28,432 : Image to text: 35.6, 68.2, 83.2, 3.0
2019-02-13 19:22:29,189 : Text to Image: 28.78, 63.0, 78.9, 3.0
2019-02-13 19:22:30,156 : Image to text: 37.5, 71.8, 83.6, 2.0
2019-02-13 19:22:30,951 : Text to Image: 30.46, 65.1, 79.94, 3.0
2019-02-13 19:22:31,795 : Image to text: 36.4, 71.0, 83.7, 2.0
2019-02-13 19:22:32,558 : Text to Image: 29.8, 64.38, 80.02, 3.0
2019-02-13 19:22:33,492 : Image to text: 36.1, 70.4, 81.6, 2.0
2019-02-13 19:22:34,264 : Text to Image: 29.24, 63.08, 79.68, 3.0
2019-02-13 19:22:34,264 : Dev mean Text to Image: 29.484, 64.1, 79.788, 3.0
2019-02-13 19:22:34,264 : Dev mean Image to text: 36.8, 70.03999999999999, 83.07999999999998, 2.1999999999999997
2019-02-13 19:22:34,264 : start epoch
2019-02-13 19:23:16,286 : samples : 64000
2019-02-13 19:23:27,279 : Image to text: 17.3, 40.02, 53.64, 9.0
2019-02-13 19:23:37,322 : Text to Image: 12.9, 33.124, 46.188, 12.0
2019-02-13 19:24:21,330 : samples : 128000
2019-02-13 19:24:34,035 : Image to text: 16.26, 39.0, 52.48, 9.0
2019-02-13 19:24:44,145 : Text to Image: 12.872, 33.024, 46.008, 13.0
2019-02-13 19:25:25,313 : samples : 192000
2019-02-13 19:25:36,127 : Image to text: 16.24, 39.52, 52.88, 9.0
2019-02-13 19:25:46,141 : Text to Image: 12.772, 33.1, 46.384, 12.0
2019-02-13 19:26:29,892 : samples : 256000
2019-02-13 19:26:42,590 : Image to text: 16.94, 40.08, 53.14, 9.0
2019-02-13 19:26:52,647 : Text to Image: 12.848, 33.752, 46.704, 12.0
2019-02-13 19:27:34,285 : samples : 320000
2019-02-13 19:27:46,574 : Image to text: 16.44, 39.36, 52.28, 9.0
2019-02-13 19:27:56,585 : Text to Image: 12.972, 33.552, 46.708, 12.0
2019-02-13 19:28:40,670 : samples : 384000
2019-02-13 19:28:53,360 : Image to text: 16.62, 39.14, 52.42, 9.0
2019-02-13 19:29:03,155 : Text to Image: 12.664, 32.968, 45.808, 13.0
2019-02-13 19:29:44,544 : samples : 448000
2019-02-13 19:29:56,155 : Image to text: 16.56, 39.62, 53.04, 9.0
2019-02-13 19:30:06,168 : Text to Image: 12.776, 33.288, 45.912, 13.0
2019-02-13 19:30:51,344 : samples : 512000
2019-02-13 19:31:04,052 : Image to text: 16.8, 39.86, 52.58, 9.0
2019-02-13 19:31:14,161 : Text to Image: 12.772, 33.304, 46.384, 12.0
2019-02-13 19:31:50,100 : Epoch 13 finished
2019-02-13 19:31:50,552 : Image to text: 35.9, 69.5, 82.0, 3.0
2019-02-13 19:31:50,915 : Text to Image: 29.56, 64.74, 80.62, 3.0
2019-02-13 19:31:51,366 : Image to text: 34.2, 67.7, 82.5, 3.0
2019-02-13 19:31:51,729 : Text to Image: 29.66, 62.98, 79.1, 3.0
2019-02-13 19:31:52,180 : Image to text: 34.3, 70.0, 83.7, 3.0
2019-02-13 19:31:52,544 : Text to Image: 30.0, 65.0, 79.88, 3.0
2019-02-13 19:31:52,995 : Image to text: 36.7, 70.0, 83.4, 3.0
2019-02-13 19:31:53,358 : Text to Image: 29.76, 65.24, 80.82, 3.0
2019-02-13 19:31:53,809 : Image to text: 36.3, 68.7, 80.8, 2.0
2019-02-13 19:31:54,172 : Text to Image: 28.76, 63.3, 79.46, 3.0
2019-02-13 19:31:54,172 : Dev mean Text to Image: 29.548000000000002, 64.252, 79.976, 3.0
2019-02-13 19:31:54,172 : Dev mean Image to text: 35.48, 69.17999999999999, 82.47999999999999, 2.8
2019-02-13 19:31:58,233 : 
Test scores | Image to text:             36.8, 69.42, 82.52000000000001, 2.6
2019-02-13 19:31:58,233 : Test scores | Text to image:             28.731999999999996, 63.92399999999999, 79.376, 3.1999999999999997

2019-02-13 19:31:58,325 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 19:31:58,533 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 19:31:59,153 : loading BERT model bert-base-uncased
2019-02-13 19:31:59,153 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:31:59,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:31:59,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv43cgb6n
2019-02-13 19:32:01,631 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:32:03,019 : Computing embeddings for train/dev/test
2019-02-13 19:34:29,750 : Computed embeddings
2019-02-13 19:34:29,751 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:35:06,593 : [('reg:1e-05', 96.27), ('reg:0.0001', 92.04), ('reg:0.001', 89.19), ('reg:0.01', 84.78)]
2019-02-13 19:35:06,593 : Validation : best param found is reg = 1e-05 with score             96.27
2019-02-13 19:35:06,593 : Evaluating...
2019-02-13 19:35:15,523 : 
Dev acc : 96.3 Test acc : 96.8 for LENGTH classification

2019-02-13 19:35:15,524 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 19:35:15,873 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 19:35:15,917 : loading BERT model bert-base-uncased
2019-02-13 19:35:15,917 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:16,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:16,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppup2sxl9
2019-02-13 19:35:18,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:19,904 : Computing embeddings for train/dev/test
2019-02-13 19:37:10,801 : Computed embeddings
2019-02-13 19:37:10,802 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:37:48,490 : [('reg:1e-05', 82.59), ('reg:0.0001', 26.88), ('reg:0.001', 1.44), ('reg:0.01', 0.59)]
2019-02-13 19:37:48,490 : Validation : best param found is reg = 1e-05 with score             82.59
2019-02-13 19:37:48,490 : Evaluating...
2019-02-13 19:37:57,294 : 
Dev acc : 82.6 Test acc : 82.3 for WORDCONTENT classification

2019-02-13 19:37:57,295 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 19:37:57,820 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 19:37:57,886 : loading BERT model bert-base-uncased
2019-02-13 19:37:57,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:37:57,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:37:57,912 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi7aknm80
2019-02-13 19:38:00,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:38:01,830 : Computing embeddings for train/dev/test
2019-02-13 19:39:41,061 : Computed embeddings
2019-02-13 19:39:41,061 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:40:09,880 : [('reg:1e-05', 32.53), ('reg:0.0001', 32.27), ('reg:0.001', 30.94), ('reg:0.01', 26.68)]
2019-02-13 19:40:09,880 : Validation : best param found is reg = 1e-05 with score             32.53
2019-02-13 19:40:09,881 : Evaluating...
2019-02-13 19:40:17,987 : 
Dev acc : 32.5 Test acc : 32.2 for DEPTH classification

2019-02-13 19:40:17,988 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 19:40:18,545 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 19:40:18,607 : loading BERT model bert-base-uncased
2019-02-13 19:40:18,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:40:18,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:40:18,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqk5tsa68
2019-02-13 19:40:21,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:40:22,551 : Computing embeddings for train/dev/test
2019-02-13 19:42:02,507 : Computed embeddings
2019-02-13 19:42:02,508 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:42:43,536 : [('reg:1e-05', 54.57), ('reg:0.0001', 53.08), ('reg:0.001', 46.73), ('reg:0.01', 34.52)]
2019-02-13 19:42:43,536 : Validation : best param found is reg = 1e-05 with score             54.57
2019-02-13 19:42:43,536 : Evaluating...
2019-02-13 19:42:59,894 : 
Dev acc : 54.6 Test acc : 53.6 for TOPCONSTITUENTS classification

2019-02-13 19:42:59,895 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 19:43:00,274 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 19:43:00,340 : loading BERT model bert-base-uncased
2019-02-13 19:43:00,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:43:00,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:43:00,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl33y9a68
2019-02-13 19:43:02,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:43:04,314 : Computing embeddings for train/dev/test
2019-02-13 19:45:04,829 : Computed embeddings
2019-02-13 19:45:04,830 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:45:39,592 : [('reg:1e-05', 50.69), ('reg:0.0001', 50.6), ('reg:0.001', 50.65), ('reg:0.01', 50.42)]
2019-02-13 19:45:39,592 : Validation : best param found is reg = 1e-05 with score             50.69
2019-02-13 19:45:39,592 : Evaluating...
2019-02-13 19:45:52,826 : 
Dev acc : 50.7 Test acc : 49.5 for BIGRAMSHIFT classification

2019-02-13 19:45:52,827 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 19:45:53,216 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 19:45:53,282 : loading BERT model bert-base-uncased
2019-02-13 19:45:53,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:45:53,399 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:45:53,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg50561j6
2019-02-13 19:45:55,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:45:57,236 : Computing embeddings for train/dev/test
2019-02-13 19:47:43,416 : Computed embeddings
2019-02-13 19:47:43,416 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:48:12,305 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.44), ('reg:0.001', 86.33), ('reg:0.01', 84.54)]
2019-02-13 19:48:12,305 : Validation : best param found is reg = 0.0001 with score             86.44
2019-02-13 19:48:12,305 : Evaluating...
2019-02-13 19:48:19,430 : 
Dev acc : 86.4 Test acc : 84.5 for TENSE classification

2019-02-13 19:48:19,432 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 19:48:20,021 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 19:48:20,084 : loading BERT model bert-base-uncased
2019-02-13 19:48:20,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:48:20,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:48:20,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpesj2kvju
2019-02-13 19:48:22,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:48:23,958 : Computing embeddings for train/dev/test
2019-02-13 19:50:14,273 : Computed embeddings
2019-02-13 19:50:14,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:51:01,058 : [('reg:1e-05', 80.78), ('reg:0.0001', 80.71), ('reg:0.001', 80.93), ('reg:0.01', 77.83)]
2019-02-13 19:51:01,058 : Validation : best param found is reg = 0.001 with score             80.93
2019-02-13 19:51:01,058 : Evaluating...
2019-02-13 19:51:14,582 : 
Dev acc : 80.9 Test acc : 79.9 for SUBJNUMBER classification

2019-02-13 19:51:14,583 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 19:51:14,986 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 19:51:15,053 : loading BERT model bert-base-uncased
2019-02-13 19:51:15,054 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:51:15,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:51:15,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpngedj23j
2019-02-13 19:51:17,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:51:19,082 : Computing embeddings for train/dev/test
2019-02-13 19:53:17,793 : Computed embeddings
2019-02-13 19:53:17,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:54:09,539 : [('reg:1e-05', 79.73), ('reg:0.0001', 79.76), ('reg:0.001', 80.21), ('reg:0.01', 78.75)]
2019-02-13 19:54:09,540 : Validation : best param found is reg = 0.001 with score             80.21
2019-02-13 19:54:09,540 : Evaluating...
2019-02-13 19:54:20,758 : 
Dev acc : 80.2 Test acc : 81.0 for OBJNUMBER classification

2019-02-13 19:54:20,759 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 19:54:21,330 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 19:54:21,399 : loading BERT model bert-base-uncased
2019-02-13 19:54:21,400 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:54:21,428 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:54:21,428 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3xsxq7g_
2019-02-13 19:54:23,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:54:25,310 : Computing embeddings for train/dev/test
2019-02-13 19:56:38,126 : Computed embeddings
2019-02-13 19:56:38,127 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:57:34,069 : [('reg:1e-05', 50.2), ('reg:0.0001', 50.2), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-13 19:57:34,070 : Validation : best param found is reg = 1e-05 with score             50.2
2019-02-13 19:57:34,070 : Evaluating...
2019-02-13 19:57:48,138 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-13 19:57:48,138 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 19:57:48,744 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 19:57:48,821 : loading BERT model bert-base-uncased
2019-02-13 19:57:48,821 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:57:48,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:57:48,854 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0wga0vns
2019-02-13 19:57:51,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:57:52,808 : Computing embeddings for train/dev/test
2019-02-13 20:00:07,504 : Computed embeddings
2019-02-13 20:00:07,504 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:01:02,803 : [('reg:1e-05', 50.06), ('reg:0.0001', 50.15), ('reg:0.001', 50.41), ('reg:0.01', 50.16)]
2019-02-13 20:01:02,803 : Validation : best param found is reg = 0.001 with score             50.41
2019-02-13 20:01:02,803 : Evaluating...
2019-02-13 20:01:16,805 : 
Dev acc : 50.4 Test acc : 50.2 for COORDINATIONINVERSION classification

2019-02-13 20:01:16,807 : total results: {'STS12': {'MSRpar': {'pearson': (0.37608910341210966, 1.305449361113563e-26), 'spearman': SpearmanrResult(correlation=0.4190600708400811, pvalue=2.973455788524349e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5699373467373037, 8.103574084921277e-66), 'spearman': SpearmanrResult(correlation=0.5753441509054089, pvalue=2.5588776681847767e-67), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4763309328096535, 2.2576438655411545e-27), 'spearman': SpearmanrResult(correlation=0.5887682326789692, pvalue=3.640580160602892e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.655365515674495, 3.0625287614551527e-93), 'spearman': SpearmanrResult(correlation=0.6771461948143819, pvalue=9.799753750931137e-102), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4939590153135734, 6.199535817945441e-26), 'spearman': SpearmanrResult(correlation=0.4404915403911901, pvalue=2.2799315637405728e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5143363827894271, 'wmean': 0.520196113139536}, 'spearman': {'mean': 0.5401620379260063, 'wmean': 0.5468672959574117}}}, 'STS13': {'FNWN': {'pearson': (0.3675728689317999, 1.9602074059305347e-07), 'spearman': SpearmanrResult(correlation=0.36460726440830216, pvalue=2.496879825345467e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6577456469075137, 3.910586594304102e-94), 'spearman': SpearmanrResult(correlation=0.6415388251047481, pvalue=3.3272464573637005e-88), 'nsamples': 750}, 'OnWN': {'pearson': (0.405358828264975, 1.340119901632524e-23), 'spearman': SpearmanrResult(correlation=0.45944123278771665, pvalue=1.2098069795384704e-30), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4768924480347629, 'wmean': 0.5267912067102642}, 'spearman': {'mean': 0.48852910743358896, 'wmean': 0.5385409489304261}}}, 'STS14': {'deft-forum': {'pearson': (0.3368883651023242, 2.0992121173197024e-13), 'spearman': SpearmanrResult(correlation=0.3553104682416424, pvalue=7.766715138294184e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.6840271733037607, 1.0036651236134263e-42), 'spearman': SpearmanrResult(correlation=0.6730380425574104, pvalue=6.268303716640072e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6223013097943583, 1.2927931577194085e-81), 'spearman': SpearmanrResult(correlation=0.5876882779979462, pvalue=7.502578890201231e-71), 'nsamples': 750}, 'images': {'pearson': (0.5856500162079894, 2.9446079126518624e-70), 'spearman': SpearmanrResult(correlation=0.5888728253102319, pvalue=3.374182831798912e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.5576104973919013, 1.6898764913426272e-62), 'spearman': SpearmanrResult(correlation=0.6216878388199086, pvalue=2.0612260983420593e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5936557343924328, 1.2948072497284076e-72), 'spearman': SpearmanrResult(correlation=0.5864121785000831, pvalue=1.767974057606539e-70), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5633555160321279, 'wmean': 0.566992289233916}, 'spearman': {'mean': 0.5688349385712038, 'wmean': 0.5734125237192239}}}, 'STS15': {'answers-forums': {'pearson': (0.4530994709025457, 2.204019314100139e-20), 'spearman': SpearmanrResult(correlation=0.448218517167679, pvalue=6.236500081011466e-20), 'nsamples': 375}, 'answers-students': {'pearson': (0.6952283215630497, 2.2906005961765553e-109), 'spearman': SpearmanrResult(correlation=0.7051249557537382, pvalue=8.65837353428262e-114), 'nsamples': 750}, 'belief': {'pearson': (0.5276254709396421, 2.9113467062348843e-28), 'spearman': SpearmanrResult(correlation=0.526845891894358, pvalue=3.605291797068943e-28), 'nsamples': 375}, 'headlines': {'pearson': (0.6781822014087495, 3.705187024361473e-102), 'spearman': SpearmanrResult(correlation=0.6724094474534524, pvalue=7.953902314224241e-100), 'nsamples': 750}, 'images': {'pearson': (0.6893929757542225, 7.65653314262036e-107), 'spearman': SpearmanrResult(correlation=0.7051025954506968, pvalue=8.8641333566024e-114), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6087056881136419, 'wmean': 0.6382914924117788}, 'spearman': {'mean': 0.611540281543985, 'wmean': 0.6425423007972265}}}, 'STS16': {'answer-answer': {'pearson': (0.4588464988240848, 1.248930303173984e-14), 'spearman': SpearmanrResult(correlation=0.5082104051298644, pvalue=4.369034288251477e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6883578657783531, 2.6535380218994988e-36), 'spearman': SpearmanrResult(correlation=0.6926994792644391, pvalue=6.403482331441476e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6703933064785108, 2.222114107480498e-31), 'spearman': SpearmanrResult(correlation=0.6758228188546684, pvalue=4.804490939185053e-32), 'nsamples': 230}, 'postediting': {'pearson': (0.7656333390720964, 2.862029131465676e-48), 'spearman': SpearmanrResult(correlation=0.7897413873655879, pvalue=2.8305091607538467e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.4425016087832092, 1.9683991165745585e-11), 'spearman': SpearmanrResult(correlation=0.44613451406985594, pvalue=1.287989913917734e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6051465237872509, 'wmean': 0.6082933817364816}, 'spearman': {'mean': 0.6225217209368831, 'wmean': 0.626429910231204}}}, 'MR': {'devacc': 74.97, 'acc': 74.47, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.86, 'acc': 76.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.08, 'acc': 87.68, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.69, 'acc': 91.65, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.82, 'acc': 80.29, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.24, 'acc': 42.4, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.76, 'acc': 83.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.41, 'acc': 70.72, 'f1': 77.24, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.6, 'acc': 81.49, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.807462851994446, 'pearson': 0.8245061073607127, 'spearman': 0.7552315731337067, 'mse': 0.32676883589578126, 'yhat': array([3.81480848, 4.23778118, 1.4352887 , ..., 3.26990125, 4.42546991,        4.58557633]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7550856779166432, 'pearson': 0.7023578299620669, 'spearman': 0.7027351035990721, 'mse': 1.3971707294445477, 'yhat': array([1.49286679, 1.30544915, 2.05201972, ..., 3.82437181, 3.74310898,        3.10179553]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.86, 'acc': 68.88, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 363.29200000000003, 'acc': [(36.8, 69.42, 82.52000000000001, 2.6), (28.731999999999996, 63.92399999999999, 79.376, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 96.27, 'acc': 96.76, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 82.59, 'acc': 82.27, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.53, 'acc': 32.25, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.57, 'acc': 53.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.69, 'acc': 49.46, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.44, 'acc': 84.52, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.93, 'acc': 79.86, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.21, 'acc': 81.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.2, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.41, 'acc': 50.22, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 20:01:16,807 : STS12 p=0.5202, STS12 s=0.5469, STS13 p=0.5268, STS13 s=0.5385, STS14 p=0.5670, STS14 s=0.5734, STS15 p=0.6383, STS15 s=0.6425, STS 16 p=0.6083, STS16 s=0.6264, STS B p=0.7024, STS B s=0.7027, STS B m=1.3972, SICK-R p=0.8245, SICK-R s=0.7552, SICK-P m=0.3268
2019-02-13 20:01:16,807 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-13 20:01:16,807 : 0.5202,0.5469,0.5268,0.5385,0.5670,0.5734,0.6383,0.6425,0.6083,0.6264,0.7024,0.7027,1.3972,0.8245,0.7552,0.3268
2019-02-13 20:01:16,807 : MR=74.47, CR=76.18, SUBJ=91.65, MPQA=87.68, SST-B=80.29, SST-F=42.40, TREC=83.20, SICK-E=81.49, SNLI=68.88, MRPC=70.72, MRPC f=77.24
2019-02-13 20:01:16,807 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-13 20:01:16,807 : 74.47,76.18,91.65,87.68,80.29,42.40,83.20,81.49,68.88,70.72,77.24
2019-02-13 20:01:16,807 : COCO r1i2t=36.80, COCO r5i2t=69.42, COCO r10i2t=82.52, COCO medr_i2t=2.60, COCO r1t2i=28.73, COCO r5t2i=63.92, COCO r10t2i=79.38, COCO medr_t2i=3.20
2019-02-13 20:01:16,807 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-13 20:01:16,807 : 36.80,69.42,82.52,2.60,28.73,63.92,79.38,3.20
2019-02-13 20:01:16,807 : SentLen=96.76, WC=82.27, TreeDepth=32.25, TopConst=53.56, BShift=49.46, Tense=84.52, SubjNum=79.86, ObjNum=81.05, SOMO=49.87, CoordInv=50.22, average=65.98
2019-02-13 20:01:16,808 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-13 20:01:16,808 : 96.76,82.27,32.25,53.56,49.46,84.52,79.86,81.05,49.87,50.22,65.98
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,808 : layer 1
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,808 : ********************************************************************************
2019-02-13 20:01:16,892 : ***** Transfer task : STS12 *****


2019-02-13 20:01:16,904 : loading BERT model bert-base-uncased
2019-02-13 20:01:16,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:01:16,922 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:01:16,922 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsvv_ny0r
2019-02-13 20:01:19,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:01:23,088 : MSRpar : pearson = 0.4116, spearman = 0.4495
2019-02-13 20:01:24,565 : MSRvid : pearson = 0.6776, spearman = 0.6797
2019-02-13 20:01:25,671 : SMTeuroparl : pearson = 0.5043, spearman = 0.6039
2019-02-13 20:01:27,557 : surprise.OnWN : pearson = 0.7067, spearman = 0.6945
2019-02-13 20:01:28,615 : surprise.SMTnews : pearson = 0.5447, spearman = 0.5010
2019-02-13 20:01:28,615 : ALL (weighted average) : Pearson = 0.5778,             Spearman = 0.5936
2019-02-13 20:01:28,615 : ALL (average) : Pearson = 0.5690,             Spearman = 0.5857

2019-02-13 20:01:28,615 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 20:01:28,625 : loading BERT model bert-base-uncased
2019-02-13 20:01:28,625 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:01:28,642 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:01:28,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf3r5j0ey
2019-02-13 20:01:31,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:01:33,278 : FNWN : pearson = 0.4569, spearman = 0.4717
2019-02-13 20:01:35,206 : headlines : pearson = 0.6849, spearman = 0.6622
2019-02-13 20:01:36,482 : OnWN : pearson = 0.5627, spearman = 0.5900
2019-02-13 20:01:36,482 : ALL (weighted average) : Pearson = 0.6105,             Spearman = 0.6112
2019-02-13 20:01:36,482 : ALL (average) : Pearson = 0.5681,             Spearman = 0.5746

2019-02-13 20:01:36,482 : ***** Transfer task : STS14 *****


2019-02-13 20:01:36,498 : loading BERT model bert-base-uncased
2019-02-13 20:01:36,498 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:01:36,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:01:36,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq8yiway_
2019-02-13 20:01:38,949 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:01:41,410 : deft-forum : pearson = 0.3706, spearman = 0.3778
2019-02-13 20:01:42,331 : deft-news : pearson = 0.7257, spearman = 0.6912
2019-02-13 20:01:43,830 : headlines : pearson = 0.6490, spearman = 0.6007
2019-02-13 20:01:45,308 : images : pearson = 0.6885, spearman = 0.6666
2019-02-13 20:01:46,807 : OnWN : pearson = 0.6819, spearman = 0.7213
2019-02-13 20:01:48,548 : tweet-news : pearson = 0.7007, spearman = 0.6655
2019-02-13 20:01:48,548 : ALL (weighted average) : Pearson = 0.6466,             Spearman = 0.6315
2019-02-13 20:01:48,548 : ALL (average) : Pearson = 0.6361,             Spearman = 0.6205

2019-02-13 20:01:48,548 : ***** Transfer task : STS15 *****


2019-02-13 20:01:48,581 : loading BERT model bert-base-uncased
2019-02-13 20:01:48,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:01:48,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:01:48,599 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb1beyj7n
2019-02-13 20:01:51,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:01:53,514 : answers-forums : pearson = 0.5669, spearman = 0.5520
2019-02-13 20:01:54,971 : answers-students : pearson = 0.7242, spearman = 0.7330
2019-02-13 20:01:56,068 : belief : pearson = 0.6536, spearman = 0.6670
2019-02-13 20:01:57,634 : headlines : pearson = 0.7103, spearman = 0.6994
2019-02-13 20:01:59,160 : images : pearson = 0.7584, spearman = 0.7661
2019-02-13 20:01:59,160 : ALL (weighted average) : Pearson = 0.7008,             Spearman = 0.7020
2019-02-13 20:01:59,160 : ALL (average) : Pearson = 0.6827,             Spearman = 0.6835

2019-02-13 20:01:59,161 : ***** Transfer task : STS16 *****


2019-02-13 20:01:59,230 : loading BERT model bert-base-uncased
2019-02-13 20:01:59,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:01:59,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:01:59,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7z7njfbn
2019-02-13 20:02:01,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:02:03,699 : answer-answer : pearson = 0.4951, spearman = 0.5277
2019-02-13 20:02:04,201 : headlines : pearson = 0.7121, spearman = 0.7129
2019-02-13 20:02:04,769 : plagiarism : pearson = 0.7685, spearman = 0.7704
2019-02-13 20:02:05,573 : postediting : pearson = 0.8227, spearman = 0.8362
2019-02-13 20:02:06,065 : question-question : pearson = 0.4694, spearman = 0.4658
2019-02-13 20:02:06,065 : ALL (weighted average) : Pearson = 0.6566,             Spearman = 0.6662
2019-02-13 20:02:06,065 : ALL (average) : Pearson = 0.6536,             Spearman = 0.6626

2019-02-13 20:02:06,066 : ***** Transfer task : MR *****


2019-02-13 20:02:06,084 : loading BERT model bert-base-uncased
2019-02-13 20:02:06,084 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:02:06,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:02:06,103 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbcvnaauq
2019-02-13 20:02:08,533 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:02:10,016 : Generating sentence embeddings
2019-02-13 20:02:27,152 : Generated sentence embeddings
2019-02-13 20:02:27,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 20:02:44,937 : Best param found at split 1: l2reg = 0.001                 with score 75.49
2019-02-13 20:03:06,126 : Best param found at split 2: l2reg = 0.001                 with score 74.87
2019-02-13 20:03:27,452 : Best param found at split 3: l2reg = 0.001                 with score 75.29
2019-02-13 20:03:50,983 : Best param found at split 4: l2reg = 0.001                 with score 74.94
2019-02-13 20:04:16,045 : Best param found at split 5: l2reg = 0.001                 with score 75.24
2019-02-13 20:04:17,348 : Dev acc : 75.17 Test acc : 74.71

2019-02-13 20:04:17,349 : ***** Transfer task : CR *****


2019-02-13 20:04:17,356 : loading BERT model bert-base-uncased
2019-02-13 20:04:17,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:04:17,377 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:04:17,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprs8r9q9j
2019-02-13 20:04:19,815 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:04:21,277 : Generating sentence embeddings
2019-02-13 20:04:26,601 : Generated sentence embeddings
2019-02-13 20:04:26,602 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 20:04:35,613 : Best param found at split 1: l2reg = 0.0001                 with score 79.17
2019-02-13 20:04:44,474 : Best param found at split 2: l2reg = 0.0001                 with score 79.36
2019-02-13 20:04:51,804 : Best param found at split 3: l2reg = 0.01                 with score 80.56
2019-02-13 20:05:00,053 : Best param found at split 4: l2reg = 0.01                 with score 79.28
2019-02-13 20:05:08,963 : Best param found at split 5: l2reg = 0.001                 with score 79.54
2019-02-13 20:05:09,302 : Dev acc : 79.58 Test acc : 78.01

2019-02-13 20:05:09,302 : ***** Transfer task : MPQA *****


2019-02-13 20:05:09,308 : loading BERT model bert-base-uncased
2019-02-13 20:05:09,308 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:05:09,327 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:05:09,327 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdxooxgkn
2019-02-13 20:05:11,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:05:13,295 : Generating sentence embeddings
2019-02-13 20:05:19,833 : Generated sentence embeddings
2019-02-13 20:05:19,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 20:05:32,096 : Best param found at split 1: l2reg = 0.01                 with score 87.33
2019-02-13 20:05:45,449 : Best param found at split 2: l2reg = 0.001                 with score 87.97
2019-02-13 20:05:58,652 : Best param found at split 3: l2reg = 0.001                 with score 88.26
2019-02-13 20:06:11,908 : Best param found at split 4: l2reg = 0.001                 with score 87.98
2019-02-13 20:06:25,569 : Best param found at split 5: l2reg = 0.01                 with score 87.84
2019-02-13 20:06:26,237 : Dev acc : 87.88 Test acc : 87.88

2019-02-13 20:06:26,238 : ***** Transfer task : SUBJ *****


2019-02-13 20:06:26,254 : loading BERT model bert-base-uncased
2019-02-13 20:06:26,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:06:26,275 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:06:26,275 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbx6azky6
2019-02-13 20:06:28,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:06:30,128 : Generating sentence embeddings
2019-02-13 20:06:44,040 : Generated sentence embeddings
2019-02-13 20:06:44,040 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 20:06:53,787 : Best param found at split 1: l2reg = 0.0001                 with score 92.91
2019-02-13 20:07:05,227 : Best param found at split 2: l2reg = 0.001                 with score 93.22
2019-02-13 20:07:14,567 : Best param found at split 3: l2reg = 1e-05                 with score 92.65
2019-02-13 20:07:31,809 : Best param found at split 4: l2reg = 1e-05                 with score 93.14
2019-02-13 20:07:53,914 : Best param found at split 5: l2reg = 0.0001                 with score 92.88
2019-02-13 20:07:54,947 : Dev acc : 92.96 Test acc : 92.45

2019-02-13 20:07:54,948 : ***** Transfer task : SST Binary classification *****


2019-02-13 20:07:55,079 : loading BERT model bert-base-uncased
2019-02-13 20:07:55,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:07:55,101 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:07:55,101 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyx7xvi44
2019-02-13 20:07:57,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:07:59,037 : Computing embedding for train
2019-02-13 20:09:18,444 : Computed train embeddings
2019-02-13 20:09:18,444 : Computing embedding for dev
2019-02-13 20:09:19,830 : Computed dev embeddings
2019-02-13 20:09:19,830 : Computing embedding for test
2019-02-13 20:09:22,831 : Computed test embeddings
2019-02-13 20:09:22,831 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:10:07,157 : [('reg:1e-05', 77.98), ('reg:0.0001', 78.67), ('reg:0.001', 77.98), ('reg:0.01', 77.52)]
2019-02-13 20:10:07,157 : Validation : best param found is reg = 0.0001 with score             78.67
2019-02-13 20:10:07,157 : Evaluating...
2019-02-13 20:10:17,612 : 
Dev acc : 78.67 Test acc : 76.55 for             SST Binary classification

2019-02-13 20:10:17,612 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 20:10:17,662 : loading BERT model bert-base-uncased
2019-02-13 20:10:17,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:10:17,685 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:10:17,685 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpub6gutas
2019-02-13 20:10:20,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:10:21,623 : Computing embedding for train
2019-02-13 20:10:35,486 : Computed train embeddings
2019-02-13 20:10:35,486 : Computing embedding for dev
2019-02-13 20:10:37,293 : Computed dev embeddings
2019-02-13 20:10:37,294 : Computing embedding for test
2019-02-13 20:10:40,839 : Computed test embeddings
2019-02-13 20:10:40,839 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:10:46,350 : [('reg:1e-05', 40.05), ('reg:0.0001', 39.87), ('reg:0.001', 39.78), ('reg:0.01', 39.87)]
2019-02-13 20:10:46,350 : Validation : best param found is reg = 1e-05 with score             40.05
2019-02-13 20:10:46,350 : Evaluating...
2019-02-13 20:10:47,540 : 
Dev acc : 40.05 Test acc : 41.22 for             SST Fine-Grained classification

2019-02-13 20:10:47,540 : ***** Transfer task : TREC *****


2019-02-13 20:10:47,553 : loading BERT model bert-base-uncased
2019-02-13 20:10:47,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:10:47,573 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:10:47,573 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpspgh3n9c
2019-02-13 20:10:50,005 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:10:57,858 : Computed train embeddings
2019-02-13 20:10:58,435 : Computed test embeddings
2019-02-13 20:10:58,435 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 20:11:14,111 : [('reg:1e-05', 79.0), ('reg:0.0001', 79.18), ('reg:0.001', 78.98), ('reg:0.01', 69.88)]
2019-02-13 20:11:14,111 : Cross-validation : best param found is reg = 0.0001             with score 79.18
2019-02-13 20:11:14,111 : Evaluating...
2019-02-13 20:11:15,042 : 
Dev acc : 79.18 Test acc : 90.0             for TREC

2019-02-13 20:11:15,043 : ***** Transfer task : MRPC *****


2019-02-13 20:11:15,064 : loading BERT model bert-base-uncased
2019-02-13 20:11:15,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:11:15,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:11:15,085 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn0e7ztit
2019-02-13 20:11:17,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:11:19,003 : Computing embedding for train
2019-02-13 20:11:32,293 : Computed train embeddings
2019-02-13 20:11:32,293 : Computing embedding for test
2019-02-13 20:11:37,921 : Computed test embeddings
2019-02-13 20:11:37,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 20:11:49,244 : [('reg:1e-05', 73.85), ('reg:0.0001', 73.75), ('reg:0.001', 73.75), ('reg:0.01', 73.04)]
2019-02-13 20:11:49,244 : Cross-validation : best param found is reg = 1e-05             with score 73.85
2019-02-13 20:11:49,244 : Evaluating...
2019-02-13 20:11:50,025 : Dev acc : 73.85 Test acc 73.22; Test F1 81.31 for MRPC.

2019-02-13 20:11:50,025 : ***** Transfer task : SICK-Entailment*****


2019-02-13 20:11:50,049 : loading BERT model bert-base-uncased
2019-02-13 20:11:50,050 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:11:50,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:11:50,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiyu_f662
2019-02-13 20:11:52,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:11:54,017 : Computing embedding for train
2019-02-13 20:12:04,203 : Computed train embeddings
2019-02-13 20:12:04,203 : Computing embedding for dev
2019-02-13 20:12:05,462 : Computed dev embeddings
2019-02-13 20:12:05,462 : Computing embedding for test
2019-02-13 20:12:16,911 : Computed test embeddings
2019-02-13 20:12:16,939 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:12:20,877 : [('reg:1e-05', 80.6), ('reg:0.0001', 80.6), ('reg:0.001', 80.4), ('reg:0.01', 81.2)]
2019-02-13 20:12:20,877 : Validation : best param found is reg = 0.01 with score             81.2
2019-02-13 20:12:20,877 : Evaluating...
2019-02-13 20:12:21,884 : 
Dev acc : 81.2 Test acc : 79.56 for                        SICK entailment

2019-02-13 20:12:21,885 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 20:12:21,912 : loading BERT model bert-base-uncased
2019-02-13 20:12:21,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:12:21,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:12:21,950 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgivcnvhn
2019-02-13 20:12:24,385 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:12:25,890 : Computing embedding for train
2019-02-13 20:12:36,013 : Computed train embeddings
2019-02-13 20:12:36,013 : Computing embedding for dev
2019-02-13 20:12:37,374 : Computed dev embeddings
2019-02-13 20:12:37,374 : Computing embedding for test
2019-02-13 20:12:48,753 : Computed test embeddings
2019-02-13 20:13:30,785 : Dev : Pearson 0.8237443352877583
2019-02-13 20:13:30,785 : Test : Pearson 0.8329372932713901 Spearman 0.7594390801065386 MSE 0.3134001024837591                        for SICK Relatedness

2019-02-13 20:13:30,786 : 

***** Transfer task : STSBenchmark*****


2019-02-13 20:13:30,855 : loading BERT model bert-base-uncased
2019-02-13 20:13:30,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:13:30,876 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:13:30,876 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxlzkd3x9
2019-02-13 20:13:33,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:13:34,823 : Computing embedding for train
2019-02-13 20:13:49,275 : Computed train embeddings
2019-02-13 20:13:49,275 : Computing embedding for dev
2019-02-13 20:13:53,334 : Computed dev embeddings
2019-02-13 20:13:53,334 : Computing embedding for test
2019-02-13 20:13:56,759 : Computed test embeddings
2019-02-13 20:14:46,470 : Dev : Pearson 0.7559527821468942
2019-02-13 20:14:46,470 : Test : Pearson 0.7058771519466621 Spearman 0.703691280303891 MSE 1.3937725223639443                        for SICK Relatedness

2019-02-13 20:14:46,471 : ***** Transfer task : SNLI Entailment*****


2019-02-13 20:14:51,162 : loading BERT model bert-base-uncased
2019-02-13 20:14:51,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:14:51,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:14:51,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0zskh77r
2019-02-13 20:14:53,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:14:55,544 : PROGRESS (encoding): 0.00%
2019-02-13 20:17:37,277 : PROGRESS (encoding): 14.56%
2019-02-13 20:19:12,093 : PROGRESS (encoding): 29.12%
2019-02-13 20:20:41,421 : PROGRESS (encoding): 43.69%
2019-02-13 20:22:19,074 : PROGRESS (encoding): 58.25%
2019-02-13 20:24:05,781 : PROGRESS (encoding): 72.81%
2019-02-13 20:25:52,541 : PROGRESS (encoding): 87.37%
2019-02-13 20:27:44,200 : PROGRESS (encoding): 0.00%
2019-02-13 20:27:57,984 : PROGRESS (encoding): 0.00%
2019-02-13 20:28:11,846 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:28:42,477 : [('reg:1e-09', 68.44)]
2019-02-13 20:28:42,478 : Validation : best param found is reg = 1e-09 with score             68.44
2019-02-13 20:28:42,478 : Evaluating...
2019-02-13 20:29:12,718 : Dev acc : 68.44 Test acc : 68.45 for SNLI

2019-02-13 20:29:12,718 : ***** Transfer task: Image Caption Retrieval *****


2019-02-13 20:29:21,424 : loading BERT model bert-base-uncased
2019-02-13 20:29:21,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:29:21,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:29:21,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplgd797uk
2019-02-13 20:29:23,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:29:25,363 : Computing embedding for train
2019-02-13 20:37:12,055 : Computed train embeddings
2019-02-13 20:37:12,056 : Computing embedding for dev
2019-02-13 20:37:31,841 : Computed dev embeddings
2019-02-13 20:37:31,841 : Computing embedding for test
2019-02-13 20:37:52,723 : Computed test embeddings
2019-02-13 20:37:52,739 : prepare data
2019-02-13 20:37:52,800 : start epoch
2019-02-13 20:38:33,353 : samples : 64000
2019-02-13 20:38:43,627 : Image to text: 8.14, 23.58, 34.2, 23.0
2019-02-13 20:38:51,109 : Text to Image: 6.34, 20.072, 30.784, 26.0
2019-02-13 20:39:33,795 : samples : 128000
2019-02-13 20:39:44,047 : Image to text: 9.68, 27.78, 39.58, 18.0
2019-02-13 20:39:51,626 : Text to Image: 7.492, 23.028, 34.2, 22.0
2019-02-13 20:40:32,671 : samples : 192000
2019-02-13 20:40:42,928 : Image to text: 10.24, 27.84, 40.0, 17.0
2019-02-13 20:40:50,484 : Text to Image: 8.136, 23.712, 35.204, 21.0
2019-02-13 20:41:31,121 : samples : 256000
2019-02-13 20:41:41,396 : Image to text: 9.72, 27.68, 40.82, 17.0
2019-02-13 20:41:48,973 : Text to Image: 8.088, 23.992, 35.628, 20.0
2019-02-13 20:42:29,112 : samples : 320000
2019-02-13 20:42:39,319 : Image to text: 10.42, 29.72, 41.38, 16.0
2019-02-13 20:42:46,846 : Text to Image: 8.704, 24.908, 36.688, 19.0
2019-02-13 20:43:27,120 : samples : 384000
2019-02-13 20:43:37,342 : Image to text: 11.72, 30.76, 42.98, 15.0
2019-02-13 20:43:44,861 : Text to Image: 9.276, 26.448, 38.188, 18.0
2019-02-13 20:44:25,349 : samples : 448000
2019-02-13 20:44:35,608 : Image to text: 11.18, 30.3, 41.98, 15.0
2019-02-13 20:44:43,163 : Text to Image: 8.52, 25.196, 36.856, 19.0
2019-02-13 20:45:24,636 : samples : 512000
2019-02-13 20:45:34,818 : Image to text: 11.9, 31.7, 43.58, 14.0
2019-02-13 20:45:42,186 : Text to Image: 9.512, 26.84, 38.32, 18.0
2019-02-13 20:46:16,237 : Epoch 1 finished
2019-02-13 20:46:16,686 : Image to text: 29.7, 60.7, 76.9, 4.0
2019-02-13 20:46:17,027 : Text to Image: 23.78, 56.28, 73.6, 4.0
2019-02-13 20:46:17,481 : Image to text: 28.3, 60.5, 75.4, 3.0
2019-02-13 20:46:17,822 : Text to Image: 23.74, 55.92, 72.88, 4.0
2019-02-13 20:46:18,274 : Image to text: 29.5, 63.0, 76.9, 3.0
2019-02-13 20:46:18,615 : Text to Image: 23.82, 56.9, 73.9, 4.0
2019-02-13 20:46:19,066 : Image to text: 30.4, 63.0, 75.0, 3.0
2019-02-13 20:46:19,407 : Text to Image: 23.24, 56.52, 73.88, 4.0
2019-02-13 20:46:19,852 : Image to text: 30.4, 61.3, 76.8, 3.0
2019-02-13 20:46:20,193 : Text to Image: 23.46, 56.34, 72.96, 4.0
2019-02-13 20:46:20,193 : Dev mean Text to Image: 23.608, 56.392, 73.444, 4.0
2019-02-13 20:46:20,193 : Dev mean Image to text: 29.659999999999997, 61.7, 76.2, 3.2
2019-02-13 20:46:20,193 : start epoch
2019-02-13 20:47:00,646 : samples : 64000
2019-02-13 20:47:10,942 : Image to text: 12.24, 32.24, 44.7, 13.0
2019-02-13 20:47:18,470 : Text to Image: 9.812, 27.48, 39.5, 17.0
2019-02-13 20:47:59,089 : samples : 128000
2019-02-13 20:48:09,377 : Image to text: 11.74, 31.12, 44.52, 14.0
2019-02-13 20:48:16,946 : Text to Image: 9.888, 27.892, 39.644, 17.0
2019-02-13 20:48:58,635 : samples : 192000
2019-02-13 20:49:08,919 : Image to text: 12.26, 32.48, 45.08, 13.0
2019-02-13 20:49:16,519 : Text to Image: 9.956, 27.72, 39.392, 17.0
2019-02-13 20:49:58,479 : samples : 256000
2019-02-13 20:50:08,795 : Image to text: 12.96, 33.86, 47.3, 12.0
2019-02-13 20:50:16,403 : Text to Image: 10.344, 28.656, 40.748, 16.0
2019-02-13 20:50:58,701 : samples : 320000
2019-02-13 20:51:08,909 : Image to text: 12.62, 32.66, 45.54, 13.0
2019-02-13 20:51:16,477 : Text to Image: 10.172, 28.244, 40.324, 16.0
2019-02-13 20:51:58,913 : samples : 384000
2019-02-13 20:52:09,109 : Image to text: 12.6, 34.1, 46.36, 13.0
2019-02-13 20:52:16,685 : Text to Image: 10.204, 28.58, 40.844, 16.0
2019-02-13 20:52:58,964 : samples : 448000
2019-02-13 20:53:09,244 : Image to text: 13.18, 33.82, 47.22, 12.0
2019-02-13 20:53:16,862 : Text to Image: 10.36, 28.728, 40.736, 16.0
2019-02-13 20:53:57,219 : samples : 512000
2019-02-13 20:54:07,397 : Image to text: 12.62, 33.52, 46.6, 12.0
2019-02-13 20:54:14,813 : Text to Image: 10.328, 29.076, 41.044, 16.0
2019-02-13 20:54:48,681 : Epoch 2 finished
2019-02-13 20:54:49,136 : Image to text: 31.9, 63.8, 79.1, 3.0
2019-02-13 20:54:49,476 : Text to Image: 25.38, 59.82, 76.64, 4.0
2019-02-13 20:54:49,934 : Image to text: 30.0, 63.3, 78.2, 3.0
2019-02-13 20:54:50,275 : Text to Image: 25.42, 57.72, 74.6, 4.0
2019-02-13 20:54:50,741 : Image to text: 30.7, 65.1, 78.0, 3.0
2019-02-13 20:54:51,081 : Text to Image: 25.86, 59.82, 75.56, 4.0
2019-02-13 20:54:51,539 : Image to text: 31.2, 65.5, 79.8, 3.0
2019-02-13 20:54:51,879 : Text to Image: 24.9, 59.2, 75.34, 4.0
2019-02-13 20:54:52,336 : Image to text: 31.9, 65.8, 78.4, 3.0
2019-02-13 20:54:52,676 : Text to Image: 25.34, 57.6, 74.54, 4.0
2019-02-13 20:54:52,677 : Dev mean Text to Image: 25.380000000000003, 58.831999999999994, 75.336, 4.0
2019-02-13 20:54:52,677 : Dev mean Image to text: 31.139999999999997, 64.7, 78.7, 3.0
2019-02-13 20:54:52,677 : start epoch
2019-02-13 20:55:33,175 : samples : 64000
2019-02-13 20:55:43,545 : Image to text: 12.6, 34.56, 47.84, 12.0
2019-02-13 20:55:51,180 : Text to Image: 10.244, 28.656, 40.74, 16.0
2019-02-13 20:56:31,259 : samples : 128000
2019-02-13 20:56:41,580 : Image to text: 13.34, 35.24, 47.72, 12.0
2019-02-13 20:56:49,235 : Text to Image: 10.844, 29.22, 41.236, 16.0
2019-02-13 20:57:29,523 : samples : 192000
2019-02-13 20:57:39,810 : Image to text: 14.1, 35.02, 48.36, 11.0
2019-02-13 20:57:47,477 : Text to Image: 10.848, 29.868, 42.412, 15.0
2019-02-13 20:58:29,726 : samples : 256000
2019-02-13 20:58:40,017 : Image to text: 13.74, 35.2, 48.28, 11.0
2019-02-13 20:58:47,665 : Text to Image: 10.776, 30.22, 42.652, 15.0
2019-02-13 20:59:31,793 : samples : 320000
2019-02-13 20:59:42,147 : Image to text: 13.14, 34.1, 46.92, 12.0
2019-02-13 20:59:49,677 : Text to Image: 10.552, 29.24, 41.476, 16.0
2019-02-13 21:00:30,592 : samples : 384000
2019-02-13 21:00:40,922 : Image to text: 13.44, 33.88, 46.42, 12.0
2019-02-13 21:00:48,589 : Text to Image: 10.332, 29.236, 41.288, 16.0
2019-02-13 21:01:29,686 : samples : 448000
2019-02-13 21:01:39,910 : Image to text: 14.26, 35.92, 49.16, 11.0
2019-02-13 21:01:47,570 : Text to Image: 11.12, 30.524, 42.964, 15.0
2019-02-13 21:02:31,547 : samples : 512000
2019-02-13 21:02:41,795 : Image to text: 14.1, 35.8, 48.7, 11.0
2019-02-13 21:02:49,269 : Text to Image: 10.996, 30.264, 42.42, 15.0
2019-02-13 21:03:24,496 : Epoch 3 finished
2019-02-13 21:03:24,950 : Image to text: 31.5, 66.0, 79.8, 3.0
2019-02-13 21:03:25,291 : Text to Image: 26.88, 61.14, 77.76, 4.0
2019-02-13 21:03:25,756 : Image to text: 31.8, 64.6, 79.6, 3.0
2019-02-13 21:03:26,097 : Text to Image: 26.24, 60.34, 75.82, 4.0
2019-02-13 21:03:26,550 : Image to text: 32.2, 65.3, 80.4, 3.0
2019-02-13 21:03:26,890 : Text to Image: 26.8, 61.28, 77.48, 4.0
2019-02-13 21:03:27,352 : Image to text: 33.0, 66.8, 80.1, 3.0
2019-02-13 21:03:27,692 : Text to Image: 25.66, 60.64, 77.36, 4.0
2019-02-13 21:03:28,143 : Image to text: 33.7, 65.4, 79.5, 3.0
2019-02-13 21:03:28,483 : Text to Image: 25.72, 59.68, 75.44, 4.0
2019-02-13 21:03:28,483 : Dev mean Text to Image: 26.259999999999998, 60.616, 76.772, 4.0
2019-02-13 21:03:28,483 : Dev mean Image to text: 32.440000000000005, 65.61999999999999, 79.88, 3.0
2019-02-13 21:03:28,484 : start epoch
2019-02-13 21:04:08,862 : samples : 64000
2019-02-13 21:04:19,145 : Image to text: 14.16, 35.16, 49.14, 11.0
2019-02-13 21:04:26,791 : Text to Image: 10.968, 30.176, 42.96, 15.0
2019-02-13 21:05:08,774 : samples : 128000
2019-02-13 21:05:19,104 : Image to text: 13.7, 35.3, 49.02, 11.0
2019-02-13 21:05:26,772 : Text to Image: 11.036, 30.088, 43.14, 14.0
2019-02-13 21:06:09,088 : samples : 192000
2019-02-13 21:06:19,460 : Image to text: 13.78, 35.76, 48.92, 11.0
2019-02-13 21:06:27,131 : Text to Image: 10.876, 29.708, 41.804, 15.0
2019-02-13 21:07:09,222 : samples : 256000
2019-02-13 21:07:19,504 : Image to text: 14.4, 35.78, 48.84, 11.0
2019-02-13 21:07:27,151 : Text to Image: 11.212, 30.536, 43.296, 14.0
2019-02-13 21:08:09,618 : samples : 320000
2019-02-13 21:08:19,868 : Image to text: 13.94, 36.04, 49.04, 11.0
2019-02-13 21:08:27,453 : Text to Image: 11.148, 30.128, 42.408, 15.0
2019-02-13 21:09:10,066 : samples : 384000
2019-02-13 21:09:20,324 : Image to text: 14.28, 36.42, 49.16, 11.0
2019-02-13 21:09:27,901 : Text to Image: 11.428, 30.504, 43.02, 14.0
2019-02-13 21:10:10,692 : samples : 448000
2019-02-13 21:10:20,904 : Image to text: 14.16, 36.06, 49.46, 11.0
2019-02-13 21:10:28,418 : Text to Image: 11.544, 30.936, 43.452, 14.0
2019-02-13 21:11:11,369 : samples : 512000
2019-02-13 21:11:21,550 : Image to text: 14.06, 35.88, 49.06, 11.0
2019-02-13 21:11:28,972 : Text to Image: 11.012, 30.416, 42.896, 15.0
2019-02-13 21:12:08,923 : Epoch 4 finished
2019-02-13 21:12:09,383 : Image to text: 35.0, 67.1, 80.9, 3.0
2019-02-13 21:12:09,724 : Text to Image: 27.68, 62.74, 78.28, 3.0
2019-02-13 21:12:10,190 : Image to text: 34.0, 66.8, 79.0, 3.0
2019-02-13 21:12:10,532 : Text to Image: 27.22, 60.88, 76.68, 4.0
2019-02-13 21:12:10,994 : Image to text: 34.3, 68.8, 82.1, 3.0
2019-02-13 21:12:11,292 : Text to Image: 27.28, 62.42, 77.98, 3.0
2019-02-13 21:12:11,688 : Image to text: 33.7, 66.2, 81.1, 3.0
2019-02-13 21:12:11,986 : Text to Image: 27.44, 61.74, 77.44, 4.0
2019-02-13 21:12:12,381 : Image to text: 35.6, 67.7, 80.9, 3.0
2019-02-13 21:12:12,678 : Text to Image: 26.62, 61.08, 76.84, 4.0
2019-02-13 21:12:12,678 : Dev mean Text to Image: 27.247999999999998, 61.772, 77.444, 3.5999999999999996
2019-02-13 21:12:12,678 : Dev mean Image to text: 34.519999999999996, 67.32000000000001, 80.80000000000001, 3.0
2019-02-13 21:12:12,679 : start epoch
2019-02-13 21:13:01,064 : samples : 64000
2019-02-13 21:13:11,274 : Image to text: 14.94, 36.54, 49.76, 11.0
2019-02-13 21:13:18,756 : Text to Image: 11.744, 31.072, 43.56, 14.0
2019-02-13 21:14:01,617 : samples : 128000
2019-02-13 21:14:11,803 : Image to text: 13.76, 36.34, 50.36, 10.0
2019-02-13 21:14:19,275 : Text to Image: 11.56, 30.984, 43.704, 14.0
2019-02-13 21:15:02,069 : samples : 192000
2019-02-13 21:15:12,251 : Image to text: 13.88, 34.82, 48.76, 11.0
2019-02-13 21:15:19,702 : Text to Image: 11.248, 30.052, 43.12, 14.0
2019-02-13 21:16:02,640 : samples : 256000
2019-02-13 21:16:12,811 : Image to text: 14.78, 36.36, 49.32, 11.0
2019-02-13 21:16:20,241 : Text to Image: 11.288, 30.9, 43.468, 14.0
2019-02-13 21:17:03,298 : samples : 320000
2019-02-13 21:17:13,477 : Image to text: 14.78, 36.84, 49.9, 11.0
2019-02-13 21:17:20,909 : Text to Image: 11.544, 31.56, 43.904, 14.0
2019-02-13 21:18:04,201 : samples : 384000
2019-02-13 21:18:14,346 : Image to text: 14.34, 37.22, 49.7, 11.0
2019-02-13 21:18:21,779 : Text to Image: 11.544, 30.736, 43.636, 14.0
2019-02-13 21:19:04,053 : samples : 448000
2019-02-13 21:19:14,246 : Image to text: 14.68, 37.16, 50.72, 10.0
2019-02-13 21:19:21,670 : Text to Image: 11.468, 31.188, 43.628, 14.0
2019-02-13 21:20:03,617 : samples : 512000
2019-02-13 21:20:13,932 : Image to text: 14.8, 37.02, 50.48, 10.0
2019-02-13 21:20:21,366 : Text to Image: 11.224, 30.608, 43.336, 14.0
2019-02-13 21:20:57,177 : Epoch 5 finished
2019-02-13 21:20:57,572 : Image to text: 33.1, 68.0, 80.6, 3.0
2019-02-13 21:20:57,871 : Text to Image: 27.56, 62.8, 79.26, 3.0
2019-02-13 21:20:58,280 : Image to text: 31.7, 67.0, 81.0, 3.0
2019-02-13 21:20:58,586 : Text to Image: 27.68, 61.12, 77.32, 4.0
2019-02-13 21:20:59,004 : Image to text: 34.6, 68.4, 83.2, 3.0
2019-02-13 21:20:59,314 : Text to Image: 27.84, 62.88, 78.22, 3.0
2019-02-13 21:20:59,737 : Image to text: 32.7, 67.3, 80.9, 3.0
2019-02-13 21:21:00,046 : Text to Image: 27.38, 62.4, 78.32, 3.0
2019-02-13 21:21:00,471 : Image to text: 35.3, 68.1, 79.5, 2.0
2019-02-13 21:21:00,781 : Text to Image: 26.18, 60.92, 76.96, 4.0
2019-02-13 21:21:00,782 : Dev mean Text to Image: 27.328, 62.024, 78.01599999999999, 3.4000000000000004
2019-02-13 21:21:00,782 : Dev mean Image to text: 33.480000000000004, 67.76, 81.03999999999999, 2.8
2019-02-13 21:21:00,782 : start epoch
2019-02-13 21:21:42,036 : samples : 64000
2019-02-13 21:21:52,194 : Image to text: 15.14, 37.68, 51.28, 10.0
2019-02-13 21:21:59,528 : Text to Image: 11.98, 31.828, 44.536, 14.0
2019-02-13 21:22:39,830 : samples : 128000
2019-02-13 21:22:49,977 : Image to text: 14.5, 37.08, 50.86, 10.0
2019-02-13 21:22:57,305 : Text to Image: 11.784, 31.344, 44.008, 14.0
2019-02-13 21:23:38,810 : samples : 192000
2019-02-13 21:23:48,916 : Image to text: 14.42, 37.56, 51.14, 10.0
2019-02-13 21:23:56,233 : Text to Image: 11.672, 31.112, 43.62, 14.0
2019-02-13 21:24:36,480 : samples : 256000
2019-02-13 21:24:46,600 : Image to text: 14.36, 36.88, 50.76, 10.0
2019-02-13 21:24:53,904 : Text to Image: 11.436, 31.284, 43.932, 14.0
2019-02-13 21:25:36,072 : samples : 320000
2019-02-13 21:25:46,187 : Image to text: 14.44, 36.68, 51.22, 10.0
2019-02-13 21:25:53,493 : Text to Image: 11.78, 31.476, 44.168, 14.0
2019-02-13 21:26:35,955 : samples : 384000
2019-02-13 21:26:46,059 : Image to text: 14.78, 37.5, 51.94, 9.0
2019-02-13 21:26:53,380 : Text to Image: 11.92, 31.608, 44.556, 14.0
2019-02-13 21:27:35,556 : samples : 448000
2019-02-13 21:27:45,646 : Image to text: 14.6, 36.54, 49.82, 11.0
2019-02-13 21:27:52,936 : Text to Image: 11.74, 31.568, 44.508, 13.0
2019-02-13 21:28:34,892 : samples : 512000
2019-02-13 21:28:45,250 : Image to text: 14.96, 36.58, 50.2, 10.0
2019-02-13 21:28:52,677 : Text to Image: 11.732, 31.604, 44.832, 13.0
2019-02-13 21:29:28,479 : Epoch 6 finished
2019-02-13 21:29:28,910 : Image to text: 34.3, 68.6, 82.7, 3.0
2019-02-13 21:29:29,222 : Text to Image: 28.62, 62.86, 79.0, 3.0
2019-02-13 21:29:29,649 : Image to text: 35.0, 66.2, 80.3, 3.0
2019-02-13 21:29:29,959 : Text to Image: 27.68, 62.0, 77.54, 3.0
2019-02-13 21:29:30,386 : Image to text: 34.5, 70.5, 83.0, 3.0
2019-02-13 21:29:30,698 : Text to Image: 28.5, 63.04, 78.72, 3.0
2019-02-13 21:29:31,123 : Image to text: 35.1, 69.2, 82.2, 3.0
2019-02-13 21:29:31,432 : Text to Image: 27.94, 62.84, 78.68, 3.0
2019-02-13 21:29:31,857 : Image to text: 34.7, 69.5, 80.6, 3.0
2019-02-13 21:29:32,164 : Text to Image: 27.5, 61.12, 76.7, 4.0
2019-02-13 21:29:32,164 : Dev mean Text to Image: 28.048000000000002, 62.372, 78.128, 3.2
2019-02-13 21:29:32,164 : Dev mean Image to text: 34.72, 68.80000000000001, 81.75999999999999, 3.0
2019-02-13 21:29:32,164 : start epoch
2019-02-13 21:30:14,994 : samples : 64000
2019-02-13 21:30:25,069 : Image to text: 14.92, 37.54, 51.28, 10.0
2019-02-13 21:30:32,360 : Text to Image: 11.952, 31.736, 44.744, 13.0
2019-02-13 21:31:15,423 : samples : 128000
2019-02-13 21:31:25,519 : Image to text: 15.22, 37.52, 51.16, 10.0
2019-02-13 21:31:32,802 : Text to Image: 12.052, 32.06, 45.056, 13.0
2019-02-13 21:32:15,909 : samples : 192000
2019-02-13 21:32:25,987 : Image to text: 15.02, 37.86, 52.04, 10.0
2019-02-13 21:32:33,291 : Text to Image: 12.152, 31.972, 44.896, 13.0
2019-02-13 21:33:18,142 : samples : 256000
2019-02-13 21:33:28,224 : Image to text: 14.86, 36.42, 50.26, 10.0
2019-02-13 21:33:35,499 : Text to Image: 11.728, 31.972, 44.548, 13.0
2019-02-13 21:34:17,919 : samples : 320000
2019-02-13 21:34:27,991 : Image to text: 14.94, 38.36, 51.6, 10.0
2019-02-13 21:34:35,263 : Text to Image: 12.244, 32.392, 45.004, 13.0
2019-02-13 21:35:17,631 : samples : 384000
2019-02-13 21:35:27,722 : Image to text: 14.92, 37.52, 51.28, 10.0
2019-02-13 21:35:34,986 : Text to Image: 11.884, 32.044, 44.764, 14.0
2019-02-13 21:36:17,171 : samples : 448000
2019-02-13 21:36:27,226 : Image to text: 14.52, 36.74, 50.68, 10.0
2019-02-13 21:36:34,501 : Text to Image: 11.756, 31.96, 44.604, 13.0
2019-02-13 21:37:15,284 : samples : 512000
2019-02-13 21:37:25,693 : Image to text: 15.84, 37.8, 51.38, 10.0
2019-02-13 21:37:33,150 : Text to Image: 11.924, 31.808, 44.632, 13.0
2019-02-13 21:38:09,964 : Epoch 7 finished
2019-02-13 21:38:10,394 : Image to text: 34.6, 68.7, 82.7, 3.0
2019-02-13 21:38:10,706 : Text to Image: 28.44, 62.96, 79.24, 3.0
2019-02-13 21:38:11,137 : Image to text: 33.7, 66.3, 81.2, 3.0
2019-02-13 21:38:11,447 : Text to Image: 27.34, 62.42, 78.18, 3.0
2019-02-13 21:38:11,877 : Image to text: 34.3, 69.7, 82.4, 2.0
2019-02-13 21:38:12,188 : Text to Image: 29.08, 63.86, 78.98, 3.0
2019-02-13 21:38:12,619 : Image to text: 34.0, 68.9, 81.3, 3.0
2019-02-13 21:38:12,929 : Text to Image: 28.66, 63.46, 79.62, 3.0
2019-02-13 21:38:13,359 : Image to text: 34.9, 69.2, 81.0, 3.0
2019-02-13 21:38:13,668 : Text to Image: 26.94, 61.26, 77.8, 4.0
2019-02-13 21:38:13,668 : Dev mean Text to Image: 28.092, 62.792, 78.764, 3.2
2019-02-13 21:38:13,668 : Dev mean Image to text: 34.3, 68.56, 81.72000000000001, 2.8000000000000003
2019-02-13 21:38:13,669 : start epoch
2019-02-13 21:38:53,695 : samples : 64000
2019-02-13 21:39:03,757 : Image to text: 15.02, 38.0, 51.06, 10.0
2019-02-13 21:39:11,011 : Text to Image: 11.732, 32.012, 44.696, 13.0
2019-02-13 21:39:53,003 : samples : 128000
2019-02-13 21:40:03,043 : Image to text: 15.46, 38.52, 52.02, 10.0
2019-02-13 21:40:10,293 : Text to Image: 12.32, 32.496, 45.4, 13.0
2019-02-13 21:40:52,953 : samples : 192000
2019-02-13 21:41:02,992 : Image to text: 15.3, 37.28, 51.02, 10.0
2019-02-13 21:41:10,242 : Text to Image: 11.94, 32.188, 44.992, 13.0
2019-02-13 21:41:52,824 : samples : 256000
2019-02-13 21:42:02,868 : Image to text: 14.84, 37.76, 51.28, 10.0
2019-02-13 21:42:10,122 : Text to Image: 11.98, 31.924, 44.548, 13.0
2019-02-13 21:42:52,536 : samples : 320000
2019-02-13 21:43:02,574 : Image to text: 15.74, 38.2, 52.14, 10.0
2019-02-13 21:43:09,824 : Text to Image: 12.164, 32.676, 45.536, 13.0
2019-02-13 21:43:51,591 : samples : 384000
2019-02-13 21:44:01,621 : Image to text: 14.74, 37.98, 51.42, 10.0
2019-02-13 21:44:08,876 : Text to Image: 12.332, 32.408, 45.128, 13.0
2019-02-13 21:44:49,127 : samples : 448000
2019-02-13 21:44:59,168 : Image to text: 14.6, 37.08, 51.26, 10.0
2019-02-13 21:45:06,425 : Text to Image: 11.996, 31.952, 44.68, 13.0
2019-02-13 21:45:47,114 : samples : 512000
2019-02-13 21:45:57,492 : Image to text: 15.5, 38.4, 52.66, 9.0
2019-02-13 21:46:04,917 : Text to Image: 12.5, 32.556, 45.38, 13.0
2019-02-13 21:46:39,487 : Epoch 8 finished
2019-02-13 21:46:39,917 : Image to text: 34.5, 68.3, 82.3, 3.0
2019-02-13 21:46:40,226 : Text to Image: 28.5, 63.94, 79.22, 3.0
2019-02-13 21:46:40,656 : Image to text: 34.9, 66.6, 81.8, 3.0
2019-02-13 21:46:40,965 : Text to Image: 28.8, 61.86, 78.06, 3.0
2019-02-13 21:46:41,396 : Image to text: 32.6, 68.6, 82.7, 3.0
2019-02-13 21:46:41,705 : Text to Image: 28.4, 64.12, 79.7, 3.0
2019-02-13 21:46:42,136 : Image to text: 35.7, 67.9, 81.4, 3.0
2019-02-13 21:46:42,448 : Text to Image: 27.78, 62.94, 79.42, 3.0
2019-02-13 21:46:42,875 : Image to text: 36.3, 68.2, 80.6, 3.0
2019-02-13 21:46:43,195 : Text to Image: 27.8, 61.24, 77.82, 4.0
2019-02-13 21:46:43,195 : Dev mean Text to Image: 28.256, 62.82000000000001, 78.844, 3.2
2019-02-13 21:46:43,195 : Dev mean Image to text: 34.8, 67.92, 81.75999999999999, 3.0
2019-02-13 21:46:43,195 : start epoch
2019-02-13 21:47:25,001 : samples : 64000
2019-02-13 21:47:35,046 : Image to text: 15.72, 38.22, 51.5, 10.0
2019-02-13 21:47:42,296 : Text to Image: 12.368, 32.444, 45.316, 13.0
2019-02-13 21:48:23,905 : samples : 128000
2019-02-13 21:48:33,955 : Image to text: 16.22, 38.82, 52.58, 9.0
2019-02-13 21:48:41,224 : Text to Image: 12.048, 32.372, 45.248, 13.0
2019-02-13 21:49:24,286 : samples : 192000
2019-02-13 21:49:34,323 : Image to text: 15.1, 39.02, 52.88, 9.0
2019-02-13 21:49:41,605 : Text to Image: 12.34, 32.6, 45.352, 13.0
2019-02-13 21:50:24,233 : samples : 256000
2019-02-13 21:50:34,292 : Image to text: 15.54, 38.86, 51.62, 10.0
2019-02-13 21:50:41,623 : Text to Image: 12.16, 32.516, 45.492, 13.0
2019-02-13 21:51:24,608 : samples : 320000
2019-02-13 21:51:34,681 : Image to text: 15.72, 38.32, 52.1, 10.0
2019-02-13 21:51:41,992 : Text to Image: 12.424, 32.688, 45.596, 13.0
2019-02-13 21:52:24,527 : samples : 384000
2019-02-13 21:52:34,579 : Image to text: 14.64, 37.78, 51.56, 10.0
2019-02-13 21:52:41,864 : Text to Image: 12.16, 32.432, 45.476, 13.0
2019-02-13 21:53:24,653 : samples : 448000
2019-02-13 21:53:34,699 : Image to text: 15.96, 38.86, 52.64, 9.0
2019-02-13 21:53:41,996 : Text to Image: 12.2, 32.552, 45.208, 13.0
2019-02-13 21:54:24,265 : samples : 512000
2019-02-13 21:54:34,631 : Image to text: 14.74, 37.86, 51.62, 10.0
2019-02-13 21:54:42,051 : Text to Image: 12.172, 32.312, 45.476, 13.0
2019-02-13 21:55:19,722 : Epoch 9 finished
2019-02-13 21:55:20,153 : Image to text: 36.3, 69.3, 82.2, 3.0
2019-02-13 21:55:20,471 : Text to Image: 29.06, 64.64, 80.26, 3.0
2019-02-13 21:55:20,902 : Image to text: 34.4, 67.5, 83.1, 3.0
2019-02-13 21:55:21,218 : Text to Image: 28.78, 62.56, 78.6, 3.0
2019-02-13 21:55:21,650 : Image to text: 34.8, 69.6, 83.7, 3.0
2019-02-13 21:55:21,968 : Text to Image: 28.62, 64.18, 80.26, 3.0
2019-02-13 21:55:22,391 : Image to text: 34.0, 69.4, 83.1, 3.0
2019-02-13 21:55:22,729 : Text to Image: 28.86, 63.74, 79.82, 3.0
2019-02-13 21:55:23,189 : Image to text: 36.6, 70.3, 82.1, 3.0
2019-02-13 21:55:23,523 : Text to Image: 28.68, 62.7, 78.32, 3.0
2019-02-13 21:55:23,523 : Dev mean Text to Image: 28.8, 63.564, 79.452, 3.0
2019-02-13 21:55:23,523 : Dev mean Image to text: 35.22, 69.22, 82.84, 3.0
2019-02-13 21:55:23,523 : start epoch
2019-02-13 21:56:06,527 : samples : 64000
2019-02-13 21:56:16,566 : Image to text: 15.94, 38.56, 52.22, 9.0
2019-02-13 21:56:23,886 : Text to Image: 12.376, 32.524, 45.652, 13.0
2019-02-13 21:57:06,772 : samples : 128000
2019-02-13 21:57:16,827 : Image to text: 15.22, 38.24, 51.6, 10.0
2019-02-13 21:57:24,119 : Text to Image: 12.748, 32.888, 45.728, 13.0
2019-02-13 21:58:07,202 : samples : 192000
2019-02-13 21:58:17,247 : Image to text: 15.76, 38.02, 52.22, 9.0
2019-02-13 21:58:24,558 : Text to Image: 12.22, 32.74, 45.208, 13.0
2019-02-13 21:59:06,413 : samples : 256000
2019-02-13 21:59:16,457 : Image to text: 15.5, 39.32, 52.96, 9.0
2019-02-13 21:59:23,758 : Text to Image: 12.188, 32.592, 45.66, 13.0
2019-02-13 22:00:06,576 : samples : 320000
2019-02-13 22:00:16,617 : Image to text: 15.86, 38.76, 52.06, 9.0
2019-02-13 22:00:23,924 : Text to Image: 12.724, 32.932, 45.972, 13.0
2019-02-13 22:01:06,213 : samples : 384000
2019-02-13 22:01:16,248 : Image to text: 15.34, 38.58, 52.34, 9.0
2019-02-13 22:01:23,533 : Text to Image: 12.26, 32.456, 45.348, 13.0
2019-02-13 22:02:03,540 : samples : 448000
2019-02-13 22:02:13,599 : Image to text: 16.36, 38.16, 52.34, 10.0
2019-02-13 22:02:20,947 : Text to Image: 12.488, 32.796, 45.78, 13.0
2019-02-13 22:03:03,585 : samples : 512000
2019-02-13 22:03:13,803 : Image to text: 15.04, 37.34, 51.76, 10.0
2019-02-13 22:03:21,234 : Text to Image: 12.204, 32.632, 45.584, 13.0
2019-02-13 22:03:56,683 : Epoch 10 finished
2019-02-13 22:03:57,114 : Image to text: 36.7, 69.0, 82.6, 3.0
2019-02-13 22:03:57,424 : Text to Image: 29.34, 63.74, 79.92, 3.0
2019-02-13 22:03:57,855 : Image to text: 35.2, 68.0, 82.0, 3.0
2019-02-13 22:03:58,164 : Text to Image: 28.66, 62.06, 78.56, 3.0
2019-02-13 22:03:58,594 : Image to text: 34.8, 70.4, 84.0, 3.0
2019-02-13 22:03:58,909 : Text to Image: 29.46, 64.88, 79.56, 3.0
2019-02-13 22:03:59,356 : Image to text: 35.0, 70.0, 83.4, 2.0
2019-02-13 22:03:59,691 : Text to Image: 28.56, 64.28, 79.76, 3.0
2019-02-13 22:04:00,145 : Image to text: 36.3, 69.8, 82.3, 3.0
2019-02-13 22:04:00,475 : Text to Image: 27.56, 62.9, 78.58, 3.0
2019-02-13 22:04:00,475 : Dev mean Text to Image: 28.716, 63.572, 79.276, 3.0
2019-02-13 22:04:00,475 : Dev mean Image to text: 35.6, 69.44, 82.86000000000001, 2.8
2019-02-13 22:04:00,476 : start epoch
2019-02-13 22:04:40,888 : samples : 64000
2019-02-13 22:04:50,935 : Image to text: 16.32, 39.5, 52.64, 9.0
2019-02-13 22:04:58,231 : Text to Image: 12.22, 32.684, 45.44, 13.0
2019-02-13 22:05:38,167 : samples : 128000
2019-02-13 22:05:48,214 : Image to text: 15.9, 38.72, 52.08, 9.0
2019-02-13 22:05:55,523 : Text to Image: 12.34, 32.484, 45.516, 13.0
2019-02-13 22:06:35,641 : samples : 192000
2019-02-13 22:06:45,670 : Image to text: 15.7, 38.82, 53.2, 9.0
2019-02-13 22:06:52,983 : Text to Image: 12.684, 32.656, 45.68, 13.0
2019-02-13 22:07:35,505 : samples : 256000
2019-02-13 22:07:45,562 : Image to text: 16.34, 39.22, 52.16, 9.0
2019-02-13 22:07:52,850 : Text to Image: 12.264, 32.716, 45.788, 13.0
2019-02-13 22:08:36,231 : samples : 320000
2019-02-13 22:08:46,276 : Image to text: 15.94, 38.58, 52.3, 10.0
2019-02-13 22:08:53,586 : Text to Image: 12.052, 32.632, 45.516, 13.0
2019-02-13 22:09:36,751 : samples : 384000
2019-02-13 22:09:46,788 : Image to text: 16.2, 39.3, 53.3, 9.0
2019-02-13 22:09:54,072 : Text to Image: 12.68, 33.536, 46.292, 13.0
2019-02-13 22:10:38,042 : samples : 448000
2019-02-13 22:10:48,095 : Image to text: 16.0, 38.7, 52.3, 10.0
2019-02-13 22:10:55,392 : Text to Image: 12.692, 33.088, 45.928, 13.0
2019-02-13 22:11:35,486 : samples : 512000
2019-02-13 22:11:45,726 : Image to text: 15.8, 38.74, 52.02, 9.0
2019-02-13 22:11:53,108 : Text to Image: 12.484, 32.876, 45.652, 13.0
2019-02-13 22:12:27,452 : Epoch 11 finished
2019-02-13 22:12:27,880 : Image to text: 37.0, 69.4, 82.5, 2.0
2019-02-13 22:12:28,187 : Text to Image: 28.78, 64.1, 79.52, 3.0
2019-02-13 22:12:28,610 : Image to text: 34.8, 68.6, 83.4, 3.0
2019-02-13 22:12:28,917 : Text to Image: 28.9, 62.62, 78.74, 3.0
2019-02-13 22:12:29,341 : Image to text: 36.4, 70.6, 82.7, 2.0
2019-02-13 22:12:29,650 : Text to Image: 30.14, 65.12, 79.2, 3.0
2019-02-13 22:12:30,078 : Image to text: 37.4, 72.1, 83.0, 2.0
2019-02-13 22:12:30,385 : Text to Image: 28.52, 64.58, 79.16, 3.0
2019-02-13 22:12:30,827 : Image to text: 36.2, 68.0, 82.8, 3.0
2019-02-13 22:12:31,161 : Text to Image: 28.2, 61.92, 78.5, 3.0
2019-02-13 22:12:31,161 : Dev mean Text to Image: 28.908, 63.66799999999999, 79.024, 3.0
2019-02-13 22:12:31,162 : Dev mean Image to text: 36.36, 69.74, 82.88, 2.4
2019-02-13 22:12:31,162 : start epoch
2019-02-13 22:13:11,617 : samples : 64000
2019-02-13 22:13:21,648 : Image to text: 15.44, 38.68, 52.4, 9.0
2019-02-13 22:13:28,930 : Text to Image: 12.372, 32.796, 45.752, 13.0
2019-02-13 22:14:10,141 : samples : 128000
2019-02-13 22:14:20,188 : Image to text: 15.74, 38.66, 52.7, 9.0
2019-02-13 22:14:27,462 : Text to Image: 12.54, 32.848, 45.92, 13.0
2019-02-13 22:15:10,697 : samples : 192000
2019-02-13 22:15:20,731 : Image to text: 16.42, 39.46, 53.28, 9.0
2019-02-13 22:15:28,017 : Text to Image: 12.724, 33.088, 46.164, 13.0
2019-02-13 22:16:11,494 : samples : 256000
2019-02-13 22:16:21,557 : Image to text: 15.86, 38.2, 52.22, 9.0
2019-02-13 22:16:28,842 : Text to Image: 12.452, 32.88, 45.772, 13.0
2019-02-13 22:17:12,112 : samples : 320000
2019-02-13 22:17:22,161 : Image to text: 15.94, 38.34, 53.04, 9.0
2019-02-13 22:17:29,446 : Text to Image: 12.604, 33.156, 45.772, 13.0
2019-02-13 22:18:12,532 : samples : 384000
2019-02-13 22:18:22,582 : Image to text: 16.04, 39.18, 53.0, 9.0
2019-02-13 22:18:29,868 : Text to Image: 12.688, 33.48, 46.228, 13.0
2019-02-13 22:19:10,340 : samples : 448000
2019-02-13 22:19:20,394 : Image to text: 16.66, 40.28, 53.42, 9.0
2019-02-13 22:19:27,683 : Text to Image: 12.912, 33.444, 46.808, 12.0
2019-02-13 22:20:08,707 : samples : 512000
2019-02-13 22:20:19,580 : Image to text: 15.9, 38.9, 53.34, 9.0
2019-02-13 22:20:29,660 : Text to Image: 12.392, 33.1, 46.364, 12.0
2019-02-13 22:21:07,344 : Epoch 12 finished
2019-02-13 22:21:08,287 : Image to text: 34.8, 68.6, 82.5, 2.0
2019-02-13 22:21:09,092 : Text to Image: 29.26, 64.92, 80.08, 3.0
2019-02-13 22:21:10,045 : Image to text: 35.4, 68.2, 82.1, 3.0
2019-02-13 22:21:10,840 : Text to Image: 28.98, 63.56, 79.94, 3.0
2019-02-13 22:21:11,801 : Image to text: 36.8, 71.7, 84.5, 2.0
2019-02-13 22:21:12,634 : Text to Image: 29.44, 65.22, 80.24, 3.0
2019-02-13 22:21:13,614 : Image to text: 36.1, 70.5, 83.0, 3.0
2019-02-13 22:21:14,337 : Text to Image: 28.9, 64.86, 80.56, 3.0
2019-02-13 22:21:15,370 : Image to text: 36.4, 69.0, 82.0, 2.0
2019-02-13 22:21:16,118 : Text to Image: 28.76, 62.96, 79.14, 3.0
2019-02-13 22:21:16,118 : Dev mean Text to Image: 29.068000000000005, 64.304, 79.99199999999999, 3.0
2019-02-13 22:21:16,118 : Dev mean Image to text: 35.9, 69.60000000000001, 82.82, 2.4
2019-02-13 22:21:16,119 : start epoch
2019-02-13 22:21:59,533 : samples : 64000
2019-02-13 22:22:09,602 : Image to text: 16.5, 39.68, 53.52, 9.0
2019-02-13 22:22:16,843 : Text to Image: 12.588, 33.236, 46.224, 12.0
2019-02-13 22:22:59,457 : samples : 128000
2019-02-13 22:23:12,113 : Image to text: 15.76, 39.46, 52.94, 9.0
2019-02-13 22:23:22,226 : Text to Image: 12.552, 32.86, 45.928, 13.0
2019-02-13 22:24:05,644 : samples : 192000
2019-02-13 22:24:15,827 : Image to text: 15.52, 39.4, 53.16, 9.0
2019-02-13 22:24:23,011 : Text to Image: 12.548, 33.152, 46.424, 12.0
2019-02-13 22:25:04,711 : samples : 256000
2019-02-13 22:25:17,308 : Image to text: 16.68, 39.88, 52.96, 9.0
2019-02-13 22:25:27,399 : Text to Image: 12.62, 33.356, 46.304, 12.0
2019-02-13 22:26:12,175 : samples : 320000
2019-02-13 22:26:22,930 : Image to text: 15.98, 39.04, 53.0, 9.0
2019-02-13 22:26:30,144 : Text to Image: 12.752, 33.408, 46.616, 12.0
2019-02-13 22:27:12,396 : samples : 384000
2019-02-13 22:27:24,975 : Image to text: 16.14, 39.24, 52.98, 9.0
2019-02-13 22:27:34,990 : Text to Image: 12.616, 33.152, 45.96, 13.0
2019-02-13 22:28:19,286 : samples : 448000
2019-02-13 22:28:29,401 : Image to text: 15.84, 38.96, 53.34, 9.0
2019-02-13 22:28:36,614 : Text to Image: 12.512, 33.172, 46.268, 12.0
2019-02-13 22:29:18,358 : samples : 512000
2019-02-13 22:29:30,983 : Image to text: 15.94, 39.5, 52.56, 9.0
2019-02-13 22:29:40,913 : Text to Image: 12.252, 32.992, 46.116, 13.0
2019-02-13 22:30:19,330 : Epoch 13 finished
2019-02-13 22:30:20,271 : Image to text: 36.2, 70.5, 82.1, 2.0
2019-02-13 22:30:21,043 : Text to Image: 29.94, 65.02, 80.72, 3.0
2019-02-13 22:30:21,954 : Image to text: 34.6, 70.8, 82.9, 3.0
2019-02-13 22:30:22,710 : Text to Image: 29.5, 63.16, 79.24, 3.0
2019-02-13 22:30:23,619 : Image to text: 35.0, 70.7, 84.1, 2.0
2019-02-13 22:30:24,453 : Text to Image: 29.88, 65.26, 80.38, 3.0
2019-02-13 22:30:25,462 : Image to text: 36.7, 72.1, 82.9, 2.0
2019-02-13 22:30:26,243 : Text to Image: 29.6, 64.88, 80.36, 3.0
2019-02-13 22:30:27,170 : Image to text: 36.9, 70.1, 81.9, 2.0
2019-02-13 22:30:27,926 : Text to Image: 28.32, 63.22, 79.12, 3.0
2019-02-13 22:30:27,927 : Dev mean Text to Image: 29.448, 64.308, 79.964, 3.0
2019-02-13 22:30:27,927 : Dev mean Image to text: 35.88, 70.83999999999999, 82.78, 2.1999999999999997
2019-02-13 22:30:27,927 : start epoch
2019-02-13 22:31:09,132 : samples : 64000
2019-02-13 22:31:21,700 : Image to text: 16.2, 39.4, 53.08, 9.0
2019-02-13 22:31:31,728 : Text to Image: 12.436, 33.112, 46.2, 13.0
2019-02-13 22:32:15,812 : samples : 128000
2019-02-13 22:32:28,510 : Image to text: 15.76, 39.38, 52.9, 9.0
2019-02-13 22:32:36,917 : Text to Image: 12.652, 33.46, 46.288, 12.0
2019-02-13 22:33:18,093 : samples : 192000
2019-02-13 22:33:30,682 : Image to text: 17.1, 39.88, 53.52, 9.0
2019-02-13 22:33:40,647 : Text to Image: 12.7, 33.332, 46.276, 12.0
2019-02-13 22:34:25,857 : samples : 256000
2019-02-13 22:34:38,548 : Image to text: 15.76, 39.42, 53.96, 9.0
2019-02-13 22:34:46,691 : Text to Image: 12.564, 33.388, 46.228, 13.0
2019-02-13 22:35:27,604 : samples : 320000
2019-02-13 22:35:40,182 : Image to text: 16.38, 39.62, 53.32, 9.0
2019-02-13 22:35:50,191 : Text to Image: 12.764, 33.28, 46.168, 13.0
2019-02-13 22:36:34,471 : samples : 384000
2019-02-13 22:36:47,183 : Image to text: 16.7, 39.82, 53.88, 9.0
2019-02-13 22:36:56,092 : Text to Image: 12.848, 33.544, 46.36, 12.0
2019-02-13 22:37:37,014 : samples : 448000
2019-02-13 22:37:47,670 : Image to text: 16.38, 39.64, 53.96, 9.0
2019-02-13 22:37:57,712 : Text to Image: 12.576, 33.296, 46.028, 13.0
2019-02-13 22:38:42,205 : samples : 512000
2019-02-13 22:38:54,865 : Image to text: 16.0, 40.74, 54.16, 8.0
2019-02-13 22:39:04,974 : Text to Image: 12.78, 33.388, 46.54, 12.0
2019-02-13 22:39:40,963 : Epoch 14 finished
2019-02-13 22:39:41,415 : Image to text: 37.4, 69.0, 81.9, 2.0
2019-02-13 22:39:41,778 : Text to Image: 30.3, 64.8, 80.28, 3.0
2019-02-13 22:39:42,229 : Image to text: 36.2, 68.8, 81.4, 2.0
2019-02-13 22:39:42,592 : Text to Image: 29.28, 63.56, 79.74, 3.0
2019-02-13 22:39:43,043 : Image to text: 36.1, 71.7, 82.7, 2.0
2019-02-13 22:39:43,406 : Text to Image: 30.06, 66.0, 80.74, 3.0
2019-02-13 22:39:43,847 : Image to text: 36.4, 70.9, 82.8, 2.0
2019-02-13 22:39:44,210 : Text to Image: 29.36, 65.12, 80.5, 3.0
2019-02-13 22:39:44,661 : Image to text: 36.6, 69.8, 81.7, 3.0
2019-02-13 22:39:45,023 : Text to Image: 28.76, 63.4, 79.4, 3.0
2019-02-13 22:39:45,023 : Dev mean Text to Image: 29.552, 64.576, 80.132, 3.0
2019-02-13 22:39:45,024 : Dev mean Image to text: 36.54, 70.04, 82.10000000000001, 2.2
2019-02-13 22:39:45,024 : start epoch
2019-02-13 22:40:26,498 : samples : 64000
2019-02-13 22:40:39,104 : Image to text: 16.46, 39.48, 52.7, 9.0
2019-02-13 22:40:49,221 : Text to Image: 12.448, 33.32, 46.272, 13.0
2019-02-13 22:41:33,933 : samples : 128000
2019-02-13 22:41:45,282 : Image to text: 15.92, 39.18, 53.06, 9.0
2019-02-13 22:41:52,498 : Text to Image: 12.404, 33.54, 46.516, 12.0
2019-02-13 22:42:33,625 : samples : 192000
2019-02-13 22:42:44,354 : Image to text: 16.8, 40.36, 54.1, 9.0
2019-02-13 22:42:52,520 : Text to Image: 12.892, 33.42, 46.556, 12.0
2019-02-13 22:43:34,942 : samples : 256000
2019-02-13 22:43:45,054 : Image to text: 16.34, 39.7, 53.48, 9.0
2019-02-13 22:43:52,278 : Text to Image: 12.88, 33.832, 47.024, 12.0
2019-02-13 22:44:33,775 : samples : 320000
2019-02-13 22:44:43,909 : Image to text: 15.98, 39.2, 53.22, 9.0
2019-02-13 22:44:51,245 : Text to Image: 12.756, 33.276, 46.704, 12.0
2019-02-13 22:45:32,162 : samples : 384000
2019-02-13 22:45:42,225 : Image to text: 16.3, 39.2, 53.26, 9.0
2019-02-13 22:45:49,476 : Text to Image: 12.96, 33.608, 46.8, 12.0
2019-02-13 22:46:31,680 : samples : 448000
2019-02-13 22:46:41,798 : Image to text: 16.74, 39.8, 53.18, 9.0
2019-02-13 22:46:49,021 : Text to Image: 12.896, 33.828, 46.832, 12.0
2019-02-13 22:47:30,456 : samples : 512000
2019-02-13 22:47:43,094 : Image to text: 16.26, 39.12, 53.14, 9.0
2019-02-13 22:47:53,263 : Text to Image: 12.756, 33.248, 46.36, 12.0
2019-02-13 22:48:29,219 : Epoch 15 finished
2019-02-13 22:48:29,677 : Image to text: 36.5, 71.2, 83.6, 2.0
2019-02-13 22:48:30,038 : Text to Image: 29.9, 64.56, 80.3, 3.0
2019-02-13 22:48:30,488 : Image to text: 34.5, 69.0, 82.5, 3.0
2019-02-13 22:48:30,850 : Text to Image: 29.58, 63.78, 79.28, 3.0
2019-02-13 22:48:31,300 : Image to text: 35.3, 72.0, 84.6, 3.0
2019-02-13 22:48:31,663 : Text to Image: 29.32, 65.18, 80.26, 3.0
2019-02-13 22:48:32,032 : Image to text: 36.2, 71.8, 83.7, 2.0
2019-02-13 22:48:32,308 : Text to Image: 29.52, 64.46, 80.44, 3.0
2019-02-13 22:48:32,669 : Image to text: 38.3, 68.9, 82.5, 2.0
2019-02-13 22:48:32,945 : Text to Image: 28.88, 63.5, 78.92, 3.0
2019-02-13 22:48:32,945 : Dev mean Text to Image: 29.439999999999998, 64.296, 79.84, 3.0
2019-02-13 22:48:32,945 : Dev mean Image to text: 36.16, 70.58, 83.38, 2.4
2019-02-13 22:48:32,945 : start epoch
2019-02-13 22:49:14,938 : samples : 64000
2019-02-13 22:49:27,611 : Image to text: 16.06, 39.7, 53.4, 9.0
2019-02-13 22:49:34,600 : Text to Image: 12.496, 33.184, 46.416, 12.0
2019-02-13 22:50:16,558 : samples : 128000
2019-02-13 22:50:26,582 : Image to text: 16.0, 39.04, 53.26, 9.0
2019-02-13 22:50:35,295 : Text to Image: 12.392, 33.052, 46.472, 12.0
2019-02-13 22:51:16,808 : samples : 192000
2019-02-13 22:51:29,310 : Image to text: 16.84, 39.52, 53.96, 9.0
2019-02-13 22:51:39,231 : Text to Image: 12.916, 33.596, 46.748, 12.0
2019-02-13 22:52:23,670 : samples : 256000
2019-02-13 22:52:36,216 : Image to text: 16.84, 40.12, 53.82, 9.0
2019-02-13 22:52:46,220 : Text to Image: 12.884, 33.828, 46.924, 12.0
2019-02-13 22:53:30,833 : samples : 320000
2019-02-13 22:53:43,481 : Image to text: 16.02, 40.66, 54.02, 9.0
2019-02-13 22:53:53,458 : Text to Image: 12.26, 33.276, 46.684, 12.0
2019-02-13 22:54:38,371 : samples : 384000
2019-02-13 22:54:51,012 : Image to text: 16.28, 39.9, 53.14, 9.0
2019-02-13 22:55:01,043 : Text to Image: 12.756, 33.416, 46.148, 12.0
2019-02-13 22:55:44,904 : samples : 448000
2019-02-13 22:55:57,577 : Image to text: 16.28, 39.54, 53.5, 9.0
2019-02-13 22:56:07,689 : Text to Image: 12.68, 33.356, 46.264, 12.0
2019-02-13 22:56:51,396 : samples : 512000
2019-02-13 22:57:04,030 : Image to text: 16.52, 39.76, 53.52, 9.0
2019-02-13 22:57:14,079 : Text to Image: 13.108, 33.964, 46.852, 12.0
2019-02-13 22:57:51,780 : Epoch 16 finished
2019-02-13 22:57:52,712 : Image to text: 36.7, 70.0, 82.9, 2.0
2019-02-13 22:57:53,462 : Text to Image: 30.0, 65.5, 80.86, 3.0
2019-02-13 22:57:54,382 : Image to text: 36.3, 68.7, 82.5, 3.0
2019-02-13 22:57:55,111 : Text to Image: 29.82, 63.56, 79.62, 3.0
2019-02-13 22:57:56,024 : Image to text: 36.6, 72.2, 85.1, 2.0
2019-02-13 22:57:56,802 : Text to Image: 29.82, 65.2, 80.14, 3.0
2019-02-13 22:57:57,704 : Image to text: 36.5, 71.5, 83.6, 2.0
2019-02-13 22:57:58,444 : Text to Image: 29.36, 64.96, 80.72, 3.0
2019-02-13 22:57:59,367 : Image to text: 38.5, 70.3, 83.3, 2.0
2019-02-13 22:58:00,126 : Text to Image: 29.16, 63.28, 79.18, 3.0
2019-02-13 22:58:00,126 : Dev mean Text to Image: 29.632, 64.5, 80.104, 3.0
2019-02-13 22:58:00,126 : Dev mean Image to text: 36.92, 70.54, 83.47999999999999, 2.1999999999999997
2019-02-13 22:58:00,127 : start epoch
2019-02-13 22:58:42,123 : samples : 64000
2019-02-13 22:58:54,792 : Image to text: 16.68, 39.96, 53.4, 9.0
2019-02-13 22:59:04,851 : Text to Image: 12.936, 33.928, 47.084, 12.0
2019-02-13 22:59:46,788 : samples : 128000
2019-02-13 22:59:59,466 : Image to text: 16.72, 39.98, 53.84, 9.0
2019-02-13 23:00:09,447 : Text to Image: 12.684, 33.752, 47.056, 12.0
2019-02-13 23:00:51,515 : samples : 192000
2019-02-13 23:01:04,100 : Image to text: 16.86, 39.76, 53.88, 9.0
2019-02-13 23:01:14,173 : Text to Image: 12.884, 33.392, 46.876, 12.0
2019-02-13 23:01:56,922 : samples : 256000
2019-02-13 23:02:09,595 : Image to text: 16.58, 39.72, 53.58, 9.0
2019-02-13 23:02:19,689 : Text to Image: 12.76, 33.24, 46.436, 12.0
2019-02-13 23:03:03,733 : samples : 320000
2019-02-13 23:03:14,084 : Image to text: 17.0, 40.18, 53.42, 9.0
2019-02-13 23:03:21,267 : Text to Image: 12.576, 33.444, 46.4, 12.0
2019-02-13 23:04:01,818 : samples : 384000
2019-02-13 23:04:11,881 : Image to text: 16.7, 39.4, 53.22, 9.0
2019-02-13 23:04:19,082 : Text to Image: 12.888, 33.596, 46.66, 12.0
2019-02-13 23:05:01,601 : samples : 448000
2019-02-13 23:05:14,523 : Image to text: 16.54, 39.74, 53.3, 9.0
2019-02-13 23:05:24,889 : Text to Image: 13.152, 33.972, 46.96, 12.0
2019-02-13 23:06:09,263 : samples : 512000
2019-02-13 23:06:22,288 : Image to text: 16.46, 39.3, 53.58, 9.0
2019-02-13 23:06:32,680 : Text to Image: 13.016, 34.036, 47.048, 12.0
2019-02-13 23:07:11,483 : Epoch 17 finished
2019-02-13 23:07:12,590 : Image to text: 37.1, 70.6, 83.2, 3.0
2019-02-13 23:07:13,491 : Text to Image: 29.86, 64.78, 80.5, 3.0
2019-02-13 23:07:14,587 : Image to text: 35.2, 69.5, 82.1, 3.0
2019-02-13 23:07:15,560 : Text to Image: 29.4, 64.16, 79.96, 3.0
2019-02-13 23:07:16,660 : Image to text: 35.1, 72.9, 83.7, 2.0
2019-02-13 23:07:17,592 : Text to Image: 29.66, 64.8, 80.28, 3.0
2019-02-13 23:07:18,626 : Image to text: 36.1, 69.3, 84.1, 3.0
2019-02-13 23:07:19,543 : Text to Image: 29.7, 65.22, 80.5, 3.0
2019-02-13 23:07:20,636 : Image to text: 37.8, 71.6, 82.9, 2.0
2019-02-13 23:07:21,504 : Text to Image: 29.6, 63.72, 79.32, 3.0
2019-02-13 23:07:21,504 : Dev mean Text to Image: 29.644, 64.536, 80.112, 3.0
2019-02-13 23:07:21,504 : Dev mean Image to text: 36.260000000000005, 70.78, 83.2, 2.6
2019-02-13 23:07:21,504 : start epoch
2019-02-13 23:08:06,561 : samples : 64000
2019-02-13 23:08:19,548 : Image to text: 16.94, 39.56, 53.32, 9.0
2019-02-13 23:08:30,111 : Text to Image: 12.9, 34.024, 46.764, 12.0
2019-02-13 23:09:15,548 : samples : 128000
2019-02-13 23:09:28,535 : Image to text: 16.28, 39.5, 54.3, 9.0
2019-02-13 23:09:39,054 : Text to Image: 12.788, 33.74, 46.536, 12.0
2019-02-13 23:10:24,802 : samples : 192000
2019-02-13 23:10:37,721 : Image to text: 16.36, 39.14, 53.38, 9.0
2019-02-13 23:10:48,158 : Text to Image: 12.872, 34.012, 47.252, 12.0
2019-02-13 23:11:33,246 : samples : 256000
2019-02-13 23:11:46,222 : Image to text: 17.18, 39.9, 53.56, 9.0
2019-02-13 23:11:56,706 : Text to Image: 12.908, 34.14, 47.156, 12.0
2019-02-13 23:12:41,578 : samples : 320000
2019-02-13 23:12:54,558 : Image to text: 16.94, 40.18, 54.26, 9.0
2019-02-13 23:13:03,873 : Text to Image: 12.788, 33.632, 47.036, 12.0
2019-02-13 23:13:46,141 : samples : 384000
2019-02-13 23:13:56,201 : Image to text: 16.94, 40.04, 54.08, 9.0
2019-02-13 23:14:03,520 : Text to Image: 12.964, 34.048, 46.976, 12.0
2019-02-13 23:14:45,709 : samples : 448000
2019-02-13 23:14:55,765 : Image to text: 16.34, 39.92, 53.62, 9.0
2019-02-13 23:15:03,080 : Text to Image: 13.208, 34.032, 47.208, 12.0
2019-02-13 23:15:45,059 : samples : 512000
2019-02-13 23:15:55,120 : Image to text: 16.54, 39.68, 54.5, 9.0
2019-02-13 23:16:02,438 : Text to Image: 12.736, 33.512, 46.62, 12.0
2019-02-13 23:16:39,987 : Epoch 18 finished
2019-02-13 23:16:40,423 : Image to text: 36.7, 70.5, 82.4, 2.0
2019-02-13 23:16:40,749 : Text to Image: 29.46, 65.7, 80.86, 3.0
2019-02-13 23:16:41,173 : Image to text: 36.2, 70.0, 83.4, 2.0
2019-02-13 23:16:41,499 : Text to Image: 28.98, 63.76, 79.42, 3.0
2019-02-13 23:16:41,939 : Image to text: 34.6, 73.6, 83.5, 2.0
2019-02-13 23:16:42,267 : Text to Image: 29.72, 65.42, 80.66, 3.0
2019-02-13 23:16:42,696 : Image to text: 34.8, 69.3, 82.7, 3.0
2019-02-13 23:16:43,024 : Text to Image: 29.58, 64.64, 80.46, 3.0
2019-02-13 23:16:43,456 : Image to text: 37.3, 70.7, 82.7, 2.0
2019-02-13 23:16:43,828 : Text to Image: 29.24, 63.38, 79.24, 3.0
2019-02-13 23:16:43,828 : Dev mean Text to Image: 29.396, 64.58, 80.128, 3.0
2019-02-13 23:16:43,828 : Dev mean Image to text: 35.92, 70.82, 82.94, 2.2
2019-02-13 23:16:43,828 : start epoch
2019-02-13 23:17:24,663 : samples : 64000
2019-02-13 23:17:35,109 : Image to text: 17.06, 39.96, 53.48, 9.0
2019-02-13 23:17:42,612 : Text to Image: 12.968, 33.872, 46.768, 12.0
2019-02-13 23:18:23,208 : samples : 128000
2019-02-13 23:18:33,670 : Image to text: 17.18, 40.58, 54.32, 9.0
2019-02-13 23:18:41,169 : Text to Image: 12.916, 33.68, 46.504, 12.0
2019-02-13 23:19:21,294 : samples : 192000
2019-02-13 23:19:31,698 : Image to text: 16.86, 39.42, 53.8, 9.0
2019-02-13 23:19:39,159 : Text to Image: 13.016, 33.724, 47.064, 12.0
2019-02-13 23:20:20,712 : samples : 256000
2019-02-13 23:20:31,063 : Image to text: 17.08, 40.4, 54.38, 9.0
2019-02-13 23:20:38,541 : Text to Image: 12.852, 34.06, 46.992, 12.0
2019-02-13 23:21:20,578 : samples : 320000
2019-02-13 23:21:30,925 : Image to text: 16.62, 39.72, 53.0, 9.0
2019-02-13 23:21:38,274 : Text to Image: 13.264, 34.092, 47.212, 12.0
2019-02-13 23:22:21,484 : samples : 384000
2019-02-13 23:22:31,607 : Image to text: 16.82, 39.86, 53.68, 9.0
2019-02-13 23:22:38,978 : Text to Image: 12.884, 33.856, 46.744, 12.0
2019-02-13 23:23:21,494 : samples : 448000
2019-02-13 23:23:31,576 : Image to text: 16.36, 39.62, 53.56, 9.0
2019-02-13 23:23:38,933 : Text to Image: 12.844, 33.74, 46.828, 12.0
2019-02-13 23:24:22,366 : samples : 512000
2019-02-13 23:24:32,415 : Image to text: 16.74, 40.62, 54.6, 9.0
2019-02-13 23:24:39,747 : Text to Image: 12.984, 33.748, 47.2, 12.0
2019-02-13 23:25:16,833 : Epoch 19 finished
2019-02-13 23:25:17,262 : Image to text: 38.3, 72.5, 84.1, 2.0
2019-02-13 23:25:17,586 : Text to Image: 30.18, 65.52, 81.06, 3.0
2019-02-13 23:25:18,005 : Image to text: 36.5, 68.9, 84.2, 3.0
2019-02-13 23:25:18,328 : Text to Image: 29.6, 64.24, 79.96, 3.0
2019-02-13 23:25:18,747 : Image to text: 36.4, 72.6, 84.6, 2.0
2019-02-13 23:25:19,077 : Text to Image: 30.72, 65.32, 80.94, 3.0
2019-02-13 23:25:19,509 : Image to text: 35.5, 71.7, 85.4, 2.0
2019-02-13 23:25:19,838 : Text to Image: 29.56, 65.04, 80.76, 3.0
2019-02-13 23:25:20,331 : Image to text: 37.3, 69.8, 82.8, 2.0
2019-02-13 23:25:20,671 : Text to Image: 28.68, 63.28, 79.1, 3.0
2019-02-13 23:25:20,671 : Dev mean Text to Image: 29.748, 64.68, 80.36399999999999, 3.0
2019-02-13 23:25:20,671 : Dev mean Image to text: 36.8, 71.1, 84.22, 2.1999999999999997
2019-02-13 23:25:20,672 : start epoch
2019-02-13 23:26:01,969 : samples : 64000
2019-02-13 23:26:12,298 : Image to text: 17.3, 40.42, 53.06, 9.0
2019-02-13 23:26:19,755 : Text to Image: 12.86, 33.956, 46.536, 12.0
2019-02-13 23:27:00,533 : samples : 128000
2019-02-13 23:27:10,814 : Image to text: 17.02, 40.14, 54.28, 9.0
2019-02-13 23:27:18,209 : Text to Image: 12.748, 33.652, 46.416, 12.0
2019-02-13 23:27:58,713 : samples : 192000
2019-02-13 23:28:08,986 : Image to text: 17.2, 40.04, 54.0, 9.0
2019-02-13 23:28:16,393 : Text to Image: 13.008, 33.92, 46.868, 12.0
2019-02-13 23:28:56,905 : samples : 256000
2019-02-13 23:29:07,186 : Image to text: 17.1, 40.46, 54.14, 9.0
2019-02-13 23:29:14,600 : Text to Image: 12.98, 33.768, 46.972, 12.0
2019-02-13 23:29:55,227 : samples : 320000
2019-02-13 23:30:05,527 : Image to text: 17.28, 39.58, 53.06, 9.0
2019-02-13 23:30:12,884 : Text to Image: 12.892, 33.852, 46.784, 12.0
2019-02-13 23:30:53,247 : samples : 384000
2019-02-13 23:31:03,302 : Image to text: 17.42, 40.82, 54.14, 9.0
2019-02-13 23:31:10,615 : Text to Image: 13.256, 34.5, 47.2, 12.0
2019-02-13 23:31:50,862 : samples : 448000
2019-02-13 23:32:00,925 : Image to text: 16.34, 39.3, 53.18, 9.0
2019-02-13 23:32:08,235 : Text to Image: 13.22, 33.984, 47.216, 12.0
2019-02-13 23:32:52,529 : samples : 512000
2019-02-13 23:33:02,575 : Image to text: 17.08, 40.28, 54.64, 8.0
2019-02-13 23:33:09,854 : Text to Image: 13.136, 34.08, 47.34, 12.0
2019-02-13 23:33:49,008 : Epoch 20 finished
2019-02-13 23:33:49,435 : Image to text: 37.4, 71.2, 83.5, 2.0
2019-02-13 23:33:49,761 : Text to Image: 29.92, 65.86, 80.56, 3.0
2019-02-13 23:33:50,183 : Image to text: 37.1, 70.8, 83.7, 3.0
2019-02-13 23:33:50,510 : Text to Image: 30.0, 64.08, 80.3, 3.0
2019-02-13 23:33:50,955 : Image to text: 36.1, 73.0, 85.0, 2.0
2019-02-13 23:33:51,287 : Text to Image: 29.74, 65.98, 80.6, 3.0
2019-02-13 23:33:51,724 : Image to text: 35.4, 71.9, 84.3, 2.0
2019-02-13 23:33:52,055 : Text to Image: 30.24, 65.64, 81.34, 3.0
2019-02-13 23:33:52,492 : Image to text: 36.9, 71.5, 82.8, 2.0
2019-02-13 23:33:52,821 : Text to Image: 29.42, 64.08, 79.38, 3.0
2019-02-13 23:33:52,821 : Dev mean Text to Image: 29.863999999999997, 65.128, 80.436, 3.0
2019-02-13 23:33:52,821 : Dev mean Image to text: 36.58, 71.68, 83.86, 2.1999999999999997
2019-02-13 23:33:52,821 : start epoch
2019-02-13 23:34:35,810 : samples : 64000
2019-02-13 23:34:46,064 : Image to text: 15.94, 39.26, 53.32, 9.0
2019-02-13 23:34:53,445 : Text to Image: 12.748, 33.512, 46.744, 12.0
2019-02-13 23:35:33,882 : samples : 128000
2019-02-13 23:35:44,129 : Image to text: 16.98, 40.5, 54.08, 9.0
2019-02-13 23:35:51,512 : Text to Image: 12.96, 33.676, 47.068, 12.0
2019-02-13 23:36:32,091 : samples : 192000
2019-02-13 23:36:42,367 : Image to text: 16.74, 39.8, 53.3, 9.0
2019-02-13 23:36:49,775 : Text to Image: 13.244, 33.94, 47.304, 12.0
2019-02-13 23:37:30,316 : samples : 256000
2019-02-13 23:37:40,593 : Image to text: 16.52, 39.42, 53.38, 9.0
2019-02-13 23:37:47,990 : Text to Image: 13.096, 33.936, 47.256, 12.0
2019-02-13 23:38:28,803 : samples : 320000
2019-02-13 23:38:39,052 : Image to text: 16.98, 40.24, 54.56, 9.0
2019-02-13 23:38:46,387 : Text to Image: 12.952, 34.172, 46.956, 12.0
2019-02-13 23:39:26,632 : samples : 384000
2019-02-13 23:39:36,685 : Image to text: 17.18, 39.88, 53.62, 9.0
2019-02-13 23:39:43,962 : Text to Image: 13.032, 33.928, 46.94, 12.0
2019-02-13 23:40:25,013 : samples : 448000
2019-02-13 23:40:35,045 : Image to text: 17.16, 41.56, 55.7, 8.0
2019-02-13 23:40:42,347 : Text to Image: 13.228, 34.412, 47.44, 12.0
2019-02-13 23:41:22,682 : samples : 512000
2019-02-13 23:41:32,729 : Image to text: 17.4, 40.48, 54.2, 9.0
2019-02-13 23:41:40,033 : Text to Image: 13.096, 34.232, 47.316, 12.0
2019-02-13 23:42:14,344 : Epoch 21 finished
2019-02-13 23:42:14,780 : Image to text: 36.9, 70.7, 84.2, 2.0
2019-02-13 23:42:15,110 : Text to Image: 30.44, 65.86, 80.34, 3.0
2019-02-13 23:42:15,543 : Image to text: 36.6, 69.9, 83.3, 2.0
2019-02-13 23:42:15,873 : Text to Image: 30.44, 63.66, 79.82, 3.0
2019-02-13 23:42:16,317 : Image to text: 37.2, 72.1, 83.9, 2.0
2019-02-13 23:42:16,647 : Text to Image: 29.62, 66.1, 80.74, 3.0
2019-02-13 23:42:17,075 : Image to text: 36.7, 71.2, 84.8, 2.0
2019-02-13 23:42:17,404 : Text to Image: 29.9, 66.02, 81.18, 3.0
2019-02-13 23:42:17,843 : Image to text: 38.2, 71.1, 82.1, 3.0
2019-02-13 23:42:18,170 : Text to Image: 29.3, 63.98, 80.14, 3.0
2019-02-13 23:42:18,171 : Dev mean Text to Image: 29.94, 65.124, 80.44399999999999, 3.0
2019-02-13 23:42:18,171 : Dev mean Image to text: 37.120000000000005, 71.0, 83.66000000000001, 2.2
2019-02-13 23:42:22,201 : 
Test scores | Image to text:             36.48, 69.66000000000001, 83.06, 2.2
2019-02-13 23:42:22,201 : Test scores | Text to image:             29.083999999999996, 64.472, 80.092, 3.0

2019-02-13 23:42:22,293 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 23:42:22,500 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 23:42:23,114 : loading BERT model bert-base-uncased
2019-02-13 23:42:23,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:42:23,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:42:23,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_vhivyf3
2019-02-13 23:42:25,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:42:26,971 : Computing embeddings for train/dev/test
2019-02-13 23:44:02,705 : Computed embeddings
2019-02-13 23:44:02,705 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:44:32,732 : [('reg:1e-05', 95.3), ('reg:0.0001', 93.41), ('reg:0.001', 89.03), ('reg:0.01', 80.01)]
2019-02-13 23:44:32,732 : Validation : best param found is reg = 1e-05 with score             95.3
2019-02-13 23:44:32,732 : Evaluating...
2019-02-13 23:44:39,479 : 
Dev acc : 95.3 Test acc : 95.3 for LENGTH classification

2019-02-13 23:44:39,480 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 23:44:39,851 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 23:44:39,895 : loading BERT model bert-base-uncased
2019-02-13 23:44:39,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:44:39,925 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:44:39,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi05uwlr2
2019-02-13 23:44:42,388 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:44:43,771 : Computing embeddings for train/dev/test
2019-02-13 23:46:13,586 : Computed embeddings
2019-02-13 23:46:13,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:46:47,262 : [('reg:1e-05', 89.99), ('reg:0.0001', 62.0), ('reg:0.001', 5.4), ('reg:0.01', 0.99)]
2019-02-13 23:46:47,262 : Validation : best param found is reg = 1e-05 with score             89.99
2019-02-13 23:46:47,262 : Evaluating...
2019-02-13 23:46:56,413 : 
Dev acc : 90.0 Test acc : 89.9 for WORDCONTENT classification

2019-02-13 23:46:56,415 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 23:46:56,790 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 23:46:56,859 : loading BERT model bert-base-uncased
2019-02-13 23:46:56,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:46:56,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:46:56,964 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_za5h34c
2019-02-13 23:46:59,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:47:00,877 : Computing embeddings for train/dev/test
2019-02-13 23:48:26,403 : Computed embeddings
2019-02-13 23:48:26,403 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:48:56,066 : [('reg:1e-05', 33.7), ('reg:0.0001', 33.24), ('reg:0.001', 31.32), ('reg:0.01', 26.41)]
2019-02-13 23:48:56,066 : Validation : best param found is reg = 1e-05 with score             33.7
2019-02-13 23:48:56,066 : Evaluating...
2019-02-13 23:49:04,006 : 
Dev acc : 33.7 Test acc : 34.0 for DEPTH classification

2019-02-13 23:49:04,007 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 23:49:04,607 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 23:49:04,673 : loading BERT model bert-base-uncased
2019-02-13 23:49:04,673 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:49:04,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:49:04,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqrr0xecu
2019-02-13 23:49:07,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:49:08,544 : Computing embeddings for train/dev/test
2019-02-13 23:50:28,164 : Computed embeddings
2019-02-13 23:50:28,164 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:51:03,266 : [('reg:1e-05', 63.66), ('reg:0.0001', 60.64), ('reg:0.001', 51.21), ('reg:0.01', 41.27)]
2019-02-13 23:51:03,266 : Validation : best param found is reg = 1e-05 with score             63.66
2019-02-13 23:51:03,266 : Evaluating...
2019-02-13 23:51:14,212 : 
Dev acc : 63.7 Test acc : 64.4 for TOPCONSTITUENTS classification

2019-02-13 23:51:14,213 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 23:51:14,585 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 23:51:14,656 : loading BERT model bert-base-uncased
2019-02-13 23:51:14,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:51:14,794 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:51:14,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr1ycblgu
2019-02-13 23:51:17,249 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:51:18,657 : Computing embeddings for train/dev/test
2019-02-13 23:52:45,249 : Computed embeddings
2019-02-13 23:52:45,249 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:53:21,485 : [('reg:1e-05', 55.18), ('reg:0.0001', 55.07), ('reg:0.001', 54.86), ('reg:0.01', 52.2)]
2019-02-13 23:53:21,485 : Validation : best param found is reg = 1e-05 with score             55.18
2019-02-13 23:53:21,485 : Evaluating...
2019-02-13 23:53:30,742 : 
Dev acc : 55.2 Test acc : 54.7 for BIGRAMSHIFT classification

2019-02-13 23:53:30,744 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 23:53:31,362 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 23:53:31,434 : loading BERT model bert-base-uncased
2019-02-13 23:53:31,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:53:31,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:53:31,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxuirqxpz
2019-02-13 23:53:33,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:53:35,326 : Computing embeddings for train/dev/test
2019-02-13 23:54:59,340 : Computed embeddings
2019-02-13 23:54:59,340 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:55:34,106 : [('reg:1e-05', 86.72), ('reg:0.0001', 86.64), ('reg:0.001', 86.32), ('reg:0.01', 86.32)]
2019-02-13 23:55:34,106 : Validation : best param found is reg = 1e-05 with score             86.72
2019-02-13 23:55:34,106 : Evaluating...
2019-02-13 23:55:42,527 : 
Dev acc : 86.7 Test acc : 84.9 for TENSE classification

2019-02-13 23:55:42,528 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 23:55:43,099 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 23:55:43,161 : loading BERT model bert-base-uncased
2019-02-13 23:55:43,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:55:43,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:55:43,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0vijg7ll
2019-02-13 23:55:45,627 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:55:47,019 : Computing embeddings for train/dev/test
2019-02-13 23:57:14,605 : Computed embeddings
2019-02-13 23:57:14,605 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:57:40,475 : [('reg:1e-05', 81.56), ('reg:0.0001', 81.64), ('reg:0.001', 81.59), ('reg:0.01', 79.84)]
2019-02-13 23:57:40,475 : Validation : best param found is reg = 0.0001 with score             81.64
2019-02-13 23:57:40,475 : Evaluating...
2019-02-13 23:57:46,335 : 
Dev acc : 81.6 Test acc : 80.6 for SUBJNUMBER classification

2019-02-13 23:57:46,337 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 23:57:46,770 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 23:57:46,838 : loading BERT model bert-base-uncased
2019-02-13 23:57:46,838 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:57:46,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:57:46,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppkpeqyy9
2019-02-13 23:57:49,317 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:57:50,754 : Computing embeddings for train/dev/test
2019-02-13 23:59:18,385 : Computed embeddings
2019-02-13 23:59:18,385 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:59:42,061 : [('reg:1e-05', 80.65), ('reg:0.0001', 80.66), ('reg:0.001', 80.42), ('reg:0.01', 78.41)]
2019-02-13 23:59:42,062 : Validation : best param found is reg = 0.0001 with score             80.66
2019-02-13 23:59:42,062 : Evaluating...
2019-02-13 23:59:47,619 : 
Dev acc : 80.7 Test acc : 81.3 for OBJNUMBER classification

2019-02-13 23:59:47,620 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 23:59:48,021 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 23:59:48,091 : loading BERT model bert-base-uncased
2019-02-13 23:59:48,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:59:48,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:59:48,218 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3by4g_7o
2019-02-13 23:59:50,670 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:59:52,069 : Computing embeddings for train/dev/test
2019-02-14 00:01:32,167 : Computed embeddings
2019-02-14 00:01:32,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:02:00,454 : [('reg:1e-05', 52.19), ('reg:0.0001', 52.17), ('reg:0.001', 52.17), ('reg:0.01', 51.52)]
2019-02-14 00:02:00,454 : Validation : best param found is reg = 1e-05 with score             52.19
2019-02-14 00:02:00,454 : Evaluating...
2019-02-14 00:02:07,829 : 
Dev acc : 52.2 Test acc : 51.9 for ODDMANOUT classification

2019-02-14 00:02:07,830 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 00:02:08,255 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 00:02:08,337 : loading BERT model bert-base-uncased
2019-02-14 00:02:08,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:02:08,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:02:08,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaa5ksoc9
2019-02-14 00:02:10,918 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:02:12,339 : Computing embeddings for train/dev/test
2019-02-14 00:03:51,611 : Computed embeddings
2019-02-14 00:03:51,612 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:04:28,721 : [('reg:1e-05', 53.87), ('reg:0.0001', 53.81), ('reg:0.001', 53.39), ('reg:0.01', 51.77)]
2019-02-14 00:04:28,722 : Validation : best param found is reg = 1e-05 with score             53.87
2019-02-14 00:04:28,722 : Evaluating...
2019-02-14 00:04:38,781 : 
Dev acc : 53.9 Test acc : 54.4 for COORDINATIONINVERSION classification

2019-02-14 00:04:38,783 : total results: {'STS12': {'MSRpar': {'pearson': (0.4115698730785093, 5.037196737842292e-32), 'spearman': SpearmanrResult(correlation=0.4495488118834225, pvalue=1.3795272049822265e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6776352699462084, 6.1947534732058175e-102), 'spearman': SpearmanrResult(correlation=0.6797159941043616, pvalue=8.714446384967709e-103), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5042994973169879, 5.556967587205416e-31), 'spearman': SpearmanrResult(correlation=0.603904918359693, pvalue=5.883444019880835e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.706654728174429, 1.7271051687514914e-114), 'spearman': SpearmanrResult(correlation=0.6944707535211025, pvalue=4.909743040333514e-109), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5447132988295186, 3.306977455407203e-32), 'spearman': SpearmanrResult(correlation=0.5010169970962378, pvalue=9.589475296576469e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5689745334691306, 'wmean': 0.5777699417312855}, 'spearman': {'mean': 0.5857314949929635, 'wmean': 0.5935971071429096}}}, 'STS13': {'FNWN': {'pearson': (0.456854546871803, 3.903171796935881e-11), 'spearman': SpearmanrResult(correlation=0.4717088553428003, pvalue=7.312691372453695e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6849128810333198, 6.04762930898994e-105), 'spearman': SpearmanrResult(correlation=0.6621610206172825, pvalue=8.17199058363817e-96), 'nsamples': 750}, 'OnWN': {'pearson': (0.5626528641183649, 3.7296084174228235e-48), 'spearman': SpearmanrResult(correlation=0.5900119464093035, pvalue=6.55693073112438e-54), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5681400973411627, 'wmean': 0.6104522846027756}, 'spearman': {'mean': 0.5746272741231288, 'wmean': 0.6111802940389136}}}, 'STS14': {'deft-forum': {'pearson': (0.3706466345753305, 4.216284966600407e-16), 'spearman': SpearmanrResult(correlation=0.3778186581508171, pvalue=1.022318892363268e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7256974962813679, 2.549129810625089e-50), 'spearman': SpearmanrResult(correlation=0.6912153567054475, pvalue=6.076064894471612e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6490164124552856, 6.779609518406149e-91), 'spearman': SpearmanrResult(correlation=0.6007184774219916, pvalue=9.492344143059358e-75), 'nsamples': 750}, 'images': {'pearson': (0.6885331838229208, 1.7818725636718904e-106), 'spearman': SpearmanrResult(correlation=0.6665817108687678, pvalue=1.5902479784666192e-97), 'nsamples': 750}, 'OnWN': {'pearson': (0.6818791198540003, 1.1147919154679959e-103), 'spearman': SpearmanrResult(correlation=0.7212699129911014, pvalue=2.0534850176300252e-121), 'nsamples': 750}, 'tweet-news': {'pearson': (0.7007430231853985, 8.28175878673569e-112), 'spearman': SpearmanrResult(correlation=0.6655428944276202, pvalue=4.037755165667666e-97), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.636085978362384, 'wmean': 0.6465677437150701}, 'spearman': {'mean': 0.6205245017609576, 'wmean': 0.6314580666564301}}}, 'STS15': {'answers-forums': {'pearson': (0.5669477516524087, 2.8331697406099518e-33), 'spearman': SpearmanrResult(correlation=0.551974653843101, pvalue=2.74906856069651e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.724199278993236, 7.427639684655011e-123), 'spearman': SpearmanrResult(correlation=0.7330307681545996, pvalue=2.566044398122116e-127), 'nsamples': 750}, 'belief': {'pearson': (0.6536142959461023, 4.622192058847672e-47), 'spearman': SpearmanrResult(correlation=0.6670153733112454, pvalue=1.3003853422191857e-49), 'nsamples': 375}, 'headlines': {'pearson': (0.7103387328288516, 3.408382438164426e-116), 'spearman': SpearmanrResult(correlation=0.6993656203781194, pvalue=3.413999303827497e-111), 'nsamples': 750}, 'images': {'pearson': (0.7583617989300914, 3.6220526574568447e-141), 'spearman': SpearmanrResult(correlation=0.7661159458479145, pvalue=9.36607391781379e-146), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.682692371670138, 'wmean': 0.7007952086378585}, 'spearman': {'mean': 0.683500472306996, 'wmean': 0.7020018369894517}}}, 'STS16': {'answer-answer': {'pearson': (0.4951157582298667, 4.094327798317896e-17), 'spearman': SpearmanrResult(correlation=0.5276593234670046, pvalue=1.3111293149757351e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7121472433808903, 7.95745265203215e-40), 'spearman': SpearmanrResult(correlation=0.7129035672165338, pvalue=6.065927543550842e-40), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7684931510491422, 4.1939265215603935e-46), 'spearman': SpearmanrResult(correlation=0.7703667937765707, pvalue=1.86930642696349e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8227194233465925, 2.6827771055389183e-61), 'spearman': SpearmanrResult(correlation=0.8362344162937558, pvalue=4.392741298360782e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.4693857011896893, 7.551090483266258e-13), 'spearman': SpearmanrResult(correlation=0.4657959407762339, pvalue=1.1866802239318185e-12), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6535722554392362, 'wmean': 0.656562092562179}, 'spearman': {'mean': 0.6625920083060197, 'wmean': 0.6662018281315822}}}, 'MR': {'devacc': 75.17, 'acc': 74.71, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.58, 'acc': 78.01, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.88, 'acc': 87.88, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.96, 'acc': 92.45, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.67, 'acc': 76.55, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.05, 'acc': 41.22, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.18, 'acc': 90.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.85, 'acc': 73.22, 'f1': 81.31, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.2, 'acc': 79.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8237443352877583, 'pearson': 0.8329372932713901, 'spearman': 0.7594390801065386, 'mse': 0.3134001024837591, 'yhat': array([3.95354949, 4.19727119, 1.23979649, ..., 3.421574  , 4.46573708,        4.32375173]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7559527821468942, 'pearson': 0.7058771519466621, 'spearman': 0.703691280303891, 'mse': 1.3937725223639443, 'yhat': array([1.44652761, 1.59174253, 1.84302307, ..., 3.64178831, 3.70489176,        3.40347257]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.44, 'acc': 68.45, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 367.548, 'acc': [(36.48, 69.66000000000001, 83.06, 2.2), (29.083999999999996, 64.472, 80.092, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 95.3, 'acc': 95.35, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 89.99, 'acc': 89.92, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.7, 'acc': 33.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.66, 'acc': 64.39, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 55.18, 'acc': 54.73, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.72, 'acc': 84.93, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.64, 'acc': 80.6, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.66, 'acc': 81.34, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 52.19, 'acc': 51.9, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.87, 'acc': 54.36, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 00:04:38,783 : STS12 p=0.5778, STS12 s=0.5936, STS13 p=0.6105, STS13 s=0.6112, STS14 p=0.6466, STS14 s=0.6315, STS15 p=0.7008, STS15 s=0.7020, STS 16 p=0.6566, STS16 s=0.6662, STS B p=0.7059, STS B s=0.7037, STS B m=1.3938, SICK-R p=0.8329, SICK-R s=0.7594, SICK-P m=0.3134
2019-02-14 00:04:38,783 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 00:04:38,783 : 0.5778,0.5936,0.6105,0.6112,0.6466,0.6315,0.7008,0.7020,0.6566,0.6662,0.7059,0.7037,1.3938,0.8329,0.7594,0.3134
2019-02-14 00:04:38,783 : MR=74.71, CR=78.01, SUBJ=92.45, MPQA=87.88, SST-B=76.55, SST-F=41.22, TREC=90.00, SICK-E=79.56, SNLI=68.45, MRPC=73.22, MRPC f=81.31
2019-02-14 00:04:38,783 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 00:04:38,784 : 74.71,78.01,92.45,87.88,76.55,41.22,90.00,79.56,68.45,73.22,81.31
2019-02-14 00:04:38,784 : COCO r1i2t=36.48, COCO r5i2t=69.66, COCO r10i2t=83.06, COCO medr_i2t=2.20, COCO r1t2i=29.08, COCO r5t2i=64.47, COCO r10t2i=80.09, COCO medr_t2i=3.00
2019-02-14 00:04:38,784 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 00:04:38,784 : 36.48,69.66,83.06,2.20,29.08,64.47,80.09,3.00
2019-02-14 00:04:38,784 : SentLen=95.35, WC=89.92, TreeDepth=33.96, TopConst=64.39, BShift=54.73, Tense=84.93, SubjNum=80.60, ObjNum=81.34, SOMO=51.90, CoordInv=54.36, average=69.15
2019-02-14 00:04:38,784 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 00:04:38,784 : 95.35,89.92,33.96,64.39,54.73,84.93,80.60,81.34,51.90,54.36,69.15
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,784 : layer 2
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,784 : ********************************************************************************
2019-02-14 00:04:38,878 : ***** Transfer task : STS12 *****


2019-02-14 00:04:38,918 : loading BERT model bert-base-uncased
2019-02-14 00:04:38,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:04:38,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:04:38,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqtc7wm4n
2019-02-14 00:04:41,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:04:44,616 : MSRpar : pearson = 0.4183, spearman = 0.4531
2019-02-14 00:04:45,401 : MSRvid : pearson = 0.6548, spearman = 0.6577
2019-02-14 00:04:46,054 : SMTeuroparl : pearson = 0.5035, spearman = 0.6006
2019-02-14 00:04:47,363 : surprise.OnWN : pearson = 0.7034, spearman = 0.6899
2019-02-14 00:04:48,085 : surprise.SMTnews : pearson = 0.5523, spearman = 0.5146
2019-02-14 00:04:48,085 : ALL (weighted average) : Pearson = 0.5740,             Spearman = 0.5893
2019-02-14 00:04:48,085 : ALL (average) : Pearson = 0.5665,             Spearman = 0.5832

2019-02-14 00:04:48,085 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 00:04:48,094 : loading BERT model bert-base-uncased
2019-02-14 00:04:48,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:04:48,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:04:48,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxbmz_v8l
2019-02-14 00:04:50,542 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:04:52,607 : FNWN : pearson = 0.4663, spearman = 0.4824
2019-02-14 00:04:53,597 : headlines : pearson = 0.6743, spearman = 0.6529
2019-02-14 00:04:54,344 : OnWN : pearson = 0.5408, spearman = 0.5667
2019-02-14 00:04:54,344 : ALL (weighted average) : Pearson = 0.5982,             Spearman = 0.5992
2019-02-14 00:04:54,345 : ALL (average) : Pearson = 0.5605,             Spearman = 0.5673

2019-02-14 00:04:54,345 : ***** Transfer task : STS14 *****


2019-02-14 00:04:54,360 : loading BERT model bert-base-uncased
2019-02-14 00:04:54,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:04:54,409 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:04:54,410 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcbuqqxm9
2019-02-14 00:04:56,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:04:58,906 : deft-forum : pearson = 0.3585, spearman = 0.3760
2019-02-14 00:04:59,615 : deft-news : pearson = 0.7372, spearman = 0.7022
2019-02-14 00:05:00,596 : headlines : pearson = 0.6352, spearman = 0.5915
2019-02-14 00:05:01,539 : images : pearson = 0.6663, spearman = 0.6472
2019-02-14 00:05:02,495 : OnWN : pearson = 0.6771, spearman = 0.7180
2019-02-14 00:05:03,749 : tweet-news : pearson = 0.6738, spearman = 0.6446
2019-02-14 00:05:03,749 : ALL (weighted average) : Pearson = 0.6325,             Spearman = 0.6215
2019-02-14 00:05:03,749 : ALL (average) : Pearson = 0.6247,             Spearman = 0.6132

2019-02-14 00:05:03,749 : ***** Transfer task : STS15 *****


2019-02-14 00:05:03,781 : loading BERT model bert-base-uncased
2019-02-14 00:05:03,781 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:05:03,798 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:05:03,799 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfjnz1zb6
2019-02-14 00:05:06,236 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:05:08,528 : answers-forums : pearson = 0.5400, spearman = 0.5348
2019-02-14 00:05:09,484 : answers-students : pearson = 0.7131, spearman = 0.7185
2019-02-14 00:05:10,342 : belief : pearson = 0.6510, spearman = 0.6739
2019-02-14 00:05:11,378 : headlines : pearson = 0.7007, spearman = 0.6893
2019-02-14 00:05:12,333 : images : pearson = 0.7447, spearman = 0.7547
2019-02-14 00:05:12,333 : ALL (weighted average) : Pearson = 0.6885,             Spearman = 0.6917
2019-02-14 00:05:12,333 : ALL (average) : Pearson = 0.6699,             Spearman = 0.6742

2019-02-14 00:05:12,333 : ***** Transfer task : STS16 *****


2019-02-14 00:05:12,410 : loading BERT model bert-base-uncased
2019-02-14 00:05:12,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:05:12,430 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:05:12,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjny4rzmz
2019-02-14 00:05:14,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:05:16,685 : answer-answer : pearson = 0.4973, spearman = 0.5231
2019-02-14 00:05:17,006 : headlines : pearson = 0.7055, spearman = 0.7050
2019-02-14 00:05:17,413 : plagiarism : pearson = 0.7417, spearman = 0.7448
2019-02-14 00:05:18,048 : postediting : pearson = 0.8125, spearman = 0.8305
2019-02-14 00:05:18,335 : question-question : pearson = 0.4694, spearman = 0.4718
2019-02-14 00:05:18,336 : ALL (weighted average) : Pearson = 0.6483,             Spearman = 0.6585
2019-02-14 00:05:18,336 : ALL (average) : Pearson = 0.6453,             Spearman = 0.6550

2019-02-14 00:05:18,336 : ***** Transfer task : MR *****


2019-02-14 00:05:18,353 : loading BERT model bert-base-uncased
2019-02-14 00:05:18,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:05:18,376 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:05:18,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwh6noqyn
2019-02-14 00:05:20,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:05:22,270 : Generating sentence embeddings
2019-02-14 00:05:35,835 : Generated sentence embeddings
2019-02-14 00:05:35,836 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 00:05:46,322 : Best param found at split 1: l2reg = 0.0001                 with score 75.16
2019-02-14 00:05:55,159 : Best param found at split 2: l2reg = 0.001                 with score 75.12
2019-02-14 00:06:05,549 : Best param found at split 3: l2reg = 0.0001                 with score 75.39
2019-02-14 00:06:16,919 : Best param found at split 4: l2reg = 0.001                 with score 74.99
2019-02-14 00:06:27,901 : Best param found at split 5: l2reg = 0.0001                 with score 74.89
2019-02-14 00:06:28,462 : Dev acc : 75.11 Test acc : 74.64

2019-02-14 00:06:28,463 : ***** Transfer task : CR *****


2019-02-14 00:06:28,470 : loading BERT model bert-base-uncased
2019-02-14 00:06:28,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:06:28,490 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:06:28,490 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz06gtwr8
2019-02-14 00:06:30,929 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:06:32,331 : Generating sentence embeddings
2019-02-14 00:06:36,441 : Generated sentence embeddings
2019-02-14 00:06:36,442 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 00:06:40,040 : Best param found at split 1: l2reg = 0.0001                 with score 79.5
2019-02-14 00:06:44,396 : Best param found at split 2: l2reg = 0.01                 with score 79.86
2019-02-14 00:06:49,029 : Best param found at split 3: l2reg = 0.01                 with score 80.76
2019-02-14 00:06:53,192 : Best param found at split 4: l2reg = 1e-05                 with score 80.07
2019-02-14 00:06:57,598 : Best param found at split 5: l2reg = 1e-05                 with score 80.3
2019-02-14 00:06:57,824 : Dev acc : 80.1 Test acc : 78.2

2019-02-14 00:06:57,824 : ***** Transfer task : MPQA *****


2019-02-14 00:06:57,830 : loading BERT model bert-base-uncased
2019-02-14 00:06:57,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:06:57,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:06:57,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5rdy51tw
2019-02-14 00:07:00,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:07:01,755 : Generating sentence embeddings
2019-02-14 00:07:05,554 : Generated sentence embeddings
2019-02-14 00:07:05,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 00:07:17,485 : Best param found at split 1: l2reg = 0.001                 with score 87.07
2019-02-14 00:07:29,511 : Best param found at split 2: l2reg = 0.001                 with score 87.92
2019-02-14 00:07:42,085 : Best param found at split 3: l2reg = 0.001                 with score 88.17
2019-02-14 00:07:52,755 : Best param found at split 4: l2reg = 1e-05                 with score 87.54
2019-02-14 00:08:05,367 : Best param found at split 5: l2reg = 0.01                 with score 87.44
2019-02-14 00:08:05,955 : Dev acc : 87.63 Test acc : 87.92

2019-02-14 00:08:05,956 : ***** Transfer task : SUBJ *****


2019-02-14 00:08:05,970 : loading BERT model bert-base-uncased
2019-02-14 00:08:05,971 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:08:05,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:08:05,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpajzep15g
2019-02-14 00:08:08,435 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:08:09,859 : Generating sentence embeddings
2019-02-14 00:08:23,204 : Generated sentence embeddings
2019-02-14 00:08:23,204 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 00:08:33,191 : Best param found at split 1: l2reg = 0.001                 with score 92.98
2019-02-14 00:08:42,105 : Best param found at split 2: l2reg = 0.001                 with score 93.61
2019-02-14 00:08:52,991 : Best param found at split 3: l2reg = 0.001                 with score 93.04
2019-02-14 00:09:05,132 : Best param found at split 4: l2reg = 0.001                 with score 93.44
2019-02-14 00:09:16,433 : Best param found at split 5: l2reg = 0.001                 with score 93.25
2019-02-14 00:09:16,855 : Dev acc : 93.26 Test acc : 92.78

2019-02-14 00:09:16,857 : ***** Transfer task : SST Binary classification *****


2019-02-14 00:09:16,993 : loading BERT model bert-base-uncased
2019-02-14 00:09:16,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:09:17,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:09:17,019 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_b6jcvq8
2019-02-14 00:09:19,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:09:20,877 : Computing embedding for train
2019-02-14 00:10:07,018 : Computed train embeddings
2019-02-14 00:10:07,018 : Computing embedding for dev
2019-02-14 00:10:07,974 : Computed dev embeddings
2019-02-14 00:10:07,974 : Computing embedding for test
2019-02-14 00:10:10,069 : Computed test embeddings
2019-02-14 00:10:10,069 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:10:32,268 : [('reg:1e-05', 79.01), ('reg:0.0001', 78.67), ('reg:0.001', 78.56), ('reg:0.01', 78.1)]
2019-02-14 00:10:32,268 : Validation : best param found is reg = 1e-05 with score             79.01
2019-02-14 00:10:32,269 : Evaluating...
2019-02-14 00:10:37,310 : 
Dev acc : 79.01 Test acc : 79.79 for             SST Binary classification

2019-02-14 00:10:37,311 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 00:10:37,360 : loading BERT model bert-base-uncased
2019-02-14 00:10:37,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:10:37,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:10:37,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa2vtd2pd
2019-02-14 00:10:39,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:10:41,204 : Computing embedding for train
2019-02-14 00:10:50,723 : Computed train embeddings
2019-02-14 00:10:50,724 : Computing embedding for dev
2019-02-14 00:10:51,941 : Computed dev embeddings
2019-02-14 00:10:51,941 : Computing embedding for test
2019-02-14 00:10:54,438 : Computed test embeddings
2019-02-14 00:10:54,438 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:10:56,888 : [('reg:1e-05', 40.24), ('reg:0.0001', 40.24), ('reg:0.001', 40.6), ('reg:0.01', 40.78)]
2019-02-14 00:10:56,889 : Validation : best param found is reg = 0.01 with score             40.78
2019-02-14 00:10:56,889 : Evaluating...
2019-02-14 00:10:57,514 : 
Dev acc : 40.78 Test acc : 40.05 for             SST Fine-Grained classification

2019-02-14 00:10:57,514 : ***** Transfer task : TREC *****


2019-02-14 00:10:57,528 : loading BERT model bert-base-uncased
2019-02-14 00:10:57,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:10:57,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:10:57,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxlhtnjbn
2019-02-14 00:10:59,993 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:11:04,834 : Computed train embeddings
2019-02-14 00:11:05,098 : Computed test embeddings
2019-02-14 00:11:05,099 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 00:11:13,070 : [('reg:1e-05', 80.52), ('reg:0.0001', 80.5), ('reg:0.001', 79.57), ('reg:0.01', 71.57)]
2019-02-14 00:11:13,070 : Cross-validation : best param found is reg = 1e-05             with score 80.52
2019-02-14 00:11:13,070 : Evaluating...
2019-02-14 00:11:13,576 : 
Dev acc : 80.52 Test acc : 90.0             for TREC

2019-02-14 00:11:13,577 : ***** Transfer task : MRPC *****


2019-02-14 00:11:13,640 : loading BERT model bert-base-uncased
2019-02-14 00:11:13,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:11:13,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:11:13,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0aze5rt5
2019-02-14 00:11:16,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:11:17,518 : Computing embedding for train
2019-02-14 00:11:27,304 : Computed train embeddings
2019-02-14 00:11:27,304 : Computing embedding for test
2019-02-14 00:11:31,975 : Computed test embeddings
2019-02-14 00:11:31,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 00:11:36,677 : [('reg:1e-05', 73.33), ('reg:0.0001', 73.38), ('reg:0.001', 73.38), ('reg:0.01', 72.99)]
2019-02-14 00:11:36,677 : Cross-validation : best param found is reg = 0.0001             with score 73.38
2019-02-14 00:11:36,677 : Evaluating...
2019-02-14 00:11:36,954 : Dev acc : 73.38 Test acc 73.8; Test F1 80.57 for MRPC.

2019-02-14 00:11:36,955 : ***** Transfer task : SICK-Entailment*****


2019-02-14 00:11:36,978 : loading BERT model bert-base-uncased
2019-02-14 00:11:36,979 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:11:36,999 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:11:37,000 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvnbrxmas
2019-02-14 00:11:39,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:11:40,858 : Computing embedding for train
2019-02-14 00:11:45,992 : Computed train embeddings
2019-02-14 00:11:45,992 : Computing embedding for dev
2019-02-14 00:11:46,666 : Computed dev embeddings
2019-02-14 00:11:46,666 : Computing embedding for test
2019-02-14 00:11:52,267 : Computed test embeddings
2019-02-14 00:11:52,296 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:11:54,084 : [('reg:1e-05', 80.4), ('reg:0.0001', 80.8), ('reg:0.001', 79.8), ('reg:0.01', 73.0)]
2019-02-14 00:11:54,084 : Validation : best param found is reg = 0.0001 with score             80.8
2019-02-14 00:11:54,084 : Evaluating...
2019-02-14 00:11:54,590 : 
Dev acc : 80.8 Test acc : 79.87 for                        SICK entailment

2019-02-14 00:11:54,590 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 00:11:54,619 : loading BERT model bert-base-uncased
2019-02-14 00:11:54,619 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:11:54,641 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:11:54,641 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuxt9cx2t
2019-02-14 00:11:57,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:11:58,507 : Computing embedding for train
2019-02-14 00:12:03,689 : Computed train embeddings
2019-02-14 00:12:03,689 : Computing embedding for dev
2019-02-14 00:12:04,382 : Computed dev embeddings
2019-02-14 00:12:04,382 : Computing embedding for test
2019-02-14 00:12:09,996 : Computed test embeddings
2019-02-14 00:12:23,317 : Dev : Pearson 0.8189028295858121
2019-02-14 00:12:23,317 : Test : Pearson 0.8291880187610355 Spearman 0.7553988012935134 MSE 0.31995341456434395                        for SICK Relatedness

2019-02-14 00:12:23,318 : 

***** Transfer task : STSBenchmark*****


2019-02-14 00:12:23,407 : loading BERT model bert-base-uncased
2019-02-14 00:12:23,407 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:12:23,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:12:23,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpckb81qay
2019-02-14 00:12:25,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:12:27,293 : Computing embedding for train
2019-02-14 00:12:36,522 : Computed train embeddings
2019-02-14 00:12:36,522 : Computing embedding for dev
2019-02-14 00:12:39,276 : Computed dev embeddings
2019-02-14 00:12:39,277 : Computing embedding for test
2019-02-14 00:12:41,455 : Computed test embeddings
2019-02-14 00:13:02,137 : Dev : Pearson 0.7464978427102179
2019-02-14 00:13:02,137 : Test : Pearson 0.702112468179167 Spearman 0.6994958784546036 MSE 1.3678795004455795                        for SICK Relatedness

2019-02-14 00:13:02,138 : ***** Transfer task : SNLI Entailment*****


2019-02-14 00:13:07,151 : loading BERT model bert-base-uncased
2019-02-14 00:13:07,151 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:13:07,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:13:07,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzoznl_5v
2019-02-14 00:13:09,720 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:13:11,314 : PROGRESS (encoding): 0.00%
2019-02-14 00:14:30,875 : PROGRESS (encoding): 14.56%
2019-02-14 00:16:01,061 : PROGRESS (encoding): 29.12%
2019-02-14 00:17:31,741 : PROGRESS (encoding): 43.69%
2019-02-14 00:19:10,010 : PROGRESS (encoding): 58.25%
2019-02-14 00:20:56,954 : PROGRESS (encoding): 72.81%
2019-02-14 00:22:43,210 : PROGRESS (encoding): 87.37%
2019-02-14 00:24:34,304 : PROGRESS (encoding): 0.00%
2019-02-14 00:24:47,957 : PROGRESS (encoding): 0.00%
2019-02-14 00:25:02,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 00:25:29,064 : [('reg:1e-09', 64.58)]
2019-02-14 00:25:29,065 : Validation : best param found is reg = 1e-09 with score             64.58
2019-02-14 00:25:29,065 : Evaluating...
2019-02-14 00:25:57,152 : Dev acc : 64.58 Test acc : 64.58 for SNLI

2019-02-14 00:25:57,152 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 00:26:06,012 : loading BERT model bert-base-uncased
2019-02-14 00:26:06,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 00:26:06,059 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 00:26:06,059 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp88ghw5na
2019-02-14 00:26:08,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 00:26:09,911 : Computing embedding for train
2019-02-14 00:33:54,509 : Computed train embeddings
2019-02-14 00:33:54,509 : Computing embedding for dev
2019-02-14 00:34:14,230 : Computed dev embeddings
2019-02-14 00:34:14,230 : Computing embedding for test
2019-02-14 00:34:34,474 : Computed test embeddings
2019-02-14 00:34:34,489 : prepare data
2019-02-14 00:34:34,551 : start epoch
2019-02-14 00:35:16,483 : samples : 64000
2019-02-14 00:35:26,751 : Image to text: 7.86, 22.72, 34.04, 23.0
2019-02-14 00:35:34,221 : Text to Image: 6.132, 20.136, 30.88, 26.0
2019-02-14 00:36:15,879 : samples : 128000
2019-02-14 00:36:26,160 : Image to text: 9.58, 26.96, 39.74, 18.0
2019-02-14 00:36:33,581 : Text to Image: 7.268, 22.96, 33.824, 22.0
2019-02-14 00:37:14,862 : samples : 192000
2019-02-14 00:37:25,200 : Image to text: 10.1, 28.66, 40.54, 16.0
2019-02-14 00:37:32,637 : Text to Image: 7.976, 23.704, 35.308, 21.0
2019-02-14 00:38:14,574 : samples : 256000
2019-02-14 00:38:24,891 : Image to text: 9.74, 27.56, 39.84, 17.0
2019-02-14 00:38:32,294 : Text to Image: 8.128, 24.312, 35.928, 20.0
2019-02-14 00:39:13,060 : samples : 320000
2019-02-14 00:39:23,345 : Image to text: 10.02, 28.72, 41.26, 16.0
2019-02-14 00:39:30,755 : Text to Image: 8.716, 24.972, 36.876, 19.0
2019-02-14 00:40:11,948 : samples : 384000
2019-02-14 00:40:22,197 : Image to text: 11.92, 30.68, 42.96, 15.0
2019-02-14 00:40:29,601 : Text to Image: 9.14, 26.388, 38.204, 18.0
2019-02-14 00:41:10,247 : samples : 448000
2019-02-14 00:41:20,589 : Image to text: 10.54, 29.88, 42.58, 15.0
2019-02-14 00:41:28,005 : Text to Image: 8.472, 24.88, 36.604, 19.0
2019-02-14 00:42:13,319 : samples : 512000
2019-02-14 00:42:23,621 : Image to text: 11.36, 31.56, 43.92, 14.0
2019-02-14 00:42:31,012 : Text to Image: 9.348, 26.796, 38.26, 18.0
2019-02-14 00:43:07,916 : Epoch 1 finished
2019-02-14 00:43:08,343 : Image to text: 29.6, 60.6, 76.4, 3.0
2019-02-14 00:43:08,671 : Text to Image: 23.52, 56.72, 73.96, 4.0
2019-02-14 00:43:09,092 : Image to text: 28.2, 61.2, 75.4, 4.0
2019-02-14 00:43:09,419 : Text to Image: 23.58, 56.32, 72.82, 4.0
2019-02-14 00:43:09,840 : Image to text: 29.9, 63.1, 77.7, 3.0
2019-02-14 00:43:10,168 : Text to Image: 24.24, 57.0, 73.82, 4.0
2019-02-14 00:43:10,608 : Image to text: 28.2, 62.5, 77.2, 3.0
2019-02-14 00:43:10,937 : Text to Image: 23.2, 56.12, 73.78, 4.0
2019-02-14 00:43:11,363 : Image to text: 28.7, 62.1, 76.4, 4.0
2019-02-14 00:43:11,691 : Text to Image: 23.28, 57.28, 72.72, 4.0
2019-02-14 00:43:11,691 : Dev mean Text to Image: 23.564, 56.688, 73.41999999999999, 4.0
2019-02-14 00:43:11,691 : Dev mean Image to text: 28.92, 61.900000000000006, 76.62, 3.4000000000000004
2019-02-14 00:43:11,692 : start epoch
2019-02-14 00:43:53,320 : samples : 64000
2019-02-14 00:44:03,642 : Image to text: 12.58, 31.34, 44.9, 13.0
2019-02-14 00:44:11,098 : Text to Image: 9.824, 27.232, 39.456, 17.0
2019-02-14 00:44:52,584 : samples : 128000
2019-02-14 00:45:02,854 : Image to text: 11.46, 30.42, 43.36, 14.0
2019-02-14 00:45:10,298 : Text to Image: 9.876, 27.832, 39.672, 17.0
2019-02-14 00:45:51,632 : samples : 192000
2019-02-14 00:46:01,914 : Image to text: 12.12, 32.44, 45.06, 13.0
2019-02-14 00:46:09,350 : Text to Image: 9.696, 27.808, 39.564, 17.0
2019-02-14 00:46:51,139 : samples : 256000
2019-02-14 00:47:01,412 : Image to text: 12.94, 33.48, 46.66, 12.0
2019-02-14 00:47:08,850 : Text to Image: 10.216, 28.388, 40.544, 16.0
2019-02-14 00:47:50,947 : samples : 320000
2019-02-14 00:48:01,212 : Image to text: 12.92, 32.78, 45.68, 13.0
2019-02-14 00:48:08,658 : Text to Image: 10.0, 28.308, 40.648, 16.0
2019-02-14 00:48:50,934 : samples : 384000
2019-02-14 00:49:01,329 : Image to text: 12.98, 33.74, 46.74, 12.0
2019-02-14 00:49:08,856 : Text to Image: 10.036, 28.48, 40.808, 16.0
2019-02-14 00:49:50,404 : samples : 448000
2019-02-14 00:50:00,729 : Image to text: 12.98, 33.88, 47.4, 12.0
2019-02-14 00:50:08,199 : Text to Image: 10.14, 28.428, 40.428, 16.0
2019-02-14 00:50:48,850 : samples : 512000
2019-02-14 00:50:59,148 : Image to text: 12.84, 32.96, 46.24, 13.0
2019-02-14 00:51:06,465 : Text to Image: 10.28, 28.836, 41.036, 16.0
2019-02-14 00:51:40,796 : Epoch 2 finished
2019-02-14 00:51:41,224 : Image to text: 30.7, 64.0, 78.9, 3.0
2019-02-14 00:51:41,552 : Text to Image: 25.28, 59.06, 76.22, 4.0
2019-02-14 00:51:41,996 : Image to text: 29.3, 61.3, 77.6, 3.0
2019-02-14 00:51:42,325 : Text to Image: 25.14, 57.64, 75.08, 4.0
2019-02-14 00:51:42,754 : Image to text: 31.6, 65.3, 78.7, 3.0
2019-02-14 00:51:43,084 : Text to Image: 25.76, 59.34, 75.42, 4.0
2019-02-14 00:51:43,523 : Image to text: 30.8, 64.7, 78.8, 3.0
2019-02-14 00:51:43,889 : Text to Image: 24.92, 58.88, 75.0, 4.0
2019-02-14 00:51:44,364 : Image to text: 30.1, 64.9, 77.8, 3.0
2019-02-14 00:51:44,705 : Text to Image: 24.62, 58.24, 74.24, 4.0
2019-02-14 00:51:44,705 : Dev mean Text to Image: 25.144, 58.632000000000005, 75.192, 4.0
2019-02-14 00:51:44,705 : Dev mean Image to text: 30.5, 64.04, 78.36, 3.0
2019-02-14 00:51:44,706 : start epoch
2019-02-14 00:52:25,108 : samples : 64000
2019-02-14 00:52:35,396 : Image to text: 12.84, 34.38, 47.74, 12.0
2019-02-14 00:52:42,856 : Text to Image: 10.448, 28.824, 40.996, 16.0
2019-02-14 00:53:22,948 : samples : 128000
2019-02-14 00:53:33,199 : Image to text: 12.78, 34.54, 47.5, 12.0
2019-02-14 00:53:40,657 : Text to Image: 10.564, 28.596, 40.94, 16.0
2019-02-14 00:54:22,730 : samples : 192000
2019-02-14 00:54:32,992 : Image to text: 13.36, 35.3, 48.0, 11.0
2019-02-14 00:54:40,461 : Text to Image: 10.68, 29.82, 42.144, 15.0
2019-02-14 00:55:22,292 : samples : 256000
2019-02-14 00:55:32,560 : Image to text: 13.96, 34.8, 47.52, 12.0
2019-02-14 00:55:40,049 : Text to Image: 10.724, 29.744, 42.42, 15.0
2019-02-14 00:56:22,245 : samples : 320000
2019-02-14 00:56:32,460 : Image to text: 13.22, 34.4, 47.48, 12.0
2019-02-14 00:56:39,914 : Text to Image: 10.608, 29.152, 41.368, 16.0
2019-02-14 00:57:22,327 : samples : 384000
2019-02-14 00:57:32,604 : Image to text: 12.3, 33.2, 46.44, 12.0
2019-02-14 00:57:40,118 : Text to Image: 10.304, 29.016, 41.284, 16.0
2019-02-14 00:58:20,819 : samples : 448000
2019-02-14 00:58:31,147 : Image to text: 14.26, 35.34, 49.22, 11.0
2019-02-14 00:58:38,662 : Text to Image: 10.844, 30.672, 42.78, 15.0
2019-02-14 00:59:19,021 : samples : 512000
2019-02-14 00:59:29,342 : Image to text: 14.02, 35.52, 48.26, 11.0
2019-02-14 00:59:36,699 : Text to Image: 10.82, 30.124, 42.608, 15.0
2019-02-14 01:00:10,895 : Epoch 3 finished
2019-02-14 01:00:11,324 : Image to text: 31.4, 65.5, 79.3, 3.0
2019-02-14 01:00:11,666 : Text to Image: 26.12, 61.7, 77.7, 4.0
2019-02-14 01:00:12,095 : Image to text: 32.0, 64.6, 78.9, 3.0
2019-02-14 01:00:12,469 : Text to Image: 26.24, 59.76, 76.12, 4.0
2019-02-14 01:00:12,936 : Image to text: 31.9, 64.7, 79.6, 3.0
2019-02-14 01:00:13,277 : Text to Image: 26.74, 61.04, 77.14, 4.0
2019-02-14 01:00:13,747 : Image to text: 33.1, 68.0, 80.6, 3.0
2019-02-14 01:00:14,087 : Text to Image: 26.86, 60.68, 77.62, 4.0
2019-02-14 01:00:14,552 : Image to text: 33.0, 66.2, 79.9, 3.0
2019-02-14 01:00:14,893 : Text to Image: 25.74, 60.1, 75.64, 4.0
2019-02-14 01:00:14,893 : Dev mean Text to Image: 26.34, 60.65599999999999, 76.84400000000001, 4.0
2019-02-14 01:00:14,893 : Dev mean Image to text: 32.28, 65.8, 79.66, 3.0
2019-02-14 01:00:14,893 : start epoch
2019-02-14 01:00:56,111 : samples : 64000
2019-02-14 01:01:06,427 : Image to text: 13.82, 35.12, 47.98, 11.0
2019-02-14 01:01:13,952 : Text to Image: 10.656, 30.064, 42.676, 15.0
2019-02-14 01:01:55,370 : samples : 128000
2019-02-14 01:02:05,607 : Image to text: 14.24, 34.76, 48.66, 11.0
2019-02-14 01:02:13,140 : Text to Image: 11.096, 30.444, 42.888, 15.0
2019-02-14 01:02:54,842 : samples : 192000
2019-02-14 01:03:05,079 : Image to text: 13.74, 34.68, 47.7, 12.0
2019-02-14 01:03:12,577 : Text to Image: 10.604, 29.628, 42.084, 15.0
2019-02-14 01:03:52,868 : samples : 256000
2019-02-14 01:04:03,118 : Image to text: 14.64, 35.86, 48.46, 11.0
2019-02-14 01:04:10,656 : Text to Image: 11.084, 30.952, 43.04, 15.0
2019-02-14 01:04:50,906 : samples : 320000
2019-02-14 01:05:01,163 : Image to text: 14.14, 35.78, 48.84, 11.0
2019-02-14 01:05:08,701 : Text to Image: 11.116, 30.288, 42.552, 15.0
2019-02-14 01:05:50,103 : samples : 384000
2019-02-14 01:06:00,480 : Image to text: 14.26, 36.58, 49.56, 11.0
2019-02-14 01:06:08,025 : Text to Image: 11.24, 30.588, 42.86, 14.0
2019-02-14 01:06:50,060 : samples : 448000
2019-02-14 01:07:00,455 : Image to text: 14.06, 36.28, 49.32, 11.0
2019-02-14 01:07:08,018 : Text to Image: 11.372, 30.584, 43.224, 14.0
2019-02-14 01:07:48,469 : samples : 512000
2019-02-14 01:07:58,722 : Image to text: 13.64, 35.8, 49.4, 11.0
2019-02-14 01:08:06,146 : Text to Image: 11.028, 30.384, 42.748, 14.0
2019-02-14 01:08:40,612 : Epoch 4 finished
2019-02-14 01:08:41,075 : Image to text: 34.7, 68.9, 82.8, 3.0
2019-02-14 01:08:41,416 : Text to Image: 27.64, 62.22, 77.84, 3.0
2019-02-14 01:08:41,882 : Image to text: 32.9, 66.1, 78.8, 3.0
2019-02-14 01:08:42,222 : Text to Image: 26.26, 60.22, 76.4, 4.0
2019-02-14 01:08:42,690 : Image to text: 35.0, 68.6, 80.9, 3.0
2019-02-14 01:08:43,031 : Text to Image: 27.5, 62.76, 77.68, 3.0
2019-02-14 01:08:43,504 : Image to text: 34.8, 67.4, 80.3, 3.0
2019-02-14 01:08:43,845 : Text to Image: 27.68, 61.8, 77.72, 4.0
2019-02-14 01:08:44,308 : Image to text: 36.1, 68.0, 80.5, 3.0
2019-02-14 01:08:44,649 : Text to Image: 26.58, 61.32, 76.48, 4.0
2019-02-14 01:08:44,649 : Dev mean Text to Image: 27.132, 61.664, 77.224, 3.5999999999999996
2019-02-14 01:08:44,649 : Dev mean Image to text: 34.699999999999996, 67.8, 80.66, 3.0
2019-02-14 01:08:44,650 : start epoch
2019-02-14 01:09:26,704 : samples : 64000
2019-02-14 01:09:37,090 : Image to text: 15.16, 36.28, 49.98, 11.0
2019-02-14 01:09:44,659 : Text to Image: 11.596, 30.98, 43.74, 14.0
2019-02-14 01:10:26,579 : samples : 128000
2019-02-14 01:10:36,887 : Image to text: 13.78, 35.92, 49.54, 11.0
2019-02-14 01:10:44,472 : Text to Image: 11.504, 30.764, 43.588, 14.0
2019-02-14 01:11:26,397 : samples : 192000
2019-02-14 01:11:36,707 : Image to text: 13.76, 34.94, 48.44, 11.0
2019-02-14 01:11:44,375 : Text to Image: 11.244, 30.312, 42.968, 15.0
2019-02-14 01:12:24,835 : samples : 256000
2019-02-14 01:12:35,092 : Image to text: 13.92, 36.42, 49.68, 11.0
2019-02-14 01:12:42,769 : Text to Image: 11.188, 30.608, 43.312, 14.0
2019-02-14 01:13:28,071 : samples : 320000
2019-02-14 01:13:38,319 : Image to text: 14.5, 36.74, 49.9, 11.0
2019-02-14 01:13:45,889 : Text to Image: 11.668, 31.404, 43.944, 14.0
2019-02-14 01:14:26,621 : samples : 384000
2019-02-14 01:14:36,845 : Image to text: 14.36, 36.18, 48.96, 11.0
2019-02-14 01:14:44,438 : Text to Image: 11.028, 30.432, 43.5, 14.0
2019-02-14 01:15:24,754 : samples : 448000
2019-02-14 01:15:34,993 : Image to text: 14.4, 36.94, 50.14, 10.0
2019-02-14 01:15:42,579 : Text to Image: 11.152, 30.944, 43.44, 14.0
2019-02-14 01:16:23,359 : samples : 512000
2019-02-14 01:16:33,560 : Image to text: 14.96, 37.22, 50.32, 10.0
2019-02-14 01:16:40,979 : Text to Image: 11.352, 30.972, 43.332, 14.0
2019-02-14 01:17:15,393 : Epoch 5 finished
2019-02-14 01:17:15,859 : Image to text: 33.0, 65.6, 80.8, 3.0
2019-02-14 01:17:16,200 : Text to Image: 27.02, 62.44, 79.42, 3.0
2019-02-14 01:17:16,664 : Image to text: 33.7, 65.8, 81.4, 3.0
2019-02-14 01:17:17,005 : Text to Image: 27.0, 60.78, 77.38, 4.0
2019-02-14 01:17:17,473 : Image to text: 33.3, 69.1, 81.8, 3.0
2019-02-14 01:17:17,813 : Text to Image: 27.58, 62.68, 78.1, 3.0
2019-02-14 01:17:18,279 : Image to text: 35.1, 66.8, 81.1, 3.0
2019-02-14 01:17:18,620 : Text to Image: 27.56, 62.02, 77.98, 3.0
2019-02-14 01:17:19,078 : Image to text: 33.5, 67.7, 81.2, 3.0
2019-02-14 01:17:19,419 : Text to Image: 26.6, 60.78, 77.68, 4.0
2019-02-14 01:17:19,419 : Dev mean Text to Image: 27.152, 61.74, 78.112, 3.4000000000000004
2019-02-14 01:17:19,419 : Dev mean Image to text: 33.72, 67.0, 81.25999999999999, 3.0
2019-02-14 01:17:19,419 : start epoch
2019-02-14 01:17:59,961 : samples : 64000
2019-02-14 01:18:10,235 : Image to text: 15.08, 36.88, 50.52, 10.0
2019-02-14 01:18:17,823 : Text to Image: 11.8, 31.98, 44.352, 14.0
2019-02-14 01:18:58,339 : samples : 128000
2019-02-14 01:19:08,568 : Image to text: 14.2, 36.9, 50.08, 10.0
2019-02-14 01:19:16,185 : Text to Image: 11.344, 30.984, 43.596, 14.0
2019-02-14 01:19:58,537 : samples : 192000
2019-02-14 01:20:08,924 : Image to text: 14.92, 37.62, 50.0, 10.0
2019-02-14 01:20:16,581 : Text to Image: 11.528, 31.076, 43.728, 14.0
2019-02-14 01:20:57,605 : samples : 256000
2019-02-14 01:21:07,880 : Image to text: 14.6, 37.26, 50.42, 10.0
2019-02-14 01:21:15,547 : Text to Image: 11.728, 31.48, 43.964, 14.0
2019-02-14 01:21:57,580 : samples : 320000
2019-02-14 01:22:07,884 : Image to text: 14.48, 37.18, 51.38, 10.0
2019-02-14 01:22:15,560 : Text to Image: 11.676, 31.476, 43.888, 14.0
2019-02-14 01:22:56,110 : samples : 384000
2019-02-14 01:23:06,494 : Image to text: 15.56, 37.48, 51.18, 10.0
2019-02-14 01:23:14,166 : Text to Image: 11.756, 31.924, 44.336, 14.0
2019-02-14 01:23:56,485 : samples : 448000
2019-02-14 01:24:06,847 : Image to text: 14.48, 36.42, 50.18, 10.0
2019-02-14 01:24:14,494 : Text to Image: 11.704, 31.748, 44.452, 13.0
2019-02-14 01:24:55,004 : samples : 512000
2019-02-14 01:25:05,241 : Image to text: 14.86, 36.96, 50.28, 10.0
2019-02-14 01:25:12,710 : Text to Image: 11.612, 31.828, 44.692, 14.0
2019-02-14 01:25:46,924 : Epoch 6 finished
2019-02-14 01:25:47,383 : Image to text: 33.8, 67.6, 82.2, 3.0
2019-02-14 01:25:47,724 : Text to Image: 27.8, 62.66, 78.48, 3.0
2019-02-14 01:25:48,187 : Image to text: 32.7, 66.2, 81.6, 3.0
2019-02-14 01:25:48,529 : Text to Image: 27.56, 61.92, 77.64, 4.0
2019-02-14 01:25:48,986 : Image to text: 35.8, 67.8, 81.1, 3.0
2019-02-14 01:25:49,328 : Text to Image: 28.7, 62.84, 78.26, 3.0
2019-02-14 01:25:49,785 : Image to text: 35.7, 70.1, 82.5, 3.0
2019-02-14 01:25:50,126 : Text to Image: 28.12, 62.8, 78.54, 3.0
2019-02-14 01:25:50,581 : Image to text: 33.2, 69.5, 80.5, 3.0
2019-02-14 01:25:50,923 : Text to Image: 27.88, 61.9, 77.06, 4.0
2019-02-14 01:25:50,923 : Dev mean Text to Image: 28.012, 62.42399999999999, 77.99600000000001, 3.4000000000000004
2019-02-14 01:25:50,923 : Dev mean Image to text: 34.24, 68.24, 81.58000000000001, 3.0
2019-02-14 01:25:50,924 : start epoch
2019-02-14 01:26:35,995 : samples : 64000
2019-02-14 01:26:46,379 : Image to text: 14.58, 37.34, 50.58, 10.0
2019-02-14 01:26:54,018 : Text to Image: 11.712, 31.356, 44.18, 14.0
2019-02-14 01:27:36,750 : samples : 128000
2019-02-14 01:27:47,119 : Image to text: 14.84, 37.66, 51.62, 10.0
2019-02-14 01:27:54,765 : Text to Image: 11.96, 31.928, 45.028, 13.0
2019-02-14 01:28:35,322 : samples : 192000
2019-02-14 01:28:45,645 : Image to text: 15.34, 38.02, 51.82, 10.0
2019-02-14 01:28:53,297 : Text to Image: 12.044, 32.14, 44.856, 13.0
2019-02-14 01:29:34,746 : samples : 256000
2019-02-14 01:29:45,014 : Image to text: 14.4, 35.82, 49.68, 11.0
2019-02-14 01:29:52,677 : Text to Image: 11.616, 31.864, 44.868, 14.0
2019-02-14 01:30:35,556 : samples : 320000
2019-02-14 01:30:45,751 : Image to text: 15.5, 37.58, 50.74, 10.0
2019-02-14 01:30:53,408 : Text to Image: 11.952, 32.152, 44.924, 13.0
2019-02-14 01:31:36,224 : samples : 384000
2019-02-14 01:31:46,454 : Image to text: 14.8, 37.98, 50.86, 10.0
2019-02-14 01:31:54,095 : Text to Image: 11.888, 32.132, 44.96, 13.0
2019-02-14 01:32:37,384 : samples : 448000
2019-02-14 01:32:47,623 : Image to text: 14.68, 36.74, 50.78, 10.0
2019-02-14 01:32:55,232 : Text to Image: 11.776, 32.12, 44.66, 13.0
2019-02-14 01:33:38,468 : samples : 512000
2019-02-14 01:33:48,760 : Image to text: 15.28, 37.54, 51.08, 10.0
2019-02-14 01:33:56,221 : Text to Image: 11.928, 31.82, 44.796, 13.0
2019-02-14 01:34:33,045 : Epoch 7 finished
2019-02-14 01:34:33,502 : Image to text: 35.1, 68.4, 81.5, 3.0
2019-02-14 01:34:33,844 : Text to Image: 28.56, 62.74, 78.88, 3.0
2019-02-14 01:34:34,306 : Image to text: 32.5, 66.3, 81.5, 3.0
2019-02-14 01:34:34,648 : Text to Image: 27.38, 62.08, 78.1, 4.0
2019-02-14 01:34:35,115 : Image to text: 35.7, 68.2, 82.1, 3.0
2019-02-14 01:34:35,458 : Text to Image: 29.06, 63.74, 79.24, 3.0
2019-02-14 01:34:35,923 : Image to text: 33.5, 68.6, 82.6, 3.0
2019-02-14 01:34:36,241 : Text to Image: 28.26, 63.32, 79.3, 3.0
2019-02-14 01:34:36,637 : Image to text: 32.7, 68.4, 80.4, 3.0
2019-02-14 01:34:36,935 : Text to Image: 27.6, 62.5, 77.26, 4.0
2019-02-14 01:34:36,936 : Dev mean Text to Image: 28.172, 62.876000000000005, 78.556, 3.4000000000000004
2019-02-14 01:34:36,936 : Dev mean Image to text: 33.9, 67.98, 81.61999999999999, 3.0
2019-02-14 01:34:36,936 : start epoch
2019-02-14 01:35:19,845 : samples : 64000
2019-02-14 01:35:30,061 : Image to text: 15.0, 37.64, 50.88, 10.0
2019-02-14 01:35:37,627 : Text to Image: 11.692, 31.828, 44.608, 13.0
2019-02-14 01:36:21,925 : samples : 128000
2019-02-14 01:36:32,072 : Image to text: 15.48, 38.64, 51.96, 10.0
2019-02-14 01:36:39,598 : Text to Image: 12.116, 32.296, 45.388, 13.0
2019-02-14 01:37:22,193 : samples : 192000
2019-02-14 01:37:32,375 : Image to text: 15.2, 37.56, 51.6, 10.0
2019-02-14 01:37:39,942 : Text to Image: 12.06, 32.376, 45.164, 13.0
2019-02-14 01:38:21,301 : samples : 256000
2019-02-14 01:38:31,519 : Image to text: 14.68, 38.58, 50.72, 10.0
2019-02-14 01:38:39,037 : Text to Image: 12.008, 31.908, 44.68, 13.0
2019-02-14 01:39:19,914 : samples : 320000
2019-02-14 01:39:30,131 : Image to text: 16.02, 38.64, 51.86, 9.0
2019-02-14 01:39:37,604 : Text to Image: 12.26, 32.628, 45.436, 13.0
2019-02-14 01:40:18,031 : samples : 384000
2019-02-14 01:40:28,214 : Image to text: 14.7, 37.6, 51.18, 10.0
2019-02-14 01:40:35,670 : Text to Image: 12.072, 32.324, 45.116, 13.0
2019-02-14 01:41:17,191 : samples : 448000
2019-02-14 01:41:27,392 : Image to text: 15.28, 36.6, 50.28, 10.0
2019-02-14 01:41:34,822 : Text to Image: 11.788, 31.908, 44.7, 13.0
2019-02-14 01:42:16,454 : samples : 512000
2019-02-14 01:42:26,365 : Image to text: 15.8, 38.64, 51.68, 10.0
2019-02-14 01:42:35,697 : Text to Image: 12.332, 32.412, 45.424, 13.0
2019-02-14 01:43:12,964 : Epoch 8 finished
2019-02-14 01:43:13,954 : Image to text: 35.1, 68.0, 82.4, 3.0
2019-02-14 01:43:14,733 : Text to Image: 28.7, 63.78, 79.24, 3.0
2019-02-14 01:43:15,714 : Image to text: 35.0, 66.4, 81.7, 3.0
2019-02-14 01:43:16,519 : Text to Image: 28.42, 61.4, 78.44, 4.0
2019-02-14 01:43:17,454 : Image to text: 34.5, 70.1, 82.1, 3.0
2019-02-14 01:43:18,280 : Text to Image: 28.22, 65.0, 79.64, 3.0
2019-02-14 01:43:19,233 : Image to text: 36.4, 68.9, 82.5, 2.0
2019-02-14 01:43:20,018 : Text to Image: 28.04, 63.38, 79.3, 3.0
2019-02-14 01:43:20,955 : Image to text: 36.5, 68.8, 81.4, 3.0
2019-02-14 01:43:21,755 : Text to Image: 27.84, 62.28, 78.04, 3.0
2019-02-14 01:43:21,755 : Dev mean Text to Image: 28.244, 63.168000000000006, 78.932, 3.2
2019-02-14 01:43:21,755 : Dev mean Image to text: 35.5, 68.44000000000001, 82.02, 2.8
2019-02-14 01:43:21,755 : start epoch
2019-02-14 01:44:06,211 : samples : 64000
2019-02-14 01:44:16,259 : Image to text: 15.32, 37.3, 51.28, 10.0
2019-02-14 01:44:23,453 : Text to Image: 12.212, 32.22, 44.78, 13.0
2019-02-14 01:45:05,743 : samples : 128000
2019-02-14 01:45:18,356 : Image to text: 15.6, 38.34, 52.34, 9.0
2019-02-14 01:45:28,437 : Text to Image: 12.016, 32.068, 44.996, 13.0
2019-02-14 01:46:12,245 : samples : 192000
2019-02-14 01:46:22,344 : Image to text: 14.86, 38.3, 52.04, 10.0
2019-02-14 01:46:29,453 : Text to Image: 12.128, 32.248, 45.232, 13.0
2019-02-14 01:47:10,725 : samples : 256000
2019-02-14 01:47:23,328 : Image to text: 15.1, 37.38, 51.4, 10.0
2019-02-14 01:47:33,420 : Text to Image: 11.844, 32.284, 45.256, 13.0
2019-02-14 01:48:17,583 : samples : 320000
2019-02-14 01:48:29,732 : Image to text: 16.08, 37.56, 52.22, 9.0
2019-02-14 01:48:36,996 : Text to Image: 12.38, 32.568, 45.356, 13.0
2019-02-14 01:49:18,199 : samples : 384000
2019-02-14 01:49:30,762 : Image to text: 15.46, 37.86, 51.44, 10.0
2019-02-14 01:49:40,740 : Text to Image: 12.26, 32.524, 45.14, 13.0
2019-02-14 01:50:24,919 : samples : 448000
2019-02-14 01:50:35,992 : Image to text: 15.64, 37.9, 52.34, 9.0
2019-02-14 01:50:43,276 : Text to Image: 12.164, 32.372, 45.088, 13.0
2019-02-14 01:51:24,072 : samples : 512000
2019-02-14 01:51:36,630 : Image to text: 15.16, 37.54, 51.74, 10.0
2019-02-14 01:51:46,600 : Text to Image: 12.116, 32.588, 45.472, 13.0
2019-02-14 01:52:23,913 : Epoch 9 finished
2019-02-14 01:52:24,842 : Image to text: 36.7, 70.0, 81.4, 3.0
2019-02-14 01:52:25,667 : Text to Image: 28.94, 64.1, 80.22, 3.0
2019-02-14 01:52:26,573 : Image to text: 33.5, 67.3, 81.7, 3.0
2019-02-14 01:52:27,420 : Text to Image: 27.9, 62.48, 78.24, 3.0
2019-02-14 01:52:28,338 : Image to text: 36.0, 68.7, 81.7, 3.0
2019-02-14 01:52:29,089 : Text to Image: 28.84, 64.06, 79.9, 3.0
2019-02-14 01:52:30,071 : Image to text: 34.3, 69.6, 81.4, 3.0
2019-02-14 01:52:30,714 : Text to Image: 28.94, 63.7, 79.62, 3.0
2019-02-14 01:52:31,635 : Image to text: 34.9, 69.8, 81.9, 3.0
2019-02-14 01:52:32,395 : Text to Image: 28.28, 63.48, 78.54, 3.0
2019-02-14 01:52:32,395 : Dev mean Text to Image: 28.58, 63.564, 79.304, 3.0
2019-02-14 01:52:32,395 : Dev mean Image to text: 35.08, 69.08, 81.62, 3.0
2019-02-14 01:52:32,395 : start epoch
2019-02-14 01:53:13,591 : samples : 64000
2019-02-14 01:53:24,664 : Image to text: 15.9, 38.94, 52.42, 9.0
2019-02-14 01:53:34,708 : Text to Image: 12.232, 32.684, 45.86, 13.0
2019-02-14 01:54:17,909 : samples : 128000
2019-02-14 01:54:30,593 : Image to text: 15.5, 37.68, 51.4, 10.0
2019-02-14 01:54:40,705 : Text to Image: 12.448, 32.724, 45.512, 13.0
2019-02-14 01:55:21,993 : samples : 192000
2019-02-14 01:55:32,324 : Image to text: 15.38, 38.02, 52.1, 10.0
2019-02-14 01:55:42,303 : Text to Image: 12.34, 32.74, 45.628, 13.0
2019-02-14 01:56:25,718 : samples : 256000
2019-02-14 01:56:38,378 : Image to text: 15.74, 38.46, 51.88, 10.0
2019-02-14 01:56:48,385 : Text to Image: 12.168, 32.812, 45.336, 13.0
2019-02-14 01:57:29,739 : samples : 320000
2019-02-14 01:57:39,797 : Image to text: 15.58, 38.64, 51.74, 10.0
2019-02-14 01:57:47,042 : Text to Image: 12.512, 32.944, 45.904, 13.0
2019-02-14 01:58:29,339 : samples : 384000
2019-02-14 01:58:41,934 : Image to text: 15.5, 38.26, 51.38, 10.0
2019-02-14 01:58:52,013 : Text to Image: 12.168, 32.448, 45.296, 13.0
2019-02-14 01:59:34,544 : samples : 448000
2019-02-14 01:59:44,587 : Image to text: 16.08, 38.76, 52.32, 9.0
2019-02-14 01:59:51,731 : Text to Image: 12.512, 32.772, 45.896, 13.0
2019-02-14 02:00:33,628 : samples : 512000
2019-02-14 02:00:46,239 : Image to text: 15.02, 37.7, 51.26, 10.0
2019-02-14 02:00:56,319 : Text to Image: 12.02, 32.676, 45.332, 13.0
2019-02-14 02:01:34,054 : Epoch 10 finished
2019-02-14 02:01:34,978 : Image to text: 35.8, 67.7, 82.9, 3.0
2019-02-14 02:01:35,766 : Text to Image: 29.08, 63.8, 80.16, 3.0
2019-02-14 02:01:36,695 : Image to text: 34.1, 67.9, 82.0, 3.0
2019-02-14 02:01:37,511 : Text to Image: 28.4, 62.28, 78.6, 4.0
2019-02-14 02:01:38,490 : Image to text: 37.2, 68.4, 82.9, 2.0
2019-02-14 02:01:39,302 : Text to Image: 29.64, 64.64, 79.58, 3.0
2019-02-14 02:01:40,234 : Image to text: 35.4, 70.1, 82.9, 3.0
2019-02-14 02:01:40,868 : Text to Image: 28.6, 64.06, 79.88, 3.0
2019-02-14 02:01:41,326 : Image to text: 33.8, 69.9, 81.4, 3.0
2019-02-14 02:01:41,693 : Text to Image: 27.92, 63.58, 78.38, 3.0
2019-02-14 02:01:41,693 : Dev mean Text to Image: 28.727999999999998, 63.672000000000004, 79.32, 3.2
2019-02-14 02:01:41,694 : Dev mean Image to text: 35.26, 68.80000000000001, 82.42, 2.8000000000000003
2019-02-14 02:01:41,694 : start epoch
2019-02-14 02:02:22,182 : samples : 64000
2019-02-14 02:02:33,521 : Image to text: 15.36, 39.24, 53.16, 9.0
2019-02-14 02:02:43,528 : Text to Image: 12.32, 32.768, 45.564, 13.0
2019-02-14 02:03:28,171 : samples : 128000
2019-02-14 02:03:40,817 : Image to text: 15.48, 38.2, 51.46, 10.0
2019-02-14 02:03:50,882 : Text to Image: 12.164, 32.476, 45.636, 13.0
2019-02-14 02:04:32,501 : samples : 192000
2019-02-14 02:04:42,450 : Image to text: 15.7, 38.78, 52.94, 9.0
2019-02-14 02:04:50,842 : Text to Image: 12.468, 32.828, 45.632, 13.0
2019-02-14 02:05:33,096 : samples : 256000
2019-02-14 02:05:44,039 : Image to text: 16.02, 38.92, 52.66, 9.0
2019-02-14 02:05:51,266 : Text to Image: 12.176, 32.584, 45.612, 13.0
2019-02-14 02:06:31,903 : samples : 320000
2019-02-14 02:06:42,417 : Image to text: 15.72, 39.06, 52.54, 9.0
2019-02-14 02:06:49,959 : Text to Image: 12.368, 32.9, 45.564, 13.0
2019-02-14 02:07:30,610 : samples : 384000
2019-02-14 02:07:40,635 : Image to text: 16.2, 39.3, 53.22, 9.0
2019-02-14 02:07:47,847 : Text to Image: 12.528, 33.528, 46.288, 13.0
2019-02-14 02:08:29,680 : samples : 448000
2019-02-14 02:08:39,721 : Image to text: 15.78, 38.12, 52.0, 10.0
2019-02-14 02:08:46,954 : Text to Image: 12.468, 33.152, 46.02, 13.0
2019-02-14 02:09:27,829 : samples : 512000
2019-02-14 02:09:37,748 : Image to text: 15.82, 38.56, 53.2, 9.0
2019-02-14 02:09:47,545 : Text to Image: 12.4, 32.876, 45.84, 13.0
2019-02-14 02:10:25,079 : Epoch 11 finished
2019-02-14 02:10:25,871 : Image to text: 35.2, 69.6, 82.3, 3.0
2019-02-14 02:10:26,234 : Text to Image: 28.1, 63.9, 79.62, 3.0
2019-02-14 02:10:26,683 : Image to text: 35.3, 68.1, 82.5, 3.0
2019-02-14 02:10:27,045 : Text to Image: 28.58, 62.46, 78.48, 3.0
2019-02-14 02:10:27,494 : Image to text: 36.5, 69.2, 82.7, 2.0
2019-02-14 02:10:27,856 : Text to Image: 30.38, 64.08, 78.92, 3.0
2019-02-14 02:10:28,306 : Image to text: 36.6, 71.1, 83.3, 2.0
2019-02-14 02:10:28,668 : Text to Image: 28.66, 63.9, 79.42, 3.0
2019-02-14 02:10:29,117 : Image to text: 36.2, 68.5, 80.9, 3.0
2019-02-14 02:10:29,480 : Text to Image: 28.28, 62.54, 78.36, 3.0
2019-02-14 02:10:29,480 : Dev mean Text to Image: 28.799999999999997, 63.37599999999999, 78.96000000000001, 3.0
2019-02-14 02:10:29,480 : Dev mean Image to text: 35.96, 69.3, 82.34, 2.6
2019-02-14 02:10:29,480 : start epoch
2019-02-14 02:11:11,191 : samples : 64000
2019-02-14 02:11:22,025 : Image to text: 15.46, 38.38, 52.14, 9.0
2019-02-14 02:11:28,924 : Text to Image: 12.176, 32.764, 45.868, 13.0
2019-02-14 02:12:11,443 : samples : 128000
2019-02-14 02:12:23,540 : Image to text: 15.62, 38.62, 52.72, 9.0
2019-02-14 02:12:30,787 : Text to Image: 12.516, 32.756, 45.804, 13.0
2019-02-14 02:13:11,665 : samples : 192000
2019-02-14 02:13:21,816 : Image to text: 15.5, 39.38, 53.1, 9.0
2019-02-14 02:13:29,609 : Text to Image: 12.576, 33.192, 45.992, 13.0
2019-02-14 02:14:11,011 : samples : 256000
2019-02-14 02:14:21,330 : Image to text: 15.84, 38.18, 52.04, 10.0
2019-02-14 02:14:31,256 : Text to Image: 12.596, 32.936, 46.06, 13.0
2019-02-14 02:15:15,553 : samples : 320000
2019-02-14 02:15:28,080 : Image to text: 16.04, 38.48, 51.74, 10.0
2019-02-14 02:15:37,991 : Text to Image: 12.468, 32.884, 45.804, 13.0
2019-02-14 02:16:22,492 : samples : 384000
2019-02-14 02:16:35,053 : Image to text: 15.62, 38.54, 52.24, 9.0
2019-02-14 02:16:45,005 : Text to Image: 12.764, 33.412, 46.216, 13.0
2019-02-14 02:17:29,589 : samples : 448000
2019-02-14 02:17:42,186 : Image to text: 15.92, 39.88, 52.64, 9.0
2019-02-14 02:17:52,221 : Text to Image: 12.916, 33.588, 46.588, 12.0
2019-02-14 02:18:36,639 : samples : 512000
2019-02-14 02:18:49,323 : Image to text: 15.96, 38.68, 52.98, 9.0
2019-02-14 02:18:59,390 : Text to Image: 12.472, 33.068, 46.324, 12.0
2019-02-14 02:19:37,386 : Epoch 12 finished
2019-02-14 02:19:38,302 : Image to text: 36.0, 69.8, 81.8, 3.0
2019-02-14 02:19:39,052 : Text to Image: 28.96, 65.14, 80.38, 3.0
2019-02-14 02:19:39,966 : Image to text: 35.2, 69.1, 82.2, 3.0
2019-02-14 02:19:40,726 : Text to Image: 28.02, 63.44, 79.16, 3.0
2019-02-14 02:19:41,665 : Image to text: 35.8, 70.6, 83.4, 2.0
2019-02-14 02:19:42,434 : Text to Image: 29.24, 64.8, 79.78, 3.0
2019-02-14 02:19:43,353 : Image to text: 35.1, 69.4, 82.4, 3.0
2019-02-14 02:19:44,101 : Text to Image: 29.36, 64.28, 80.22, 3.0
2019-02-14 02:19:45,011 : Image to text: 36.8, 69.4, 82.3, 3.0
2019-02-14 02:19:45,759 : Text to Image: 28.58, 63.58, 79.58, 3.0
2019-02-14 02:19:45,759 : Dev mean Text to Image: 28.832, 64.248, 79.824, 3.0
2019-02-14 02:19:45,759 : Dev mean Image to text: 35.78, 69.66, 82.41999999999999, 2.8000000000000003
2019-02-14 02:19:45,760 : start epoch
2019-02-14 02:20:28,923 : samples : 64000
2019-02-14 02:20:41,593 : Image to text: 15.88, 39.7, 53.6, 9.0
2019-02-14 02:20:51,687 : Text to Image: 12.608, 33.196, 45.928, 13.0
2019-02-14 02:21:34,722 : samples : 128000
2019-02-14 02:21:47,341 : Image to text: 15.6, 38.46, 52.48, 9.0
2019-02-14 02:21:57,383 : Text to Image: 12.42, 33.04, 46.044, 13.0
2019-02-14 02:22:40,034 : samples : 192000
2019-02-14 02:22:52,669 : Image to text: 15.8, 39.3, 53.7, 9.0
2019-02-14 02:23:02,733 : Text to Image: 12.36, 33.22, 46.256, 13.0
2019-02-14 02:23:45,713 : samples : 256000
2019-02-14 02:23:58,344 : Image to text: 16.76, 40.06, 53.06, 9.0
2019-02-14 02:24:08,360 : Text to Image: 12.72, 33.516, 46.432, 12.0
2019-02-14 02:24:51,302 : samples : 320000
2019-02-14 02:25:03,958 : Image to text: 15.94, 39.34, 52.34, 9.0
2019-02-14 02:25:14,021 : Text to Image: 12.568, 33.504, 46.596, 12.0
2019-02-14 02:25:58,670 : samples : 384000
2019-02-14 02:26:09,912 : Image to text: 15.56, 38.92, 52.6, 9.0
2019-02-14 02:26:17,082 : Text to Image: 12.616, 33.288, 45.912, 13.0
2019-02-14 02:26:58,072 : samples : 448000
2019-02-14 02:27:08,122 : Image to text: 15.66, 38.74, 52.86, 9.0
2019-02-14 02:27:15,338 : Text to Image: 12.228, 32.792, 45.66, 13.0
2019-02-14 02:27:57,786 : samples : 512000
2019-02-14 02:28:10,658 : Image to text: 16.04, 38.84, 53.08, 9.0
2019-02-14 02:28:21,053 : Text to Image: 12.432, 33.068, 46.048, 13.0
2019-02-14 02:28:58,394 : Epoch 13 finished
2019-02-14 02:28:59,457 : Image to text: 35.9, 68.3, 81.7, 3.0
2019-02-14 02:29:00,327 : Text to Image: 29.5, 65.18, 80.9, 3.0
2019-02-14 02:29:01,434 : Image to text: 34.7, 68.8, 82.8, 3.0
2019-02-14 02:29:02,333 : Text to Image: 28.9, 62.8, 79.34, 3.0
2019-02-14 02:29:03,376 : Image to text: 35.7, 68.8, 83.0, 3.0
2019-02-14 02:29:04,293 : Text to Image: 29.8, 64.96, 80.12, 3.0
2019-02-14 02:29:05,388 : Image to text: 35.9, 68.8, 83.3, 2.0
2019-02-14 02:29:06,324 : Text to Image: 29.32, 65.08, 80.56, 3.0
2019-02-14 02:29:07,439 : Image to text: 35.3, 69.7, 81.3, 2.0
2019-02-14 02:29:08,363 : Text to Image: 28.46, 63.64, 79.46, 3.0
2019-02-14 02:29:08,363 : Dev mean Text to Image: 29.196, 64.332, 80.076, 3.0
2019-02-14 02:29:08,363 : Dev mean Image to text: 35.5, 68.88, 82.41999999999999, 2.5999999999999996
2019-02-14 02:29:08,364 : start epoch
2019-02-14 02:29:53,777 : samples : 64000
2019-02-14 02:30:06,831 : Image to text: 15.84, 39.08, 53.38, 9.0
2019-02-14 02:30:17,376 : Text to Image: 12.508, 33.348, 46.444, 13.0
2019-02-14 02:31:02,437 : samples : 128000
2019-02-14 02:31:15,460 : Image to text: 16.36, 39.84, 53.54, 9.0
2019-02-14 02:31:26,006 : Text to Image: 12.788, 33.524, 46.052, 13.0
2019-02-14 02:32:11,402 : samples : 192000
2019-02-14 02:32:24,349 : Image to text: 16.66, 39.62, 53.4, 9.0
2019-02-14 02:32:34,834 : Text to Image: 12.604, 33.052, 45.928, 13.0
2019-02-14 02:33:19,945 : samples : 256000
2019-02-14 02:33:32,870 : Image to text: 15.7, 39.38, 52.56, 9.0
2019-02-14 02:33:43,327 : Text to Image: 12.516, 33.292, 46.256, 12.0
2019-02-14 02:34:28,130 : samples : 320000
2019-02-14 02:34:41,073 : Image to text: 15.76, 39.16, 53.2, 9.0
2019-02-14 02:34:51,526 : Text to Image: 12.624, 33.22, 46.284, 12.0
2019-02-14 02:35:36,847 : samples : 384000
2019-02-14 02:35:49,826 : Image to text: 16.62, 39.22, 53.38, 9.0
2019-02-14 02:35:59,250 : Text to Image: 12.524, 33.62, 46.356, 12.0
2019-02-14 02:36:41,882 : samples : 448000
2019-02-14 02:36:51,925 : Image to text: 16.12, 39.42, 53.26, 9.0
2019-02-14 02:36:59,227 : Text to Image: 12.556, 33.24, 46.068, 13.0
2019-02-14 02:37:41,964 : samples : 512000
2019-02-14 02:37:52,011 : Image to text: 16.46, 39.84, 53.44, 9.0
2019-02-14 02:37:59,322 : Text to Image: 12.444, 33.56, 46.316, 12.0
2019-02-14 02:38:35,898 : Epoch 14 finished
2019-02-14 02:38:36,328 : Image to text: 36.5, 69.5, 82.7, 3.0
2019-02-14 02:38:36,655 : Text to Image: 30.42, 65.1, 79.9, 3.0
2019-02-14 02:38:37,080 : Image to text: 35.2, 68.9, 81.5, 3.0
2019-02-14 02:38:37,407 : Text to Image: 28.6, 63.22, 79.48, 3.0
2019-02-14 02:38:37,835 : Image to text: 37.7, 69.4, 82.2, 2.0
2019-02-14 02:38:38,165 : Text to Image: 30.26, 65.06, 80.28, 3.0
2019-02-14 02:38:38,606 : Image to text: 36.5, 71.0, 83.7, 3.0
2019-02-14 02:38:38,935 : Text to Image: 29.38, 64.84, 80.54, 3.0
2019-02-14 02:38:39,364 : Image to text: 34.6, 70.5, 82.0, 2.0
2019-02-14 02:38:39,694 : Text to Image: 28.68, 63.8, 79.58, 3.0
2019-02-14 02:38:39,694 : Dev mean Text to Image: 29.468, 64.40400000000001, 79.956, 3.0
2019-02-14 02:38:39,694 : Dev mean Image to text: 36.1, 69.86, 82.42000000000002, 2.6
2019-02-14 02:38:39,694 : start epoch
2019-02-14 02:39:20,675 : samples : 64000
2019-02-14 02:39:30,994 : Image to text: 16.06, 38.78, 52.92, 9.0
2019-02-14 02:39:38,441 : Text to Image: 12.508, 33.416, 46.356, 12.0
2019-02-14 02:40:19,560 : samples : 128000
2019-02-14 02:40:29,948 : Image to text: 16.46, 39.66, 52.52, 9.0
2019-02-14 02:40:37,408 : Text to Image: 12.204, 33.388, 46.104, 12.0
2019-02-14 02:41:21,568 : samples : 192000
2019-02-14 02:41:31,872 : Image to text: 16.6, 39.88, 53.62, 9.0
2019-02-14 02:41:39,290 : Text to Image: 12.668, 33.412, 46.372, 12.0
2019-02-14 02:42:19,911 : samples : 256000
2019-02-14 02:42:30,199 : Image to text: 16.4, 39.92, 53.5, 9.0
2019-02-14 02:42:37,618 : Text to Image: 12.824, 33.78, 46.816, 12.0
2019-02-14 02:43:20,202 : samples : 320000
2019-02-14 02:43:30,546 : Image to text: 16.1, 39.06, 53.16, 9.0
2019-02-14 02:43:37,993 : Text to Image: 12.864, 33.468, 46.436, 12.0
2019-02-14 02:44:19,492 : samples : 384000
2019-02-14 02:44:29,809 : Image to text: 15.98, 38.82, 53.26, 9.0
2019-02-14 02:44:37,193 : Text to Image: 12.764, 33.448, 46.588, 12.0
2019-02-14 02:45:19,930 : samples : 448000
2019-02-14 02:45:29,975 : Image to text: 16.68, 39.22, 53.34, 9.0
2019-02-14 02:45:37,287 : Text to Image: 12.968, 33.956, 46.808, 12.0
2019-02-14 02:46:19,778 : samples : 512000
2019-02-14 02:46:29,837 : Image to text: 16.3, 39.66, 53.28, 9.0
2019-02-14 02:46:37,147 : Text to Image: 12.768, 33.412, 46.368, 12.0
2019-02-14 02:47:13,087 : Epoch 15 finished
2019-02-14 02:47:13,518 : Image to text: 36.5, 70.4, 82.9, 3.0
2019-02-14 02:47:13,846 : Text to Image: 29.96, 64.66, 80.24, 3.0
2019-02-14 02:47:14,272 : Image to text: 35.6, 68.9, 83.6, 3.0
2019-02-14 02:47:14,600 : Text to Image: 28.78, 63.36, 79.32, 3.0
2019-02-14 02:47:15,042 : Image to text: 38.2, 70.7, 83.4, 2.0
2019-02-14 02:47:15,374 : Text to Image: 29.5, 64.6, 79.9, 3.0
2019-02-14 02:47:15,806 : Image to text: 37.9, 71.4, 83.4, 2.0
2019-02-14 02:47:16,136 : Text to Image: 29.3, 64.64, 80.16, 3.0
2019-02-14 02:47:16,566 : Image to text: 37.2, 69.3, 81.6, 2.0
2019-02-14 02:47:16,895 : Text to Image: 29.0, 63.9, 78.96, 3.0
2019-02-14 02:47:16,896 : Dev mean Text to Image: 29.308000000000003, 64.232, 79.716, 3.0
2019-02-14 02:47:16,896 : Dev mean Image to text: 37.08, 70.14, 82.97999999999999, 2.4
2019-02-14 02:47:16,896 : start epoch
2019-02-14 02:47:59,083 : samples : 64000
2019-02-14 02:48:09,422 : Image to text: 16.54, 39.86, 53.44, 9.0
2019-02-14 02:48:16,865 : Text to Image: 12.712, 33.544, 46.616, 12.0
2019-02-14 02:48:58,401 : samples : 128000
2019-02-14 02:49:08,710 : Image to text: 16.28, 39.44, 53.1, 9.0
2019-02-14 02:49:16,118 : Text to Image: 12.6, 32.976, 46.024, 13.0
2019-02-14 02:49:57,930 : samples : 192000
2019-02-14 02:50:08,218 : Image to text: 16.12, 39.28, 52.82, 9.0
2019-02-14 02:50:15,625 : Text to Image: 12.952, 33.692, 46.404, 12.0
2019-02-14 02:50:57,310 : samples : 256000
2019-02-14 02:51:07,584 : Image to text: 16.32, 40.12, 53.58, 9.0
2019-02-14 02:51:14,983 : Text to Image: 12.7, 33.708, 46.74, 12.0
2019-02-14 02:51:56,799 : samples : 320000
2019-02-14 02:52:07,041 : Image to text: 16.06, 39.58, 53.56, 9.0
2019-02-14 02:52:14,437 : Text to Image: 12.384, 33.244, 46.348, 12.0
2019-02-14 02:52:55,282 : samples : 384000
2019-02-14 02:53:05,518 : Image to text: 15.78, 39.46, 53.58, 9.0
2019-02-14 02:53:12,907 : Text to Image: 12.576, 33.192, 46.18, 13.0
2019-02-14 02:53:52,791 : samples : 448000
2019-02-14 02:54:02,821 : Image to text: 16.5, 39.62, 53.54, 9.0
2019-02-14 02:54:10,077 : Text to Image: 12.464, 33.364, 46.592, 12.0
2019-02-14 02:54:50,425 : samples : 512000
2019-02-14 02:55:00,474 : Image to text: 17.08, 39.72, 53.68, 9.0
2019-02-14 02:55:07,728 : Text to Image: 13.144, 33.972, 46.944, 12.0
2019-02-14 02:55:45,734 : Epoch 16 finished
2019-02-14 02:55:46,175 : Image to text: 37.3, 68.8, 82.9, 2.0
2019-02-14 02:55:46,505 : Text to Image: 30.26, 65.42, 80.36, 3.0
2019-02-14 02:55:46,936 : Image to text: 36.0, 69.0, 82.0, 2.0
2019-02-14 02:55:47,267 : Text to Image: 29.38, 63.0, 79.9, 3.0
2019-02-14 02:55:47,697 : Image to text: 36.2, 70.8, 84.3, 2.0
2019-02-14 02:55:48,027 : Text to Image: 29.84, 65.34, 79.98, 3.0
2019-02-14 02:55:48,470 : Image to text: 36.1, 70.5, 83.1, 2.0
2019-02-14 02:55:48,800 : Text to Image: 28.92, 65.18, 80.66, 3.0
2019-02-14 02:55:49,229 : Image to text: 38.7, 69.4, 82.5, 2.0
2019-02-14 02:55:49,560 : Text to Image: 29.58, 63.64, 79.42, 3.0
2019-02-14 02:55:49,560 : Dev mean Text to Image: 29.596, 64.51599999999999, 80.06400000000001, 3.0
2019-02-14 02:55:49,560 : Dev mean Image to text: 36.86, 69.7, 82.96000000000001, 2.0
2019-02-14 02:55:49,560 : start epoch
2019-02-14 02:56:30,770 : samples : 64000
2019-02-14 02:56:41,091 : Image to text: 16.26, 39.82, 53.96, 9.0
2019-02-14 02:56:48,512 : Text to Image: 12.924, 34.02, 46.952, 12.0
2019-02-14 02:57:29,196 : samples : 128000
2019-02-14 02:57:39,450 : Image to text: 16.34, 39.86, 54.24, 9.0
2019-02-14 02:57:46,837 : Text to Image: 12.612, 33.516, 46.524, 12.0
2019-02-14 02:58:27,184 : samples : 192000
2019-02-14 02:58:37,426 : Image to text: 16.92, 40.3, 53.8, 9.0
2019-02-14 02:58:44,791 : Text to Image: 13.004, 33.676, 46.64, 12.0
2019-02-14 02:59:28,623 : samples : 256000
2019-02-14 02:59:38,889 : Image to text: 16.24, 39.76, 53.9, 9.0
2019-02-14 02:59:46,266 : Text to Image: 12.516, 33.236, 46.34, 12.0
2019-02-14 03:00:27,144 : samples : 320000
2019-02-14 03:00:37,390 : Image to text: 16.16, 40.1, 54.32, 9.0
2019-02-14 03:00:44,779 : Text to Image: 12.644, 33.264, 46.496, 12.0
2019-02-14 03:01:29,619 : samples : 384000
2019-02-14 03:01:39,895 : Image to text: 16.72, 39.5, 53.24, 9.0
2019-02-14 03:01:47,271 : Text to Image: 12.776, 33.488, 46.576, 12.0
2019-02-14 03:02:27,610 : samples : 448000
2019-02-14 03:02:37,670 : Image to text: 16.24, 39.52, 53.6, 9.0
2019-02-14 03:02:44,940 : Text to Image: 12.916, 33.804, 47.08, 12.0
2019-02-14 03:03:25,065 : samples : 512000
2019-02-14 03:03:35,123 : Image to text: 16.52, 39.4, 53.2, 9.0
2019-02-14 03:03:42,392 : Text to Image: 13.004, 33.708, 46.812, 12.0
2019-02-14 03:04:16,451 : Epoch 17 finished
2019-02-14 03:04:16,870 : Image to text: 37.6, 70.9, 83.2, 2.0
2019-02-14 03:04:17,194 : Text to Image: 29.2, 64.46, 80.42, 3.0
2019-02-14 03:04:17,625 : Image to text: 36.5, 69.9, 82.9, 3.0
2019-02-14 03:04:17,950 : Text to Image: 29.3, 63.62, 79.7, 3.0
2019-02-14 03:04:18,376 : Image to text: 36.0, 69.8, 84.0, 2.0
2019-02-14 03:04:18,705 : Text to Image: 29.7, 64.94, 80.34, 3.0
2019-02-14 03:04:19,134 : Image to text: 37.7, 71.4, 83.7, 2.0
2019-02-14 03:04:19,469 : Text to Image: 29.58, 65.24, 80.26, 3.0
2019-02-14 03:04:19,901 : Image to text: 35.2, 70.1, 82.5, 3.0
2019-02-14 03:04:20,229 : Text to Image: 29.02, 63.8, 79.34, 3.0
2019-02-14 03:04:20,229 : Dev mean Text to Image: 29.36, 64.412, 80.012, 3.0
2019-02-14 03:04:20,229 : Dev mean Image to text: 36.6, 70.42, 83.25999999999999, 2.4
2019-02-14 03:04:20,230 : start epoch
2019-02-14 03:05:00,546 : samples : 64000
2019-02-14 03:05:10,777 : Image to text: 16.46, 39.78, 54.86, 9.0
2019-02-14 03:05:18,161 : Text to Image: 13.02, 34.14, 46.94, 12.0
2019-02-14 03:05:58,495 : samples : 128000
2019-02-14 03:06:08,728 : Image to text: 16.56, 39.46, 54.24, 9.0
2019-02-14 03:06:16,111 : Text to Image: 12.8, 33.876, 46.76, 12.0
2019-02-14 03:06:56,384 : samples : 192000
2019-02-14 03:07:06,629 : Image to text: 16.44, 39.5, 52.96, 9.0
2019-02-14 03:07:14,011 : Text to Image: 12.916, 33.816, 46.856, 12.0
2019-02-14 03:07:54,450 : samples : 256000
2019-02-14 03:08:04,715 : Image to text: 15.92, 39.78, 53.38, 9.0
2019-02-14 03:08:12,116 : Text to Image: 12.912, 34.184, 46.872, 12.0
2019-02-14 03:08:52,457 : samples : 320000
2019-02-14 03:09:02,740 : Image to text: 16.32, 38.94, 53.66, 9.0
2019-02-14 03:09:10,153 : Text to Image: 12.78, 33.66, 46.856, 12.0
2019-02-14 03:09:50,489 : samples : 384000
2019-02-14 03:10:00,757 : Image to text: 16.54, 40.3, 54.0, 9.0
2019-02-14 03:10:08,145 : Text to Image: 12.896, 33.776, 46.812, 12.0
2019-02-14 03:10:48,425 : samples : 448000
2019-02-14 03:10:58,492 : Image to text: 16.46, 40.14, 53.96, 9.0
2019-02-14 03:11:05,791 : Text to Image: 13.136, 34.084, 47.124, 12.0
2019-02-14 03:11:45,768 : samples : 512000
2019-02-14 03:11:55,844 : Image to text: 16.38, 39.72, 53.34, 9.0
2019-02-14 03:12:03,149 : Text to Image: 12.62, 33.388, 46.416, 12.0
2019-02-14 03:12:37,391 : Epoch 18 finished
2019-02-14 03:12:37,821 : Image to text: 38.0, 69.1, 82.8, 2.0
2019-02-14 03:12:38,145 : Text to Image: 30.12, 65.36, 80.5, 3.0
2019-02-14 03:12:38,571 : Image to text: 36.3, 69.4, 82.8, 3.0
2019-02-14 03:12:38,899 : Text to Image: 28.9, 63.02, 79.44, 3.0
2019-02-14 03:12:39,330 : Image to text: 36.6, 68.4, 83.9, 2.0
2019-02-14 03:12:39,668 : Text to Image: 29.82, 65.28, 79.92, 3.0
2019-02-14 03:12:40,107 : Image to text: 36.7, 69.2, 82.5, 2.0
2019-02-14 03:12:40,439 : Text to Image: 29.28, 64.86, 80.06, 3.0
2019-02-14 03:12:40,872 : Image to text: 35.5, 70.9, 82.9, 2.0
2019-02-14 03:12:41,208 : Text to Image: 29.1, 64.22, 79.2, 3.0
2019-02-14 03:12:41,208 : Dev mean Text to Image: 29.444000000000003, 64.548, 79.824, 3.0
2019-02-14 03:12:41,208 : Dev mean Image to text: 36.62, 69.4, 82.98, 2.1999999999999997
2019-02-14 03:12:41,208 : start epoch
2019-02-14 03:13:21,489 : samples : 64000
2019-02-14 03:13:31,750 : Image to text: 16.72, 40.14, 53.7, 9.0
2019-02-14 03:13:39,132 : Text to Image: 12.808, 33.712, 46.768, 12.0
2019-02-14 03:14:20,321 : samples : 128000
2019-02-14 03:14:30,586 : Image to text: 16.52, 40.16, 54.06, 9.0
2019-02-14 03:14:37,976 : Text to Image: 12.848, 33.616, 46.528, 12.0
2019-02-14 03:15:18,531 : samples : 192000
2019-02-14 03:15:28,802 : Image to text: 16.9, 40.04, 54.18, 9.0
2019-02-14 03:15:36,232 : Text to Image: 13.144, 33.988, 47.108, 12.0
2019-02-14 03:16:17,257 : samples : 256000
2019-02-14 03:16:27,587 : Image to text: 16.68, 40.4, 53.72, 9.0
2019-02-14 03:16:34,990 : Text to Image: 12.856, 33.816, 46.776, 12.0
2019-02-14 03:17:16,479 : samples : 320000
2019-02-14 03:17:26,748 : Image to text: 16.52, 40.14, 53.84, 9.0
2019-02-14 03:17:34,156 : Text to Image: 13.132, 34.188, 47.264, 12.0
2019-02-14 03:18:14,673 : samples : 384000
2019-02-14 03:18:24,907 : Image to text: 16.48, 39.48, 53.74, 9.0
2019-02-14 03:18:32,282 : Text to Image: 12.812, 33.632, 46.572, 12.0
2019-02-14 03:19:14,073 : samples : 448000
2019-02-14 03:19:24,203 : Image to text: 16.08, 40.18, 53.64, 9.0
2019-02-14 03:19:31,535 : Text to Image: 12.724, 33.588, 46.712, 12.0
2019-02-14 03:20:12,774 : samples : 512000
2019-02-14 03:20:22,907 : Image to text: 16.04, 40.38, 54.38, 9.0
2019-02-14 03:20:30,240 : Text to Image: 12.928, 33.984, 46.808, 12.0
2019-02-14 03:21:07,061 : Epoch 19 finished
2019-02-14 03:21:07,496 : Image to text: 36.8, 72.8, 83.5, 2.0
2019-02-14 03:21:07,824 : Text to Image: 30.18, 65.72, 80.88, 3.0
2019-02-14 03:21:08,251 : Image to text: 36.4, 68.8, 83.8, 2.0
2019-02-14 03:21:08,578 : Text to Image: 28.96, 63.88, 79.74, 3.0
2019-02-14 03:21:09,007 : Image to text: 36.3, 71.3, 83.4, 2.0
2019-02-14 03:21:09,339 : Text to Image: 30.18, 65.62, 80.46, 3.0
2019-02-14 03:21:09,791 : Image to text: 36.8, 71.0, 84.4, 2.0
2019-02-14 03:21:10,121 : Text to Image: 29.54, 65.48, 81.2, 3.0
2019-02-14 03:21:10,553 : Image to text: 37.5, 70.5, 82.7, 2.0
2019-02-14 03:21:10,883 : Text to Image: 29.32, 64.1, 79.3, 3.0
2019-02-14 03:21:10,883 : Dev mean Text to Image: 29.636, 64.96, 80.31599999999999, 3.0
2019-02-14 03:21:10,883 : Dev mean Image to text: 36.76, 70.88, 83.56, 2.0
2019-02-14 03:21:10,883 : start epoch
2019-02-14 03:21:52,275 : samples : 64000
2019-02-14 03:22:02,601 : Image to text: 16.52, 39.92, 53.72, 9.0
2019-02-14 03:22:10,055 : Text to Image: 12.8, 33.688, 46.78, 12.0
2019-02-14 03:22:51,975 : samples : 128000
2019-02-14 03:23:02,350 : Image to text: 16.94, 39.86, 53.66, 9.0
2019-02-14 03:23:09,785 : Text to Image: 12.688, 33.472, 46.228, 12.0
2019-02-14 03:23:51,816 : samples : 192000
2019-02-14 03:24:02,194 : Image to text: 16.38, 40.44, 54.28, 9.0
2019-02-14 03:24:09,642 : Text to Image: 12.956, 33.904, 47.024, 12.0
2019-02-14 03:24:51,410 : samples : 256000
2019-02-14 03:25:01,783 : Image to text: 16.78, 40.0, 54.1, 9.0
2019-02-14 03:25:09,221 : Text to Image: 13.06, 33.508, 46.788, 12.0
2019-02-14 03:25:51,054 : samples : 320000
2019-02-14 03:26:01,339 : Image to text: 16.58, 40.12, 54.24, 9.0
2019-02-14 03:26:08,739 : Text to Image: 13.024, 33.992, 46.844, 12.0
2019-02-14 03:26:50,510 : samples : 384000
2019-02-14 03:27:00,825 : Image to text: 17.38, 41.04, 54.58, 8.0
2019-02-14 03:27:08,237 : Text to Image: 13.116, 34.16, 47.04, 12.0
2019-02-14 03:27:51,093 : samples : 448000
2019-02-14 03:28:01,286 : Image to text: 16.34, 40.04, 53.48, 9.0
2019-02-14 03:28:08,784 : Text to Image: 13.028, 34.008, 47.108, 12.0
2019-02-14 03:28:53,904 : samples : 512000
2019-02-14 03:29:04,078 : Image to text: 17.02, 40.36, 55.38, 8.0
2019-02-14 03:29:11,549 : Text to Image: 13.036, 34.144, 47.064, 12.0
2019-02-14 03:29:47,931 : Epoch 20 finished
2019-02-14 03:29:48,357 : Image to text: 38.0, 69.8, 82.7, 2.0
2019-02-14 03:29:48,682 : Text to Image: 30.58, 66.12, 80.8, 3.0
2019-02-14 03:29:49,102 : Image to text: 37.8, 71.5, 82.8, 2.0
2019-02-14 03:29:49,429 : Text to Image: 29.22, 63.92, 79.76, 3.0
2019-02-14 03:29:49,858 : Image to text: 35.1, 70.4, 83.9, 2.0
2019-02-14 03:29:50,188 : Text to Image: 29.82, 65.92, 80.8, 3.0
2019-02-14 03:29:50,627 : Image to text: 36.6, 70.7, 84.1, 2.0
2019-02-14 03:29:50,950 : Text to Image: 30.28, 65.62, 80.86, 3.0
2019-02-14 03:29:51,391 : Image to text: 36.5, 70.3, 82.3, 2.0
2019-02-14 03:29:51,721 : Text to Image: 28.88, 64.62, 79.66, 3.0
2019-02-14 03:29:51,722 : Dev mean Text to Image: 29.756, 65.24000000000001, 80.376, 3.0
2019-02-14 03:29:51,722 : Dev mean Image to text: 36.8, 70.54, 83.16, 2.0
2019-02-14 03:29:55,536 : 
Test scores | Image to text:             36.760000000000005, 70.86, 83.22, 2.6
2019-02-14 03:29:55,536 : Test scores | Text to image:             29.088, 64.41199999999999, 79.98, 3.0

2019-02-14 03:29:55,629 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 03:29:55,839 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 03:29:56,512 : loading BERT model bert-base-uncased
2019-02-14 03:29:56,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:29:56,547 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:29:56,547 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphnepj8sz
2019-02-14 03:29:58,992 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:30:00,393 : Computing embeddings for train/dev/test
2019-02-14 03:31:36,748 : Computed embeddings
2019-02-14 03:31:36,748 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:32:08,618 : [('reg:1e-05', 94.48), ('reg:0.0001', 92.61), ('reg:0.001', 88.45), ('reg:0.01', 79.35)]
2019-02-14 03:32:08,619 : Validation : best param found is reg = 1e-05 with score             94.48
2019-02-14 03:32:08,619 : Evaluating...
2019-02-14 03:32:17,573 : 
Dev acc : 94.5 Test acc : 94.3 for LENGTH classification

2019-02-14 03:32:17,574 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 03:32:17,908 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 03:32:17,952 : loading BERT model bert-base-uncased
2019-02-14 03:32:17,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:32:17,982 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:32:17,982 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbmp93pel
2019-02-14 03:32:20,411 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:32:21,793 : Computing embeddings for train/dev/test
2019-02-14 03:33:51,021 : Computed embeddings
2019-02-14 03:33:51,021 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:34:25,939 : [('reg:1e-05', 88.15), ('reg:0.0001', 60.7), ('reg:0.001', 5.52), ('reg:0.01', 0.98)]
2019-02-14 03:34:25,939 : Validation : best param found is reg = 1e-05 with score             88.15
2019-02-14 03:34:25,939 : Evaluating...
2019-02-14 03:34:36,725 : 
Dev acc : 88.2 Test acc : 88.2 for WORDCONTENT classification

2019-02-14 03:34:36,727 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 03:34:37,104 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 03:34:37,172 : loading BERT model bert-base-uncased
2019-02-14 03:34:37,172 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:34:37,200 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:34:37,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7c3f418f
2019-02-14 03:34:39,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:34:41,043 : Computing embeddings for train/dev/test
2019-02-14 03:36:04,970 : Computed embeddings
2019-02-14 03:36:04,971 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:36:33,373 : [('reg:1e-05', 35.34), ('reg:0.0001', 34.98), ('reg:0.001', 32.85), ('reg:0.01', 27.64)]
2019-02-14 03:36:33,373 : Validation : best param found is reg = 1e-05 with score             35.34
2019-02-14 03:36:33,373 : Evaluating...
2019-02-14 03:36:40,664 : 
Dev acc : 35.3 Test acc : 35.1 for DEPTH classification

2019-02-14 03:36:40,665 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 03:36:41,078 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 03:36:41,144 : loading BERT model bert-base-uncased
2019-02-14 03:36:41,144 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:36:41,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:36:41,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpip5ec_qo
2019-02-14 03:36:43,648 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:36:45,064 : Computing embeddings for train/dev/test
2019-02-14 03:38:04,479 : Computed embeddings
2019-02-14 03:38:04,480 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:38:35,372 : [('reg:1e-05', 69.94), ('reg:0.0001', 66.75), ('reg:0.001', 57.05), ('reg:0.01', 44.3)]
2019-02-14 03:38:35,373 : Validation : best param found is reg = 1e-05 with score             69.94
2019-02-14 03:38:35,373 : Evaluating...
2019-02-14 03:38:44,049 : 
Dev acc : 69.9 Test acc : 70.0 for TOPCONSTITUENTS classification

2019-02-14 03:38:44,050 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 03:38:44,446 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 03:38:44,517 : loading BERT model bert-base-uncased
2019-02-14 03:38:44,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:38:44,553 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:38:44,553 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4oyrxoov
2019-02-14 03:38:47,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:38:48,453 : Computing embeddings for train/dev/test
2019-02-14 03:40:13,956 : Computed embeddings
2019-02-14 03:40:13,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:40:54,190 : [('reg:1e-05', 69.84), ('reg:0.0001', 69.85), ('reg:0.001', 68.85), ('reg:0.01', 64.67)]
2019-02-14 03:40:54,191 : Validation : best param found is reg = 0.0001 with score             69.85
2019-02-14 03:40:54,191 : Evaluating...
2019-02-14 03:41:06,578 : 
Dev acc : 69.8 Test acc : 69.5 for BIGRAMSHIFT classification

2019-02-14 03:41:06,580 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 03:41:07,001 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 03:41:07,071 : loading BERT model bert-base-uncased
2019-02-14 03:41:07,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:41:07,202 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:41:07,202 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkjbv23ui
2019-02-14 03:41:09,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:41:11,079 : Computing embeddings for train/dev/test
2019-02-14 03:42:34,982 : Computed embeddings
2019-02-14 03:42:34,982 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:42:56,915 : [('reg:1e-05', 87.51), ('reg:0.0001', 87.57), ('reg:0.001', 87.76), ('reg:0.01', 87.67)]
2019-02-14 03:42:56,916 : Validation : best param found is reg = 0.001 with score             87.76
2019-02-14 03:42:56,916 : Evaluating...
2019-02-14 03:43:03,187 : 
Dev acc : 87.8 Test acc : 87.1 for TENSE classification

2019-02-14 03:43:03,188 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 03:43:03,620 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 03:43:03,688 : loading BERT model bert-base-uncased
2019-02-14 03:43:03,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:43:03,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:43:03,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2hum0yyq
2019-02-14 03:43:06,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:43:07,688 : Computing embeddings for train/dev/test
2019-02-14 03:44:37,182 : Computed embeddings
2019-02-14 03:44:37,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:45:05,807 : [('reg:1e-05', 81.55), ('reg:0.0001', 81.49), ('reg:0.001', 81.22), ('reg:0.01', 79.07)]
2019-02-14 03:45:05,807 : Validation : best param found is reg = 1e-05 with score             81.55
2019-02-14 03:45:05,807 : Evaluating...
2019-02-14 03:45:13,202 : 
Dev acc : 81.5 Test acc : 80.2 for SUBJNUMBER classification

2019-02-14 03:45:13,203 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 03:45:13,834 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 03:45:13,907 : loading BERT model bert-base-uncased
2019-02-14 03:45:13,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:45:13,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:45:13,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsq1aqtvz
2019-02-14 03:45:16,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:45:17,819 : Computing embeddings for train/dev/test
2019-02-14 03:46:44,742 : Computed embeddings
2019-02-14 03:46:44,742 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:47:12,274 : [('reg:1e-05', 81.13), ('reg:0.0001', 81.13), ('reg:0.001', 81.13), ('reg:0.01', 80.04)]
2019-02-14 03:47:12,274 : Validation : best param found is reg = 1e-05 with score             81.13
2019-02-14 03:47:12,274 : Evaluating...
2019-02-14 03:47:17,519 : 
Dev acc : 81.1 Test acc : 82.1 for OBJNUMBER classification

2019-02-14 03:47:17,520 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 03:47:17,956 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 03:47:18,030 : loading BERT model bert-base-uncased
2019-02-14 03:47:18,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:47:18,061 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:47:18,061 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0bj9d04m
2019-02-14 03:47:20,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:47:21,894 : Computing embeddings for train/dev/test
2019-02-14 03:49:01,919 : Computed embeddings
2019-02-14 03:49:01,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:49:35,477 : [('reg:1e-05', 53.62), ('reg:0.0001', 53.7), ('reg:0.001', 53.43), ('reg:0.01', 52.53)]
2019-02-14 03:49:35,477 : Validation : best param found is reg = 0.0001 with score             53.7
2019-02-14 03:49:35,477 : Evaluating...
2019-02-14 03:49:44,664 : 
Dev acc : 53.7 Test acc : 53.1 for ODDMANOUT classification

2019-02-14 03:49:44,665 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 03:49:45,095 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 03:49:45,173 : loading BERT model bert-base-uncased
2019-02-14 03:49:45,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:49:45,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:49:45,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6x982_ne
2019-02-14 03:49:47,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:49:49,080 : Computing embeddings for train/dev/test
2019-02-14 03:51:27,339 : Computed embeddings
2019-02-14 03:51:27,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:51:59,073 : [('reg:1e-05', 56.68), ('reg:0.0001', 56.7), ('reg:0.001', 55.35), ('reg:0.01', 52.19)]
2019-02-14 03:51:59,073 : Validation : best param found is reg = 0.0001 with score             56.7
2019-02-14 03:51:59,073 : Evaluating...
2019-02-14 03:52:09,663 : 
Dev acc : 56.7 Test acc : 56.9 for COORDINATIONINVERSION classification

2019-02-14 03:52:09,666 : total results: {'STS12': {'MSRpar': {'pearson': (0.4182776298763544, 4.009544040927623e-33), 'spearman': SpearmanrResult(correlation=0.4531390767686579, pvalue=2.987053443266199e-39), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6547588145686539, 5.159713227403249e-93), 'spearman': SpearmanrResult(correlation=0.6577489276214195, pvalue=3.899457927592248e-94), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5035465825045548, 7.021171962595966e-31), 'spearman': SpearmanrResult(correlation=0.6005500630230636, pvalue=2.5188878312361366e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.7034486168293742, 5.0057087724969825e-113), 'spearman': SpearmanrResult(correlation=0.6899093802430127, pvalue=4.603434101188696e-107), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5523038388810086, 3.0671000827197627e-33), 'spearman': SpearmanrResult(correlation=0.5146082245288787, pvalue=2.326907891944589e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5664670965319892, 'wmean': 0.5739578214410875}, 'spearman': {'mean': 0.5831911344370064, 'wmean': 0.5893111965860446}}}, 'STS13': {'FNWN': {'pearson': (0.4663138574665031, 1.3561283720130955e-11), 'spearman': SpearmanrResult(correlation=0.4824026214265076, pvalue=2.0813734335483648e-12), 'nsamples': 189}, 'headlines': {'pearson': (0.6742946517275877, 1.396170085842768e-100), 'spearman': SpearmanrResult(correlation=0.6529476788926748, pvalue=2.43113227188067e-92), 'nsamples': 750}, 'OnWN': {'pearson': (0.5408038648717477, 6.266680797501094e-44), 'spearman': SpearmanrResult(correlation=0.5666622621140254, pvalue=5.777279809397946e-49), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5604707913552794, 'wmean': 0.598163517366607}, 'spearman': {'mean': 0.5673375208110693, 'wmean': 0.5991882557767229}}}, 'STS14': {'deft-forum': {'pearson': (0.358526296480026, 4.2710104638203365e-15), 'spearman': SpearmanrResult(correlation=0.37597647117925004, pvalue=1.4760552109254407e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7371679501825404, 1.1592913068246085e-52), 'spearman': SpearmanrResult(correlation=0.702178189706483, pvalue=7.174817238205543e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.6351805589438418, 5.647216658005599e-86), 'spearman': SpearmanrResult(correlation=0.591457299001131, pvalue=5.83426199260668e-72), 'nsamples': 750}, 'images': {'pearson': (0.6662551223699058, 2.132381356039771e-97), 'spearman': SpearmanrResult(correlation=0.6471559783303287, pvalue=3.2194125472797167e-90), 'nsamples': 750}, 'OnWN': {'pearson': (0.6771000863892735, 1.0232324755047387e-101), 'spearman': SpearmanrResult(correlation=0.7180305121248204, pvalue=7.678853411345571e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6737914231024885, 2.224313048234764e-100), 'spearman': SpearmanrResult(correlation=0.6446073778947741, pvalue=2.672748896762475e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6246702395780127, 'wmean': 0.6324620297533082}, 'spearman': {'mean': 0.6132343047061312, 'wmean': 0.6215416651882394}}}, 'STS15': {'answers-forums': {'pearson': (0.5400388365746589, 8.969218644482813e-30), 'spearman': SpearmanrResult(correlation=0.534836115433142, pvalue=3.924743799460956e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.7131401618711889, 1.6524978252075517e-117), 'spearman': SpearmanrResult(correlation=0.7184863496963562, pvalue=4.627201647010825e-120), 'nsamples': 750}, 'belief': {'pearson': (0.651043782717649, 1.378419935592714e-46), 'spearman': SpearmanrResult(correlation=0.6738990913139672, pvalue=5.642580861866252e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.7007304599115733, 8.389754352650724e-112), 'spearman': SpearmanrResult(correlation=0.6892560463809452, pvalue=8.760749636141584e-107), 'nsamples': 750}, 'images': {'pearson': (0.7447351001105315, 1.6363747771828698e-133), 'spearman': SpearmanrResult(correlation=0.754715774865417, pvalue=4.533122024172415e-139), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6699376682371203, 'wmean': 0.6885367578848619}, 'spearman': {'mean': 0.6742386755379656, 'wmean': 0.6917064435790683}}}, 'STS16': {'answer-answer': {'pearson': (0.49733128732383236, 2.8227680262699535e-17), 'spearman': SpearmanrResult(correlation=0.5230606102238952, pvalue=3.065891912514043e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.7055317794135227, 8.238092174042106e-39), 'spearman': SpearmanrResult(correlation=0.7050411877615843, pvalue=9.771859639016578e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7416860171521443, 2.0238224937715924e-41), 'spearman': SpearmanrResult(correlation=0.7447615618247146, pvalue=6.291332960622266e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.8124814584149181, 1.2254937195950086e-58), 'spearman': SpearmanrResult(correlation=0.8304927467333736, pvalue=1.958677586370302e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.46938291616388883, 7.553754686308985e-13), 'spearman': SpearmanrResult(correlation=0.4718380924718441, pvalue=5.52797247105356e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6452826916936611, 'wmean': 0.6483422001102079}, 'spearman': {'mean': 0.6550388398030823, 'wmean': 0.6584841496616752}}}, 'MR': {'devacc': 75.11, 'acc': 74.64, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.1, 'acc': 78.2, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.63, 'acc': 87.92, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.26, 'acc': 92.78, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.01, 'acc': 79.79, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.78, 'acc': 40.05, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.52, 'acc': 90.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.38, 'acc': 73.8, 'f1': 80.57, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.8, 'acc': 79.87, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8189028295858121, 'pearson': 0.8291880187610355, 'spearman': 0.7553988012935134, 'mse': 0.31995341456434395, 'yhat': array([3.73129528, 4.39443017, 1.30646466, ..., 3.17756424, 4.48952683,        4.19583398]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7464978427102179, 'pearson': 0.702112468179167, 'spearman': 0.6994958784546036, 'mse': 1.3678795004455795, 'yhat': array([1.43416875, 1.8825786 , 1.99594349, ..., 3.85884025, 3.90270777,        3.49578379]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.58, 'acc': 64.58, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 366.11199999999997, 'acc': [(36.760000000000005, 70.86, 83.22, 2.6), (29.088, 64.41199999999999, 79.98, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 94.48, 'acc': 94.26, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 88.15, 'acc': 88.25, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.34, 'acc': 35.13, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.94, 'acc': 70.03, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 69.85, 'acc': 69.46, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.76, 'acc': 87.14, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.55, 'acc': 80.24, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.13, 'acc': 82.12, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.7, 'acc': 53.13, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.7, 'acc': 56.88, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 03:52:09,666 : STS12 p=0.5740, STS12 s=0.5893, STS13 p=0.5982, STS13 s=0.5992, STS14 p=0.6325, STS14 s=0.6215, STS15 p=0.6885, STS15 s=0.6917, STS 16 p=0.6483, STS16 s=0.6585, STS B p=0.7021, STS B s=0.6995, STS B m=1.3679, SICK-R p=0.8292, SICK-R s=0.7554, SICK-P m=0.3200
2019-02-14 03:52:09,666 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 03:52:09,666 : 0.5740,0.5893,0.5982,0.5992,0.6325,0.6215,0.6885,0.6917,0.6483,0.6585,0.7021,0.6995,1.3679,0.8292,0.7554,0.3200
2019-02-14 03:52:09,666 : MR=74.64, CR=78.20, SUBJ=92.78, MPQA=87.92, SST-B=79.79, SST-F=40.05, TREC=90.00, SICK-E=79.87, SNLI=64.58, MRPC=73.80, MRPC f=80.57
2019-02-14 03:52:09,666 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 03:52:09,666 : 74.64,78.20,92.78,87.92,79.79,40.05,90.00,79.87,64.58,73.80,80.57
2019-02-14 03:52:09,666 : COCO r1i2t=36.76, COCO r5i2t=70.86, COCO r10i2t=83.22, COCO medr_i2t=2.60, COCO r1t2i=29.09, COCO r5t2i=64.41, COCO r10t2i=79.98, COCO medr_t2i=3.00
2019-02-14 03:52:09,666 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 03:52:09,666 : 36.76,70.86,83.22,2.60,29.09,64.41,79.98,3.00
2019-02-14 03:52:09,666 : SentLen=94.26, WC=88.25, TreeDepth=35.13, TopConst=70.03, BShift=69.46, Tense=87.14, SubjNum=80.24, ObjNum=82.12, SOMO=53.13, CoordInv=56.88, average=71.66
2019-02-14 03:52:09,666 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 03:52:09,666 : 94.26,88.25,35.13,70.03,69.46,87.14,80.24,82.12,53.13,56.88,71.66
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,666 : layer 3
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,666 : ********************************************************************************
2019-02-14 03:52:09,761 : ***** Transfer task : STS12 *****


2019-02-14 03:52:09,773 : loading BERT model bert-base-uncased
2019-02-14 03:52:09,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:09,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:09,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf55k423j
2019-02-14 03:52:12,249 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:15,490 : MSRpar : pearson = 0.4122, spearman = 0.4495
2019-02-14 03:52:16,280 : MSRvid : pearson = 0.6125, spearman = 0.6165
2019-02-14 03:52:16,922 : SMTeuroparl : pearson = 0.4964, spearman = 0.5871
2019-02-14 03:52:18,131 : surprise.OnWN : pearson = 0.6903, spearman = 0.6824
2019-02-14 03:52:18,795 : surprise.SMTnews : pearson = 0.5818, spearman = 0.5174
2019-02-14 03:52:18,796 : ALL (weighted average) : Pearson = 0.5619,             Spearman = 0.5750
2019-02-14 03:52:18,796 : ALL (average) : Pearson = 0.5587,             Spearman = 0.5706

2019-02-14 03:52:18,796 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 03:52:18,806 : loading BERT model bert-base-uncased
2019-02-14 03:52:18,806 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:18,825 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:18,825 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3da0gywu
2019-02-14 03:52:21,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:23,316 : FNWN : pearson = 0.4317, spearman = 0.4437
2019-02-14 03:52:24,204 : headlines : pearson = 0.6611, spearman = 0.6387
2019-02-14 03:52:24,890 : OnWN : pearson = 0.5134, spearman = 0.5429
2019-02-14 03:52:24,890 : ALL (weighted average) : Pearson = 0.5770,             Spearman = 0.5783
2019-02-14 03:52:24,890 : ALL (average) : Pearson = 0.5354,             Spearman = 0.5418

2019-02-14 03:52:24,890 : ***** Transfer task : STS14 *****


2019-02-14 03:52:24,907 : loading BERT model bert-base-uncased
2019-02-14 03:52:24,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:24,926 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:24,926 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp09gqnk4v
2019-02-14 03:52:27,371 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:29,426 : deft-forum : pearson = 0.3486, spearman = 0.3706
2019-02-14 03:52:30,150 : deft-news : pearson = 0.7287, spearman = 0.6948
2019-02-14 03:52:31,139 : headlines : pearson = 0.6192, spearman = 0.5769
2019-02-14 03:52:32,121 : images : pearson = 0.6202, spearman = 0.6103
2019-02-14 03:52:33,109 : OnWN : pearson = 0.6598, spearman = 0.7035
2019-02-14 03:52:34,513 : tweet-news : pearson = 0.6601, spearman = 0.6312
2019-02-14 03:52:34,513 : ALL (weighted average) : Pearson = 0.6120,             Spearman = 0.6044
2019-02-14 03:52:34,514 : ALL (average) : Pearson = 0.6061,             Spearman = 0.5979

2019-02-14 03:52:34,514 : ***** Transfer task : STS15 *****


2019-02-14 03:52:34,562 : loading BERT model bert-base-uncased
2019-02-14 03:52:34,562 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:34,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:34,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu2buxyqj
2019-02-14 03:52:37,031 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:39,409 : answers-forums : pearson = 0.5330, spearman = 0.5291
2019-02-14 03:52:40,470 : answers-students : pearson = 0.7067, spearman = 0.7100
2019-02-14 03:52:41,440 : belief : pearson = 0.6371, spearman = 0.6525
2019-02-14 03:52:42,578 : headlines : pearson = 0.6794, spearman = 0.6692
2019-02-14 03:52:43,664 : images : pearson = 0.7290, spearman = 0.7401
2019-02-14 03:52:43,664 : ALL (weighted average) : Pearson = 0.6750,             Spearman = 0.6775
2019-02-14 03:52:43,664 : ALL (average) : Pearson = 0.6570,             Spearman = 0.6601

2019-02-14 03:52:43,665 : ***** Transfer task : STS16 *****


2019-02-14 03:52:43,733 : loading BERT model bert-base-uncased
2019-02-14 03:52:43,733 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:43,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:43,752 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkvi9lbhy
2019-02-14 03:52:46,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:48,011 : answer-answer : pearson = 0.4821, spearman = 0.5125
2019-02-14 03:52:48,320 : headlines : pearson = 0.6921, spearman = 0.6954
2019-02-14 03:52:48,715 : plagiarism : pearson = 0.7368, spearman = 0.7445
2019-02-14 03:52:49,339 : postediting : pearson = 0.7986, spearman = 0.8268
2019-02-14 03:52:49,623 : question-question : pearson = 0.4169, spearman = 0.4174
2019-02-14 03:52:49,624 : ALL (weighted average) : Pearson = 0.6292,             Spearman = 0.6438
2019-02-14 03:52:49,624 : ALL (average) : Pearson = 0.6253,             Spearman = 0.6393

2019-02-14 03:52:49,624 : ***** Transfer task : MR *****


2019-02-14 03:52:49,642 : loading BERT model bert-base-uncased
2019-02-14 03:52:49,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:52:49,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:52:49,662 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvw7aj7ad
2019-02-14 03:52:52,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:52:53,604 : Generating sentence embeddings
2019-02-14 03:53:07,240 : Generated sentence embeddings
2019-02-14 03:53:07,240 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 03:53:17,684 : Best param found at split 1: l2reg = 0.001                 with score 75.67
2019-02-14 03:53:28,091 : Best param found at split 2: l2reg = 0.001                 with score 75.66
2019-02-14 03:53:38,668 : Best param found at split 3: l2reg = 0.001                 with score 75.8
2019-02-14 03:53:50,054 : Best param found at split 4: l2reg = 1e-05                 with score 75.45
2019-02-14 03:54:00,843 : Best param found at split 5: l2reg = 0.0001                 with score 75.15
2019-02-14 03:54:01,455 : Dev acc : 75.55 Test acc : 74.53

2019-02-14 03:54:01,457 : ***** Transfer task : CR *****


2019-02-14 03:54:01,464 : loading BERT model bert-base-uncased
2019-02-14 03:54:01,464 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:54:01,488 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:54:01,488 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6ckp2xrg
2019-02-14 03:54:03,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:54:05,365 : Generating sentence embeddings
2019-02-14 03:54:09,105 : Generated sentence embeddings
2019-02-14 03:54:09,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 03:54:13,592 : Best param found at split 1: l2reg = 0.0001                 with score 79.89
2019-02-14 03:54:17,967 : Best param found at split 2: l2reg = 1e-05                 with score 80.23
2019-02-14 03:54:22,477 : Best param found at split 3: l2reg = 0.0001                 with score 80.76
2019-02-14 03:54:27,150 : Best param found at split 4: l2reg = 0.0001                 with score 80.67
2019-02-14 03:54:31,954 : Best param found at split 5: l2reg = 0.01                 with score 80.27
2019-02-14 03:54:32,177 : Dev acc : 80.36 Test acc : 78.57

2019-02-14 03:54:32,178 : ***** Transfer task : MPQA *****


2019-02-14 03:54:32,183 : loading BERT model bert-base-uncased
2019-02-14 03:54:32,184 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:54:32,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:54:32,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgdv1h7r9
2019-02-14 03:54:34,653 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:54:36,111 : Generating sentence embeddings
2019-02-14 03:54:39,914 : Generated sentence embeddings
2019-02-14 03:54:39,914 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 03:54:51,929 : Best param found at split 1: l2reg = 0.0001                 with score 87.08
2019-02-14 03:55:03,344 : Best param found at split 2: l2reg = 0.0001                 with score 87.36
2019-02-14 03:55:13,831 : Best param found at split 3: l2reg = 0.001                 with score 88.06
2019-02-14 03:55:25,501 : Best param found at split 4: l2reg = 1e-05                 with score 87.63
2019-02-14 03:55:38,867 : Best param found at split 5: l2reg = 0.01                 with score 87.59
2019-02-14 03:55:39,562 : Dev acc : 87.54 Test acc : 87.72

2019-02-14 03:55:39,563 : ***** Transfer task : SUBJ *****


2019-02-14 03:55:39,581 : loading BERT model bert-base-uncased
2019-02-14 03:55:39,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:55:39,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:55:39,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqkd3wk_d
2019-02-14 03:55:42,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:55:43,485 : Generating sentence embeddings
2019-02-14 03:55:56,856 : Generated sentence embeddings
2019-02-14 03:55:56,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 03:56:08,406 : Best param found at split 1: l2reg = 0.001                 with score 93.55
2019-02-14 03:56:19,464 : Best param found at split 2: l2reg = 0.001                 with score 93.84
2019-02-14 03:56:30,714 : Best param found at split 3: l2reg = 0.001                 with score 93.38
2019-02-14 03:56:42,619 : Best param found at split 4: l2reg = 0.001                 with score 93.65
2019-02-14 03:56:54,288 : Best param found at split 5: l2reg = 0.001                 with score 93.43
2019-02-14 03:56:54,947 : Dev acc : 93.57 Test acc : 93.16

2019-02-14 03:56:54,948 : ***** Transfer task : SST Binary classification *****


2019-02-14 03:56:55,081 : loading BERT model bert-base-uncased
2019-02-14 03:56:55,081 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:56:55,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:56:55,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0bjo_n9n
2019-02-14 03:56:57,551 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:56:58,985 : Computing embedding for train
2019-02-14 03:57:45,083 : Computed train embeddings
2019-02-14 03:57:45,083 : Computing embedding for dev
2019-02-14 03:57:46,060 : Computed dev embeddings
2019-02-14 03:57:46,060 : Computing embedding for test
2019-02-14 03:57:48,074 : Computed test embeddings
2019-02-14 03:57:48,074 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:58:10,687 : [('reg:1e-05', 81.08), ('reg:0.0001', 81.08), ('reg:0.001', 80.39), ('reg:0.01', 79.13)]
2019-02-14 03:58:10,687 : Validation : best param found is reg = 1e-05 with score             81.08
2019-02-14 03:58:10,687 : Evaluating...
2019-02-14 03:58:16,266 : 
Dev acc : 81.08 Test acc : 79.9 for             SST Binary classification

2019-02-14 03:58:16,266 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 03:58:16,315 : loading BERT model bert-base-uncased
2019-02-14 03:58:16,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:58:16,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:58:16,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph373ib9i
2019-02-14 03:58:18,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:58:20,165 : Computing embedding for train
2019-02-14 03:58:30,051 : Computed train embeddings
2019-02-14 03:58:30,051 : Computing embedding for dev
2019-02-14 03:58:31,274 : Computed dev embeddings
2019-02-14 03:58:31,274 : Computing embedding for test
2019-02-14 03:58:33,695 : Computed test embeddings
2019-02-14 03:58:33,696 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:58:36,091 : [('reg:1e-05', 40.87), ('reg:0.0001', 40.78), ('reg:0.001', 41.05), ('reg:0.01', 41.05)]
2019-02-14 03:58:36,091 : Validation : best param found is reg = 0.001 with score             41.05
2019-02-14 03:58:36,091 : Evaluating...
2019-02-14 03:58:36,704 : 
Dev acc : 41.05 Test acc : 41.58 for             SST Fine-Grained classification

2019-02-14 03:58:36,704 : ***** Transfer task : TREC *****


2019-02-14 03:58:36,718 : loading BERT model bert-base-uncased
2019-02-14 03:58:36,718 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:58:36,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:58:36,741 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxloz3meb
2019-02-14 03:58:39,183 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:58:44,061 : Computed train embeddings
2019-02-14 03:58:44,326 : Computed test embeddings
2019-02-14 03:58:44,326 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 03:58:51,340 : [('reg:1e-05', 79.12), ('reg:0.0001', 79.07), ('reg:0.001', 78.3), ('reg:0.01', 70.08)]
2019-02-14 03:58:51,340 : Cross-validation : best param found is reg = 1e-05             with score 79.12
2019-02-14 03:58:51,340 : Evaluating...
2019-02-14 03:58:51,781 : 
Dev acc : 79.12 Test acc : 88.4             for TREC

2019-02-14 03:58:51,782 : ***** Transfer task : MRPC *****


2019-02-14 03:58:51,805 : loading BERT model bert-base-uncased
2019-02-14 03:58:51,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:58:51,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:58:51,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbu2el5j8
2019-02-14 03:58:54,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:58:55,748 : Computing embedding for train
2019-02-14 03:59:05,521 : Computed train embeddings
2019-02-14 03:59:05,521 : Computing embedding for test
2019-02-14 03:59:09,791 : Computed test embeddings
2019-02-14 03:59:09,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 03:59:15,043 : [('reg:1e-05', 73.38), ('reg:0.0001', 73.43), ('reg:0.001', 73.33), ('reg:0.01', 73.11)]
2019-02-14 03:59:15,043 : Cross-validation : best param found is reg = 0.0001             with score 73.43
2019-02-14 03:59:15,043 : Evaluating...
2019-02-14 03:59:15,365 : Dev acc : 73.43 Test acc 71.54; Test F1 77.3 for MRPC.

2019-02-14 03:59:15,366 : ***** Transfer task : SICK-Entailment*****


2019-02-14 03:59:15,434 : loading BERT model bert-base-uncased
2019-02-14 03:59:15,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:59:15,456 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:59:15,456 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpogadez6x
2019-02-14 03:59:17,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:59:19,326 : Computing embedding for train
2019-02-14 03:59:25,084 : Computed train embeddings
2019-02-14 03:59:25,084 : Computing embedding for dev
2019-02-14 03:59:25,844 : Computed dev embeddings
2019-02-14 03:59:25,844 : Computing embedding for test
2019-02-14 03:59:31,555 : Computed test embeddings
2019-02-14 03:59:31,583 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 03:59:33,164 : [('reg:1e-05', 81.2), ('reg:0.0001', 81.0), ('reg:0.001', 81.0), ('reg:0.01', 75.2)]
2019-02-14 03:59:33,164 : Validation : best param found is reg = 1e-05 with score             81.2
2019-02-14 03:59:33,164 : Evaluating...
2019-02-14 03:59:33,589 : 
Dev acc : 81.2 Test acc : 78.89 for                        SICK entailment

2019-02-14 03:59:33,590 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 03:59:33,617 : loading BERT model bert-base-uncased
2019-02-14 03:59:33,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 03:59:33,675 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 03:59:33,675 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp58ygir0c
2019-02-14 03:59:36,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 03:59:37,537 : Computing embedding for train
2019-02-14 03:59:42,682 : Computed train embeddings
2019-02-14 03:59:42,682 : Computing embedding for dev
2019-02-14 03:59:43,369 : Computed dev embeddings
2019-02-14 03:59:43,369 : Computing embedding for test
2019-02-14 03:59:48,988 : Computed test embeddings
2019-02-14 04:00:09,253 : Dev : Pearson 0.8214254889327501
2019-02-14 04:00:09,253 : Test : Pearson 0.8254965591046618 Spearman 0.7533282709130273 MSE 0.32435933615366774                        for SICK Relatedness

2019-02-14 04:00:09,254 : 

***** Transfer task : STSBenchmark*****


2019-02-14 04:00:09,326 : loading BERT model bert-base-uncased
2019-02-14 04:00:09,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:00:09,348 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:00:09,348 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_t2ja4nd
2019-02-14 04:00:11,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:00:13,242 : Computing embedding for train
2019-02-14 04:00:22,127 : Computed train embeddings
2019-02-14 04:00:22,127 : Computing embedding for dev
2019-02-14 04:00:24,869 : Computed dev embeddings
2019-02-14 04:00:24,869 : Computing embedding for test
2019-02-14 04:00:27,100 : Computed test embeddings
2019-02-14 04:00:47,917 : Dev : Pearson 0.7541006643513408
2019-02-14 04:00:47,917 : Test : Pearson 0.6870015019307263 Spearman 0.6822661773333344 MSE 1.3792724835892651                        for SICK Relatedness

2019-02-14 04:00:47,917 : ***** Transfer task : SNLI Entailment*****


2019-02-14 04:00:52,808 : loading BERT model bert-base-uncased
2019-02-14 04:00:52,808 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:00:52,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:00:52,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphtyz6lr0
2019-02-14 04:00:55,411 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:00:57,039 : PROGRESS (encoding): 0.00%
2019-02-14 04:02:16,298 : PROGRESS (encoding): 14.56%
2019-02-14 04:03:45,247 : PROGRESS (encoding): 29.12%
2019-02-14 04:05:14,977 : PROGRESS (encoding): 43.69%
2019-02-14 04:06:53,037 : PROGRESS (encoding): 58.25%
2019-02-14 04:08:39,320 : PROGRESS (encoding): 72.81%
2019-02-14 04:10:25,814 : PROGRESS (encoding): 87.37%
2019-02-14 04:12:17,784 : PROGRESS (encoding): 0.00%
2019-02-14 04:12:31,353 : PROGRESS (encoding): 0.00%
2019-02-14 04:12:44,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:13:17,609 : [('reg:1e-09', 68.16)]
2019-02-14 04:13:17,610 : Validation : best param found is reg = 1e-09 with score             68.16
2019-02-14 04:13:17,610 : Evaluating...
2019-02-14 04:13:51,153 : Dev acc : 68.16 Test acc : 68.14 for SNLI

2019-02-14 04:13:51,153 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 04:13:59,820 : loading BERT model bert-base-uncased
2019-02-14 04:13:59,820 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:13:59,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:13:59,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpegt_8cpm
2019-02-14 04:14:02,311 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:14:03,717 : Computing embedding for train
2019-02-14 04:21:48,345 : Computed train embeddings
2019-02-14 04:21:48,346 : Computing embedding for dev
2019-02-14 04:22:07,718 : Computed dev embeddings
2019-02-14 04:22:07,718 : Computing embedding for test
2019-02-14 04:22:27,910 : Computed test embeddings
2019-02-14 04:22:27,926 : prepare data
2019-02-14 04:22:27,991 : start epoch
2019-02-14 04:23:09,718 : samples : 64000
2019-02-14 04:23:22,366 : Image to text: 8.18, 23.98, 33.94, 23.0
2019-02-14 04:23:32,475 : Text to Image: 6.248, 19.972, 30.612, 26.0
2019-02-14 04:24:17,020 : samples : 128000
2019-02-14 04:24:28,059 : Image to text: 9.34, 26.12, 38.94, 18.0
2019-02-14 04:24:35,240 : Text to Image: 7.16, 22.148, 33.34, 22.0
2019-02-14 04:25:17,483 : samples : 192000
2019-02-14 04:25:30,030 : Image to text: 10.22, 28.42, 39.78, 17.0
2019-02-14 04:25:40,075 : Text to Image: 8.08, 23.444, 34.892, 21.0
2019-02-14 04:26:24,763 : samples : 256000
2019-02-14 04:26:35,194 : Image to text: 9.76, 27.82, 39.9, 17.0
2019-02-14 04:26:42,296 : Text to Image: 8.316, 24.364, 36.132, 20.0
2019-02-14 04:27:22,502 : samples : 320000
2019-02-14 04:27:34,293 : Image to text: 9.74, 27.42, 40.26, 16.0
2019-02-14 04:27:44,377 : Text to Image: 8.656, 25.024, 36.56, 19.0
2019-02-14 04:28:27,800 : samples : 384000
2019-02-14 04:28:40,440 : Image to text: 11.12, 30.66, 43.52, 15.0
2019-02-14 04:28:50,181 : Text to Image: 9.008, 26.176, 37.952, 18.0
2019-02-14 04:29:30,913 : samples : 448000
2019-02-14 04:29:43,182 : Image to text: 10.58, 30.54, 42.32, 15.0
2019-02-14 04:29:53,184 : Text to Image: 8.408, 24.848, 36.688, 19.0
2019-02-14 04:30:36,817 : samples : 512000
2019-02-14 04:30:49,380 : Image to text: 11.86, 31.52, 44.4, 14.0
2019-02-14 04:30:56,989 : Text to Image: 9.572, 26.976, 38.744, 18.0
2019-02-14 04:31:31,618 : Epoch 1 finished
2019-02-14 04:31:32,080 : Image to text: 27.8, 60.6, 76.2, 4.0
2019-02-14 04:31:32,443 : Text to Image: 22.98, 56.92, 73.74, 4.0
2019-02-14 04:31:32,893 : Image to text: 28.5, 62.6, 77.1, 3.0
2019-02-14 04:31:33,254 : Text to Image: 23.72, 55.14, 73.02, 4.0
2019-02-14 04:31:33,704 : Image to text: 27.1, 63.0, 75.3, 3.0
2019-02-14 04:31:34,067 : Text to Image: 23.72, 56.36, 73.22, 4.0
2019-02-14 04:31:34,517 : Image to text: 29.1, 62.8, 75.3, 3.0
2019-02-14 04:31:34,878 : Text to Image: 23.12, 56.42, 73.32, 4.0
2019-02-14 04:31:35,240 : Image to text: 28.6, 63.4, 76.7, 3.0
2019-02-14 04:31:35,517 : Text to Image: 22.78, 57.0, 72.84, 4.0
2019-02-14 04:31:35,517 : Dev mean Text to Image: 23.264, 56.367999999999995, 73.228, 4.0
2019-02-14 04:31:35,517 : Dev mean Image to text: 28.22, 62.48, 76.12, 3.2
2019-02-14 04:31:35,517 : start epoch
2019-02-14 04:32:18,529 : samples : 64000
2019-02-14 04:32:31,134 : Image to text: 12.06, 32.16, 45.38, 13.0
2019-02-14 04:32:41,209 : Text to Image: 9.856, 27.192, 39.196, 17.0
2019-02-14 04:33:24,026 : samples : 128000
2019-02-14 04:33:34,065 : Image to text: 11.72, 30.68, 43.06, 14.0
2019-02-14 04:33:41,293 : Text to Image: 9.604, 27.508, 39.556, 17.0
2019-02-14 04:34:23,473 : samples : 192000
2019-02-14 04:34:36,092 : Image to text: 12.46, 32.82, 45.34, 13.0
2019-02-14 04:34:46,131 : Text to Image: 9.592, 27.936, 39.788, 17.0
2019-02-14 04:35:28,994 : samples : 256000
2019-02-14 04:35:39,036 : Image to text: 13.24, 33.08, 45.46, 13.0
2019-02-14 04:35:46,247 : Text to Image: 10.212, 28.416, 40.6, 16.0
2019-02-14 04:36:28,424 : samples : 320000
2019-02-14 04:36:40,942 : Image to text: 12.6, 33.54, 46.48, 12.0
2019-02-14 04:36:50,913 : Text to Image: 10.028, 27.996, 40.436, 16.0
2019-02-14 04:37:35,201 : samples : 384000
2019-02-14 04:37:45,251 : Image to text: 12.62, 33.24, 45.82, 13.0
2019-02-14 04:37:52,495 : Text to Image: 10.056, 28.064, 40.384, 16.0
2019-02-14 04:38:34,117 : samples : 448000
2019-02-14 04:38:46,675 : Image to text: 12.82, 34.0, 47.36, 12.0
2019-02-14 04:38:56,627 : Text to Image: 10.432, 28.664, 40.86, 16.0
2019-02-14 04:39:41,200 : samples : 512000
2019-02-14 04:39:52,996 : Image to text: 12.54, 33.56, 46.18, 13.0
2019-02-14 04:40:00,171 : Text to Image: 10.416, 28.376, 40.648, 16.0
2019-02-14 04:40:35,302 : Epoch 2 finished
2019-02-14 04:40:36,255 : Image to text: 30.5, 62.8, 77.6, 3.0
2019-02-14 04:40:37,024 : Text to Image: 25.24, 59.12, 75.66, 4.0
2019-02-14 04:40:37,964 : Image to text: 29.0, 65.8, 79.9, 3.0
2019-02-14 04:40:38,735 : Text to Image: 24.8, 57.94, 74.66, 4.0
2019-02-14 04:40:39,657 : Image to text: 30.3, 64.3, 78.4, 3.0
2019-02-14 04:40:40,423 : Text to Image: 25.68, 59.08, 75.06, 4.0
2019-02-14 04:40:41,332 : Image to text: 30.5, 64.6, 79.1, 3.0
2019-02-14 04:40:42,102 : Text to Image: 24.78, 58.14, 74.52, 4.0
2019-02-14 04:40:43,005 : Image to text: 31.4, 64.8, 78.7, 3.0
2019-02-14 04:40:43,770 : Text to Image: 24.5, 57.98, 74.46, 4.0
2019-02-14 04:40:43,770 : Dev mean Text to Image: 25.0, 58.452, 74.872, 4.0
2019-02-14 04:40:43,770 : Dev mean Image to text: 30.340000000000003, 64.46, 78.74, 3.0
2019-02-14 04:40:43,771 : start epoch
2019-02-14 04:41:28,251 : samples : 64000
2019-02-14 04:41:40,857 : Image to text: 12.88, 34.26, 47.2, 12.0
2019-02-14 04:41:50,930 : Text to Image: 10.392, 28.756, 40.96, 16.0
2019-02-14 04:42:32,948 : samples : 128000
2019-02-14 04:42:43,003 : Image to text: 12.86, 33.66, 47.38, 12.0
2019-02-14 04:42:49,944 : Text to Image: 10.436, 28.488, 40.592, 16.0
2019-02-14 04:43:33,348 : samples : 192000
2019-02-14 04:43:45,993 : Image to text: 13.54, 35.02, 48.16, 12.0
2019-02-14 04:43:56,058 : Text to Image: 10.636, 29.196, 41.672, 15.0
2019-02-14 04:44:39,136 : samples : 256000
2019-02-14 04:44:49,177 : Image to text: 13.66, 35.7, 48.06, 12.0
2019-02-14 04:44:56,405 : Text to Image: 10.924, 29.944, 42.18, 15.0
2019-02-14 04:45:38,347 : samples : 320000
2019-02-14 04:45:49,070 : Image to text: 12.64, 33.7, 46.92, 12.0
2019-02-14 04:45:59,440 : Text to Image: 10.288, 29.088, 41.188, 16.0
2019-02-14 04:46:40,257 : samples : 384000
2019-02-14 04:46:50,294 : Image to text: 13.12, 33.66, 47.12, 12.0
2019-02-14 04:46:57,537 : Text to Image: 10.208, 29.22, 41.22, 15.0
2019-02-14 04:47:38,991 : samples : 448000
2019-02-14 04:47:49,271 : Image to text: 14.2, 35.38, 48.72, 11.0
2019-02-14 04:47:56,545 : Text to Image: 11.088, 30.344, 42.484, 15.0
2019-02-14 04:48:37,429 : samples : 512000
2019-02-14 04:48:49,818 : Image to text: 13.4, 35.18, 48.5, 11.0
2019-02-14 04:48:57,939 : Text to Image: 10.844, 30.004, 42.252, 15.0
2019-02-14 04:49:32,851 : Epoch 3 finished
2019-02-14 04:49:33,307 : Image to text: 30.7, 65.6, 78.9, 3.0
2019-02-14 04:49:33,668 : Text to Image: 26.18, 60.94, 77.86, 4.0
2019-02-14 04:49:34,115 : Image to text: 31.9, 67.0, 80.4, 3.0
2019-02-14 04:49:34,477 : Text to Image: 26.3, 60.32, 76.06, 4.0
2019-02-14 04:49:34,924 : Image to text: 31.5, 65.3, 80.5, 3.0
2019-02-14 04:49:35,287 : Text to Image: 26.76, 61.2, 76.58, 4.0
2019-02-14 04:49:35,735 : Image to text: 33.0, 68.0, 80.9, 3.0
2019-02-14 04:49:36,094 : Text to Image: 26.62, 60.62, 77.64, 4.0
2019-02-14 04:49:36,543 : Image to text: 34.3, 68.4, 80.7, 3.0
2019-02-14 04:49:36,903 : Text to Image: 26.24, 59.64, 76.06, 4.0
2019-02-14 04:49:36,903 : Dev mean Text to Image: 26.419999999999995, 60.544, 76.83999999999999, 4.0
2019-02-14 04:49:36,903 : Dev mean Image to text: 32.28, 66.86, 80.28000000000002, 3.0
2019-02-14 04:49:36,903 : start epoch
2019-02-14 04:50:18,609 : samples : 64000
2019-02-14 04:50:31,239 : Image to text: 13.46, 35.48, 48.5, 11.0
2019-02-14 04:50:41,392 : Text to Image: 10.78, 29.8, 42.616, 15.0
2019-02-14 04:51:23,175 : samples : 128000
2019-02-14 04:51:35,626 : Image to text: 13.7, 35.48, 48.96, 11.0
2019-02-14 04:51:43,451 : Text to Image: 10.968, 30.508, 42.88, 15.0
2019-02-14 04:52:25,202 : samples : 192000
2019-02-14 04:52:37,708 : Image to text: 13.98, 34.82, 48.36, 12.0
2019-02-14 04:52:44,907 : Text to Image: 10.68, 29.652, 41.984, 15.0
2019-02-14 04:53:25,942 : samples : 256000
2019-02-14 04:53:36,007 : Image to text: 14.22, 35.84, 49.5, 11.0
2019-02-14 04:53:43,252 : Text to Image: 11.144, 30.688, 43.18, 14.0
2019-02-14 04:54:24,423 : samples : 320000
2019-02-14 04:54:34,475 : Image to text: 14.56, 35.56, 48.78, 11.0
2019-02-14 04:54:41,721 : Text to Image: 11.136, 30.436, 42.632, 15.0
2019-02-14 04:55:22,363 : samples : 384000
2019-02-14 04:55:34,875 : Image to text: 14.38, 36.78, 50.14, 10.0
2019-02-14 04:55:44,777 : Text to Image: 11.448, 30.364, 42.912, 15.0
2019-02-14 04:56:29,236 : samples : 448000
2019-02-14 04:56:41,736 : Image to text: 14.16, 36.12, 49.98, 11.0
2019-02-14 04:56:51,677 : Text to Image: 11.444, 30.596, 43.148, 14.0
2019-02-14 04:57:35,848 : samples : 512000
2019-02-14 04:57:48,389 : Image to text: 13.46, 35.78, 49.08, 11.0
2019-02-14 04:57:58,334 : Text to Image: 11.048, 30.148, 42.876, 14.0
2019-02-14 04:58:35,846 : Epoch 4 finished
2019-02-14 04:58:36,755 : Image to text: 32.0, 67.8, 81.7, 3.0
2019-02-14 04:58:37,479 : Text to Image: 26.92, 61.68, 78.48, 4.0
2019-02-14 04:58:38,363 : Image to text: 33.7, 66.4, 79.6, 3.0
2019-02-14 04:58:39,090 : Text to Image: 26.54, 60.02, 76.84, 4.0
2019-02-14 04:58:40,005 : Image to text: 32.8, 67.2, 81.8, 3.0
2019-02-14 04:58:40,723 : Text to Image: 27.06, 62.86, 77.62, 3.0
2019-02-14 04:58:41,612 : Image to text: 36.0, 68.0, 81.6, 3.0
2019-02-14 04:58:42,342 : Text to Image: 26.98, 62.06, 77.52, 4.0
2019-02-14 04:58:43,255 : Image to text: 36.1, 68.2, 81.3, 2.0
2019-02-14 04:58:44,016 : Text to Image: 26.52, 61.64, 76.96, 4.0
2019-02-14 04:58:44,016 : Dev mean Text to Image: 26.804000000000002, 61.652, 77.484, 3.8
2019-02-14 04:58:44,016 : Dev mean Image to text: 34.12, 67.52000000000001, 81.19999999999999, 2.8
2019-02-14 04:58:44,017 : start epoch
2019-02-14 04:59:27,859 : samples : 64000
2019-02-14 04:59:40,448 : Image to text: 14.36, 36.56, 50.18, 10.0
2019-02-14 04:59:50,492 : Text to Image: 11.632, 31.256, 44.016, 14.0
2019-02-14 05:00:34,635 : samples : 128000
2019-02-14 05:00:47,228 : Image to text: 14.48, 36.18, 48.62, 11.0
2019-02-14 05:00:57,250 : Text to Image: 11.452, 30.644, 43.548, 14.0
2019-02-14 05:01:40,881 : samples : 192000
2019-02-14 05:01:53,503 : Image to text: 13.64, 34.52, 48.54, 11.0
2019-02-14 05:02:03,532 : Text to Image: 11.024, 30.164, 42.944, 14.0
2019-02-14 05:02:46,048 : samples : 256000
2019-02-14 05:02:58,640 : Image to text: 13.9, 36.2, 49.9, 11.0
2019-02-14 05:03:08,668 : Text to Image: 11.316, 30.724, 43.42, 14.0
2019-02-14 05:03:51,741 : samples : 320000
2019-02-14 05:04:04,373 : Image to text: 14.52, 36.36, 50.42, 10.0
2019-02-14 05:04:14,376 : Text to Image: 11.588, 31.58, 44.04, 14.0
2019-02-14 05:04:56,625 : samples : 384000
2019-02-14 05:05:09,222 : Image to text: 14.38, 36.84, 49.96, 11.0
2019-02-14 05:05:19,291 : Text to Image: 11.304, 30.708, 43.748, 14.0
2019-02-14 05:06:02,442 : samples : 448000
2019-02-14 05:06:15,081 : Image to text: 14.48, 36.46, 50.08, 10.0
2019-02-14 05:06:25,157 : Text to Image: 11.156, 30.752, 43.272, 14.0
2019-02-14 05:07:09,246 : samples : 512000
2019-02-14 05:07:19,330 : Image to text: 15.54, 37.24, 50.56, 10.0
2019-02-14 05:07:26,459 : Text to Image: 11.36, 30.716, 43.556, 14.0
2019-02-14 05:08:01,292 : Epoch 5 finished
2019-02-14 05:08:01,744 : Image to text: 34.1, 66.4, 80.5, 3.0
2019-02-14 05:08:02,095 : Text to Image: 26.82, 62.28, 79.34, 4.0
2019-02-14 05:08:02,547 : Image to text: 34.8, 67.7, 82.2, 3.0
2019-02-14 05:08:02,892 : Text to Image: 26.76, 60.6, 77.12, 4.0
2019-02-14 05:08:03,344 : Image to text: 34.0, 68.5, 82.1, 3.0
2019-02-14 05:08:03,695 : Text to Image: 27.36, 62.16, 77.6, 3.0
2019-02-14 05:08:04,147 : Image to text: 35.7, 66.3, 81.1, 3.0
2019-02-14 05:08:04,499 : Text to Image: 27.8, 61.64, 78.0, 3.0
2019-02-14 05:08:04,951 : Image to text: 35.2, 69.9, 80.5, 3.0
2019-02-14 05:08:05,303 : Text to Image: 26.94, 61.08, 77.48, 4.0
2019-02-14 05:08:05,303 : Dev mean Text to Image: 27.136000000000003, 61.552, 77.908, 3.6000000000000005
2019-02-14 05:08:05,303 : Dev mean Image to text: 34.76, 67.75999999999999, 81.28, 3.0
2019-02-14 05:08:05,303 : start epoch
2019-02-14 05:08:46,590 : samples : 64000
2019-02-14 05:08:56,501 : Image to text: 14.86, 37.26, 51.04, 10.0
2019-02-14 05:09:03,233 : Text to Image: 11.612, 31.62, 44.148, 14.0
2019-02-14 05:09:47,864 : samples : 128000
2019-02-14 05:10:00,718 : Image to text: 13.98, 36.84, 50.46, 10.0
2019-02-14 05:10:11,107 : Text to Image: 11.316, 31.02, 43.74, 14.0
2019-02-14 05:10:55,789 : samples : 192000
2019-02-14 05:11:08,673 : Image to text: 14.46, 37.02, 50.28, 10.0
2019-02-14 05:11:19,092 : Text to Image: 11.532, 31.024, 43.88, 14.0
2019-02-14 05:12:04,139 : samples : 256000
2019-02-14 05:12:17,141 : Image to text: 14.9, 36.7, 49.86, 11.0
2019-02-14 05:12:27,587 : Text to Image: 11.44, 31.28, 43.92, 14.0
2019-02-14 05:13:12,778 : samples : 320000
2019-02-14 05:13:25,761 : Image to text: 14.1, 36.64, 51.06, 10.0
2019-02-14 05:13:36,214 : Text to Image: 11.636, 31.188, 44.112, 14.0
2019-02-14 05:14:21,469 : samples : 384000
2019-02-14 05:14:34,454 : Image to text: 15.26, 37.9, 51.4, 10.0
2019-02-14 05:14:44,934 : Text to Image: 11.624, 31.952, 44.364, 14.0
2019-02-14 05:15:29,900 : samples : 448000
2019-02-14 05:15:42,780 : Image to text: 14.62, 37.18, 50.62, 10.0
2019-02-14 05:15:53,250 : Text to Image: 11.72, 31.588, 44.408, 14.0
2019-02-14 05:16:38,807 : samples : 512000
2019-02-14 05:16:50,810 : Image to text: 14.78, 36.36, 50.42, 10.0
2019-02-14 05:17:01,255 : Text to Image: 11.516, 31.48, 44.572, 13.0
2019-02-14 05:17:38,110 : Epoch 6 finished
2019-02-14 05:17:38,553 : Image to text: 33.5, 68.1, 80.9, 3.0
2019-02-14 05:17:38,883 : Text to Image: 27.36, 62.64, 78.92, 3.0
2019-02-14 05:17:39,313 : Image to text: 34.6, 67.3, 82.6, 3.0
2019-02-14 05:17:39,641 : Text to Image: 27.38, 61.62, 77.64, 3.0
2019-02-14 05:17:40,080 : Image to text: 36.5, 69.7, 82.7, 3.0
2019-02-14 05:17:40,408 : Text to Image: 28.66, 63.34, 78.64, 3.0
2019-02-14 05:17:40,835 : Image to text: 36.0, 69.0, 81.6, 3.0
2019-02-14 05:17:41,163 : Text to Image: 28.18, 62.64, 78.62, 3.0
2019-02-14 05:17:41,588 : Image to text: 33.5, 68.4, 81.1, 3.0
2019-02-14 05:17:41,916 : Text to Image: 27.58, 61.6, 77.6, 4.0
2019-02-14 05:17:41,916 : Dev mean Text to Image: 27.832, 62.368, 78.28399999999999, 3.2
2019-02-14 05:17:41,916 : Dev mean Image to text: 34.82, 68.5, 81.78, 3.0
2019-02-14 05:17:41,917 : start epoch
2019-02-14 05:18:23,252 : samples : 64000
2019-02-14 05:18:33,599 : Image to text: 14.62, 36.76, 50.86, 10.0
2019-02-14 05:18:41,033 : Text to Image: 11.576, 31.344, 43.876, 14.0
2019-02-14 05:19:22,505 : samples : 128000
2019-02-14 05:19:32,840 : Image to text: 14.64, 38.32, 51.9, 9.0
2019-02-14 05:19:40,301 : Text to Image: 11.788, 31.848, 44.604, 13.0
2019-02-14 05:20:21,946 : samples : 192000
2019-02-14 05:20:32,282 : Image to text: 15.42, 37.52, 52.48, 10.0
2019-02-14 05:20:39,693 : Text to Image: 12.068, 32.432, 44.556, 14.0
2019-02-14 05:21:21,710 : samples : 256000
2019-02-14 05:21:32,009 : Image to text: 14.0, 36.34, 49.68, 11.0
2019-02-14 05:21:39,425 : Text to Image: 11.748, 32.072, 44.788, 14.0
2019-02-14 05:22:21,378 : samples : 320000
2019-02-14 05:22:31,679 : Image to text: 14.86, 37.1, 51.46, 10.0
2019-02-14 05:22:39,066 : Text to Image: 12.048, 31.96, 44.688, 13.0
2019-02-14 05:23:20,738 : samples : 384000
2019-02-14 05:23:31,152 : Image to text: 14.6, 37.52, 51.3, 10.0
2019-02-14 05:23:38,581 : Text to Image: 11.784, 31.904, 44.92, 13.0
2019-02-14 05:24:20,788 : samples : 448000
2019-02-14 05:24:31,183 : Image to text: 14.62, 37.6, 51.12, 10.0
2019-02-14 05:24:38,632 : Text to Image: 11.84, 32.052, 44.864, 13.0
2019-02-14 05:25:20,664 : samples : 512000
2019-02-14 05:25:31,036 : Image to text: 14.42, 37.92, 51.14, 10.0
2019-02-14 05:25:38,461 : Text to Image: 11.832, 32.024, 44.716, 13.0
2019-02-14 05:26:14,430 : Epoch 7 finished
2019-02-14 05:26:14,848 : Image to text: 33.8, 67.2, 81.8, 3.0
2019-02-14 05:26:15,171 : Text to Image: 28.52, 62.94, 79.32, 3.0
2019-02-14 05:26:15,602 : Image to text: 33.8, 67.9, 83.4, 3.0
2019-02-14 05:26:15,929 : Text to Image: 27.36, 62.28, 77.72, 3.0
2019-02-14 05:26:16,356 : Image to text: 35.1, 69.4, 83.1, 3.0
2019-02-14 05:26:16,685 : Text to Image: 28.76, 64.2, 79.38, 3.0
2019-02-14 05:26:17,129 : Image to text: 34.4, 68.1, 81.2, 3.0
2019-02-14 05:26:17,459 : Text to Image: 28.06, 63.06, 79.78, 3.0
2019-02-14 05:26:17,893 : Image to text: 33.3, 68.1, 80.3, 3.0
2019-02-14 05:26:18,223 : Text to Image: 28.2, 62.02, 77.82, 3.0
2019-02-14 05:26:18,224 : Dev mean Text to Image: 28.18, 62.900000000000006, 78.80399999999999, 3.0
2019-02-14 05:26:18,224 : Dev mean Image to text: 34.08, 68.14, 81.96000000000001, 3.0
2019-02-14 05:26:18,224 : start epoch
2019-02-14 05:27:01,920 : samples : 64000
2019-02-14 05:27:12,321 : Image to text: 15.0, 37.04, 51.3, 10.0
2019-02-14 05:27:19,756 : Text to Image: 11.728, 31.792, 44.668, 13.0
2019-02-14 05:28:01,464 : samples : 128000
2019-02-14 05:28:11,816 : Image to text: 15.28, 38.14, 51.16, 10.0
2019-02-14 05:28:19,261 : Text to Image: 12.344, 32.452, 45.248, 13.0
2019-02-14 05:29:00,399 : samples : 192000
2019-02-14 05:29:10,719 : Image to text: 15.4, 38.2, 51.76, 10.0
2019-02-14 05:29:18,138 : Text to Image: 11.936, 32.32, 45.192, 13.0
2019-02-14 05:30:00,184 : samples : 256000
2019-02-14 05:30:10,490 : Image to text: 14.96, 37.48, 50.7, 10.0
2019-02-14 05:30:17,914 : Text to Image: 11.772, 31.812, 44.54, 13.0
2019-02-14 05:30:59,692 : samples : 320000
2019-02-14 05:31:10,088 : Image to text: 16.06, 39.02, 52.26, 9.0
2019-02-14 05:31:17,532 : Text to Image: 12.216, 32.82, 45.68, 13.0
2019-02-14 05:31:58,952 : samples : 384000
2019-02-14 05:32:09,233 : Image to text: 15.42, 38.0, 52.16, 10.0
2019-02-14 05:32:16,661 : Text to Image: 12.244, 32.124, 45.424, 13.0
2019-02-14 05:32:58,521 : samples : 448000
2019-02-14 05:33:08,825 : Image to text: 15.18, 37.32, 50.42, 10.0
2019-02-14 05:33:16,244 : Text to Image: 11.964, 31.86, 44.744, 13.0
2019-02-14 05:33:57,744 : samples : 512000
2019-02-14 05:34:08,025 : Image to text: 15.76, 39.08, 53.18, 9.0
2019-02-14 05:34:15,469 : Text to Image: 12.292, 32.42, 45.356, 13.0
2019-02-14 05:34:51,821 : Epoch 8 finished
2019-02-14 05:34:52,261 : Image to text: 35.1, 67.3, 81.4, 3.0
2019-02-14 05:34:52,588 : Text to Image: 28.9, 63.48, 79.1, 3.0
2019-02-14 05:34:53,014 : Image to text: 34.0, 68.1, 82.5, 3.0
2019-02-14 05:34:53,341 : Text to Image: 27.72, 61.9, 77.78, 3.0
2019-02-14 05:34:53,776 : Image to text: 36.8, 69.5, 81.7, 3.0
2019-02-14 05:34:54,107 : Text to Image: 28.18, 63.98, 79.44, 3.0
2019-02-14 05:34:54,539 : Image to text: 36.2, 68.7, 82.2, 3.0
2019-02-14 05:34:54,870 : Text to Image: 27.78, 62.9, 79.64, 3.0
2019-02-14 05:34:55,305 : Image to text: 35.8, 68.2, 81.7, 3.0
2019-02-14 05:34:55,635 : Text to Image: 27.44, 61.94, 78.34, 3.0
2019-02-14 05:34:55,635 : Dev mean Text to Image: 28.003999999999998, 62.839999999999996, 78.86, 3.0
2019-02-14 05:34:55,635 : Dev mean Image to text: 35.58, 68.36, 81.9, 3.0
2019-02-14 05:34:55,636 : start epoch
2019-02-14 05:35:38,058 : samples : 64000
2019-02-14 05:35:48,347 : Image to text: 15.92, 38.24, 51.42, 10.0
2019-02-14 05:35:55,769 : Text to Image: 12.192, 32.296, 45.316, 13.0
2019-02-14 05:36:38,887 : samples : 128000
2019-02-14 05:36:49,159 : Image to text: 15.08, 38.1, 51.62, 10.0
2019-02-14 05:36:56,582 : Text to Image: 11.604, 32.076, 45.012, 13.0
2019-02-14 05:37:38,988 : samples : 192000
2019-02-14 05:37:49,293 : Image to text: 14.86, 37.5, 52.08, 10.0
2019-02-14 05:37:56,723 : Text to Image: 11.892, 32.312, 45.072, 13.0
2019-02-14 05:38:39,125 : samples : 256000
2019-02-14 05:38:49,409 : Image to text: 15.2, 38.76, 52.08, 9.0
2019-02-14 05:38:56,790 : Text to Image: 12.18, 32.476, 45.188, 13.0
2019-02-14 05:39:39,252 : samples : 320000
2019-02-14 05:39:49,573 : Image to text: 16.16, 37.92, 51.76, 10.0
2019-02-14 05:39:57,006 : Text to Image: 12.232, 32.408, 45.152, 13.0
2019-02-14 05:40:38,780 : samples : 384000
2019-02-14 05:40:49,093 : Image to text: 15.26, 37.74, 51.58, 10.0
2019-02-14 05:40:56,529 : Text to Image: 12.304, 32.368, 45.544, 13.0
2019-02-14 05:41:37,758 : samples : 448000
2019-02-14 05:41:48,093 : Image to text: 15.66, 38.94, 52.54, 9.0
2019-02-14 05:41:55,546 : Text to Image: 12.44, 32.668, 45.496, 13.0
2019-02-14 05:42:36,960 : samples : 512000
2019-02-14 05:42:47,293 : Image to text: 15.08, 37.12, 51.42, 10.0
2019-02-14 05:42:54,746 : Text to Image: 12.296, 32.596, 45.472, 13.0
2019-02-14 05:43:29,219 : Epoch 9 finished
2019-02-14 05:43:29,654 : Image to text: 35.1, 67.7, 82.3, 3.0
2019-02-14 05:43:29,983 : Text to Image: 28.68, 63.68, 80.46, 3.0
2019-02-14 05:43:30,412 : Image to text: 34.8, 68.1, 82.6, 3.0
2019-02-14 05:43:30,740 : Text to Image: 28.02, 62.5, 78.88, 3.0
2019-02-14 05:43:31,168 : Image to text: 34.9, 68.6, 83.7, 3.0
2019-02-14 05:43:31,497 : Text to Image: 28.44, 64.42, 79.9, 3.0
2019-02-14 05:43:31,948 : Image to text: 34.2, 69.8, 83.4, 3.0
2019-02-14 05:43:32,278 : Text to Image: 28.86, 63.48, 79.84, 3.0
2019-02-14 05:43:32,710 : Image to text: 34.8, 70.6, 83.3, 2.0
2019-02-14 05:43:33,039 : Text to Image: 28.04, 62.9, 78.56, 3.0
2019-02-14 05:43:33,039 : Dev mean Text to Image: 28.407999999999998, 63.396, 79.528, 3.0
2019-02-14 05:43:33,039 : Dev mean Image to text: 34.76, 68.96, 83.06, 2.8
2019-02-14 05:43:33,040 : start epoch
2019-02-14 05:44:13,922 : samples : 64000
2019-02-14 05:44:24,186 : Image to text: 15.8, 38.56, 52.44, 9.0
2019-02-14 05:44:31,583 : Text to Image: 12.164, 32.732, 45.684, 13.0
2019-02-14 05:45:12,086 : samples : 128000
2019-02-14 05:45:22,340 : Image to text: 15.46, 38.38, 52.42, 10.0
2019-02-14 05:45:29,729 : Text to Image: 12.276, 33.176, 45.552, 13.0
2019-02-14 05:46:13,372 : samples : 192000
2019-02-14 05:46:23,645 : Image to text: 16.22, 38.2, 51.88, 10.0
2019-02-14 05:46:31,062 : Text to Image: 12.268, 32.588, 45.312, 13.0
2019-02-14 05:47:12,505 : samples : 256000
2019-02-14 05:47:22,844 : Image to text: 15.36, 38.12, 52.2, 10.0
2019-02-14 05:47:30,277 : Text to Image: 12.104, 32.324, 45.3, 13.0
2019-02-14 05:48:12,120 : samples : 320000
2019-02-14 05:48:22,496 : Image to text: 15.6, 38.62, 51.94, 10.0
2019-02-14 05:48:29,946 : Text to Image: 12.356, 32.752, 45.88, 13.0
2019-02-14 05:49:12,523 : samples : 384000
2019-02-14 05:49:22,844 : Image to text: 15.68, 38.54, 51.78, 10.0
2019-02-14 05:49:30,251 : Text to Image: 12.224, 32.608, 45.464, 13.0
2019-02-14 05:50:10,650 : samples : 448000
2019-02-14 05:50:20,952 : Image to text: 16.16, 38.74, 52.82, 9.0
2019-02-14 05:50:28,368 : Text to Image: 12.392, 32.768, 45.892, 13.0
2019-02-14 05:51:08,568 : samples : 512000
2019-02-14 05:51:18,849 : Image to text: 15.5, 37.96, 51.9, 10.0
2019-02-14 05:51:26,250 : Text to Image: 12.144, 32.824, 45.844, 13.0
2019-02-14 05:52:00,204 : Epoch 10 finished
2019-02-14 05:52:00,624 : Image to text: 36.2, 66.7, 82.4, 3.0
2019-02-14 05:52:00,949 : Text to Image: 28.64, 63.78, 80.28, 3.0
2019-02-14 05:52:01,381 : Image to text: 34.6, 68.7, 83.5, 3.0
2019-02-14 05:52:01,705 : Text to Image: 28.5, 62.52, 78.7, 3.0
2019-02-14 05:52:02,129 : Image to text: 36.5, 70.1, 83.3, 2.0
2019-02-14 05:52:02,456 : Text to Image: 28.68, 64.18, 79.3, 3.0
2019-02-14 05:52:02,894 : Image to text: 35.4, 70.7, 83.0, 2.0
2019-02-14 05:52:03,221 : Text to Image: 28.76, 64.36, 79.74, 3.0
2019-02-14 05:52:03,648 : Image to text: 35.4, 69.7, 81.4, 3.0
2019-02-14 05:52:03,974 : Text to Image: 28.04, 63.34, 79.04, 3.0
2019-02-14 05:52:03,974 : Dev mean Text to Image: 28.524000000000004, 63.636, 79.412, 3.0
2019-02-14 05:52:03,974 : Dev mean Image to text: 35.62, 69.17999999999999, 82.72, 2.6
2019-02-14 05:52:03,975 : start epoch
2019-02-14 05:52:44,873 : samples : 64000
2019-02-14 05:52:55,249 : Image to text: 16.1, 38.8, 53.24, 9.0
2019-02-14 05:53:02,645 : Text to Image: 12.488, 32.652, 45.572, 13.0
2019-02-14 05:53:42,991 : samples : 128000
2019-02-14 05:53:53,315 : Image to text: 15.66, 38.94, 51.68, 10.0
2019-02-14 05:54:00,687 : Text to Image: 12.056, 32.6, 45.608, 13.0
2019-02-14 05:54:40,912 : samples : 192000
2019-02-14 05:54:51,160 : Image to text: 16.02, 39.12, 52.54, 9.0
2019-02-14 05:54:58,545 : Text to Image: 12.236, 32.54, 45.348, 13.0
2019-02-14 05:55:39,103 : samples : 256000
2019-02-14 05:55:49,330 : Image to text: 16.12, 38.54, 52.24, 9.0
2019-02-14 05:55:56,694 : Text to Image: 12.136, 32.892, 45.684, 13.0
2019-02-14 05:56:38,389 : samples : 320000
2019-02-14 05:56:48,756 : Image to text: 15.42, 39.26, 52.16, 9.0
2019-02-14 05:56:56,166 : Text to Image: 12.124, 32.736, 45.6, 13.0
2019-02-14 05:57:37,188 : samples : 384000
2019-02-14 05:57:47,417 : Image to text: 15.92, 39.32, 52.92, 9.0
2019-02-14 05:57:54,790 : Text to Image: 12.624, 33.808, 46.384, 12.0
2019-02-14 05:58:35,437 : samples : 448000
2019-02-14 05:58:45,725 : Image to text: 16.08, 38.2, 51.96, 10.0
2019-02-14 05:58:53,132 : Text to Image: 12.46, 33.168, 46.108, 13.0
2019-02-14 05:59:34,055 : samples : 512000
2019-02-14 05:59:44,348 : Image to text: 16.08, 38.94, 52.88, 9.0
2019-02-14 05:59:51,763 : Text to Image: 12.396, 33.092, 45.872, 13.0
2019-02-14 06:00:27,315 : Epoch 11 finished
2019-02-14 06:00:27,740 : Image to text: 33.8, 67.9, 80.9, 3.0
2019-02-14 06:00:28,067 : Text to Image: 28.52, 64.12, 79.82, 3.0
2019-02-14 06:00:28,506 : Image to text: 36.5, 69.3, 83.9, 3.0
2019-02-14 06:00:28,836 : Text to Image: 28.28, 62.8, 78.24, 3.0
2019-02-14 06:00:29,269 : Image to text: 36.8, 68.8, 82.5, 3.0
2019-02-14 06:00:29,600 : Text to Image: 29.84, 64.2, 79.28, 3.0
2019-02-14 06:00:30,051 : Image to text: 38.8, 70.6, 84.4, 2.0
2019-02-14 06:00:30,384 : Text to Image: 28.68, 63.62, 79.7, 3.0
2019-02-14 06:00:30,817 : Image to text: 37.2, 69.4, 82.1, 2.0
2019-02-14 06:00:31,146 : Text to Image: 28.18, 62.66, 78.58, 3.0
2019-02-14 06:00:31,147 : Dev mean Text to Image: 28.7, 63.480000000000004, 79.124, 3.0
2019-02-14 06:00:31,147 : Dev mean Image to text: 36.62, 69.2, 82.76, 2.5999999999999996
2019-02-14 06:00:31,147 : start epoch
2019-02-14 06:01:12,782 : samples : 64000
2019-02-14 06:01:23,092 : Image to text: 15.46, 38.54, 52.96, 9.0
2019-02-14 06:01:30,505 : Text to Image: 12.492, 33.052, 46.084, 13.0
2019-02-14 06:02:11,550 : samples : 128000
2019-02-14 06:02:21,951 : Image to text: 15.72, 39.1, 52.66, 9.0
2019-02-14 06:02:29,422 : Text to Image: 12.16, 32.588, 45.904, 13.0
2019-02-14 06:03:11,112 : samples : 192000
2019-02-14 06:03:21,549 : Image to text: 16.04, 39.62, 54.08, 9.0
2019-02-14 06:03:29,016 : Text to Image: 12.68, 33.464, 46.288, 12.0
2019-02-14 06:04:09,806 : samples : 256000
2019-02-14 06:04:20,232 : Image to text: 15.78, 39.08, 52.26, 9.0
2019-02-14 06:04:27,660 : Text to Image: 12.42, 32.8, 45.632, 13.0
2019-02-14 06:05:09,148 : samples : 320000
2019-02-14 06:05:19,537 : Image to text: 15.38, 38.8, 52.24, 9.0
2019-02-14 06:05:27,030 : Text to Image: 12.356, 33.136, 45.948, 13.0
2019-02-14 06:06:07,733 : samples : 384000
2019-02-14 06:06:18,143 : Image to text: 15.68, 39.92, 53.28, 9.0
2019-02-14 06:06:25,542 : Text to Image: 12.716, 33.4, 46.24, 13.0
2019-02-14 06:07:06,151 : samples : 448000
2019-02-14 06:07:16,348 : Image to text: 16.12, 39.5, 53.04, 9.0
2019-02-14 06:07:23,703 : Text to Image: 12.888, 33.552, 46.3, 12.0
2019-02-14 06:08:04,010 : samples : 512000
2019-02-14 06:08:14,215 : Image to text: 16.06, 39.22, 53.16, 9.0
2019-02-14 06:08:21,574 : Text to Image: 12.5, 33.232, 46.212, 13.0
2019-02-14 06:08:55,525 : Epoch 12 finished
2019-02-14 06:08:55,955 : Image to text: 34.5, 67.3, 80.8, 3.0
2019-02-14 06:08:56,278 : Text to Image: 28.26, 65.04, 80.36, 3.0
2019-02-14 06:08:56,697 : Image to text: 35.9, 69.6, 82.7, 2.0
2019-02-14 06:08:57,020 : Text to Image: 28.92, 62.96, 79.24, 3.0
2019-02-14 06:08:57,445 : Image to text: 35.7, 69.9, 83.2, 3.0
2019-02-14 06:08:57,773 : Text to Image: 28.88, 64.98, 79.86, 3.0
2019-02-14 06:08:58,215 : Image to text: 38.2, 69.9, 82.6, 3.0
2019-02-14 06:08:58,544 : Text to Image: 29.42, 64.38, 80.42, 3.0
2019-02-14 06:08:58,977 : Image to text: 36.1, 68.2, 82.4, 2.0
2019-02-14 06:08:59,306 : Text to Image: 29.28, 64.08, 79.66, 3.0
2019-02-14 06:08:59,306 : Dev mean Text to Image: 28.951999999999998, 64.288, 79.908, 3.0
2019-02-14 06:08:59,307 : Dev mean Image to text: 36.08, 68.98, 82.34, 2.6
2019-02-14 06:08:59,307 : start epoch
2019-02-14 06:09:40,668 : samples : 64000
2019-02-14 06:09:50,968 : Image to text: 16.12, 40.4, 53.66, 9.0
2019-02-14 06:09:58,398 : Text to Image: 12.684, 32.908, 46.096, 12.0
2019-02-14 06:10:38,580 : samples : 128000
2019-02-14 06:10:49,006 : Image to text: 15.5, 38.66, 53.24, 9.0
2019-02-14 06:10:56,463 : Text to Image: 12.42, 33.204, 45.744, 13.0
2019-02-14 06:11:37,430 : samples : 192000
2019-02-14 06:11:47,814 : Image to text: 16.04, 39.66, 53.54, 9.0
2019-02-14 06:11:55,244 : Text to Image: 12.488, 33.392, 46.596, 12.0
2019-02-14 06:12:37,810 : samples : 256000
2019-02-14 06:12:48,182 : Image to text: 17.5, 40.28, 53.92, 9.0
2019-02-14 06:12:55,614 : Text to Image: 12.856, 33.752, 46.444, 12.0
2019-02-14 06:13:37,176 : samples : 320000
2019-02-14 06:13:47,404 : Image to text: 15.82, 38.68, 52.34, 9.0
2019-02-14 06:13:54,781 : Text to Image: 12.62, 33.712, 46.512, 12.0
2019-02-14 06:14:35,633 : samples : 384000
2019-02-14 06:14:45,988 : Image to text: 15.94, 39.2, 53.38, 9.0
2019-02-14 06:14:53,472 : Text to Image: 12.516, 33.16, 45.78, 13.0
2019-02-14 06:15:34,664 : samples : 448000
2019-02-14 06:15:44,948 : Image to text: 15.64, 38.2, 52.6, 9.0
2019-02-14 06:15:52,355 : Text to Image: 12.26, 33.052, 46.384, 12.0
2019-02-14 06:16:33,279 : samples : 512000
2019-02-14 06:16:43,580 : Image to text: 16.52, 38.78, 52.34, 9.0
2019-02-14 06:16:50,962 : Text to Image: 12.376, 33.216, 46.016, 13.0
2019-02-14 06:17:25,180 : Epoch 13 finished
2019-02-14 06:17:25,605 : Image to text: 33.7, 68.2, 83.0, 3.0
2019-02-14 06:17:25,933 : Text to Image: 29.42, 65.08, 81.02, 3.0
2019-02-14 06:17:26,369 : Image to text: 35.9, 70.2, 84.2, 2.0
2019-02-14 06:17:26,695 : Text to Image: 29.2, 63.46, 79.12, 3.0
2019-02-14 06:17:27,125 : Image to text: 36.2, 69.5, 84.3, 3.0
2019-02-14 06:17:27,456 : Text to Image: 29.72, 65.12, 80.22, 3.0
2019-02-14 06:17:27,904 : Image to text: 38.4, 70.1, 83.4, 2.0
2019-02-14 06:17:28,236 : Text to Image: 29.3, 64.98, 80.84, 3.0
2019-02-14 06:17:28,677 : Image to text: 36.6, 69.9, 82.0, 2.0
2019-02-14 06:17:29,009 : Text to Image: 28.84, 63.68, 78.98, 3.0
2019-02-14 06:17:29,010 : Dev mean Text to Image: 29.296, 64.464, 80.03600000000002, 3.0
2019-02-14 06:17:29,010 : Dev mean Image to text: 36.16, 69.58, 83.38, 2.4
2019-02-14 06:17:29,010 : start epoch
2019-02-14 06:18:10,014 : samples : 64000
2019-02-14 06:18:20,273 : Image to text: 15.88, 39.58, 53.88, 9.0
2019-02-14 06:18:27,654 : Text to Image: 12.536, 33.308, 46.336, 12.0
2019-02-14 06:19:08,438 : samples : 128000
2019-02-14 06:19:18,704 : Image to text: 16.3, 39.32, 54.02, 9.0
2019-02-14 06:19:26,096 : Text to Image: 12.632, 33.184, 45.96, 13.0
2019-02-14 06:20:07,978 : samples : 192000
2019-02-14 06:20:18,261 : Image to text: 16.9, 40.36, 54.0, 9.0
2019-02-14 06:20:25,659 : Text to Image: 12.604, 33.56, 46.576, 12.0
2019-02-14 06:21:06,268 : samples : 256000
2019-02-14 06:21:16,569 : Image to text: 16.3, 39.62, 53.52, 9.0
2019-02-14 06:21:24,017 : Text to Image: 12.552, 33.284, 46.132, 12.0
2019-02-14 06:22:04,699 : samples : 320000
2019-02-14 06:22:15,019 : Image to text: 16.22, 40.1, 53.62, 9.0
2019-02-14 06:22:22,452 : Text to Image: 12.632, 33.372, 46.66, 12.0
2019-02-14 06:23:03,024 : samples : 384000
2019-02-14 06:23:13,298 : Image to text: 16.04, 39.38, 53.78, 9.0
2019-02-14 06:23:20,681 : Text to Image: 12.712, 33.752, 46.6, 12.0
2019-02-14 06:24:01,025 : samples : 448000
2019-02-14 06:24:11,258 : Image to text: 16.32, 39.38, 53.14, 9.0
2019-02-14 06:24:18,631 : Text to Image: 12.188, 32.892, 45.84, 13.0
2019-02-14 06:24:59,590 : samples : 512000
2019-02-14 06:25:09,885 : Image to text: 16.46, 39.58, 54.16, 9.0
2019-02-14 06:25:17,279 : Text to Image: 12.624, 33.508, 46.6, 12.0
2019-02-14 06:25:51,956 : Epoch 14 finished
2019-02-14 06:25:52,390 : Image to text: 35.8, 67.3, 81.7, 2.0
2019-02-14 06:25:52,716 : Text to Image: 29.68, 64.82, 80.28, 3.0
2019-02-14 06:25:53,144 : Image to text: 35.8, 70.2, 83.1, 2.0
2019-02-14 06:25:53,472 : Text to Image: 28.56, 63.56, 79.46, 3.0
2019-02-14 06:25:53,902 : Image to text: 35.2, 69.5, 83.3, 3.0
2019-02-14 06:25:54,231 : Text to Image: 29.68, 65.5, 80.62, 3.0
2019-02-14 06:25:54,672 : Image to text: 37.5, 70.7, 83.8, 2.0
2019-02-14 06:25:55,002 : Text to Image: 29.22, 64.86, 80.6, 3.0
2019-02-14 06:25:55,431 : Image to text: 36.8, 68.8, 81.7, 3.0
2019-02-14 06:25:55,761 : Text to Image: 29.54, 63.98, 79.58, 3.0
2019-02-14 06:25:55,762 : Dev mean Text to Image: 29.336, 64.544, 80.108, 3.0
2019-02-14 06:25:55,762 : Dev mean Image to text: 36.22, 69.3, 82.72, 2.4
2019-02-14 06:25:55,762 : start epoch
2019-02-14 06:26:37,800 : samples : 64000
2019-02-14 06:26:48,117 : Image to text: 16.3, 39.34, 52.92, 9.0
2019-02-14 06:26:55,526 : Text to Image: 12.636, 33.372, 46.44, 12.0
2019-02-14 06:27:37,650 : samples : 128000
2019-02-14 06:27:48,039 : Image to text: 16.08, 39.46, 53.02, 9.0
2019-02-14 06:27:55,445 : Text to Image: 12.364, 33.452, 46.38, 12.0
2019-02-14 06:28:37,206 : samples : 192000
2019-02-14 06:28:47,597 : Image to text: 16.24, 39.96, 53.18, 9.0
2019-02-14 06:28:55,012 : Text to Image: 12.836, 33.364, 46.448, 12.0
2019-02-14 06:29:36,958 : samples : 256000
2019-02-14 06:29:47,298 : Image to text: 16.84, 40.08, 54.08, 9.0
2019-02-14 06:29:54,758 : Text to Image: 12.796, 33.692, 46.776, 12.0
2019-02-14 06:30:36,004 : samples : 320000
2019-02-14 06:30:46,339 : Image to text: 16.34, 39.26, 53.16, 9.0
2019-02-14 06:30:53,790 : Text to Image: 12.668, 33.712, 46.956, 12.0
2019-02-14 06:31:35,119 : samples : 384000
2019-02-14 06:31:45,544 : Image to text: 16.84, 39.46, 53.0, 9.0
2019-02-14 06:31:53,009 : Text to Image: 12.804, 33.6, 46.788, 12.0
2019-02-14 06:32:33,941 : samples : 448000
2019-02-14 06:32:44,378 : Image to text: 17.64, 40.2, 53.72, 9.0
2019-02-14 06:32:51,829 : Text to Image: 13.04, 33.776, 46.9, 12.0
2019-02-14 06:33:33,347 : samples : 512000
2019-02-14 06:33:43,667 : Image to text: 16.62, 39.6, 53.9, 9.0
2019-02-14 06:33:51,136 : Text to Image: 12.772, 33.628, 46.484, 12.0
2019-02-14 06:34:25,467 : Epoch 15 finished
2019-02-14 06:34:25,894 : Image to text: 36.3, 69.8, 83.9, 3.0
2019-02-14 06:34:26,222 : Text to Image: 29.46, 64.7, 80.32, 3.0
2019-02-14 06:34:26,662 : Image to text: 35.7, 70.3, 83.9, 3.0
2019-02-14 06:34:26,989 : Text to Image: 29.12, 63.54, 78.84, 3.0
2019-02-14 06:34:27,417 : Image to text: 36.7, 71.8, 84.0, 2.0
2019-02-14 06:34:27,747 : Text to Image: 29.8, 65.02, 79.56, 3.0
2019-02-14 06:34:28,191 : Image to text: 38.5, 71.7, 84.4, 2.0
2019-02-14 06:34:28,523 : Text to Image: 29.8, 64.78, 80.68, 3.0
2019-02-14 06:34:28,959 : Image to text: 35.9, 70.7, 84.0, 2.0
2019-02-14 06:34:29,291 : Text to Image: 28.74, 64.02, 79.18, 3.0
2019-02-14 06:34:29,291 : Dev mean Text to Image: 29.384, 64.412, 79.71600000000001, 3.0
2019-02-14 06:34:29,292 : Dev mean Image to text: 36.620000000000005, 70.86, 84.04, 2.4
2019-02-14 06:34:29,292 : start epoch
2019-02-14 06:35:11,168 : samples : 64000
2019-02-14 06:35:21,534 : Image to text: 16.56, 39.42, 53.68, 9.0
2019-02-14 06:35:29,033 : Text to Image: 12.604, 33.748, 46.64, 12.0
2019-02-14 06:36:10,741 : samples : 128000
2019-02-14 06:36:21,171 : Image to text: 16.3, 39.4, 53.16, 9.0
2019-02-14 06:36:28,647 : Text to Image: 12.56, 33.212, 46.212, 13.0
2019-02-14 06:37:09,510 : samples : 192000
2019-02-14 06:37:19,817 : Image to text: 15.42, 39.0, 53.12, 9.0
2019-02-14 06:37:27,210 : Text to Image: 12.8, 33.624, 46.512, 12.0
2019-02-14 06:38:08,853 : samples : 256000
2019-02-14 06:38:19,208 : Image to text: 16.88, 40.26, 54.06, 9.0
2019-02-14 06:38:26,654 : Text to Image: 12.72, 33.844, 46.632, 12.0
2019-02-14 06:39:07,319 : samples : 320000
2019-02-14 06:39:17,750 : Image to text: 16.36, 40.1, 53.5, 9.0
2019-02-14 06:39:25,221 : Text to Image: 12.376, 33.384, 46.404, 12.0
2019-02-14 06:40:07,516 : samples : 384000
2019-02-14 06:40:17,800 : Image to text: 16.38, 39.96, 53.52, 9.0
2019-02-14 06:40:25,211 : Text to Image: 12.552, 33.312, 46.456, 12.0
2019-02-14 06:41:06,753 : samples : 448000
2019-02-14 06:41:17,034 : Image to text: 17.0, 39.9, 53.86, 9.0
2019-02-14 06:41:24,421 : Text to Image: 12.6, 33.264, 46.712, 12.0
2019-02-14 06:42:07,547 : samples : 512000
2019-02-14 06:42:17,805 : Image to text: 16.68, 40.46, 54.3, 9.0
2019-02-14 06:42:25,201 : Text to Image: 13.08, 34.252, 47.084, 12.0
2019-02-14 06:43:01,735 : Epoch 16 finished
2019-02-14 06:43:02,179 : Image to text: 35.6, 69.2, 83.1, 3.0
2019-02-14 06:43:02,509 : Text to Image: 29.96, 65.78, 80.94, 3.0
2019-02-14 06:43:02,938 : Image to text: 36.4, 70.1, 83.3, 2.0
2019-02-14 06:43:03,267 : Text to Image: 29.34, 63.76, 79.34, 3.0
2019-02-14 06:43:03,709 : Image to text: 36.6, 69.9, 83.8, 2.0
2019-02-14 06:43:04,042 : Text to Image: 30.06, 65.0, 80.3, 3.0
2019-02-14 06:43:04,480 : Image to text: 37.3, 70.3, 83.7, 2.0
2019-02-14 06:43:04,813 : Text to Image: 28.94, 65.0, 80.98, 3.0
2019-02-14 06:43:05,261 : Image to text: 36.7, 69.4, 82.8, 2.0
2019-02-14 06:43:05,592 : Text to Image: 28.6, 63.8, 79.86, 3.0
2019-02-14 06:43:05,592 : Dev mean Text to Image: 29.380000000000003, 64.668, 80.28399999999999, 3.0
2019-02-14 06:43:05,592 : Dev mean Image to text: 36.52, 69.78, 83.34, 2.1999999999999997
2019-02-14 06:43:05,592 : start epoch
2019-02-14 06:43:47,685 : samples : 64000
2019-02-14 06:43:58,016 : Image to text: 16.88, 39.36, 53.76, 9.0
2019-02-14 06:44:05,444 : Text to Image: 12.848, 34.132, 47.008, 12.0
2019-02-14 06:44:47,507 : samples : 128000
2019-02-14 06:44:57,821 : Image to text: 16.98, 40.4, 53.88, 9.0
2019-02-14 06:45:05,222 : Text to Image: 12.58, 33.776, 46.508, 12.0
2019-02-14 06:45:46,723 : samples : 192000
2019-02-14 06:45:57,127 : Image to text: 16.82, 40.1, 54.62, 8.0
2019-02-14 06:46:04,577 : Text to Image: 13.008, 33.792, 46.764, 12.0
2019-02-14 06:46:44,942 : samples : 256000
2019-02-14 06:46:55,224 : Image to text: 16.42, 40.44, 54.56, 9.0
2019-02-14 06:47:02,635 : Text to Image: 12.488, 33.096, 46.076, 13.0
2019-02-14 06:47:43,263 : samples : 320000
2019-02-14 06:47:53,587 : Image to text: 16.84, 40.86, 54.72, 9.0
2019-02-14 06:48:01,002 : Text to Image: 12.752, 33.432, 46.672, 12.0
2019-02-14 06:48:41,732 : samples : 384000
2019-02-14 06:48:52,037 : Image to text: 16.62, 39.16, 53.42, 9.0
2019-02-14 06:48:59,447 : Text to Image: 12.932, 33.412, 46.704, 12.0
2019-02-14 06:49:40,232 : samples : 448000
2019-02-14 06:49:50,578 : Image to text: 16.38, 40.3, 53.4, 9.0
2019-02-14 06:49:57,999 : Text to Image: 13.0, 33.728, 46.816, 12.0
2019-02-14 06:50:39,673 : samples : 512000
2019-02-14 06:50:50,004 : Image to text: 15.68, 39.1, 52.92, 9.0
2019-02-14 06:50:57,443 : Text to Image: 12.972, 33.752, 46.768, 12.0
2019-02-14 06:51:32,137 : Epoch 17 finished
2019-02-14 06:51:32,559 : Image to text: 36.4, 69.7, 84.0, 3.0
2019-02-14 06:51:32,885 : Text to Image: 29.18, 65.42, 80.88, 3.0
2019-02-14 06:51:33,313 : Image to text: 37.0, 69.7, 83.3, 2.0
2019-02-14 06:51:33,642 : Text to Image: 29.6, 63.72, 79.36, 3.0
2019-02-14 06:51:34,086 : Image to text: 37.0, 70.1, 83.5, 2.0
2019-02-14 06:51:34,416 : Text to Image: 29.58, 64.94, 80.14, 3.0
2019-02-14 06:51:34,852 : Image to text: 36.5, 70.3, 84.1, 2.0
2019-02-14 06:51:35,183 : Text to Image: 29.36, 64.76, 80.88, 3.0
2019-02-14 06:51:35,628 : Image to text: 36.4, 71.1, 83.0, 3.0
2019-02-14 06:51:35,957 : Text to Image: 29.06, 64.02, 79.52, 3.0
2019-02-14 06:51:35,958 : Dev mean Text to Image: 29.356, 64.572, 80.15599999999999, 3.0
2019-02-14 06:51:35,958 : Dev mean Image to text: 36.66, 70.18, 83.57999999999998, 2.4
2019-02-14 06:51:35,958 : start epoch
2019-02-14 06:52:17,838 : samples : 64000
2019-02-14 06:52:28,174 : Image to text: 16.48, 39.74, 54.12, 9.0
2019-02-14 06:52:35,610 : Text to Image: 12.952, 34.224, 47.164, 12.0
2019-02-14 06:53:18,259 : samples : 128000
2019-02-14 06:53:28,721 : Image to text: 16.32, 39.52, 53.96, 9.0
2019-02-14 06:53:36,213 : Text to Image: 12.764, 33.852, 46.672, 12.0
2019-02-14 06:54:16,945 : samples : 192000
2019-02-14 06:54:27,263 : Image to text: 16.48, 39.46, 53.96, 9.0
2019-02-14 06:54:34,696 : Text to Image: 13.084, 34.08, 47.052, 12.0
2019-02-14 06:55:16,295 : samples : 256000
2019-02-14 06:55:26,599 : Image to text: 16.34, 39.3, 53.22, 9.0
2019-02-14 06:55:34,028 : Text to Image: 13.048, 34.068, 47.228, 12.0
2019-02-14 06:56:14,589 : samples : 320000
2019-02-14 06:56:24,941 : Image to text: 16.32, 40.08, 53.58, 9.0
2019-02-14 06:56:32,362 : Text to Image: 12.8, 33.696, 47.088, 12.0
2019-02-14 06:57:12,735 : samples : 384000
2019-02-14 06:57:22,999 : Image to text: 16.36, 39.9, 53.88, 9.0
2019-02-14 06:57:30,411 : Text to Image: 12.648, 33.812, 46.708, 12.0
2019-02-14 06:58:12,506 : samples : 448000
2019-02-14 06:58:22,936 : Image to text: 16.88, 40.66, 54.3, 9.0
2019-02-14 06:58:30,387 : Text to Image: 12.968, 33.976, 47.16, 12.0
2019-02-14 06:59:12,801 : samples : 512000
2019-02-14 06:59:23,209 : Image to text: 16.42, 39.98, 53.88, 9.0
2019-02-14 06:59:30,694 : Text to Image: 12.448, 33.516, 46.484, 12.0
2019-02-14 07:00:07,356 : Epoch 18 finished
2019-02-14 07:00:08,283 : Image to text: 35.3, 70.4, 82.6, 3.0
2019-02-14 07:00:09,047 : Text to Image: 29.9, 65.38, 80.64, 3.0
2019-02-14 07:00:09,975 : Image to text: 36.8, 69.8, 84.4, 2.0
2019-02-14 07:00:10,749 : Text to Image: 28.8, 62.9, 79.48, 3.0
2019-02-14 07:00:11,676 : Image to text: 35.9, 70.0, 83.6, 3.0
2019-02-14 07:00:12,457 : Text to Image: 29.68, 65.38, 80.16, 3.0
2019-02-14 07:00:13,383 : Image to text: 36.9, 69.2, 83.5, 2.0
2019-02-14 07:00:14,160 : Text to Image: 29.76, 65.12, 80.44, 3.0
2019-02-14 07:00:15,093 : Image to text: 36.3, 70.3, 83.4, 3.0
2019-02-14 07:00:15,857 : Text to Image: 29.16, 64.3, 79.52, 3.0
2019-02-14 07:00:15,857 : Dev mean Text to Image: 29.46, 64.616, 80.048, 3.0
2019-02-14 07:00:15,857 : Dev mean Image to text: 36.239999999999995, 69.94, 83.5, 2.6
2019-02-14 07:00:24,399 : 
Test scores | Image to text:             36.8, 70.96000000000001, 83.1, 2.0
2019-02-14 07:00:24,399 : Test scores | Text to image:             29.0, 64.228, 79.592, 3.1999999999999997

2019-02-14 07:00:24,492 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 07:00:24,860 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 07:00:25,500 : loading BERT model bert-base-uncased
2019-02-14 07:00:25,500 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:00:25,532 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:00:25,532 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9m_ra5py
2019-02-14 07:00:27,968 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:00:29,479 : Computing embeddings for train/dev/test
2019-02-14 07:02:47,517 : Computed embeddings
2019-02-14 07:02:47,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:03:38,753 : [('reg:1e-05', 94.5), ('reg:0.0001', 92.78), ('reg:0.001', 89.1), ('reg:0.01', 83.03)]
2019-02-14 07:03:38,753 : Validation : best param found is reg = 1e-05 with score             94.5
2019-02-14 07:03:38,754 : Evaluating...
2019-02-14 07:03:56,455 : 
Dev acc : 94.5 Test acc : 93.8 for LENGTH classification

2019-02-14 07:03:56,456 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 07:03:56,831 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 07:03:56,876 : loading BERT model bert-base-uncased
2019-02-14 07:03:56,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:03:56,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:03:56,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1jfidc31
2019-02-14 07:03:59,345 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:04:00,831 : Computing embeddings for train/dev/test
2019-02-14 07:06:07,509 : Computed embeddings
2019-02-14 07:06:07,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:07:12,106 : [('reg:1e-05', 82.58), ('reg:0.0001', 45.23), ('reg:0.001', 2.87), ('reg:0.01', 0.86)]
2019-02-14 07:07:12,106 : Validation : best param found is reg = 1e-05 with score             82.58
2019-02-14 07:07:12,106 : Evaluating...
2019-02-14 07:07:31,282 : 
Dev acc : 82.6 Test acc : 83.2 for WORDCONTENT classification

2019-02-14 07:07:31,284 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 07:07:31,599 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 07:07:31,671 : loading BERT model bert-base-uncased
2019-02-14 07:07:31,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:07:31,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:07:31,695 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwv3ndpr2
2019-02-14 07:07:34,138 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:07:35,634 : Computing embeddings for train/dev/test
2019-02-14 07:09:19,420 : Computed embeddings
2019-02-14 07:09:19,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:10:13,728 : [('reg:1e-05', 35.72), ('reg:0.0001', 35.26), ('reg:0.001', 33.49), ('reg:0.01', 27.99)]
2019-02-14 07:10:13,728 : Validation : best param found is reg = 1e-05 with score             35.72
2019-02-14 07:10:13,728 : Evaluating...
2019-02-14 07:10:28,602 : 
Dev acc : 35.7 Test acc : 36.4 for DEPTH classification

2019-02-14 07:10:28,603 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 07:10:28,995 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 07:10:29,058 : loading BERT model bert-base-uncased
2019-02-14 07:10:29,058 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:10:29,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:10:29,085 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9hba6cqc
2019-02-14 07:10:31,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:10:33,001 : Computing embeddings for train/dev/test
2019-02-14 07:12:13,053 : Computed embeddings
2019-02-14 07:12:13,053 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:12:49,278 : [('reg:1e-05', 75.67), ('reg:0.0001', 68.54), ('reg:0.001', 61.76), ('reg:0.01', 47.36)]
2019-02-14 07:12:49,278 : Validation : best param found is reg = 1e-05 with score             75.67
2019-02-14 07:12:49,278 : Evaluating...
2019-02-14 07:13:05,760 : 
Dev acc : 75.7 Test acc : 76.2 for TOPCONSTITUENTS classification

2019-02-14 07:13:05,761 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 07:13:06,120 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 07:13:06,187 : loading BERT model bert-base-uncased
2019-02-14 07:13:06,187 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:13:06,310 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:13:06,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxnc_1vui
2019-02-14 07:13:08,751 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:13:10,212 : Computing embeddings for train/dev/test
2019-02-14 07:15:11,888 : Computed embeddings
2019-02-14 07:15:11,888 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:15:45,199 : [('reg:1e-05', 83.06), ('reg:0.0001', 83.2), ('reg:0.001', 82.84), ('reg:0.01', 80.96)]
2019-02-14 07:15:45,199 : Validation : best param found is reg = 0.0001 with score             83.2
2019-02-14 07:15:45,199 : Evaluating...
2019-02-14 07:16:01,181 : 
Dev acc : 83.2 Test acc : 82.3 for BIGRAMSHIFT classification

2019-02-14 07:16:01,182 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 07:16:01,763 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 07:16:01,829 : loading BERT model bert-base-uncased
2019-02-14 07:16:01,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:16:01,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:16:01,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0c6zeumw
2019-02-14 07:16:04,300 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:16:05,753 : Computing embeddings for train/dev/test
2019-02-14 07:18:09,961 : Computed embeddings
2019-02-14 07:18:09,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:18:40,931 : [('reg:1e-05', 88.76), ('reg:0.0001', 88.85), ('reg:0.001', 88.86), ('reg:0.01', 87.95)]
2019-02-14 07:18:40,932 : Validation : best param found is reg = 0.001 with score             88.86
2019-02-14 07:18:40,932 : Evaluating...
2019-02-14 07:18:54,583 : 
Dev acc : 88.9 Test acc : 87.5 for TENSE classification

2019-02-14 07:18:54,584 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 07:18:54,982 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 07:18:55,045 : loading BERT model bert-base-uncased
2019-02-14 07:18:55,045 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:18:55,166 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:18:55,166 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpruzh_38w
2019-02-14 07:18:57,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:18:59,054 : Computing embeddings for train/dev/test
2019-02-14 07:21:09,968 : Computed embeddings
2019-02-14 07:21:09,968 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:21:38,630 : [('reg:1e-05', 83.22), ('reg:0.0001', 83.13), ('reg:0.001', 83.09), ('reg:0.01', 79.39)]
2019-02-14 07:21:38,630 : Validation : best param found is reg = 1e-05 with score             83.22
2019-02-14 07:21:38,630 : Evaluating...
2019-02-14 07:21:50,073 : 
Dev acc : 83.2 Test acc : 82.0 for SUBJNUMBER classification

2019-02-14 07:21:50,074 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 07:21:50,667 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 07:21:50,735 : loading BERT model bert-base-uncased
2019-02-14 07:21:50,735 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:21:50,764 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:21:50,764 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptrdxhwcw
2019-02-14 07:21:53,207 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:21:54,654 : Computing embeddings for train/dev/test
2019-02-14 07:24:04,689 : Computed embeddings
2019-02-14 07:24:04,689 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:24:37,882 : [('reg:1e-05', 81.83), ('reg:0.0001', 81.9), ('reg:0.001', 82.14), ('reg:0.01', 81.05)]
2019-02-14 07:24:37,882 : Validation : best param found is reg = 0.001 with score             82.14
2019-02-14 07:24:37,883 : Evaluating...
2019-02-14 07:24:49,008 : 
Dev acc : 82.1 Test acc : 83.2 for OBJNUMBER classification

2019-02-14 07:24:49,009 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 07:24:49,581 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 07:24:49,650 : loading BERT model bert-base-uncased
2019-02-14 07:24:49,651 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:24:49,680 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:24:49,680 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn2j1zoxp
2019-02-14 07:24:52,121 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:24:53,551 : Computing embeddings for train/dev/test
2019-02-14 07:27:22,015 : Computed embeddings
2019-02-14 07:27:22,015 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:27:54,771 : [('reg:1e-05', 55.27), ('reg:0.0001', 55.36), ('reg:0.001', 55.43), ('reg:0.01', 54.08)]
2019-02-14 07:27:54,771 : Validation : best param found is reg = 0.001 with score             55.43
2019-02-14 07:27:54,771 : Evaluating...
2019-02-14 07:28:02,912 : 
Dev acc : 55.4 Test acc : 54.7 for ODDMANOUT classification

2019-02-14 07:28:02,913 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 07:28:03,346 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 07:28:03,422 : loading BERT model bert-base-uncased
2019-02-14 07:28:03,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:28:03,454 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:28:03,454 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo1ae1stk
2019-02-14 07:28:05,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:28:07,384 : Computing embeddings for train/dev/test
2019-02-14 07:30:34,089 : Computed embeddings
2019-02-14 07:30:34,090 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:31:12,328 : [('reg:1e-05', 58.29), ('reg:0.0001', 58.25), ('reg:0.001', 56.99), ('reg:0.01', 53.33)]
2019-02-14 07:31:12,329 : Validation : best param found is reg = 1e-05 with score             58.29
2019-02-14 07:31:12,329 : Evaluating...
2019-02-14 07:31:25,277 : 
Dev acc : 58.3 Test acc : 57.5 for COORDINATIONINVERSION classification

2019-02-14 07:31:25,279 : total results: {'STS12': {'MSRpar': {'pearson': (0.4122456455722078, 3.913682182910535e-32), 'spearman': SpearmanrResult(correlation=0.44948434961678857, pvalue=1.417714229712767e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6124712561814787, 2.0158885430663785e-78), 'spearman': SpearmanrResult(correlation=0.6164935672784585, pvalue=1.0271103230884742e-79), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49642057409684104, 6.2422988180080164e-30), 'spearman': SpearmanrResult(correlation=0.5871412116024212, pvalue=7.120809986628736e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6903378428222363, 3.0158210394227256e-107), 'spearman': SpearmanrResult(correlation=0.6824265845644653, pvalue=6.606087513374995e-104), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5817961432689654, 1.6304216984223552e-37), 'spearman': SpearmanrResult(correlation=0.5174057064265033, pvalue=1.0599003884416308e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5586542923883459, 'wmean': 0.5618676843972682}, 'spearman': {'mean': 0.5705902838977274, 'wmean': 0.5750469977749261}}}, 'STS13': {'FNWN': {'pearson': (0.4316507453480641, 5.598682753760639e-10), 'spearman': SpearmanrResult(correlation=0.4437471917139449, pvalue=1.6023606043777242e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6611246440338685, 2.037999244436058e-95), 'spearman': SpearmanrResult(correlation=0.6387417278151295, pvalue=3.2320020256014613e-87), 'nsamples': 750}, 'OnWN': {'pearson': (0.5134248076237046, 4.708672346249424e-39), 'spearman': SpearmanrResult(correlation=0.5429412723938822, pvalue=2.4962400460884943e-44), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5354000656685457, 'wmean': 0.5769711939820559}, 'spearman': {'mean': 0.5418100639743189, 'wmean': 0.5783430459388337}}}, 'STS14': {'deft-forum': {'pearson': (0.3486173657342487, 2.6386129656458704e-14), 'spearman': SpearmanrResult(correlation=0.3706453066145085, pvalue=4.2173775416606534e-16), 'nsamples': 450}, 'deft-news': {'pearson': (0.7287466349706161, 6.244358588633391e-51), 'spearman': SpearmanrResult(correlation=0.6947878758913093, pvalue=1.4619414683567465e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6191869945266408, 1.365775089146611e-80), 'spearman': SpearmanrResult(correlation=0.5769139131989853, pvalue=9.270785723823293e-68), 'nsamples': 750}, 'images': {'pearson': (0.6201959076933803, 6.381981787844762e-81), 'spearman': SpearmanrResult(correlation=0.6103094472889243, pvalue=9.809603321409968e-78), 'nsamples': 750}, 'OnWN': {'pearson': (0.6597832033785311, 6.615405102783912e-95), 'spearman': SpearmanrResult(correlation=0.7034703483671217, pvalue=4.8935180133109334e-113), 'nsamples': 750}, 'tweet-news': {'pearson': (0.660140886254614, 4.835784131323053e-95), 'spearman': SpearmanrResult(correlation=0.6312319533242494, pvalue=1.2894139388531029e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6061118320930051, 'wmean': 0.6119952130563924}, 'spearman': {'mean': 0.5978931407808498, 'wmean': 0.6044455993009019}}}, 'STS15': {'answers-forums': {'pearson': (0.5329998576134684, 6.56785346123051e-29), 'spearman': SpearmanrResult(correlation=0.5290520583234161, pvalue=1.9659201019738108e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.7067028451159878, 1.6414362694803704e-114), 'spearman': SpearmanrResult(correlation=0.7099606083283496, pvalue=5.113969434638802e-116), 'nsamples': 750}, 'belief': {'pearson': (0.6371198478202044, 4.282945720000134e-44), 'spearman': SpearmanrResult(correlation=0.6524788349262951, pvalue=7.49947698664825e-47), 'nsamples': 375}, 'headlines': {'pearson': (0.6794238809261226, 1.1488103416518632e-102), 'spearman': SpearmanrResult(correlation=0.6691812858040742, pvalue=1.519101160478686e-98), 'nsamples': 750}, 'images': {'pearson': (0.7290030236103133, 2.9236125248392767e-125), 'spearman': SpearmanrResult(correlation=0.7400708065099588, pvalue=5.283412422472168e-131), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6570498910172193, 'wmean': 0.675047400592315}, 'spearman': {'mean': 0.6601487187784187, 'wmean': 0.6774945368168095}}}, 'STS16': {'answer-answer': {'pearson': (0.482130172613554, 3.4327851414151413e-16), 'spearman': SpearmanrResult(correlation=0.5125017445871457, pvalue=2.054595247973864e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.692121084301449, 7.749956558181332e-37), 'spearman': SpearmanrResult(correlation=0.6953879439364248, pvalue=2.621679372764277e-37), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7367920637883043, 1.255773939825235e-40), 'spearman': SpearmanrResult(correlation=0.7444714927716893, pvalue=7.029288584890972e-42), 'nsamples': 230}, 'postediting': {'pearson': (0.7986098287909918, 2.76851707150746e-55), 'spearman': SpearmanrResult(correlation=0.8268180111880888, pvalue=2.066868467502132e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.41685188052738614, 3.4218850489924195e-10), 'spearman': SpearmanrResult(correlation=0.4174292728933146, pvalue=3.2172162055461497e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.625301006004337, 'wmean': 0.6292109863081276}, 'spearman': {'mean': 0.6393216930753326, 'wmean': 0.6437957818443422}}}, 'MR': {'devacc': 75.55, 'acc': 74.53, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.36, 'acc': 78.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.54, 'acc': 87.72, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.57, 'acc': 93.16, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.08, 'acc': 79.9, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.05, 'acc': 41.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.12, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.43, 'acc': 71.54, 'f1': 77.3, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.2, 'acc': 78.89, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8214254889327501, 'pearson': 0.8254965591046618, 'spearman': 0.7533282709130273, 'mse': 0.32435933615366774, 'yhat': array([3.61022094, 4.2984634 , 1.16710751, ..., 3.11644899, 4.49683737,        4.31626656]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7541006643513408, 'pearson': 0.6870015019307263, 'spearman': 0.6822661773333344, 'mse': 1.3792724835892651, 'yhat': array([1.2243143 , 1.49555881, 1.69258163, ..., 3.8669485 , 3.9621369 ,        3.56594004]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.16, 'acc': 68.14, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 365.03200000000004, 'acc': [(36.8, 70.96000000000001, 83.1, 2.0), (29.0, 64.228, 79.592, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 94.5, 'acc': 93.77, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 82.58, 'acc': 83.23, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 35.72, 'acc': 36.38, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.67, 'acc': 76.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.2, 'acc': 82.26, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.86, 'acc': 87.48, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.22, 'acc': 81.97, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.14, 'acc': 83.16, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.43, 'acc': 54.74, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.29, 'acc': 57.51, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 07:31:25,279 : STS12 p=0.5619, STS12 s=0.5750, STS13 p=0.5770, STS13 s=0.5783, STS14 p=0.6120, STS14 s=0.6044, STS15 p=0.6750, STS15 s=0.6775, STS 16 p=0.6292, STS16 s=0.6438, STS B p=0.6870, STS B s=0.6823, STS B m=1.3793, SICK-R p=0.8255, SICK-R s=0.7533, SICK-P m=0.3244
2019-02-14 07:31:25,279 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 07:31:25,279 : 0.5619,0.5750,0.5770,0.5783,0.6120,0.6044,0.6750,0.6775,0.6292,0.6438,0.6870,0.6823,1.3793,0.8255,0.7533,0.3244
2019-02-14 07:31:25,280 : MR=74.53, CR=78.57, SUBJ=93.16, MPQA=87.72, SST-B=79.90, SST-F=41.58, TREC=88.40, SICK-E=78.89, SNLI=68.14, MRPC=71.54, MRPC f=77.30
2019-02-14 07:31:25,280 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 07:31:25,280 : 74.53,78.57,93.16,87.72,79.90,41.58,88.40,78.89,68.14,71.54,77.30
2019-02-14 07:31:25,280 : COCO r1i2t=36.80, COCO r5i2t=70.96, COCO r10i2t=83.10, COCO medr_i2t=2.00, COCO r1t2i=29.00, COCO r5t2i=64.23, COCO r10t2i=79.59, COCO medr_t2i=3.20
2019-02-14 07:31:25,280 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 07:31:25,280 : 36.80,70.96,83.10,2.00,29.00,64.23,79.59,3.20
2019-02-14 07:31:25,280 : SentLen=93.77, WC=83.23, TreeDepth=36.38, TopConst=76.19, BShift=82.26, Tense=87.48, SubjNum=81.97, ObjNum=83.16, SOMO=54.74, CoordInv=57.51, average=73.67
2019-02-14 07:31:25,280 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 07:31:25,280 : 93.77,83.23,36.38,76.19,82.26,87.48,81.97,83.16,54.74,57.51,73.67
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,280 : layer 4
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,280 : ********************************************************************************
2019-02-14 07:31:25,370 : ***** Transfer task : STS12 *****


2019-02-14 07:31:25,383 : loading BERT model bert-base-uncased
2019-02-14 07:31:25,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:25,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:25,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp95fh09_1
2019-02-14 07:31:27,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:31,275 : MSRpar : pearson = 0.4053, spearman = 0.4401
2019-02-14 07:31:32,728 : MSRvid : pearson = 0.5781, spearman = 0.5837
2019-02-14 07:31:33,754 : SMTeuroparl : pearson = 0.5028, spearman = 0.5862
2019-02-14 07:31:35,423 : surprise.OnWN : pearson = 0.6740, spearman = 0.6744
2019-02-14 07:31:36,367 : surprise.SMTnews : pearson = 0.6073, spearman = 0.5229
2019-02-14 07:31:36,367 : ALL (weighted average) : Pearson = 0.5522,             Spearman = 0.5635
2019-02-14 07:31:36,368 : ALL (average) : Pearson = 0.5535,             Spearman = 0.5615

2019-02-14 07:31:36,368 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 07:31:36,378 : loading BERT model bert-base-uncased
2019-02-14 07:31:36,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:36,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:36,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxn2zn8_8
2019-02-14 07:31:38,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:40,758 : FNWN : pearson = 0.3721, spearman = 0.3791
2019-02-14 07:31:41,545 : headlines : pearson = 0.6481, spearman = 0.6271
2019-02-14 07:31:42,783 : OnWN : pearson = 0.4799, spearman = 0.5121
2019-02-14 07:31:42,783 : ALL (weighted average) : Pearson = 0.5504,             Spearman = 0.5528
2019-02-14 07:31:42,783 : ALL (average) : Pearson = 0.5000,             Spearman = 0.5061

2019-02-14 07:31:42,783 : ***** Transfer task : STS14 *****


2019-02-14 07:31:42,800 : loading BERT model bert-base-uncased
2019-02-14 07:31:42,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:42,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:42,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa7oo_oys
2019-02-14 07:31:45,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:47,223 : deft-forum : pearson = 0.3297, spearman = 0.3628
2019-02-14 07:31:47,852 : deft-news : pearson = 0.7316, spearman = 0.7011
2019-02-14 07:31:48,724 : headlines : pearson = 0.6097, spearman = 0.5688
2019-02-14 07:31:49,560 : images : pearson = 0.5920, spearman = 0.5876
2019-02-14 07:31:50,534 : OnWN : pearson = 0.6403, spearman = 0.6846
2019-02-14 07:31:52,395 : tweet-news : pearson = 0.6551, spearman = 0.6215
2019-02-14 07:31:52,395 : ALL (weighted average) : Pearson = 0.5975,             Spearman = 0.5921
2019-02-14 07:31:52,395 : ALL (average) : Pearson = 0.5931,             Spearman = 0.5877

2019-02-14 07:31:52,395 : ***** Transfer task : STS15 *****


2019-02-14 07:31:52,429 : loading BERT model bert-base-uncased
2019-02-14 07:31:52,429 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:52,447 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:52,447 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuufsvc63
2019-02-14 07:31:54,919 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:57,085 : answers-forums : pearson = 0.5174, spearman = 0.5263
2019-02-14 07:31:58,605 : answers-students : pearson = 0.6866, spearman = 0.6903
2019-02-14 07:31:59,795 : belief : pearson = 0.6272, spearman = 0.6531
2019-02-14 07:32:01,820 : headlines : pearson = 0.6671, spearman = 0.6585
2019-02-14 07:32:03,851 : images : pearson = 0.7074, spearman = 0.7206
2019-02-14 07:32:03,851 : ALL (weighted average) : Pearson = 0.6584,             Spearman = 0.6648
2019-02-14 07:32:03,851 : ALL (average) : Pearson = 0.6412,             Spearman = 0.6498

2019-02-14 07:32:03,851 : ***** Transfer task : STS16 *****


2019-02-14 07:32:03,919 : loading BERT model bert-base-uncased
2019-02-14 07:32:03,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:32:03,937 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:32:03,937 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvsvou5zp
2019-02-14 07:32:06,382 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:32:08,830 : answer-answer : pearson = 0.4763, spearman = 0.5058
2019-02-14 07:32:09,739 : headlines : pearson = 0.6773, spearman = 0.6804
2019-02-14 07:32:10,731 : plagiarism : pearson = 0.7214, spearman = 0.7256
2019-02-14 07:32:12,021 : postediting : pearson = 0.7849, spearman = 0.8222
2019-02-14 07:32:12,596 : question-question : pearson = 0.3540, spearman = 0.3487
2019-02-14 07:32:12,596 : ALL (weighted average) : Pearson = 0.6080,             Spearman = 0.6225
2019-02-14 07:32:12,596 : ALL (average) : Pearson = 0.6028,             Spearman = 0.6165

2019-02-14 07:32:12,596 : ***** Transfer task : MR *****


2019-02-14 07:32:12,615 : loading BERT model bert-base-uncased
2019-02-14 07:32:12,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:32:12,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:32:12,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9rr0odf3
2019-02-14 07:32:15,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:32:16,585 : Generating sentence embeddings
2019-02-14 07:32:31,443 : Generated sentence embeddings
2019-02-14 07:32:31,443 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:32:42,447 : Best param found at split 1: l2reg = 0.001                 with score 76.17
2019-02-14 07:32:53,479 : Best param found at split 2: l2reg = 0.001                 with score 76.11
2019-02-14 07:33:06,451 : Best param found at split 3: l2reg = 0.001                 with score 76.72
2019-02-14 07:33:20,400 : Best param found at split 4: l2reg = 1e-05                 with score 76.05
2019-02-14 07:33:35,732 : Best param found at split 5: l2reg = 0.001                 with score 76.38
2019-02-14 07:33:37,128 : Dev acc : 76.29 Test acc : 75.63

2019-02-14 07:33:37,129 : ***** Transfer task : CR *****


2019-02-14 07:33:37,136 : loading BERT model bert-base-uncased
2019-02-14 07:33:37,137 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:33:37,158 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:33:37,158 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk3_q1036
2019-02-14 07:33:39,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:33:41,041 : Generating sentence embeddings
2019-02-14 07:33:45,217 : Generated sentence embeddings
2019-02-14 07:33:45,217 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:33:50,041 : Best param found at split 1: l2reg = 1e-05                 with score 80.33
2019-02-14 07:33:54,884 : Best param found at split 2: l2reg = 0.001                 with score 80.92
2019-02-14 07:34:00,016 : Best param found at split 3: l2reg = 1e-05                 with score 80.86
2019-02-14 07:34:03,870 : Best param found at split 4: l2reg = 0.001                 with score 81.03
2019-02-14 07:34:10,533 : Best param found at split 5: l2reg = 0.0001                 with score 80.54
2019-02-14 07:34:10,776 : Dev acc : 80.74 Test acc : 80.19

2019-02-14 07:34:10,777 : ***** Transfer task : MPQA *****


2019-02-14 07:34:10,782 : loading BERT model bert-base-uncased
2019-02-14 07:34:10,783 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:34:10,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:34:10,802 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcdj_767g
2019-02-14 07:34:13,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:34:14,686 : Generating sentence embeddings
2019-02-14 07:34:19,846 : Generated sentence embeddings
2019-02-14 07:34:19,847 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:34:33,017 : Best param found at split 1: l2reg = 0.001                 with score 87.87
2019-02-14 07:34:46,839 : Best param found at split 2: l2reg = 0.001                 with score 87.8
2019-02-14 07:35:01,481 : Best param found at split 3: l2reg = 0.001                 with score 88.39
2019-02-14 07:35:14,985 : Best param found at split 4: l2reg = 0.001                 with score 88.17
2019-02-14 07:35:36,201 : Best param found at split 5: l2reg = 0.01                 with score 87.93
2019-02-14 07:35:37,132 : Dev acc : 88.03 Test acc : 87.36

2019-02-14 07:35:37,133 : ***** Transfer task : SUBJ *****


2019-02-14 07:35:37,149 : loading BERT model bert-base-uncased
2019-02-14 07:35:37,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:35:37,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:35:37,169 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa9umorct
2019-02-14 07:35:39,615 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:35:41,042 : Generating sentence embeddings
2019-02-14 07:35:55,542 : Generated sentence embeddings
2019-02-14 07:35:55,543 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:36:06,709 : Best param found at split 1: l2reg = 0.001                 with score 94.16
2019-02-14 07:36:20,017 : Best param found at split 2: l2reg = 0.001                 with score 94.4
2019-02-14 07:36:30,364 : Best param found at split 3: l2reg = 0.0001                 with score 94.07
2019-02-14 07:36:43,546 : Best param found at split 4: l2reg = 0.001                 with score 94.4
2019-02-14 07:36:55,246 : Best param found at split 5: l2reg = 0.001                 with score 93.98
2019-02-14 07:36:56,450 : Dev acc : 94.2 Test acc : 93.85

2019-02-14 07:36:56,451 : ***** Transfer task : SST Binary classification *****


2019-02-14 07:36:56,578 : loading BERT model bert-base-uncased
2019-02-14 07:36:56,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:36:56,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:36:56,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsu2sk6q2
2019-02-14 07:36:59,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:37:00,499 : Computing embedding for train
2019-02-14 07:38:14,597 : Computed train embeddings
2019-02-14 07:38:14,597 : Computing embedding for dev
2019-02-14 07:38:15,678 : Computed dev embeddings
2019-02-14 07:38:15,678 : Computing embedding for test
2019-02-14 07:38:17,906 : Computed test embeddings
2019-02-14 07:38:17,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:38:49,240 : [('reg:1e-05', 81.65), ('reg:0.0001', 81.42), ('reg:0.001', 81.65), ('reg:0.01', 79.82)]
2019-02-14 07:38:49,240 : Validation : best param found is reg = 1e-05 with score             81.65
2019-02-14 07:38:49,241 : Evaluating...
2019-02-14 07:38:54,162 : 
Dev acc : 81.65 Test acc : 80.18 for             SST Binary classification

2019-02-14 07:38:54,163 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 07:38:54,213 : loading BERT model bert-base-uncased
2019-02-14 07:38:54,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:38:54,235 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:38:54,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphmn1f0zi
2019-02-14 07:38:56,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:38:58,125 : Computing embedding for train
2019-02-14 07:39:09,393 : Computed train embeddings
2019-02-14 07:39:09,393 : Computing embedding for dev
2019-02-14 07:39:10,754 : Computed dev embeddings
2019-02-14 07:39:10,754 : Computing embedding for test
2019-02-14 07:39:13,468 : Computed test embeddings
2019-02-14 07:39:13,468 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:39:15,709 : [('reg:1e-05', 42.14), ('reg:0.0001', 42.14), ('reg:0.001', 42.05), ('reg:0.01', 41.96)]
2019-02-14 07:39:15,709 : Validation : best param found is reg = 1e-05 with score             42.14
2019-02-14 07:39:15,709 : Evaluating...
2019-02-14 07:39:16,414 : 
Dev acc : 42.14 Test acc : 42.58 for             SST Fine-Grained classification

2019-02-14 07:39:16,415 : ***** Transfer task : TREC *****


2019-02-14 07:39:16,427 : loading BERT model bert-base-uncased
2019-02-14 07:39:16,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:39:16,447 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:39:16,447 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5en9ems
2019-02-14 07:39:18,900 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:39:26,925 : Computed train embeddings
2019-02-14 07:39:27,538 : Computed test embeddings
2019-02-14 07:39:27,538 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 07:39:41,366 : [('reg:1e-05', 82.06), ('reg:0.0001', 81.86), ('reg:0.001', 80.46), ('reg:0.01', 72.67)]
2019-02-14 07:39:41,366 : Cross-validation : best param found is reg = 1e-05             with score 82.06
2019-02-14 07:39:41,366 : Evaluating...
2019-02-14 07:39:41,876 : 
Dev acc : 82.06 Test acc : 90.8             for TREC

2019-02-14 07:39:41,876 : ***** Transfer task : MRPC *****


2019-02-14 07:39:41,897 : loading BERT model bert-base-uncased
2019-02-14 07:39:41,897 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:39:41,919 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:39:41,920 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdrg8o34d
2019-02-14 07:39:44,370 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:39:45,776 : Computing embedding for train
2019-02-14 07:39:56,247 : Computed train embeddings
2019-02-14 07:39:56,247 : Computing embedding for test
2019-02-14 07:40:01,355 : Computed test embeddings
2019-02-14 07:40:01,371 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 07:40:10,196 : [('reg:1e-05', 73.6), ('reg:0.0001', 73.26), ('reg:0.001', 73.45), ('reg:0.01', 73.6)]
2019-02-14 07:40:10,196 : Cross-validation : best param found is reg = 1e-05             with score 73.6
2019-02-14 07:40:10,196 : Evaluating...
2019-02-14 07:40:10,556 : Dev acc : 73.6 Test acc 74.09; Test F1 80.4 for MRPC.

2019-02-14 07:40:10,556 : ***** Transfer task : SICK-Entailment*****


2019-02-14 07:40:10,619 : loading BERT model bert-base-uncased
2019-02-14 07:40:10,619 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:40:10,639 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:40:10,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1b_l9uf5
2019-02-14 07:40:13,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:40:14,592 : Computing embedding for train
2019-02-14 07:40:22,834 : Computed train embeddings
2019-02-14 07:40:22,835 : Computing embedding for dev
2019-02-14 07:40:23,834 : Computed dev embeddings
2019-02-14 07:40:23,834 : Computing embedding for test
2019-02-14 07:40:32,490 : Computed test embeddings
2019-02-14 07:40:32,518 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:40:34,229 : [('reg:1e-05', 78.8), ('reg:0.0001', 79.0), ('reg:0.001', 78.6), ('reg:0.01', 71.0)]
2019-02-14 07:40:34,229 : Validation : best param found is reg = 0.0001 with score             79.0
2019-02-14 07:40:34,229 : Evaluating...
2019-02-14 07:40:34,726 : 
Dev acc : 79.0 Test acc : 79.32 for                        SICK entailment

2019-02-14 07:40:34,727 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 07:40:34,753 : loading BERT model bert-base-uncased
2019-02-14 07:40:34,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:40:34,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:40:34,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6_29e4_8
2019-02-14 07:40:37,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:40:38,697 : Computing embedding for train
2019-02-14 07:40:45,116 : Computed train embeddings
2019-02-14 07:40:45,116 : Computing embedding for dev
2019-02-14 07:40:45,930 : Computed dev embeddings
2019-02-14 07:40:45,930 : Computing embedding for test
2019-02-14 07:40:52,820 : Computed test embeddings
2019-02-14 07:41:12,961 : Dev : Pearson 0.8081018837451983
2019-02-14 07:41:12,961 : Test : Pearson 0.8210946306785972 Spearman 0.7497532476361182 MSE 0.33389116918705375                        for SICK Relatedness

2019-02-14 07:41:12,962 : 

***** Transfer task : STSBenchmark*****


2019-02-14 07:41:13,001 : loading BERT model bert-base-uncased
2019-02-14 07:41:13,001 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:41:13,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:41:13,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp90z0n032
2019-02-14 07:41:15,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:41:16,895 : Computing embedding for train
2019-02-14 07:41:26,694 : Computed train embeddings
2019-02-14 07:41:26,694 : Computing embedding for dev
2019-02-14 07:41:29,542 : Computed dev embeddings
2019-02-14 07:41:29,542 : Computing embedding for test
2019-02-14 07:41:31,900 : Computed test embeddings
2019-02-14 07:41:58,549 : Dev : Pearson 0.7464539187484868
2019-02-14 07:41:58,549 : Test : Pearson 0.6798474619844024 Spearman 0.6748435995029207 MSE 1.4105237914184845                        for SICK Relatedness

2019-02-14 07:41:58,549 : ***** Transfer task : SNLI Entailment*****


2019-02-14 07:42:03,261 : loading BERT model bert-base-uncased
2019-02-14 07:42:03,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:42:03,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:42:03,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ur0aegm
2019-02-14 07:42:05,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:42:07,439 : PROGRESS (encoding): 0.00%
2019-02-14 07:43:50,280 : PROGRESS (encoding): 14.56%
2019-02-14 07:45:46,067 : PROGRESS (encoding): 29.12%
2019-02-14 07:47:59,337 : PROGRESS (encoding): 43.69%
2019-02-14 07:50:20,907 : PROGRESS (encoding): 58.25%
2019-02-14 07:52:53,045 : PROGRESS (encoding): 72.81%
2019-02-14 07:55:27,770 : PROGRESS (encoding): 87.37%
2019-02-14 07:58:00,020 : PROGRESS (encoding): 0.00%
2019-02-14 07:58:22,293 : PROGRESS (encoding): 0.00%
2019-02-14 07:58:41,439 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:59:50,439 : [('reg:1e-09', 61.2)]
2019-02-14 07:59:50,439 : Validation : best param found is reg = 1e-09 with score             61.2
2019-02-14 07:59:50,439 : Evaluating...
2019-02-14 08:00:58,737 : Dev acc : 61.2 Test acc : 61.48 for SNLI

2019-02-14 08:00:58,738 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 08:01:07,430 : loading BERT model bert-base-uncased
2019-02-14 08:01:07,430 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 08:01:07,475 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 08:01:07,475 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp72avmc_p
2019-02-14 08:01:09,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 08:01:11,340 : Computing embedding for train
2019-02-14 08:11:12,592 : Computed train embeddings
2019-02-14 08:11:12,592 : Computing embedding for dev
2019-02-14 08:11:44,777 : Computed dev embeddings
2019-02-14 08:11:44,777 : Computing embedding for test
2019-02-14 08:12:17,592 : Computed test embeddings
2019-02-14 08:12:17,610 : prepare data
2019-02-14 08:12:17,672 : start epoch
2019-02-14 08:13:02,726 : samples : 64000
2019-02-14 08:13:15,629 : Image to text: 8.38, 24.06, 35.18, 22.0
2019-02-14 08:13:26,109 : Text to Image: 6.368, 20.308, 30.988, 25.0
2019-02-14 08:14:11,130 : samples : 128000
2019-02-14 08:14:23,998 : Image to text: 9.14, 25.94, 37.56, 18.0
2019-02-14 08:14:34,414 : Text to Image: 7.04, 22.064, 33.308, 22.0
2019-02-14 08:15:19,088 : samples : 192000
2019-02-14 08:15:32,010 : Image to text: 10.46, 27.32, 39.28, 17.0
2019-02-14 08:15:41,052 : Text to Image: 7.996, 23.92, 34.988, 21.0
2019-02-14 08:16:25,477 : samples : 256000
2019-02-14 08:16:35,744 : Image to text: 9.98, 26.9, 38.92, 18.0
2019-02-14 08:16:43,134 : Text to Image: 8.288, 24.456, 35.82, 20.0
2019-02-14 08:17:24,555 : samples : 320000
2019-02-14 08:17:34,839 : Image to text: 10.54, 28.18, 40.56, 16.0
2019-02-14 08:17:42,229 : Text to Image: 8.428, 25.268, 36.36, 20.0
2019-02-14 08:18:22,706 : samples : 384000
2019-02-14 08:18:32,998 : Image to text: 10.44, 29.92, 41.72, 15.0
2019-02-14 08:18:40,405 : Text to Image: 9.056, 25.888, 37.716, 18.0
2019-02-14 08:19:21,238 : samples : 448000
2019-02-14 08:19:31,520 : Image to text: 10.62, 28.36, 41.26, 16.0
2019-02-14 08:19:38,931 : Text to Image: 8.216, 24.668, 36.564, 19.0
2019-02-14 08:20:19,515 : samples : 512000
2019-02-14 08:20:29,787 : Image to text: 11.46, 30.56, 43.16, 14.0
2019-02-14 08:20:37,198 : Text to Image: 9.536, 26.792, 38.508, 18.0
2019-02-14 08:21:11,690 : Epoch 1 finished
2019-02-14 08:21:12,119 : Image to text: 26.3, 60.4, 74.8, 3.0
2019-02-14 08:21:12,431 : Text to Image: 24.2, 56.86, 73.76, 4.0
2019-02-14 08:21:12,858 : Image to text: 27.3, 62.4, 77.5, 3.0
2019-02-14 08:21:13,168 : Text to Image: 23.82, 55.38, 73.08, 4.0
2019-02-14 08:21:13,606 : Image to text: 26.8, 61.2, 75.5, 3.0
2019-02-14 08:21:13,938 : Text to Image: 24.0, 57.0, 73.46, 4.0
2019-02-14 08:21:14,384 : Image to text: 30.3, 62.2, 76.9, 3.0
2019-02-14 08:21:14,713 : Text to Image: 24.04, 56.84, 73.82, 4.0
2019-02-14 08:21:15,142 : Image to text: 28.7, 61.5, 76.1, 3.0
2019-02-14 08:21:15,471 : Text to Image: 23.06, 56.8, 73.36, 4.0
2019-02-14 08:21:15,471 : Dev mean Text to Image: 23.823999999999998, 56.576, 73.496, 4.0
2019-02-14 08:21:15,471 : Dev mean Image to text: 27.880000000000003, 61.540000000000006, 76.16, 3.0
2019-02-14 08:21:15,472 : start epoch
2019-02-14 08:21:59,079 : samples : 64000
2019-02-14 08:22:09,097 : Image to text: 11.52, 31.6, 44.1, 14.0
2019-02-14 08:22:16,415 : Text to Image: 9.748, 27.072, 39.208, 17.0
2019-02-14 08:22:59,744 : samples : 128000
2019-02-14 08:23:09,785 : Image to text: 11.34, 30.0, 42.74, 15.0
2019-02-14 08:23:17,143 : Text to Image: 9.768, 27.468, 39.788, 17.0
2019-02-14 08:24:02,891 : samples : 192000
2019-02-14 08:24:12,922 : Image to text: 11.94, 31.54, 44.14, 14.0
2019-02-14 08:24:20,249 : Text to Image: 9.784, 28.052, 39.816, 17.0
2019-02-14 08:25:00,708 : samples : 256000
2019-02-14 08:25:11,058 : Image to text: 12.28, 32.3, 44.42, 13.0
2019-02-14 08:25:18,513 : Text to Image: 10.132, 28.348, 40.364, 16.0
2019-02-14 08:25:59,391 : samples : 320000
2019-02-14 08:26:09,716 : Image to text: 12.44, 32.62, 45.52, 13.0
2019-02-14 08:26:17,161 : Text to Image: 10.172, 28.276, 40.44, 16.0
2019-02-14 08:26:58,207 : samples : 384000
2019-02-14 08:27:08,531 : Image to text: 12.46, 32.28, 44.42, 14.0
2019-02-14 08:27:15,953 : Text to Image: 10.144, 27.972, 40.136, 16.0
2019-02-14 08:27:56,820 : samples : 448000
2019-02-14 08:28:07,111 : Image to text: 12.4, 33.72, 46.68, 12.0
2019-02-14 08:28:14,557 : Text to Image: 10.368, 28.836, 40.972, 16.0
2019-02-14 08:28:55,337 : samples : 512000
2019-02-14 08:29:05,648 : Image to text: 11.9, 32.72, 45.8, 13.0
2019-02-14 08:29:13,088 : Text to Image: 10.408, 28.28, 40.544, 16.0
2019-02-14 08:29:47,736 : Epoch 2 finished
2019-02-14 08:29:48,166 : Image to text: 28.5, 61.8, 77.8, 3.0
2019-02-14 08:29:48,481 : Text to Image: 25.0, 58.4, 75.9, 4.0
2019-02-14 08:29:48,912 : Image to text: 29.2, 64.9, 79.1, 3.0
2019-02-14 08:29:49,224 : Text to Image: 24.58, 58.3, 74.74, 4.0
2019-02-14 08:29:49,665 : Image to text: 28.3, 64.4, 78.9, 3.0
2019-02-14 08:29:50,001 : Text to Image: 25.4, 59.02, 75.42, 4.0
2019-02-14 08:29:50,455 : Image to text: 30.1, 64.0, 79.3, 3.0
2019-02-14 08:29:50,788 : Text to Image: 24.72, 58.5, 74.58, 4.0
2019-02-14 08:29:51,219 : Image to text: 31.1, 64.1, 77.3, 3.0
2019-02-14 08:29:51,549 : Text to Image: 25.1, 58.4, 74.7, 4.0
2019-02-14 08:29:51,550 : Dev mean Text to Image: 24.96, 58.523999999999994, 75.068, 4.0
2019-02-14 08:29:51,550 : Dev mean Image to text: 29.439999999999998, 63.839999999999996, 78.47999999999999, 3.0
2019-02-14 08:29:51,550 : start epoch
2019-02-14 08:30:32,049 : samples : 64000
2019-02-14 08:30:42,075 : Image to text: 12.78, 33.68, 46.92, 12.0
2019-02-14 08:30:49,385 : Text to Image: 10.576, 28.748, 41.008, 16.0
2019-02-14 08:31:30,790 : samples : 128000
2019-02-14 08:31:40,833 : Image to text: 12.84, 33.28, 46.08, 12.0
2019-02-14 08:31:48,172 : Text to Image: 10.268, 28.496, 40.584, 16.0
2019-02-14 08:32:29,618 : samples : 192000
2019-02-14 08:32:39,652 : Image to text: 13.5, 34.66, 47.8, 12.0
2019-02-14 08:32:46,959 : Text to Image: 10.524, 29.22, 41.84, 15.0
2019-02-14 08:33:27,817 : samples : 256000
2019-02-14 08:33:38,111 : Image to text: 13.76, 34.82, 47.74, 12.0
2019-02-14 08:33:45,535 : Text to Image: 11.068, 29.944, 42.692, 15.0
2019-02-14 08:34:28,166 : samples : 320000
2019-02-14 08:34:38,473 : Image to text: 12.6, 33.24, 45.64, 13.0
2019-02-14 08:34:45,946 : Text to Image: 10.236, 28.748, 41.092, 16.0
2019-02-14 08:35:28,562 : samples : 384000
2019-02-14 08:35:38,894 : Image to text: 12.5, 33.14, 46.28, 13.0
2019-02-14 08:35:46,320 : Text to Image: 10.416, 29.052, 41.108, 16.0
2019-02-14 08:36:27,017 : samples : 448000
2019-02-14 08:36:37,271 : Image to text: 13.06, 34.34, 47.62, 12.0
2019-02-14 08:36:44,715 : Text to Image: 10.872, 29.912, 42.468, 15.0
2019-02-14 08:37:25,303 : samples : 512000
2019-02-14 08:37:35,565 : Image to text: 12.96, 34.86, 48.5, 11.0
2019-02-14 08:37:43,014 : Text to Image: 11.056, 30.204, 42.588, 15.0
2019-02-14 08:38:17,532 : Epoch 3 finished
2019-02-14 08:38:17,962 : Image to text: 29.3, 64.6, 81.2, 3.0
2019-02-14 08:38:18,272 : Text to Image: 26.66, 60.96, 77.54, 4.0
2019-02-14 08:38:18,713 : Image to text: 31.7, 65.4, 80.2, 3.0
2019-02-14 08:38:19,050 : Text to Image: 26.2, 60.36, 76.32, 4.0
2019-02-14 08:38:19,513 : Image to text: 30.1, 65.3, 80.4, 3.0
2019-02-14 08:38:19,848 : Text to Image: 26.78, 61.4, 76.96, 4.0
2019-02-14 08:38:20,303 : Image to text: 33.1, 67.3, 81.1, 3.0
2019-02-14 08:38:20,633 : Text to Image: 26.8, 61.42, 77.6, 4.0
2019-02-14 08:38:21,061 : Image to text: 35.2, 68.2, 80.1, 3.0
2019-02-14 08:38:21,389 : Text to Image: 26.06, 60.42, 76.44, 4.0
2019-02-14 08:38:21,390 : Dev mean Text to Image: 26.5, 60.91199999999999, 76.972, 4.0
2019-02-14 08:38:21,390 : Dev mean Image to text: 31.880000000000003, 66.16, 80.6, 3.0
2019-02-14 08:38:21,390 : start epoch
2019-02-14 08:39:04,318 : samples : 64000
2019-02-14 08:39:14,355 : Image to text: 13.24, 35.1, 48.86, 11.0
2019-02-14 08:39:21,699 : Text to Image: 10.812, 30.08, 42.672, 15.0
2019-02-14 08:40:03,149 : samples : 128000
2019-02-14 08:40:13,209 : Image to text: 13.66, 34.66, 47.9, 11.0
2019-02-14 08:40:20,540 : Text to Image: 11.12, 30.096, 42.668, 15.0
2019-02-14 08:41:02,965 : samples : 192000
2019-02-14 08:41:13,025 : Image to text: 13.5, 34.3, 47.5, 12.0
2019-02-14 08:41:20,343 : Text to Image: 10.776, 29.856, 42.004, 15.0
2019-02-14 08:42:01,336 : samples : 256000
2019-02-14 08:42:11,571 : Image to text: 13.68, 35.08, 48.88, 11.0
2019-02-14 08:42:18,956 : Text to Image: 11.112, 30.464, 43.12, 15.0
2019-02-14 08:43:00,721 : samples : 320000
2019-02-14 08:43:10,946 : Image to text: 13.48, 35.36, 49.3, 11.0
2019-02-14 08:43:18,309 : Text to Image: 11.296, 30.64, 42.872, 15.0
2019-02-14 08:43:59,508 : samples : 384000
2019-02-14 08:44:09,811 : Image to text: 13.54, 36.0, 49.58, 11.0
2019-02-14 08:44:17,226 : Text to Image: 11.28, 30.404, 42.748, 14.0
2019-02-14 08:44:58,576 : samples : 448000
2019-02-14 08:45:08,798 : Image to text: 13.94, 35.68, 50.1, 10.0
2019-02-14 08:45:16,177 : Text to Image: 11.616, 30.824, 43.284, 14.0
2019-02-14 08:45:57,695 : samples : 512000
2019-02-14 08:46:07,915 : Image to text: 13.78, 35.02, 48.6, 11.0
2019-02-14 08:46:15,286 : Text to Image: 11.12, 30.3, 43.168, 15.0
2019-02-14 08:46:51,044 : Epoch 4 finished
2019-02-14 08:46:51,492 : Image to text: 32.4, 66.4, 81.4, 3.0
2019-02-14 08:46:51,824 : Text to Image: 27.02, 61.16, 77.98, 4.0
2019-02-14 08:46:52,277 : Image to text: 33.0, 66.6, 79.7, 3.0
2019-02-14 08:46:52,609 : Text to Image: 26.36, 60.2, 76.54, 4.0
2019-02-14 08:46:53,048 : Image to text: 32.7, 68.3, 80.4, 3.0
2019-02-14 08:46:53,396 : Text to Image: 27.6, 62.28, 78.06, 3.0
2019-02-14 08:46:53,836 : Image to text: 36.0, 69.0, 81.2, 2.0
2019-02-14 08:46:54,167 : Text to Image: 27.62, 61.7, 77.76, 4.0
2019-02-14 08:46:54,612 : Image to text: 34.0, 67.5, 80.6, 3.0
2019-02-14 08:46:54,941 : Text to Image: 26.32, 61.68, 77.34, 4.0
2019-02-14 08:46:54,941 : Dev mean Text to Image: 26.984, 61.403999999999996, 77.536, 3.8
2019-02-14 08:46:54,941 : Dev mean Image to text: 33.62, 67.56, 80.66, 2.8
2019-02-14 08:46:54,942 : start epoch
2019-02-14 08:47:36,772 : samples : 64000
2019-02-14 08:47:46,835 : Image to text: 13.9, 35.68, 48.84, 11.0
2019-02-14 08:47:54,172 : Text to Image: 11.444, 31.38, 44.184, 14.0
2019-02-14 08:48:34,111 : samples : 128000
2019-02-14 08:48:44,170 : Image to text: 13.48, 35.3, 48.92, 11.0
2019-02-14 08:48:51,482 : Text to Image: 11.308, 30.592, 43.288, 14.0
2019-02-14 08:49:32,687 : samples : 192000
2019-02-14 08:49:42,772 : Image to text: 12.92, 34.18, 47.54, 12.0
2019-02-14 08:49:50,111 : Text to Image: 11.044, 30.16, 43.048, 15.0
2019-02-14 08:50:32,464 : samples : 256000
2019-02-14 08:50:42,787 : Image to text: 13.7, 35.7, 48.6, 11.0
2019-02-14 08:50:50,215 : Text to Image: 11.532, 30.876, 43.508, 14.0
2019-02-14 08:51:31,397 : samples : 320000
2019-02-14 08:51:41,705 : Image to text: 14.68, 36.3, 49.88, 11.0
2019-02-14 08:51:49,147 : Text to Image: 11.628, 31.476, 44.332, 14.0
2019-02-14 08:52:29,518 : samples : 384000
2019-02-14 08:52:39,844 : Image to text: 14.12, 35.76, 49.0, 11.0
2019-02-14 08:52:47,299 : Text to Image: 11.396, 30.916, 43.584, 14.0
2019-02-14 08:53:28,112 : samples : 448000
2019-02-14 08:53:38,491 : Image to text: 13.56, 35.32, 48.94, 11.0
2019-02-14 08:53:45,918 : Text to Image: 11.22, 30.232, 43.124, 15.0
2019-02-14 08:54:26,407 : samples : 512000
2019-02-14 08:54:36,638 : Image to text: 14.64, 36.32, 50.4, 10.0
2019-02-14 08:54:44,028 : Text to Image: 11.6, 30.92, 43.584, 14.0
2019-02-14 08:55:18,533 : Epoch 5 finished
2019-02-14 08:55:18,979 : Image to text: 32.8, 67.5, 80.9, 3.0
2019-02-14 08:55:19,314 : Text to Image: 27.0, 62.02, 78.74, 4.0
2019-02-14 08:55:19,766 : Image to text: 33.5, 67.6, 81.4, 3.0
2019-02-14 08:55:20,097 : Text to Image: 26.54, 59.88, 77.24, 4.0
2019-02-14 08:55:20,529 : Image to text: 31.2, 67.4, 81.1, 3.0
2019-02-14 08:55:20,859 : Text to Image: 27.64, 62.06, 77.5, 3.0
2019-02-14 08:55:21,289 : Image to text: 33.7, 67.1, 81.2, 3.0
2019-02-14 08:55:21,619 : Text to Image: 27.68, 61.64, 77.16, 3.0
2019-02-14 08:55:22,061 : Image to text: 33.6, 66.4, 80.5, 3.0
2019-02-14 08:55:22,390 : Text to Image: 27.04, 61.44, 77.54, 4.0
2019-02-14 08:55:22,390 : Dev mean Text to Image: 27.18, 61.408, 77.636, 3.6000000000000005
2019-02-14 08:55:22,390 : Dev mean Image to text: 32.96, 67.2, 81.02000000000001, 3.0
2019-02-14 08:55:22,390 : start epoch
2019-02-14 08:56:02,829 : samples : 64000
2019-02-14 08:56:12,912 : Image to text: 14.1, 36.26, 50.28, 10.0
2019-02-14 08:56:20,283 : Text to Image: 11.704, 31.544, 44.104, 14.0
2019-02-14 08:57:00,599 : samples : 128000
2019-02-14 08:57:10,759 : Image to text: 13.68, 36.52, 49.88, 11.0
2019-02-14 08:57:18,113 : Text to Image: 11.224, 31.12, 43.644, 14.0
2019-02-14 08:57:59,154 : samples : 192000
2019-02-14 08:58:09,333 : Image to text: 14.6, 36.18, 50.94, 10.0
2019-02-14 08:58:16,769 : Text to Image: 11.564, 31.296, 43.712, 14.0
2019-02-14 08:58:57,349 : samples : 256000
2019-02-14 08:59:07,623 : Image to text: 14.54, 36.02, 50.0, 10.0
2019-02-14 08:59:15,061 : Text to Image: 11.488, 31.356, 44.128, 14.0
2019-02-14 08:59:59,053 : samples : 320000
2019-02-14 09:00:09,324 : Image to text: 14.2, 36.88, 50.5, 10.0
2019-02-14 09:00:16,743 : Text to Image: 11.788, 31.412, 44.12, 14.0
2019-02-14 09:00:57,400 : samples : 384000
2019-02-14 09:01:07,647 : Image to text: 14.36, 36.66, 50.14, 10.0
2019-02-14 09:01:15,059 : Text to Image: 11.556, 31.54, 44.464, 14.0
2019-02-14 09:01:55,821 : samples : 448000
2019-02-14 09:02:06,132 : Image to text: 14.24, 36.46, 49.92, 11.0
2019-02-14 09:02:13,564 : Text to Image: 11.812, 31.62, 44.608, 14.0
2019-02-14 09:02:54,945 : samples : 512000
2019-02-14 09:03:05,271 : Image to text: 14.54, 36.3, 49.84, 11.0
2019-02-14 09:03:12,714 : Text to Image: 11.648, 31.72, 44.54, 13.0
2019-02-14 09:03:48,522 : Epoch 6 finished
2019-02-14 09:03:48,959 : Image to text: 32.5, 68.1, 80.2, 3.0
2019-02-14 09:03:49,288 : Text to Image: 27.54, 62.84, 78.82, 3.0
2019-02-14 09:03:49,713 : Image to text: 31.9, 68.6, 82.1, 3.0
2019-02-14 09:03:50,041 : Text to Image: 28.2, 61.42, 77.86, 4.0
2019-02-14 09:03:50,483 : Image to text: 33.6, 67.1, 81.2, 3.0
2019-02-14 09:03:50,815 : Text to Image: 28.56, 63.22, 78.6, 3.0
2019-02-14 09:03:51,251 : Image to text: 35.4, 70.0, 82.6, 2.0
2019-02-14 09:03:51,585 : Text to Image: 28.18, 62.16, 78.22, 3.0
2019-02-14 09:03:52,036 : Image to text: 34.0, 68.1, 81.3, 3.0
2019-02-14 09:03:52,369 : Text to Image: 27.8, 61.84, 77.6, 4.0
2019-02-14 09:03:52,369 : Dev mean Text to Image: 28.055999999999997, 62.296, 78.22, 3.4000000000000004
2019-02-14 09:03:52,369 : Dev mean Image to text: 33.48, 68.38, 81.47999999999999, 2.8
2019-02-14 09:03:52,369 : start epoch
2019-02-14 09:04:35,410 : samples : 64000
2019-02-14 09:04:45,518 : Image to text: 14.58, 36.7, 51.36, 10.0
2019-02-14 09:04:52,905 : Text to Image: 11.76, 31.608, 44.124, 14.0
2019-02-14 09:05:35,721 : samples : 128000
2019-02-14 09:05:45,830 : Image to text: 14.64, 37.48, 50.82, 10.0
2019-02-14 09:05:53,234 : Text to Image: 12.032, 32.308, 44.928, 13.0
2019-02-14 09:06:35,685 : samples : 192000
2019-02-14 09:06:45,790 : Image to text: 15.04, 37.44, 51.38, 10.0
2019-02-14 09:06:53,143 : Text to Image: 11.928, 32.176, 44.728, 13.0
2019-02-14 09:07:35,500 : samples : 256000
2019-02-14 09:07:45,827 : Image to text: 13.24, 35.86, 49.2, 11.0
2019-02-14 09:07:53,291 : Text to Image: 11.6, 31.776, 44.448, 14.0
2019-02-14 09:08:33,982 : samples : 320000
2019-02-14 09:08:44,273 : Image to text: 14.66, 37.4, 51.58, 10.0
2019-02-14 09:08:51,687 : Text to Image: 12.096, 31.96, 44.552, 14.0
2019-02-14 09:09:32,336 : samples : 384000
2019-02-14 09:09:42,588 : Image to text: 14.4, 36.64, 50.38, 10.0
2019-02-14 09:09:49,998 : Text to Image: 11.752, 32.016, 44.572, 13.0
2019-02-14 09:10:33,046 : samples : 448000
2019-02-14 09:10:43,515 : Image to text: 14.34, 36.92, 51.28, 10.0
2019-02-14 09:10:50,963 : Text to Image: 11.804, 32.0, 45.016, 13.0
2019-02-14 09:11:33,082 : samples : 512000
2019-02-14 09:11:43,443 : Image to text: 14.62, 37.28, 51.26, 10.0
2019-02-14 09:11:50,854 : Text to Image: 12.064, 32.524, 45.168, 13.0
2019-02-14 09:12:26,980 : Epoch 7 finished
2019-02-14 09:12:27,425 : Image to text: 32.3, 67.3, 80.8, 3.0
2019-02-14 09:12:27,754 : Text to Image: 27.74, 62.7, 78.84, 3.0
2019-02-14 09:12:28,182 : Image to text: 32.3, 67.6, 82.1, 3.0
2019-02-14 09:12:28,512 : Text to Image: 28.02, 62.56, 77.4, 3.0
2019-02-14 09:12:28,952 : Image to text: 32.9, 68.0, 82.1, 3.0
2019-02-14 09:12:29,283 : Text to Image: 28.28, 64.26, 79.5, 3.0
2019-02-14 09:12:29,715 : Image to text: 35.2, 69.0, 82.2, 3.0
2019-02-14 09:12:30,046 : Text to Image: 28.44, 63.32, 79.76, 3.0
2019-02-14 09:12:30,490 : Image to text: 34.0, 67.2, 80.6, 3.0
2019-02-14 09:12:30,822 : Text to Image: 27.84, 62.74, 77.72, 3.0
2019-02-14 09:12:30,822 : Dev mean Text to Image: 28.064, 63.116, 78.644, 3.0
2019-02-14 09:12:30,822 : Dev mean Image to text: 33.339999999999996, 67.82, 81.56, 3.0
2019-02-14 09:12:30,822 : start epoch
2019-02-14 09:13:14,680 : samples : 64000
2019-02-14 09:13:24,805 : Image to text: 14.32, 36.8, 50.4, 10.0
2019-02-14 09:13:32,165 : Text to Image: 11.6, 31.808, 44.42, 13.0
2019-02-14 09:14:13,499 : samples : 128000
2019-02-14 09:14:23,608 : Image to text: 14.68, 37.44, 50.18, 10.0
2019-02-14 09:14:31,001 : Text to Image: 12.008, 32.292, 45.196, 13.0
2019-02-14 09:15:11,351 : samples : 192000
2019-02-14 09:15:21,456 : Image to text: 14.76, 36.94, 50.14, 10.0
2019-02-14 09:15:28,830 : Text to Image: 11.92, 32.312, 45.244, 13.0
2019-02-14 09:16:09,877 : samples : 256000
2019-02-14 09:16:20,138 : Image to text: 14.34, 36.72, 50.52, 10.0
2019-02-14 09:16:27,530 : Text to Image: 11.888, 31.916, 44.748, 13.0
2019-02-14 09:17:09,708 : samples : 320000
2019-02-14 09:17:19,971 : Image to text: 15.36, 38.66, 51.98, 10.0
2019-02-14 09:17:27,409 : Text to Image: 12.248, 32.936, 45.56, 13.0
2019-02-14 09:18:08,910 : samples : 384000
2019-02-14 09:18:19,274 : Image to text: 14.34, 37.24, 51.06, 10.0
2019-02-14 09:18:26,752 : Text to Image: 12.148, 32.192, 44.96, 13.0
2019-02-14 09:19:08,590 : samples : 448000
2019-02-14 09:19:18,950 : Image to text: 14.34, 37.1, 50.6, 10.0
2019-02-14 09:19:26,414 : Text to Image: 12.068, 32.128, 44.616, 13.0
2019-02-14 09:20:06,978 : samples : 512000
2019-02-14 09:20:17,325 : Image to text: 15.2, 38.04, 51.9, 10.0
2019-02-14 09:20:24,785 : Text to Image: 12.492, 32.756, 45.824, 13.0
2019-02-14 09:20:59,727 : Epoch 8 finished
2019-02-14 09:21:00,165 : Image to text: 33.9, 66.7, 81.8, 3.0
2019-02-14 09:21:00,491 : Text to Image: 28.22, 62.98, 78.72, 3.0
2019-02-14 09:21:00,914 : Image to text: 33.6, 67.6, 82.2, 3.0
2019-02-14 09:21:01,240 : Text to Image: 27.82, 62.52, 78.14, 4.0
2019-02-14 09:21:01,663 : Image to text: 33.8, 67.8, 81.4, 3.0
2019-02-14 09:21:01,994 : Text to Image: 28.62, 63.56, 78.8, 3.0
2019-02-14 09:21:02,430 : Image to text: 34.8, 69.8, 82.8, 3.0
2019-02-14 09:21:02,759 : Text to Image: 28.1, 63.3, 79.56, 3.0
2019-02-14 09:21:03,186 : Image to text: 37.1, 68.9, 80.9, 2.0
2019-02-14 09:21:03,515 : Text to Image: 27.84, 61.98, 78.12, 4.0
2019-02-14 09:21:03,515 : Dev mean Text to Image: 28.120000000000005, 62.867999999999995, 78.66799999999999, 3.4000000000000004
2019-02-14 09:21:03,516 : Dev mean Image to text: 34.64, 68.16, 81.82, 2.8
2019-02-14 09:21:03,516 : start epoch
2019-02-14 09:21:48,281 : samples : 64000
2019-02-14 09:21:58,380 : Image to text: 14.8, 37.92, 51.26, 10.0
2019-02-14 09:22:05,778 : Text to Image: 12.268, 32.556, 45.096, 13.0
2019-02-14 09:22:46,733 : samples : 128000
2019-02-14 09:22:56,834 : Image to text: 14.88, 37.66, 51.1, 10.0
2019-02-14 09:23:04,183 : Text to Image: 12.012, 32.524, 45.34, 13.0
2019-02-14 09:23:44,831 : samples : 192000
2019-02-14 09:23:54,932 : Image to text: 14.94, 37.22, 51.36, 10.0
2019-02-14 09:24:02,346 : Text to Image: 12.084, 32.46, 44.948, 13.0
2019-02-14 09:24:43,498 : samples : 256000
2019-02-14 09:24:53,794 : Image to text: 15.06, 37.58, 51.68, 10.0
2019-02-14 09:25:01,228 : Text to Image: 12.072, 32.408, 45.54, 13.0
2019-02-14 09:25:42,592 : samples : 320000
2019-02-14 09:25:52,903 : Image to text: 15.5, 38.3, 51.84, 10.0
2019-02-14 09:26:00,402 : Text to Image: 12.12, 32.436, 45.352, 13.0
2019-02-14 09:26:41,330 : samples : 384000
2019-02-14 09:26:51,674 : Image to text: 14.2, 36.96, 50.92, 10.0
2019-02-14 09:26:59,116 : Text to Image: 12.04, 32.5, 45.536, 13.0
2019-02-14 09:27:39,640 : samples : 448000
2019-02-14 09:27:49,936 : Image to text: 15.16, 37.38, 50.92, 10.0
2019-02-14 09:27:57,356 : Text to Image: 12.268, 32.544, 45.452, 13.0
2019-02-14 09:28:40,920 : samples : 512000
2019-02-14 09:28:51,198 : Image to text: 14.48, 36.96, 51.46, 10.0
2019-02-14 09:28:58,616 : Text to Image: 12.16, 32.432, 45.368, 13.0
2019-02-14 09:29:36,554 : Epoch 9 finished
2019-02-14 09:29:37,002 : Image to text: 34.4, 67.7, 82.1, 3.0
2019-02-14 09:29:37,334 : Text to Image: 29.06, 63.28, 79.84, 3.0
2019-02-14 09:29:37,768 : Image to text: 34.3, 68.8, 82.8, 3.0
2019-02-14 09:29:38,099 : Text to Image: 27.82, 62.78, 78.66, 3.0
2019-02-14 09:29:38,542 : Image to text: 33.3, 67.3, 82.1, 3.0
2019-02-14 09:29:38,873 : Text to Image: 28.46, 64.3, 79.44, 3.0
2019-02-14 09:29:39,303 : Image to text: 35.9, 69.6, 83.8, 3.0
2019-02-14 09:29:39,633 : Text to Image: 29.66, 63.5, 79.34, 3.0
2019-02-14 09:29:40,073 : Image to text: 35.4, 69.1, 81.8, 2.0
2019-02-14 09:29:40,403 : Text to Image: 28.5, 63.18, 78.32, 3.0
2019-02-14 09:29:40,403 : Dev mean Text to Image: 28.7, 63.408, 79.12, 3.0
2019-02-14 09:29:40,403 : Dev mean Image to text: 34.66, 68.49999999999999, 82.52, 2.8
2019-02-14 09:29:40,404 : start epoch
2019-02-14 09:30:21,385 : samples : 64000
2019-02-14 09:30:31,555 : Image to text: 15.02, 37.62, 51.1, 10.0
2019-02-14 09:30:38,990 : Text to Image: 12.052, 32.7, 45.764, 13.0
2019-02-14 09:31:26,912 : samples : 128000
2019-02-14 09:31:37,016 : Image to text: 14.74, 37.78, 51.96, 10.0
2019-02-14 09:31:44,386 : Text to Image: 12.272, 33.04, 45.8, 13.0
2019-02-14 09:32:26,131 : samples : 192000
2019-02-14 09:32:36,231 : Image to text: 15.1, 37.94, 51.58, 10.0
2019-02-14 09:32:43,591 : Text to Image: 12.488, 32.744, 45.624, 13.0
2019-02-14 09:33:25,712 : samples : 256000
2019-02-14 09:33:36,121 : Image to text: 14.72, 37.8, 51.72, 10.0
2019-02-14 09:33:43,573 : Text to Image: 12.076, 32.224, 45.464, 13.0
2019-02-14 09:34:24,056 : samples : 320000
2019-02-14 09:34:34,366 : Image to text: 15.22, 37.92, 51.38, 10.0
2019-02-14 09:34:41,781 : Text to Image: 12.328, 32.964, 45.856, 13.0
2019-02-14 09:35:22,340 : samples : 384000
2019-02-14 09:35:32,574 : Image to text: 15.12, 37.28, 51.62, 10.0
2019-02-14 09:35:39,947 : Text to Image: 12.252, 32.412, 45.592, 13.0
2019-02-14 09:36:20,700 : samples : 448000
2019-02-14 09:36:30,982 : Image to text: 15.1, 38.68, 52.46, 9.0
2019-02-14 09:36:38,399 : Text to Image: 12.46, 33.404, 46.3, 12.0
2019-02-14 09:37:19,731 : samples : 512000
2019-02-14 09:37:29,998 : Image to text: 14.86, 37.0, 50.8, 10.0
2019-02-14 09:37:37,385 : Text to Image: 12.14, 32.788, 45.912, 13.0
2019-02-14 09:38:12,438 : Epoch 10 finished
2019-02-14 09:38:12,867 : Image to text: 33.9, 68.9, 82.2, 3.0
2019-02-14 09:38:13,195 : Text to Image: 29.02, 64.16, 80.02, 3.0
2019-02-14 09:38:13,634 : Image to text: 34.5, 71.1, 83.4, 3.0
2019-02-14 09:38:13,962 : Text to Image: 28.9, 62.88, 78.84, 3.0
2019-02-14 09:38:14,389 : Image to text: 34.5, 69.1, 82.2, 2.0
2019-02-14 09:38:14,717 : Text to Image: 29.46, 64.14, 79.18, 3.0
2019-02-14 09:38:15,158 : Image to text: 35.9, 70.5, 84.1, 2.0
2019-02-14 09:38:15,486 : Text to Image: 29.56, 64.2, 79.7, 3.0
2019-02-14 09:38:15,912 : Image to text: 35.2, 68.2, 81.0, 3.0
2019-02-14 09:38:16,240 : Text to Image: 27.84, 63.68, 79.6, 3.0
2019-02-14 09:38:16,240 : Dev mean Text to Image: 28.955999999999996, 63.81200000000001, 79.468, 3.0
2019-02-14 09:38:16,240 : Dev mean Image to text: 34.8, 69.56, 82.58, 2.6
2019-02-14 09:38:16,240 : start epoch
2019-02-14 09:38:59,243 : samples : 64000
2019-02-14 09:39:09,377 : Image to text: 15.14, 38.18, 52.22, 10.0
2019-02-14 09:39:16,733 : Text to Image: 12.332, 32.912, 45.828, 13.0
2019-02-14 09:39:57,674 : samples : 128000
2019-02-14 09:40:07,787 : Image to text: 14.82, 38.64, 51.66, 10.0
2019-02-14 09:40:15,138 : Text to Image: 12.208, 32.876, 45.804, 13.0
2019-02-14 09:40:56,892 : samples : 192000
2019-02-14 09:41:06,978 : Image to text: 16.08, 38.96, 52.56, 9.0
2019-02-14 09:41:14,349 : Text to Image: 12.496, 32.692, 45.852, 13.0
2019-02-14 09:41:55,039 : samples : 256000
2019-02-14 09:42:05,261 : Image to text: 15.96, 39.0, 52.92, 9.0
2019-02-14 09:42:12,651 : Text to Image: 12.352, 33.036, 45.952, 13.0
2019-02-14 09:42:54,281 : samples : 320000
2019-02-14 09:43:04,694 : Image to text: 15.06, 38.18, 52.52, 9.0
2019-02-14 09:43:12,127 : Text to Image: 12.308, 32.728, 45.58, 13.0
2019-02-14 09:43:54,196 : samples : 384000
2019-02-14 09:44:04,461 : Image to text: 15.2, 38.58, 52.2, 9.0
2019-02-14 09:44:11,849 : Text to Image: 12.384, 33.868, 46.6, 12.0
2019-02-14 09:44:53,223 : samples : 448000
2019-02-14 09:45:03,478 : Image to text: 15.42, 37.9, 52.08, 10.0
2019-02-14 09:45:10,861 : Text to Image: 12.46, 33.564, 46.208, 13.0
2019-02-14 09:45:51,988 : samples : 512000
2019-02-14 09:46:02,354 : Image to text: 15.06, 38.66, 52.36, 9.0
2019-02-14 09:46:09,803 : Text to Image: 12.272, 32.968, 46.14, 13.0
2019-02-14 09:46:44,808 : Epoch 11 finished
2019-02-14 09:46:45,237 : Image to text: 33.4, 68.9, 81.6, 3.0
2019-02-14 09:46:45,566 : Text to Image: 28.94, 63.56, 79.18, 3.0
2019-02-14 09:46:46,005 : Image to text: 35.3, 69.2, 83.0, 2.0
2019-02-14 09:46:46,332 : Text to Image: 28.86, 62.76, 78.14, 3.0
2019-02-14 09:46:46,758 : Image to text: 36.1, 69.5, 82.1, 3.0
2019-02-14 09:46:47,086 : Text to Image: 29.6, 64.2, 79.58, 3.0
2019-02-14 09:46:47,524 : Image to text: 36.0, 70.7, 84.5, 2.0
2019-02-14 09:46:47,853 : Text to Image: 29.04, 64.64, 79.46, 3.0
2019-02-14 09:46:48,281 : Image to text: 37.2, 68.5, 81.7, 2.0
2019-02-14 09:46:48,610 : Text to Image: 28.46, 63.28, 78.84, 3.0
2019-02-14 09:46:48,610 : Dev mean Text to Image: 28.98, 63.687999999999995, 79.04, 3.0
2019-02-14 09:46:48,610 : Dev mean Image to text: 35.6, 69.36, 82.58000000000001, 2.4
2019-02-14 09:46:48,610 : start epoch
2019-02-14 09:47:30,710 : samples : 64000
2019-02-14 09:47:40,814 : Image to text: 15.18, 37.56, 52.58, 9.0
2019-02-14 09:47:48,210 : Text to Image: 12.596, 33.396, 46.404, 12.0
2019-02-14 09:48:34,952 : samples : 128000
2019-02-14 09:48:45,072 : Image to text: 14.52, 38.4, 52.38, 9.0
2019-02-14 09:48:52,485 : Text to Image: 12.108, 32.912, 46.028, 13.0
2019-02-14 09:49:36,206 : samples : 192000
2019-02-14 09:49:46,318 : Image to text: 15.68, 38.84, 52.92, 9.0
2019-02-14 09:49:53,685 : Text to Image: 12.484, 33.6, 46.624, 12.0
2019-02-14 09:50:34,461 : samples : 256000
2019-02-14 09:50:44,810 : Image to text: 15.56, 37.74, 52.36, 10.0
2019-02-14 09:50:52,229 : Text to Image: 12.412, 33.18, 45.996, 13.0
2019-02-14 09:51:33,156 : samples : 320000
2019-02-14 09:51:43,423 : Image to text: 15.14, 37.68, 51.58, 10.0
2019-02-14 09:51:50,837 : Text to Image: 12.176, 32.864, 46.024, 13.0
2019-02-14 09:52:31,521 : samples : 384000
2019-02-14 09:52:41,853 : Image to text: 15.04, 38.5, 53.2, 9.0
2019-02-14 09:52:49,300 : Text to Image: 12.712, 33.304, 46.156, 12.0
2019-02-14 09:53:29,916 : samples : 448000
2019-02-14 09:53:40,281 : Image to text: 15.4, 38.76, 53.4, 9.0
2019-02-14 09:53:47,719 : Text to Image: 12.892, 33.56, 46.536, 12.0
2019-02-14 09:54:28,462 : samples : 512000
2019-02-14 09:54:38,840 : Image to text: 15.28, 39.14, 53.86, 9.0
2019-02-14 09:54:46,305 : Text to Image: 12.696, 33.312, 46.612, 12.0
2019-02-14 09:55:21,946 : Epoch 12 finished
2019-02-14 09:55:22,376 : Image to text: 34.5, 68.3, 80.8, 3.0
2019-02-14 09:55:22,710 : Text to Image: 28.9, 64.26, 80.24, 3.0
2019-02-14 09:55:23,145 : Image to text: 34.2, 69.4, 84.6, 3.0
2019-02-14 09:55:23,474 : Text to Image: 29.02, 63.62, 79.42, 3.0
2019-02-14 09:55:23,902 : Image to text: 34.3, 70.3, 83.6, 3.0
2019-02-14 09:55:24,231 : Text to Image: 29.24, 64.78, 79.68, 3.0
2019-02-14 09:55:24,671 : Image to text: 37.3, 71.3, 83.0, 2.0
2019-02-14 09:55:25,001 : Text to Image: 29.52, 64.22, 79.6, 3.0
2019-02-14 09:55:25,431 : Image to text: 37.8, 68.4, 81.6, 2.0
2019-02-14 09:55:25,761 : Text to Image: 29.52, 64.32, 79.48, 3.0
2019-02-14 09:55:25,761 : Dev mean Text to Image: 29.24, 64.24, 79.684, 3.0
2019-02-14 09:55:25,761 : Dev mean Image to text: 35.620000000000005, 69.53999999999999, 82.72, 2.5999999999999996
2019-02-14 09:55:25,762 : start epoch
2019-02-14 09:56:07,300 : samples : 64000
2019-02-14 09:56:17,440 : Image to text: 15.8, 38.78, 53.44, 9.0
2019-02-14 09:56:24,837 : Text to Image: 12.236, 33.108, 46.264, 12.0
2019-02-14 09:57:06,189 : samples : 128000
2019-02-14 09:57:16,310 : Image to text: 15.08, 38.26, 52.34, 10.0
2019-02-14 09:57:23,679 : Text to Image: 12.38, 33.244, 46.288, 13.0
2019-02-14 09:58:06,168 : samples : 192000
2019-02-14 09:58:16,293 : Image to text: 15.88, 38.12, 52.66, 9.0
2019-02-14 09:58:23,679 : Text to Image: 12.36, 33.588, 46.836, 12.0
2019-02-14 09:59:05,181 : samples : 256000
2019-02-14 09:59:15,402 : Image to text: 16.9, 39.88, 52.9, 9.0
2019-02-14 09:59:22,799 : Text to Image: 12.392, 34.004, 46.708, 12.0
2019-02-14 10:00:03,934 : samples : 320000
2019-02-14 10:00:14,306 : Image to text: 15.88, 38.32, 52.24, 10.0
2019-02-14 10:00:21,763 : Text to Image: 12.604, 33.688, 46.704, 12.0
2019-02-14 10:01:03,478 : samples : 384000
2019-02-14 10:01:13,753 : Image to text: 14.76, 38.48, 51.58, 10.0
2019-02-14 10:01:21,174 : Text to Image: 12.392, 33.252, 46.192, 13.0
2019-02-14 10:02:01,642 : samples : 448000
2019-02-14 10:02:11,931 : Image to text: 15.74, 38.46, 52.42, 9.0
2019-02-14 10:02:19,354 : Text to Image: 12.376, 33.344, 46.376, 12.0
2019-02-14 10:02:59,678 : samples : 512000
2019-02-14 10:03:10,002 : Image to text: 15.04, 38.2, 52.62, 9.0
2019-02-14 10:03:17,416 : Text to Image: 12.672, 33.008, 46.052, 13.0
2019-02-14 10:03:51,886 : Epoch 13 finished
2019-02-14 10:03:52,312 : Image to text: 33.7, 68.4, 82.9, 3.0
2019-02-14 10:03:52,640 : Text to Image: 29.34, 64.66, 80.36, 3.0
2019-02-14 10:03:53,066 : Image to text: 36.6, 71.9, 84.8, 2.0
2019-02-14 10:03:53,398 : Text to Image: 29.4, 64.2, 79.5, 3.0
2019-02-14 10:03:53,832 : Image to text: 33.5, 68.3, 82.6, 3.0
2019-02-14 10:03:54,163 : Text to Image: 29.54, 65.78, 80.14, 3.0
2019-02-14 10:03:54,593 : Image to text: 37.4, 71.5, 82.7, 2.0
2019-02-14 10:03:54,924 : Text to Image: 30.16, 65.04, 80.52, 3.0
2019-02-14 10:03:55,366 : Image to text: 38.3, 68.9, 82.1, 2.0
2019-02-14 10:03:55,694 : Text to Image: 28.86, 63.88, 79.56, 3.0
2019-02-14 10:03:55,694 : Dev mean Text to Image: 29.46, 64.712, 80.016, 3.0
2019-02-14 10:03:55,694 : Dev mean Image to text: 35.9, 69.8, 83.02, 2.4
2019-02-14 10:03:55,694 : start epoch
2019-02-14 10:04:36,234 : samples : 64000
2019-02-14 10:04:46,365 : Image to text: 15.3, 38.72, 53.48, 9.0
2019-02-14 10:04:53,743 : Text to Image: 12.656, 33.464, 46.58, 12.0
2019-02-14 10:05:34,761 : samples : 128000
2019-02-14 10:05:44,867 : Image to text: 15.76, 39.24, 53.04, 9.0
2019-02-14 10:05:52,244 : Text to Image: 12.736, 33.444, 46.204, 13.0
2019-02-14 10:06:34,218 : samples : 192000
2019-02-14 10:06:44,305 : Image to text: 15.8, 38.3, 53.44, 9.0
2019-02-14 10:06:51,711 : Text to Image: 12.848, 33.496, 46.584, 12.0
2019-02-14 10:07:33,923 : samples : 256000
2019-02-14 10:07:44,151 : Image to text: 15.78, 39.14, 52.44, 9.0
2019-02-14 10:07:51,518 : Text to Image: 12.432, 33.76, 46.376, 12.0
2019-02-14 10:08:32,233 : samples : 320000
2019-02-14 10:08:42,572 : Image to text: 15.92, 38.18, 52.66, 9.0
2019-02-14 10:08:50,047 : Text to Image: 12.428, 33.248, 46.776, 12.0
2019-02-14 10:09:30,942 : samples : 384000
2019-02-14 10:09:41,342 : Image to text: 15.78, 39.18, 52.84, 9.0
2019-02-14 10:09:48,819 : Text to Image: 12.628, 33.76, 46.464, 12.0
2019-02-14 10:10:29,988 : samples : 448000
2019-02-14 10:10:40,451 : Image to text: 15.56, 38.56, 52.94, 9.0
2019-02-14 10:10:47,920 : Text to Image: 12.18, 33.204, 46.284, 13.0
2019-02-14 10:11:30,556 : samples : 512000
2019-02-14 10:11:40,985 : Image to text: 15.78, 39.2, 54.0, 9.0
2019-02-14 10:11:48,448 : Text to Image: 12.596, 33.824, 46.724, 12.0
2019-02-14 10:12:23,894 : Epoch 14 finished
2019-02-14 10:12:24,324 : Image to text: 34.5, 68.9, 81.1, 3.0
2019-02-14 10:12:24,665 : Text to Image: 29.24, 64.92, 80.16, 3.0
2019-02-14 10:12:25,093 : Image to text: 34.7, 71.5, 83.4, 3.0
2019-02-14 10:12:25,422 : Text to Image: 28.96, 63.84, 79.52, 3.0
2019-02-14 10:12:25,850 : Image to text: 36.5, 69.4, 83.2, 3.0
2019-02-14 10:12:26,187 : Text to Image: 29.7, 65.42, 80.48, 3.0
2019-02-14 10:12:26,613 : Image to text: 35.6, 71.1, 83.2, 2.0
2019-02-14 10:12:26,941 : Text to Image: 29.32, 65.1, 80.26, 3.0
2019-02-14 10:12:27,367 : Image to text: 36.3, 68.9, 81.9, 3.0
2019-02-14 10:12:27,695 : Text to Image: 29.36, 63.88, 79.32, 3.0
2019-02-14 10:12:27,695 : Dev mean Text to Image: 29.316, 64.63199999999999, 79.94800000000001, 3.0
2019-02-14 10:12:27,695 : Dev mean Image to text: 35.52, 69.96000000000001, 82.56, 2.8
2019-02-14 10:12:27,695 : start epoch
2019-02-14 10:13:09,846 : samples : 64000
2019-02-14 10:13:19,977 : Image to text: 15.28, 38.82, 51.72, 10.0
2019-02-14 10:13:27,389 : Text to Image: 12.612, 33.824, 46.656, 12.0
2019-02-14 10:14:10,223 : samples : 128000
2019-02-14 10:14:20,334 : Image to text: 15.8, 39.36, 52.34, 9.0
2019-02-14 10:14:27,748 : Text to Image: 12.6, 33.552, 46.76, 12.0
2019-02-14 10:15:09,755 : samples : 192000
2019-02-14 10:15:19,874 : Image to text: 15.6, 39.6, 53.74, 9.0
2019-02-14 10:15:27,284 : Text to Image: 12.696, 33.812, 46.488, 12.0
2019-02-14 10:16:08,115 : samples : 256000
2019-02-14 10:16:18,445 : Image to text: 15.32, 39.2, 52.88, 9.0
2019-02-14 10:16:25,843 : Text to Image: 13.02, 33.748, 46.92, 12.0
2019-02-14 10:17:06,352 : samples : 320000
2019-02-14 10:17:16,588 : Image to text: 15.88, 39.38, 53.2, 9.0
2019-02-14 10:17:23,960 : Text to Image: 12.932, 33.892, 47.036, 12.0
2019-02-14 10:18:05,452 : samples : 384000
2019-02-14 10:18:15,856 : Image to text: 15.44, 39.5, 53.04, 9.0
2019-02-14 10:18:23,311 : Text to Image: 12.816, 33.9, 46.804, 12.0
2019-02-14 10:19:05,574 : samples : 448000
2019-02-14 10:19:15,980 : Image to text: 16.7, 39.3, 53.2, 9.0
2019-02-14 10:19:23,431 : Text to Image: 13.1, 34.048, 47.144, 12.0
2019-02-14 10:20:05,138 : samples : 512000
2019-02-14 10:20:15,461 : Image to text: 15.5, 39.22, 52.8, 9.0
2019-02-14 10:20:22,895 : Text to Image: 12.764, 33.604, 46.588, 12.0
2019-02-14 10:20:58,610 : Epoch 15 finished
2019-02-14 10:20:59,042 : Image to text: 34.7, 69.4, 83.1, 3.0
2019-02-14 10:20:59,367 : Text to Image: 29.76, 64.4, 80.12, 3.0
2019-02-14 10:20:59,796 : Image to text: 36.1, 71.3, 85.0, 3.0
2019-02-14 10:21:00,126 : Text to Image: 29.32, 64.64, 79.48, 3.0
2019-02-14 10:21:00,570 : Image to text: 36.0, 72.1, 82.2, 2.0
2019-02-14 10:21:00,902 : Text to Image: 29.44, 64.96, 79.74, 3.0
2019-02-14 10:21:01,338 : Image to text: 36.9, 72.6, 84.6, 2.0
2019-02-14 10:21:01,684 : Text to Image: 29.78, 65.3, 80.8, 3.0
2019-02-14 10:21:02,125 : Image to text: 38.0, 69.7, 82.5, 2.0
2019-02-14 10:21:02,457 : Text to Image: 29.3, 63.72, 79.3, 3.0
2019-02-14 10:21:02,458 : Dev mean Text to Image: 29.52, 64.604, 79.888, 3.0
2019-02-14 10:21:02,458 : Dev mean Image to text: 36.339999999999996, 71.02, 83.48, 2.4
2019-02-14 10:21:02,458 : start epoch
2019-02-14 10:21:45,356 : samples : 64000
2019-02-14 10:21:55,530 : Image to text: 15.44, 39.1, 52.96, 9.0
2019-02-14 10:22:02,943 : Text to Image: 12.684, 33.72, 46.888, 12.0
2019-02-14 10:22:44,905 : samples : 128000
2019-02-14 10:22:55,088 : Image to text: 15.7, 39.02, 53.52, 9.0
2019-02-14 10:23:02,485 : Text to Image: 12.62, 33.54, 46.564, 12.0
2019-02-14 10:23:44,154 : samples : 192000
2019-02-14 10:23:54,329 : Image to text: 15.2, 38.56, 53.94, 9.0
2019-02-14 10:24:01,772 : Text to Image: 12.916, 33.864, 46.928, 12.0
2019-02-14 10:24:43,522 : samples : 256000
2019-02-14 10:24:55,964 : Image to text: 15.0, 39.6, 53.66, 9.0
2019-02-14 10:25:05,958 : Text to Image: 12.664, 34.152, 47.048, 12.0
2019-02-14 10:25:50,885 : samples : 320000
2019-02-14 10:26:03,501 : Image to text: 15.72, 39.8, 54.0, 9.0
2019-02-14 10:26:13,498 : Text to Image: 12.604, 33.712, 46.704, 12.0
2019-02-14 10:26:54,688 : samples : 384000
2019-02-14 10:27:06,117 : Image to text: 16.28, 39.38, 53.0, 9.0
2019-02-14 10:27:16,094 : Text to Image: 12.616, 33.672, 46.412, 12.0
2019-02-14 10:27:59,813 : samples : 448000
2019-02-14 10:28:12,458 : Image to text: 15.78, 39.58, 53.76, 9.0
2019-02-14 10:28:22,448 : Text to Image: 12.556, 33.568, 46.736, 12.0
2019-02-14 10:29:02,835 : samples : 512000
2019-02-14 10:29:13,671 : Image to text: 16.22, 39.86, 53.18, 9.0
2019-02-14 10:29:23,709 : Text to Image: 13.048, 34.532, 47.516, 12.0
2019-02-14 10:29:59,952 : Epoch 16 finished
2019-02-14 10:30:00,941 : Image to text: 34.8, 70.3, 83.4, 3.0
2019-02-14 10:30:01,723 : Text to Image: 29.68, 64.92, 80.66, 3.0
2019-02-14 10:30:02,666 : Image to text: 36.1, 70.1, 84.1, 2.0
2019-02-14 10:30:03,452 : Text to Image: 29.6, 63.58, 80.04, 3.0
2019-02-14 10:30:04,437 : Image to text: 35.3, 70.5, 82.7, 3.0
2019-02-14 10:30:05,243 : Text to Image: 30.12, 65.12, 80.32, 3.0
2019-02-14 10:30:06,208 : Image to text: 36.0, 72.2, 84.6, 2.0
2019-02-14 10:30:07,036 : Text to Image: 29.5, 65.54, 80.36, 3.0
2019-02-14 10:30:08,001 : Image to text: 37.0, 70.0, 83.0, 2.0
2019-02-14 10:30:08,803 : Text to Image: 29.16, 63.58, 79.74, 3.0
2019-02-14 10:30:08,803 : Dev mean Text to Image: 29.612000000000002, 64.548, 80.22399999999999, 3.0
2019-02-14 10:30:08,803 : Dev mean Image to text: 35.839999999999996, 70.62, 83.56, 2.4
2019-02-14 10:30:08,804 : start epoch
2019-02-14 10:30:50,760 : samples : 64000
2019-02-14 10:31:00,828 : Image to text: 16.78, 39.58, 53.24, 9.0
2019-02-14 10:31:07,723 : Text to Image: 12.848, 34.248, 47.128, 12.0
2019-02-14 10:31:50,778 : samples : 128000
2019-02-14 10:32:03,370 : Image to text: 16.72, 39.84, 53.96, 9.0
2019-02-14 10:32:13,358 : Text to Image: 12.616, 34.196, 47.084, 12.0
2019-02-14 10:32:55,086 : samples : 192000
2019-02-14 10:33:05,173 : Image to text: 16.46, 39.66, 53.22, 9.0
2019-02-14 10:33:12,332 : Text to Image: 12.924, 33.856, 47.02, 12.0
2019-02-14 10:33:55,078 : samples : 256000
2019-02-14 10:34:07,709 : Image to text: 15.78, 39.22, 53.84, 9.0
2019-02-14 10:34:17,741 : Text to Image: 12.756, 33.464, 46.604, 12.0
2019-02-14 10:35:00,743 : samples : 320000
2019-02-14 10:35:10,817 : Image to text: 15.6, 39.86, 53.96, 9.0
2019-02-14 10:35:18,030 : Text to Image: 12.9, 33.792, 46.852, 12.0
2019-02-14 10:35:59,929 : samples : 384000
2019-02-14 10:36:12,512 : Image to text: 16.24, 39.32, 53.5, 9.0
2019-02-14 10:36:22,548 : Text to Image: 12.72, 33.896, 46.748, 12.0
2019-02-14 10:37:05,415 : samples : 448000
2019-02-14 10:37:15,487 : Image to text: 15.86, 39.14, 53.22, 9.0
2019-02-14 10:37:22,693 : Text to Image: 13.04, 34.052, 47.028, 12.0
2019-02-14 10:38:04,481 : samples : 512000
2019-02-14 10:38:17,034 : Image to text: 15.72, 38.38, 52.86, 9.0
2019-02-14 10:38:27,013 : Text to Image: 12.8, 33.864, 46.788, 12.0
2019-02-14 10:39:04,709 : Epoch 17 finished
2019-02-14 10:39:05,639 : Image to text: 35.5, 70.0, 82.3, 3.0
2019-02-14 10:39:06,383 : Text to Image: 28.88, 64.34, 80.18, 3.0
2019-02-14 10:39:07,309 : Image to text: 36.6, 70.9, 83.6, 2.0
2019-02-14 10:39:08,081 : Text to Image: 29.34, 64.08, 79.76, 3.0
2019-02-14 10:39:09,038 : Image to text: 34.5, 69.1, 83.6, 3.0
2019-02-14 10:39:09,655 : Text to Image: 30.2, 65.46, 80.8, 3.0
2019-02-14 10:39:10,105 : Image to text: 35.9, 72.0, 84.1, 2.0
2019-02-14 10:39:10,467 : Text to Image: 29.94, 65.08, 80.7, 3.0
2019-02-14 10:39:10,917 : Image to text: 36.6, 71.3, 82.8, 2.0
2019-02-14 10:39:11,279 : Text to Image: 29.52, 64.14, 79.32, 3.0
2019-02-14 10:39:11,279 : Dev mean Text to Image: 29.576, 64.61999999999999, 80.15200000000002, 3.0
2019-02-14 10:39:11,279 : Dev mean Image to text: 35.82, 70.66, 83.28, 2.4
2019-02-14 10:39:15,346 : 
Test scores | Image to text:             36.74, 71.19999999999999, 84.06, 2.2
2019-02-14 10:39:15,346 : Test scores | Text to image:             29.195999999999998, 64.628, 79.87599999999999, 3.0

2019-02-14 10:39:15,441 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 10:39:15,801 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 10:39:16,428 : loading BERT model bert-base-uncased
2019-02-14 10:39:16,429 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:39:16,459 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:39:16,459 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1l3eteas
2019-02-14 10:39:18,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:39:20,325 : Computing embeddings for train/dev/test
2019-02-14 10:41:10,345 : Computed embeddings
2019-02-14 10:41:10,345 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:42:12,169 : [('reg:1e-05', 93.54), ('reg:0.0001', 92.74), ('reg:0.001', 87.19), ('reg:0.01', 81.66)]
2019-02-14 10:42:12,169 : Validation : best param found is reg = 1e-05 with score             93.54
2019-02-14 10:42:12,169 : Evaluating...
2019-02-14 10:42:28,086 : 
Dev acc : 93.5 Test acc : 94.3 for LENGTH classification

2019-02-14 10:42:28,086 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 10:42:28,460 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 10:42:28,506 : loading BERT model bert-base-uncased
2019-02-14 10:42:28,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:42:28,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:42:28,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkya7hzy_
2019-02-14 10:42:30,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:42:32,388 : Computing embeddings for train/dev/test
2019-02-14 10:44:16,274 : Computed embeddings
2019-02-14 10:44:16,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:45:24,226 : [('reg:1e-05', 75.5), ('reg:0.0001', 37.35), ('reg:0.001', 2.5), ('reg:0.01', 0.83)]
2019-02-14 10:45:24,226 : Validation : best param found is reg = 1e-05 with score             75.5
2019-02-14 10:45:24,226 : Evaluating...
2019-02-14 10:45:43,369 : 
Dev acc : 75.5 Test acc : 75.7 for WORDCONTENT classification

2019-02-14 10:45:43,370 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 10:45:43,683 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 10:45:43,755 : loading BERT model bert-base-uncased
2019-02-14 10:45:43,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:45:43,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:45:43,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp__y49_ev
2019-02-14 10:45:46,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:45:47,686 : Computing embeddings for train/dev/test
2019-02-14 10:47:33,995 : Computed embeddings
2019-02-14 10:47:33,995 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:48:21,191 : [('reg:1e-05', 37.56), ('reg:0.0001', 37.13), ('reg:0.001', 34.79), ('reg:0.01', 29.14)]
2019-02-14 10:48:21,191 : Validation : best param found is reg = 1e-05 with score             37.56
2019-02-14 10:48:21,191 : Evaluating...
2019-02-14 10:48:35,683 : 
Dev acc : 37.6 Test acc : 37.3 for DEPTH classification

2019-02-14 10:48:35,683 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 10:48:36,076 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 10:48:36,139 : loading BERT model bert-base-uncased
2019-02-14 10:48:36,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:48:36,166 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:48:36,166 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd4vtd8af
2019-02-14 10:48:38,598 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:48:40,063 : Computing embeddings for train/dev/test
2019-02-14 10:50:37,454 : Computed embeddings
2019-02-14 10:50:37,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:51:09,128 : [('reg:1e-05', 75.1), ('reg:0.0001', 73.22), ('reg:0.001', 66.73), ('reg:0.01', 49.65)]
2019-02-14 10:51:09,129 : Validation : best param found is reg = 1e-05 with score             75.1
2019-02-14 10:51:09,129 : Evaluating...
2019-02-14 10:51:16,488 : 
Dev acc : 75.1 Test acc : 75.7 for TOPCONSTITUENTS classification

2019-02-14 10:51:16,489 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 10:51:16,845 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 10:51:16,912 : loading BERT model bert-base-uncased
2019-02-14 10:51:16,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:51:17,034 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:51:17,034 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcln_o355
2019-02-14 10:51:19,516 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:51:20,900 : Computing embeddings for train/dev/test
2019-02-14 10:53:05,866 : Computed embeddings
2019-02-14 10:53:05,866 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:53:51,834 : [('reg:1e-05', 86.15), ('reg:0.0001', 86.17), ('reg:0.001', 85.91), ('reg:0.01', 84.67)]
2019-02-14 10:53:51,834 : Validation : best param found is reg = 0.0001 with score             86.17
2019-02-14 10:53:51,835 : Evaluating...
2019-02-14 10:54:01,802 : 
Dev acc : 86.2 Test acc : 85.2 for BIGRAMSHIFT classification

2019-02-14 10:54:01,803 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 10:54:02,384 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 10:54:02,450 : loading BERT model bert-base-uncased
2019-02-14 10:54:02,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:54:02,481 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:54:02,481 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq1wr4ho4
2019-02-14 10:54:04,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:54:06,339 : Computing embeddings for train/dev/test
2019-02-14 10:55:43,182 : Computed embeddings
2019-02-14 10:55:43,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:56:18,227 : [('reg:1e-05', 88.77), ('reg:0.0001', 88.77), ('reg:0.001', 88.94), ('reg:0.01', 89.18)]
2019-02-14 10:56:18,227 : Validation : best param found is reg = 0.01 with score             89.18
2019-02-14 10:56:18,227 : Evaluating...
2019-02-14 10:56:26,015 : 
Dev acc : 89.2 Test acc : 87.9 for TENSE classification

2019-02-14 10:56:26,016 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 10:56:26,416 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 10:56:26,481 : loading BERT model bert-base-uncased
2019-02-14 10:56:26,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:56:26,601 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:56:26,601 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxkfohozk
2019-02-14 10:56:29,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:56:30,479 : Computing embeddings for train/dev/test
2019-02-14 10:58:12,981 : Computed embeddings
2019-02-14 10:58:12,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:58:58,373 : [('reg:1e-05', 84.37), ('reg:0.0001', 84.22), ('reg:0.001', 84.18), ('reg:0.01', 81.42)]
2019-02-14 10:58:58,374 : Validation : best param found is reg = 1e-05 with score             84.37
2019-02-14 10:58:58,374 : Evaluating...
2019-02-14 10:59:04,511 : 
Dev acc : 84.4 Test acc : 82.5 for SUBJNUMBER classification

2019-02-14 10:59:04,512 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 10:59:05,114 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 10:59:05,182 : loading BERT model bert-base-uncased
2019-02-14 10:59:05,183 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:59:05,212 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:59:05,212 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2z87aw9
2019-02-14 10:59:07,659 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:59:09,091 : Computing embeddings for train/dev/test
2019-02-14 11:00:59,476 : Computed embeddings
2019-02-14 11:00:59,476 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:01:39,322 : [('reg:1e-05', 83.09), ('reg:0.0001', 83.13), ('reg:0.001', 83.02), ('reg:0.01', 82.37)]
2019-02-14 11:01:39,323 : Validation : best param found is reg = 0.0001 with score             83.13
2019-02-14 11:01:39,323 : Evaluating...
2019-02-14 11:01:45,975 : 
Dev acc : 83.1 Test acc : 83.6 for OBJNUMBER classification

2019-02-14 11:01:45,976 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 11:01:46,577 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 11:01:46,648 : loading BERT model bert-base-uncased
2019-02-14 11:01:46,648 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:01:46,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:01:46,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk2f437x7
2019-02-14 11:01:49,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:01:50,603 : Computing embeddings for train/dev/test
2019-02-14 11:03:43,410 : Computed embeddings
2019-02-14 11:03:43,410 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:04:22,230 : [('reg:1e-05', 58.44), ('reg:0.0001', 58.35), ('reg:0.001', 57.95), ('reg:0.01', 57.04)]
2019-02-14 11:04:22,230 : Validation : best param found is reg = 1e-05 with score             58.44
2019-02-14 11:04:22,230 : Evaluating...
2019-02-14 11:04:30,001 : 
Dev acc : 58.4 Test acc : 59.6 for ODDMANOUT classification

2019-02-14 11:04:30,002 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 11:04:30,429 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 11:04:30,508 : loading BERT model bert-base-uncased
2019-02-14 11:04:30,508 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:04:30,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:04:30,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpffpjjn3f
2019-02-14 11:04:32,979 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:04:34,391 : Computing embeddings for train/dev/test
2019-02-14 11:06:23,930 : Computed embeddings
2019-02-14 11:06:23,930 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:07:10,456 : [('reg:1e-05', 58.07), ('reg:0.0001', 58.04), ('reg:0.001', 57.87), ('reg:0.01', 54.91)]
2019-02-14 11:07:10,457 : Validation : best param found is reg = 1e-05 with score             58.07
2019-02-14 11:07:10,457 : Evaluating...
2019-02-14 11:07:21,372 : 
Dev acc : 58.1 Test acc : 58.2 for COORDINATIONINVERSION classification

2019-02-14 11:07:21,374 : total results: {'STS12': {'MSRpar': {'pearson': (0.4052528229448153, 5.184459789409608e-31), 'spearman': SpearmanrResult(correlation=0.4400593881107694, pvalue=7.2164011687994715e-37), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5781198381657037, 4.234063823707794e-68), 'spearman': SpearmanrResult(correlation=0.5836619905624902, pvalue=1.1070533954923591e-69), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5028136618251258, 8.811336708865506e-31), 'spearman': SpearmanrResult(correlation=0.5862438845437978, pvalue=1.029235710476498e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6739990337448989, 1.8357000764257304e-100), 'spearman': SpearmanrResult(correlation=0.6743744579876841, pvalue=1.29665873698275e-100), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6072765076987535, 1.4199422666845886e-41), 'spearman': SpearmanrResult(correlation=0.5229234273429784, pvalue=2.1994797382015973e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5534923728758594, 'wmean': 0.5521633103253214}, 'spearman': {'mean': 0.5614526297095439, 'wmean': 0.5634827117153023}}}, 'STS13': {'FNWN': {'pearson': (0.372052938326867, 1.3536997629773848e-07), 'spearman': SpearmanrResult(correlation=0.37907053180836076, pvalue=7.494541225597373e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6480739546601892, 1.4946342365735147e-90), 'spearman': SpearmanrResult(correlation=0.6270650187950015, pvalue=3.333233309079082e-83), 'nsamples': 750}, 'OnWN': {'pearson': (0.4799013404642696, 1.1758670471369962e-33), 'spearman': SpearmanrResult(correlation=0.5121057629576942, pvalue=7.884516133825847e-39), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.500009411150442, 'wmean': 0.5503987488929166}, 'spearman': {'mean': 0.5060804378536855, 'wmean': 0.5528229517515318}}}, 'STS14': {'deft-forum': {'pearson': (0.3297163897194047, 7.149623797714282e-13), 'spearman': SpearmanrResult(correlation=0.36278776982239624, pvalue=1.913451362224233e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.7316315992034294, 1.620960515455753e-51), 'spearman': SpearmanrResult(correlation=0.7010947043537464, pvalue=1.1226882586341086e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.6096815694875363, 1.549690787976233e-77), 'spearman': SpearmanrResult(correlation=0.5687851153263359, pvalue=1.67836905301294e-65), 'nsamples': 750}, 'images': {'pearson': (0.591974697686206, 4.09800000733299e-72), 'spearman': SpearmanrResult(correlation=0.587642212053968, pvalue=7.738874210426582e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.6403114394717663, 9.049464380856232e-88), 'spearman': SpearmanrResult(correlation=0.6846258207900822, pvalue=7.979986687577142e-105), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6550704230979825, 3.947614578733825e-93), 'spearman': SpearmanrResult(correlation=0.6214669372400929, pvalue=2.4376274745663605e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5930643531110542, 'wmean': 0.5975041206513011}, 'spearman': {'mean': 0.5877337599311037, 'wmean': 0.5921261258090831}}}, 'STS15': {'answers-forums': {'pearson': (0.5173888238590508, 4.61682608729839e-27), 'spearman': SpearmanrResult(correlation=0.5262692414717844, pvalue=4.22147237319601e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.6866462683166453, 1.1258635488865303e-105), 'spearman': SpearmanrResult(correlation=0.6903386065150742, pvalue=3.013546484299612e-107), 'nsamples': 750}, 'belief': {'pearson': (0.6272192956018731, 2.1250343371543077e-42), 'spearman': SpearmanrResult(correlation=0.6531186102084776, pvalue=5.711032913433934e-47), 'nsamples': 375}, 'headlines': {'pearson': (0.6671394152527296, 9.627940695541496e-98), 'spearman': SpearmanrResult(correlation=0.6585473295372856, pvalue=1.946881284978932e-94), 'nsamples': 750}, 'images': {'pearson': (0.7074201564527084, 7.679068079133998e-115), 'spearman': SpearmanrResult(correlation=0.7205976358327468, pvalue=4.3725201690880265e-121), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6411627918966014, 'wmean': 0.6583774749381364}, 'spearman': {'mean': 0.6497742847130737, 'wmean': 0.6647943744313094}}}, 'STS16': {'answer-answer': {'pearson': (0.4762949438543628, 8.670321108450283e-16), 'spearman': SpearmanrResult(correlation=0.5057588483084345, pvalue=6.6912905917088936e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6772714026658299, 8.95968090179346e-35), 'spearman': SpearmanrResult(correlation=0.6804121707855832, pvalue=3.358356003305362e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7213897471078284, 3.0374011440175776e-38), 'spearman': SpearmanrResult(correlation=0.7256435564145253, pvalue=6.925401333608475e-39), 'nsamples': 230}, 'postediting': {'pearson': (0.7848939179183156, 3.2340557437920288e-52), 'spearman': SpearmanrResult(correlation=0.8221923351859188, pvalue=3.712524094429376e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.35401062850828596, 1.4526489564571263e-07), 'spearman': SpearmanrResult(correlation=0.34867486159786426, pvalue=2.2939403507917073e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6027721280109246, 'wmean': 0.607960770799242}, 'spearman': {'mean': 0.6165363544584652, 'wmean': 0.62248850913205}}}, 'MR': {'devacc': 76.29, 'acc': 75.63, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.74, 'acc': 80.19, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.03, 'acc': 87.36, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.2, 'acc': 93.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.65, 'acc': 80.18, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.14, 'acc': 42.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.06, 'acc': 90.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.6, 'acc': 74.09, 'f1': 80.4, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.0, 'acc': 79.32, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8081018837451983, 'pearson': 0.8210946306785972, 'spearman': 0.7497532476361182, 'mse': 0.33389116918705375, 'yhat': array([3.79825653, 4.24169418, 1.33630719, ..., 3.09608245, 4.51618008,        4.46230123]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7464539187484868, 'pearson': 0.6798474619844024, 'spearman': 0.6748435995029207, 'mse': 1.4105237914184845, 'yhat': array([1.45598282, 1.54532538, 1.98314118, ..., 3.90942835, 3.81228405,        3.45631741]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.2, 'acc': 61.48, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 364.85200000000003, 'acc': [(36.74, 71.19999999999999, 84.06, 2.2), (29.195999999999998, 64.628, 79.87599999999999, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 93.54, 'acc': 94.31, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 75.5, 'acc': 75.74, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.56, 'acc': 37.32, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.1, 'acc': 75.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.17, 'acc': 85.16, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.18, 'acc': 87.92, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.37, 'acc': 82.48, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.13, 'acc': 83.57, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.44, 'acc': 59.63, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 58.07, 'acc': 58.21, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 11:07:21,374 : STS12 p=0.5522, STS12 s=0.5635, STS13 p=0.5504, STS13 s=0.5528, STS14 p=0.5975, STS14 s=0.5921, STS15 p=0.6584, STS15 s=0.6648, STS 16 p=0.6080, STS16 s=0.6225, STS B p=0.6798, STS B s=0.6748, STS B m=1.4105, SICK-R p=0.8211, SICK-R s=0.7498, SICK-P m=0.3339
2019-02-14 11:07:21,374 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 11:07:21,374 : 0.5522,0.5635,0.5504,0.5528,0.5975,0.5921,0.6584,0.6648,0.6080,0.6225,0.6798,0.6748,1.4105,0.8211,0.7498,0.3339
2019-02-14 11:07:21,374 : MR=75.63, CR=80.19, SUBJ=93.85, MPQA=87.36, SST-B=80.18, SST-F=42.58, TREC=90.80, SICK-E=79.32, SNLI=61.48, MRPC=74.09, MRPC f=80.40
2019-02-14 11:07:21,374 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 11:07:21,374 : 75.63,80.19,93.85,87.36,80.18,42.58,90.80,79.32,61.48,74.09,80.40
2019-02-14 11:07:21,374 : COCO r1i2t=36.74, COCO r5i2t=71.20, COCO r10i2t=84.06, COCO medr_i2t=2.20, COCO r1t2i=29.20, COCO r5t2i=64.63, COCO r10t2i=79.88, COCO medr_t2i=3.00
2019-02-14 11:07:21,374 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 11:07:21,374 : 36.74,71.20,84.06,2.20,29.20,64.63,79.88,3.00
2019-02-14 11:07:21,374 : SentLen=94.31, WC=75.74, TreeDepth=37.32, TopConst=75.70, BShift=85.16, Tense=87.92, SubjNum=82.48, ObjNum=83.57, SOMO=59.63, CoordInv=58.21, average=74.00
2019-02-14 11:07:21,374 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 11:07:21,374 : 94.31,75.74,37.32,75.70,85.16,87.92,82.48,83.57,59.63,58.21,74.00
2019-02-14 11:07:21,374 : ********************************************************************************
2019-02-14 11:07:21,374 : ********************************************************************************
2019-02-14 11:07:21,375 : ********************************************************************************
2019-02-14 11:07:21,375 : layer 5
2019-02-14 11:07:21,375 : ********************************************************************************
2019-02-14 11:07:21,375 : ********************************************************************************
2019-02-14 11:07:21,375 : ********************************************************************************
2019-02-14 11:07:21,465 : ***** Transfer task : STS12 *****


2019-02-14 11:07:21,477 : loading BERT model bert-base-uncased
2019-02-14 11:07:21,477 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:07:21,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:07:21,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp29oo1vil
2019-02-14 11:07:23,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:07:27,437 : MSRpar : pearson = 0.4167, spearman = 0.4487
2019-02-14 11:07:28,587 : MSRvid : pearson = 0.5432, spearman = 0.5497
2019-02-14 11:07:29,438 : SMTeuroparl : pearson = 0.4812, spearman = 0.5806
2019-02-14 11:07:30,963 : surprise.OnWN : pearson = 0.6512, spearman = 0.6550
2019-02-14 11:07:31,774 : surprise.SMTnews : pearson = 0.6241, spearman = 0.5168
2019-02-14 11:07:31,774 : ALL (weighted average) : Pearson = 0.5400,             Spearman = 0.5511
2019-02-14 11:07:31,774 : ALL (average) : Pearson = 0.5433,             Spearman = 0.5502

2019-02-14 11:07:31,774 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 11:07:31,784 : loading BERT model bert-base-uncased
2019-02-14 11:07:31,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:07:31,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:07:31,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxej_jzi6
2019-02-14 11:07:34,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:07:36,429 : FNWN : pearson = 0.3742, spearman = 0.3874
2019-02-14 11:07:37,708 : headlines : pearson = 0.6364, spearman = 0.6133
2019-02-14 11:07:38,734 : OnWN : pearson = 0.4712, spearman = 0.5008
2019-02-14 11:07:38,734 : ALL (weighted average) : Pearson = 0.5416,             Spearman = 0.5428
2019-02-14 11:07:38,734 : ALL (average) : Pearson = 0.4939,             Spearman = 0.5005

2019-02-14 11:07:38,734 : ***** Transfer task : STS14 *****


2019-02-14 11:07:38,749 : loading BERT model bert-base-uncased
2019-02-14 11:07:38,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:07:38,768 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:07:38,768 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4wuvtqp8
2019-02-14 11:07:41,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:07:43,603 : deft-forum : pearson = 0.3202, spearman = 0.3497
2019-02-14 11:07:44,470 : deft-news : pearson = 0.7425, spearman = 0.7111
2019-02-14 11:07:45,875 : headlines : pearson = 0.5986, spearman = 0.5515
2019-02-14 11:07:47,236 : images : pearson = 0.5862, spearman = 0.5811
2019-02-14 11:07:48,609 : OnWN : pearson = 0.6462, spearman = 0.6825
2019-02-14 11:07:50,238 : tweet-news : pearson = 0.6661, spearman = 0.6246
2019-02-14 11:07:50,238 : ALL (weighted average) : Pearson = 0.5973,             Spearman = 0.5868
2019-02-14 11:07:50,238 : ALL (average) : Pearson = 0.5933,             Spearman = 0.5834

2019-02-14 11:07:50,239 : ***** Transfer task : STS15 *****


2019-02-14 11:07:50,272 : loading BERT model bert-base-uncased
2019-02-14 11:07:50,272 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:07:50,290 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:07:50,290 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptefidkdo
2019-02-14 11:07:52,726 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:07:55,228 : answers-forums : pearson = 0.5297, spearman = 0.5239
2019-02-14 11:07:56,617 : answers-students : pearson = 0.6772, spearman = 0.6842
2019-02-14 11:07:57,669 : belief : pearson = 0.6507, spearman = 0.6778
2019-02-14 11:07:59,139 : headlines : pearson = 0.6557, spearman = 0.6453
2019-02-14 11:08:00,561 : images : pearson = 0.7158, spearman = 0.7263
2019-02-14 11:08:00,561 : ALL (weighted average) : Pearson = 0.6597,             Spearman = 0.6642
2019-02-14 11:08:00,561 : ALL (average) : Pearson = 0.6458,             Spearman = 0.6515

2019-02-14 11:08:00,561 : ***** Transfer task : STS16 *****


2019-02-14 11:08:00,629 : loading BERT model bert-base-uncased
2019-02-14 11:08:00,629 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:08:00,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:08:00,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkx2kc_i6
2019-02-14 11:08:03,084 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:08:05,070 : answer-answer : pearson = 0.5104, spearman = 0.5336
2019-02-14 11:08:05,577 : headlines : pearson = 0.6597, spearman = 0.6599
2019-02-14 11:08:06,121 : plagiarism : pearson = 0.7512, spearman = 0.7620
2019-02-14 11:08:06,877 : postediting : pearson = 0.7949, spearman = 0.8308
2019-02-14 11:08:07,354 : question-question : pearson = 0.3393, spearman = 0.3394
2019-02-14 11:08:07,354 : ALL (weighted average) : Pearson = 0.6168,             Spearman = 0.6313
2019-02-14 11:08:07,354 : ALL (average) : Pearson = 0.6111,             Spearman = 0.6252

2019-02-14 11:08:07,354 : ***** Transfer task : MR *****


2019-02-14 11:08:07,373 : loading BERT model bert-base-uncased
2019-02-14 11:08:07,373 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:08:07,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:08:07,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu3cvlgj_
2019-02-14 11:08:09,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:08:11,402 : Generating sentence embeddings
2019-02-14 11:08:28,195 : Generated sentence embeddings
2019-02-14 11:08:28,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 11:08:48,125 : Best param found at split 1: l2reg = 0.01                 with score 77.12
2019-02-14 11:09:08,522 : Best param found at split 2: l2reg = 0.001                 with score 76.73
2019-02-14 11:09:30,506 : Best param found at split 3: l2reg = 0.001                 with score 77.27
2019-02-14 11:09:52,272 : Best param found at split 4: l2reg = 0.001                 with score 76.87
2019-02-14 11:10:17,803 : Best param found at split 5: l2reg = 0.0001                 with score 77.23
2019-02-14 11:10:19,077 : Dev acc : 77.04 Test acc : 76.63

2019-02-14 11:10:19,078 : ***** Transfer task : CR *****


2019-02-14 11:10:19,085 : loading BERT model bert-base-uncased
2019-02-14 11:10:19,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:10:19,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:10:19,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphk3kwqat
2019-02-14 11:10:21,543 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:10:23,035 : Generating sentence embeddings
2019-02-14 11:10:27,414 : Generated sentence embeddings
2019-02-14 11:10:27,414 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 11:10:34,802 : Best param found at split 1: l2reg = 0.0001                 with score 80.29
2019-02-14 11:10:43,518 : Best param found at split 2: l2reg = 0.001                 with score 81.05
2019-02-14 11:10:51,460 : Best param found at split 3: l2reg = 1e-05                 with score 81.49
2019-02-14 11:10:59,103 : Best param found at split 4: l2reg = 0.001                 with score 80.67
2019-02-14 11:11:07,300 : Best param found at split 5: l2reg = 1e-05                 with score 80.6
2019-02-14 11:11:07,680 : Dev acc : 80.82 Test acc : 80.45

2019-02-14 11:11:07,680 : ***** Transfer task : MPQA *****


2019-02-14 11:11:07,686 : loading BERT model bert-base-uncased
2019-02-14 11:11:07,686 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:11:07,705 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:11:07,706 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvgfi1qe9
2019-02-14 11:11:10,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:11:11,660 : Generating sentence embeddings
2019-02-14 11:11:19,566 : Generated sentence embeddings
2019-02-14 11:11:19,567 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 11:11:42,559 : Best param found at split 1: l2reg = 0.001                 with score 87.52
2019-02-14 11:12:06,755 : Best param found at split 2: l2reg = 0.001                 with score 87.81
2019-02-14 11:12:31,298 : Best param found at split 3: l2reg = 0.01                 with score 87.25
2019-02-14 11:12:54,449 : Best param found at split 4: l2reg = 0.0001                 with score 87.86
2019-02-14 11:13:18,982 : Best param found at split 5: l2reg = 0.001                 with score 87.51
2019-02-14 11:13:20,313 : Dev acc : 87.59 Test acc : 88.03

2019-02-14 11:13:20,314 : ***** Transfer task : SUBJ *****


2019-02-14 11:13:20,330 : loading BERT model bert-base-uncased
2019-02-14 11:13:20,330 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:13:20,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:13:20,350 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwmqofyhn
2019-02-14 11:13:22,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:13:24,267 : Generating sentence embeddings
2019-02-14 11:13:39,679 : Generated sentence embeddings
2019-02-14 11:13:39,680 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 11:13:58,834 : Best param found at split 1: l2reg = 1e-05                 with score 94.59
2019-02-14 11:14:21,463 : Best param found at split 2: l2reg = 1e-05                 with score 94.9
2019-02-14 11:14:42,922 : Best param found at split 3: l2reg = 0.001                 with score 94.39
2019-02-14 11:15:05,926 : Best param found at split 4: l2reg = 0.001                 with score 94.91
2019-02-14 11:15:27,094 : Best param found at split 5: l2reg = 0.001                 with score 94.45
2019-02-14 11:15:28,398 : Dev acc : 94.65 Test acc : 94.29

2019-02-14 11:15:28,399 : ***** Transfer task : SST Binary classification *****


2019-02-14 11:15:28,529 : loading BERT model bert-base-uncased
2019-02-14 11:15:28,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:15:28,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:15:28,552 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7lt1kyf5
2019-02-14 11:15:30,984 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:15:32,454 : Computing embedding for train
2019-02-14 11:16:41,065 : Computed train embeddings
2019-02-14 11:16:41,065 : Computing embedding for dev
2019-02-14 11:16:42,240 : Computed dev embeddings
2019-02-14 11:16:42,240 : Computing embedding for test
2019-02-14 11:16:44,875 : Computed test embeddings
2019-02-14 11:16:44,876 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:17:26,870 : [('reg:1e-05', 80.62), ('reg:0.0001', 80.73), ('reg:0.001', 81.31), ('reg:0.01', 80.73)]
2019-02-14 11:17:26,870 : Validation : best param found is reg = 0.001 with score             81.31
2019-02-14 11:17:26,870 : Evaluating...
2019-02-14 11:17:37,619 : 
Dev acc : 81.31 Test acc : 82.04 for             SST Binary classification

2019-02-14 11:17:37,619 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 11:17:37,667 : loading BERT model bert-base-uncased
2019-02-14 11:17:37,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:17:37,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:17:37,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpur2p9nkt
2019-02-14 11:17:40,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:17:41,614 : Computing embedding for train
2019-02-14 11:17:53,771 : Computed train embeddings
2019-02-14 11:17:53,771 : Computing embedding for dev
2019-02-14 11:17:55,434 : Computed dev embeddings
2019-02-14 11:17:55,434 : Computing embedding for test
2019-02-14 11:17:58,707 : Computed test embeddings
2019-02-14 11:17:58,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:18:04,592 : [('reg:1e-05', 41.05), ('reg:0.0001', 40.96), ('reg:0.001', 41.05), ('reg:0.01', 41.33)]
2019-02-14 11:18:04,592 : Validation : best param found is reg = 0.01 with score             41.33
2019-02-14 11:18:04,592 : Evaluating...
2019-02-14 11:18:05,871 : 
Dev acc : 41.33 Test acc : 42.35 for             SST Fine-Grained classification

2019-02-14 11:18:05,871 : ***** Transfer task : TREC *****


2019-02-14 11:18:05,884 : loading BERT model bert-base-uncased
2019-02-14 11:18:05,884 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:18:05,904 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:18:05,904 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplkad_ka4
2019-02-14 11:18:08,338 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:18:15,131 : Computed train embeddings
2019-02-14 11:18:15,588 : Computed test embeddings
2019-02-14 11:18:15,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 11:18:31,946 : [('reg:1e-05', 83.05), ('reg:0.0001', 83.09), ('reg:0.001', 81.64), ('reg:0.01', 75.74)]
2019-02-14 11:18:31,946 : Cross-validation : best param found is reg = 0.0001             with score 83.09
2019-02-14 11:18:31,946 : Evaluating...
2019-02-14 11:18:32,900 : 
Dev acc : 83.09 Test acc : 91.0             for TREC

2019-02-14 11:18:32,901 : ***** Transfer task : MRPC *****


2019-02-14 11:18:32,962 : loading BERT model bert-base-uncased
2019-02-14 11:18:32,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:18:32,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:18:32,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1f2co0hn
2019-02-14 11:18:35,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:18:36,908 : Computing embedding for train
2019-02-14 11:18:49,585 : Computed train embeddings
2019-02-14 11:18:49,585 : Computing embedding for test
2019-02-14 11:18:54,609 : Computed test embeddings
2019-02-14 11:18:54,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 11:19:05,516 : [('reg:1e-05', 73.87), ('reg:0.0001', 73.92), ('reg:0.001', 73.82), ('reg:0.01', 73.53)]
2019-02-14 11:19:05,516 : Cross-validation : best param found is reg = 0.0001             with score 73.92
2019-02-14 11:19:05,516 : Evaluating...
2019-02-14 11:19:06,240 : Dev acc : 73.92 Test acc 74.14; Test F1 80.34 for MRPC.

2019-02-14 11:19:06,240 : ***** Transfer task : SICK-Entailment*****


2019-02-14 11:19:06,264 : loading BERT model bert-base-uncased
2019-02-14 11:19:06,264 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:19:06,320 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:19:06,320 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwoygaiji
2019-02-14 11:19:08,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:19:10,231 : Computing embedding for train
2019-02-14 11:19:19,660 : Computed train embeddings
2019-02-14 11:19:19,660 : Computing embedding for dev
2019-02-14 11:19:20,729 : Computed dev embeddings
2019-02-14 11:19:20,729 : Computing embedding for test
2019-02-14 11:19:30,770 : Computed test embeddings
2019-02-14 11:19:30,797 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:19:34,143 : [('reg:1e-05', 80.2), ('reg:0.0001', 80.0), ('reg:0.001', 79.4), ('reg:0.01', 74.0)]
2019-02-14 11:19:34,143 : Validation : best param found is reg = 1e-05 with score             80.2
2019-02-14 11:19:34,143 : Evaluating...
2019-02-14 11:19:34,984 : 
Dev acc : 80.2 Test acc : 78.71 for                        SICK entailment

2019-02-14 11:19:34,984 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 11:19:35,011 : loading BERT model bert-base-uncased
2019-02-14 11:19:35,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:19:35,031 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:19:35,032 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpenaemam6
2019-02-14 11:19:37,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:19:38,928 : Computing embedding for train
2019-02-14 11:19:48,520 : Computed train embeddings
2019-02-14 11:19:48,521 : Computing embedding for dev
2019-02-14 11:19:50,036 : Computed dev embeddings
2019-02-14 11:19:50,037 : Computing embedding for test
2019-02-14 11:20:00,654 : Computed test embeddings
2019-02-14 11:20:43,149 : Dev : Pearson 0.8062596673158293
2019-02-14 11:20:43,149 : Test : Pearson 0.8068017446765752 Spearman 0.7402394011926606 MSE 0.3554067758133547                        for SICK Relatedness

2019-02-14 11:20:43,149 : 

***** Transfer task : STSBenchmark*****


2019-02-14 11:20:43,230 : loading BERT model bert-base-uncased
2019-02-14 11:20:43,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:20:43,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:20:43,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp093_omqc
2019-02-14 11:20:45,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:20:47,138 : Computing embedding for train
2019-02-14 11:20:59,526 : Computed train embeddings
2019-02-14 11:20:59,526 : Computing embedding for dev
2019-02-14 11:21:03,683 : Computed dev embeddings
2019-02-14 11:21:03,683 : Computing embedding for test
2019-02-14 11:21:06,042 : Computed test embeddings
2019-02-14 11:21:30,043 : Dev : Pearson 0.7480499822719735
2019-02-14 11:21:30,043 : Test : Pearson 0.6836250222714078 Spearman 0.6770241178091033 MSE 1.4143526060633713                        for SICK Relatedness

2019-02-14 11:21:30,043 : ***** Transfer task : SNLI Entailment*****


2019-02-14 11:21:34,959 : loading BERT model bert-base-uncased
2019-02-14 11:21:34,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:21:35,085 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:21:35,086 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps8kiwioo
2019-02-14 11:21:37,548 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:21:39,159 : PROGRESS (encoding): 0.00%
2019-02-14 11:23:06,005 : PROGRESS (encoding): 14.56%
2019-02-14 11:25:03,545 : PROGRESS (encoding): 29.12%
2019-02-14 11:27:54,004 : PROGRESS (encoding): 43.69%
2019-02-14 11:30:51,272 : PROGRESS (encoding): 58.25%
2019-02-14 11:33:54,240 : PROGRESS (encoding): 72.81%
2019-02-14 11:36:18,503 : PROGRESS (encoding): 87.37%
2019-02-14 11:38:09,127 : PROGRESS (encoding): 0.00%
2019-02-14 11:38:22,853 : PROGRESS (encoding): 0.00%
2019-02-14 11:38:37,208 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:39:09,296 : [('reg:1e-09', 63.67)]
2019-02-14 11:39:09,297 : Validation : best param found is reg = 1e-09 with score             63.67
2019-02-14 11:39:09,297 : Evaluating...
2019-02-14 11:39:43,593 : Dev acc : 63.67 Test acc : 64.04 for SNLI

2019-02-14 11:39:43,593 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 11:39:52,580 : loading BERT model bert-base-uncased
2019-02-14 11:39:52,580 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:39:52,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:39:52,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuu5rsdnw
2019-02-14 11:39:55,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:39:56,516 : Computing embedding for train
2019-02-14 11:47:37,971 : Computed train embeddings
2019-02-14 11:47:37,971 : Computing embedding for dev
2019-02-14 11:47:57,494 : Computed dev embeddings
2019-02-14 11:47:57,495 : Computing embedding for test
2019-02-14 11:48:18,979 : Computed test embeddings
2019-02-14 11:48:18,995 : prepare data
2019-02-14 11:48:19,056 : start epoch
2019-02-14 11:48:59,125 : samples : 64000
2019-02-14 11:49:09,233 : Image to text: 7.66, 22.98, 34.4, 22.0
2019-02-14 11:49:16,651 : Text to Image: 6.368, 20.176, 30.836, 25.0
2019-02-14 11:49:57,170 : samples : 128000
2019-02-14 11:50:07,323 : Image to text: 8.6, 25.82, 37.14, 20.0
2019-02-14 11:50:14,731 : Text to Image: 6.788, 22.088, 33.408, 23.0
2019-02-14 11:50:55,912 : samples : 192000
2019-02-14 11:51:06,028 : Image to text: 10.0, 27.72, 39.36, 17.0
2019-02-14 11:51:13,426 : Text to Image: 8.18, 23.816, 35.152, 21.0
2019-02-14 11:51:55,982 : samples : 256000
2019-02-14 11:52:06,103 : Image to text: 9.36, 25.92, 39.08, 18.0
2019-02-14 11:52:13,522 : Text to Image: 8.288, 24.496, 36.06, 20.0
2019-02-14 11:52:53,617 : samples : 320000
2019-02-14 11:53:03,703 : Image to text: 9.48, 27.24, 39.16, 17.0
2019-02-14 11:53:11,079 : Text to Image: 8.064, 24.86, 36.392, 19.0
2019-02-14 11:53:53,375 : samples : 384000
2019-02-14 11:54:03,623 : Image to text: 9.94, 28.96, 41.62, 16.0
2019-02-14 11:54:11,028 : Text to Image: 8.968, 25.808, 37.572, 19.0
2019-02-14 11:54:52,304 : samples : 448000
2019-02-14 11:55:02,597 : Image to text: 10.18, 27.52, 40.74, 16.0
2019-02-14 11:55:10,011 : Text to Image: 8.272, 24.844, 36.82, 19.0
2019-02-14 11:55:52,110 : samples : 512000
2019-02-14 11:56:02,407 : Image to text: 11.1, 31.24, 43.24, 15.0
2019-02-14 11:56:09,837 : Text to Image: 9.136, 26.476, 38.5, 18.0
2019-02-14 11:56:45,186 : Epoch 1 finished
2019-02-14 11:56:45,635 : Image to text: 29.7, 60.5, 76.6, 4.0
2019-02-14 11:56:45,977 : Text to Image: 24.02, 57.46, 73.74, 4.0
2019-02-14 11:56:46,372 : Image to text: 26.6, 60.5, 75.6, 4.0
2019-02-14 11:56:46,670 : Text to Image: 23.36, 55.18, 72.92, 5.0
2019-02-14 11:56:47,065 : Image to text: 29.7, 61.1, 76.3, 4.0
2019-02-14 11:56:47,364 : Text to Image: 23.42, 56.96, 73.7, 4.0
2019-02-14 11:56:47,761 : Image to text: 28.9, 61.9, 76.2, 3.0
2019-02-14 11:56:48,066 : Text to Image: 24.12, 57.16, 73.7, 4.0
2019-02-14 11:56:48,483 : Image to text: 28.2, 61.9, 76.2, 3.0
2019-02-14 11:56:48,790 : Text to Image: 23.26, 57.66, 73.58, 4.0
2019-02-14 11:56:48,790 : Dev mean Text to Image: 23.636000000000003, 56.884, 73.528, 4.2
2019-02-14 11:56:48,790 : Dev mean Image to text: 28.619999999999997, 61.17999999999999, 76.17999999999999, 3.6000000000000005
2019-02-14 11:56:48,791 : start epoch
2019-02-14 11:57:29,641 : samples : 64000
2019-02-14 11:57:39,831 : Image to text: 11.32, 30.32, 43.48, 14.0
2019-02-14 11:57:47,266 : Text to Image: 9.348, 26.888, 39.012, 18.0
2019-02-14 11:58:28,653 : samples : 128000
2019-02-14 11:58:38,795 : Image to text: 10.44, 28.96, 41.64, 15.0
2019-02-14 11:58:46,246 : Text to Image: 9.316, 26.628, 38.772, 17.0
2019-02-14 11:59:29,658 : samples : 192000
2019-02-14 11:59:39,801 : Image to text: 11.22, 31.12, 44.0, 14.0
2019-02-14 11:59:47,206 : Text to Image: 9.652, 27.84, 40.08, 17.0
2019-02-14 12:00:28,688 : samples : 256000
2019-02-14 12:00:38,802 : Image to text: 12.28, 32.04, 44.46, 14.0
2019-02-14 12:00:46,171 : Text to Image: 9.832, 28.232, 40.404, 16.0
2019-02-14 12:01:27,923 : samples : 320000
2019-02-14 12:01:38,026 : Image to text: 11.24, 32.0, 45.56, 13.0
2019-02-14 12:01:45,409 : Text to Image: 9.844, 28.368, 40.26, 16.0
2019-02-14 12:02:27,881 : samples : 384000
2019-02-14 12:02:38,221 : Image to text: 12.0, 31.94, 44.5, 13.0
2019-02-14 12:02:45,675 : Text to Image: 10.068, 27.92, 40.132, 16.0
2019-02-14 12:03:27,831 : samples : 448000
2019-02-14 12:03:38,089 : Image to text: 12.4, 32.94, 46.04, 13.0
2019-02-14 12:03:45,512 : Text to Image: 10.34, 28.82, 40.824, 16.0
2019-02-14 12:04:27,319 : samples : 512000
2019-02-14 12:04:37,530 : Image to text: 11.8, 32.0, 44.44, 13.0
2019-02-14 12:04:44,941 : Text to Image: 10.172, 28.384, 40.608, 16.0
2019-02-14 12:05:20,900 : Epoch 2 finished
2019-02-14 12:05:21,296 : Image to text: 28.4, 62.3, 77.3, 4.0
2019-02-14 12:05:21,597 : Text to Image: 25.42, 59.48, 76.3, 4.0
2019-02-14 12:05:21,994 : Image to text: 29.7, 62.9, 76.7, 3.0
2019-02-14 12:05:22,294 : Text to Image: 24.52, 57.52, 74.92, 4.0
2019-02-14 12:05:22,700 : Image to text: 29.1, 66.2, 78.8, 3.0
2019-02-14 12:05:23,008 : Text to Image: 25.7, 59.1, 75.78, 4.0
2019-02-14 12:05:23,424 : Image to text: 30.0, 63.1, 77.0, 3.0
2019-02-14 12:05:23,734 : Text to Image: 25.34, 59.14, 74.62, 4.0
2019-02-14 12:05:24,155 : Image to text: 30.3, 64.6, 78.4, 3.0
2019-02-14 12:05:24,462 : Text to Image: 24.4, 59.96, 75.14, 4.0
2019-02-14 12:05:24,462 : Dev mean Text to Image: 25.075999999999997, 59.040000000000006, 75.352, 4.0
2019-02-14 12:05:24,462 : Dev mean Image to text: 29.5, 63.82000000000001, 77.64, 3.2
2019-02-14 12:05:24,462 : start epoch
2019-02-14 15:27:37,022 : ********************************************************************************
2019-02-14 15:27:37,022 : ********************************************************************************
2019-02-14 15:27:37,022 : ********************************************************************************
2019-02-14 15:27:37,022 : layer 5
2019-02-14 15:27:37,023 : ********************************************************************************
2019-02-14 15:27:37,023 : ********************************************************************************
2019-02-14 15:27:37,023 : ********************************************************************************
2019-02-14 15:27:37,023 : ***** Transfer task : STS12 *****


2019-02-14 15:27:37,063 : loading BERT model bert-base-uncased
2019-02-14 15:27:37,063 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:27:37,082 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:27:37,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp9abli8i
2019-02-14 15:27:39,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:27:45,671 : MSRpar : pearson = 0.4167, spearman = 0.4487
2019-02-14 15:27:46,415 : MSRvid : pearson = 0.5432, spearman = 0.5497
2019-02-14 15:27:47,050 : SMTeuroparl : pearson = 0.4812, spearman = 0.5806
2019-02-14 15:27:48,229 : surprise.OnWN : pearson = 0.6512, spearman = 0.6550
2019-02-14 15:27:48,877 : surprise.SMTnews : pearson = 0.6241, spearman = 0.5168
2019-02-14 15:27:48,877 : ALL (weighted average) : Pearson = 0.5400,             Spearman = 0.5511
2019-02-14 15:27:48,877 : ALL (average) : Pearson = 0.5433,             Spearman = 0.5502

2019-02-14 15:27:48,877 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 15:27:48,886 : loading BERT model bert-base-uncased
2019-02-14 15:27:48,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:27:48,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:27:48,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp363752a7
2019-02-14 15:27:51,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:27:53,623 : FNWN : pearson = 0.3742, spearman = 0.3874
2019-02-14 15:27:54,598 : headlines : pearson = 0.6364, spearman = 0.6133
2019-02-14 15:27:55,332 : OnWN : pearson = 0.4712, spearman = 0.5008
2019-02-14 15:27:55,333 : ALL (weighted average) : Pearson = 0.5416,             Spearman = 0.5428
2019-02-14 15:27:55,333 : ALL (average) : Pearson = 0.4939,             Spearman = 0.5005

2019-02-14 15:27:55,333 : ***** Transfer task : STS14 *****


2019-02-14 15:27:55,352 : loading BERT model bert-base-uncased
2019-02-14 15:27:55,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:27:55,403 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:27:55,403 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmgxazz9w
2019-02-14 15:27:57,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:28:00,041 : deft-forum : pearson = 0.3202, spearman = 0.3497
2019-02-14 15:28:00,820 : deft-news : pearson = 0.7425, spearman = 0.7111
2019-02-14 15:28:01,859 : headlines : pearson = 0.5986, spearman = 0.5515
2019-02-14 15:28:02,785 : images : pearson = 0.5862, spearman = 0.5811
2019-02-14 15:28:03,726 : OnWN : pearson = 0.6462, spearman = 0.6825
2019-02-14 15:28:04,960 : tweet-news : pearson = 0.6661, spearman = 0.6246
2019-02-14 15:28:04,961 : ALL (weighted average) : Pearson = 0.5973,             Spearman = 0.5868
2019-02-14 15:28:04,961 : ALL (average) : Pearson = 0.5933,             Spearman = 0.5834

2019-02-14 15:28:04,961 : ***** Transfer task : STS15 *****


2019-02-14 15:28:04,996 : loading BERT model bert-base-uncased
2019-02-14 15:28:04,996 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:28:05,014 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:28:05,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8nv155my
2019-02-14 15:28:07,467 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:28:09,743 : answers-forums : pearson = 0.5297, spearman = 0.5239
2019-02-14 15:28:10,643 : answers-students : pearson = 0.6772, spearman = 0.6842
2019-02-14 15:28:11,510 : belief : pearson = 0.6507, spearman = 0.6778
2019-02-14 15:28:12,531 : headlines : pearson = 0.6557, spearman = 0.6453
2019-02-14 15:28:13,504 : images : pearson = 0.7158, spearman = 0.7263
2019-02-14 15:28:13,505 : ALL (weighted average) : Pearson = 0.6597,             Spearman = 0.6642
2019-02-14 15:28:13,505 : ALL (average) : Pearson = 0.6458,             Spearman = 0.6515

2019-02-14 15:28:13,505 : ***** Transfer task : STS16 *****


2019-02-14 15:28:13,581 : loading BERT model bert-base-uncased
2019-02-14 15:28:13,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:28:13,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:28:13,599 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb317er62
2019-02-14 15:28:16,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:28:17,969 : answer-answer : pearson = 0.5104, spearman = 0.5336
2019-02-14 15:28:18,278 : headlines : pearson = 0.6597, spearman = 0.6599
2019-02-14 15:28:18,674 : plagiarism : pearson = 0.7512, spearman = 0.7620
2019-02-14 15:28:19,306 : postediting : pearson = 0.7949, spearman = 0.8308
2019-02-14 15:28:19,585 : question-question : pearson = 0.3393, spearman = 0.3394
2019-02-14 15:28:19,585 : ALL (weighted average) : Pearson = 0.6168,             Spearman = 0.6313
2019-02-14 15:28:19,585 : ALL (average) : Pearson = 0.6111,             Spearman = 0.6252

2019-02-14 15:28:19,586 : ***** Transfer task : MR *****


2019-02-14 15:28:19,607 : loading BERT model bert-base-uncased
2019-02-14 15:28:19,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:28:19,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:28:19,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjckptw3m
2019-02-14 15:28:22,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:28:23,645 : Generating sentence embeddings
2019-02-14 15:28:37,061 : Generated sentence embeddings
2019-02-14 15:28:37,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 15:28:47,852 : Best param found at split 1: l2reg = 0.01                 with score 77.12
2019-02-14 15:28:58,588 : Best param found at split 2: l2reg = 0.001                 with score 76.73
2019-02-14 15:29:09,758 : Best param found at split 3: l2reg = 0.001                 with score 77.27
2019-02-14 15:29:20,777 : Best param found at split 4: l2reg = 0.001                 with score 76.87
2019-02-14 15:29:33,756 : Best param found at split 5: l2reg = 0.0001                 with score 77.23
2019-02-14 15:29:34,454 : Dev acc : 77.04 Test acc : 76.63

2019-02-14 15:29:34,456 : ***** Transfer task : CR *****


2019-02-14 15:29:34,463 : loading BERT model bert-base-uncased
2019-02-14 15:29:34,463 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:29:34,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:29:34,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppq9u89bo
2019-02-14 15:29:36,937 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:29:38,377 : Generating sentence embeddings
2019-02-14 15:29:42,083 : Generated sentence embeddings
2019-02-14 15:29:42,084 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 15:29:46,039 : Best param found at split 1: l2reg = 0.0001                 with score 80.29
2019-02-14 15:29:49,831 : Best param found at split 2: l2reg = 0.001                 with score 81.05
2019-02-14 15:29:53,912 : Best param found at split 3: l2reg = 1e-05                 with score 81.49
2019-02-14 15:29:57,666 : Best param found at split 4: l2reg = 0.001                 with score 80.67
2019-02-14 15:30:01,857 : Best param found at split 5: l2reg = 1e-05                 with score 80.6
2019-02-14 15:30:02,059 : Dev acc : 80.82 Test acc : 80.45

2019-02-14 15:30:02,060 : ***** Transfer task : MPQA *****


2019-02-14 15:30:02,065 : loading BERT model bert-base-uncased
2019-02-14 15:30:02,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:30:02,084 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:30:02,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptl9u7vob
2019-02-14 15:30:04,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:30:05,955 : Generating sentence embeddings
2019-02-14 15:30:09,685 : Generated sentence embeddings
2019-02-14 15:30:09,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 15:30:22,332 : Best param found at split 1: l2reg = 0.001                 with score 87.52
2019-02-14 15:30:34,978 : Best param found at split 2: l2reg = 0.001                 with score 87.81
2019-02-14 15:30:48,681 : Best param found at split 3: l2reg = 0.01                 with score 87.25
2019-02-14 15:31:00,271 : Best param found at split 4: l2reg = 0.0001                 with score 87.86
2019-02-14 15:31:13,983 : Best param found at split 5: l2reg = 0.001                 with score 87.51
2019-02-14 15:31:14,784 : Dev acc : 87.59 Test acc : 88.03

2019-02-14 15:31:14,785 : ***** Transfer task : SUBJ *****


2019-02-14 15:31:14,803 : loading BERT model bert-base-uncased
2019-02-14 15:31:14,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:31:14,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:31:14,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvstooepj
2019-02-14 15:31:17,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:31:18,746 : Generating sentence embeddings
2019-02-14 15:31:32,080 : Generated sentence embeddings
2019-02-14 15:31:32,081 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 15:31:43,422 : Best param found at split 1: l2reg = 1e-05                 with score 94.59
2019-02-14 15:31:54,728 : Best param found at split 2: l2reg = 1e-05                 with score 94.9
2019-02-14 15:32:05,509 : Best param found at split 3: l2reg = 0.001                 with score 94.39
2019-02-14 15:32:17,882 : Best param found at split 4: l2reg = 0.001                 with score 94.91
2019-02-14 15:32:29,308 : Best param found at split 5: l2reg = 0.001                 with score 94.45
2019-02-14 15:32:29,964 : Dev acc : 94.65 Test acc : 94.29

2019-02-14 15:32:29,965 : ***** Transfer task : SST Binary classification *****


2019-02-14 15:32:30,103 : loading BERT model bert-base-uncased
2019-02-14 15:32:30,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:32:30,128 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:32:30,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0rt_7f5i
2019-02-14 15:32:32,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:32:34,034 : Computing embedding for train
2019-02-14 15:33:19,362 : Computed train embeddings
2019-02-14 15:33:19,363 : Computing embedding for dev
2019-02-14 15:33:20,419 : Computed dev embeddings
2019-02-14 15:33:20,419 : Computing embedding for test
2019-02-14 15:33:22,690 : Computed test embeddings
2019-02-14 15:33:22,690 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:34:05,307 : [('reg:1e-05', 80.62), ('reg:0.0001', 80.73), ('reg:0.001', 81.31), ('reg:0.01', 80.73)]
2019-02-14 15:34:05,307 : Validation : best param found is reg = 0.001 with score             81.31
2019-02-14 15:34:05,307 : Evaluating...
2019-02-14 15:34:16,791 : 
Dev acc : 81.31 Test acc : 82.04 for             SST Binary classification

2019-02-14 15:34:16,791 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 15:34:16,841 : loading BERT model bert-base-uncased
2019-02-14 15:34:16,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:34:16,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:34:16,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpty62uvws
2019-02-14 15:34:19,305 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:34:20,792 : Computing embedding for train
2019-02-14 15:34:35,141 : Computed train embeddings
2019-02-14 15:34:35,142 : Computing embedding for dev
2019-02-14 15:34:37,049 : Computed dev embeddings
2019-02-14 15:34:37,049 : Computing embedding for test
2019-02-14 15:34:40,891 : Computed test embeddings
2019-02-14 15:34:40,891 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:34:47,069 : [('reg:1e-05', 41.05), ('reg:0.0001', 40.96), ('reg:0.001', 41.05), ('reg:0.01', 41.33)]
2019-02-14 15:34:47,069 : Validation : best param found is reg = 0.01 with score             41.33
2019-02-14 15:34:47,069 : Evaluating...
2019-02-14 15:34:48,388 : 
Dev acc : 41.33 Test acc : 42.35 for             SST Fine-Grained classification

2019-02-14 15:34:48,388 : ***** Transfer task : TREC *****


2019-02-14 15:34:48,402 : loading BERT model bert-base-uncased
2019-02-14 15:34:48,402 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:34:48,421 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:34:48,421 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgavks8nu
2019-02-14 15:34:50,893 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:34:58,462 : Computed train embeddings
2019-02-14 15:34:58,884 : Computed test embeddings
2019-02-14 15:34:58,884 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 15:35:10,095 : [('reg:1e-05', 83.05), ('reg:0.0001', 83.09), ('reg:0.001', 81.64), ('reg:0.01', 75.74)]
2019-02-14 15:35:10,095 : Cross-validation : best param found is reg = 0.0001             with score 83.09
2019-02-14 15:35:10,095 : Evaluating...
2019-02-14 15:35:10,546 : 
Dev acc : 83.09 Test acc : 91.0             for TREC

2019-02-14 15:35:10,547 : ***** Transfer task : MRPC *****


2019-02-14 15:35:10,597 : loading BERT model bert-base-uncased
2019-02-14 15:35:10,598 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:35:10,618 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:35:10,618 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpojuo2oof
2019-02-14 15:35:13,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:35:14,474 : Computing embedding for train
2019-02-14 15:35:25,301 : Computed train embeddings
2019-02-14 15:35:25,301 : Computing embedding for test
2019-02-14 15:35:29,953 : Computed test embeddings
2019-02-14 15:35:29,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 15:35:36,808 : [('reg:1e-05', 73.87), ('reg:0.0001', 73.92), ('reg:0.001', 73.82), ('reg:0.01', 73.53)]
2019-02-14 15:35:36,808 : Cross-validation : best param found is reg = 0.0001             with score 73.92
2019-02-14 15:35:36,808 : Evaluating...
2019-02-14 15:35:37,217 : Dev acc : 73.92 Test acc 74.14; Test F1 80.34 for MRPC.

2019-02-14 15:35:37,217 : ***** Transfer task : SICK-Entailment*****


2019-02-14 15:35:37,241 : loading BERT model bert-base-uncased
2019-02-14 15:35:37,241 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:35:37,296 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:35:37,297 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5n87_w9
2019-02-14 15:35:39,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:35:41,149 : Computing embedding for train
2019-02-14 15:35:47,534 : Computed train embeddings
2019-02-14 15:35:47,535 : Computing embedding for dev
2019-02-14 15:35:48,347 : Computed dev embeddings
2019-02-14 15:35:48,347 : Computing embedding for test
2019-02-14 15:35:55,237 : Computed test embeddings
2019-02-14 15:35:55,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:35:56,831 : [('reg:1e-05', 80.2), ('reg:0.0001', 80.0), ('reg:0.001', 79.4), ('reg:0.01', 74.0)]
2019-02-14 15:35:56,831 : Validation : best param found is reg = 1e-05 with score             80.2
2019-02-14 15:35:56,831 : Evaluating...
2019-02-14 15:35:57,175 : 
Dev acc : 80.2 Test acc : 78.71 for                        SICK entailment

2019-02-14 15:35:57,176 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 15:35:57,203 : loading BERT model bert-base-uncased
2019-02-14 15:35:57,203 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:35:57,223 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:35:57,223 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr_7211mh
2019-02-14 15:35:59,694 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:36:01,199 : Computing embedding for train
2019-02-14 15:36:08,251 : Computed train embeddings
2019-02-14 15:36:08,251 : Computing embedding for dev
2019-02-14 15:36:09,186 : Computed dev embeddings
2019-02-14 15:36:09,186 : Computing embedding for test
2019-02-14 15:36:17,563 : Computed test embeddings
2019-02-14 15:36:58,091 : Dev : Pearson 0.8062596673158293
2019-02-14 15:36:58,091 : Test : Pearson 0.8068017446765752 Spearman 0.7402394011926606 MSE 0.3554067758133547                        for SICK Relatedness

2019-02-14 15:36:58,092 : 

***** Transfer task : STSBenchmark*****


2019-02-14 15:36:58,154 : loading BERT model bert-base-uncased
2019-02-14 15:36:58,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:36:58,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:36:58,175 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp7ws20x4
2019-02-14 15:37:00,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:37:02,118 : Computing embedding for train
2019-02-14 15:37:16,974 : Computed train embeddings
2019-02-14 15:37:16,975 : Computing embedding for dev
2019-02-14 15:37:21,334 : Computed dev embeddings
2019-02-14 15:37:21,334 : Computing embedding for test
2019-02-14 15:37:25,213 : Computed test embeddings
2019-02-14 15:38:03,932 : Dev : Pearson 0.7480499822719735
2019-02-14 15:38:03,932 : Test : Pearson 0.6836250222714078 Spearman 0.6770241178091033 MSE 1.4143526060633713                        for SICK Relatedness

2019-02-14 15:38:03,932 : ***** Transfer task : SNLI Entailment*****


2019-02-14 15:38:08,735 : loading BERT model bert-base-uncased
2019-02-14 15:38:08,735 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:38:08,857 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:38:08,857 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgn_5frlj
2019-02-14 15:38:11,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:38:13,045 : PROGRESS (encoding): 0.00%
2019-02-14 15:40:25,112 : PROGRESS (encoding): 14.56%
2019-02-14 15:42:20,249 : PROGRESS (encoding): 29.12%
2019-02-14 15:44:35,057 : PROGRESS (encoding): 43.69%
2019-02-14 15:46:59,846 : PROGRESS (encoding): 58.25%
2019-02-14 15:49:08,472 : PROGRESS (encoding): 72.81%
2019-02-14 15:51:41,776 : PROGRESS (encoding): 87.37%
2019-02-14 15:54:13,145 : PROGRESS (encoding): 0.00%
2019-02-14 15:54:33,103 : PROGRESS (encoding): 0.00%
2019-02-14 15:54:48,899 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:55:21,323 : [('reg:1e-09', 63.67)]
2019-02-14 15:55:21,323 : Validation : best param found is reg = 1e-09 with score             63.67
2019-02-14 15:55:21,323 : Evaluating...
2019-02-14 15:56:12,053 : Dev acc : 63.67 Test acc : 64.04 for SNLI

2019-02-14 15:56:12,054 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 15:56:20,779 : loading BERT model bert-base-uncased
2019-02-14 15:56:20,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:56:20,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:56:20,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphe57m5fp
2019-02-14 15:56:23,262 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:56:24,756 : Computing embedding for train
2019-02-14 16:06:44,039 : Computed train embeddings
2019-02-14 16:06:44,039 : Computing embedding for dev
2019-02-14 16:07:06,739 : Computed dev embeddings
2019-02-14 16:07:06,740 : Computing embedding for test
2019-02-14 16:07:30,467 : Computed test embeddings
2019-02-14 16:07:30,486 : prepare data
2019-02-14 16:07:30,548 : start epoch
2019-02-14 16:08:12,213 : samples : 64000
2019-02-14 16:08:24,025 : Image to text: 7.66, 22.98, 34.4, 22.0
2019-02-14 16:08:31,175 : Text to Image: 6.368, 20.176, 30.836, 25.0
2019-02-14 16:09:11,572 : samples : 128000
2019-02-14 16:09:21,531 : Image to text: 8.6, 25.82, 37.14, 20.0
2019-02-14 16:09:29,755 : Text to Image: 6.788, 22.088, 33.408, 23.0
2019-02-14 16:10:10,426 : samples : 192000
2019-02-14 16:10:20,431 : Image to text: 10.0, 27.72, 39.36, 17.0
2019-02-14 16:10:27,667 : Text to Image: 8.18, 23.816, 35.152, 21.0
2019-02-14 16:11:08,761 : samples : 256000
2019-02-14 16:11:20,808 : Image to text: 9.36, 25.92, 39.08, 18.0
2019-02-14 16:11:27,964 : Text to Image: 8.288, 24.496, 36.06, 20.0
2019-02-14 16:12:08,741 : samples : 320000
2019-02-14 16:12:18,790 : Image to text: 9.48, 27.24, 39.16, 17.0
2019-02-14 16:12:26,026 : Text to Image: 8.064, 24.86, 36.392, 19.0
2019-02-14 16:13:09,127 : samples : 384000
2019-02-14 16:13:21,763 : Image to text: 9.94, 28.96, 41.62, 16.0
2019-02-14 16:13:28,996 : Text to Image: 8.968, 25.808, 37.572, 19.0
2019-02-14 16:14:10,505 : samples : 448000
2019-02-14 16:14:21,393 : Image to text: 10.18, 27.52, 40.74, 16.0
2019-02-14 16:14:28,402 : Text to Image: 8.272, 24.844, 36.82, 19.0
2019-02-14 16:15:10,356 : samples : 512000
2019-02-14 16:15:22,596 : Image to text: 11.1, 31.24, 43.24, 15.0
2019-02-14 16:15:29,818 : Text to Image: 9.136, 26.476, 38.5, 18.0
2019-02-14 16:16:04,093 : Epoch 1 finished
2019-02-14 16:16:04,544 : Image to text: 29.7, 60.5, 76.6, 4.0
2019-02-14 16:16:04,912 : Text to Image: 24.02, 57.46, 73.74, 4.0
2019-02-14 16:16:05,363 : Image to text: 26.6, 60.5, 75.6, 4.0
2019-02-14 16:16:05,731 : Text to Image: 23.36, 55.18, 72.92, 5.0
2019-02-14 16:16:06,182 : Image to text: 29.7, 61.1, 76.3, 4.0
2019-02-14 16:16:06,550 : Text to Image: 23.42, 56.96, 73.7, 4.0
2019-02-14 16:16:07,001 : Image to text: 28.9, 61.9, 76.2, 3.0
2019-02-14 16:16:07,369 : Text to Image: 24.12, 57.16, 73.7, 4.0
2019-02-14 16:16:07,820 : Image to text: 28.2, 61.9, 76.2, 3.0
2019-02-14 16:16:08,187 : Text to Image: 23.26, 57.66, 73.58, 4.0
2019-02-14 16:16:08,188 : Dev mean Text to Image: 23.636000000000003, 56.884, 73.528, 4.2
2019-02-14 16:16:08,188 : Dev mean Image to text: 28.619999999999997, 61.17999999999999, 76.17999999999999, 3.6000000000000005
2019-02-14 16:16:08,188 : start epoch
2019-02-14 16:16:49,245 : samples : 64000
2019-02-14 16:16:59,282 : Image to text: 11.32, 30.32, 43.48, 14.0
2019-02-14 16:17:06,521 : Text to Image: 9.348, 26.888, 39.012, 18.0
2019-02-14 16:17:47,927 : samples : 128000
2019-02-14 16:18:00,415 : Image to text: 10.44, 28.96, 41.64, 15.0
2019-02-14 16:18:10,341 : Text to Image: 9.316, 26.628, 38.772, 17.0
2019-02-14 16:18:53,823 : samples : 192000
2019-02-14 16:19:06,336 : Image to text: 11.22, 31.12, 44.0, 14.0
2019-02-14 16:19:16,333 : Text to Image: 9.652, 27.84, 40.08, 17.0
2019-02-14 16:19:59,886 : samples : 256000
2019-02-14 16:20:12,454 : Image to text: 12.28, 32.04, 44.46, 14.0
2019-02-14 16:20:22,441 : Text to Image: 9.832, 28.232, 40.404, 16.0
2019-02-14 16:21:06,200 : samples : 320000
2019-02-14 16:21:18,788 : Image to text: 11.24, 32.0, 45.56, 13.0
2019-02-14 16:21:28,779 : Text to Image: 9.844, 28.368, 40.26, 16.0
2019-02-14 16:22:12,382 : samples : 384000
2019-02-14 16:22:25,056 : Image to text: 12.0, 31.94, 44.5, 13.0
2019-02-14 16:22:35,057 : Text to Image: 10.068, 27.92, 40.132, 16.0
2019-02-14 16:23:17,734 : samples : 448000
2019-02-14 16:23:30,303 : Image to text: 12.4, 32.94, 46.04, 13.0
2019-02-14 16:23:40,374 : Text to Image: 10.34, 28.82, 40.824, 16.0
2019-02-14 16:24:24,280 : samples : 512000
2019-02-14 16:24:36,870 : Image to text: 11.8, 32.0, 44.44, 13.0
2019-02-14 16:24:46,898 : Text to Image: 10.172, 28.384, 40.608, 16.0
2019-02-14 16:25:23,982 : Epoch 2 finished
2019-02-14 16:25:24,898 : Image to text: 28.4, 62.3, 77.3, 4.0
2019-02-14 16:25:25,645 : Text to Image: 25.42, 59.48, 76.3, 4.0
2019-02-14 16:25:26,578 : Image to text: 29.7, 62.9, 76.7, 3.0
2019-02-14 16:25:27,332 : Text to Image: 24.52, 57.52, 74.92, 4.0
2019-02-14 16:25:28,211 : Image to text: 29.1, 66.2, 78.8, 3.0
2019-02-14 16:25:28,954 : Text to Image: 25.7, 59.1, 75.78, 4.0
2019-02-14 16:25:29,887 : Image to text: 30.0, 63.1, 77.0, 3.0
2019-02-14 16:25:30,653 : Text to Image: 25.34, 59.14, 74.62, 4.0
2019-02-14 16:25:31,549 : Image to text: 30.3, 64.6, 78.4, 3.0
2019-02-14 16:25:32,321 : Text to Image: 24.4, 59.96, 75.14, 4.0
2019-02-14 16:25:32,321 : Dev mean Text to Image: 25.075999999999997, 59.040000000000006, 75.352, 4.0
2019-02-14 16:25:32,321 : Dev mean Image to text: 29.5, 63.82000000000001, 77.64, 3.2
2019-02-14 16:25:32,322 : start epoch
2019-02-14 16:26:14,632 : samples : 64000
2019-02-14 16:26:27,237 : Image to text: 12.84, 33.46, 46.6, 12.0
2019-02-14 16:26:37,225 : Text to Image: 10.536, 28.832, 41.332, 16.0
2019-02-14 16:27:19,628 : samples : 128000
2019-02-14 16:27:32,265 : Image to text: 12.02, 32.6, 45.86, 13.0
2019-02-14 16:27:42,252 : Text to Image: 10.276, 28.26, 40.9, 16.0
2019-02-14 16:28:25,685 : samples : 192000
2019-02-14 16:28:38,355 : Image to text: 13.02, 33.64, 46.68, 12.0
2019-02-14 16:28:48,557 : Text to Image: 10.428, 29.296, 41.6, 15.0
2019-02-14 16:29:30,989 : samples : 256000
2019-02-14 16:29:41,048 : Image to text: 13.02, 34.32, 47.16, 12.0
2019-02-14 16:29:48,274 : Text to Image: 10.712, 29.948, 42.708, 15.0
2019-02-14 16:30:28,500 : samples : 320000
2019-02-14 16:30:38,585 : Image to text: 12.0, 32.42, 45.34, 13.0
2019-02-14 16:30:45,809 : Text to Image: 9.98, 28.324, 40.992, 16.0
2019-02-14 16:31:27,784 : samples : 384000
2019-02-14 16:31:40,684 : Image to text: 12.08, 32.96, 46.64, 12.0
2019-02-14 16:31:51,075 : Text to Image: 10.46, 29.408, 41.632, 15.0
2019-02-14 16:32:35,047 : samples : 448000
2019-02-14 16:32:47,962 : Image to text: 13.34, 35.02, 48.42, 11.0
2019-02-14 16:32:58,319 : Text to Image: 10.872, 30.372, 42.724, 15.0
2019-02-14 16:33:43,609 : samples : 512000
2019-02-14 16:33:56,559 : Image to text: 13.16, 34.34, 48.32, 11.0
2019-02-14 16:34:07,040 : Text to Image: 10.948, 30.44, 42.764, 15.0
2019-02-14 16:34:45,262 : Epoch 3 finished
2019-02-14 16:34:46,341 : Image to text: 30.5, 63.7, 80.1, 3.0
2019-02-14 16:34:47,225 : Text to Image: 26.52, 61.34, 77.84, 4.0
2019-02-14 16:34:48,287 : Image to text: 31.1, 66.3, 80.6, 3.0
2019-02-14 16:34:49,160 : Text to Image: 26.16, 59.88, 76.52, 4.0
2019-02-14 16:34:50,382 : Image to text: 32.5, 65.1, 79.9, 3.0
2019-02-14 16:34:51,356 : Text to Image: 27.24, 61.24, 77.02, 4.0
2019-02-14 16:34:52,474 : Image to text: 34.6, 66.7, 79.7, 3.0
2019-02-14 16:34:53,370 : Text to Image: 27.26, 61.82, 77.32, 4.0
2019-02-14 16:34:54,431 : Image to text: 34.4, 67.5, 80.9, 3.0
2019-02-14 16:34:55,359 : Text to Image: 26.08, 61.18, 77.02, 4.0
2019-02-14 16:34:55,359 : Dev mean Text to Image: 26.652, 61.09200000000001, 77.14399999999999, 4.0
2019-02-14 16:34:55,359 : Dev mean Image to text: 32.620000000000005, 65.86, 80.24000000000001, 3.0
2019-02-14 16:34:55,360 : start epoch
2019-02-14 16:35:40,532 : samples : 64000
2019-02-14 16:35:53,561 : Image to text: 12.74, 35.3, 48.18, 11.0
2019-02-14 16:36:04,096 : Text to Image: 10.696, 29.992, 42.768, 15.0
2019-02-14 16:36:49,415 : samples : 128000
2019-02-14 16:37:02,332 : Image to text: 12.78, 34.3, 47.82, 11.0
2019-02-14 16:37:12,793 : Text to Image: 10.624, 30.016, 42.764, 15.0
2019-02-14 16:37:57,250 : samples : 192000
2019-02-14 16:38:10,227 : Image to text: 12.7, 33.84, 48.1, 12.0
2019-02-14 16:38:20,692 : Text to Image: 10.776, 29.572, 42.124, 15.0
2019-02-14 16:39:05,763 : samples : 256000
2019-02-14 16:39:18,693 : Image to text: 13.5, 34.18, 48.6, 11.0
2019-02-14 16:39:27,603 : Text to Image: 11.108, 30.44, 43.184, 14.0
2019-02-14 16:40:11,185 : samples : 320000
2019-02-14 16:40:21,282 : Image to text: 13.16, 34.8, 48.78, 11.0
2019-02-14 16:40:28,657 : Text to Image: 11.18, 30.32, 42.676, 15.0
2019-02-14 16:41:11,652 : samples : 384000
2019-02-14 16:41:21,732 : Image to text: 13.38, 35.8, 48.4, 11.0
2019-02-14 16:41:29,067 : Text to Image: 11.04, 29.908, 42.548, 15.0
2019-02-14 16:42:11,775 : samples : 448000
2019-02-14 16:42:21,840 : Image to text: 13.86, 35.92, 49.56, 11.0
2019-02-14 16:42:29,159 : Text to Image: 11.536, 30.776, 43.244, 14.0
2019-02-14 16:43:11,578 : samples : 512000
2019-02-14 16:43:21,623 : Image to text: 13.54, 35.12, 48.54, 11.0
2019-02-14 16:43:28,938 : Text to Image: 11.068, 30.396, 43.084, 15.0
2019-02-14 16:44:05,212 : Epoch 4 finished
2019-02-14 16:44:05,646 : Image to text: 31.5, 64.8, 80.9, 3.0
2019-02-14 16:44:05,984 : Text to Image: 27.28, 61.8, 77.94, 3.0
2019-02-14 16:44:06,446 : Image to text: 33.6, 66.0, 79.2, 3.0
2019-02-14 16:44:06,777 : Text to Image: 27.2, 59.68, 76.46, 4.0
2019-02-14 16:44:07,211 : Image to text: 31.2, 66.2, 80.8, 3.0
2019-02-14 16:44:07,544 : Text to Image: 27.1, 62.26, 77.9, 4.0
2019-02-14 16:44:07,984 : Image to text: 32.7, 68.6, 81.1, 3.0
2019-02-14 16:44:08,317 : Text to Image: 27.14, 61.6, 77.36, 3.0
2019-02-14 16:44:08,802 : Image to text: 34.5, 67.6, 81.4, 3.0
2019-02-14 16:44:09,142 : Text to Image: 26.96, 61.98, 77.48, 4.0
2019-02-14 16:44:09,142 : Dev mean Text to Image: 27.136000000000003, 61.464, 77.428, 3.6000000000000005
2019-02-14 16:44:09,142 : Dev mean Image to text: 32.699999999999996, 66.64, 80.67999999999999, 3.0
2019-02-14 16:44:09,143 : start epoch
2019-02-14 16:44:51,039 : samples : 64000
2019-02-14 16:45:01,367 : Image to text: 14.14, 36.32, 49.5, 11.0
2019-02-14 16:45:08,794 : Text to Image: 11.412, 31.256, 44.052, 14.0
2019-02-14 16:45:50,895 : samples : 128000
2019-02-14 16:46:01,197 : Image to text: 13.1, 35.14, 48.96, 11.0
2019-02-14 16:46:08,608 : Text to Image: 11.232, 30.564, 43.352, 14.0
2019-02-14 16:46:50,471 : samples : 192000
2019-02-14 16:47:00,812 : Image to text: 12.56, 33.92, 47.44, 12.0
2019-02-14 16:47:08,216 : Text to Image: 10.76, 30.236, 43.048, 14.0
2019-02-14 16:47:50,219 : samples : 256000
2019-02-14 16:48:00,533 : Image to text: 13.76, 34.82, 49.1, 11.0
2019-02-14 16:48:07,901 : Text to Image: 11.304, 30.516, 43.54, 14.0
2019-02-14 16:48:50,251 : samples : 320000
2019-02-14 16:49:00,293 : Image to text: 13.98, 36.12, 49.6, 11.0
2019-02-14 16:49:07,566 : Text to Image: 11.268, 31.452, 44.036, 14.0
2019-02-14 16:49:50,292 : samples : 384000
2019-02-14 16:50:00,319 : Image to text: 14.06, 36.12, 49.34, 11.0
2019-02-14 16:50:07,577 : Text to Image: 11.236, 30.96, 43.9, 14.0
2019-02-14 16:50:50,571 : samples : 448000
2019-02-14 16:51:00,603 : Image to text: 13.6, 34.58, 48.5, 11.0
2019-02-14 16:51:07,914 : Text to Image: 10.948, 30.296, 42.972, 15.0
2019-02-14 16:51:50,502 : samples : 512000
2019-02-14 16:52:00,544 : Image to text: 14.24, 36.0, 49.46, 11.0
2019-02-14 16:52:07,796 : Text to Image: 11.256, 30.876, 43.768, 14.0
2019-02-14 16:52:44,795 : Epoch 5 finished
2019-02-14 16:52:45,239 : Image to text: 32.7, 64.2, 79.6, 3.0
2019-02-14 16:52:45,571 : Text to Image: 26.36, 61.92, 78.84, 4.0
2019-02-14 16:52:46,003 : Image to text: 34.0, 67.9, 80.2, 3.0
2019-02-14 16:52:46,335 : Text to Image: 26.06, 59.82, 76.86, 4.0
2019-02-14 16:52:46,775 : Image to text: 33.1, 66.4, 79.8, 3.0
2019-02-14 16:52:47,105 : Text to Image: 27.3, 61.98, 77.96, 3.0
2019-02-14 16:52:47,533 : Image to text: 33.3, 67.2, 81.4, 3.0
2019-02-14 16:52:47,862 : Text to Image: 27.4, 61.66, 77.32, 3.0
2019-02-14 16:52:48,301 : Image to text: 33.4, 68.3, 80.6, 3.0
2019-02-14 16:52:48,631 : Text to Image: 27.46, 62.12, 77.66, 4.0
2019-02-14 16:52:48,632 : Dev mean Text to Image: 26.916, 61.5, 77.728, 3.6000000000000005
2019-02-14 16:52:48,632 : Dev mean Image to text: 33.3, 66.8, 80.32, 3.0
2019-02-14 16:52:48,632 : start epoch
2019-02-14 16:53:30,163 : samples : 64000
2019-02-14 16:53:40,394 : Image to text: 13.72, 35.68, 50.06, 10.0
2019-02-14 16:53:47,770 : Text to Image: 11.468, 31.416, 44.048, 14.0
2019-02-14 16:54:28,552 : samples : 128000
2019-02-14 16:54:38,802 : Image to text: 14.06, 35.84, 49.36, 11.0
2019-02-14 16:54:46,236 : Text to Image: 11.444, 30.728, 43.604, 14.0
2019-02-14 16:55:28,264 : samples : 192000
2019-02-14 16:55:38,625 : Image to text: 13.86, 36.48, 50.24, 10.0
2019-02-14 16:55:46,067 : Text to Image: 11.56, 31.172, 43.744, 14.0
2019-02-14 16:56:27,796 : samples : 256000
2019-02-14 16:56:38,117 : Image to text: 14.1, 36.5, 50.3, 10.0
2019-02-14 16:56:45,521 : Text to Image: 11.452, 31.268, 44.22, 14.0
2019-02-14 16:57:27,700 : samples : 320000
2019-02-14 16:57:37,787 : Image to text: 13.76, 36.34, 49.94, 11.0
2019-02-14 16:57:45,098 : Text to Image: 11.524, 31.168, 43.976, 14.0
2019-02-14 16:58:28,681 : samples : 384000
2019-02-14 16:58:38,787 : Image to text: 14.46, 37.22, 52.14, 10.0
2019-02-14 16:58:46,108 : Text to Image: 11.728, 31.464, 44.584, 13.0
2019-02-14 16:59:28,833 : samples : 448000
2019-02-14 16:59:38,912 : Image to text: 14.0, 37.22, 50.56, 10.0
2019-02-14 16:59:46,185 : Text to Image: 11.736, 31.824, 44.624, 13.0
2019-02-14 17:00:34,424 : samples : 512000
2019-02-14 17:00:44,525 : Image to text: 14.04, 36.06, 49.7, 11.0
2019-02-14 17:00:51,828 : Text to Image: 11.54, 31.648, 44.308, 14.0
2019-02-14 17:01:27,143 : Epoch 6 finished
2019-02-14 17:01:27,560 : Image to text: 32.7, 65.6, 80.1, 3.0
2019-02-14 17:01:27,884 : Text to Image: 27.88, 62.74, 78.76, 3.0
2019-02-14 17:01:28,313 : Image to text: 33.6, 68.1, 80.0, 3.0
2019-02-14 17:01:28,638 : Text to Image: 28.26, 61.26, 77.76, 4.0
2019-02-14 17:01:29,060 : Image to text: 34.3, 66.9, 81.6, 3.0
2019-02-14 17:01:29,389 : Text to Image: 28.32, 63.34, 78.7, 3.0
2019-02-14 17:01:29,829 : Image to text: 34.1, 69.1, 81.2, 2.0
2019-02-14 17:01:30,160 : Text to Image: 27.86, 62.46, 78.28, 3.0
2019-02-14 17:01:30,592 : Image to text: 36.3, 69.1, 81.9, 3.0
2019-02-14 17:01:30,924 : Text to Image: 28.38, 62.9, 77.68, 3.0
2019-02-14 17:01:30,924 : Dev mean Text to Image: 28.14, 62.53999999999999, 78.236, 3.2
2019-02-14 17:01:30,924 : Dev mean Image to text: 34.2, 67.75999999999999, 80.96000000000001, 2.8
2019-02-14 17:01:30,924 : start epoch
2019-02-14 17:02:11,493 : samples : 64000
2019-02-14 17:02:21,906 : Image to text: 14.5, 36.88, 50.46, 10.0
2019-02-14 17:02:29,344 : Text to Image: 11.66, 31.58, 44.364, 14.0
2019-02-14 17:03:10,234 : samples : 128000
2019-02-14 17:03:20,587 : Image to text: 14.38, 36.94, 51.24, 10.0
2019-02-14 17:03:28,052 : Text to Image: 11.712, 31.584, 44.804, 13.0
2019-02-14 17:04:08,294 : samples : 192000
2019-02-14 17:04:18,683 : Image to text: 15.12, 37.34, 50.42, 10.0
2019-02-14 17:04:26,100 : Text to Image: 12.004, 32.596, 45.232, 13.0
2019-02-14 17:05:07,199 : samples : 256000
2019-02-14 17:05:17,557 : Image to text: 13.62, 35.42, 49.18, 11.0
2019-02-14 17:05:24,958 : Text to Image: 11.592, 31.876, 44.504, 14.0
2019-02-14 17:06:04,390 : samples : 320000
2019-02-14 17:06:14,488 : Image to text: 14.86, 36.7, 50.36, 10.0
2019-02-14 17:06:21,822 : Text to Image: 11.992, 31.92, 44.932, 13.0
2019-02-14 17:07:01,612 : samples : 384000
2019-02-14 17:07:11,690 : Image to text: 14.18, 37.44, 51.04, 10.0
2019-02-14 17:07:18,990 : Text to Image: 11.804, 31.92, 45.096, 13.0
2019-02-14 17:07:58,430 : samples : 448000
2019-02-14 17:08:08,505 : Image to text: 14.38, 37.02, 51.38, 10.0
2019-02-14 17:08:15,805 : Text to Image: 11.944, 32.252, 45.172, 13.0
2019-02-14 17:08:55,424 : samples : 512000
2019-02-14 17:09:05,495 : Image to text: 14.66, 37.2, 50.7, 10.0
2019-02-14 17:09:12,792 : Text to Image: 12.0, 32.284, 45.26, 13.0
2019-02-14 17:09:46,523 : Epoch 7 finished
2019-02-14 17:09:46,953 : Image to text: 31.9, 66.4, 80.5, 3.0
2019-02-14 17:09:47,277 : Text to Image: 27.34, 63.06, 78.9, 3.0
2019-02-14 17:09:47,696 : Image to text: 33.0, 67.4, 80.9, 3.0
2019-02-14 17:09:48,020 : Text to Image: 27.48, 61.9, 77.66, 3.0
2019-02-14 17:09:48,441 : Image to text: 34.7, 68.3, 81.3, 3.0
2019-02-14 17:09:48,782 : Text to Image: 28.48, 63.34, 79.42, 3.0
2019-02-14 17:09:49,212 : Image to text: 34.3, 68.6, 80.9, 3.0
2019-02-14 17:09:49,543 : Text to Image: 28.86, 63.36, 79.28, 3.0
2019-02-14 17:09:49,984 : Image to text: 33.2, 67.6, 80.4, 3.0
2019-02-14 17:09:50,319 : Text to Image: 28.96, 63.16, 77.98, 3.0
2019-02-14 17:09:50,319 : Dev mean Text to Image: 28.224000000000004, 62.96399999999999, 78.648, 3.0
2019-02-14 17:09:50,319 : Dev mean Image to text: 33.42, 67.66, 80.8, 3.0
2019-02-14 17:09:50,319 : start epoch
2019-02-14 17:10:32,177 : samples : 64000
2019-02-14 17:10:42,421 : Image to text: 14.34, 36.14, 49.86, 11.0
2019-02-14 17:10:49,811 : Text to Image: 11.808, 31.9, 44.644, 13.0
2019-02-14 17:11:30,592 : samples : 128000
2019-02-14 17:11:40,876 : Image to text: 14.58, 36.68, 51.12, 10.0
2019-02-14 17:11:48,274 : Text to Image: 11.712, 32.28, 45.008, 13.0
2019-02-14 17:12:29,584 : samples : 192000
2019-02-14 17:12:39,822 : Image to text: 14.54, 37.08, 50.22, 10.0
2019-02-14 17:12:47,270 : Text to Image: 11.944, 32.3, 45.468, 13.0
2019-02-14 17:13:27,256 : samples : 256000
2019-02-14 17:13:37,626 : Image to text: 14.92, 36.78, 50.92, 10.0
2019-02-14 17:13:45,052 : Text to Image: 12.072, 32.06, 45.216, 13.0
2019-02-14 17:14:25,570 : samples : 320000
2019-02-14 17:14:35,655 : Image to text: 15.26, 38.08, 52.1, 10.0
2019-02-14 17:14:42,976 : Text to Image: 12.34, 32.768, 45.736, 13.0
2019-02-14 17:15:25,686 : samples : 384000
2019-02-14 17:15:35,771 : Image to text: 14.66, 37.14, 50.62, 10.0
2019-02-14 17:15:43,056 : Text to Image: 12.248, 32.52, 45.352, 13.0
2019-02-14 17:16:24,992 : samples : 448000
2019-02-14 17:16:35,071 : Image to text: 14.2, 36.92, 50.76, 10.0
2019-02-14 17:16:42,397 : Text to Image: 11.76, 31.908, 44.736, 13.0
2019-02-14 17:17:21,949 : samples : 512000
2019-02-14 17:17:32,021 : Image to text: 15.14, 37.72, 51.7, 10.0
2019-02-14 17:17:39,323 : Text to Image: 12.336, 32.812, 45.736, 13.0
2019-02-14 17:18:12,981 : Epoch 8 finished
2019-02-14 17:18:13,398 : Image to text: 33.5, 66.3, 80.2, 3.0
2019-02-14 17:18:13,721 : Text to Image: 27.4, 63.18, 78.92, 3.0
2019-02-14 17:18:14,139 : Image to text: 34.3, 67.8, 81.5, 3.0
2019-02-14 17:18:14,462 : Text to Image: 28.0, 61.56, 78.06, 4.0
2019-02-14 17:18:14,890 : Image to text: 33.6, 69.2, 81.8, 3.0
2019-02-14 17:18:15,218 : Text to Image: 28.06, 63.74, 79.16, 3.0
2019-02-14 17:18:15,645 : Image to text: 34.5, 68.9, 81.4, 3.0
2019-02-14 17:18:15,974 : Text to Image: 28.52, 63.1, 79.18, 3.0
2019-02-14 17:18:16,414 : Image to text: 34.9, 68.7, 82.0, 3.0
2019-02-14 17:18:16,743 : Text to Image: 28.0, 62.5, 78.26, 3.0
2019-02-14 17:18:16,743 : Dev mean Text to Image: 27.996000000000002, 62.816, 78.71600000000001, 3.2
2019-02-14 17:18:16,743 : Dev mean Image to text: 34.16, 68.17999999999999, 81.38, 3.0
2019-02-14 17:18:16,743 : start epoch
2019-02-14 17:18:56,695 : samples : 64000
2019-02-14 17:19:07,042 : Image to text: 15.08, 37.88, 51.7, 10.0
2019-02-14 17:19:14,470 : Text to Image: 11.984, 32.188, 44.86, 13.0
2019-02-14 17:19:54,360 : samples : 128000
2019-02-14 17:20:04,708 : Image to text: 15.08, 37.68, 50.96, 10.0
2019-02-14 17:20:12,132 : Text to Image: 11.672, 32.132, 45.112, 13.0
2019-02-14 17:20:52,780 : samples : 192000
2019-02-14 17:21:03,143 : Image to text: 13.86, 36.78, 51.36, 10.0
2019-02-14 17:21:10,582 : Text to Image: 11.828, 31.856, 44.832, 13.0
2019-02-14 17:21:51,477 : samples : 256000
2019-02-14 17:22:01,812 : Image to text: 15.62, 38.22, 51.84, 10.0
2019-02-14 17:22:09,238 : Text to Image: 12.268, 32.536, 45.628, 13.0
2019-02-14 17:22:51,628 : samples : 320000
2019-02-14 17:23:01,756 : Image to text: 15.96, 37.72, 51.98, 10.0
2019-02-14 17:23:09,078 : Text to Image: 12.244, 32.956, 45.72, 13.0
2019-02-14 17:23:51,445 : samples : 384000
2019-02-14 17:24:01,519 : Image to text: 13.7, 36.7, 51.28, 10.0
2019-02-14 17:24:08,883 : Text to Image: 11.896, 32.408, 45.304, 13.0
2019-02-14 17:24:50,965 : samples : 448000
2019-02-14 17:25:01,090 : Image to text: 15.0, 38.46, 51.78, 10.0
2019-02-14 17:25:08,450 : Text to Image: 12.308, 32.58, 45.532, 13.0
2019-02-14 17:25:48,901 : samples : 512000
2019-02-14 17:25:59,033 : Image to text: 14.3, 36.84, 51.36, 10.0
2019-02-14 17:26:06,391 : Text to Image: 11.944, 32.64, 45.464, 13.0
2019-02-14 17:26:41,703 : Epoch 9 finished
2019-02-14 17:26:42,130 : Image to text: 33.8, 67.2, 81.3, 3.0
2019-02-14 17:26:42,457 : Text to Image: 28.62, 63.92, 79.76, 3.0
2019-02-14 17:26:42,884 : Image to text: 34.3, 69.5, 81.5, 3.0
2019-02-14 17:26:43,213 : Text to Image: 27.84, 62.14, 78.08, 3.0
2019-02-14 17:26:43,650 : Image to text: 34.1, 67.6, 83.1, 3.0
2019-02-14 17:26:43,979 : Text to Image: 28.72, 64.34, 79.44, 3.0
2019-02-14 17:26:44,404 : Image to text: 36.0, 69.4, 83.0, 2.0
2019-02-14 17:26:44,733 : Text to Image: 29.36, 63.28, 79.32, 3.0
2019-02-14 17:26:45,174 : Image to text: 35.1, 70.4, 81.6, 2.0
2019-02-14 17:26:45,505 : Text to Image: 29.0, 63.92, 78.26, 3.0
2019-02-14 17:26:45,505 : Dev mean Text to Image: 28.708000000000002, 63.52, 78.97200000000001, 3.0
2019-02-14 17:26:45,505 : Dev mean Image to text: 34.66, 68.82000000000001, 82.1, 2.5999999999999996
2019-02-14 17:26:45,505 : start epoch
2019-02-14 17:27:25,836 : samples : 64000
2019-02-14 17:27:36,212 : Image to text: 15.38, 38.18, 51.72, 10.0
2019-02-14 17:27:43,665 : Text to Image: 12.092, 32.896, 45.968, 13.0
2019-02-14 17:28:24,487 : samples : 128000
2019-02-14 17:28:34,888 : Image to text: 14.88, 37.0, 51.36, 10.0
2019-02-14 17:28:42,303 : Text to Image: 12.464, 32.88, 45.812, 13.0
2019-02-14 17:29:25,970 : samples : 192000
2019-02-14 17:29:36,373 : Image to text: 15.16, 38.16, 51.82, 10.0
2019-02-14 17:29:43,811 : Text to Image: 12.176, 32.596, 45.256, 13.0
2019-02-14 17:30:25,407 : samples : 256000
2019-02-14 17:30:35,809 : Image to text: 14.8, 37.44, 51.7, 10.0
2019-02-14 17:30:43,263 : Text to Image: 11.84, 32.22, 45.54, 13.0
2019-02-14 17:31:24,808 : samples : 320000
2019-02-14 17:31:34,925 : Image to text: 14.26, 37.52, 51.46, 10.0
2019-02-14 17:31:42,261 : Text to Image: 12.3, 32.888, 45.996, 13.0
2019-02-14 17:32:24,772 : samples : 384000
2019-02-14 17:32:34,901 : Image to text: 15.26, 37.56, 51.22, 10.0
2019-02-14 17:32:42,293 : Text to Image: 11.788, 32.316, 45.296, 13.0
2019-02-14 17:33:24,308 : samples : 448000
2019-02-14 17:33:34,430 : Image to text: 15.78, 38.24, 52.54, 10.0
2019-02-14 17:33:41,808 : Text to Image: 12.34, 33.156, 46.136, 13.0
2019-02-14 17:34:21,363 : samples : 512000
2019-02-14 17:34:31,490 : Image to text: 14.58, 37.32, 50.92, 10.0
2019-02-14 17:34:38,838 : Text to Image: 12.252, 32.812, 45.56, 13.0
2019-02-14 17:35:12,580 : Epoch 10 finished
2019-02-14 17:35:12,998 : Image to text: 33.2, 67.4, 81.5, 3.0
2019-02-14 17:35:13,321 : Text to Image: 28.32, 63.94, 79.7, 3.0
2019-02-14 17:35:13,750 : Image to text: 33.9, 68.1, 81.1, 3.0
2019-02-14 17:35:14,074 : Text to Image: 28.6, 62.26, 78.52, 3.0
2019-02-14 17:35:14,494 : Image to text: 35.4, 69.1, 83.0, 3.0
2019-02-14 17:35:14,823 : Text to Image: 28.7, 64.1, 79.0, 3.0
2019-02-14 17:35:15,265 : Image to text: 33.9, 70.1, 82.1, 3.0
2019-02-14 17:35:15,596 : Text to Image: 29.26, 63.8, 79.64, 3.0
2019-02-14 17:35:16,025 : Image to text: 35.1, 69.1, 82.3, 3.0
2019-02-14 17:35:16,367 : Text to Image: 28.2, 63.76, 79.3, 3.0
2019-02-14 17:35:16,368 : Dev mean Text to Image: 28.616000000000003, 63.572, 79.232, 3.0
2019-02-14 17:35:16,368 : Dev mean Image to text: 34.300000000000004, 68.75999999999999, 82.0, 3.0
2019-02-14 17:35:16,368 : start epoch
2019-02-14 17:36:00,137 : samples : 64000
2019-02-14 17:36:10,515 : Image to text: 14.94, 37.86, 52.16, 10.0
2019-02-14 17:36:17,972 : Text to Image: 12.16, 32.62, 45.428, 13.0
2019-02-14 17:36:58,440 : samples : 128000
2019-02-14 17:37:08,758 : Image to text: 14.92, 38.06, 52.42, 9.0
2019-02-14 17:37:16,179 : Text to Image: 12.152, 32.904, 45.896, 13.0
2019-02-14 17:37:56,641 : samples : 192000
2019-02-14 17:38:06,994 : Image to text: 14.6, 38.24, 52.22, 10.0
2019-02-14 17:38:14,940 : Text to Image: 12.168, 32.888, 45.632, 13.0
2019-02-14 17:38:55,843 : samples : 256000
2019-02-14 17:39:06,244 : Image to text: 15.8, 39.1, 52.42, 9.0
2019-02-14 17:39:13,682 : Text to Image: 12.392, 33.096, 46.156, 13.0
2019-02-14 17:39:53,319 : samples : 320000
2019-02-14 17:40:03,459 : Image to text: 15.24, 38.46, 52.62, 9.0
2019-02-14 17:40:10,777 : Text to Image: 11.98, 32.72, 45.496, 13.0
2019-02-14 17:40:52,179 : samples : 384000
2019-02-14 17:41:02,301 : Image to text: 15.44, 39.24, 52.72, 9.0
2019-02-14 17:41:09,650 : Text to Image: 12.628, 34.076, 46.868, 12.0
2019-02-14 17:41:50,214 : samples : 448000
2019-02-14 17:42:00,311 : Image to text: 15.1, 37.42, 51.54, 10.0
2019-02-14 17:42:07,613 : Text to Image: 12.44, 33.44, 46.348, 13.0
2019-02-14 17:42:47,417 : samples : 512000
2019-02-14 17:42:57,512 : Image to text: 14.7, 38.22, 53.28, 9.0
2019-02-14 17:43:04,844 : Text to Image: 12.268, 33.104, 46.164, 13.0
2019-02-14 17:43:38,841 : Epoch 11 finished
2019-02-14 17:43:39,259 : Image to text: 33.0, 67.5, 80.8, 3.0
2019-02-14 17:43:39,584 : Text to Image: 28.28, 63.44, 79.28, 3.0
2019-02-14 17:43:40,020 : Image to text: 35.8, 69.3, 81.9, 3.0
2019-02-14 17:43:40,350 : Text to Image: 28.04, 61.98, 78.18, 3.0
2019-02-14 17:43:40,781 : Image to text: 35.5, 68.6, 81.6, 3.0
2019-02-14 17:43:41,126 : Text to Image: 28.52, 64.12, 79.18, 3.0
2019-02-14 17:43:41,561 : Image to text: 36.9, 70.0, 83.6, 3.0
2019-02-14 17:43:41,892 : Text to Image: 28.72, 64.32, 79.76, 3.0
2019-02-14 17:43:42,337 : Image to text: 35.7, 69.5, 82.8, 2.0
2019-02-14 17:43:42,668 : Text to Image: 28.76, 62.98, 79.06, 3.0
2019-02-14 17:43:42,669 : Dev mean Text to Image: 28.464, 63.367999999999995, 79.092, 3.0
2019-02-14 17:43:42,669 : Dev mean Image to text: 35.379999999999995, 68.98, 82.14000000000001, 2.8
2019-02-14 17:43:42,669 : start epoch
2019-02-14 17:44:23,179 : samples : 64000
2019-02-14 17:44:33,584 : Image to text: 14.96, 38.5, 52.34, 9.0
2019-02-14 17:44:41,021 : Text to Image: 12.344, 33.26, 46.156, 13.0
2019-02-14 17:45:22,668 : samples : 128000
2019-02-14 17:45:33,012 : Image to text: 15.04, 38.1, 51.86, 10.0
2019-02-14 17:45:40,442 : Text to Image: 12.144, 32.772, 45.744, 13.0
2019-02-14 17:46:20,843 : samples : 192000
2019-02-14 17:46:31,214 : Image to text: 15.34, 37.96, 52.24, 9.0
2019-02-14 17:46:38,646 : Text to Image: 12.312, 33.12, 46.244, 12.0
2019-02-14 17:47:18,800 : samples : 256000
2019-02-14 17:47:29,065 : Image to text: 15.24, 38.38, 52.1, 10.0
2019-02-14 17:47:36,469 : Text to Image: 12.3, 32.8, 45.768, 13.0
2019-02-14 17:48:16,268 : samples : 320000
2019-02-14 17:48:26,363 : Image to text: 15.24, 37.72, 51.32, 10.0
2019-02-14 17:48:33,666 : Text to Image: 12.38, 33.088, 46.204, 13.0
2019-02-14 17:49:16,810 : samples : 384000
2019-02-14 17:49:26,913 : Image to text: 15.7, 38.36, 52.58, 9.0
2019-02-14 17:49:34,255 : Text to Image: 12.668, 33.38, 46.148, 12.0
2019-02-14 17:50:17,031 : samples : 448000
2019-02-14 17:50:27,102 : Image to text: 16.16, 39.16, 52.98, 9.0
2019-02-14 17:50:34,397 : Text to Image: 12.416, 33.44, 46.328, 12.0
2019-02-14 17:51:16,448 : samples : 512000
2019-02-14 17:51:26,505 : Image to text: 15.6, 39.26, 52.98, 9.0
2019-02-14 17:51:33,812 : Text to Image: 12.396, 33.224, 46.228, 13.0
2019-02-14 17:52:10,094 : Epoch 12 finished
2019-02-14 17:52:10,525 : Image to text: 34.2, 68.4, 81.7, 3.0
2019-02-14 17:52:10,853 : Text to Image: 28.0, 64.7, 80.2, 3.0
2019-02-14 17:52:11,279 : Image to text: 34.0, 69.5, 81.9, 2.0
2019-02-14 17:52:11,609 : Text to Image: 28.16, 63.46, 79.32, 3.0
2019-02-14 17:52:12,055 : Image to text: 34.6, 70.2, 81.5, 3.0
2019-02-14 17:52:12,386 : Text to Image: 29.46, 65.24, 79.58, 3.0
2019-02-14 17:52:12,823 : Image to text: 36.0, 71.3, 82.6, 2.0
2019-02-14 17:52:13,156 : Text to Image: 29.38, 64.0, 79.94, 3.0
2019-02-14 17:52:13,601 : Image to text: 36.5, 71.1, 82.1, 2.0
2019-02-14 17:52:13,933 : Text to Image: 29.66, 64.66, 79.72, 3.0
2019-02-14 17:52:13,933 : Dev mean Text to Image: 28.932000000000002, 64.412, 79.752, 3.0
2019-02-14 17:52:13,933 : Dev mean Image to text: 35.06, 70.1, 81.96, 2.4
2019-02-14 17:52:13,933 : start epoch
2019-02-14 17:52:55,325 : samples : 64000
2019-02-14 17:53:05,567 : Image to text: 15.62, 39.1, 53.08, 9.0
2019-02-14 17:53:12,954 : Text to Image: 12.232, 33.076, 46.08, 12.0
2019-02-14 17:53:53,856 : samples : 128000
2019-02-14 17:54:04,087 : Image to text: 15.36, 38.18, 52.62, 9.0
2019-02-14 17:54:11,458 : Text to Image: 12.396, 33.0, 45.484, 13.0
2019-02-14 17:54:53,033 : samples : 192000
2019-02-14 17:55:03,321 : Image to text: 15.96, 39.28, 53.04, 9.0
2019-02-14 17:55:10,724 : Text to Image: 12.468, 33.396, 46.452, 12.0
2019-02-14 17:55:51,640 : samples : 256000
2019-02-14 17:56:02,008 : Image to text: 16.12, 39.74, 54.02, 9.0
2019-02-14 17:56:09,358 : Text to Image: 12.36, 33.632, 46.476, 12.0
2019-02-14 17:56:50,991 : samples : 320000
2019-02-14 17:57:01,044 : Image to text: 15.22, 38.08, 52.6, 10.0
2019-02-14 17:57:08,393 : Text to Image: 12.544, 33.312, 46.452, 12.0
2019-02-14 17:57:49,838 : samples : 384000
2019-02-14 17:57:59,843 : Image to text: 14.38, 38.62, 52.28, 10.0
2019-02-14 17:58:07,163 : Text to Image: 12.408, 33.14, 46.336, 12.0
2019-02-14 17:58:46,558 : samples : 448000
2019-02-14 17:58:56,654 : Image to text: 15.58, 38.28, 52.94, 9.0
2019-02-14 17:59:04,013 : Text to Image: 12.484, 33.076, 46.372, 12.0
2019-02-14 17:59:44,352 : samples : 512000
2019-02-14 17:59:54,396 : Image to text: 15.46, 38.62, 52.96, 9.0
2019-02-14 18:00:01,708 : Text to Image: 12.284, 32.908, 46.244, 12.0
2019-02-14 18:00:40,701 : Epoch 13 finished
2019-02-14 18:00:41,150 : Image to text: 34.7, 67.8, 82.3, 3.0
2019-02-14 18:00:41,479 : Text to Image: 28.78, 64.42, 80.64, 3.0
2019-02-14 18:00:41,923 : Image to text: 35.7, 69.1, 81.9, 2.0
2019-02-14 18:00:42,250 : Text to Image: 29.26, 63.58, 79.56, 3.0
2019-02-14 18:00:42,678 : Image to text: 33.8, 68.4, 82.6, 3.0
2019-02-14 18:00:43,006 : Text to Image: 29.46, 65.78, 79.68, 3.0
2019-02-14 18:00:43,445 : Image to text: 35.9, 69.0, 82.8, 3.0
2019-02-14 18:00:43,773 : Text to Image: 29.66, 65.04, 80.28, 3.0
2019-02-14 18:00:44,200 : Image to text: 36.1, 69.7, 82.7, 2.0
2019-02-14 18:00:44,540 : Text to Image: 30.08, 64.14, 79.66, 3.0
2019-02-14 18:00:44,540 : Dev mean Text to Image: 29.448, 64.592, 79.964, 3.0
2019-02-14 18:00:44,540 : Dev mean Image to text: 35.24, 68.8, 82.46000000000001, 2.6
2019-02-14 18:00:44,541 : start epoch
2019-02-14 18:01:24,159 : samples : 64000
2019-02-14 18:01:34,424 : Image to text: 15.18, 38.78, 52.8, 9.0
2019-02-14 18:01:41,736 : Text to Image: 12.404, 33.42, 46.28, 12.0
2019-02-14 18:02:21,536 : samples : 128000
2019-02-14 18:02:31,778 : Image to text: 15.58, 39.5, 52.86, 9.0
2019-02-14 18:02:39,090 : Text to Image: 12.416, 33.204, 46.068, 13.0
2019-02-14 18:03:22,439 : samples : 192000
2019-02-14 18:03:32,734 : Image to text: 16.12, 39.78, 53.36, 9.0
2019-02-14 18:03:40,034 : Text to Image: 12.484, 33.252, 46.564, 12.0
2019-02-14 18:04:19,663 : samples : 256000
2019-02-14 18:04:29,891 : Image to text: 15.78, 39.16, 52.48, 9.0
2019-02-14 18:04:37,260 : Text to Image: 12.36, 33.256, 46.424, 12.0
2019-02-14 18:05:16,696 : samples : 320000
2019-02-14 18:05:26,694 : Image to text: 15.5, 38.8, 52.66, 9.0
2019-02-14 18:05:34,042 : Text to Image: 12.176, 32.688, 46.132, 12.0
2019-02-14 18:06:14,508 : samples : 384000
2019-02-14 18:06:24,564 : Image to text: 16.3, 38.84, 52.88, 9.0
2019-02-14 18:06:31,875 : Text to Image: 12.472, 33.624, 46.376, 12.0
2019-02-14 18:07:13,613 : samples : 448000
2019-02-14 18:07:23,601 : Image to text: 15.48, 39.22, 53.02, 9.0
2019-02-14 18:07:30,941 : Text to Image: 12.316, 33.008, 46.132, 13.0
2019-02-14 18:08:10,315 : samples : 512000
2019-02-14 18:08:20,416 : Image to text: 15.1, 39.3, 53.78, 9.0
2019-02-14 18:08:27,697 : Text to Image: 12.644, 33.788, 46.788, 12.0
2019-02-14 18:09:01,216 : Epoch 14 finished
2019-02-14 18:09:01,631 : Image to text: 33.7, 65.9, 80.4, 3.0
2019-02-14 18:09:01,953 : Text to Image: 28.76, 64.96, 80.32, 3.0
2019-02-14 18:09:02,383 : Image to text: 33.4, 68.4, 81.4, 3.0
2019-02-14 18:09:02,709 : Text to Image: 28.6, 62.9, 78.62, 3.0
2019-02-14 18:09:03,133 : Image to text: 37.5, 68.5, 82.3, 2.0
2019-02-14 18:09:03,461 : Text to Image: 29.16, 65.5, 79.98, 3.0
2019-02-14 18:09:03,899 : Image to text: 35.0, 69.5, 84.3, 2.0
2019-02-14 18:09:04,226 : Text to Image: 28.72, 65.06, 79.84, 3.0
2019-02-14 18:09:04,652 : Image to text: 36.9, 69.4, 82.2, 2.0
2019-02-14 18:09:04,991 : Text to Image: 29.64, 64.16, 79.84, 3.0
2019-02-14 18:09:04,991 : Dev mean Text to Image: 28.976000000000003, 64.51599999999999, 79.72, 3.0
2019-02-14 18:09:04,991 : Dev mean Image to text: 35.300000000000004, 68.34, 82.12, 2.4
2019-02-14 18:09:04,991 : start epoch
2019-02-14 18:09:44,507 : samples : 64000
2019-02-14 18:09:54,762 : Image to text: 15.56, 38.58, 52.98, 9.0
2019-02-14 18:10:02,122 : Text to Image: 12.672, 33.472, 46.424, 12.0
2019-02-14 18:10:42,304 : samples : 128000
2019-02-14 18:10:52,561 : Image to text: 14.74, 38.58, 52.6, 9.0
2019-02-14 18:10:59,902 : Text to Image: 12.184, 33.028, 46.352, 12.0
2019-02-14 18:11:39,537 : samples : 192000
2019-02-14 18:11:49,816 : Image to text: 15.2, 39.36, 53.46, 9.0
2019-02-14 18:11:57,176 : Text to Image: 12.684, 33.34, 46.372, 12.0
2019-02-14 18:12:37,548 : samples : 256000
2019-02-14 18:12:47,751 : Image to text: 15.58, 38.62, 53.52, 9.0
2019-02-14 18:12:55,089 : Text to Image: 12.58, 33.524, 46.912, 12.0
2019-02-14 18:13:37,015 : samples : 320000
2019-02-14 18:13:47,019 : Image to text: 16.06, 38.86, 53.3, 9.0
2019-02-14 18:13:54,311 : Text to Image: 12.724, 33.892, 46.72, 12.0
2019-02-14 18:14:39,218 : samples : 384000
2019-02-14 18:14:49,292 : Image to text: 15.38, 39.1, 53.04, 9.0
2019-02-14 18:14:56,589 : Text to Image: 12.736, 33.616, 46.696, 12.0
2019-02-14 18:15:36,261 : samples : 448000
2019-02-14 18:15:46,314 : Image to text: 16.12, 39.56, 53.48, 9.0
2019-02-14 18:15:53,640 : Text to Image: 12.924, 33.716, 46.58, 12.0
2019-02-14 18:16:34,924 : samples : 512000
2019-02-14 18:16:44,974 : Image to text: 14.4, 38.7, 52.64, 9.0
2019-02-14 18:16:52,318 : Text to Image: 12.512, 33.276, 46.192, 13.0
2019-02-14 18:17:28,205 : Epoch 15 finished
2019-02-14 18:17:28,631 : Image to text: 35.1, 67.2, 81.7, 3.0
2019-02-14 18:17:28,952 : Text to Image: 29.24, 64.68, 80.54, 3.0
2019-02-14 18:17:29,368 : Image to text: 35.5, 70.2, 81.8, 2.0
2019-02-14 18:17:29,690 : Text to Image: 29.46, 63.52, 79.2, 3.0
2019-02-14 18:17:30,127 : Image to text: 35.0, 70.0, 82.7, 3.0
2019-02-14 18:17:30,459 : Text to Image: 28.82, 64.98, 79.84, 3.0
2019-02-14 18:17:30,891 : Image to text: 36.2, 71.6, 83.6, 2.0
2019-02-14 18:17:31,232 : Text to Image: 29.32, 64.82, 80.2, 3.0
2019-02-14 18:17:31,660 : Image to text: 37.6, 70.0, 82.2, 2.0
2019-02-14 18:17:31,989 : Text to Image: 29.78, 64.12, 79.18, 3.0
2019-02-14 18:17:31,989 : Dev mean Text to Image: 29.324, 64.424, 79.792, 3.0
2019-02-14 18:17:31,989 : Dev mean Image to text: 35.88, 69.80000000000001, 82.4, 2.4
2019-02-14 18:17:31,989 : start epoch
2019-02-14 18:18:13,992 : samples : 64000
2019-02-14 18:18:24,258 : Image to text: 15.76, 39.2, 53.66, 9.0
2019-02-14 18:18:31,644 : Text to Image: 12.656, 33.552, 46.612, 12.0
2019-02-14 18:19:12,722 : samples : 128000
2019-02-14 18:19:23,009 : Image to text: 15.56, 39.88, 53.2, 9.0
2019-02-14 18:19:30,383 : Text to Image: 12.696, 33.316, 46.42, 12.0
2019-02-14 18:20:10,889 : samples : 192000
2019-02-14 18:20:21,182 : Image to text: 15.56, 40.4, 53.74, 9.0
2019-02-14 18:20:28,562 : Text to Image: 12.56, 33.688, 46.772, 12.0
2019-02-14 18:21:09,621 : samples : 256000
2019-02-14 18:21:19,834 : Image to text: 15.36, 39.04, 53.36, 9.0
2019-02-14 18:21:27,134 : Text to Image: 12.592, 33.712, 46.82, 12.0
2019-02-14 18:22:09,398 : samples : 320000
2019-02-14 18:22:21,952 : Image to text: 16.46, 39.82, 53.74, 9.0
2019-02-14 18:22:31,963 : Text to Image: 12.3, 33.472, 46.764, 12.0
2019-02-14 18:23:15,630 : samples : 384000
2019-02-14 18:23:25,717 : Image to text: 15.66, 39.2, 53.12, 9.0
2019-02-14 18:23:32,988 : Text to Image: 12.26, 33.384, 46.324, 12.0
2019-02-14 18:24:14,441 : samples : 448000
2019-02-14 18:24:26,965 : Image to text: 15.54, 38.76, 53.44, 9.0
2019-02-14 18:24:37,000 : Text to Image: 12.524, 33.492, 46.628, 12.0
2019-02-14 18:25:20,820 : samples : 512000
2019-02-14 18:25:30,935 : Image to text: 15.78, 39.52, 53.54, 9.0
2019-02-14 18:25:38,051 : Text to Image: 12.984, 33.852, 47.108, 12.0
2019-02-14 18:26:12,426 : Epoch 16 finished
2019-02-14 18:26:12,784 : Image to text: 34.3, 68.8, 82.5, 3.0
2019-02-14 18:26:13,059 : Text to Image: 28.48, 65.2, 80.64, 3.0
2019-02-14 18:26:13,434 : Image to text: 36.7, 71.4, 82.8, 2.0
2019-02-14 18:26:13,708 : Text to Image: 29.38, 63.22, 79.82, 3.0
2019-02-14 18:26:14,083 : Image to text: 35.4, 70.1, 83.1, 3.0
2019-02-14 18:26:14,359 : Text to Image: 30.16, 64.96, 79.78, 3.0
2019-02-14 18:26:14,733 : Image to text: 36.3, 71.9, 83.0, 3.0
2019-02-14 18:26:15,010 : Text to Image: 29.24, 64.66, 80.02, 3.0
2019-02-14 18:26:15,392 : Image to text: 36.8, 70.2, 82.9, 3.0
2019-02-14 18:26:15,687 : Text to Image: 29.18, 64.42, 80.18, 3.0
2019-02-14 18:26:15,687 : Dev mean Text to Image: 29.287999999999997, 64.492, 80.088, 3.0
2019-02-14 18:26:15,687 : Dev mean Image to text: 35.9, 70.48, 82.86, 2.8000000000000003
2019-02-14 18:26:15,688 : start epoch
2019-02-14 18:26:58,207 : samples : 64000
2019-02-14 18:27:10,826 : Image to text: 16.16, 39.96, 54.0, 9.0
2019-02-14 18:27:20,913 : Text to Image: 12.844, 34.0, 47.352, 12.0
2019-02-14 18:28:02,682 : samples : 128000
2019-02-14 18:28:12,749 : Image to text: 15.5, 39.92, 53.5, 9.0
2019-02-14 18:28:19,708 : Text to Image: 12.732, 33.564, 46.58, 12.0
2019-02-14 18:29:02,054 : samples : 192000
2019-02-14 18:29:14,616 : Image to text: 15.9, 39.32, 52.84, 9.0
2019-02-14 18:29:24,617 : Text to Image: 12.616, 33.732, 46.66, 12.0
2019-02-14 18:30:05,719 : samples : 256000
2019-02-14 18:30:15,797 : Image to text: 15.7, 39.76, 53.84, 9.0
2019-02-14 18:30:23,007 : Text to Image: 12.528, 33.632, 46.428, 12.0
2019-02-14 18:31:04,198 : samples : 320000
2019-02-14 18:31:16,760 : Image to text: 16.02, 40.28, 54.04, 9.0
2019-02-14 18:31:26,745 : Text to Image: 12.596, 33.608, 46.612, 12.0
2019-02-14 18:32:08,933 : samples : 384000
2019-02-14 18:32:18,983 : Image to text: 15.66, 40.08, 54.02, 9.0
2019-02-14 18:32:26,155 : Text to Image: 12.524, 33.484, 46.496, 12.0
2019-02-14 18:33:07,701 : samples : 448000
2019-02-14 18:33:20,211 : Image to text: 15.74, 39.7, 54.24, 9.0
2019-02-14 18:33:30,196 : Text to Image: 12.804, 33.876, 46.972, 12.0
2019-02-14 18:34:13,402 : samples : 512000
2019-02-14 18:34:23,465 : Image to text: 15.16, 39.2, 53.64, 9.0
2019-02-14 18:34:30,699 : Text to Image: 12.828, 33.664, 46.916, 12.0
2019-02-14 18:35:06,502 : Epoch 17 finished
2019-02-14 18:35:07,441 : Image to text: 35.7, 67.4, 81.1, 3.0
2019-02-14 18:35:08,186 : Text to Image: 29.06, 64.6, 81.0, 3.0
2019-02-14 18:35:09,118 : Image to text: 35.9, 70.1, 82.1, 2.0
2019-02-14 18:35:09,858 : Text to Image: 28.72, 63.4, 79.32, 3.0
2019-02-14 18:35:10,771 : Image to text: 35.8, 68.1, 83.3, 3.0
2019-02-14 18:35:11,534 : Text to Image: 29.22, 64.96, 80.32, 3.0
2019-02-14 18:35:12,454 : Image to text: 35.4, 70.0, 83.5, 2.0
2019-02-14 18:35:13,231 : Text to Image: 29.78, 65.02, 80.24, 3.0
2019-02-14 18:35:14,113 : Image to text: 37.4, 71.0, 81.5, 2.0
2019-02-14 18:35:14,893 : Text to Image: 30.36, 64.62, 79.56, 3.0
2019-02-14 18:35:14,894 : Dev mean Text to Image: 29.427999999999997, 64.52, 80.08800000000001, 3.0
2019-02-14 18:35:14,894 : Dev mean Image to text: 36.04, 69.32, 82.3, 2.4
2019-02-14 18:35:23,337 : 
Test scores | Image to text:             37.059999999999995, 70.64, 83.86, 2.4
2019-02-14 18:35:23,337 : Test scores | Text to image:             29.252, 64.176, 79.756, 3.0

2019-02-14 18:35:23,433 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 18:35:23,641 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 18:35:24,262 : loading BERT model bert-base-uncased
2019-02-14 18:35:24,263 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:35:24,293 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:35:24,293 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv6c1h3ae
2019-02-14 18:35:26,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:35:28,143 : Computing embeddings for train/dev/test
2019-02-14 18:37:38,356 : Computed embeddings
2019-02-14 18:37:38,356 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:38:22,447 : [('reg:1e-05', 92.42), ('reg:0.0001', 91.12), ('reg:0.001', 86.58), ('reg:0.01', 80.69)]
2019-02-14 18:38:22,447 : Validation : best param found is reg = 1e-05 with score             92.42
2019-02-14 18:38:22,447 : Evaluating...
2019-02-14 18:38:38,324 : 
Dev acc : 92.4 Test acc : 92.3 for LENGTH classification

2019-02-14 18:38:38,324 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 18:38:38,665 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 18:38:38,710 : loading BERT model bert-base-uncased
2019-02-14 18:38:38,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:38:38,808 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:38:38,808 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpraf4kkwn
2019-02-14 18:38:41,206 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:38:42,634 : Computing embeddings for train/dev/test
2019-02-14 18:40:47,350 : Computed embeddings
2019-02-14 18:40:47,350 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:41:30,852 : [('reg:1e-05', 71.9), ('reg:0.0001', 35.5), ('reg:0.001', 2.9), ('reg:0.01', 0.94)]
2019-02-14 18:41:30,852 : Validation : best param found is reg = 1e-05 with score             71.9
2019-02-14 18:41:30,852 : Evaluating...
2019-02-14 18:41:48,949 : 
Dev acc : 71.9 Test acc : 72.0 for WORDCONTENT classification

2019-02-14 18:41:48,950 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 18:41:49,463 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 18:41:49,528 : loading BERT model bert-base-uncased
2019-02-14 18:41:49,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:41:49,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:41:49,554 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7o0zwtwr
2019-02-14 18:41:51,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:41:53,422 : Computing embeddings for train/dev/test
2019-02-14 18:44:00,746 : Computed embeddings
2019-02-14 18:44:00,746 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:44:35,880 : [('reg:1e-05', 38.19), ('reg:0.0001', 37.88), ('reg:0.001', 35.81), ('reg:0.01', 29.9)]
2019-02-14 18:44:35,880 : Validation : best param found is reg = 1e-05 with score             38.19
2019-02-14 18:44:35,880 : Evaluating...
2019-02-14 18:44:49,551 : 
Dev acc : 38.2 Test acc : 38.3 for DEPTH classification

2019-02-14 18:44:49,552 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 18:44:50,103 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 18:44:50,166 : loading BERT model bert-base-uncased
2019-02-14 18:44:50,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:44:50,192 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:44:50,193 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0kfr7vf4
2019-02-14 18:44:52,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:44:54,094 : Computing embeddings for train/dev/test
2019-02-14 18:46:59,491 : Computed embeddings
2019-02-14 18:46:59,491 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:47:25,330 : [('reg:1e-05', 76.12), ('reg:0.0001', 75.28), ('reg:0.001', 68.31), ('reg:0.01', 54.57)]
2019-02-14 18:47:25,330 : Validation : best param found is reg = 1e-05 with score             76.12
2019-02-14 18:47:25,330 : Evaluating...
2019-02-14 18:47:30,149 : 
Dev acc : 76.1 Test acc : 76.0 for TOPCONSTITUENTS classification

2019-02-14 18:47:30,151 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 18:47:30,530 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 18:47:30,596 : loading BERT model bert-base-uncased
2019-02-14 18:47:30,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:47:30,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:47:30,626 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzdvvcjq0
2019-02-14 18:47:33,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:47:34,475 : Computing embeddings for train/dev/test
2019-02-14 18:49:18,431 : Computed embeddings
2019-02-14 18:49:18,431 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:50:02,961 : [('reg:1e-05', 87.7), ('reg:0.0001', 87.73), ('reg:0.001', 87.43), ('reg:0.01', 86.23)]
2019-02-14 18:50:02,961 : Validation : best param found is reg = 0.0001 with score             87.73
2019-02-14 18:50:02,961 : Evaluating...
2019-02-14 18:50:13,219 : 
Dev acc : 87.7 Test acc : 87.0 for BIGRAMSHIFT classification

2019-02-14 18:50:13,220 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 18:50:13,608 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 18:50:13,673 : loading BERT model bert-base-uncased
2019-02-14 18:50:13,673 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:50:13,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:50:13,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdstoe2hl
2019-02-14 18:50:16,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:50:17,631 : Computing embeddings for train/dev/test
2019-02-14 18:51:52,087 : Computed embeddings
2019-02-14 18:51:52,088 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:52:17,891 : [('reg:1e-05', 89.86), ('reg:0.0001', 89.97), ('reg:0.001', 90.11), ('reg:0.01', 89.77)]
2019-02-14 18:52:17,891 : Validation : best param found is reg = 0.001 with score             90.11
2019-02-14 18:52:17,891 : Evaluating...
2019-02-14 18:52:25,979 : 
Dev acc : 90.1 Test acc : 89.1 for TENSE classification

2019-02-14 18:52:25,980 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 18:52:26,554 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 18:52:26,617 : loading BERT model bert-base-uncased
2019-02-14 18:52:26,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:52:26,644 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:52:26,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4k1n_nro
2019-02-14 18:52:29,068 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:52:30,479 : Computing embeddings for train/dev/test
2019-02-14 18:54:17,245 : Computed embeddings
2019-02-14 18:54:17,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:54:46,305 : [('reg:1e-05', 85.54), ('reg:0.0001', 85.83), ('reg:0.001', 84.74), ('reg:0.01', 81.84)]
2019-02-14 18:54:46,305 : Validation : best param found is reg = 0.0001 with score             85.83
2019-02-14 18:54:46,305 : Evaluating...
2019-02-14 18:54:53,556 : 
Dev acc : 85.8 Test acc : 84.4 for SUBJNUMBER classification

2019-02-14 18:54:53,557 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 18:54:53,952 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 18:54:54,018 : loading BERT model bert-base-uncased
2019-02-14 18:54:54,019 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:54:54,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:54:54,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmsk4cm8a
2019-02-14 18:54:56,584 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:54:58,041 : Computing embeddings for train/dev/test
2019-02-14 18:57:01,555 : Computed embeddings
2019-02-14 18:57:01,556 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:57:47,938 : [('reg:1e-05', 82.78), ('reg:0.0001', 82.9), ('reg:0.001', 82.71), ('reg:0.01', 82.32)]
2019-02-14 18:57:47,938 : Validation : best param found is reg = 0.0001 with score             82.9
2019-02-14 18:57:47,938 : Evaluating...
2019-02-14 18:57:58,171 : 
Dev acc : 82.9 Test acc : 83.3 for OBJNUMBER classification

2019-02-14 18:57:58,172 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 18:57:58,736 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 18:57:58,804 : loading BERT model bert-base-uncased
2019-02-14 18:57:58,804 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:57:58,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:57:58,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu7h22ypr
2019-02-14 18:58:01,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:58:02,703 : Computing embeddings for train/dev/test
2019-02-14 18:59:56,795 : Computed embeddings
2019-02-14 18:59:56,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:00:34,168 : [('reg:1e-05', 60.56), ('reg:0.0001', 60.43), ('reg:0.001', 60.29), ('reg:0.01', 59.44)]
2019-02-14 19:00:34,168 : Validation : best param found is reg = 1e-05 with score             60.56
2019-02-14 19:00:34,168 : Evaluating...
2019-02-14 19:00:43,120 : 
Dev acc : 60.6 Test acc : 60.0 for ODDMANOUT classification

2019-02-14 19:00:43,121 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 19:00:43,714 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 19:00:43,790 : loading BERT model bert-base-uncased
2019-02-14 19:00:43,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:00:43,822 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:00:43,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6dool_a4
2019-02-14 19:00:46,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:00:47,693 : Computing embeddings for train/dev/test
2019-02-14 19:02:42,209 : Computed embeddings
2019-02-14 19:02:42,210 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:03:45,656 : [('reg:1e-05', 60.34), ('reg:0.0001', 60.28), ('reg:0.001', 60.5), ('reg:0.01', 57.62)]
2019-02-14 19:03:45,657 : Validation : best param found is reg = 0.001 with score             60.5
2019-02-14 19:03:45,657 : Evaluating...
2019-02-14 19:03:58,968 : 
Dev acc : 60.5 Test acc : 60.7 for COORDINATIONINVERSION classification

2019-02-14 19:03:58,970 : total results: {'STS12': {'MSRpar': {'pearson': (0.41669596205122517, 7.319884047588117e-33), 'spearman': SpearmanrResult(correlation=0.4487255545506555, pvalue=1.954273269227737e-38), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5432490003562456, 8.38830050498777e-59), 'spearman': SpearmanrResult(correlation=0.5497239670717528, pvalue=1.9036278513612374e-60), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4811778186479036, 5.647202926571223e-28), 'spearman': SpearmanrResult(correlation=0.5806051613512803, pvalue=1.0151783328283563e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6512017928775169, 1.0724313235545436e-91), 'spearman': SpearmanrResult(correlation=0.6550084584482017, pvalue=4.163615908006536e-93), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6240798566228135, 1.8508973122707708e-44), 'spearman': SpearmanrResult(correlation=0.5167617123125526, pvalue=1.2710530444438913e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5432808861111409, 'wmean': 0.5399705752946046}, 'spearman': {'mean': 0.5501649707468885, 'wmean': 0.5510872513918609}}}, 'STS13': {'FNWN': {'pearson': (0.3742114451539917, 1.130280266334392e-07), 'spearman': SpearmanrResult(correlation=0.3873953297993763, pvalue=3.6491946642880904e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6363809779242725, 2.1621238459811184e-86), 'spearman': SpearmanrResult(correlation=0.6133400885143385, pvalue=1.0636160778850496e-78), 'nsamples': 750}, 'OnWN': {'pearson': (0.47118471177356785, 2.3912712679216086e-32), 'spearman': SpearmanrResult(correlation=0.5007545959131352, pvalue=6.073961325689327e-37), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.49392571161727733, 'wmean': 0.5415642132548536}, 'spearman': {'mean': 0.50049667140895, 'wmean': 0.5427640746834033}}}, 'STS14': {'deft-forum': {'pearson': (0.3201617974094088, 3.4829266590887763e-12), 'spearman': SpearmanrResult(correlation=0.3497009138857929, pvalue=2.1689233637305787e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7425218103435448, 8.481446298931032e-54), 'spearman': SpearmanrResult(correlation=0.7110929059828098, pvalue=1.6667671471929202e-47), 'nsamples': 300}, 'headlines': {'pearson': (0.598646387833195, 4.066385401711838e-74), 'spearman': SpearmanrResult(correlation=0.5515319688829481, pvalue=6.517011488727611e-61), 'nsamples': 750}, 'images': {'pearson': (0.5862322844564154, 1.9944444146905862e-70), 'spearman': SpearmanrResult(correlation=0.581066443681681, pvalue=6.153476930165846e-69), 'nsamples': 750}, 'OnWN': {'pearson': (0.6462210667084184, 7.014314039669795e-90), 'spearman': SpearmanrResult(correlation=0.6825060996062785, pvalue=6.122045693755671e-104), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6660622555870068, 2.535274515572247e-97), 'spearman': SpearmanrResult(correlation=0.6245560023250033, pvalue=2.3069428105775976e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5933076003896648, 'wmean': 0.5972535594336197}, 'spearman': {'mean': 0.583409055727419, 'wmean': 0.5867836450441022}}}, 'STS15': {'answers-forums': {'pearson': (0.5296536027763514, 1.6650109688828526e-28), 'spearman': SpearmanrResult(correlation=0.5238985205823194, pvalue=8.050136745234755e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.6772401016819416, 8.974260202816976e-102), 'spearman': SpearmanrResult(correlation=0.6842424544070798, pvalue=1.1550504041076335e-104), 'nsamples': 750}, 'belief': {'pearson': (0.650704606673009, 1.590932941116758e-46), 'spearman': SpearmanrResult(correlation=0.6777514236488688, pvalue=9.39063035425924e-52), 'nsamples': 375}, 'headlines': {'pearson': (0.6556784218639292, 2.339022111745915e-93), 'spearman': SpearmanrResult(correlation=0.6453079503227963, pvalue=1.496802639406859e-89), 'nsamples': 750}, 'images': {'pearson': (0.715839691722635, 8.639446377074131e-119), 'spearman': SpearmanrResult(correlation=0.7263371494331214, pvalue=6.410558970814167e-124), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6458232849435732, 'wmean': 0.6597343299982965}, 'spearman': {'mean': 0.6515074996788371, 'wmean': 0.6641781315696479}}}, 'STS16': {'answer-answer': {'pearson': (0.5103682319808112, 2.9936754612944755e-18), 'spearman': SpearmanrResult(correlation=0.5336369445038536, pvalue=4.262392012110318e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6596956782923812, 1.7487203658749342e-32), 'spearman': SpearmanrResult(correlation=0.6598682798206951, pvalue=1.6633197204473855e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7512440803852835, 5.0694599893400114e-43), 'spearman': SpearmanrResult(correlation=0.7620399583183082, pvalue=6.403563153269467e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7948514960789547, 2.0229651452346283e-54), 'spearman': SpearmanrResult(correlation=0.8307692316239217, pvalue=1.636713318306325e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.33927848568295105, 5.025637693354026e-07), 'spearman': SpearmanrResult(correlation=0.3394442799746902, pvalue=4.957670318161775e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6110875944840763, 'wmean': 0.6168101702002915}, 'spearman': {'mean': 0.6251517388482938, 'wmean': 0.6313420936117116}}}, 'MR': {'devacc': 77.04, 'acc': 76.63, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.82, 'acc': 80.45, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.59, 'acc': 88.03, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.65, 'acc': 94.29, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.31, 'acc': 82.04, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.33, 'acc': 42.35, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 83.09, 'acc': 91.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.92, 'acc': 74.14, 'f1': 80.34, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 78.71, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8062596673158293, 'pearson': 0.8068017446765752, 'spearman': 0.7402394011926606, 'mse': 0.3554067758133547, 'yhat': array([3.94677961, 4.24509852, 1.21235522, ..., 2.97456187, 4.67383004,        4.44367676]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7480499822719735, 'pearson': 0.6836250222714078, 'spearman': 0.6770241178091033, 'mse': 1.4143526060633713, 'yhat': array([1.79079639, 1.93888161, 2.15114161, ..., 3.96303776, 3.87706636,        3.70890691]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.67, 'acc': 64.04, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 363.108, 'acc': [(37.059999999999995, 70.64, 83.86, 2.4), (29.252, 64.176, 79.756, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 92.42, 'acc': 92.32, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 71.9, 'acc': 72.0, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.19, 'acc': 38.26, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.12, 'acc': 76.04, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.73, 'acc': 86.99, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.11, 'acc': 89.07, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.83, 'acc': 84.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.9, 'acc': 83.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.56, 'acc': 60.03, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 60.5, 'acc': 60.71, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 19:03:58,970 : STS12 p=0.5400, STS12 s=0.5511, STS13 p=0.5416, STS13 s=0.5428, STS14 p=0.5973, STS14 s=0.5868, STS15 p=0.6597, STS15 s=0.6642, STS 16 p=0.6168, STS16 s=0.6313, STS B p=0.6836, STS B s=0.6770, STS B m=1.4144, SICK-R p=0.8068, SICK-R s=0.7402, SICK-P m=0.3554
2019-02-14 19:03:58,970 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 19:03:58,970 : 0.5400,0.5511,0.5416,0.5428,0.5973,0.5868,0.6597,0.6642,0.6168,0.6313,0.6836,0.6770,1.4144,0.8068,0.7402,0.3554
2019-02-14 19:03:58,970 : MR=76.63, CR=80.45, SUBJ=94.29, MPQA=88.03, SST-B=82.04, SST-F=42.35, TREC=91.00, SICK-E=78.71, SNLI=64.04, MRPC=74.14, MRPC f=80.34
2019-02-14 19:03:58,971 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 19:03:58,971 : 76.63,80.45,94.29,88.03,82.04,42.35,91.00,78.71,64.04,74.14,80.34
2019-02-14 19:03:58,971 : COCO r1i2t=37.06, COCO r5i2t=70.64, COCO r10i2t=83.86, COCO medr_i2t=2.40, COCO r1t2i=29.25, COCO r5t2i=64.18, COCO r10t2i=79.76, COCO medr_t2i=3.00
2019-02-14 19:03:58,971 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 19:03:58,971 : 37.06,70.64,83.86,2.40,29.25,64.18,79.76,3.00
2019-02-14 19:03:58,971 : SentLen=92.32, WC=72.00, TreeDepth=38.26, TopConst=76.04, BShift=86.99, Tense=89.07, SubjNum=84.38, ObjNum=83.35, SOMO=60.03, CoordInv=60.71, average=74.31
2019-02-14 19:03:58,971 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 19:03:58,971 : 92.32,72.00,38.26,76.04,86.99,89.07,84.38,83.35,60.03,60.71,74.31
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:58,971 : layer 6
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:58,971 : ********************************************************************************
2019-02-14 19:03:59,055 : ***** Transfer task : STS12 *****


2019-02-14 19:03:59,067 : loading BERT model bert-base-uncased
2019-02-14 19:03:59,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:03:59,084 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:03:59,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiayrkjif
2019-02-14 19:04:01,485 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:04,878 : MSRpar : pearson = 0.3999, spearman = 0.4281
2019-02-14 19:04:06,099 : MSRvid : pearson = 0.4839, spearman = 0.4969
2019-02-14 19:04:07,005 : SMTeuroparl : pearson = 0.4647, spearman = 0.5811
2019-02-14 19:04:08,612 : surprise.OnWN : pearson = 0.6455, spearman = 0.6475
2019-02-14 19:04:09,492 : surprise.SMTnews : pearson = 0.6555, spearman = 0.5368
2019-02-14 19:04:09,493 : ALL (weighted average) : Pearson = 0.5218,             Spearman = 0.5342
2019-02-14 19:04:09,493 : ALL (average) : Pearson = 0.5299,             Spearman = 0.5381

2019-02-14 19:04:09,493 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 19:04:09,502 : loading BERT model bert-base-uncased
2019-02-14 19:04:09,503 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:04:09,520 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:04:09,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5fpy2zab
2019-02-14 19:04:11,918 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:14,063 : FNWN : pearson = 0.3267, spearman = 0.3481
2019-02-14 19:04:15,474 : headlines : pearson = 0.6275, spearman = 0.6051
2019-02-14 19:04:16,611 : OnWN : pearson = 0.4352, spearman = 0.4644
2019-02-14 19:04:16,611 : ALL (weighted average) : Pearson = 0.5177,             Spearman = 0.5201
2019-02-14 19:04:16,611 : ALL (average) : Pearson = 0.4632,             Spearman = 0.4725

2019-02-14 19:04:16,611 : ***** Transfer task : STS14 *****


2019-02-14 19:04:16,628 : loading BERT model bert-base-uncased
2019-02-14 19:04:16,628 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:04:16,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:04:16,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnh8lj_be
2019-02-14 19:04:19,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:21,559 : deft-forum : pearson = 0.3019, spearman = 0.3307
2019-02-14 19:04:22,496 : deft-news : pearson = 0.7436, spearman = 0.7125
2019-02-14 19:04:24,065 : headlines : pearson = 0.5852, spearman = 0.5402
2019-02-14 19:04:25,619 : images : pearson = 0.5560, spearman = 0.5507
2019-02-14 19:04:27,220 : OnWN : pearson = 0.6241, spearman = 0.6608
2019-02-14 19:04:29,132 : tweet-news : pearson = 0.6649, spearman = 0.6198
2019-02-14 19:04:29,132 : ALL (weighted average) : Pearson = 0.5818,             Spearman = 0.5710
2019-02-14 19:04:29,132 : ALL (average) : Pearson = 0.5793,             Spearman = 0.5691

2019-02-14 19:04:29,132 : ***** Transfer task : STS15 *****


2019-02-14 19:04:29,165 : loading BERT model bert-base-uncased
2019-02-14 19:04:29,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:04:29,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:04:29,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmu_9ve3a
2019-02-14 19:04:31,597 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:33,939 : answers-forums : pearson = 0.5410, spearman = 0.5321
2019-02-14 19:04:35,169 : answers-students : pearson = 0.6891, spearman = 0.6949
2019-02-14 19:04:36,129 : belief : pearson = 0.6459, spearman = 0.6760
2019-02-14 19:04:37,444 : headlines : pearson = 0.6419, spearman = 0.6321
2019-02-14 19:04:38,711 : images : pearson = 0.6817, spearman = 0.6897
2019-02-14 19:04:38,711 : ALL (weighted average) : Pearson = 0.6515,             Spearman = 0.6552
2019-02-14 19:04:38,711 : ALL (average) : Pearson = 0.6399,             Spearman = 0.6450

2019-02-14 19:04:38,711 : ***** Transfer task : STS16 *****


2019-02-14 19:04:38,778 : loading BERT model bert-base-uncased
2019-02-14 19:04:38,778 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:04:38,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:04:38,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp07i2tgd1
2019-02-14 19:04:41,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:43,116 : answer-answer : pearson = 0.5459, spearman = 0.5658
2019-02-14 19:04:43,536 : headlines : pearson = 0.6578, spearman = 0.6567
2019-02-14 19:04:44,012 : plagiarism : pearson = 0.7634, spearman = 0.7733
2019-02-14 19:04:44,691 : postediting : pearson = 0.7980, spearman = 0.8327
2019-02-14 19:04:45,096 : question-question : pearson = 0.2986, spearman = 0.2923
2019-02-14 19:04:45,096 : ALL (weighted average) : Pearson = 0.6199,             Spearman = 0.6318
2019-02-14 19:04:45,097 : ALL (average) : Pearson = 0.6128,             Spearman = 0.6242

2019-02-14 19:04:45,097 : ***** Transfer task : MR *****


2019-02-14 19:04:45,113 : loading BERT model bert-base-uncased
2019-02-14 19:04:45,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:04:45,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:04:45,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeghfhcrw
2019-02-14 19:04:47,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:04:49,021 : Generating sentence embeddings
2019-02-14 19:05:03,897 : Generated sentence embeddings
2019-02-14 19:05:03,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 19:05:22,974 : Best param found at split 1: l2reg = 0.0001                 with score 77.67
2019-02-14 19:05:46,383 : Best param found at split 2: l2reg = 0.0001                 with score 77.08
2019-02-14 19:06:06,498 : Best param found at split 3: l2reg = 0.001                 with score 77.98
2019-02-14 19:06:29,582 : Best param found at split 4: l2reg = 0.0001                 with score 77.81
2019-02-14 19:06:50,791 : Best param found at split 5: l2reg = 1e-05                 with score 78.0
2019-02-14 19:06:52,240 : Dev acc : 77.71 Test acc : 77.68

2019-02-14 19:06:52,241 : ***** Transfer task : CR *****


2019-02-14 19:06:52,249 : loading BERT model bert-base-uncased
2019-02-14 19:06:52,249 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:06:52,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:06:52,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpexgagiys
2019-02-14 19:06:54,697 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:06:56,152 : Generating sentence embeddings
2019-02-14 19:07:00,528 : Generated sentence embeddings
2019-02-14 19:07:00,529 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 19:07:08,098 : Best param found at split 1: l2reg = 0.001                 with score 81.68
2019-02-14 19:07:15,936 : Best param found at split 2: l2reg = 1e-05                 with score 81.88
2019-02-14 19:07:24,374 : Best param found at split 3: l2reg = 1e-05                 with score 82.42
2019-02-14 19:07:33,118 : Best param found at split 4: l2reg = 0.001                 with score 82.03
2019-02-14 19:07:42,095 : Best param found at split 5: l2reg = 1e-05                 with score 82.26
2019-02-14 19:07:42,535 : Dev acc : 82.05 Test acc : 81.24

2019-02-14 19:07:42,535 : ***** Transfer task : MPQA *****


2019-02-14 19:07:42,540 : loading BERT model bert-base-uncased
2019-02-14 19:07:42,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:07:42,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:07:42,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl0wuri6_
2019-02-14 19:07:44,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:07:46,470 : Generating sentence embeddings
2019-02-14 19:07:55,122 : Generated sentence embeddings
2019-02-14 19:07:55,122 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 19:08:19,160 : Best param found at split 1: l2reg = 0.01                 with score 87.69
2019-02-14 19:08:43,631 : Best param found at split 2: l2reg = 0.001                 with score 88.0
2019-02-14 19:09:08,402 : Best param found at split 3: l2reg = 0.001                 with score 87.81
2019-02-14 19:09:32,464 : Best param found at split 4: l2reg = 0.001                 with score 88.33
2019-02-14 19:09:54,968 : Best param found at split 5: l2reg = 1e-05                 with score 87.36
2019-02-14 19:09:56,488 : Dev acc : 87.84 Test acc : 88.41

2019-02-14 19:09:56,489 : ***** Transfer task : SUBJ *****


2019-02-14 19:09:56,505 : loading BERT model bert-base-uncased
2019-02-14 19:09:56,505 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:09:56,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:09:56,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7vp1e6tf
2019-02-14 19:09:58,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:10:00,406 : Generating sentence embeddings
2019-02-14 19:10:17,267 : Generated sentence embeddings
2019-02-14 19:10:17,268 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 19:10:36,746 : Best param found at split 1: l2reg = 0.001                 with score 94.96
2019-02-14 19:10:58,261 : Best param found at split 2: l2reg = 0.001                 with score 94.94
2019-02-14 19:11:19,618 : Best param found at split 3: l2reg = 0.001                 with score 94.44
2019-02-14 19:11:43,128 : Best param found at split 4: l2reg = 0.001                 with score 95.1
2019-02-14 19:12:01,817 : Best param found at split 5: l2reg = 0.001                 with score 94.58
2019-02-14 19:12:03,410 : Dev acc : 94.8 Test acc : 94.63

2019-02-14 19:12:03,411 : ***** Transfer task : SST Binary classification *****


2019-02-14 19:12:03,538 : loading BERT model bert-base-uncased
2019-02-14 19:12:03,538 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:12:03,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:12:03,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv4czsj3g
2019-02-14 19:12:05,955 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:12:07,392 : Computing embedding for train
2019-02-14 19:13:17,440 : Computed train embeddings
2019-02-14 19:13:17,440 : Computing embedding for dev
2019-02-14 19:13:18,554 : Computed dev embeddings
2019-02-14 19:13:18,554 : Computing embedding for test
2019-02-14 19:13:20,988 : Computed test embeddings
2019-02-14 19:13:20,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:14:07,959 : [('reg:1e-05', 81.77), ('reg:0.0001', 81.88), ('reg:0.001', 82.11), ('reg:0.01', 81.54)]
2019-02-14 19:14:07,959 : Validation : best param found is reg = 0.001 with score             82.11
2019-02-14 19:14:07,959 : Evaluating...
2019-02-14 19:14:18,584 : 
Dev acc : 82.11 Test acc : 81.66 for             SST Binary classification

2019-02-14 19:14:18,584 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 19:14:18,633 : loading BERT model bert-base-uncased
2019-02-14 19:14:18,633 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:14:18,654 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:14:18,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc9ktw6yt
2019-02-14 19:14:21,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:14:22,507 : Computing embedding for train
2019-02-14 19:14:35,107 : Computed train embeddings
2019-02-14 19:14:35,107 : Computing embedding for dev
2019-02-14 19:14:36,898 : Computed dev embeddings
2019-02-14 19:14:36,898 : Computing embedding for test
2019-02-14 19:14:39,996 : Computed test embeddings
2019-02-14 19:14:39,996 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:14:45,638 : [('reg:1e-05', 42.6), ('reg:0.0001', 42.78), ('reg:0.001', 42.05), ('reg:0.01', 41.87)]
2019-02-14 19:14:45,638 : Validation : best param found is reg = 0.0001 with score             42.78
2019-02-14 19:14:45,639 : Evaluating...
2019-02-14 19:14:47,090 : 
Dev acc : 42.78 Test acc : 45.11 for             SST Fine-Grained classification

2019-02-14 19:14:47,090 : ***** Transfer task : TREC *****


2019-02-14 19:14:47,104 : loading BERT model bert-base-uncased
2019-02-14 19:14:47,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:14:47,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:14:47,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppqt49fvl
2019-02-14 19:14:49,514 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:14:56,755 : Computed train embeddings
2019-02-14 19:14:57,265 : Computed test embeddings
2019-02-14 19:14:57,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 19:15:13,407 : [('reg:1e-05', 82.92), ('reg:0.0001', 82.96), ('reg:0.001', 82.26), ('reg:0.01', 77.18)]
2019-02-14 19:15:13,407 : Cross-validation : best param found is reg = 0.0001             with score 82.96
2019-02-14 19:15:13,407 : Evaluating...
2019-02-14 19:15:14,405 : 
Dev acc : 82.96 Test acc : 91.4             for TREC

2019-02-14 19:15:14,405 : ***** Transfer task : MRPC *****


2019-02-14 19:15:14,427 : loading BERT model bert-base-uncased
2019-02-14 19:15:14,427 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:15:14,448 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:15:14,448 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwg6c259a
2019-02-14 19:15:16,873 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:15:18,320 : Computing embedding for train
2019-02-14 19:15:30,553 : Computed train embeddings
2019-02-14 19:15:30,553 : Computing embedding for test
2019-02-14 19:15:35,766 : Computed test embeddings
2019-02-14 19:15:35,782 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 19:15:47,057 : [('reg:1e-05', 74.68), ('reg:0.0001', 74.9), ('reg:0.001', 74.76), ('reg:0.01', 72.74)]
2019-02-14 19:15:47,057 : Cross-validation : best param found is reg = 0.0001             with score 74.9
2019-02-14 19:15:47,057 : Evaluating...
2019-02-14 19:15:47,771 : Dev acc : 74.9 Test acc 72.93; Test F1 78.9 for MRPC.

2019-02-14 19:15:47,771 : ***** Transfer task : SICK-Entailment*****


2019-02-14 19:15:47,832 : loading BERT model bert-base-uncased
2019-02-14 19:15:47,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:15:47,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:15:47,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzrvsb5w0
2019-02-14 19:15:50,270 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:15:51,765 : Computing embedding for train
2019-02-14 19:16:01,497 : Computed train embeddings
2019-02-14 19:16:01,497 : Computing embedding for dev
2019-02-14 19:16:02,765 : Computed dev embeddings
2019-02-14 19:16:02,765 : Computing embedding for test
2019-02-14 19:16:14,657 : Computed test embeddings
2019-02-14 19:16:14,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:16:17,646 : [('reg:1e-05', 77.8), ('reg:0.0001', 78.2), ('reg:0.001', 78.8), ('reg:0.01', 77.2)]
2019-02-14 19:16:17,646 : Validation : best param found is reg = 0.001 with score             78.8
2019-02-14 19:16:17,646 : Evaluating...
2019-02-14 19:16:18,000 : 
Dev acc : 78.8 Test acc : 77.71 for                        SICK entailment

2019-02-14 19:16:18,001 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 19:16:18,029 : loading BERT model bert-base-uncased
2019-02-14 19:16:18,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:16:18,091 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:16:18,091 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1plbvskt
2019-02-14 19:16:20,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:16:21,996 : Computing embedding for train
2019-02-14 19:16:30,434 : Computed train embeddings
2019-02-14 19:16:30,434 : Computing embedding for dev
2019-02-14 19:16:31,539 : Computed dev embeddings
2019-02-14 19:16:31,539 : Computing embedding for test
2019-02-14 19:16:41,596 : Computed test embeddings
2019-02-14 19:17:08,117 : Dev : Pearson 0.791794694684616
2019-02-14 19:17:08,117 : Test : Pearson 0.802239181943406 Spearman 0.7393939491242097 MSE 0.3627628387879465                        for SICK Relatedness

2019-02-14 19:17:08,118 : 

***** Transfer task : STSBenchmark*****


2019-02-14 19:17:08,157 : loading BERT model bert-base-uncased
2019-02-14 19:17:08,157 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:17:08,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:17:08,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsmo6fgfm
2019-02-14 19:17:10,629 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:17:12,051 : Computing embedding for train
2019-02-14 19:17:21,099 : Computed train embeddings
2019-02-14 19:17:21,099 : Computing embedding for dev
2019-02-14 19:17:23,761 : Computed dev embeddings
2019-02-14 19:17:23,761 : Computing embedding for test
2019-02-14 19:17:25,947 : Computed test embeddings
2019-02-14 19:17:50,608 : Dev : Pearson 0.7283543310232476
2019-02-14 19:17:50,608 : Test : Pearson 0.6755339489390899 Spearman 0.6671697691282354 MSE 1.4424014281618733                        for SICK Relatedness

2019-02-14 19:17:50,608 : ***** Transfer task : SNLI Entailment*****


2019-02-14 19:17:55,363 : loading BERT model bert-base-uncased
2019-02-14 19:17:55,363 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:17:55,436 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:17:55,436 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp260he5sn
2019-02-14 19:17:57,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:17:59,512 : PROGRESS (encoding): 0.00%
2019-02-14 19:19:43,427 : PROGRESS (encoding): 14.56%
2019-02-14 19:22:28,453 : PROGRESS (encoding): 29.12%
2019-02-14 19:25:18,033 : PROGRESS (encoding): 43.69%
2019-02-14 19:28:11,329 : PROGRESS (encoding): 58.25%
2019-02-14 19:30:37,970 : PROGRESS (encoding): 72.81%
2019-02-14 19:32:22,661 : PROGRESS (encoding): 87.37%
2019-02-14 19:34:13,457 : PROGRESS (encoding): 0.00%
2019-02-14 19:34:27,137 : PROGRESS (encoding): 0.00%
2019-02-14 19:34:40,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:35:08,528 : [('reg:1e-09', 66.79)]
2019-02-14 19:35:08,528 : Validation : best param found is reg = 1e-09 with score             66.79
2019-02-14 19:35:08,528 : Evaluating...
2019-02-14 19:35:36,387 : Dev acc : 66.79 Test acc : 66.86 for SNLI

2019-02-14 19:35:36,387 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 19:35:45,198 : loading BERT model bert-base-uncased
2019-02-14 19:35:45,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:35:45,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:35:45,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_80gw7ct
2019-02-14 19:35:47,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:35:49,049 : Computing embedding for train
2019-02-14 19:43:22,092 : Computed train embeddings
2019-02-14 19:43:22,093 : Computing embedding for dev
2019-02-14 19:43:42,537 : Computed dev embeddings
2019-02-14 19:43:42,537 : Computing embedding for test
2019-02-14 19:44:02,433 : Computed test embeddings
2019-02-14 19:44:02,449 : prepare data
2019-02-14 19:44:02,514 : start epoch
2019-02-14 19:44:43,014 : samples : 64000
2019-02-14 19:44:53,269 : Image to text: 8.2, 23.14, 33.82, 22.0
2019-02-14 19:45:00,641 : Text to Image: 6.304, 19.808, 30.456, 26.0
2019-02-14 19:45:40,620 : samples : 128000
2019-02-14 19:45:50,832 : Image to text: 9.44, 25.6, 37.42, 19.0
2019-02-14 19:45:58,207 : Text to Image: 7.028, 22.096, 33.108, 23.0
2019-02-14 19:46:39,490 : samples : 192000
2019-02-14 19:46:49,740 : Image to text: 9.52, 26.94, 38.94, 18.0
2019-02-14 19:46:57,120 : Text to Image: 7.948, 23.228, 34.36, 22.0
2019-02-14 19:47:36,867 : samples : 256000
2019-02-14 19:47:47,016 : Image to text: 8.96, 26.82, 39.28, 17.0
2019-02-14 19:47:54,329 : Text to Image: 8.164, 24.18, 35.52, 21.0
2019-02-14 19:48:34,600 : samples : 320000
2019-02-14 19:48:44,851 : Image to text: 8.88, 27.1, 39.42, 17.0
2019-02-14 19:48:52,211 : Text to Image: 8.352, 25.104, 36.604, 20.0
2019-02-14 19:49:33,140 : samples : 384000
2019-02-14 19:49:43,301 : Image to text: 10.08, 28.64, 41.36, 15.0
2019-02-14 19:49:50,629 : Text to Image: 8.592, 25.688, 37.36, 19.0
2019-02-14 19:50:31,122 : samples : 448000
2019-02-14 19:50:41,374 : Image to text: 9.76, 27.84, 39.94, 16.0
2019-02-14 19:50:48,732 : Text to Image: 8.292, 24.616, 36.876, 19.0
2019-02-14 19:51:29,512 : samples : 512000
2019-02-14 19:51:39,748 : Image to text: 11.4, 30.14, 42.78, 15.0
2019-02-14 19:51:47,078 : Text to Image: 9.06, 26.164, 38.184, 18.0
2019-02-14 19:52:21,071 : Epoch 1 finished
2019-02-14 19:52:21,503 : Image to text: 28.1, 60.7, 74.4, 4.0
2019-02-14 19:52:21,828 : Text to Image: 24.06, 56.96, 73.14, 4.0
2019-02-14 19:52:22,251 : Image to text: 25.4, 62.0, 75.4, 3.0
2019-02-14 19:52:22,590 : Text to Image: 23.14, 54.94, 72.62, 5.0
2019-02-14 19:52:23,017 : Image to text: 27.9, 61.3, 76.9, 3.0
2019-02-14 19:52:23,344 : Text to Image: 22.96, 56.1, 73.64, 4.0
2019-02-14 19:52:23,783 : Image to text: 28.7, 62.1, 75.8, 3.0
2019-02-14 19:52:24,107 : Text to Image: 23.54, 56.54, 73.64, 4.0
2019-02-14 19:52:24,528 : Image to text: 28.0, 61.4, 76.5, 4.0
2019-02-14 19:52:24,851 : Text to Image: 23.1, 56.5, 72.88, 4.0
2019-02-14 19:52:24,851 : Dev mean Text to Image: 23.360000000000003, 56.208, 73.184, 4.2
2019-02-14 19:52:24,851 : Dev mean Image to text: 27.620000000000005, 61.5, 75.8, 3.4000000000000004
2019-02-14 19:52:24,852 : start epoch
2019-02-14 19:53:05,766 : samples : 64000
2019-02-14 19:53:15,976 : Image to text: 10.98, 30.58, 43.2, 15.0
2019-02-14 19:53:23,298 : Text to Image: 8.94, 26.576, 38.524, 18.0
2019-02-14 19:54:04,364 : samples : 128000
2019-02-14 19:54:14,578 : Image to text: 10.16, 29.94, 42.2, 15.0
2019-02-14 19:54:21,907 : Text to Image: 8.892, 26.404, 38.484, 18.0
2019-02-14 19:55:02,984 : samples : 192000
2019-02-14 19:55:13,164 : Image to text: 10.66, 30.66, 43.52, 14.0
2019-02-14 19:55:20,489 : Text to Image: 9.52, 27.544, 39.368, 17.0
2019-02-14 19:56:00,225 : samples : 256000
2019-02-14 19:56:10,390 : Image to text: 11.72, 31.08, 43.56, 14.0
2019-02-14 19:56:17,757 : Text to Image: 9.652, 28.148, 40.032, 17.0
2019-02-14 19:57:00,519 : samples : 320000
2019-02-14 19:57:10,707 : Image to text: 11.26, 31.58, 45.04, 14.0
2019-02-14 19:57:18,074 : Text to Image: 9.94, 28.46, 40.316, 16.0
2019-02-14 19:57:59,557 : samples : 384000
2019-02-14 19:58:09,741 : Image to text: 11.82, 31.42, 44.5, 13.0
2019-02-14 19:58:17,097 : Text to Image: 9.72, 27.504, 39.868, 17.0
2019-02-14 19:58:58,673 : samples : 448000
2019-02-14 19:59:08,821 : Image to text: 12.42, 32.8, 46.2, 13.0
2019-02-14 19:59:16,136 : Text to Image: 10.092, 28.656, 40.712, 16.0
2019-02-14 19:59:57,009 : samples : 512000
2019-02-14 20:00:07,257 : Image to text: 11.22, 31.4, 44.86, 13.0
2019-02-14 20:00:14,621 : Text to Image: 10.188, 28.116, 40.544, 16.0
2019-02-14 20:00:48,522 : Epoch 2 finished
2019-02-14 20:00:48,935 : Image to text: 31.7, 62.4, 77.1, 3.0
2019-02-14 20:00:49,257 : Text to Image: 25.4, 59.62, 76.18, 4.0
2019-02-14 20:00:49,676 : Image to text: 28.9, 63.8, 76.0, 3.0
2019-02-14 20:00:50,008 : Text to Image: 24.64, 57.38, 75.02, 4.0
2019-02-14 20:00:50,437 : Image to text: 29.6, 64.5, 78.3, 3.0
2019-02-14 20:00:50,764 : Text to Image: 25.64, 58.66, 75.48, 4.0
2019-02-14 20:00:51,192 : Image to text: 30.3, 63.8, 78.5, 3.0
2019-02-14 20:00:51,532 : Text to Image: 24.94, 59.06, 75.14, 4.0
2019-02-14 20:00:51,956 : Image to text: 31.4, 62.7, 77.7, 3.0
2019-02-14 20:00:52,281 : Text to Image: 25.4, 59.02, 75.04, 4.0
2019-02-14 20:00:52,281 : Dev mean Text to Image: 25.204, 58.748000000000005, 75.372, 4.0
2019-02-14 20:00:52,282 : Dev mean Image to text: 30.380000000000003, 63.44, 77.52000000000001, 3.0
2019-02-14 20:00:52,282 : start epoch
2019-02-14 20:01:32,002 : samples : 64000
2019-02-14 20:01:42,288 : Image to text: 12.5, 33.24, 46.1, 12.0
2019-02-14 20:01:49,628 : Text to Image: 10.26, 28.512, 40.804, 16.0
2019-02-14 20:02:29,744 : samples : 128000
2019-02-14 20:02:39,868 : Image to text: 11.06, 32.06, 44.82, 13.0
2019-02-14 20:02:47,209 : Text to Image: 10.108, 28.308, 40.444, 16.0
2019-02-14 20:03:26,951 : samples : 192000
2019-02-14 20:03:37,085 : Image to text: 12.64, 33.84, 46.8, 12.0
2019-02-14 20:03:44,435 : Text to Image: 10.576, 29.204, 41.848, 15.0
2019-02-14 20:04:25,297 : samples : 256000
2019-02-14 20:04:35,472 : Image to text: 12.8, 33.42, 46.94, 12.0
2019-02-14 20:04:42,833 : Text to Image: 10.56, 29.444, 41.984, 15.0
2019-02-14 20:05:23,076 : samples : 320000
2019-02-14 20:05:33,352 : Image to text: 11.48, 31.64, 44.02, 14.0
2019-02-14 20:05:40,710 : Text to Image: 9.66, 27.692, 39.932, 16.0
2019-02-14 20:06:20,663 : samples : 384000
2019-02-14 20:06:30,901 : Image to text: 12.4, 33.1, 46.02, 12.0
2019-02-14 20:06:38,267 : Text to Image: 10.472, 29.056, 41.488, 15.0
2019-02-14 20:07:20,385 : samples : 448000
2019-02-14 20:07:30,646 : Image to text: 13.58, 34.74, 48.2, 12.0
2019-02-14 20:07:37,971 : Text to Image: 11.048, 30.288, 42.748, 15.0
2019-02-14 20:08:18,051 : samples : 512000
2019-02-14 20:08:28,209 : Image to text: 13.16, 34.34, 48.06, 11.0
2019-02-14 20:08:35,600 : Text to Image: 10.688, 30.068, 42.4, 15.0
2019-02-14 20:09:09,719 : Epoch 3 finished
2019-02-14 20:09:10,144 : Image to text: 29.7, 64.7, 79.3, 3.0
2019-02-14 20:09:10,471 : Text to Image: 26.44, 61.42, 78.02, 4.0
2019-02-14 20:09:10,897 : Image to text: 31.2, 65.1, 79.5, 3.0
2019-02-14 20:09:11,228 : Text to Image: 25.62, 59.48, 76.0, 4.0
2019-02-14 20:09:11,656 : Image to text: 33.6, 64.3, 79.0, 3.0
2019-02-14 20:09:11,981 : Text to Image: 26.84, 61.24, 76.66, 4.0
2019-02-14 20:09:12,403 : Image to text: 33.1, 67.3, 81.4, 3.0
2019-02-14 20:09:12,728 : Text to Image: 27.4, 60.9, 77.02, 4.0
2019-02-14 20:09:13,167 : Image to text: 33.5, 66.4, 79.6, 3.0
2019-02-14 20:09:13,494 : Text to Image: 26.42, 60.84, 76.06, 4.0
2019-02-14 20:09:13,494 : Dev mean Text to Image: 26.544000000000004, 60.775999999999996, 76.752, 4.0
2019-02-14 20:09:13,494 : Dev mean Image to text: 32.22, 65.56, 79.76, 3.0
2019-02-14 20:09:13,495 : start epoch
2019-02-14 20:09:53,219 : samples : 64000
2019-02-14 20:10:03,414 : Image to text: 12.66, 34.94, 47.92, 11.0
2019-02-14 20:10:10,747 : Text to Image: 10.48, 29.496, 42.18, 15.0
2019-02-14 20:10:52,471 : samples : 128000
2019-02-14 20:11:02,598 : Image to text: 12.92, 34.1, 47.54, 12.0
2019-02-14 20:11:09,966 : Text to Image: 10.572, 29.852, 42.316, 15.0
2019-02-14 20:11:51,787 : samples : 192000
2019-02-14 20:12:01,911 : Image to text: 12.52, 33.56, 47.84, 11.0
2019-02-14 20:12:09,290 : Text to Image: 10.54, 29.34, 41.772, 15.0
2019-02-14 20:12:48,919 : samples : 256000
2019-02-14 20:12:59,031 : Image to text: 13.36, 34.96, 48.1, 11.0
2019-02-14 20:13:06,438 : Text to Image: 10.82, 30.344, 42.948, 14.0
2019-02-14 20:13:49,188 : samples : 320000
2019-02-14 20:13:59,465 : Image to text: 13.2, 35.16, 48.84, 11.0
2019-02-14 20:14:06,834 : Text to Image: 11.148, 30.212, 43.12, 14.0
2019-02-14 20:14:46,898 : samples : 384000
2019-02-14 20:14:57,202 : Image to text: 13.12, 34.02, 48.04, 12.0
2019-02-14 20:15:04,595 : Text to Image: 10.9, 29.44, 41.912, 15.0
2019-02-14 20:15:44,439 : samples : 448000
2019-02-14 20:15:54,687 : Image to text: 13.72, 35.34, 48.7, 11.0
2019-02-14 20:16:02,034 : Text to Image: 11.292, 30.54, 42.844, 15.0
2019-02-14 20:16:43,702 : samples : 512000
2019-02-14 20:16:53,973 : Image to text: 13.48, 34.82, 48.52, 11.0
2019-02-14 20:17:01,348 : Text to Image: 10.964, 30.068, 42.972, 14.0
2019-02-14 20:17:35,719 : Epoch 4 finished
2019-02-14 20:17:36,140 : Image to text: 30.7, 66.8, 80.7, 3.0
2019-02-14 20:17:36,462 : Text to Image: 25.98, 61.3, 78.16, 4.0
2019-02-14 20:17:36,882 : Image to text: 33.0, 65.6, 79.2, 3.0
2019-02-14 20:17:37,209 : Text to Image: 26.44, 59.54, 76.04, 4.0
2019-02-14 20:17:37,636 : Image to text: 32.6, 65.2, 80.2, 3.0
2019-02-14 20:17:37,965 : Text to Image: 26.38, 61.2, 77.04, 4.0
2019-02-14 20:17:38,402 : Image to text: 33.2, 66.3, 80.3, 3.0
2019-02-14 20:17:38,727 : Text to Image: 26.68, 61.3, 77.22, 4.0
2019-02-14 20:17:39,148 : Image to text: 34.4, 66.9, 80.9, 3.0
2019-02-14 20:17:39,472 : Text to Image: 26.54, 61.64, 76.96, 4.0
2019-02-14 20:17:39,472 : Dev mean Text to Image: 26.404, 60.995999999999995, 77.084, 4.0
2019-02-14 20:17:39,472 : Dev mean Image to text: 32.78, 66.16, 80.25999999999999, 3.0
2019-02-14 20:17:39,473 : start epoch
2019-02-14 20:18:21,436 : samples : 64000
2019-02-14 20:18:31,518 : Image to text: 13.7, 35.12, 48.66, 11.0
2019-02-14 20:18:38,875 : Text to Image: 11.228, 30.76, 43.728, 14.0
2019-02-14 20:19:18,504 : samples : 128000
2019-02-14 20:19:28,555 : Image to text: 13.42, 33.92, 48.2, 11.0
2019-02-14 20:19:35,943 : Text to Image: 11.108, 30.136, 43.0, 14.0
2019-02-14 20:20:15,506 : samples : 192000
2019-02-14 20:20:25,547 : Image to text: 12.76, 33.48, 47.0, 12.0
2019-02-14 20:20:32,900 : Text to Image: 10.72, 30.26, 42.652, 14.0
2019-02-14 20:21:13,396 : samples : 256000
2019-02-14 20:21:23,420 : Image to text: 13.04, 34.7, 48.08, 11.0
2019-02-14 20:21:30,778 : Text to Image: 11.084, 30.592, 43.032, 14.0
2019-02-14 20:22:10,907 : samples : 320000
2019-02-14 20:22:21,223 : Image to text: 13.4, 35.3, 49.88, 11.0
2019-02-14 20:22:28,596 : Text to Image: 11.248, 31.096, 43.708, 14.0
2019-02-14 20:23:09,788 : samples : 384000
2019-02-14 20:23:20,098 : Image to text: 13.62, 35.32, 48.32, 11.0
2019-02-14 20:23:27,463 : Text to Image: 10.956, 30.964, 43.832, 14.0
2019-02-14 20:24:09,004 : samples : 448000
2019-02-14 20:24:19,321 : Image to text: 12.94, 35.2, 48.28, 11.0
2019-02-14 20:24:26,701 : Text to Image: 10.856, 30.24, 42.684, 15.0
2019-02-14 20:25:07,344 : samples : 512000
2019-02-14 20:25:17,653 : Image to text: 14.08, 35.58, 49.14, 11.0
2019-02-14 20:25:25,036 : Text to Image: 11.192, 30.784, 43.532, 14.0
2019-02-14 20:25:59,706 : Epoch 5 finished
2019-02-14 20:26:00,133 : Image to text: 34.2, 65.4, 79.6, 3.0
2019-02-14 20:26:00,457 : Text to Image: 26.62, 62.04, 78.3, 4.0
2019-02-14 20:26:00,871 : Image to text: 32.7, 66.1, 80.0, 3.0
2019-02-14 20:26:01,192 : Text to Image: 26.12, 59.82, 76.8, 4.0
2019-02-14 20:26:01,626 : Image to text: 32.2, 65.5, 79.2, 3.0
2019-02-14 20:26:01,951 : Text to Image: 26.84, 61.48, 77.58, 4.0
2019-02-14 20:26:02,376 : Image to text: 34.2, 67.7, 80.7, 3.0
2019-02-14 20:26:02,704 : Text to Image: 27.06, 61.56, 77.68, 3.0
2019-02-14 20:26:03,145 : Image to text: 35.0, 68.2, 81.1, 3.0
2019-02-14 20:26:03,472 : Text to Image: 27.52, 61.34, 77.48, 4.0
2019-02-14 20:26:03,473 : Dev mean Text to Image: 26.832, 61.248000000000005, 77.568, 3.8000000000000007
2019-02-14 20:26:03,473 : Dev mean Image to text: 33.660000000000004, 66.58, 80.12, 3.0
2019-02-14 20:26:03,473 : start epoch
2019-02-14 20:26:45,168 : samples : 64000
2019-02-14 20:26:55,263 : Image to text: 14.16, 35.78, 49.68, 11.0
2019-02-14 20:27:02,662 : Text to Image: 11.448, 31.172, 43.888, 14.0
2019-02-14 20:27:49,425 : samples : 128000
2019-02-14 20:27:59,476 : Image to text: 13.76, 35.68, 49.04, 11.0
2019-02-14 20:28:06,873 : Text to Image: 11.16, 30.556, 43.496, 14.0
2019-02-14 20:28:46,804 : samples : 192000
2019-02-14 20:28:56,876 : Image to text: 13.84, 35.7, 49.1, 11.0
2019-02-14 20:29:04,278 : Text to Image: 11.364, 30.76, 43.408, 14.0
2019-02-14 20:29:46,443 : samples : 256000
2019-02-14 20:29:56,522 : Image to text: 14.32, 36.04, 49.76, 11.0
2019-02-14 20:30:03,929 : Text to Image: 11.388, 31.436, 44.06, 14.0
2019-02-14 20:30:45,480 : samples : 320000
2019-02-14 20:30:55,814 : Image to text: 13.08, 35.14, 48.8, 11.0
2019-02-14 20:31:03,196 : Text to Image: 11.38, 30.832, 43.596, 14.0
2019-02-14 20:31:46,234 : samples : 384000
2019-02-14 20:31:56,530 : Image to text: 14.0, 36.98, 50.9, 10.0
2019-02-14 20:32:03,915 : Text to Image: 11.496, 31.192, 43.932, 14.0
2019-02-14 20:32:46,605 : samples : 448000
2019-02-14 20:32:56,870 : Image to text: 13.78, 36.96, 50.2, 10.0
2019-02-14 20:33:04,247 : Text to Image: 11.456, 31.272, 44.288, 14.0
2019-02-14 20:33:45,794 : samples : 512000
2019-02-14 20:33:56,035 : Image to text: 13.76, 35.84, 49.54, 11.0
2019-02-14 20:34:03,385 : Text to Image: 11.112, 31.136, 44.02, 14.0
2019-02-14 20:34:38,644 : Epoch 6 finished
2019-02-14 20:34:39,072 : Image to text: 33.6, 67.4, 80.1, 3.0
2019-02-14 20:34:39,394 : Text to Image: 27.54, 63.02, 78.7, 3.0
2019-02-14 20:34:39,811 : Image to text: 33.5, 67.6, 81.4, 3.0
2019-02-14 20:34:40,134 : Text to Image: 27.82, 61.62, 77.58, 4.0
2019-02-14 20:34:40,569 : Image to text: 34.0, 67.3, 79.8, 3.0
2019-02-14 20:34:40,897 : Text to Image: 27.76, 62.24, 78.16, 3.0
2019-02-14 20:34:41,324 : Image to text: 35.1, 68.4, 81.1, 2.0
2019-02-14 20:34:41,665 : Text to Image: 27.58, 62.5, 78.56, 3.0
2019-02-14 20:34:42,092 : Image to text: 36.3, 68.6, 81.8, 3.0
2019-02-14 20:34:42,419 : Text to Image: 28.26, 62.24, 77.36, 3.0
2019-02-14 20:34:42,419 : Dev mean Text to Image: 27.792, 62.324000000000005, 78.07199999999999, 3.2
2019-02-14 20:34:42,419 : Dev mean Image to text: 34.5, 67.86, 80.83999999999999, 2.8
2019-02-14 20:34:42,419 : start epoch
2019-02-14 20:35:28,949 : samples : 64000
2019-02-14 20:35:39,065 : Image to text: 13.98, 37.14, 50.56, 10.0
2019-02-14 20:35:46,402 : Text to Image: 11.544, 31.472, 43.956, 14.0
2019-02-14 20:36:28,241 : samples : 128000
2019-02-14 20:36:38,371 : Image to text: 13.46, 37.12, 51.44, 10.0
2019-02-14 20:36:45,721 : Text to Image: 11.308, 31.6, 44.428, 13.0
2019-02-14 20:37:27,842 : samples : 192000
2019-02-14 20:37:37,978 : Image to text: 14.28, 36.48, 49.76, 11.0
2019-02-14 20:37:45,313 : Text to Image: 11.656, 31.924, 44.748, 13.0
2019-02-14 20:38:27,195 : samples : 256000
2019-02-14 20:38:37,318 : Image to text: 13.74, 35.14, 48.72, 11.0
2019-02-14 20:38:44,681 : Text to Image: 11.512, 31.624, 44.66, 13.0
2019-02-14 20:39:24,578 : samples : 320000
2019-02-14 20:39:34,829 : Image to text: 14.54, 36.12, 50.82, 10.0
2019-02-14 20:39:42,175 : Text to Image: 11.736, 31.916, 44.604, 14.0
2019-02-14 20:40:21,848 : samples : 384000
2019-02-14 20:40:32,117 : Image to text: 13.58, 36.62, 50.46, 10.0
2019-02-14 20:40:39,493 : Text to Image: 11.52, 31.76, 44.644, 13.0
2019-02-14 20:41:19,351 : samples : 448000
2019-02-14 20:41:29,618 : Image to text: 14.3, 37.1, 51.34, 10.0
2019-02-14 20:41:36,987 : Text to Image: 11.724, 32.332, 45.088, 13.0
2019-02-14 20:42:17,741 : samples : 512000
2019-02-14 20:42:28,029 : Image to text: 14.52, 37.9, 50.7, 10.0
2019-02-14 20:42:35,460 : Text to Image: 11.724, 32.232, 45.04, 13.0
2019-02-14 20:43:09,557 : Epoch 7 finished
2019-02-14 20:43:09,971 : Image to text: 31.3, 66.2, 81.0, 3.0
2019-02-14 20:43:10,293 : Text to Image: 27.82, 62.56, 78.88, 3.0
2019-02-14 20:43:10,722 : Image to text: 33.1, 65.9, 79.7, 3.0
2019-02-14 20:43:11,046 : Text to Image: 27.64, 61.38, 77.48, 4.0
2019-02-14 20:43:11,470 : Image to text: 33.2, 68.9, 81.3, 3.0
2019-02-14 20:43:11,797 : Text to Image: 28.44, 63.06, 79.2, 3.0
2019-02-14 20:43:12,236 : Image to text: 33.1, 68.1, 81.3, 3.0
2019-02-14 20:43:12,564 : Text to Image: 28.66, 62.74, 78.72, 3.0
2019-02-14 20:43:12,989 : Image to text: 33.5, 66.0, 79.9, 3.0
2019-02-14 20:43:13,329 : Text to Image: 28.02, 62.0, 77.86, 3.0
2019-02-14 20:43:13,329 : Dev mean Text to Image: 28.116, 62.348, 78.428, 3.2
2019-02-14 20:43:13,329 : Dev mean Image to text: 32.84, 67.02, 80.64, 3.0
2019-02-14 20:43:13,329 : start epoch
2019-02-14 20:43:52,927 : samples : 64000
2019-02-14 20:44:03,066 : Image to text: 13.94, 36.14, 49.66, 11.0
2019-02-14 20:44:10,439 : Text to Image: 11.584, 31.58, 44.52, 13.0
2019-02-14 20:44:50,160 : samples : 128000
2019-02-14 20:45:00,257 : Image to text: 14.22, 36.58, 49.96, 11.0
2019-02-14 20:45:07,654 : Text to Image: 11.316, 31.696, 44.752, 13.0
2019-02-14 20:45:47,144 : samples : 192000
2019-02-14 20:45:57,203 : Image to text: 14.78, 36.8, 50.88, 10.0
2019-02-14 20:46:04,576 : Text to Image: 11.8, 31.956, 45.104, 13.0
2019-02-14 20:46:45,909 : samples : 256000
2019-02-14 20:46:55,983 : Image to text: 13.78, 36.76, 50.9, 10.0
2019-02-14 20:47:03,322 : Text to Image: 11.684, 32.044, 44.788, 13.0
2019-02-14 20:47:44,574 : samples : 320000
2019-02-14 20:47:57,056 : Image to text: 14.58, 37.68, 51.24, 10.0
2019-02-14 20:48:06,994 : Text to Image: 12.08, 32.528, 45.332, 13.0
2019-02-14 20:48:50,693 : samples : 384000
2019-02-14 20:49:03,270 : Image to text: 13.54, 36.68, 50.62, 10.0
2019-02-14 20:49:13,235 : Text to Image: 11.86, 31.848, 44.748, 13.0
2019-02-14 20:49:53,722 : samples : 448000
2019-02-14 20:50:04,215 : Image to text: 14.02, 37.18, 50.26, 10.0
2019-02-14 20:50:14,147 : Text to Image: 11.644, 31.772, 44.8, 13.0
2019-02-14 20:50:56,763 : samples : 512000
2019-02-14 20:51:09,334 : Image to text: 14.34, 37.02, 50.36, 10.0
2019-02-14 20:51:19,295 : Text to Image: 11.704, 32.424, 45.384, 13.0
2019-02-14 20:51:53,851 : Epoch 8 finished
2019-02-14 20:51:54,302 : Image to text: 33.6, 66.8, 79.9, 3.0
2019-02-14 20:51:54,669 : Text to Image: 27.88, 63.22, 79.42, 3.0
2019-02-14 20:51:55,120 : Image to text: 34.6, 67.9, 82.7, 3.0
2019-02-14 20:51:55,486 : Text to Image: 27.86, 61.9, 78.16, 4.0
2019-02-14 20:51:55,938 : Image to text: 32.4, 67.7, 80.9, 3.0
2019-02-14 20:51:56,305 : Text to Image: 27.48, 63.46, 79.28, 3.0
2019-02-14 20:51:56,756 : Image to text: 34.0, 68.7, 81.8, 3.0
2019-02-14 20:51:57,123 : Text to Image: 28.1, 62.6, 79.44, 3.0
2019-02-14 20:51:57,575 : Image to text: 35.1, 68.5, 82.2, 3.0
2019-02-14 20:51:57,941 : Text to Image: 27.9, 62.26, 78.18, 4.0
2019-02-14 20:51:57,941 : Dev mean Text to Image: 27.844, 62.688, 78.896, 3.4000000000000004
2019-02-14 20:51:57,941 : Dev mean Image to text: 33.940000000000005, 67.92, 81.5, 3.0
2019-02-14 20:51:57,942 : start epoch
2019-02-14 20:52:38,552 : samples : 64000
2019-02-14 20:52:51,073 : Image to text: 14.56, 37.18, 51.12, 10.0
2019-02-14 20:53:01,053 : Text to Image: 11.672, 31.388, 44.496, 13.0
2019-02-14 20:53:44,447 : samples : 128000
2019-02-14 20:53:54,794 : Image to text: 14.46, 37.1, 51.16, 10.0
2019-02-14 20:54:02,017 : Text to Image: 11.556, 31.856, 45.048, 13.0
2019-02-14 20:54:43,228 : samples : 192000
2019-02-14 20:54:55,729 : Image to text: 13.9, 36.78, 50.58, 10.0
2019-02-14 20:55:05,687 : Text to Image: 11.752, 31.624, 44.6, 13.0
2019-02-14 20:55:49,089 : samples : 256000
2019-02-14 20:55:59,153 : Image to text: 14.62, 37.9, 51.1, 10.0
2019-02-14 20:56:06,402 : Text to Image: 12.028, 32.236, 45.296, 13.0
2019-02-14 20:56:46,813 : samples : 320000
2019-02-14 20:56:59,302 : Image to text: 14.3, 37.46, 51.38, 10.0
2019-02-14 20:57:09,276 : Text to Image: 12.008, 32.404, 45.324, 13.0
2019-02-14 20:57:52,763 : samples : 384000
2019-02-14 20:58:05,147 : Image to text: 14.18, 36.48, 50.26, 10.0
2019-02-14 20:58:12,395 : Text to Image: 11.616, 32.256, 45.248, 13.0
2019-02-14 20:58:52,720 : samples : 448000
2019-02-14 20:59:02,935 : Image to text: 14.52, 37.2, 51.18, 10.0
2019-02-14 20:59:12,854 : Text to Image: 11.86, 32.016, 44.764, 13.0
2019-02-14 20:59:54,772 : samples : 512000
2019-02-14 21:00:07,278 : Image to text: 13.94, 36.56, 51.1, 10.0
2019-02-14 21:00:17,228 : Text to Image: 11.968, 32.408, 45.108, 13.0
2019-02-14 21:00:51,437 : Epoch 9 finished
2019-02-14 21:00:51,902 : Image to text: 34.8, 67.4, 81.6, 3.0
2019-02-14 21:00:52,265 : Text to Image: 29.46, 63.96, 79.6, 3.0
2019-02-14 21:00:52,715 : Image to text: 33.5, 68.5, 82.1, 3.0
2019-02-14 21:00:53,075 : Text to Image: 28.06, 61.92, 78.26, 4.0
2019-02-14 21:00:53,510 : Image to text: 34.2, 67.8, 80.6, 3.0
2019-02-14 21:00:53,870 : Text to Image: 28.8, 63.62, 79.26, 3.0
2019-02-14 21:00:54,316 : Image to text: 35.9, 69.5, 82.7, 3.0
2019-02-14 21:00:54,676 : Text to Image: 29.58, 63.4, 79.16, 3.0
2019-02-14 21:00:55,121 : Image to text: 36.8, 68.6, 82.4, 3.0
2019-02-14 21:00:55,480 : Text to Image: 29.38, 63.14, 78.34, 3.0
2019-02-14 21:00:55,481 : Dev mean Text to Image: 29.056000000000004, 63.208000000000006, 78.924, 3.2
2019-02-14 21:00:55,481 : Dev mean Image to text: 35.04, 68.35999999999999, 81.88, 3.0
2019-02-14 21:00:55,481 : start epoch
2019-02-14 21:01:35,825 : samples : 64000
2019-02-14 21:01:48,355 : Image to text: 14.94, 36.96, 51.26, 10.0
2019-02-14 21:01:58,274 : Text to Image: 11.932, 32.464, 45.46, 13.0
2019-02-14 21:02:40,480 : samples : 128000
2019-02-14 21:02:50,490 : Image to text: 14.9, 37.96, 51.64, 10.0
2019-02-14 21:02:57,661 : Text to Image: 12.244, 32.688, 46.016, 13.0
2019-02-14 21:03:38,382 : samples : 192000
2019-02-14 21:03:50,826 : Image to text: 14.64, 38.26, 51.96, 10.0
2019-02-14 21:04:00,749 : Text to Image: 12.248, 32.74, 45.568, 13.0
2019-02-14 21:04:43,385 : samples : 256000
2019-02-14 21:04:54,190 : Image to text: 14.5, 37.4, 51.88, 10.0
2019-02-14 21:05:01,318 : Text to Image: 11.808, 32.18, 44.992, 13.0
2019-02-14 21:05:41,607 : samples : 320000
2019-02-14 21:05:54,137 : Image to text: 14.44, 38.02, 51.34, 10.0
2019-02-14 21:06:04,097 : Text to Image: 12.1, 32.6, 45.948, 13.0
2019-02-14 21:06:47,244 : samples : 384000
2019-02-14 21:06:59,787 : Image to text: 14.42, 36.92, 51.4, 10.0
2019-02-14 21:07:09,742 : Text to Image: 11.856, 31.956, 45.124, 13.0
2019-02-14 21:07:49,676 : samples : 448000
2019-02-14 21:08:00,013 : Image to text: 14.78, 37.22, 52.0, 10.0
2019-02-14 21:08:09,923 : Text to Image: 12.108, 32.996, 46.072, 13.0
2019-02-14 21:08:52,847 : samples : 512000
2019-02-14 21:09:05,351 : Image to text: 14.36, 36.5, 50.84, 10.0
2019-02-14 21:09:15,320 : Text to Image: 11.892, 32.408, 45.36, 13.0
2019-02-14 21:09:50,399 : Epoch 10 finished
2019-02-14 21:09:50,862 : Image to text: 35.3, 68.0, 82.2, 3.0
2019-02-14 21:09:51,224 : Text to Image: 28.78, 64.4, 79.84, 3.0
2019-02-14 21:09:51,673 : Image to text: 33.1, 66.7, 81.2, 3.0
2019-02-14 21:09:52,021 : Text to Image: 28.4, 61.82, 78.5, 3.0
2019-02-14 21:09:52,470 : Image to text: 34.0, 69.3, 80.3, 3.0
2019-02-14 21:09:52,833 : Text to Image: 28.72, 63.1, 78.5, 3.0
2019-02-14 21:09:53,282 : Image to text: 33.2, 70.2, 82.5, 3.0
2019-02-14 21:09:53,644 : Text to Image: 28.6, 63.86, 79.62, 3.0
2019-02-14 21:09:54,094 : Image to text: 33.2, 68.6, 83.6, 3.0
2019-02-14 21:09:54,456 : Text to Image: 27.98, 62.52, 78.78, 3.0
2019-02-14 21:09:54,456 : Dev mean Text to Image: 28.496, 63.14, 79.048, 3.0
2019-02-14 21:09:54,456 : Dev mean Image to text: 33.760000000000005, 68.56, 81.96000000000001, 3.0
2019-02-14 21:09:54,456 : start epoch
2019-02-14 21:10:34,694 : samples : 64000
2019-02-14 21:10:45,591 : Image to text: 14.88, 38.08, 52.04, 10.0
2019-02-14 21:10:53,733 : Text to Image: 11.956, 32.496, 45.224, 13.0
2019-02-14 21:11:34,800 : samples : 128000
2019-02-14 21:11:44,811 : Image to text: 14.22, 37.22, 51.04, 10.0
2019-02-14 21:11:52,012 : Text to Image: 11.888, 32.56, 45.648, 13.0
2019-02-14 21:12:32,057 : samples : 192000
2019-02-14 21:12:42,055 : Image to text: 14.56, 38.34, 51.76, 10.0
2019-02-14 21:12:48,865 : Text to Image: 12.12, 32.5, 45.472, 13.0
2019-02-14 21:13:28,840 : samples : 256000
2019-02-14 21:13:38,781 : Image to text: 14.88, 38.88, 52.36, 9.0
2019-02-14 21:13:45,928 : Text to Image: 11.908, 32.596, 45.748, 13.0
2019-02-14 21:14:26,648 : samples : 320000
2019-02-14 21:14:36,560 : Image to text: 14.6, 38.22, 52.14, 10.0
2019-02-14 21:14:43,646 : Text to Image: 11.936, 32.392, 45.432, 13.0
2019-02-14 21:15:24,425 : samples : 384000
2019-02-14 21:15:37,022 : Image to text: 15.38, 38.16, 52.3, 10.0
2019-02-14 21:15:47,101 : Text to Image: 12.312, 33.472, 46.564, 12.0
2019-02-14 21:16:28,235 : samples : 448000
2019-02-14 21:16:40,636 : Image to text: 15.5, 38.06, 51.76, 10.0
2019-02-14 21:16:48,328 : Text to Image: 12.036, 32.996, 45.884, 13.0
2019-02-14 21:17:29,546 : samples : 512000
2019-02-14 21:17:41,609 : Image to text: 14.52, 38.56, 51.58, 10.0
2019-02-14 21:17:48,860 : Text to Image: 12.216, 32.996, 46.08, 13.0
2019-02-14 21:18:23,444 : Epoch 11 finished
2019-02-14 21:18:23,898 : Image to text: 35.0, 67.8, 82.5, 3.0
2019-02-14 21:18:24,256 : Text to Image: 28.74, 63.54, 79.52, 3.0
2019-02-14 21:18:24,702 : Image to text: 34.2, 69.5, 82.9, 3.0
2019-02-14 21:18:25,060 : Text to Image: 28.62, 62.28, 78.2, 3.0
2019-02-14 21:18:25,505 : Image to text: 36.3, 67.0, 81.8, 3.0
2019-02-14 21:18:25,865 : Text to Image: 28.3, 63.84, 79.36, 3.0
2019-02-14 21:18:26,310 : Image to text: 35.2, 71.0, 83.2, 2.0
2019-02-14 21:18:26,669 : Text to Image: 29.0, 64.08, 78.88, 3.0
2019-02-14 21:18:27,115 : Image to text: 35.7, 70.1, 81.9, 3.0
2019-02-14 21:18:27,473 : Text to Image: 29.16, 62.84, 78.62, 3.0
2019-02-14 21:18:27,473 : Dev mean Text to Image: 28.764, 63.316, 78.916, 3.0
2019-02-14 21:18:27,473 : Dev mean Image to text: 35.28, 69.08, 82.46000000000001, 2.8
2019-02-14 21:18:27,474 : start epoch
2019-02-14 21:19:07,675 : samples : 64000
2019-02-14 21:19:17,579 : Image to text: 14.84, 38.02, 51.82, 10.0
2019-02-14 21:19:27,341 : Text to Image: 12.224, 33.084, 45.884, 13.0
2019-02-14 21:20:07,854 : samples : 128000
2019-02-14 21:20:20,252 : Image to text: 13.78, 36.66, 50.92, 10.0
2019-02-14 21:20:30,018 : Text to Image: 12.024, 32.568, 45.756, 13.0
2019-02-14 21:21:12,500 : samples : 192000
2019-02-14 21:21:24,946 : Image to text: 14.9, 38.42, 52.44, 9.0
2019-02-14 21:21:34,833 : Text to Image: 12.32, 33.28, 46.22, 13.0
2019-02-14 21:22:17,479 : samples : 256000
2019-02-14 21:22:29,923 : Image to text: 14.94, 38.38, 51.34, 10.0
2019-02-14 21:22:39,844 : Text to Image: 12.152, 32.768, 45.78, 13.0
2019-02-14 21:23:22,725 : samples : 320000
2019-02-14 21:23:35,173 : Image to text: 14.5, 37.36, 51.46, 10.0
2019-02-14 21:23:45,022 : Text to Image: 12.232, 32.692, 45.696, 13.0
2019-02-14 21:24:27,528 : samples : 384000
2019-02-14 21:24:40,082 : Image to text: 14.98, 37.98, 52.08, 9.0
2019-02-14 21:24:49,946 : Text to Image: 12.688, 33.464, 46.208, 13.0
2019-02-14 21:25:31,737 : samples : 448000
2019-02-14 21:25:44,227 : Image to text: 15.92, 38.48, 52.72, 9.0
2019-02-14 21:25:54,114 : Text to Image: 12.516, 33.088, 45.96, 13.0
2019-02-14 21:26:36,827 : samples : 512000
2019-02-14 21:26:49,336 : Image to text: 15.16, 38.82, 53.16, 9.0
2019-02-14 21:26:59,238 : Text to Image: 12.308, 33.092, 46.196, 13.0
2019-02-14 21:27:35,022 : Epoch 12 finished
2019-02-14 21:27:35,911 : Image to text: 33.7, 67.5, 81.3, 3.0
2019-02-14 21:27:36,647 : Text to Image: 27.94, 64.64, 80.1, 3.0
2019-02-14 21:27:37,552 : Image to text: 34.2, 69.6, 81.2, 3.0
2019-02-14 21:27:38,310 : Text to Image: 28.52, 63.36, 79.06, 3.0
2019-02-14 21:27:39,223 : Image to text: 34.4, 68.5, 81.9, 3.0
2019-02-14 21:27:39,963 : Text to Image: 29.08, 64.18, 79.34, 3.0
2019-02-14 21:27:40,858 : Image to text: 35.8, 70.0, 81.1, 3.0
2019-02-14 21:27:41,600 : Text to Image: 29.14, 63.5, 79.86, 3.0
2019-02-14 21:27:42,510 : Image to text: 37.2, 69.0, 82.0, 2.0
2019-02-14 21:27:43,248 : Text to Image: 29.64, 63.62, 78.94, 3.0
2019-02-14 21:27:43,248 : Dev mean Text to Image: 28.864, 63.860000000000014, 79.46000000000001, 3.0
2019-02-14 21:27:43,248 : Dev mean Image to text: 35.06, 68.92, 81.5, 2.8
2019-02-14 21:27:43,248 : start epoch
2019-02-14 21:28:23,779 : samples : 64000
2019-02-14 21:28:36,344 : Image to text: 15.4, 38.52, 52.34, 10.0
2019-02-14 21:28:46,236 : Text to Image: 12.108, 32.92, 45.964, 13.0
2019-02-14 21:29:27,171 : samples : 128000
2019-02-14 21:29:39,686 : Image to text: 15.2, 38.36, 52.12, 10.0
2019-02-14 21:29:49,642 : Text to Image: 12.124, 32.58, 45.66, 13.0
2019-02-14 21:30:32,301 : samples : 192000
2019-02-14 21:30:44,846 : Image to text: 15.12, 37.74, 52.06, 10.0
2019-02-14 21:30:54,845 : Text to Image: 12.132, 33.092, 46.016, 13.0
2019-02-14 21:31:36,876 : samples : 256000
2019-02-14 21:31:46,917 : Image to text: 15.38, 39.02, 53.54, 9.0
2019-02-14 21:31:54,185 : Text to Image: 12.432, 33.264, 46.416, 12.0
2019-02-14 21:32:34,221 : samples : 320000
2019-02-14 21:32:44,263 : Image to text: 14.84, 38.0, 51.58, 10.0
2019-02-14 21:32:51,508 : Text to Image: 12.252, 33.496, 46.5, 12.0
2019-02-14 21:33:32,044 : samples : 384000
2019-02-14 21:33:44,822 : Image to text: 14.02, 37.62, 52.34, 9.0
2019-02-14 21:33:55,188 : Text to Image: 12.244, 33.024, 46.064, 13.0
2019-02-14 21:34:38,498 : samples : 448000
2019-02-14 21:34:51,372 : Image to text: 14.96, 39.2, 52.56, 9.0
2019-02-14 21:35:01,661 : Text to Image: 12.176, 32.904, 45.936, 13.0
2019-02-14 21:35:45,334 : samples : 512000
2019-02-14 21:35:58,128 : Image to text: 15.5, 38.66, 52.8, 9.0
2019-02-14 21:36:08,410 : Text to Image: 12.44, 32.96, 45.852, 13.0
2019-02-14 21:36:45,385 : Epoch 13 finished
2019-02-14 21:36:46,449 : Image to text: 35.0, 68.5, 82.8, 3.0
2019-02-14 21:36:47,301 : Text to Image: 29.28, 65.02, 80.48, 3.0
2019-02-14 21:36:48,320 : Image to text: 35.5, 70.1, 82.0, 2.0
2019-02-14 21:36:49,224 : Text to Image: 29.12, 63.24, 79.34, 3.0
2019-02-14 21:36:50,247 : Image to text: 33.4, 67.6, 80.8, 3.0
2019-02-14 21:36:51,169 : Text to Image: 29.24, 65.16, 80.16, 3.0
2019-02-14 21:36:52,229 : Image to text: 36.4, 71.2, 83.3, 3.0
2019-02-14 21:36:53,110 : Text to Image: 29.36, 64.44, 80.18, 3.0
2019-02-14 21:36:54,141 : Image to text: 37.5, 69.4, 82.7, 2.0
2019-02-14 21:36:54,974 : Text to Image: 29.74, 64.38, 78.98, 3.0
2019-02-14 21:36:54,974 : Dev mean Text to Image: 29.348, 64.448, 79.828, 3.0
2019-02-14 21:36:54,974 : Dev mean Image to text: 35.56, 69.36, 82.32, 2.6
2019-02-14 21:36:54,974 : start epoch
2019-02-14 21:37:38,847 : samples : 64000
2019-02-14 21:37:51,681 : Image to text: 14.52, 37.96, 52.38, 9.0
2019-02-14 21:38:02,078 : Text to Image: 12.172, 33.152, 46.288, 12.0
2019-02-14 21:38:45,779 : samples : 128000
2019-02-14 21:38:58,611 : Image to text: 15.22, 38.8, 52.4, 9.0
2019-02-14 21:39:08,965 : Text to Image: 12.248, 32.74, 45.92, 13.0
2019-02-14 21:39:52,504 : samples : 192000
2019-02-14 21:40:05,324 : Image to text: 15.06, 38.84, 53.4, 9.0
2019-02-14 21:40:15,759 : Text to Image: 12.512, 32.784, 46.076, 13.0
2019-02-14 21:40:59,025 : samples : 256000
2019-02-14 21:41:11,754 : Image to text: 15.46, 38.3, 52.42, 9.0
2019-02-14 21:41:22,208 : Text to Image: 12.276, 33.244, 46.468, 12.0
2019-02-14 21:42:03,150 : samples : 320000
2019-02-14 21:42:12,999 : Image to text: 15.26, 38.56, 52.9, 9.0
2019-02-14 21:42:20,107 : Text to Image: 12.3, 32.872, 46.08, 13.0
2019-02-14 21:43:01,352 : samples : 384000
2019-02-14 21:43:11,182 : Image to text: 14.94, 39.22, 52.56, 9.0
2019-02-14 21:43:18,267 : Text to Image: 12.464, 32.98, 46.288, 12.0
2019-02-14 21:43:59,900 : samples : 448000
2019-02-14 21:44:09,710 : Image to text: 14.34, 37.74, 51.7, 10.0
2019-02-14 21:44:16,812 : Text to Image: 11.952, 32.688, 45.996, 13.0
2019-02-14 21:44:57,059 : samples : 512000
2019-02-14 21:45:06,906 : Image to text: 16.0, 38.76, 52.54, 9.0
2019-02-14 21:45:13,959 : Text to Image: 12.524, 33.66, 46.56, 12.0
2019-02-14 21:45:48,967 : Epoch 14 finished
2019-02-14 21:45:49,382 : Image to text: 33.1, 66.8, 80.9, 3.0
2019-02-14 21:45:49,697 : Text to Image: 28.6, 64.42, 79.88, 3.0
2019-02-14 21:45:50,105 : Image to text: 33.8, 68.5, 80.9, 3.0
2019-02-14 21:45:50,424 : Text to Image: 28.58, 62.84, 79.22, 3.0
2019-02-14 21:45:50,837 : Image to text: 35.4, 67.7, 81.2, 3.0
2019-02-14 21:45:51,154 : Text to Image: 29.02, 64.96, 80.04, 3.0
2019-02-14 21:45:51,568 : Image to text: 34.6, 69.7, 82.8, 2.0
2019-02-14 21:45:51,900 : Text to Image: 28.84, 64.5, 80.02, 3.0
2019-02-14 21:45:52,316 : Image to text: 36.7, 69.5, 81.2, 3.0
2019-02-14 21:45:52,635 : Text to Image: 28.76, 63.98, 79.7, 3.0
2019-02-14 21:45:52,635 : Dev mean Text to Image: 28.760000000000005, 64.14, 79.772, 3.0
2019-02-14 21:45:52,635 : Dev mean Image to text: 34.720000000000006, 68.44000000000001, 81.4, 2.8
2019-02-14 21:45:56,336 : 
Test scores | Image to text:             35.66, 69.88, 82.4, 2.6
2019-02-14 21:45:56,336 : Test scores | Text to image:             29.204, 64.21600000000001, 79.608, 3.0

2019-02-14 21:45:56,427 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 21:45:56,781 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 21:45:57,379 : loading BERT model bert-base-uncased
2019-02-14 21:45:57,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:45:57,407 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:45:57,408 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph29b91hp
2019-02-14 21:45:59,744 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:46:01,109 : Computing embeddings for train/dev/test
2019-02-14 21:47:35,822 : Computed embeddings
2019-02-14 21:47:35,822 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:48:04,570 : [('reg:1e-05', 90.69), ('reg:0.0001', 89.73), ('reg:0.001', 85.91), ('reg:0.01', 77.28)]
2019-02-14 21:48:04,570 : Validation : best param found is reg = 1e-05 with score             90.69
2019-02-14 21:48:04,570 : Evaluating...
2019-02-14 21:48:12,520 : 
Dev acc : 90.7 Test acc : 90.7 for LENGTH classification

2019-02-14 21:48:12,521 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 21:48:12,851 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 21:48:12,894 : loading BERT model bert-base-uncased
2019-02-14 21:48:12,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:48:12,923 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:48:12,923 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8tnukjc7
2019-02-14 21:48:15,272 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:48:16,642 : Computing embeddings for train/dev/test
2019-02-14 21:49:44,625 : Computed embeddings
2019-02-14 21:49:44,625 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:50:21,165 : [('reg:1e-05', 69.3), ('reg:0.0001', 34.36), ('reg:0.001', 2.87), ('reg:0.01', 0.86)]
2019-02-14 21:50:21,165 : Validation : best param found is reg = 1e-05 with score             69.3
2019-02-14 21:50:21,166 : Evaluating...
2019-02-14 21:50:31,548 : 
Dev acc : 69.3 Test acc : 69.0 for WORDCONTENT classification

2019-02-14 21:50:31,549 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 21:50:31,920 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 21:50:31,989 : loading BERT model bert-base-uncased
2019-02-14 21:50:31,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:50:32,095 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:50:32,095 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfam1dwzm
2019-02-14 21:50:34,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:50:35,838 : Computing embeddings for train/dev/test
2019-02-14 21:52:00,072 : Computed embeddings
2019-02-14 21:52:00,072 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:52:24,157 : [('reg:1e-05', 38.84), ('reg:0.0001', 38.55), ('reg:0.001', 36.27), ('reg:0.01', 30.93)]
2019-02-14 21:52:24,158 : Validation : best param found is reg = 1e-05 with score             38.84
2019-02-14 21:52:24,158 : Evaluating...
2019-02-14 21:52:30,427 : 
Dev acc : 38.8 Test acc : 38.6 for DEPTH classification

2019-02-14 21:52:30,428 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 21:52:30,818 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 21:52:30,884 : loading BERT model bert-base-uncased
2019-02-14 21:52:30,884 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:52:31,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:52:31,008 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1p0c42xq
2019-02-14 21:52:33,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:52:34,832 : Computing embeddings for train/dev/test
2019-02-14 21:53:52,767 : Computed embeddings
2019-02-14 21:53:52,767 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:54:18,212 : [('reg:1e-05', 75.05), ('reg:0.0001', 74.69), ('reg:0.001', 68.79), ('reg:0.01', 57.87)]
2019-02-14 21:54:18,212 : Validation : best param found is reg = 1e-05 with score             75.05
2019-02-14 21:54:18,212 : Evaluating...
2019-02-14 21:54:24,608 : 
Dev acc : 75.0 Test acc : 75.5 for TOPCONSTITUENTS classification

2019-02-14 21:54:24,609 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 21:54:25,168 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 21:54:25,237 : loading BERT model bert-base-uncased
2019-02-14 21:54:25,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:54:25,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:54:25,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppgnsz_l_
2019-02-14 21:54:27,685 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:54:29,076 : Computing embeddings for train/dev/test
2019-02-14 21:55:53,228 : Computed embeddings
2019-02-14 21:55:53,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:56:24,473 : [('reg:1e-05', 88.8), ('reg:0.0001', 88.79), ('reg:0.001', 88.52), ('reg:0.01', 87.25)]
2019-02-14 21:56:24,474 : Validation : best param found is reg = 1e-05 with score             88.8
2019-02-14 21:56:24,474 : Evaluating...
2019-02-14 21:56:33,211 : 
Dev acc : 88.8 Test acc : 88.5 for BIGRAMSHIFT classification

2019-02-14 21:56:33,212 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 21:56:33,651 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 21:56:33,722 : loading BERT model bert-base-uncased
2019-02-14 21:56:33,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:56:33,755 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:56:33,755 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl6sa0i85
2019-02-14 21:56:36,110 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:56:37,512 : Computing embeddings for train/dev/test
2019-02-14 21:58:00,073 : Computed embeddings
2019-02-14 21:58:00,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:58:29,829 : [('reg:1e-05', 89.11), ('reg:0.0001', 89.17), ('reg:0.001', 89.66), ('reg:0.01', 89.48)]
2019-02-14 21:58:29,829 : Validation : best param found is reg = 0.001 with score             89.66
2019-02-14 21:58:29,829 : Evaluating...
2019-02-14 21:58:37,708 : 
Dev acc : 89.7 Test acc : 88.2 for TENSE classification

2019-02-14 21:58:37,709 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 21:58:38,150 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 21:58:38,218 : loading BERT model bert-base-uncased
2019-02-14 21:58:38,218 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:58:38,345 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:58:38,346 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp53tlehdz
2019-02-14 21:58:40,697 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:58:42,089 : Computing embeddings for train/dev/test
2019-02-14 22:00:09,067 : Computed embeddings
2019-02-14 22:00:09,067 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:00:37,565 : [('reg:1e-05', 85.6), ('reg:0.0001', 85.5), ('reg:0.001', 85.39), ('reg:0.01', 82.4)]
2019-02-14 22:00:37,566 : Validation : best param found is reg = 1e-05 with score             85.6
2019-02-14 22:00:37,566 : Evaluating...
2019-02-14 22:00:46,074 : 
Dev acc : 85.6 Test acc : 84.7 for SUBJNUMBER classification

2019-02-14 22:00:46,075 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 22:00:46,511 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 22:00:46,583 : loading BERT model bert-base-uncased
2019-02-14 22:00:46,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:00:46,715 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:00:46,715 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp794lo5ie
2019-02-14 22:00:49,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:00:50,449 : Computing embeddings for train/dev/test
2019-02-14 22:02:16,186 : Computed embeddings
2019-02-14 22:02:16,186 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:02:53,798 : [('reg:1e-05', 83.44), ('reg:0.0001', 83.35), ('reg:0.001', 83.76), ('reg:0.01', 82.84)]
2019-02-14 22:02:53,798 : Validation : best param found is reg = 0.001 with score             83.76
2019-02-14 22:02:53,798 : Evaluating...
2019-02-14 22:03:03,135 : 
Dev acc : 83.8 Test acc : 84.3 for OBJNUMBER classification

2019-02-14 22:03:03,136 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 22:03:03,684 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 22:03:03,751 : loading BERT model bert-base-uncased
2019-02-14 22:03:03,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:03:03,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:03:03,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplcplsf4d
2019-02-14 22:03:06,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:03:07,455 : Computing embeddings for train/dev/test
2019-02-14 22:04:44,924 : Computed embeddings
2019-02-14 22:04:44,924 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:05:14,946 : [('reg:1e-05', 62.92), ('reg:0.0001', 62.85), ('reg:0.001', 62.53), ('reg:0.01', 61.87)]
2019-02-14 22:05:14,946 : Validation : best param found is reg = 1e-05 with score             62.92
2019-02-14 22:05:14,946 : Evaluating...
2019-02-14 22:05:21,822 : 
Dev acc : 62.9 Test acc : 62.2 for ODDMANOUT classification

2019-02-14 22:05:21,823 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 22:05:22,274 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 22:05:22,357 : loading BERT model bert-base-uncased
2019-02-14 22:05:22,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:05:22,395 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:05:22,395 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb5ozhr2r
2019-02-14 22:05:24,773 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:05:26,168 : Computing embeddings for train/dev/test
2019-02-14 22:07:03,418 : Computed embeddings
2019-02-14 22:07:03,418 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:07:37,578 : [('reg:1e-05', 64.07), ('reg:0.0001', 64.08), ('reg:0.001', 63.62), ('reg:0.01', 62.31)]
2019-02-14 22:07:37,578 : Validation : best param found is reg = 0.0001 with score             64.08
2019-02-14 22:07:37,578 : Evaluating...
2019-02-14 22:07:44,710 : 
Dev acc : 64.1 Test acc : 64.0 for COORDINATIONINVERSION classification

2019-02-14 22:07:44,712 : total results: {'STS12': {'MSRpar': {'pearson': (0.39987696810347373, 3.6258325420391065e-30), 'spearman': SpearmanrResult(correlation=0.42810782471775116, pvalue=8.848200416801382e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4839341278601113, 2.7477406701135628e-45), 'spearman': SpearmanrResult(correlation=0.4969126491646342, pvalue=5.068994125325079e-48), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46469820335602596, 5.747226270672293e-26), 'spearman': SpearmanrResult(correlation=0.5811379211127722, pvalue=8.19316081824216e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6455041418376783, 1.2721327826421232e-89), 'spearman': SpearmanrResult(correlation=0.6474581138614914, pvalue=2.5016307021251743e-90), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6555340899797732, 2.328048731436526e-50), 'spearman': SpearmanrResult(correlation=0.5368096745101075, pvalue=3.691018262363576e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5299095062274125, 'wmean': 0.5218278653775075}, 'spearman': {'mean': 0.5380852366733513, 'wmean': 0.5341982968880962}}}, 'STS13': {'FNWN': {'pearson': (0.3267387874508668, 4.460343551413744e-06), 'spearman': SpearmanrResult(correlation=0.34806457935937474, pvalue=9.215811950757538e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6275274563974401, 2.329016726859941e-83), 'spearman': SpearmanrResult(correlation=0.6051046990267782, pvalue=4.212414215182887e-76), 'nsamples': 750}, 'OnWN': {'pearson': (0.4352071701250386, 2.4931316379733817e-27), 'spearman': SpearmanrResult(correlation=0.46436159142579775, pvalue=2.3810198257234e-31), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.46315780465778184, 'wmean': 0.5177002970442937}, 'spearman': {'mean': 0.47251028993731686, 'wmean': 0.5200797217059187}}}, 'STS14': {'deft-forum': {'pearson': (0.3019240870963417, 6.14330050410409e-11), 'spearman': SpearmanrResult(correlation=0.3306725895101315, pvalue=6.083106612833528e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7436307262953313, 4.8944090294914674e-54), 'spearman': SpearmanrResult(correlation=0.7124588364696102, pvalue=9.24739516663663e-48), 'nsamples': 300}, 'headlines': {'pearson': (0.5851754708229826, 4.042740722587093e-70), 'spearman': SpearmanrResult(correlation=0.5401767427911999, pvalue=4.9126171939649495e-58), 'nsamples': 750}, 'images': {'pearson': (0.55600150784908, 4.476182597624546e-62), 'spearman': SpearmanrResult(correlation=0.5507212057792595, pvalue=1.0547751230837684e-60), 'nsamples': 750}, 'OnWN': {'pearson': (0.6241044000454519, 3.261723689218141e-82), 'spearman': SpearmanrResult(correlation=0.6607922410812295, pvalue=2.729989547748402e-95), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6648915451584066, 7.228432627234121e-97), 'spearman': SpearmanrResult(correlation=0.6197994846551127, pvalue=8.608542801857631e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5792879562112657, 'wmean': 0.5817559333303718}, 'spearman': {'mean': 0.5691035167144238, 'wmean': 0.570975352520145}}}, 'STS15': {'answers-forums': {'pearson': (0.5409696348688737, 6.868880217893726e-30), 'spearman': SpearmanrResult(correlation=0.5320623092598971, pvalue=8.532334952175063e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.6890741589706266, 1.047635680574934e-106), 'spearman': SpearmanrResult(correlation=0.6949460583651783, pvalue=3.043957536872437e-109), 'nsamples': 750}, 'belief': {'pearson': (0.6458556861655027, 1.2112624062624087e-45), 'spearman': SpearmanrResult(correlation=0.6760403512783062, pvalue=2.0896179024618307e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.6419238118671431, 2.4287115390374725e-88), 'spearman': SpearmanrResult(correlation=0.6321219746923213, pvalue=6.395771982682992e-85), 'nsamples': 750}, 'images': {'pearson': (0.6816544393402457, 1.381380107711421e-103), 'spearman': SpearmanrResult(correlation=0.689661417127059, pvalue=5.87806112357306e-107), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6398955462424784, 'wmean': 0.6515162676738009}, 'spearman': {'mean': 0.6449664221445524, 'wmean': 0.6551951951134152}}}, 'STS16': {'answer-answer': {'pearson': (0.5459480092434162, 3.9226408812001924e-21), 'spearman': SpearmanrResult(correlation=0.5657926282459426, pvalue=6.775691537511024e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6578206494131161, 3.006200494660631e-32), 'spearman': SpearmanrResult(correlation=0.6566768120810473, pvalue=4.1757465461607004e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7633579091317967, 3.695959591591333e-45), 'spearman': SpearmanrResult(correlation=0.7733140757091234, pvalue=5.162319277518417e-47), 'nsamples': 230}, 'postediting': {'pearson': (0.7979953435642065, 3.8434136389495176e-55), 'spearman': SpearmanrResult(correlation=0.8327431661758349, pvalue=4.498134703490814e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.2986284508699804, 1.1233149195517652e-05), 'spearman': SpearmanrResult(correlation=0.29228122263339584, pvalue=1.7527250060032648e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6127500724445032, 'wmean': 0.6198690263182961}, 'spearman': {'mean': 0.6241615809690688, 'wmean': 0.6318399656602294}}}, 'MR': {'devacc': 77.71, 'acc': 77.68, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 82.05, 'acc': 81.24, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.84, 'acc': 88.41, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.8, 'acc': 94.63, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.11, 'acc': 81.66, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.78, 'acc': 45.11, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 82.96, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.9, 'acc': 72.93, 'f1': 78.9, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.8, 'acc': 77.71, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.791794694684616, 'pearson': 0.802239181943406, 'spearman': 0.7393939491242097, 'mse': 0.3627628387879465, 'yhat': array([3.51093882, 4.03542727, 1.80998682, ..., 3.15631378, 4.41123722,        4.52352101]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7283543310232476, 'pearson': 0.6755339489390899, 'spearman': 0.6671697691282354, 'mse': 1.4424014281618733, 'yhat': array([1.9585546 , 2.0015251 , 2.13933595, ..., 3.95861833, 3.84973922,        3.61874943]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.79, 'acc': 66.86, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 360.86400000000003, 'acc': [(35.66, 69.88, 82.4, 2.6), (29.204, 64.21600000000001, 79.608, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 90.69, 'acc': 90.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 69.3, 'acc': 68.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.84, 'acc': 38.64, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.05, 'acc': 75.52, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.8, 'acc': 88.45, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.66, 'acc': 88.18, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.6, 'acc': 84.69, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.76, 'acc': 84.31, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.92, 'acc': 62.21, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 64.08, 'acc': 64.05, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 22:07:44,712 : STS12 p=0.5218, STS12 s=0.5342, STS13 p=0.5177, STS13 s=0.5201, STS14 p=0.5818, STS14 s=0.5710, STS15 p=0.6515, STS15 s=0.6552, STS 16 p=0.6199, STS16 s=0.6318, STS B p=0.6755, STS B s=0.6672, STS B m=1.4424, SICK-R p=0.8022, SICK-R s=0.7394, SICK-P m=0.3628
2019-02-14 22:07:44,712 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 22:07:44,712 : 0.5218,0.5342,0.5177,0.5201,0.5818,0.5710,0.6515,0.6552,0.6199,0.6318,0.6755,0.6672,1.4424,0.8022,0.7394,0.3628
2019-02-14 22:07:44,712 : MR=77.68, CR=81.24, SUBJ=94.63, MPQA=88.41, SST-B=81.66, SST-F=45.11, TREC=91.40, SICK-E=77.71, SNLI=66.86, MRPC=72.93, MRPC f=78.90
2019-02-14 22:07:44,712 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 22:07:44,712 : 77.68,81.24,94.63,88.41,81.66,45.11,91.40,77.71,66.86,72.93,78.90
2019-02-14 22:07:44,712 : COCO r1i2t=35.66, COCO r5i2t=69.88, COCO r10i2t=82.40, COCO medr_i2t=2.60, COCO r1t2i=29.20, COCO r5t2i=64.22, COCO r10t2i=79.61, COCO medr_t2i=3.00
2019-02-14 22:07:44,712 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 22:07:44,712 : 35.66,69.88,82.40,2.60,29.20,64.22,79.61,3.00
2019-02-14 22:07:44,712 : SentLen=90.69, WC=68.98, TreeDepth=38.64, TopConst=75.52, BShift=88.45, Tense=88.18, SubjNum=84.69, ObjNum=84.31, SOMO=62.21, CoordInv=64.05, average=74.57
2019-02-14 22:07:44,712 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 22:07:44,712 : 90.69,68.98,38.64,75.52,88.45,88.18,84.69,84.31,62.21,64.05,74.57
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,713 : layer 7
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,713 : ********************************************************************************
2019-02-14 22:07:44,806 : ***** Transfer task : STS12 *****


2019-02-14 22:07:44,818 : loading BERT model bert-base-uncased
2019-02-14 22:07:44,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:07:44,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:07:44,837 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw_wjsuhq
2019-02-14 22:07:47,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:07:50,361 : MSRpar : pearson = 0.3924, spearman = 0.4191
2019-02-14 22:07:51,136 : MSRvid : pearson = 0.4716, spearman = 0.4905
2019-02-14 22:07:51,734 : SMTeuroparl : pearson = 0.4642, spearman = 0.5847
2019-02-14 22:07:52,920 : surprise.OnWN : pearson = 0.6317, spearman = 0.6294
2019-02-14 22:07:53,574 : surprise.SMTnews : pearson = 0.6679, spearman = 0.5461
2019-02-14 22:07:53,574 : ALL (weighted average) : Pearson = 0.5152,             Spearman = 0.5278
2019-02-14 22:07:53,575 : ALL (average) : Pearson = 0.5256,             Spearman = 0.5339

2019-02-14 22:07:53,575 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 22:07:53,585 : loading BERT model bert-base-uncased
2019-02-14 22:07:53,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:07:53,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:07:53,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzqujvni7
2019-02-14 22:07:55,981 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:07:57,901 : FNWN : pearson = 0.3250, spearman = 0.3498
2019-02-14 22:07:58,795 : headlines : pearson = 0.6159, spearman = 0.5903
2019-02-14 22:07:59,460 : OnWN : pearson = 0.4151, spearman = 0.4339
2019-02-14 22:07:59,460 : ALL (weighted average) : Pearson = 0.5042,             Spearman = 0.5015
2019-02-14 22:07:59,460 : ALL (average) : Pearson = 0.4520,             Spearman = 0.4580

2019-02-14 22:07:59,460 : ***** Transfer task : STS14 *****


2019-02-14 22:07:59,476 : loading BERT model bert-base-uncased
2019-02-14 22:07:59,477 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:07:59,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:07:59,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz41ymb8m
2019-02-14 22:08:01,919 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:08:03,960 : deft-forum : pearson = 0.3059, spearman = 0.3317
2019-02-14 22:08:04,641 : deft-news : pearson = 0.7434, spearman = 0.7054
2019-02-14 22:08:05,605 : headlines : pearson = 0.5790, spearman = 0.5362
2019-02-14 22:08:06,565 : images : pearson = 0.5357, spearman = 0.5299
2019-02-14 22:08:07,534 : OnWN : pearson = 0.6167, spearman = 0.6475
2019-02-14 22:08:08,815 : tweet-news : pearson = 0.6710, spearman = 0.6142
2019-02-14 22:08:08,815 : ALL (weighted average) : Pearson = 0.5767,             Spearman = 0.5618
2019-02-14 22:08:08,815 : ALL (average) : Pearson = 0.5753,             Spearman = 0.5608

2019-02-14 22:08:08,816 : ***** Transfer task : STS15 *****


2019-02-14 22:08:08,849 : loading BERT model bert-base-uncased
2019-02-14 22:08:08,849 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:08:08,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:08:08,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpswaaxyrs
2019-02-14 22:08:11,189 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:08:13,434 : answers-forums : pearson = 0.5636, spearman = 0.5603
2019-02-14 22:08:14,340 : answers-students : pearson = 0.6789, spearman = 0.6845
2019-02-14 22:08:15,198 : belief : pearson = 0.6593, spearman = 0.6908
2019-02-14 22:08:16,224 : headlines : pearson = 0.6272, spearman = 0.6190
2019-02-14 22:08:17,201 : images : pearson = 0.6602, spearman = 0.6669
2019-02-14 22:08:17,202 : ALL (weighted average) : Pearson = 0.6444,             Spearman = 0.6490
2019-02-14 22:08:17,202 : ALL (average) : Pearson = 0.6378,             Spearman = 0.6443

2019-02-14 22:08:17,202 : ***** Transfer task : STS16 *****


2019-02-14 22:08:17,242 : loading BERT model bert-base-uncased
2019-02-14 22:08:17,242 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:08:17,260 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:08:17,260 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps8zn8spv
2019-02-14 22:08:19,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:08:21,316 : answer-answer : pearson = 0.5644, spearman = 0.5845
2019-02-14 22:08:21,633 : headlines : pearson = 0.6490, spearman = 0.6468
2019-02-14 22:08:22,031 : plagiarism : pearson = 0.7653, spearman = 0.7772
2019-02-14 22:08:22,656 : postediting : pearson = 0.8062, spearman = 0.8399
2019-02-14 22:08:22,936 : question-question : pearson = 0.3339, spearman = 0.3299
2019-02-14 22:08:22,936 : ALL (weighted average) : Pearson = 0.6302,             Spearman = 0.6426
2019-02-14 22:08:22,936 : ALL (average) : Pearson = 0.6238,             Spearman = 0.6357

2019-02-14 22:08:22,936 : ***** Transfer task : MR *****


2019-02-14 22:08:22,987 : loading BERT model bert-base-uncased
2019-02-14 22:08:22,988 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:08:23,009 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:08:23,009 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5emt6x70
2019-02-14 22:08:25,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:08:26,727 : Generating sentence embeddings
2019-02-14 22:08:40,277 : Generated sentence embeddings
2019-02-14 22:08:40,278 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 22:08:51,083 : Best param found at split 1: l2reg = 1e-05                 with score 78.41
2019-02-14 22:09:02,038 : Best param found at split 2: l2reg = 1e-05                 with score 78.5
2019-02-14 22:09:15,517 : Best param found at split 3: l2reg = 1e-05                 with score 79.1
2019-02-14 22:09:27,318 : Best param found at split 4: l2reg = 0.001                 with score 78.65
2019-02-14 22:09:40,509 : Best param found at split 5: l2reg = 1e-05                 with score 79.16
2019-02-14 22:09:41,185 : Dev acc : 78.76 Test acc : 78.01

2019-02-14 22:09:41,186 : ***** Transfer task : CR *****


2019-02-14 22:09:41,193 : loading BERT model bert-base-uncased
2019-02-14 22:09:41,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:09:41,213 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:09:41,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbv2y7327
2019-02-14 22:09:43,659 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:09:45,036 : Generating sentence embeddings
2019-02-14 22:09:48,904 : Generated sentence embeddings
2019-02-14 22:09:48,904 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 22:09:52,656 : Best param found at split 1: l2reg = 1e-05                 with score 83.44
2019-02-14 22:09:56,762 : Best param found at split 2: l2reg = 1e-05                 with score 83.7
2019-02-14 22:10:01,257 : Best param found at split 3: l2reg = 0.0001                 with score 84.37
2019-02-14 22:10:06,165 : Best param found at split 4: l2reg = 0.01                 with score 84.01
2019-02-14 22:10:10,847 : Best param found at split 5: l2reg = 1e-05                 with score 83.55
2019-02-14 22:10:11,042 : Dev acc : 83.81 Test acc : 82.62

2019-02-14 22:10:11,042 : ***** Transfer task : MPQA *****


2019-02-14 22:10:11,048 : loading BERT model bert-base-uncased
2019-02-14 22:10:11,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:10:11,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:10:11,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplvurfqpx
2019-02-14 22:10:13,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:10:14,873 : Generating sentence embeddings
2019-02-14 22:10:18,609 : Generated sentence embeddings
2019-02-14 22:10:18,610 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 22:10:31,529 : Best param found at split 1: l2reg = 0.001                 with score 88.79
2019-02-14 22:10:43,333 : Best param found at split 2: l2reg = 0.001                 with score 88.57
2019-02-14 22:10:54,591 : Best param found at split 3: l2reg = 0.001                 with score 86.28
2019-02-14 22:11:07,411 : Best param found at split 4: l2reg = 1e-05                 with score 88.87
2019-02-14 22:11:19,625 : Best param found at split 5: l2reg = 0.001                 with score 87.85
2019-02-14 22:11:20,224 : Dev acc : 88.07 Test acc : 88.14

2019-02-14 22:11:20,225 : ***** Transfer task : SUBJ *****


2019-02-14 22:11:20,242 : loading BERT model bert-base-uncased
2019-02-14 22:11:20,242 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:11:20,300 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:11:20,300 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptb9f20u9
2019-02-14 22:11:22,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:11:24,110 : Generating sentence embeddings
2019-02-14 22:11:37,707 : Generated sentence embeddings
2019-02-14 22:11:37,707 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 22:11:47,947 : Best param found at split 1: l2reg = 0.001                 with score 94.85
2019-02-14 22:11:59,077 : Best param found at split 2: l2reg = 0.001                 with score 94.98
2019-02-14 22:12:08,988 : Best param found at split 3: l2reg = 1e-05                 with score 94.4
2019-02-14 22:12:20,348 : Best param found at split 4: l2reg = 0.001                 with score 95.24
2019-02-14 22:12:32,393 : Best param found at split 5: l2reg = 0.001                 with score 95.0
2019-02-14 22:12:32,904 : Dev acc : 94.89 Test acc : 94.27

2019-02-14 22:12:32,905 : ***** Transfer task : SST Binary classification *****


2019-02-14 22:12:33,042 : loading BERT model bert-base-uncased
2019-02-14 22:12:33,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:12:33,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:12:33,063 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0lkvpp_e
2019-02-14 22:12:35,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:12:36,806 : Computing embedding for train
2019-02-14 22:13:21,544 : Computed train embeddings
2019-02-14 22:13:21,544 : Computing embedding for dev
2019-02-14 22:13:22,460 : Computed dev embeddings
2019-02-14 22:13:22,460 : Computing embedding for test
2019-02-14 22:13:24,466 : Computed test embeddings
2019-02-14 22:13:24,466 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:13:45,631 : [('reg:1e-05', 83.26), ('reg:0.0001', 83.37), ('reg:0.001', 83.37), ('reg:0.01', 82.57)]
2019-02-14 22:13:45,631 : Validation : best param found is reg = 0.0001 with score             83.37
2019-02-14 22:13:45,631 : Evaluating...
2019-02-14 22:13:50,951 : 
Dev acc : 83.37 Test acc : 83.42 for             SST Binary classification

2019-02-14 22:13:50,951 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 22:13:51,000 : loading BERT model bert-base-uncased
2019-02-14 22:13:51,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:13:51,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:13:51,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4k8u70nd
2019-02-14 22:13:53,430 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:13:54,846 : Computing embedding for train
2019-02-14 22:14:04,339 : Computed train embeddings
2019-02-14 22:14:04,340 : Computing embedding for dev
2019-02-14 22:14:05,572 : Computed dev embeddings
2019-02-14 22:14:05,572 : Computing embedding for test
2019-02-14 22:14:08,015 : Computed test embeddings
2019-02-14 22:14:08,015 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:14:10,650 : [('reg:1e-05', 42.05), ('reg:0.0001', 42.23), ('reg:0.001', 42.78), ('reg:0.01', 43.23)]
2019-02-14 22:14:10,650 : Validation : best param found is reg = 0.01 with score             43.23
2019-02-14 22:14:10,650 : Evaluating...
2019-02-14 22:14:11,409 : 
Dev acc : 43.23 Test acc : 42.67 for             SST Fine-Grained classification

2019-02-14 22:14:11,409 : ***** Transfer task : TREC *****


2019-02-14 22:14:11,423 : loading BERT model bert-base-uncased
2019-02-14 22:14:11,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:14:11,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:14:11,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppg5hk8_l
2019-02-14 22:14:13,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:14:18,693 : Computed train embeddings
2019-02-14 22:14:18,965 : Computed test embeddings
2019-02-14 22:14:18,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 22:14:26,440 : [('reg:1e-05', 84.54), ('reg:0.0001', 84.37), ('reg:0.001', 83.77), ('reg:0.01', 77.92)]
2019-02-14 22:14:26,440 : Cross-validation : best param found is reg = 1e-05             with score 84.54
2019-02-14 22:14:26,441 : Evaluating...
2019-02-14 22:14:26,907 : 
Dev acc : 84.54 Test acc : 93.0             for TREC

2019-02-14 22:14:26,908 : ***** Transfer task : MRPC *****


2019-02-14 22:14:26,969 : loading BERT model bert-base-uncased
2019-02-14 22:14:26,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:14:26,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:14:26,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpef0edz6c
2019-02-14 22:14:29,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:14:30,811 : Computing embedding for train
2019-02-14 22:14:41,118 : Computed train embeddings
2019-02-14 22:14:41,118 : Computing embedding for test
2019-02-14 22:14:45,672 : Computed test embeddings
2019-02-14 22:14:45,689 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 22:14:51,064 : [('reg:1e-05', 74.63), ('reg:0.0001', 74.56), ('reg:0.001', 74.24), ('reg:0.01', 72.65)]
2019-02-14 22:14:51,064 : Cross-validation : best param found is reg = 1e-05             with score 74.63
2019-02-14 22:14:51,064 : Evaluating...
2019-02-14 22:14:51,348 : Dev acc : 74.63 Test acc 70.26; Test F1 76.35 for MRPC.

2019-02-14 22:14:51,348 : ***** Transfer task : SICK-Entailment*****


2019-02-14 22:14:51,372 : loading BERT model bert-base-uncased
2019-02-14 22:14:51,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:14:51,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:14:51,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp030r3tly
2019-02-14 22:14:53,738 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:14:55,179 : Computing embedding for train
2019-02-14 22:15:00,294 : Computed train embeddings
2019-02-14 22:15:00,294 : Computing embedding for dev
2019-02-14 22:15:00,981 : Computed dev embeddings
2019-02-14 22:15:00,981 : Computing embedding for test
2019-02-14 22:15:06,479 : Computed test embeddings
2019-02-14 22:15:06,509 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:15:08,101 : [('reg:1e-05', 78.4), ('reg:0.0001', 78.0), ('reg:0.001', 77.8), ('reg:0.01', 78.0)]
2019-02-14 22:15:08,102 : Validation : best param found is reg = 1e-05 with score             78.4
2019-02-14 22:15:08,102 : Evaluating...
2019-02-14 22:15:08,508 : 
Dev acc : 78.4 Test acc : 78.16 for                        SICK entailment

2019-02-14 22:15:08,508 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 22:15:08,537 : loading BERT model bert-base-uncased
2019-02-14 22:15:08,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:15:08,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:15:08,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkgwlkypd
2019-02-14 22:15:10,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:15:12,374 : Computing embedding for train
2019-02-14 22:15:17,445 : Computed train embeddings
2019-02-14 22:15:17,445 : Computing embedding for dev
2019-02-14 22:15:18,149 : Computed dev embeddings
2019-02-14 22:15:18,149 : Computing embedding for test
2019-02-14 22:15:23,660 : Computed test embeddings
2019-02-14 22:15:40,041 : Dev : Pearson 0.7878668978345402
2019-02-14 22:15:40,041 : Test : Pearson 0.8056425074402782 Spearman 0.7394980348519594 MSE 0.35792682498059625                        for SICK Relatedness

2019-02-14 22:15:40,042 : 

***** Transfer task : STSBenchmark*****


2019-02-14 22:15:40,123 : loading BERT model bert-base-uncased
2019-02-14 22:15:40,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:15:40,141 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:15:40,141 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa6kwqj1d
2019-02-14 22:15:42,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:15:43,812 : Computing embedding for train
2019-02-14 22:15:51,885 : Computed train embeddings
2019-02-14 22:15:51,885 : Computing embedding for dev
2019-02-14 22:15:54,312 : Computed dev embeddings
2019-02-14 22:15:54,312 : Computing embedding for test
2019-02-14 22:15:56,259 : Computed test embeddings
2019-02-14 22:16:17,236 : Dev : Pearson 0.7271555770577297
2019-02-14 22:16:17,236 : Test : Pearson 0.668318077803957 Spearman 0.6647568027610277 MSE 1.4627233652334592                        for SICK Relatedness

2019-02-14 22:16:17,237 : ***** Transfer task : SNLI Entailment*****


2019-02-14 22:16:22,165 : loading BERT model bert-base-uncased
2019-02-14 22:16:22,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:16:22,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:16:22,293 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps5q2h3zg
2019-02-14 22:16:24,691 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:16:26,292 : PROGRESS (encoding): 0.00%
2019-02-14 22:17:44,783 : PROGRESS (encoding): 14.56%
2019-02-14 22:19:11,695 : PROGRESS (encoding): 29.12%
2019-02-14 22:20:39,377 : PROGRESS (encoding): 43.69%
2019-02-14 22:22:14,045 : PROGRESS (encoding): 58.25%
2019-02-14 22:23:59,395 : PROGRESS (encoding): 72.81%
2019-02-14 22:25:42,672 : PROGRESS (encoding): 87.37%
2019-02-14 22:27:32,446 : PROGRESS (encoding): 0.00%
2019-02-14 22:27:46,308 : PROGRESS (encoding): 0.00%
2019-02-14 22:28:00,089 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 22:28:31,751 : [('reg:1e-09', 64.34)]
2019-02-14 22:28:31,751 : Validation : best param found is reg = 1e-09 with score             64.34
2019-02-14 22:28:31,751 : Evaluating...
2019-02-14 22:29:06,345 : Dev acc : 64.34 Test acc : 65.51 for SNLI

2019-02-14 22:29:06,345 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 22:29:15,176 : loading BERT model bert-base-uncased
2019-02-14 22:29:15,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 22:29:15,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 22:29:15,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7khjxy2d
2019-02-14 22:29:17,597 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 22:29:18,991 : Computing embedding for train
2019-02-14 22:36:51,389 : Computed train embeddings
2019-02-14 22:36:51,389 : Computing embedding for dev
2019-02-14 22:37:10,916 : Computed dev embeddings
2019-02-14 22:37:10,916 : Computing embedding for test
2019-02-14 22:37:31,999 : Computed test embeddings
2019-02-14 22:37:32,014 : prepare data
2019-02-14 22:37:32,076 : start epoch
2019-02-14 22:38:11,135 : samples : 64000
2019-02-14 22:38:21,023 : Image to text: 8.28, 24.48, 35.14, 22.0
2019-02-14 22:38:28,115 : Text to Image: 6.572, 20.608, 31.212, 25.0
2019-02-14 22:39:07,720 : samples : 128000
2019-02-14 22:39:17,546 : Image to text: 9.12, 25.12, 36.92, 19.0
2019-02-14 22:39:24,604 : Text to Image: 6.916, 22.08, 33.268, 23.0
2019-02-14 22:40:03,622 : samples : 192000
2019-02-14 22:40:13,417 : Image to text: 10.04, 27.32, 38.94, 17.0
2019-02-14 22:40:20,497 : Text to Image: 7.928, 23.38, 34.96, 21.0
2019-02-14 22:40:59,465 : samples : 256000
2019-02-14 22:41:09,279 : Image to text: 9.28, 26.6, 38.14, 18.0
2019-02-14 22:41:16,371 : Text to Image: 7.94, 23.896, 35.472, 21.0
2019-02-14 22:41:55,312 : samples : 320000
2019-02-14 22:42:05,161 : Image to text: 10.02, 26.74, 38.14, 18.0
2019-02-14 22:42:12,209 : Text to Image: 8.22, 24.516, 36.116, 20.0
2019-02-14 22:42:51,135 : samples : 384000
2019-02-14 22:43:00,944 : Image to text: 10.18, 28.78, 41.24, 16.0
2019-02-14 22:43:07,997 : Text to Image: 8.632, 25.672, 37.364, 19.0
2019-02-14 22:43:47,078 : samples : 448000
2019-02-14 22:43:57,207 : Image to text: 10.22, 28.28, 40.7, 16.0
2019-02-14 22:44:04,471 : Text to Image: 8.4, 24.976, 37.288, 19.0
2019-02-14 22:44:44,631 : samples : 512000
2019-02-14 22:44:54,775 : Image to text: 11.58, 29.88, 42.62, 15.0
2019-02-14 22:45:02,047 : Text to Image: 9.296, 26.436, 38.428, 18.0
2019-02-14 22:45:36,666 : Epoch 1 finished
2019-02-14 22:45:37,087 : Image to text: 27.4, 60.3, 75.5, 4.0
2019-02-14 22:45:37,395 : Text to Image: 23.58, 56.68, 73.68, 4.0
2019-02-14 22:45:37,805 : Image to text: 28.0, 61.0, 73.5, 4.0
2019-02-14 22:45:38,114 : Text to Image: 22.42, 54.56, 71.94, 5.0
2019-02-14 22:45:38,532 : Image to text: 28.9, 62.2, 76.7, 3.0
2019-02-14 22:45:38,840 : Text to Image: 22.84, 56.26, 73.62, 4.0
2019-02-14 22:45:39,250 : Image to text: 29.2, 61.5, 76.0, 4.0
2019-02-14 22:45:39,555 : Text to Image: 24.06, 56.58, 73.16, 4.0
2019-02-14 22:45:39,963 : Image to text: 26.8, 60.4, 76.3, 4.0
2019-02-14 22:45:40,265 : Text to Image: 23.1, 57.18, 72.82, 4.0
2019-02-14 22:45:40,265 : Dev mean Text to Image: 23.2, 56.252, 73.044, 4.2
2019-02-14 22:45:40,265 : Dev mean Image to text: 28.06, 61.08, 75.60000000000001, 3.8
2019-02-14 22:45:40,266 : start epoch
2019-02-14 22:46:21,038 : samples : 64000
2019-02-14 22:46:30,726 : Image to text: 11.06, 30.2, 42.26, 15.0
2019-02-14 22:46:37,646 : Text to Image: 9.036, 26.556, 38.72, 18.0
2019-02-14 22:47:20,562 : samples : 128000
2019-02-14 22:47:30,443 : Image to text: 10.1, 29.52, 42.02, 15.0
2019-02-14 22:47:37,590 : Text to Image: 8.812, 25.912, 38.32, 18.0
2019-02-14 22:48:20,763 : samples : 192000
2019-02-14 22:48:30,648 : Image to text: 11.1, 30.64, 43.26, 14.0
2019-02-14 22:48:37,792 : Text to Image: 9.644, 27.472, 39.624, 17.0
2019-02-14 22:49:19,525 : samples : 256000
2019-02-14 22:49:29,478 : Image to text: 12.04, 31.62, 44.84, 14.0
2019-02-14 22:49:36,735 : Text to Image: 9.78, 27.86, 40.28, 17.0
2019-02-14 22:50:19,347 : samples : 320000
2019-02-14 22:50:29,255 : Image to text: 12.28, 32.06, 44.6, 13.0
2019-02-14 22:50:36,492 : Text to Image: 9.884, 28.464, 40.584, 16.0
2019-02-14 22:51:18,543 : samples : 384000
2019-02-14 22:51:28,260 : Image to text: 12.14, 31.66, 45.3, 13.0
2019-02-14 22:51:35,305 : Text to Image: 9.676, 27.832, 40.144, 17.0
2019-02-14 22:52:16,852 : samples : 448000
2019-02-14 22:52:27,012 : Image to text: 11.94, 32.66, 45.92, 13.0
2019-02-14 22:52:34,287 : Text to Image: 10.196, 28.476, 40.552, 16.0
2019-02-14 22:53:14,940 : samples : 512000
2019-02-14 22:53:25,120 : Image to text: 11.76, 31.7, 44.98, 13.0
2019-02-14 22:53:32,360 : Text to Image: 10.196, 28.36, 40.94, 16.0
2019-02-14 22:54:07,160 : Epoch 2 finished
2019-02-14 22:54:07,584 : Image to text: 30.6, 64.4, 77.9, 3.0
2019-02-14 22:54:07,892 : Text to Image: 25.68, 59.58, 75.64, 4.0
2019-02-14 22:54:08,307 : Image to text: 29.1, 62.7, 77.6, 3.0
2019-02-14 22:54:08,615 : Text to Image: 24.58, 56.72, 74.7, 4.0
2019-02-14 22:54:09,022 : Image to text: 30.5, 64.0, 77.8, 3.0
2019-02-14 22:54:09,348 : Text to Image: 25.52, 58.42, 75.36, 4.0
2019-02-14 22:54:09,789 : Image to text: 31.3, 64.1, 78.1, 3.0
2019-02-14 22:54:10,112 : Text to Image: 25.32, 58.3, 74.58, 4.0
2019-02-14 22:54:10,533 : Image to text: 31.2, 62.9, 76.8, 3.0
2019-02-14 22:54:10,865 : Text to Image: 25.02, 58.98, 75.16, 4.0
2019-02-14 22:54:10,865 : Dev mean Text to Image: 25.223999999999997, 58.4, 75.088, 4.0
2019-02-14 22:54:10,865 : Dev mean Image to text: 30.54, 63.62, 77.63999999999999, 3.0
2019-02-14 22:54:10,866 : start epoch
2019-02-14 22:54:51,619 : samples : 64000
2019-02-14 22:55:01,385 : Image to text: 12.66, 32.48, 45.92, 13.0
2019-02-14 22:55:08,515 : Text to Image: 10.192, 28.844, 41.072, 16.0
2019-02-14 22:55:50,106 : samples : 128000
2019-02-14 22:55:59,837 : Image to text: 11.96, 32.56, 45.66, 13.0
2019-02-14 22:56:06,954 : Text to Image: 10.36, 28.5, 40.888, 16.0
2019-02-14 22:56:48,570 : samples : 192000
2019-02-14 22:56:58,296 : Image to text: 13.02, 33.28, 46.98, 12.0
2019-02-14 22:57:05,374 : Text to Image: 10.456, 29.332, 41.884, 15.0
2019-02-14 22:57:44,631 : samples : 256000
2019-02-14 22:57:54,426 : Image to text: 13.5, 33.72, 47.08, 12.0
2019-02-14 22:58:01,744 : Text to Image: 10.544, 29.76, 42.18, 15.0
2019-02-14 22:58:43,434 : samples : 320000
2019-02-14 22:58:53,321 : Image to text: 12.42, 32.18, 45.36, 13.0
2019-02-14 22:59:00,565 : Text to Image: 9.696, 27.88, 40.344, 16.0
2019-02-14 22:59:42,817 : samples : 384000
2019-02-14 22:59:52,756 : Image to text: 12.64, 33.88, 46.3, 12.0
2019-02-14 22:59:59,954 : Text to Image: 10.276, 29.036, 41.768, 15.0
2019-02-14 23:00:42,803 : samples : 448000
2019-02-14 23:00:53,054 : Image to text: 13.16, 35.46, 47.66, 12.0
2019-02-14 23:01:00,401 : Text to Image: 10.78, 30.256, 42.652, 15.0
2019-02-14 23:01:41,657 : samples : 512000
2019-02-14 23:01:51,753 : Image to text: 13.4, 34.78, 47.54, 12.0
2019-02-14 23:01:58,974 : Text to Image: 10.704, 30.012, 42.5, 15.0
2019-02-14 23:02:33,704 : Epoch 3 finished
2019-02-14 23:02:34,126 : Image to text: 31.4, 66.2, 80.4, 3.0
2019-02-14 23:02:34,434 : Text to Image: 26.0, 61.2, 77.56, 4.0
2019-02-14 23:02:34,858 : Image to text: 32.3, 64.6, 77.9, 3.0
2019-02-14 23:02:35,184 : Text to Image: 25.64, 59.78, 75.86, 4.0
2019-02-14 23:02:35,633 : Image to text: 33.8, 64.8, 79.1, 3.0
2019-02-14 23:02:35,955 : Text to Image: 26.12, 60.92, 76.88, 4.0
2019-02-14 23:02:36,396 : Image to text: 32.9, 67.9, 80.1, 3.0
2019-02-14 23:02:36,715 : Text to Image: 27.1, 61.28, 76.84, 4.0
2019-02-14 23:02:37,132 : Image to text: 33.7, 67.2, 81.3, 3.0
2019-02-14 23:02:37,463 : Text to Image: 26.68, 61.24, 76.84, 4.0
2019-02-14 23:02:37,464 : Dev mean Text to Image: 26.308, 60.884, 76.796, 4.0
2019-02-14 23:02:37,464 : Dev mean Image to text: 32.82, 66.14, 79.75999999999999, 3.0
2019-02-14 23:02:37,464 : start epoch
2019-02-14 23:03:18,646 : samples : 64000
2019-02-14 23:03:28,373 : Image to text: 13.62, 34.6, 48.3, 11.0
2019-02-14 23:03:35,539 : Text to Image: 10.36, 29.52, 42.368, 15.0
2019-02-14 23:04:17,617 : samples : 128000
2019-02-14 23:04:27,365 : Image to text: 14.06, 34.42, 47.14, 12.0
2019-02-14 23:04:34,473 : Text to Image: 10.62, 29.92, 42.568, 15.0
2019-02-14 23:05:17,101 : samples : 192000
2019-02-14 23:05:27,054 : Image to text: 13.32, 34.28, 48.08, 11.0
2019-02-14 23:05:34,319 : Text to Image: 10.584, 29.34, 41.824, 15.0
2019-02-14 23:06:16,511 : samples : 256000
2019-02-14 23:06:26,430 : Image to text: 13.28, 34.12, 47.1, 12.0
2019-02-14 23:06:33,744 : Text to Image: 10.972, 30.244, 42.868, 15.0
2019-02-14 23:07:15,484 : samples : 320000
2019-02-14 23:07:25,395 : Image to text: 14.06, 35.3, 48.6, 11.0
2019-02-14 23:07:32,628 : Text to Image: 11.076, 30.008, 42.54, 15.0
2019-02-14 23:08:14,020 : samples : 384000
2019-02-14 23:08:23,933 : Image to text: 13.64, 35.7, 48.62, 11.0
2019-02-14 23:08:31,086 : Text to Image: 11.012, 30.016, 42.484, 15.0
2019-02-14 23:09:12,711 : samples : 448000
2019-02-14 23:09:22,835 : Image to text: 14.22, 35.56, 49.06, 11.0
2019-02-14 23:09:30,085 : Text to Image: 11.28, 30.336, 43.088, 15.0
2019-02-14 23:10:09,488 : samples : 512000
2019-02-14 23:10:19,632 : Image to text: 13.88, 34.8, 48.1, 11.0
2019-02-14 23:10:26,874 : Text to Image: 10.936, 30.484, 43.032, 14.0
2019-02-14 23:11:00,307 : Epoch 4 finished
2019-02-14 23:11:00,736 : Image to text: 32.8, 66.7, 80.9, 3.0
2019-02-14 23:11:01,051 : Text to Image: 26.42, 62.08, 77.76, 4.0
2019-02-14 23:11:01,480 : Image to text: 31.5, 63.3, 78.7, 3.0
2019-02-14 23:11:01,826 : Text to Image: 26.4, 59.72, 76.4, 4.0
2019-02-14 23:11:02,257 : Image to text: 31.0, 65.7, 80.5, 3.0
2019-02-14 23:11:02,583 : Text to Image: 26.2, 60.96, 77.3, 4.0
2019-02-14 23:11:03,021 : Image to text: 34.0, 67.5, 80.3, 3.0
2019-02-14 23:11:03,346 : Text to Image: 26.68, 61.22, 77.3, 4.0
2019-02-14 23:11:03,766 : Image to text: 32.9, 65.3, 79.8, 3.0
2019-02-14 23:11:04,084 : Text to Image: 26.44, 61.3, 76.8, 4.0
2019-02-14 23:11:04,085 : Dev mean Text to Image: 26.428, 61.056, 77.112, 4.0
2019-02-14 23:11:04,085 : Dev mean Image to text: 32.44, 65.7, 80.03999999999999, 3.0
2019-02-14 23:11:04,085 : start epoch
2019-02-14 23:11:43,095 : samples : 64000
2019-02-14 23:11:52,846 : Image to text: 14.48, 35.58, 48.46, 11.0
2019-02-14 23:11:59,921 : Text to Image: 11.192, 30.9, 43.624, 14.0
2019-02-14 23:12:39,631 : samples : 128000
2019-02-14 23:12:49,417 : Image to text: 13.98, 34.8, 48.46, 11.0
2019-02-14 23:12:56,539 : Text to Image: 11.28, 30.528, 43.552, 14.0
2019-02-14 23:13:35,371 : samples : 192000
2019-02-14 23:13:45,118 : Image to text: 12.96, 33.9, 47.6, 12.0
2019-02-14 23:13:52,313 : Text to Image: 10.964, 30.224, 43.392, 14.0
2019-02-14 23:14:31,450 : samples : 256000
2019-02-14 23:14:41,175 : Image to text: 14.14, 34.48, 48.04, 12.0
2019-02-14 23:14:48,279 : Text to Image: 11.168, 30.384, 43.084, 14.0
2019-02-14 23:15:33,955 : samples : 320000
2019-02-14 23:15:43,712 : Image to text: 13.44, 35.2, 48.68, 11.0
2019-02-14 23:15:50,854 : Text to Image: 11.384, 30.896, 43.828, 14.0
2019-02-14 23:16:29,726 : samples : 384000
2019-02-14 23:16:39,485 : Image to text: 14.8, 34.98, 48.64, 11.0
2019-02-14 23:16:46,599 : Text to Image: 11.32, 30.932, 43.96, 14.0
2019-02-14 23:17:25,548 : samples : 448000
2019-02-14 23:17:35,688 : Image to text: 13.14, 34.76, 48.66, 11.0
2019-02-14 23:17:42,928 : Text to Image: 10.772, 30.248, 42.872, 15.0
2019-02-14 23:18:22,021 : samples : 512000
2019-02-14 23:18:32,198 : Image to text: 14.34, 36.74, 49.14, 11.0
2019-02-14 23:18:39,434 : Text to Image: 11.456, 31.104, 43.908, 14.0
2019-02-14 23:19:13,096 : Epoch 5 finished
2019-02-14 23:19:13,517 : Image to text: 33.2, 66.7, 80.5, 3.0
2019-02-14 23:19:13,823 : Text to Image: 26.38, 61.6, 78.4, 4.0
2019-02-14 23:19:14,237 : Image to text: 32.3, 67.2, 79.7, 3.0
2019-02-14 23:19:14,539 : Text to Image: 26.64, 59.84, 76.08, 4.0
2019-02-14 23:19:14,963 : Image to text: 32.8, 66.4, 80.5, 3.0
2019-02-14 23:19:15,295 : Text to Image: 26.94, 61.0, 77.68, 4.0
2019-02-14 23:19:15,732 : Image to text: 33.6, 68.5, 80.7, 3.0
2019-02-14 23:19:16,052 : Text to Image: 27.02, 61.28, 76.98, 4.0
2019-02-14 23:19:16,468 : Image to text: 34.8, 66.1, 80.0, 3.0
2019-02-14 23:19:16,786 : Text to Image: 26.62, 61.64, 76.86, 4.0
2019-02-14 23:19:16,786 : Dev mean Text to Image: 26.72, 61.072, 77.2, 4.0
2019-02-14 23:19:16,786 : Dev mean Image to text: 33.34, 66.98, 80.28, 3.0
2019-02-14 23:19:16,786 : start epoch
2019-02-14 23:20:03,029 : samples : 64000
2019-02-14 23:20:12,779 : Image to text: 13.6, 36.2, 49.22, 11.0
2019-02-14 23:20:19,846 : Text to Image: 11.356, 31.452, 44.036, 14.0
2019-02-14 23:20:58,527 : samples : 128000
2019-02-14 23:21:08,325 : Image to text: 14.34, 35.52, 49.1, 11.0
2019-02-14 23:21:15,412 : Text to Image: 11.272, 30.928, 43.648, 14.0
2019-02-14 23:21:54,223 : samples : 192000
2019-02-14 23:22:03,982 : Image to text: 14.5, 36.52, 50.18, 10.0
2019-02-14 23:22:11,053 : Text to Image: 11.436, 31.148, 43.792, 14.0
2019-02-14 23:22:51,492 : samples : 256000
2019-02-14 23:23:01,269 : Image to text: 14.42, 36.4, 50.44, 10.0
2019-02-14 23:23:08,341 : Text to Image: 11.548, 31.436, 44.108, 14.0
2019-02-14 23:23:50,734 : samples : 320000
2019-02-14 23:24:00,497 : Image to text: 13.66, 36.18, 49.28, 11.0
2019-02-14 23:24:07,623 : Text to Image: 11.364, 30.996, 43.452, 14.0
2019-02-14 23:24:49,799 : samples : 384000
2019-02-14 23:24:59,536 : Image to text: 14.3, 36.24, 50.4, 10.0
2019-02-14 23:25:06,639 : Text to Image: 11.388, 30.936, 43.852, 14.0
2019-02-14 23:25:48,096 : samples : 448000
2019-02-14 23:25:58,120 : Image to text: 14.22, 36.74, 50.26, 10.0
2019-02-14 23:26:05,368 : Text to Image: 11.484, 31.308, 44.076, 14.0
2019-02-14 23:26:46,080 : samples : 512000
2019-02-14 23:26:56,115 : Image to text: 14.44, 35.72, 49.04, 11.0
2019-02-14 23:27:03,390 : Text to Image: 10.852, 31.056, 43.86, 14.0
2019-02-14 23:27:38,221 : Epoch 6 finished
2019-02-14 23:27:38,641 : Image to text: 34.0, 68.3, 80.3, 3.0
2019-02-14 23:27:38,945 : Text to Image: 27.6, 62.8, 78.6, 3.0
2019-02-14 23:27:39,361 : Image to text: 32.7, 67.8, 80.5, 3.0
2019-02-14 23:27:39,662 : Text to Image: 26.96, 61.42, 77.14, 4.0
2019-02-14 23:27:40,072 : Image to text: 35.4, 69.2, 80.9, 3.0
2019-02-14 23:27:40,373 : Text to Image: 27.2, 62.0, 77.98, 3.0
2019-02-14 23:27:40,786 : Image to text: 34.6, 69.3, 82.1, 2.0
2019-02-14 23:27:41,086 : Text to Image: 27.8, 62.48, 77.5, 3.0
2019-02-14 23:27:41,500 : Image to text: 34.4, 67.9, 81.5, 3.0
2019-02-14 23:27:41,804 : Text to Image: 28.5, 62.04, 77.5, 3.0
2019-02-14 23:27:41,804 : Dev mean Text to Image: 27.612, 62.147999999999996, 77.744, 3.2
2019-02-14 23:27:41,804 : Dev mean Image to text: 34.220000000000006, 68.5, 81.05999999999999, 2.8
2019-02-14 23:27:41,804 : start epoch
2019-02-14 23:28:24,076 : samples : 64000
2019-02-14 23:28:33,863 : Image to text: 14.18, 36.3, 50.34, 10.0
2019-02-14 23:28:40,858 : Text to Image: 11.584, 31.516, 44.296, 14.0
2019-02-14 23:29:24,377 : samples : 128000
2019-02-14 23:29:34,130 : Image to text: 14.62, 36.8, 50.72, 10.0
2019-02-14 23:29:41,139 : Text to Image: 11.568, 31.504, 44.612, 13.0
2019-02-14 23:30:23,367 : samples : 192000
2019-02-14 23:30:33,120 : Image to text: 14.86, 36.88, 50.5, 10.0
2019-02-14 23:30:40,118 : Text to Image: 11.888, 31.932, 44.68, 13.0
2019-02-14 23:31:22,279 : samples : 256000
2019-02-14 23:31:32,043 : Image to text: 14.44, 36.18, 49.14, 11.0
2019-02-14 23:31:39,041 : Text to Image: 11.404, 31.468, 44.568, 13.0
2019-02-14 23:32:18,757 : samples : 320000
2019-02-14 23:32:28,488 : Image to text: 14.2, 36.16, 49.56, 11.0
2019-02-14 23:32:35,514 : Text to Image: 11.956, 31.888, 44.696, 13.0
2019-02-14 23:33:14,348 : samples : 384000
2019-02-14 23:33:24,155 : Image to text: 13.72, 36.24, 50.4, 10.0
2019-02-14 23:33:31,181 : Text to Image: 11.32, 31.564, 44.648, 13.0
2019-02-14 23:34:10,179 : samples : 448000
2019-02-14 23:34:20,540 : Image to text: 14.18, 37.24, 50.76, 10.0
2019-02-14 23:34:30,486 : Text to Image: 11.676, 32.344, 44.904, 13.0
2019-02-14 23:35:13,133 : samples : 512000
2019-02-14 23:35:25,640 : Image to text: 15.28, 38.1, 51.48, 10.0
2019-02-14 23:35:35,606 : Text to Image: 11.9, 32.24, 45.296, 13.0
2019-02-14 23:36:10,661 : Epoch 7 finished
2019-02-14 23:36:11,122 : Image to text: 33.3, 67.7, 81.6, 3.0
2019-02-14 23:36:11,483 : Text to Image: 28.06, 63.5, 79.04, 3.0
2019-02-14 23:36:11,931 : Image to text: 32.6, 65.1, 78.9, 3.0
2019-02-14 23:36:12,279 : Text to Image: 27.06, 62.0, 77.72, 4.0
2019-02-14 23:36:12,727 : Image to text: 32.9, 67.7, 81.9, 3.0
2019-02-14 23:36:13,089 : Text to Image: 27.9, 63.5, 79.0, 3.0
2019-02-14 23:36:13,537 : Image to text: 35.2, 68.7, 82.2, 3.0
2019-02-14 23:36:13,897 : Text to Image: 28.66, 63.56, 79.22, 3.0
2019-02-14 23:36:14,345 : Image to text: 33.6, 65.9, 80.0, 3.0
2019-02-14 23:36:14,706 : Text to Image: 28.12, 62.42, 78.1, 3.0
2019-02-14 23:36:14,706 : Dev mean Text to Image: 27.96, 62.996, 78.616, 3.2
2019-02-14 23:36:14,707 : Dev mean Image to text: 33.519999999999996, 67.02000000000001, 80.92, 3.0
2019-02-14 23:36:14,707 : start epoch
2019-02-14 23:36:55,530 : samples : 64000
2019-02-14 23:37:07,964 : Image to text: 14.28, 36.66, 49.6, 11.0
2019-02-14 23:37:17,888 : Text to Image: 11.628, 31.776, 44.468, 13.0
2019-02-14 23:38:01,380 : samples : 128000
2019-02-14 23:38:12,421 : Image to text: 14.88, 37.32, 49.36, 11.0
2019-02-14 23:38:19,521 : Text to Image: 11.424, 31.708, 44.56, 13.0
2019-02-14 23:38:59,806 : samples : 192000
2019-02-14 23:39:12,331 : Image to text: 14.38, 36.92, 50.32, 10.0
2019-02-14 23:39:22,284 : Text to Image: 11.76, 32.184, 44.844, 13.0
2019-02-14 23:40:05,836 : samples : 256000
2019-02-14 23:40:18,092 : Image to text: 14.38, 36.12, 50.06, 10.0
2019-02-14 23:40:25,233 : Text to Image: 11.696, 31.828, 44.68, 13.0
2019-02-14 23:41:06,523 : samples : 320000
2019-02-14 23:41:18,958 : Image to text: 14.78, 37.7, 51.08, 10.0
2019-02-14 23:41:28,884 : Text to Image: 12.048, 32.588, 45.228, 13.0
2019-02-14 23:42:12,223 : samples : 384000
2019-02-14 23:42:22,362 : Image to text: 13.88, 36.34, 50.14, 10.0
2019-02-14 23:42:29,506 : Text to Image: 11.976, 32.044, 45.128, 13.0
2019-02-14 23:43:10,221 : samples : 448000
2019-02-14 23:43:22,644 : Image to text: 14.08, 36.52, 51.36, 10.0
2019-02-14 23:43:32,559 : Text to Image: 11.624, 32.024, 44.94, 13.0
2019-02-14 23:44:15,594 : samples : 512000
2019-02-14 23:44:26,720 : Image to text: 14.82, 37.18, 50.78, 10.0
2019-02-14 23:44:33,912 : Text to Image: 12.052, 32.44, 45.376, 13.0
2019-02-14 23:45:08,089 : Epoch 8 finished
2019-02-14 23:45:08,543 : Image to text: 35.4, 68.1, 80.7, 3.0
2019-02-14 23:45:08,903 : Text to Image: 28.54, 63.42, 79.54, 3.0
2019-02-14 23:45:09,351 : Image to text: 32.8, 67.1, 81.1, 3.0
2019-02-14 23:45:09,711 : Text to Image: 27.54, 62.38, 78.64, 3.0
2019-02-14 23:45:10,159 : Image to text: 32.8, 67.3, 81.7, 3.0
2019-02-14 23:45:10,530 : Text to Image: 27.58, 63.28, 79.06, 3.0
2019-02-14 23:45:10,997 : Image to text: 34.2, 67.5, 81.5, 3.0
2019-02-14 23:45:11,357 : Text to Image: 28.58, 63.18, 79.08, 3.0
2019-02-14 23:45:11,802 : Image to text: 34.3, 68.3, 80.9, 3.0
2019-02-14 23:45:12,161 : Text to Image: 28.06, 63.12, 78.3, 3.0
2019-02-14 23:45:12,161 : Dev mean Text to Image: 28.060000000000002, 63.07599999999999, 78.924, 3.0
2019-02-14 23:45:12,161 : Dev mean Image to text: 33.9, 67.66, 81.18, 3.0
2019-02-14 23:45:12,161 : start epoch
2019-02-14 23:45:53,152 : samples : 64000
2019-02-14 23:46:05,668 : Image to text: 14.9, 37.78, 51.08, 10.0
2019-02-14 23:46:15,628 : Text to Image: 11.836, 32.032, 44.868, 13.0
2019-02-14 23:46:57,558 : samples : 128000
2019-02-14 23:47:07,576 : Image to text: 14.92, 36.98, 50.62, 10.0
2019-02-14 23:47:14,753 : Text to Image: 11.664, 31.988, 44.896, 13.0
2019-02-14 23:47:56,397 : samples : 192000
2019-02-14 23:48:08,902 : Image to text: 14.3, 36.84, 50.18, 10.0
2019-02-14 23:48:18,850 : Text to Image: 11.768, 31.876, 44.72, 13.0
2019-02-14 23:49:01,297 : samples : 256000
2019-02-14 23:49:11,267 : Image to text: 14.22, 37.14, 50.9, 10.0
2019-02-14 23:49:18,373 : Text to Image: 11.908, 32.288, 45.28, 13.0
2019-02-14 23:49:59,554 : samples : 320000
2019-02-14 23:50:12,011 : Image to text: 15.02, 37.6, 51.04, 10.0
2019-02-14 23:50:21,920 : Text to Image: 11.928, 32.792, 45.52, 13.0
2019-02-14 23:51:04,667 : samples : 384000
2019-02-14 23:51:14,635 : Image to text: 14.18, 36.78, 49.82, 11.0
2019-02-14 23:51:21,662 : Text to Image: 11.784, 32.568, 45.06, 13.0
2019-02-14 23:52:01,914 : samples : 448000
2019-02-14 23:52:14,432 : Image to text: 15.28, 37.52, 51.3, 10.0
2019-02-14 23:52:24,440 : Text to Image: 11.864, 32.1, 44.456, 13.0
2019-02-14 23:53:07,491 : samples : 512000
2019-02-14 23:53:19,739 : Image to text: 14.52, 37.38, 51.18, 10.0
2019-02-14 23:53:26,934 : Text to Image: 11.796, 32.068, 45.62, 13.0
2019-02-14 23:54:01,169 : Epoch 9 finished
2019-02-14 23:54:01,632 : Image to text: 36.2, 68.0, 82.1, 2.0
2019-02-14 23:54:01,918 : Text to Image: 29.44, 64.36, 80.0, 3.0
2019-02-14 23:54:02,276 : Image to text: 34.1, 66.6, 81.5, 3.0
2019-02-14 23:54:02,551 : Text to Image: 28.02, 62.16, 78.18, 4.0
2019-02-14 23:54:02,910 : Image to text: 34.3, 67.9, 80.4, 3.0
2019-02-14 23:54:03,185 : Text to Image: 28.82, 63.32, 79.48, 3.0
2019-02-14 23:54:03,548 : Image to text: 34.5, 69.1, 82.6, 3.0
2019-02-14 23:54:03,822 : Text to Image: 28.74, 63.84, 79.28, 3.0
2019-02-14 23:54:04,187 : Image to text: 36.0, 68.2, 82.3, 2.0
2019-02-14 23:54:04,454 : Text to Image: 29.5, 63.76, 78.4, 3.0
2019-02-14 23:54:04,454 : Dev mean Text to Image: 28.903999999999996, 63.488, 79.06800000000001, 3.2
2019-02-14 23:54:04,454 : Dev mean Image to text: 35.02, 67.96000000000001, 81.78, 2.6
2019-02-14 23:54:04,455 : start epoch
2019-02-14 23:54:46,361 : samples : 64000
2019-02-14 23:54:58,889 : Image to text: 15.12, 37.46, 51.48, 10.0
2019-02-14 23:55:08,876 : Text to Image: 12.068, 32.496, 45.716, 13.0
2019-02-14 23:55:51,267 : samples : 128000
2019-02-14 23:56:01,286 : Image to text: 15.44, 37.48, 51.18, 10.0
2019-02-14 23:56:08,425 : Text to Image: 12.124, 32.816, 45.472, 13.0
2019-02-14 23:56:49,246 : samples : 192000
2019-02-14 23:56:59,919 : Image to text: 14.94, 38.24, 52.1, 9.0
2019-02-14 23:57:07,140 : Text to Image: 12.424, 32.644, 45.608, 13.0
2019-02-14 23:57:48,374 : samples : 256000
2019-02-14 23:57:58,391 : Image to text: 14.94, 38.0, 51.8, 10.0
2019-02-14 23:58:05,611 : Text to Image: 12.068, 32.272, 45.444, 13.0
2019-02-14 23:58:46,690 : samples : 320000
2019-02-14 23:58:56,598 : Image to text: 14.4, 37.46, 50.76, 10.0
2019-02-14 23:59:04,580 : Text to Image: 11.976, 32.836, 45.812, 13.0
2019-02-14 23:59:45,578 : samples : 384000
2019-02-14 23:59:55,608 : Image to text: 14.82, 38.24, 51.56, 10.0
2019-02-15 00:00:02,840 : Text to Image: 12.048, 32.22, 45.472, 13.0
2019-02-15 00:00:44,151 : samples : 448000
2019-02-15 00:00:54,133 : Image to text: 14.48, 37.94, 52.1, 9.0
2019-02-15 00:01:01,327 : Text to Image: 12.084, 32.788, 45.836, 13.0
2019-02-15 00:01:41,723 : samples : 512000
2019-02-15 00:01:54,186 : Image to text: 14.62, 36.84, 50.14, 10.0
2019-02-15 00:02:04,269 : Text to Image: 11.82, 32.248, 45.364, 13.0
2019-02-15 00:02:39,853 : Epoch 10 finished
2019-02-15 00:02:40,308 : Image to text: 35.1, 68.9, 81.8, 3.0
2019-02-15 00:02:40,670 : Text to Image: 28.94, 64.4, 79.94, 3.0
2019-02-15 00:02:41,113 : Image to text: 32.1, 67.7, 81.0, 3.0
2019-02-15 00:02:41,474 : Text to Image: 28.12, 62.3, 78.38, 4.0
2019-02-15 00:02:41,923 : Image to text: 34.6, 69.3, 82.3, 3.0
2019-02-15 00:02:42,285 : Text to Image: 28.72, 62.78, 78.76, 3.0
2019-02-15 00:02:42,734 : Image to text: 35.9, 69.4, 83.4, 2.0
2019-02-15 00:02:43,096 : Text to Image: 28.96, 63.92, 79.08, 3.0
2019-02-15 00:02:43,545 : Image to text: 33.4, 67.1, 82.1, 3.0
2019-02-15 00:02:43,906 : Text to Image: 28.44, 63.3, 78.82, 3.0
2019-02-15 00:02:43,906 : Dev mean Text to Image: 28.636000000000003, 63.34, 78.996, 3.2
2019-02-15 00:02:43,906 : Dev mean Image to text: 34.22, 68.48, 82.12, 2.8
2019-02-15 00:02:43,906 : start epoch
2019-02-15 00:03:24,565 : samples : 64000
2019-02-15 00:03:35,061 : Image to text: 14.58, 37.82, 51.66, 10.0
2019-02-15 00:03:45,214 : Text to Image: 12.076, 32.364, 45.46, 13.0
2019-02-15 00:04:25,964 : samples : 128000
2019-02-15 00:04:35,936 : Image to text: 15.38, 37.96, 51.34, 10.0
2019-02-15 00:04:43,123 : Text to Image: 12.012, 33.12, 46.004, 13.0
2019-02-15 00:05:23,448 : samples : 192000
2019-02-15 00:05:33,386 : Image to text: 14.82, 37.24, 50.84, 10.0
2019-02-15 00:05:42,175 : Text to Image: 12.04, 32.616, 45.68, 13.0
2019-02-15 00:06:22,329 : samples : 256000
2019-02-15 00:06:33,785 : Image to text: 15.62, 38.26, 52.68, 9.0
2019-02-15 00:06:43,602 : Text to Image: 12.068, 32.396, 45.452, 13.0
2019-02-15 00:07:26,097 : samples : 320000
2019-02-15 00:07:38,503 : Image to text: 15.44, 37.8, 51.56, 10.0
2019-02-15 00:07:48,312 : Text to Image: 12.052, 32.46, 45.512, 13.0
2019-02-15 00:08:30,786 : samples : 384000
2019-02-15 00:08:43,176 : Image to text: 14.84, 38.78, 51.96, 10.0
2019-02-15 00:08:53,050 : Text to Image: 12.404, 33.588, 46.452, 12.0
2019-02-15 00:09:35,537 : samples : 448000
2019-02-15 00:09:47,994 : Image to text: 15.48, 38.82, 52.24, 9.0
2019-02-15 00:09:57,826 : Text to Image: 12.132, 33.192, 45.884, 13.0
2019-02-15 00:10:39,956 : samples : 512000
2019-02-15 00:10:52,452 : Image to text: 14.86, 38.22, 51.84, 10.0
2019-02-15 00:11:02,405 : Text to Image: 12.14, 33.116, 46.088, 13.0
2019-02-15 00:11:38,544 : Epoch 11 finished
2019-02-15 00:11:39,461 : Image to text: 35.1, 68.7, 80.8, 3.0
2019-02-15 00:11:40,217 : Text to Image: 28.88, 63.98, 79.7, 3.0
2019-02-15 00:11:41,113 : Image to text: 33.1, 69.2, 81.5, 3.0
2019-02-15 00:11:41,838 : Text to Image: 28.12, 62.88, 77.9, 3.0
2019-02-15 00:11:42,723 : Image to text: 34.0, 68.4, 82.5, 2.0
2019-02-15 00:11:43,447 : Text to Image: 28.8, 64.02, 79.36, 3.0
2019-02-15 00:11:44,348 : Image to text: 36.4, 69.0, 81.5, 2.0
2019-02-15 00:11:45,087 : Text to Image: 28.72, 64.0, 78.6, 3.0
2019-02-15 00:11:45,999 : Image to text: 35.1, 68.3, 81.4, 3.0
2019-02-15 00:11:46,772 : Text to Image: 29.0, 63.06, 78.22, 3.0
2019-02-15 00:11:46,772 : Dev mean Text to Image: 28.704, 63.588, 78.756, 3.0
2019-02-15 00:11:46,772 : Dev mean Image to text: 34.74, 68.72, 81.54, 2.6
2019-02-15 00:11:46,772 : start epoch
2019-02-15 00:12:28,379 : samples : 64000
2019-02-15 00:12:40,927 : Image to text: 15.74, 38.34, 51.2, 10.0
2019-02-15 00:12:50,832 : Text to Image: 12.296, 33.228, 46.236, 13.0
2019-02-15 00:13:31,247 : samples : 128000
2019-02-15 00:13:43,757 : Image to text: 14.48, 37.54, 51.3, 10.0
2019-02-15 00:13:53,726 : Text to Image: 12.088, 32.8, 45.604, 13.0
2019-02-15 00:14:33,808 : samples : 192000
2019-02-15 00:14:46,333 : Image to text: 15.4, 38.94, 52.52, 9.0
2019-02-15 00:14:56,316 : Text to Image: 12.512, 33.296, 46.244, 13.0
2019-02-15 00:15:37,231 : samples : 256000
2019-02-15 00:15:49,730 : Image to text: 14.58, 38.12, 51.28, 10.0
2019-02-15 00:15:59,643 : Text to Image: 12.244, 32.672, 45.4, 13.0
2019-02-15 00:16:41,353 : samples : 320000
2019-02-15 00:16:53,963 : Image to text: 14.64, 37.08, 51.26, 10.0
2019-02-15 00:17:03,947 : Text to Image: 12.272, 32.736, 45.64, 13.0
2019-02-15 00:17:46,852 : samples : 384000
2019-02-15 00:17:58,427 : Image to text: 14.7, 38.7, 51.9, 9.0
2019-02-15 00:18:05,576 : Text to Image: 12.596, 33.48, 45.972, 13.0
2019-02-15 00:18:46,263 : samples : 448000
2019-02-15 00:18:56,297 : Image to text: 16.12, 39.1, 52.4, 9.0
2019-02-15 00:19:03,548 : Text to Image: 12.428, 32.992, 45.932, 13.0
2019-02-15 00:19:44,085 : samples : 512000
2019-02-15 00:19:54,133 : Image to text: 15.96, 38.34, 52.86, 9.0
2019-02-15 00:20:01,255 : Text to Image: 12.516, 33.172, 46.06, 13.0
2019-02-15 00:20:36,693 : Epoch 12 finished
2019-02-15 00:20:37,811 : Image to text: 35.5, 68.5, 82.5, 3.0
2019-02-15 00:20:38,704 : Text to Image: 29.28, 64.64, 80.64, 3.0
2019-02-15 00:20:39,738 : Image to text: 33.4, 67.4, 80.1, 3.0
2019-02-15 00:20:40,581 : Text to Image: 28.42, 63.32, 78.98, 3.0
2019-02-15 00:20:41,645 : Image to text: 33.5, 69.8, 81.9, 3.0
2019-02-15 00:20:42,573 : Text to Image: 29.46, 63.82, 79.58, 3.0
2019-02-15 00:20:43,660 : Image to text: 36.1, 69.8, 81.9, 2.0
2019-02-15 00:20:44,589 : Text to Image: 28.82, 63.96, 79.86, 3.0
2019-02-15 00:20:45,620 : Image to text: 36.1, 69.0, 81.3, 3.0
2019-02-15 00:20:46,607 : Text to Image: 29.46, 63.66, 79.56, 3.0
2019-02-15 00:20:46,607 : Dev mean Text to Image: 29.087999999999997, 63.879999999999995, 79.724, 3.0
2019-02-15 00:20:46,607 : Dev mean Image to text: 34.92, 68.9, 81.53999999999999, 2.8
2019-02-15 00:20:46,607 : start epoch
2019-02-15 00:21:29,472 : samples : 64000
2019-02-15 00:21:42,284 : Image to text: 15.02, 38.96, 52.18, 9.0
2019-02-15 00:21:52,649 : Text to Image: 12.364, 33.184, 46.144, 13.0
2019-02-15 00:22:36,628 : samples : 128000
2019-02-15 00:22:49,468 : Image to text: 15.32, 38.06, 51.62, 10.0
2019-02-15 00:22:59,778 : Text to Image: 12.328, 32.736, 45.436, 13.0
2019-02-15 00:23:43,423 : samples : 192000
2019-02-15 00:23:56,235 : Image to text: 14.72, 38.74, 52.1, 9.0
2019-02-15 00:24:06,610 : Text to Image: 12.184, 33.096, 46.108, 13.0
2019-02-15 00:24:50,534 : samples : 256000
2019-02-15 00:25:03,352 : Image to text: 16.16, 39.6, 53.14, 9.0
2019-02-15 00:25:13,730 : Text to Image: 12.484, 33.308, 46.46, 12.0
2019-02-15 00:25:57,644 : samples : 320000
2019-02-15 00:26:10,472 : Image to text: 15.64, 38.12, 52.04, 10.0
2019-02-15 00:26:20,852 : Text to Image: 12.304, 33.292, 46.36, 12.0
2019-02-15 00:27:04,215 : samples : 384000
2019-02-15 00:27:17,057 : Image to text: 15.04, 38.4, 52.18, 10.0
2019-02-15 00:27:27,415 : Text to Image: 12.336, 33.088, 46.028, 13.0
2019-02-15 00:28:11,301 : samples : 448000
2019-02-15 00:28:24,003 : Image to text: 15.54, 38.22, 52.04, 9.0
2019-02-15 00:28:31,255 : Text to Image: 12.464, 33.196, 46.408, 12.0
2019-02-15 00:29:12,604 : samples : 512000
2019-02-15 00:29:22,513 : Image to text: 15.56, 38.3, 51.84, 10.0
2019-02-15 00:29:29,697 : Text to Image: 12.264, 33.172, 45.608, 13.0
2019-02-15 00:30:05,395 : Epoch 13 finished
2019-02-15 00:30:05,878 : Image to text: 34.6, 70.6, 82.2, 3.0
2019-02-15 00:30:06,210 : Text to Image: 29.48, 64.56, 80.54, 3.0
2019-02-15 00:30:06,692 : Image to text: 33.8, 69.5, 81.2, 3.0
2019-02-15 00:30:07,024 : Text to Image: 28.62, 62.8, 78.98, 3.0
2019-02-15 00:30:07,503 : Image to text: 33.4, 70.0, 82.6, 3.0
2019-02-15 00:30:07,834 : Text to Image: 29.12, 64.9, 80.0, 3.0
2019-02-15 00:30:08,315 : Image to text: 36.1, 69.0, 83.0, 2.0
2019-02-15 00:30:08,646 : Text to Image: 29.88, 64.8, 79.94, 3.0
2019-02-15 00:30:09,130 : Image to text: 36.6, 69.7, 82.7, 2.0
2019-02-15 00:30:09,461 : Text to Image: 29.0, 64.44, 78.8, 3.0
2019-02-15 00:30:09,461 : Dev mean Text to Image: 29.220000000000002, 64.3, 79.652, 3.0
2019-02-15 00:30:09,461 : Dev mean Image to text: 34.9, 69.75999999999999, 82.34, 2.5999999999999996
2019-02-15 00:30:09,461 : start epoch
2019-02-15 00:30:50,999 : samples : 64000
2019-02-15 00:31:01,065 : Image to text: 14.7, 38.28, 52.4, 9.0
2019-02-15 00:31:08,428 : Text to Image: 12.344, 33.3, 46.304, 12.0
2019-02-15 00:31:47,796 : samples : 128000
2019-02-15 00:31:57,938 : Image to text: 15.18, 38.7, 51.74, 10.0
2019-02-15 00:32:05,292 : Text to Image: 12.26, 32.608, 45.96, 13.0
2019-02-15 00:32:45,293 : samples : 192000
2019-02-15 00:32:55,438 : Image to text: 15.92, 39.02, 52.9, 9.0
2019-02-15 00:33:02,818 : Text to Image: 12.288, 32.672, 45.86, 13.0
2019-02-15 00:33:42,805 : samples : 256000
2019-02-15 00:33:52,992 : Image to text: 15.38, 38.86, 53.16, 9.0
2019-02-15 00:34:00,342 : Text to Image: 12.412, 33.44, 46.432, 12.0
2019-02-15 00:34:39,558 : samples : 320000
2019-02-15 00:34:49,712 : Image to text: 15.44, 38.56, 52.6, 9.0
2019-02-15 00:34:57,055 : Text to Image: 12.2, 32.936, 46.008, 13.0
2019-02-15 00:35:36,593 : samples : 384000
2019-02-15 00:35:46,782 : Image to text: 15.76, 38.78, 52.28, 9.0
2019-02-15 00:35:54,179 : Text to Image: 12.468, 33.444, 46.452, 12.0
2019-02-15 00:36:33,692 : samples : 448000
2019-02-15 00:36:43,647 : Image to text: 15.16, 37.92, 51.6, 10.0
2019-02-15 00:36:50,920 : Text to Image: 12.044, 32.5, 45.644, 13.0
2019-02-15 00:37:29,984 : samples : 512000
2019-02-15 00:37:39,940 : Image to text: 16.02, 39.92, 53.36, 9.0
2019-02-15 00:37:47,204 : Text to Image: 12.584, 33.476, 46.64, 12.0
2019-02-15 00:38:22,537 : Epoch 14 finished
2019-02-15 00:38:23,017 : Image to text: 36.1, 68.7, 80.6, 3.0
2019-02-15 00:38:23,347 : Text to Image: 29.34, 64.14, 80.52, 3.0
2019-02-15 00:38:23,829 : Image to text: 34.9, 68.3, 81.4, 3.0
2019-02-15 00:38:24,158 : Text to Image: 28.72, 62.4, 79.44, 3.0
2019-02-15 00:38:24,639 : Image to text: 34.8, 69.4, 82.0, 3.0
2019-02-15 00:38:24,969 : Text to Image: 29.3, 65.04, 80.0, 3.0
2019-02-15 00:38:25,451 : Image to text: 36.4, 68.7, 81.2, 2.0
2019-02-15 00:38:25,785 : Text to Image: 29.42, 65.06, 79.56, 3.0
2019-02-15 00:38:26,266 : Image to text: 36.1, 68.6, 80.8, 3.0
2019-02-15 00:38:26,598 : Text to Image: 28.92, 64.32, 79.42, 3.0
2019-02-15 00:38:26,598 : Dev mean Text to Image: 29.14, 64.19200000000001, 79.788, 3.0
2019-02-15 00:38:26,598 : Dev mean Image to text: 35.66, 68.74000000000001, 81.19999999999999, 2.8
2019-02-15 00:38:30,498 : 
Test scores | Image to text:             35.68, 68.92, 82.44000000000001, 2.6
2019-02-15 00:38:30,498 : Test scores | Text to image:             29.072, 64.116, 79.65599999999999, 3.0

2019-02-15 00:38:30,590 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 00:38:30,799 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 00:38:31,408 : loading BERT model bert-base-uncased
2019-02-15 00:38:31,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:38:31,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:38:31,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm1chb5_5
2019-02-15 00:38:33,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:38:35,129 : Computing embeddings for train/dev/test
2019-02-15 00:40:09,310 : Computed embeddings
2019-02-15 00:40:09,310 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:40:43,517 : [('reg:1e-05', 89.77), ('reg:0.0001', 88.98), ('reg:0.001', 82.85), ('reg:0.01', 72.69)]
2019-02-15 00:40:43,517 : Validation : best param found is reg = 1e-05 with score             89.77
2019-02-15 00:40:43,517 : Evaluating...
2019-02-15 00:40:53,542 : 
Dev acc : 89.8 Test acc : 89.6 for LENGTH classification

2019-02-15 00:40:53,543 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 00:40:53,908 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 00:40:53,954 : loading BERT model bert-base-uncased
2019-02-15 00:40:53,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:40:53,987 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:40:53,987 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw23531st
2019-02-15 00:40:56,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:40:57,728 : Computing embeddings for train/dev/test
2019-02-15 00:42:25,808 : Computed embeddings
2019-02-15 00:42:25,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:43:02,178 : [('reg:1e-05', 65.55), ('reg:0.0001', 37.55), ('reg:0.001', 4.06), ('reg:0.01', 0.92)]
2019-02-15 00:43:02,179 : Validation : best param found is reg = 1e-05 with score             65.55
2019-02-15 00:43:02,179 : Evaluating...
2019-02-15 00:43:12,450 : 
Dev acc : 65.5 Test acc : 65.8 for WORDCONTENT classification

2019-02-15 00:43:12,451 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 00:43:12,827 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 00:43:12,896 : loading BERT model bert-base-uncased
2019-02-15 00:43:12,897 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:43:12,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:43:12,924 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_kz4s9zj
2019-02-15 00:43:15,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:43:16,674 : Computing embeddings for train/dev/test
2019-02-15 00:44:40,770 : Computed embeddings
2019-02-15 00:44:40,770 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:45:02,309 : [('reg:1e-05', 38.46), ('reg:0.0001', 38.27), ('reg:0.001', 36.31), ('reg:0.01', 30.37)]
2019-02-15 00:45:02,309 : Validation : best param found is reg = 1e-05 with score             38.46
2019-02-15 00:45:02,309 : Evaluating...
2019-02-15 00:45:08,497 : 
Dev acc : 38.5 Test acc : 38.6 for DEPTH classification

2019-02-15 00:45:08,498 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 00:45:08,904 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 00:45:08,970 : loading BERT model bert-base-uncased
2019-02-15 00:45:08,971 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:45:09,093 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:45:09,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx8g_mbhn
2019-02-15 00:45:11,481 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:45:12,876 : Computing embeddings for train/dev/test
2019-02-15 00:46:31,232 : Computed embeddings
2019-02-15 00:46:31,232 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:46:55,597 : [('reg:1e-05', 74.51), ('reg:0.0001', 73.63), ('reg:0.001', 67.13), ('reg:0.01', 58.58)]
2019-02-15 00:46:55,597 : Validation : best param found is reg = 1e-05 with score             74.51
2019-02-15 00:46:55,597 : Evaluating...
2019-02-15 00:47:01,890 : 
Dev acc : 74.5 Test acc : 74.0 for TOPCONSTITUENTS classification

2019-02-15 00:47:01,891 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 00:47:02,255 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 00:47:02,325 : loading BERT model bert-base-uncased
2019-02-15 00:47:02,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:47:02,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:47:02,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0iighsl5
2019-02-15 00:47:04,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:47:06,203 : Computing embeddings for train/dev/test
2019-02-15 00:48:31,068 : Computed embeddings
2019-02-15 00:48:31,068 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:48:59,242 : [('reg:1e-05', 90.1), ('reg:0.0001', 90.04), ('reg:0.001', 90.06), ('reg:0.01', 88.52)]
2019-02-15 00:48:59,242 : Validation : best param found is reg = 1e-05 with score             90.1
2019-02-15 00:48:59,242 : Evaluating...
2019-02-15 00:49:06,757 : 
Dev acc : 90.1 Test acc : 89.5 for BIGRAMSHIFT classification

2019-02-15 00:49:06,758 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 00:49:07,361 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 00:49:07,431 : loading BERT model bert-base-uncased
2019-02-15 00:49:07,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:49:07,465 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:49:07,465 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8m_i4bmc
2019-02-15 00:49:09,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:49:11,248 : Computing embeddings for train/dev/test
2019-02-15 00:50:34,259 : Computed embeddings
2019-02-15 00:50:34,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:50:59,036 : [('reg:1e-05', 89.65), ('reg:0.0001', 89.74), ('reg:0.001', 89.86), ('reg:0.01', 89.67)]
2019-02-15 00:50:59,036 : Validation : best param found is reg = 0.001 with score             89.86
2019-02-15 00:50:59,036 : Evaluating...
2019-02-15 00:51:06,513 : 
Dev acc : 89.9 Test acc : 88.9 for TENSE classification

2019-02-15 00:51:06,514 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 00:51:06,962 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 00:51:07,029 : loading BERT model bert-base-uncased
2019-02-15 00:51:07,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:51:07,060 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:51:07,061 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzx1hd4ny
2019-02-15 00:51:09,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:51:10,811 : Computing embeddings for train/dev/test
2019-02-15 00:52:38,643 : Computed embeddings
2019-02-15 00:52:38,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:53:06,088 : [('reg:1e-05', 86.37), ('reg:0.0001', 86.27), ('reg:0.001', 86.46), ('reg:0.01', 85.4)]
2019-02-15 00:53:06,088 : Validation : best param found is reg = 0.001 with score             86.46
2019-02-15 00:53:06,088 : Evaluating...
2019-02-15 00:53:12,290 : 
Dev acc : 86.5 Test acc : 86.6 for SUBJNUMBER classification

2019-02-15 00:53:12,291 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 00:53:12,740 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 00:53:12,811 : loading BERT model bert-base-uncased
2019-02-15 00:53:12,812 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:53:12,842 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:53:12,842 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvfiyb1cl
2019-02-15 00:53:15,257 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:53:16,657 : Computing embeddings for train/dev/test
2019-02-15 00:54:42,857 : Computed embeddings
2019-02-15 00:54:42,858 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:55:19,568 : [('reg:1e-05', 83.66), ('reg:0.0001', 83.65), ('reg:0.001', 83.61), ('reg:0.01', 82.44)]
2019-02-15 00:55:19,568 : Validation : best param found is reg = 1e-05 with score             83.66
2019-02-15 00:55:19,568 : Evaluating...
2019-02-15 00:55:27,461 : 
Dev acc : 83.7 Test acc : 84.5 for OBJNUMBER classification

2019-02-15 00:55:27,462 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 00:55:27,843 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 00:55:27,910 : loading BERT model bert-base-uncased
2019-02-15 00:55:27,910 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:55:28,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:55:28,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5eykoims
2019-02-15 00:55:30,375 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:55:31,742 : Computing embeddings for train/dev/test
2019-02-15 00:57:09,292 : Computed embeddings
2019-02-15 00:57:09,293 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:57:41,085 : [('reg:1e-05', 65.02), ('reg:0.0001', 64.96), ('reg:0.001', 64.8), ('reg:0.01', 63.73)]
2019-02-15 00:57:41,085 : Validation : best param found is reg = 1e-05 with score             65.02
2019-02-15 00:57:41,086 : Evaluating...
2019-02-15 00:57:49,750 : 
Dev acc : 65.0 Test acc : 64.6 for ODDMANOUT classification

2019-02-15 00:57:49,751 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 00:57:50,165 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 00:57:50,245 : loading BERT model bert-base-uncased
2019-02-15 00:57:50,245 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:57:50,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:57:50,382 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphbvn3zk0
2019-02-15 00:57:52,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:57:54,159 : Computing embeddings for train/dev/test
2019-02-15 00:59:31,226 : Computed embeddings
2019-02-15 00:59:31,227 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:59:59,629 : [('reg:1e-05', 67.62), ('reg:0.0001', 67.44), ('reg:0.001', 69.94), ('reg:0.01', 66.04)]
2019-02-15 00:59:59,630 : Validation : best param found is reg = 0.001 with score             69.94
2019-02-15 00:59:59,630 : Evaluating...
2019-02-15 01:00:06,015 : 
Dev acc : 69.9 Test acc : 69.3 for COORDINATIONINVERSION classification

2019-02-15 01:00:06,017 : total results: {'STS12': {'MSRpar': {'pearson': (0.3924020319939267, 5.110540932933977e-29), 'spearman': SpearmanrResult(correlation=0.4190738932657634, pvalue=2.9577727188863983e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.47162075912408014, 8.445546325743629e-43), 'spearman': SpearmanrResult(correlation=0.49051205726496033, pvalue=1.1693451991403414e-46), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46416593300116343, 6.645117651820876e-26), 'spearman': SpearmanrResult(correlation=0.5847147533163599, pvalue=1.9230923240514167e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6317233154812663, 8.758057096270178e-85), 'spearman': SpearmanrResult(correlation=0.6293590219685372, pvalue=5.5966812076654985e-84), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.667909471111825, 7.0284900138157e-53), 'spearman': SpearmanrResult(correlation=0.5460680688746249, pvalue=2.1728970254519727e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5255643021424523, 'wmean': 0.515237330170723}, 'spearman': {'mean': 0.5339455589380492, 'wmean': 0.5278230246549647}}}, 'STS13': {'FNWN': {'pearson': (0.32499569866746186, 5.047889574288899e-06), 'spearman': SpearmanrResult(correlation=0.3498446463955529, pvalue=8.036016272545803e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6159097504304419, 1.5864569540560712e-79), 'spearman': SpearmanrResult(correlation=0.5902683769867664, pvalue=1.310579983990421e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.41511402113403356, 8.888723949730702e-25), 'spearman': SpearmanrResult(correlation=0.43389061963754916, pvalue=3.709027701142556e-27), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4520064900773125, 'wmean': 0.5041569771514498}, 'spearman': {'mean': 0.45800121433995616, 'wmean': 0.5014897056836662}}}, 'STS14': {'deft-forum': {'pearson': (0.3059469671024042, 3.3173982700092136e-11), 'spearman': SpearmanrResult(correlation=0.3317155514959915, pvalue=5.09708439361095e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7434077473106647, 5.4677746570982e-54), 'spearman': SpearmanrResult(correlation=0.7054090593408965, pvalue=1.8651850848580034e-46), 'nsamples': 300}, 'headlines': {'pearson': (0.5790238274332603, 2.347931446148623e-68), 'spearman': SpearmanrResult(correlation=0.5361764450306294, pvalue=4.7756458772479184e-57), 'nsamples': 750}, 'images': {'pearson': (0.5356773794172422, 6.328920326927348e-57), 'spearman': SpearmanrResult(correlation=0.5298844982241631, pvalue=1.6070702528854876e-55), 'nsamples': 750}, 'OnWN': {'pearson': (0.6166935741395135, 8.84793508414423e-80), 'spearman': SpearmanrResult(correlation=0.6475438836983032, pvalue=2.3286306185683466e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.670981045133349, 2.947154321986188e-99), 'spearman': SpearmanrResult(correlation=0.6141684573114827, pvalue=5.770704330507687e-79), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.575288423422739, 'wmean': 0.5766614210618147}, 'spearman': {'mean': 0.5608163158502445, 'wmean': 0.5617932477797064}}}, 'STS15': {'answers-forums': {'pearson': (0.5635858292899794, 8.076388605188986e-33), 'spearman': SpearmanrResult(correlation=0.5603225341018957, pvalue=2.207360477871102e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.6789339114920632, 1.8248550928792453e-102), 'spearman': SpearmanrResult(correlation=0.6844870668803423, pvalue=9.123410263499205e-105), 'nsamples': 750}, 'belief': {'pearson': (0.6592526715400386, 4.0512455843000424e-48), 'spearman': SpearmanrResult(correlation=0.6907674703659085, pvalue=1.7825095300566316e-54), 'nsamples': 375}, 'headlines': {'pearson': (0.6271697762001254, 3.0734001506537003e-83), 'spearman': SpearmanrResult(correlation=0.6190311166778797, pvalue=1.5357571088527542e-80), 'nsamples': 750}, 'images': {'pearson': (0.660192855528555, 4.6203864139896516e-95), 'spearman': SpearmanrResult(correlation=0.6668800735718796, pvalue=1.2160010524068813e-97), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6378270088101523, 'wmean': 0.6444289484089382}, 'spearman': {'mean': 0.6442976523195811, 'wmean': 0.6489858148410009}}}, 'STS16': {'answer-answer': {'pearson': (0.5643685628397859, 9.149683270743288e-23), 'spearman': SpearmanrResult(correlation=0.5845260755580295, pvalue=1.1339889247163047e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6490067712757833, 3.646728357603356e-31), 'spearman': SpearmanrResult(correlation=0.6468326482926066, pvalue=6.6646181964762275e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7652704464132425, 1.6541091849053773e-45), 'spearman': SpearmanrResult(correlation=0.7771756512889494, pvalue=9.284316153054097e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8062280074637963, 4.3039328643622865e-57), 'spearman': SpearmanrResult(correlation=0.8399125031710475, pvalue=3.567092603866178e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.33391115160413, 7.776502946586766e-07), 'spearman': SpearmanrResult(correlation=0.3298970644326125, pvalue=1.0721346647484549e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6237569879193476, 'wmean': 0.6302458416445623}, 'spearman': {'mean': 0.635668788548649, 'wmean': 0.6426378496232787}}}, 'MR': {'devacc': 78.76, 'acc': 78.01, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.81, 'acc': 82.62, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.07, 'acc': 88.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.89, 'acc': 94.27, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.37, 'acc': 83.42, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.23, 'acc': 42.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.54, 'acc': 93.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.63, 'acc': 70.26, 'f1': 76.35, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.4, 'acc': 78.16, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7878668978345402, 'pearson': 0.8056425074402782, 'spearman': 0.7394980348519594, 'mse': 0.35792682498059625, 'yhat': array([3.38725744, 3.85517148, 1.81030706, ..., 3.17998841, 4.39484073,        4.4519012 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7271555770577297, 'pearson': 0.668318077803957, 'spearman': 0.6647568027610277, 'mse': 1.4627233652334592, 'yhat': array([1.62377271, 1.71060255, 2.2759178 , ..., 3.95847193, 3.93101264,        3.52923849]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.34, 'acc': 65.51, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 360.17199999999997, 'acc': [(35.68, 68.92, 82.44000000000001, 2.6), (29.072, 64.116, 79.65599999999999, 3.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 89.77, 'acc': 89.57, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 65.55, 'acc': 65.84, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 38.46, 'acc': 38.56, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.51, 'acc': 73.97, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.1, 'acc': 89.45, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.86, 'acc': 88.94, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.46, 'acc': 86.57, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.66, 'acc': 84.48, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.02, 'acc': 64.61, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.94, 'acc': 69.28, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 01:00:06,017 : STS12 p=0.5152, STS12 s=0.5278, STS13 p=0.5042, STS13 s=0.5015, STS14 p=0.5767, STS14 s=0.5618, STS15 p=0.6444, STS15 s=0.6490, STS 16 p=0.6302, STS16 s=0.6426, STS B p=0.6683, STS B s=0.6648, STS B m=1.4627, SICK-R p=0.8056, SICK-R s=0.7395, SICK-P m=0.3579
2019-02-15 01:00:06,017 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 01:00:06,017 : 0.5152,0.5278,0.5042,0.5015,0.5767,0.5618,0.6444,0.6490,0.6302,0.6426,0.6683,0.6648,1.4627,0.8056,0.7395,0.3579
2019-02-15 01:00:06,017 : MR=78.01, CR=82.62, SUBJ=94.27, MPQA=88.14, SST-B=83.42, SST-F=42.67, TREC=93.00, SICK-E=78.16, SNLI=65.51, MRPC=70.26, MRPC f=76.35
2019-02-15 01:00:06,017 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 01:00:06,017 : 78.01,82.62,94.27,88.14,83.42,42.67,93.00,78.16,65.51,70.26,76.35
2019-02-15 01:00:06,017 : COCO r1i2t=35.68, COCO r5i2t=68.92, COCO r10i2t=82.44, COCO medr_i2t=2.60, COCO r1t2i=29.07, COCO r5t2i=64.12, COCO r10t2i=79.66, COCO medr_t2i=3.00
2019-02-15 01:00:06,017 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 01:00:06,017 : 35.68,68.92,82.44,2.60,29.07,64.12,79.66,3.00
2019-02-15 01:00:06,017 : SentLen=89.57, WC=65.84, TreeDepth=38.56, TopConst=73.97, BShift=89.45, Tense=88.94, SubjNum=86.57, ObjNum=84.48, SOMO=64.61, CoordInv=69.28, average=75.13
2019-02-15 01:00:06,017 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 01:00:06,017 : 89.57,65.84,38.56,73.97,89.45,88.94,86.57,84.48,64.61,69.28,75.13
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,017 : layer 8
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,017 : ********************************************************************************
2019-02-15 01:00:06,111 : ***** Transfer task : STS12 *****


2019-02-15 01:00:06,151 : loading BERT model bert-base-uncased
2019-02-15 01:00:06,151 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:06,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:06,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptqq08wpu
2019-02-15 01:00:08,626 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:11,830 : MSRpar : pearson = 0.4010, spearman = 0.4245
2019-02-15 01:00:12,603 : MSRvid : pearson = 0.4220, spearman = 0.4449
2019-02-15 01:00:13,202 : SMTeuroparl : pearson = 0.4798, spearman = 0.5882
2019-02-15 01:00:14,385 : surprise.OnWN : pearson = 0.6257, spearman = 0.6279
2019-02-15 01:00:15,040 : surprise.SMTnews : pearson = 0.6661, spearman = 0.5625
2019-02-15 01:00:15,040 : ALL (weighted average) : Pearson = 0.5060,             Spearman = 0.5204
2019-02-15 01:00:15,040 : ALL (average) : Pearson = 0.5189,             Spearman = 0.5296

2019-02-15 01:00:15,040 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 01:00:15,049 : loading BERT model bert-base-uncased
2019-02-15 01:00:15,049 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:15,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:15,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp19rl2yff
2019-02-15 01:00:17,419 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:19,420 : FNWN : pearson = 0.3227, spearman = 0.3535
2019-02-15 01:00:20,287 : headlines : pearson = 0.6106, spearman = 0.5848
2019-02-15 01:00:20,962 : OnWN : pearson = 0.4210, spearman = 0.4402
2019-02-15 01:00:20,962 : ALL (weighted average) : Pearson = 0.5034,             Spearman = 0.5016
2019-02-15 01:00:20,962 : ALL (average) : Pearson = 0.4515,             Spearman = 0.4595

2019-02-15 01:00:20,962 : ***** Transfer task : STS14 *****


2019-02-15 01:00:20,978 : loading BERT model bert-base-uncased
2019-02-15 01:00:20,978 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:21,030 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:21,031 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ciu9wsg
2019-02-15 01:00:23,379 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:25,432 : deft-forum : pearson = 0.2863, spearman = 0.3008
2019-02-15 01:00:26,117 : deft-news : pearson = 0.7435, spearman = 0.7101
2019-02-15 01:00:27,119 : headlines : pearson = 0.5712, spearman = 0.5276
2019-02-15 01:00:28,159 : images : pearson = 0.5312, spearman = 0.5192
2019-02-15 01:00:29,213 : OnWN : pearson = 0.6150, spearman = 0.6463
2019-02-15 01:00:30,599 : tweet-news : pearson = 0.6725, spearman = 0.6075
2019-02-15 01:00:30,599 : ALL (weighted average) : Pearson = 0.5718,             Spearman = 0.5530
2019-02-15 01:00:30,599 : ALL (average) : Pearson = 0.5699,             Spearman = 0.5519

2019-02-15 01:00:30,600 : ***** Transfer task : STS15 *****


2019-02-15 01:00:30,646 : loading BERT model bert-base-uncased
2019-02-15 01:00:30,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:30,664 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:30,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkws_n3zf
2019-02-15 01:00:33,051 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:35,370 : answers-forums : pearson = 0.5626, spearman = 0.5576
2019-02-15 01:00:36,407 : answers-students : pearson = 0.6751, spearman = 0.6813
2019-02-15 01:00:37,358 : belief : pearson = 0.6443, spearman = 0.6727
2019-02-15 01:00:38,479 : headlines : pearson = 0.6189, spearman = 0.6123
2019-02-15 01:00:39,420 : images : pearson = 0.6555, spearman = 0.6624
2019-02-15 01:00:39,420 : ALL (weighted average) : Pearson = 0.6382,             Spearman = 0.6428
2019-02-15 01:00:39,420 : ALL (average) : Pearson = 0.6313,             Spearman = 0.6373

2019-02-15 01:00:39,420 : ***** Transfer task : STS16 *****


2019-02-15 01:00:39,487 : loading BERT model bert-base-uncased
2019-02-15 01:00:39,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:39,504 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:39,504 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8yqoeykf
2019-02-15 01:00:41,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:43,579 : answer-answer : pearson = 0.5608, spearman = 0.5761
2019-02-15 01:00:43,881 : headlines : pearson = 0.6451, spearman = 0.6454
2019-02-15 01:00:44,266 : plagiarism : pearson = 0.7616, spearman = 0.7771
2019-02-15 01:00:44,878 : postediting : pearson = 0.8046, spearman = 0.8429
2019-02-15 01:00:45,159 : question-question : pearson = 0.2971, spearman = 0.2978
2019-02-15 01:00:45,159 : ALL (weighted average) : Pearson = 0.6211,             Spearman = 0.6355
2019-02-15 01:00:45,159 : ALL (average) : Pearson = 0.6138,             Spearman = 0.6279

2019-02-15 01:00:45,159 : ***** Transfer task : MR *****


2019-02-15 01:00:45,174 : loading BERT model bert-base-uncased
2019-02-15 01:00:45,175 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:00:45,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:00:45,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptz2njwsa
2019-02-15 01:00:47,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:00:49,041 : Generating sentence embeddings
2019-02-15 01:01:02,506 : Generated sentence embeddings
2019-02-15 01:01:02,506 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 01:01:13,987 : Best param found at split 1: l2reg = 0.001                 with score 79.2
2019-02-15 01:01:25,813 : Best param found at split 2: l2reg = 0.01                 with score 79.6
2019-02-15 01:01:36,731 : Best param found at split 3: l2reg = 0.01                 with score 80.14
2019-02-15 01:01:48,041 : Best param found at split 4: l2reg = 0.01                 with score 79.94
2019-02-15 01:02:01,626 : Best param found at split 5: l2reg = 0.001                 with score 79.89
2019-02-15 01:02:02,335 : Dev acc : 79.75 Test acc : 79.34

2019-02-15 01:02:02,337 : ***** Transfer task : CR *****


2019-02-15 01:02:02,344 : loading BERT model bert-base-uncased
2019-02-15 01:02:02,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:02:02,366 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:02:02,366 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaeqc9xcz
2019-02-15 01:02:04,733 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:02:06,115 : Generating sentence embeddings
2019-02-15 01:02:09,838 : Generated sentence embeddings
2019-02-15 01:02:09,839 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 01:02:13,963 : Best param found at split 1: l2reg = 0.001                 with score 85.62
2019-02-15 01:02:18,280 : Best param found at split 2: l2reg = 0.001                 with score 85.06
2019-02-15 01:02:22,631 : Best param found at split 3: l2reg = 0.001                 with score 85.83
2019-02-15 01:02:27,148 : Best param found at split 4: l2reg = 0.001                 with score 85.2
2019-02-15 01:02:31,085 : Best param found at split 5: l2reg = 1e-05                 with score 84.94
2019-02-15 01:02:31,254 : Dev acc : 85.33 Test acc : 84.4

2019-02-15 01:02:31,254 : ***** Transfer task : MPQA *****


2019-02-15 01:02:31,259 : loading BERT model bert-base-uncased
2019-02-15 01:02:31,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:02:31,278 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:02:31,278 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphzx1x6fx
2019-02-15 01:02:33,618 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:02:35,009 : Generating sentence embeddings
2019-02-15 01:02:38,651 : Generated sentence embeddings
2019-02-15 01:02:38,651 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 01:02:51,003 : Best param found at split 1: l2reg = 0.01                 with score 88.74
2019-02-15 01:03:04,307 : Best param found at split 2: l2reg = 0.01                 with score 88.6
2019-02-15 01:03:16,959 : Best param found at split 3: l2reg = 0.001                 with score 88.41
2019-02-15 01:03:29,809 : Best param found at split 4: l2reg = 0.001                 with score 89.26
2019-02-15 01:03:42,708 : Best param found at split 5: l2reg = 0.01                 with score 88.66
2019-02-15 01:03:43,420 : Dev acc : 88.73 Test acc : 88.73

2019-02-15 01:03:43,421 : ***** Transfer task : SUBJ *****


2019-02-15 01:03:43,435 : loading BERT model bert-base-uncased
2019-02-15 01:03:43,435 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:03:43,460 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:03:43,460 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmhp39eul
2019-02-15 01:03:45,857 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:03:47,254 : Generating sentence embeddings
2019-02-15 01:04:00,393 : Generated sentence embeddings
2019-02-15 01:04:00,393 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 01:04:10,864 : Best param found at split 1: l2reg = 0.001                 with score 94.89
2019-02-15 01:04:21,580 : Best param found at split 2: l2reg = 0.001                 with score 95.18
2019-02-15 01:04:32,830 : Best param found at split 3: l2reg = 1e-05                 with score 94.65
2019-02-15 01:04:44,357 : Best param found at split 4: l2reg = 0.001                 with score 95.52
2019-02-15 01:04:56,154 : Best param found at split 5: l2reg = 0.001                 with score 95.11
2019-02-15 01:04:56,848 : Dev acc : 95.07 Test acc : 94.68

2019-02-15 01:04:56,849 : ***** Transfer task : SST Binary classification *****


2019-02-15 01:04:56,985 : loading BERT model bert-base-uncased
2019-02-15 01:04:56,985 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:04:57,008 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:04:57,009 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpecxuj_22
2019-02-15 01:04:59,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:05:00,749 : Computing embedding for train
2019-02-15 01:05:46,181 : Computed train embeddings
2019-02-15 01:05:46,182 : Computing embedding for dev
2019-02-15 01:05:47,107 : Computed dev embeddings
2019-02-15 01:05:47,107 : Computing embedding for test
2019-02-15 01:05:49,105 : Computed test embeddings
2019-02-15 01:05:49,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 01:06:13,405 : [('reg:1e-05', 84.06), ('reg:0.0001', 84.17), ('reg:0.001', 84.4), ('reg:0.01', 83.72)]
2019-02-15 01:06:13,406 : Validation : best param found is reg = 0.001 with score             84.4
2019-02-15 01:06:13,406 : Evaluating...
2019-02-15 01:06:19,478 : 
Dev acc : 84.4 Test acc : 84.84 for             SST Binary classification

2019-02-15 01:06:19,479 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 01:06:19,590 : loading BERT model bert-base-uncased
2019-02-15 01:06:19,590 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:06:19,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:06:19,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphjxcq4on
2019-02-15 01:06:21,948 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:06:23,312 : Computing embedding for train
2019-02-15 01:06:32,759 : Computed train embeddings
2019-02-15 01:06:32,759 : Computing embedding for dev
2019-02-15 01:06:33,990 : Computed dev embeddings
2019-02-15 01:06:33,990 : Computing embedding for test
2019-02-15 01:06:36,425 : Computed test embeddings
2019-02-15 01:06:36,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 01:06:39,056 : [('reg:1e-05', 42.23), ('reg:0.0001', 42.23), ('reg:0.001', 42.33), ('reg:0.01', 42.51)]
2019-02-15 01:06:39,056 : Validation : best param found is reg = 0.01 with score             42.51
2019-02-15 01:06:39,056 : Evaluating...
2019-02-15 01:06:39,800 : 
Dev acc : 42.51 Test acc : 43.62 for             SST Fine-Grained classification

2019-02-15 01:06:39,800 : ***** Transfer task : TREC *****


2019-02-15 01:06:39,814 : loading BERT model bert-base-uncased
2019-02-15 01:06:39,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:06:39,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:06:39,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8m64i4f4
2019-02-15 01:06:42,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:06:46,983 : Computed train embeddings
2019-02-15 01:06:47,247 : Computed test embeddings
2019-02-15 01:06:47,247 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 01:06:54,924 : [('reg:1e-05', 85.4), ('reg:0.0001', 85.38), ('reg:0.001', 84.39), ('reg:0.01', 79.16)]
2019-02-15 01:06:54,924 : Cross-validation : best param found is reg = 1e-05             with score 85.4
2019-02-15 01:06:54,925 : Evaluating...
2019-02-15 01:06:55,324 : 
Dev acc : 85.4 Test acc : 92.0             for TREC

2019-02-15 01:06:55,325 : ***** Transfer task : MRPC *****


2019-02-15 01:06:55,347 : loading BERT model bert-base-uncased
2019-02-15 01:06:55,347 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:06:55,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:06:55,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbc53d18t
2019-02-15 01:06:57,722 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:06:59,094 : Computing embedding for train
2019-02-15 01:07:09,222 : Computed train embeddings
2019-02-15 01:07:09,222 : Computing embedding for test
2019-02-15 01:07:13,774 : Computed test embeddings
2019-02-15 01:07:13,790 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 01:07:19,492 : [('reg:1e-05', 75.12), ('reg:0.0001', 75.39), ('reg:0.001', 75.42), ('reg:0.01', 73.23)]
2019-02-15 01:07:19,492 : Cross-validation : best param found is reg = 0.001             with score 75.42
2019-02-15 01:07:19,492 : Evaluating...
2019-02-15 01:07:19,820 : Dev acc : 75.42 Test acc 73.39; Test F1 79.79 for MRPC.

2019-02-15 01:07:19,820 : ***** Transfer task : SICK-Entailment*****


2019-02-15 01:07:19,844 : loading BERT model bert-base-uncased
2019-02-15 01:07:19,844 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:07:19,897 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:07:19,898 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwgr3hdl8
2019-02-15 01:07:22,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:07:23,707 : Computing embedding for train
2019-02-15 01:07:28,786 : Computed train embeddings
2019-02-15 01:07:28,786 : Computing embedding for dev
2019-02-15 01:07:29,478 : Computed dev embeddings
2019-02-15 01:07:29,478 : Computing embedding for test
2019-02-15 01:07:35,011 : Computed test embeddings
2019-02-15 01:07:35,040 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 01:07:36,651 : [('reg:1e-05', 79.4), ('reg:0.0001', 78.0), ('reg:0.001', 79.6), ('reg:0.01', 77.2)]
2019-02-15 01:07:36,651 : Validation : best param found is reg = 0.001 with score             79.6
2019-02-15 01:07:36,651 : Evaluating...
2019-02-15 01:07:37,053 : 
Dev acc : 79.6 Test acc : 77.49 for                        SICK entailment

2019-02-15 01:07:37,054 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 01:07:37,082 : loading BERT model bert-base-uncased
2019-02-15 01:07:37,082 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:07:37,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:07:37,103 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwqtt79zq
2019-02-15 01:07:39,539 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:07:40,951 : Computing embedding for train
2019-02-15 01:07:46,025 : Computed train embeddings
2019-02-15 01:07:46,025 : Computing embedding for dev
2019-02-15 01:07:46,742 : Computed dev embeddings
2019-02-15 01:07:46,742 : Computing embedding for test
2019-02-15 01:07:52,314 : Computed test embeddings
2019-02-15 01:08:09,788 : Dev : Pearson 0.8049233398271021
2019-02-15 01:08:09,788 : Test : Pearson 0.8083412013242576 Spearman 0.743372157010972 MSE 0.35310002413158736                        for SICK Relatedness

2019-02-15 01:08:09,789 : 

***** Transfer task : STSBenchmark*****


2019-02-15 01:08:09,827 : loading BERT model bert-base-uncased
2019-02-15 01:08:09,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:08:09,855 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:08:09,856 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm9ji0gav
2019-02-15 01:08:12,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:08:13,582 : Computing embedding for train
2019-02-15 01:08:21,663 : Computed train embeddings
2019-02-15 01:08:21,663 : Computing embedding for dev
2019-02-15 01:08:24,075 : Computed dev embeddings
2019-02-15 01:08:24,075 : Computing embedding for test
2019-02-15 01:08:26,020 : Computed test embeddings
2019-02-15 01:08:48,066 : Dev : Pearson 0.724498551024837
2019-02-15 01:08:48,066 : Test : Pearson 0.6705213744602287 Spearman 0.6645309038555148 MSE 1.448943953210956                        for SICK Relatedness

2019-02-15 01:08:48,067 : ***** Transfer task : SNLI Entailment*****


2019-02-15 01:08:53,055 : loading BERT model bert-base-uncased
2019-02-15 01:08:53,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:08:53,185 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:08:53,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo33yec5n
2019-02-15 01:08:55,548 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:08:57,112 : PROGRESS (encoding): 0.00%
2019-02-15 01:10:15,690 : PROGRESS (encoding): 14.56%
2019-02-15 01:11:42,460 : PROGRESS (encoding): 29.12%
2019-02-15 01:13:10,047 : PROGRESS (encoding): 43.69%
2019-02-15 01:14:44,707 : PROGRESS (encoding): 58.25%
2019-02-15 01:16:30,017 : PROGRESS (encoding): 72.81%
2019-02-15 01:18:13,394 : PROGRESS (encoding): 87.37%
2019-02-15 01:20:03,360 : PROGRESS (encoding): 0.00%
2019-02-15 01:20:16,934 : PROGRESS (encoding): 0.00%
2019-02-15 01:20:30,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 01:21:08,195 : [('reg:1e-09', 65.22)]
2019-02-15 01:21:08,195 : Validation : best param found is reg = 1e-09 with score             65.22
2019-02-15 01:21:08,195 : Evaluating...
2019-02-15 01:21:45,493 : Dev acc : 65.22 Test acc : 66.12 for SNLI

2019-02-15 01:21:45,493 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 01:21:54,455 : loading BERT model bert-base-uncased
2019-02-15 01:21:54,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:21:54,508 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:21:54,509 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_0c3ibnk
2019-02-15 01:21:56,860 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:21:58,261 : Computing embedding for train
2019-02-15 01:29:30,148 : Computed train embeddings
2019-02-15 01:29:30,148 : Computing embedding for dev
2019-02-15 01:29:49,781 : Computed dev embeddings
2019-02-15 01:29:49,781 : Computing embedding for test
2019-02-15 01:30:11,029 : Computed test embeddings
2019-02-15 01:30:11,045 : prepare data
2019-02-15 01:30:11,110 : start epoch
2019-02-15 01:30:52,118 : samples : 64000
2019-02-15 01:31:02,037 : Image to text: 7.84, 23.32, 34.36, 22.0
2019-02-15 01:31:09,270 : Text to Image: 6.292, 19.784, 30.516, 26.0
2019-02-15 01:31:50,356 : samples : 128000
2019-02-15 01:32:00,312 : Image to text: 9.22, 25.02, 36.36, 20.0
2019-02-15 01:32:07,545 : Text to Image: 6.872, 21.708, 32.688, 23.0
2019-02-15 01:32:48,913 : samples : 192000
2019-02-15 01:32:58,834 : Image to text: 10.64, 27.78, 39.8, 17.0
2019-02-15 01:33:06,070 : Text to Image: 7.896, 23.072, 34.56, 21.0
2019-02-15 01:33:46,549 : samples : 256000
2019-02-15 01:33:56,518 : Image to text: 9.74, 27.18, 39.6, 17.0
2019-02-15 01:34:03,750 : Text to Image: 7.888, 23.892, 35.42, 21.0
2019-02-15 01:34:44,715 : samples : 320000
2019-02-15 01:34:54,731 : Image to text: 9.6, 26.48, 38.24, 18.0
2019-02-15 01:35:01,979 : Text to Image: 8.16, 24.644, 36.024, 20.0
2019-02-15 01:35:43,318 : samples : 384000
2019-02-15 01:35:53,306 : Image to text: 10.24, 29.24, 41.7, 16.0
2019-02-15 01:36:00,561 : Text to Image: 8.592, 25.488, 37.144, 19.0
2019-02-15 01:36:46,268 : samples : 448000
2019-02-15 01:36:56,207 : Image to text: 9.9, 28.44, 40.96, 16.0
2019-02-15 01:37:03,460 : Text to Image: 8.324, 24.672, 36.888, 19.0
2019-02-15 01:37:44,663 : samples : 512000
2019-02-15 01:37:54,653 : Image to text: 11.42, 29.24, 42.26, 15.0
2019-02-15 01:38:01,931 : Text to Image: 9.08, 26.04, 38.112, 18.0
2019-02-15 01:38:36,830 : Epoch 1 finished
2019-02-15 01:38:37,303 : Image to text: 27.9, 60.4, 74.8, 4.0
2019-02-15 01:38:37,633 : Text to Image: 23.9, 57.12, 73.2, 4.0
2019-02-15 01:38:38,103 : Image to text: 26.9, 60.6, 74.7, 4.0
2019-02-15 01:38:38,432 : Text to Image: 22.52, 55.5, 72.54, 5.0
2019-02-15 01:38:38,903 : Image to text: 29.4, 62.1, 76.9, 3.0
2019-02-15 01:38:39,233 : Text to Image: 22.88, 56.66, 73.44, 4.0
2019-02-15 01:38:39,707 : Image to text: 29.3, 61.6, 76.0, 3.0
2019-02-15 01:38:40,036 : Text to Image: 23.86, 56.08, 73.22, 4.0
2019-02-15 01:38:40,506 : Image to text: 27.1, 61.1, 76.5, 3.0
2019-02-15 01:38:40,858 : Text to Image: 23.26, 57.8, 72.78, 4.0
2019-02-15 01:38:40,858 : Dev mean Text to Image: 23.284, 56.632000000000005, 73.036, 4.2
2019-02-15 01:38:40,858 : Dev mean Image to text: 28.119999999999997, 61.160000000000004, 75.78, 3.4000000000000004
2019-02-15 01:38:40,858 : start epoch
2019-02-15 01:39:22,530 : samples : 64000
2019-02-15 01:39:32,512 : Image to text: 10.02, 29.9, 42.32, 15.0
2019-02-15 01:39:39,750 : Text to Image: 8.824, 25.856, 38.016, 18.0
2019-02-15 01:40:21,236 : samples : 128000
2019-02-15 01:40:31,222 : Image to text: 10.24, 29.3, 41.94, 15.0
2019-02-15 01:40:38,468 : Text to Image: 8.772, 25.944, 37.916, 18.0
2019-02-15 01:41:19,843 : samples : 192000
2019-02-15 01:41:29,843 : Image to text: 11.26, 30.24, 43.0, 14.0
2019-02-15 01:41:37,099 : Text to Image: 9.756, 27.304, 39.516, 17.0
2019-02-15 01:42:18,319 : samples : 256000
2019-02-15 01:42:28,350 : Image to text: 12.46, 31.06, 43.78, 14.0
2019-02-15 01:42:35,620 : Text to Image: 9.716, 27.844, 39.96, 17.0
2019-02-15 01:43:16,877 : samples : 320000
2019-02-15 01:43:26,922 : Image to text: 12.16, 31.62, 44.98, 13.0
2019-02-15 01:43:34,206 : Text to Image: 9.9, 28.132, 40.572, 16.0
2019-02-15 01:44:15,635 : samples : 384000
2019-02-15 01:44:25,643 : Image to text: 11.76, 31.44, 44.08, 14.0
2019-02-15 01:44:32,911 : Text to Image: 9.608, 27.448, 40.14, 16.0
2019-02-15 01:45:14,067 : samples : 448000
2019-02-15 01:45:24,064 : Image to text: 12.08, 32.28, 45.3, 13.0
2019-02-15 01:45:31,334 : Text to Image: 10.152, 28.22, 40.492, 16.0
2019-02-15 01:46:10,456 : samples : 512000
2019-02-15 01:46:20,534 : Image to text: 11.82, 30.94, 44.58, 14.0
2019-02-15 01:46:27,768 : Text to Image: 9.928, 27.804, 40.228, 16.0
2019-02-15 01:47:01,072 : Epoch 2 finished
2019-02-15 01:47:01,537 : Image to text: 30.6, 63.5, 77.7, 3.0
2019-02-15 01:47:01,867 : Text to Image: 25.08, 59.62, 76.16, 4.0
2019-02-15 01:47:02,337 : Image to text: 28.3, 62.5, 76.8, 4.0
2019-02-15 01:47:02,667 : Text to Image: 24.3, 57.3, 75.14, 4.0
2019-02-15 01:47:03,134 : Image to text: 29.6, 63.2, 78.2, 3.0
2019-02-15 01:47:03,463 : Text to Image: 25.22, 58.58, 75.56, 4.0
2019-02-15 01:47:03,936 : Image to text: 30.5, 62.9, 77.0, 3.0
2019-02-15 01:47:04,266 : Text to Image: 25.22, 58.34, 74.84, 4.0
2019-02-15 01:47:04,734 : Image to text: 30.3, 64.3, 76.8, 3.0
2019-02-15 01:47:05,062 : Text to Image: 25.32, 58.84, 74.72, 4.0
2019-02-15 01:47:05,062 : Dev mean Text to Image: 25.028000000000002, 58.536, 75.284, 4.0
2019-02-15 01:47:05,062 : Dev mean Image to text: 29.860000000000007, 63.28, 77.3, 3.2
2019-02-15 01:47:05,063 : start epoch
2019-02-15 01:47:44,163 : samples : 64000
2019-02-15 01:47:54,164 : Image to text: 12.62, 32.26, 45.52, 13.0
2019-02-15 01:48:01,433 : Text to Image: 10.084, 28.08, 40.704, 16.0
2019-02-15 01:48:40,507 : samples : 128000
2019-02-15 01:48:50,481 : Image to text: 12.08, 32.36, 45.16, 13.0
2019-02-15 01:48:57,728 : Text to Image: 10.212, 28.412, 40.728, 16.0
2019-02-15 01:49:36,724 : samples : 192000
2019-02-15 01:49:46,687 : Image to text: 12.46, 33.18, 45.84, 13.0
2019-02-15 01:49:53,939 : Text to Image: 10.268, 28.6, 41.476, 15.0
2019-02-15 01:50:33,050 : samples : 256000
2019-02-15 01:50:43,002 : Image to text: 13.52, 33.58, 46.62, 12.0
2019-02-15 01:50:50,244 : Text to Image: 10.632, 29.208, 41.664, 15.0
2019-02-15 01:51:30,786 : samples : 320000
2019-02-15 01:51:40,734 : Image to text: 12.38, 32.08, 44.42, 14.0
2019-02-15 01:51:47,994 : Text to Image: 9.588, 27.768, 40.064, 16.0
2019-02-15 01:52:28,205 : samples : 384000
2019-02-15 01:52:38,111 : Image to text: 12.4, 31.84, 45.24, 13.0
2019-02-15 01:52:45,316 : Text to Image: 10.336, 28.736, 41.14, 16.0
2019-02-15 01:53:26,469 : samples : 448000
2019-02-15 01:53:36,459 : Image to text: 13.2, 34.12, 47.98, 11.0
2019-02-15 01:53:43,690 : Text to Image: 10.696, 29.808, 42.44, 15.0
2019-02-15 01:54:22,735 : samples : 512000
2019-02-15 01:54:32,683 : Image to text: 13.18, 34.8, 47.64, 12.0
2019-02-15 01:54:39,926 : Text to Image: 10.624, 29.912, 42.496, 15.0
2019-02-15 01:55:13,499 : Epoch 3 finished
2019-02-15 01:55:13,972 : Image to text: 31.3, 64.9, 79.8, 3.0
2019-02-15 01:55:14,302 : Text to Image: 25.44, 61.18, 77.32, 4.0
2019-02-15 01:55:14,769 : Image to text: 31.1, 64.9, 79.5, 3.0
2019-02-15 01:55:15,104 : Text to Image: 25.28, 59.68, 75.76, 4.0
2019-02-15 01:55:15,554 : Image to text: 31.0, 64.9, 81.2, 3.0
2019-02-15 01:55:15,890 : Text to Image: 26.24, 61.12, 76.64, 4.0
2019-02-15 01:55:16,343 : Image to text: 32.6, 66.7, 80.3, 3.0
2019-02-15 01:55:16,676 : Text to Image: 26.9, 61.18, 76.74, 4.0
2019-02-15 01:55:17,141 : Image to text: 33.3, 67.3, 81.1, 3.0
2019-02-15 01:55:17,472 : Text to Image: 26.46, 60.48, 76.12, 4.0
2019-02-15 01:55:17,472 : Dev mean Text to Image: 26.064, 60.72800000000001, 76.516, 4.0
2019-02-15 01:55:17,472 : Dev mean Image to text: 31.86, 65.74, 80.38, 3.0
2019-02-15 01:55:17,472 : start epoch
2019-02-15 01:55:58,912 : samples : 64000
2019-02-15 01:56:08,749 : Image to text: 13.1, 34.38, 46.96, 12.0
2019-02-15 01:56:15,325 : Text to Image: 10.488, 29.296, 41.996, 15.0
2019-02-15 01:56:57,343 : samples : 128000
2019-02-15 01:57:09,871 : Image to text: 13.74, 34.12, 47.52, 12.0
2019-02-15 01:57:19,832 : Text to Image: 10.704, 29.668, 41.964, 15.0
2019-02-15 01:58:01,722 : samples : 192000
2019-02-15 01:58:11,733 : Image to text: 12.26, 32.58, 46.72, 12.0
2019-02-15 01:58:18,890 : Text to Image: 10.272, 28.64, 41.164, 16.0
2019-02-15 01:58:59,561 : samples : 256000
2019-02-15 01:59:11,969 : Image to text: 12.74, 32.94, 47.06, 12.0
2019-02-15 01:59:21,893 : Text to Image: 10.544, 29.756, 42.576, 15.0
2019-02-15 02:00:05,012 : samples : 320000
2019-02-15 02:00:16,638 : Image to text: 13.2, 34.1, 47.42, 12.0
2019-02-15 02:00:23,703 : Text to Image: 10.664, 29.592, 42.32, 15.0
2019-02-15 02:01:03,244 : samples : 384000
2019-02-15 02:01:15,720 : Image to text: 13.16, 34.36, 46.84, 12.0
2019-02-15 02:01:25,642 : Text to Image: 10.752, 29.396, 42.164, 15.0
2019-02-15 02:02:08,804 : samples : 448000
2019-02-15 02:02:21,315 : Image to text: 13.46, 34.9, 48.7, 11.0
2019-02-15 02:02:29,094 : Text to Image: 11.172, 29.988, 42.7, 14.0
2019-02-15 02:03:09,572 : samples : 512000
2019-02-15 02:03:21,984 : Image to text: 13.32, 34.24, 47.04, 12.0
2019-02-15 02:03:31,899 : Text to Image: 10.436, 29.64, 42.548, 15.0
2019-02-15 02:04:08,436 : Epoch 4 finished
2019-02-15 02:04:09,341 : Image to text: 32.4, 68.0, 81.4, 3.0
2019-02-15 02:04:10,077 : Text to Image: 26.44, 62.5, 78.12, 4.0
2019-02-15 02:04:10,999 : Image to text: 31.0, 65.5, 78.8, 3.0
2019-02-15 02:04:11,770 : Text to Image: 26.74, 60.42, 76.42, 4.0
2019-02-15 02:04:12,694 : Image to text: 30.7, 66.6, 80.3, 3.0
2019-02-15 02:04:13,433 : Text to Image: 27.24, 61.28, 77.46, 4.0
2019-02-15 02:04:14,325 : Image to text: 34.2, 66.7, 79.3, 3.0
2019-02-15 02:04:15,062 : Text to Image: 26.98, 61.1, 77.46, 4.0
2019-02-15 02:04:16,005 : Image to text: 34.7, 66.9, 80.9, 3.0
2019-02-15 02:04:16,747 : Text to Image: 26.58, 61.1, 76.84, 4.0
2019-02-15 02:04:16,747 : Dev mean Text to Image: 26.796, 61.28, 77.25999999999999, 4.0
2019-02-15 02:04:16,747 : Dev mean Image to text: 32.6, 66.74, 80.13999999999999, 3.0
2019-02-15 02:04:16,748 : start epoch
2019-02-15 02:04:57,065 : samples : 64000
2019-02-15 02:05:06,969 : Image to text: 14.2, 35.64, 48.52, 11.0
2019-02-15 02:05:15,038 : Text to Image: 11.244, 30.908, 43.76, 14.0
2019-02-15 02:05:56,556 : samples : 128000
2019-02-15 02:06:09,075 : Image to text: 12.82, 34.58, 48.26, 11.0
2019-02-15 02:06:19,002 : Text to Image: 10.74, 30.028, 42.992, 15.0
2019-02-15 02:06:59,912 : samples : 192000
2019-02-15 02:07:09,921 : Image to text: 12.34, 34.28, 48.02, 12.0
2019-02-15 02:07:17,112 : Text to Image: 10.624, 30.084, 42.82, 14.0
2019-02-15 02:07:58,391 : samples : 256000
2019-02-15 02:08:10,885 : Image to text: 13.08, 34.64, 48.62, 11.0
2019-02-15 02:08:20,838 : Text to Image: 11.12, 30.388, 43.164, 14.0
2019-02-15 02:09:02,951 : samples : 320000
2019-02-15 02:09:12,980 : Image to text: 13.64, 35.76, 48.58, 11.0
2019-02-15 02:09:20,148 : Text to Image: 11.136, 30.62, 43.6, 14.0
2019-02-15 02:10:00,809 : samples : 384000
2019-02-15 02:10:13,266 : Image to text: 14.46, 34.52, 47.78, 12.0
2019-02-15 02:10:23,194 : Text to Image: 11.1, 30.552, 43.712, 14.0
2019-02-15 02:11:05,575 : samples : 448000
2019-02-15 02:11:15,616 : Image to text: 13.18, 34.78, 48.94, 11.0
2019-02-15 02:11:22,797 : Text to Image: 10.728, 30.276, 42.78, 15.0
2019-02-15 02:12:03,621 : samples : 512000
2019-02-15 02:12:16,046 : Image to text: 13.88, 35.4, 48.5, 11.0
2019-02-15 02:12:25,927 : Text to Image: 11.344, 30.768, 43.584, 14.0
2019-02-15 02:13:02,747 : Epoch 5 finished
2019-02-15 02:13:03,665 : Image to text: 32.2, 66.7, 80.1, 3.0
2019-02-15 02:13:04,421 : Text to Image: 26.64, 61.3, 77.86, 4.0
2019-02-15 02:13:05,361 : Image to text: 32.4, 65.8, 79.2, 3.0
2019-02-15 02:13:06,139 : Text to Image: 26.14, 59.72, 76.52, 4.0
2019-02-15 02:13:07,081 : Image to text: 31.8, 67.1, 81.8, 3.0
2019-02-15 02:13:07,839 : Text to Image: 26.56, 61.56, 77.08, 4.0
2019-02-15 02:13:08,814 : Image to text: 33.2, 66.7, 81.6, 3.0
2019-02-15 02:13:09,295 : Text to Image: 26.5, 61.52, 77.32, 4.0
2019-02-15 02:13:10,194 : Image to text: 34.3, 66.6, 80.3, 3.0
2019-02-15 02:13:10,931 : Text to Image: 26.68, 61.36, 76.78, 4.0
2019-02-15 02:13:10,932 : Dev mean Text to Image: 26.503999999999998, 61.092000000000006, 77.112, 4.0
2019-02-15 02:13:10,932 : Dev mean Image to text: 32.78, 66.58, 80.6, 3.0
2019-02-15 02:13:10,932 : start epoch
2019-02-15 02:13:51,560 : samples : 64000
2019-02-15 02:14:03,145 : Image to text: 13.92, 35.56, 48.8, 11.0
2019-02-15 02:14:13,028 : Text to Image: 11.248, 31.264, 43.732, 14.0
2019-02-15 02:14:56,290 : samples : 128000
2019-02-15 02:15:08,806 : Image to text: 13.44, 35.24, 48.24, 11.0
2019-02-15 02:15:18,756 : Text to Image: 11.092, 30.22, 43.288, 14.0
2019-02-15 02:15:59,494 : samples : 192000
2019-02-15 02:16:09,511 : Image to text: 13.62, 36.46, 49.7, 11.0
2019-02-15 02:16:16,667 : Text to Image: 11.096, 30.864, 43.492, 14.0
2019-02-15 02:16:57,280 : samples : 256000
2019-02-15 02:17:09,787 : Image to text: 14.14, 35.82, 48.5, 11.0
2019-02-15 02:17:19,760 : Text to Image: 11.072, 30.636, 43.832, 14.0
2019-02-15 02:18:02,396 : samples : 320000
2019-02-15 02:18:12,305 : Image to text: 13.42, 36.2, 49.76, 11.0
2019-02-15 02:18:19,375 : Text to Image: 11.608, 30.656, 43.688, 14.0
2019-02-15 02:18:59,536 : samples : 384000
2019-02-15 02:19:10,820 : Image to text: 14.4, 36.38, 48.78, 11.0
2019-02-15 02:19:18,789 : Text to Image: 11.524, 31.036, 43.852, 14.0
2019-02-15 02:19:59,190 : samples : 448000
2019-02-15 02:20:09,200 : Image to text: 13.38, 35.9, 49.44, 11.0
2019-02-15 02:20:16,381 : Text to Image: 11.124, 30.992, 43.632, 14.0
2019-02-15 02:20:55,995 : samples : 512000
2019-02-15 02:21:05,713 : Image to text: 14.04, 34.94, 48.28, 11.0
2019-02-15 02:21:13,801 : Text to Image: 10.8, 30.712, 43.568, 14.0
2019-02-15 02:21:47,580 : Epoch 6 finished
2019-02-15 02:21:48,033 : Image to text: 35.6, 68.3, 80.5, 3.0
2019-02-15 02:21:48,392 : Text to Image: 27.9, 62.98, 78.88, 3.0
2019-02-15 02:21:48,840 : Image to text: 33.6, 68.6, 79.8, 3.0
2019-02-15 02:21:49,199 : Text to Image: 27.44, 62.04, 77.36, 4.0
2019-02-15 02:21:49,643 : Image to text: 33.1, 67.5, 81.0, 3.0
2019-02-15 02:21:50,003 : Text to Image: 27.14, 61.72, 77.82, 4.0
2019-02-15 02:21:50,451 : Image to text: 34.8, 68.6, 80.4, 3.0
2019-02-15 02:21:50,810 : Text to Image: 28.42, 62.66, 77.64, 3.0
2019-02-15 02:21:51,256 : Image to text: 35.3, 67.6, 82.3, 2.0
2019-02-15 02:21:51,617 : Text to Image: 28.14, 61.74, 77.14, 3.0
2019-02-15 02:21:51,617 : Dev mean Text to Image: 27.808000000000003, 62.227999999999994, 77.768, 3.4000000000000004
2019-02-15 02:21:51,617 : Dev mean Image to text: 34.480000000000004, 68.11999999999999, 80.80000000000001, 2.8
2019-02-15 02:21:51,617 : start epoch
2019-02-15 02:22:32,603 : samples : 64000
2019-02-15 02:22:42,533 : Image to text: 14.68, 36.76, 49.94, 11.0
2019-02-15 02:22:49,737 : Text to Image: 11.324, 31.012, 43.856, 14.0
2019-02-15 02:23:29,980 : samples : 128000
2019-02-15 02:23:41,582 : Image to text: 14.1, 36.28, 49.72, 11.0
2019-02-15 02:23:51,636 : Text to Image: 11.708, 31.408, 44.608, 13.0
2019-02-15 02:24:33,731 : samples : 192000
2019-02-15 02:24:43,728 : Image to text: 14.18, 36.56, 50.3, 10.0
2019-02-15 02:24:50,901 : Text to Image: 11.772, 31.512, 44.544, 13.0
2019-02-15 02:25:31,695 : samples : 256000
2019-02-15 02:25:44,207 : Image to text: 13.98, 35.3, 48.44, 11.0
2019-02-15 02:25:52,080 : Text to Image: 11.188, 31.212, 44.2, 14.0
2019-02-15 02:26:32,866 : samples : 320000
2019-02-15 02:26:42,803 : Image to text: 13.64, 36.02, 49.1, 11.0
2019-02-15 02:26:49,641 : Text to Image: 11.572, 31.328, 44.672, 14.0
2019-02-15 02:27:30,247 : samples : 384000
2019-02-15 02:27:41,548 : Image to text: 13.3, 36.24, 49.78, 11.0
2019-02-15 02:27:51,380 : Text to Image: 11.324, 31.5, 44.476, 14.0
2019-02-15 02:28:33,540 : samples : 448000
2019-02-15 02:28:45,940 : Image to text: 13.56, 36.72, 49.94, 11.0
2019-02-15 02:28:55,724 : Text to Image: 11.532, 31.64, 44.82, 13.0
2019-02-15 02:29:38,275 : samples : 512000
2019-02-15 02:29:50,698 : Image to text: 15.18, 36.82, 50.1, 10.0
2019-02-15 02:30:00,500 : Text to Image: 11.884, 31.98, 44.776, 13.0
2019-02-15 02:30:36,619 : Epoch 7 finished
2019-02-15 02:30:37,524 : Image to text: 33.5, 68.8, 80.6, 3.0
2019-02-15 02:30:38,243 : Text to Image: 28.16, 62.68, 78.84, 3.0
2019-02-15 02:30:39,118 : Image to text: 30.1, 66.7, 81.2, 3.0
2019-02-15 02:30:39,836 : Text to Image: 27.48, 61.92, 77.88, 4.0
2019-02-15 02:30:40,698 : Image to text: 33.0, 67.9, 82.2, 3.0
2019-02-15 02:30:41,435 : Text to Image: 27.52, 63.84, 79.0, 3.0
2019-02-15 02:30:42,292 : Image to text: 34.4, 68.7, 81.0, 2.0
2019-02-15 02:30:42,999 : Text to Image: 28.0, 63.28, 79.3, 3.0
2019-02-15 02:30:43,900 : Image to text: 33.4, 68.1, 80.5, 3.0
2019-02-15 02:30:44,604 : Text to Image: 28.18, 63.2, 77.96, 3.0
2019-02-15 02:30:44,604 : Dev mean Text to Image: 27.868, 62.984, 78.596, 3.2
2019-02-15 02:30:44,604 : Dev mean Image to text: 32.879999999999995, 68.04, 81.1, 2.8
2019-02-15 02:30:44,604 : start epoch
2019-02-15 02:31:26,866 : samples : 64000
2019-02-15 02:31:39,339 : Image to text: 13.88, 36.72, 50.36, 10.0
2019-02-15 02:31:49,261 : Text to Image: 11.612, 31.488, 44.552, 14.0
2019-02-15 02:32:31,945 : samples : 128000
2019-02-15 02:32:44,397 : Image to text: 14.32, 37.22, 49.6, 11.0
2019-02-15 02:32:54,326 : Text to Image: 11.424, 31.484, 44.748, 13.0
2019-02-15 02:33:35,774 : samples : 192000
2019-02-15 02:33:48,295 : Image to text: 13.52, 36.2, 50.18, 10.0
2019-02-15 02:33:58,269 : Text to Image: 11.644, 31.756, 45.048, 13.0
2019-02-15 02:34:38,680 : samples : 256000
2019-02-15 02:34:51,171 : Image to text: 14.48, 36.44, 50.16, 10.0
2019-02-15 02:35:01,119 : Text to Image: 11.46, 31.168, 44.356, 13.0
2019-02-15 02:35:41,464 : samples : 320000
2019-02-15 02:35:53,988 : Image to text: 14.74, 37.2, 50.28, 10.0
2019-02-15 02:36:03,962 : Text to Image: 11.748, 32.476, 45.308, 13.0
2019-02-15 02:36:44,939 : samples : 384000
2019-02-15 02:36:57,471 : Image to text: 14.14, 36.66, 50.44, 10.0
2019-02-15 02:37:07,425 : Text to Image: 11.672, 31.848, 44.768, 13.0
2019-02-15 02:37:49,259 : samples : 448000
2019-02-15 02:38:01,813 : Image to text: 14.2, 36.8, 50.06, 10.0
2019-02-15 02:38:11,903 : Text to Image: 11.328, 31.752, 44.6, 13.0
2019-02-15 02:38:54,829 : samples : 512000
2019-02-15 02:39:06,799 : Image to text: 14.36, 36.02, 50.44, 10.0
2019-02-15 02:39:14,043 : Text to Image: 11.932, 32.024, 45.292, 13.0
2019-02-15 02:39:48,493 : Epoch 8 finished
2019-02-15 02:39:48,941 : Image to text: 35.2, 67.1, 81.0, 3.0
2019-02-15 02:39:49,317 : Text to Image: 28.26, 63.44, 79.0, 3.0
2019-02-15 02:39:49,763 : Image to text: 34.8, 67.9, 81.3, 3.0
2019-02-15 02:39:50,129 : Text to Image: 27.48, 62.12, 78.52, 3.0
2019-02-15 02:39:50,581 : Image to text: 33.8, 67.4, 80.9, 3.0
2019-02-15 02:39:50,958 : Text to Image: 27.78, 63.52, 79.02, 3.0
2019-02-15 02:39:51,411 : Image to text: 33.0, 67.9, 81.1, 3.0
2019-02-15 02:39:51,788 : Text to Image: 28.72, 63.48, 78.84, 3.0
2019-02-15 02:39:52,238 : Image to text: 34.0, 68.7, 81.3, 3.0
2019-02-15 02:39:52,615 : Text to Image: 28.24, 62.64, 78.12, 3.0
2019-02-15 02:39:52,616 : Dev mean Text to Image: 28.096, 63.04, 78.69999999999999, 3.0
2019-02-15 02:39:52,616 : Dev mean Image to text: 34.16, 67.8, 81.11999999999998, 3.0
2019-02-15 02:39:52,616 : start epoch
2019-02-15 02:40:32,919 : samples : 64000
2019-02-15 02:40:42,886 : Image to text: 15.34, 37.06, 50.96, 10.0
2019-02-15 02:40:50,108 : Text to Image: 11.764, 31.796, 44.416, 13.0
2019-02-15 02:41:31,335 : samples : 128000
2019-02-15 02:41:44,113 : Image to text: 14.38, 37.3, 50.84, 10.0
2019-02-15 02:41:54,466 : Text to Image: 11.556, 31.704, 45.108, 13.0
2019-02-15 02:42:37,594 : samples : 192000
2019-02-15 02:42:50,461 : Image to text: 13.56, 36.42, 50.22, 10.0
2019-02-15 02:43:00,689 : Text to Image: 11.536, 31.568, 44.72, 13.0
2019-02-15 02:43:44,427 : samples : 256000
2019-02-15 02:43:57,197 : Image to text: 14.7, 37.6, 50.72, 10.0
2019-02-15 02:44:07,534 : Text to Image: 11.928, 32.204, 45.244, 13.0
2019-02-15 02:44:51,020 : samples : 320000
2019-02-15 02:45:03,876 : Image to text: 15.38, 37.1, 50.08, 10.0
2019-02-15 02:45:14,293 : Text to Image: 11.88, 32.224, 45.32, 13.0
2019-02-15 02:45:58,150 : samples : 384000
2019-02-15 02:46:10,959 : Image to text: 14.62, 36.84, 50.72, 10.0
2019-02-15 02:46:21,391 : Text to Image: 11.888, 32.088, 45.408, 13.0
2019-02-15 02:47:04,978 : samples : 448000
2019-02-15 02:47:17,771 : Image to text: 15.26, 37.58, 51.24, 10.0
2019-02-15 02:47:28,022 : Text to Image: 11.828, 31.944, 45.028, 13.0
2019-02-15 02:48:11,324 : samples : 512000
2019-02-15 02:48:24,165 : Image to text: 14.34, 37.7, 50.66, 10.0
2019-02-15 02:48:34,583 : Text to Image: 11.788, 32.38, 45.34, 13.0
2019-02-15 02:49:11,715 : Epoch 9 finished
2019-02-15 02:49:12,851 : Image to text: 34.7, 67.3, 81.2, 3.0
2019-02-15 02:49:13,725 : Text to Image: 29.04, 63.52, 79.86, 3.0
2019-02-15 02:49:14,775 : Image to text: 33.7, 68.3, 80.8, 3.0
2019-02-15 02:49:15,726 : Text to Image: 27.88, 62.66, 78.56, 3.0
2019-02-15 02:49:16,755 : Image to text: 33.9, 67.2, 81.3, 3.0
2019-02-15 02:49:17,671 : Text to Image: 28.7, 64.08, 79.28, 3.0
2019-02-15 02:49:18,774 : Image to text: 34.4, 69.2, 80.9, 3.0
2019-02-15 02:49:19,636 : Text to Image: 28.56, 63.66, 79.5, 3.0
2019-02-15 02:49:20,336 : Image to text: 35.9, 68.2, 81.7, 3.0
2019-02-15 02:49:20,653 : Text to Image: 28.86, 63.36, 78.36, 3.0
2019-02-15 02:49:20,653 : Dev mean Text to Image: 28.608000000000004, 63.456, 79.112, 3.0
2019-02-15 02:49:20,653 : Dev mean Image to text: 34.519999999999996, 68.04, 81.18, 3.0
2019-02-15 02:49:20,654 : start epoch
2019-02-15 02:49:59,574 : samples : 64000
2019-02-15 02:50:09,376 : Image to text: 14.66, 37.18, 50.86, 10.0
2019-02-15 02:50:16,446 : Text to Image: 11.92, 32.368, 45.652, 13.0
2019-02-15 02:50:55,295 : samples : 128000
2019-02-15 02:51:05,057 : Image to text: 15.18, 37.86, 51.78, 10.0
2019-02-15 02:51:12,071 : Text to Image: 12.236, 32.496, 45.732, 13.0
2019-02-15 02:51:50,928 : samples : 192000
2019-02-15 02:52:00,718 : Image to text: 15.44, 38.44, 51.96, 10.0
2019-02-15 02:52:07,719 : Text to Image: 12.284, 32.32, 45.604, 13.0
2019-02-15 02:52:46,584 : samples : 256000
2019-02-15 02:52:56,384 : Image to text: 14.88, 38.54, 51.76, 10.0
2019-02-15 02:53:03,490 : Text to Image: 11.896, 32.204, 45.364, 13.0
2019-02-15 02:53:44,831 : samples : 320000
2019-02-15 02:53:54,596 : Image to text: 14.64, 37.28, 51.36, 10.0
2019-02-15 02:54:01,631 : Text to Image: 11.748, 32.5, 45.404, 13.0
2019-02-15 02:54:43,476 : samples : 384000
2019-02-15 02:54:53,229 : Image to text: 15.78, 38.1, 51.66, 10.0
2019-02-15 02:55:00,240 : Text to Image: 11.852, 32.384, 45.328, 13.0
2019-02-15 02:55:40,886 : samples : 448000
2019-02-15 02:55:50,661 : Image to text: 14.86, 38.02, 51.9, 10.0
2019-02-15 02:55:57,678 : Text to Image: 12.056, 32.328, 45.404, 13.0
2019-02-15 02:56:37,579 : samples : 512000
2019-02-15 02:56:47,351 : Image to text: 14.26, 36.7, 50.54, 10.0
2019-02-15 02:56:54,354 : Text to Image: 11.672, 32.056, 45.012, 13.0
2019-02-15 02:57:33,140 : Epoch 10 finished
2019-02-15 02:57:33,587 : Image to text: 34.1, 66.9, 82.0, 3.0
2019-02-15 02:57:34,017 : Text to Image: 28.74, 64.2, 80.08, 3.0
2019-02-15 02:57:34,456 : Image to text: 32.5, 67.6, 81.9, 3.0
2019-02-15 02:57:34,886 : Text to Image: 27.48, 62.36, 78.58, 3.0
2019-02-15 02:57:35,325 : Image to text: 34.2, 69.9, 82.2, 3.0
2019-02-15 02:57:35,755 : Text to Image: 28.78, 63.56, 79.4, 3.0
2019-02-15 02:57:36,194 : Image to text: 33.5, 69.7, 82.0, 3.0
2019-02-15 02:57:36,624 : Text to Image: 28.6, 64.38, 79.2, 3.0
2019-02-15 02:57:37,063 : Image to text: 34.0, 68.9, 82.1, 3.0
2019-02-15 02:57:37,371 : Text to Image: 28.68, 63.02, 78.5, 3.0
2019-02-15 02:57:37,372 : Dev mean Text to Image: 28.456, 63.504, 79.152, 3.0
2019-02-15 02:57:37,372 : Dev mean Image to text: 33.66, 68.6, 82.04, 3.0
2019-02-15 02:57:37,372 : start epoch
2019-02-15 02:58:16,560 : samples : 64000
2019-02-15 02:58:26,274 : Image to text: 14.84, 38.04, 51.4, 10.0
2019-02-15 02:58:33,284 : Text to Image: 11.964, 32.292, 45.44, 13.0
2019-02-15 02:59:14,667 : samples : 128000
2019-02-15 02:59:24,390 : Image to text: 15.16, 38.12, 51.3, 10.0
2019-02-15 02:59:31,426 : Text to Image: 11.732, 32.736, 46.128, 13.0
2019-02-15 03:00:13,931 : samples : 192000
2019-02-15 03:00:23,733 : Image to text: 14.64, 38.56, 51.9, 10.0
2019-02-15 03:00:30,752 : Text to Image: 12.004, 32.392, 45.412, 13.0
2019-02-15 03:01:16,967 : samples : 256000
2019-02-15 03:01:26,760 : Image to text: 15.26, 38.18, 52.0, 10.0
2019-02-15 03:01:33,841 : Text to Image: 11.9, 32.372, 45.424, 13.0
2019-02-15 03:02:15,381 : samples : 320000
2019-02-15 03:02:25,140 : Image to text: 14.66, 36.86, 50.48, 10.0
2019-02-15 03:02:32,150 : Text to Image: 11.76, 31.992, 45.176, 13.0
2019-02-15 03:03:12,672 : samples : 384000
2019-02-15 03:03:22,416 : Image to text: 14.76, 37.62, 51.5, 10.0
2019-02-15 03:03:29,445 : Text to Image: 12.116, 33.056, 46.504, 12.0
2019-02-15 03:04:09,665 : samples : 448000
2019-02-15 03:04:19,404 : Image to text: 15.3, 37.88, 51.88, 10.0
2019-02-15 03:04:26,398 : Text to Image: 12.144, 33.044, 45.808, 13.0
2019-02-15 03:05:09,686 : samples : 512000
2019-02-15 03:05:19,483 : Image to text: 15.48, 38.06, 51.5, 10.0
2019-02-15 03:05:26,475 : Text to Image: 12.228, 32.864, 46.276, 12.0
2019-02-15 03:06:06,625 : Epoch 11 finished
2019-02-15 03:06:07,073 : Image to text: 33.6, 68.5, 81.3, 3.0
2019-02-15 03:06:07,459 : Text to Image: 28.92, 63.4, 79.1, 3.0
2019-02-15 03:06:07,898 : Image to text: 33.5, 69.3, 82.4, 3.0
2019-02-15 03:06:08,284 : Text to Image: 28.1, 63.08, 78.66, 3.0
2019-02-15 03:06:08,722 : Image to text: 34.8, 68.9, 82.0, 3.0
2019-02-15 03:06:09,108 : Text to Image: 28.1, 63.96, 79.56, 3.0
2019-02-15 03:06:09,547 : Image to text: 35.7, 70.1, 82.3, 2.0
2019-02-15 03:06:09,933 : Text to Image: 28.36, 64.1, 79.38, 3.0
2019-02-15 03:06:10,372 : Image to text: 36.3, 68.1, 82.0, 3.0
2019-02-15 03:06:10,708 : Text to Image: 28.8, 62.94, 78.22, 3.0
2019-02-15 03:06:10,708 : Dev mean Text to Image: 28.456000000000003, 63.496, 78.984, 3.0
2019-02-15 03:06:10,709 : Dev mean Image to text: 34.78, 68.98, 82.0, 2.8
2019-02-15 03:06:10,709 : start epoch
2019-02-15 03:06:49,711 : samples : 64000
2019-02-15 03:06:59,471 : Image to text: 15.5, 38.62, 51.66, 10.0
2019-02-15 03:07:06,502 : Text to Image: 12.276, 32.832, 46.196, 13.0
2019-02-15 03:07:45,539 : samples : 128000
2019-02-15 03:07:55,304 : Image to text: 14.26, 37.34, 51.52, 10.0
2019-02-15 03:08:02,351 : Text to Image: 11.832, 32.592, 45.764, 13.0
2019-02-15 03:08:44,305 : samples : 192000
2019-02-15 03:08:54,083 : Image to text: 15.4, 37.94, 52.28, 9.0
2019-02-15 03:09:01,134 : Text to Image: 12.356, 33.144, 46.16, 12.0
2019-02-15 03:09:43,193 : samples : 256000
2019-02-15 03:09:52,990 : Image to text: 14.68, 37.56, 51.58, 10.0
2019-02-15 03:09:59,995 : Text to Image: 12.056, 32.316, 45.556, 13.0
2019-02-15 03:10:42,241 : samples : 320000
2019-02-15 03:10:51,995 : Image to text: 14.74, 37.24, 51.24, 10.0
2019-02-15 03:10:59,004 : Text to Image: 12.076, 32.6, 45.128, 13.0
2019-02-15 03:11:41,274 : samples : 384000
2019-02-15 03:11:51,040 : Image to text: 14.56, 38.04, 52.36, 9.0
2019-02-15 03:11:58,049 : Text to Image: 12.468, 33.0, 46.048, 12.0
2019-02-15 03:12:40,387 : samples : 448000
2019-02-15 03:12:50,199 : Image to text: 15.82, 38.8, 52.52, 9.0
2019-02-15 03:12:57,222 : Text to Image: 12.248, 32.692, 45.884, 13.0
2019-02-15 03:13:39,487 : samples : 512000
2019-02-15 03:13:49,264 : Image to text: 14.86, 38.24, 52.54, 9.0
2019-02-15 03:13:56,286 : Text to Image: 12.34, 32.852, 46.32, 13.0
2019-02-15 03:14:32,289 : Epoch 12 finished
2019-02-15 03:14:32,730 : Image to text: 34.0, 67.6, 81.4, 3.0
2019-02-15 03:14:33,133 : Text to Image: 28.68, 64.1, 80.14, 3.0
2019-02-15 03:14:33,575 : Image to text: 34.3, 68.7, 81.5, 3.0
2019-02-15 03:14:33,978 : Text to Image: 27.54, 63.3, 79.14, 3.0
2019-02-15 03:14:34,419 : Image to text: 34.2, 68.7, 83.2, 3.0
2019-02-15 03:14:34,822 : Text to Image: 28.8, 64.22, 79.58, 3.0
2019-02-15 03:14:35,262 : Image to text: 34.7, 69.9, 81.7, 3.0
2019-02-15 03:14:35,665 : Text to Image: 28.86, 64.04, 79.92, 3.0
2019-02-15 03:14:36,074 : Image to text: 35.5, 68.5, 82.2, 3.0
2019-02-15 03:14:36,389 : Text to Image: 29.04, 64.22, 78.84, 3.0
2019-02-15 03:14:36,389 : Dev mean Text to Image: 28.583999999999996, 63.976, 79.524, 3.0
2019-02-15 03:14:36,389 : Dev mean Image to text: 34.54, 68.68, 82.0, 3.0
2019-02-15 03:14:36,390 : start epoch
2019-02-15 03:15:18,324 : samples : 64000
2019-02-15 03:15:28,076 : Image to text: 15.22, 38.5, 52.54, 9.0
2019-02-15 03:15:35,218 : Text to Image: 12.216, 32.932, 46.1, 13.0
2019-02-15 03:16:17,526 : samples : 128000
2019-02-15 03:16:27,303 : Image to text: 14.42, 37.96, 51.86, 10.0
2019-02-15 03:16:34,339 : Text to Image: 12.068, 32.392, 45.332, 13.0
2019-02-15 03:17:16,553 : samples : 192000
2019-02-15 03:17:26,291 : Image to text: 15.18, 37.74, 51.44, 10.0
2019-02-15 03:17:33,324 : Text to Image: 11.952, 32.68, 46.212, 13.0
2019-02-15 03:18:15,565 : samples : 256000
2019-02-15 03:18:25,308 : Image to text: 16.18, 39.24, 53.62, 9.0
2019-02-15 03:18:32,376 : Text to Image: 12.396, 33.192, 46.124, 13.0
2019-02-15 03:19:14,217 : samples : 320000
2019-02-15 03:19:23,989 : Image to text: 15.4, 38.96, 52.56, 9.0
2019-02-15 03:19:31,013 : Text to Image: 12.268, 33.096, 46.58, 12.0
2019-02-15 03:20:13,042 : samples : 384000
2019-02-15 03:20:22,787 : Image to text: 15.22, 37.42, 51.32, 10.0
2019-02-15 03:20:29,793 : Text to Image: 12.044, 32.872, 46.172, 12.0
2019-02-15 03:21:12,866 : samples : 448000
2019-02-15 03:21:22,753 : Image to text: 15.18, 38.3, 52.18, 9.0
2019-02-15 03:21:29,784 : Text to Image: 12.216, 32.9, 46.028, 13.0
2019-02-15 03:22:11,992 : samples : 512000
2019-02-15 03:22:21,804 : Image to text: 15.72, 38.52, 51.5, 10.0
2019-02-15 03:22:28,814 : Text to Image: 12.056, 32.904, 45.908, 13.0
2019-02-15 03:23:02,232 : Epoch 13 finished
2019-02-15 03:23:02,673 : Image to text: 33.7, 69.1, 82.1, 3.0
2019-02-15 03:23:03,050 : Text to Image: 29.48, 64.58, 80.48, 3.0
2019-02-15 03:23:03,491 : Image to text: 35.2, 70.7, 83.3, 3.0
2019-02-15 03:23:03,868 : Text to Image: 27.96, 63.02, 79.46, 3.0
2019-02-15 03:23:04,309 : Image to text: 34.1, 68.2, 83.2, 3.0
2019-02-15 03:23:04,687 : Text to Image: 29.02, 64.74, 79.82, 3.0
2019-02-15 03:23:05,128 : Image to text: 34.5, 69.5, 82.5, 3.0
2019-02-15 03:23:05,505 : Text to Image: 29.12, 64.88, 80.0, 3.0
2019-02-15 03:23:05,910 : Image to text: 36.2, 69.4, 82.8, 3.0
2019-02-15 03:23:06,224 : Text to Image: 29.56, 64.32, 78.62, 3.0
2019-02-15 03:23:06,224 : Dev mean Text to Image: 29.028, 64.308, 79.676, 3.0
2019-02-15 03:23:06,225 : Dev mean Image to text: 34.74, 69.38, 82.78, 3.0
2019-02-15 03:23:06,225 : start epoch
2019-02-15 03:23:45,755 : samples : 64000
2019-02-15 03:23:55,497 : Image to text: 14.6, 38.5, 52.56, 9.0
2019-02-15 03:24:02,523 : Text to Image: 12.244, 33.208, 46.34, 12.0
2019-02-15 03:24:42,401 : samples : 128000
2019-02-15 03:24:52,210 : Image to text: 14.86, 38.1, 51.98, 10.0
2019-02-15 03:24:59,215 : Text to Image: 12.272, 32.62, 45.804, 13.0
2019-02-15 03:25:38,779 : samples : 192000
2019-02-15 03:25:48,575 : Image to text: 16.12, 38.92, 52.62, 9.0
2019-02-15 03:25:55,641 : Text to Image: 12.084, 32.632, 45.84, 13.0
2019-02-15 03:26:35,808 : samples : 256000
2019-02-15 03:26:45,571 : Image to text: 15.76, 38.28, 52.34, 10.0
2019-02-15 03:26:52,594 : Text to Image: 12.428, 33.272, 46.184, 12.0
2019-02-15 03:27:33,724 : samples : 320000
2019-02-15 03:27:43,458 : Image to text: 15.02, 38.58, 52.7, 9.0
2019-02-15 03:27:50,518 : Text to Image: 12.1, 32.916, 46.012, 13.0
2019-02-15 03:28:31,790 : samples : 384000
2019-02-15 03:28:41,522 : Image to text: 14.92, 38.88, 51.6, 10.0
2019-02-15 03:28:48,536 : Text to Image: 12.328, 33.192, 46.272, 12.0
2019-02-15 03:29:27,611 : samples : 448000
2019-02-15 03:29:37,398 : Image to text: 15.0, 38.18, 51.84, 10.0
2019-02-15 03:29:44,407 : Text to Image: 11.88, 32.408, 45.348, 13.0
2019-02-15 03:30:25,769 : samples : 512000
2019-02-15 03:30:35,529 : Image to text: 15.88, 39.12, 52.54, 9.0
2019-02-15 03:30:42,529 : Text to Image: 12.38, 33.48, 46.844, 12.0
2019-02-15 03:31:17,181 : Epoch 14 finished
2019-02-15 03:31:17,622 : Image to text: 34.0, 69.8, 81.9, 3.0
2019-02-15 03:31:18,018 : Text to Image: 29.52, 65.1, 80.22, 3.0
2019-02-15 03:31:18,459 : Image to text: 34.4, 69.8, 82.6, 3.0
2019-02-15 03:31:18,855 : Text to Image: 28.38, 63.22, 79.66, 3.0
2019-02-15 03:31:19,296 : Image to text: 34.2, 68.2, 82.9, 3.0
2019-02-15 03:31:19,693 : Text to Image: 28.58, 64.74, 80.42, 3.0
2019-02-15 03:31:20,134 : Image to text: 37.1, 68.9, 81.8, 3.0
2019-02-15 03:31:20,531 : Text to Image: 29.04, 65.02, 80.3, 3.0
2019-02-15 03:31:20,972 : Image to text: 36.0, 68.2, 81.5, 3.0
2019-02-15 03:31:21,296 : Text to Image: 29.8, 64.4, 79.4, 3.0
2019-02-15 03:31:21,296 : Dev mean Text to Image: 29.064, 64.496, 80.0, 3.0
2019-02-15 03:31:21,296 : Dev mean Image to text: 35.14, 68.98, 82.14, 3.0
2019-02-15 03:31:21,296 : start epoch
2019-02-15 03:32:03,654 : samples : 64000
2019-02-15 03:32:13,412 : Image to text: 15.48, 38.2, 52.36, 9.0
2019-02-15 03:32:20,424 : Text to Image: 12.428, 33.312, 46.304, 12.0
2019-02-15 03:33:01,992 : samples : 128000
2019-02-15 03:33:11,735 : Image to text: 15.04, 37.5, 51.46, 10.0
2019-02-15 03:33:18,785 : Text to Image: 12.28, 32.672, 45.86, 13.0
2019-02-15 03:34:00,998 : samples : 192000
2019-02-15 03:34:10,773 : Image to text: 15.36, 38.92, 52.46, 9.0
2019-02-15 03:34:17,815 : Text to Image: 12.284, 33.116, 46.208, 12.0
2019-02-15 03:34:59,591 : samples : 256000
2019-02-15 03:35:09,382 : Image to text: 15.68, 38.9, 52.36, 9.0
2019-02-15 03:35:16,447 : Text to Image: 12.36, 33.184, 46.536, 12.0
2019-02-15 03:35:59,266 : samples : 320000
2019-02-15 03:36:09,065 : Image to text: 15.4, 38.44, 52.14, 9.0
2019-02-15 03:36:16,131 : Text to Image: 12.732, 33.432, 46.656, 12.0
2019-02-15 03:36:58,507 : samples : 384000
2019-02-15 03:37:08,277 : Image to text: 15.48, 38.62, 52.5, 9.0
2019-02-15 03:37:15,302 : Text to Image: 12.268, 33.284, 46.436, 12.0
2019-02-15 03:37:56,439 : samples : 448000
2019-02-15 03:38:06,103 : Image to text: 15.96, 38.9, 52.5, 9.0
2019-02-15 03:38:13,075 : Text to Image: 12.68, 33.516, 46.54, 12.0
2019-02-15 03:38:54,817 : samples : 512000
2019-02-15 03:39:04,500 : Image to text: 15.08, 38.7, 51.96, 9.0
2019-02-15 03:39:11,497 : Text to Image: 12.64, 33.132, 46.384, 12.0
2019-02-15 03:39:47,739 : Epoch 15 finished
2019-02-15 03:39:48,180 : Image to text: 37.0, 69.0, 83.1, 2.0
2019-02-15 03:39:48,575 : Text to Image: 29.64, 64.46, 80.22, 3.0
2019-02-15 03:39:49,016 : Image to text: 34.2, 70.6, 84.4, 3.0
2019-02-15 03:39:49,411 : Text to Image: 28.06, 63.4, 79.02, 3.0
2019-02-15 03:39:49,852 : Image to text: 35.2, 69.1, 82.3, 3.0
2019-02-15 03:39:50,246 : Text to Image: 28.42, 64.48, 79.24, 3.0
2019-02-15 03:39:50,687 : Image to text: 36.3, 69.9, 83.2, 2.0
2019-02-15 03:39:51,013 : Text to Image: 29.24, 64.22, 80.28, 3.0
2019-02-15 03:39:51,421 : Image to text: 38.4, 70.6, 82.0, 2.0
2019-02-15 03:39:51,740 : Text to Image: 29.68, 63.66, 78.84, 3.0
2019-02-15 03:39:51,740 : Dev mean Text to Image: 29.008, 64.04400000000001, 79.52, 3.0
2019-02-15 03:39:51,740 : Dev mean Image to text: 36.22, 69.84, 83.0, 2.4
2019-02-15 03:39:51,741 : start epoch
2019-02-15 03:40:31,254 : samples : 64000
2019-02-15 03:40:40,992 : Image to text: 15.26, 38.48, 52.42, 9.0
2019-02-15 03:40:48,065 : Text to Image: 12.352, 33.06, 46.568, 12.0
2019-02-15 03:41:26,985 : samples : 128000
2019-02-15 03:41:36,767 : Image to text: 14.54, 37.64, 51.74, 10.0
2019-02-15 03:41:43,802 : Text to Image: 12.252, 32.628, 46.032, 13.0
2019-02-15 03:42:28,969 : samples : 192000
2019-02-15 03:42:38,741 : Image to text: 16.04, 39.3, 52.52, 9.0
2019-02-15 03:42:45,795 : Text to Image: 12.58, 33.508, 46.572, 12.0
2019-02-15 03:43:24,754 : samples : 256000
2019-02-15 03:43:34,544 : Image to text: 15.34, 39.06, 52.44, 9.0
2019-02-15 03:43:41,574 : Text to Image: 12.344, 33.464, 46.908, 12.0
2019-02-15 03:44:21,607 : samples : 320000
2019-02-15 03:44:31,368 : Image to text: 15.74, 39.66, 53.22, 9.0
2019-02-15 03:44:38,370 : Text to Image: 12.444, 33.452, 46.6, 12.0
2019-02-15 03:45:17,821 : samples : 384000
2019-02-15 03:45:27,577 : Image to text: 15.22, 38.64, 52.66, 9.0
2019-02-15 03:45:34,631 : Text to Image: 12.392, 33.028, 46.008, 13.0
2019-02-15 03:46:13,604 : samples : 448000
2019-02-15 03:46:23,360 : Image to text: 14.7, 38.14, 51.92, 10.0
2019-02-15 03:46:30,419 : Text to Image: 12.272, 33.284, 46.404, 12.0
2019-02-15 03:47:09,297 : samples : 512000
2019-02-15 03:47:19,090 : Image to text: 16.14, 39.5, 52.66, 9.0
2019-02-15 03:47:26,091 : Text to Image: 12.752, 33.58, 46.776, 12.0
2019-02-15 03:48:03,080 : Epoch 16 finished
2019-02-15 03:48:03,520 : Image to text: 36.4, 67.8, 83.3, 3.0
2019-02-15 03:48:03,905 : Text to Image: 29.5, 64.76, 80.76, 3.0
2019-02-15 03:48:04,345 : Image to text: 36.4, 70.4, 83.5, 3.0
2019-02-15 03:48:04,730 : Text to Image: 28.92, 63.54, 79.78, 3.0
2019-02-15 03:48:05,170 : Image to text: 35.2, 69.6, 83.9, 3.0
2019-02-15 03:48:05,554 : Text to Image: 29.34, 64.66, 79.92, 3.0
2019-02-15 03:48:05,994 : Image to text: 36.2, 69.7, 82.6, 2.0
2019-02-15 03:48:06,378 : Text to Image: 29.52, 64.78, 80.7, 3.0
2019-02-15 03:48:06,792 : Image to text: 35.8, 69.9, 82.8, 3.0
2019-02-15 03:48:07,107 : Text to Image: 29.54, 64.32, 79.5, 3.0
2019-02-15 03:48:07,107 : Dev mean Text to Image: 29.363999999999997, 64.412, 80.132, 3.0
2019-02-15 03:48:07,107 : Dev mean Image to text: 36.0, 69.48, 83.22, 2.8
2019-02-15 03:48:07,108 : start epoch
2019-02-15 03:48:47,229 : samples : 64000
2019-02-15 03:48:56,988 : Image to text: 16.28, 39.08, 52.96, 9.0
2019-02-15 03:49:04,008 : Text to Image: 12.676, 33.836, 46.964, 12.0
2019-02-15 03:49:46,134 : samples : 128000
2019-02-15 03:49:55,870 : Image to text: 15.66, 38.66, 52.58, 9.0
2019-02-15 03:50:02,928 : Text to Image: 12.412, 33.52, 46.896, 12.0
2019-02-15 03:50:44,982 : samples : 192000
2019-02-15 03:50:54,766 : Image to text: 15.6, 38.92, 52.56, 9.0
2019-02-15 03:51:01,793 : Text to Image: 12.596, 33.548, 46.6, 12.0
2019-02-15 03:51:43,594 : samples : 256000
2019-02-15 03:51:53,343 : Image to text: 15.26, 39.5, 53.56, 9.0
2019-02-15 03:52:00,367 : Text to Image: 12.264, 33.244, 46.46, 12.0
2019-02-15 03:52:41,467 : samples : 320000
2019-02-15 03:52:51,228 : Image to text: 15.22, 39.3, 53.14, 9.0
2019-02-15 03:52:58,167 : Text to Image: 12.424, 33.444, 46.492, 12.0
2019-02-15 03:53:38,885 : samples : 384000
2019-02-15 03:53:48,664 : Image to text: 14.56, 38.66, 51.36, 10.0
2019-02-15 03:53:55,587 : Text to Image: 12.228, 33.276, 46.564, 12.0
2019-02-15 03:54:37,125 : samples : 448000
2019-02-15 03:54:46,820 : Image to text: 15.98, 38.88, 52.58, 9.0
2019-02-15 03:54:53,741 : Text to Image: 12.712, 33.804, 46.572, 12.0
2019-02-15 03:55:34,274 : samples : 512000
2019-02-15 03:55:43,992 : Image to text: 15.88, 38.16, 51.98, 10.0
2019-02-15 03:55:51,009 : Text to Image: 12.36, 33.396, 46.688, 12.0
2019-02-15 03:56:26,341 : Epoch 17 finished
2019-02-15 03:56:26,754 : Image to text: 35.5, 69.1, 82.0, 3.0
2019-02-15 03:56:27,187 : Text to Image: 29.74, 65.14, 80.7, 3.0
2019-02-15 03:56:27,623 : Image to text: 35.6, 69.6, 82.5, 3.0
2019-02-15 03:56:28,054 : Text to Image: 27.92, 63.72, 79.74, 3.0
2019-02-15 03:56:28,489 : Image to text: 33.3, 70.3, 82.4, 3.0
2019-02-15 03:56:28,929 : Text to Image: 29.62, 65.26, 79.96, 3.0
2019-02-15 03:56:29,368 : Image to text: 35.2, 68.2, 81.7, 3.0
2019-02-15 03:56:29,800 : Text to Image: 29.58, 65.36, 80.82, 3.0
2019-02-15 03:56:30,246 : Image to text: 34.9, 70.0, 81.4, 3.0
2019-02-15 03:56:30,686 : Text to Image: 29.4, 64.9, 79.62, 3.0
2019-02-15 03:56:30,687 : Dev mean Text to Image: 29.252, 64.876, 80.168, 3.0
2019-02-15 03:56:30,687 : Dev mean Image to text: 34.9, 69.44, 82.0, 3.0
2019-02-15 03:56:30,687 : start epoch
2019-02-15 03:57:12,467 : samples : 64000
2019-02-15 03:57:22,298 : Image to text: 16.2, 39.06, 52.46, 9.0
2019-02-15 03:57:29,238 : Text to Image: 12.616, 33.764, 47.036, 12.0
2019-02-15 03:58:08,244 : samples : 128000
2019-02-15 03:58:17,931 : Image to text: 15.82, 39.22, 52.68, 9.0
2019-02-15 03:58:24,864 : Text to Image: 12.3, 33.288, 46.352, 12.0
2019-02-15 03:59:03,780 : samples : 192000
2019-02-15 03:59:13,569 : Image to text: 15.3, 38.84, 52.98, 9.0
2019-02-15 03:59:20,631 : Text to Image: 12.432, 33.544, 46.732, 12.0
2019-02-15 03:59:59,451 : samples : 256000
2019-02-15 04:00:09,225 : Image to text: 15.74, 38.9, 52.9, 9.0
2019-02-15 04:00:16,222 : Text to Image: 12.696, 33.788, 47.3, 12.0
2019-02-15 04:00:55,145 : samples : 320000
2019-02-15 04:01:04,816 : Image to text: 15.02, 39.18, 53.06, 9.0
2019-02-15 04:01:11,778 : Text to Image: 12.652, 33.556, 46.792, 12.0
2019-02-15 04:01:50,721 : samples : 384000
2019-02-15 04:02:00,468 : Image to text: 15.26, 38.82, 52.42, 9.0
2019-02-15 04:02:07,391 : Text to Image: 12.232, 33.304, 46.684, 12.0
2019-02-15 04:02:46,279 : samples : 448000
2019-02-15 04:02:55,956 : Image to text: 15.66, 39.84, 52.76, 9.0
2019-02-15 04:03:02,992 : Text to Image: 12.596, 33.804, 46.968, 12.0
2019-02-15 04:03:41,865 : samples : 512000
2019-02-15 04:03:51,628 : Image to text: 15.18, 39.18, 52.92, 9.0
2019-02-15 04:03:58,593 : Text to Image: 12.272, 33.156, 46.428, 12.0
2019-02-15 04:04:31,827 : Epoch 18 finished
2019-02-15 04:04:32,248 : Image to text: 34.6, 70.0, 82.2, 3.0
2019-02-15 04:04:32,656 : Text to Image: 28.98, 64.6, 80.22, 3.0
2019-02-15 04:04:33,092 : Image to text: 34.6, 69.5, 83.2, 3.0
2019-02-15 04:04:33,499 : Text to Image: 28.8, 63.72, 79.8, 3.0
2019-02-15 04:04:33,935 : Image to text: 34.0, 69.5, 82.1, 3.0
2019-02-15 04:04:34,342 : Text to Image: 28.66, 64.84, 79.66, 3.0
2019-02-15 04:04:34,779 : Image to text: 35.4, 67.2, 81.9, 3.0
2019-02-15 04:04:35,186 : Text to Image: 29.62, 64.46, 80.08, 3.0
2019-02-15 04:04:35,622 : Image to text: 35.6, 70.0, 83.0, 2.0
2019-02-15 04:04:36,029 : Text to Image: 29.96, 64.86, 79.44, 3.0
2019-02-15 04:04:36,029 : Dev mean Text to Image: 29.204, 64.496, 79.83999999999999, 3.0
2019-02-15 04:04:36,029 : Dev mean Image to text: 34.839999999999996, 69.24, 82.47999999999999, 2.8
2019-02-15 04:04:39,669 : 
Test scores | Image to text:             35.94, 69.96000000000001, 82.84, 2.4
2019-02-15 04:04:39,669 : Test scores | Text to image:             28.931999999999995, 64.184, 79.536, 3.1999999999999997

2019-02-15 04:04:39,760 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 04:04:39,966 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 04:04:40,626 : loading BERT model bert-base-uncased
2019-02-15 04:04:40,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:04:40,660 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:04:40,660 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdd4tmd4x
2019-02-15 04:04:43,028 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:04:44,392 : Computing embeddings for train/dev/test
2019-02-15 04:06:18,480 : Computed embeddings
2019-02-15 04:06:18,480 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:06:49,110 : [('reg:1e-05', 87.73), ('reg:0.0001', 86.91), ('reg:0.001', 80.81), ('reg:0.01', 69.73)]
2019-02-15 04:06:49,110 : Validation : best param found is reg = 1e-05 with score             87.73
2019-02-15 04:06:49,111 : Evaluating...
2019-02-15 04:06:57,804 : 
Dev acc : 87.7 Test acc : 86.8 for LENGTH classification

2019-02-15 04:06:57,805 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 04:06:58,181 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 04:06:58,227 : loading BERT model bert-base-uncased
2019-02-15 04:06:58,227 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:06:58,258 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:06:58,258 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_4lsuntv
2019-02-15 04:07:00,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:07:02,035 : Computing embeddings for train/dev/test
2019-02-15 04:08:30,805 : Computed embeddings
2019-02-15 04:08:30,805 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:09:06,066 : [('reg:1e-05', 62.19), ('reg:0.0001', 38.04), ('reg:0.001', 4.79), ('reg:0.01', 0.97)]
2019-02-15 04:09:06,066 : Validation : best param found is reg = 1e-05 with score             62.19
2019-02-15 04:09:06,066 : Evaluating...
2019-02-15 04:09:16,136 : 
Dev acc : 62.2 Test acc : 61.9 for WORDCONTENT classification

2019-02-15 04:09:16,138 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 04:09:16,515 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 04:09:16,584 : loading BERT model bert-base-uncased
2019-02-15 04:09:16,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:09:16,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:09:16,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphfhuqict
2019-02-15 04:09:19,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:09:20,479 : Computing embeddings for train/dev/test
2019-02-15 04:10:44,382 : Computed embeddings
2019-02-15 04:10:44,382 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:11:10,213 : [('reg:1e-05', 37.92), ('reg:0.0001', 37.67), ('reg:0.001', 35.96), ('reg:0.01', 29.84)]
2019-02-15 04:11:10,213 : Validation : best param found is reg = 1e-05 with score             37.92
2019-02-15 04:11:10,213 : Evaluating...
2019-02-15 04:11:16,576 : 
Dev acc : 37.9 Test acc : 37.8 for DEPTH classification

2019-02-15 04:11:16,577 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 04:11:16,970 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 04:11:17,035 : loading BERT model bert-base-uncased
2019-02-15 04:11:17,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:11:17,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:11:17,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpealz_k4m
2019-02-15 04:11:19,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:11:20,962 : Computing embeddings for train/dev/test
2019-02-15 04:12:38,889 : Computed embeddings
2019-02-15 04:12:38,889 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:13:04,101 : [('reg:1e-05', 74.49), ('reg:0.0001', 73.33), ('reg:0.001', 68.72), ('reg:0.01', 59.0)]
2019-02-15 04:13:04,101 : Validation : best param found is reg = 1e-05 with score             74.49
2019-02-15 04:13:04,101 : Evaluating...
2019-02-15 04:13:08,645 : 
Dev acc : 74.5 Test acc : 73.7 for TOPCONSTITUENTS classification

2019-02-15 04:13:08,647 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 04:13:09,197 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 04:13:09,266 : loading BERT model bert-base-uncased
2019-02-15 04:13:09,267 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:13:09,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:13:09,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfqf66kis
2019-02-15 04:13:11,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:13:13,026 : Computing embeddings for train/dev/test
2019-02-15 04:14:37,211 : Computed embeddings
2019-02-15 04:14:37,211 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:15:09,998 : [('reg:1e-05', 90.58), ('reg:0.0001', 90.6), ('reg:0.001', 90.41), ('reg:0.01', 89.23)]
2019-02-15 04:15:09,999 : Validation : best param found is reg = 0.0001 with score             90.6
2019-02-15 04:15:09,999 : Evaluating...
2019-02-15 04:15:17,853 : 
Dev acc : 90.6 Test acc : 90.6 for BIGRAMSHIFT classification

2019-02-15 04:15:17,854 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 04:15:18,281 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 04:15:18,351 : loading BERT model bert-base-uncased
2019-02-15 04:15:18,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:15:18,386 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:15:18,386 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpedhr55gj
2019-02-15 04:15:20,794 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:15:22,183 : Computing embeddings for train/dev/test
2019-02-15 04:16:44,786 : Computed embeddings
2019-02-15 04:16:44,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:17:14,192 : [('reg:1e-05', 89.84), ('reg:0.0001', 89.78), ('reg:0.001', 89.9), ('reg:0.01', 89.83)]
2019-02-15 04:17:14,193 : Validation : best param found is reg = 0.001 with score             89.9
2019-02-15 04:17:14,193 : Evaluating...
2019-02-15 04:17:21,809 : 
Dev acc : 89.9 Test acc : 89.4 for TENSE classification

2019-02-15 04:17:21,811 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 04:17:22,255 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 04:17:22,321 : loading BERT model bert-base-uncased
2019-02-15 04:17:22,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:17:22,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:17:22,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ml0nt7p
2019-02-15 04:17:24,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:17:26,117 : Computing embeddings for train/dev/test
2019-02-15 04:18:53,936 : Computed embeddings
2019-02-15 04:18:53,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:19:21,002 : [('reg:1e-05', 86.18), ('reg:0.0001', 86.29), ('reg:0.001', 86.02), ('reg:0.01', 84.31)]
2019-02-15 04:19:21,002 : Validation : best param found is reg = 0.0001 with score             86.29
2019-02-15 04:19:21,002 : Evaluating...
2019-02-15 04:19:27,229 : 
Dev acc : 86.3 Test acc : 85.2 for SUBJNUMBER classification

2019-02-15 04:19:27,230 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 04:19:27,666 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 04:19:27,737 : loading BERT model bert-base-uncased
2019-02-15 04:19:27,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:19:27,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:19:27,865 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwaqudfjd
2019-02-15 04:19:30,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:19:31,628 : Computing embeddings for train/dev/test
2019-02-15 04:20:57,982 : Computed embeddings
2019-02-15 04:20:57,982 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:21:31,816 : [('reg:1e-05', 83.31), ('reg:0.0001', 83.32), ('reg:0.001', 83.18), ('reg:0.01', 82.44)]
2019-02-15 04:21:31,816 : Validation : best param found is reg = 0.0001 with score             83.32
2019-02-15 04:21:31,816 : Evaluating...
2019-02-15 04:21:40,549 : 
Dev acc : 83.3 Test acc : 83.8 for OBJNUMBER classification

2019-02-15 04:21:40,551 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 04:21:40,966 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 04:21:41,041 : loading BERT model bert-base-uncased
2019-02-15 04:21:41,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:21:41,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:21:41,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz35zkyon
2019-02-15 04:21:43,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:21:44,973 : Computing embeddings for train/dev/test
2019-02-15 04:23:22,571 : Computed embeddings
2019-02-15 04:23:22,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:23:49,434 : [('reg:1e-05', 65.4), ('reg:0.0001', 65.34), ('reg:0.001', 65.5), ('reg:0.01', 65.09)]
2019-02-15 04:23:49,434 : Validation : best param found is reg = 0.001 with score             65.5
2019-02-15 04:23:49,434 : Evaluating...
2019-02-15 04:23:56,925 : 
Dev acc : 65.5 Test acc : 65.3 for ODDMANOUT classification

2019-02-15 04:23:56,926 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 04:23:57,568 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 04:23:57,651 : loading BERT model bert-base-uncased
2019-02-15 04:23:57,651 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:23:57,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:23:57,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptz05m05z
2019-02-15 04:24:00,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:24:01,502 : Computing embeddings for train/dev/test
2019-02-15 04:25:38,484 : Computed embeddings
2019-02-15 04:25:38,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:26:06,817 : [('reg:1e-05', 71.79), ('reg:0.0001', 71.77), ('reg:0.001', 71.39), ('reg:0.01', 67.01)]
2019-02-15 04:26:06,817 : Validation : best param found is reg = 1e-05 with score             71.79
2019-02-15 04:26:06,817 : Evaluating...
2019-02-15 04:26:14,318 : 
Dev acc : 71.8 Test acc : 71.4 for COORDINATIONINVERSION classification

2019-02-15 04:26:14,320 : total results: {'STS12': {'MSRpar': {'pearson': (0.4009581636643262, 2.4590128084466852e-30), 'spearman': SpearmanrResult(correlation=0.42454353270071554, pvalue=3.578606093766433e-34), 'nsamples': 750}, 'MSRvid': {'pearson': (0.422023412402888, 9.515479295839915e-34), 'spearman': SpearmanrResult(correlation=0.44485743218310975, pvalue=9.911738920172136e-38), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47982453914212286, 8.333612480326332e-28), 'spearman': SpearmanrResult(correlation=0.5881634748078525, pvalue=4.673692536589134e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6256901734364018, 9.642702212486319e-83), 'spearman': SpearmanrResult(correlation=0.6279365365625404, pvalue=1.6952135736947276e-83), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.666138511801448, 1.6402961048214583e-52), 'spearman': SpearmanrResult(correlation=0.5625196694579535, pvalue=1.1335533654184154e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5189269600894374, 'wmean': 0.5059628512878777}, 'spearman': {'mean': 0.5296041291424344, 'wmean': 0.5204039604682439}}}, 'STS13': {'FNWN': {'pearson': (0.32274857953926, 5.914289229748303e-06), 'spearman': SpearmanrResult(correlation=0.3534635814811164, pvalue=6.06680383769636e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.6106439889285629, 7.685216926350378e-78), 'spearman': SpearmanrResult(correlation=0.5847961372190434, pvalue=5.206473459568935e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.42097769189942813, 1.6662203218310701e-25), 'spearman': SpearmanrResult(correlation=0.44023041577508676, pvalue=5.389312450970203e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4514567534557503, 'wmean': 0.5034339722566143}, 'spearman': {'mean': 0.4594967114917488, 'wmean': 0.5015806553760249}}}, 'STS14': {'deft-forum': {'pearson': (0.2862534538602421, 6.195885331865943e-10), 'spearman': SpearmanrResult(correlation=0.3007672448253487, pvalue=7.321440230046186e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7434526599166917, 5.347172540593258e-54), 'spearman': SpearmanrResult(correlation=0.7100594995545526, pvalue=2.5969067648197485e-47), 'nsamples': 300}, 'headlines': {'pearson': (0.571222603431986, 3.5849416081480236e-66), 'spearman': SpearmanrResult(correlation=0.5276114032429411, pvalue=5.6213502432295096e-55), 'nsamples': 750}, 'images': {'pearson': (0.5312260295340473, 7.6409660215855e-56), 'spearman': SpearmanrResult(correlation=0.5192375737315853, pvalue=5.2233135357569645e-53), 'nsamples': 750}, 'OnWN': {'pearson': (0.6149819995042345, 3.1597755811911554e-79), 'spearman': SpearmanrResult(correlation=0.6463494301526064, pvalue=6.304047875518373e-90), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6724796365758339, 7.456704694649256e-100), 'spearman': SpearmanrResult(correlation=0.6075222407809413, pvalue=7.410392453510771e-77), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5699360638038393, 'wmean': 0.5718086810657846}, 'spearman': {'mean': 0.5519245653813292, 'wmean': 0.5530409589250208}}}, 'STS15': {'answers-forums': {'pearson': (0.5625592878763502, 1.1094267945420228e-32), 'spearman': SpearmanrResult(correlation=0.5575985718078277, pvalue=5.066197508413609e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.6751364674029731, 6.392874728171972e-101), 'spearman': SpearmanrResult(correlation=0.6812584812357168, pvalue=2.01472230267085e-103), 'nsamples': 750}, 'belief': {'pearson': (0.6442897318844597, 2.3149242058874142e-45), 'spearman': SpearmanrResult(correlation=0.6727478752294631, pvalue=9.592237469746419e-51), 'nsamples': 375}, 'headlines': {'pearson': (0.6188911560553052, 1.706237373529436e-80), 'spearman': SpearmanrResult(correlation=0.6122691801278722, pvalue=2.3384434964134333e-78), 'nsamples': 750}, 'images': {'pearson': (0.6555384140861834, 2.6389002906605188e-93), 'spearman': SpearmanrResult(correlation=0.6624221408579154, pvalue=6.487575535632345e-96), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6312830114610544, 'wmean': 0.6382476368562167}, 'spearman': {'mean': 0.6372592498517591, 'wmean': 0.6427807564350374}}}, 'STS16': {'answer-answer': {'pearson': (0.5608342039560309, 1.9163278699692928e-22), 'spearman': SpearmanrResult(correlation=0.5761307199663898, pvalue=7.324159334007489e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6451171842168822, 1.0688169373808515e-30), 'spearman': SpearmanrResult(correlation=0.6454226918105143, pvalue=9.828067409659457e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7616263325031424, 7.603511990816543e-45), 'spearman': SpearmanrResult(correlation=0.7771130284363965, pvalue=9.548872332720755e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8045524480610218, 1.0925313413815377e-56), 'spearman': SpearmanrResult(correlation=0.8429261884780218, pvalue=4.3460087601746527e-67), 'nsamples': 244}, 'question-question': {'pearson': (0.29711499711465517, 1.2502228685099508e-05), 'spearman': SpearmanrResult(correlation=0.2978320227508343, pvalue=1.1884926212787783e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6138490331703466, 'wmean': 0.6211365555433478}, 'spearman': {'mean': 0.6278849302884313, 'wmean': 0.6355011234538059}}}, 'MR': {'devacc': 79.75, 'acc': 79.34, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.33, 'acc': 84.4, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.73, 'acc': 88.73, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.07, 'acc': 94.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.4, 'acc': 84.84, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 43.62, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 85.4, 'acc': 92.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 75.42, 'acc': 73.39, 'f1': 79.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 77.49, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8049233398271021, 'pearson': 0.8083412013242576, 'spearman': 0.743372157010972, 'mse': 0.35310002413158736, 'yhat': array([2.65290246, 3.81894881, 1.66134218, ..., 3.21806341, 4.4088747 ,        4.42255518]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.724498551024837, 'pearson': 0.6705213744602287, 'spearman': 0.6645309038555148, 'mse': 1.448943953210956, 'yhat': array([1.46583604, 1.70530432, 2.3650643 , ..., 3.89356629, 3.83307051,        3.62722068]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.22, 'acc': 66.12, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 362.608, 'acc': [(35.94, 69.96000000000001, 82.84, 2.4), (28.931999999999995, 64.184, 79.536, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 87.73, 'acc': 86.81, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 62.19, 'acc': 61.94, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 37.92, 'acc': 37.83, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.49, 'acc': 73.72, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.6, 'acc': 90.56, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.9, 'acc': 89.39, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.29, 'acc': 85.2, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 83.32, 'acc': 83.78, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.5, 'acc': 65.27, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.79, 'acc': 71.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 04:26:14,320 : STS12 p=0.5060, STS12 s=0.5204, STS13 p=0.5034, STS13 s=0.5016, STS14 p=0.5718, STS14 s=0.5530, STS15 p=0.6382, STS15 s=0.6428, STS 16 p=0.6211, STS16 s=0.6355, STS B p=0.6705, STS B s=0.6645, STS B m=1.4489, SICK-R p=0.8083, SICK-R s=0.7434, SICK-P m=0.3531
2019-02-15 04:26:14,320 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 04:26:14,320 : 0.5060,0.5204,0.5034,0.5016,0.5718,0.5530,0.6382,0.6428,0.6211,0.6355,0.6705,0.6645,1.4489,0.8083,0.7434,0.3531
2019-02-15 04:26:14,320 : MR=79.34, CR=84.40, SUBJ=94.68, MPQA=88.73, SST-B=84.84, SST-F=43.62, TREC=92.00, SICK-E=77.49, SNLI=66.12, MRPC=73.39, MRPC f=79.79
2019-02-15 04:26:14,320 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 04:26:14,320 : 79.34,84.40,94.68,88.73,84.84,43.62,92.00,77.49,66.12,73.39,79.79
2019-02-15 04:26:14,320 : COCO r1i2t=35.94, COCO r5i2t=69.96, COCO r10i2t=82.84, COCO medr_i2t=2.40, COCO r1t2i=28.93, COCO r5t2i=64.18, COCO r10t2i=79.54, COCO medr_t2i=3.20
2019-02-15 04:26:14,320 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 04:26:14,320 : 35.94,69.96,82.84,2.40,28.93,64.18,79.54,3.20
2019-02-15 04:26:14,320 : SentLen=86.81, WC=61.94, TreeDepth=37.83, TopConst=73.72, BShift=90.56, Tense=89.39, SubjNum=85.20, ObjNum=83.78, SOMO=65.27, CoordInv=71.37, average=74.59
2019-02-15 04:26:14,320 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 04:26:14,320 : 86.81,61.94,37.83,73.72,90.56,89.39,85.20,83.78,65.27,71.37,74.59
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,320 : layer 9
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,320 : ********************************************************************************
2019-02-15 04:26:14,413 : ***** Transfer task : STS12 *****


2019-02-15 04:26:14,426 : loading BERT model bert-base-uncased
2019-02-15 04:26:14,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:14,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:14,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjhn0adyq
2019-02-15 04:26:16,853 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:20,012 : MSRpar : pearson = 0.3828, spearman = 0.4113
2019-02-15 04:26:20,779 : MSRvid : pearson = 0.3291, spearman = 0.3695
2019-02-15 04:26:21,397 : SMTeuroparl : pearson = 0.4882, spearman = 0.5826
2019-02-15 04:26:22,568 : surprise.OnWN : pearson = 0.5922, spearman = 0.5924
2019-02-15 04:26:23,227 : surprise.SMTnews : pearson = 0.6180, spearman = 0.5632
2019-02-15 04:26:23,227 : ALL (weighted average) : Pearson = 0.4661,             Spearman = 0.4897
2019-02-15 04:26:23,227 : ALL (average) : Pearson = 0.4821,             Spearman = 0.5038

2019-02-15 04:26:23,227 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 04:26:23,236 : loading BERT model bert-base-uncased
2019-02-15 04:26:23,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:23,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:23,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2jafd_og
2019-02-15 04:26:25,626 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:27,632 : FNWN : pearson = 0.3203, spearman = 0.3505
2019-02-15 04:26:28,495 : headlines : pearson = 0.5885, spearman = 0.5582
2019-02-15 04:26:29,169 : OnWN : pearson = 0.4330, spearman = 0.4489
2019-02-15 04:26:29,169 : ALL (weighted average) : Pearson = 0.4965,             Spearman = 0.4912
2019-02-15 04:26:29,169 : ALL (average) : Pearson = 0.4473,             Spearman = 0.4525

2019-02-15 04:26:29,169 : ***** Transfer task : STS14 *****


2019-02-15 04:26:29,186 : loading BERT model bert-base-uncased
2019-02-15 04:26:29,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:29,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:29,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplr2ifaxx
2019-02-15 04:26:31,638 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:33,690 : deft-forum : pearson = 0.2593, spearman = 0.2632
2019-02-15 04:26:34,422 : deft-news : pearson = 0.7287, spearman = 0.6962
2019-02-15 04:26:35,505 : headlines : pearson = 0.5537, spearman = 0.5104
2019-02-15 04:26:36,541 : images : pearson = 0.4282, spearman = 0.4384
2019-02-15 04:26:37,593 : OnWN : pearson = 0.6203, spearman = 0.6486
2019-02-15 04:26:38,973 : tweet-news : pearson = 0.6670, spearman = 0.5889
2019-02-15 04:26:38,973 : ALL (weighted average) : Pearson = 0.5432,             Spearman = 0.5245
2019-02-15 04:26:38,974 : ALL (average) : Pearson = 0.5429,             Spearman = 0.5243

2019-02-15 04:26:38,974 : ***** Transfer task : STS15 *****


2019-02-15 04:26:39,023 : loading BERT model bert-base-uncased
2019-02-15 04:26:39,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:39,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:39,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm4g20dvf
2019-02-15 04:26:41,378 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:43,668 : answers-forums : pearson = 0.5607, spearman = 0.5609
2019-02-15 04:26:44,700 : answers-students : pearson = 0.6554, spearman = 0.6646
2019-02-15 04:26:45,658 : belief : pearson = 0.6660, spearman = 0.6872
2019-02-15 04:26:46,646 : headlines : pearson = 0.6003, spearman = 0.5940
2019-02-15 04:26:47,589 : images : pearson = 0.6202, spearman = 0.6277
2019-02-15 04:26:47,589 : ALL (weighted average) : Pearson = 0.6223,             Spearman = 0.6276
2019-02-15 04:26:47,589 : ALL (average) : Pearson = 0.6205,             Spearman = 0.6269

2019-02-15 04:26:47,589 : ***** Transfer task : STS16 *****


2019-02-15 04:26:47,654 : loading BERT model bert-base-uncased
2019-02-15 04:26:47,654 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:47,672 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:47,672 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplav6ah1d
2019-02-15 04:26:50,004 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:51,742 : answer-answer : pearson = 0.5332, spearman = 0.5294
2019-02-15 04:26:52,044 : headlines : pearson = 0.6373, spearman = 0.6453
2019-02-15 04:26:52,427 : plagiarism : pearson = 0.7408, spearman = 0.7547
2019-02-15 04:26:53,039 : postediting : pearson = 0.7986, spearman = 0.8279
2019-02-15 04:26:53,317 : question-question : pearson = 0.2776, spearman = 0.2775
2019-02-15 04:26:53,317 : ALL (weighted average) : Pearson = 0.6049,             Spearman = 0.6145
2019-02-15 04:26:53,317 : ALL (average) : Pearson = 0.5975,             Spearman = 0.6070

2019-02-15 04:26:53,317 : ***** Transfer task : MR *****


2019-02-15 04:26:53,332 : loading BERT model bert-base-uncased
2019-02-15 04:26:53,333 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:26:53,353 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:26:53,353 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwmqk166o
2019-02-15 04:26:55,703 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:26:57,119 : Generating sentence embeddings
2019-02-15 04:27:10,550 : Generated sentence embeddings
2019-02-15 04:27:10,550 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 04:27:22,131 : Best param found at split 1: l2reg = 1e-05                 with score 80.96
2019-02-15 04:27:35,021 : Best param found at split 2: l2reg = 0.001                 with score 81.03
2019-02-15 04:27:46,099 : Best param found at split 3: l2reg = 0.001                 with score 81.49
2019-02-15 04:27:57,828 : Best param found at split 4: l2reg = 0.001                 with score 81.09
2019-02-15 04:28:10,251 : Best param found at split 5: l2reg = 0.0001                 with score 80.97
2019-02-15 04:28:10,898 : Dev acc : 81.11 Test acc : 81.05

2019-02-15 04:28:10,899 : ***** Transfer task : CR *****


2019-02-15 04:28:10,907 : loading BERT model bert-base-uncased
2019-02-15 04:28:10,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:28:10,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:28:10,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5lkjtavr
2019-02-15 04:28:13,291 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:28:14,677 : Generating sentence embeddings
2019-02-15 04:28:18,366 : Generated sentence embeddings
2019-02-15 04:28:18,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 04:28:22,286 : Best param found at split 1: l2reg = 0.01                 with score 87.05
2019-02-15 04:28:26,528 : Best param found at split 2: l2reg = 0.01                 with score 87.12
2019-02-15 04:28:30,296 : Best param found at split 3: l2reg = 1e-05                 with score 87.25
2019-02-15 04:28:34,429 : Best param found at split 4: l2reg = 0.01                 with score 87.09
2019-02-15 04:28:38,530 : Best param found at split 5: l2reg = 0.01                 with score 87.32
2019-02-15 04:28:38,737 : Dev acc : 87.17 Test acc : 87.02

2019-02-15 04:28:38,737 : ***** Transfer task : MPQA *****


2019-02-15 04:28:38,742 : loading BERT model bert-base-uncased
2019-02-15 04:28:38,742 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:28:38,761 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:28:38,761 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm9vwj7xm
2019-02-15 04:28:41,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:28:42,500 : Generating sentence embeddings
2019-02-15 04:28:46,154 : Generated sentence embeddings
2019-02-15 04:28:46,155 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 04:28:57,616 : Best param found at split 1: l2reg = 0.001                 with score 88.68
2019-02-15 04:29:10,128 : Best param found at split 2: l2reg = 0.0001                 with score 88.4
2019-02-15 04:29:22,337 : Best param found at split 3: l2reg = 0.01                 with score 88.01
2019-02-15 04:29:34,286 : Best param found at split 4: l2reg = 0.001                 with score 89.07
2019-02-15 04:29:46,413 : Best param found at split 5: l2reg = 0.01                 with score 88.65
2019-02-15 04:29:47,075 : Dev acc : 88.56 Test acc : 88.64

2019-02-15 04:29:47,076 : ***** Transfer task : SUBJ *****


2019-02-15 04:29:47,092 : loading BERT model bert-base-uncased
2019-02-15 04:29:47,092 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:29:47,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:29:47,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9yu73kei
2019-02-15 04:29:49,557 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:29:50,942 : Generating sentence embeddings
2019-02-15 04:30:04,072 : Generated sentence embeddings
2019-02-15 04:30:04,072 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 04:30:15,243 : Best param found at split 1: l2reg = 0.001                 with score 95.35
2019-02-15 04:30:27,142 : Best param found at split 2: l2reg = 0.001                 with score 95.52
2019-02-15 04:30:38,226 : Best param found at split 3: l2reg = 1e-05                 with score 95.04
2019-02-15 04:30:49,181 : Best param found at split 4: l2reg = 0.001                 with score 95.8
2019-02-15 04:30:59,889 : Best param found at split 5: l2reg = 0.001                 with score 95.32
2019-02-15 04:31:00,402 : Dev acc : 95.41 Test acc : 94.77

2019-02-15 04:31:00,403 : ***** Transfer task : SST Binary classification *****


2019-02-15 04:31:00,534 : loading BERT model bert-base-uncased
2019-02-15 04:31:00,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:31:00,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:31:00,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7kt47cpu
2019-02-15 04:31:02,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:31:04,288 : Computing embedding for train
2019-02-15 04:31:49,097 : Computed train embeddings
2019-02-15 04:31:49,097 : Computing embedding for dev
2019-02-15 04:31:50,017 : Computed dev embeddings
2019-02-15 04:31:50,017 : Computing embedding for test
2019-02-15 04:31:51,998 : Computed test embeddings
2019-02-15 04:31:51,998 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:32:13,970 : [('reg:1e-05', 85.78), ('reg:0.0001', 85.89), ('reg:0.001', 85.67), ('reg:0.01', 85.44)]
2019-02-15 04:32:13,970 : Validation : best param found is reg = 0.0001 with score             85.89
2019-02-15 04:32:13,970 : Evaluating...
2019-02-15 04:32:19,370 : 
Dev acc : 85.89 Test acc : 85.39 for             SST Binary classification

2019-02-15 04:32:19,370 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 04:32:19,421 : loading BERT model bert-base-uncased
2019-02-15 04:32:19,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:32:19,442 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:32:19,442 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf7d8b2xr
2019-02-15 04:32:21,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:32:23,162 : Computing embedding for train
2019-02-15 04:32:32,440 : Computed train embeddings
2019-02-15 04:32:32,440 : Computing embedding for dev
2019-02-15 04:32:33,674 : Computed dev embeddings
2019-02-15 04:32:33,674 : Computing embedding for test
2019-02-15 04:32:36,054 : Computed test embeddings
2019-02-15 04:32:36,054 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:32:38,807 : [('reg:1e-05', 44.5), ('reg:0.0001', 44.41), ('reg:0.001', 45.05), ('reg:0.01', 44.32)]
2019-02-15 04:32:38,808 : Validation : best param found is reg = 0.001 with score             45.05
2019-02-15 04:32:38,808 : Evaluating...
2019-02-15 04:32:39,566 : 
Dev acc : 45.05 Test acc : 43.53 for             SST Fine-Grained classification

2019-02-15 04:32:39,566 : ***** Transfer task : TREC *****


2019-02-15 04:32:39,581 : loading BERT model bert-base-uncased
2019-02-15 04:32:39,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:32:39,601 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:32:39,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp551tbun3
2019-02-15 04:32:42,005 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:32:46,866 : Computed train embeddings
2019-02-15 04:32:47,129 : Computed test embeddings
2019-02-15 04:32:47,130 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 04:32:54,674 : [('reg:1e-05', 86.56), ('reg:0.0001', 86.44), ('reg:0.001', 85.95), ('reg:0.01', 80.89)]
2019-02-15 04:32:54,674 : Cross-validation : best param found is reg = 1e-05             with score 86.56
2019-02-15 04:32:54,674 : Evaluating...
2019-02-15 04:32:55,127 : 
Dev acc : 86.56 Test acc : 93.2             for TREC

2019-02-15 04:32:55,128 : ***** Transfer task : MRPC *****


2019-02-15 04:32:55,151 : loading BERT model bert-base-uncased
2019-02-15 04:32:55,151 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:32:55,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:32:55,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1w5pl51w
2019-02-15 04:32:57,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:32:58,953 : Computing embedding for train
2019-02-15 04:33:08,488 : Computed train embeddings
2019-02-15 04:33:08,488 : Computing embedding for test
2019-02-15 04:33:12,848 : Computed test embeddings
2019-02-15 04:33:12,863 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 04:33:18,474 : [('reg:1e-05', 75.47), ('reg:0.0001', 75.22), ('reg:0.001', 74.73), ('reg:0.01', 74.31)]
2019-02-15 04:33:18,474 : Cross-validation : best param found is reg = 1e-05             with score 75.47
2019-02-15 04:33:18,474 : Evaluating...
2019-02-15 04:33:18,891 : Dev acc : 75.47 Test acc 72.52; Test F1 78.74 for MRPC.

2019-02-15 04:33:18,891 : ***** Transfer task : SICK-Entailment*****


2019-02-15 04:33:18,955 : loading BERT model bert-base-uncased
2019-02-15 04:33:18,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:33:18,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:33:18,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8_zxri2i
2019-02-15 04:33:21,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:33:22,692 : Computing embedding for train
2019-02-15 04:33:27,668 : Computed train embeddings
2019-02-15 04:33:27,668 : Computing embedding for dev
2019-02-15 04:33:28,327 : Computed dev embeddings
2019-02-15 04:33:28,327 : Computing embedding for test
2019-02-15 04:33:33,784 : Computed test embeddings
2019-02-15 04:33:33,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:33:35,523 : [('reg:1e-05', 77.4), ('reg:0.0001', 77.6), ('reg:0.001', 78.2), ('reg:0.01', 76.4)]
2019-02-15 04:33:35,523 : Validation : best param found is reg = 0.001 with score             78.2
2019-02-15 04:33:35,524 : Evaluating...
2019-02-15 04:33:35,932 : 
Dev acc : 78.2 Test acc : 77.8 for                        SICK entailment

2019-02-15 04:33:35,933 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 04:33:35,961 : loading BERT model bert-base-uncased
2019-02-15 04:33:35,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:33:36,023 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:33:36,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqrpp9dmm
2019-02-15 04:33:38,427 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:33:39,838 : Computing embedding for train
2019-02-15 04:33:44,960 : Computed train embeddings
2019-02-15 04:33:44,960 : Computing embedding for dev
2019-02-15 04:33:45,668 : Computed dev embeddings
2019-02-15 04:33:45,669 : Computing embedding for test
2019-02-15 04:33:51,197 : Computed test embeddings
2019-02-15 04:34:08,307 : Dev : Pearson 0.8000069157066063
2019-02-15 04:34:08,307 : Test : Pearson 0.8012832496078378 Spearman 0.736573993322525 MSE 0.3644766684858283                        for SICK Relatedness

2019-02-15 04:34:08,308 : 

***** Transfer task : STSBenchmark*****


2019-02-15 04:34:08,350 : loading BERT model bert-base-uncased
2019-02-15 04:34:08,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:34:08,380 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:34:08,381 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptbs22gj5
2019-02-15 04:34:10,763 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:34:12,143 : Computing embedding for train
2019-02-15 04:34:21,200 : Computed train embeddings
2019-02-15 04:34:21,200 : Computing embedding for dev
2019-02-15 04:34:23,845 : Computed dev embeddings
2019-02-15 04:34:23,846 : Computing embedding for test
2019-02-15 04:34:25,787 : Computed test embeddings
2019-02-15 04:34:47,284 : Dev : Pearson 0.6956992452228284
2019-02-15 04:34:47,285 : Test : Pearson 0.6451650982494892 Spearman 0.6412762430950625 MSE 1.5022715488749476                        for SICK Relatedness

2019-02-15 04:34:47,285 : ***** Transfer task : SNLI Entailment*****


2019-02-15 04:34:52,127 : loading BERT model bert-base-uncased
2019-02-15 04:34:52,127 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:34:52,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:34:52,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpismc19qi
2019-02-15 04:34:54,638 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:34:56,243 : PROGRESS (encoding): 0.00%
2019-02-15 04:36:14,142 : PROGRESS (encoding): 14.56%
2019-02-15 04:37:41,376 : PROGRESS (encoding): 29.12%
2019-02-15 04:39:08,260 : PROGRESS (encoding): 43.69%
2019-02-15 04:40:43,085 : PROGRESS (encoding): 58.25%
2019-02-15 04:43:22,628 : PROGRESS (encoding): 72.81%
2019-02-15 04:45:38,281 : PROGRESS (encoding): 87.37%
2019-02-15 04:48:06,345 : PROGRESS (encoding): 0.00%
2019-02-15 04:48:24,984 : PROGRESS (encoding): 0.00%
2019-02-15 04:48:44,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:50:00,798 : [('reg:1e-09', 65.2)]
2019-02-15 04:50:00,798 : Validation : best param found is reg = 1e-09 with score             65.2
2019-02-15 04:50:00,798 : Evaluating...
2019-02-15 04:51:03,115 : Dev acc : 65.2 Test acc : 65.22 for SNLI

2019-02-15 04:51:03,115 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 04:51:11,636 : loading BERT model bert-base-uncased
2019-02-15 04:51:11,637 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:51:11,682 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:51:11,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkti4fg25
2019-02-15 04:51:14,018 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:51:15,472 : Computing embedding for train
2019-02-15 05:00:54,852 : Computed train embeddings
2019-02-15 05:00:54,852 : Computing embedding for dev
2019-02-15 05:01:16,298 : Computed dev embeddings
2019-02-15 05:01:16,298 : Computing embedding for test
2019-02-15 05:01:43,089 : Computed test embeddings
2019-02-15 05:01:43,104 : prepare data
2019-02-15 05:01:43,167 : start epoch
2019-02-15 05:02:26,164 : samples : 64000
2019-02-15 05:02:38,686 : Image to text: 7.82, 23.62, 35.14, 22.0
2019-02-15 05:02:48,637 : Text to Image: 6.548, 20.24, 30.88, 26.0
2019-02-15 05:03:28,931 : samples : 128000
2019-02-15 05:03:40,268 : Image to text: 9.28, 25.58, 36.76, 19.0
2019-02-15 05:03:50,160 : Text to Image: 7.0, 22.024, 32.976, 23.0
2019-02-15 05:04:32,304 : samples : 192000
2019-02-15 05:04:44,751 : Image to text: 9.8, 27.16, 39.24, 17.0
2019-02-15 05:04:54,684 : Text to Image: 7.672, 23.02, 34.248, 22.0
2019-02-15 05:05:35,029 : samples : 256000
2019-02-15 05:05:46,074 : Image to text: 9.8, 27.84, 40.1, 17.0
2019-02-15 05:05:55,995 : Text to Image: 7.816, 23.836, 35.356, 21.0
2019-02-15 05:06:39,456 : samples : 320000
2019-02-15 05:06:51,952 : Image to text: 9.26, 27.34, 38.6, 17.0
2019-02-15 05:07:01,931 : Text to Image: 7.728, 24.14, 35.352, 21.0
2019-02-15 05:07:42,972 : samples : 384000
2019-02-15 05:07:52,706 : Image to text: 9.98, 29.64, 41.26, 15.0
2019-02-15 05:08:02,218 : Text to Image: 8.54, 25.184, 36.824, 19.0
2019-02-15 05:08:45,055 : samples : 448000
2019-02-15 05:08:57,573 : Image to text: 9.94, 28.74, 40.68, 16.0
2019-02-15 05:09:07,501 : Text to Image: 8.256, 24.832, 36.828, 19.0
2019-02-15 05:09:48,899 : samples : 512000
2019-02-15 05:09:58,898 : Image to text: 11.18, 29.48, 42.22, 16.0
2019-02-15 05:10:05,978 : Text to Image: 8.808, 26.204, 38.064, 18.0
2019-02-15 05:10:38,670 : Epoch 1 finished
2019-02-15 05:10:39,027 : Image to text: 25.1, 58.7, 74.3, 4.0
2019-02-15 05:10:39,288 : Text to Image: 23.38, 56.06, 72.58, 4.0
2019-02-15 05:10:39,646 : Image to text: 25.9, 60.1, 74.2, 4.0
2019-02-15 05:10:39,906 : Text to Image: 22.46, 55.06, 72.88, 5.0
2019-02-15 05:10:40,264 : Image to text: 26.3, 60.9, 75.8, 4.0
2019-02-15 05:10:40,524 : Text to Image: 22.66, 55.9, 72.72, 4.0
2019-02-15 05:10:40,884 : Image to text: 26.4, 60.2, 75.9, 4.0
2019-02-15 05:10:41,145 : Text to Image: 23.18, 56.62, 73.44, 4.0
2019-02-15 05:10:41,503 : Image to text: 26.5, 61.6, 76.0, 3.0
2019-02-15 05:10:41,763 : Text to Image: 23.04, 56.86, 72.64, 4.0
2019-02-15 05:10:41,763 : Dev mean Text to Image: 22.944, 56.099999999999994, 72.852, 4.2
2019-02-15 05:10:41,763 : Dev mean Image to text: 26.04, 60.3, 75.24, 3.8000000000000003
2019-02-15 05:10:41,763 : start epoch
2019-02-15 05:11:20,104 : samples : 64000
2019-02-15 05:11:29,432 : Image to text: 10.92, 29.64, 41.62, 16.0
2019-02-15 05:11:35,800 : Text to Image: 8.792, 25.832, 37.992, 18.0
2019-02-15 05:12:14,074 : samples : 128000
2019-02-15 05:12:23,449 : Image to text: 10.3, 28.94, 41.34, 16.0
2019-02-15 05:12:29,776 : Text to Image: 8.448, 25.572, 37.372, 19.0
2019-02-15 05:13:07,994 : samples : 192000
2019-02-15 05:13:17,352 : Image to text: 11.5, 30.68, 43.54, 14.0
2019-02-15 05:13:23,708 : Text to Image: 9.776, 27.492, 39.496, 17.0
2019-02-15 05:14:01,887 : samples : 256000
2019-02-15 05:14:11,166 : Image to text: 11.94, 31.64, 43.18, 14.0
2019-02-15 05:14:17,510 : Text to Image: 9.676, 27.58, 39.616, 17.0
2019-02-15 05:14:55,679 : samples : 320000
2019-02-15 05:15:04,986 : Image to text: 11.76, 31.38, 44.58, 14.0
2019-02-15 05:15:11,317 : Text to Image: 9.66, 27.848, 39.984, 17.0
2019-02-15 05:15:49,488 : samples : 384000
2019-02-15 05:15:58,770 : Image to text: 11.46, 31.16, 43.94, 14.0
2019-02-15 05:16:05,136 : Text to Image: 9.124, 27.164, 39.284, 17.0
2019-02-15 05:16:43,301 : samples : 448000
2019-02-15 05:16:52,700 : Image to text: 11.78, 32.26, 45.36, 13.0
2019-02-15 05:16:59,037 : Text to Image: 9.9, 27.98, 40.216, 16.0
2019-02-15 05:17:37,289 : samples : 512000
2019-02-15 05:17:46,580 : Image to text: 11.6, 30.64, 43.68, 14.0
2019-02-15 05:17:52,910 : Text to Image: 9.44, 27.452, 39.784, 17.0
2019-02-15 05:18:25,436 : Epoch 2 finished
2019-02-15 05:18:25,797 : Image to text: 29.8, 64.8, 79.0, 3.0
2019-02-15 05:18:26,058 : Text to Image: 24.88, 58.9, 75.54, 4.0
2019-02-15 05:18:26,416 : Image to text: 26.8, 61.2, 76.2, 4.0
2019-02-15 05:18:26,676 : Text to Image: 23.52, 56.84, 74.24, 4.0
2019-02-15 05:18:27,034 : Image to text: 29.7, 63.4, 78.7, 3.0
2019-02-15 05:18:27,294 : Text to Image: 24.96, 58.38, 74.76, 4.0
2019-02-15 05:18:27,652 : Image to text: 28.7, 63.5, 77.6, 3.0
2019-02-15 05:18:27,912 : Text to Image: 24.52, 58.24, 74.38, 4.0
2019-02-15 05:18:28,269 : Image to text: 28.7, 63.4, 77.6, 3.0
2019-02-15 05:18:28,529 : Text to Image: 24.32, 58.14, 74.06, 4.0
2019-02-15 05:18:28,529 : Dev mean Text to Image: 24.44, 58.099999999999994, 74.596, 4.0
2019-02-15 05:18:28,529 : Dev mean Image to text: 28.740000000000002, 63.26, 77.82, 3.2
2019-02-15 05:18:28,529 : start epoch
2019-02-15 05:19:06,645 : samples : 64000
2019-02-15 05:19:15,972 : Image to text: 12.14, 32.1, 44.64, 14.0
2019-02-15 05:19:22,296 : Text to Image: 9.78, 27.952, 40.184, 17.0
2019-02-15 05:20:00,362 : samples : 128000
2019-02-15 05:20:09,679 : Image to text: 12.66, 33.1, 45.96, 13.0
2019-02-15 05:20:16,044 : Text to Image: 9.96, 28.116, 40.368, 16.0
2019-02-15 05:20:54,136 : samples : 192000
2019-02-15 05:21:03,501 : Image to text: 12.9, 33.02, 45.94, 13.0
2019-02-15 05:21:09,831 : Text to Image: 10.464, 28.616, 40.752, 16.0
2019-02-15 05:21:47,927 : samples : 256000
2019-02-15 05:21:57,214 : Image to text: 13.84, 33.58, 46.22, 12.0
2019-02-15 05:22:03,570 : Text to Image: 10.408, 29.1, 41.096, 16.0
2019-02-15 05:22:41,681 : samples : 320000
2019-02-15 05:22:51,000 : Image to text: 11.8, 32.22, 44.78, 13.0
2019-02-15 05:22:57,334 : Text to Image: 9.596, 27.956, 40.156, 17.0
2019-02-15 05:23:35,563 : samples : 384000
2019-02-15 05:23:44,860 : Image to text: 12.06, 32.14, 44.66, 13.0
2019-02-15 05:23:51,200 : Text to Image: 9.92, 28.216, 40.7, 16.0
2019-02-15 05:24:29,364 : samples : 448000
2019-02-15 05:24:38,671 : Image to text: 12.64, 33.42, 46.38, 12.0
2019-02-15 05:24:45,009 : Text to Image: 10.42, 29.112, 42.028, 15.0
2019-02-15 05:25:23,552 : samples : 512000
2019-02-15 05:25:32,886 : Image to text: 13.6, 34.82, 47.88, 12.0
2019-02-15 05:25:39,226 : Text to Image: 10.696, 29.932, 42.36, 15.0
2019-02-15 05:26:12,210 : Epoch 3 finished
2019-02-15 05:26:12,569 : Image to text: 30.4, 65.6, 80.4, 3.0
2019-02-15 05:26:12,831 : Text to Image: 24.96, 61.1, 76.86, 4.0
2019-02-15 05:26:13,190 : Image to text: 29.5, 63.9, 78.4, 3.0
2019-02-15 05:26:13,451 : Text to Image: 24.46, 59.36, 75.36, 4.0
2019-02-15 05:26:13,810 : Image to text: 31.6, 65.4, 79.6, 3.0
2019-02-15 05:26:14,072 : Text to Image: 25.86, 60.02, 76.58, 4.0
2019-02-15 05:26:14,431 : Image to text: 33.0, 67.0, 80.4, 3.0
2019-02-15 05:26:14,693 : Text to Image: 26.28, 60.22, 76.7, 4.0
2019-02-15 05:26:15,060 : Image to text: 31.3, 66.6, 80.4, 3.0
2019-02-15 05:26:15,332 : Text to Image: 25.52, 60.1, 75.8, 4.0
2019-02-15 05:26:15,332 : Dev mean Text to Image: 25.416, 60.16, 76.25999999999999, 4.0
2019-02-15 05:26:15,332 : Dev mean Image to text: 31.159999999999997, 65.7, 79.84, 3.0
2019-02-15 05:26:15,333 : start epoch
2019-02-15 05:26:53,857 : samples : 64000
2019-02-15 05:27:03,223 : Image to text: 12.4, 33.78, 45.98, 13.0
2019-02-15 05:27:09,568 : Text to Image: 10.096, 28.584, 41.396, 15.0
2019-02-15 05:27:48,119 : samples : 128000
2019-02-15 05:27:57,452 : Image to text: 12.72, 33.52, 47.58, 12.0
2019-02-15 05:28:03,814 : Text to Image: 10.3, 29.34, 41.5, 15.0
2019-02-15 05:28:42,331 : samples : 192000
2019-02-15 05:28:51,665 : Image to text: 13.06, 33.84, 47.1, 12.0
2019-02-15 05:28:58,013 : Text to Image: 10.364, 29.008, 41.384, 16.0
2019-02-15 05:29:36,535 : samples : 256000
2019-02-15 05:29:45,904 : Image to text: 12.44, 33.58, 46.52, 12.0
2019-02-15 05:29:52,266 : Text to Image: 10.604, 29.632, 42.656, 15.0
2019-02-15 05:30:30,773 : samples : 320000
2019-02-15 05:30:40,163 : Image to text: 13.02, 34.46, 47.24, 12.0
2019-02-15 05:30:46,543 : Text to Image: 10.708, 29.592, 42.148, 15.0
2019-02-15 05:31:25,091 : samples : 384000
2019-02-15 05:31:34,456 : Image to text: 13.62, 34.78, 47.6, 12.0
2019-02-15 05:31:40,821 : Text to Image: 10.432, 29.38, 42.252, 15.0
2019-02-15 05:32:19,403 : samples : 448000
2019-02-15 05:32:28,729 : Image to text: 13.2, 34.4, 47.8, 11.0
2019-02-15 05:32:35,118 : Text to Image: 10.692, 29.648, 42.388, 15.0
2019-02-15 05:33:13,610 : samples : 512000
2019-02-15 05:33:22,958 : Image to text: 13.06, 33.98, 47.62, 11.0
2019-02-15 05:33:29,317 : Text to Image: 10.784, 29.856, 42.352, 15.0
2019-02-15 05:34:02,224 : Epoch 4 finished
2019-02-15 05:34:02,583 : Image to text: 31.3, 67.2, 81.8, 3.0
2019-02-15 05:34:02,843 : Text to Image: 26.18, 62.02, 77.6, 4.0
2019-02-15 05:34:03,202 : Image to text: 30.7, 64.6, 78.2, 3.0
2019-02-15 05:34:03,462 : Text to Image: 25.98, 59.4, 76.04, 4.0
2019-02-15 05:34:03,820 : Image to text: 32.0, 67.1, 80.9, 3.0
2019-02-15 05:34:04,081 : Text to Image: 26.6, 60.94, 77.04, 4.0
2019-02-15 05:34:04,439 : Image to text: 33.3, 66.3, 81.1, 3.0
2019-02-15 05:34:04,700 : Text to Image: 26.78, 61.74, 77.52, 4.0
2019-02-15 05:34:05,057 : Image to text: 32.4, 67.8, 80.7, 3.0
2019-02-15 05:34:05,317 : Text to Image: 26.44, 61.04, 76.16, 4.0
2019-02-15 05:34:05,317 : Dev mean Text to Image: 26.395999999999997, 61.02799999999999, 76.872, 4.0
2019-02-15 05:34:05,318 : Dev mean Image to text: 31.939999999999998, 66.6, 80.54, 3.0
2019-02-15 05:34:05,318 : start epoch
2019-02-15 05:34:44,044 : samples : 64000
2019-02-15 05:34:53,457 : Image to text: 14.34, 36.08, 48.6, 11.0
2019-02-15 05:34:59,804 : Text to Image: 10.96, 30.66, 43.456, 14.0
2019-02-15 05:35:38,581 : samples : 128000
2019-02-15 05:35:47,918 : Image to text: 13.32, 34.18, 47.78, 11.0
2019-02-15 05:35:54,268 : Text to Image: 10.692, 29.976, 42.524, 15.0
2019-02-15 05:36:33,074 : samples : 192000
2019-02-15 05:36:42,389 : Image to text: 12.72, 33.94, 47.36, 12.0
2019-02-15 05:36:48,751 : Text to Image: 10.5, 29.616, 42.64, 15.0
2019-02-15 05:37:27,508 : samples : 256000
2019-02-15 05:37:36,880 : Image to text: 13.56, 34.78, 47.36, 12.0
2019-02-15 05:37:43,229 : Text to Image: 10.776, 30.12, 42.808, 15.0
2019-02-15 05:38:22,085 : samples : 320000
2019-02-15 05:38:31,487 : Image to text: 13.84, 35.28, 48.16, 11.0
2019-02-15 05:38:37,832 : Text to Image: 11.064, 30.456, 43.344, 14.0
2019-02-15 05:39:16,646 : samples : 384000
2019-02-15 05:39:25,995 : Image to text: 13.28, 34.12, 46.86, 12.0
2019-02-15 05:39:32,384 : Text to Image: 10.86, 30.308, 43.08, 14.0
2019-02-15 05:40:11,223 : samples : 448000
2019-02-15 05:40:20,614 : Image to text: 12.68, 34.94, 47.72, 12.0
2019-02-15 05:40:26,958 : Text to Image: 10.648, 29.716, 42.42, 15.0
2019-02-15 05:41:05,778 : samples : 512000
2019-02-15 05:41:15,123 : Image to text: 14.28, 34.52, 48.36, 11.0
2019-02-15 05:41:21,476 : Text to Image: 11.06, 30.792, 43.048, 14.0
2019-02-15 05:41:54,338 : Epoch 5 finished
2019-02-15 05:41:54,697 : Image to text: 31.4, 66.0, 80.5, 3.0
2019-02-15 05:41:54,958 : Text to Image: 25.82, 61.46, 77.28, 4.0
2019-02-15 05:41:55,317 : Image to text: 30.6, 63.3, 78.9, 3.0
2019-02-15 05:41:55,578 : Text to Image: 25.44, 59.54, 75.94, 4.0
2019-02-15 05:41:55,940 : Image to text: 33.6, 66.2, 80.5, 3.0
2019-02-15 05:41:56,201 : Text to Image: 26.42, 61.0, 76.64, 4.0
2019-02-15 05:41:56,559 : Image to text: 33.1, 66.8, 80.8, 3.0
2019-02-15 05:41:56,820 : Text to Image: 26.02, 61.16, 77.14, 4.0
2019-02-15 05:41:57,178 : Image to text: 32.7, 65.2, 80.4, 3.0
2019-02-15 05:41:57,439 : Text to Image: 26.0, 60.96, 76.92, 4.0
2019-02-15 05:41:57,440 : Dev mean Text to Image: 25.939999999999998, 60.824, 76.78399999999999, 4.0
2019-02-15 05:41:57,440 : Dev mean Image to text: 32.28, 65.5, 80.22, 3.0
2019-02-15 05:41:57,440 : start epoch
2019-02-15 05:42:36,057 : samples : 64000
2019-02-15 05:42:45,377 : Image to text: 13.86, 36.08, 48.88, 11.0
2019-02-15 05:42:51,720 : Text to Image: 11.204, 31.112, 43.844, 14.0
2019-02-15 05:43:30,275 : samples : 128000
2019-02-15 05:43:39,585 : Image to text: 13.82, 34.94, 48.62, 11.0
2019-02-15 05:43:45,945 : Text to Image: 10.968, 30.716, 43.236, 14.0
2019-02-15 05:44:24,195 : samples : 192000
2019-02-15 05:44:33,550 : Image to text: 14.02, 35.6, 49.82, 11.0
2019-02-15 05:44:39,897 : Text to Image: 10.952, 30.536, 43.224, 14.0
2019-02-15 05:45:18,352 : samples : 256000
2019-02-15 05:45:27,657 : Image to text: 13.14, 35.58, 49.06, 11.0
2019-02-15 05:45:34,026 : Text to Image: 10.9, 30.42, 43.624, 14.0
2019-02-15 05:46:12,291 : samples : 320000
2019-02-15 05:46:21,615 : Image to text: 13.82, 35.26, 48.74, 11.0
2019-02-15 05:46:27,960 : Text to Image: 10.984, 30.384, 43.22, 14.0
2019-02-15 05:47:06,221 : samples : 384000
2019-02-15 05:47:15,573 : Image to text: 14.16, 36.04, 49.26, 11.0
2019-02-15 05:47:21,927 : Text to Image: 10.976, 30.256, 43.184, 14.0
2019-02-15 05:48:00,194 : samples : 448000
2019-02-15 05:48:09,519 : Image to text: 13.6, 35.04, 48.98, 11.0
2019-02-15 05:48:15,885 : Text to Image: 10.896, 30.148, 43.096, 14.0
2019-02-15 05:48:54,063 : samples : 512000
2019-02-15 05:49:03,429 : Image to text: 14.3, 35.1, 49.0, 11.0
2019-02-15 05:49:09,759 : Text to Image: 10.884, 30.54, 43.4, 14.0
2019-02-15 05:49:42,217 : Epoch 6 finished
2019-02-15 05:49:42,576 : Image to text: 34.4, 68.5, 82.3, 2.0
2019-02-15 05:49:42,837 : Text to Image: 27.34, 63.2, 78.64, 3.0
2019-02-15 05:49:43,195 : Image to text: 34.2, 65.3, 79.7, 3.0
2019-02-15 05:49:43,456 : Text to Image: 26.7, 60.94, 77.22, 4.0
2019-02-15 05:49:43,815 : Image to text: 34.6, 68.6, 81.2, 3.0
2019-02-15 05:49:44,075 : Text to Image: 27.14, 62.06, 77.48, 4.0
2019-02-15 05:49:44,434 : Image to text: 36.0, 68.6, 80.7, 3.0
2019-02-15 05:49:44,695 : Text to Image: 28.22, 61.86, 77.94, 3.0
2019-02-15 05:49:45,061 : Image to text: 33.6, 68.3, 81.7, 3.0
2019-02-15 05:49:45,333 : Text to Image: 27.56, 61.94, 77.16, 3.0
2019-02-15 05:49:45,333 : Dev mean Text to Image: 27.392000000000003, 62.0, 77.688, 3.4000000000000004
2019-02-15 05:49:45,333 : Dev mean Image to text: 34.56, 67.86, 81.12, 2.8000000000000003
2019-02-15 05:49:45,333 : start epoch
2019-02-15 05:50:23,419 : samples : 64000
2019-02-15 05:50:32,772 : Image to text: 14.3, 36.58, 49.34, 11.0
2019-02-15 05:50:39,101 : Text to Image: 11.096, 30.64, 43.192, 14.0
2019-02-15 05:51:17,491 : samples : 128000
2019-02-15 05:51:26,838 : Image to text: 14.9, 36.32, 49.82, 11.0
2019-02-15 05:51:33,226 : Text to Image: 11.572, 31.184, 44.14, 14.0
2019-02-15 05:52:12,003 : samples : 192000
2019-02-15 05:52:21,340 : Image to text: 14.46, 36.56, 49.68, 11.0
2019-02-15 05:52:27,691 : Text to Image: 11.404, 31.6, 44.46, 14.0
2019-02-15 05:53:06,308 : samples : 256000
2019-02-15 05:53:15,672 : Image to text: 14.08, 35.42, 48.8, 11.0
2019-02-15 05:53:22,014 : Text to Image: 11.064, 30.632, 43.44, 14.0
2019-02-15 05:54:00,669 : samples : 320000
2019-02-15 05:54:10,019 : Image to text: 13.72, 36.22, 50.12, 10.0
2019-02-15 05:54:16,385 : Text to Image: 11.396, 31.22, 43.96, 14.0
2019-02-15 05:54:55,012 : samples : 384000
2019-02-15 05:55:04,418 : Image to text: 13.5, 36.5, 49.18, 11.0
2019-02-15 05:55:10,764 : Text to Image: 11.264, 30.888, 44.1, 14.0
2019-02-15 05:55:49,427 : samples : 448000
2019-02-15 05:55:58,773 : Image to text: 14.18, 36.64, 50.42, 10.0
2019-02-15 05:56:05,124 : Text to Image: 11.428, 31.436, 44.34, 14.0
2019-02-15 05:56:43,443 : samples : 512000
2019-02-15 05:56:52,781 : Image to text: 14.86, 36.88, 50.18, 10.0
2019-02-15 05:56:59,123 : Text to Image: 11.476, 31.576, 44.388, 14.0
2019-02-15 05:57:31,676 : Epoch 7 finished
2019-02-15 05:57:32,037 : Image to text: 32.7, 68.1, 80.7, 3.0
2019-02-15 05:57:32,299 : Text to Image: 27.36, 62.76, 78.82, 3.0
2019-02-15 05:57:32,658 : Image to text: 31.2, 64.8, 79.3, 3.0
2019-02-15 05:57:32,920 : Text to Image: 26.46, 61.6, 77.86, 4.0
2019-02-15 05:57:33,279 : Image to text: 32.5, 69.5, 80.9, 3.0
2019-02-15 05:57:33,540 : Text to Image: 28.18, 62.96, 77.94, 3.0
2019-02-15 05:57:33,899 : Image to text: 33.7, 68.3, 80.2, 3.0
2019-02-15 05:57:34,160 : Text to Image: 28.12, 62.78, 79.02, 3.0
2019-02-15 05:57:34,518 : Image to text: 31.9, 67.2, 79.6, 3.0
2019-02-15 05:57:34,779 : Text to Image: 27.74, 62.32, 77.84, 4.0
2019-02-15 05:57:34,779 : Dev mean Text to Image: 27.572000000000003, 62.483999999999995, 78.29599999999999, 3.4000000000000004
2019-02-15 05:57:34,779 : Dev mean Image to text: 32.400000000000006, 67.58, 80.14, 3.0
2019-02-15 05:57:34,779 : start epoch
2019-02-15 05:58:12,961 : samples : 64000
2019-02-15 05:58:22,306 : Image to text: 14.1, 36.4, 50.12, 10.0
2019-02-15 05:58:28,650 : Text to Image: 11.248, 31.2, 43.864, 14.0
2019-02-15 05:59:06,887 : samples : 128000
2019-02-15 05:59:16,235 : Image to text: 14.16, 36.94, 50.14, 10.0
2019-02-15 05:59:22,569 : Text to Image: 11.48, 31.252, 44.228, 14.0
2019-02-15 06:00:00,760 : samples : 192000
2019-02-15 06:00:10,108 : Image to text: 14.32, 36.78, 49.98, 11.0
2019-02-15 06:00:16,485 : Text to Image: 11.508, 31.664, 44.756, 13.0
2019-02-15 06:00:54,706 : samples : 256000
2019-02-15 06:01:04,045 : Image to text: 14.02, 36.94, 50.16, 10.0
2019-02-15 06:01:10,383 : Text to Image: 11.368, 31.26, 44.608, 13.0
2019-02-15 06:01:48,523 : samples : 320000
2019-02-15 06:01:57,865 : Image to text: 14.7, 36.56, 50.5, 10.0
2019-02-15 06:02:04,230 : Text to Image: 11.424, 31.92, 44.944, 13.0
2019-02-15 06:02:42,378 : samples : 384000
2019-02-15 06:02:51,707 : Image to text: 13.84, 35.76, 50.22, 10.0
2019-02-15 06:02:58,050 : Text to Image: 11.66, 31.504, 44.456, 14.0
2019-02-15 06:03:36,246 : samples : 448000
2019-02-15 06:03:45,583 : Image to text: 14.56, 36.68, 50.66, 10.0
2019-02-15 06:03:51,913 : Text to Image: 11.484, 31.18, 44.188, 14.0
2019-02-15 06:04:30,124 : samples : 512000
2019-02-15 06:04:39,464 : Image to text: 13.4, 36.04, 50.26, 10.0
2019-02-15 06:04:45,825 : Text to Image: 11.744, 31.484, 44.664, 13.0
2019-02-15 06:05:18,377 : Epoch 8 finished
2019-02-15 06:05:18,736 : Image to text: 34.7, 69.1, 81.8, 3.0
2019-02-15 06:05:18,997 : Text to Image: 28.44, 63.3, 79.04, 3.0
2019-02-15 06:05:19,354 : Image to text: 33.5, 66.9, 80.8, 3.0
2019-02-15 06:05:19,615 : Text to Image: 26.38, 61.66, 77.7, 4.0
2019-02-15 06:05:19,973 : Image to text: 34.4, 68.2, 81.3, 3.0
2019-02-15 06:05:20,233 : Text to Image: 27.26, 63.18, 78.54, 3.0
2019-02-15 06:05:20,591 : Image to text: 34.9, 68.7, 81.5, 2.0
2019-02-15 06:05:20,852 : Text to Image: 28.16, 63.58, 78.62, 3.0
2019-02-15 06:05:21,209 : Image to text: 33.4, 69.1, 81.6, 3.0
2019-02-15 06:05:21,469 : Text to Image: 27.62, 61.92, 77.48, 4.0
2019-02-15 06:05:21,469 : Dev mean Text to Image: 27.572000000000003, 62.728, 78.276, 3.4000000000000004
2019-02-15 06:05:21,469 : Dev mean Image to text: 34.18, 68.4, 81.4, 2.8
2019-02-15 06:05:21,470 : start epoch
2019-02-15 06:05:59,597 : samples : 64000
2019-02-15 06:06:08,985 : Image to text: 15.08, 37.36, 50.3, 10.0
2019-02-15 06:06:15,346 : Text to Image: 11.7, 31.636, 44.24, 14.0
2019-02-15 06:06:53,488 : samples : 128000
2019-02-15 06:07:02,813 : Image to text: 14.8, 36.94, 50.78, 10.0
2019-02-15 06:07:09,163 : Text to Image: 11.444, 31.432, 44.284, 14.0
2019-02-15 06:07:47,377 : samples : 192000
2019-02-15 06:07:56,706 : Image to text: 14.2, 36.32, 50.76, 10.0
2019-02-15 06:08:03,062 : Text to Image: 11.532, 31.592, 44.4, 14.0
2019-02-15 06:08:41,225 : samples : 256000
2019-02-15 06:08:50,599 : Image to text: 14.76, 36.94, 50.52, 10.0
2019-02-15 06:08:56,936 : Text to Image: 11.64, 31.992, 45.124, 13.0
2019-02-15 06:09:35,123 : samples : 320000
2019-02-15 06:09:44,439 : Image to text: 14.62, 37.02, 50.62, 10.0
2019-02-15 06:09:50,831 : Text to Image: 11.88, 31.696, 45.14, 13.0
2019-02-15 06:10:28,961 : samples : 384000
2019-02-15 06:10:38,321 : Image to text: 14.6, 36.76, 49.8, 11.0
2019-02-15 06:10:44,662 : Text to Image: 11.636, 32.044, 44.724, 13.0
2019-02-15 06:11:22,741 : samples : 448000
2019-02-15 06:11:32,111 : Image to text: 14.5, 36.86, 50.4, 10.0
2019-02-15 06:11:38,445 : Text to Image: 11.348, 31.188, 44.304, 14.0
2019-02-15 06:12:16,510 : samples : 512000
2019-02-15 06:12:25,822 : Image to text: 14.74, 36.96, 50.7, 10.0
2019-02-15 06:12:32,258 : Text to Image: 11.684, 31.82, 44.96, 13.0
2019-02-15 06:13:04,614 : Epoch 9 finished
2019-02-15 06:13:04,973 : Image to text: 34.7, 68.0, 82.6, 3.0
2019-02-15 06:13:05,233 : Text to Image: 29.26, 63.54, 79.54, 3.0
2019-02-15 06:13:05,591 : Image to text: 34.1, 68.6, 82.2, 3.0
2019-02-15 06:13:05,852 : Text to Image: 26.94, 61.9, 78.44, 4.0
2019-02-15 06:13:06,210 : Image to text: 35.2, 67.9, 82.2, 3.0
2019-02-15 06:13:06,472 : Text to Image: 27.94, 63.04, 78.76, 3.0
2019-02-15 06:13:06,831 : Image to text: 34.7, 68.4, 82.1, 3.0
2019-02-15 06:13:07,092 : Text to Image: 27.84, 62.92, 79.28, 3.0
2019-02-15 06:13:07,451 : Image to text: 34.2, 68.7, 82.4, 3.0
2019-02-15 06:13:07,712 : Text to Image: 28.64, 62.84, 78.12, 3.0
2019-02-15 06:13:07,712 : Dev mean Text to Image: 28.124000000000002, 62.848, 78.828, 3.2
2019-02-15 06:13:07,712 : Dev mean Image to text: 34.580000000000005, 68.32000000000001, 82.30000000000001, 3.0
2019-02-15 06:13:07,712 : start epoch
2019-02-15 06:13:45,746 : samples : 64000
2019-02-15 06:13:55,057 : Image to text: 14.56, 36.86, 50.4, 10.0
2019-02-15 06:14:01,428 : Text to Image: 11.572, 32.012, 44.888, 13.0
2019-02-15 06:14:39,427 : samples : 128000
2019-02-15 06:14:48,781 : Image to text: 15.26, 37.54, 51.46, 10.0
2019-02-15 06:14:55,117 : Text to Image: 11.804, 32.252, 45.02, 13.0
2019-02-15 06:15:33,146 : samples : 192000
2019-02-15 06:15:42,474 : Image to text: 14.86, 37.48, 51.04, 10.0
2019-02-15 06:15:48,842 : Text to Image: 12.204, 32.076, 44.908, 13.0
2019-02-15 06:16:27,014 : samples : 256000
2019-02-15 06:16:36,400 : Image to text: 14.92, 37.8, 50.58, 10.0
2019-02-15 06:16:42,776 : Text to Image: 11.772, 31.908, 44.684, 13.0
2019-02-15 06:17:21,233 : samples : 320000
2019-02-15 06:17:30,581 : Image to text: 14.52, 37.58, 51.3, 10.0
2019-02-15 06:17:36,925 : Text to Image: 11.784, 32.204, 45.068, 13.0
2019-02-15 06:18:15,415 : samples : 384000
2019-02-15 06:18:25,019 : Image to text: 15.14, 37.4, 51.22, 10.0
2019-02-15 06:18:31,430 : Text to Image: 11.664, 31.844, 44.884, 13.0
2019-02-15 06:19:10,525 : samples : 448000
2019-02-15 06:19:19,891 : Image to text: 14.22, 37.88, 51.3, 10.0
2019-02-15 06:19:26,232 : Text to Image: 11.844, 32.116, 45.04, 13.0
2019-02-15 06:20:04,490 : samples : 512000
2019-02-15 06:20:13,817 : Image to text: 14.64, 36.54, 49.92, 11.0
2019-02-15 06:20:20,217 : Text to Image: 11.54, 31.888, 44.776, 13.0
2019-02-15 06:20:52,766 : Epoch 10 finished
2019-02-15 06:20:53,125 : Image to text: 34.1, 67.6, 81.7, 3.0
2019-02-15 06:20:53,385 : Text to Image: 28.66, 63.74, 79.52, 3.0
2019-02-15 06:20:53,743 : Image to text: 32.8, 67.1, 81.8, 3.0
2019-02-15 06:20:54,004 : Text to Image: 26.78, 61.98, 78.08, 4.0
2019-02-15 06:20:54,362 : Image to text: 35.0, 68.4, 81.3, 3.0
2019-02-15 06:20:54,622 : Text to Image: 28.26, 62.82, 78.64, 3.0
2019-02-15 06:20:54,980 : Image to text: 32.6, 69.7, 82.2, 3.0
2019-02-15 06:20:55,241 : Text to Image: 28.34, 63.46, 79.28, 3.0
2019-02-15 06:20:55,600 : Image to text: 34.0, 68.8, 81.1, 3.0
2019-02-15 06:20:55,863 : Text to Image: 27.7, 62.38, 78.46, 3.0
2019-02-15 06:20:55,864 : Dev mean Text to Image: 27.948, 62.876, 78.79599999999999, 3.2
2019-02-15 06:20:55,864 : Dev mean Image to text: 33.699999999999996, 68.32000000000001, 81.62, 3.0
2019-02-15 06:20:55,864 : start epoch
2019-02-15 06:21:34,183 : samples : 64000
2019-02-15 06:21:43,514 : Image to text: 14.44, 37.02, 51.42, 10.0
2019-02-15 06:21:49,861 : Text to Image: 11.656, 31.84, 45.096, 13.0
2019-02-15 06:22:28,078 : samples : 128000
2019-02-15 06:22:37,449 : Image to text: 14.52, 36.96, 51.08, 10.0
2019-02-15 06:22:43,797 : Text to Image: 11.48, 31.928, 45.176, 13.0
2019-02-15 06:23:22,068 : samples : 192000
2019-02-15 06:23:31,445 : Image to text: 14.6, 37.94, 52.08, 9.0
2019-02-15 06:23:37,788 : Text to Image: 11.788, 32.268, 45.388, 13.0
2019-02-15 06:24:16,075 : samples : 256000
2019-02-15 06:24:25,412 : Image to text: 15.18, 37.88, 51.62, 10.0
2019-02-15 06:24:31,781 : Text to Image: 11.972, 32.172, 44.9, 13.0
2019-02-15 06:25:10,062 : samples : 320000
2019-02-15 06:25:19,395 : Image to text: 14.86, 37.2, 51.24, 10.0
2019-02-15 06:25:25,732 : Text to Image: 11.624, 32.032, 44.852, 13.0
2019-02-15 06:26:04,076 : samples : 384000
2019-02-15 06:26:13,403 : Image to text: 14.46, 38.64, 51.2, 10.0
2019-02-15 06:26:19,756 : Text to Image: 11.768, 33.036, 45.968, 13.0
2019-02-15 06:26:58,033 : samples : 448000
2019-02-15 06:27:07,472 : Image to text: 14.94, 37.58, 51.54, 10.0
2019-02-15 06:27:13,840 : Text to Image: 11.732, 32.528, 45.224, 13.0
2019-02-15 06:27:52,056 : samples : 512000
2019-02-15 06:28:01,417 : Image to text: 15.28, 37.36, 51.26, 10.0
2019-02-15 06:28:07,764 : Text to Image: 12.004, 32.536, 45.64, 13.0
2019-02-15 06:28:40,346 : Epoch 11 finished
2019-02-15 06:28:40,709 : Image to text: 35.3, 69.9, 82.8, 3.0
2019-02-15 06:28:40,970 : Text to Image: 29.04, 63.82, 78.6, 3.0
2019-02-15 06:28:41,329 : Image to text: 35.2, 68.8, 80.5, 3.0
2019-02-15 06:28:41,589 : Text to Image: 27.28, 61.28, 78.04, 4.0
2019-02-15 06:28:41,947 : Image to text: 36.0, 68.7, 82.4, 3.0
2019-02-15 06:28:42,208 : Text to Image: 28.76, 63.7, 78.7, 3.0
2019-02-15 06:28:42,566 : Image to text: 36.1, 69.4, 82.7, 2.0
2019-02-15 06:28:42,827 : Text to Image: 27.74, 63.32, 79.46, 3.0
2019-02-15 06:28:43,184 : Image to text: 35.1, 67.9, 82.0, 3.0
2019-02-15 06:28:43,444 : Text to Image: 28.12, 62.92, 78.36, 3.0
2019-02-15 06:28:43,444 : Dev mean Text to Image: 28.188000000000002, 63.007999999999996, 78.63199999999999, 3.2
2019-02-15 06:28:43,444 : Dev mean Image to text: 35.540000000000006, 68.94000000000001, 82.08000000000001, 2.8
2019-02-15 06:28:43,445 : start epoch
2019-02-15 06:29:21,749 : samples : 64000
2019-02-15 06:29:31,098 : Image to text: 15.26, 38.2, 52.5, 9.0
2019-02-15 06:29:37,433 : Text to Image: 12.108, 32.492, 45.42, 13.0
2019-02-15 06:30:15,617 : samples : 128000
2019-02-15 06:30:24,957 : Image to text: 14.22, 36.96, 51.0, 10.0
2019-02-15 06:30:31,326 : Text to Image: 11.912, 32.288, 45.576, 13.0
2019-02-15 06:31:10,133 : samples : 192000
2019-02-15 06:31:19,562 : Image to text: 14.96, 37.64, 51.96, 10.0
2019-02-15 06:31:25,913 : Text to Image: 12.012, 32.62, 45.748, 13.0
2019-02-15 06:32:04,755 : samples : 256000
2019-02-15 06:32:14,121 : Image to text: 15.26, 38.06, 51.24, 10.0
2019-02-15 06:32:20,485 : Text to Image: 12.18, 32.332, 45.38, 13.0
2019-02-15 06:32:59,254 : samples : 320000
2019-02-15 06:33:08,619 : Image to text: 15.08, 38.36, 51.54, 10.0
2019-02-15 06:33:14,982 : Text to Image: 11.892, 32.172, 45.024, 13.0
2019-02-15 06:33:53,828 : samples : 384000
2019-02-15 06:34:03,201 : Image to text: 14.84, 38.32, 52.22, 9.0
2019-02-15 06:34:09,550 : Text to Image: 12.364, 32.676, 45.688, 13.0
2019-02-15 06:34:48,377 : samples : 448000
2019-02-15 06:34:57,739 : Image to text: 16.28, 38.5, 52.18, 9.0
2019-02-15 06:35:04,138 : Text to Image: 11.94, 32.364, 45.472, 13.0
2019-02-15 06:35:42,885 : samples : 512000
2019-02-15 06:35:52,317 : Image to text: 15.26, 38.68, 52.42, 9.0
2019-02-15 06:35:58,667 : Text to Image: 12.244, 32.632, 45.596, 13.0
2019-02-15 06:36:31,655 : Epoch 12 finished
2019-02-15 06:36:32,015 : Image to text: 34.0, 68.2, 82.9, 3.0
2019-02-15 06:36:32,276 : Text to Image: 29.12, 64.62, 79.9, 3.0
2019-02-15 06:36:32,635 : Image to text: 33.8, 65.6, 81.3, 3.0
2019-02-15 06:36:32,896 : Text to Image: 28.02, 62.18, 78.74, 3.0
2019-02-15 06:36:33,254 : Image to text: 35.1, 66.2, 81.1, 2.0
2019-02-15 06:36:33,515 : Text to Image: 28.54, 63.76, 79.54, 3.0
2019-02-15 06:36:33,874 : Image to text: 35.1, 71.3, 81.6, 3.0
2019-02-15 06:36:34,135 : Text to Image: 29.0, 64.04, 80.02, 3.0
2019-02-15 06:36:34,494 : Image to text: 34.4, 67.3, 81.6, 3.0
2019-02-15 06:36:34,755 : Text to Image: 29.0, 64.16, 78.9, 3.0
2019-02-15 06:36:34,755 : Dev mean Text to Image: 28.736000000000004, 63.751999999999995, 79.42, 3.0
2019-02-15 06:36:34,755 : Dev mean Image to text: 34.48, 67.72, 81.69999999999999, 2.8000000000000003
2019-02-15 06:36:37,852 : 
Test scores | Image to text:             34.42, 68.92, 81.52000000000001, 3.0
2019-02-15 06:36:37,852 : Test scores | Text to image:             27.964, 62.983999999999995, 78.51599999999999, 3.1999999999999997

2019-02-15 06:36:37,952 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 06:36:38,301 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 06:36:38,907 : loading BERT model bert-base-uncased
2019-02-15 06:36:38,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:36:38,935 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:36:38,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0u7g29vn
2019-02-15 06:36:41,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:36:42,584 : Computing embeddings for train/dev/test
2019-02-15 06:38:01,570 : Computed embeddings
2019-02-15 06:38:01,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:38:52,457 : [('reg:1e-05', 85.35), ('reg:0.0001', 83.63), ('reg:0.001', 78.35), ('reg:0.01', 63.56)]
2019-02-15 06:38:52,457 : Validation : best param found is reg = 1e-05 with score             85.35
2019-02-15 06:38:52,457 : Evaluating...
2019-02-15 06:39:05,207 : 
Dev acc : 85.3 Test acc : 85.4 for LENGTH classification

2019-02-15 06:39:05,208 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 06:39:05,538 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 06:39:05,585 : loading BERT model bert-base-uncased
2019-02-15 06:39:05,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:39:05,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:39:05,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzd7c6lvu
2019-02-15 06:39:07,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:39:09,243 : Computing embeddings for train/dev/test
2019-02-15 06:40:23,047 : Computed embeddings
2019-02-15 06:40:23,047 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:41:14,711 : [('reg:1e-05', 57.61), ('reg:0.0001', 36.71), ('reg:0.001', 4.91), ('reg:0.01', 0.97)]
2019-02-15 06:41:14,711 : Validation : best param found is reg = 1e-05 with score             57.61
2019-02-15 06:41:14,712 : Evaluating...
2019-02-15 06:41:22,874 : 
Dev acc : 57.6 Test acc : 57.6 for WORDCONTENT classification

2019-02-15 06:41:22,876 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 06:41:23,216 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 06:41:23,280 : loading BERT model bert-base-uncased
2019-02-15 06:41:23,280 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:41:23,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:41:23,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp59lnbzrh
2019-02-15 06:41:25,652 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:41:26,995 : Computing embeddings for train/dev/test
2019-02-15 06:42:36,280 : Computed embeddings
2019-02-15 06:42:36,280 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:43:19,440 : [('reg:1e-05', 36.17), ('reg:0.0001', 36.1), ('reg:0.001', 34.51), ('reg:0.01', 29.23)]
2019-02-15 06:43:19,440 : Validation : best param found is reg = 1e-05 with score             36.17
2019-02-15 06:43:19,440 : Evaluating...
2019-02-15 06:43:30,113 : 
Dev acc : 36.2 Test acc : 35.3 for DEPTH classification

2019-02-15 06:43:30,114 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 06:43:30,480 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 06:43:30,542 : loading BERT model bert-base-uncased
2019-02-15 06:43:30,542 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:43:30,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:43:30,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8punl85j
2019-02-15 06:43:32,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:43:34,271 : Computing embeddings for train/dev/test
2019-02-15 06:44:39,151 : Computed embeddings
2019-02-15 06:44:39,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:45:27,001 : [('reg:1e-05', 75.96), ('reg:0.0001', 74.75), ('reg:0.001', 68.06), ('reg:0.01', 57.24)]
2019-02-15 06:45:27,001 : Validation : best param found is reg = 1e-05 with score             75.96
2019-02-15 06:45:27,001 : Evaluating...
2019-02-15 06:45:37,272 : 
Dev acc : 76.0 Test acc : 76.0 for TOPCONSTITUENTS classification

2019-02-15 06:45:37,273 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 06:45:37,777 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 06:45:37,841 : loading BERT model bert-base-uncased
2019-02-15 06:45:37,841 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:45:37,870 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:45:37,871 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd7v7wr7k
2019-02-15 06:45:40,175 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:45:41,529 : Computing embeddings for train/dev/test
2019-02-15 06:46:51,504 : Computed embeddings
2019-02-15 06:46:51,504 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:47:46,462 : [('reg:1e-05', 90.54), ('reg:0.0001', 90.2), ('reg:0.001', 90.07), ('reg:0.01', 88.69)]
2019-02-15 06:47:46,462 : Validation : best param found is reg = 1e-05 with score             90.54
2019-02-15 06:47:46,462 : Evaluating...
2019-02-15 06:48:03,023 : 
Dev acc : 90.5 Test acc : 90.0 for BIGRAMSHIFT classification

2019-02-15 06:48:03,024 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 06:48:03,573 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 06:48:03,637 : loading BERT model bert-base-uncased
2019-02-15 06:48:03,637 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:48:03,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:48:03,666 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmhe44ojn
2019-02-15 06:48:05,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:48:07,276 : Computing embeddings for train/dev/test
2019-02-15 06:49:15,699 : Computed embeddings
2019-02-15 06:49:15,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:49:58,428 : [('reg:1e-05', 89.49), ('reg:0.0001', 89.61), ('reg:0.001', 90.04), ('reg:0.01', 90.15)]
2019-02-15 06:49:58,429 : Validation : best param found is reg = 0.01 with score             90.15
2019-02-15 06:49:58,429 : Evaluating...
2019-02-15 06:50:08,845 : 
Dev acc : 90.2 Test acc : 89.5 for TENSE classification

2019-02-15 06:50:08,846 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 06:50:09,255 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 06:50:09,316 : loading BERT model bert-base-uncased
2019-02-15 06:50:09,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:50:09,341 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:50:09,341 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp93kjpfmk
2019-02-15 06:50:11,631 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:50:12,971 : Computing embeddings for train/dev/test
2019-02-15 06:51:25,674 : Computed embeddings
2019-02-15 06:51:25,674 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:52:11,496 : [('reg:1e-05', 85.59), ('reg:0.0001', 85.67), ('reg:0.001', 85.79), ('reg:0.01', 84.93)]
2019-02-15 06:52:11,496 : Validation : best param found is reg = 0.001 with score             85.79
2019-02-15 06:52:11,496 : Evaluating...
2019-02-15 06:52:21,743 : 
Dev acc : 85.8 Test acc : 85.9 for SUBJNUMBER classification

2019-02-15 06:52:21,744 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 06:52:22,142 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 06:52:22,206 : loading BERT model bert-base-uncased
2019-02-15 06:52:22,206 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:52:22,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:52:22,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyehi6si6
2019-02-15 06:52:24,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:52:25,941 : Computing embeddings for train/dev/test
2019-02-15 06:53:37,328 : Computed embeddings
2019-02-15 06:53:37,328 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:54:22,833 : [('reg:1e-05', 82.05), ('reg:0.0001', 82.02), ('reg:0.001', 81.86), ('reg:0.01', 80.79)]
2019-02-15 06:54:22,833 : Validation : best param found is reg = 1e-05 with score             82.05
2019-02-15 06:54:22,833 : Evaluating...
2019-02-15 06:54:36,165 : 
Dev acc : 82.0 Test acc : 82.3 for OBJNUMBER classification

2019-02-15 06:54:36,166 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 06:54:36,537 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 06:54:36,605 : loading BERT model bert-base-uncased
2019-02-15 06:54:36,605 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:54:36,724 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:54:36,724 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn8gtfffs
2019-02-15 06:54:39,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:54:40,379 : Computing embeddings for train/dev/test
2019-02-15 06:56:02,376 : Computed embeddings
2019-02-15 06:56:02,376 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:56:46,638 : [('reg:1e-05', 65.86), ('reg:0.0001', 65.85), ('reg:0.001', 65.8), ('reg:0.01', 65.2)]
2019-02-15 06:56:46,638 : Validation : best param found is reg = 1e-05 with score             65.86
2019-02-15 06:56:46,638 : Evaluating...
2019-02-15 06:56:59,405 : 
Dev acc : 65.9 Test acc : 65.5 for ODDMANOUT classification

2019-02-15 06:56:59,406 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 06:56:59,991 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 06:57:00,067 : loading BERT model bert-base-uncased
2019-02-15 06:57:00,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:57:00,097 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:57:00,097 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2d_eoiwv
2019-02-15 06:57:02,393 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:57:03,729 : Computing embeddings for train/dev/test
2019-02-15 06:58:25,168 : Computed embeddings
2019-02-15 06:58:25,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 06:58:55,273 : [('reg:1e-05', 70.52), ('reg:0.0001', 70.32), ('reg:0.001', 69.39), ('reg:0.01', 67.36)]
2019-02-15 06:58:55,273 : Validation : best param found is reg = 1e-05 with score             70.52
2019-02-15 06:58:55,273 : Evaluating...
2019-02-15 06:59:02,983 : 
Dev acc : 70.5 Test acc : 70.3 for COORDINATIONINVERSION classification

2019-02-15 06:59:02,985 : total results: {'STS12': {'MSRpar': {'pearson': (0.3828151259012993, 1.3792388318332394e-27), 'spearman': SpearmanrResult(correlation=0.4112583963770455, pvalue=5.657478670992601e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3291030760088587, 2.1041572932514723e-20), 'spearman': SpearmanrResult(correlation=0.369507392647296, pvalue=1.1195175402053012e-25), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48819885929891493, 7.291906855929607e-29), 'spearman': SpearmanrResult(correlation=0.5826315527518718, pvalue=4.482794726754743e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5921556820933219, 3.6211176833430453e-72), 'spearman': SpearmanrResult(correlation=0.5924335763710942, pvalue=2.994174846102081e-72), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6180302217145976, 2.1189500583553706e-43), 'spearman': SpearmanrResult(correlation=0.5631719647854353, pvalue=9.147317638453837e-35), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48206059300339843, 'wmean': 0.46613022776220603}, 'spearman': {'mean': 0.5038005765865485, 'wmean': 0.48971461412775885}}}, 'STS13': {'FNWN': {'pearson': (0.32025624012500503, 7.039822301099561e-06), 'spearman': SpearmanrResult(correlation=0.3504611661097847, pvalue=7.662146373482486e-07), 'nsamples': 189}, 'headlines': {'pearson': (0.5885193593828035, 4.284253158185351e-71), 'spearman': SpearmanrResult(correlation=0.5582382056762079, pvalue=1.1539402900416263e-62), 'nsamples': 750}, 'OnWN': {'pearson': (0.4329782618444952, 4.8794205155135395e-27), 'spearman': SpearmanrResult(correlation=0.4489265413514015, pvalue=3.5757180162607706e-29), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4472512871174346, 'wmean': 0.49654583587699364}, 'spearman': {'mean': 0.452541971045798, 'wmean': 0.49117573623336097}}}, 'STS14': {'deft-forum': {'pearson': (0.25926635833519024, 2.4014034407490316e-08), 'spearman': SpearmanrResult(correlation=0.26324895385057673, pvalue=1.4353260786388081e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.7286834968974828, 6.430162989544107e-51), 'spearman': SpearmanrResult(correlation=0.6961760636778783, pvalue=8.357320448299725e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5536685992746222, 1.8205999525218607e-61), 'spearman': SpearmanrResult(correlation=0.5103776615530352, pvalue=5.513387879996314e-51), 'nsamples': 750}, 'images': {'pearson': (0.42816357646345854, 8.655753653454326e-35), 'spearman': SpearmanrResult(correlation=0.4384485898641238, pvalue=1.3953771383681995e-36), 'nsamples': 750}, 'OnWN': {'pearson': (0.6203270709048873, 5.779768065261601e-81), 'spearman': SpearmanrResult(correlation=0.6486463661064126, pvalue=9.250288619984482e-91), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6670283414239054, 1.0640815920823196e-97), 'spearman': SpearmanrResult(correlation=0.5888555482703328, pvalue=3.41382023492567e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5428562405499244, 'wmean': 0.543244160365396}, 'spearman': {'mean': 0.5242921972203932, 'wmean': 0.5245495927150804}}}, 'STS15': {'answers-forums': {'pearson': (0.5606909538224976, 1.971593224937272e-32), 'spearman': SpearmanrResult(correlation=0.560940963193196, pvalue=1.8259676477691671e-32), 'nsamples': 375}, 'answers-students': {'pearson': (0.655364212752704, 3.0659654433576466e-93), 'spearman': SpearmanrResult(correlation=0.6645993466553045, pvalue=9.38189825987902e-97), 'nsamples': 750}, 'belief': {'pearson': (0.6659891896926381, 2.0610344388712928e-49), 'spearman': SpearmanrResult(correlation=0.6872367218304624, pvalue=1.008021469085547e-53), 'nsamples': 375}, 'headlines': {'pearson': (0.6003190730228357, 1.2575411258461446e-74), 'spearman': SpearmanrResult(correlation=0.593983199140433, pvalue=1.0336906562857055e-72), 'nsamples': 750}, 'images': {'pearson': (0.6201764073572635, 6.476694886268889e-81), 'spearman': SpearmanrResult(correlation=0.6276716567021452, pvalue=2.082425897712839e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6205079673295877, 'wmean': 0.6222999412225928}, 'spearman': {'mean': 0.6268863775043082, 'wmean': 0.6275857612524279}}}, 'STS16': {'answer-answer': {'pearson': (0.5331622171576518, 4.6640449308039206e-20), 'spearman': SpearmanrResult(correlation=0.5294180813322269, pvalue=9.442088323119602e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6373108757495696, 8.830760648461452e-30), 'spearman': SpearmanrResult(correlation=0.6452597785042347, pvalue=1.027785607363864e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7408326713043127, 2.790536038332583e-41), 'spearman': SpearmanrResult(correlation=0.754694047222435, pvalue=1.2852623758828504e-43), 'nsamples': 230}, 'postediting': {'pearson': (0.7986297636075997, 2.7391574301305685e-55), 'spearman': SpearmanrResult(correlation=0.8279465722413261, pvalue=1.0083578057039882e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.27755221784152473, 4.727847388318004e-05), 'spearman': SpearmanrResult(correlation=0.27753398112505406, pvalue=4.733497623980791e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5974975491321317, 'wmean': 0.604872851154141}, 'spearman': {'mean': 0.6069704920850554, 'wmean': 0.6144562175793592}}}, 'MR': {'devacc': 81.11, 'acc': 81.05, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.17, 'acc': 87.02, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.56, 'acc': 88.64, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.41, 'acc': 94.77, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.89, 'acc': 85.39, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.05, 'acc': 43.53, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 86.56, 'acc': 93.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 75.47, 'acc': 72.52, 'f1': 78.74, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 77.8, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8000069157066063, 'pearson': 0.8012832496078378, 'spearman': 0.736573993322525, 'mse': 0.3644766684858283, 'yhat': array([2.4171165 , 3.82702025, 1.57020752, ..., 3.15177663, 4.48826416,        4.3184512 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6956992452228284, 'pearson': 0.6451650982494892, 'spearman': 0.6412762430950625, 'mse': 1.5022715488749476, 'yhat': array([1.5141775 , 1.99109381, 2.41360134, ..., 3.8456554 , 3.69567911,        3.63102346]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.2, 'acc': 65.22, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 356.38800000000003, 'acc': [(34.42, 68.92, 81.52000000000001, 3.0), (27.964, 62.983999999999995, 78.51599999999999, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 85.35, 'acc': 85.37, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 57.61, 'acc': 57.6, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 36.17, 'acc': 35.27, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 75.96, 'acc': 75.97, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.54, 'acc': 89.97, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.15, 'acc': 89.54, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.79, 'acc': 85.89, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.05, 'acc': 82.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.86, 'acc': 65.51, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.52, 'acc': 70.35, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 06:59:02,986 : STS12 p=0.4661, STS12 s=0.4897, STS13 p=0.4965, STS13 s=0.4912, STS14 p=0.5432, STS14 s=0.5245, STS15 p=0.6223, STS15 s=0.6276, STS 16 p=0.6049, STS16 s=0.6145, STS B p=0.6452, STS B s=0.6413, STS B m=1.5023, SICK-R p=0.8013, SICK-R s=0.7366, SICK-P m=0.3645
2019-02-15 06:59:02,986 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 06:59:02,986 : 0.4661,0.4897,0.4965,0.4912,0.5432,0.5245,0.6223,0.6276,0.6049,0.6145,0.6452,0.6413,1.5023,0.8013,0.7366,0.3645
2019-02-15 06:59:02,986 : MR=81.05, CR=87.02, SUBJ=94.77, MPQA=88.64, SST-B=85.39, SST-F=43.53, TREC=93.20, SICK-E=77.80, SNLI=65.22, MRPC=72.52, MRPC f=78.74
2019-02-15 06:59:02,986 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 06:59:02,986 : 81.05,87.02,94.77,88.64,85.39,43.53,93.20,77.80,65.22,72.52,78.74
2019-02-15 06:59:02,986 : COCO r1i2t=34.42, COCO r5i2t=68.92, COCO r10i2t=81.52, COCO medr_i2t=3.00, COCO r1t2i=27.96, COCO r5t2i=62.98, COCO r10t2i=78.52, COCO medr_t2i=3.20
2019-02-15 06:59:02,986 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 06:59:02,986 : 34.42,68.92,81.52,3.00,27.96,62.98,78.52,3.20
2019-02-15 06:59:02,986 : SentLen=85.37, WC=57.60, TreeDepth=35.27, TopConst=75.97, BShift=89.97, Tense=89.54, SubjNum=85.89, ObjNum=82.35, SOMO=65.51, CoordInv=70.35, average=73.78
2019-02-15 06:59:02,986 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 06:59:02,986 : 85.37,57.60,35.27,75.97,89.97,89.54,85.89,82.35,65.51,70.35,73.78
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:02,986 : layer 10
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:02,986 : ********************************************************************************
2019-02-15 06:59:03,073 : ***** Transfer task : STS12 *****


2019-02-15 06:59:03,085 : loading BERT model bert-base-uncased
2019-02-15 06:59:03,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:03,102 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:03,102 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0vhi5bnb
2019-02-15 06:59:05,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:08,267 : MSRpar : pearson = 0.3743, spearman = 0.4151
2019-02-15 06:59:08,915 : MSRvid : pearson = 0.3791, spearman = 0.4113
2019-02-15 06:59:09,468 : SMTeuroparl : pearson = 0.5222, spearman = 0.6099
2019-02-15 06:59:10,457 : surprise.OnWN : pearson = 0.5902, spearman = 0.5909
2019-02-15 06:59:11,013 : surprise.SMTnews : pearson = 0.5917, spearman = 0.5589
2019-02-15 06:59:11,014 : ALL (weighted average) : Pearson = 0.4773,             Spearman = 0.5038
2019-02-15 06:59:11,014 : ALL (average) : Pearson = 0.4915,             Spearman = 0.5172

2019-02-15 06:59:11,014 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 06:59:11,022 : loading BERT model bert-base-uncased
2019-02-15 06:59:11,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:11,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:11,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuvzlq40q
2019-02-15 06:59:13,323 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:15,164 : FNWN : pearson = 0.3346, spearman = 0.3854
2019-02-15 06:59:15,926 : headlines : pearson = 0.6114, spearman = 0.5797
2019-02-15 06:59:16,496 : OnWN : pearson = 0.5665, spearman = 0.5776
2019-02-15 06:59:16,496 : ALL (weighted average) : Pearson = 0.5597,             Spearman = 0.5545
2019-02-15 06:59:16,496 : ALL (average) : Pearson = 0.5042,             Spearman = 0.5143

2019-02-15 06:59:16,497 : ***** Transfer task : STS14 *****


2019-02-15 06:59:16,512 : loading BERT model bert-base-uncased
2019-02-15 06:59:16,512 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:16,529 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:16,529 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp34j17oo7
2019-02-15 06:59:18,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:20,733 : deft-forum : pearson = 0.2913, spearman = 0.2889
2019-02-15 06:59:21,339 : deft-news : pearson = 0.7533, spearman = 0.7214
2019-02-15 06:59:22,177 : headlines : pearson = 0.5759, spearman = 0.5278
2019-02-15 06:59:22,982 : images : pearson = 0.4319, spearman = 0.4331
2019-02-15 06:59:23,797 : OnWN : pearson = 0.6978, spearman = 0.7210
2019-02-15 06:59:24,864 : tweet-news : pearson = 0.6670, spearman = 0.5886
2019-02-15 06:59:24,864 : ALL (weighted average) : Pearson = 0.5698,             Spearman = 0.5465
2019-02-15 06:59:24,864 : ALL (average) : Pearson = 0.5695,             Spearman = 0.5468

2019-02-15 06:59:24,865 : ***** Transfer task : STS15 *****


2019-02-15 06:59:24,896 : loading BERT model bert-base-uncased
2019-02-15 06:59:24,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:24,913 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:24,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv2aka1ou
2019-02-15 06:59:27,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:29,275 : answers-forums : pearson = 0.5931, spearman = 0.5962
2019-02-15 06:59:30,082 : answers-students : pearson = 0.5937, spearman = 0.6128
2019-02-15 06:59:30,820 : belief : pearson = 0.6926, spearman = 0.7215
2019-02-15 06:59:31,681 : headlines : pearson = 0.6364, spearman = 0.6284
2019-02-15 06:59:32,501 : images : pearson = 0.6318, spearman = 0.6412
2019-02-15 06:59:32,501 : ALL (weighted average) : Pearson = 0.6262,             Spearman = 0.6353
2019-02-15 06:59:32,501 : ALL (average) : Pearson = 0.6295,             Spearman = 0.6400

2019-02-15 06:59:32,501 : ***** Transfer task : STS16 *****


2019-02-15 06:59:32,565 : loading BERT model bert-base-uncased
2019-02-15 06:59:32,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:32,582 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:32,582 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3iruelhi
2019-02-15 06:59:34,868 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:36,553 : answer-answer : pearson = 0.5548, spearman = 0.5392
2019-02-15 06:59:36,816 : headlines : pearson = 0.6461, spearman = 0.6555
2019-02-15 06:59:37,151 : plagiarism : pearson = 0.7671, spearman = 0.7783
2019-02-15 06:59:37,684 : postediting : pearson = 0.8073, spearman = 0.8305
2019-02-15 06:59:37,926 : question-question : pearson = 0.4378, spearman = 0.4305
2019-02-15 06:59:37,926 : ALL (weighted average) : Pearson = 0.6465,             Spearman = 0.6507
2019-02-15 06:59:37,927 : ALL (average) : Pearson = 0.6426,             Spearman = 0.6468

2019-02-15 06:59:37,927 : ***** Transfer task : MR *****


2019-02-15 06:59:37,942 : loading BERT model bert-base-uncased
2019-02-15 06:59:37,942 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 06:59:37,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 06:59:37,963 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp413fr1mv
2019-02-15 06:59:40,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 06:59:41,654 : Generating sentence embeddings
2019-02-15 06:59:53,091 : Generated sentence embeddings
2019-02-15 06:59:53,092 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 07:00:05,155 : Best param found at split 1: l2reg = 0.0001                 with score 80.98
2019-02-15 07:00:15,941 : Best param found at split 2: l2reg = 0.0001                 with score 81.11
2019-02-15 07:00:30,576 : Best param found at split 3: l2reg = 0.01                 with score 81.76
2019-02-15 07:00:38,844 : Best param found at split 4: l2reg = 1e-05                 with score 81.01
2019-02-15 07:00:48,282 : Best param found at split 5: l2reg = 0.01                 with score 81.56
2019-02-15 07:00:48,648 : Dev acc : 81.28 Test acc : 80.46

2019-02-15 07:00:48,649 : ***** Transfer task : CR *****


2019-02-15 07:00:48,657 : loading BERT model bert-base-uncased
2019-02-15 07:00:48,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:00:48,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:00:48,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsea3k2ks
2019-02-15 07:00:50,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:00:52,330 : Generating sentence embeddings
2019-02-15 07:00:55,484 : Generated sentence embeddings
2019-02-15 07:00:55,484 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 07:00:57,885 : Best param found at split 1: l2reg = 0.01                 with score 87.08
2019-02-15 07:01:00,910 : Best param found at split 2: l2reg = 0.0001                 with score 87.28
2019-02-15 07:01:06,630 : Best param found at split 3: l2reg = 0.001                 with score 87.75
2019-02-15 07:01:11,880 : Best param found at split 4: l2reg = 0.01                 with score 86.99
2019-02-15 07:01:16,468 : Best param found at split 5: l2reg = 0.01                 with score 87.65
2019-02-15 07:01:16,893 : Dev acc : 87.35 Test acc : 86.23

2019-02-15 07:01:16,893 : ***** Transfer task : MPQA *****


2019-02-15 07:01:16,904 : loading BERT model bert-base-uncased
2019-02-15 07:01:16,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:01:16,932 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:01:16,932 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnhejvjc6
2019-02-15 07:01:19,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:01:20,617 : Generating sentence embeddings
2019-02-15 07:01:23,772 : Generated sentence embeddings
2019-02-15 07:01:23,772 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 07:01:34,949 : Best param found at split 1: l2reg = 0.001                 with score 88.65
2019-02-15 07:01:45,370 : Best param found at split 2: l2reg = 0.001                 with score 87.67
2019-02-15 07:01:56,753 : Best param found at split 3: l2reg = 0.01                 with score 87.34
2019-02-15 07:02:10,581 : Best param found at split 4: l2reg = 0.001                 with score 88.78
2019-02-15 07:02:27,250 : Best param found at split 5: l2reg = 0.001                 with score 87.94
2019-02-15 07:02:28,052 : Dev acc : 88.08 Test acc : 88.39

2019-02-15 07:02:28,054 : ***** Transfer task : SUBJ *****


2019-02-15 07:02:28,075 : loading BERT model bert-base-uncased
2019-02-15 07:02:28,076 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:02:28,101 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:02:28,102 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprrhtvn71
2019-02-15 07:02:30,419 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:02:31,777 : Generating sentence embeddings
2019-02-15 07:02:43,008 : Generated sentence embeddings
2019-02-15 07:02:43,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 07:02:52,375 : Best param found at split 1: l2reg = 0.0001                 with score 95.5
2019-02-15 07:03:04,633 : Best param found at split 2: l2reg = 0.001                 with score 95.48
2019-02-15 07:03:15,982 : Best param found at split 3: l2reg = 0.0001                 with score 95.32
2019-02-15 07:03:27,082 : Best param found at split 4: l2reg = 0.001                 with score 95.74
2019-02-15 07:03:38,444 : Best param found at split 5: l2reg = 0.0001                 with score 95.41
2019-02-15 07:03:39,127 : Dev acc : 95.49 Test acc : 94.95

2019-02-15 07:03:39,129 : ***** Transfer task : SST Binary classification *****


2019-02-15 07:03:39,268 : loading BERT model bert-base-uncased
2019-02-15 07:03:39,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:03:39,290 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:03:39,290 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxw25jrof
2019-02-15 07:03:41,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:03:42,924 : Computing embedding for train
2019-02-15 07:04:20,595 : Computed train embeddings
2019-02-15 07:04:20,595 : Computing embedding for dev
2019-02-15 07:04:21,400 : Computed dev embeddings
2019-02-15 07:04:21,400 : Computing embedding for test
2019-02-15 07:04:23,082 : Computed test embeddings
2019-02-15 07:04:23,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 07:04:58,309 : [('reg:1e-05', 84.63), ('reg:0.0001', 84.52), ('reg:0.001', 86.47), ('reg:0.01', 84.98)]
2019-02-15 07:04:58,310 : Validation : best param found is reg = 0.001 with score             86.47
2019-02-15 07:04:58,310 : Evaluating...
2019-02-15 07:05:10,042 : 
Dev acc : 86.47 Test acc : 85.5 for             SST Binary classification

2019-02-15 07:05:10,042 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 07:05:10,110 : loading BERT model bert-base-uncased
2019-02-15 07:05:10,111 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:05:10,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:05:10,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1xefss9t
2019-02-15 07:05:12,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:05:13,771 : Computing embedding for train
2019-02-15 07:05:21,790 : Computed train embeddings
2019-02-15 07:05:21,791 : Computing embedding for dev
2019-02-15 07:05:22,831 : Computed dev embeddings
2019-02-15 07:05:22,831 : Computing embedding for test
2019-02-15 07:05:24,894 : Computed test embeddings
2019-02-15 07:05:24,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 07:05:29,571 : [('reg:1e-05', 44.6), ('reg:0.0001', 44.78), ('reg:0.001', 45.23), ('reg:0.01', 45.69)]
2019-02-15 07:05:29,572 : Validation : best param found is reg = 0.01 with score             45.69
2019-02-15 07:05:29,572 : Evaluating...
2019-02-15 07:05:30,703 : 
Dev acc : 45.69 Test acc : 44.93 for             SST Fine-Grained classification

2019-02-15 07:05:30,704 : ***** Transfer task : TREC *****


2019-02-15 07:05:30,721 : loading BERT model bert-base-uncased
2019-02-15 07:05:30,721 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:05:30,742 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:05:30,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi3ah0jn3
2019-02-15 07:05:33,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:05:37,307 : Computed train embeddings
2019-02-15 07:05:37,531 : Computed test embeddings
2019-02-15 07:05:37,531 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 07:05:48,810 : [('reg:1e-05', 82.67), ('reg:0.0001', 84.26), ('reg:0.001', 83.49), ('reg:0.01', 79.67)]
2019-02-15 07:05:48,810 : Cross-validation : best param found is reg = 0.0001             with score 84.26
2019-02-15 07:05:48,811 : Evaluating...
2019-02-15 07:05:49,657 : 
Dev acc : 84.26 Test acc : 91.4             for TREC

2019-02-15 07:05:49,658 : ***** Transfer task : MRPC *****


2019-02-15 07:05:49,689 : loading BERT model bert-base-uncased
2019-02-15 07:05:49,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:05:49,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:05:49,713 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp451qteet
2019-02-15 07:05:52,030 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:05:53,366 : Computing embedding for train
2019-02-15 07:06:01,555 : Computed train embeddings
2019-02-15 07:06:01,556 : Computing embedding for test
2019-02-15 07:06:05,093 : Computed test embeddings
2019-02-15 07:06:05,108 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 07:06:10,901 : [('reg:1e-05', 74.93), ('reg:0.0001', 74.53), ('reg:0.001', 74.73), ('reg:0.01', 74.66)]
2019-02-15 07:06:10,901 : Cross-validation : best param found is reg = 1e-05             with score 74.93
2019-02-15 07:06:10,901 : Evaluating...
2019-02-15 07:06:11,294 : Dev acc : 74.93 Test acc 73.33; Test F1 80.31 for MRPC.

2019-02-15 07:06:11,295 : ***** Transfer task : SICK-Entailment*****


2019-02-15 07:06:11,357 : loading BERT model bert-base-uncased
2019-02-15 07:06:11,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:06:11,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:06:11,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7zx73seg
2019-02-15 07:06:13,666 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:06:15,027 : Computing embedding for train
2019-02-15 07:06:19,390 : Computed train embeddings
2019-02-15 07:06:19,390 : Computing embedding for dev
2019-02-15 07:06:19,961 : Computed dev embeddings
2019-02-15 07:06:19,961 : Computing embedding for test
2019-02-15 07:06:24,624 : Computed test embeddings
2019-02-15 07:06:24,650 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 07:06:27,618 : [('reg:1e-05', 75.8), ('reg:0.0001', 76.2), ('reg:0.001', 75.4), ('reg:0.01', 73.8)]
2019-02-15 07:06:27,618 : Validation : best param found is reg = 0.0001 with score             76.2
2019-02-15 07:06:27,618 : Evaluating...
2019-02-15 07:06:28,424 : 
Dev acc : 76.2 Test acc : 76.68 for                        SICK entailment

2019-02-15 07:06:28,424 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 07:06:28,458 : loading BERT model bert-base-uncased
2019-02-15 07:06:28,458 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:06:28,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:06:28,518 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps4rm47my
2019-02-15 07:06:30,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:06:32,239 : Computing embedding for train
2019-02-15 07:06:36,601 : Computed train embeddings
2019-02-15 07:06:36,601 : Computing embedding for dev
2019-02-15 07:06:37,172 : Computed dev embeddings
2019-02-15 07:06:37,172 : Computing embedding for test
2019-02-15 07:06:41,840 : Computed test embeddings
2019-02-15 07:07:11,430 : Dev : Pearson 0.8092191887799094
2019-02-15 07:07:11,430 : Test : Pearson 0.8085801008942196 Spearman 0.7411812882756645 MSE 0.3533752033657978                        for SICK Relatedness

2019-02-15 07:07:11,430 : 

***** Transfer task : STSBenchmark*****


2019-02-15 07:07:11,469 : loading BERT model bert-base-uncased
2019-02-15 07:07:11,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:07:11,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:07:11,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvzpt1t92
2019-02-15 07:07:13,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:07:15,134 : Computing embedding for train
2019-02-15 07:07:22,145 : Computed train embeddings
2019-02-15 07:07:22,145 : Computing embedding for dev
2019-02-15 07:07:24,214 : Computed dev embeddings
2019-02-15 07:07:24,214 : Computing embedding for test
2019-02-15 07:07:25,892 : Computed test embeddings
2019-02-15 07:08:02,722 : Dev : Pearson 0.7005391109290238
2019-02-15 07:08:02,722 : Test : Pearson 0.6565506226440389 Spearman 0.6497495904346781 MSE 1.5007045658522677                        for SICK Relatedness

2019-02-15 07:08:02,722 : ***** Transfer task : SNLI Entailment*****


2019-02-15 07:08:07,334 : loading BERT model bert-base-uncased
2019-02-15 07:08:07,334 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:08:07,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:08:07,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoktwesxu
2019-02-15 07:08:09,733 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:08:11,247 : PROGRESS (encoding): 0.00%
2019-02-15 07:09:16,190 : PROGRESS (encoding): 14.56%
2019-02-15 07:10:28,714 : PROGRESS (encoding): 29.12%
2019-02-15 07:11:41,706 : PROGRESS (encoding): 43.69%
2019-02-15 07:13:00,854 : PROGRESS (encoding): 58.25%
2019-02-15 07:14:28,812 : PROGRESS (encoding): 72.81%
2019-02-15 07:15:56,606 : PROGRESS (encoding): 87.37%
2019-02-15 07:17:29,861 : PROGRESS (encoding): 0.00%
2019-02-15 07:17:41,401 : PROGRESS (encoding): 0.00%
2019-02-15 07:17:52,528 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 07:18:16,786 : [('reg:1e-09', 59.86)]
2019-02-15 07:18:16,786 : Validation : best param found is reg = 1e-09 with score             59.86
2019-02-15 07:18:16,786 : Evaluating...
2019-02-15 07:18:40,947 : Dev acc : 59.86 Test acc : 59.72 for SNLI

2019-02-15 07:18:40,948 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 07:18:49,350 : loading BERT model bert-base-uncased
2019-02-15 07:18:49,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 07:18:49,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 07:18:49,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_shj8hm4
2019-02-15 07:18:51,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 07:18:53,019 : Computing embedding for train
2019-02-15 07:25:10,519 : Computed train embeddings
2019-02-15 07:25:10,519 : Computing embedding for dev
2019-02-15 07:25:26,952 : Computed dev embeddings
2019-02-15 07:25:26,953 : Computing embedding for test
2019-02-15 07:25:43,893 : Computed test embeddings
2019-02-15 07:25:43,908 : prepare data
2019-02-15 07:25:43,969 : start epoch
2019-02-15 07:26:22,308 : samples : 64000
2019-02-15 07:26:31,651 : Image to text: 7.64, 23.9, 34.82, 21.0
2019-02-15 07:26:38,190 : Text to Image: 6.292, 20.104, 30.44, 27.0
2019-02-15 07:27:16,381 : samples : 128000
2019-02-15 07:27:25,666 : Image to text: 9.34, 25.9, 37.88, 18.0
2019-02-15 07:27:32,034 : Text to Image: 7.332, 22.284, 33.468, 23.0
2019-02-15 07:28:10,248 : samples : 192000
2019-02-15 07:28:19,561 : Image to text: 9.68, 26.6, 38.58, 18.0
2019-02-15 07:28:25,885 : Text to Image: 7.424, 22.776, 33.816, 22.0
2019-02-15 07:29:04,068 : samples : 256000
2019-02-15 07:29:13,373 : Image to text: 10.32, 27.76, 39.78, 17.0
2019-02-15 07:29:19,721 : Text to Image: 8.004, 23.756, 34.912, 21.0
2019-02-15 07:29:57,994 : samples : 320000
2019-02-15 07:30:07,332 : Image to text: 9.42, 26.08, 37.58, 18.0
2019-02-15 07:30:13,662 : Text to Image: 7.964, 23.764, 35.252, 21.0
2019-02-15 07:30:51,836 : samples : 384000
2019-02-15 07:31:01,172 : Image to text: 10.74, 29.42, 41.46, 16.0
2019-02-15 07:31:07,548 : Text to Image: 8.556, 25.528, 37.276, 19.0
2019-02-15 07:31:45,818 : samples : 448000
2019-02-15 07:31:55,128 : Image to text: 10.2, 29.1, 41.04, 16.0
2019-02-15 07:32:01,508 : Text to Image: 8.168, 24.556, 36.164, 20.0
2019-02-15 07:32:39,677 : samples : 512000
2019-02-15 07:32:49,002 : Image to text: 10.98, 28.94, 41.58, 16.0
2019-02-15 07:32:55,337 : Text to Image: 8.8, 25.756, 37.676, 19.0
2019-02-15 07:33:27,945 : Epoch 1 finished
2019-02-15 07:33:28,305 : Image to text: 26.2, 60.7, 75.7, 4.0
2019-02-15 07:33:28,568 : Text to Image: 23.86, 55.6, 72.82, 4.0
2019-02-15 07:33:28,939 : Image to text: 26.1, 58.9, 73.2, 4.0
2019-02-15 07:33:29,210 : Text to Image: 22.06, 54.76, 72.7, 5.0
2019-02-15 07:33:29,581 : Image to text: 26.5, 59.9, 75.6, 4.0
2019-02-15 07:33:29,854 : Text to Image: 21.8, 56.26, 73.28, 4.0
2019-02-15 07:33:30,225 : Image to text: 27.2, 59.8, 73.5, 4.0
2019-02-15 07:33:30,488 : Text to Image: 22.22, 56.58, 73.0, 4.0
2019-02-15 07:33:30,847 : Image to text: 27.0, 60.7, 76.1, 4.0
2019-02-15 07:33:31,108 : Text to Image: 22.98, 55.8, 72.12, 4.0
2019-02-15 07:33:31,108 : Dev mean Text to Image: 22.584, 55.8, 72.784, 4.2
2019-02-15 07:33:31,108 : Dev mean Image to text: 26.6, 60.00000000000001, 74.82, 4.0
2019-02-15 07:33:31,108 : start epoch
2019-02-15 07:34:09,323 : samples : 64000
2019-02-15 07:34:18,622 : Image to text: 10.6, 29.74, 42.0, 15.0
2019-02-15 07:34:24,954 : Text to Image: 8.852, 25.92, 38.268, 18.0
2019-02-15 07:35:03,177 : samples : 128000
2019-02-15 07:35:12,486 : Image to text: 10.06, 28.84, 40.66, 16.0
2019-02-15 07:35:18,855 : Text to Image: 8.392, 25.528, 37.76, 18.0
2019-02-15 07:35:57,103 : samples : 192000
2019-02-15 07:36:06,436 : Image to text: 11.4, 31.06, 42.78, 15.0
2019-02-15 07:36:12,767 : Text to Image: 9.456, 27.056, 39.188, 17.0
2019-02-15 07:36:50,889 : samples : 256000
2019-02-15 07:37:00,195 : Image to text: 11.58, 31.06, 43.48, 14.0
2019-02-15 07:37:06,609 : Text to Image: 9.408, 27.348, 39.488, 17.0
2019-02-15 07:37:44,744 : samples : 320000
2019-02-15 07:37:54,065 : Image to text: 11.28, 30.5, 43.84, 14.0
2019-02-15 07:38:00,417 : Text to Image: 9.372, 27.148, 39.78, 17.0
2019-02-15 07:38:38,547 : samples : 384000
2019-02-15 07:38:47,869 : Image to text: 11.78, 30.46, 43.52, 14.0
2019-02-15 07:38:54,201 : Text to Image: 9.32, 26.912, 39.18, 17.0
2019-02-15 07:39:32,309 : samples : 448000
2019-02-15 07:39:41,594 : Image to text: 11.94, 32.02, 45.02, 13.0
2019-02-15 07:39:47,945 : Text to Image: 9.664, 27.664, 39.944, 17.0
2019-02-15 07:40:25,891 : samples : 512000
2019-02-15 07:40:35,237 : Image to text: 11.62, 30.38, 43.72, 14.0
2019-02-15 07:40:41,574 : Text to Image: 9.408, 27.452, 39.848, 17.0
2019-02-15 07:41:13,893 : Epoch 2 finished
2019-02-15 07:41:14,251 : Image to text: 29.1, 63.2, 78.3, 3.0
2019-02-15 07:41:14,512 : Text to Image: 24.72, 58.9, 74.92, 4.0
2019-02-15 07:41:14,872 : Image to text: 26.5, 59.4, 76.2, 4.0
2019-02-15 07:41:15,142 : Text to Image: 23.2, 56.6, 74.08, 4.0
2019-02-15 07:41:15,505 : Image to text: 30.5, 62.6, 78.4, 3.0
2019-02-15 07:41:15,766 : Text to Image: 24.72, 58.52, 74.88, 4.0
2019-02-15 07:41:16,126 : Image to text: 29.9, 63.7, 76.3, 3.0
2019-02-15 07:41:16,387 : Text to Image: 25.02, 58.84, 75.16, 4.0
2019-02-15 07:41:16,745 : Image to text: 29.1, 63.0, 77.9, 3.0
2019-02-15 07:41:17,006 : Text to Image: 24.6, 58.2, 74.68, 4.0
2019-02-15 07:41:17,006 : Dev mean Text to Image: 24.451999999999998, 58.212, 74.744, 4.0
2019-02-15 07:41:17,006 : Dev mean Image to text: 29.02, 62.38, 77.42, 3.2
2019-02-15 07:41:17,007 : start epoch
2019-02-15 07:41:54,794 : samples : 64000
2019-02-15 07:42:04,143 : Image to text: 12.24, 31.62, 43.52, 14.0
2019-02-15 07:42:10,478 : Text to Image: 9.54, 27.664, 39.456, 17.0
2019-02-15 07:42:48,434 : samples : 128000
2019-02-15 07:42:57,742 : Image to text: 11.76, 32.16, 45.16, 13.0
2019-02-15 07:43:04,130 : Text to Image: 9.548, 27.796, 39.76, 17.0
2019-02-15 07:43:41,914 : samples : 192000
2019-02-15 07:43:51,268 : Image to text: 12.38, 32.8, 45.54, 13.0
2019-02-15 07:43:57,609 : Text to Image: 10.372, 28.232, 40.6, 16.0
2019-02-15 07:44:35,549 : samples : 256000
2019-02-15 07:44:44,847 : Image to text: 13.02, 33.5, 46.18, 13.0
2019-02-15 07:44:51,245 : Text to Image: 10.388, 29.124, 41.188, 16.0
2019-02-15 07:45:29,058 : samples : 320000
2019-02-15 07:45:38,398 : Image to text: 11.84, 31.54, 44.66, 13.0
2019-02-15 07:45:44,742 : Text to Image: 9.672, 27.432, 39.976, 17.0
2019-02-15 07:46:22,564 : samples : 384000
2019-02-15 07:46:31,902 : Image to text: 12.26, 32.66, 45.42, 13.0
2019-02-15 07:46:38,235 : Text to Image: 9.8, 28.028, 40.276, 16.0
2019-02-15 07:47:16,069 : samples : 448000
2019-02-15 07:47:25,370 : Image to text: 12.22, 32.9, 46.3, 13.0
2019-02-15 07:47:31,740 : Text to Image: 10.456, 29.076, 41.644, 15.0
2019-02-15 07:48:09,536 : samples : 512000
2019-02-15 07:48:18,866 : Image to text: 12.84, 34.2, 47.88, 12.0
2019-02-15 07:48:25,205 : Text to Image: 10.412, 29.292, 41.508, 15.0
2019-02-15 07:48:57,395 : Epoch 3 finished
2019-02-15 07:48:57,754 : Image to text: 30.9, 64.5, 79.5, 3.0
2019-02-15 07:48:58,014 : Text to Image: 24.92, 60.04, 75.98, 4.0
2019-02-15 07:48:58,372 : Image to text: 28.7, 62.6, 77.2, 3.0
2019-02-15 07:48:58,633 : Text to Image: 24.06, 59.22, 75.56, 4.0
2019-02-15 07:48:58,991 : Image to text: 30.7, 65.1, 77.9, 3.0
2019-02-15 07:48:59,251 : Text to Image: 25.74, 59.72, 75.52, 4.0
2019-02-15 07:48:59,610 : Image to text: 31.6, 66.4, 79.0, 3.0
2019-02-15 07:48:59,874 : Text to Image: 25.66, 60.78, 77.08, 4.0
2019-02-15 07:49:00,245 : Image to text: 31.5, 64.9, 77.9, 3.0
2019-02-15 07:49:00,508 : Text to Image: 25.4, 59.28, 76.02, 4.0
2019-02-15 07:49:00,508 : Dev mean Text to Image: 25.156, 59.80799999999999, 76.032, 4.0
2019-02-15 07:49:00,508 : Dev mean Image to text: 30.68, 64.7, 78.3, 3.0
2019-02-15 07:49:00,508 : start epoch
2019-02-15 07:49:38,401 : samples : 64000
2019-02-15 07:49:47,753 : Image to text: 12.26, 32.84, 46.04, 12.0
2019-02-15 07:49:54,089 : Text to Image: 10.1, 28.792, 41.388, 16.0
2019-02-15 07:50:32,315 : samples : 128000
2019-02-15 07:50:41,638 : Image to text: 13.02, 33.68, 47.44, 12.0
2019-02-15 07:50:47,998 : Text to Image: 10.292, 29.008, 41.644, 15.0
2019-02-15 07:51:26,166 : samples : 192000
2019-02-15 07:51:35,553 : Image to text: 12.24, 33.84, 46.7, 12.0
2019-02-15 07:51:41,896 : Text to Image: 10.424, 29.072, 41.412, 16.0
2019-02-15 07:52:20,089 : samples : 256000
2019-02-15 07:52:29,401 : Image to text: 12.52, 33.34, 46.26, 12.0
2019-02-15 07:52:35,770 : Text to Image: 10.62, 29.536, 42.62, 15.0
2019-02-15 07:53:13,919 : samples : 320000
2019-02-15 07:53:23,234 : Image to text: 12.46, 33.74, 47.36, 12.0
2019-02-15 07:53:29,576 : Text to Image: 10.444, 29.204, 41.928, 15.0
2019-02-15 07:54:07,761 : samples : 384000
2019-02-15 07:54:17,079 : Image to text: 12.88, 34.92, 47.72, 12.0
2019-02-15 07:54:23,412 : Text to Image: 10.784, 29.416, 42.056, 15.0
2019-02-15 07:55:01,654 : samples : 448000
2019-02-15 07:55:10,963 : Image to text: 13.48, 34.26, 47.04, 12.0
2019-02-15 07:55:17,325 : Text to Image: 10.576, 29.612, 42.348, 15.0
2019-02-15 07:55:55,581 : samples : 512000
2019-02-15 07:56:04,919 : Image to text: 12.72, 33.82, 47.16, 12.0
2019-02-15 07:56:11,258 : Text to Image: 10.48, 29.296, 41.792, 15.0
2019-02-15 07:56:43,782 : Epoch 4 finished
2019-02-15 07:56:44,142 : Image to text: 31.3, 66.0, 81.3, 3.0
2019-02-15 07:56:44,403 : Text to Image: 26.44, 61.82, 77.5, 4.0
2019-02-15 07:56:44,763 : Image to text: 30.6, 65.2, 77.0, 3.0
2019-02-15 07:56:45,029 : Text to Image: 25.1, 59.3, 76.48, 4.0
2019-02-15 07:56:45,395 : Image to text: 32.8, 66.1, 80.1, 3.0
2019-02-15 07:56:45,658 : Text to Image: 26.1, 61.14, 76.92, 4.0
2019-02-15 07:56:46,018 : Image to text: 34.5, 68.4, 80.7, 3.0
2019-02-15 07:56:46,279 : Text to Image: 26.28, 61.52, 77.64, 4.0
2019-02-15 07:56:46,638 : Image to text: 33.3, 67.0, 80.2, 3.0
2019-02-15 07:56:46,898 : Text to Image: 26.08, 61.1, 76.52, 4.0
2019-02-15 07:56:46,898 : Dev mean Text to Image: 26.0, 60.976, 77.012, 4.0
2019-02-15 07:56:46,898 : Dev mean Image to text: 32.49999999999999, 66.54, 79.85999999999999, 3.0
2019-02-15 07:56:46,899 : start epoch
2019-02-15 07:57:25,024 : samples : 64000
2019-02-15 07:57:34,389 : Image to text: 13.82, 35.5, 48.4, 11.0
2019-02-15 07:57:40,733 : Text to Image: 11.244, 30.032, 43.02, 14.0
2019-02-15 07:58:18,930 : samples : 128000
2019-02-15 07:58:28,239 : Image to text: 12.74, 34.52, 47.48, 12.0
2019-02-15 07:58:34,598 : Text to Image: 10.612, 29.932, 42.608, 15.0
2019-02-15 07:59:12,732 : samples : 192000
2019-02-15 07:59:22,091 : Image to text: 12.66, 33.72, 47.56, 12.0
2019-02-15 07:59:28,425 : Text to Image: 10.496, 29.636, 42.448, 15.0
2019-02-15 08:00:06,664 : samples : 256000
2019-02-15 08:00:16,015 : Image to text: 13.14, 34.4, 47.98, 12.0
2019-02-15 08:00:22,354 : Text to Image: 11.0, 29.984, 42.628, 15.0
2019-02-15 08:01:00,600 : samples : 320000
2019-02-15 08:01:09,926 : Image to text: 13.28, 34.86, 48.04, 11.0
2019-02-15 08:01:16,303 : Text to Image: 11.088, 30.128, 43.184, 14.0
2019-02-15 08:01:54,480 : samples : 384000
2019-02-15 08:02:03,829 : Image to text: 13.76, 34.24, 47.54, 12.0
2019-02-15 08:02:10,164 : Text to Image: 10.924, 30.132, 42.984, 14.0
2019-02-15 08:02:48,293 : samples : 448000
2019-02-15 08:02:57,632 : Image to text: 12.92, 34.58, 48.18, 11.0
2019-02-15 08:03:04,020 : Text to Image: 10.456, 29.66, 42.264, 15.0
2019-02-15 08:03:42,171 : samples : 512000
2019-02-15 08:03:51,532 : Image to text: 13.38, 35.06, 48.26, 11.0
2019-02-15 08:03:57,882 : Text to Image: 10.968, 30.292, 42.844, 15.0
2019-02-15 08:04:30,348 : Epoch 5 finished
2019-02-15 08:04:30,710 : Image to text: 30.4, 65.1, 80.0, 3.0
2019-02-15 08:04:30,973 : Text to Image: 25.14, 60.52, 76.84, 4.0
2019-02-15 08:04:31,335 : Image to text: 30.6, 63.5, 77.9, 3.0
2019-02-15 08:04:31,599 : Text to Image: 24.44, 58.64, 75.62, 4.0
2019-02-15 08:04:31,958 : Image to text: 32.4, 66.5, 79.5, 3.0
2019-02-15 08:04:32,219 : Text to Image: 26.3, 60.38, 76.56, 4.0
2019-02-15 08:04:32,578 : Image to text: 34.9, 64.3, 79.1, 3.0
2019-02-15 08:04:32,838 : Text to Image: 25.56, 61.48, 77.06, 4.0
2019-02-15 08:04:33,196 : Image to text: 33.1, 65.6, 80.0, 3.0
2019-02-15 08:04:33,457 : Text to Image: 25.74, 61.12, 76.88, 4.0
2019-02-15 08:04:33,457 : Dev mean Text to Image: 25.436, 60.428, 76.592, 4.0
2019-02-15 08:04:33,457 : Dev mean Image to text: 32.28, 65.0, 79.30000000000001, 3.0
2019-02-15 08:04:33,457 : start epoch
2019-02-15 08:05:11,595 : samples : 64000
2019-02-15 08:05:20,922 : Image to text: 13.56, 35.5, 49.4, 11.0
2019-02-15 08:05:27,255 : Text to Image: 11.444, 31.088, 43.912, 14.0
2019-02-15 08:06:05,426 : samples : 128000
2019-02-15 08:06:14,715 : Image to text: 13.26, 34.58, 48.02, 11.0
2019-02-15 08:06:21,080 : Text to Image: 10.864, 30.424, 43.1, 14.0
2019-02-15 08:06:59,179 : samples : 192000
2019-02-15 08:07:08,499 : Image to text: 13.62, 35.46, 49.02, 11.0
2019-02-15 08:07:14,837 : Text to Image: 10.876, 30.232, 43.1, 14.0
2019-02-15 08:07:52,988 : samples : 256000
2019-02-15 08:08:02,323 : Image to text: 13.08, 34.98, 48.52, 11.0
2019-02-15 08:08:08,673 : Text to Image: 10.896, 29.836, 42.66, 14.0
2019-02-15 08:08:46,863 : samples : 320000
2019-02-15 08:08:56,201 : Image to text: 13.1, 34.7, 49.02, 11.0
2019-02-15 08:09:02,559 : Text to Image: 10.976, 30.348, 42.984, 15.0
2019-02-15 08:09:40,687 : samples : 384000
2019-02-15 08:09:50,024 : Image to text: 13.74, 35.48, 49.08, 11.0
2019-02-15 08:09:56,358 : Text to Image: 11.136, 30.508, 43.444, 14.0
2019-02-15 08:10:34,696 : samples : 448000
2019-02-15 08:10:44,033 : Image to text: 13.64, 35.26, 49.02, 11.0
2019-02-15 08:10:50,456 : Text to Image: 10.928, 30.368, 43.192, 14.0
2019-02-15 08:11:28,679 : samples : 512000
2019-02-15 08:11:38,042 : Image to text: 14.08, 35.1, 47.66, 12.0
2019-02-15 08:11:44,385 : Text to Image: 10.84, 30.384, 43.352, 14.0
2019-02-15 08:12:16,956 : Epoch 6 finished
2019-02-15 08:12:17,317 : Image to text: 32.8, 67.8, 80.9, 3.0
2019-02-15 08:12:17,579 : Text to Image: 26.46, 62.36, 78.42, 4.0
2019-02-15 08:12:17,940 : Image to text: 31.7, 66.4, 79.6, 3.0
2019-02-15 08:12:18,202 : Text to Image: 25.28, 60.32, 76.78, 4.0
2019-02-15 08:12:18,562 : Image to text: 33.8, 67.1, 80.3, 3.0
2019-02-15 08:12:18,824 : Text to Image: 27.02, 61.94, 77.7, 3.0
2019-02-15 08:12:19,184 : Image to text: 34.5, 67.7, 81.2, 3.0
2019-02-15 08:12:19,446 : Text to Image: 27.4, 61.94, 77.54, 3.0
2019-02-15 08:12:19,806 : Image to text: 34.4, 66.1, 80.4, 3.0
2019-02-15 08:12:20,066 : Text to Image: 27.8, 61.56, 76.56, 4.0
2019-02-15 08:12:20,066 : Dev mean Text to Image: 26.792, 61.623999999999995, 77.4, 3.6000000000000005
2019-02-15 08:12:20,066 : Dev mean Image to text: 33.44, 67.02, 80.48, 3.0
2019-02-15 08:12:20,066 : start epoch
2019-02-15 08:12:58,298 : samples : 64000
2019-02-15 08:13:07,614 : Image to text: 14.32, 36.54, 49.66, 11.0
2019-02-15 08:13:13,985 : Text to Image: 11.132, 30.644, 43.432, 14.0
2019-02-15 08:13:52,224 : samples : 128000
2019-02-15 08:14:01,552 : Image to text: 13.68, 36.28, 49.92, 11.0
2019-02-15 08:14:07,899 : Text to Image: 11.42, 30.916, 43.948, 14.0
2019-02-15 08:14:46,155 : samples : 192000
2019-02-15 08:14:55,449 : Image to text: 14.42, 36.14, 49.54, 11.0
2019-02-15 08:15:01,841 : Text to Image: 11.308, 31.424, 43.992, 14.0
2019-02-15 08:15:39,939 : samples : 256000
2019-02-15 08:15:49,287 : Image to text: 13.38, 34.88, 47.88, 12.0
2019-02-15 08:15:55,623 : Text to Image: 11.004, 30.728, 43.46, 14.0
2019-02-15 08:16:33,820 : samples : 320000
2019-02-15 08:16:43,158 : Image to text: 13.6, 34.9, 49.02, 11.0
2019-02-15 08:16:49,584 : Text to Image: 11.292, 30.892, 43.612, 14.0
2019-02-15 08:17:27,699 : samples : 384000
2019-02-15 08:17:37,046 : Image to text: 13.6, 35.12, 49.12, 11.0
2019-02-15 08:17:43,382 : Text to Image: 11.284, 30.94, 43.904, 14.0
2019-02-15 08:18:21,491 : samples : 448000
2019-02-15 08:18:30,841 : Image to text: 13.96, 36.3, 50.0, 10.0
2019-02-15 08:18:37,182 : Text to Image: 11.388, 31.392, 44.384, 13.0
2019-02-15 08:19:15,276 : samples : 512000
2019-02-15 08:19:24,641 : Image to text: 14.74, 36.74, 50.04, 10.0
2019-02-15 08:19:31,003 : Text to Image: 11.416, 31.408, 44.348, 14.0
2019-02-15 08:20:03,539 : Epoch 7 finished
2019-02-15 08:20:03,899 : Image to text: 33.1, 66.4, 80.6, 3.0
2019-02-15 08:20:04,160 : Text to Image: 26.52, 63.2, 78.18, 4.0
2019-02-15 08:20:04,519 : Image to text: 29.7, 64.7, 80.3, 3.0
2019-02-15 08:20:04,780 : Text to Image: 26.26, 61.5, 77.56, 4.0
2019-02-15 08:20:05,139 : Image to text: 33.3, 67.2, 81.0, 3.0
2019-02-15 08:20:05,401 : Text to Image: 28.18, 63.24, 78.36, 3.0
2019-02-15 08:20:05,760 : Image to text: 34.2, 66.2, 80.8, 3.0
2019-02-15 08:20:06,021 : Text to Image: 27.88, 63.26, 78.68, 3.0
2019-02-15 08:20:06,380 : Image to text: 31.7, 67.3, 80.8, 3.0
2019-02-15 08:20:06,641 : Text to Image: 27.68, 62.54, 77.64, 3.0
2019-02-15 08:20:06,641 : Dev mean Text to Image: 27.304000000000002, 62.748000000000005, 78.08400000000002, 3.4000000000000004
2019-02-15 08:20:06,641 : Dev mean Image to text: 32.4, 66.36, 80.69999999999999, 3.0
2019-02-15 08:20:06,641 : start epoch
2019-02-15 08:20:44,775 : samples : 64000
2019-02-15 08:20:54,195 : Image to text: 13.58, 36.46, 49.76, 11.0
2019-02-15 08:21:00,569 : Text to Image: 11.204, 30.98, 44.0, 14.0
2019-02-15 08:21:38,702 : samples : 128000
2019-02-15 08:21:48,082 : Image to text: 14.14, 35.72, 49.16, 11.0
2019-02-15 08:21:54,426 : Text to Image: 11.268, 31.076, 44.164, 14.0
2019-02-15 08:22:32,846 : samples : 192000
2019-02-15 08:22:42,176 : Image to text: 14.12, 35.62, 50.08, 10.0
2019-02-15 08:22:48,571 : Text to Image: 11.324, 31.512, 44.616, 13.0
2019-02-15 08:23:26,990 : samples : 256000
2019-02-15 08:23:36,332 : Image to text: 13.96, 36.08, 50.1, 10.0
2019-02-15 08:23:42,680 : Text to Image: 11.292, 31.072, 44.236, 14.0
2019-02-15 08:24:21,083 : samples : 320000
2019-02-15 08:24:30,442 : Image to text: 14.2, 36.76, 50.04, 10.0
2019-02-15 08:24:36,782 : Text to Image: 11.532, 31.732, 44.892, 13.0
2019-02-15 08:25:14,977 : samples : 384000
2019-02-15 08:25:24,343 : Image to text: 12.76, 35.54, 48.88, 11.0
2019-02-15 08:25:30,704 : Text to Image: 11.436, 31.304, 44.248, 14.0
2019-02-15 08:26:08,972 : samples : 448000
2019-02-15 08:26:18,295 : Image to text: 14.08, 36.38, 50.4, 10.0
2019-02-15 08:26:24,633 : Text to Image: 11.368, 31.292, 44.228, 14.0
2019-02-15 08:27:02,835 : samples : 512000
2019-02-15 08:27:12,185 : Image to text: 13.8, 35.6, 49.04, 11.0
2019-02-15 08:27:18,566 : Text to Image: 11.3, 31.156, 44.284, 13.0
2019-02-15 08:27:51,060 : Epoch 8 finished
2019-02-15 08:27:51,420 : Image to text: 34.9, 67.6, 80.5, 3.0
2019-02-15 08:27:51,682 : Text to Image: 27.92, 63.3, 78.76, 3.0
2019-02-15 08:27:52,042 : Image to text: 31.6, 66.0, 79.7, 3.0
2019-02-15 08:27:52,304 : Text to Image: 26.3, 61.28, 77.98, 4.0
2019-02-15 08:27:52,664 : Image to text: 34.3, 67.5, 80.6, 3.0
2019-02-15 08:27:52,925 : Text to Image: 27.82, 62.96, 79.02, 3.0
2019-02-15 08:27:53,285 : Image to text: 35.4, 69.1, 80.3, 2.0
2019-02-15 08:27:53,546 : Text to Image: 28.42, 63.44, 79.14, 3.0
2019-02-15 08:27:53,905 : Image to text: 34.3, 67.7, 80.0, 3.0
2019-02-15 08:27:54,165 : Text to Image: 27.8, 62.04, 77.96, 3.0
2019-02-15 08:27:54,165 : Dev mean Text to Image: 27.652, 62.604, 78.572, 3.2
2019-02-15 08:27:54,165 : Dev mean Image to text: 34.1, 67.58, 80.22, 2.8
2019-02-15 08:27:54,165 : start epoch
2019-02-15 08:28:32,416 : samples : 64000
2019-02-15 08:28:41,750 : Image to text: 14.12, 35.98, 50.44, 10.0
2019-02-15 08:28:48,130 : Text to Image: 11.196, 31.06, 44.048, 14.0
2019-02-15 08:29:26,267 : samples : 128000
2019-02-15 08:29:35,616 : Image to text: 14.26, 36.68, 49.54, 11.0
2019-02-15 08:29:41,957 : Text to Image: 11.34, 31.248, 44.056, 14.0
2019-02-15 08:30:20,128 : samples : 192000
2019-02-15 08:30:29,449 : Image to text: 14.36, 36.28, 50.16, 10.0
2019-02-15 08:30:35,805 : Text to Image: 11.396, 31.424, 44.444, 14.0
2019-02-15 08:31:14,014 : samples : 256000
2019-02-15 08:31:23,341 : Image to text: 14.6, 36.66, 50.22, 10.0
2019-02-15 08:31:29,686 : Text to Image: 11.624, 31.588, 44.76, 13.0
2019-02-15 08:32:07,899 : samples : 320000
2019-02-15 08:32:17,276 : Image to text: 15.04, 36.1, 49.64, 11.0
2019-02-15 08:32:23,617 : Text to Image: 11.648, 31.544, 44.732, 13.0
2019-02-15 08:33:01,658 : samples : 384000
2019-02-15 08:33:10,985 : Image to text: 14.46, 35.98, 49.92, 11.0
2019-02-15 08:33:17,355 : Text to Image: 11.744, 31.688, 44.448, 14.0
2019-02-15 08:33:55,960 : samples : 448000
2019-02-15 08:34:05,420 : Image to text: 14.46, 37.3, 51.18, 10.0
2019-02-15 08:34:11,801 : Text to Image: 11.324, 30.972, 44.048, 14.0
2019-02-15 08:34:50,351 : samples : 512000
2019-02-15 08:34:59,942 : Image to text: 14.52, 36.24, 50.14, 10.0
2019-02-15 08:35:06,367 : Text to Image: 11.516, 31.536, 44.332, 14.0
2019-02-15 08:35:39,164 : Epoch 9 finished
2019-02-15 08:35:39,525 : Image to text: 33.9, 69.8, 82.7, 3.0
2019-02-15 08:35:39,786 : Text to Image: 28.18, 63.7, 79.18, 3.0
2019-02-15 08:35:40,146 : Image to text: 31.7, 66.8, 80.3, 3.0
2019-02-15 08:35:40,407 : Text to Image: 26.94, 61.66, 78.06, 4.0
2019-02-15 08:35:40,769 : Image to text: 36.2, 68.2, 82.0, 3.0
2019-02-15 08:35:41,030 : Text to Image: 28.04, 63.38, 79.16, 3.0
2019-02-15 08:35:41,389 : Image to text: 33.0, 68.3, 81.7, 3.0
2019-02-15 08:35:41,650 : Text to Image: 28.54, 63.3, 79.64, 3.0
2019-02-15 08:35:42,009 : Image to text: 34.2, 69.0, 81.4, 3.0
2019-02-15 08:35:42,270 : Text to Image: 28.32, 62.6, 78.04, 4.0
2019-02-15 08:35:42,270 : Dev mean Text to Image: 28.004000000000005, 62.928, 78.816, 3.4000000000000004
2019-02-15 08:35:42,270 : Dev mean Image to text: 33.800000000000004, 68.42, 81.61999999999999, 3.0
2019-02-15 08:35:42,270 : start epoch
2019-02-15 08:36:20,645 : samples : 64000
2019-02-15 08:36:30,033 : Image to text: 13.6, 35.9, 51.02, 10.0
2019-02-15 08:36:36,390 : Text to Image: 11.504, 31.852, 44.652, 13.0
2019-02-15 08:37:14,496 : samples : 128000
2019-02-15 08:37:23,857 : Image to text: 14.38, 37.48, 51.22, 10.0
2019-02-15 08:37:30,218 : Text to Image: 11.92, 31.968, 45.184, 13.0
2019-02-15 08:38:08,424 : samples : 192000
2019-02-15 08:38:17,784 : Image to text: 15.14, 37.58, 51.0, 10.0
2019-02-15 08:38:24,204 : Text to Image: 12.084, 31.892, 45.1, 13.0
2019-02-15 08:39:02,312 : samples : 256000
2019-02-15 08:39:11,636 : Image to text: 14.38, 36.86, 50.08, 10.0
2019-02-15 08:39:17,992 : Text to Image: 11.344, 31.316, 44.4, 13.0
2019-02-15 08:39:56,120 : samples : 320000
2019-02-15 08:40:05,471 : Image to text: 14.64, 37.32, 50.84, 10.0
2019-02-15 08:40:11,813 : Text to Image: 11.7, 31.988, 45.068, 13.0
2019-02-15 08:40:49,958 : samples : 384000
2019-02-15 08:40:59,269 : Image to text: 14.56, 36.94, 50.44, 10.0
2019-02-15 08:41:05,626 : Text to Image: 11.424, 31.48, 44.516, 13.0
2019-02-15 08:41:43,789 : samples : 448000
2019-02-15 08:41:53,142 : Image to text: 14.36, 37.74, 50.98, 10.0
2019-02-15 08:41:59,479 : Text to Image: 11.728, 32.008, 45.08, 13.0
2019-02-15 08:42:37,633 : samples : 512000
2019-02-15 08:42:46,995 : Image to text: 14.42, 35.98, 49.3, 11.0
2019-02-15 08:42:53,335 : Text to Image: 11.6, 31.908, 44.804, 13.0
2019-02-15 08:43:25,793 : Epoch 10 finished
2019-02-15 08:43:26,153 : Image to text: 31.3, 69.4, 82.2, 3.0
2019-02-15 08:43:26,414 : Text to Image: 28.46, 64.3, 79.16, 3.0
2019-02-15 08:43:26,774 : Image to text: 31.3, 68.0, 81.2, 3.0
2019-02-15 08:43:27,034 : Text to Image: 26.72, 61.72, 78.12, 4.0
2019-02-15 08:43:27,394 : Image to text: 33.7, 66.0, 81.4, 3.0
2019-02-15 08:43:27,655 : Text to Image: 28.76, 63.18, 79.14, 3.0
2019-02-15 08:43:28,014 : Image to text: 33.6, 69.9, 81.9, 3.0
2019-02-15 08:43:28,275 : Text to Image: 28.12, 63.88, 79.7, 3.0
2019-02-15 08:43:28,635 : Image to text: 35.0, 68.0, 82.0, 3.0
2019-02-15 08:43:28,898 : Text to Image: 27.9, 63.14, 78.58, 3.0
2019-02-15 08:43:28,898 : Dev mean Text to Image: 27.991999999999997, 63.244, 78.94, 3.2
2019-02-15 08:43:28,898 : Dev mean Image to text: 32.98, 68.25999999999999, 81.74000000000001, 3.0
2019-02-15 08:43:28,898 : start epoch
2019-02-15 08:44:07,079 : samples : 64000
2019-02-15 08:44:16,437 : Image to text: 14.68, 36.8, 50.32, 10.0
2019-02-15 08:44:22,770 : Text to Image: 11.588, 31.908, 44.828, 13.0
2019-02-15 08:45:00,891 : samples : 128000
2019-02-15 08:45:10,230 : Image to text: 14.72, 36.4, 50.2, 10.0
2019-02-15 08:45:16,616 : Text to Image: 11.392, 31.644, 45.124, 13.0
2019-02-15 08:45:54,786 : samples : 192000
2019-02-15 08:46:04,099 : Image to text: 14.06, 37.14, 51.24, 10.0
2019-02-15 08:46:10,440 : Text to Image: 11.608, 31.8, 45.012, 13.0
2019-02-15 08:46:48,640 : samples : 256000
2019-02-15 08:46:57,974 : Image to text: 14.56, 38.42, 51.8, 10.0
2019-02-15 08:47:04,393 : Text to Image: 11.904, 32.108, 45.084, 13.0
2019-02-15 08:47:42,515 : samples : 320000
2019-02-15 08:47:51,911 : Image to text: 14.94, 37.84, 50.84, 10.0
2019-02-15 08:47:58,253 : Text to Image: 11.716, 31.964, 44.844, 13.0
2019-02-15 08:48:36,431 : samples : 384000
2019-02-15 08:48:45,788 : Image to text: 14.66, 37.4, 51.02, 10.0
2019-02-15 08:48:52,129 : Text to Image: 11.796, 32.58, 45.772, 13.0
2019-02-15 08:49:30,304 : samples : 448000
2019-02-15 08:49:39,594 : Image to text: 14.8, 36.88, 51.04, 10.0
2019-02-15 08:49:45,961 : Text to Image: 11.748, 32.176, 45.16, 13.0
2019-02-15 08:50:24,122 : samples : 512000
2019-02-15 08:50:33,473 : Image to text: 14.68, 37.86, 51.24, 10.0
2019-02-15 08:50:39,815 : Text to Image: 11.932, 32.396, 45.604, 13.0
2019-02-15 08:51:12,377 : Epoch 11 finished
2019-02-15 08:51:12,737 : Image to text: 35.6, 68.0, 82.2, 3.0
2019-02-15 08:51:12,998 : Text to Image: 28.14, 63.92, 78.96, 3.0
2019-02-15 08:51:13,358 : Image to text: 32.0, 68.0, 81.9, 3.0
2019-02-15 08:51:13,618 : Text to Image: 26.88, 61.64, 78.16, 4.0
2019-02-15 08:51:13,977 : Image to text: 35.8, 67.1, 82.3, 3.0
2019-02-15 08:51:14,237 : Text to Image: 29.22, 63.84, 79.36, 3.0
2019-02-15 08:51:14,596 : Image to text: 35.2, 67.5, 80.8, 3.0
2019-02-15 08:51:14,860 : Text to Image: 28.26, 64.36, 79.74, 3.0
2019-02-15 08:51:15,231 : Image to text: 34.5, 66.8, 81.5, 3.0
2019-02-15 08:51:15,494 : Text to Image: 28.44, 62.94, 78.24, 3.0
2019-02-15 08:51:15,494 : Dev mean Text to Image: 28.188000000000002, 63.34, 78.892, 3.2
2019-02-15 08:51:15,494 : Dev mean Image to text: 34.62, 67.47999999999999, 81.74000000000001, 3.0
2019-02-15 08:51:15,494 : start epoch
2019-02-15 08:51:53,738 : samples : 64000
2019-02-15 08:52:03,146 : Image to text: 14.88, 37.66, 51.62, 10.0
2019-02-15 08:52:09,539 : Text to Image: 11.952, 32.284, 45.236, 13.0
2019-02-15 08:52:47,745 : samples : 128000
2019-02-15 08:52:57,084 : Image to text: 14.32, 36.9, 50.3, 10.0
2019-02-15 08:53:03,461 : Text to Image: 11.848, 32.164, 45.424, 13.0
2019-02-15 08:53:41,636 : samples : 192000
2019-02-15 08:53:51,012 : Image to text: 15.18, 37.1, 50.56, 10.0
2019-02-15 08:53:57,354 : Text to Image: 12.052, 32.3, 45.432, 13.0
2019-02-15 08:54:35,587 : samples : 256000
2019-02-15 08:54:44,896 : Image to text: 15.42, 37.24, 50.78, 10.0
2019-02-15 08:54:51,302 : Text to Image: 11.864, 31.96, 45.236, 13.0
2019-02-15 08:55:29,324 : samples : 320000
2019-02-15 08:55:38,664 : Image to text: 15.12, 37.92, 51.26, 10.0
2019-02-15 08:55:45,010 : Text to Image: 11.764, 31.792, 44.852, 13.0
2019-02-15 08:56:23,103 : samples : 384000
2019-02-15 08:56:32,462 : Image to text: 14.5, 37.4, 51.52, 10.0
2019-02-15 08:56:38,799 : Text to Image: 11.952, 32.42, 45.72, 13.0
2019-02-15 08:57:16,863 : samples : 448000
2019-02-15 08:57:26,190 : Image to text: 15.52, 38.54, 51.86, 10.0
2019-02-15 08:57:32,574 : Text to Image: 12.08, 32.0, 45.112, 13.0
2019-02-15 08:58:10,592 : samples : 512000
2019-02-15 08:58:19,946 : Image to text: 15.48, 38.52, 51.52, 10.0
2019-02-15 08:58:26,283 : Text to Image: 12.024, 32.184, 45.472, 13.0
2019-02-15 08:58:58,763 : Epoch 12 finished
2019-02-15 08:58:59,122 : Image to text: 34.7, 68.5, 81.4, 3.0
2019-02-15 08:58:59,383 : Text to Image: 27.92, 64.02, 79.34, 3.0
2019-02-15 08:58:59,743 : Image to text: 32.8, 68.5, 81.3, 3.0
2019-02-15 08:59:00,012 : Text to Image: 26.96, 61.96, 79.0, 3.0
2019-02-15 08:59:00,385 : Image to text: 36.3, 67.8, 81.4, 3.0
2019-02-15 08:59:00,657 : Text to Image: 28.4, 64.36, 79.7, 3.0
2019-02-15 08:59:01,018 : Image to text: 34.8, 69.4, 82.4, 3.0
2019-02-15 08:59:01,282 : Text to Image: 28.6, 64.12, 80.38, 3.0
2019-02-15 08:59:01,644 : Image to text: 33.5, 68.6, 81.9, 3.0
2019-02-15 08:59:01,904 : Text to Image: 28.44, 63.32, 79.1, 3.0
2019-02-15 08:59:01,904 : Dev mean Text to Image: 28.064, 63.556, 79.504, 3.0
2019-02-15 08:59:01,904 : Dev mean Image to text: 34.42, 68.56, 81.68, 3.0
2019-02-15 08:59:01,904 : start epoch
2019-02-15 08:59:39,913 : samples : 64000
2019-02-15 08:59:49,264 : Image to text: 14.82, 38.2, 51.52, 10.0
2019-02-15 08:59:55,600 : Text to Image: 11.94, 32.232, 45.528, 13.0
2019-02-15 09:00:33,686 : samples : 128000
2019-02-15 09:00:43,007 : Image to text: 14.96, 37.98, 52.08, 10.0
2019-02-15 09:00:49,374 : Text to Image: 11.664, 32.16, 45.084, 13.0
2019-02-15 09:01:27,515 : samples : 192000
2019-02-15 09:01:36,907 : Image to text: 15.12, 37.1, 51.2, 10.0
2019-02-15 09:01:43,258 : Text to Image: 11.748, 32.368, 45.324, 13.0
2019-02-15 09:02:21,374 : samples : 256000
2019-02-15 09:02:30,724 : Image to text: 15.06, 38.82, 52.68, 9.0
2019-02-15 09:02:37,063 : Text to Image: 12.08, 32.636, 45.768, 13.0
2019-02-15 09:03:15,118 : samples : 320000
2019-02-15 09:03:24,437 : Image to text: 14.36, 38.04, 51.28, 10.0
2019-02-15 09:03:30,848 : Text to Image: 12.2, 32.424, 45.904, 13.0
2019-02-15 09:04:08,933 : samples : 384000
2019-02-15 09:04:18,284 : Image to text: 15.1, 37.08, 51.22, 10.0
2019-02-15 09:04:24,622 : Text to Image: 11.852, 32.504, 45.644, 13.0
2019-02-15 09:05:02,739 : samples : 448000
2019-02-15 09:05:12,082 : Image to text: 15.6, 37.84, 51.38, 10.0
2019-02-15 09:05:18,440 : Text to Image: 11.884, 32.18, 45.248, 13.0
2019-02-15 09:05:57,041 : samples : 512000
2019-02-15 09:06:06,470 : Image to text: 14.82, 37.98, 51.48, 10.0
2019-02-15 09:06:12,845 : Text to Image: 11.836, 31.992, 45.264, 13.0
2019-02-15 09:06:45,839 : Epoch 13 finished
2019-02-15 09:06:46,202 : Image to text: 34.6, 71.1, 83.5, 3.0
2019-02-15 09:06:46,466 : Text to Image: 28.26, 64.9, 80.34, 3.0
2019-02-15 09:06:46,830 : Image to text: 32.5, 66.5, 82.0, 3.0
2019-02-15 09:06:47,093 : Text to Image: 26.62, 62.34, 78.8, 3.0
2019-02-15 09:06:47,457 : Image to text: 35.2, 69.2, 82.8, 3.0
2019-02-15 09:06:47,720 : Text to Image: 29.3, 65.0, 79.68, 3.0
2019-02-15 09:06:48,080 : Image to text: 35.3, 67.8, 82.0, 3.0
2019-02-15 09:06:48,342 : Text to Image: 28.76, 64.9, 80.58, 3.0
2019-02-15 09:06:48,702 : Image to text: 34.0, 69.2, 80.9, 3.0
2019-02-15 09:06:48,963 : Text to Image: 28.76, 64.1, 78.92, 3.0
2019-02-15 09:06:48,963 : Dev mean Text to Image: 28.340000000000003, 64.24799999999999, 79.664, 3.0
2019-02-15 09:06:48,963 : Dev mean Image to text: 34.32, 68.76, 82.24000000000001, 3.0
2019-02-15 09:06:48,963 : start epoch
2019-02-15 09:07:27,664 : samples : 64000
2019-02-15 09:07:37,044 : Image to text: 14.4, 38.26, 51.72, 10.0
2019-02-15 09:07:43,397 : Text to Image: 11.928, 32.34, 45.548, 13.0
2019-02-15 09:08:22,073 : samples : 128000
2019-02-15 09:08:31,432 : Image to text: 14.94, 38.02, 51.72, 10.0
2019-02-15 09:08:37,783 : Text to Image: 11.948, 32.024, 45.084, 13.0
2019-02-15 09:09:16,427 : samples : 192000
2019-02-15 09:09:25,760 : Image to text: 14.82, 38.38, 51.44, 10.0
2019-02-15 09:09:32,156 : Text to Image: 11.844, 32.22, 45.352, 13.0
2019-02-15 09:10:10,714 : samples : 256000
2019-02-15 09:10:20,070 : Image to text: 15.06, 38.46, 52.3, 10.0
2019-02-15 09:10:26,422 : Text to Image: 12.132, 32.756, 45.776, 13.0
2019-02-15 09:11:05,062 : samples : 320000
2019-02-15 09:11:14,407 : Image to text: 14.62, 38.02, 50.96, 10.0
2019-02-15 09:11:20,777 : Text to Image: 12.0, 32.268, 45.364, 13.0
2019-02-15 09:11:59,351 : samples : 384000
2019-02-15 09:12:08,710 : Image to text: 15.02, 38.74, 52.48, 9.0
2019-02-15 09:12:15,072 : Text to Image: 12.008, 32.624, 45.476, 13.0
2019-02-15 09:12:53,583 : samples : 448000
2019-02-15 09:13:02,954 : Image to text: 14.26, 36.82, 50.42, 10.0
2019-02-15 09:13:09,306 : Text to Image: 11.872, 32.252, 45.296, 13.0
2019-02-15 09:13:47,872 : samples : 512000
2019-02-15 09:13:57,210 : Image to text: 15.24, 38.48, 52.24, 9.0
2019-02-15 09:14:03,598 : Text to Image: 12.16, 32.856, 46.06, 12.0
2019-02-15 09:14:36,375 : Epoch 14 finished
2019-02-15 09:14:36,735 : Image to text: 32.8, 69.4, 81.6, 3.0
2019-02-15 09:14:36,996 : Text to Image: 28.44, 64.44, 79.8, 3.0
2019-02-15 09:14:37,356 : Image to text: 31.6, 67.1, 82.1, 3.0
2019-02-15 09:14:37,617 : Text to Image: 27.6, 62.34, 78.88, 4.0
2019-02-15 09:14:37,977 : Image to text: 36.5, 67.7, 81.4, 2.0
2019-02-15 09:14:38,238 : Text to Image: 28.56, 65.0, 79.92, 3.0
2019-02-15 09:14:38,597 : Image to text: 35.2, 67.1, 81.2, 3.0
2019-02-15 09:14:38,859 : Text to Image: 27.82, 64.2, 80.06, 3.0
2019-02-15 09:14:39,217 : Image to text: 34.8, 67.6, 81.2, 3.0
2019-02-15 09:14:39,478 : Text to Image: 28.6, 63.16, 78.9, 3.0
2019-02-15 09:14:39,478 : Dev mean Text to Image: 28.204, 63.827999999999996, 79.512, 3.2
2019-02-15 09:14:39,478 : Dev mean Image to text: 34.18, 67.77999999999999, 81.5, 2.8000000000000003
2019-02-15 09:14:39,478 : start epoch
2019-02-15 09:15:18,107 : samples : 64000
2019-02-15 09:15:27,439 : Image to text: 14.82, 37.94, 51.54, 10.0
2019-02-15 09:15:33,809 : Text to Image: 12.076, 32.6, 45.684, 13.0
2019-02-15 09:16:12,343 : samples : 128000
2019-02-15 09:16:21,698 : Image to text: 15.28, 37.74, 51.72, 10.0
2019-02-15 09:16:28,046 : Text to Image: 12.024, 32.16, 45.604, 13.0
2019-02-15 09:17:06,619 : samples : 192000
2019-02-15 09:17:15,969 : Image to text: 15.44, 38.14, 52.56, 9.0
2019-02-15 09:17:22,317 : Text to Image: 12.024, 32.468, 45.808, 13.0
2019-02-15 09:18:00,734 : samples : 256000
2019-02-15 09:18:10,086 : Image to text: 14.92, 38.18, 51.48, 10.0
2019-02-15 09:18:16,444 : Text to Image: 11.9, 32.632, 45.576, 13.0
2019-02-15 09:18:54,522 : samples : 320000
2019-02-15 09:19:03,899 : Image to text: 15.7, 37.9, 51.9, 10.0
2019-02-15 09:19:10,241 : Text to Image: 12.296, 32.852, 45.832, 13.0
2019-02-15 09:19:48,403 : samples : 384000
2019-02-15 09:19:57,724 : Image to text: 15.8, 38.32, 51.22, 10.0
2019-02-15 09:20:04,104 : Text to Image: 11.972, 32.84, 45.788, 13.0
2019-02-15 09:20:42,203 : samples : 448000
2019-02-15 09:20:51,544 : Image to text: 14.98, 37.72, 51.86, 10.0
2019-02-15 09:20:57,888 : Text to Image: 12.2, 32.44, 45.928, 13.0
2019-02-15 09:21:36,225 : samples : 512000
2019-02-15 09:21:45,589 : Image to text: 15.26, 38.36, 51.36, 10.0
2019-02-15 09:21:51,949 : Text to Image: 12.4, 32.752, 45.648, 13.0
2019-02-15 09:22:24,686 : Epoch 15 finished
2019-02-15 09:22:25,046 : Image to text: 33.2, 69.9, 83.3, 3.0
2019-02-15 09:22:25,307 : Text to Image: 28.5, 64.8, 80.18, 3.0
2019-02-15 09:22:25,670 : Image to text: 32.0, 67.0, 81.3, 3.0
2019-02-15 09:22:25,933 : Text to Image: 26.66, 62.86, 79.0, 3.0
2019-02-15 09:22:26,293 : Image to text: 36.1, 69.5, 81.4, 2.0
2019-02-15 09:22:26,555 : Text to Image: 29.22, 64.72, 79.2, 3.0
2019-02-15 09:22:26,914 : Image to text: 36.8, 70.1, 82.3, 2.0
2019-02-15 09:22:27,176 : Text to Image: 28.82, 64.42, 80.32, 3.0
2019-02-15 09:22:27,536 : Image to text: 35.0, 69.9, 82.0, 3.0
2019-02-15 09:22:27,797 : Text to Image: 28.4, 63.4, 79.04, 3.0
2019-02-15 09:22:27,797 : Dev mean Text to Image: 28.319999999999997, 64.03999999999999, 79.548, 3.0
2019-02-15 09:22:27,797 : Dev mean Image to text: 34.620000000000005, 69.28, 82.06, 2.6
2019-02-15 09:22:27,798 : start epoch
2019-02-15 09:23:06,290 : samples : 64000
2019-02-15 09:23:15,669 : Image to text: 14.0, 38.7, 52.1, 9.0
2019-02-15 09:23:22,017 : Text to Image: 11.908, 32.372, 45.832, 13.0
2019-02-15 09:24:00,471 : samples : 128000
2019-02-15 09:24:09,834 : Image to text: 14.58, 37.16, 50.92, 10.0
2019-02-15 09:24:16,211 : Text to Image: 11.788, 32.188, 45.536, 13.0
2019-02-15 09:24:54,450 : samples : 192000
2019-02-15 09:25:03,823 : Image to text: 15.32, 38.22, 52.5, 9.0
2019-02-15 09:25:10,164 : Text to Image: 12.228, 32.8, 45.772, 13.0
2019-02-15 09:25:48,391 : samples : 256000
2019-02-15 09:25:57,735 : Image to text: 15.18, 38.1, 51.86, 10.0
2019-02-15 09:26:04,116 : Text to Image: 12.004, 32.776, 45.996, 13.0
2019-02-15 09:26:42,316 : samples : 320000
2019-02-15 09:26:51,677 : Image to text: 15.3, 37.76, 52.4, 10.0
2019-02-15 09:26:58,027 : Text to Image: 12.216, 32.6, 45.94, 13.0
2019-02-15 09:27:36,221 : samples : 384000
2019-02-15 09:27:45,538 : Image to text: 14.48, 38.02, 52.16, 9.0
2019-02-15 09:27:51,883 : Text to Image: 11.656, 31.96, 45.02, 13.0
2019-02-15 09:28:30,110 : samples : 448000
2019-02-15 09:28:39,446 : Image to text: 15.52, 37.98, 51.88, 10.0
2019-02-15 09:28:45,820 : Text to Image: 12.188, 32.876, 46.14, 13.0
2019-02-15 09:29:24,017 : samples : 512000
2019-02-15 09:29:33,374 : Image to text: 15.52, 38.28, 51.88, 10.0
2019-02-15 09:29:39,732 : Text to Image: 12.28, 32.86, 46.292, 12.0
2019-02-15 09:30:12,485 : Epoch 16 finished
2019-02-15 09:30:12,846 : Image to text: 35.2, 70.6, 83.1, 2.0
2019-02-15 09:30:13,107 : Text to Image: 28.84, 64.24, 79.76, 3.0
2019-02-15 09:30:13,467 : Image to text: 34.8, 69.6, 82.7, 3.0
2019-02-15 09:30:13,729 : Text to Image: 27.34, 63.26, 79.04, 3.0
2019-02-15 09:30:14,089 : Image to text: 36.6, 70.9, 82.7, 2.0
2019-02-15 09:30:14,351 : Text to Image: 28.96, 64.74, 79.78, 3.0
2019-02-15 09:30:14,711 : Image to text: 36.1, 70.7, 83.4, 2.0
2019-02-15 09:30:14,981 : Text to Image: 29.14, 64.02, 80.34, 3.0
2019-02-15 09:30:15,356 : Image to text: 35.9, 71.0, 82.6, 3.0
2019-02-15 09:30:15,628 : Text to Image: 28.66, 63.92, 79.54, 3.0
2019-02-15 09:30:15,628 : Dev mean Text to Image: 28.587999999999997, 64.036, 79.69200000000001, 3.0
2019-02-15 09:30:15,628 : Dev mean Image to text: 35.72, 70.56, 82.89999999999999, 2.4
2019-02-15 09:30:15,629 : start epoch
2019-02-15 09:30:54,387 : samples : 64000
2019-02-15 09:31:03,768 : Image to text: 15.44, 38.7, 52.28, 9.0
2019-02-15 09:31:10,113 : Text to Image: 12.308, 33.196, 46.22, 13.0
2019-02-15 09:31:48,690 : samples : 128000
2019-02-15 09:31:58,042 : Image to text: 15.08, 37.98, 52.86, 9.0
2019-02-15 09:32:04,506 : Text to Image: 12.052, 32.74, 46.244, 12.0
2019-02-15 09:32:43,036 : samples : 192000
2019-02-15 09:32:52,519 : Image to text: 15.36, 37.54, 51.54, 10.0
2019-02-15 09:32:58,865 : Text to Image: 12.192, 32.88, 45.992, 13.0
2019-02-15 09:33:37,417 : samples : 256000
2019-02-15 09:33:46,801 : Image to text: 14.78, 38.62, 52.16, 10.0
2019-02-15 09:33:53,159 : Text to Image: 11.88, 32.456, 45.56, 13.0
2019-02-15 09:34:31,742 : samples : 320000
2019-02-15 09:34:41,075 : Image to text: 14.86, 37.56, 51.56, 10.0
2019-02-15 09:34:47,456 : Text to Image: 12.02, 32.552, 45.552, 13.0
2019-02-15 09:35:26,077 : samples : 384000
2019-02-15 09:35:35,442 : Image to text: 14.8, 38.52, 52.2, 9.0
2019-02-15 09:35:41,793 : Text to Image: 12.232, 32.916, 46.18, 12.0
2019-02-15 09:36:20,361 : samples : 448000
2019-02-15 09:36:29,725 : Image to text: 15.04, 38.54, 52.0, 10.0
2019-02-15 09:36:36,142 : Text to Image: 12.328, 32.972, 46.12, 13.0
2019-02-15 09:37:14,686 : samples : 512000
2019-02-15 09:37:24,055 : Image to text: 14.96, 37.2, 51.08, 10.0
2019-02-15 09:37:30,437 : Text to Image: 12.092, 32.912, 46.384, 12.0
2019-02-15 09:38:03,211 : Epoch 17 finished
2019-02-15 09:38:03,570 : Image to text: 34.4, 68.4, 82.5, 2.0
2019-02-15 09:38:03,832 : Text to Image: 28.92, 64.4, 80.48, 3.0
2019-02-15 09:38:04,191 : Image to text: 33.8, 68.4, 81.6, 3.0
2019-02-15 09:38:04,452 : Text to Image: 27.3, 62.94, 79.4, 3.0
2019-02-15 09:38:04,812 : Image to text: 36.5, 68.6, 82.8, 3.0
2019-02-15 09:38:05,073 : Text to Image: 29.6, 64.88, 79.46, 3.0
2019-02-15 09:38:05,433 : Image to text: 35.2, 69.6, 81.5, 3.0
2019-02-15 09:38:05,695 : Text to Image: 29.62, 64.94, 80.94, 3.0
2019-02-15 09:38:06,054 : Image to text: 35.6, 67.9, 80.8, 3.0
2019-02-15 09:38:06,315 : Text to Image: 28.92, 63.74, 79.06, 3.0
2019-02-15 09:38:06,315 : Dev mean Text to Image: 28.872, 64.18, 79.868, 3.0
2019-02-15 09:38:06,315 : Dev mean Image to text: 35.1, 68.58, 81.83999999999999, 2.8000000000000003
2019-02-15 09:38:09,417 : 
Test scores | Image to text:             34.36000000000001, 69.42, 82.72, 2.8000000000000003
2019-02-15 09:38:09,417 : Test scores | Text to image:             28.772, 63.483999999999995, 79.29599999999999, 3.1999999999999997

2019-02-15 09:38:09,554 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 09:38:09,903 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 09:38:10,514 : loading BERT model bert-base-uncased
2019-02-15 09:38:10,514 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:38:10,543 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:38:10,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyewgbiqa
2019-02-15 09:38:12,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:38:14,200 : Computing embeddings for train/dev/test
2019-02-15 09:39:33,065 : Computed embeddings
2019-02-15 09:39:33,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:40:26,358 : [('reg:1e-05', 82.02), ('reg:0.0001', 80.44), ('reg:0.001', 73.79), ('reg:0.01', 58.7)]
2019-02-15 09:40:26,358 : Validation : best param found is reg = 1e-05 with score             82.02
2019-02-15 09:40:26,358 : Evaluating...
2019-02-15 09:40:42,258 : 
Dev acc : 82.0 Test acc : 82.6 for LENGTH classification

2019-02-15 09:40:42,259 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 09:40:42,594 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 09:40:42,642 : loading BERT model bert-base-uncased
2019-02-15 09:40:42,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:40:42,671 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:40:42,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsbjtmqul
2019-02-15 09:40:44,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:40:46,337 : Computing embeddings for train/dev/test
2019-02-15 09:42:00,080 : Computed embeddings
2019-02-15 09:42:00,081 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:42:57,681 : [('reg:1e-05', 54.73), ('reg:0.0001', 34.06), ('reg:0.001', 5.16), ('reg:0.01', 1.08)]
2019-02-15 09:42:57,681 : Validation : best param found is reg = 1e-05 with score             54.73
2019-02-15 09:42:57,681 : Evaluating...
2019-02-15 09:43:13,464 : 
Dev acc : 54.7 Test acc : 55.0 for WORDCONTENT classification

2019-02-15 09:43:13,466 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 09:43:13,814 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 09:43:13,879 : loading BERT model bert-base-uncased
2019-02-15 09:43:13,879 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:43:13,971 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:43:13,971 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg1ofo395
2019-02-15 09:43:16,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:43:17,662 : Computing embeddings for train/dev/test
2019-02-15 09:44:26,721 : Computed embeddings
2019-02-15 09:44:26,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:44:59,155 : [('reg:1e-05', 34.66), ('reg:0.0001', 34.32), ('reg:0.001', 32.86), ('reg:0.01', 28.83)]
2019-02-15 09:44:59,155 : Validation : best param found is reg = 1e-05 with score             34.66
2019-02-15 09:44:59,155 : Evaluating...
2019-02-15 09:45:08,798 : 
Dev acc : 34.7 Test acc : 34.5 for DEPTH classification

2019-02-15 09:45:08,799 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 09:45:09,163 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 09:45:09,224 : loading BERT model bert-base-uncased
2019-02-15 09:45:09,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:45:09,329 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:45:09,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptvyx0y6j
2019-02-15 09:45:11,614 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:45:12,955 : Computing embeddings for train/dev/test
2019-02-15 09:46:17,766 : Computed embeddings
2019-02-15 09:46:17,766 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:47:14,417 : [('reg:1e-05', 70.96), ('reg:0.0001', 69.97), ('reg:0.001', 64.56), ('reg:0.01', 54.62)]
2019-02-15 09:47:14,417 : Validation : best param found is reg = 1e-05 with score             70.96
2019-02-15 09:47:14,417 : Evaluating...
2019-02-15 09:47:22,027 : 
Dev acc : 71.0 Test acc : 70.7 for TOPCONSTITUENTS classification

2019-02-15 09:47:22,028 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 09:47:22,531 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 09:47:22,595 : loading BERT model bert-base-uncased
2019-02-15 09:47:22,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:47:22,624 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:47:22,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv6qso49i
2019-02-15 09:47:24,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:47:26,283 : Computing embeddings for train/dev/test
2019-02-15 09:48:36,168 : Computed embeddings
2019-02-15 09:48:36,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:49:36,894 : [('reg:1e-05', 89.85), ('reg:0.0001', 89.84), ('reg:0.001', 89.54), ('reg:0.01', 88.01)]
2019-02-15 09:49:36,894 : Validation : best param found is reg = 1e-05 with score             89.85
2019-02-15 09:49:36,894 : Evaluating...
2019-02-15 09:49:54,019 : 
Dev acc : 89.8 Test acc : 89.3 for BIGRAMSHIFT classification

2019-02-15 09:49:54,021 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 09:49:54,573 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 09:49:54,637 : loading BERT model bert-base-uncased
2019-02-15 09:49:54,637 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:49:54,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:49:54,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcd6g3h63
2019-02-15 09:49:57,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:49:58,360 : Computing embeddings for train/dev/test
2019-02-15 09:51:06,741 : Computed embeddings
2019-02-15 09:51:06,741 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:51:48,302 : [('reg:1e-05', 88.89), ('reg:0.0001', 88.92), ('reg:0.001', 89.27), ('reg:0.01', 89.89)]
2019-02-15 09:51:48,303 : Validation : best param found is reg = 0.01 with score             89.89
2019-02-15 09:51:48,303 : Evaluating...
2019-02-15 09:51:59,018 : 
Dev acc : 89.9 Test acc : 89.1 for TENSE classification

2019-02-15 09:51:59,020 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 09:51:59,435 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 09:51:59,496 : loading BERT model bert-base-uncased
2019-02-15 09:51:59,496 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:51:59,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:51:59,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmputcvy0aw
2019-02-15 09:52:01,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:52:03,212 : Computing embeddings for train/dev/test
2019-02-15 09:53:15,876 : Computed embeddings
2019-02-15 09:53:15,876 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:54:02,590 : [('reg:1e-05', 85.08), ('reg:0.0001', 85.26), ('reg:0.001', 84.98), ('reg:0.01', 83.57)]
2019-02-15 09:54:02,590 : Validation : best param found is reg = 0.0001 with score             85.26
2019-02-15 09:54:02,590 : Evaluating...
2019-02-15 09:54:18,390 : 
Dev acc : 85.3 Test acc : 85.2 for SUBJNUMBER classification

2019-02-15 09:54:18,391 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 09:54:18,796 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 09:54:18,863 : loading BERT model bert-base-uncased
2019-02-15 09:54:18,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:54:18,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:54:18,976 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8y4m9qxm
2019-02-15 09:54:21,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:54:22,628 : Computing embeddings for train/dev/test
2019-02-15 09:55:34,052 : Computed embeddings
2019-02-15 09:55:34,053 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:56:25,865 : [('reg:1e-05', 81.03), ('reg:0.0001', 81.23), ('reg:0.001', 81.27), ('reg:0.01', 79.96)]
2019-02-15 09:56:25,865 : Validation : best param found is reg = 0.001 with score             81.27
2019-02-15 09:56:25,865 : Evaluating...
2019-02-15 09:56:38,916 : 
Dev acc : 81.3 Test acc : 81.1 for OBJNUMBER classification

2019-02-15 09:56:38,917 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 09:56:39,489 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 09:56:39,555 : loading BERT model bert-base-uncased
2019-02-15 09:56:39,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:56:39,582 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:56:39,582 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj9liey59
2019-02-15 09:56:41,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:56:43,217 : Computing embeddings for train/dev/test
2019-02-15 09:58:05,059 : Computed embeddings
2019-02-15 09:58:05,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 09:58:45,529 : [('reg:1e-05', 64.73), ('reg:0.0001', 64.71), ('reg:0.001', 64.32), ('reg:0.01', 63.84)]
2019-02-15 09:58:45,529 : Validation : best param found is reg = 1e-05 with score             64.73
2019-02-15 09:58:45,529 : Evaluating...
2019-02-15 09:58:56,447 : 
Dev acc : 64.7 Test acc : 64.7 for ODDMANOUT classification

2019-02-15 09:58:56,448 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 09:58:56,835 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 09:58:56,909 : loading BERT model bert-base-uncased
2019-02-15 09:58:56,909 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 09:58:57,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 09:58:57,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9_oxk4vf
2019-02-15 09:58:59,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 09:59:00,664 : Computing embeddings for train/dev/test
2019-02-15 10:00:22,080 : Computed embeddings
2019-02-15 10:00:22,080 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 10:01:10,444 : [('reg:1e-05', 69.2), ('reg:0.0001', 69.14), ('reg:0.001', 69.04), ('reg:0.01', 66.66)]
2019-02-15 10:01:10,444 : Validation : best param found is reg = 1e-05 with score             69.2
2019-02-15 10:01:10,444 : Evaluating...
2019-02-15 10:01:18,390 : 
Dev acc : 69.2 Test acc : 69.5 for COORDINATIONINVERSION classification

2019-02-15 10:01:18,393 : total results: {'STS12': {'MSRpar': {'pearson': (0.3742786620814638, 2.369308900915339e-26), 'spearman': SpearmanrResult(correlation=0.4150805944186608, pvalue=1.3491109933950578e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.37905832937284883, 4.871469703875893e-27), 'spearman': SpearmanrResult(correlation=0.41128935430933333, pvalue=5.592585408505575e-32), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.522178372260006, 1.807385103460622e-33), 'spearman': SpearmanrResult(correlation=0.6098767342614917, pvalue=4.233081104025531e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5901947596461897, 1.3777767214121588e-71), 'spearman': SpearmanrResult(correlation=0.5909172198464765, pvalue=8.430028428744891e-72), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5916586139290059, 4.8256589945921295e-39), 'spearman': SpearmanrResult(correlation=0.5588820693635927, pvalue=3.7164605758760252e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4914737474579029, 'wmean': 0.4772845795207184}, 'spearman': {'mean': 0.517209194439911, 'wmean': 0.503826493929521}}}, 'STS13': {'FNWN': {'pearson': (0.3346327564804086, 2.522343460750998e-06), 'spearman': SpearmanrResult(correlation=0.38543796879913433, pvalue=4.3298049094046294e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6114286465469412, 4.330613404595034e-78), 'spearman': SpearmanrResult(correlation=0.579740632518533, pvalue=1.469154651504318e-68), 'nsamples': 750}, 'OnWN': {'pearson': (0.5664761648789631, 6.303222174376555e-49), 'spearman': SpearmanrResult(correlation=0.5776253390670718, pvalue=3.0866998863855346e-51), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5041791893021043, 'wmean': 0.5597401362547343}, 'spearman': {'mean': 0.5142679801282464, 'wmean': 0.5544673771390423}}}, 'STS14': {'deft-forum': {'pearson': (0.2913342078398123, 2.9741796904408413e-10), 'spearman': SpearmanrResult(correlation=0.2889242575345343, pvalue=4.220326818443346e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.7532784307803164, 3.622918995433998e-56), 'spearman': SpearmanrResult(correlation=0.721396255191701, pvalue=1.795859492492797e-49), 'nsamples': 300}, 'headlines': {'pearson': (0.5759365092168149, 1.7455860486360548e-67), 'spearman': SpearmanrResult(correlation=0.5277903312999938, pvalue=5.095469749864704e-55), 'nsamples': 750}, 'images': {'pearson': (0.43191546366640104, 1.95211625985353e-35), 'spearman': SpearmanrResult(correlation=0.43309890127134026, pvalue=1.2156227146774e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.6977728684611177, 1.738594104089402e-110), 'spearman': SpearmanrResult(correlation=0.7209761024148984, pvalue=2.8580113346045624e-121), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6670184579009216, 1.0735926132625847e-97), 'spearman': SpearmanrResult(correlation=0.588574431450969, pvalue=4.127864639464488e-71), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5695426563108973, 'wmean': 0.5697510392522539}, 'spearman': {'mean': 0.5467933798605727, 'wmean': 0.5464705646069203}}}, 'STS15': {'answers-forums': {'pearson': (0.5931354490340288, 5.2851661123871735e-37), 'spearman': SpearmanrResult(correlation=0.5961946150554808, pvalue=1.8405105064354963e-37), 'nsamples': 375}, 'answers-students': {'pearson': (0.5936650947407499, 1.2865027589212489e-72), 'spearman': SpearmanrResult(correlation=0.6128044494565882, pvalue=1.577898460210915e-78), 'nsamples': 750}, 'belief': {'pearson': (0.6926102355214029, 7.145054349637188e-55), 'spearman': SpearmanrResult(correlation=0.7215043846279858, pvalue=1.6292464312163388e-61), 'nsamples': 375}, 'headlines': {'pearson': (0.6364431957103518, 2.056929123929693e-86), 'spearman': SpearmanrResult(correlation=0.6284001387579632, pvalue=1.1820640352963218e-83), 'nsamples': 750}, 'images': {'pearson': (0.6317730426904485, 8.421517625152642e-85), 'spearman': SpearmanrResult(correlation=0.6412043778238958, pvalue=4.3720808884194143e-88), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6295254035393965, 'wmean': 0.6261885438548166}, 'spearman': {'mean': 0.6400215931443828, 'wmean': 0.6353146164700451}}}, 'STS16': {'answer-answer': {'pearson': (0.5547949170060706, 6.64199712751142e-22), 'spearman': SpearmanrResult(correlation=0.539198663163022, pvalue=1.4683960436427394e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6461203225350568, 8.11170002083837e-31), 'spearman': SpearmanrResult(correlation=0.6554579242231074, pvalue=5.917420672452006e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7670784303649677, 7.680977758156366e-46), 'spearman': SpearmanrResult(correlation=0.7782677938374732, pvalue=5.679419665236807e-48), 'nsamples': 230}, 'postediting': {'pearson': (0.8073122033623287, 2.3440705018490744e-57), 'spearman': SpearmanrResult(correlation=0.8304533685822956, pvalue=2.009366649806199e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.4377921623098922, 3.3859021718555937e-11), 'spearman': SpearmanrResult(correlation=0.43050297360406875, pvalue=7.712692379408209e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6426196071156632, 'wmean': 0.646469348868372}, 'spearman': {'mean': 0.6467761446819934, 'wmean': 0.650735935560633}}}, 'MR': {'devacc': 81.28, 'acc': 80.46, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.35, 'acc': 86.23, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 88.08, 'acc': 88.39, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.49, 'acc': 94.95, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.47, 'acc': 85.5, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.69, 'acc': 44.93, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.26, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.93, 'acc': 73.33, 'f1': 80.31, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 76.68, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8092191887799094, 'pearson': 0.8085801008942196, 'spearman': 0.7411812882756645, 'mse': 0.3533752033657978, 'yhat': array([3.0136602 , 3.76507893, 1.66455727, ..., 3.17183616, 4.63199878,        4.71425885]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7005391109290238, 'pearson': 0.6565506226440389, 'spearman': 0.6497495904346781, 'mse': 1.5007045658522677, 'yhat': array([1.35970011, 1.71370868, 2.61022639, ..., 3.77048035, 3.69078057,        3.3895704 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.86, 'acc': 59.72, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 361.496, 'acc': [(34.36000000000001, 69.42, 82.72, 2.8000000000000003), (28.772, 63.483999999999995, 79.29599999999999, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 82.02, 'acc': 82.62, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 54.73, 'acc': 55.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.66, 'acc': 34.49, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.96, 'acc': 70.71, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.85, 'acc': 89.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.89, 'acc': 89.08, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.26, 'acc': 85.21, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.27, 'acc': 81.07, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.73, 'acc': 64.73, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.2, 'acc': 69.46, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 10:01:18,393 : STS12 p=0.4773, STS12 s=0.5038, STS13 p=0.5597, STS13 s=0.5545, STS14 p=0.5698, STS14 s=0.5465, STS15 p=0.6262, STS15 s=0.6353, STS 16 p=0.6465, STS16 s=0.6507, STS B p=0.6566, STS B s=0.6497, STS B m=1.5007, SICK-R p=0.8086, SICK-R s=0.7412, SICK-P m=0.3534
2019-02-15 10:01:18,393 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 10:01:18,393 : 0.4773,0.5038,0.5597,0.5545,0.5698,0.5465,0.6262,0.6353,0.6465,0.6507,0.6566,0.6497,1.5007,0.8086,0.7412,0.3534
2019-02-15 10:01:18,393 : MR=80.46, CR=86.23, SUBJ=94.95, MPQA=88.39, SST-B=85.50, SST-F=44.93, TREC=91.40, SICK-E=76.68, SNLI=59.72, MRPC=73.33, MRPC f=80.31
2019-02-15 10:01:18,393 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 10:01:18,393 : 80.46,86.23,94.95,88.39,85.50,44.93,91.40,76.68,59.72,73.33,80.31
2019-02-15 10:01:18,393 : COCO r1i2t=34.36, COCO r5i2t=69.42, COCO r10i2t=82.72, COCO medr_i2t=2.80, COCO r1t2i=28.77, COCO r5t2i=63.48, COCO r10t2i=79.30, COCO medr_t2i=3.20
2019-02-15 10:01:18,393 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 10:01:18,393 : 34.36,69.42,82.72,2.80,28.77,63.48,79.30,3.20
2019-02-15 10:01:18,393 : SentLen=82.62, WC=55.03, TreeDepth=34.49, TopConst=70.71, BShift=89.34, Tense=89.08, SubjNum=85.21, ObjNum=81.07, SOMO=64.73, CoordInv=69.46, average=72.17
2019-02-15 10:01:18,393 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 10:01:18,393 : 82.62,55.03,34.49,70.71,89.34,89.08,85.21,81.07,64.73,69.46,72.17
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,394 : layer 11
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,394 : ********************************************************************************
2019-02-15 10:01:18,498 : ***** Transfer task : STS12 *****


2019-02-15 10:01:18,534 : loading BERT model bert-base-uncased
2019-02-15 10:01:18,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:18,550 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:18,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgzgkfj9b
2019-02-15 10:01:20,831 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:23,709 : MSRpar : pearson = 0.3727, spearman = 0.4188
2019-02-15 10:01:24,355 : MSRvid : pearson = 0.4332, spearman = 0.4563
2019-02-15 10:01:24,908 : SMTeuroparl : pearson = 0.5116, spearman = 0.6016
2019-02-15 10:01:25,895 : surprise.OnWN : pearson = 0.5700, spearman = 0.5780
2019-02-15 10:01:26,449 : surprise.SMTnews : pearson = 0.5530, spearman = 0.5570
2019-02-15 10:01:26,449 : ALL (weighted average) : Pearson = 0.4786,             Spearman = 0.5110
2019-02-15 10:01:26,449 : ALL (average) : Pearson = 0.4881,             Spearman = 0.5223

2019-02-15 10:01:26,449 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 10:01:26,460 : loading BERT model bert-base-uncased
2019-02-15 10:01:26,460 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:26,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:26,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr8ujwu9x
2019-02-15 10:01:28,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:30,654 : FNWN : pearson = 0.3218, spearman = 0.3989
2019-02-15 10:01:31,408 : headlines : pearson = 0.6145, spearman = 0.5847
2019-02-15 10:01:31,974 : OnWN : pearson = 0.5643, spearman = 0.5790
2019-02-15 10:01:31,974 : ALL (weighted average) : Pearson = 0.5589,             Spearman = 0.5592
2019-02-15 10:01:31,974 : ALL (average) : Pearson = 0.5002,             Spearman = 0.5209

2019-02-15 10:01:31,974 : ***** Transfer task : STS14 *****


2019-02-15 10:01:31,989 : loading BERT model bert-base-uncased
2019-02-15 10:01:31,989 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:32,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:32,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpywqicqz6
2019-02-15 10:01:34,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:36,243 : deft-forum : pearson = 0.3093, spearman = 0.3024
2019-02-15 10:01:36,847 : deft-news : pearson = 0.7619, spearman = 0.7241
2019-02-15 10:01:37,680 : headlines : pearson = 0.5811, spearman = 0.5296
2019-02-15 10:01:38,481 : images : pearson = 0.4476, spearman = 0.4436
2019-02-15 10:01:39,292 : OnWN : pearson = 0.6964, spearman = 0.7190
2019-02-15 10:01:40,356 : tweet-news : pearson = 0.6447, spearman = 0.5694
2019-02-15 10:01:40,356 : ALL (weighted average) : Pearson = 0.5720,             Spearman = 0.5465
2019-02-15 10:01:40,356 : ALL (average) : Pearson = 0.5735,             Spearman = 0.5480

2019-02-15 10:01:40,357 : ***** Transfer task : STS15 *****


2019-02-15 10:01:40,413 : loading BERT model bert-base-uncased
2019-02-15 10:01:40,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:40,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:40,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpars2p4da
2019-02-15 10:01:42,718 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:44,787 : answers-forums : pearson = 0.5766, spearman = 0.5893
2019-02-15 10:01:45,592 : answers-students : pearson = 0.5470, spearman = 0.5735
2019-02-15 10:01:46,323 : belief : pearson = 0.6770, spearman = 0.7064
2019-02-15 10:01:47,181 : headlines : pearson = 0.6408, spearman = 0.6345
2019-02-15 10:01:48,002 : images : pearson = 0.6345, spearman = 0.6435
2019-02-15 10:01:48,002 : ALL (weighted average) : Pearson = 0.6123,             Spearman = 0.6249
2019-02-15 10:01:48,002 : ALL (average) : Pearson = 0.6152,             Spearman = 0.6294

2019-02-15 10:01:48,002 : ***** Transfer task : STS16 *****


2019-02-15 10:01:48,056 : loading BERT model bert-base-uncased
2019-02-15 10:01:48,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:48,073 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:48,073 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5fo3idu1
2019-02-15 10:01:50,375 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:52,061 : answer-answer : pearson = 0.5358, spearman = 0.5214
2019-02-15 10:01:52,324 : headlines : pearson = 0.6446, spearman = 0.6519
2019-02-15 10:01:52,659 : plagiarism : pearson = 0.7591, spearman = 0.7712
2019-02-15 10:01:53,190 : postediting : pearson = 0.8021, spearman = 0.8328
2019-02-15 10:01:53,431 : question-question : pearson = 0.4799, spearman = 0.4727
2019-02-15 10:01:53,431 : ALL (weighted average) : Pearson = 0.6469,             Spearman = 0.6527
2019-02-15 10:01:53,431 : ALL (average) : Pearson = 0.6443,             Spearman = 0.6500

2019-02-15 10:01:53,431 : ***** Transfer task : MR *****


2019-02-15 10:01:53,479 : loading BERT model bert-base-uncased
2019-02-15 10:01:53,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:01:53,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:01:53,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_oaq0uma
2019-02-15 10:01:55,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:01:57,159 : Generating sentence embeddings
2019-02-15 10:02:08,584 : Generated sentence embeddings
2019-02-15 10:02:08,584 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 10:02:23,099 : Best param found at split 1: l2reg = 1e-05                 with score 80.85
2019-02-15 10:02:37,147 : Best param found at split 2: l2reg = 0.0001                 with score 81.03
2019-02-15 10:02:49,432 : Best param found at split 3: l2reg = 0.001                 with score 81.3
2019-02-15 10:02:59,023 : Best param found at split 4: l2reg = 1e-05                 with score 80.66
2019-02-15 10:03:13,813 : Best param found at split 5: l2reg = 0.001                 with score 81.11
2019-02-15 10:03:14,689 : Dev acc : 80.99 Test acc : 80.2

2019-02-15 10:03:14,691 : ***** Transfer task : CR *****


2019-02-15 10:03:14,704 : loading BERT model bert-base-uncased
2019-02-15 10:03:14,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:03:14,729 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:03:14,729 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeh532_4b
2019-02-15 10:03:17,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:03:18,371 : Generating sentence embeddings
2019-02-15 10:03:21,501 : Generated sentence embeddings
2019-02-15 10:03:21,502 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 10:03:25,527 : Best param found at split 1: l2reg = 0.01                 with score 86.82
2019-02-15 10:03:29,166 : Best param found at split 2: l2reg = 0.001                 with score 86.92
2019-02-15 10:03:33,204 : Best param found at split 3: l2reg = 1e-05                 with score 87.22
2019-02-15 10:03:40,873 : Best param found at split 4: l2reg = 0.01                 with score 87.06
2019-02-15 10:03:49,170 : Best param found at split 5: l2reg = 0.0001                 with score 87.36
2019-02-15 10:03:49,556 : Dev acc : 87.08 Test acc : 85.38

2019-02-15 10:03:49,557 : ***** Transfer task : MPQA *****


2019-02-15 10:03:49,608 : loading BERT model bert-base-uncased
2019-02-15 10:03:49,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:03:49,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:03:49,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_hwn5d8_
2019-02-15 10:03:51,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:03:53,275 : Generating sentence embeddings
2019-02-15 10:03:56,433 : Generated sentence embeddings
2019-02-15 10:03:56,433 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 10:04:10,350 : Best param found at split 1: l2reg = 0.001                 with score 87.94
2019-02-15 10:04:22,352 : Best param found at split 2: l2reg = 0.01                 with score 87.74
2019-02-15 10:04:37,155 : Best param found at split 3: l2reg = 0.01                 with score 87.22
2019-02-15 10:04:50,760 : Best param found at split 4: l2reg = 1e-05                 with score 88.41
2019-02-15 10:05:02,369 : Best param found at split 5: l2reg = 0.0001                 with score 87.76
2019-02-15 10:05:02,892 : Dev acc : 87.81 Test acc : 88.22

2019-02-15 10:05:02,893 : ***** Transfer task : SUBJ *****


2019-02-15 10:05:02,908 : loading BERT model bert-base-uncased
2019-02-15 10:05:02,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:05:02,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:05:02,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph6i633f3
2019-02-15 10:05:05,266 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:05:06,643 : Generating sentence embeddings
2019-02-15 10:05:17,904 : Generated sentence embeddings
2019-02-15 10:05:17,904 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 10:05:29,721 : Best param found at split 1: l2reg = 0.001                 with score 95.64
2019-02-15 10:05:43,918 : Best param found at split 2: l2reg = 0.0001                 with score 95.55
2019-02-15 10:05:54,427 : Best param found at split 3: l2reg = 0.001                 with score 95.35
2019-02-15 10:06:05,564 : Best param found at split 4: l2reg = 1e-05                 with score 95.69
2019-02-15 10:06:16,930 : Best param found at split 5: l2reg = 0.001                 with score 95.39
2019-02-15 10:06:18,128 : Dev acc : 95.52 Test acc : 95.14

2019-02-15 10:06:18,129 : ***** Transfer task : SST Binary classification *****


2019-02-15 10:06:18,233 : loading BERT model bert-base-uncased
2019-02-15 10:06:18,233 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:06:18,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:06:18,306 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphqt832zf
2019-02-15 10:06:20,621 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:06:21,977 : Computing embedding for train
2019-02-15 10:06:59,486 : Computed train embeddings
2019-02-15 10:06:59,486 : Computing embedding for dev
2019-02-15 10:07:00,295 : Computed dev embeddings
2019-02-15 10:07:00,295 : Computing embedding for test
2019-02-15 10:07:01,967 : Computed test embeddings
2019-02-15 10:07:01,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 10:07:30,633 : [('reg:1e-05', 85.32), ('reg:0.0001', 85.21), ('reg:0.001', 85.44), ('reg:0.01', 85.09)]
2019-02-15 10:07:30,634 : Validation : best param found is reg = 0.001 with score             85.44
2019-02-15 10:07:30,634 : Evaluating...
2019-02-15 10:07:38,723 : 
Dev acc : 85.44 Test acc : 85.34 for             SST Binary classification

2019-02-15 10:07:38,724 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 10:07:38,793 : loading BERT model bert-base-uncased
2019-02-15 10:07:38,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:07:38,815 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:07:38,815 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyhvp1smw
2019-02-15 10:07:41,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:07:42,460 : Computing embedding for train
2019-02-15 10:07:50,482 : Computed train embeddings
2019-02-15 10:07:50,482 : Computing embedding for dev
2019-02-15 10:07:51,523 : Computed dev embeddings
2019-02-15 10:07:51,523 : Computing embedding for test
2019-02-15 10:07:53,586 : Computed test embeddings
2019-02-15 10:07:53,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 10:07:59,177 : [('reg:1e-05', 45.14), ('reg:0.0001', 44.87), ('reg:0.001', 45.87), ('reg:0.01', 44.5)]
2019-02-15 10:07:59,178 : Validation : best param found is reg = 0.001 with score             45.87
2019-02-15 10:07:59,178 : Evaluating...
2019-02-15 10:08:00,678 : 
Dev acc : 45.87 Test acc : 46.11 for             SST Fine-Grained classification

2019-02-15 10:08:00,678 : ***** Transfer task : TREC *****


2019-02-15 10:08:00,698 : loading BERT model bert-base-uncased
2019-02-15 10:08:00,699 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:08:00,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:08:00,723 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprtiuzv0p
2019-02-15 10:08:03,060 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:08:07,281 : Computed train embeddings
2019-02-15 10:08:07,505 : Computed test embeddings
2019-02-15 10:08:07,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 10:08:17,320 : [('reg:1e-05', 84.1), ('reg:0.0001', 84.21), ('reg:0.001', 83.73), ('reg:0.01', 75.47)]
2019-02-15 10:08:17,320 : Cross-validation : best param found is reg = 0.0001             with score 84.21
2019-02-15 10:08:17,320 : Evaluating...
2019-02-15 10:08:17,986 : 
Dev acc : 84.21 Test acc : 91.6             for TREC

2019-02-15 10:08:17,987 : ***** Transfer task : MRPC *****


2019-02-15 10:08:18,016 : loading BERT model bert-base-uncased
2019-02-15 10:08:18,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:08:18,044 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:08:18,044 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2_l4b50m
2019-02-15 10:08:20,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:08:21,677 : Computing embedding for train
2019-02-15 10:08:29,847 : Computed train embeddings
2019-02-15 10:08:29,847 : Computing embedding for test
2019-02-15 10:08:33,397 : Computed test embeddings
2019-02-15 10:08:33,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 10:08:38,002 : [('reg:1e-05', 73.72), ('reg:0.0001', 73.5), ('reg:0.001', 73.26), ('reg:0.01', 72.15)]
2019-02-15 10:08:38,002 : Cross-validation : best param found is reg = 1e-05             with score 73.72
2019-02-15 10:08:38,002 : Evaluating...
2019-02-15 10:08:38,390 : Dev acc : 73.72 Test acc 73.33; Test F1 80.57 for MRPC.

2019-02-15 10:08:38,390 : ***** Transfer task : SICK-Entailment*****


2019-02-15 10:08:38,447 : loading BERT model bert-base-uncased
2019-02-15 10:08:38,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:08:38,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:08:38,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8h23kiwm
2019-02-15 10:08:40,765 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:08:42,113 : Computing embedding for train
2019-02-15 10:08:46,464 : Computed train embeddings
2019-02-15 10:08:46,464 : Computing embedding for dev
2019-02-15 10:08:47,035 : Computed dev embeddings
2019-02-15 10:08:47,035 : Computing embedding for test
2019-02-15 10:08:51,694 : Computed test embeddings
2019-02-15 10:08:51,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 10:08:54,394 : [('reg:1e-05', 76.8), ('reg:0.0001', 75.4), ('reg:0.001', 75.4), ('reg:0.01', 75.6)]
2019-02-15 10:08:54,394 : Validation : best param found is reg = 1e-05 with score             76.8
2019-02-15 10:08:54,394 : Evaluating...
2019-02-15 10:08:55,200 : 
Dev acc : 76.8 Test acc : 77.23 for                        SICK entailment

2019-02-15 10:08:55,201 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 10:08:55,236 : loading BERT model bert-base-uncased
2019-02-15 10:08:55,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:08:55,296 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:08:55,297 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8rgubcvr
2019-02-15 10:08:57,642 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:08:58,985 : Computing embedding for train
2019-02-15 10:09:03,349 : Computed train embeddings
2019-02-15 10:09:03,349 : Computing embedding for dev
2019-02-15 10:09:03,921 : Computed dev embeddings
2019-02-15 10:09:03,921 : Computing embedding for test
2019-02-15 10:09:08,580 : Computed test embeddings
2019-02-15 10:09:37,057 : Dev : Pearson 0.8057024050424405
2019-02-15 10:09:37,057 : Test : Pearson 0.8129063299277852 Spearman 0.7428656341923232 MSE 0.34691890003171844                        for SICK Relatedness

2019-02-15 10:09:37,058 : 

***** Transfer task : STSBenchmark*****


2019-02-15 10:09:37,105 : loading BERT model bert-base-uncased
2019-02-15 10:09:37,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:09:37,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:09:37,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqf_tbpnq
2019-02-15 10:09:39,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:09:40,760 : Computing embedding for train
2019-02-15 10:09:47,784 : Computed train embeddings
2019-02-15 10:09:47,784 : Computing embedding for dev
2019-02-15 10:09:49,855 : Computed dev embeddings
2019-02-15 10:09:49,855 : Computing embedding for test
2019-02-15 10:09:51,530 : Computed test embeddings
2019-02-15 10:10:28,792 : Dev : Pearson 0.7190162624647348
2019-02-15 10:10:28,792 : Test : Pearson 0.6504388803995269 Spearman 0.64292642411981 MSE 1.48381559769896                        for SICK Relatedness

2019-02-15 10:10:28,792 : ***** Transfer task : SNLI Entailment*****


2019-02-15 10:10:33,321 : loading BERT model bert-base-uncased
2019-02-15 10:10:33,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:10:33,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:10:33,437 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw4rz14ri
2019-02-15 10:10:35,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:10:37,208 : PROGRESS (encoding): 0.00%
2019-02-15 10:11:42,137 : PROGRESS (encoding): 14.56%
2019-02-15 10:12:54,573 : PROGRESS (encoding): 29.12%
2019-02-15 10:14:07,422 : PROGRESS (encoding): 43.69%
2019-02-15 10:15:26,462 : PROGRESS (encoding): 58.25%
2019-02-15 10:16:54,352 : PROGRESS (encoding): 72.81%
2019-02-15 10:18:22,076 : PROGRESS (encoding): 87.37%
2019-02-15 10:19:55,232 : PROGRESS (encoding): 0.00%
2019-02-15 10:20:06,751 : PROGRESS (encoding): 0.00%
2019-02-15 10:20:17,859 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 10:20:45,953 : [('reg:1e-09', 65.7)]
2019-02-15 10:20:45,953 : Validation : best param found is reg = 1e-09 with score             65.7
2019-02-15 10:20:45,953 : Evaluating...
2019-02-15 10:21:13,173 : Dev acc : 65.7 Test acc : 65.78 for SNLI

2019-02-15 10:21:13,173 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 10:21:22,857 : loading BERT model bert-base-uncased
2019-02-15 10:21:22,857 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 10:21:22,900 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 10:21:22,900 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu13fj05p
2019-02-15 10:21:25,176 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 10:21:26,524 : Computing embedding for train
2019-02-15 10:27:45,013 : Computed train embeddings
2019-02-15 10:27:45,013 : Computing embedding for dev
2019-02-15 10:28:01,555 : Computed dev embeddings
2019-02-15 10:28:01,555 : Computing embedding for test
2019-02-15 10:28:18,558 : Computed test embeddings
2019-02-15 10:28:18,573 : prepare data
2019-02-15 10:28:18,634 : start epoch
2019-02-15 10:28:56,823 : samples : 64000
2019-02-15 10:29:06,173 : Image to text: 7.9, 23.3, 34.4, 23.0
2019-02-15 10:29:12,511 : Text to Image: 6.148, 19.588, 30.0, 27.0
2019-02-15 10:29:50,743 : samples : 128000
2019-02-15 10:30:00,049 : Image to text: 9.38, 26.38, 37.88, 18.0
2019-02-15 10:30:06,417 : Text to Image: 7.3, 21.952, 33.2, 23.0
2019-02-15 10:30:44,946 : samples : 192000
2019-02-15 10:30:54,268 : Image to text: 9.94, 26.54, 39.12, 18.0
2019-02-15 10:31:00,638 : Text to Image: 7.62, 22.76, 33.7, 22.0
2019-02-15 10:31:39,124 : samples : 256000
2019-02-15 10:31:48,442 : Image to text: 10.0, 27.12, 39.9, 17.0
2019-02-15 10:31:54,788 : Text to Image: 7.932, 23.232, 34.564, 21.0
2019-02-15 10:32:33,325 : samples : 320000
2019-02-15 10:32:42,634 : Image to text: 8.98, 26.68, 38.84, 18.0
2019-02-15 10:32:49,046 : Text to Image: 8.008, 23.776, 35.288, 21.0
2019-02-15 10:33:27,264 : samples : 384000
2019-02-15 10:33:36,581 : Image to text: 10.76, 29.5, 41.12, 16.0
2019-02-15 10:33:42,913 : Text to Image: 8.552, 25.284, 37.212, 19.0
2019-02-15 10:34:20,969 : samples : 448000
2019-02-15 10:34:30,287 : Image to text: 10.14, 29.12, 40.72, 17.0
2019-02-15 10:34:36,633 : Text to Image: 8.184, 24.14, 36.152, 20.0
2019-02-15 10:35:14,780 : samples : 512000
2019-02-15 10:35:24,061 : Image to text: 11.02, 29.48, 41.74, 16.0
2019-02-15 10:35:30,427 : Text to Image: 9.032, 25.728, 37.476, 18.0
2019-02-15 10:36:02,858 : Epoch 1 finished
2019-02-15 10:36:03,216 : Image to text: 27.8, 60.4, 75.0, 4.0
2019-02-15 10:36:03,478 : Text to Image: 23.38, 55.62, 72.68, 4.0
2019-02-15 10:36:03,836 : Image to text: 27.5, 59.9, 74.6, 4.0
2019-02-15 10:36:04,097 : Text to Image: 21.3, 54.46, 72.2, 5.0
2019-02-15 10:36:04,455 : Image to text: 26.5, 59.4, 75.9, 4.0
2019-02-15 10:36:04,716 : Text to Image: 22.12, 55.94, 72.86, 4.0
2019-02-15 10:36:05,074 : Image to text: 26.5, 59.6, 73.4, 4.0
2019-02-15 10:36:05,335 : Text to Image: 22.8, 56.06, 73.46, 4.0
2019-02-15 10:36:05,693 : Image to text: 28.0, 60.9, 76.1, 4.0
2019-02-15 10:36:05,953 : Text to Image: 23.04, 56.12, 71.9, 4.0
2019-02-15 10:36:05,953 : Dev mean Text to Image: 22.528000000000002, 55.64, 72.62, 4.2
2019-02-15 10:36:05,953 : Dev mean Image to text: 27.259999999999998, 60.04, 75.0, 4.0
2019-02-15 10:36:05,954 : start epoch
2019-02-15 10:36:44,069 : samples : 64000
2019-02-15 10:36:53,394 : Image to text: 10.32, 30.04, 42.32, 16.0
2019-02-15 10:36:59,731 : Text to Image: 8.868, 25.728, 37.704, 18.0
2019-02-15 10:37:37,835 : samples : 128000
2019-02-15 10:37:47,175 : Image to text: 10.2, 27.76, 39.82, 17.0
2019-02-15 10:37:53,552 : Text to Image: 8.468, 25.308, 37.236, 19.0
2019-02-15 10:38:31,705 : samples : 192000
2019-02-15 10:38:41,008 : Image to text: 11.12, 30.4, 42.64, 15.0
2019-02-15 10:38:47,366 : Text to Image: 9.28, 26.708, 38.944, 17.0
2019-02-15 10:39:25,531 : samples : 256000
2019-02-15 10:39:34,853 : Image to text: 11.76, 31.32, 43.1, 14.0
2019-02-15 10:39:41,187 : Text to Image: 9.424, 27.132, 39.34, 17.0
2019-02-15 10:40:19,304 : samples : 320000
2019-02-15 10:40:28,598 : Image to text: 11.14, 30.14, 43.38, 14.0
2019-02-15 10:40:34,966 : Text to Image: 9.292, 26.864, 39.4, 17.0
2019-02-15 10:41:13,018 : samples : 384000
2019-02-15 10:41:22,373 : Image to text: 11.62, 30.22, 43.16, 14.0
2019-02-15 10:41:28,728 : Text to Image: 9.428, 26.548, 38.936, 17.0
2019-02-15 10:42:06,618 : samples : 448000
2019-02-15 10:42:15,936 : Image to text: 11.42, 31.74, 44.98, 13.0
2019-02-15 10:42:22,269 : Text to Image: 9.32, 27.248, 39.448, 17.0
2019-02-15 10:43:00,181 : samples : 512000
2019-02-15 10:43:09,496 : Image to text: 11.44, 30.42, 43.86, 14.0
2019-02-15 10:43:15,859 : Text to Image: 9.576, 27.26, 39.644, 17.0
2019-02-15 10:43:48,189 : Epoch 2 finished
2019-02-15 10:43:48,547 : Image to text: 30.1, 62.1, 76.9, 3.0
2019-02-15 10:43:48,808 : Text to Image: 24.54, 58.02, 74.62, 4.0
2019-02-15 10:43:49,166 : Image to text: 27.3, 60.0, 75.3, 4.0
2019-02-15 10:43:49,427 : Text to Image: 22.76, 56.24, 74.02, 4.0
2019-02-15 10:43:49,784 : Image to text: 28.7, 63.4, 77.4, 3.0
2019-02-15 10:43:50,046 : Text to Image: 24.52, 57.62, 74.8, 4.0
2019-02-15 10:43:50,404 : Image to text: 29.6, 63.1, 75.6, 3.0
2019-02-15 10:43:50,666 : Text to Image: 24.18, 57.76, 74.36, 4.0
2019-02-15 10:43:51,024 : Image to text: 29.8, 64.0, 76.6, 3.0
2019-02-15 10:43:51,285 : Text to Image: 24.62, 57.78, 74.02, 4.0
2019-02-15 10:43:51,285 : Dev mean Text to Image: 24.124000000000002, 57.48400000000001, 74.364, 4.0
2019-02-15 10:43:51,285 : Dev mean Image to text: 29.1, 62.519999999999996, 76.36, 3.2
2019-02-15 10:43:51,285 : start epoch
2019-02-15 10:44:29,077 : samples : 64000
2019-02-15 10:44:38,410 : Image to text: 11.64, 31.72, 43.8, 14.0
2019-02-15 10:44:44,740 : Text to Image: 9.464, 27.404, 39.608, 17.0
2019-02-15 10:45:22,616 : samples : 128000
2019-02-15 10:45:31,963 : Image to text: 11.3, 32.74, 45.06, 13.0
2019-02-15 10:45:38,305 : Text to Image: 9.752, 27.796, 39.588, 17.0
2019-02-15 10:46:16,138 : samples : 192000
2019-02-15 10:46:25,450 : Image to text: 12.84, 32.24, 45.46, 13.0
2019-02-15 10:46:31,852 : Text to Image: 10.288, 28.124, 40.34, 16.0
2019-02-15 10:47:09,728 : samples : 256000
2019-02-15 10:47:19,064 : Image to text: 12.76, 33.82, 46.28, 12.0
2019-02-15 10:47:25,400 : Text to Image: 10.44, 28.852, 40.972, 16.0
2019-02-15 10:48:03,245 : samples : 320000
2019-02-15 10:48:12,534 : Image to text: 11.86, 31.82, 44.62, 13.0
2019-02-15 10:48:18,908 : Text to Image: 9.612, 27.192, 39.732, 17.0
2019-02-15 10:48:56,729 : samples : 384000
2019-02-15 10:49:06,062 : Image to text: 12.38, 32.3, 44.96, 13.0
2019-02-15 10:49:12,403 : Text to Image: 9.76, 27.732, 40.072, 16.0
2019-02-15 10:49:50,251 : samples : 448000
2019-02-15 10:49:59,557 : Image to text: 12.6, 33.02, 46.28, 12.0
2019-02-15 10:50:05,964 : Text to Image: 10.452, 29.092, 41.604, 15.0
2019-02-15 10:50:44,023 : samples : 512000
2019-02-15 10:50:53,393 : Image to text: 13.14, 34.64, 47.56, 12.0
2019-02-15 10:50:59,745 : Text to Image: 10.228, 28.768, 41.348, 16.0
2019-02-15 10:51:32,370 : Epoch 3 finished
2019-02-15 10:51:32,728 : Image to text: 31.2, 66.0, 78.0, 3.0
2019-02-15 10:51:32,990 : Text to Image: 25.06, 60.34, 76.38, 4.0
2019-02-15 10:51:33,348 : Image to text: 28.3, 61.8, 77.1, 3.0
2019-02-15 10:51:33,609 : Text to Image: 23.7, 59.24, 75.54, 4.0
2019-02-15 10:51:33,968 : Image to text: 31.6, 64.5, 78.0, 3.0
2019-02-15 10:51:34,229 : Text to Image: 25.84, 59.6, 76.48, 4.0
2019-02-15 10:51:34,587 : Image to text: 33.7, 66.5, 79.7, 3.0
2019-02-15 10:51:34,849 : Text to Image: 26.16, 60.08, 76.76, 4.0
2019-02-15 10:51:35,206 : Image to text: 32.7, 66.7, 78.8, 3.0
2019-02-15 10:51:35,467 : Text to Image: 25.7, 60.18, 76.26, 4.0
2019-02-15 10:51:35,467 : Dev mean Text to Image: 25.291999999999998, 59.888000000000005, 76.28399999999999, 4.0
2019-02-15 10:51:35,467 : Dev mean Image to text: 31.5, 65.10000000000001, 78.32000000000001, 3.0
2019-02-15 10:51:35,467 : start epoch
2019-02-15 10:52:13,688 : samples : 64000
2019-02-15 10:52:23,015 : Image to text: 12.24, 33.06, 45.92, 13.0
2019-02-15 10:52:29,352 : Text to Image: 10.028, 28.78, 41.216, 16.0
2019-02-15 10:53:07,626 : samples : 128000
2019-02-15 10:53:16,951 : Image to text: 13.18, 33.96, 46.88, 12.0
2019-02-15 10:53:23,290 : Text to Image: 10.216, 28.728, 41.116, 16.0
2019-02-15 10:54:01,429 : samples : 192000
2019-02-15 10:54:10,743 : Image to text: 12.2, 33.52, 46.42, 12.0
2019-02-15 10:54:17,103 : Text to Image: 10.428, 28.66, 41.276, 16.0
2019-02-15 10:54:55,220 : samples : 256000
2019-02-15 10:55:04,621 : Image to text: 12.52, 33.24, 46.78, 12.0
2019-02-15 10:55:10,961 : Text to Image: 10.452, 29.432, 41.94, 15.0
2019-02-15 10:55:49,190 : samples : 320000
2019-02-15 10:55:58,511 : Image to text: 12.78, 33.7, 46.44, 12.0
2019-02-15 10:56:04,894 : Text to Image: 10.296, 29.108, 41.716, 15.0
2019-02-15 10:56:43,232 : samples : 384000
2019-02-15 10:56:52,558 : Image to text: 13.3, 34.46, 47.0, 12.0
2019-02-15 10:56:58,903 : Text to Image: 10.368, 29.064, 41.724, 15.0
2019-02-15 10:57:37,137 : samples : 448000
2019-02-15 10:57:46,476 : Image to text: 13.26, 33.94, 47.44, 12.0
2019-02-15 10:57:52,810 : Text to Image: 10.616, 29.108, 41.672, 15.0
2019-02-15 10:58:31,088 : samples : 512000
2019-02-15 10:58:40,393 : Image to text: 12.72, 33.56, 47.08, 12.0
2019-02-15 10:58:46,767 : Text to Image: 10.328, 28.84, 41.248, 16.0
2019-02-15 10:59:19,423 : Epoch 4 finished
2019-02-15 10:59:19,785 : Image to text: 33.3, 65.8, 81.4, 3.0
2019-02-15 10:59:20,049 : Text to Image: 26.28, 61.34, 76.94, 4.0
2019-02-15 10:59:20,410 : Image to text: 30.5, 65.2, 78.8, 3.0
2019-02-15 10:59:20,674 : Text to Image: 24.8, 59.16, 76.44, 4.0
2019-02-15 10:59:21,035 : Image to text: 33.0, 64.7, 79.9, 3.0
2019-02-15 10:59:21,299 : Text to Image: 26.04, 60.24, 76.54, 4.0
2019-02-15 10:59:21,661 : Image to text: 31.2, 66.9, 80.3, 3.0
2019-02-15 10:59:21,924 : Text to Image: 26.16, 60.84, 77.34, 4.0
2019-02-15 10:59:22,285 : Image to text: 34.1, 67.7, 80.1, 3.0
2019-02-15 10:59:22,547 : Text to Image: 26.08, 60.62, 76.28, 4.0
2019-02-15 10:59:22,547 : Dev mean Text to Image: 25.872, 60.44, 76.708, 4.0
2019-02-15 10:59:22,547 : Dev mean Image to text: 32.42, 66.06, 80.1, 3.0
2019-02-15 10:59:22,547 : start epoch
2019-02-15 11:00:00,951 : samples : 64000
2019-02-15 11:00:10,259 : Image to text: 13.54, 34.48, 47.9, 12.0
2019-02-15 11:00:16,648 : Text to Image: 11.1, 30.044, 42.788, 15.0
2019-02-15 11:00:54,996 : samples : 128000
2019-02-15 11:01:04,340 : Image to text: 12.5, 33.68, 46.68, 12.0
2019-02-15 11:01:10,687 : Text to Image: 10.684, 29.56, 42.144, 15.0
2019-02-15 11:01:49,169 : samples : 192000
2019-02-15 11:01:58,586 : Image to text: 13.38, 33.9, 47.82, 12.0
2019-02-15 11:02:04,981 : Text to Image: 10.336, 29.204, 41.936, 15.0
2019-02-15 11:02:43,413 : samples : 256000
2019-02-15 11:02:52,813 : Image to text: 13.58, 34.52, 47.58, 12.0
2019-02-15 11:02:59,161 : Text to Image: 10.812, 29.724, 42.304, 15.0
2019-02-15 11:03:37,546 : samples : 320000
2019-02-15 11:03:46,907 : Image to text: 13.42, 34.22, 47.7, 12.0
2019-02-15 11:03:53,251 : Text to Image: 10.896, 30.108, 42.864, 15.0
2019-02-15 11:04:31,584 : samples : 384000
2019-02-15 11:04:40,895 : Image to text: 13.22, 33.98, 47.26, 12.0
2019-02-15 11:04:47,260 : Text to Image: 10.832, 29.956, 42.804, 15.0
2019-02-15 11:05:25,605 : samples : 448000
2019-02-15 11:05:34,941 : Image to text: 12.9, 34.38, 48.16, 11.0
2019-02-15 11:05:41,283 : Text to Image: 10.396, 29.24, 42.04, 15.0
2019-02-15 11:06:19,723 : samples : 512000
2019-02-15 11:06:29,032 : Image to text: 13.9, 34.44, 47.6, 12.0
2019-02-15 11:06:35,408 : Text to Image: 10.9, 30.016, 42.42, 15.0
2019-02-15 11:07:08,054 : Epoch 5 finished
2019-02-15 11:07:08,412 : Image to text: 32.4, 66.4, 80.7, 3.0
2019-02-15 11:07:08,673 : Text to Image: 25.0, 60.52, 77.06, 4.0
2019-02-15 11:07:09,031 : Image to text: 29.8, 63.5, 78.2, 3.0
2019-02-15 11:07:09,293 : Text to Image: 24.04, 58.76, 75.66, 4.0
2019-02-15 11:07:09,651 : Image to text: 34.9, 65.1, 80.9, 3.0
2019-02-15 11:07:09,912 : Text to Image: 26.38, 60.36, 76.9, 4.0
2019-02-15 11:07:10,270 : Image to text: 33.9, 65.6, 80.0, 3.0
2019-02-15 11:07:10,534 : Text to Image: 25.88, 61.0, 76.6, 4.0
2019-02-15 11:07:10,896 : Image to text: 32.4, 64.3, 78.5, 3.0
2019-02-15 11:07:11,158 : Text to Image: 26.14, 60.94, 76.52, 4.0
2019-02-15 11:07:11,158 : Dev mean Text to Image: 25.488, 60.316, 76.548, 4.0
2019-02-15 11:07:11,158 : Dev mean Image to text: 32.67999999999999, 64.97999999999999, 79.66, 3.0
2019-02-15 11:07:11,158 : start epoch
2019-02-15 11:07:49,386 : samples : 64000
2019-02-15 11:07:58,730 : Image to text: 13.2, 34.92, 49.1, 11.0
2019-02-15 11:08:05,122 : Text to Image: 11.284, 30.696, 43.256, 14.0
2019-02-15 11:08:43,393 : samples : 128000
2019-02-15 11:08:52,726 : Image to text: 13.7, 34.8, 48.44, 11.0
2019-02-15 11:08:59,071 : Text to Image: 10.756, 29.944, 42.604, 15.0
2019-02-15 11:09:37,311 : samples : 192000
2019-02-15 11:09:46,674 : Image to text: 13.96, 35.2, 48.62, 11.0
2019-02-15 11:09:53,013 : Text to Image: 10.732, 29.952, 42.672, 15.0
2019-02-15 11:10:31,316 : samples : 256000
2019-02-15 11:10:40,631 : Image to text: 12.96, 34.46, 47.64, 12.0
2019-02-15 11:10:46,999 : Text to Image: 10.5, 29.548, 42.332, 15.0
2019-02-15 11:11:25,199 : samples : 320000
2019-02-15 11:11:34,554 : Image to text: 13.22, 34.32, 47.8, 11.0
2019-02-15 11:11:40,899 : Text to Image: 10.72, 29.956, 42.244, 15.0
2019-02-15 11:12:19,032 : samples : 384000
2019-02-15 11:12:28,344 : Image to text: 13.78, 35.68, 49.32, 11.0
2019-02-15 11:12:34,702 : Text to Image: 10.912, 30.42, 43.32, 14.0
2019-02-15 11:13:12,940 : samples : 448000
2019-02-15 11:13:22,291 : Image to text: 13.76, 35.72, 48.92, 11.0
2019-02-15 11:13:28,638 : Text to Image: 11.036, 30.384, 43.116, 14.0
2019-02-15 11:14:06,862 : samples : 512000
2019-02-15 11:14:16,176 : Image to text: 13.36, 34.26, 47.32, 12.0
2019-02-15 11:14:22,517 : Text to Image: 10.908, 29.96, 42.788, 14.0
2019-02-15 11:14:55,064 : Epoch 6 finished
2019-02-15 11:14:55,423 : Image to text: 32.9, 66.8, 82.3, 3.0
2019-02-15 11:14:55,689 : Text to Image: 25.6, 62.18, 77.98, 4.0
2019-02-15 11:14:56,048 : Image to text: 31.9, 65.8, 78.9, 3.0
2019-02-15 11:14:56,309 : Text to Image: 24.82, 60.22, 76.36, 4.0
2019-02-15 11:14:56,667 : Image to text: 34.4, 66.8, 80.2, 3.0
2019-02-15 11:14:56,929 : Text to Image: 27.24, 61.52, 77.72, 4.0
2019-02-15 11:14:57,288 : Image to text: 34.9, 68.5, 80.0, 3.0
2019-02-15 11:14:57,550 : Text to Image: 27.7, 61.46, 77.8, 3.0
2019-02-15 11:14:57,908 : Image to text: 35.5, 67.6, 79.8, 3.0
2019-02-15 11:14:58,170 : Text to Image: 28.02, 61.56, 76.6, 4.0
2019-02-15 11:14:58,170 : Dev mean Text to Image: 26.676, 61.388000000000005, 77.292, 3.8000000000000007
2019-02-15 11:14:58,170 : Dev mean Image to text: 33.92, 67.1, 80.24, 3.0
2019-02-15 11:14:58,170 : start epoch
2019-02-15 11:15:36,330 : samples : 64000
2019-02-15 11:15:45,656 : Image to text: 14.16, 35.94, 48.56, 11.0
2019-02-15 11:15:52,005 : Text to Image: 11.1, 30.356, 43.14, 14.0
2019-02-15 11:16:30,158 : samples : 128000
2019-02-15 11:16:39,493 : Image to text: 13.8, 35.84, 49.28, 11.0
2019-02-15 11:16:45,855 : Text to Image: 11.3, 30.76, 43.408, 14.0
2019-02-15 11:17:24,015 : samples : 192000
2019-02-15 11:17:33,332 : Image to text: 13.98, 36.18, 49.34, 11.0
2019-02-15 11:17:39,672 : Text to Image: 11.304, 30.784, 43.752, 14.0
2019-02-15 11:18:17,811 : samples : 256000
2019-02-15 11:18:27,109 : Image to text: 13.06, 34.16, 47.58, 12.0
2019-02-15 11:18:33,495 : Text to Image: 11.164, 30.368, 43.204, 14.0
2019-02-15 11:19:11,648 : samples : 320000
2019-02-15 11:19:20,972 : Image to text: 13.7, 35.46, 48.84, 11.0
2019-02-15 11:19:27,313 : Text to Image: 11.424, 30.596, 43.292, 14.0
2019-02-15 11:20:05,476 : samples : 384000
2019-02-15 11:20:14,808 : Image to text: 13.62, 35.5, 49.46, 11.0
2019-02-15 11:20:21,196 : Text to Image: 11.408, 30.788, 43.424, 14.0
2019-02-15 11:20:59,583 : samples : 448000
2019-02-15 11:21:08,925 : Image to text: 13.8, 35.62, 50.06, 10.0
2019-02-15 11:21:15,295 : Text to Image: 11.436, 31.236, 44.06, 14.0
2019-02-15 11:21:53,648 : samples : 512000
2019-02-15 11:22:02,968 : Image to text: 14.52, 36.64, 49.42, 11.0
2019-02-15 11:22:09,315 : Text to Image: 11.292, 30.936, 43.832, 14.0
2019-02-15 11:22:41,841 : Epoch 7 finished
2019-02-15 11:22:42,200 : Image to text: 34.8, 67.7, 81.4, 3.0
2019-02-15 11:22:42,461 : Text to Image: 26.54, 62.48, 78.74, 4.0
2019-02-15 11:22:42,820 : Image to text: 31.6, 64.5, 78.2, 3.0
2019-02-15 11:22:43,081 : Text to Image: 25.7, 60.86, 77.56, 4.0
2019-02-15 11:22:43,440 : Image to text: 33.0, 67.4, 81.7, 3.0
2019-02-15 11:22:43,702 : Text to Image: 27.48, 62.5, 78.64, 3.0
2019-02-15 11:22:44,060 : Image to text: 34.7, 66.4, 80.2, 3.0
2019-02-15 11:22:44,322 : Text to Image: 27.76, 62.9, 78.56, 4.0
2019-02-15 11:22:44,680 : Image to text: 32.9, 67.5, 80.8, 3.0
2019-02-15 11:22:44,948 : Text to Image: 28.18, 62.34, 77.42, 4.0
2019-02-15 11:22:44,948 : Dev mean Text to Image: 27.132, 62.215999999999994, 78.184, 3.8
2019-02-15 11:22:44,948 : Dev mean Image to text: 33.4, 66.7, 80.46000000000001, 3.0
2019-02-15 11:22:44,948 : start epoch
2019-02-15 11:23:23,192 : samples : 64000
2019-02-15 11:23:32,548 : Image to text: 13.82, 37.0, 49.9, 11.0
2019-02-15 11:23:38,888 : Text to Image: 11.032, 30.896, 43.604, 14.0
2019-02-15 11:24:17,067 : samples : 128000
2019-02-15 11:24:26,385 : Image to text: 14.14, 35.46, 49.3, 11.0
2019-02-15 11:24:32,753 : Text to Image: 11.252, 30.96, 43.732, 14.0
2019-02-15 11:25:10,876 : samples : 192000
2019-02-15 11:25:20,263 : Image to text: 13.46, 35.18, 48.64, 11.0
2019-02-15 11:25:26,601 : Text to Image: 11.224, 30.96, 44.092, 14.0
2019-02-15 11:26:04,827 : samples : 256000
2019-02-15 11:26:14,133 : Image to text: 13.82, 35.96, 49.94, 11.0
2019-02-15 11:26:20,496 : Text to Image: 11.144, 30.948, 43.94, 14.0
2019-02-15 11:26:58,673 : samples : 320000
2019-02-15 11:27:08,019 : Image to text: 14.72, 36.72, 50.38, 10.0
2019-02-15 11:27:14,361 : Text to Image: 11.64, 31.432, 44.38, 14.0
2019-02-15 11:27:52,507 : samples : 384000
2019-02-15 11:28:01,855 : Image to text: 12.88, 34.82, 48.62, 11.0
2019-02-15 11:28:08,198 : Text to Image: 11.336, 30.808, 43.692, 14.0
2019-02-15 11:28:46,393 : samples : 448000
2019-02-15 11:28:55,704 : Image to text: 14.18, 36.32, 50.48, 10.0
2019-02-15 11:29:02,140 : Text to Image: 11.428, 31.228, 43.984, 14.0
2019-02-15 11:29:40,353 : samples : 512000
2019-02-15 11:29:49,814 : Image to text: 13.8, 35.42, 49.18, 11.0
2019-02-15 11:29:56,157 : Text to Image: 11.396, 31.144, 44.148, 14.0
2019-02-15 11:30:28,626 : Epoch 8 finished
2019-02-15 11:30:28,985 : Image to text: 34.4, 67.6, 79.8, 3.0
2019-02-15 11:30:29,247 : Text to Image: 27.3, 62.88, 78.16, 3.0
2019-02-15 11:30:29,606 : Image to text: 31.4, 66.1, 80.3, 3.0
2019-02-15 11:30:29,872 : Text to Image: 25.88, 61.04, 78.06, 4.0
2019-02-15 11:30:30,243 : Image to text: 35.1, 65.9, 80.4, 3.0
2019-02-15 11:30:30,514 : Text to Image: 27.7, 62.58, 78.44, 3.0
2019-02-15 11:30:30,874 : Image to text: 34.5, 68.9, 81.0, 3.0
2019-02-15 11:30:31,137 : Text to Image: 27.92, 62.9, 79.34, 3.0
2019-02-15 11:30:31,497 : Image to text: 36.0, 66.6, 79.3, 3.0
2019-02-15 11:30:31,760 : Text to Image: 27.96, 62.14, 77.82, 4.0
2019-02-15 11:30:31,760 : Dev mean Text to Image: 27.351999999999997, 62.30799999999999, 78.364, 3.4000000000000004
2019-02-15 11:30:31,760 : Dev mean Image to text: 34.28, 67.02, 80.16, 3.0
2019-02-15 11:30:31,760 : start epoch
2019-02-15 11:31:09,871 : samples : 64000
2019-02-15 11:31:19,217 : Image to text: 13.8, 35.8, 50.18, 10.0
2019-02-15 11:31:25,557 : Text to Image: 11.232, 31.076, 43.844, 14.0
2019-02-15 11:32:03,675 : samples : 128000
2019-02-15 11:32:12,999 : Image to text: 14.34, 36.46, 49.44, 11.0
2019-02-15 11:32:19,383 : Text to Image: 11.236, 30.804, 44.24, 14.0
2019-02-15 11:32:57,449 : samples : 192000
2019-02-15 11:33:06,792 : Image to text: 14.32, 36.02, 50.16, 10.0
2019-02-15 11:33:13,145 : Text to Image: 11.36, 31.124, 43.948, 14.0
2019-02-15 11:33:51,387 : samples : 256000
2019-02-15 11:34:00,742 : Image to text: 14.4, 36.6, 50.82, 10.0
2019-02-15 11:34:07,092 : Text to Image: 11.704, 31.196, 44.192, 14.0
2019-02-15 11:34:45,592 : samples : 320000
2019-02-15 11:34:54,950 : Image to text: 14.74, 35.66, 49.98, 11.0
2019-02-15 11:35:01,355 : Text to Image: 11.588, 31.644, 44.332, 13.0
2019-02-15 11:35:39,878 : samples : 384000
2019-02-15 11:35:49,233 : Image to text: 14.2, 35.7, 48.74, 11.0
2019-02-15 11:35:55,582 : Text to Image: 11.436, 31.392, 44.144, 14.0
2019-02-15 11:36:34,173 : samples : 448000
2019-02-15 11:36:43,496 : Image to text: 13.82, 35.98, 49.98, 11.0
2019-02-15 11:36:49,962 : Text to Image: 11.204, 30.488, 43.524, 14.0
2019-02-15 11:37:28,489 : samples : 512000
2019-02-15 11:37:37,808 : Image to text: 14.62, 37.12, 50.72, 10.0
2019-02-15 11:37:44,159 : Text to Image: 11.612, 31.564, 44.616, 13.0
2019-02-15 11:38:16,992 : Epoch 9 finished
2019-02-15 11:38:17,351 : Image to text: 33.7, 68.3, 81.5, 3.0
2019-02-15 11:38:17,613 : Text to Image: 26.98, 63.18, 78.52, 3.0
2019-02-15 11:38:17,971 : Image to text: 33.2, 66.1, 79.5, 3.0
2019-02-15 11:38:18,233 : Text to Image: 26.32, 60.88, 77.96, 4.0
2019-02-15 11:38:18,592 : Image to text: 34.9, 66.7, 80.7, 3.0
2019-02-15 11:38:18,854 : Text to Image: 27.76, 62.94, 79.02, 3.0
2019-02-15 11:38:19,212 : Image to text: 33.7, 67.3, 81.6, 3.0
2019-02-15 11:38:19,474 : Text to Image: 28.48, 63.04, 79.22, 3.0
2019-02-15 11:38:19,833 : Image to text: 33.6, 67.6, 79.8, 3.0
2019-02-15 11:38:20,094 : Text to Image: 28.64, 62.44, 77.72, 3.0
2019-02-15 11:38:20,094 : Dev mean Text to Image: 27.636000000000003, 62.495999999999995, 78.488, 3.2
2019-02-15 11:38:20,094 : Dev mean Image to text: 33.82, 67.2, 80.61999999999999, 3.0
2019-02-15 11:38:20,095 : start epoch
2019-02-15 11:38:58,564 : samples : 64000
2019-02-15 11:39:08,024 : Image to text: 14.2, 36.24, 49.84, 11.0
2019-02-15 11:39:14,370 : Text to Image: 11.488, 31.132, 44.336, 14.0
2019-02-15 11:39:52,884 : samples : 128000
2019-02-15 11:40:02,247 : Image to text: 14.74, 37.16, 50.98, 10.0
2019-02-15 11:40:08,604 : Text to Image: 11.856, 31.96, 44.896, 13.0
2019-02-15 11:40:47,127 : samples : 192000
2019-02-15 11:40:56,476 : Image to text: 14.94, 37.02, 50.4, 10.0
2019-02-15 11:41:02,855 : Text to Image: 11.9, 31.632, 44.692, 13.0
2019-02-15 11:41:41,380 : samples : 256000
2019-02-15 11:41:50,720 : Image to text: 13.98, 36.74, 50.26, 10.0
2019-02-15 11:41:57,069 : Text to Image: 11.308, 31.24, 44.416, 14.0
2019-02-15 11:42:35,569 : samples : 320000
2019-02-15 11:42:44,905 : Image to text: 14.7, 37.62, 50.92, 10.0
2019-02-15 11:42:51,267 : Text to Image: 11.656, 31.604, 44.76, 13.0
2019-02-15 11:43:29,746 : samples : 384000
2019-02-15 11:43:39,152 : Image to text: 14.9, 36.38, 50.38, 10.0
2019-02-15 11:43:45,522 : Text to Image: 11.5, 31.572, 44.52, 13.0
2019-02-15 11:44:24,013 : samples : 448000
2019-02-15 11:44:33,373 : Image to text: 14.86, 36.62, 50.68, 10.0
2019-02-15 11:44:39,713 : Text to Image: 11.792, 31.664, 44.712, 13.0
2019-02-15 11:45:18,251 : samples : 512000
2019-02-15 11:45:27,564 : Image to text: 14.5, 35.82, 48.7, 11.0
2019-02-15 11:45:33,942 : Text to Image: 11.604, 31.36, 44.612, 13.0
2019-02-15 11:46:06,663 : Epoch 10 finished
2019-02-15 11:46:07,023 : Image to text: 34.5, 67.7, 82.4, 3.0
2019-02-15 11:46:07,286 : Text to Image: 27.98, 63.18, 79.6, 3.0
2019-02-15 11:46:07,646 : Image to text: 31.8, 67.8, 81.1, 3.0
2019-02-15 11:46:07,908 : Text to Image: 26.08, 61.76, 78.04, 4.0
2019-02-15 11:46:08,267 : Image to text: 34.7, 67.8, 81.6, 3.0
2019-02-15 11:46:08,529 : Text to Image: 28.42, 63.42, 79.42, 3.0
2019-02-15 11:46:08,890 : Image to text: 35.1, 67.8, 80.6, 3.0
2019-02-15 11:46:09,153 : Text to Image: 28.02, 63.32, 79.36, 3.0
2019-02-15 11:46:09,512 : Image to text: 33.5, 68.0, 80.8, 3.0
2019-02-15 11:46:09,774 : Text to Image: 28.0, 63.46, 78.22, 3.0
2019-02-15 11:46:09,774 : Dev mean Text to Image: 27.699999999999996, 63.028, 78.928, 3.2
2019-02-15 11:46:09,774 : Dev mean Image to text: 33.92, 67.82, 81.3, 3.0
2019-02-15 11:46:09,775 : start epoch
2019-02-15 11:46:48,322 : samples : 64000
2019-02-15 11:46:57,681 : Image to text: 14.2, 36.32, 49.8, 11.0
2019-02-15 11:47:04,061 : Text to Image: 11.424, 31.24, 44.496, 13.0
2019-02-15 11:47:42,360 : samples : 128000
2019-02-15 11:47:51,740 : Image to text: 14.38, 35.82, 49.6, 11.0
2019-02-15 11:47:58,086 : Text to Image: 11.416, 31.368, 44.496, 13.0
2019-02-15 11:48:36,273 : samples : 192000
2019-02-15 11:48:45,627 : Image to text: 14.66, 36.62, 49.92, 11.0
2019-02-15 11:48:51,969 : Text to Image: 11.592, 31.688, 44.76, 13.0
2019-02-15 11:49:30,173 : samples : 256000
2019-02-15 11:49:39,579 : Image to text: 14.72, 37.86, 50.82, 10.0
2019-02-15 11:49:45,949 : Text to Image: 11.476, 31.156, 44.572, 13.0
2019-02-15 11:50:24,234 : samples : 320000
2019-02-15 11:50:33,594 : Image to text: 14.54, 36.74, 51.26, 10.0
2019-02-15 11:50:39,933 : Text to Image: 11.492, 31.56, 44.54, 14.0
2019-02-15 11:51:18,163 : samples : 384000
2019-02-15 11:51:27,493 : Image to text: 14.72, 37.44, 51.0, 10.0
2019-02-15 11:51:33,914 : Text to Image: 11.6, 32.308, 45.4, 13.0
2019-02-15 11:52:12,145 : samples : 448000
2019-02-15 11:52:21,501 : Image to text: 14.24, 37.08, 50.14, 10.0
2019-02-15 11:52:27,860 : Text to Image: 11.816, 31.8, 44.724, 13.0
2019-02-15 11:53:06,162 : samples : 512000
2019-02-15 11:53:15,528 : Image to text: 15.62, 37.42, 51.02, 10.0
2019-02-15 11:53:21,870 : Text to Image: 12.056, 32.148, 45.044, 13.0
2019-02-15 11:53:54,401 : Epoch 11 finished
2019-02-15 11:53:54,760 : Image to text: 36.0, 67.9, 81.2, 3.0
2019-02-15 11:53:55,021 : Text to Image: 27.62, 63.74, 78.88, 3.0
2019-02-15 11:53:55,381 : Image to text: 31.7, 66.5, 80.6, 3.0
2019-02-15 11:53:55,646 : Text to Image: 25.88, 61.68, 78.32, 4.0
2019-02-15 11:53:56,007 : Image to text: 35.9, 68.2, 81.7, 3.0
2019-02-15 11:53:56,269 : Text to Image: 28.66, 64.24, 79.18, 3.0
2019-02-15 11:53:56,627 : Image to text: 36.1, 67.8, 81.7, 3.0
2019-02-15 11:53:56,888 : Text to Image: 27.92, 64.52, 79.76, 3.0
2019-02-15 11:53:57,248 : Image to text: 34.4, 67.7, 81.4, 3.0
2019-02-15 11:53:57,509 : Text to Image: 28.4, 62.6, 78.12, 3.0
2019-02-15 11:53:57,509 : Dev mean Text to Image: 27.695999999999998, 63.355999999999995, 78.85199999999999, 3.2
2019-02-15 11:53:57,509 : Dev mean Image to text: 34.82, 67.62, 81.32000000000001, 3.0
2019-02-15 11:53:57,509 : start epoch
2019-02-15 11:54:35,645 : samples : 64000
2019-02-15 11:54:44,986 : Image to text: 14.62, 37.36, 51.28, 10.0
2019-02-15 11:54:51,342 : Text to Image: 11.976, 32.164, 45.012, 13.0
2019-02-15 11:55:29,310 : samples : 128000
2019-02-15 11:55:38,670 : Image to text: 14.64, 36.5, 50.98, 10.0
2019-02-15 11:55:45,029 : Text to Image: 11.76, 31.744, 44.9, 13.0
2019-02-15 11:56:22,961 : samples : 192000
2019-02-15 11:56:32,350 : Image to text: 15.06, 37.44, 51.16, 10.0
2019-02-15 11:56:38,746 : Text to Image: 11.996, 31.908, 45.288, 13.0
2019-02-15 11:57:16,718 : samples : 256000
2019-02-15 11:57:26,058 : Image to text: 15.62, 37.4, 50.4, 10.0
2019-02-15 11:57:32,422 : Text to Image: 11.716, 31.692, 44.712, 13.0
2019-02-15 11:58:10,333 : samples : 320000
2019-02-15 11:58:19,718 : Image to text: 14.64, 36.54, 50.02, 10.0
2019-02-15 11:58:26,072 : Text to Image: 11.616, 31.524, 44.38, 14.0
2019-02-15 11:59:04,084 : samples : 384000
2019-02-15 11:59:13,420 : Image to text: 14.26, 36.56, 49.7, 11.0
2019-02-15 11:59:19,840 : Text to Image: 11.936, 32.124, 45.036, 13.0
2019-02-15 11:59:57,851 : samples : 448000
2019-02-15 12:00:07,231 : Image to text: 15.1, 37.22, 50.98, 10.0
2019-02-15 12:00:13,582 : Text to Image: 11.948, 32.144, 44.952, 13.0
2019-02-15 12:00:51,573 : samples : 512000
2019-02-15 12:01:00,964 : Image to text: 14.62, 37.82, 51.66, 10.0
2019-02-15 12:01:07,328 : Text to Image: 11.896, 31.92, 44.968, 13.0
2019-02-15 12:01:39,645 : Epoch 12 finished
2019-02-15 12:01:40,005 : Image to text: 34.5, 67.4, 81.4, 3.0
2019-02-15 12:01:40,266 : Text to Image: 27.64, 63.38, 79.26, 3.0
2019-02-15 12:01:40,629 : Image to text: 33.4, 68.2, 80.7, 3.0
2019-02-15 12:01:40,891 : Text to Image: 26.12, 62.12, 78.74, 4.0
2019-02-15 12:01:41,250 : Image to text: 35.1, 68.2, 82.7, 3.0
2019-02-15 12:01:41,512 : Text to Image: 28.34, 64.66, 80.14, 3.0
2019-02-15 12:01:41,871 : Image to text: 36.0, 68.7, 80.9, 3.0
2019-02-15 12:01:42,133 : Text to Image: 28.44, 63.68, 80.34, 3.0
2019-02-15 12:01:42,492 : Image to text: 33.8, 67.2, 80.9, 3.0
2019-02-15 12:01:42,754 : Text to Image: 28.36, 63.02, 78.88, 3.0
2019-02-15 12:01:42,754 : Dev mean Text to Image: 27.780000000000005, 63.372, 79.472, 3.2
2019-02-15 12:01:42,754 : Dev mean Image to text: 34.56, 67.94000000000001, 81.32, 3.0
2019-02-15 12:01:42,754 : start epoch
2019-02-15 12:02:20,752 : samples : 64000
2019-02-15 12:02:30,106 : Image to text: 15.2, 37.68, 51.54, 10.0
2019-02-15 12:02:36,457 : Text to Image: 12.036, 32.164, 45.368, 13.0
2019-02-15 12:03:14,420 : samples : 128000
2019-02-15 12:03:23,766 : Image to text: 14.48, 37.66, 50.8, 10.0
2019-02-15 12:03:30,132 : Text to Image: 11.888, 31.584, 44.476, 13.0
2019-02-15 12:04:08,114 : samples : 192000
2019-02-15 12:04:17,462 : Image to text: 14.68, 36.84, 51.08, 10.0
2019-02-15 12:04:23,812 : Text to Image: 11.64, 31.664, 44.98, 13.0
2019-02-15 12:05:01,786 : samples : 256000
2019-02-15 12:05:11,130 : Image to text: 15.06, 38.14, 52.32, 9.0
2019-02-15 12:05:17,518 : Text to Image: 11.824, 32.272, 45.2, 13.0
2019-02-15 12:05:55,413 : samples : 320000
2019-02-15 12:06:04,785 : Image to text: 14.38, 38.06, 51.6, 10.0
2019-02-15 12:06:11,140 : Text to Image: 12.1, 32.42, 45.516, 13.0
2019-02-15 12:06:49,061 : samples : 384000
2019-02-15 12:06:58,400 : Image to text: 14.46, 37.1, 51.18, 10.0
2019-02-15 12:07:04,920 : Text to Image: 11.704, 32.0, 45.3, 13.0
2019-02-15 12:07:42,839 : samples : 448000
2019-02-15 12:07:52,216 : Image to text: 15.12, 37.1, 51.02, 10.0
2019-02-15 12:07:58,566 : Text to Image: 11.924, 32.06, 45.016, 13.0
2019-02-15 12:08:36,604 : samples : 512000
2019-02-15 12:08:45,953 : Image to text: 14.74, 37.5, 50.9, 10.0
2019-02-15 12:08:52,296 : Text to Image: 11.8, 31.808, 44.88, 13.0
2019-02-15 12:09:24,783 : Epoch 13 finished
2019-02-15 12:09:25,142 : Image to text: 35.3, 69.9, 82.7, 3.0
2019-02-15 12:09:25,404 : Text to Image: 28.36, 64.22, 79.96, 3.0
2019-02-15 12:09:25,768 : Image to text: 33.1, 66.7, 81.1, 3.0
2019-02-15 12:09:26,032 : Text to Image: 26.58, 62.5, 78.58, 4.0
2019-02-15 12:09:26,390 : Image to text: 35.8, 69.4, 82.4, 3.0
2019-02-15 12:09:26,652 : Text to Image: 29.32, 64.76, 80.0, 3.0
2019-02-15 12:09:27,010 : Image to text: 35.4, 69.6, 81.2, 2.0
2019-02-15 12:09:27,272 : Text to Image: 29.04, 64.66, 79.94, 3.0
2019-02-15 12:09:27,631 : Image to text: 34.5, 70.0, 82.2, 3.0
2019-02-15 12:09:27,892 : Text to Image: 28.32, 63.42, 78.62, 3.0
2019-02-15 12:09:27,893 : Dev mean Text to Image: 28.323999999999998, 63.912000000000006, 79.42, 3.2
2019-02-15 12:09:27,893 : Dev mean Image to text: 34.82, 69.12, 81.91999999999999, 2.8
2019-02-15 12:09:27,893 : start epoch
2019-02-15 12:10:06,104 : samples : 64000
2019-02-15 12:10:15,467 : Image to text: 15.68, 38.02, 51.64, 10.0
2019-02-15 12:10:21,814 : Text to Image: 11.908, 32.072, 45.264, 13.0
2019-02-15 12:10:59,987 : samples : 128000
2019-02-15 12:11:09,364 : Image to text: 14.68, 37.18, 50.62, 10.0
2019-02-15 12:11:15,738 : Text to Image: 11.564, 31.58, 44.72, 13.0
2019-02-15 12:11:53,938 : samples : 192000
2019-02-15 12:12:03,278 : Image to text: 14.8, 37.58, 51.02, 10.0
2019-02-15 12:12:09,620 : Text to Image: 11.84, 31.592, 44.676, 13.0
2019-02-15 12:12:47,831 : samples : 256000
2019-02-15 12:12:57,172 : Image to text: 14.92, 38.04, 51.72, 10.0
2019-02-15 12:13:03,573 : Text to Image: 12.236, 32.464, 45.704, 13.0
2019-02-15 12:13:41,748 : samples : 320000
2019-02-15 12:13:51,131 : Image to text: 14.76, 36.9, 50.9, 10.0
2019-02-15 12:13:57,487 : Text to Image: 11.904, 31.752, 44.828, 13.0
2019-02-15 12:14:35,681 : samples : 384000
2019-02-15 12:14:44,991 : Image to text: 14.78, 38.12, 51.68, 10.0
2019-02-15 12:14:51,343 : Text to Image: 12.008, 32.352, 45.22, 13.0
2019-02-15 12:15:29,576 : samples : 448000
2019-02-15 12:15:38,928 : Image to text: 14.06, 36.68, 50.78, 10.0
2019-02-15 12:15:45,298 : Text to Image: 11.716, 31.804, 45.08, 13.0
2019-02-15 12:16:23,593 : samples : 512000
2019-02-15 12:16:32,959 : Image to text: 15.02, 38.24, 51.9, 10.0
2019-02-15 12:16:39,307 : Text to Image: 12.308, 32.636, 45.796, 13.0
2019-02-15 12:17:11,832 : Epoch 14 finished
2019-02-15 12:17:12,190 : Image to text: 35.6, 69.5, 81.6, 3.0
2019-02-15 12:17:12,452 : Text to Image: 27.72, 63.48, 79.5, 3.0
2019-02-15 12:17:12,810 : Image to text: 31.8, 66.7, 81.2, 3.0
2019-02-15 12:17:13,072 : Text to Image: 26.86, 62.06, 78.78, 4.0
2019-02-15 12:17:13,431 : Image to text: 35.5, 67.7, 80.8, 3.0
2019-02-15 12:17:13,693 : Text to Image: 28.74, 64.82, 79.74, 3.0
2019-02-15 12:17:14,052 : Image to text: 35.0, 66.2, 80.4, 3.0
2019-02-15 12:17:14,315 : Text to Image: 28.32, 63.92, 79.94, 3.0
2019-02-15 12:17:14,674 : Image to text: 33.7, 67.2, 80.7, 3.0
2019-02-15 12:17:14,943 : Text to Image: 28.16, 63.0, 78.7, 3.0
2019-02-15 12:17:14,943 : Dev mean Text to Image: 27.96, 63.456, 79.332, 3.2
2019-02-15 12:17:14,943 : Dev mean Image to text: 34.32, 67.46000000000001, 80.94, 3.0
2019-02-15 12:17:14,943 : start epoch
2019-02-15 12:17:53,155 : samples : 64000
2019-02-15 12:18:02,573 : Image to text: 15.7, 36.84, 51.16, 10.0
2019-02-15 12:18:08,946 : Text to Image: 12.08, 32.372, 45.448, 13.0
2019-02-15 12:18:47,154 : samples : 128000
2019-02-15 12:18:56,487 : Image to text: 14.74, 37.06, 50.44, 10.0
2019-02-15 12:19:02,848 : Text to Image: 11.812, 32.028, 45.284, 13.0
2019-02-15 12:19:41,131 : samples : 192000
2019-02-15 12:19:50,512 : Image to text: 14.98, 37.58, 50.92, 10.0
2019-02-15 12:19:56,869 : Text to Image: 11.992, 32.34, 45.44, 13.0
2019-02-15 12:20:35,052 : samples : 256000
2019-02-15 12:20:44,390 : Image to text: 14.92, 37.7, 51.08, 10.0
2019-02-15 12:20:50,757 : Text to Image: 12.248, 32.364, 45.62, 13.0
2019-02-15 12:21:28,954 : samples : 320000
2019-02-15 12:21:38,323 : Image to text: 15.18, 37.8, 50.94, 10.0
2019-02-15 12:21:44,679 : Text to Image: 12.192, 32.704, 45.632, 13.0
2019-02-15 12:22:22,899 : samples : 384000
2019-02-15 12:22:32,294 : Image to text: 15.48, 37.96, 51.36, 10.0
2019-02-15 12:22:38,637 : Text to Image: 11.996, 32.456, 45.472, 13.0
2019-02-15 12:23:16,831 : samples : 448000
2019-02-15 12:23:26,188 : Image to text: 15.12, 37.86, 51.16, 10.0
2019-02-15 12:23:32,616 : Text to Image: 12.228, 32.284, 45.724, 13.0
2019-02-15 12:24:10,829 : samples : 512000
2019-02-15 12:24:20,160 : Image to text: 14.7, 37.12, 50.98, 10.0
2019-02-15 12:24:26,511 : Text to Image: 12.06, 32.276, 45.276, 13.0
2019-02-15 12:24:59,025 : Epoch 15 finished
2019-02-15 12:24:59,384 : Image to text: 35.7, 69.2, 83.2, 3.0
2019-02-15 12:24:59,647 : Text to Image: 28.44, 64.4, 79.28, 3.0
2019-02-15 12:25:00,016 : Image to text: 33.0, 68.3, 81.4, 3.0
2019-02-15 12:25:00,287 : Text to Image: 26.14, 62.74, 78.92, 4.0
2019-02-15 12:25:00,646 : Image to text: 36.7, 68.2, 82.9, 3.0
2019-02-15 12:25:00,908 : Text to Image: 28.98, 64.06, 79.86, 3.0
2019-02-15 12:25:01,270 : Image to text: 37.4, 68.8, 81.6, 2.0
2019-02-15 12:25:01,539 : Text to Image: 28.48, 64.32, 79.94, 3.0
2019-02-15 12:25:01,905 : Image to text: 35.5, 69.0, 81.0, 3.0
2019-02-15 12:25:02,172 : Text to Image: 28.8, 62.42, 78.64, 3.0
2019-02-15 12:25:02,172 : Dev mean Text to Image: 28.168, 63.588, 79.328, 3.2
2019-02-15 12:25:02,172 : Dev mean Image to text: 35.660000000000004, 68.7, 82.02, 2.8
2019-02-15 12:25:02,172 : start epoch
2019-02-15 12:25:40,337 : samples : 64000
2019-02-15 12:25:49,717 : Image to text: 14.7, 37.0, 51.4, 10.0
2019-02-15 12:25:56,065 : Text to Image: 11.764, 32.184, 45.32, 13.0
2019-02-15 12:26:34,248 : samples : 128000
2019-02-15 12:26:43,575 : Image to text: 14.38, 36.32, 50.82, 10.0
2019-02-15 12:26:49,947 : Text to Image: 11.684, 32.256, 45.28, 13.0
2019-02-15 12:27:28,172 : samples : 192000
2019-02-15 12:27:37,567 : Image to text: 14.96, 37.6, 51.68, 10.0
2019-02-15 12:27:43,910 : Text to Image: 12.052, 32.552, 45.76, 13.0
2019-02-15 12:28:22,103 : samples : 256000
2019-02-15 12:28:31,482 : Image to text: 14.96, 38.2, 51.0, 10.0
2019-02-15 12:28:37,846 : Text to Image: 11.768, 32.568, 45.84, 13.0
2019-02-15 12:29:16,092 : samples : 320000
2019-02-15 12:29:25,420 : Image to text: 15.54, 37.96, 51.44, 10.0
2019-02-15 12:29:31,788 : Text to Image: 11.936, 32.26, 45.864, 13.0
2019-02-15 12:30:10,014 : samples : 384000
2019-02-15 12:30:19,359 : Image to text: 14.98, 37.2, 50.74, 10.0
2019-02-15 12:30:25,706 : Text to Image: 11.74, 31.928, 44.92, 13.0
2019-02-15 12:31:03,984 : samples : 448000
2019-02-15 12:31:13,360 : Image to text: 15.54, 38.5, 51.46, 10.0
2019-02-15 12:31:19,734 : Text to Image: 12.108, 32.456, 45.788, 13.0
2019-02-15 12:31:58,454 : samples : 512000
2019-02-15 12:32:07,821 : Image to text: 15.26, 38.96, 51.92, 9.0
2019-02-15 12:32:14,197 : Text to Image: 12.168, 32.552, 45.652, 13.0
2019-02-15 12:32:47,088 : Epoch 16 finished
2019-02-15 12:32:47,450 : Image to text: 36.1, 69.4, 83.2, 3.0
2019-02-15 12:32:47,713 : Text to Image: 27.54, 64.0, 79.6, 3.0
2019-02-15 12:32:48,073 : Image to text: 32.8, 67.3, 82.2, 3.0
2019-02-15 12:32:48,335 : Text to Image: 26.78, 62.68, 79.26, 3.0
2019-02-15 12:32:48,694 : Image to text: 36.9, 70.8, 83.3, 2.0
2019-02-15 12:32:48,956 : Text to Image: 29.06, 64.1, 79.16, 3.0
2019-02-15 12:32:49,316 : Image to text: 36.6, 69.3, 83.1, 2.0
2019-02-15 12:32:49,578 : Text to Image: 28.86, 63.62, 80.4, 3.0
2019-02-15 12:32:49,937 : Image to text: 35.0, 69.1, 81.9, 3.0
2019-02-15 12:32:50,199 : Text to Image: 29.04, 63.18, 78.7, 3.0
2019-02-15 12:32:50,199 : Dev mean Text to Image: 28.256, 63.51599999999999, 79.42399999999999, 3.0
2019-02-15 12:32:50,200 : Dev mean Image to text: 35.480000000000004, 69.17999999999999, 82.73999999999998, 2.6
2019-02-15 12:32:50,200 : start epoch
2019-02-15 12:33:28,815 : samples : 64000
2019-02-15 12:33:38,192 : Image to text: 15.22, 37.94, 51.44, 10.0
2019-02-15 12:33:44,544 : Text to Image: 12.168, 32.66, 45.76, 13.0
2019-02-15 12:34:23,343 : samples : 128000
2019-02-15 12:34:32,900 : Image to text: 15.22, 38.0, 52.08, 9.0
2019-02-15 12:34:39,248 : Text to Image: 12.096, 32.544, 45.824, 13.0
2019-02-15 12:35:17,896 : samples : 192000
2019-02-15 12:35:27,222 : Image to text: 14.92, 37.54, 51.1, 10.0
2019-02-15 12:35:33,603 : Text to Image: 12.116, 32.236, 45.74, 13.0
2019-02-15 12:36:12,190 : samples : 256000
2019-02-15 12:36:21,590 : Image to text: 14.62, 37.16, 51.54, 10.0
2019-02-15 12:36:27,945 : Text to Image: 12.132, 32.3, 45.092, 13.0
2019-02-15 12:37:06,604 : samples : 320000
2019-02-15 12:37:15,986 : Image to text: 14.62, 38.04, 52.0, 10.0
2019-02-15 12:37:22,345 : Text to Image: 11.996, 32.224, 45.336, 13.0
2019-02-15 12:38:00,952 : samples : 384000
2019-02-15 12:38:10,289 : Image to text: 14.76, 38.38, 52.5, 9.0
2019-02-15 12:38:16,682 : Text to Image: 12.148, 32.512, 45.86, 13.0
2019-02-15 12:38:55,294 : samples : 448000
2019-02-15 12:39:04,652 : Image to text: 15.1, 37.46, 50.96, 10.0
2019-02-15 12:39:11,004 : Text to Image: 12.216, 32.864, 46.052, 13.0
2019-02-15 12:39:49,661 : samples : 512000
2019-02-15 12:39:59,004 : Image to text: 14.66, 37.4, 50.82, 10.0
2019-02-15 12:40:05,444 : Text to Image: 11.896, 32.448, 45.524, 13.0
2019-02-15 12:40:38,308 : Epoch 17 finished
2019-02-15 12:40:38,668 : Image to text: 34.5, 68.7, 83.3, 3.0
2019-02-15 12:40:38,930 : Text to Image: 28.38, 64.3, 80.14, 3.0
2019-02-15 12:40:39,289 : Image to text: 32.0, 67.8, 80.7, 3.0
2019-02-15 12:40:39,551 : Text to Image: 26.48, 62.8, 79.2, 3.0
2019-02-15 12:40:39,910 : Image to text: 36.1, 68.2, 82.7, 3.0
2019-02-15 12:40:40,172 : Text to Image: 29.26, 64.66, 80.04, 3.0
2019-02-15 12:40:40,532 : Image to text: 36.3, 70.1, 81.8, 3.0
2019-02-15 12:40:40,796 : Text to Image: 29.5, 64.12, 79.88, 3.0
2019-02-15 12:40:41,155 : Image to text: 33.5, 68.1, 81.3, 3.0
2019-02-15 12:40:41,417 : Text to Image: 28.84, 63.52, 78.56, 3.0
2019-02-15 12:40:41,417 : Dev mean Text to Image: 28.492000000000004, 63.88, 79.56400000000001, 3.0
2019-02-15 12:40:41,417 : Dev mean Image to text: 34.480000000000004, 68.58, 81.95999999999998, 3.0
2019-02-15 12:40:44,520 : 
Test scores | Image to text:             34.46, 69.14, 82.19999999999999, 2.8000000000000003
2019-02-15 12:40:44,520 : Test scores | Text to image:             28.356, 63.236000000000004, 79.21600000000001, 3.1999999999999997

2019-02-15 12:40:44,658 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 12:40:44,865 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 12:40:45,483 : loading BERT model bert-base-uncased
2019-02-15 12:40:45,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:40:45,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:40:45,514 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpop5ex847
2019-02-15 12:40:47,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:40:49,171 : Computing embeddings for train/dev/test
2019-02-15 12:42:07,860 : Computed embeddings
2019-02-15 12:42:07,860 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:43:10,577 : [('reg:1e-05', 78.63), ('reg:0.0001', 75.42), ('reg:0.001', 70.39), ('reg:0.01', 55.2)]
2019-02-15 12:43:10,577 : Validation : best param found is reg = 1e-05 with score             78.63
2019-02-15 12:43:10,577 : Evaluating...
2019-02-15 12:43:28,988 : 
Dev acc : 78.6 Test acc : 79.2 for LENGTH classification

2019-02-15 12:43:28,989 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 12:43:29,344 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 12:43:29,390 : loading BERT model bert-base-uncased
2019-02-15 12:43:29,390 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:43:29,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:43:29,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvtfk0pkk
2019-02-15 12:43:31,736 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:43:33,078 : Computing embeddings for train/dev/test
2019-02-15 12:44:46,687 : Computed embeddings
2019-02-15 12:44:46,687 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:45:46,674 : [('reg:1e-05', 53.21), ('reg:0.0001', 31.31), ('reg:0.001', 4.76), ('reg:0.01', 1.09)]
2019-02-15 12:45:46,674 : Validation : best param found is reg = 1e-05 with score             53.21
2019-02-15 12:45:46,674 : Evaluating...
2019-02-15 12:46:02,472 : 
Dev acc : 53.2 Test acc : 52.9 for WORDCONTENT classification

2019-02-15 12:46:02,473 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 12:46:02,782 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 12:46:02,851 : loading BERT model bert-base-uncased
2019-02-15 12:46:02,851 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:46:02,875 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:46:02,875 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdr6raley
2019-02-15 12:46:05,199 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:46:06,537 : Computing embeddings for train/dev/test
2019-02-15 12:47:15,621 : Computed embeddings
2019-02-15 12:47:15,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:47:54,193 : [('reg:1e-05', 34.0), ('reg:0.0001', 33.98), ('reg:0.001', 33.37), ('reg:0.01', 28.79)]
2019-02-15 12:47:54,194 : Validation : best param found is reg = 1e-05 with score             34.0
2019-02-15 12:47:54,194 : Evaluating...
2019-02-15 12:48:03,826 : 
Dev acc : 34.0 Test acc : 33.0 for DEPTH classification

2019-02-15 12:48:03,828 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 12:48:04,203 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 12:48:04,265 : loading BERT model bert-base-uncased
2019-02-15 12:48:04,265 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:48:04,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:48:04,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbvfxqx79
2019-02-15 12:48:06,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:48:07,925 : Computing embeddings for train/dev/test
2019-02-15 12:49:12,756 : Computed embeddings
2019-02-15 12:49:12,756 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:50:05,244 : [('reg:1e-05', 70.47), ('reg:0.0001', 70.25), ('reg:0.001', 63.7), ('reg:0.01', 52.97)]
2019-02-15 12:50:05,244 : Validation : best param found is reg = 1e-05 with score             70.47
2019-02-15 12:50:05,244 : Evaluating...
2019-02-15 12:50:15,208 : 
Dev acc : 70.5 Test acc : 70.0 for TOPCONSTITUENTS classification

2019-02-15 12:50:15,210 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 12:50:15,560 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 12:50:15,625 : loading BERT model bert-base-uncased
2019-02-15 12:50:15,625 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:50:15,653 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:50:15,653 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpolywjr4a
2019-02-15 12:50:18,044 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:50:19,393 : Computing embeddings for train/dev/test
2019-02-15 12:51:29,285 : Computed embeddings
2019-02-15 12:51:29,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:52:25,604 : [('reg:1e-05', 89.26), ('reg:0.0001', 89.39), ('reg:0.001', 89.29), ('reg:0.01', 88.06)]
2019-02-15 12:52:25,604 : Validation : best param found is reg = 0.0001 with score             89.39
2019-02-15 12:52:25,604 : Evaluating...
2019-02-15 12:52:40,319 : 
Dev acc : 89.4 Test acc : 88.8 for BIGRAMSHIFT classification

2019-02-15 12:52:40,321 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 12:52:40,701 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 12:52:40,765 : loading BERT model bert-base-uncased
2019-02-15 12:52:40,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:52:40,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:52:40,873 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk9jxw6ck
2019-02-15 12:52:43,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:52:44,511 : Computing embeddings for train/dev/test
2019-02-15 12:53:52,861 : Computed embeddings
2019-02-15 12:53:52,861 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:54:39,661 : [('reg:1e-05', 89.78), ('reg:0.0001', 89.99), ('reg:0.001', 90.34), ('reg:0.01', 90.07)]
2019-02-15 12:54:39,662 : Validation : best param found is reg = 0.001 with score             90.34
2019-02-15 12:54:39,662 : Evaluating...
2019-02-15 12:54:48,566 : 
Dev acc : 90.3 Test acc : 89.3 for TENSE classification

2019-02-15 12:54:48,568 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 12:54:48,951 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 12:54:49,012 : loading BERT model bert-base-uncased
2019-02-15 12:54:49,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:54:49,121 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:54:49,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa92zfhak
2019-02-15 12:54:51,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:54:52,743 : Computing embeddings for train/dev/test
2019-02-15 12:56:05,384 : Computed embeddings
2019-02-15 12:56:05,384 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:56:53,242 : [('reg:1e-05', 84.7), ('reg:0.0001', 84.69), ('reg:0.001', 84.71), ('reg:0.01', 82.95)]
2019-02-15 12:56:53,242 : Validation : best param found is reg = 0.001 with score             84.71
2019-02-15 12:56:53,242 : Evaluating...
2019-02-15 12:57:04,189 : 
Dev acc : 84.7 Test acc : 84.7 for SUBJNUMBER classification

2019-02-15 12:57:04,190 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 12:57:04,752 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 12:57:04,817 : loading BERT model bert-base-uncased
2019-02-15 12:57:04,817 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:57:04,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:57:04,843 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy5m7z5fi
2019-02-15 12:57:07,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:57:08,482 : Computing embeddings for train/dev/test
2019-02-15 12:58:19,801 : Computed embeddings
2019-02-15 12:58:19,801 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 12:59:02,570 : [('reg:1e-05', 80.05), ('reg:0.0001', 79.85), ('reg:0.001', 80.1), ('reg:0.01', 79.24)]
2019-02-15 12:59:02,570 : Validation : best param found is reg = 0.001 with score             80.1
2019-02-15 12:59:02,570 : Evaluating...
2019-02-15 12:59:13,825 : 
Dev acc : 80.1 Test acc : 80.8 for OBJNUMBER classification

2019-02-15 12:59:13,826 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 12:59:14,223 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 12:59:14,291 : loading BERT model bert-base-uncased
2019-02-15 12:59:14,291 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 12:59:14,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 12:59:14,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprdoy01nv
2019-02-15 12:59:16,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 12:59:18,026 : Computing embeddings for train/dev/test
2019-02-15 13:00:39,969 : Computed embeddings
2019-02-15 13:00:39,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:01:31,236 : [('reg:1e-05', 65.14), ('reg:0.0001', 65.12), ('reg:0.001', 65.1), ('reg:0.01', 64.23)]
2019-02-15 13:01:31,237 : Validation : best param found is reg = 1e-05 with score             65.14
2019-02-15 13:01:31,237 : Evaluating...
2019-02-15 13:01:36,792 : 
Dev acc : 65.1 Test acc : 65.1 for ODDMANOUT classification

2019-02-15 13:01:36,793 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 13:01:37,205 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 13:01:37,281 : loading BERT model bert-base-uncased
2019-02-15 13:01:37,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:01:37,310 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:01:37,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvwhixnw6
2019-02-15 13:01:39,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:01:41,041 : Computing embeddings for train/dev/test
2019-02-15 13:03:02,479 : Computed embeddings
2019-02-15 13:03:02,479 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:03:48,107 : [('reg:1e-05', 69.41), ('reg:0.0001', 69.37), ('reg:0.001', 68.91), ('reg:0.01', 66.93)]
2019-02-15 13:03:48,107 : Validation : best param found is reg = 1e-05 with score             69.41
2019-02-15 13:03:48,107 : Evaluating...
2019-02-15 13:04:03,320 : 
Dev acc : 69.4 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-15 13:04:03,322 : total results: {'STS12': {'MSRpar': {'pearson': (0.3727095667593454, 3.9596299931259383e-26), 'spearman': SpearmanrResult(correlation=0.41876266143179286, pvalue=3.331594344982482e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4332361253636729, 1.15051849264387e-35), 'spearman': SpearmanrResult(correlation=0.45630312680945373, pvalue=7.639038079993273e-40), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5116008319491429, 5.580004510886832e-32), 'spearman': SpearmanrResult(correlation=0.6016258362864673, pvalue=1.5830708708838964e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5699934004347451, 7.820989789068989e-66), 'spearman': SpearmanrResult(correlation=0.5779932288617731, pvalue=4.5978848132958986e-68), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5530105138953818, 2.4503858273253974e-33), 'spearman': SpearmanrResult(correlation=0.5570492843584547, pvalue=6.7229177023961166e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48811008768045766, 'wmean': 0.4785811764244647}, 'spearman': {'mean': 0.5223468275495884, 'wmean': 0.5110048861460028}}}, 'STS13': {'FNWN': {'pearson': (0.32177085147682594, 6.33378417332863e-06), 'spearman': SpearmanrResult(correlation=0.3989381468768162, pvalue=1.300883157892006e-08), 'nsamples': 189}, 'headlines': {'pearson': (0.6145097960792294, 4.483049894079435e-79), 'spearman': SpearmanrResult(correlation=0.5847331997850223, pvalue=5.4294827068766644e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.5643187054722828, 1.7238118664274123e-48), 'spearman': SpearmanrResult(correlation=0.5790412431574151, pvalue=1.547884460804139e-51), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5001997843427793, 'wmean': 0.5588532211723286}, 'spearman': {'mean': 0.520904196606418, 'wmean': 0.5591942313398632}}}, 'STS14': {'deft-forum': {'pearson': (0.3093400814837979, 1.9583031906093414e-11), 'spearman': SpearmanrResult(correlation=0.30243175972397235, pvalue=5.686661845212124e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.7618896879419697, 3.735220035716604e-58), 'spearman': SpearmanrResult(correlation=0.7240513284811, pvalue=5.404936503661116e-50), 'nsamples': 300}, 'headlines': {'pearson': (0.5810527986442682, 6.208964613620758e-69), 'spearman': SpearmanrResult(correlation=0.5296322025963927, pvalue=1.847553501231274e-55), 'nsamples': 750}, 'images': {'pearson': (0.4475822620251421, 3.1648585612655135e-38), 'spearman': SpearmanrResult(correlation=0.4436376708236111, pvalue=1.6467927012255007e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.6964211639659849, 6.862688561044599e-110), 'spearman': SpearmanrResult(correlation=0.7189876175740287, pvalue=2.64796373502888e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6446591974386136, 2.5606856648023767e-89), 'spearman': SpearmanrResult(correlation=0.5693918808146343, pvalue=1.1442643118026033e-65), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5734908652499627, 'wmean': 0.5720150692282151}, 'spearman': {'mean': 0.5480220766689565, 'wmean': 0.5465457918070981}}}, 'STS15': {'answers-forums': {'pearson': (0.5766060474551941, 1.3057866187078612e-34), 'spearman': SpearmanrResult(correlation=0.5892628940732605, pvalue=1.9770403071093515e-36), 'nsamples': 375}, 'answers-students': {'pearson': (0.5470171728937847, 9.359394986207948e-60), 'spearman': SpearmanrResult(correlation=0.573504508317169, pvalue=8.351507493699605e-67), 'nsamples': 750}, 'belief': {'pearson': (0.6770424013428654, 1.3089468271549008e-51), 'spearman': SpearmanrResult(correlation=0.7064090963233781, pvalue=6.0685676584761764e-58), 'nsamples': 375}, 'headlines': {'pearson': (0.6407857340589851, 6.150937849564486e-88), 'spearman': SpearmanrResult(correlation=0.6345383917516699, pvalue=9.42164922708702e-86), 'nsamples': 750}, 'images': {'pearson': (0.634463377125025, 1.0001373075395758e-85), 'spearman': SpearmanrResult(correlation=0.6435299597822182, pvalue=6.499903808466876e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6151829465751708, 'wmean': 0.612272627119206}, 'spearman': {'mean': 0.6294489700495391, 'wmean': 0.624852213762344}}}, 'STS16': {'answer-answer': {'pearson': (0.5357751252758719, 2.8362120679404524e-20), 'spearman': SpearmanrResult(correlation=0.5214389997915273, pvalue=4.1239509444388844e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6445526917826722, 1.2477173691309513e-30), 'spearman': SpearmanrResult(correlation=0.6518797553957988, pvalue=1.6313889009854508e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7591483129615296, 2.1122113712595788e-44), 'spearman': SpearmanrResult(correlation=0.7712375019184672, pvalue=1.2807283570216581e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8020770615916597, 4.2559296082647035e-56), 'spearman': SpearmanrResult(correlation=0.8328135441479744, pvalue=4.29435254489274e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.47985988747192027, 1.9584524104617907e-13), 'spearman': SpearmanrResult(correlation=0.47269092886229475, pvalue=4.95690806614973e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6442826158167307, 'wmean': 0.6468652053668676}, 'spearman': {'mean': 0.6500121460232124, 'wmean': 0.6527378578298269}}}, 'MR': {'devacc': 80.99, 'acc': 80.2, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.08, 'acc': 85.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.81, 'acc': 88.22, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.52, 'acc': 95.14, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.44, 'acc': 85.34, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.87, 'acc': 46.11, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 84.21, 'acc': 91.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.72, 'acc': 73.33, 'f1': 80.57, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.8, 'acc': 77.23, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8057024050424405, 'pearson': 0.8129063299277852, 'spearman': 0.7428656341923232, 'mse': 0.34691890003171844, 'yhat': array([3.21954153, 3.83916256, 1.49827067, ..., 3.39341568, 4.67290041,        4.79184747]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7190162624647348, 'pearson': 0.6504388803995269, 'spearman': 0.64292642411981, 'mse': 1.48381559769896, 'yhat': array([1.62349783, 1.19161571, 2.82833047, ..., 3.81651634, 3.41112515,        3.51477407]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.7, 'acc': 65.78, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 358.596, 'acc': [(34.46, 69.14, 82.19999999999999, 2.8000000000000003), (28.356, 63.236000000000004, 79.21600000000001, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 78.63, 'acc': 79.16, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 53.21, 'acc': 52.92, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.0, 'acc': 33.04, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.47, 'acc': 70.01, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.39, 'acc': 88.76, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.34, 'acc': 89.29, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.71, 'acc': 84.71, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.1, 'acc': 80.82, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.14, 'acc': 65.07, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.41, 'acc': 69.37, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 13:04:03,322 : STS12 p=0.4786, STS12 s=0.5110, STS13 p=0.5589, STS13 s=0.5592, STS14 p=0.5720, STS14 s=0.5465, STS15 p=0.6123, STS15 s=0.6249, STS 16 p=0.6469, STS16 s=0.6527, STS B p=0.6504, STS B s=0.6429, STS B m=1.4838, SICK-R p=0.8129, SICK-R s=0.7429, SICK-P m=0.3469
2019-02-15 13:04:03,322 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 13:04:03,322 : 0.4786,0.5110,0.5589,0.5592,0.5720,0.5465,0.6123,0.6249,0.6469,0.6527,0.6504,0.6429,1.4838,0.8129,0.7429,0.3469
2019-02-15 13:04:03,322 : MR=80.20, CR=85.38, SUBJ=95.14, MPQA=88.22, SST-B=85.34, SST-F=46.11, TREC=91.60, SICK-E=77.23, SNLI=65.78, MRPC=73.33, MRPC f=80.57
2019-02-15 13:04:03,322 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 13:04:03,323 : 80.20,85.38,95.14,88.22,85.34,46.11,91.60,77.23,65.78,73.33,80.57
2019-02-15 13:04:03,323 : COCO r1i2t=34.46, COCO r5i2t=69.14, COCO r10i2t=82.20, COCO medr_i2t=2.80, COCO r1t2i=28.36, COCO r5t2i=63.24, COCO r10t2i=79.22, COCO medr_t2i=3.20
2019-02-15 13:04:03,323 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 13:04:03,323 : 34.46,69.14,82.20,2.80,28.36,63.24,79.22,3.20
2019-02-15 13:04:03,323 : SentLen=79.16, WC=52.92, TreeDepth=33.04, TopConst=70.01, BShift=88.76, Tense=89.29, SubjNum=84.71, ObjNum=80.82, SOMO=65.07, CoordInv=69.37, average=71.31
2019-02-15 13:04:03,323 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 13:04:03,323 : 79.16,52.92,33.04,70.01,88.76,89.29,84.71,80.82,65.07,69.37,71.31
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,323 : layer 12
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,323 : ********************************************************************************
2019-02-15 13:04:03,414 : ***** Transfer task : STS12 *****


2019-02-15 13:04:03,426 : loading BERT model bert-base-uncased
2019-02-15 13:04:03,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:03,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:03,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmi6suxgz
2019-02-15 13:04:05,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:08,671 : MSRpar : pearson = 0.3508, spearman = 0.4008
2019-02-15 13:04:09,316 : MSRvid : pearson = 0.3575, spearman = 0.3877
2019-02-15 13:04:09,867 : SMTeuroparl : pearson = 0.4790, spearman = 0.5790
2019-02-15 13:04:10,856 : surprise.OnWN : pearson = 0.5792, spearman = 0.5796
2019-02-15 13:04:11,408 : surprise.SMTnews : pearson = 0.5833, spearman = 0.5418
2019-02-15 13:04:11,408 : ALL (weighted average) : Pearson = 0.4563,             Spearman = 0.4852
2019-02-15 13:04:11,408 : ALL (average) : Pearson = 0.4700,             Spearman = 0.4978

2019-02-15 13:04:11,408 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 13:04:11,417 : loading BERT model bert-base-uncased
2019-02-15 13:04:11,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:11,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:11,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbol9_cbz
2019-02-15 13:04:13,712 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:15,568 : FNWN : pearson = 0.4203, spearman = 0.4471
2019-02-15 13:04:16,320 : headlines : pearson = 0.6471, spearman = 0.6215
2019-02-15 13:04:16,884 : OnWN : pearson = 0.5585, spearman = 0.5710
2019-02-15 13:04:16,884 : ALL (weighted average) : Pearson = 0.5854,             Spearman = 0.5806
2019-02-15 13:04:16,884 : ALL (average) : Pearson = 0.5420,             Spearman = 0.5465

2019-02-15 13:04:16,884 : ***** Transfer task : STS14 *****


2019-02-15 13:04:16,900 : loading BERT model bert-base-uncased
2019-02-15 13:04:16,900 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:16,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:16,917 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3gp5k_3b
2019-02-15 13:04:19,311 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:21,242 : deft-forum : pearson = 0.3352, spearman = 0.3277
2019-02-15 13:04:21,846 : deft-news : pearson = 0.7513, spearman = 0.7168
2019-02-15 13:04:22,679 : headlines : pearson = 0.5995, spearman = 0.5421
2019-02-15 13:04:23,481 : images : pearson = 0.4518, spearman = 0.4449
2019-02-15 13:04:24,291 : OnWN : pearson = 0.6977, spearman = 0.7181
2019-02-15 13:04:25,359 : tweet-news : pearson = 0.6614, spearman = 0.5933
2019-02-15 13:04:25,359 : ALL (weighted average) : Pearson = 0.5824,             Spearman = 0.5564
2019-02-15 13:04:25,359 : ALL (average) : Pearson = 0.5828,             Spearman = 0.5572

2019-02-15 13:04:25,359 : ***** Transfer task : STS15 *****


2019-02-15 13:04:25,391 : loading BERT model bert-base-uncased
2019-02-15 13:04:25,391 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:25,408 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:25,409 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphl2n9vpy
2019-02-15 13:04:27,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:29,788 : answers-forums : pearson = 0.6046, spearman = 0.6016
2019-02-15 13:04:30,597 : answers-students : pearson = 0.5642, spearman = 0.5680
2019-02-15 13:04:31,333 : belief : pearson = 0.7071, spearman = 0.7154
2019-02-15 13:04:32,191 : headlines : pearson = 0.6711, spearman = 0.6600
2019-02-15 13:04:33,010 : images : pearson = 0.6188, spearman = 0.6244
2019-02-15 13:04:33,011 : ALL (weighted average) : Pearson = 0.6275,             Spearman = 0.6277
2019-02-15 13:04:33,011 : ALL (average) : Pearson = 0.6332,             Spearman = 0.6339

2019-02-15 13:04:33,011 : ***** Transfer task : STS16 *****


2019-02-15 13:04:33,088 : loading BERT model bert-base-uncased
2019-02-15 13:04:33,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:33,105 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:33,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp780c271w
2019-02-15 13:04:35,388 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:37,067 : answer-answer : pearson = 0.5535, spearman = 0.5656
2019-02-15 13:04:37,330 : headlines : pearson = 0.6538, spearman = 0.6575
2019-02-15 13:04:37,665 : plagiarism : pearson = 0.7692, spearman = 0.7857
2019-02-15 13:04:38,196 : postediting : pearson = 0.8058, spearman = 0.8243
2019-02-15 13:04:38,438 : question-question : pearson = 0.5275, spearman = 0.5162
2019-02-15 13:04:38,438 : ALL (weighted average) : Pearson = 0.6637,             Spearman = 0.6721
2019-02-15 13:04:38,438 : ALL (average) : Pearson = 0.6619,             Spearman = 0.6699

2019-02-15 13:04:38,438 : ***** Transfer task : MR *****


2019-02-15 13:04:38,455 : loading BERT model bert-base-uncased
2019-02-15 13:04:38,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:04:38,473 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:04:38,473 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpihji5bn5
2019-02-15 13:04:40,801 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:04:42,177 : Generating sentence embeddings
2019-02-15 13:04:53,601 : Generated sentence embeddings
2019-02-15 13:04:53,601 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 13:05:05,178 : Best param found at split 1: l2reg = 0.001                 with score 80.14
2019-02-15 13:05:18,620 : Best param found at split 2: l2reg = 1e-05                 with score 80.47
2019-02-15 13:05:33,135 : Best param found at split 3: l2reg = 1e-05                 with score 80.6
2019-02-15 13:05:43,596 : Best param found at split 4: l2reg = 0.01                 with score 80.25
2019-02-15 13:05:55,625 : Best param found at split 5: l2reg = 0.001                 with score 80.88
2019-02-15 13:05:56,157 : Dev acc : 80.47 Test acc : 79.53

2019-02-15 13:05:56,158 : ***** Transfer task : CR *****


2019-02-15 13:05:56,166 : loading BERT model bert-base-uncased
2019-02-15 13:05:56,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:05:56,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:05:56,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprcozbi4c
2019-02-15 13:05:58,539 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:05:59,884 : Generating sentence embeddings
2019-02-15 13:06:03,046 : Generated sentence embeddings
2019-02-15 13:06:03,047 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 13:06:06,613 : Best param found at split 1: l2reg = 0.01                 with score 86.72
2019-02-15 13:06:09,852 : Best param found at split 2: l2reg = 0.0001                 with score 86.52
2019-02-15 13:06:13,565 : Best param found at split 3: l2reg = 0.001                 with score 87.05
2019-02-15 13:06:19,137 : Best param found at split 4: l2reg = 0.0001                 with score 86.96
2019-02-15 13:06:22,948 : Best param found at split 5: l2reg = 1e-05                 with score 86.89
2019-02-15 13:06:23,139 : Dev acc : 86.83 Test acc : 85.72

2019-02-15 13:06:23,139 : ***** Transfer task : MPQA *****


2019-02-15 13:06:23,146 : loading BERT model bert-base-uncased
2019-02-15 13:06:23,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:06:23,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:06:23,165 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgb5b5_6y
2019-02-15 13:06:25,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:06:26,873 : Generating sentence embeddings
2019-02-15 13:06:30,022 : Generated sentence embeddings
2019-02-15 13:06:30,022 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 13:06:41,917 : Best param found at split 1: l2reg = 0.0001                 with score 87.95
2019-02-15 13:06:53,584 : Best param found at split 2: l2reg = 0.0001                 with score 87.28
2019-02-15 13:07:05,053 : Best param found at split 3: l2reg = 0.001                 with score 86.92
2019-02-15 13:07:18,315 : Best param found at split 4: l2reg = 1e-05                 with score 87.98
2019-02-15 13:07:32,252 : Best param found at split 5: l2reg = 0.001                 with score 87.65
2019-02-15 13:07:32,769 : Dev acc : 87.56 Test acc : 87.63

2019-02-15 13:07:32,770 : ***** Transfer task : SUBJ *****


2019-02-15 13:07:32,785 : loading BERT model bert-base-uncased
2019-02-15 13:07:32,786 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:07:32,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:07:32,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0oyic8pz
2019-02-15 13:07:35,138 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:07:36,504 : Generating sentence embeddings
2019-02-15 13:07:47,767 : Generated sentence embeddings
2019-02-15 13:07:47,768 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 13:08:00,172 : Best param found at split 1: l2reg = 0.0001                 with score 95.38
2019-02-15 13:08:11,651 : Best param found at split 2: l2reg = 0.001                 with score 95.5
2019-02-15 13:08:24,240 : Best param found at split 3: l2reg = 1e-05                 with score 95.22
2019-02-15 13:08:35,860 : Best param found at split 4: l2reg = 0.0001                 with score 95.67
2019-02-15 13:08:46,724 : Best param found at split 5: l2reg = 1e-05                 with score 95.38
2019-02-15 13:08:47,967 : Dev acc : 95.43 Test acc : 95.05

2019-02-15 13:08:47,968 : ***** Transfer task : SST Binary classification *****


2019-02-15 13:08:48,109 : loading BERT model bert-base-uncased
2019-02-15 13:08:48,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:08:48,130 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:08:48,130 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp_c1bjem
2019-02-15 13:08:50,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:08:51,748 : Computing embedding for train
2019-02-15 13:09:29,419 : Computed train embeddings
2019-02-15 13:09:29,419 : Computing embedding for dev
2019-02-15 13:09:30,230 : Computed dev embeddings
2019-02-15 13:09:30,230 : Computing embedding for test
2019-02-15 13:09:31,906 : Computed test embeddings
2019-02-15 13:09:31,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:10:00,002 : [('reg:1e-05', 86.12), ('reg:0.0001', 86.24), ('reg:0.001', 86.12), ('reg:0.01', 84.17)]
2019-02-15 13:10:00,002 : Validation : best param found is reg = 0.0001 with score             86.24
2019-02-15 13:10:00,003 : Evaluating...
2019-02-15 13:10:06,630 : 
Dev acc : 86.24 Test acc : 85.06 for             SST Binary classification

2019-02-15 13:10:06,630 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 13:10:06,694 : loading BERT model bert-base-uncased
2019-02-15 13:10:06,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:10:06,718 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:10:06,718 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_x71dqle
2019-02-15 13:10:09,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:10:10,364 : Computing embedding for train
2019-02-15 13:10:18,377 : Computed train embeddings
2019-02-15 13:10:18,377 : Computing embedding for dev
2019-02-15 13:10:19,415 : Computed dev embeddings
2019-02-15 13:10:19,415 : Computing embedding for test
2019-02-15 13:10:21,476 : Computed test embeddings
2019-02-15 13:10:21,476 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:10:26,429 : [('reg:1e-05', 44.14), ('reg:0.0001', 44.23), ('reg:0.001', 44.23), ('reg:0.01', 44.87)]
2019-02-15 13:10:26,430 : Validation : best param found is reg = 0.01 with score             44.87
2019-02-15 13:10:26,430 : Evaluating...
2019-02-15 13:10:27,897 : 
Dev acc : 44.87 Test acc : 46.38 for             SST Fine-Grained classification

2019-02-15 13:10:27,898 : ***** Transfer task : TREC *****


2019-02-15 13:10:27,919 : loading BERT model bert-base-uncased
2019-02-15 13:10:27,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:10:27,943 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:10:27,943 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmjyv9n58
2019-02-15 13:10:30,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:10:34,499 : Computed train embeddings
2019-02-15 13:10:34,724 : Computed test embeddings
2019-02-15 13:10:34,724 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 13:10:40,573 : [('reg:1e-05', 80.37), ('reg:0.0001', 80.28), ('reg:0.001', 79.69), ('reg:0.01', 72.45)]
2019-02-15 13:10:40,573 : Cross-validation : best param found is reg = 1e-05             with score 80.37
2019-02-15 13:10:40,573 : Evaluating...
2019-02-15 13:10:40,967 : 
Dev acc : 80.37 Test acc : 89.8             for TREC

2019-02-15 13:10:40,967 : ***** Transfer task : MRPC *****


2019-02-15 13:10:40,989 : loading BERT model bert-base-uncased
2019-02-15 13:10:40,989 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:10:41,010 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:10:41,010 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp28w6ruj0
2019-02-15 13:10:43,304 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:10:44,663 : Computing embedding for train
2019-02-15 13:10:52,873 : Computed train embeddings
2019-02-15 13:10:52,873 : Computing embedding for test
2019-02-15 13:10:56,409 : Computed test embeddings
2019-02-15 13:10:56,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 13:11:01,990 : [('reg:1e-05', 73.38), ('reg:0.0001', 73.48), ('reg:0.001', 73.01), ('reg:0.01', 73.31)]
2019-02-15 13:11:01,990 : Cross-validation : best param found is reg = 0.0001             with score 73.48
2019-02-15 13:11:01,990 : Evaluating...
2019-02-15 13:11:02,253 : Dev acc : 73.48 Test acc 73.45; Test F1 80.79 for MRPC.

2019-02-15 13:11:02,254 : ***** Transfer task : SICK-Entailment*****


2019-02-15 13:11:02,315 : loading BERT model bert-base-uncased
2019-02-15 13:11:02,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:11:02,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:11:02,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe4n8crn5
2019-02-15 13:11:04,642 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:11:05,995 : Computing embedding for train
2019-02-15 13:11:10,356 : Computed train embeddings
2019-02-15 13:11:10,356 : Computing embedding for dev
2019-02-15 13:11:10,930 : Computed dev embeddings
2019-02-15 13:11:10,930 : Computing embedding for test
2019-02-15 13:11:15,604 : Computed test embeddings
2019-02-15 13:11:15,631 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:11:16,950 : [('reg:1e-05', 79.2), ('reg:0.0001', 79.2), ('reg:0.001', 78.8), ('reg:0.01', 76.2)]
2019-02-15 13:11:16,950 : Validation : best param found is reg = 1e-05 with score             79.2
2019-02-15 13:11:16,950 : Evaluating...
2019-02-15 13:11:17,330 : 
Dev acc : 79.2 Test acc : 77.15 for                        SICK entailment

2019-02-15 13:11:17,330 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 13:11:17,361 : loading BERT model bert-base-uncased
2019-02-15 13:11:17,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:11:17,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:11:17,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb7ufz6vt
2019-02-15 13:11:19,801 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:11:21,167 : Computing embedding for train
2019-02-15 13:11:25,532 : Computed train embeddings
2019-02-15 13:11:25,532 : Computing embedding for dev
2019-02-15 13:11:26,107 : Computed dev embeddings
2019-02-15 13:11:26,107 : Computing embedding for test
2019-02-15 13:11:30,788 : Computed test embeddings
2019-02-15 13:12:02,157 : Dev : Pearson 0.8171522572469139
2019-02-15 13:12:02,157 : Test : Pearson 0.8094042913214028 Spearman 0.7361093992751818 MSE 0.35301990357161456                        for SICK Relatedness

2019-02-15 13:12:02,159 : 

***** Transfer task : STSBenchmark*****


2019-02-15 13:12:02,206 : loading BERT model bert-base-uncased
2019-02-15 13:12:02,206 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:12:02,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:12:02,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_w99tr_p
2019-02-15 13:12:04,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:12:05,869 : Computing embedding for train
2019-02-15 13:12:12,867 : Computed train embeddings
2019-02-15 13:12:12,867 : Computing embedding for dev
2019-02-15 13:12:14,936 : Computed dev embeddings
2019-02-15 13:12:14,936 : Computing embedding for test
2019-02-15 13:12:16,618 : Computed test embeddings
2019-02-15 13:12:55,253 : Dev : Pearson 0.7140077083391813
2019-02-15 13:12:55,253 : Test : Pearson 0.6605640727130131 Spearman 0.651771579777153 MSE 1.4713919112985543                        for SICK Relatedness

2019-02-15 13:12:55,253 : ***** Transfer task : SNLI Entailment*****


2019-02-15 13:12:59,837 : loading BERT model bert-base-uncased
2019-02-15 13:12:59,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:12:59,954 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:12:59,954 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy3beyd4h
2019-02-15 13:13:02,268 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:13:03,778 : PROGRESS (encoding): 0.00%
2019-02-15 13:14:08,577 : PROGRESS (encoding): 14.56%
2019-02-15 13:15:21,030 : PROGRESS (encoding): 29.12%
2019-02-15 13:16:33,902 : PROGRESS (encoding): 43.69%
2019-02-15 13:17:53,015 : PROGRESS (encoding): 58.25%
2019-02-15 13:19:21,060 : PROGRESS (encoding): 72.81%
2019-02-15 13:20:48,705 : PROGRESS (encoding): 87.37%
2019-02-15 13:22:21,921 : PROGRESS (encoding): 0.00%
2019-02-15 13:22:33,437 : PROGRESS (encoding): 0.00%
2019-02-15 13:22:44,530 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 13:23:06,213 : [('reg:1e-09', 65.98)]
2019-02-15 13:23:06,213 : Validation : best param found is reg = 1e-09 with score             65.98
2019-02-15 13:23:06,213 : Evaluating...
2019-02-15 13:23:26,662 : Dev acc : 65.98 Test acc : 66.32 for SNLI

2019-02-15 13:23:26,662 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 13:23:35,113 : loading BERT model bert-base-uncased
2019-02-15 13:23:35,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 13:23:35,156 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 13:23:35,156 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8q1p75pu
2019-02-15 13:23:37,435 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 13:23:38,785 : Computing embedding for train
2019-02-15 13:29:57,130 : Computed train embeddings
2019-02-15 13:29:57,130 : Computing embedding for dev
2019-02-15 13:30:13,588 : Computed dev embeddings
2019-02-15 13:30:13,588 : Computing embedding for test
2019-02-15 13:30:30,554 : Computed test embeddings
2019-02-15 13:30:30,570 : prepare data
2019-02-15 13:30:30,631 : start epoch
2019-02-15 13:31:08,566 : samples : 64000
2019-02-15 13:31:17,870 : Image to text: 7.66, 23.5, 34.76, 21.0
2019-02-15 13:31:24,195 : Text to Image: 6.456, 19.76, 29.868, 27.0
2019-02-15 13:32:02,367 : samples : 128000
2019-02-15 13:32:11,662 : Image to text: 9.34, 25.7, 37.64, 19.0
2019-02-15 13:32:18,043 : Text to Image: 7.036, 21.868, 32.96, 23.0
2019-02-15 13:32:56,186 : samples : 192000
2019-02-15 13:33:05,498 : Image to text: 9.18, 26.34, 38.02, 19.0
2019-02-15 13:33:11,829 : Text to Image: 7.184, 21.908, 32.856, 23.0
2019-02-15 13:33:49,947 : samples : 256000
2019-02-15 13:33:59,229 : Image to text: 9.48, 26.48, 38.62, 17.0
2019-02-15 13:34:05,580 : Text to Image: 7.58, 22.94, 34.152, 22.0
2019-02-15 13:34:43,897 : samples : 320000
2019-02-15 13:34:53,212 : Image to text: 9.14, 26.18, 37.62, 19.0
2019-02-15 13:34:59,543 : Text to Image: 7.776, 23.272, 34.624, 21.0
2019-02-15 13:35:37,681 : samples : 384000
2019-02-15 13:35:47,031 : Image to text: 9.92, 27.36, 39.76, 17.0
2019-02-15 13:35:53,351 : Text to Image: 8.144, 24.228, 36.052, 20.0
2019-02-15 13:36:31,732 : samples : 448000
2019-02-15 13:36:41,042 : Image to text: 10.12, 28.44, 40.56, 16.0
2019-02-15 13:36:47,418 : Text to Image: 8.016, 23.768, 35.744, 20.0
2019-02-15 13:37:26,095 : samples : 512000
2019-02-15 13:37:35,391 : Image to text: 10.38, 28.82, 40.48, 16.0
2019-02-15 13:37:41,739 : Text to Image: 8.488, 24.944, 36.88, 19.0
2019-02-15 13:38:14,611 : Epoch 1 finished
2019-02-15 13:38:14,980 : Image to text: 27.7, 60.0, 74.5, 4.0
2019-02-15 13:38:15,253 : Text to Image: 22.44, 54.88, 72.52, 5.0
2019-02-15 13:38:15,615 : Image to text: 25.9, 56.6, 72.4, 4.0
2019-02-15 13:38:15,876 : Text to Image: 20.5, 53.92, 70.74, 5.0
2019-02-15 13:38:16,235 : Image to text: 26.0, 58.5, 74.3, 4.0
2019-02-15 13:38:16,496 : Text to Image: 21.5, 54.58, 71.56, 5.0
2019-02-15 13:38:16,855 : Image to text: 25.2, 59.2, 72.0, 4.0
2019-02-15 13:38:17,116 : Text to Image: 22.5, 54.44, 72.4, 5.0
2019-02-15 13:38:17,474 : Image to text: 26.5, 59.8, 75.0, 4.0
2019-02-15 13:38:17,734 : Text to Image: 22.18, 54.86, 71.42, 5.0
2019-02-15 13:38:17,734 : Dev mean Text to Image: 21.824, 54.536, 71.72800000000001, 5.0
2019-02-15 13:38:17,734 : Dev mean Image to text: 26.259999999999998, 58.82, 73.64, 4.0
2019-02-15 13:38:17,735 : start epoch
2019-02-15 13:38:56,324 : samples : 64000
2019-02-15 13:39:05,674 : Image to text: 10.12, 28.76, 41.12, 16.0
2019-02-15 13:39:12,014 : Text to Image: 8.5, 25.044, 37.096, 19.0
2019-02-15 13:39:50,659 : samples : 128000
2019-02-15 13:39:59,987 : Image to text: 9.98, 27.92, 40.48, 16.0
2019-02-15 13:40:06,446 : Text to Image: 8.416, 24.916, 36.852, 19.0
2019-02-15 13:40:44,603 : samples : 192000
2019-02-15 13:40:53,912 : Image to text: 11.0, 29.88, 42.14, 15.0
2019-02-15 13:41:00,288 : Text to Image: 9.14, 26.188, 38.068, 18.0
2019-02-15 13:41:38,557 : samples : 256000
2019-02-15 13:41:47,864 : Image to text: 11.38, 30.62, 42.18, 15.0
2019-02-15 13:41:54,195 : Text to Image: 9.224, 26.96, 38.712, 18.0
2019-02-15 13:42:32,402 : samples : 320000
2019-02-15 13:42:41,758 : Image to text: 11.44, 30.38, 42.38, 15.0
2019-02-15 13:42:48,245 : Text to Image: 9.0, 26.276, 38.58, 18.0
2019-02-15 13:43:26,396 : samples : 384000
2019-02-15 13:43:35,740 : Image to text: 10.98, 29.88, 42.36, 15.0
2019-02-15 13:43:42,074 : Text to Image: 8.688, 25.792, 37.636, 18.0
2019-02-15 13:44:20,188 : samples : 448000
2019-02-15 13:44:29,489 : Image to text: 11.08, 30.4, 43.18, 14.0
2019-02-15 13:44:35,860 : Text to Image: 8.924, 26.06, 38.36, 18.0
2019-02-15 13:45:13,903 : samples : 512000
2019-02-15 13:45:23,216 : Image to text: 11.2, 30.02, 42.58, 15.0
2019-02-15 13:45:29,553 : Text to Image: 9.2, 26.776, 38.944, 17.0
2019-02-15 13:46:01,836 : Epoch 2 finished
2019-02-15 13:46:02,198 : Image to text: 30.1, 62.3, 77.0, 3.0
2019-02-15 13:46:02,459 : Text to Image: 23.08, 57.36, 74.42, 4.0
2019-02-15 13:46:02,818 : Image to text: 26.9, 59.3, 74.3, 4.0
2019-02-15 13:46:03,079 : Text to Image: 22.2, 56.16, 73.26, 4.0
2019-02-15 13:46:03,437 : Image to text: 29.7, 60.3, 77.0, 4.0
2019-02-15 13:46:03,699 : Text to Image: 24.22, 57.68, 73.9, 4.0
2019-02-15 13:46:04,058 : Image to text: 30.4, 62.3, 75.2, 3.0
2019-02-15 13:46:04,318 : Text to Image: 23.4, 57.34, 74.34, 4.0
2019-02-15 13:46:04,676 : Image to text: 26.7, 63.8, 77.6, 3.0
2019-02-15 13:46:04,936 : Text to Image: 23.66, 57.18, 73.86, 4.0
2019-02-15 13:46:04,937 : Dev mean Text to Image: 23.311999999999998, 57.144, 73.956, 4.0
2019-02-15 13:46:04,937 : Dev mean Image to text: 28.76, 61.599999999999994, 76.22, 3.4000000000000004
2019-02-15 13:46:04,937 : start epoch
2019-02-15 13:46:42,763 : samples : 64000
2019-02-15 13:46:52,110 : Image to text: 11.32, 30.08, 42.48, 15.0
2019-02-15 13:46:58,447 : Text to Image: 8.944, 26.16, 38.44, 18.0
2019-02-15 13:47:36,429 : samples : 128000
2019-02-15 13:47:45,758 : Image to text: 11.24, 31.24, 44.18, 14.0
2019-02-15 13:47:52,097 : Text to Image: 9.244, 26.784, 39.164, 17.0
2019-02-15 13:48:30,308 : samples : 192000
2019-02-15 13:48:39,617 : Image to text: 12.44, 32.8, 45.28, 13.0
2019-02-15 13:48:45,979 : Text to Image: 9.716, 27.64, 39.696, 17.0
2019-02-15 13:49:24,149 : samples : 256000
2019-02-15 13:49:33,487 : Image to text: 12.94, 32.22, 45.74, 13.0
2019-02-15 13:49:39,822 : Text to Image: 10.108, 28.4, 40.736, 16.0
2019-02-15 13:50:17,987 : samples : 320000
2019-02-15 13:50:27,302 : Image to text: 11.56, 31.32, 43.66, 14.0
2019-02-15 13:50:33,660 : Text to Image: 9.24, 26.448, 38.64, 17.0
2019-02-15 13:51:11,835 : samples : 384000
2019-02-15 13:51:21,164 : Image to text: 12.04, 31.7, 44.68, 14.0
2019-02-15 13:51:27,504 : Text to Image: 9.424, 27.216, 39.456, 17.0
2019-02-15 13:52:05,679 : samples : 448000
2019-02-15 13:52:14,992 : Image to text: 12.08, 31.64, 44.34, 13.0
2019-02-15 13:52:21,358 : Text to Image: 9.804, 28.208, 40.736, 16.0
2019-02-15 13:52:59,538 : samples : 512000
2019-02-15 13:53:08,967 : Image to text: 12.62, 32.82, 45.94, 12.0
2019-02-15 13:53:15,337 : Text to Image: 9.676, 28.268, 40.7, 16.0
2019-02-15 13:53:47,858 : Epoch 3 finished
2019-02-15 13:53:48,219 : Image to text: 30.2, 63.9, 78.8, 3.0
2019-02-15 13:53:48,480 : Text to Image: 23.74, 59.52, 76.14, 4.0
2019-02-15 13:53:48,839 : Image to text: 29.3, 63.1, 75.9, 3.0
2019-02-15 13:53:49,100 : Text to Image: 23.48, 58.98, 75.5, 4.0
2019-02-15 13:53:49,459 : Image to text: 29.4, 64.3, 78.0, 3.0
2019-02-15 13:53:49,720 : Text to Image: 24.66, 59.08, 76.06, 4.0
2019-02-15 13:53:50,078 : Image to text: 32.3, 63.7, 79.8, 3.0
2019-02-15 13:53:50,339 : Text to Image: 25.86, 59.74, 76.5, 4.0
2019-02-15 13:53:50,698 : Image to text: 31.0, 63.9, 78.6, 3.0
2019-02-15 13:53:50,959 : Text to Image: 24.92, 59.48, 75.84, 4.0
2019-02-15 13:53:50,959 : Dev mean Text to Image: 24.531999999999996, 59.36, 76.00800000000001, 4.0
2019-02-15 13:53:50,959 : Dev mean Image to text: 30.44, 63.78, 78.22, 3.0
2019-02-15 13:53:50,959 : start epoch
2019-02-15 13:54:29,084 : samples : 64000
2019-02-15 13:54:38,430 : Image to text: 11.56, 32.04, 45.1, 13.0
2019-02-15 13:54:44,764 : Text to Image: 9.656, 27.908, 40.456, 16.0
2019-02-15 13:55:22,945 : samples : 128000
2019-02-15 13:55:32,270 : Image to text: 13.24, 33.14, 45.92, 13.0
2019-02-15 13:55:38,646 : Text to Image: 9.888, 28.104, 40.412, 16.0
2019-02-15 13:56:16,796 : samples : 192000
2019-02-15 13:56:26,118 : Image to text: 12.22, 32.66, 45.3, 13.0
2019-02-15 13:56:32,504 : Text to Image: 9.976, 28.036, 40.312, 16.0
2019-02-15 13:57:10,636 : samples : 256000
2019-02-15 13:57:19,950 : Image to text: 12.42, 32.66, 45.96, 13.0
2019-02-15 13:57:26,293 : Text to Image: 10.212, 28.872, 41.708, 15.0
2019-02-15 13:58:04,415 : samples : 320000
2019-02-15 13:58:13,734 : Image to text: 12.26, 33.04, 46.54, 12.0
2019-02-15 13:58:20,124 : Text to Image: 9.9, 28.796, 41.288, 16.0
2019-02-15 13:58:58,325 : samples : 384000
2019-02-15 13:59:07,662 : Image to text: 12.38, 32.26, 46.12, 13.0
2019-02-15 13:59:14,001 : Text to Image: 9.956, 28.484, 40.86, 16.0
2019-02-15 13:59:52,139 : samples : 448000
2019-02-15 14:00:01,501 : Image to text: 12.86, 34.06, 47.06, 12.0
2019-02-15 14:00:07,851 : Text to Image: 10.132, 28.628, 41.096, 16.0
2019-02-15 14:00:46,032 : samples : 512000
2019-02-15 14:00:55,317 : Image to text: 12.6, 33.04, 46.32, 12.0
2019-02-15 14:01:01,690 : Text to Image: 9.952, 28.428, 40.96, 16.0
2019-02-15 14:01:34,202 : Epoch 4 finished
2019-02-15 14:01:34,565 : Image to text: 33.1, 64.9, 80.2, 3.0
2019-02-15 14:01:34,828 : Text to Image: 25.78, 61.24, 76.16, 4.0
2019-02-15 14:01:35,187 : Image to text: 29.1, 63.5, 79.5, 3.0
2019-02-15 14:01:35,447 : Text to Image: 24.66, 58.72, 76.12, 4.0
2019-02-15 14:01:35,806 : Image to text: 32.3, 66.2, 80.0, 3.0
2019-02-15 14:01:36,067 : Text to Image: 25.84, 60.0, 76.18, 4.0
2019-02-15 14:01:36,425 : Image to text: 31.8, 66.6, 79.5, 3.0
2019-02-15 14:01:36,686 : Text to Image: 25.9, 59.8, 77.06, 4.0
2019-02-15 14:01:37,045 : Image to text: 32.7, 66.8, 79.8, 3.0
2019-02-15 14:01:37,305 : Text to Image: 25.94, 60.18, 75.92, 4.0
2019-02-15 14:01:37,305 : Dev mean Text to Image: 25.624000000000002, 59.98800000000001, 76.288, 4.0
2019-02-15 14:01:37,305 : Dev mean Image to text: 31.799999999999997, 65.6, 79.8, 3.0
2019-02-15 14:01:37,305 : start epoch
2019-02-15 14:02:15,421 : samples : 64000
2019-02-15 14:02:24,754 : Image to text: 13.42, 34.3, 47.5, 12.0
2019-02-15 14:02:31,142 : Text to Image: 10.5, 29.116, 42.048, 15.0
2019-02-15 14:03:09,280 : samples : 128000
2019-02-15 14:03:18,618 : Image to text: 12.58, 32.86, 45.0, 13.0
2019-02-15 14:03:24,950 : Text to Image: 10.08, 28.58, 40.932, 16.0
2019-02-15 14:04:03,057 : samples : 192000
2019-02-15 14:04:12,354 : Image to text: 13.12, 32.98, 46.36, 12.0
2019-02-15 14:04:18,708 : Text to Image: 10.14, 28.604, 41.432, 15.0
2019-02-15 14:04:56,811 : samples : 256000
2019-02-15 14:05:06,206 : Image to text: 12.58, 32.74, 46.66, 12.0
2019-02-15 14:05:12,543 : Text to Image: 10.3, 28.612, 41.496, 15.0
2019-02-15 14:05:50,773 : samples : 320000
2019-02-15 14:06:00,077 : Image to text: 13.04, 33.64, 46.92, 12.0
2019-02-15 14:06:06,416 : Text to Image: 10.672, 29.516, 42.328, 15.0
2019-02-15 14:06:44,496 : samples : 384000
2019-02-15 14:06:53,835 : Image to text: 11.84, 33.12, 46.58, 12.0
2019-02-15 14:07:00,193 : Text to Image: 10.264, 29.336, 41.948, 15.0
2019-02-15 14:07:38,282 : samples : 448000
2019-02-15 14:07:47,690 : Image to text: 12.6, 33.04, 46.76, 12.0
2019-02-15 14:07:54,020 : Text to Image: 9.908, 28.716, 41.288, 16.0
2019-02-15 14:08:32,146 : samples : 512000
2019-02-15 14:08:41,432 : Image to text: 12.8, 33.62, 46.46, 12.0
2019-02-15 14:08:47,797 : Text to Image: 10.668, 29.072, 41.792, 15.0
2019-02-15 14:09:20,271 : Epoch 5 finished
2019-02-15 14:09:20,631 : Image to text: 31.1, 65.3, 80.9, 3.0
2019-02-15 14:09:20,893 : Text to Image: 24.7, 60.04, 76.44, 4.0
2019-02-15 14:09:21,253 : Image to text: 31.0, 64.6, 77.8, 3.0
2019-02-15 14:09:21,514 : Text to Image: 23.32, 59.1, 75.8, 4.0
2019-02-15 14:09:21,874 : Image to text: 32.5, 65.5, 79.1, 3.0
2019-02-15 14:09:22,135 : Text to Image: 26.14, 59.46, 76.82, 4.0
2019-02-15 14:09:22,494 : Image to text: 32.5, 65.8, 79.6, 3.0
2019-02-15 14:09:22,755 : Text to Image: 24.9, 60.36, 76.6, 4.0
2019-02-15 14:09:23,114 : Image to text: 31.8, 66.7, 79.9, 3.0
2019-02-15 14:09:23,375 : Text to Image: 25.68, 60.54, 76.26, 4.0
2019-02-15 14:09:23,375 : Dev mean Text to Image: 24.947999999999997, 59.900000000000006, 76.384, 4.0
2019-02-15 14:09:23,375 : Dev mean Image to text: 31.78, 65.58, 79.46, 3.0
2019-02-15 14:09:23,375 : start epoch
2019-02-15 14:10:01,521 : samples : 64000
2019-02-15 14:10:10,832 : Image to text: 13.6, 34.54, 47.82, 11.0
2019-02-15 14:10:17,242 : Text to Image: 10.892, 30.176, 42.688, 15.0
2019-02-15 14:10:55,375 : samples : 128000
2019-02-15 14:11:04,723 : Image to text: 13.06, 33.68, 46.74, 12.0
2019-02-15 14:11:11,066 : Text to Image: 10.176, 28.992, 41.492, 15.0
2019-02-15 14:11:49,169 : samples : 192000
2019-02-15 14:11:58,480 : Image to text: 12.96, 34.08, 46.86, 12.0
2019-02-15 14:12:04,832 : Text to Image: 10.288, 29.396, 42.084, 15.0
2019-02-15 14:12:43,001 : samples : 256000
2019-02-15 14:12:52,314 : Image to text: 12.88, 33.56, 46.94, 12.0
2019-02-15 14:12:58,654 : Text to Image: 10.196, 29.076, 41.472, 15.0
2019-02-15 14:13:36,787 : samples : 320000
2019-02-15 14:13:46,133 : Image to text: 13.56, 34.08, 46.7, 12.0
2019-02-15 14:13:52,467 : Text to Image: 10.268, 29.284, 42.136, 15.0
2019-02-15 14:14:30,650 : samples : 384000
2019-02-15 14:14:39,975 : Image to text: 14.08, 34.68, 47.96, 11.0
2019-02-15 14:14:46,349 : Text to Image: 10.596, 29.732, 42.456, 15.0
2019-02-15 14:15:24,730 : samples : 448000
2019-02-15 14:15:34,072 : Image to text: 13.72, 34.7, 48.5, 11.0
2019-02-15 14:15:40,415 : Text to Image: 10.432, 29.7, 42.632, 15.0
2019-02-15 14:16:18,641 : samples : 512000
2019-02-15 14:16:27,974 : Image to text: 13.4, 34.16, 46.62, 12.0
2019-02-15 14:16:34,355 : Text to Image: 10.516, 29.536, 42.436, 15.0
2019-02-15 14:17:07,135 : Epoch 6 finished
2019-02-15 14:17:07,496 : Image to text: 31.7, 66.1, 80.4, 3.0
2019-02-15 14:17:07,759 : Text to Image: 25.14, 61.14, 77.42, 4.0
2019-02-15 14:17:08,119 : Image to text: 30.3, 64.6, 77.7, 3.0
2019-02-15 14:17:08,379 : Text to Image: 24.46, 60.36, 76.26, 4.0
2019-02-15 14:17:08,742 : Image to text: 32.1, 65.6, 79.5, 3.0
2019-02-15 14:17:09,006 : Text to Image: 26.34, 61.04, 77.18, 4.0
2019-02-15 14:17:09,368 : Image to text: 34.0, 67.7, 80.4, 3.0
2019-02-15 14:17:09,632 : Text to Image: 26.88, 61.56, 77.22, 4.0
2019-02-15 14:17:09,993 : Image to text: 32.9, 66.0, 79.0, 3.0
2019-02-15 14:17:10,257 : Text to Image: 26.58, 60.98, 76.5, 4.0
2019-02-15 14:17:10,257 : Dev mean Text to Image: 25.88, 61.01599999999999, 76.916, 4.0
2019-02-15 14:17:10,257 : Dev mean Image to text: 32.2, 66.0, 79.4, 3.0
2019-02-15 14:17:10,258 : start epoch
2019-02-15 14:17:48,532 : samples : 64000
2019-02-15 14:17:57,881 : Image to text: 13.64, 35.14, 48.54, 11.0
2019-02-15 14:18:04,239 : Text to Image: 10.752, 29.736, 42.5, 15.0
2019-02-15 14:18:42,762 : samples : 128000
2019-02-15 14:18:52,339 : Image to text: 14.28, 35.08, 48.54, 11.0
2019-02-15 14:18:58,858 : Text to Image: 11.108, 30.448, 43.164, 14.0
2019-02-15 14:19:38,508 : samples : 192000
2019-02-15 14:19:48,281 : Image to text: 13.82, 34.98, 48.34, 11.0
2019-02-15 14:19:55,016 : Text to Image: 10.928, 29.9, 42.804, 15.0
2019-02-15 14:20:34,961 : samples : 256000
2019-02-15 14:20:44,762 : Image to text: 12.88, 33.52, 46.54, 12.0
2019-02-15 14:20:51,484 : Text to Image: 10.64, 29.684, 42.288, 15.0
2019-02-15 14:21:31,507 : samples : 320000
2019-02-15 14:21:41,291 : Image to text: 13.14, 34.58, 47.94, 12.0
2019-02-15 14:21:48,039 : Text to Image: 10.82, 29.9, 42.752, 15.0
2019-02-15 14:22:28,241 : samples : 384000
2019-02-15 14:22:38,050 : Image to text: 13.52, 34.96, 48.24, 11.0
2019-02-15 14:22:44,724 : Text to Image: 11.024, 30.148, 43.072, 14.0
2019-02-15 14:23:24,743 : samples : 448000
2019-02-15 14:23:34,541 : Image to text: 13.72, 35.32, 49.08, 11.0
2019-02-15 14:23:41,280 : Text to Image: 11.1, 30.636, 43.408, 14.0
2019-02-15 14:24:21,242 : samples : 512000
2019-02-15 14:24:30,947 : Image to text: 13.96, 35.38, 48.68, 11.0
2019-02-15 14:24:37,584 : Text to Image: 11.064, 30.796, 43.292, 14.0
2019-02-15 14:25:11,752 : Epoch 7 finished
2019-02-15 14:25:12,129 : Image to text: 32.6, 66.8, 80.9, 3.0
2019-02-15 14:25:12,406 : Text to Image: 26.18, 62.02, 78.16, 4.0
2019-02-15 14:25:12,782 : Image to text: 31.7, 65.1, 79.5, 3.0
2019-02-15 14:25:13,058 : Text to Image: 25.96, 61.02, 77.16, 4.0
2019-02-15 14:25:13,435 : Image to text: 31.5, 65.3, 79.8, 3.0
2019-02-15 14:25:13,711 : Text to Image: 26.9, 62.5, 78.56, 4.0
2019-02-15 14:25:14,087 : Image to text: 32.2, 66.5, 80.3, 3.0
2019-02-15 14:25:14,363 : Text to Image: 27.44, 62.2, 78.62, 4.0
2019-02-15 14:25:14,740 : Image to text: 32.9, 65.4, 79.6, 3.0
2019-02-15 14:25:15,019 : Text to Image: 27.34, 61.62, 77.2, 4.0
2019-02-15 14:25:15,019 : Dev mean Text to Image: 26.764, 61.872, 77.94, 4.0
2019-02-15 14:25:15,019 : Dev mean Image to text: 32.18, 65.82, 80.02, 3.0
2019-02-15 14:25:15,019 : start epoch
2019-02-15 14:25:54,919 : samples : 64000
2019-02-15 14:26:04,633 : Image to text: 14.14, 35.14, 49.46, 11.0
2019-02-15 14:26:11,239 : Text to Image: 10.96, 30.556, 43.236, 14.0
2019-02-15 14:26:50,874 : samples : 128000
2019-02-15 14:27:00,512 : Image to text: 13.44, 34.64, 47.68, 12.0
2019-02-15 14:27:07,127 : Text to Image: 10.76, 30.38, 43.228, 14.0
2019-02-15 14:27:46,993 : samples : 192000
2019-02-15 14:27:56,681 : Image to text: 12.74, 34.42, 48.0, 12.0
2019-02-15 14:28:03,388 : Text to Image: 10.876, 30.36, 43.34, 14.0
2019-02-15 14:28:43,401 : samples : 256000
2019-02-15 14:28:53,139 : Image to text: 13.3, 34.96, 48.22, 11.0
2019-02-15 14:28:59,846 : Text to Image: 10.96, 30.516, 43.236, 14.0
2019-02-15 14:29:39,602 : samples : 320000
2019-02-15 14:29:49,265 : Image to text: 14.02, 35.7, 49.36, 11.0
2019-02-15 14:29:55,891 : Text to Image: 11.272, 30.756, 43.572, 14.0
2019-02-15 14:30:35,270 : samples : 384000
2019-02-15 14:30:44,933 : Image to text: 13.14, 34.78, 47.86, 11.0
2019-02-15 14:30:51,650 : Text to Image: 11.14, 30.24, 42.792, 14.0
2019-02-15 14:31:30,955 : samples : 448000
2019-02-15 14:31:40,633 : Image to text: 13.42, 35.36, 48.96, 11.0
2019-02-15 14:31:47,318 : Text to Image: 11.096, 30.28, 43.06, 14.0
2019-02-15 14:32:26,635 : samples : 512000
2019-02-15 14:32:36,277 : Image to text: 13.46, 35.44, 48.32, 11.0
2019-02-15 14:32:42,901 : Text to Image: 11.036, 30.556, 43.532, 14.0
2019-02-15 14:33:16,292 : Epoch 8 finished
2019-02-15 14:33:16,665 : Image to text: 34.7, 66.5, 79.5, 3.0
2019-02-15 14:33:16,938 : Text to Image: 27.14, 62.32, 78.4, 3.0
2019-02-15 14:33:17,311 : Image to text: 31.2, 66.4, 79.8, 3.0
2019-02-15 14:33:17,584 : Text to Image: 26.48, 61.18, 77.86, 4.0
2019-02-15 14:33:17,957 : Image to text: 31.3, 66.1, 81.9, 3.0
2019-02-15 14:33:18,230 : Text to Image: 26.76, 62.52, 78.26, 4.0
2019-02-15 14:33:18,603 : Image to text: 34.7, 68.4, 81.0, 3.0
2019-02-15 14:33:18,877 : Text to Image: 27.44, 61.9, 78.6, 4.0
2019-02-15 14:33:19,249 : Image to text: 33.4, 66.2, 79.8, 3.0
2019-02-15 14:33:19,522 : Text to Image: 27.28, 62.06, 77.56, 4.0
2019-02-15 14:33:19,522 : Dev mean Text to Image: 27.02, 61.996, 78.136, 3.8
2019-02-15 14:33:19,522 : Dev mean Image to text: 33.06, 66.72, 80.39999999999999, 3.0
2019-02-15 14:33:19,523 : start epoch
2019-02-15 14:33:58,702 : samples : 64000
2019-02-15 14:34:08,338 : Image to text: 13.76, 35.32, 48.92, 11.0
2019-02-15 14:34:14,966 : Text to Image: 10.856, 30.136, 42.832, 14.0
2019-02-15 14:34:54,181 : samples : 128000
2019-02-15 14:35:03,860 : Image to text: 14.06, 35.04, 48.48, 11.0
2019-02-15 14:35:10,490 : Text to Image: 10.784, 30.164, 43.196, 14.0
2019-02-15 14:35:49,680 : samples : 192000
2019-02-15 14:35:59,318 : Image to text: 14.04, 35.16, 49.0, 11.0
2019-02-15 14:36:05,955 : Text to Image: 11.056, 30.48, 43.228, 14.0
2019-02-15 14:36:45,119 : samples : 256000
2019-02-15 14:36:54,758 : Image to text: 13.82, 35.36, 48.64, 11.0
2019-02-15 14:37:01,407 : Text to Image: 11.26, 30.792, 43.836, 14.0
2019-02-15 14:37:40,593 : samples : 320000
2019-02-15 14:37:50,267 : Image to text: 13.92, 35.68, 48.78, 11.0
2019-02-15 14:37:56,892 : Text to Image: 11.192, 30.7, 44.036, 14.0
2019-02-15 14:38:36,104 : samples : 384000
2019-02-15 14:38:45,782 : Image to text: 14.06, 34.72, 48.46, 11.0
2019-02-15 14:38:52,400 : Text to Image: 11.26, 30.744, 43.536, 14.0
2019-02-15 14:39:31,613 : samples : 448000
2019-02-15 14:39:41,269 : Image to text: 13.84, 35.88, 48.64, 11.0
2019-02-15 14:39:47,897 : Text to Image: 10.956, 30.304, 43.332, 14.0
2019-02-15 14:40:27,102 : samples : 512000
2019-02-15 14:40:36,739 : Image to text: 14.44, 35.4, 49.22, 11.0
2019-02-15 14:40:43,357 : Text to Image: 11.24, 30.492, 43.56, 14.0
2019-02-15 14:41:16,753 : Epoch 9 finished
2019-02-15 14:41:17,129 : Image to text: 35.1, 69.4, 82.3, 3.0
2019-02-15 14:41:17,405 : Text to Image: 27.04, 62.64, 78.3, 4.0
2019-02-15 14:41:17,782 : Image to text: 31.1, 65.1, 79.9, 3.0
2019-02-15 14:41:18,057 : Text to Image: 25.68, 60.78, 77.86, 4.0
2019-02-15 14:41:18,434 : Image to text: 34.3, 67.1, 81.1, 3.0
2019-02-15 14:41:18,710 : Text to Image: 27.84, 62.52, 78.5, 3.0
2019-02-15 14:41:19,087 : Image to text: 33.6, 66.4, 80.9, 3.0
2019-02-15 14:41:19,364 : Text to Image: 27.3, 61.72, 78.58, 3.0
2019-02-15 14:41:19,740 : Image to text: 33.8, 66.5, 80.0, 3.0
2019-02-15 14:41:20,016 : Text to Image: 28.56, 62.16, 77.66, 3.0
2019-02-15 14:41:20,016 : Dev mean Text to Image: 27.284000000000002, 61.964, 78.18, 3.4000000000000004
2019-02-15 14:41:20,016 : Dev mean Image to text: 33.58, 66.89999999999999, 80.84, 3.0
2019-02-15 14:41:20,016 : start epoch
2019-02-15 14:41:59,171 : samples : 64000
2019-02-15 14:42:08,850 : Image to text: 14.62, 35.42, 48.72, 11.0
2019-02-15 14:42:15,476 : Text to Image: 10.948, 30.612, 43.652, 14.0
2019-02-15 14:42:54,499 : samples : 128000
2019-02-15 14:43:04,161 : Image to text: 14.24, 36.0, 49.34, 11.0
2019-02-15 14:43:10,786 : Text to Image: 11.708, 31.256, 44.248, 14.0
2019-02-15 14:43:49,795 : samples : 192000
2019-02-15 14:43:59,460 : Image to text: 14.18, 36.22, 49.8, 11.0
2019-02-15 14:44:06,101 : Text to Image: 11.368, 31.196, 44.016, 14.0
2019-02-15 14:44:45,397 : samples : 256000
2019-02-15 14:44:55,059 : Image to text: 14.0, 35.76, 48.96, 11.0
2019-02-15 14:45:01,686 : Text to Image: 10.72, 30.064, 43.012, 14.0
2019-02-15 14:45:41,641 : samples : 320000
2019-02-15 14:45:51,400 : Image to text: 14.18, 36.08, 49.2, 11.0
2019-02-15 14:45:58,083 : Text to Image: 11.176, 30.88, 43.748, 14.0
2019-02-15 14:46:38,334 : samples : 384000
2019-02-15 14:46:48,153 : Image to text: 14.28, 36.6, 49.12, 11.0
2019-02-15 14:46:54,932 : Text to Image: 11.132, 30.848, 43.908, 14.0
2019-02-15 14:47:35,504 : samples : 448000
2019-02-15 14:47:45,316 : Image to text: 14.6, 35.78, 49.52, 11.0
2019-02-15 14:47:52,055 : Text to Image: 11.324, 30.988, 44.128, 14.0
2019-02-15 14:48:32,579 : samples : 512000
2019-02-15 14:48:42,242 : Image to text: 14.1, 35.68, 48.48, 11.0
2019-02-15 14:48:48,884 : Text to Image: 11.104, 30.972, 44.204, 14.0
2019-02-15 14:49:22,403 : Epoch 10 finished
2019-02-15 14:49:22,777 : Image to text: 33.2, 66.4, 80.9, 3.0
2019-02-15 14:49:23,050 : Text to Image: 27.64, 63.3, 79.26, 3.0
2019-02-15 14:49:23,425 : Image to text: 32.1, 67.4, 78.9, 3.0
2019-02-15 14:49:23,698 : Text to Image: 25.58, 60.88, 77.56, 4.0
2019-02-15 14:49:24,072 : Image to text: 32.9, 66.0, 82.0, 3.0
2019-02-15 14:49:24,346 : Text to Image: 27.04, 62.82, 79.02, 3.0
2019-02-15 14:49:24,719 : Image to text: 33.7, 66.7, 79.9, 3.0
2019-02-15 14:49:24,993 : Text to Image: 27.16, 62.56, 78.78, 3.0
2019-02-15 14:49:25,367 : Image to text: 34.6, 68.2, 80.8, 3.0
2019-02-15 14:49:25,642 : Text to Image: 27.38, 62.7, 78.3, 3.0
2019-02-15 14:49:25,642 : Dev mean Text to Image: 26.96, 62.452, 78.584, 3.2
2019-02-15 14:49:25,642 : Dev mean Image to text: 33.300000000000004, 66.94, 80.5, 3.0
2019-02-15 14:49:25,642 : start epoch
2019-02-15 14:50:05,111 : samples : 64000
2019-02-15 14:50:14,781 : Image to text: 14.24, 35.16, 49.16, 11.0
2019-02-15 14:50:21,417 : Text to Image: 10.904, 30.612, 43.324, 14.0
2019-02-15 14:51:00,944 : samples : 128000
2019-02-15 14:51:10,634 : Image to text: 14.12, 35.56, 49.06, 11.0
2019-02-15 14:51:17,299 : Text to Image: 11.196, 30.744, 43.856, 14.0
2019-02-15 14:51:56,896 : samples : 192000
2019-02-15 14:52:06,595 : Image to text: 14.5, 36.34, 49.44, 11.0
2019-02-15 14:52:13,221 : Text to Image: 11.296, 30.9, 43.748, 14.0
2019-02-15 14:52:52,619 : samples : 256000
2019-02-15 14:53:02,318 : Image to text: 14.36, 36.76, 50.32, 10.0
2019-02-15 14:53:08,953 : Text to Image: 11.088, 30.76, 43.724, 14.0
2019-02-15 14:53:48,642 : samples : 320000
2019-02-15 14:53:58,355 : Image to text: 14.66, 36.64, 50.06, 10.0
2019-02-15 14:54:05,010 : Text to Image: 11.028, 30.992, 43.744, 14.0
2019-02-15 14:54:44,934 : samples : 384000
2019-02-15 14:54:54,634 : Image to text: 14.12, 36.12, 49.74, 11.0
2019-02-15 14:55:01,307 : Text to Image: 11.448, 31.376, 44.672, 13.0
2019-02-15 14:55:41,307 : samples : 448000
2019-02-15 14:55:51,039 : Image to text: 13.88, 35.64, 48.76, 11.0
2019-02-15 14:55:57,679 : Text to Image: 11.272, 31.096, 43.844, 14.0
2019-02-15 14:56:37,705 : samples : 512000
2019-02-15 14:56:47,464 : Image to text: 15.08, 36.26, 50.22, 10.0
2019-02-15 14:56:54,153 : Text to Image: 11.652, 31.548, 44.46, 14.0
2019-02-15 14:57:28,195 : Epoch 11 finished
2019-02-15 14:57:28,570 : Image to text: 32.4, 66.9, 80.6, 3.0
2019-02-15 14:57:28,843 : Text to Image: 27.94, 62.5, 78.48, 3.0
2019-02-15 14:57:29,216 : Image to text: 33.4, 65.5, 79.1, 3.0
2019-02-15 14:57:29,490 : Text to Image: 25.5, 61.34, 77.8, 4.0
2019-02-15 14:57:29,866 : Image to text: 34.5, 66.5, 81.4, 3.0
2019-02-15 14:57:30,145 : Text to Image: 27.64, 63.0, 78.6, 3.0
2019-02-15 14:57:30,519 : Image to text: 33.1, 66.9, 80.6, 3.0
2019-02-15 14:57:30,792 : Text to Image: 27.22, 62.98, 78.88, 3.0
2019-02-15 14:57:31,165 : Image to text: 33.2, 66.8, 79.8, 3.0
2019-02-15 14:57:31,439 : Text to Image: 28.12, 62.08, 77.96, 3.0
2019-02-15 14:57:31,439 : Dev mean Text to Image: 27.284, 62.379999999999995, 78.344, 3.2
2019-02-15 14:57:31,439 : Dev mean Image to text: 33.32000000000001, 66.52000000000001, 80.3, 3.0
2019-02-15 14:57:31,439 : start epoch
2019-02-15 14:58:11,431 : samples : 64000
2019-02-15 14:58:21,128 : Image to text: 14.04, 35.8, 49.66, 11.0
2019-02-15 14:58:27,758 : Text to Image: 11.456, 31.3, 44.112, 14.0
2019-02-15 14:59:07,818 : samples : 128000
2019-02-15 14:59:17,570 : Image to text: 13.76, 36.16, 49.88, 11.0
2019-02-15 14:59:24,337 : Text to Image: 11.264, 31.184, 44.124, 14.0
2019-02-15 15:00:05,605 : samples : 192000
2019-02-15 15:00:15,486 : Image to text: 14.42, 37.02, 50.08, 10.0
2019-02-15 15:00:22,204 : Text to Image: 11.592, 31.46, 44.532, 13.0
2019-02-15 15:01:04,284 : samples : 256000
2019-02-15 15:01:14,308 : Image to text: 14.64, 35.68, 49.68, 11.0
2019-02-15 15:01:21,249 : Text to Image: 11.308, 31.124, 44.364, 14.0
2019-02-15 15:02:04,635 : samples : 320000
2019-02-15 15:02:14,760 : Image to text: 14.62, 35.8, 49.24, 11.0
2019-02-15 15:02:21,676 : Text to Image: 11.128, 30.664, 43.94, 14.0
2019-02-15 15:03:04,832 : samples : 384000
2019-02-15 15:03:14,932 : Image to text: 14.38, 35.42, 49.46, 11.0
2019-02-15 15:03:21,817 : Text to Image: 11.7, 31.5, 44.388, 13.0
2019-02-15 15:04:04,976 : samples : 448000
2019-02-15 15:04:15,061 : Image to text: 14.46, 37.02, 50.42, 10.0
2019-02-15 15:04:21,828 : Text to Image: 11.448, 31.24, 44.048, 14.0
2019-02-15 15:05:03,607 : samples : 512000
2019-02-15 15:05:13,594 : Image to text: 14.8, 37.06, 50.66, 10.0
2019-02-15 15:05:20,393 : Text to Image: 11.784, 31.384, 44.392, 14.0
2019-02-15 15:05:56,208 : Epoch 12 finished
2019-02-15 15:05:56,591 : Image to text: 33.9, 67.7, 80.3, 3.0
2019-02-15 15:05:56,874 : Text to Image: 27.4, 62.84, 78.8, 3.0
2019-02-15 15:05:57,257 : Image to text: 31.0, 66.7, 80.0, 3.0
2019-02-15 15:05:57,540 : Text to Image: 26.08, 61.76, 78.3, 4.0
2019-02-15 15:05:57,922 : Image to text: 34.5, 66.8, 81.5, 3.0
2019-02-15 15:05:58,205 : Text to Image: 27.36, 63.72, 79.5, 3.0
2019-02-15 15:05:58,587 : Image to text: 33.3, 66.8, 80.2, 3.0
2019-02-15 15:05:58,869 : Text to Image: 27.32, 62.84, 78.98, 3.0
2019-02-15 15:05:59,252 : Image to text: 34.2, 66.8, 79.5, 3.0
2019-02-15 15:05:59,534 : Text to Image: 28.02, 62.96, 78.54, 3.0
2019-02-15 15:05:59,534 : Dev mean Text to Image: 27.235999999999997, 62.824, 78.824, 3.2
2019-02-15 15:05:59,534 : Dev mean Image to text: 33.38, 66.96000000000001, 80.30000000000001, 3.0
2019-02-15 15:05:59,534 : start epoch
2019-02-15 15:06:42,519 : samples : 64000
2019-02-15 15:06:52,608 : Image to text: 14.4, 37.12, 51.08, 10.0
2019-02-15 15:06:59,614 : Text to Image: 11.584, 31.18, 44.544, 13.0
2019-02-15 15:07:43,048 : samples : 128000
2019-02-15 15:07:53,061 : Image to text: 14.64, 37.34, 50.34, 10.0
2019-02-15 15:08:00,010 : Text to Image: 11.216, 31.008, 44.136, 14.0
2019-02-15 15:08:43,242 : samples : 192000
2019-02-15 15:08:53,280 : Image to text: 14.76, 36.1, 49.78, 11.0
2019-02-15 15:09:00,221 : Text to Image: 11.232, 31.092, 44.252, 13.0
2019-02-15 15:09:43,481 : samples : 256000
2019-02-15 15:09:53,569 : Image to text: 15.04, 37.88, 50.82, 10.0
2019-02-15 15:10:00,579 : Text to Image: 11.524, 31.652, 44.5, 13.0
2019-02-15 15:10:43,841 : samples : 320000
2019-02-15 15:10:53,919 : Image to text: 14.58, 36.36, 50.06, 10.0
2019-02-15 15:11:00,907 : Text to Image: 11.54, 31.736, 44.756, 13.0
2019-02-15 15:11:43,965 : samples : 384000
2019-02-15 15:11:53,989 : Image to text: 13.9, 35.92, 50.2, 10.0
2019-02-15 15:12:00,951 : Text to Image: 11.356, 31.28, 44.452, 14.0
2019-02-15 15:12:44,046 : samples : 448000
2019-02-15 15:12:54,135 : Image to text: 14.82, 36.8, 50.24, 10.0
2019-02-15 15:13:01,101 : Text to Image: 11.28, 31.224, 44.256, 14.0
2019-02-15 15:13:44,020 : samples : 512000
2019-02-15 15:13:54,115 : Image to text: 14.34, 36.36, 49.74, 11.0
2019-02-15 15:14:01,083 : Text to Image: 11.276, 31.036, 44.16, 14.0
2019-02-15 15:14:37,392 : Epoch 13 finished
2019-02-15 15:14:37,785 : Image to text: 34.0, 68.2, 81.1, 3.0
2019-02-15 15:14:38,080 : Text to Image: 26.86, 63.76, 79.42, 3.0
2019-02-15 15:14:38,475 : Image to text: 31.7, 65.8, 79.6, 3.0
2019-02-15 15:14:38,766 : Text to Image: 26.48, 61.78, 78.32, 4.0
2019-02-15 15:14:39,164 : Image to text: 32.9, 68.7, 82.5, 3.0
2019-02-15 15:14:39,450 : Text to Image: 28.08, 64.3, 79.7, 3.0
2019-02-15 15:14:39,845 : Image to text: 34.7, 67.6, 81.0, 3.0
2019-02-15 15:14:40,144 : Text to Image: 28.2, 63.72, 79.26, 3.0
2019-02-15 15:14:40,537 : Image to text: 33.3, 67.7, 80.6, 3.0
2019-02-15 15:14:40,833 : Text to Image: 27.86, 63.3, 78.3, 3.0
2019-02-15 15:14:40,834 : Dev mean Text to Image: 27.496, 63.372, 79.0, 3.2
2019-02-15 15:14:40,834 : Dev mean Image to text: 33.32, 67.60000000000001, 80.96000000000001, 3.0
2019-02-15 15:14:40,834 : start epoch
2019-02-15 15:15:24,077 : samples : 64000
2019-02-15 15:15:34,128 : Image to text: 14.84, 37.04, 51.06, 10.0
2019-02-15 15:15:41,044 : Text to Image: 11.608, 31.14, 44.468, 14.0
2019-02-15 15:16:23,941 : samples : 128000
2019-02-15 15:16:34,058 : Image to text: 14.88, 36.26, 49.8, 11.0
2019-02-15 15:16:40,962 : Text to Image: 11.28, 31.052, 43.84, 14.0
2019-02-15 15:17:24,034 : samples : 192000
2019-02-15 15:17:34,057 : Image to text: 14.6, 37.26, 50.76, 10.0
2019-02-15 15:17:41,006 : Text to Image: 11.396, 31.168, 44.292, 14.0
2019-02-15 15:18:24,267 : samples : 256000
2019-02-15 15:18:34,358 : Image to text: 14.88, 37.04, 50.48, 10.0
2019-02-15 15:18:41,400 : Text to Image: 11.704, 31.592, 44.604, 13.0
2019-02-15 15:19:23,983 : samples : 320000
2019-02-15 15:19:34,021 : Image to text: 14.42, 36.5, 50.02, 10.0
2019-02-15 15:19:41,051 : Text to Image: 11.592, 31.232, 44.48, 14.0
2019-02-15 15:20:24,237 : samples : 384000
2019-02-15 15:20:34,225 : Image to text: 15.06, 36.76, 50.24, 10.0
2019-02-15 15:20:41,171 : Text to Image: 11.748, 31.58, 44.672, 13.0
2019-02-15 15:21:23,934 : samples : 448000
2019-02-15 15:21:34,018 : Image to text: 14.0, 36.04, 50.34, 10.0
2019-02-15 15:21:40,968 : Text to Image: 11.456, 31.184, 44.364, 14.0
2019-02-15 15:22:25,150 : samples : 512000
2019-02-15 15:22:37,270 : Image to text: 14.7, 37.7, 50.84, 10.0
2019-02-15 15:22:45,530 : Text to Image: 12.008, 32.052, 45.088, 13.0
2019-02-15 15:23:29,439 : Epoch 14 finished
2019-02-15 15:23:29,826 : Image to text: 35.2, 68.5, 81.9, 3.0
2019-02-15 15:23:30,118 : Text to Image: 27.16, 63.66, 79.56, 3.0
2019-02-15 15:23:30,512 : Image to text: 31.6, 67.0, 80.4, 3.0
2019-02-15 15:23:30,801 : Text to Image: 26.0, 62.08, 78.66, 4.0
2019-02-15 15:23:31,199 : Image to text: 33.4, 67.5, 80.9, 3.0
2019-02-15 15:23:31,493 : Text to Image: 28.02, 63.86, 79.72, 3.0
2019-02-15 15:23:31,888 : Image to text: 32.5, 66.4, 80.2, 3.0
2019-02-15 15:23:32,188 : Text to Image: 27.56, 62.78, 79.2, 3.0
2019-02-15 15:23:32,591 : Image to text: 33.9, 66.5, 80.8, 3.0
2019-02-15 15:23:32,887 : Text to Image: 27.98, 62.34, 78.62, 3.0
2019-02-15 15:23:32,887 : Dev mean Text to Image: 27.344, 62.944, 79.152, 3.2
2019-02-15 15:23:32,887 : Dev mean Image to text: 33.32, 67.18, 80.84, 3.0
2019-02-15 15:23:36,326 : 
Test scores | Image to text:             32.5, 67.58, 81.4, 3.0
2019-02-15 15:23:36,327 : Test scores | Text to image:             27.807999999999996, 62.956, 78.744, 3.1999999999999997

2019-02-15 15:23:36,484 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 15:23:36,882 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 15:23:37,638 : loading BERT model bert-base-uncased
2019-02-15 15:23:37,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:23:37,677 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:23:37,677 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpumitkx_8
2019-02-15 15:23:40,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:23:41,693 : Computing embeddings for train/dev/test
2019-02-15 15:25:04,273 : Computed embeddings
2019-02-15 15:25:04,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:25:52,296 : [('reg:1e-05', 77.03), ('reg:0.0001', 76.26), ('reg:0.001', 70.35), ('reg:0.01', 52.81)]
2019-02-15 15:25:52,296 : Validation : best param found is reg = 1e-05 with score             77.03
2019-02-15 15:25:52,296 : Evaluating...
2019-02-15 15:26:07,734 : 
Dev acc : 77.0 Test acc : 77.6 for LENGTH classification

2019-02-15 15:26:07,735 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 15:26:08,095 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 15:26:08,149 : loading BERT model bert-base-uncased
2019-02-15 15:26:08,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:26:08,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:26:08,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq8s9z44u
2019-02-15 15:26:10,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:26:12,203 : Computing embeddings for train/dev/test
2019-02-15 15:27:29,394 : Computed embeddings
2019-02-15 15:27:29,394 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:28:15,034 : [('reg:1e-05', 45.09), ('reg:0.0001', 17.89), ('reg:0.001', 2.02), ('reg:0.01', 0.82)]
2019-02-15 15:28:15,034 : Validation : best param found is reg = 1e-05 with score             45.09
2019-02-15 15:28:15,034 : Evaluating...
2019-02-15 15:28:29,978 : 
Dev acc : 45.1 Test acc : 45.0 for WORDCONTENT classification

2019-02-15 15:28:29,979 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 15:28:30,377 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 15:28:30,454 : loading BERT model bert-base-uncased
2019-02-15 15:28:30,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:28:30,567 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:28:30,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp6d27i7x
2019-02-15 15:28:33,100 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:28:34,582 : Computing embeddings for train/dev/test
2019-02-15 15:29:47,052 : Computed embeddings
2019-02-15 15:29:47,052 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:30:17,161 : [('reg:1e-05', 33.96), ('reg:0.0001', 33.88), ('reg:0.001', 32.65), ('reg:0.01', 28.1)]
2019-02-15 15:30:17,161 : Validation : best param found is reg = 1e-05 with score             33.96
2019-02-15 15:30:17,161 : Evaluating...
2019-02-15 15:30:24,484 : 
Dev acc : 34.0 Test acc : 33.8 for DEPTH classification

2019-02-15 15:30:24,485 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 15:30:25,121 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 15:30:25,195 : loading BERT model bert-base-uncased
2019-02-15 15:30:25,195 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:30:25,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:30:25,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp34h_x5p0
2019-02-15 15:30:27,756 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:30:29,270 : Computing embeddings for train/dev/test
2019-02-15 15:31:37,460 : Computed embeddings
2019-02-15 15:31:37,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:32:20,444 : [('reg:1e-05', 67.37), ('reg:0.0001', 65.0), ('reg:0.001', 58.57), ('reg:0.01', 47.12)]
2019-02-15 15:32:20,444 : Validation : best param found is reg = 1e-05 with score             67.37
2019-02-15 15:32:20,444 : Evaluating...
2019-02-15 15:32:30,902 : 
Dev acc : 67.4 Test acc : 66.8 for TOPCONSTITUENTS classification

2019-02-15 15:32:30,903 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 15:32:31,337 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 15:32:31,410 : loading BERT model bert-base-uncased
2019-02-15 15:32:31,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:32:31,460 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:32:31,460 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp067irho3
2019-02-15 15:32:34,015 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:32:35,518 : Computing embeddings for train/dev/test
2019-02-15 15:33:48,967 : Computed embeddings
2019-02-15 15:33:48,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:34:31,606 : [('reg:1e-05', 88.51), ('reg:0.0001', 88.49), ('reg:0.001', 88.21), ('reg:0.01', 86.35)]
2019-02-15 15:34:31,606 : Validation : best param found is reg = 1e-05 with score             88.51
2019-02-15 15:34:31,606 : Evaluating...
2019-02-15 15:34:43,328 : 
Dev acc : 88.5 Test acc : 88.2 for BIGRAMSHIFT classification

2019-02-15 15:34:43,330 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 15:34:43,772 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 15:34:43,845 : loading BERT model bert-base-uncased
2019-02-15 15:34:43,846 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:34:43,983 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:34:43,983 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbkjo8_3m
2019-02-15 15:34:46,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:34:48,044 : Computing embeddings for train/dev/test
2019-02-15 15:35:59,794 : Computed embeddings
2019-02-15 15:35:59,794 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:36:34,066 : [('reg:1e-05', 89.73), ('reg:0.0001', 89.83), ('reg:0.001', 90.14), ('reg:0.01', 90.04)]
2019-02-15 15:36:34,066 : Validation : best param found is reg = 0.001 with score             90.14
2019-02-15 15:36:34,066 : Evaluating...
2019-02-15 15:36:43,535 : 
Dev acc : 90.1 Test acc : 88.2 for TENSE classification

2019-02-15 15:36:43,536 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 15:36:44,001 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 15:36:44,079 : loading BERT model bert-base-uncased
2019-02-15 15:36:44,080 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:36:44,225 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:36:44,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz8z72mpf
2019-02-15 15:36:46,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:36:48,264 : Computing embeddings for train/dev/test
2019-02-15 15:38:04,148 : Computed embeddings
2019-02-15 15:38:04,148 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:38:45,906 : [('reg:1e-05', 84.6), ('reg:0.0001', 84.43), ('reg:0.001', 84.13), ('reg:0.01', 83.17)]
2019-02-15 15:38:45,906 : Validation : best param found is reg = 1e-05 with score             84.6
2019-02-15 15:38:45,906 : Evaluating...
2019-02-15 15:38:58,332 : 
Dev acc : 84.6 Test acc : 84.6 for SUBJNUMBER classification

2019-02-15 15:38:58,333 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 15:38:58,963 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 15:38:59,040 : loading BERT model bert-base-uncased
2019-02-15 15:38:59,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:38:59,078 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:38:59,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqj8ipnli
2019-02-15 15:39:01,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:39:03,091 : Computing embeddings for train/dev/test
2019-02-15 15:40:25,102 : Computed embeddings
2019-02-15 15:40:25,102 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:41:07,953 : [('reg:1e-05', 80.0), ('reg:0.0001', 80.05), ('reg:0.001', 80.03), ('reg:0.01', 78.52)]
2019-02-15 15:41:07,953 : Validation : best param found is reg = 0.0001 with score             80.05
2019-02-15 15:41:07,953 : Evaluating...
2019-02-15 15:41:20,102 : 
Dev acc : 80.0 Test acc : 80.4 for OBJNUMBER classification

2019-02-15 15:41:20,103 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 15:41:20,566 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 15:41:20,642 : loading BERT model bert-base-uncased
2019-02-15 15:41:20,643 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:41:20,677 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:41:20,677 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptn_v48sc
2019-02-15 15:41:23,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:41:24,677 : Computing embeddings for train/dev/test
2019-02-15 15:42:50,523 : Computed embeddings
2019-02-15 15:42:50,523 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:43:26,552 : [('reg:1e-05', 64.68), ('reg:0.0001', 64.57), ('reg:0.001', 64.32), ('reg:0.01', 65.45)]
2019-02-15 15:43:26,552 : Validation : best param found is reg = 0.01 with score             65.45
2019-02-15 15:43:26,552 : Evaluating...
2019-02-15 15:43:39,941 : 
Dev acc : 65.5 Test acc : 64.5 for ODDMANOUT classification

2019-02-15 15:43:39,942 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 15:43:40,443 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 15:43:40,526 : loading BERT model bert-base-uncased
2019-02-15 15:43:40,526 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:43:40,564 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:43:40,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptfd_8se7
2019-02-15 15:43:43,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:43:44,590 : Computing embeddings for train/dev/test
2019-02-15 15:45:09,991 : Computed embeddings
2019-02-15 15:45:09,991 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:45:57,731 : [('reg:1e-05', 69.08), ('reg:0.0001', 69.01), ('reg:0.001', 69.13), ('reg:0.01', 66.42)]
2019-02-15 15:45:57,731 : Validation : best param found is reg = 0.001 with score             69.13
2019-02-15 15:45:57,731 : Evaluating...
2019-02-15 15:46:10,176 : 
Dev acc : 69.1 Test acc : 68.1 for COORDINATIONINVERSION classification

2019-02-15 15:46:10,178 : total results: {'STS12': {'MSRpar': {'pearson': (0.35080611007412454, 3.8509397697871774e-23), 'spearman': SpearmanrResult(correlation=0.4008028075384826, pvalue=2.600350039258681e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3574974469821697, 4.980765183966073e-24), 'spearman': SpearmanrResult(correlation=0.3876519061632382, pvalue=2.6515414058605344e-28), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47895325858574384, 1.0696520322533151e-27), 'spearman': SpearmanrResult(correlation=0.5790452258483904, pvalue=1.8972858221541758e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.579227905979551, 2.0547811960456315e-68), 'spearman': SpearmanrResult(correlation=0.5796082436605509, pvalue=1.6021836917087366e-68), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5833381830425967, 9.477792856074256e-38), 'spearman': SpearmanrResult(correlation=0.5418387768776708, pvalue=8.011118562566017e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4699645809328371, 'wmean': 0.45631920141626003}, 'spearman': {'mean': 0.49778939201766653, 'wmean': 0.48520677241322574}}}, 'STS13': {'FNWN': {'pearson': (0.42027695078457045, 1.7369187958770378e-09), 'spearman': SpearmanrResult(correlation=0.44706410341349023, pvalue=1.127214813064322e-10), 'nsamples': 189}, 'headlines': {'pearson': (0.6471272790637843, 3.297436370324476e-90), 'spearman': SpearmanrResult(correlation=0.6214995473872796, pvalue=2.378033101697607e-81), 'nsamples': 750}, 'OnWN': {'pearson': (0.5584891342237713, 2.5186058231640175e-47), 'spearman': SpearmanrResult(correlation=0.5709919911435829, pvalue=7.490799700775691e-50), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5419644546907088, 'wmean': 0.5853934715304385}, 'spearman': {'mean': 0.5465185473147843, 'wmean': 0.5806308554114396}}}, 'STS14': {'deft-forum': {'pearson': (0.33517344658242065, 2.8221797349044877e-13), 'spearman': SpearmanrResult(correlation=0.3277425344689775, pvalue=9.961699853695555e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7512845294591148, 1.0172324277558216e-55), 'spearman': SpearmanrResult(correlation=0.7167636208178597, pvalue=1.4118964428312582e-48), 'nsamples': 300}, 'headlines': {'pearson': (0.599546608117327, 2.1640877173086343e-74), 'spearman': SpearmanrResult(correlation=0.5421053873616071, pvalue=1.6231026312930347e-58), 'nsamples': 750}, 'images': {'pearson': (0.45178731422362517, 5.3254700661549696e-39), 'spearman': SpearmanrResult(correlation=0.4449333417950941, pvalue=9.602813917283093e-38), 'nsamples': 750}, 'OnWN': {'pearson': (0.6977044107045871, 1.8641386901213634e-110), 'spearman': SpearmanrResult(correlation=0.7181222547272003, pvalue=6.935189463119958e-120), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6613557625942664, 1.662782500912532e-95), 'spearman': SpearmanrResult(correlation=0.5932714516885115, pvalue=1.685963704398521e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5828086786135568, 'wmean': 0.5824023950745808}, 'spearman': {'mean': 0.5571564318098751, 'wmean': 0.5563566809161886}}}, 'STS15': {'answers-forums': {'pearson': (0.604621376837723, 9.490032616618947e-39), 'spearman': SpearmanrResult(correlation=0.6016348976758708, pvalue=2.7415764251583826e-38), 'nsamples': 375}, 'answers-students': {'pearson': (0.5641964122454406, 2.9640317150703668e-64), 'spearman': SpearmanrResult(correlation=0.5679701297522847, pvalue=2.8041053842547244e-65), 'nsamples': 750}, 'belief': {'pearson': (0.7071331680987738, 4.140029596376785e-58), 'spearman': SpearmanrResult(correlation=0.7153901150377215, pvalue=4.857987485564283e-60), 'nsamples': 375}, 'headlines': {'pearson': (0.6711056834764186, 2.629637991690932e-99), 'spearman': SpearmanrResult(correlation=0.6600259181469976, pvalue=5.3484735708935256e-95), 'nsamples': 750}, 'images': {'pearson': (0.6187577796030145, 1.8861846688906345e-80), 'spearman': SpearmanrResult(correlation=0.6244125179508605, pvalue=2.5754458030205485e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6331628840522742, 'wmean': 0.6274842869482805}, 'spearman': {'mean': 0.633886715712747, 'wmean': 0.6277302680517347}}}, 'STS16': {'answer-answer': {'pearson': (0.553468988693486, 8.696745208819695e-22), 'spearman': SpearmanrResult(correlation=0.5656283486651251, pvalue=7.015113546336354e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6537779600151308, 9.542377841938514e-32), 'spearman': SpearmanrResult(correlation=0.6574766500311147, pvalue=3.318975284972055e-32), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7692302676644358, 3.0545856657446224e-46), 'spearman': SpearmanrResult(correlation=0.7857314358961456, pvalue=1.829635252464683e-49), 'nsamples': 230}, 'postediting': {'pearson': (0.8057528659945581, 5.610354568038406e-57), 'spearman': SpearmanrResult(correlation=0.824265134032028, pvalue=1.0283332861930353e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.5274997481312688, 2.2836776102210805e-16), 'spearman': SpearmanrResult(correlation=0.5162063366811672, pvalue=1.2470251147478226e-15), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6619459660997759, 'wmean': 0.6636980972991912}, 'spearman': {'mean': 0.6698615810611162, 'wmean': 0.6720972459907097}}}, 'MR': {'devacc': 80.47, 'acc': 79.53, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 86.83, 'acc': 85.72, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.56, 'acc': 87.63, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.43, 'acc': 95.05, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.24, 'acc': 85.06, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.87, 'acc': 46.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.37, 'acc': 89.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.48, 'acc': 73.45, 'f1': 80.79, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.2, 'acc': 77.15, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8171522572469139, 'pearson': 0.8094042913214028, 'spearman': 0.7361093992751818, 'mse': 0.35301990357161456, 'yhat': array([3.32618537, 4.0652178 , 2.40994995, ..., 3.43443938, 4.50666863,        4.73824627]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7140077083391813, 'pearson': 0.6605640727130131, 'spearman': 0.651771579777153, 'mse': 1.4713919112985543, 'yhat': array([2.14575773, 1.58156135, 2.25255849, ..., 3.84748113, 3.76941291,        3.73962837]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.98, 'acc': 66.32, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 351.748, 'acc': [(32.5, 67.58, 81.4, 3.0), (27.807999999999996, 62.956, 78.744, 3.1999999999999997)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 77.03, 'acc': 77.61, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 45.09, 'acc': 45.03, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 33.96, 'acc': 33.77, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 67.37, 'acc': 66.77, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.51, 'acc': 88.23, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.14, 'acc': 88.17, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.6, 'acc': 84.61, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.05, 'acc': 80.4, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.45, 'acc': 64.51, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.13, 'acc': 68.1, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 15:46:10,178 : STS12 p=0.4563, STS12 s=0.4852, STS13 p=0.5854, STS13 s=0.5806, STS14 p=0.5824, STS14 s=0.5564, STS15 p=0.6275, STS15 s=0.6277, STS 16 p=0.6637, STS16 s=0.6721, STS B p=0.6606, STS B s=0.6518, STS B m=1.4714, SICK-R p=0.8094, SICK-R s=0.7361, SICK-P m=0.3530
2019-02-15 15:46:10,178 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 15:46:10,178 : 0.4563,0.4852,0.5854,0.5806,0.5824,0.5564,0.6275,0.6277,0.6637,0.6721,0.6606,0.6518,1.4714,0.8094,0.7361,0.3530
2019-02-15 15:46:10,178 : MR=79.53, CR=85.72, SUBJ=95.05, MPQA=87.63, SST-B=85.06, SST-F=46.38, TREC=89.80, SICK-E=77.15, SNLI=66.32, MRPC=73.45, MRPC f=80.79
2019-02-15 15:46:10,178 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 15:46:10,178 : 79.53,85.72,95.05,87.63,85.06,46.38,89.80,77.15,66.32,73.45,80.79
2019-02-15 15:46:10,178 : COCO r1i2t=32.50, COCO r5i2t=67.58, COCO r10i2t=81.40, COCO medr_i2t=3.00, COCO r1t2i=27.81, COCO r5t2i=62.96, COCO r10t2i=78.74, COCO medr_t2i=3.20
2019-02-15 15:46:10,178 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 15:46:10,179 : 32.50,67.58,81.40,3.00,27.81,62.96,78.74,3.20
2019-02-15 15:46:10,179 : SentLen=77.61, WC=45.03, TreeDepth=33.77, TopConst=66.77, BShift=88.23, Tense=88.17, SubjNum=84.61, ObjNum=80.40, SOMO=64.51, CoordInv=68.10, average=69.72
2019-02-15 15:46:10,179 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 15:46:10,179 : 77.61,45.03,33.77,66.77,88.23,88.17,84.61,80.40,64.51,68.10,69.72
