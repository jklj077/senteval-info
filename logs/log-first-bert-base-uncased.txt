2019-02-15 14:19:06,070 : ********************************************************************************
2019-02-15 14:19:06,070 : ********************************************************************************
2019-02-15 14:19:06,071 : ********************************************************************************
2019-02-15 14:19:06,071 : layer 0
2019-02-15 14:19:06,072 : ********************************************************************************
2019-02-15 14:19:06,072 : ********************************************************************************
2019-02-15 14:19:06,072 : ********************************************************************************
2019-02-15 14:19:06,072 : ***** Transfer task : STS12 *****


2019-02-15 14:19:06,122 : loading BERT model bert-base-uncased
2019-02-15 14:19:06,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:06,141 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:06,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp11_oyxsk
2019-02-15 14:19:08,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:15,308 : MSRpar : pearson = nan, spearman = nan
2019-02-15 14:19:15,958 : MSRvid : pearson = nan, spearman = nan
2019-02-15 14:19:16,512 : SMTeuroparl : pearson = nan, spearman = nan
2019-02-15 14:19:17,865 : surprise.OnWN : pearson = nan, spearman = nan
2019-02-15 14:19:18,802 : surprise.SMTnews : pearson = nan, spearman = nan
2019-02-15 14:19:18,803 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-15 14:19:18,803 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-15 14:19:18,803 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 14:19:18,812 : loading BERT model bert-base-uncased
2019-02-15 14:19:18,812 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:18,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:18,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy8rco8l_
2019-02-15 14:19:21,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:23,345 : FNWN : pearson = nan, spearman = nan
2019-02-15 14:19:24,531 : headlines : pearson = nan, spearman = nan
2019-02-15 14:19:25,790 : OnWN : pearson = nan, spearman = nan
2019-02-15 14:19:25,790 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-15 14:19:25,790 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-15 14:19:25,790 : ***** Transfer task : STS14 *****


2019-02-15 14:19:25,823 : loading BERT model bert-base-uncased
2019-02-15 14:19:25,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:25,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:25,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqj1x1971
2019-02-15 14:19:28,275 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:31,428 : deft-forum : pearson = nan, spearman = nan
2019-02-15 14:19:32,784 : deft-news : pearson = nan, spearman = nan
2019-02-15 14:19:35,524 : headlines : pearson = nan, spearman = nan
2019-02-15 14:19:37,735 : images : pearson = nan, spearman = nan
2019-02-15 14:19:38,861 : OnWN : pearson = nan, spearman = nan
2019-02-15 14:19:40,270 : tweet-news : pearson = nan, spearman = nan
2019-02-15 14:19:40,270 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-15 14:19:40,270 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-15 14:19:40,270 : ***** Transfer task : STS15 *****


2019-02-15 14:19:40,305 : loading BERT model bert-base-uncased
2019-02-15 14:19:40,305 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:40,324 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:40,324 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprcjg_xvz
2019-02-15 14:19:42,756 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:45,126 : answers-forums : pearson = nan, spearman = nan
2019-02-15 14:19:46,228 : answers-students : pearson = nan, spearman = nan
2019-02-15 14:19:47,163 : belief : pearson = nan, spearman = nan
2019-02-15 14:19:48,325 : headlines : pearson = nan, spearman = nan
2019-02-15 14:19:49,455 : images : pearson = nan, spearman = nan
2019-02-15 14:19:49,455 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-15 14:19:49,455 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-15 14:19:49,455 : ***** Transfer task : STS16 *****


2019-02-15 14:19:49,526 : loading BERT model bert-base-uncased
2019-02-15 14:19:49,526 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:49,543 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:49,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpftc0a76e
2019-02-15 14:19:52,013 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:53,924 : answer-answer : pearson = nan, spearman = nan
2019-02-15 14:19:54,290 : headlines : pearson = nan, spearman = nan
2019-02-15 14:19:54,729 : plagiarism : pearson = nan, spearman = nan
2019-02-15 14:19:55,396 : postediting : pearson = nan, spearman = nan
2019-02-15 14:19:55,727 : question-question : pearson = nan, spearman = nan
2019-02-15 14:19:55,727 : ALL (weighted average) : Pearson = nan,             Spearman = nan
2019-02-15 14:19:55,727 : ALL (average) : Pearson = nan,             Spearman = nan

2019-02-15 14:19:55,727 : ***** Transfer task : MR *****


2019-02-15 14:19:55,747 : loading BERT model bert-base-uncased
2019-02-15 14:19:55,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:19:55,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:19:55,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz2ckkcz9
2019-02-15 14:19:58,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:19:59,773 : Generating sentence embeddings
2019-02-15 14:20:14,089 : Generated sentence embeddings
2019-02-15 14:20:14,090 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:20:23,181 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-02-15 14:20:32,922 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-02-15 14:20:43,019 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-02-15 14:20:54,124 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-02-15 14:21:03,427 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-02-15 14:21:03,977 : Dev acc : 50.0 Test acc : 50.0

2019-02-15 14:21:03,978 : ***** Transfer task : CR *****


2019-02-15 14:21:03,986 : loading BERT model bert-base-uncased
2019-02-15 14:21:03,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:21:04,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:21:04,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplidyhsrc
2019-02-15 14:21:06,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:21:07,921 : Generating sentence embeddings
2019-02-15 14:21:11,769 : Generated sentence embeddings
2019-02-15 14:21:11,770 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:21:14,660 : Best param found at split 1: l2reg = 1e-05                 with score 63.76
2019-02-15 14:21:19,573 : Best param found at split 2: l2reg = 1e-05                 with score 63.76
2019-02-15 14:21:22,557 : Best param found at split 3: l2reg = 1e-05                 with score 63.77
2019-02-15 14:21:25,552 : Best param found at split 4: l2reg = 0.01                 with score 63.75
2019-02-15 14:21:29,498 : Best param found at split 5: l2reg = 1e-05                 with score 63.75
2019-02-15 14:21:29,698 : Dev acc : 63.76 Test acc : 63.76

2019-02-15 14:21:29,698 : ***** Transfer task : MPQA *****


2019-02-15 14:21:29,704 : loading BERT model bert-base-uncased
2019-02-15 14:21:29,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:21:29,724 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:21:29,724 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp21dzf7yf
2019-02-15 14:21:32,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:21:33,649 : Generating sentence embeddings
2019-02-15 14:21:38,628 : Generated sentence embeddings
2019-02-15 14:21:38,629 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:21:48,722 : Best param found at split 1: l2reg = 1e-05                 with score 68.78
2019-02-15 14:21:59,208 : Best param found at split 2: l2reg = 0.01                 with score 53.76
2019-02-15 14:22:10,388 : Best param found at split 3: l2reg = 1e-05                 with score 68.77
2019-02-15 14:22:21,102 : Best param found at split 4: l2reg = 1e-05                 with score 68.77
2019-02-15 14:22:30,809 : Best param found at split 5: l2reg = 1e-05                 with score 68.77
2019-02-15 14:22:31,231 : Dev acc : 65.77 Test acc : 68.77

2019-02-15 14:22:31,232 : ***** Transfer task : SUBJ *****


2019-02-15 14:22:31,249 : loading BERT model bert-base-uncased
2019-02-15 14:22:31,249 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:22:31,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:22:31,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp__vfw7n1
2019-02-15 14:22:33,704 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:22:35,096 : Generating sentence embeddings
2019-02-15 14:22:50,133 : Generated sentence embeddings
2019-02-15 14:22:50,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 14:23:01,836 : Best param found at split 1: l2reg = 1e-05                 with score 50.0
2019-02-15 14:23:11,232 : Best param found at split 2: l2reg = 1e-05                 with score 50.0
2019-02-15 14:23:20,839 : Best param found at split 3: l2reg = 1e-05                 with score 50.0
2019-02-15 14:23:30,335 : Best param found at split 4: l2reg = 1e-05                 with score 50.0
2019-02-15 14:23:39,686 : Best param found at split 5: l2reg = 1e-05                 with score 50.0
2019-02-15 14:23:40,303 : Dev acc : 50.0 Test acc : 50.0

2019-02-15 14:23:40,304 : ***** Transfer task : SST Binary classification *****


2019-02-15 14:23:40,434 : loading BERT model bert-base-uncased
2019-02-15 14:23:40,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:23:40,456 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:23:40,456 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyf3awcz5
2019-02-15 14:23:42,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:23:44,325 : Computing embedding for train
2019-02-15 14:24:34,557 : Computed train embeddings
2019-02-15 14:24:34,557 : Computing embedding for dev
2019-02-15 14:24:35,580 : Computed dev embeddings
2019-02-15 14:24:35,580 : Computing embedding for test
2019-02-15 14:24:37,814 : Computed test embeddings
2019-02-15 14:24:37,814 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:25:17,498 : [('reg:1e-05', 50.92), ('reg:0.0001', 50.92), ('reg:0.001', 50.92), ('reg:0.01', 50.92)]
2019-02-15 14:25:17,498 : Validation : best param found is reg = 1e-05 with score             50.92
2019-02-15 14:25:17,498 : Evaluating...
2019-02-15 14:25:22,385 : 
Dev acc : 50.92 Test acc : 49.92 for             SST Binary classification

2019-02-15 14:25:22,392 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 14:25:22,445 : loading BERT model bert-base-uncased
2019-02-15 14:25:22,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:25:22,468 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:25:22,468 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_xiixjt6
2019-02-15 14:25:24,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:25:26,347 : Computing embedding for train
2019-02-15 14:25:36,790 : Computed train embeddings
2019-02-15 14:25:36,790 : Computing embedding for dev
2019-02-15 14:25:38,142 : Computed dev embeddings
2019-02-15 14:25:38,142 : Computing embedding for test
2019-02-15 14:25:40,823 : Computed test embeddings
2019-02-15 14:25:40,823 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:25:42,662 : [('reg:1e-05', 26.25), ('reg:0.0001', 26.25), ('reg:0.001', 25.34), ('reg:0.01', 25.34)]
2019-02-15 14:25:42,662 : Validation : best param found is reg = 1e-05 with score             26.25
2019-02-15 14:25:42,662 : Evaluating...
2019-02-15 14:25:43,126 : 
Dev acc : 26.25 Test acc : 28.64 for             SST Fine-Grained classification

2019-02-15 14:25:43,127 : ***** Transfer task : TREC *****


2019-02-15 14:25:43,141 : loading BERT model bert-base-uncased
2019-02-15 14:25:43,141 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:25:43,162 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:25:43,162 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7nv2k2lz
2019-02-15 14:25:45,605 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:25:51,178 : Computed train embeddings
2019-02-15 14:25:51,505 : Computed test embeddings
2019-02-15 14:25:51,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 14:25:57,047 : [('reg:1e-05', 22.74), ('reg:0.0001', 22.74), ('reg:0.001', 22.74), ('reg:0.01', 22.74)]
2019-02-15 14:25:57,047 : Cross-validation : best param found is reg = 1e-05             with score 22.74
2019-02-15 14:25:57,047 : Evaluating...
2019-02-15 14:25:57,383 : 
Dev acc : 22.74 Test acc : 18.8             for TREC

2019-02-15 14:25:57,384 : ***** Transfer task : MRPC *****


2019-02-15 14:25:57,442 : loading BERT model bert-base-uncased
2019-02-15 14:25:57,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:25:57,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:25:57,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1imyckzk
2019-02-15 14:25:59,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:01,258 : Computing embedding for train
2019-02-15 14:26:14,408 : Computed train embeddings
2019-02-15 14:26:14,408 : Computing embedding for test
2019-02-15 14:26:20,778 : Computed test embeddings
2019-02-15 14:26:20,794 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 14:26:25,868 : [('reg:1e-05', 67.54), ('reg:0.0001', 67.54), ('reg:0.001', 67.54), ('reg:0.01', 67.54)]
2019-02-15 14:26:25,869 : Cross-validation : best param found is reg = 1e-05             with score 67.54
2019-02-15 14:26:25,869 : Evaluating...
2019-02-15 14:26:26,115 : Dev acc : 67.54 Test acc 66.49; Test F1 79.87 for MRPC.

2019-02-15 14:26:26,115 : ***** Transfer task : SICK-Entailment*****


2019-02-15 14:26:26,140 : loading BERT model bert-base-uncased
2019-02-15 14:26:26,141 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:26:26,163 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:26:26,163 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp54_uhd4h
2019-02-15 14:26:28,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:30,186 : Computing embedding for train
2019-02-15 14:26:37,716 : Computed train embeddings
2019-02-15 14:26:37,716 : Computing embedding for dev
2019-02-15 14:26:38,511 : Computed dev embeddings
2019-02-15 14:26:38,511 : Computing embedding for test
2019-02-15 14:26:45,221 : Computed test embeddings
2019-02-15 14:26:45,249 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:26:46,829 : [('reg:1e-05', 56.4), ('reg:0.0001', 56.4), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-15 14:26:46,829 : Validation : best param found is reg = 1e-05 with score             56.4
2019-02-15 14:26:46,829 : Evaluating...
2019-02-15 14:26:47,235 : 
Dev acc : 56.4 Test acc : 56.69 for                        SICK entailment

2019-02-15 14:26:47,236 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 14:26:47,264 : loading BERT model bert-base-uncased
2019-02-15 14:26:47,264 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:26:47,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:26:47,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6ng0yr89
2019-02-15 14:26:49,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:26:51,171 : Computing embedding for train
2019-02-15 14:26:59,869 : Computed train embeddings
2019-02-15 14:26:59,869 : Computing embedding for dev
2019-02-15 14:27:01,338 : Computed dev embeddings
2019-02-15 14:27:01,338 : Computing embedding for test
2019-02-15 14:27:11,754 : Computed test embeddings
2019-02-15 14:27:37,353 : Dev : Pearson 0
2019-02-15 14:27:37,353 : Test : Pearson 0 Spearman 0 MSE 1.020079206584388                        for SICK Relatedness

2019-02-15 14:27:37,354 : 

***** Transfer task : STSBenchmark*****


2019-02-15 14:27:37,441 : loading BERT model bert-base-uncased
2019-02-15 14:27:37,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:27:37,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:27:37,462 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz2cez9_d
2019-02-15 14:27:39,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:27:41,368 : Computing embedding for train
2019-02-15 14:27:52,993 : Computed train embeddings
2019-02-15 14:27:52,993 : Computing embedding for dev
2019-02-15 14:27:56,180 : Computed dev embeddings
2019-02-15 14:27:56,180 : Computing embedding for test
2019-02-15 14:27:59,090 : Computed test embeddings
2019-02-15 14:28:25,874 : Dev : Pearson 0
2019-02-15 14:28:25,874 : Test : Pearson 0 Spearman 0 MSE 2.474329571821447                        for SICK Relatedness

2019-02-15 14:28:25,874 : ***** Transfer task : SNLI Entailment*****


2019-02-15 14:28:30,822 : loading BERT model bert-base-uncased
2019-02-15 14:28:30,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:28:30,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:28:30,950 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw7jnm4iy
2019-02-15 14:28:33,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:28:35,079 : PROGRESS (encoding): 0.00%
2019-02-15 14:30:15,825 : PROGRESS (encoding): 14.56%
2019-02-15 14:32:22,653 : PROGRESS (encoding): 29.12%
2019-02-15 14:34:32,002 : PROGRESS (encoding): 43.69%
2019-02-15 14:36:50,728 : PROGRESS (encoding): 58.25%
2019-02-15 14:39:19,738 : PROGRESS (encoding): 72.81%
2019-02-15 14:41:50,502 : PROGRESS (encoding): 87.37%
2019-02-15 14:44:22,727 : PROGRESS (encoding): 0.00%
2019-02-15 14:44:43,507 : PROGRESS (encoding): 0.00%
2019-02-15 14:45:03,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 14:46:08,659 : [('reg:1e-09', 33.82)]
2019-02-15 14:46:08,659 : Validation : best param found is reg = 1e-09 with score             33.82
2019-02-15 14:46:08,659 : Evaluating...
2019-02-15 14:46:51,324 : Dev acc : 33.82 Test acc : 34.28 for SNLI

2019-02-15 14:46:51,324 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 14:47:00,255 : loading BERT model bert-base-uncased
2019-02-15 14:47:00,255 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 14:47:00,323 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 14:47:00,323 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4i6rnvt0
2019-02-15 14:47:02,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 14:47:04,227 : Computing embedding for train
2019-02-15 14:57:57,642 : Computed train embeddings
2019-02-15 14:57:57,642 : Computing embedding for dev
2019-02-15 14:58:27,663 : Computed dev embeddings
2019-02-15 14:58:27,663 : Computing embedding for test
2019-02-15 14:58:58,836 : Computed test embeddings
2019-02-15 14:58:58,854 : prepare data
2019-02-15 14:58:58,919 : start epoch
2019-02-15 14:59:44,212 : samples : 64000
2019-02-15 14:59:51,550 : Image to text: 0.04, 0.08, 0.1, 12480.0
2019-02-15 15:00:01,984 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:00:45,935 : samples : 128000
2019-02-15 15:00:50,598 : Image to text: 0.02, 0.08, 0.1, 12498.0
2019-02-15 15:00:57,977 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:01:41,722 : samples : 192000
2019-02-15 15:01:46,477 : Image to text: 0.02, 0.06, 0.08, 12484.0
2019-02-15 15:01:54,021 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:02:37,143 : samples : 256000
2019-02-15 15:02:41,911 : Image to text: 0.02, 0.06, 0.06, 12497.0
2019-02-15 15:02:49,384 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:03:32,126 : samples : 320000
2019-02-15 15:03:36,929 : Image to text: 0.02, 0.04, 0.06, 12481.0
2019-02-15 15:03:44,529 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:04:27,802 : samples : 384000
2019-02-15 15:04:32,116 : Image to text: 0.02, 0.06, 0.08, 12503.0
2019-02-15 15:04:39,444 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:05:21,423 : samples : 448000
2019-02-15 15:05:26,323 : Image to text: 0.04, 0.1, 0.1, 12475.0
2019-02-15 15:05:33,622 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:06:16,132 : samples : 512000
2019-02-15 15:06:21,197 : Image to text: 0.0, 0.04, 0.08, 12476.0
2019-02-15 15:06:28,824 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:07:05,645 : Epoch 1 finished
2019-02-15 15:07:05,853 : Image to text: 0.1, 0.3, 0.3, 2497.0
2019-02-15 15:07:06,076 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:06,289 : Image to text: 0.1, 0.3, 0.3, 2494.0
2019-02-15 15:07:06,526 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:06,745 : Image to text: 0.2, 0.4, 0.5, 2491.0
2019-02-15 15:07:06,962 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:07,189 : Image to text: 0.2, 0.4, 0.5, 2491.0
2019-02-15 15:07:07,411 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:07,628 : Image to text: 0.1, 0.3, 0.4, 2491.0
2019-02-15 15:07:07,851 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:07,851 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:07:07,851 : Dev mean Image to text: 0.13999999999999999, 0.34, 0.4, 2492.8
2019-02-15 15:07:07,851 : start epoch
2019-02-15 15:07:51,246 : samples : 64000
2019-02-15 15:07:55,989 : Image to text: 0.04, 0.08, 0.1, 12488.0
2019-02-15 15:08:03,585 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:08:47,017 : samples : 128000
2019-02-15 15:08:51,824 : Image to text: 0.02, 0.06, 0.08, 12486.0
2019-02-15 15:08:59,428 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:09:43,157 : samples : 192000
2019-02-15 15:09:47,929 : Image to text: 0.0, 0.04, 0.06, 12488.0
2019-02-15 15:09:55,470 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:10:38,870 : samples : 256000
2019-02-15 15:10:43,624 : Image to text: 0.0, 0.04, 0.06, 12502.0
2019-02-15 15:10:51,127 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:11:34,695 : samples : 320000
2019-02-15 15:11:39,462 : Image to text: 0.0, 0.06, 0.08, 12497.0
2019-02-15 15:11:47,024 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:12:30,497 : samples : 384000
2019-02-15 15:12:35,284 : Image to text: 0.02, 0.04, 0.06, 12473.0
2019-02-15 15:12:42,844 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:13:25,865 : samples : 448000
2019-02-15 15:13:30,630 : Image to text: 0.02, 0.04, 0.04, 12494.0
2019-02-15 15:13:38,243 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:14:20,986 : samples : 512000
2019-02-15 15:14:25,498 : Image to text: 0.02, 0.06, 0.1, 12495.0
2019-02-15 15:14:33,071 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:15:10,345 : Epoch 2 finished
2019-02-15 15:15:10,590 : Image to text: 0.1, 0.3, 0.3, 2491.0
2019-02-15 15:15:10,855 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:11,097 : Image to text: 0.1, 0.3, 0.5, 2492.0
2019-02-15 15:15:11,365 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:11,610 : Image to text: 0.1, 0.3, 0.4, 2498.0
2019-02-15 15:15:11,886 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:12,133 : Image to text: 0.1, 0.3, 0.4, 2491.0
2019-02-15 15:15:12,397 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:12,640 : Image to text: 0.1, 0.3, 0.5, 2491.0
2019-02-15 15:15:12,904 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:12,904 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:15:12,904 : Dev mean Image to text: 0.1, 0.3, 0.42000000000000004, 2492.6
2019-02-15 15:15:12,904 : start epoch
2019-02-15 15:15:56,036 : samples : 64000
2019-02-15 15:16:00,827 : Image to text: 0.02, 0.06, 0.08, 12493.0
2019-02-15 15:16:08,316 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:16:51,661 : samples : 128000
2019-02-15 15:16:56,423 : Image to text: 0.02, 0.02, 0.04, 12495.0
2019-02-15 15:17:03,937 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:17:47,049 : samples : 192000
2019-02-15 15:17:51,821 : Image to text: 0.02, 0.08, 0.1, 12496.0
2019-02-15 15:17:59,306 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:18:43,158 : samples : 256000
2019-02-15 15:18:47,925 : Image to text: 0.02, 0.04, 0.04, 12500.0
2019-02-15 15:18:55,440 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:19:38,643 : samples : 320000
2019-02-15 15:19:43,456 : Image to text: 0.02, 0.06, 0.1, 12477.0
2019-02-15 15:19:50,962 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:20:35,120 : samples : 384000
2019-02-15 15:20:39,885 : Image to text: 0.04, 0.1, 0.12, 12481.0
2019-02-15 15:20:47,473 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:21:31,565 : samples : 448000
2019-02-15 15:21:36,401 : Image to text: 0.02, 0.06, 0.08, 12496.0
2019-02-15 15:21:43,922 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:22:30,311 : samples : 512000
2019-02-15 15:22:36,075 : Image to text: 0.02, 0.06, 0.08, 12481.0
2019-02-15 15:22:44,851 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:23:28,558 : Epoch 3 finished
2019-02-15 15:23:28,792 : Image to text: 0.1, 0.2, 0.4, 2491.0
2019-02-15 15:23:29,023 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:29,240 : Image to text: 0.1, 0.4, 0.5, 2484.0
2019-02-15 15:23:29,470 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:29,676 : Image to text: 0.2, 0.3, 0.5, 2491.0
2019-02-15 15:23:29,895 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:30,104 : Image to text: 0.1, 0.3, 0.4, 2491.0
2019-02-15 15:23:30,322 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:30,532 : Image to text: 0.1, 0.4, 0.6, 2485.0
2019-02-15 15:23:30,746 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:30,746 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:23:30,746 : Dev mean Image to text: 0.12000000000000001, 0.32, 0.48000000000000004, 2488.4
2019-02-15 15:23:30,746 : start epoch
2019-02-15 15:24:13,270 : samples : 64000
2019-02-15 15:24:18,236 : Image to text: 0.02, 0.06, 0.08, 12492.0
2019-02-15 15:24:26,104 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:25:08,848 : samples : 128000
2019-02-15 15:25:13,643 : Image to text: 0.02, 0.06, 0.06, 12489.0
2019-02-15 15:25:21,106 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:26:04,272 : samples : 192000
2019-02-15 15:26:09,131 : Image to text: 0.0, 0.06, 0.08, 12498.0
2019-02-15 15:26:16,655 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:26:59,932 : samples : 256000
2019-02-15 15:27:04,648 : Image to text: 0.04, 0.06, 0.08, 12486.0
2019-02-15 15:27:12,163 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:27:54,496 : samples : 320000
2019-02-15 15:27:59,296 : Image to text: 0.04, 0.06, 0.08, 12487.0
2019-02-15 15:28:07,376 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:28:51,225 : samples : 384000
2019-02-15 15:28:56,048 : Image to text: 0.0, 0.06, 0.1, 12487.0
2019-02-15 15:29:03,597 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:29:47,200 : samples : 448000
2019-02-15 15:29:51,972 : Image to text: 0.04, 0.08, 0.1, 12484.0
2019-02-15 15:29:59,454 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:30:42,422 : samples : 512000
2019-02-15 15:30:47,197 : Image to text: 0.0, 0.04, 0.06, 12503.0
2019-02-15 15:30:54,750 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:31:31,342 : Epoch 4 finished
2019-02-15 15:31:31,567 : Image to text: 0.2, 0.4, 0.5, 2492.0
2019-02-15 15:31:31,805 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:32,030 : Image to text: 0.1, 0.3, 0.3, 2491.0
2019-02-15 15:31:32,255 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:32,476 : Image to text: 0.1, 0.2, 0.3, 2491.0
2019-02-15 15:31:32,711 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:32,927 : Image to text: 0.1, 0.3, 0.4, 2490.0
2019-02-15 15:31:33,168 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:33,398 : Image to text: 0.1, 0.2, 0.5, 2491.0
2019-02-15 15:31:33,621 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:33,621 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:31:33,621 : Dev mean Image to text: 0.12000000000000001, 0.28, 0.4, 2491.0
2019-02-15 15:31:33,621 : start epoch
2019-02-15 15:32:16,921 : samples : 64000
2019-02-15 15:32:21,337 : Image to text: 0.04, 0.1, 0.1, 12499.0
2019-02-15 15:32:28,760 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:33:12,072 : samples : 128000
2019-02-15 15:33:17,134 : Image to text: 0.02, 0.1, 0.12, 12502.0
2019-02-15 15:33:24,529 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:34:07,964 : samples : 192000
2019-02-15 15:34:13,112 : Image to text: 0.02, 0.06, 0.06, 12502.0
2019-02-15 15:34:20,615 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:35:04,170 : samples : 256000
2019-02-15 15:35:09,026 : Image to text: 0.02, 0.08, 0.1, 12492.0
2019-02-15 15:35:16,532 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:35:59,464 : samples : 320000
2019-02-15 15:36:04,235 : Image to text: 0.04, 0.06, 0.08, 12507.0
2019-02-15 15:36:11,706 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:36:54,920 : samples : 384000
2019-02-15 15:37:00,054 : Image to text: 0.02, 0.04, 0.04, 12480.0
2019-02-15 15:37:07,370 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:37:50,668 : samples : 448000
2019-02-15 15:37:55,699 : Image to text: 0.0, 0.02, 0.02, 12501.0
2019-02-15 15:38:02,947 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:38:46,788 : samples : 512000
2019-02-15 15:38:51,582 : Image to text: 0.04, 0.08, 0.08, 12481.0
2019-02-15 15:38:59,020 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:39:36,802 : Epoch 5 finished
2019-02-15 15:39:37,107 : Image to text: 0.1, 0.2, 0.3, 2500.0
2019-02-15 15:39:37,448 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:37,763 : Image to text: 0.1, 0.3, 0.4, 2491.0
2019-02-15 15:39:38,107 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:38,418 : Image to text: 0.1, 0.4, 0.5, 2485.0
2019-02-15 15:39:38,663 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:38,935 : Image to text: 0.1, 0.3, 0.3, 2490.0
2019-02-15 15:39:39,182 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:39,457 : Image to text: 0.1, 0.3, 0.3, 2493.0
2019-02-15 15:39:39,752 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:39,752 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:39:39,753 : Dev mean Image to text: 0.1, 0.3, 0.36000000000000004, 2491.8
2019-02-15 15:39:39,753 : start epoch
2019-02-15 15:40:33,436 : samples : 64000
2019-02-15 15:40:38,166 : Image to text: 0.02, 0.06, 0.08, 12481.0
2019-02-15 15:40:45,758 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:41:29,017 : samples : 128000
2019-02-15 15:41:33,781 : Image to text: 0.02, 0.1, 0.1, 12484.0
2019-02-15 15:41:41,317 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:42:24,085 : samples : 192000
2019-02-15 15:42:28,687 : Image to text: 0.0, 0.06, 0.06, 12500.0
2019-02-15 15:42:36,180 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:43:19,679 : samples : 256000
2019-02-15 15:43:24,241 : Image to text: 0.04, 0.06, 0.06, 12499.0
2019-02-15 15:43:31,611 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:44:14,895 : samples : 320000
2019-02-15 15:44:19,992 : Image to text: 0.02, 0.06, 0.08, 12494.0
2019-02-15 15:44:27,385 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:45:10,165 : samples : 384000
2019-02-15 15:45:14,933 : Image to text: 0.02, 0.06, 0.1, 12476.0
2019-02-15 15:45:22,367 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:46:05,385 : samples : 448000
2019-02-15 15:46:09,865 : Image to text: 0.04, 0.04, 0.06, 12489.0
2019-02-15 15:46:17,364 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:47:00,909 : samples : 512000
2019-02-15 15:47:05,904 : Image to text: 0.02, 0.06, 0.08, 12488.0
2019-02-15 15:47:13,297 : Text to Image: 0.02, 0.1, 0.2, 2500.0
2019-02-15 15:47:50,173 : Epoch 6 finished
2019-02-15 15:47:50,406 : Image to text: 0.1, 0.3, 0.4, 2498.0
2019-02-15 15:47:50,639 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:50,854 : Image to text: 0.2, 0.3, 0.6, 2491.0
2019-02-15 15:47:51,084 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:51,296 : Image to text: 0.1, 0.3, 0.4, 2491.0
2019-02-15 15:47:51,525 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:51,750 : Image to text: 0.1, 0.2, 0.2, 2491.0
2019-02-15 15:47:51,983 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:52,194 : Image to text: 0.1, 0.3, 0.4, 2494.0
2019-02-15 15:47:52,417 : Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:52,417 : Dev mean Text to Image: 0.1, 0.5, 1.0, 500.0
2019-02-15 15:47:52,418 : Dev mean Image to text: 0.12000000000000001, 0.28, 0.4, 2493.0
2019-02-15 15:47:54,655 : 
Test scores | Image to text:             0.06, 0.26, 0.38, 2490.7999999999997
2019-02-15 15:47:54,655 : Test scores | Text to image:             0.1, 0.5, 1.0, 500.0

2019-02-15 15:47:54,750 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 15:47:54,977 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 15:47:55,656 : loading BERT model bert-base-uncased
2019-02-15 15:47:55,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:47:55,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:47:55,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi7kz0l3h
2019-02-15 15:47:58,164 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:47:59,622 : Computing embeddings for train/dev/test
2019-02-15 15:49:34,586 : Computed embeddings
2019-02-15 15:49:34,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:50:10,082 : [('reg:1e-05', 16.67), ('reg:0.0001', 16.67), ('reg:0.001', 16.67), ('reg:0.01', 16.67)]
2019-02-15 15:50:10,082 : Validation : best param found is reg = 1e-05 with score             16.67
2019-02-15 15:50:10,082 : Evaluating...
2019-02-15 15:50:18,111 : 
Dev acc : 16.7 Test acc : 16.7 for LENGTH classification

2019-02-15 15:50:18,112 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 15:50:18,467 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 15:50:18,517 : loading BERT model bert-base-uncased
2019-02-15 15:50:18,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:50:18,547 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:50:18,547 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph0uuvs6e
2019-02-15 15:50:21,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:50:22,510 : Computing embeddings for train/dev/test
2019-02-15 15:51:50,744 : Computed embeddings
2019-02-15 15:51:50,744 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:52:26,342 : [('reg:1e-05', 0.1), ('reg:0.0001', 0.1), ('reg:0.001', 0.1), ('reg:0.01', 0.1)]
2019-02-15 15:52:26,342 : Validation : best param found is reg = 1e-05 with score             0.1
2019-02-15 15:52:26,343 : Evaluating...
2019-02-15 15:52:34,670 : 
Dev acc : 0.1 Test acc : 0.1 for WORDCONTENT classification

2019-02-15 15:52:34,671 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 15:52:35,026 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 15:52:35,105 : loading BERT model bert-base-uncased
2019-02-15 15:52:35,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:52:35,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:52:35,231 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3e_3s3cd
2019-02-15 15:52:37,720 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:52:39,203 : Computing embeddings for train/dev/test
2019-02-15 15:54:01,470 : Computed embeddings
2019-02-15 15:54:01,471 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:54:35,970 : [('reg:1e-05', 18.07), ('reg:0.0001', 18.07), ('reg:0.001', 18.07), ('reg:0.01', 18.07)]
2019-02-15 15:54:35,971 : Validation : best param found is reg = 1e-05 with score             18.07
2019-02-15 15:54:35,971 : Evaluating...
2019-02-15 15:54:44,881 : 
Dev acc : 18.1 Test acc : 17.9 for DEPTH classification

2019-02-15 15:54:44,882 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 15:54:45,296 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 15:54:45,366 : loading BERT model bert-base-uncased
2019-02-15 15:54:45,366 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:54:45,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:54:45,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1vgmp8ky
2019-02-15 15:54:48,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:54:49,450 : Computing embeddings for train/dev/test
2019-02-15 15:56:07,396 : Computed embeddings
2019-02-15 15:56:07,396 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:56:34,867 : [('reg:1e-05', 5.0), ('reg:0.0001', 5.0), ('reg:0.001', 5.0), ('reg:0.01', 5.0)]
2019-02-15 15:56:34,867 : Validation : best param found is reg = 1e-05 with score             5.0
2019-02-15 15:56:34,867 : Evaluating...
2019-02-15 15:56:40,343 : 
Dev acc : 5.0 Test acc : 5.0 for TOPCONSTITUENTS classification

2019-02-15 15:56:40,344 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 15:56:41,066 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 15:56:41,154 : loading BERT model bert-base-uncased
2019-02-15 15:56:41,154 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:56:41,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:56:41,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf20fxwm0
2019-02-15 15:56:44,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:56:46,408 : Computing embeddings for train/dev/test
2019-02-15 15:58:13,834 : Computed embeddings
2019-02-15 15:58:13,834 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 15:58:47,984 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 15:58:47,985 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 15:58:47,985 : Evaluating...
2019-02-15 15:58:57,057 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-15 15:58:57,059 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 15:58:57,667 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 15:58:57,738 : loading BERT model bert-base-uncased
2019-02-15 15:58:57,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 15:58:57,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 15:58:57,775 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmtm38oiu
2019-02-15 15:59:00,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 15:59:01,697 : Computing embeddings for train/dev/test
2019-02-15 16:00:23,779 : Computed embeddings
2019-02-15 16:00:23,779 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:00:55,843 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 16:00:55,844 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 16:00:55,844 : Evaluating...
2019-02-15 16:01:03,433 : 
Dev acc : 50.0 Test acc : 50.0 for TENSE classification

2019-02-15 16:01:03,434 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 16:01:03,897 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 16:01:03,969 : loading BERT model bert-base-uncased
2019-02-15 16:01:03,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:01:04,000 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:01:04,000 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi8_ftafa
2019-02-15 16:01:06,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:01:07,954 : Computing embeddings for train/dev/test
2019-02-15 16:02:34,726 : Computed embeddings
2019-02-15 16:02:34,726 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:03:07,787 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 16:03:07,787 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 16:03:07,787 : Evaluating...
2019-02-15 16:03:16,426 : 
Dev acc : 50.0 Test acc : 50.0 for SUBJNUMBER classification

2019-02-15 16:03:16,428 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 16:03:16,880 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 16:03:16,957 : loading BERT model bert-base-uncased
2019-02-15 16:03:16,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:03:17,095 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:03:17,095 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpof4igwax
2019-02-15 16:03:19,582 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:03:21,054 : Computing embeddings for train/dev/test
2019-02-15 16:04:47,074 : Computed embeddings
2019-02-15 16:04:47,074 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:05:20,706 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 16:05:20,707 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 16:05:20,707 : Evaluating...
2019-02-15 16:05:29,403 : 
Dev acc : 50.0 Test acc : 50.0 for OBJNUMBER classification

2019-02-15 16:05:29,404 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 16:05:29,828 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 16:05:29,904 : loading BERT model bert-base-uncased
2019-02-15 16:05:29,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:05:30,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:05:30,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmhfv607f
2019-02-15 16:05:32,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:05:34,014 : Computing embeddings for train/dev/test
2019-02-15 16:07:12,711 : Computed embeddings
2019-02-15 16:07:12,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:07:49,055 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-15 16:07:49,055 : Validation : best param found is reg = 1e-05 with score             50.19
2019-02-15 16:07:49,055 : Evaluating...
2019-02-15 16:07:57,953 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-15 16:07:57,954 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 16:07:58,572 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 16:07:58,658 : loading BERT model bert-base-uncased
2019-02-15 16:07:58,658 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:07:58,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:07:58,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsu2_la3u
2019-02-15 16:08:01,206 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:08:02,651 : Computing embeddings for train/dev/test
2019-02-15 16:09:39,336 : Computed embeddings
2019-02-15 16:09:39,337 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:10:13,417 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 16:10:13,417 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 16:10:13,417 : Evaluating...
2019-02-15 16:10:21,856 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-02-15 16:10:21,858 : total results: {'STS12': {'MSRpar': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'MSRvid': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 399}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS13': {'FNWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 189}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 561}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS14': {'deft-forum': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 450}, 'deft-news': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 300}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'OnWN': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'tweet-news': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS15': {'answers-forums': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'answers-students': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'belief': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 375}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'images': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 750}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'STS16': {'answer-answer': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 254}, 'headlines': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 249}, 'plagiarism': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 230}, 'postediting': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 244}, 'question-question': {'pearson': (nan, 1.0), 'spearman': SpearmanrResult(correlation=nan, pvalue=nan), 'nsamples': 209}, 'all': {'pearson': {'mean': nan, 'wmean': nan}, 'spearman': {'mean': nan, 'wmean': nan}}}, 'MR': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 63.76, 'acc': 63.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 65.77, 'acc': 68.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 50.92, 'acc': 49.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 26.25, 'acc': 28.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 22.74, 'acc': 18.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.54, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 56.4, 'acc': 56.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 1.020079206584388, 'yhat': array([3.57972671, 3.57972671, 3.57972671, ..., 3.57972671, 3.57972671,        3.57972671]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0, 'pearson': 0, 'spearman': 0, 'mse': 2.474329571821447, 'yhat': array([2.99368074, 2.99368074, 2.99368074, ..., 2.99368074, 2.99368074,        2.99368074]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 33.82, 'acc': 34.28, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 2.52, 'acc': [(0.06, 0.26, 0.38, 2490.7999999999997), (0.1, 0.5, 1.0, 500.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 16.67, 'acc': 16.67, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.1, 'acc': 0.1, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 18.07, 'acc': 17.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 5.0, 'acc': 5.0, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 16:10:21,858 : STS12 p=nan, STS12 s=nan, STS13 p=nan, STS13 s=nan, STS14 p=nan, STS14 s=nan, STS15 p=nan, STS15 s=nan, STS 16 p=nan, STS16 s=nan, STS B p=0.0000, STS B s=0.0000, STS B m=2.4743, SICK-R p=0.0000, SICK-R s=0.0000, SICK-P m=1.0201
2019-02-15 16:10:21,858 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 16:10:21,858 : nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,0.0000,0.0000,2.4743,0.0000,0.0000,1.0201
2019-02-15 16:10:21,858 : MR=50.00, CR=63.76, SUBJ=50.00, MPQA=68.77, SST-B=49.92, SST-F=28.64, TREC=18.80, SICK-E=56.69, SNLI=34.28, MRPC=66.49, MRPC f=79.87
2019-02-15 16:10:21,858 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 16:10:21,858 : 50.00,63.76,50.00,68.77,49.92,28.64,18.80,56.69,34.28,66.49,79.87
2019-02-15 16:10:21,858 : COCO r1i2t=0.06, COCO r5i2t=0.26, COCO r10i2t=0.38, COCO medr_i2t=2490.80, COCO r1t2i=0.10, COCO r5t2i=0.50, COCO r10t2i=1.00, COCO medr_t2i=500.00
2019-02-15 16:10:21,858 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 16:10:21,858 : 0.06,0.26,0.38,2490.80,0.10,0.50,1.00,500.00
2019-02-15 16:10:21,858 : SentLen=16.67, WC=0.10, TreeDepth=17.88, TopConst=5.00, BShift=50.00, Tense=50.00, SubjNum=50.00, ObjNum=50.00, SOMO=49.87, CoordInv=50.00, average=33.95
2019-02-15 16:10:21,858 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 16:10:21,858 : 16.67,0.10,17.88,5.00,50.00,50.00,50.00,50.00,49.87,50.00,33.95
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,858 : layer 1
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,858 : ********************************************************************************
2019-02-15 16:10:21,945 : ***** Transfer task : STS12 *****


2019-02-15 16:10:21,957 : loading BERT model bert-base-uncased
2019-02-15 16:10:21,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:10:21,975 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:10:21,975 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6m7jpoa5
2019-02-15 16:10:24,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:10:27,741 : MSRpar : pearson = 0.1038, spearman = 0.1476
2019-02-15 16:10:28,508 : MSRvid : pearson = 0.1145, spearman = 0.1538
2019-02-15 16:10:29,125 : SMTeuroparl : pearson = 0.2852, spearman = 0.5018
2019-02-15 16:10:30,315 : surprise.OnWN : pearson = 0.0625, spearman = 0.1267
2019-02-15 16:10:30,967 : surprise.SMTnews : pearson = 0.3811, spearman = 0.2912
2019-02-15 16:10:30,967 : ALL (weighted average) : Pearson = 0.1588,             Spearman = 0.2148
2019-02-15 16:10:30,967 : ALL (average) : Pearson = 0.1894,             Spearman = 0.2442

2019-02-15 16:10:30,967 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 16:10:30,976 : loading BERT model bert-base-uncased
2019-02-15 16:10:30,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:10:30,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:10:30,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0wg0whx8
2019-02-15 16:10:33,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:10:35,495 : FNWN : pearson = 0.1196, spearman = 0.1837
2019-02-15 16:10:36,392 : headlines : pearson = 0.0731, spearman = 0.2303
2019-02-15 16:10:37,062 : OnWN : pearson = 0.0764, spearman = 0.0332
2019-02-15 16:10:37,062 : ALL (weighted average) : Pearson = 0.0802,             Spearman = 0.1507
2019-02-15 16:10:37,062 : ALL (average) : Pearson = 0.0897,             Spearman = 0.1491

2019-02-15 16:10:37,062 : ***** Transfer task : STS14 *****


2019-02-15 16:10:37,079 : loading BERT model bert-base-uncased
2019-02-15 16:10:37,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:10:37,099 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:10:37,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq6a1yyhv
2019-02-15 16:10:39,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:10:41,753 : deft-forum : pearson = 0.0830, spearman = 0.1252
2019-02-15 16:10:42,445 : deft-news : pearson = 0.2601, spearman = 0.4118
2019-02-15 16:10:43,453 : headlines : pearson = 0.0891, spearman = 0.1901
2019-02-15 16:10:44,422 : images : pearson = 0.0273, spearman = 0.1951
2019-02-15 16:10:45,393 : OnWN : pearson = 0.0961, spearman = 0.1133
2019-02-15 16:10:46,623 : tweet-news : pearson = 0.1473, spearman = 0.2135
2019-02-15 16:10:46,623 : ALL (weighted average) : Pearson = 0.1027,             Spearman = 0.1904
2019-02-15 16:10:46,623 : ALL (average) : Pearson = 0.1172,             Spearman = 0.2082

2019-02-15 16:10:46,623 : ***** Transfer task : STS15 *****


2019-02-15 16:10:46,659 : loading BERT model bert-base-uncased
2019-02-15 16:10:46,659 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:10:46,680 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:10:46,680 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpglns6_72
2019-02-15 16:10:49,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:10:51,533 : answers-forums : pearson = 0.1216, spearman = 0.1637
2019-02-15 16:10:52,460 : answers-students : pearson = 0.2266, spearman = 0.2630
2019-02-15 16:10:53,334 : belief : pearson = 0.0679, spearman = 0.1827
2019-02-15 16:10:54,342 : headlines : pearson = 0.1860, spearman = 0.3372
2019-02-15 16:10:55,324 : images : pearson = -0.0243, spearman = 0.2210
2019-02-15 16:10:55,325 : ALL (weighted average) : Pearson = 0.1207,             Spearman = 0.2486
2019-02-15 16:10:55,325 : ALL (average) : Pearson = 0.1155,             Spearman = 0.2335

2019-02-15 16:10:55,325 : ***** Transfer task : STS16 *****


2019-02-15 16:10:55,398 : loading BERT model bert-base-uncased
2019-02-15 16:10:55,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:10:55,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:10:55,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb9oqcm4m
2019-02-15 16:10:57,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:10:59,769 : answer-answer : pearson = 0.0996, spearman = 0.2157
2019-02-15 16:11:00,100 : headlines : pearson = 0.2045, spearman = 0.3990
2019-02-15 16:11:00,507 : plagiarism : pearson = 0.1424, spearman = 0.3428
2019-02-15 16:11:01,136 : postediting : pearson = 0.1802, spearman = 0.6355
2019-02-15 16:11:01,413 : question-question : pearson = -0.1246, spearman = -0.1493
2019-02-15 16:11:01,413 : ALL (weighted average) : Pearson = 0.1070,             Spearman = 0.3009
2019-02-15 16:11:01,413 : ALL (average) : Pearson = 0.1004,             Spearman = 0.2888

2019-02-15 16:11:01,413 : ***** Transfer task : MR *****


2019-02-15 16:11:01,432 : loading BERT model bert-base-uncased
2019-02-15 16:11:01,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:11:01,457 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:11:01,457 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqmi_g7bt
2019-02-15 16:11:03,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:11:05,482 : Generating sentence embeddings
2019-02-15 16:11:20,091 : Generated sentence embeddings
2019-02-15 16:11:20,092 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 16:11:37,013 : Best param found at split 1: l2reg = 1e-05                 with score 57.9
2019-02-15 16:11:55,143 : Best param found at split 2: l2reg = 1e-05                 with score 57.21
2019-02-15 16:12:12,257 : Best param found at split 3: l2reg = 1e-05                 with score 56.86
2019-02-15 16:12:28,225 : Best param found at split 4: l2reg = 1e-05                 with score 53.77
2019-02-15 16:12:46,203 : Best param found at split 5: l2reg = 1e-05                 with score 57.67
2019-02-15 16:12:47,043 : Dev acc : 56.68 Test acc : 54.12

2019-02-15 16:12:47,044 : ***** Transfer task : CR *****


2019-02-15 16:12:47,052 : loading BERT model bert-base-uncased
2019-02-15 16:12:47,052 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:12:47,073 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:12:47,074 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpibgsk0rg
2019-02-15 16:12:49,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:12:51,060 : Generating sentence embeddings
2019-02-15 16:12:54,753 : Generated sentence embeddings
2019-02-15 16:12:54,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 16:12:59,944 : Best param found at split 1: l2reg = 0.01                 with score 65.55
2019-02-15 16:13:04,306 : Best param found at split 2: l2reg = 0.001                 with score 64.46
2019-02-15 16:13:10,308 : Best param found at split 3: l2reg = 1e-05                 with score 65.86
2019-02-15 16:13:15,580 : Best param found at split 4: l2reg = 0.001                 with score 65.74
2019-02-15 16:13:19,399 : Best param found at split 5: l2reg = 0.001                 with score 65.18
2019-02-15 16:13:19,704 : Dev acc : 65.36 Test acc : 64.16

2019-02-15 16:13:19,704 : ***** Transfer task : MPQA *****


2019-02-15 16:13:19,710 : loading BERT model bert-base-uncased
2019-02-15 16:13:19,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:13:19,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:13:19,731 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwj1ysj3q
2019-02-15 16:13:22,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:13:23,578 : Generating sentence embeddings
2019-02-15 16:13:27,290 : Generated sentence embeddings
2019-02-15 16:13:27,291 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 16:13:37,372 : Best param found at split 1: l2reg = 0.0001                 with score 80.6
2019-02-15 16:13:48,257 : Best param found at split 2: l2reg = 0.001                 with score 80.29
2019-02-15 16:14:00,532 : Best param found at split 3: l2reg = 0.001                 with score 79.26
2019-02-15 16:14:13,628 : Best param found at split 4: l2reg = 1e-05                 with score 81.15
2019-02-15 16:14:26,608 : Best param found at split 5: l2reg = 1e-05                 with score 80.05
2019-02-15 16:14:27,387 : Dev acc : 80.27 Test acc : 81.85

2019-02-15 16:14:27,388 : ***** Transfer task : SUBJ *****


2019-02-15 16:14:27,409 : loading BERT model bert-base-uncased
2019-02-15 16:14:27,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:14:27,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:14:27,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq6x05ia3
2019-02-15 16:14:30,043 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:14:31,489 : Generating sentence embeddings
2019-02-15 16:14:44,673 : Generated sentence embeddings
2019-02-15 16:14:44,674 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 16:15:03,402 : Best param found at split 1: l2reg = 1e-05                 with score 80.02
2019-02-15 16:15:23,465 : Best param found at split 2: l2reg = 1e-05                 with score 81.14
2019-02-15 16:15:44,489 : Best param found at split 3: l2reg = 1e-05                 with score 81.91
2019-02-15 16:16:03,264 : Best param found at split 4: l2reg = 1e-05                 with score 80.69
2019-02-15 16:16:21,641 : Best param found at split 5: l2reg = 1e-05                 with score 78.76
2019-02-15 16:16:23,018 : Dev acc : 80.5 Test acc : 82.41

2019-02-15 16:16:23,019 : ***** Transfer task : SST Binary classification *****


2019-02-15 16:16:23,162 : loading BERT model bert-base-uncased
2019-02-15 16:16:23,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:16:23,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:16:23,188 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxon0hptj
2019-02-15 16:16:25,666 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:16:27,124 : Computing embedding for train
2019-02-15 16:17:12,610 : Computed train embeddings
2019-02-15 16:17:12,611 : Computing embedding for dev
2019-02-15 16:17:13,648 : Computed dev embeddings
2019-02-15 16:17:13,648 : Computing embedding for test
2019-02-15 16:17:15,764 : Computed test embeddings
2019-02-15 16:17:15,764 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:17:41,647 : [('reg:1e-05', 68.46), ('reg:0.0001', 68.69), ('reg:0.001', 63.76), ('reg:0.01', 52.64)]
2019-02-15 16:17:41,648 : Validation : best param found is reg = 0.0001 with score             68.69
2019-02-15 16:17:41,648 : Evaluating...
2019-02-15 16:17:47,966 : 
Dev acc : 68.69 Test acc : 70.57 for             SST Binary classification

2019-02-15 16:17:47,966 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 16:17:48,025 : loading BERT model bert-base-uncased
2019-02-15 16:17:48,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:17:48,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:17:48,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2yswsop
2019-02-15 16:17:50,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:17:52,017 : Computing embedding for train
2019-02-15 16:18:01,434 : Computed train embeddings
2019-02-15 16:18:01,435 : Computing embedding for dev
2019-02-15 16:18:02,679 : Computed dev embeddings
2019-02-15 16:18:02,679 : Computing embedding for test
2019-02-15 16:18:05,123 : Computed test embeddings
2019-02-15 16:18:05,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:18:08,211 : [('reg:1e-05', 31.88), ('reg:0.0001', 29.61), ('reg:0.001', 31.97), ('reg:0.01', 26.25)]
2019-02-15 16:18:08,212 : Validation : best param found is reg = 0.001 with score             31.97
2019-02-15 16:18:08,212 : Evaluating...
2019-02-15 16:18:08,898 : 
Dev acc : 31.97 Test acc : 30.72 for             SST Fine-Grained classification

2019-02-15 16:18:08,899 : ***** Transfer task : TREC *****


2019-02-15 16:18:08,912 : loading BERT model bert-base-uncased
2019-02-15 16:18:08,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:18:08,932 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:18:08,933 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxpjbqcgh
2019-02-15 16:18:11,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:18:16,651 : Computed train embeddings
2019-02-15 16:18:16,940 : Computed test embeddings
2019-02-15 16:18:16,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 16:18:28,053 : [('reg:1e-05', 50.34), ('reg:0.0001', 49.77), ('reg:0.001', 48.45), ('reg:0.01', 36.59)]
2019-02-15 16:18:28,053 : Cross-validation : best param found is reg = 1e-05             with score 50.34
2019-02-15 16:18:28,054 : Evaluating...
2019-02-15 16:18:28,551 : 
Dev acc : 50.34 Test acc : 62.2             for TREC

2019-02-15 16:18:28,552 : ***** Transfer task : MRPC *****


2019-02-15 16:18:28,575 : loading BERT model bert-base-uncased
2019-02-15 16:18:28,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:18:28,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:18:28,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjxr22c7h
2019-02-15 16:18:31,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:18:32,602 : Computing embedding for train
2019-02-15 16:18:42,295 : Computed train embeddings
2019-02-15 16:18:42,295 : Computing embedding for test
2019-02-15 16:18:46,443 : Computed test embeddings
2019-02-15 16:18:46,460 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 16:18:54,463 : [('reg:1e-05', 68.65), ('reg:0.0001', 68.52), ('reg:0.001', 68.06), ('reg:0.01', 67.66)]
2019-02-15 16:18:54,463 : Cross-validation : best param found is reg = 1e-05             with score 68.65
2019-02-15 16:18:54,463 : Evaluating...
2019-02-15 16:18:54,959 : Dev acc : 68.65 Test acc 68.93; Test F1 80.03 for MRPC.

2019-02-15 16:18:54,960 : ***** Transfer task : SICK-Entailment*****


2019-02-15 16:18:55,031 : loading BERT model bert-base-uncased
2019-02-15 16:18:55,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:18:55,055 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:18:55,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy2akfogt
2019-02-15 16:18:57,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:18:59,027 : Computing embedding for train
2019-02-15 16:19:04,143 : Computed train embeddings
2019-02-15 16:19:04,143 : Computing embedding for dev
2019-02-15 16:19:04,842 : Computed dev embeddings
2019-02-15 16:19:04,842 : Computing embedding for test
2019-02-15 16:19:10,561 : Computed test embeddings
2019-02-15 16:19:10,589 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:19:13,056 : [('reg:1e-05', 71.8), ('reg:0.0001', 71.8), ('reg:0.001', 71.4), ('reg:0.01', 62.8)]
2019-02-15 16:19:13,056 : Validation : best param found is reg = 1e-05 with score             71.8
2019-02-15 16:19:13,057 : Evaluating...
2019-02-15 16:19:13,520 : 
Dev acc : 71.8 Test acc : 68.95 for                        SICK entailment

2019-02-15 16:19:13,521 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 16:19:13,549 : loading BERT model bert-base-uncased
2019-02-15 16:19:13,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:19:13,609 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:19:13,609 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgv5tig1e
2019-02-15 16:19:16,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:19:17,532 : Computing embedding for train
2019-02-15 16:19:22,757 : Computed train embeddings
2019-02-15 16:19:22,757 : Computing embedding for dev
2019-02-15 16:19:23,422 : Computed dev embeddings
2019-02-15 16:19:23,422 : Computing embedding for test
2019-02-15 16:19:28,839 : Computed test embeddings
2019-02-15 16:20:25,516 : Dev : Pearson 0.697939563530309
2019-02-15 16:20:25,517 : Test : Pearson 0.7213960553443098 Spearman 0.6669834363031751 MSE 0.4894853563904378                        for SICK Relatedness

2019-02-15 16:20:25,517 : 

***** Transfer task : STSBenchmark*****


2019-02-15 16:20:25,564 : loading BERT model bert-base-uncased
2019-02-15 16:20:25,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:20:25,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:20:25,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj9o_wb86
2019-02-15 16:20:28,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:20:29,576 : Computing embedding for train
2019-02-15 16:20:37,754 : Computed train embeddings
2019-02-15 16:20:37,754 : Computing embedding for dev
2019-02-15 16:20:40,200 : Computed dev embeddings
2019-02-15 16:20:40,201 : Computing embedding for test
2019-02-15 16:20:42,140 : Computed test embeddings
2019-02-15 16:21:28,262 : Dev : Pearson 0.5584027781621694
2019-02-15 16:21:28,262 : Test : Pearson 0.4944005510509023 Spearman 0.4807060159651944 MSE 1.9982841152386206                        for SICK Relatedness

2019-02-15 16:21:28,262 : ***** Transfer task : SNLI Entailment*****


2019-02-15 16:21:33,278 : loading BERT model bert-base-uncased
2019-02-15 16:21:33,278 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:21:33,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:21:33,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf_x1s5do
2019-02-15 16:21:35,899 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:21:37,566 : PROGRESS (encoding): 0.00%
2019-02-15 16:22:55,163 : PROGRESS (encoding): 14.56%
2019-02-15 16:24:23,041 : PROGRESS (encoding): 29.12%
2019-02-15 16:25:49,793 : PROGRESS (encoding): 43.69%
2019-02-15 16:27:23,731 : PROGRESS (encoding): 58.25%
2019-02-15 16:29:08,929 : PROGRESS (encoding): 72.81%
2019-02-15 16:30:54,943 : PROGRESS (encoding): 87.37%
2019-02-15 16:32:46,504 : PROGRESS (encoding): 0.00%
2019-02-15 16:33:01,083 : PROGRESS (encoding): 0.00%
2019-02-15 16:33:14,000 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 16:33:56,482 : [('reg:1e-09', 57.69)]
2019-02-15 16:33:56,483 : Validation : best param found is reg = 1e-09 with score             57.69
2019-02-15 16:33:56,483 : Evaluating...
2019-02-15 16:34:39,328 : Dev acc : 57.69 Test acc : 57.7 for SNLI

2019-02-15 16:34:39,328 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 16:34:48,571 : loading BERT model bert-base-uncased
2019-02-15 16:34:48,571 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 16:34:48,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 16:34:48,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcery4yc3
2019-02-15 16:34:51,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 16:34:52,565 : Computing embedding for train
2019-02-15 16:42:19,880 : Computed train embeddings
2019-02-15 16:42:19,880 : Computing embedding for dev
2019-02-15 16:42:39,469 : Computed dev embeddings
2019-02-15 16:42:39,470 : Computing embedding for test
2019-02-15 16:42:59,984 : Computed test embeddings
2019-02-15 16:43:00,001 : prepare data
2019-02-15 16:43:00,070 : start epoch
2019-02-15 16:43:42,894 : samples : 64000
2019-02-15 16:43:53,125 : Image to text: 2.56, 9.24, 15.12, 68.0
2019-02-15 16:44:00,493 : Text to Image: 1.604, 6.14, 10.828, 96.0
2019-02-15 16:44:43,334 : samples : 128000
2019-02-15 16:44:53,567 : Image to text: 2.86, 10.64, 17.48, 56.0
2019-02-15 16:45:00,933 : Text to Image: 2.008, 7.708, 12.912, 80.0
2019-02-15 16:45:44,331 : samples : 192000
2019-02-15 16:45:54,570 : Image to text: 3.68, 13.54, 21.32, 46.0
2019-02-15 16:46:02,024 : Text to Image: 2.616, 9.328, 15.812, 62.0
2019-02-15 16:46:44,804 : samples : 256000
2019-02-15 16:46:55,047 : Image to text: 4.5, 15.6, 23.92, 40.0
2019-02-15 16:47:02,540 : Text to Image: 3.376, 11.892, 19.348, 49.0
2019-02-15 16:47:49,885 : samples : 320000
2019-02-15 16:48:02,670 : Image to text: 4.42, 15.32, 23.9, 41.0
2019-02-15 16:48:11,425 : Text to Image: 2.904, 11.128, 18.472, 50.0
2019-02-15 16:48:54,558 : samples : 384000
2019-02-15 16:49:04,837 : Image to text: 4.9, 16.4, 25.2, 37.0
2019-02-15 16:49:12,248 : Text to Image: 3.632, 13.152, 21.152, 44.0
2019-02-15 16:49:55,141 : samples : 448000
2019-02-15 16:50:05,438 : Image to text: 4.68, 15.82, 24.5, 36.0
2019-02-15 16:50:12,837 : Text to Image: 3.312, 12.196, 20.204, 47.0
2019-02-15 16:50:56,105 : samples : 512000
2019-02-15 16:51:06,851 : Image to text: 5.16, 16.54, 25.6, 35.0
2019-02-15 16:51:16,869 : Text to Image: 3.784, 13.232, 21.328, 43.0
2019-02-15 16:51:55,231 : Epoch 1 finished
2019-02-15 16:51:56,228 : Image to text: 14.9, 42.5, 58.5, 8.0
2019-02-15 16:51:56,963 : Text to Image: 11.16, 34.42, 50.6, 10.0
2019-02-15 16:51:57,932 : Image to text: 16.2, 43.1, 58.0, 7.0
2019-02-15 16:51:58,691 : Text to Image: 10.28, 35.54, 52.14, 10.0
2019-02-15 16:51:59,627 : Image to text: 14.4, 41.2, 58.3, 8.0
2019-02-15 16:52:00,455 : Text to Image: 10.72, 34.86, 50.8, 10.0
2019-02-15 16:52:01,437 : Image to text: 14.9, 41.8, 58.2, 8.0
2019-02-15 16:52:02,240 : Text to Image: 10.94, 33.54, 50.5, 10.0
2019-02-15 16:52:03,236 : Image to text: 17.6, 41.8, 57.9, 8.0
2019-02-15 16:52:03,996 : Text to Image: 10.98, 35.16, 51.18, 10.0
2019-02-15 16:52:03,996 : Dev mean Text to Image: 10.816, 34.70399999999999, 51.044, 10.0
2019-02-15 16:52:03,996 : Dev mean Image to text: 15.600000000000001, 42.08, 58.17999999999999, 7.799999999999999
2019-02-15 16:52:03,997 : start epoch
2019-02-15 16:52:48,538 : samples : 64000
2019-02-15 16:52:58,792 : Image to text: 4.78, 16.62, 25.52, 35.0
2019-02-15 16:53:06,128 : Text to Image: 3.132, 11.54, 18.872, 49.0
2019-02-15 16:53:48,633 : samples : 128000
2019-02-15 16:54:01,132 : Image to text: 5.06, 16.5, 26.22, 34.0
2019-02-15 16:54:11,107 : Text to Image: 3.432, 12.328, 20.468, 45.0
2019-02-15 16:54:56,481 : samples : 192000
2019-02-15 16:55:09,029 : Image to text: 4.86, 17.56, 27.14, 33.0
2019-02-15 16:55:17,847 : Text to Image: 3.952, 14.272, 22.844, 40.0
2019-02-15 16:55:59,391 : samples : 256000
2019-02-15 16:56:09,609 : Image to text: 5.08, 17.38, 26.82, 33.0
2019-02-15 16:56:16,882 : Text to Image: 3.652, 12.8, 21.028, 43.0
2019-02-15 16:56:59,847 : samples : 320000
2019-02-15 16:57:12,402 : Image to text: 6.32, 18.76, 29.26, 28.0
2019-02-15 16:57:22,476 : Text to Image: 4.6, 16.056, 25.468, 34.0
2019-02-15 16:58:07,246 : samples : 384000
2019-02-15 16:58:17,454 : Image to text: 5.76, 19.02, 29.04, 29.0
2019-02-15 16:58:24,895 : Text to Image: 4.4, 15.276, 24.668, 35.0
2019-02-15 16:59:07,461 : samples : 448000
2019-02-15 16:59:19,959 : Image to text: 5.88, 19.66, 30.38, 28.0
2019-02-15 16:59:29,968 : Text to Image: 4.188, 14.944, 24.06, 37.0
2019-02-15 17:00:14,900 : samples : 512000
2019-02-15 17:00:26,212 : Image to text: 6.1, 18.56, 28.3, 30.0
2019-02-15 17:00:33,585 : Text to Image: 4.132, 14.868, 23.82, 37.0
2019-02-15 17:01:09,640 : Epoch 2 finished
2019-02-15 17:01:10,109 : Image to text: 18.6, 46.6, 61.7, 7.0
2019-02-15 17:01:10,501 : Text to Image: 14.9, 40.84, 57.12, 8.0
2019-02-15 17:01:10,963 : Image to text: 17.2, 44.6, 61.1, 7.0
2019-02-15 17:01:11,339 : Text to Image: 13.32, 40.12, 58.38, 8.0
2019-02-15 17:01:11,798 : Image to text: 17.5, 47.5, 62.8, 6.0
2019-02-15 17:01:12,173 : Text to Image: 13.38, 39.94, 56.32, 8.0
2019-02-15 17:01:12,641 : Image to text: 19.1, 45.7, 61.2, 7.0
2019-02-15 17:01:13,012 : Text to Image: 14.02, 39.36, 56.1, 8.0
2019-02-15 17:01:13,470 : Image to text: 19.6, 47.9, 63.4, 6.0
2019-02-15 17:01:13,860 : Text to Image: 15.02, 39.86, 56.44, 8.0
2019-02-15 17:01:13,860 : Dev mean Text to Image: 14.128, 40.024, 56.872, 8.0
2019-02-15 17:01:13,860 : Dev mean Image to text: 18.400000000000002, 46.46, 62.040000000000006, 6.6000000000000005
2019-02-15 17:01:13,861 : start epoch
2019-02-15 17:01:57,265 : samples : 64000
2019-02-15 17:02:09,808 : Image to text: 5.88, 18.6, 29.18, 30.0
2019-02-15 17:02:19,836 : Text to Image: 3.936, 13.316, 21.748, 42.0
2019-02-15 17:03:04,666 : samples : 128000
2019-02-15 17:03:14,918 : Image to text: 6.58, 20.54, 31.44, 26.0
2019-02-15 17:03:22,268 : Text to Image: 5.188, 17.488, 27.588, 30.0
2019-02-15 17:04:05,214 : samples : 192000
2019-02-15 17:04:17,737 : Image to text: 6.18, 20.2, 30.0, 27.0
2019-02-15 17:04:27,873 : Text to Image: 4.652, 15.592, 25.272, 35.0
2019-02-15 17:05:19,078 : samples : 256000
2019-02-15 17:05:30,645 : Image to text: 6.76, 20.56, 30.72, 27.0
2019-02-15 17:05:38,030 : Text to Image: 5.056, 16.792, 26.004, 33.0
2019-02-15 17:06:20,487 : samples : 320000
2019-02-15 17:06:30,797 : Image to text: 6.8, 21.54, 31.98, 25.0
2019-02-15 17:06:38,223 : Text to Image: 5.604, 18.34, 28.34, 29.0
2019-02-15 17:07:21,864 : samples : 384000
2019-02-15 17:07:34,477 : Image to text: 6.62, 21.0, 31.92, 25.0
2019-02-15 17:07:44,500 : Text to Image: 5.24, 17.32, 27.08, 32.0
2019-02-15 17:08:28,701 : samples : 448000
2019-02-15 17:08:38,949 : Image to text: 5.7, 19.24, 29.26, 28.0
2019-02-15 17:08:46,327 : Text to Image: 4.344, 14.732, 23.728, 38.0
2019-02-15 17:09:29,181 : samples : 512000
2019-02-15 17:09:41,704 : Image to text: 5.56, 18.26, 28.44, 30.0
2019-02-15 17:09:51,708 : Text to Image: 3.74, 13.352, 22.004, 41.0
2019-02-15 17:10:30,314 : Epoch 3 finished
2019-02-15 17:10:31,315 : Image to text: 20.0, 48.3, 62.2, 6.0
2019-02-15 17:10:32,110 : Text to Image: 13.84, 41.8, 58.7, 8.0
2019-02-15 17:10:33,078 : Image to text: 17.9, 45.7, 63.3, 6.0
2019-02-15 17:10:33,852 : Text to Image: 13.56, 40.78, 59.28, 8.0
2019-02-15 17:10:34,749 : Image to text: 18.1, 47.8, 61.4, 6.0
2019-02-15 17:10:35,483 : Text to Image: 12.58, 39.5, 56.66, 8.0
2019-02-15 17:10:36,413 : Image to text: 17.6, 49.1, 65.9, 6.0
2019-02-15 17:10:37,157 : Text to Image: 13.04, 40.78, 57.7, 8.0
2019-02-15 17:10:38,085 : Image to text: 21.0, 51.3, 65.4, 5.0
2019-02-15 17:10:38,903 : Text to Image: 13.52, 41.88, 58.24, 8.0
2019-02-15 17:10:38,903 : Dev mean Text to Image: 13.308, 40.94799999999999, 58.116, 8.0
2019-02-15 17:10:38,903 : Dev mean Image to text: 18.919999999999998, 48.44, 63.64, 5.8
2019-02-15 17:10:38,903 : start epoch
2019-02-15 17:11:21,824 : samples : 64000
2019-02-15 17:11:32,086 : Image to text: 5.8, 18.94, 28.96, 28.0
2019-02-15 17:11:39,382 : Text to Image: 4.24, 14.664, 23.628, 38.0
2019-02-15 17:12:22,459 : samples : 128000
2019-02-15 17:12:35,024 : Image to text: 6.58, 19.82, 30.4, 28.0
2019-02-15 17:12:45,091 : Text to Image: 4.468, 15.724, 24.624, 37.0
2019-02-15 17:13:30,514 : samples : 192000
2019-02-15 17:13:42,194 : Image to text: 6.76, 20.92, 32.24, 25.0
2019-02-15 17:13:49,559 : Text to Image: 5.2, 16.696, 26.472, 32.0
2019-02-15 17:14:31,963 : samples : 256000
2019-02-15 17:14:44,107 : Image to text: 7.28, 22.42, 33.08, 25.0
2019-02-15 17:14:54,072 : Text to Image: 5.392, 18.024, 27.756, 30.0
2019-02-15 17:15:39,784 : samples : 320000
2019-02-15 17:15:52,358 : Image to text: 6.54, 19.64, 30.56, 27.0
2019-02-15 17:16:02,364 : Text to Image: 5.116, 17.328, 26.924, 31.0
2019-02-15 17:16:45,254 : samples : 384000
2019-02-15 17:16:55,393 : Image to text: 6.2, 19.44, 30.4, 27.0
2019-02-15 17:17:03,830 : Text to Image: 4.948, 17.056, 26.776, 31.0
2019-02-15 17:17:47,403 : samples : 448000
2019-02-15 17:17:58,201 : Image to text: 7.0, 22.06, 32.6, 24.0
2019-02-15 17:18:05,552 : Text to Image: 5.688, 18.664, 28.452, 30.0
2019-02-15 17:18:48,079 : samples : 512000
2019-02-15 17:18:58,365 : Image to text: 5.62, 20.18, 30.58, 27.0
2019-02-15 17:19:05,779 : Text to Image: 5.028, 16.736, 26.46, 32.0
2019-02-15 17:19:42,058 : Epoch 4 finished
2019-02-15 17:19:42,516 : Image to text: 20.2, 49.6, 65.9, 6.0
2019-02-15 17:19:42,879 : Text to Image: 16.52, 44.92, 61.88, 7.0
2019-02-15 17:19:43,328 : Image to text: 21.2, 50.8, 66.4, 5.0
2019-02-15 17:19:43,690 : Text to Image: 15.96, 45.06, 62.44, 7.0
2019-02-15 17:19:44,139 : Image to text: 19.8, 50.7, 64.3, 5.0
2019-02-15 17:19:44,503 : Text to Image: 15.92, 44.3, 62.34, 7.0
2019-02-15 17:19:44,957 : Image to text: 21.6, 50.1, 67.3, 5.0
2019-02-15 17:19:45,330 : Text to Image: 16.56, 45.06, 62.44, 7.0
2019-02-15 17:19:45,790 : Image to text: 23.5, 50.4, 65.8, 5.0
2019-02-15 17:19:46,162 : Text to Image: 17.16, 46.04, 61.62, 7.0
2019-02-15 17:19:46,162 : Dev mean Text to Image: 16.424, 45.076, 62.144, 7.0
2019-02-15 17:19:46,162 : Dev mean Image to text: 21.26, 50.31999999999999, 65.94, 5.2
2019-02-15 17:19:46,163 : start epoch
2019-02-15 17:20:28,745 : samples : 64000
2019-02-15 17:20:38,977 : Image to text: 7.08, 21.18, 32.22, 25.0
2019-02-15 17:20:46,396 : Text to Image: 5.132, 18.048, 27.804, 30.0
2019-02-15 17:21:34,588 : samples : 128000
2019-02-15 17:21:47,440 : Image to text: 6.5, 21.74, 32.62, 24.0
2019-02-15 17:21:55,819 : Text to Image: 5.228, 18.432, 28.712, 28.0
2019-02-15 17:22:38,484 : samples : 192000
2019-02-15 17:22:48,753 : Image to text: 7.02, 22.16, 32.76, 24.0
2019-02-15 17:22:56,154 : Text to Image: 5.464, 18.536, 28.456, 29.0
2019-02-15 17:23:38,944 : samples : 256000
2019-02-15 17:23:50,127 : Image to text: 7.02, 21.68, 33.08, 23.0
2019-02-15 17:24:00,226 : Text to Image: 5.864, 19.04, 29.368, 28.0
2019-02-15 17:24:44,776 : samples : 320000
2019-02-15 17:24:55,071 : Image to text: 7.14, 22.32, 33.2, 23.0
2019-02-15 17:25:02,343 : Text to Image: 5.856, 19.04, 29.456, 27.0
2019-02-15 17:25:45,755 : samples : 384000
2019-02-15 17:25:57,231 : Image to text: 7.3, 23.04, 33.64, 23.0
2019-02-15 17:26:06,643 : Text to Image: 6.184, 19.324, 29.452, 28.0
2019-02-15 17:26:49,882 : samples : 448000
2019-02-15 17:27:00,073 : Image to text: 6.64, 20.9, 31.82, 25.0
2019-02-15 17:27:07,504 : Text to Image: 5.08, 16.872, 26.32, 32.0
2019-02-15 17:27:50,799 : samples : 512000
2019-02-15 17:28:00,886 : Image to text: 7.38, 22.36, 33.36, 24.0
2019-02-15 17:28:08,958 : Text to Image: 5.7, 18.892, 29.34, 28.0
2019-02-15 17:28:47,173 : Epoch 5 finished
2019-02-15 17:28:48,051 : Image to text: 19.4, 48.2, 64.0, 6.0
2019-02-15 17:28:48,791 : Text to Image: 13.52, 40.46, 57.22, 8.0
2019-02-15 17:28:49,668 : Image to text: 18.1, 47.4, 63.7, 6.0
2019-02-15 17:28:50,379 : Text to Image: 13.68, 39.92, 57.26, 8.0
2019-02-15 17:28:51,294 : Image to text: 17.7, 46.9, 63.2, 6.0
2019-02-15 17:28:52,004 : Text to Image: 13.56, 38.96, 55.7, 8.0
2019-02-15 17:28:52,950 : Image to text: 19.5, 49.3, 64.2, 6.0
2019-02-15 17:28:53,666 : Text to Image: 14.44, 40.74, 58.32, 8.0
2019-02-15 17:28:54,556 : Image to text: 21.2, 52.6, 68.9, 5.0
2019-02-15 17:28:55,313 : Text to Image: 14.56, 41.48, 57.64, 8.0
2019-02-15 17:28:55,313 : Dev mean Text to Image: 13.951999999999998, 40.312000000000005, 57.228, 8.0
2019-02-15 17:28:55,313 : Dev mean Image to text: 19.18, 48.879999999999995, 64.8, 5.8
2019-02-15 17:28:55,313 : start epoch
2019-02-15 17:29:40,357 : samples : 64000
2019-02-15 17:29:52,894 : Image to text: 7.48, 22.7, 32.64, 23.0
2019-02-15 17:30:02,889 : Text to Image: 5.696, 19.188, 29.308, 28.0
2019-02-15 17:30:47,856 : samples : 128000
2019-02-15 17:31:00,450 : Image to text: 7.46, 23.0, 34.24, 22.0
2019-02-15 17:31:10,430 : Text to Image: 6.172, 19.92, 30.724, 26.0
2019-02-15 17:31:55,215 : samples : 192000
2019-02-15 17:32:07,774 : Image to text: 7.48, 23.28, 34.44, 22.0
2019-02-15 17:32:17,781 : Text to Image: 6.164, 19.868, 30.52, 27.0
2019-02-15 17:33:02,682 : samples : 256000
2019-02-15 17:33:15,238 : Image to text: 7.42, 21.94, 32.72, 24.0
2019-02-15 17:33:25,209 : Text to Image: 5.848, 18.932, 28.924, 29.0
2019-02-15 17:34:09,783 : samples : 320000
2019-02-15 17:34:22,414 : Image to text: 7.92, 23.64, 34.24, 22.0
2019-02-15 17:34:32,465 : Text to Image: 6.324, 19.924, 30.28, 27.0
2019-02-15 17:35:16,810 : samples : 384000
2019-02-15 17:35:29,401 : Image to text: 8.26, 23.1, 34.9, 21.0
2019-02-15 17:35:39,467 : Text to Image: 6.572, 20.896, 31.232, 26.0
2019-02-15 17:36:23,620 : samples : 448000
2019-02-15 17:36:36,279 : Image to text: 7.54, 23.12, 33.5, 22.0
2019-02-15 17:36:46,259 : Text to Image: 6.132, 19.488, 29.644, 28.0
2019-02-15 17:37:29,732 : samples : 512000
2019-02-15 17:37:42,269 : Image to text: 7.68, 22.52, 34.46, 21.0
2019-02-15 17:37:52,271 : Text to Image: 5.848, 18.98, 29.356, 28.0
2019-02-15 17:38:38,471 : Epoch 6 finished
2019-02-15 17:38:39,460 : Image to text: 21.6, 50.9, 66.2, 5.0
2019-02-15 17:38:40,476 : Text to Image: 16.04, 45.36, 63.04, 7.0
2019-02-15 17:38:41,406 : Image to text: 20.3, 49.0, 66.0, 6.0
2019-02-15 17:38:42,168 : Text to Image: 15.62, 43.92, 60.66, 7.0
2019-02-15 17:38:43,098 : Image to text: 19.2, 49.3, 66.0, 6.0
2019-02-15 17:38:43,882 : Text to Image: 15.38, 43.38, 59.92, 7.0
2019-02-15 17:38:44,799 : Image to text: 20.7, 53.1, 67.9, 5.0
2019-02-15 17:38:45,590 : Text to Image: 14.92, 42.92, 60.4, 7.0
2019-02-15 17:38:46,511 : Image to text: 22.2, 52.7, 68.3, 5.0
2019-02-15 17:38:47,310 : Text to Image: 16.24, 44.5, 61.12, 7.0
2019-02-15 17:38:47,310 : Dev mean Text to Image: 15.639999999999999, 44.016, 61.028000000000006, 7.0
2019-02-15 17:38:47,310 : Dev mean Image to text: 20.799999999999997, 51.0, 66.88, 5.4
2019-02-15 17:38:47,310 : start epoch
2019-02-15 17:39:32,760 : samples : 64000
2019-02-15 17:39:44,727 : Image to text: 7.38, 22.16, 32.76, 24.0
2019-02-15 17:39:52,073 : Text to Image: 5.204, 17.704, 27.356, 31.0
2019-02-15 17:40:34,627 : samples : 128000
2019-02-15 17:40:44,855 : Image to text: 7.32, 22.22, 33.26, 23.0
2019-02-15 17:40:52,208 : Text to Image: 5.876, 18.74, 29.48, 28.0
2019-02-15 17:41:35,224 : samples : 192000
2019-02-15 17:41:48,060 : Image to text: 7.38, 23.06, 34.16, 23.0
2019-02-15 17:41:58,523 : Text to Image: 5.696, 19.26, 29.452, 28.0
2019-02-15 17:42:44,066 : samples : 256000
2019-02-15 17:42:56,924 : Image to text: 8.18, 23.1, 33.86, 23.0
2019-02-15 17:43:07,243 : Text to Image: 5.992, 19.288, 29.544, 28.0
2019-02-15 17:43:53,435 : samples : 320000
2019-02-15 17:44:06,264 : Image to text: 6.38, 20.88, 32.24, 25.0
2019-02-15 17:44:16,705 : Text to Image: 4.732, 16.6, 26.356, 33.0
2019-02-15 17:45:02,721 : samples : 384000
2019-02-15 17:45:15,564 : Image to text: 6.9, 22.12, 33.52, 23.0
2019-02-15 17:45:26,136 : Text to Image: 5.524, 18.972, 28.864, 28.0
2019-02-15 17:46:12,229 : samples : 448000
2019-02-15 17:46:25,079 : Image to text: 7.7, 23.8, 35.14, 21.0
2019-02-15 17:46:35,565 : Text to Image: 6.324, 20.456, 31.216, 26.0
2019-02-15 17:47:21,317 : samples : 512000
2019-02-15 17:47:34,104 : Image to text: 7.78, 23.82, 35.64, 21.0
2019-02-15 17:47:44,489 : Text to Image: 6.62, 20.716, 31.376, 26.0
2019-02-15 17:48:23,468 : Epoch 7 finished
2019-02-15 17:48:24,547 : Image to text: 22.3, 52.3, 67.7, 5.0
2019-02-15 17:48:25,479 : Text to Image: 18.16, 48.58, 65.2, 6.0
2019-02-15 17:48:26,514 : Image to text: 21.2, 52.0, 68.8, 5.0
2019-02-15 17:48:27,412 : Text to Image: 18.16, 48.18, 66.0, 6.0
2019-02-15 17:48:28,492 : Image to text: 19.7, 53.3, 67.6, 5.0
2019-02-15 17:48:29,385 : Text to Image: 17.24, 47.94, 64.94, 6.0
2019-02-15 17:48:30,430 : Image to text: 22.9, 54.3, 70.9, 5.0
2019-02-15 17:48:31,262 : Text to Image: 18.0, 48.28, 65.58, 6.0
2019-02-15 17:48:32,357 : Image to text: 23.5, 54.3, 70.4, 4.0
2019-02-15 17:48:33,288 : Text to Image: 18.64, 48.3, 65.16, 6.0
2019-02-15 17:48:33,288 : Dev mean Text to Image: 18.04, 48.256, 65.376, 6.0
2019-02-15 17:48:33,288 : Dev mean Image to text: 21.919999999999998, 53.239999999999995, 69.08, 4.8
2019-02-15 17:48:33,288 : start epoch
2019-02-15 17:49:18,977 : samples : 64000
2019-02-15 17:49:30,920 : Image to text: 7.24, 23.18, 34.72, 22.0
2019-02-15 17:49:38,446 : Text to Image: 5.84, 19.372, 29.712, 27.0
2019-02-15 17:50:21,151 : samples : 128000
2019-02-15 17:50:31,595 : Image to text: 7.92, 24.38, 35.04, 21.0
2019-02-15 17:50:39,157 : Text to Image: 6.328, 20.32, 30.856, 26.0
2019-02-15 17:51:21,767 : samples : 192000
2019-02-15 17:51:32,213 : Image to text: 7.96, 24.36, 36.02, 20.0
2019-02-15 17:51:39,784 : Text to Image: 6.708, 20.912, 31.556, 25.0
2019-02-15 17:52:22,215 : samples : 256000
2019-02-15 17:52:32,597 : Image to text: 7.56, 23.4, 34.54, 21.0
2019-02-15 17:52:40,218 : Text to Image: 6.108, 19.628, 29.988, 27.0
2019-02-15 17:53:22,900 : samples : 320000
2019-02-15 17:53:33,270 : Image to text: 7.34, 23.3, 35.38, 22.0
2019-02-15 17:53:40,818 : Text to Image: 6.052, 19.612, 29.976, 27.0
2019-02-15 17:54:23,568 : samples : 384000
2019-02-15 17:54:33,897 : Image to text: 8.34, 24.14, 35.54, 20.0
2019-02-15 17:54:41,151 : Text to Image: 6.396, 19.992, 30.656, 26.0
2019-02-15 17:55:33,687 : samples : 448000
2019-02-15 17:55:43,984 : Image to text: 8.2, 23.9, 36.3, 20.0
2019-02-15 17:55:51,573 : Text to Image: 6.632, 21.324, 32.116, 24.0
2019-02-15 17:56:34,004 : samples : 512000
2019-02-15 17:56:44,375 : Image to text: 7.6, 23.52, 34.7, 21.0
2019-02-15 17:56:51,951 : Text to Image: 6.216, 20.156, 30.736, 26.0
2019-02-15 17:57:28,408 : Epoch 8 finished
2019-02-15 17:57:28,869 : Image to text: 18.9, 50.6, 66.0, 5.0
2019-02-15 17:57:29,224 : Text to Image: 16.58, 45.14, 62.72, 7.0
2019-02-15 17:57:29,680 : Image to text: 20.9, 50.8, 68.0, 5.0
2019-02-15 17:57:30,021 : Text to Image: 15.94, 45.3, 62.0, 7.0
2019-02-15 17:57:30,475 : Image to text: 19.3, 51.7, 66.0, 5.0
2019-02-15 17:57:30,824 : Text to Image: 16.56, 45.3, 62.56, 6.0
2019-02-15 17:57:31,280 : Image to text: 21.1, 51.7, 68.2, 5.0
2019-02-15 17:57:31,629 : Text to Image: 16.2, 44.7, 62.2, 7.0
2019-02-15 17:57:32,089 : Image to text: 22.3, 53.7, 68.8, 5.0
2019-02-15 17:57:32,443 : Text to Image: 17.54, 44.64, 61.52, 7.0
2019-02-15 17:57:32,443 : Dev mean Text to Image: 16.564, 45.016000000000005, 62.2, 6.800000000000001
2019-02-15 17:57:32,443 : Dev mean Image to text: 20.5, 51.7, 67.4, 5.0
2019-02-15 17:57:32,443 : start epoch
2019-02-15 17:58:14,892 : samples : 64000
2019-02-15 17:58:25,260 : Image to text: 7.96, 24.52, 36.18, 20.0
2019-02-15 17:58:32,744 : Text to Image: 6.736, 20.764, 31.216, 25.0
2019-02-15 17:59:15,758 : samples : 128000
2019-02-15 17:59:26,147 : Image to text: 8.2, 23.66, 34.7, 21.0
2019-02-15 17:59:33,666 : Text to Image: 6.588, 20.24, 30.928, 26.0
2019-02-15 18:00:16,036 : samples : 192000
2019-02-15 18:00:26,622 : Image to text: 8.44, 24.24, 35.6, 21.0
2019-02-15 18:00:34,235 : Text to Image: 6.476, 20.76, 31.172, 26.0
2019-02-15 18:01:17,896 : samples : 256000
2019-02-15 18:01:28,239 : Image to text: 8.44, 24.34, 36.38, 21.0
2019-02-15 18:01:35,743 : Text to Image: 6.644, 21.176, 31.572, 25.0
2019-02-15 18:02:18,226 : samples : 320000
2019-02-15 18:02:28,509 : Image to text: 8.42, 24.5, 36.2, 20.0
2019-02-15 18:02:36,020 : Text to Image: 7.076, 21.624, 32.152, 25.0
2019-02-15 18:03:18,985 : samples : 384000
2019-02-15 18:03:29,209 : Image to text: 8.56, 24.24, 35.24, 21.0
2019-02-15 18:03:36,744 : Text to Image: 6.424, 20.244, 30.648, 27.0
2019-02-15 18:04:19,884 : samples : 448000
2019-02-15 18:04:30,130 : Image to text: 8.18, 25.3, 36.54, 19.0
2019-02-15 18:04:37,627 : Text to Image: 6.436, 20.896, 31.4, 25.0
2019-02-15 18:05:19,923 : samples : 512000
2019-02-15 18:05:30,215 : Image to text: 8.26, 25.68, 37.42, 19.0
2019-02-15 18:05:37,736 : Text to Image: 6.984, 21.408, 32.36, 24.0
2019-02-15 18:06:14,299 : Epoch 9 finished
2019-02-15 18:06:14,744 : Image to text: 21.8, 53.7, 68.1, 5.0
2019-02-15 18:06:15,076 : Text to Image: 18.66, 48.84, 65.72, 6.0
2019-02-15 18:06:15,516 : Image to text: 21.8, 50.5, 68.3, 5.0
2019-02-15 18:06:15,853 : Text to Image: 17.9, 48.24, 66.04, 6.0
2019-02-15 18:06:16,291 : Image to text: 19.0, 53.0, 68.2, 5.0
2019-02-15 18:06:16,671 : Text to Image: 17.2, 47.44, 65.22, 6.0
2019-02-15 18:06:17,122 : Image to text: 24.1, 54.1, 68.3, 5.0
2019-02-15 18:06:17,464 : Text to Image: 17.7, 48.68, 66.08, 6.0
2019-02-15 18:06:17,904 : Image to text: 23.2, 53.1, 68.2, 5.0
2019-02-15 18:06:18,258 : Text to Image: 18.92, 48.26, 65.06, 6.0
2019-02-15 18:06:18,259 : Dev mean Text to Image: 18.075999999999997, 48.292, 65.624, 6.0
2019-02-15 18:06:18,259 : Dev mean Image to text: 21.98, 52.879999999999995, 68.22, 5.0
2019-02-15 18:06:22,276 : 
Test scores | Image to text:             21.319999999999997, 51.96, 67.39999999999999, 5.0
2019-02-15 18:06:22,277 : Test scores | Text to image:             17.860000000000003, 48.028000000000006, 65.32400000000001, 6.0

2019-02-15 18:06:22,379 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 18:06:22,744 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 18:06:23,387 : loading BERT model bert-base-uncased
2019-02-15 18:06:23,387 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:06:23,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:06:23,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb0x3mffp
2019-02-15 18:06:25,934 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:06:27,353 : Computing embeddings for train/dev/test
2019-02-15 18:08:00,456 : Computed embeddings
2019-02-15 18:08:00,456 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:08:46,065 : [('reg:1e-05', 71.76), ('reg:0.0001', 70.3), ('reg:0.001', 63.13), ('reg:0.01', 42.64)]
2019-02-15 18:08:46,065 : Validation : best param found is reg = 1e-05 with score             71.76
2019-02-15 18:08:46,065 : Evaluating...
2019-02-15 18:08:55,954 : 
Dev acc : 71.8 Test acc : 71.9 for LENGTH classification

2019-02-15 18:08:55,955 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 18:08:56,468 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 18:08:56,520 : loading BERT model bert-base-uncased
2019-02-15 18:08:56,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:08:56,557 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:08:56,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp_bfgnm1
2019-02-15 18:08:59,054 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:09:00,521 : Computing embeddings for train/dev/test
2019-02-15 18:10:29,651 : Computed embeddings
2019-02-15 18:10:29,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:11:13,805 : [('reg:1e-05', 0.81), ('reg:0.0001', 0.25), ('reg:0.001', 0.15), ('reg:0.01', 0.1)]
2019-02-15 18:11:13,805 : Validation : best param found is reg = 1e-05 with score             0.81
2019-02-15 18:11:13,805 : Evaluating...
2019-02-15 18:11:24,440 : 
Dev acc : 0.8 Test acc : 0.8 for WORDCONTENT classification

2019-02-15 18:11:24,441 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 18:11:24,953 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 18:11:25,021 : loading BERT model bert-base-uncased
2019-02-15 18:11:25,021 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:11:25,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:11:25,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpayu3v43h
2019-02-15 18:11:27,405 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:11:28,813 : Computing embeddings for train/dev/test
2019-02-15 18:12:55,313 : Computed embeddings
2019-02-15 18:12:55,313 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:13:33,041 : [('reg:1e-05', 25.72), ('reg:0.0001', 25.84), ('reg:0.001', 23.09), ('reg:0.01', 22.56)]
2019-02-15 18:13:33,041 : Validation : best param found is reg = 0.0001 with score             25.84
2019-02-15 18:13:33,041 : Evaluating...
2019-02-15 18:13:41,216 : 
Dev acc : 25.8 Test acc : 26.1 for DEPTH classification

2019-02-15 18:13:41,217 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 18:13:41,650 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 18:13:41,715 : loading BERT model bert-base-uncased
2019-02-15 18:13:41,715 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:13:41,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:13:41,744 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfrgtkyfc
2019-02-15 18:13:44,272 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:13:45,729 : Computing embeddings for train/dev/test
2019-02-15 18:15:02,913 : Computed embeddings
2019-02-15 18:15:02,913 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:15:58,082 : [('reg:1e-05', 50.57), ('reg:0.0001', 44.22), ('reg:0.001', 31.25), ('reg:0.01', 16.24)]
2019-02-15 18:15:58,082 : Validation : best param found is reg = 1e-05 with score             50.57
2019-02-15 18:15:58,082 : Evaluating...
2019-02-15 18:16:13,058 : 
Dev acc : 50.6 Test acc : 51.0 for TOPCONSTITUENTS classification

2019-02-15 18:16:13,059 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 18:16:13,458 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 18:16:13,530 : loading BERT model bert-base-uncased
2019-02-15 18:16:13,530 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:16:13,566 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:16:13,567 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpecp5o5rg
2019-02-15 18:16:16,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:16:17,520 : Computing embeddings for train/dev/test
2019-02-15 18:17:41,277 : Computed embeddings
2019-02-15 18:17:41,277 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:18:14,552 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 18:18:14,552 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-15 18:18:14,552 : Evaluating...
2019-02-15 18:18:23,510 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-15 18:18:23,511 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 18:18:23,932 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 18:18:24,009 : loading BERT model bert-base-uncased
2019-02-15 18:18:24,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:18:24,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:18:24,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcmu49ady
2019-02-15 18:18:26,631 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:18:28,137 : Computing embeddings for train/dev/test
2019-02-15 18:19:50,444 : Computed embeddings
2019-02-15 18:19:50,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:20:45,099 : [('reg:1e-05', 79.94), ('reg:0.0001', 79.73), ('reg:0.001', 78.33), ('reg:0.01', 68.1)]
2019-02-15 18:20:45,099 : Validation : best param found is reg = 1e-05 with score             79.94
2019-02-15 18:20:45,099 : Evaluating...
2019-02-15 18:20:59,270 : 
Dev acc : 79.9 Test acc : 78.2 for TENSE classification

2019-02-15 18:20:59,271 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 18:20:59,670 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 18:20:59,737 : loading BERT model bert-base-uncased
2019-02-15 18:20:59,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:20:59,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:20:59,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi6axe894
2019-02-15 18:21:02,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:21:03,749 : Computing embeddings for train/dev/test
2019-02-15 18:22:29,972 : Computed embeddings
2019-02-15 18:22:29,972 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:23:20,801 : [('reg:1e-05', 74.42), ('reg:0.0001', 73.9), ('reg:0.001', 71.77), ('reg:0.01', 56.24)]
2019-02-15 18:23:20,801 : Validation : best param found is reg = 1e-05 with score             74.42
2019-02-15 18:23:20,801 : Evaluating...
2019-02-15 18:23:35,542 : 
Dev acc : 74.4 Test acc : 73.4 for SUBJNUMBER classification

2019-02-15 18:23:35,543 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 18:23:36,198 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 18:23:36,276 : loading BERT model bert-base-uncased
2019-02-15 18:23:36,276 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:23:36,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:23:36,311 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvg3rj6j_
2019-02-15 18:23:38,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:23:40,254 : Computing embeddings for train/dev/test
2019-02-15 18:25:05,802 : Computed embeddings
2019-02-15 18:25:05,802 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:25:58,041 : [('reg:1e-05', 71.16), ('reg:0.0001', 70.62), ('reg:0.001', 62.48), ('reg:0.01', 60.82)]
2019-02-15 18:25:58,042 : Validation : best param found is reg = 1e-05 with score             71.16
2019-02-15 18:25:58,042 : Evaluating...
2019-02-15 18:26:11,915 : 
Dev acc : 71.2 Test acc : 71.9 for OBJNUMBER classification

2019-02-15 18:26:11,916 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 18:26:12,355 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 18:26:12,436 : loading BERT model bert-base-uncased
2019-02-15 18:26:12,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:26:12,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:26:12,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1v3ru8wm
2019-02-15 18:26:14,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:26:16,455 : Computing embeddings for train/dev/test
2019-02-15 18:27:53,526 : Computed embeddings
2019-02-15 18:27:53,526 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:28:22,935 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-15 18:28:22,935 : Validation : best param found is reg = 1e-05 with score             50.19
2019-02-15 18:28:22,935 : Evaluating...
2019-02-15 18:28:29,716 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-15 18:28:29,717 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 18:28:30,363 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 18:28:30,481 : loading BERT model bert-base-uncased
2019-02-15 18:28:30,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:28:30,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:28:30,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp5fquycz
2019-02-15 18:28:33,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:28:35,935 : Computing embeddings for train/dev/test
2019-02-15 18:30:16,016 : Computed embeddings
2019-02-15 18:30:16,016 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:30:54,944 : [('reg:1e-05', 52.13), ('reg:0.0001', 50.03), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-15 18:30:54,944 : Validation : best param found is reg = 1e-05 with score             52.13
2019-02-15 18:30:54,944 : Evaluating...
2019-02-15 18:31:05,658 : 
Dev acc : 52.1 Test acc : 51.8 for COORDINATIONINVERSION classification

2019-02-15 18:31:05,660 : total results: {'STS12': {'MSRpar': {'pearson': (0.10380424942562065, 0.004430754024065819), 'spearman': SpearmanrResult(correlation=0.14761811776682998, pvalue=4.945273020979249e-05), 'nsamples': 750}, 'MSRvid': {'pearson': (0.11454030961837812, 0.001678384357147286), 'spearman': SpearmanrResult(correlation=0.15381061031829224, pvalue=2.3322475054148977e-05), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.28517971103148354, 4.883730298968081e-10), 'spearman': SpearmanrResult(correlation=0.5017880205922501, pvalue=1.2096690067571207e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.06245784237201789, 0.08739640410855704), 'spearman': SpearmanrResult(correlation=0.12674077352775195, pvalue=0.0005029336895913375), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.38109829191164263, 3.0657187916444786e-15), 'spearman': SpearmanrResult(correlation=0.2911581198574834, pvalue=3.094281818427784e-09), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.18941608087182854, 'wmean': 0.15880228664678533}, 'spearman': {'mean': 0.24422312841252153, 'wmean': 0.21480692325760434}}}, 'STS13': {'FNWN': {'pearson': (0.11963480087641581, 0.10106964967955051), 'spearman': SpearmanrResult(correlation=0.18372020206777637, pvalue=0.011388165408677183), 'nsamples': 189}, 'headlines': {'pearson': (0.07309505026120544, 0.0453784961388967), 'spearman': SpearmanrResult(correlation=0.23026393007211166, pvalue=1.7540278049854389e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.07640805050606723, 0.07054957054143243), 'spearman': SpearmanrResult(correlation=0.03317657890478938, pvalue=0.4328846102314525), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.0897126338812295, 'wmean': 0.08019812093030027}, 'spearman': {'mean': 0.1490535703482258, 'wmean': 0.1506887510069869}}}, 'STS14': {'deft-forum': {'pearson': (0.0830406025191273, 0.07845902357732634), 'spearman': SpearmanrResult(correlation=0.1251657667948887, pvalue=0.007854918528360094), 'nsamples': 450}, 'deft-news': {'pearson': (0.26012087363413466, 4.984269664556529e-06), 'spearman': SpearmanrResult(correlation=0.4117529233836087, pvalue=1.0506523650820577e-13), 'nsamples': 300}, 'headlines': {'pearson': (0.08909981973681941, 0.014651306531006035), 'spearman': SpearmanrResult(correlation=0.1901364319417861, pvalue=1.5514456990945782e-07), 'nsamples': 750}, 'images': {'pearson': (0.027269066687729573, 0.45585639072982576), 'spearman': SpearmanrResult(correlation=0.19510216928988197, pvalue=7.203406241497916e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.09607507735599727, 0.00846771112077922), 'spearman': SpearmanrResult(correlation=0.1133339946004993, pvalue=0.0018796116330401012), 'nsamples': 750}, 'tweet-news': {'pearson': (0.1473089606456684, 5.13040774787193e-05), 'spearman': SpearmanrResult(correlation=0.2134627407786209, pvalue=3.5403738476846613e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.11715240009657941, 'wmean': 0.10272512707826899}, 'spearman': {'mean': 0.20815900446488092, 'wmean': 0.19036719320823298}}}, 'STS15': {'answers-forums': {'pearson': (0.12158114216686705, 0.018508138202662203), 'spearman': SpearmanrResult(correlation=0.16374204764325878, pvalue=0.001463798670735793), 'nsamples': 375}, 'answers-students': {'pearson': (0.22656941258579405, 3.467128075018036e-10), 'spearman': SpearmanrResult(correlation=0.26299109969089624, pvalue=2.4885223708616667e-13), 'nsamples': 750}, 'belief': {'pearson': (0.0678607706750548, 0.1897756752333678), 'spearman': SpearmanrResult(correlation=0.1826704910069743, pvalue=0.00037718792800990966), 'nsamples': 375}, 'headlines': {'pearson': (0.18598490301931533, 2.9015547628717895e-07), 'spearman': SpearmanrResult(correlation=0.33717873902465495, pvalue=2.136511030457333e-21), 'nsamples': 750}, 'images': {'pearson': (-0.024335388238084395, 0.5057697497834123), 'spearman': SpearmanrResult(correlation=0.22103565473014256, pvalue=9.413150963712061e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.11553216804178938, 'wmean': 0.12073497094699648}, 'spearman': {'mean': 0.23352360641918538, 'wmean': 0.24860294069270256}}}, 'STS16': {'answer-answer': {'pearson': (0.0996322199747233, 0.11319648980704425), 'spearman': SpearmanrResult(correlation=0.2157206466623911, pvalue=0.0005363502974616452), 'nsamples': 254}, 'headlines': {'pearson': (0.20449626093914425, 0.001174422538880629), 'spearman': SpearmanrResult(correlation=0.3990100203819182, pvalue=6.22834103527279e-11), 'nsamples': 249}, 'plagiarism': {'pearson': (0.1424344034537375, 0.030820925710232608), 'spearman': SpearmanrResult(correlation=0.34282705286133175, pvalue=9.638568668189056e-08), 'nsamples': 230}, 'postediting': {'pearson': (0.1802028795897441, 0.004749416093869213), 'spearman': SpearmanrResult(correlation=0.6355357099108896, pvalue=5.211609268901804e-29), 'nsamples': 244}, 'question-question': {'pearson': (-0.12464601726875066, 0.07214554603331674), 'spearman': SpearmanrResult(correlation=-0.14928117797745077, pvalue=0.03098288398281293), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.1004239493377197, 'wmean': 0.10700215063449825}, 'spearman': {'mean': 0.288762450367816, 'wmean': 0.3009002601234579}}}, 'MR': {'devacc': 56.68, 'acc': 54.12, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 65.36, 'acc': 64.16, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.27, 'acc': 81.85, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 80.5, 'acc': 82.41, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 68.69, 'acc': 70.57, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 31.97, 'acc': 30.72, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 50.34, 'acc': 62.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.65, 'acc': 68.93, 'f1': 80.03, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.8, 'acc': 68.95, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.697939563530309, 'pearson': 0.7213960553443098, 'spearman': 0.6669834363031751, 'mse': 0.4894853563904378, 'yhat': array([3.13010049, 4.23965664, 1.71305823, ..., 3.43935882, 4.44825269,        4.60157129]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5584027781621694, 'pearson': 0.4944005510509023, 'spearman': 0.4807060159651944, 'mse': 1.9982841152386206, 'yhat': array([2.11734551, 2.34330568, 2.58218821, ..., 3.41403858, 3.19600335,        3.21360939]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.69, 'acc': 57.7, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 275.912, 'acc': [(21.319999999999997, 51.96, 67.39999999999999, 5.0), (17.860000000000003, 48.028000000000006, 65.32400000000001, 6.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 71.76, 'acc': 71.9, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.81, 'acc': 0.79, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.84, 'acc': 26.12, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 50.57, 'acc': 50.99, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 79.94, 'acc': 78.2, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.42, 'acc': 73.41, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 71.16, 'acc': 71.87, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 52.13, 'acc': 51.83, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 18:31:05,660 : STS12 p=0.1588, STS12 s=0.2148, STS13 p=0.0802, STS13 s=0.1507, STS14 p=0.1027, STS14 s=0.1904, STS15 p=0.1207, STS15 s=0.2486, STS 16 p=0.1070, STS16 s=0.3009, STS B p=0.4944, STS B s=0.4807, STS B m=1.9983, SICK-R p=0.7214, SICK-R s=0.6670, SICK-P m=0.4895
2019-02-15 18:31:05,660 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 18:31:05,660 : 0.1588,0.2148,0.0802,0.1507,0.1027,0.1904,0.1207,0.2486,0.1070,0.3009,0.4944,0.4807,1.9983,0.7214,0.6670,0.4895
2019-02-15 18:31:05,660 : MR=54.12, CR=64.16, SUBJ=82.41, MPQA=81.85, SST-B=70.57, SST-F=30.72, TREC=62.20, SICK-E=68.95, SNLI=57.70, MRPC=68.93, MRPC f=80.03
2019-02-15 18:31:05,660 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 18:31:05,660 : 54.12,64.16,82.41,81.85,70.57,30.72,62.20,68.95,57.70,68.93,80.03
2019-02-15 18:31:05,660 : COCO r1i2t=21.32, COCO r5i2t=51.96, COCO r10i2t=67.40, COCO medr_i2t=5.00, COCO r1t2i=17.86, COCO r5t2i=48.03, COCO r10t2i=65.32, COCO medr_t2i=6.00
2019-02-15 18:31:05,660 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 18:31:05,660 : 21.32,51.96,67.40,5.00,17.86,48.03,65.32,6.00
2019-02-15 18:31:05,660 : SentLen=71.90, WC=0.79, TreeDepth=26.12, TopConst=50.99, BShift=50.00, Tense=78.20, SubjNum=73.41, ObjNum=71.87, SOMO=49.87, CoordInv=51.83, average=52.50
2019-02-15 18:31:05,660 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 18:31:05,660 : 71.90,0.79,26.12,50.99,50.00,78.20,73.41,71.87,49.87,51.83,52.50
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,661 : layer 2
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,661 : ********************************************************************************
2019-02-15 18:31:05,758 : ***** Transfer task : STS12 *****


2019-02-15 18:31:05,771 : loading BERT model bert-base-uncased
2019-02-15 18:31:05,771 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:05,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:05,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpndmo5n37
2019-02-15 18:31:08,308 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:11,569 : MSRpar : pearson = 0.2096, spearman = 0.2474
2019-02-15 18:31:12,386 : MSRvid : pearson = 0.1847, spearman = 0.2012
2019-02-15 18:31:13,099 : SMTeuroparl : pearson = 0.4128, spearman = 0.5718
2019-02-15 18:31:14,385 : surprise.OnWN : pearson = 0.1799, spearman = 0.2066
2019-02-15 18:31:15,092 : surprise.SMTnews : pearson = 0.4149, spearman = 0.3424
2019-02-15 18:31:15,092 : ALL (weighted average) : Pearson = 0.2528,             Spearman = 0.2865
2019-02-15 18:31:15,092 : ALL (average) : Pearson = 0.2804,             Spearman = 0.3139

2019-02-15 18:31:15,092 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 18:31:15,102 : loading BERT model bert-base-uncased
2019-02-15 18:31:15,102 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:15,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:15,120 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbbycefpd
2019-02-15 18:31:17,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:19,710 : FNWN : pearson = 0.2459, spearman = 0.2579
2019-02-15 18:31:20,675 : headlines : pearson = 0.0788, spearman = 0.2285
2019-02-15 18:31:21,400 : OnWN : pearson = 0.1281, spearman = 0.1231
2019-02-15 18:31:21,401 : ALL (weighted average) : Pearson = 0.1183,             Spearman = 0.1928
2019-02-15 18:31:21,401 : ALL (average) : Pearson = 0.1509,             Spearman = 0.2032

2019-02-15 18:31:21,401 : ***** Transfer task : STS14 *****


2019-02-15 18:31:21,416 : loading BERT model bert-base-uncased
2019-02-15 18:31:21,416 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:21,433 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:21,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4pzvclau
2019-02-15 18:31:23,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:26,051 : deft-forum : pearson = 0.1154, spearman = 0.1576
2019-02-15 18:31:26,752 : deft-news : pearson = 0.3808, spearman = 0.4890
2019-02-15 18:31:27,716 : headlines : pearson = 0.1046, spearman = 0.1956
2019-02-15 18:31:28,642 : images : pearson = 0.1633, spearman = 0.2808
2019-02-15 18:31:29,574 : OnWN : pearson = 0.1605, spearman = 0.1957
2019-02-15 18:31:30,793 : tweet-news : pearson = 0.2102, spearman = 0.2721
2019-02-15 18:31:30,793 : ALL (weighted average) : Pearson = 0.1720,             Spearman = 0.2469
2019-02-15 18:31:30,793 : ALL (average) : Pearson = 0.1891,             Spearman = 0.2651

2019-02-15 18:31:30,793 : ***** Transfer task : STS15 *****


2019-02-15 18:31:30,827 : loading BERT model bert-base-uncased
2019-02-15 18:31:30,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:30,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:30,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy250grsn
2019-02-15 18:31:33,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:35,678 : answers-forums : pearson = 0.2032, spearman = 0.2481
2019-02-15 18:31:36,604 : answers-students : pearson = 0.2900, spearman = 0.3214
2019-02-15 18:31:37,463 : belief : pearson = 0.2431, spearman = 0.3387
2019-02-15 18:31:38,474 : headlines : pearson = 0.1952, spearman = 0.3342
2019-02-15 18:31:39,451 : images : pearson = 0.0614, spearman = 0.3329
2019-02-15 18:31:39,451 : ALL (weighted average) : Pearson = 0.1924,             Spearman = 0.3205
2019-02-15 18:31:39,451 : ALL (average) : Pearson = 0.1986,             Spearman = 0.3151

2019-02-15 18:31:39,451 : ***** Transfer task : STS16 *****


2019-02-15 18:31:39,525 : loading BERT model bert-base-uncased
2019-02-15 18:31:39,525 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:39,546 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:39,546 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpea5h6ifn
2019-02-15 18:31:42,060 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:43,940 : answer-answer : pearson = 0.1994, spearman = 0.2954
2019-02-15 18:31:44,247 : headlines : pearson = 0.2209, spearman = 0.4161
2019-02-15 18:31:44,637 : plagiarism : pearson = 0.3155, spearman = 0.4268
2019-02-15 18:31:45,257 : postediting : pearson = 0.4109, spearman = 0.7151
2019-02-15 18:31:45,533 : question-question : pearson = -0.0203, spearman = -0.0059
2019-02-15 18:31:45,534 : ALL (weighted average) : Pearson = 0.2312,             Spearman = 0.3795
2019-02-15 18:31:45,534 : ALL (average) : Pearson = 0.2253,             Spearman = 0.3695

2019-02-15 18:31:45,534 : ***** Transfer task : MR *****


2019-02-15 18:31:45,556 : loading BERT model bert-base-uncased
2019-02-15 18:31:45,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:31:45,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:31:45,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp43nf7l1x
2019-02-15 18:31:48,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:31:49,624 : Generating sentence embeddings
2019-02-15 18:32:03,076 : Generated sentence embeddings
2019-02-15 18:32:03,077 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 18:32:19,272 : Best param found at split 1: l2reg = 1e-05                 with score 66.87
2019-02-15 18:32:36,404 : Best param found at split 2: l2reg = 1e-05                 with score 63.61
2019-02-15 18:32:53,716 : Best param found at split 3: l2reg = 0.001                 with score 62.61
2019-02-15 18:33:11,645 : Best param found at split 4: l2reg = 0.0001                 with score 64.14
2019-02-15 18:33:29,002 : Best param found at split 5: l2reg = 1e-05                 with score 61.66
2019-02-15 18:33:30,365 : Dev acc : 63.78 Test acc : 63.21

2019-02-15 18:33:30,366 : ***** Transfer task : CR *****


2019-02-15 18:33:30,377 : loading BERT model bert-base-uncased
2019-02-15 18:33:30,377 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:33:30,401 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:33:30,401 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp88dmrrc3
2019-02-15 18:33:32,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:33:34,356 : Generating sentence embeddings
2019-02-15 18:33:38,085 : Generated sentence embeddings
2019-02-15 18:33:38,086 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 18:33:41,934 : Best param found at split 1: l2reg = 0.0001                 with score 66.51
2019-02-15 18:33:47,180 : Best param found at split 2: l2reg = 0.001                 with score 66.68
2019-02-15 18:33:53,176 : Best param found at split 3: l2reg = 1e-05                 with score 69.44
2019-02-15 18:33:58,781 : Best param found at split 4: l2reg = 1e-05                 with score 68.22
2019-02-15 18:34:05,331 : Best param found at split 5: l2reg = 0.001                 with score 67.06
2019-02-15 18:34:05,664 : Dev acc : 67.58 Test acc : 65.75

2019-02-15 18:34:05,664 : ***** Transfer task : MPQA *****


2019-02-15 18:34:05,671 : loading BERT model bert-base-uncased
2019-02-15 18:34:05,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:34:05,692 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:34:05,692 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptg9hsg13
2019-02-15 18:34:08,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:34:09,764 : Generating sentence embeddings
2019-02-15 18:34:13,504 : Generated sentence embeddings
2019-02-15 18:34:13,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 18:34:28,279 : Best param found at split 1: l2reg = 0.0001                 with score 78.3
2019-02-15 18:34:44,391 : Best param found at split 2: l2reg = 0.001                 with score 81.03
2019-02-15 18:35:04,727 : Best param found at split 3: l2reg = 0.0001                 with score 80.67
2019-02-15 18:35:21,141 : Best param found at split 4: l2reg = 1e-05                 with score 80.0
2019-02-15 18:35:40,040 : Best param found at split 5: l2reg = 0.001                 with score 81.62
2019-02-15 18:35:41,375 : Dev acc : 80.32 Test acc : 82.04

2019-02-15 18:35:41,376 : ***** Transfer task : SUBJ *****


2019-02-15 18:35:41,398 : loading BERT model bert-base-uncased
2019-02-15 18:35:41,398 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:35:41,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:35:41,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqnrgs7mv
2019-02-15 18:35:43,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:35:45,430 : Generating sentence embeddings
2019-02-15 18:35:58,751 : Generated sentence embeddings
2019-02-15 18:35:58,752 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 18:36:16,320 : Best param found at split 1: l2reg = 0.001                 with score 89.32
2019-02-15 18:36:35,777 : Best param found at split 2: l2reg = 1e-05                 with score 90.18
2019-02-15 18:36:54,867 : Best param found at split 3: l2reg = 0.0001                 with score 89.92
2019-02-15 18:37:10,598 : Best param found at split 4: l2reg = 1e-05                 with score 89.52
2019-02-15 18:37:28,580 : Best param found at split 5: l2reg = 0.0001                 with score 89.15
2019-02-15 18:37:30,098 : Dev acc : 89.62 Test acc : 89.34

2019-02-15 18:37:30,099 : ***** Transfer task : SST Binary classification *****


2019-02-15 18:37:30,235 : loading BERT model bert-base-uncased
2019-02-15 18:37:30,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:37:30,267 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:37:30,267 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3jjbvbwl
2019-02-15 18:37:32,758 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:37:34,236 : Computing embedding for train
2019-02-15 18:38:19,716 : Computed train embeddings
2019-02-15 18:38:19,716 : Computing embedding for dev
2019-02-15 18:38:20,651 : Computed dev embeddings
2019-02-15 18:38:20,651 : Computing embedding for test
2019-02-15 18:38:22,649 : Computed test embeddings
2019-02-15 18:38:22,649 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:38:56,708 : [('reg:1e-05', 76.61), ('reg:0.0001', 76.61), ('reg:0.001', 72.02), ('reg:0.01', 68.12)]
2019-02-15 18:38:56,708 : Validation : best param found is reg = 1e-05 with score             76.61
2019-02-15 18:38:56,708 : Evaluating...
2019-02-15 18:39:08,643 : 
Dev acc : 76.61 Test acc : 77.32 for             SST Binary classification

2019-02-15 18:39:08,644 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 18:39:08,704 : loading BERT model bert-base-uncased
2019-02-15 18:39:08,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:39:08,727 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:39:08,727 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1m7ez4s2
2019-02-15 18:39:11,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:39:12,660 : Computing embedding for train
2019-02-15 18:39:22,262 : Computed train embeddings
2019-02-15 18:39:22,262 : Computing embedding for dev
2019-02-15 18:39:23,462 : Computed dev embeddings
2019-02-15 18:39:23,462 : Computing embedding for test
2019-02-15 18:39:25,850 : Computed test embeddings
2019-02-15 18:39:25,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:39:29,879 : [('reg:1e-05', 33.97), ('reg:0.0001', 33.79), ('reg:0.001', 33.7), ('reg:0.01', 30.06)]
2019-02-15 18:39:29,879 : Validation : best param found is reg = 1e-05 with score             33.97
2019-02-15 18:39:29,879 : Evaluating...
2019-02-15 18:39:31,434 : 
Dev acc : 33.97 Test acc : 35.29 for             SST Fine-Grained classification

2019-02-15 18:39:31,435 : ***** Transfer task : TREC *****


2019-02-15 18:39:31,452 : loading BERT model bert-base-uncased
2019-02-15 18:39:31,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:39:31,476 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:39:31,476 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwdbeo45b
2019-02-15 18:39:33,988 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:39:38,917 : Computed train embeddings
2019-02-15 18:39:39,185 : Computed test embeddings
2019-02-15 18:39:39,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 18:39:51,783 : [('reg:1e-05', 55.58), ('reg:0.0001', 54.83), ('reg:0.001', 49.45), ('reg:0.01', 35.03)]
2019-02-15 18:39:51,783 : Cross-validation : best param found is reg = 1e-05             with score 55.58
2019-02-15 18:39:51,783 : Evaluating...
2019-02-15 18:39:52,520 : 
Dev acc : 55.58 Test acc : 70.6             for TREC

2019-02-15 18:39:52,521 : ***** Transfer task : MRPC *****


2019-02-15 18:39:52,577 : loading BERT model bert-base-uncased
2019-02-15 18:39:52,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:39:52,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:39:52,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp0oo3m5n
2019-02-15 18:39:55,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:39:56,568 : Computing embedding for train
2019-02-15 18:40:06,170 : Computed train embeddings
2019-02-15 18:40:06,170 : Computing embedding for test
2019-02-15 18:40:10,480 : Computed test embeddings
2019-02-15 18:40:10,496 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 18:40:18,848 : [('reg:1e-05', 69.92), ('reg:0.0001', 69.06), ('reg:0.001', 69.26), ('reg:0.01', 68.64)]
2019-02-15 18:40:18,848 : Cross-validation : best param found is reg = 1e-05             with score 69.92
2019-02-15 18:40:18,848 : Evaluating...
2019-02-15 18:40:19,689 : Dev acc : 69.92 Test acc 66.78; Test F1 72.78 for MRPC.

2019-02-15 18:40:19,690 : ***** Transfer task : SICK-Entailment*****


2019-02-15 18:40:19,717 : loading BERT model bert-base-uncased
2019-02-15 18:40:19,718 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:40:19,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:40:19,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpapshh_yi
2019-02-15 18:40:22,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:40:23,649 : Computing embedding for train
2019-02-15 18:40:28,689 : Computed train embeddings
2019-02-15 18:40:28,690 : Computing embedding for dev
2019-02-15 18:40:29,364 : Computed dev embeddings
2019-02-15 18:40:29,364 : Computing embedding for test
2019-02-15 18:40:34,879 : Computed test embeddings
2019-02-15 18:40:34,907 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:40:37,367 : [('reg:1e-05', 67.2), ('reg:0.0001', 67.8), ('reg:0.001', 62.6), ('reg:0.01', 56.4)]
2019-02-15 18:40:37,367 : Validation : best param found is reg = 0.0001 with score             67.8
2019-02-15 18:40:37,367 : Evaluating...
2019-02-15 18:40:37,963 : 
Dev acc : 67.8 Test acc : 66.02 for                        SICK entailment

2019-02-15 18:40:37,963 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 18:40:37,992 : loading BERT model bert-base-uncased
2019-02-15 18:40:37,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:40:38,013 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:40:38,014 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptvanlyng
2019-02-15 18:40:40,532 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:40:42,006 : Computing embedding for train
2019-02-15 18:40:47,110 : Computed train embeddings
2019-02-15 18:40:47,110 : Computing embedding for dev
2019-02-15 18:40:47,781 : Computed dev embeddings
2019-02-15 18:40:47,781 : Computing embedding for test
2019-02-15 18:40:53,252 : Computed test embeddings
2019-02-15 18:41:31,099 : Dev : Pearson 0.7366201715154551
2019-02-15 18:41:31,099 : Test : Pearson 0.7360099592786526 Spearman 0.6785485397063359 MSE 0.46972610856880587                        for SICK Relatedness

2019-02-15 18:41:31,100 : 

***** Transfer task : STSBenchmark*****


2019-02-15 18:41:31,185 : loading BERT model bert-base-uncased
2019-02-15 18:41:31,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:41:31,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:41:31,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpph9fdjaf
2019-02-15 18:41:33,660 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:41:35,092 : Computing embedding for train
2019-02-15 18:41:43,350 : Computed train embeddings
2019-02-15 18:41:43,350 : Computing embedding for dev
2019-02-15 18:41:45,843 : Computed dev embeddings
2019-02-15 18:41:45,843 : Computing embedding for test
2019-02-15 18:41:47,825 : Computed test embeddings
2019-02-15 18:42:17,457 : Dev : Pearson 0.6165401906253217
2019-02-15 18:42:17,458 : Test : Pearson 0.5297957746046918 Spearman 0.5172197936854424 MSE 1.9830345758340644                        for SICK Relatedness

2019-02-15 18:42:17,458 : ***** Transfer task : SNLI Entailment*****


2019-02-15 18:42:22,447 : loading BERT model bert-base-uncased
2019-02-15 18:42:22,447 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:42:22,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:42:22,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe60lx1_b
2019-02-15 18:42:25,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:42:26,798 : PROGRESS (encoding): 0.00%
2019-02-15 18:43:44,724 : PROGRESS (encoding): 14.56%
2019-02-15 18:45:10,974 : PROGRESS (encoding): 29.12%
2019-02-15 18:46:41,608 : PROGRESS (encoding): 43.69%
2019-02-15 18:48:15,289 : PROGRESS (encoding): 58.25%
2019-02-15 18:50:00,042 : PROGRESS (encoding): 72.81%
2019-02-15 18:51:43,148 : PROGRESS (encoding): 87.37%
2019-02-15 18:53:32,929 : PROGRESS (encoding): 0.00%
2019-02-15 18:53:48,976 : PROGRESS (encoding): 0.00%
2019-02-15 18:54:01,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 18:54:44,173 : [('reg:1e-09', 57.62)]
2019-02-15 18:54:44,173 : Validation : best param found is reg = 1e-09 with score             57.62
2019-02-15 18:54:44,173 : Evaluating...
2019-02-15 18:55:25,653 : Dev acc : 57.62 Test acc : 57.39 for SNLI

2019-02-15 18:55:25,653 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 18:55:35,168 : loading BERT model bert-base-uncased
2019-02-15 18:55:35,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 18:55:35,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 18:55:35,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1l2581k6
2019-02-15 18:55:37,745 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 18:55:39,185 : Computing embedding for train
2019-02-15 19:03:10,179 : Computed train embeddings
2019-02-15 19:03:10,179 : Computing embedding for dev
2019-02-15 19:03:29,151 : Computed dev embeddings
2019-02-15 19:03:29,151 : Computing embedding for test
2019-02-15 19:03:49,091 : Computed test embeddings
2019-02-15 19:03:49,110 : prepare data
2019-02-15 19:03:49,177 : start epoch
2019-02-15 19:04:32,139 : samples : 64000
2019-02-15 19:04:42,648 : Image to text: 4.34, 13.94, 22.24, 42.0
2019-02-15 19:04:50,216 : Text to Image: 3.128, 11.22, 18.276, 51.0
2019-02-15 19:05:32,347 : samples : 128000
2019-02-15 19:05:42,823 : Image to text: 5.32, 17.22, 26.26, 33.0
2019-02-15 19:05:50,442 : Text to Image: 3.472, 12.936, 21.128, 43.0
2019-02-15 19:06:33,351 : samples : 192000
2019-02-15 19:06:43,803 : Image to text: 5.52, 17.92, 27.64, 30.0
2019-02-15 19:06:51,414 : Text to Image: 4.188, 14.652, 23.1, 39.0
2019-02-15 19:07:34,379 : samples : 256000
2019-02-15 19:07:44,866 : Image to text: 6.28, 19.28, 28.76, 30.0
2019-02-15 19:07:52,496 : Text to Image: 4.452, 15.248, 24.1, 37.0
2019-02-15 19:08:35,210 : samples : 320000
2019-02-15 19:08:45,644 : Image to text: 6.52, 20.06, 29.18, 29.0
2019-02-15 19:08:53,243 : Text to Image: 4.356, 15.416, 24.484, 36.0
2019-02-15 19:09:35,903 : samples : 384000
2019-02-15 19:09:46,344 : Image to text: 6.52, 20.66, 31.28, 25.0
2019-02-15 19:09:53,969 : Text to Image: 5.228, 17.128, 26.744, 32.0
2019-02-15 19:10:36,592 : samples : 448000
2019-02-15 19:10:47,096 : Image to text: 7.34, 21.28, 31.32, 26.0
2019-02-15 19:10:54,810 : Text to Image: 5.392, 18.296, 27.74, 29.0
2019-02-15 19:11:37,981 : samples : 512000
2019-02-15 19:11:48,460 : Image to text: 6.72, 20.86, 30.82, 25.0
2019-02-15 19:11:56,313 : Text to Image: 5.004, 17.244, 26.756, 32.0
2019-02-15 19:12:33,146 : Epoch 1 finished
2019-02-15 19:12:33,594 : Image to text: 17.4, 48.5, 66.7, 6.0
2019-02-15 19:12:33,936 : Text to Image: 15.34, 41.52, 59.98, 8.0
2019-02-15 19:12:34,391 : Image to text: 19.4, 50.6, 66.1, 5.0
2019-02-15 19:12:34,735 : Text to Image: 14.34, 42.44, 60.76, 7.0
2019-02-15 19:12:35,166 : Image to text: 19.6, 48.4, 66.0, 6.0
2019-02-15 19:12:35,500 : Text to Image: 14.36, 41.74, 57.8, 8.0
2019-02-15 19:12:35,932 : Image to text: 20.4, 49.0, 66.0, 6.0
2019-02-15 19:12:36,272 : Text to Image: 13.82, 41.16, 58.62, 8.0
2019-02-15 19:12:36,727 : Image to text: 20.5, 50.9, 66.2, 5.0
2019-02-15 19:12:37,064 : Text to Image: 14.88, 43.06, 59.98, 7.0
2019-02-15 19:12:37,064 : Dev mean Text to Image: 14.547999999999998, 41.984, 59.428, 7.6
2019-02-15 19:12:37,064 : Dev mean Image to text: 19.46, 49.48, 66.19999999999999, 5.6000000000000005
2019-02-15 19:12:37,065 : start epoch
2019-02-15 19:13:19,532 : samples : 64000
2019-02-15 19:13:29,983 : Image to text: 7.16, 22.34, 33.0, 23.0
2019-02-15 19:13:37,647 : Text to Image: 5.568, 18.316, 28.256, 29.0
2019-02-15 19:14:19,797 : samples : 128000
2019-02-15 19:14:30,272 : Image to text: 6.78, 21.64, 32.94, 23.0
2019-02-15 19:14:38,096 : Text to Image: 5.852, 19.08, 29.104, 28.0
2019-02-15 19:15:21,118 : samples : 192000
2019-02-15 19:15:31,540 : Image to text: 7.22, 22.32, 33.54, 23.0
2019-02-15 19:15:39,228 : Text to Image: 5.764, 18.712, 28.816, 29.0
2019-02-15 19:16:21,859 : samples : 256000
2019-02-15 19:16:32,351 : Image to text: 8.06, 24.08, 34.5, 22.0
2019-02-15 19:16:40,097 : Text to Image: 6.484, 20.284, 30.66, 26.0
2019-02-15 19:17:22,388 : samples : 320000
2019-02-15 19:17:32,830 : Image to text: 7.98, 23.62, 34.68, 21.0
2019-02-15 19:17:40,518 : Text to Image: 6.116, 19.452, 29.716, 27.0
2019-02-15 19:18:23,086 : samples : 384000
2019-02-15 19:18:33,549 : Image to text: 7.12, 22.16, 33.76, 23.0
2019-02-15 19:18:41,218 : Text to Image: 5.284, 18.008, 27.936, 30.0
2019-02-15 19:19:28,716 : samples : 448000
2019-02-15 19:19:41,703 : Image to text: 7.68, 24.36, 35.18, 21.0
2019-02-15 19:19:49,983 : Text to Image: 6.212, 20.228, 30.924, 25.0
2019-02-15 19:20:32,628 : samples : 512000
2019-02-15 19:20:43,128 : Image to text: 7.54, 23.6, 34.9, 22.0
2019-02-15 19:20:50,720 : Text to Image: 5.612, 18.644, 28.604, 29.0
2019-02-15 19:21:26,789 : Epoch 2 finished
2019-02-15 19:21:27,241 : Image to text: 21.5, 51.9, 66.8, 5.0
2019-02-15 19:21:27,586 : Text to Image: 18.34, 48.76, 65.76, 6.0
2019-02-15 19:21:28,047 : Image to text: 21.4, 53.6, 68.7, 5.0
2019-02-15 19:21:28,390 : Text to Image: 18.0, 48.46, 66.12, 6.0
2019-02-15 19:21:28,850 : Image to text: 23.2, 52.4, 69.4, 5.0
2019-02-15 19:21:29,189 : Text to Image: 17.84, 48.2, 65.7, 6.0
2019-02-15 19:21:29,629 : Image to text: 23.3, 53.8, 70.3, 5.0
2019-02-15 19:21:29,978 : Text to Image: 18.16, 48.3, 66.2, 6.0
2019-02-15 19:21:30,424 : Image to text: 24.2, 54.7, 70.2, 4.0
2019-02-15 19:21:30,754 : Text to Image: 18.14, 49.04, 65.96, 6.0
2019-02-15 19:21:30,754 : Dev mean Text to Image: 18.096, 48.552, 65.94800000000001, 6.0
2019-02-15 19:21:30,754 : Dev mean Image to text: 22.72, 53.28, 69.08000000000001, 4.8
2019-02-15 19:21:30,755 : start epoch
2019-02-15 19:22:14,150 : samples : 64000
2019-02-15 19:22:24,661 : Image to text: 8.02, 22.86, 34.62, 22.0
2019-02-15 19:22:32,309 : Text to Image: 6.032, 19.152, 29.252, 28.0
2019-02-15 19:23:14,501 : samples : 128000
2019-02-15 19:23:24,983 : Image to text: 8.22, 23.3, 35.32, 21.0
2019-02-15 19:23:32,573 : Text to Image: 6.604, 20.828, 31.196, 25.0
2019-02-15 19:24:15,149 : samples : 192000
2019-02-15 19:24:25,528 : Image to text: 7.98, 24.66, 36.84, 20.0
2019-02-15 19:24:33,158 : Text to Image: 6.596, 20.892, 31.484, 25.0
2019-02-15 19:25:16,075 : samples : 256000
2019-02-15 19:25:26,574 : Image to text: 8.3, 23.5, 34.72, 22.0
2019-02-15 19:25:34,343 : Text to Image: 6.468, 20.108, 30.576, 26.0
2019-02-15 19:26:17,222 : samples : 320000
2019-02-15 19:26:27,690 : Image to text: 7.86, 23.64, 35.02, 21.0
2019-02-15 19:26:35,333 : Text to Image: 6.272, 18.952, 29.04, 28.0
2019-02-15 19:27:18,329 : samples : 384000
2019-02-15 19:27:28,776 : Image to text: 7.88, 24.64, 36.0, 20.0
2019-02-15 19:27:36,502 : Text to Image: 6.84, 21.076, 31.764, 24.0
2019-02-15 19:28:19,323 : samples : 448000
2019-02-15 19:28:29,723 : Image to text: 8.66, 25.14, 36.3, 20.0
2019-02-15 19:28:37,342 : Text to Image: 7.084, 21.532, 32.684, 24.0
2019-02-15 19:29:19,866 : samples : 512000
2019-02-15 19:29:30,342 : Image to text: 7.8, 24.1, 35.62, 21.0
2019-02-15 19:29:37,941 : Text to Image: 5.156, 18.464, 28.408, 29.0
2019-02-15 19:30:14,384 : Epoch 3 finished
2019-02-15 19:30:14,826 : Image to text: 22.3, 52.6, 69.3, 5.0
2019-02-15 19:30:15,160 : Text to Image: 17.8, 48.88, 66.02, 6.0
2019-02-15 19:30:15,595 : Image to text: 21.2, 53.1, 70.0, 5.0
2019-02-15 19:30:15,941 : Text to Image: 18.2, 49.34, 66.58, 6.0
2019-02-15 19:30:16,385 : Image to text: 23.6, 53.4, 69.7, 5.0
2019-02-15 19:30:16,720 : Text to Image: 17.9, 49.82, 66.84, 6.0
2019-02-15 19:30:17,168 : Image to text: 24.3, 54.3, 71.2, 5.0
2019-02-15 19:30:17,510 : Text to Image: 18.42, 50.0, 67.4, 5.0
2019-02-15 19:30:17,974 : Image to text: 25.2, 55.1, 71.0, 4.0
2019-02-15 19:30:18,329 : Text to Image: 18.74, 50.5, 67.34, 5.0
2019-02-15 19:30:18,329 : Dev mean Text to Image: 18.212, 49.708, 66.836, 5.6
2019-02-15 19:30:18,329 : Dev mean Image to text: 23.32, 53.7, 70.24, 4.8
2019-02-15 19:30:18,330 : start epoch
2019-02-15 19:31:00,930 : samples : 64000
2019-02-15 19:31:11,398 : Image to text: 8.9, 25.3, 37.12, 19.0
2019-02-15 19:31:19,076 : Text to Image: 7.188, 21.2, 32.192, 24.0
2019-02-15 19:32:01,643 : samples : 128000
2019-02-15 19:32:12,117 : Image to text: 9.22, 26.12, 37.94, 19.0
2019-02-15 19:32:19,808 : Text to Image: 6.832, 21.264, 32.216, 24.0
2019-02-15 19:33:02,820 : samples : 192000
2019-02-15 19:33:13,258 : Image to text: 8.02, 24.3, 35.72, 20.0
2019-02-15 19:33:20,888 : Text to Image: 6.76, 21.028, 31.464, 25.0
2019-02-15 19:34:03,226 : samples : 256000
2019-02-15 19:34:13,696 : Image to text: 9.08, 25.6, 37.62, 19.0
2019-02-15 19:34:21,351 : Text to Image: 7.436, 22.364, 33.552, 22.0
2019-02-15 19:35:03,462 : samples : 320000
2019-02-15 19:35:13,898 : Image to text: 8.68, 25.64, 37.0, 19.0
2019-02-15 19:35:21,549 : Text to Image: 7.408, 22.628, 34.088, 22.0
2019-02-15 19:36:06,106 : samples : 384000
2019-02-15 19:36:18,973 : Image to text: 7.92, 24.42, 36.3, 20.0
2019-02-15 19:36:27,577 : Text to Image: 6.808, 21.564, 32.72, 24.0
2019-02-15 19:37:11,882 : samples : 448000
2019-02-15 19:37:22,354 : Image to text: 8.86, 26.18, 38.3, 18.0
2019-02-15 19:37:29,896 : Text to Image: 7.424, 22.392, 33.38, 23.0
2019-02-15 19:38:12,561 : samples : 512000
2019-02-15 19:38:22,986 : Image to text: 8.5, 25.64, 37.6, 19.0
2019-02-15 19:38:30,560 : Text to Image: 7.204, 22.528, 33.324, 23.0
2019-02-15 19:39:06,331 : Epoch 4 finished
2019-02-15 19:39:06,772 : Image to text: 24.4, 54.9, 72.2, 4.0
2019-02-15 19:39:07,114 : Text to Image: 18.94, 48.88, 67.58, 6.0
2019-02-15 19:39:07,549 : Image to text: 23.0, 55.2, 71.5, 5.0
2019-02-15 19:39:07,890 : Text to Image: 18.8, 49.64, 67.4, 6.0
2019-02-15 19:39:08,331 : Image to text: 24.4, 57.6, 73.6, 4.0
2019-02-15 19:39:08,674 : Text to Image: 19.14, 49.22, 67.08, 6.0
2019-02-15 19:39:09,115 : Image to text: 26.5, 57.9, 73.1, 4.0
2019-02-15 19:39:09,456 : Text to Image: 19.32, 49.04, 66.42, 6.0
2019-02-15 19:39:09,912 : Image to text: 28.4, 57.6, 72.3, 4.0
2019-02-15 19:39:10,252 : Text to Image: 19.76, 50.02, 66.64, 5.0
2019-02-15 19:39:10,252 : Dev mean Text to Image: 19.192000000000004, 49.36, 67.024, 5.8
2019-02-15 19:39:10,252 : Dev mean Image to text: 25.34, 56.64, 72.53999999999999, 4.2
2019-02-15 19:39:10,252 : start epoch
2019-02-15 19:39:53,429 : samples : 64000
2019-02-15 19:40:06,016 : Image to text: 9.2, 26.6, 37.96, 18.0
2019-02-15 19:40:16,099 : Text to Image: 7.692, 23.028, 34.208, 22.0
2019-02-15 19:41:01,751 : samples : 128000
2019-02-15 19:41:13,573 : Image to text: 9.4, 26.46, 38.38, 18.0
2019-02-15 19:41:20,938 : Text to Image: 7.184, 22.616, 33.772, 22.0
2019-02-15 19:42:03,546 : samples : 192000
2019-02-15 19:42:14,118 : Image to text: 9.16, 26.38, 38.28, 18.0
2019-02-15 19:42:24,110 : Text to Image: 7.332, 22.688, 33.652, 23.0
2019-02-15 19:43:08,603 : samples : 256000
2019-02-15 19:43:21,204 : Image to text: 8.9, 25.8, 38.8, 18.0
2019-02-15 19:43:31,213 : Text to Image: 7.724, 23.476, 34.948, 21.0
2019-02-15 19:44:13,454 : samples : 320000
2019-02-15 19:44:23,646 : Image to text: 9.16, 26.58, 39.18, 18.0
2019-02-15 19:44:30,823 : Text to Image: 7.74, 23.136, 34.644, 21.0
2019-02-15 19:45:13,150 : samples : 384000
2019-02-15 19:45:25,719 : Image to text: 9.08, 26.84, 38.6, 17.0
2019-02-15 19:45:35,733 : Text to Image: 7.576, 22.632, 33.8, 22.0
2019-02-15 19:46:20,701 : samples : 448000
2019-02-15 19:46:32,489 : Image to text: 8.94, 26.0, 38.76, 18.0
2019-02-15 19:46:39,877 : Text to Image: 7.536, 23.156, 34.18, 22.0
2019-02-15 19:47:22,504 : samples : 512000
2019-02-15 19:47:33,241 : Image to text: 9.48, 27.5, 39.2, 17.0
2019-02-15 19:47:43,176 : Text to Image: 7.904, 23.256, 34.756, 21.0
2019-02-15 19:48:21,078 : Epoch 5 finished
2019-02-15 19:48:22,030 : Image to text: 24.3, 55.9, 70.7, 4.0
2019-02-15 19:48:22,837 : Text to Image: 18.86, 50.2, 67.92, 5.0
2019-02-15 19:48:23,790 : Image to text: 24.2, 55.8, 73.6, 5.0
2019-02-15 19:48:24,555 : Text to Image: 18.76, 49.56, 68.0, 6.0
2019-02-15 19:48:25,544 : Image to text: 25.2, 57.5, 72.5, 4.0
2019-02-15 19:48:26,295 : Text to Image: 19.14, 50.34, 67.1, 5.0
2019-02-15 19:48:27,259 : Image to text: 24.5, 57.9, 72.7, 4.0
2019-02-15 19:48:28,038 : Text to Image: 19.54, 51.04, 68.1, 5.0
2019-02-15 19:48:28,992 : Image to text: 26.2, 58.3, 72.9, 4.0
2019-02-15 19:48:29,739 : Text to Image: 20.0, 51.38, 68.32, 5.0
2019-02-15 19:48:29,739 : Dev mean Text to Image: 19.259999999999998, 50.504000000000005, 67.88799999999999, 5.2
2019-02-15 19:48:29,739 : Dev mean Image to text: 24.880000000000003, 57.08, 72.48, 4.2
2019-02-15 19:48:29,740 : start epoch
2019-02-15 19:49:13,122 : samples : 64000
2019-02-15 19:49:23,389 : Image to text: 9.24, 26.84, 38.76, 18.0
2019-02-15 19:49:30,776 : Text to Image: 7.84, 23.536, 34.564, 21.0
2019-02-15 19:50:14,113 : samples : 128000
2019-02-15 19:50:26,698 : Image to text: 7.94, 25.52, 37.74, 18.0
2019-02-15 19:50:36,712 : Text to Image: 7.108, 21.84, 32.832, 24.0
2019-02-15 19:51:21,132 : samples : 192000
2019-02-15 19:51:31,361 : Image to text: 9.26, 26.42, 38.34, 19.0
2019-02-15 19:51:38,701 : Text to Image: 7.452, 22.908, 34.388, 22.0
2019-02-15 19:52:21,246 : samples : 256000
2019-02-15 19:52:33,694 : Image to text: 9.86, 28.0, 39.54, 18.0
2019-02-15 19:52:43,901 : Text to Image: 7.912, 23.552, 34.756, 22.0
2019-02-15 19:53:34,910 : samples : 320000
2019-02-15 19:53:47,260 : Image to text: 8.98, 26.22, 38.58, 18.0
2019-02-15 19:53:54,592 : Text to Image: 7.696, 22.816, 33.732, 22.0
2019-02-15 19:54:37,319 : samples : 384000
2019-02-15 19:54:47,524 : Image to text: 9.42, 26.98, 38.58, 17.0
2019-02-15 19:54:54,782 : Text to Image: 7.82, 23.464, 34.848, 21.0
2019-02-15 19:55:38,813 : samples : 448000
2019-02-15 19:55:51,383 : Image to text: 9.44, 27.64, 39.72, 17.0
2019-02-15 19:56:01,452 : Text to Image: 7.656, 23.016, 34.424, 21.0
2019-02-15 19:56:45,074 : samples : 512000
2019-02-15 19:56:55,275 : Image to text: 9.74, 27.64, 39.04, 17.0
2019-02-15 19:57:02,611 : Text to Image: 7.388, 23.064, 34.384, 21.0
2019-02-15 19:57:39,335 : Epoch 6 finished
2019-02-15 19:57:40,265 : Image to text: 24.0, 55.4, 72.6, 4.0
2019-02-15 19:57:41,016 : Text to Image: 19.76, 52.54, 70.94, 5.0
2019-02-15 19:57:41,937 : Image to text: 24.7, 58.3, 72.2, 4.0
2019-02-15 19:57:42,719 : Text to Image: 20.66, 51.68, 69.0, 5.0
2019-02-15 19:57:43,653 : Image to text: 24.3, 59.6, 73.3, 4.0
2019-02-15 19:57:44,429 : Text to Image: 20.64, 53.74, 69.44, 5.0
2019-02-15 19:57:45,348 : Image to text: 25.5, 60.0, 75.7, 4.0
2019-02-15 19:57:46,133 : Text to Image: 20.2, 52.74, 69.78, 5.0
2019-02-15 19:57:47,051 : Image to text: 28.7, 59.6, 73.9, 4.0
2019-02-15 19:57:47,801 : Text to Image: 21.06, 52.42, 69.96, 5.0
2019-02-15 19:57:47,801 : Dev mean Text to Image: 20.464, 52.624, 69.824, 5.0
2019-02-15 19:57:47,801 : Dev mean Image to text: 25.439999999999998, 58.580000000000005, 73.54, 4.0
2019-02-15 19:57:47,801 : start epoch
2019-02-15 19:58:32,299 : samples : 64000
2019-02-15 19:58:44,905 : Image to text: 9.16, 27.14, 40.04, 17.0
2019-02-15 19:58:54,939 : Text to Image: 8.276, 24.476, 35.996, 20.0
2019-02-15 19:59:37,513 : samples : 128000
2019-02-15 19:59:47,724 : Image to text: 9.62, 27.7, 40.12, 17.0
2019-02-15 19:59:55,127 : Text to Image: 7.352, 23.044, 34.268, 22.0
2019-02-15 20:00:38,916 : samples : 192000
2019-02-15 20:00:51,509 : Image to text: 9.48, 27.38, 39.58, 17.0
2019-02-15 20:01:01,566 : Text to Image: 8.156, 24.472, 36.192, 20.0
2019-02-15 20:01:46,301 : samples : 256000
2019-02-15 20:01:56,547 : Image to text: 9.52, 27.16, 38.68, 18.0
2019-02-15 20:02:03,898 : Text to Image: 7.956, 23.236, 34.752, 22.0
2019-02-15 20:02:46,422 : samples : 320000
2019-02-15 20:02:59,018 : Image to text: 9.74, 27.58, 39.46, 17.0
2019-02-15 20:03:09,030 : Text to Image: 8.104, 24.12, 35.38, 21.0
2019-02-15 20:03:54,798 : samples : 384000
2019-02-15 20:04:07,361 : Image to text: 9.96, 27.7, 40.36, 17.0
2019-02-15 20:04:17,163 : Text to Image: 8.124, 24.332, 35.492, 21.0
2019-02-15 20:04:59,773 : samples : 448000
2019-02-15 20:05:09,976 : Image to text: 9.54, 27.8, 40.24, 16.0
2019-02-15 20:05:17,423 : Text to Image: 8.66, 24.88, 36.792, 19.0
2019-02-15 20:06:00,401 : samples : 512000
2019-02-15 20:06:12,236 : Image to text: 9.66, 28.62, 40.5, 17.0
2019-02-15 20:06:21,266 : Text to Image: 8.54, 24.5, 36.044, 20.0
2019-02-15 20:06:57,631 : Epoch 7 finished
2019-02-15 20:06:58,105 : Image to text: 25.1, 56.4, 72.1, 4.0
2019-02-15 20:06:58,469 : Text to Image: 20.5, 54.06, 71.12, 5.0
2019-02-15 20:06:58,927 : Image to text: 24.4, 57.7, 74.0, 4.0
2019-02-15 20:06:59,296 : Text to Image: 21.62, 53.48, 71.2, 5.0
2019-02-15 20:06:59,759 : Image to text: 24.0, 58.3, 74.4, 4.0
2019-02-15 20:07:00,127 : Text to Image: 21.46, 54.66, 71.5, 5.0
2019-02-15 20:07:00,589 : Image to text: 25.9, 58.0, 73.5, 4.0
2019-02-15 20:07:00,958 : Text to Image: 21.32, 54.0, 71.28, 5.0
2019-02-15 20:07:01,430 : Image to text: 26.6, 60.4, 73.7, 4.0
2019-02-15 20:07:01,792 : Text to Image: 22.82, 53.28, 70.58, 5.0
2019-02-15 20:07:01,792 : Dev mean Text to Image: 21.544, 53.895999999999994, 71.136, 5.0
2019-02-15 20:07:01,792 : Dev mean Image to text: 25.2, 58.160000000000004, 73.53999999999999, 4.0
2019-02-15 20:07:01,792 : start epoch
2019-02-15 20:07:44,059 : samples : 64000
2019-02-15 20:07:54,282 : Image to text: 10.12, 27.94, 39.62, 17.0
2019-02-15 20:08:01,596 : Text to Image: 7.924, 24.168, 35.64, 21.0
2019-02-15 20:08:44,707 : samples : 128000
2019-02-15 20:08:54,861 : Image to text: 9.74, 28.1, 39.92, 17.0
2019-02-15 20:09:02,261 : Text to Image: 7.964, 24.056, 35.496, 20.0
2019-02-15 20:09:49,996 : samples : 192000
2019-02-15 20:10:02,114 : Image to text: 10.32, 29.0, 41.04, 16.0
2019-02-15 20:10:12,460 : Text to Image: 8.096, 23.776, 35.124, 21.0
2019-02-15 20:10:55,205 : samples : 256000
2019-02-15 20:11:05,489 : Image to text: 9.94, 27.88, 39.9, 17.0
2019-02-15 20:11:12,821 : Text to Image: 7.928, 23.936, 35.328, 20.0
2019-02-15 20:11:55,448 : samples : 320000
2019-02-15 20:12:06,753 : Image to text: 10.78, 29.84, 42.36, 15.0
2019-02-15 20:12:16,873 : Text to Image: 8.696, 25.172, 36.728, 19.0
2019-02-15 20:13:01,310 : samples : 384000
2019-02-15 20:13:11,543 : Image to text: 10.44, 29.44, 41.04, 16.0
2019-02-15 20:13:18,894 : Text to Image: 8.308, 24.688, 36.532, 19.0
2019-02-15 20:14:02,276 : samples : 448000
2019-02-15 20:14:13,353 : Image to text: 10.74, 29.68, 41.78, 15.0
2019-02-15 20:14:23,241 : Text to Image: 8.732, 25.344, 36.696, 20.0
2019-02-15 20:15:06,457 : samples : 512000
2019-02-15 20:15:16,790 : Image to text: 10.22, 28.78, 41.98, 15.0
2019-02-15 20:15:24,299 : Text to Image: 8.392, 24.604, 35.996, 20.0
2019-02-15 20:16:01,282 : Epoch 8 finished
2019-02-15 20:16:01,758 : Image to text: 27.5, 58.3, 72.9, 4.0
2019-02-15 20:16:02,129 : Text to Image: 21.06, 53.24, 71.3, 5.0
2019-02-15 20:16:02,592 : Image to text: 25.1, 58.0, 73.8, 4.0
2019-02-15 20:16:02,967 : Text to Image: 21.46, 52.9, 70.68, 5.0
2019-02-15 20:16:03,417 : Image to text: 26.1, 58.3, 71.8, 4.0
2019-02-15 20:16:03,780 : Text to Image: 21.96, 54.3, 71.32, 5.0
2019-02-15 20:16:04,228 : Image to text: 25.0, 58.2, 73.8, 4.0
2019-02-15 20:16:04,590 : Text to Image: 21.82, 54.26, 71.52, 5.0
2019-02-15 20:16:05,036 : Image to text: 28.3, 59.2, 75.5, 4.0
2019-02-15 20:16:05,399 : Text to Image: 22.74, 54.12, 70.82, 5.0
2019-02-15 20:16:05,399 : Dev mean Text to Image: 21.808, 53.763999999999996, 71.128, 5.0
2019-02-15 20:16:05,399 : Dev mean Image to text: 26.400000000000002, 58.400000000000006, 73.56, 4.0
2019-02-15 20:16:05,400 : start epoch
2019-02-15 20:16:49,318 : samples : 64000
2019-02-15 20:17:01,877 : Image to text: 9.48, 28.0, 39.96, 16.0
2019-02-15 20:17:11,773 : Text to Image: 8.108, 23.944, 35.528, 21.0
2019-02-15 20:17:56,570 : samples : 128000
2019-02-15 20:18:09,119 : Image to text: 9.92, 27.76, 40.8, 17.0
2019-02-15 20:18:19,008 : Text to Image: 7.808, 23.476, 34.832, 21.0
2019-02-15 20:19:03,716 : samples : 192000
2019-02-15 20:19:16,337 : Image to text: 9.98, 28.4, 40.64, 17.0
2019-02-15 20:19:26,332 : Text to Image: 8.368, 24.404, 35.912, 20.0
2019-02-15 20:20:11,090 : samples : 256000
2019-02-15 20:20:23,608 : Image to text: 10.44, 28.2, 40.2, 16.0
2019-02-15 20:20:33,600 : Text to Image: 7.78, 23.64, 35.216, 21.0
2019-02-15 20:21:18,300 : samples : 320000
2019-02-15 20:21:30,898 : Image to text: 10.92, 30.1, 42.62, 15.0
2019-02-15 20:21:40,873 : Text to Image: 8.964, 25.928, 37.556, 18.0
2019-02-15 20:22:25,421 : samples : 384000
2019-02-15 20:22:38,039 : Image to text: 10.58, 28.1, 40.78, 16.0
2019-02-15 20:22:48,123 : Text to Image: 8.256, 24.964, 36.612, 19.0
2019-02-15 20:23:32,470 : samples : 448000
2019-02-15 20:23:45,077 : Image to text: 10.38, 28.78, 40.4, 16.0
2019-02-15 20:23:55,084 : Text to Image: 8.748, 25.148, 36.796, 19.0
2019-02-15 20:24:38,731 : samples : 512000
2019-02-15 20:24:51,314 : Image to text: 10.12, 28.82, 41.54, 16.0
2019-02-15 20:25:01,392 : Text to Image: 8.644, 25.324, 36.564, 20.0
2019-02-15 20:25:38,790 : Epoch 9 finished
2019-02-15 20:25:39,730 : Image to text: 24.5, 58.5, 73.1, 4.0
2019-02-15 20:25:40,477 : Text to Image: 21.32, 53.8, 71.32, 5.0
2019-02-15 20:25:41,390 : Image to text: 25.5, 58.7, 73.0, 4.0
2019-02-15 20:25:42,153 : Text to Image: 21.6, 53.32, 71.16, 5.0
2019-02-15 20:25:43,049 : Image to text: 25.5, 58.8, 74.7, 4.0
2019-02-15 20:25:43,822 : Text to Image: 22.04, 55.16, 71.66, 5.0
2019-02-15 20:25:44,747 : Image to text: 27.4, 59.7, 74.6, 4.0
2019-02-15 20:25:45,484 : Text to Image: 21.82, 53.48, 71.0, 5.0
2019-02-15 20:25:46,413 : Image to text: 27.8, 60.1, 75.8, 4.0
2019-02-15 20:25:47,186 : Text to Image: 22.9, 54.56, 71.3, 5.0
2019-02-15 20:25:47,186 : Dev mean Text to Image: 21.936, 54.064, 71.288, 5.0
2019-02-15 20:25:47,186 : Dev mean Image to text: 26.14, 59.16, 74.24, 4.0
2019-02-15 20:25:47,187 : start epoch
2019-02-15 20:26:35,620 : samples : 64000
2019-02-15 20:26:48,957 : Image to text: 10.04, 28.76, 40.56, 16.0
2019-02-15 20:26:59,219 : Text to Image: 8.388, 24.728, 36.272, 20.0
2019-02-15 20:27:44,294 : samples : 128000
2019-02-15 20:27:57,004 : Image to text: 11.1, 29.58, 42.18, 15.0
2019-02-15 20:28:05,879 : Text to Image: 8.576, 25.54, 37.128, 19.0
2019-02-15 20:28:48,227 : samples : 192000
2019-02-15 20:28:58,437 : Image to text: 10.16, 28.82, 40.74, 16.0
2019-02-15 20:29:05,842 : Text to Image: 8.18, 24.428, 35.78, 20.0
2019-02-15 20:29:49,371 : samples : 256000
2019-02-15 20:30:02,201 : Image to text: 10.34, 29.06, 41.28, 16.0
2019-02-15 20:30:12,591 : Text to Image: 8.308, 24.476, 36.3, 20.0
2019-02-15 20:30:58,045 : samples : 320000
2019-02-15 20:31:10,910 : Image to text: 11.22, 29.14, 41.98, 15.0
2019-02-15 20:31:21,236 : Text to Image: 8.564, 25.144, 37.22, 19.0
2019-02-15 20:32:07,176 : samples : 384000
2019-02-15 20:32:20,021 : Image to text: 10.52, 29.4, 41.3, 16.0
2019-02-15 20:32:30,497 : Text to Image: 8.872, 25.772, 37.82, 18.0
2019-02-15 20:33:16,493 : samples : 448000
2019-02-15 20:33:29,345 : Image to text: 9.1, 27.24, 39.84, 17.0
2019-02-15 20:33:39,874 : Text to Image: 7.984, 23.372, 34.608, 21.0
2019-02-15 20:34:25,652 : samples : 512000
2019-02-15 20:34:38,559 : Image to text: 10.2, 29.36, 41.6, 16.0
2019-02-15 20:34:49,075 : Text to Image: 8.42, 25.24, 36.6, 19.0
2019-02-15 20:35:27,763 : Epoch 10 finished
2019-02-15 20:35:28,881 : Image to text: 25.6, 59.6, 75.2, 4.0
2019-02-15 20:35:29,794 : Text to Image: 22.16, 54.44, 72.1, 5.0
2019-02-15 20:35:30,845 : Image to text: 27.6, 58.3, 73.8, 4.0
2019-02-15 20:35:31,795 : Text to Image: 21.76, 54.72, 71.84, 5.0
2019-02-15 20:35:32,831 : Image to text: 26.9, 61.2, 74.9, 4.0
2019-02-15 20:35:33,736 : Text to Image: 22.38, 55.26, 72.04, 5.0
2019-02-15 20:35:34,806 : Image to text: 26.8, 60.0, 75.6, 4.0
2019-02-15 20:35:35,660 : Text to Image: 22.48, 54.94, 71.38, 4.0
2019-02-15 20:35:36,720 : Image to text: 29.1, 61.9, 76.3, 4.0
2019-02-15 20:35:37,542 : Text to Image: 23.24, 55.0, 71.54, 5.0
2019-02-15 20:35:37,542 : Dev mean Text to Image: 22.404, 54.872, 71.78, 4.8
2019-02-15 20:35:37,542 : Dev mean Image to text: 27.2, 60.2, 75.16, 4.0
2019-02-15 20:35:37,543 : start epoch
2019-02-15 20:36:22,876 : samples : 64000
2019-02-15 20:36:35,814 : Image to text: 10.84, 30.38, 42.44, 15.0
2019-02-15 20:36:46,250 : Text to Image: 9.176, 26.216, 38.052, 18.0
2019-02-15 20:37:31,843 : samples : 128000
2019-02-15 20:37:42,925 : Image to text: 10.62, 28.82, 40.7, 16.0
2019-02-15 20:37:50,465 : Text to Image: 8.54, 25.464, 36.988, 19.0
2019-02-15 20:38:33,565 : samples : 192000
2019-02-15 20:38:44,081 : Image to text: 10.6, 29.8, 41.7, 15.0
2019-02-15 20:38:51,585 : Text to Image: 8.728, 25.124, 36.992, 19.0
2019-02-15 20:39:34,589 : samples : 256000
2019-02-15 20:39:45,142 : Image to text: 10.82, 30.24, 42.24, 15.0
2019-02-15 20:39:52,680 : Text to Image: 8.64, 24.952, 36.572, 19.0
2019-02-15 20:40:35,164 : samples : 320000
2019-02-15 20:40:45,609 : Image to text: 10.38, 28.82, 41.4, 16.0
2019-02-15 20:40:53,181 : Text to Image: 8.316, 25.232, 36.808, 19.0
2019-02-15 20:41:35,387 : samples : 384000
2019-02-15 20:41:45,844 : Image to text: 9.92, 29.7, 42.48, 15.0
2019-02-15 20:41:53,389 : Text to Image: 8.796, 25.784, 37.492, 19.0
2019-02-15 20:42:36,409 : samples : 448000
2019-02-15 20:42:46,801 : Image to text: 10.34, 29.64, 42.04, 15.0
2019-02-15 20:42:54,112 : Text to Image: 8.592, 25.112, 36.724, 19.0
2019-02-15 20:43:46,614 : samples : 512000
2019-02-15 20:43:57,042 : Image to text: 9.78, 29.88, 41.62, 15.0
2019-02-15 20:44:04,620 : Text to Image: 8.476, 24.876, 36.584, 20.0
2019-02-15 20:44:41,027 : Epoch 11 finished
2019-02-15 20:44:41,479 : Image to text: 25.6, 58.2, 73.5, 4.0
2019-02-15 20:44:41,818 : Text to Image: 21.54, 55.36, 72.58, 4.0
2019-02-15 20:44:42,281 : Image to text: 26.7, 59.0, 74.6, 4.0
2019-02-15 20:44:42,637 : Text to Image: 22.44, 54.92, 71.96, 5.0
2019-02-15 20:44:43,106 : Image to text: 27.9, 59.9, 74.4, 4.0
2019-02-15 20:44:43,470 : Text to Image: 22.5, 55.14, 71.98, 4.0
2019-02-15 20:44:43,915 : Image to text: 27.1, 60.6, 76.0, 4.0
2019-02-15 20:44:44,215 : Text to Image: 21.92, 55.52, 72.72, 4.0
2019-02-15 20:44:44,617 : Image to text: 29.3, 59.7, 74.8, 4.0
2019-02-15 20:44:44,917 : Text to Image: 23.88, 55.14, 72.08, 4.0
2019-02-15 20:44:44,917 : Dev mean Text to Image: 22.456, 55.215999999999994, 72.264, 4.2
2019-02-15 20:44:44,917 : Dev mean Image to text: 27.32, 59.480000000000004, 74.66, 4.0
2019-02-15 20:44:44,917 : start epoch
2019-02-15 20:45:27,689 : samples : 64000
2019-02-15 20:45:38,071 : Image to text: 10.76, 30.04, 42.6, 15.0
2019-02-15 20:45:45,711 : Text to Image: 9.316, 26.436, 38.248, 18.0
2019-02-15 20:46:28,289 : samples : 128000
2019-02-15 20:46:38,659 : Image to text: 10.44, 29.36, 42.56, 15.0
2019-02-15 20:46:46,186 : Text to Image: 8.56, 24.768, 36.364, 20.0
2019-02-15 20:47:28,354 : samples : 192000
2019-02-15 20:47:38,843 : Image to text: 10.38, 29.66, 41.86, 15.0
2019-02-15 20:47:46,404 : Text to Image: 9.004, 25.964, 37.684, 19.0
2019-02-15 20:48:28,692 : samples : 256000
2019-02-15 20:48:39,197 : Image to text: 10.62, 29.98, 42.68, 15.0
2019-02-15 20:48:46,715 : Text to Image: 8.544, 25.108, 36.808, 19.0
2019-02-15 20:49:29,004 : samples : 320000
2019-02-15 20:49:39,508 : Image to text: 10.36, 29.34, 42.94, 15.0
2019-02-15 20:49:47,069 : Text to Image: 8.604, 24.908, 36.536, 20.0
2019-02-15 20:50:29,459 : samples : 384000
2019-02-15 20:50:39,901 : Image to text: 11.0, 30.6, 43.22, 15.0
2019-02-15 20:50:47,469 : Text to Image: 8.988, 25.604, 37.624, 19.0
2019-02-15 20:51:29,805 : samples : 448000
2019-02-15 20:51:40,153 : Image to text: 10.84, 30.36, 43.06, 15.0
2019-02-15 20:51:47,661 : Text to Image: 9.008, 26.132, 37.916, 19.0
2019-02-15 20:52:30,325 : samples : 512000
2019-02-15 20:52:40,800 : Image to text: 10.7, 29.38, 41.86, 15.0
2019-02-15 20:52:48,333 : Text to Image: 8.968, 26.124, 37.932, 18.0
2019-02-15 20:53:24,567 : Epoch 12 finished
2019-02-15 20:53:25,068 : Image to text: 26.4, 56.3, 73.3, 4.0
2019-02-15 20:53:25,412 : Text to Image: 21.18, 53.94, 71.0, 5.0
2019-02-15 20:53:25,865 : Image to text: 25.7, 60.1, 75.3, 4.0
2019-02-15 20:53:26,207 : Text to Image: 21.78, 53.8, 71.12, 5.0
2019-02-15 20:53:26,663 : Image to text: 28.6, 60.5, 75.0, 4.0
2019-02-15 20:53:27,011 : Text to Image: 22.34, 54.28, 71.8, 5.0
2019-02-15 20:53:27,483 : Image to text: 27.4, 60.4, 75.6, 4.0
2019-02-15 20:53:27,838 : Text to Image: 20.64, 53.48, 71.36, 5.0
2019-02-15 20:53:28,302 : Image to text: 30.1, 60.7, 74.9, 4.0
2019-02-15 20:53:28,656 : Text to Image: 22.62, 54.32, 70.9, 5.0
2019-02-15 20:53:28,656 : Dev mean Text to Image: 21.712, 53.964, 71.236, 5.0
2019-02-15 20:53:28,656 : Dev mean Image to text: 27.64, 59.6, 74.82, 4.0
2019-02-15 20:53:28,656 : start epoch
2019-02-15 20:54:11,802 : samples : 64000
2019-02-15 20:54:22,257 : Image to text: 10.48, 29.9, 42.46, 15.0
2019-02-15 20:54:29,977 : Text to Image: 8.564, 24.956, 36.836, 19.0
2019-02-15 20:55:12,320 : samples : 128000
2019-02-15 20:55:22,745 : Image to text: 10.56, 28.86, 41.32, 15.0
2019-02-15 20:55:30,318 : Text to Image: 8.556, 25.436, 36.884, 19.0
2019-02-15 20:56:12,934 : samples : 192000
2019-02-15 20:56:23,269 : Image to text: 10.5, 29.2, 42.08, 15.0
2019-02-15 20:56:30,816 : Text to Image: 9.016, 25.9, 37.896, 18.0
2019-02-15 20:57:13,082 : samples : 256000
2019-02-15 20:57:23,500 : Image to text: 10.96, 30.5, 43.34, 14.0
2019-02-15 20:57:31,026 : Text to Image: 9.0, 26.256, 38.188, 18.0
2019-02-15 20:58:13,485 : samples : 320000
2019-02-15 20:58:23,898 : Image to text: 11.1, 29.4, 42.1, 15.0
2019-02-15 20:58:31,468 : Text to Image: 8.56, 25.568, 37.616, 18.0
2019-02-15 20:59:13,794 : samples : 384000
2019-02-15 20:59:24,221 : Image to text: 11.0, 28.58, 42.24, 15.0
2019-02-15 20:59:31,786 : Text to Image: 8.56, 25.32, 37.024, 19.0
2019-02-15 21:00:23,443 : samples : 448000
2019-02-15 21:00:34,907 : Image to text: 10.76, 29.14, 42.3, 15.0
2019-02-15 21:00:42,429 : Text to Image: 8.348, 25.672, 37.464, 19.0
2019-02-15 21:01:25,336 : samples : 512000
2019-02-15 21:01:35,722 : Image to text: 10.7, 30.6, 42.74, 15.0
2019-02-15 21:01:43,247 : Text to Image: 9.064, 25.844, 37.836, 18.0
2019-02-15 21:02:19,744 : Epoch 13 finished
2019-02-15 21:02:20,197 : Image to text: 26.3, 59.4, 74.6, 4.0
2019-02-15 21:02:20,553 : Text to Image: 22.28, 54.66, 72.0, 5.0
2019-02-15 21:02:21,016 : Image to text: 25.4, 60.2, 74.3, 4.0
2019-02-15 21:02:21,371 : Text to Image: 23.18, 55.46, 71.84, 5.0
2019-02-15 21:02:21,833 : Image to text: 29.3, 60.7, 75.1, 3.0
2019-02-15 21:02:22,188 : Text to Image: 22.16, 56.56, 72.48, 4.0
2019-02-15 21:02:22,656 : Image to text: 28.6, 61.0, 76.0, 4.0
2019-02-15 21:02:23,012 : Text to Image: 21.56, 55.02, 72.18, 5.0
2019-02-15 21:02:23,498 : Image to text: 29.7, 60.1, 76.8, 4.0
2019-02-15 21:02:23,861 : Text to Image: 22.7, 55.52, 72.36, 5.0
2019-02-15 21:02:23,862 : Dev mean Text to Image: 22.375999999999998, 55.444, 72.172, 4.8
2019-02-15 21:02:23,862 : Dev mean Image to text: 27.86, 60.28, 75.36, 3.8
2019-02-15 21:02:23,862 : start epoch
2019-02-15 21:03:06,502 : samples : 64000
2019-02-15 21:03:16,992 : Image to text: 10.94, 30.58, 43.04, 15.0
2019-02-15 21:03:24,719 : Text to Image: 9.296, 26.808, 38.844, 18.0
2019-02-15 21:04:07,626 : samples : 128000
2019-02-15 21:04:17,953 : Image to text: 11.44, 29.88, 43.26, 15.0
2019-02-15 21:04:25,485 : Text to Image: 9.488, 27.112, 38.912, 17.0
2019-02-15 21:05:08,144 : samples : 192000
2019-02-15 21:05:18,641 : Image to text: 11.14, 30.82, 43.78, 14.0
2019-02-15 21:05:26,153 : Text to Image: 9.312, 26.12, 38.0, 18.0
2019-02-15 21:06:08,677 : samples : 256000
2019-02-15 21:06:19,047 : Image to text: 11.0, 30.76, 43.52, 14.0
2019-02-15 21:06:26,602 : Text to Image: 9.16, 25.892, 37.696, 19.0
2019-02-15 21:07:09,561 : samples : 320000
2019-02-15 21:07:20,039 : Image to text: 10.68, 29.44, 42.3, 15.0
2019-02-15 21:07:27,590 : Text to Image: 8.452, 25.06, 37.356, 19.0
2019-02-15 21:08:09,791 : samples : 384000
2019-02-15 21:08:20,197 : Image to text: 10.58, 29.34, 42.44, 15.0
2019-02-15 21:08:27,728 : Text to Image: 8.94, 26.024, 37.868, 18.0
2019-02-15 21:09:10,847 : samples : 448000
2019-02-15 21:09:21,260 : Image to text: 10.84, 31.3, 43.52, 14.0
2019-02-15 21:09:28,787 : Text to Image: 8.748, 26.36, 38.208, 18.0
2019-02-15 21:10:11,085 : samples : 512000
2019-02-15 21:10:21,437 : Image to text: 10.08, 29.5, 42.76, 14.0
2019-02-15 21:10:28,971 : Text to Image: 8.96, 26.1, 38.032, 18.0
2019-02-15 21:11:05,348 : Epoch 14 finished
2019-02-15 21:11:05,803 : Image to text: 27.5, 60.8, 75.7, 4.0
2019-02-15 21:11:06,146 : Text to Image: 22.82, 57.32, 73.4, 4.0
2019-02-15 21:11:06,613 : Image to text: 25.8, 61.2, 76.4, 4.0
2019-02-15 21:11:06,963 : Text to Image: 24.14, 56.8, 73.4, 4.0
2019-02-15 21:11:07,423 : Image to text: 28.6, 61.1, 75.4, 4.0
2019-02-15 21:11:07,768 : Text to Image: 23.86, 57.6, 73.02, 4.0
2019-02-15 21:11:08,235 : Image to text: 27.4, 63.4, 78.2, 3.0
2019-02-15 21:11:08,579 : Text to Image: 23.76, 57.74, 74.0, 4.0
2019-02-15 21:11:09,017 : Image to text: 29.4, 63.1, 75.2, 3.0
2019-02-15 21:11:09,358 : Text to Image: 24.84, 56.54, 72.86, 4.0
2019-02-15 21:11:09,358 : Dev mean Text to Image: 23.884, 57.199999999999996, 73.336, 4.0
2019-02-15 21:11:09,358 : Dev mean Image to text: 27.740000000000002, 61.92, 76.18, 3.6000000000000005
2019-02-15 21:11:09,358 : start epoch
2019-02-15 21:11:51,818 : samples : 64000
2019-02-15 21:12:02,262 : Image to text: 11.44, 31.58, 43.96, 14.0
2019-02-15 21:12:09,989 : Text to Image: 9.144, 26.472, 38.304, 18.0
2019-02-15 21:12:52,449 : samples : 128000
2019-02-15 21:13:02,762 : Image to text: 11.08, 31.56, 44.66, 14.0
2019-02-15 21:13:10,303 : Text to Image: 8.952, 26.452, 38.2, 18.0
2019-02-15 21:13:52,601 : samples : 192000
2019-02-15 21:14:02,995 : Image to text: 11.06, 30.88, 43.2, 15.0
2019-02-15 21:14:10,567 : Text to Image: 9.404, 26.948, 38.808, 18.0
2019-02-15 21:14:53,374 : samples : 256000
2019-02-15 21:15:03,710 : Image to text: 10.44, 29.94, 42.74, 14.0
2019-02-15 21:15:11,239 : Text to Image: 8.8, 25.94, 37.712, 19.0
2019-02-15 21:15:53,725 : samples : 320000
2019-02-15 21:16:03,992 : Image to text: 10.56, 30.1, 42.0, 15.0
2019-02-15 21:16:11,528 : Text to Image: 8.804, 25.496, 36.948, 19.0
2019-02-15 21:16:59,156 : samples : 384000
2019-02-15 21:17:12,064 : Image to text: 10.88, 30.72, 42.64, 15.0
2019-02-15 21:17:20,249 : Text to Image: 9.196, 26.5, 38.136, 18.0
2019-02-15 21:18:03,005 : samples : 448000
2019-02-15 21:18:13,289 : Image to text: 10.8, 30.36, 42.78, 15.0
2019-02-15 21:18:20,813 : Text to Image: 9.252, 26.544, 38.444, 18.0
2019-02-15 21:19:03,890 : samples : 512000
2019-02-15 21:19:14,206 : Image to text: 11.26, 30.22, 43.34, 15.0
2019-02-15 21:19:21,687 : Text to Image: 8.808, 26.468, 38.264, 18.0
2019-02-15 21:19:58,449 : Epoch 15 finished
2019-02-15 21:19:58,895 : Image to text: 28.1, 59.1, 74.6, 4.0
2019-02-15 21:19:59,260 : Text to Image: 23.12, 56.2, 73.44, 4.0
2019-02-15 21:19:59,724 : Image to text: 27.5, 59.3, 75.7, 4.0
2019-02-15 21:20:00,080 : Text to Image: 23.44, 54.32, 72.16, 5.0
2019-02-15 21:20:00,549 : Image to text: 29.1, 61.8, 76.3, 3.0
2019-02-15 21:20:00,889 : Text to Image: 22.52, 57.04, 72.52, 4.0
2019-02-15 21:20:01,332 : Image to text: 26.7, 61.0, 78.0, 4.0
2019-02-15 21:20:01,672 : Text to Image: 22.56, 55.36, 72.34, 4.0
2019-02-15 21:20:02,124 : Image to text: 31.8, 61.9, 75.8, 3.0
2019-02-15 21:20:02,477 : Text to Image: 23.68, 55.56, 72.5, 4.0
2019-02-15 21:20:02,477 : Dev mean Text to Image: 23.064, 55.696000000000005, 72.592, 4.2
2019-02-15 21:20:02,477 : Dev mean Image to text: 28.64, 60.61999999999999, 76.08, 3.6
2019-02-15 21:20:02,477 : start epoch
2019-02-15 21:20:45,185 : samples : 64000
2019-02-15 21:20:55,750 : Image to text: 10.1, 30.44, 42.5, 15.0
2019-02-15 21:21:03,365 : Text to Image: 8.42, 24.5, 36.496, 20.0
2019-02-15 21:21:45,636 : samples : 128000
2019-02-15 21:21:56,034 : Image to text: 10.72, 30.9, 43.0, 14.0
2019-02-15 21:22:03,553 : Text to Image: 8.992, 26.208, 38.164, 18.0
2019-02-15 21:22:46,164 : samples : 192000
2019-02-15 21:22:56,494 : Image to text: 10.36, 29.58, 42.28, 15.0
2019-02-15 21:23:03,978 : Text to Image: 8.992, 26.06, 37.96, 18.0
2019-02-15 21:23:46,379 : samples : 256000
2019-02-15 21:23:56,674 : Image to text: 11.38, 31.5, 43.98, 14.0
2019-02-15 21:24:04,244 : Text to Image: 9.712, 26.876, 38.912, 17.0
2019-02-15 21:24:47,271 : samples : 320000
2019-02-15 21:24:57,629 : Image to text: 10.26, 29.64, 42.4, 15.0
2019-02-15 21:25:05,207 : Text to Image: 8.252, 24.968, 36.852, 19.0
2019-02-15 21:25:48,114 : samples : 384000
2019-02-15 21:25:58,417 : Image to text: 11.04, 30.7, 42.9, 14.0
2019-02-15 21:26:06,007 : Text to Image: 9.22, 26.52, 38.5, 18.0
2019-02-15 21:26:49,099 : samples : 448000
2019-02-15 21:26:59,381 : Image to text: 11.08, 30.68, 44.0, 14.0
2019-02-15 21:27:06,891 : Text to Image: 9.052, 25.692, 37.528, 19.0
2019-02-15 21:27:49,933 : samples : 512000
2019-02-15 21:28:00,188 : Image to text: 11.24, 31.34, 43.56, 14.0
2019-02-15 21:28:07,668 : Text to Image: 8.948, 26.272, 38.0, 18.0
2019-02-15 21:28:44,178 : Epoch 16 finished
2019-02-15 21:28:44,625 : Image to text: 28.9, 59.0, 74.7, 4.0
2019-02-15 21:28:44,966 : Text to Image: 24.08, 56.9, 73.72, 4.0
2019-02-15 21:28:45,408 : Image to text: 25.6, 61.5, 76.0, 4.0
2019-02-15 21:28:45,744 : Text to Image: 24.04, 56.76, 73.1, 4.0
2019-02-15 21:28:46,253 : Image to text: 28.2, 63.7, 76.1, 3.0
2019-02-15 21:28:46,593 : Text to Image: 23.28, 56.94, 73.82, 4.0
2019-02-15 21:28:47,069 : Image to text: 25.5, 62.7, 76.5, 4.0
2019-02-15 21:28:47,409 : Text to Image: 24.02, 57.18, 73.98, 4.0
2019-02-15 21:28:47,861 : Image to text: 30.7, 61.4, 75.9, 3.0
2019-02-15 21:28:48,212 : Text to Image: 23.92, 56.62, 72.76, 4.0
2019-02-15 21:28:48,212 : Dev mean Text to Image: 23.868000000000002, 56.879999999999995, 73.476, 4.0
2019-02-15 21:28:48,212 : Dev mean Image to text: 27.78, 61.660000000000004, 75.84, 3.6
2019-02-15 21:28:52,234 : 
Test scores | Image to text:             28.1, 60.25999999999999, 75.62, 3.6
2019-02-15 21:28:52,235 : Test scores | Text to image:             23.1, 56.596000000000004, 73.52799999999999, 4.0

2019-02-15 21:28:52,388 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 21:28:52,609 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 21:28:53,281 : loading BERT model bert-base-uncased
2019-02-15 21:28:53,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:28:53,315 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:28:53,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy9fzty44
2019-02-15 21:28:55,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:28:57,325 : Computing embeddings for train/dev/test
2019-02-15 21:30:31,256 : Computed embeddings
2019-02-15 21:30:31,256 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:31:14,413 : [('reg:1e-05', 82.1), ('reg:0.0001', 83.51), ('reg:0.001', 76.18), ('reg:0.01', 65.05)]
2019-02-15 21:31:14,413 : Validation : best param found is reg = 0.0001 with score             83.51
2019-02-15 21:31:14,414 : Evaluating...
2019-02-15 21:31:24,023 : 
Dev acc : 83.5 Test acc : 84.2 for LENGTH classification

2019-02-15 21:31:24,024 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 21:31:24,399 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 21:31:24,453 : loading BERT model bert-base-uncased
2019-02-15 21:31:24,453 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:31:24,567 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:31:24,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3672jc1s
2019-02-15 21:31:27,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:31:28,587 : Computing embeddings for train/dev/test
2019-02-15 21:32:57,739 : Computed embeddings
2019-02-15 21:32:57,739 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:33:33,696 : [('reg:1e-05', 2.09), ('reg:0.0001', 0.54), ('reg:0.001', 0.22), ('reg:0.01', 0.18)]
2019-02-15 21:33:33,696 : Validation : best param found is reg = 1e-05 with score             2.09
2019-02-15 21:33:33,696 : Evaluating...
2019-02-15 21:33:44,918 : 
Dev acc : 2.1 Test acc : 2.2 for WORDCONTENT classification

2019-02-15 21:33:44,919 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 21:33:45,639 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 21:33:45,731 : loading BERT model bert-base-uncased
2019-02-15 21:33:45,731 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:33:45,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:33:45,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpli3anro6
2019-02-15 21:33:48,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:33:50,609 : Computing embeddings for train/dev/test
2019-02-15 21:35:15,529 : Computed embeddings
2019-02-15 21:35:15,529 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:35:53,025 : [('reg:1e-05', 30.53), ('reg:0.0001', 28.9), ('reg:0.001', 26.21), ('reg:0.01', 24.8)]
2019-02-15 21:35:53,025 : Validation : best param found is reg = 1e-05 with score             30.53
2019-02-15 21:35:53,025 : Evaluating...
2019-02-15 21:36:03,438 : 
Dev acc : 30.5 Test acc : 30.9 for DEPTH classification

2019-02-15 21:36:03,439 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 21:36:04,035 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 21:36:04,106 : loading BERT model bert-base-uncased
2019-02-15 21:36:04,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:36:04,136 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:36:04,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsqojmbtc
2019-02-15 21:36:06,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:36:08,042 : Computing embeddings for train/dev/test
2019-02-15 21:37:26,076 : Computed embeddings
2019-02-15 21:37:26,076 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:38:24,861 : [('reg:1e-05', 51.96), ('reg:0.0001', 50.08), ('reg:0.001', 36.04), ('reg:0.01', 17.24)]
2019-02-15 21:38:24,861 : Validation : best param found is reg = 1e-05 with score             51.96
2019-02-15 21:38:24,861 : Evaluating...
2019-02-15 21:38:40,571 : 
Dev acc : 52.0 Test acc : 52.6 for TOPCONSTITUENTS classification

2019-02-15 21:38:40,572 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 21:38:40,998 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 21:38:41,070 : loading BERT model bert-base-uncased
2019-02-15 21:38:41,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:38:41,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:38:41,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgruiq8yd
2019-02-15 21:38:43,620 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:38:45,072 : Computing embeddings for train/dev/test
2019-02-15 21:40:08,570 : Computed embeddings
2019-02-15 21:40:08,570 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:40:45,893 : [('reg:1e-05', 50.03), ('reg:0.0001', 50.62), ('reg:0.001', 50.08), ('reg:0.01', 50.0)]
2019-02-15 21:40:45,893 : Validation : best param found is reg = 0.0001 with score             50.62
2019-02-15 21:40:45,893 : Evaluating...
2019-02-15 21:40:56,204 : 
Dev acc : 50.6 Test acc : 50.8 for BIGRAMSHIFT classification

2019-02-15 21:40:56,205 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 21:40:56,640 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 21:40:56,714 : loading BERT model bert-base-uncased
2019-02-15 21:40:56,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:40:56,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:40:56,851 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6kag7vhs
2019-02-15 21:40:59,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:41:00,800 : Computing embeddings for train/dev/test
2019-02-15 21:42:23,618 : Computed embeddings
2019-02-15 21:42:23,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:43:15,030 : [('reg:1e-05', 86.3), ('reg:0.0001', 86.35), ('reg:0.001', 85.32), ('reg:0.01', 79.47)]
2019-02-15 21:43:15,030 : Validation : best param found is reg = 0.0001 with score             86.35
2019-02-15 21:43:15,030 : Evaluating...
2019-02-15 21:43:27,314 : 
Dev acc : 86.3 Test acc : 83.7 for TENSE classification

2019-02-15 21:43:27,315 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 21:43:27,921 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 21:43:27,988 : loading BERT model bert-base-uncased
2019-02-15 21:43:27,988 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:43:28,032 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:43:28,033 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3zymr1ds
2019-02-15 21:43:30,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:43:31,990 : Computing embeddings for train/dev/test
2019-02-15 21:44:58,155 : Computed embeddings
2019-02-15 21:44:58,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:45:50,470 : [('reg:1e-05', 79.58), ('reg:0.0001', 80.38), ('reg:0.001', 78.38), ('reg:0.01', 69.26)]
2019-02-15 21:45:50,470 : Validation : best param found is reg = 0.0001 with score             80.38
2019-02-15 21:45:50,470 : Evaluating...
2019-02-15 21:46:05,576 : 
Dev acc : 80.4 Test acc : 79.3 for SUBJNUMBER classification

2019-02-15 21:46:05,577 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 21:46:06,028 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 21:46:06,111 : loading BERT model bert-base-uncased
2019-02-15 21:46:06,111 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:46:06,250 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:46:06,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp80_dvhh1
2019-02-15 21:46:08,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:46:10,218 : Computing embeddings for train/dev/test
2019-02-15 21:47:36,164 : Computed embeddings
2019-02-15 21:47:36,164 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:48:30,266 : [('reg:1e-05', 76.57), ('reg:0.0001', 73.58), ('reg:0.001', 72.42), ('reg:0.01', 57.15)]
2019-02-15 21:48:30,266 : Validation : best param found is reg = 1e-05 with score             76.57
2019-02-15 21:48:30,266 : Evaluating...
2019-02-15 21:48:48,323 : 
Dev acc : 76.6 Test acc : 77.7 for OBJNUMBER classification

2019-02-15 21:48:48,324 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 21:48:48,943 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 21:48:49,014 : loading BERT model bert-base-uncased
2019-02-15 21:48:49,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:48:49,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:48:49,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmn1xw2kp
2019-02-15 21:48:51,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:48:53,044 : Computing embeddings for train/dev/test
2019-02-15 21:50:31,575 : Computed embeddings
2019-02-15 21:50:31,576 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:50:55,122 : [('reg:1e-05', 50.71), ('reg:0.0001', 50.64), ('reg:0.001', 50.41), ('reg:0.01', 51.93)]
2019-02-15 21:50:55,122 : Validation : best param found is reg = 0.01 with score             51.93
2019-02-15 21:50:55,122 : Evaluating...
2019-02-15 21:51:02,026 : 
Dev acc : 51.9 Test acc : 51.2 for ODDMANOUT classification

2019-02-15 21:51:02,027 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 21:51:02,703 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 21:51:02,788 : loading BERT model bert-base-uncased
2019-02-15 21:51:02,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:51:02,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:51:02,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp08l93y6k
2019-02-15 21:51:05,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:51:06,730 : Computing embeddings for train/dev/test
2019-02-15 21:52:44,001 : Computed embeddings
2019-02-15 21:52:44,001 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 21:53:31,408 : [('reg:1e-05', 50.2), ('reg:0.0001', 51.28), ('reg:0.001', 50.21), ('reg:0.01', 50.0)]
2019-02-15 21:53:31,409 : Validation : best param found is reg = 0.0001 with score             51.28
2019-02-15 21:53:31,409 : Evaluating...
2019-02-15 21:53:43,648 : 
Dev acc : 51.3 Test acc : 51.1 for COORDINATIONINVERSION classification

2019-02-15 21:53:43,650 : total results: {'STS12': {'MSRpar': {'pearson': (0.20957768183758127, 6.855901076166273e-09), 'spearman': SpearmanrResult(correlation=0.24741431001937889, pvalue=6.351686450790725e-12), 'nsamples': 750}, 'MSRvid': {'pearson': (0.18471185471575277, 3.505861737099239e-07), 'spearman': SpearmanrResult(correlation=0.20121675418462498, pvalue=2.7240956444257893e-08), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41281892468462666, 2.5743881745885668e-20), 'spearman': SpearmanrResult(correlation=0.5718226882234826, pvalue=3.2867977380275963e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.17988673811149283, 7.097050513707926e-07), 'spearman': SpearmanrResult(correlation=0.20657415984555558, pvalue=1.1329284525694145e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4148563456234101, 4.980731049406594e-18), 'spearman': SpearmanrResult(correlation=0.3423739796529586, pvalue=2.0495124066658055e-12), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.2803703089945727, 'wmean': 0.25278113717265266}, 'spearman': {'mean': 0.3138803783852001, 'wmean': 0.2865115025139249}}}, 'STS13': {'FNWN': {'pearson': (0.24586318113138844, 0.0006493101505026304), 'spearman': SpearmanrResult(correlation=0.25793242628177665, pvalue=0.0003392994535591399), 'nsamples': 189}, 'headlines': {'pearson': (0.07875680340889082, 0.03103708016774191), 'spearman': SpearmanrResult(correlation=0.22852501310264486, pvalue=2.4208034834259683e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.12811032494546332, 0.0023648612978812667), 'spearman': SpearmanrResult(correlation=0.12313483392889595, pvalue=0.003487742307300251), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1509101031619142, 'wmean': 0.11827042405660362}, 'spearman': {'mean': 0.20319742443777247, 'wmean': 0.19281442015223338}}}, 'STS14': {'deft-forum': {'pearson': (0.11538986797424855, 0.014318720628070786), 'spearman': SpearmanrResult(correlation=0.15759037257978045, pvalue=0.0007945580345750862), 'nsamples': 450}, 'deft-news': {'pearson': (0.3808147918343299, 8.650564732105277e-12), 'spearman': SpearmanrResult(correlation=0.48903093589926244, pvalue=1.9234982512927177e-19), 'nsamples': 300}, 'headlines': {'pearson': (0.1046278100233268, 0.004124914763125461), 'spearman': SpearmanrResult(correlation=0.1956135268678275, pvalue=6.648598509658943e-08), 'nsamples': 750}, 'images': {'pearson': (0.16331443292658118, 6.947057618015882e-06), 'spearman': SpearmanrResult(correlation=0.280788783809463, pvalue=4.686484727978056e-15), 'nsamples': 750}, 'OnWN': {'pearson': (0.1604798190674563, 1.0043078046707245e-05), 'spearman': SpearmanrResult(correlation=0.19565235632519398, pvalue=6.608200588558437e-08), 'nsamples': 750}, 'tweet-news': {'pearson': (0.21016073181674608, 6.213560278373888e-09), 'spearman': SpearmanrResult(correlation=0.2721255406021118, pvalue=3.360828208949676e-14), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.18913124227378145, 'wmean': 0.1720285262704783}, 'spearman': {'mean': 0.2651335860139399, 'wmean': 0.24686936110243388}}}, 'STS15': {'answers-forums': {'pearson': (0.20323692246214264, 7.371232774080578e-05), 'spearman': SpearmanrResult(correlation=0.248123586044143, pvalue=1.1437554771474135e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.2899532222833136, 5.400155565378655e-16), 'spearman': SpearmanrResult(correlation=0.32144379626940583, pvalue=1.730463249037383e-19), 'nsamples': 750}, 'belief': {'pearson': (0.24307959719984862, 1.9061256308860306e-06), 'spearman': SpearmanrResult(correlation=0.33870375932452595, pvalue=1.6142628611099424e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.19519008016870132, 7.10494099375917e-08), 'spearman': SpearmanrResult(correlation=0.3342449351273671, pvalue=4.943182389278818e-21), 'nsamples': 750}, 'images': {'pearson': (0.06142596476738297, 0.0927627072967018), 'spearman': SpearmanrResult(correlation=0.3328533172725625, pvalue=7.335689625056953e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.19857715737627782, 'wmean': 0.1924318817625984}, 'spearman': {'mean': 0.3150738788076009, 'wmean': 0.3204889303384175}}}, 'STS16': {'answer-answer': {'pearson': (0.19935456198975998, 0.0014049382306962708), 'spearman': SpearmanrResult(correlation=0.2953564743200639, pvalue=1.6569713189231106e-06), 'nsamples': 254}, 'headlines': {'pearson': (0.22091862588123087, 0.00044476783842506515), 'spearman': SpearmanrResult(correlation=0.41609470221111844, pvalue=7.606418081870445e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.31548401382228575, 1.0407160095857589e-06), 'spearman': SpearmanrResult(correlation=0.4267501919902511, pvalue=1.3523522996894843e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.4109399739539846, 2.329614183774486e-11), 'spearman': SpearmanrResult(correlation=0.7150904669175994, pvalue=1.6665324094692384e-39), 'nsamples': 244}, 'question-question': {'pearson': (-0.02034711293960753, 0.7699648165834302), 'spearman': SpearmanrResult(correlation=-0.0058821611953608485, pvalue=0.9326365254824067), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.22527001254153073, 'wmean': 0.2312166330601564}, 'spearman': {'mean': 0.3694819348487344, 'wmean': 0.3794547822290778}}}, 'MR': {'devacc': 63.78, 'acc': 63.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 67.58, 'acc': 65.75, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.32, 'acc': 82.04, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.62, 'acc': 89.34, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.61, 'acc': 77.32, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.97, 'acc': 35.29, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 55.58, 'acc': 70.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.92, 'acc': 66.78, 'f1': 72.78, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 67.8, 'acc': 66.02, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7366201715154551, 'pearson': 0.7360099592786526, 'spearman': 0.6785485397063359, 'mse': 0.46972610856880587, 'yhat': array([2.0335987 , 3.76765732, 2.07680388, ..., 3.32845714, 4.45162953,        4.55134006]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6165401906253217, 'pearson': 0.5297957746046918, 'spearman': 0.5172197936854424, 'mse': 1.9830345758340644, 'yhat': array([2.68352167, 2.41053469, 2.11342493, ..., 3.5773854 , 3.62337375,        3.32789398]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.62, 'acc': 57.39, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 320.26, 'acc': [(28.1, 60.25999999999999, 75.62, 3.6), (23.1, 56.596000000000004, 73.52799999999999, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 83.51, 'acc': 84.18, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 2.09, 'acc': 2.17, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.53, 'acc': 30.85, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 51.96, 'acc': 52.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.62, 'acc': 50.79, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.35, 'acc': 83.71, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.38, 'acc': 79.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.57, 'acc': 77.65, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.93, 'acc': 51.17, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 51.28, 'acc': 51.13, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 21:53:43,650 : STS12 p=0.2528, STS12 s=0.2865, STS13 p=0.1183, STS13 s=0.1928, STS14 p=0.1720, STS14 s=0.2469, STS15 p=0.1924, STS15 s=0.3205, STS 16 p=0.2312, STS16 s=0.3795, STS B p=0.5298, STS B s=0.5172, STS B m=1.9830, SICK-R p=0.7360, SICK-R s=0.6785, SICK-P m=0.4697
2019-02-15 21:53:43,650 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 21:53:43,650 : 0.2528,0.2865,0.1183,0.1928,0.1720,0.2469,0.1924,0.3205,0.2312,0.3795,0.5298,0.5172,1.9830,0.7360,0.6785,0.4697
2019-02-15 21:53:43,650 : MR=63.21, CR=65.75, SUBJ=89.34, MPQA=82.04, SST-B=77.32, SST-F=35.29, TREC=70.60, SICK-E=66.02, SNLI=57.39, MRPC=66.78, MRPC f=72.78
2019-02-15 21:53:43,650 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 21:53:43,651 : 63.21,65.75,89.34,82.04,77.32,35.29,70.60,66.02,57.39,66.78,72.78
2019-02-15 21:53:43,651 : COCO r1i2t=28.10, COCO r5i2t=60.26, COCO r10i2t=75.62, COCO medr_i2t=3.60, COCO r1t2i=23.10, COCO r5t2i=56.60, COCO r10t2i=73.53, COCO medr_t2i=4.00
2019-02-15 21:53:43,651 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 21:53:43,651 : 28.10,60.26,75.62,3.60,23.10,56.60,73.53,4.00
2019-02-15 21:53:43,651 : SentLen=84.18, WC=2.17, TreeDepth=30.85, TopConst=52.56, BShift=50.79, Tense=83.71, SubjNum=79.30, ObjNum=77.65, SOMO=51.17, CoordInv=51.13, average=56.35
2019-02-15 21:53:43,651 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 21:53:43,651 : 84.18,2.17,30.85,52.56,50.79,83.71,79.30,77.65,51.17,51.13,56.35
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,651 : layer 3
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,651 : ********************************************************************************
2019-02-15 21:53:43,741 : ***** Transfer task : STS12 *****


2019-02-15 21:53:43,754 : loading BERT model bert-base-uncased
2019-02-15 21:53:43,754 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:53:43,772 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:53:43,772 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps2bidogs
2019-02-15 21:53:46,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:53:49,695 : MSRpar : pearson = 0.2095, spearman = 0.2580
2019-02-15 21:53:50,481 : MSRvid : pearson = 0.1817, spearman = 0.2457
2019-02-15 21:53:51,113 : SMTeuroparl : pearson = 0.4043, spearman = 0.5703
2019-02-15 21:53:52,253 : surprise.OnWN : pearson = 0.1086, spearman = 0.1731
2019-02-15 21:53:52,900 : surprise.SMTnews : pearson = 0.4768, spearman = 0.3707
2019-02-15 21:53:52,900 : ALL (weighted average) : Pearson = 0.2415,             Spearman = 0.2951
2019-02-15 21:53:52,900 : ALL (average) : Pearson = 0.2762,             Spearman = 0.3236

2019-02-15 21:53:52,900 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 21:53:52,912 : loading BERT model bert-base-uncased
2019-02-15 21:53:52,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:53:52,933 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:53:52,933 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4sqohcsr
2019-02-15 21:53:55,434 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:53:57,481 : FNWN : pearson = 0.2314, spearman = 0.2417
2019-02-15 21:53:58,365 : headlines : pearson = 0.0666, spearman = 0.2287
2019-02-15 21:53:59,029 : OnWN : pearson = 0.1808, spearman = 0.1936
2019-02-15 21:53:59,030 : ALL (weighted average) : Pearson = 0.1301,             Spearman = 0.2172
2019-02-15 21:53:59,030 : ALL (average) : Pearson = 0.1596,             Spearman = 0.2213

2019-02-15 21:53:59,030 : ***** Transfer task : STS14 *****


2019-02-15 21:53:59,050 : loading BERT model bert-base-uncased
2019-02-15 21:53:59,051 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:53:59,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:53:59,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjgky__vm
2019-02-15 21:54:01,597 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:54:03,706 : deft-forum : pearson = 0.1024, spearman = 0.1631
2019-02-15 21:54:04,434 : deft-news : pearson = 0.3203, spearman = 0.5082
2019-02-15 21:54:05,430 : headlines : pearson = 0.0884, spearman = 0.2009
2019-02-15 21:54:06,357 : images : pearson = 0.1570, spearman = 0.2858
2019-02-15 21:54:07,316 : OnWN : pearson = 0.1698, spearman = 0.2143
2019-02-15 21:54:08,517 : tweet-news : pearson = 0.1509, spearman = 0.2409
2019-02-15 21:54:08,517 : ALL (weighted average) : Pearson = 0.1511,             Spearman = 0.2486
2019-02-15 21:54:08,517 : ALL (average) : Pearson = 0.1648,             Spearman = 0.2689

2019-02-15 21:54:08,517 : ***** Transfer task : STS15 *****


2019-02-15 21:54:08,561 : loading BERT model bert-base-uncased
2019-02-15 21:54:08,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:54:08,586 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:54:08,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk5dy4pyp
2019-02-15 21:54:11,099 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:54:13,446 : answers-forums : pearson = 0.1666, spearman = 0.2199
2019-02-15 21:54:14,340 : answers-students : pearson = 0.2645, spearman = 0.3200
2019-02-15 21:54:15,197 : belief : pearson = 0.1637, spearman = 0.3282
2019-02-15 21:54:16,205 : headlines : pearson = 0.1780, spearman = 0.3409
2019-02-15 21:54:17,181 : images : pearson = 0.0057, spearman = 0.3311
2019-02-15 21:54:17,181 : ALL (weighted average) : Pearson = 0.1533,             Spearman = 0.3165
2019-02-15 21:54:17,181 : ALL (average) : Pearson = 0.1557,             Spearman = 0.3080

2019-02-15 21:54:17,181 : ***** Transfer task : STS16 *****


2019-02-15 21:54:17,260 : loading BERT model bert-base-uncased
2019-02-15 21:54:17,260 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:54:17,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:54:17,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpebpu3pe2
2019-02-15 21:54:19,785 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:54:21,647 : answer-answer : pearson = 0.1853, spearman = 0.3211
2019-02-15 21:54:21,939 : headlines : pearson = 0.1928, spearman = 0.4197
2019-02-15 21:54:22,342 : plagiarism : pearson = 0.2437, spearman = 0.4166
2019-02-15 21:54:22,974 : postediting : pearson = 0.3143, spearman = 0.7303
2019-02-15 21:54:23,254 : question-question : pearson = 0.0586, spearman = 0.1374
2019-02-15 21:54:23,255 : ALL (weighted average) : Pearson = 0.2024,             Spearman = 0.4121
2019-02-15 21:54:23,255 : ALL (average) : Pearson = 0.1989,             Spearman = 0.4050

2019-02-15 21:54:23,255 : ***** Transfer task : MR *****


2019-02-15 21:54:23,274 : loading BERT model bert-base-uncased
2019-02-15 21:54:23,274 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:54:23,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:54:23,298 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_ckdxtx7
2019-02-15 21:54:25,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:54:27,300 : Generating sentence embeddings
2019-02-15 21:54:40,660 : Generated sentence embeddings
2019-02-15 21:54:40,660 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 21:54:57,575 : Best param found at split 1: l2reg = 0.0001                 with score 64.07
2019-02-15 21:55:17,206 : Best param found at split 2: l2reg = 1e-05                 with score 65.03
2019-02-15 21:55:34,919 : Best param found at split 3: l2reg = 1e-05                 with score 61.84
2019-02-15 21:55:52,719 : Best param found at split 4: l2reg = 0.0001                 with score 63.01
2019-02-15 21:56:09,721 : Best param found at split 5: l2reg = 0.0001                 with score 64.29
2019-02-15 21:56:10,923 : Dev acc : 63.65 Test acc : 66.26

2019-02-15 21:56:10,925 : ***** Transfer task : CR *****


2019-02-15 21:56:10,937 : loading BERT model bert-base-uncased
2019-02-15 21:56:10,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:56:10,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:56:10,963 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp021ayxic
2019-02-15 21:56:13,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:56:14,960 : Generating sentence embeddings
2019-02-15 21:56:18,624 : Generated sentence embeddings
2019-02-15 21:56:18,625 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 21:56:24,748 : Best param found at split 1: l2reg = 0.01                 with score 68.33
2019-02-15 21:56:30,173 : Best param found at split 2: l2reg = 0.0001                 with score 65.85
2019-02-15 21:56:35,738 : Best param found at split 3: l2reg = 0.0001                 with score 69.74
2019-02-15 21:56:41,065 : Best param found at split 4: l2reg = 0.0001                 with score 69.61
2019-02-15 21:56:47,178 : Best param found at split 5: l2reg = 0.0001                 with score 68.49
2019-02-15 21:56:47,531 : Dev acc : 68.4 Test acc : 65.75

2019-02-15 21:56:47,531 : ***** Transfer task : MPQA *****


2019-02-15 21:56:47,536 : loading BERT model bert-base-uncased
2019-02-15 21:56:47,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:56:47,557 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:56:47,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6nlfjlpr
2019-02-15 21:56:50,032 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:56:51,540 : Generating sentence embeddings
2019-02-15 21:56:55,360 : Generated sentence embeddings
2019-02-15 21:56:55,361 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 21:57:11,599 : Best param found at split 1: l2reg = 0.0001                 with score 78.21
2019-02-15 21:57:29,379 : Best param found at split 2: l2reg = 0.01                 with score 78.55
2019-02-15 21:57:48,084 : Best param found at split 3: l2reg = 0.0001                 with score 80.9
2019-02-15 21:58:04,422 : Best param found at split 4: l2reg = 1e-05                 with score 80.02
2019-02-15 21:58:26,697 : Best param found at split 5: l2reg = 0.0001                 with score 82.09
2019-02-15 21:58:27,433 : Dev acc : 79.95 Test acc : 82.32

2019-02-15 21:58:27,434 : ***** Transfer task : SUBJ *****


2019-02-15 21:58:27,448 : loading BERT model bert-base-uncased
2019-02-15 21:58:27,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 21:58:27,472 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 21:58:27,472 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpif03zybg
2019-02-15 21:58:29,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 21:58:31,385 : Generating sentence embeddings
2019-02-15 21:58:44,627 : Generated sentence embeddings
2019-02-15 21:58:44,628 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 21:59:00,859 : Best param found at split 1: l2reg = 1e-05                 with score 89.84
2019-02-15 21:59:19,273 : Best param found at split 2: l2reg = 0.001                 with score 90.6
2019-02-15 21:59:37,076 : Best param found at split 3: l2reg = 1e-05                 with score 89.6
2019-02-15 21:59:54,062 : Best param found at split 4: l2reg = 1e-05                 with score 90.19
2019-02-15 22:00:12,750 : Best param found at split 5: l2reg = 0.001                 with score 88.98
2019-02-15 22:00:13,607 : Dev acc : 89.84 Test acc : 87.94

2019-02-15 22:00:13,608 : ***** Transfer task : SST Binary classification *****


2019-02-15 22:00:13,747 : loading BERT model bert-base-uncased
2019-02-15 22:00:13,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:00:13,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:00:13,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbh3fjwez
2019-02-15 22:00:16,247 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:00:17,699 : Computing embedding for train
2019-02-15 22:01:03,133 : Computed train embeddings
2019-02-15 22:01:03,133 : Computing embedding for dev
2019-02-15 22:01:04,083 : Computed dev embeddings
2019-02-15 22:01:04,084 : Computing embedding for test
2019-02-15 22:01:06,060 : Computed test embeddings
2019-02-15 22:01:06,060 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:01:33,619 : [('reg:1e-05', 74.2), ('reg:0.0001', 74.43), ('reg:0.001', 75.69), ('reg:0.01', 61.58)]
2019-02-15 22:01:33,619 : Validation : best param found is reg = 0.001 with score             75.69
2019-02-15 22:01:33,619 : Evaluating...
2019-02-15 22:01:39,037 : 
Dev acc : 75.69 Test acc : 73.48 for             SST Binary classification

2019-02-15 22:01:39,037 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 22:01:39,095 : loading BERT model bert-base-uncased
2019-02-15 22:01:39,095 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:01:39,119 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:01:39,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz1calup0
2019-02-15 22:01:41,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:01:43,049 : Computing embedding for train
2019-02-15 22:01:52,747 : Computed train embeddings
2019-02-15 22:01:52,747 : Computing embedding for dev
2019-02-15 22:01:53,955 : Computed dev embeddings
2019-02-15 22:01:53,955 : Computing embedding for test
2019-02-15 22:01:56,359 : Computed test embeddings
2019-02-15 22:01:56,359 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:01:59,989 : [('reg:1e-05', 35.6), ('reg:0.0001', 31.15), ('reg:0.001', 33.88), ('reg:0.01', 33.7)]
2019-02-15 22:01:59,989 : Validation : best param found is reg = 1e-05 with score             35.6
2019-02-15 22:01:59,989 : Evaluating...
2019-02-15 22:02:01,148 : 
Dev acc : 35.6 Test acc : 36.02 for             SST Fine-Grained classification

2019-02-15 22:02:01,148 : ***** Transfer task : TREC *****


2019-02-15 22:02:01,169 : loading BERT model bert-base-uncased
2019-02-15 22:02:01,170 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:02:01,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:02:01,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw4boat5g
2019-02-15 22:02:03,693 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:02:08,564 : Computed train embeddings
2019-02-15 22:02:08,831 : Computed test embeddings
2019-02-15 22:02:08,831 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 22:02:22,197 : [('reg:1e-05', 52.0), ('reg:0.0001', 51.78), ('reg:0.001', 46.0), ('reg:0.01', 38.06)]
2019-02-15 22:02:22,197 : Cross-validation : best param found is reg = 1e-05             with score 52.0
2019-02-15 22:02:22,197 : Evaluating...
2019-02-15 22:02:23,048 : 
Dev acc : 52.0 Test acc : 75.2             for TREC

2019-02-15 22:02:23,049 : ***** Transfer task : MRPC *****


2019-02-15 22:02:23,073 : loading BERT model bert-base-uncased
2019-02-15 22:02:23,073 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:02:23,095 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:02:23,095 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqhgi_r2e
2019-02-15 22:02:25,570 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:02:27,082 : Computing embedding for train
2019-02-15 22:02:36,740 : Computed train embeddings
2019-02-15 22:02:36,740 : Computing embedding for test
2019-02-15 22:02:41,049 : Computed test embeddings
2019-02-15 22:02:41,066 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 22:02:49,767 : [('reg:1e-05', 69.18), ('reg:0.0001', 68.64), ('reg:0.001', 69.5), ('reg:0.01', 67.86)]
2019-02-15 22:02:49,767 : Cross-validation : best param found is reg = 0.001             with score 69.5
2019-02-15 22:02:49,767 : Evaluating...
2019-02-15 22:02:50,117 : Dev acc : 69.5 Test acc 71.83; Test F1 80.94 for MRPC.

2019-02-15 22:02:50,118 : ***** Transfer task : SICK-Entailment*****


2019-02-15 22:02:50,182 : loading BERT model bert-base-uncased
2019-02-15 22:02:50,183 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:02:50,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:02:50,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbesxkwct
2019-02-15 22:02:52,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:02:54,143 : Computing embedding for train
2019-02-15 22:02:59,195 : Computed train embeddings
2019-02-15 22:02:59,195 : Computing embedding for dev
2019-02-15 22:02:59,874 : Computed dev embeddings
2019-02-15 22:02:59,874 : Computing embedding for test
2019-02-15 22:03:05,367 : Computed test embeddings
2019-02-15 22:03:05,395 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:03:07,651 : [('reg:1e-05', 68.6), ('reg:0.0001', 69.6), ('reg:0.001', 66.6), ('reg:0.01', 61.2)]
2019-02-15 22:03:07,651 : Validation : best param found is reg = 0.0001 with score             69.6
2019-02-15 22:03:07,651 : Evaluating...
2019-02-15 22:03:08,209 : 
Dev acc : 69.6 Test acc : 67.77 for                        SICK entailment

2019-02-15 22:03:08,209 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 22:03:08,239 : loading BERT model bert-base-uncased
2019-02-15 22:03:08,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:03:08,304 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:03:08,304 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaw_2s0fq
2019-02-15 22:03:10,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:03:12,302 : Computing embedding for train
2019-02-15 22:03:17,437 : Computed train embeddings
2019-02-15 22:03:17,437 : Computing embedding for dev
2019-02-15 22:03:18,122 : Computed dev embeddings
2019-02-15 22:03:18,122 : Computing embedding for test
2019-02-15 22:03:23,616 : Computed test embeddings
2019-02-15 22:04:09,882 : Dev : Pearson 0.7572475777026654
2019-02-15 22:04:09,882 : Test : Pearson 0.7527080905415069 Spearman 0.6883958069583715 MSE 0.4448644346280351                        for SICK Relatedness

2019-02-15 22:04:09,883 : 

***** Transfer task : STSBenchmark*****


2019-02-15 22:04:09,958 : loading BERT model bert-base-uncased
2019-02-15 22:04:09,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:04:09,981 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:04:09,981 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprkudrm8k
2019-02-15 22:04:12,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:04:14,017 : Computing embedding for train
2019-02-15 22:04:22,290 : Computed train embeddings
2019-02-15 22:04:22,290 : Computing embedding for dev
2019-02-15 22:04:24,718 : Computed dev embeddings
2019-02-15 22:04:24,719 : Computing embedding for test
2019-02-15 22:04:26,661 : Computed test embeddings
2019-02-15 22:04:57,120 : Dev : Pearson 0.6342857221656193
2019-02-15 22:04:57,120 : Test : Pearson 0.5413199282897899 Spearman 0.5347957204141336 MSE 1.9346301160062207                        for SICK Relatedness

2019-02-15 22:04:57,121 : ***** Transfer task : SNLI Entailment*****


2019-02-15 22:05:02,067 : loading BERT model bert-base-uncased
2019-02-15 22:05:02,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:05:02,158 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:05:02,158 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ez3lhdx
2019-02-15 22:05:04,660 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:05:06,337 : PROGRESS (encoding): 0.00%
2019-02-15 22:06:24,085 : PROGRESS (encoding): 14.56%
2019-02-15 22:07:54,272 : PROGRESS (encoding): 29.12%
2019-02-15 22:09:21,380 : PROGRESS (encoding): 43.69%
2019-02-15 22:10:55,061 : PROGRESS (encoding): 58.25%
2019-02-15 22:12:39,781 : PROGRESS (encoding): 72.81%
2019-02-15 22:14:23,441 : PROGRESS (encoding): 87.37%
2019-02-15 22:16:14,439 : PROGRESS (encoding): 0.00%
2019-02-15 22:16:27,935 : PROGRESS (encoding): 0.00%
2019-02-15 22:16:41,041 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 22:17:58,913 : [('reg:1e-09', 57.11)]
2019-02-15 22:17:58,913 : Validation : best param found is reg = 1e-09 with score             57.11
2019-02-15 22:17:58,913 : Evaluating...
2019-02-15 22:20:05,744 : Dev acc : 57.11 Test acc : 56.59 for SNLI

2019-02-15 22:20:05,744 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 22:20:14,934 : loading BERT model bert-base-uncased
2019-02-15 22:20:14,934 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 22:20:14,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 22:20:15,000 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8bknkp4w
2019-02-15 22:20:17,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 22:20:18,967 : Computing embedding for train
2019-02-15 22:29:55,068 : Computed train embeddings
2019-02-15 22:29:55,068 : Computing embedding for dev
2019-02-15 22:30:20,229 : Computed dev embeddings
2019-02-15 22:30:20,229 : Computing embedding for test
2019-02-15 22:30:48,300 : Computed test embeddings
2019-02-15 22:30:48,320 : prepare data
2019-02-15 22:30:48,386 : start epoch
2019-02-15 22:31:35,216 : samples : 64000
2019-02-15 22:31:45,469 : Image to text: 3.98, 13.0, 20.74, 45.0
2019-02-15 22:31:52,912 : Text to Image: 2.56, 9.824, 16.62, 58.0
2019-02-15 22:32:37,174 : samples : 128000
2019-02-15 22:32:47,646 : Image to text: 5.12, 16.1, 25.16, 36.0
2019-02-15 22:32:54,929 : Text to Image: 2.948, 11.072, 18.556, 51.0
2019-02-15 22:33:41,455 : samples : 192000
2019-02-15 22:33:54,062 : Image to text: 6.26, 18.6, 28.62, 31.0
2019-02-15 22:34:04,110 : Text to Image: 4.204, 14.408, 22.804, 39.0
2019-02-15 22:34:49,088 : samples : 256000
2019-02-15 22:34:59,449 : Image to text: 5.82, 18.96, 28.66, 30.0
2019-02-15 22:35:07,073 : Text to Image: 4.172, 14.788, 23.536, 38.0
2019-02-15 22:35:51,532 : samples : 320000
2019-02-15 22:36:02,115 : Image to text: 5.6, 18.76, 27.82, 31.0
2019-02-15 22:36:09,431 : Text to Image: 3.62, 13.268, 21.712, 40.0
2019-02-15 22:36:55,470 : samples : 384000
2019-02-15 22:37:08,095 : Image to text: 6.16, 20.22, 31.38, 25.0
2019-02-15 22:37:18,198 : Text to Image: 4.872, 16.56, 26.008, 33.0
2019-02-15 22:38:02,835 : samples : 448000
2019-02-15 22:38:13,422 : Image to text: 6.62, 20.86, 30.46, 27.0
2019-02-15 22:38:21,020 : Text to Image: 5.196, 17.38, 27.108, 30.0
2019-02-15 22:39:06,209 : samples : 512000
2019-02-15 22:39:18,789 : Image to text: 7.34, 21.92, 33.0, 24.0
2019-02-15 22:39:28,862 : Text to Image: 5.348, 18.28, 28.42, 30.0
2019-02-15 22:40:08,410 : Epoch 1 finished
2019-02-15 22:40:08,902 : Image to text: 17.7, 47.5, 66.8, 6.0
2019-02-15 22:40:09,285 : Text to Image: 14.82, 42.14, 59.7, 7.0
2019-02-15 22:40:09,763 : Image to text: 20.3, 50.8, 64.7, 5.0
2019-02-15 22:40:10,150 : Text to Image: 14.42, 41.34, 59.38, 8.0
2019-02-15 22:40:10,623 : Image to text: 17.9, 50.1, 66.4, 5.0
2019-02-15 22:40:11,014 : Text to Image: 14.42, 42.08, 59.32, 7.0
2019-02-15 22:40:11,484 : Image to text: 19.8, 48.6, 64.7, 6.0
2019-02-15 22:40:11,871 : Text to Image: 14.26, 41.32, 58.38, 8.0
2019-02-15 22:40:12,342 : Image to text: 18.6, 51.0, 67.5, 5.0
2019-02-15 22:40:12,717 : Text to Image: 14.78, 43.02, 59.82, 7.0
2019-02-15 22:40:12,717 : Dev mean Text to Image: 14.54, 41.980000000000004, 59.32000000000001, 7.4
2019-02-15 22:40:12,717 : Dev mean Image to text: 18.86, 49.599999999999994, 66.02, 5.4
2019-02-15 22:40:12,717 : start epoch
2019-02-15 22:40:56,505 : samples : 64000
2019-02-15 22:41:08,127 : Image to text: 7.58, 23.54, 34.26, 23.0
2019-02-15 22:41:17,403 : Text to Image: 5.62, 18.396, 28.464, 28.0
2019-02-15 22:42:09,646 : samples : 128000
2019-02-15 22:42:22,311 : Image to text: 7.06, 22.26, 33.14, 23.0
2019-02-15 22:42:32,313 : Text to Image: 5.808, 18.672, 29.088, 28.0
2019-02-15 22:43:17,247 : samples : 192000
2019-02-15 22:43:27,874 : Image to text: 7.0, 22.5, 33.5, 23.0
2019-02-15 22:43:35,288 : Text to Image: 6.16, 18.956, 28.848, 29.0
2019-02-15 22:44:19,978 : samples : 256000
2019-02-15 22:44:32,558 : Image to text: 7.54, 22.38, 33.66, 23.0
2019-02-15 22:44:42,636 : Text to Image: 6.096, 19.192, 29.364, 27.0
2019-02-15 22:45:29,966 : samples : 320000
2019-02-15 22:45:42,562 : Image to text: 8.14, 24.12, 35.06, 21.0
2019-02-15 22:45:50,949 : Text to Image: 6.148, 19.808, 30.436, 26.0
2019-02-15 22:46:35,087 : samples : 384000
2019-02-15 22:46:45,523 : Image to text: 7.14, 22.4, 32.82, 24.0
2019-02-15 22:46:53,310 : Text to Image: 5.452, 17.876, 27.944, 29.0
2019-02-15 22:47:39,722 : samples : 448000
2019-02-15 22:47:52,362 : Image to text: 7.92, 22.52, 33.52, 23.0
2019-02-15 22:48:02,423 : Text to Image: 5.24, 17.944, 27.796, 29.0
2019-02-15 22:48:48,301 : samples : 512000
2019-02-15 22:48:59,026 : Image to text: 7.44, 22.88, 34.4, 22.0
2019-02-15 22:49:06,562 : Text to Image: 5.52, 18.332, 28.116, 29.0
2019-02-15 22:49:44,020 : Epoch 2 finished
2019-02-15 22:49:44,935 : Image to text: 20.0, 50.0, 65.7, 5.0
2019-02-15 22:49:45,821 : Text to Image: 15.9, 43.9, 61.3, 7.0
2019-02-15 22:49:46,774 : Image to text: 20.6, 50.0, 65.5, 5.0
2019-02-15 22:49:47,165 : Text to Image: 15.62, 44.02, 61.92, 7.0
2019-02-15 22:49:47,532 : Image to text: 21.4, 52.7, 68.8, 5.0
2019-02-15 22:49:47,829 : Text to Image: 15.84, 44.86, 62.5, 7.0
2019-02-15 22:49:48,206 : Image to text: 21.3, 50.8, 68.6, 5.0
2019-02-15 22:49:48,493 : Text to Image: 16.46, 44.24, 61.94, 7.0
2019-02-15 22:49:48,873 : Image to text: 21.4, 50.3, 66.9, 5.0
2019-02-15 22:49:49,160 : Text to Image: 15.9, 45.38, 62.12, 7.0
2019-02-15 22:49:49,161 : Dev mean Text to Image: 15.944, 44.48, 61.955999999999996, 7.0
2019-02-15 22:49:49,161 : Dev mean Image to text: 20.939999999999998, 50.760000000000005, 67.1, 5.0
2019-02-15 22:49:49,161 : start epoch
2019-02-15 22:50:34,639 : samples : 64000
2019-02-15 22:50:45,374 : Image to text: 7.88, 23.56, 34.88, 22.0
2019-02-15 22:50:52,940 : Text to Image: 5.856, 18.88, 28.752, 29.0
2019-02-15 22:51:37,500 : samples : 128000
2019-02-15 22:51:48,158 : Image to text: 8.28, 24.38, 35.34, 21.0
2019-02-15 22:51:55,736 : Text to Image: 6.58, 20.852, 31.716, 25.0
2019-02-15 22:52:39,693 : samples : 192000
2019-02-15 22:52:50,843 : Image to text: 7.84, 24.32, 36.0, 20.0
2019-02-15 22:52:58,452 : Text to Image: 6.4, 21.032, 31.476, 25.0
2019-02-15 22:53:43,005 : samples : 256000
2019-02-15 22:53:53,591 : Image to text: 7.86, 24.24, 36.7, 20.0
2019-02-15 22:54:01,234 : Text to Image: 6.508, 20.968, 31.524, 25.0
2019-02-15 22:54:45,742 : samples : 320000
2019-02-15 22:54:56,317 : Image to text: 8.36, 24.4, 35.78, 20.0
2019-02-15 22:55:03,903 : Text to Image: 6.372, 19.752, 29.716, 27.0
2019-02-15 22:55:48,748 : samples : 384000
2019-02-15 22:55:59,351 : Image to text: 8.94, 25.66, 37.32, 19.0
2019-02-15 22:56:06,953 : Text to Image: 6.836, 21.308, 32.16, 24.0
2019-02-15 22:56:51,623 : samples : 448000
2019-02-15 22:57:02,214 : Image to text: 8.92, 25.72, 37.52, 19.0
2019-02-15 22:57:09,800 : Text to Image: 7.132, 21.16, 31.968, 24.0
2019-02-15 22:57:54,037 : samples : 512000
2019-02-15 22:58:04,464 : Image to text: 8.6, 24.14, 35.44, 21.0
2019-02-15 22:58:14,569 : Text to Image: 5.168, 17.728, 27.92, 29.0
2019-02-15 22:58:59,288 : Epoch 3 finished
2019-02-15 22:58:59,785 : Image to text: 22.0, 52.5, 70.5, 5.0
2019-02-15 22:59:00,162 : Text to Image: 17.52, 48.48, 66.9, 6.0
2019-02-15 22:59:00,637 : Image to text: 21.4, 52.5, 67.7, 5.0
2019-02-15 22:59:01,020 : Text to Image: 17.44, 47.98, 64.76, 6.0
2019-02-15 22:59:01,497 : Image to text: 22.4, 53.1, 68.6, 5.0
2019-02-15 22:59:01,878 : Text to Image: 17.32, 48.14, 65.86, 6.0
2019-02-15 22:59:02,356 : Image to text: 24.1, 54.3, 70.6, 4.0
2019-02-15 22:59:02,742 : Text to Image: 17.64, 47.3, 64.7, 6.0
2019-02-15 22:59:03,218 : Image to text: 23.7, 53.7, 70.7, 5.0
2019-02-15 22:59:03,598 : Text to Image: 17.84, 49.08, 65.48, 6.0
2019-02-15 22:59:03,598 : Dev mean Text to Image: 17.552000000000003, 48.196, 65.54, 6.0
2019-02-15 22:59:03,598 : Dev mean Image to text: 22.72, 53.220000000000006, 69.62, 4.8
2019-02-15 22:59:03,598 : start epoch
2019-02-15 22:59:48,176 : samples : 64000
2019-02-15 23:00:00,284 : Image to text: 8.68, 25.74, 37.78, 19.0
2019-02-15 23:00:09,069 : Text to Image: 7.0, 21.796, 32.656, 23.0
2019-02-15 23:00:54,202 : samples : 128000
2019-02-15 23:01:04,779 : Image to text: 9.44, 26.78, 38.06, 18.0
2019-02-15 23:01:12,334 : Text to Image: 7.044, 21.876, 32.572, 23.0
2019-02-15 23:01:57,031 : samples : 192000
2019-02-15 23:02:07,514 : Image to text: 8.06, 24.5, 36.22, 20.0
2019-02-15 23:02:15,472 : Text to Image: 6.428, 20.496, 31.156, 25.0
2019-02-15 23:03:00,336 : samples : 256000
2019-02-15 23:03:10,629 : Image to text: 8.64, 26.0, 37.38, 19.0
2019-02-15 23:03:20,351 : Text to Image: 7.052, 21.66, 32.664, 23.0
2019-02-15 23:04:06,871 : samples : 320000
2019-02-15 23:04:19,429 : Image to text: 8.52, 25.6, 38.12, 18.0
2019-02-15 23:04:29,364 : Text to Image: 7.556, 23.356, 34.272, 22.0
2019-02-15 23:05:15,770 : samples : 384000
2019-02-15 23:05:28,354 : Image to text: 8.12, 25.68, 37.32, 19.0
2019-02-15 23:05:38,301 : Text to Image: 7.116, 22.228, 33.776, 22.0
2019-02-15 23:06:24,982 : samples : 448000
2019-02-15 23:06:37,571 : Image to text: 8.84, 25.54, 37.74, 19.0
2019-02-15 23:06:47,555 : Text to Image: 7.232, 21.832, 33.056, 23.0
2019-02-15 23:07:33,439 : samples : 512000
2019-02-15 23:07:45,991 : Image to text: 9.24, 26.6, 38.82, 18.0
2019-02-15 23:07:56,062 : Text to Image: 7.608, 22.972, 34.272, 21.0
2019-02-15 23:08:35,743 : Epoch 4 finished
2019-02-15 23:08:36,662 : Image to text: 22.1, 56.5, 71.3, 4.0
2019-02-15 23:08:37,456 : Text to Image: 19.04, 49.62, 67.72, 6.0
2019-02-15 23:08:38,366 : Image to text: 24.5, 54.4, 70.9, 5.0
2019-02-15 23:08:39,131 : Text to Image: 18.14, 48.3, 66.12, 6.0
2019-02-15 23:08:40,078 : Image to text: 26.1, 59.2, 73.4, 4.0
2019-02-15 23:08:40,833 : Text to Image: 18.76, 49.1, 66.92, 6.0
2019-02-15 23:08:41,765 : Image to text: 25.1, 58.2, 74.1, 4.0
2019-02-15 23:08:42,531 : Text to Image: 18.3, 49.12, 65.68, 6.0
2019-02-15 23:08:43,469 : Image to text: 27.0, 56.5, 72.2, 4.0
2019-02-15 23:08:44,233 : Text to Image: 19.0, 49.88, 66.28, 6.0
2019-02-15 23:08:44,233 : Dev mean Text to Image: 18.648, 49.204, 66.54400000000001, 6.0
2019-02-15 23:08:44,233 : Dev mean Image to text: 24.96, 56.959999999999994, 72.38000000000001, 4.2
2019-02-15 23:08:44,234 : start epoch
2019-02-15 23:09:30,421 : samples : 64000
2019-02-15 23:09:43,035 : Image to text: 9.9, 27.58, 38.52, 18.0
2019-02-15 23:09:53,103 : Text to Image: 7.304, 22.744, 33.996, 22.0
2019-02-15 23:10:38,768 : samples : 128000
2019-02-15 23:10:51,426 : Image to text: 9.26, 26.76, 38.3, 18.0
2019-02-15 23:11:01,464 : Text to Image: 7.036, 22.384, 33.52, 22.0
2019-02-15 23:11:47,604 : samples : 192000
2019-02-15 23:12:00,201 : Image to text: 9.68, 27.14, 39.08, 18.0
2019-02-15 23:12:10,223 : Text to Image: 7.316, 22.28, 32.96, 23.0
2019-02-15 23:12:56,441 : samples : 256000
2019-02-15 23:13:09,045 : Image to text: 8.96, 26.0, 37.8, 18.0
2019-02-15 23:13:19,078 : Text to Image: 7.04, 22.748, 33.96, 22.0
2019-02-15 23:14:04,464 : samples : 320000
2019-02-15 23:14:17,224 : Image to text: 9.52, 26.78, 38.98, 18.0
2019-02-15 23:14:26,329 : Text to Image: 6.904, 22.14, 33.26, 23.0
2019-02-15 23:15:12,531 : samples : 384000
2019-02-15 23:15:24,275 : Image to text: 9.24, 27.36, 38.98, 18.0
2019-02-15 23:15:32,774 : Text to Image: 7.152, 22.52, 33.524, 23.0
2019-02-15 23:16:21,716 : samples : 448000
2019-02-15 23:16:32,097 : Image to text: 9.32, 26.0, 37.72, 19.0
2019-02-15 23:16:39,641 : Text to Image: 7.016, 21.588, 32.612, 23.0
2019-02-15 23:17:27,900 : samples : 512000
2019-02-15 23:17:40,788 : Image to text: 9.72, 27.22, 40.48, 16.0
2019-02-15 23:17:51,199 : Text to Image: 7.596, 23.212, 34.752, 21.0
2019-02-15 23:18:31,327 : Epoch 5 finished
2019-02-15 23:18:32,428 : Image to text: 23.4, 57.5, 71.8, 4.0
2019-02-15 23:18:33,325 : Text to Image: 18.92, 51.36, 69.08, 5.0
2019-02-15 23:18:34,412 : Image to text: 23.6, 56.9, 71.6, 4.0
2019-02-15 23:18:35,317 : Text to Image: 19.12, 49.78, 67.92, 6.0
2019-02-15 23:18:36,304 : Image to text: 24.8, 56.7, 71.9, 4.0
2019-02-15 23:18:37,181 : Text to Image: 19.06, 51.16, 67.72, 5.0
2019-02-15 23:18:38,263 : Image to text: 26.1, 58.2, 73.8, 4.0
2019-02-15 23:18:39,124 : Text to Image: 19.34, 50.9, 68.86, 5.0
2019-02-15 23:18:40,208 : Image to text: 25.5, 58.7, 74.5, 4.0
2019-02-15 23:18:41,051 : Text to Image: 19.58, 51.36, 68.68, 5.0
2019-02-15 23:18:41,052 : Dev mean Text to Image: 19.204, 50.912, 68.452, 5.2
2019-02-15 23:18:41,052 : Dev mean Image to text: 24.68, 57.6, 72.72, 4.0
2019-02-15 23:18:41,052 : start epoch
2019-02-15 23:19:27,644 : samples : 64000
2019-02-15 23:19:40,570 : Image to text: 9.52, 27.54, 40.24, 17.0
2019-02-15 23:19:51,019 : Text to Image: 7.584, 23.428, 34.8, 21.0
2019-02-15 23:20:38,650 : samples : 128000
2019-02-15 23:20:51,549 : Image to text: 7.86, 25.68, 37.5, 18.0
2019-02-15 23:21:02,034 : Text to Image: 6.792, 21.2, 31.944, 24.0
2019-02-15 23:21:49,508 : samples : 192000
2019-02-15 23:22:02,451 : Image to text: 9.32, 26.02, 38.28, 18.0
2019-02-15 23:22:12,972 : Text to Image: 7.216, 22.152, 33.6, 22.0
2019-02-15 23:22:59,479 : samples : 256000
2019-02-15 23:23:12,424 : Image to text: 10.42, 27.9, 40.98, 16.0
2019-02-15 23:23:22,897 : Text to Image: 7.828, 23.86, 35.304, 20.0
2019-02-15 23:24:09,835 : samples : 320000
2019-02-15 23:24:22,763 : Image to text: 9.78, 27.0, 40.26, 16.0
2019-02-15 23:24:31,920 : Text to Image: 7.948, 23.428, 34.48, 21.0
2019-02-15 23:25:18,057 : samples : 384000
2019-02-15 23:25:28,699 : Image to text: 8.88, 27.58, 39.82, 17.0
2019-02-15 23:25:36,486 : Text to Image: 7.508, 22.832, 34.2, 22.0
2019-02-15 23:26:20,861 : samples : 448000
2019-02-15 23:26:31,569 : Image to text: 9.7, 28.08, 40.9, 16.0
2019-02-15 23:26:39,288 : Text to Image: 7.9, 24.096, 35.716, 20.0
2019-02-15 23:27:24,073 : samples : 512000
2019-02-15 23:27:34,780 : Image to text: 10.0, 27.6, 40.62, 16.0
2019-02-15 23:27:42,497 : Text to Image: 7.268, 23.116, 34.176, 21.0
2019-02-15 23:28:20,130 : Epoch 6 finished
2019-02-15 23:28:20,605 : Image to text: 23.9, 58.0, 74.3, 4.0
2019-02-15 23:28:20,952 : Text to Image: 20.3, 52.9, 70.8, 5.0
2019-02-15 23:28:21,412 : Image to text: 24.3, 56.8, 73.0, 4.0
2019-02-15 23:28:21,784 : Text to Image: 20.02, 51.54, 68.88, 5.0
2019-02-15 23:28:22,245 : Image to text: 25.8, 58.6, 74.0, 4.0
2019-02-15 23:28:22,601 : Text to Image: 20.86, 53.88, 70.16, 5.0
2019-02-15 23:28:23,071 : Image to text: 26.4, 58.8, 74.8, 4.0
2019-02-15 23:28:23,411 : Text to Image: 20.76, 52.7, 70.46, 5.0
2019-02-15 23:28:23,862 : Image to text: 26.0, 58.8, 74.0, 4.0
2019-02-15 23:28:24,201 : Text to Image: 20.88, 53.1, 70.14, 5.0
2019-02-15 23:28:24,201 : Dev mean Text to Image: 20.564, 52.824, 70.08800000000001, 5.0
2019-02-15 23:28:24,201 : Dev mean Image to text: 25.279999999999998, 58.199999999999996, 74.02000000000001, 4.0
2019-02-15 23:28:24,202 : start epoch
2019-02-15 23:29:08,549 : samples : 64000
2019-02-15 23:29:19,149 : Image to text: 9.6, 27.54, 40.12, 16.0
2019-02-15 23:29:26,836 : Text to Image: 8.128, 24.624, 35.98, 20.0
2019-02-15 23:30:12,203 : samples : 128000
2019-02-15 23:30:22,841 : Image to text: 9.68, 27.56, 39.32, 17.0
2019-02-15 23:30:30,547 : Text to Image: 7.196, 22.192, 33.464, 23.0
2019-02-15 23:31:14,493 : samples : 192000
2019-02-15 23:31:25,141 : Image to text: 10.38, 27.86, 40.34, 16.0
2019-02-15 23:31:32,792 : Text to Image: 8.028, 24.408, 36.244, 20.0
2019-02-15 23:32:16,380 : samples : 256000
2019-02-15 23:32:28,131 : Image to text: 10.14, 27.3, 39.86, 17.0
2019-02-15 23:32:37,134 : Text to Image: 8.096, 24.072, 35.704, 20.0
2019-02-15 23:33:27,905 : samples : 320000
2019-02-15 23:33:38,522 : Image to text: 9.06, 26.84, 39.58, 18.0
2019-02-15 23:33:46,176 : Text to Image: 7.892, 23.22, 34.612, 21.0
2019-02-15 23:34:30,449 : samples : 384000
2019-02-15 23:34:41,065 : Image to text: 9.2, 28.36, 40.4, 17.0
2019-02-15 23:34:48,700 : Text to Image: 8.168, 24.092, 35.924, 20.0
2019-02-15 23:35:33,329 : samples : 448000
2019-02-15 23:35:43,996 : Image to text: 9.44, 28.16, 40.98, 16.0
2019-02-15 23:35:51,719 : Text to Image: 8.496, 25.296, 37.092, 19.0
2019-02-15 23:36:36,695 : samples : 512000
2019-02-15 23:36:47,385 : Image to text: 9.96, 28.06, 40.8, 16.0
2019-02-15 23:36:54,978 : Text to Image: 7.868, 23.584, 35.08, 21.0
2019-02-15 23:37:32,040 : Epoch 7 finished
2019-02-15 23:37:32,487 : Image to text: 24.0, 56.1, 73.2, 4.0
2019-02-15 23:37:32,824 : Text to Image: 19.36, 51.86, 69.4, 5.0
2019-02-15 23:37:33,263 : Image to text: 23.7, 57.1, 72.5, 4.0
2019-02-15 23:37:33,609 : Text to Image: 20.6, 52.12, 69.82, 5.0
2019-02-15 23:37:34,077 : Image to text: 24.0, 57.7, 72.9, 4.0
2019-02-15 23:37:34,432 : Text to Image: 20.44, 52.16, 69.72, 5.0
2019-02-15 23:37:34,895 : Image to text: 24.9, 58.1, 73.5, 4.0
2019-02-15 23:37:35,250 : Text to Image: 20.32, 52.18, 69.64, 5.0
2019-02-15 23:37:35,694 : Image to text: 26.7, 57.9, 74.1, 4.0
2019-02-15 23:37:36,038 : Text to Image: 21.02, 51.6, 69.08, 5.0
2019-02-15 23:37:36,038 : Dev mean Text to Image: 20.348, 51.984, 69.532, 5.0
2019-02-15 23:37:36,038 : Dev mean Image to text: 24.66, 57.379999999999995, 73.24, 4.0
2019-02-15 23:37:36,038 : start epoch
2019-02-15 23:38:21,080 : samples : 64000
2019-02-15 23:38:31,676 : Image to text: 9.72, 27.42, 38.94, 17.0
2019-02-15 23:38:39,333 : Text to Image: 7.548, 22.488, 34.144, 22.0
2019-02-15 23:39:24,024 : samples : 128000
2019-02-15 23:39:34,515 : Image to text: 10.06, 28.9, 41.42, 16.0
2019-02-15 23:39:42,240 : Text to Image: 8.192, 24.692, 36.252, 19.0
2019-02-15 23:40:26,031 : samples : 192000
2019-02-15 23:40:36,535 : Image to text: 10.54, 29.72, 42.9, 15.0
2019-02-15 23:40:44,161 : Text to Image: 8.62, 25.14, 36.856, 19.0
2019-02-15 23:41:28,794 : samples : 256000
2019-02-15 23:41:39,323 : Image to text: 10.7, 27.94, 40.68, 16.0
2019-02-15 23:41:46,982 : Text to Image: 7.78, 24.176, 35.728, 20.0
2019-02-15 23:42:31,878 : samples : 320000
2019-02-15 23:42:42,387 : Image to text: 10.22, 28.72, 41.68, 15.0
2019-02-15 23:42:49,952 : Text to Image: 8.44, 24.816, 36.28, 20.0
2019-02-15 23:43:34,224 : samples : 384000
2019-02-15 23:43:44,880 : Image to text: 10.88, 29.24, 42.32, 15.0
2019-02-15 23:43:52,587 : Text to Image: 8.184, 24.42, 36.196, 20.0
2019-02-15 23:44:37,293 : samples : 448000
2019-02-15 23:44:47,993 : Image to text: 10.94, 30.3, 43.4, 14.0
2019-02-15 23:44:55,678 : Text to Image: 9.036, 25.912, 37.572, 19.0
2019-02-15 23:45:40,270 : samples : 512000
2019-02-15 23:45:50,711 : Image to text: 9.92, 29.28, 42.18, 15.0
2019-02-15 23:45:58,320 : Text to Image: 8.42, 24.448, 35.988, 20.0
2019-02-15 23:46:35,950 : Epoch 8 finished
2019-02-15 23:46:36,427 : Image to text: 26.6, 56.9, 74.9, 4.0
2019-02-15 23:46:36,772 : Text to Image: 20.26, 52.3, 70.46, 5.0
2019-02-15 23:46:37,227 : Image to text: 25.9, 58.5, 73.5, 4.0
2019-02-15 23:46:37,578 : Text to Image: 20.14, 52.48, 69.68, 5.0
2019-02-15 23:46:38,025 : Image to text: 24.4, 60.2, 72.5, 4.0
2019-02-15 23:46:38,363 : Text to Image: 19.9, 52.64, 70.68, 5.0
2019-02-15 23:46:38,811 : Image to text: 28.1, 60.7, 75.9, 4.0
2019-02-15 23:46:39,156 : Text to Image: 21.54, 53.2, 70.72, 5.0
2019-02-15 23:46:39,611 : Image to text: 28.5, 60.4, 76.9, 4.0
2019-02-15 23:46:39,959 : Text to Image: 20.64, 53.34, 70.16, 5.0
2019-02-15 23:46:39,959 : Dev mean Text to Image: 20.496000000000002, 52.791999999999994, 70.34, 5.0
2019-02-15 23:46:39,959 : Dev mean Image to text: 26.7, 59.339999999999996, 74.74, 4.0
2019-02-15 23:46:39,960 : start epoch
2019-02-15 23:47:24,958 : samples : 64000
2019-02-15 23:47:35,416 : Image to text: 10.16, 28.84, 41.48, 15.0
2019-02-15 23:47:43,065 : Text to Image: 8.3, 24.812, 36.076, 20.0
2019-02-15 23:48:28,500 : samples : 128000
2019-02-15 23:48:39,020 : Image to text: 10.0, 28.58, 40.98, 15.0
2019-02-15 23:48:46,649 : Text to Image: 8.236, 24.64, 35.816, 20.0
2019-02-15 23:49:31,591 : samples : 192000
2019-02-15 23:49:43,508 : Image to text: 10.68, 28.36, 40.82, 16.0
2019-02-15 23:49:52,062 : Text to Image: 8.656, 24.876, 36.44, 19.0
2019-02-15 23:50:40,372 : samples : 256000
2019-02-15 23:50:50,883 : Image to text: 10.28, 28.72, 41.12, 16.0
2019-02-15 23:50:58,558 : Text to Image: 7.804, 23.844, 35.548, 20.0
2019-02-15 23:51:43,792 : samples : 320000
2019-02-15 23:51:54,017 : Image to text: 11.14, 30.12, 42.86, 15.0
2019-02-15 23:52:01,645 : Text to Image: 9.008, 26.156, 37.448, 19.0
2019-02-15 23:52:46,147 : samples : 384000
2019-02-15 23:52:56,909 : Image to text: 10.1, 28.98, 41.64, 16.0
2019-02-15 23:53:04,608 : Text to Image: 8.168, 24.396, 35.908, 20.0
2019-02-15 23:53:49,045 : samples : 448000
2019-02-15 23:53:59,719 : Image to text: 9.86, 29.7, 41.78, 15.0
2019-02-15 23:54:07,386 : Text to Image: 8.456, 24.684, 36.196, 20.0
2019-02-15 23:54:52,007 : samples : 512000
2019-02-15 23:55:02,545 : Image to text: 10.9, 29.96, 42.72, 15.0
2019-02-15 23:55:10,214 : Text to Image: 8.592, 25.488, 37.064, 19.0
2019-02-15 23:55:48,043 : Epoch 9 finished
2019-02-15 23:55:48,519 : Image to text: 25.4, 58.2, 76.4, 4.0
2019-02-15 23:55:48,883 : Text to Image: 22.1, 55.2, 72.38, 5.0
2019-02-15 23:55:49,354 : Image to text: 26.2, 59.4, 74.9, 4.0
2019-02-15 23:55:49,714 : Text to Image: 21.52, 53.5, 71.76, 5.0
2019-02-15 23:55:50,175 : Image to text: 27.9, 60.0, 74.1, 3.0
2019-02-15 23:55:50,540 : Text to Image: 22.26, 55.3, 71.98, 4.0
2019-02-15 23:55:51,008 : Image to text: 26.0, 60.7, 76.7, 4.0
2019-02-15 23:55:51,343 : Text to Image: 22.3, 54.54, 71.8, 5.0
2019-02-15 23:55:51,802 : Image to text: 27.3, 61.7, 74.7, 3.0
2019-02-15 23:55:52,157 : Text to Image: 22.34, 55.18, 71.14, 4.0
2019-02-15 23:55:52,157 : Dev mean Text to Image: 22.104, 54.744, 71.812, 4.6
2019-02-15 23:55:52,157 : Dev mean Image to text: 26.560000000000002, 60.0, 75.36, 3.6
2019-02-15 23:55:52,157 : start epoch
2019-02-15 23:56:37,221 : samples : 64000
2019-02-15 23:56:47,733 : Image to text: 10.3, 29.8, 42.5, 15.0
2019-02-15 23:56:55,430 : Text to Image: 8.96, 25.372, 37.428, 19.0
2019-02-15 23:57:40,196 : samples : 128000
2019-02-15 23:57:50,498 : Image to text: 11.24, 30.78, 44.12, 14.0
2019-02-15 23:57:58,052 : Text to Image: 9.308, 26.688, 38.328, 18.0
2019-02-15 23:58:43,000 : samples : 192000
2019-02-15 23:58:53,477 : Image to text: 10.62, 29.74, 43.02, 14.0
2019-02-15 23:59:01,126 : Text to Image: 8.428, 24.624, 36.048, 20.0
2019-02-15 23:59:45,376 : samples : 256000
2019-02-15 23:59:55,870 : Image to text: 10.56, 29.76, 42.16, 15.0
2019-02-16 00:00:03,523 : Text to Image: 8.648, 25.228, 36.804, 19.0
2019-02-16 00:00:47,923 : samples : 320000
2019-02-16 00:00:58,263 : Image to text: 11.14, 29.66, 42.56, 14.0
2019-02-16 00:01:05,845 : Text to Image: 8.608, 25.396, 37.168, 19.0
2019-02-16 00:01:50,705 : samples : 384000
2019-02-16 00:02:01,328 : Image to text: 10.48, 29.04, 41.5, 16.0
2019-02-16 00:02:09,104 : Text to Image: 8.74, 25.312, 37.46, 18.0
2019-02-16 00:02:53,299 : samples : 448000
2019-02-16 00:03:03,907 : Image to text: 9.14, 27.7, 39.68, 17.0
2019-02-16 00:03:11,597 : Text to Image: 7.78, 23.24, 34.7, 21.0
2019-02-16 00:03:55,570 : samples : 512000
2019-02-16 00:04:06,145 : Image to text: 10.56, 29.94, 42.48, 15.0
2019-02-16 00:04:13,832 : Text to Image: 8.572, 25.416, 37.124, 19.0
2019-02-16 00:04:51,625 : Epoch 10 finished
2019-02-16 00:04:52,081 : Image to text: 24.6, 58.8, 75.8, 4.0
2019-02-16 00:04:52,431 : Text to Image: 22.4, 54.24, 72.06, 5.0
2019-02-16 00:04:52,884 : Image to text: 27.9, 57.9, 74.1, 4.0
2019-02-16 00:04:53,236 : Text to Image: 21.3, 53.82, 71.22, 5.0
2019-02-16 00:04:53,698 : Image to text: 27.1, 59.7, 73.9, 4.0
2019-02-16 00:04:54,057 : Text to Image: 21.96, 55.12, 71.52, 5.0
2019-02-16 00:04:54,541 : Image to text: 27.7, 62.1, 75.9, 3.0
2019-02-16 00:04:54,900 : Text to Image: 22.36, 54.82, 71.34, 5.0
2019-02-16 00:04:55,367 : Image to text: 28.1, 61.4, 76.3, 4.0
2019-02-16 00:04:55,725 : Text to Image: 22.46, 54.9, 71.94, 5.0
2019-02-16 00:04:55,725 : Dev mean Text to Image: 22.096, 54.58, 71.616, 5.0
2019-02-16 00:04:55,725 : Dev mean Image to text: 27.080000000000002, 59.980000000000004, 75.2, 3.8000000000000007
2019-02-16 00:04:55,725 : start epoch
2019-02-16 00:05:40,773 : samples : 64000
2019-02-16 00:05:51,269 : Image to text: 10.82, 29.96, 43.88, 14.0
2019-02-16 00:05:58,958 : Text to Image: 9.08, 26.476, 38.416, 18.0
2019-02-16 00:06:46,690 : samples : 128000
2019-02-16 00:06:58,989 : Image to text: 10.88, 29.36, 41.66, 15.0
2019-02-16 00:07:07,590 : Text to Image: 8.516, 24.804, 36.972, 19.0
2019-02-16 00:07:52,819 : samples : 192000
2019-02-16 00:08:03,311 : Image to text: 11.06, 29.02, 41.76, 15.0
2019-02-16 00:08:10,942 : Text to Image: 8.164, 24.616, 36.544, 20.0
2019-02-16 00:08:55,434 : samples : 256000
2019-02-16 00:09:05,859 : Image to text: 10.6, 30.44, 42.52, 15.0
2019-02-16 00:09:13,499 : Text to Image: 8.168, 24.616, 36.452, 20.0
2019-02-16 00:09:57,340 : samples : 320000
2019-02-16 00:10:07,830 : Image to text: 10.72, 29.82, 42.36, 15.0
2019-02-16 00:10:15,468 : Text to Image: 8.172, 24.724, 36.424, 20.0
2019-02-16 00:11:00,257 : samples : 384000
2019-02-16 00:11:11,036 : Image to text: 10.62, 29.7, 42.94, 14.0
2019-02-16 00:11:18,761 : Text to Image: 8.388, 25.376, 37.06, 19.0
2019-02-16 00:12:03,314 : samples : 448000
2019-02-16 00:12:14,025 : Image to text: 10.32, 29.3, 42.16, 15.0
2019-02-16 00:12:21,758 : Text to Image: 8.648, 25.236, 36.94, 19.0
2019-02-16 00:13:06,376 : samples : 512000
2019-02-16 00:13:17,010 : Image to text: 10.64, 30.06, 42.98, 15.0
2019-02-16 00:13:24,724 : Text to Image: 8.716, 24.988, 36.872, 19.0
2019-02-16 00:14:02,895 : Epoch 11 finished
2019-02-16 00:14:03,354 : Image to text: 27.5, 60.5, 76.1, 4.0
2019-02-16 00:14:03,690 : Text to Image: 23.28, 55.78, 74.44, 4.0
2019-02-16 00:14:04,150 : Image to text: 26.6, 60.5, 75.6, 4.0
2019-02-16 00:14:04,496 : Text to Image: 23.08, 55.6, 72.72, 4.0
2019-02-16 00:14:04,951 : Image to text: 29.2, 61.6, 77.2, 3.0
2019-02-16 00:14:05,291 : Text to Image: 23.08, 55.84, 72.42, 4.0
2019-02-16 00:14:05,736 : Image to text: 29.3, 60.3, 76.7, 3.0
2019-02-16 00:14:06,092 : Text to Image: 22.5, 56.76, 73.38, 4.0
2019-02-16 00:14:06,560 : Image to text: 29.1, 61.6, 76.3, 4.0
2019-02-16 00:14:06,909 : Text to Image: 22.42, 56.08, 72.9, 4.0
2019-02-16 00:14:06,909 : Dev mean Text to Image: 22.872, 56.01200000000001, 73.172, 4.0
2019-02-16 00:14:06,909 : Dev mean Image to text: 28.34, 60.9, 76.38000000000001, 3.6000000000000005
2019-02-16 00:14:06,909 : start epoch
2019-02-16 00:14:51,651 : samples : 64000
2019-02-16 00:15:02,075 : Image to text: 11.0, 30.94, 44.18, 14.0
2019-02-16 00:15:09,714 : Text to Image: 9.264, 26.828, 39.04, 17.0
2019-02-16 00:15:54,019 : samples : 128000
2019-02-16 00:16:04,337 : Image to text: 10.4, 30.7, 43.06, 14.0
2019-02-16 00:16:11,982 : Text to Image: 8.368, 24.9, 36.548, 20.0
2019-02-16 00:16:57,523 : samples : 192000
2019-02-16 00:17:07,997 : Image to text: 11.12, 30.26, 43.0, 14.0
2019-02-16 00:17:15,573 : Text to Image: 9.072, 26.212, 38.38, 18.0
2019-02-16 00:18:00,856 : samples : 256000
2019-02-16 00:18:11,338 : Image to text: 11.2, 31.3, 43.9, 14.0
2019-02-16 00:18:18,930 : Text to Image: 8.72, 25.548, 37.732, 18.0
2019-02-16 00:19:02,850 : samples : 320000
2019-02-16 00:19:13,227 : Image to text: 10.86, 29.7, 42.54, 15.0
2019-02-16 00:19:20,884 : Text to Image: 7.884, 24.152, 35.596, 20.0
2019-02-16 00:20:04,863 : samples : 384000
2019-02-16 00:20:15,465 : Image to text: 10.94, 30.08, 43.34, 14.0
2019-02-16 00:20:23,149 : Text to Image: 8.988, 25.828, 37.44, 18.0
2019-02-16 00:21:07,440 : samples : 448000
2019-02-16 00:21:18,031 : Image to text: 11.5, 30.36, 42.74, 15.0
2019-02-16 00:21:25,738 : Text to Image: 9.4, 26.588, 38.764, 18.0
2019-02-16 00:22:09,313 : samples : 512000
2019-02-16 00:22:19,942 : Image to text: 10.68, 29.58, 42.48, 15.0
2019-02-16 00:22:27,634 : Text to Image: 8.984, 26.148, 38.416, 18.0
2019-02-16 00:23:05,141 : Epoch 12 finished
2019-02-16 00:23:05,603 : Image to text: 25.7, 56.1, 73.6, 4.0
2019-02-16 00:23:05,938 : Text to Image: 19.54, 52.98, 70.36, 5.0
2019-02-16 00:23:06,394 : Image to text: 25.3, 58.2, 73.4, 4.0
2019-02-16 00:23:06,748 : Text to Image: 20.22, 52.8, 70.02, 5.0
2019-02-16 00:23:07,208 : Image to text: 28.5, 59.7, 75.5, 4.0
2019-02-16 00:23:07,551 : Text to Image: 21.36, 53.62, 70.9, 5.0
2019-02-16 00:23:08,009 : Image to text: 26.7, 60.0, 74.4, 4.0
2019-02-16 00:23:08,348 : Text to Image: 20.24, 52.06, 70.36, 5.0
2019-02-16 00:23:08,798 : Image to text: 28.6, 57.5, 73.0, 4.0
2019-02-16 00:23:09,119 : Text to Image: 20.56, 52.5, 69.9, 5.0
2019-02-16 00:23:09,119 : Dev mean Text to Image: 20.384, 52.792, 70.308, 5.0
2019-02-16 00:23:09,119 : Dev mean Image to text: 26.96, 58.3, 73.98, 4.0
2019-02-16 00:23:09,119 : start epoch
2019-02-16 00:23:59,485 : samples : 64000
2019-02-16 00:24:11,225 : Image to text: 10.26, 29.62, 42.92, 14.0
2019-02-16 00:24:19,530 : Text to Image: 8.496, 24.904, 36.524, 19.0
2019-02-16 00:25:03,511 : samples : 128000
2019-02-16 00:25:13,955 : Image to text: 11.32, 29.7, 42.6, 14.0
2019-02-16 00:25:21,566 : Text to Image: 8.764, 25.608, 37.656, 19.0
2019-02-16 00:26:06,331 : samples : 192000
2019-02-16 00:26:16,859 : Image to text: 11.1, 29.9, 42.88, 14.0
2019-02-16 00:26:24,467 : Text to Image: 9.044, 26.544, 38.44, 18.0
2019-02-16 00:27:09,613 : samples : 256000
2019-02-16 00:27:20,080 : Image to text: 11.04, 30.58, 43.1, 15.0
2019-02-16 00:27:27,560 : Text to Image: 8.86, 25.904, 37.656, 18.0
2019-02-16 00:28:12,231 : samples : 320000
2019-02-16 00:28:22,717 : Image to text: 11.3, 30.12, 43.3, 14.0
2019-02-16 00:28:30,349 : Text to Image: 8.736, 25.964, 37.732, 18.0
2019-02-16 00:29:15,235 : samples : 384000
2019-02-16 00:29:25,933 : Image to text: 11.38, 30.66, 44.16, 14.0
2019-02-16 00:29:33,666 : Text to Image: 8.92, 25.92, 37.82, 18.0
2019-02-16 00:30:18,261 : samples : 448000
2019-02-16 00:30:28,813 : Image to text: 11.34, 30.62, 43.5, 14.0
2019-02-16 00:30:36,370 : Text to Image: 8.452, 25.56, 37.528, 19.0
2019-02-16 00:31:20,840 : samples : 512000
2019-02-16 00:31:31,471 : Image to text: 10.78, 29.62, 43.28, 14.0
2019-02-16 00:31:39,194 : Text to Image: 9.044, 25.984, 38.404, 18.0
2019-02-16 00:32:16,886 : Epoch 13 finished
2019-02-16 00:32:17,334 : Image to text: 26.9, 59.1, 75.6, 4.0
2019-02-16 00:32:17,669 : Text to Image: 22.4, 55.78, 72.82, 5.0
2019-02-16 00:32:18,121 : Image to text: 26.6, 60.5, 76.4, 4.0
2019-02-16 00:32:18,465 : Text to Image: 22.3, 55.36, 72.96, 5.0
2019-02-16 00:32:18,928 : Image to text: 28.3, 61.7, 75.4, 3.0
2019-02-16 00:32:19,282 : Text to Image: 22.3, 56.62, 72.62, 4.0
2019-02-16 00:32:19,741 : Image to text: 29.7, 62.8, 75.9, 3.0
2019-02-16 00:32:20,082 : Text to Image: 22.46, 55.46, 73.14, 4.0
2019-02-16 00:32:20,545 : Image to text: 28.8, 61.0, 76.8, 4.0
2019-02-16 00:32:20,882 : Text to Image: 23.02, 55.34, 72.68, 4.0
2019-02-16 00:32:20,883 : Dev mean Text to Image: 22.496, 55.712, 72.844, 4.3999999999999995
2019-02-16 00:32:20,883 : Dev mean Image to text: 28.059999999999995, 61.02000000000001, 76.02000000000001, 3.6000000000000005
2019-02-16 00:32:25,079 : 
Test scores | Image to text:             27.82, 60.699999999999996, 75.63999999999999, 3.6
2019-02-16 00:32:25,079 : Test scores | Text to image:             22.484, 55.712, 72.61999999999999, 4.4

2019-02-16 00:32:25,180 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 00:32:25,561 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 00:32:26,284 : loading BERT model bert-base-uncased
2019-02-16 00:32:26,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:32:26,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:32:26,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5aqekzbs
2019-02-16 00:32:28,858 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:32:30,397 : Computing embeddings for train/dev/test
2019-02-16 00:34:05,412 : Computed embeddings
2019-02-16 00:34:05,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:34:59,430 : [('reg:1e-05', 87.25), ('reg:0.0001', 86.75), ('reg:0.001', 80.58), ('reg:0.01', 49.72)]
2019-02-16 00:34:59,431 : Validation : best param found is reg = 1e-05 with score             87.25
2019-02-16 00:34:59,431 : Evaluating...
2019-02-16 00:35:17,342 : 
Dev acc : 87.2 Test acc : 87.2 for LENGTH classification

2019-02-16 00:35:17,343 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 00:35:17,703 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 00:35:17,752 : loading BERT model bert-base-uncased
2019-02-16 00:35:17,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:35:17,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:35:17,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu850huo7
2019-02-16 00:35:20,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:35:21,836 : Computing embeddings for train/dev/test
2019-02-16 00:36:50,959 : Computed embeddings
2019-02-16 00:36:50,959 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:37:46,516 : [('reg:1e-05', 1.99), ('reg:0.0001', 0.69), ('reg:0.001', 0.16), ('reg:0.01', 0.14)]
2019-02-16 00:37:46,516 : Validation : best param found is reg = 1e-05 with score             1.99
2019-02-16 00:37:46,516 : Evaluating...
2019-02-16 00:38:04,238 : 
Dev acc : 2.0 Test acc : 1.9 for WORDCONTENT classification

2019-02-16 00:38:04,239 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 00:38:04,624 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 00:38:04,698 : loading BERT model bert-base-uncased
2019-02-16 00:38:04,698 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:38:04,814 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:38:04,814 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9pmi_9bg
2019-02-16 00:38:07,316 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:38:08,809 : Computing embeddings for train/dev/test
2019-02-16 00:39:31,735 : Computed embeddings
2019-02-16 00:39:31,735 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:40:18,453 : [('reg:1e-05', 30.11), ('reg:0.0001', 28.76), ('reg:0.001', 25.71), ('reg:0.01', 22.12)]
2019-02-16 00:40:18,453 : Validation : best param found is reg = 1e-05 with score             30.11
2019-02-16 00:40:18,454 : Evaluating...
2019-02-16 00:40:28,352 : 
Dev acc : 30.1 Test acc : 30.2 for DEPTH classification

2019-02-16 00:40:28,353 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 00:40:28,741 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 00:40:28,807 : loading BERT model bert-base-uncased
2019-02-16 00:40:28,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:40:28,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:40:28,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcfnnidqs
2019-02-16 00:40:31,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:40:33,327 : Computing embeddings for train/dev/test
2019-02-16 00:41:56,618 : Computed embeddings
2019-02-16 00:41:56,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:43:04,374 : [('reg:1e-05', 47.49), ('reg:0.0001', 46.83), ('reg:0.001', 34.72), ('reg:0.01', 15.29)]
2019-02-16 00:43:04,374 : Validation : best param found is reg = 1e-05 with score             47.49
2019-02-16 00:43:04,374 : Evaluating...
2019-02-16 00:43:21,941 : 
Dev acc : 47.5 Test acc : 48.0 for TOPCONSTITUENTS classification

2019-02-16 00:43:21,942 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 00:43:22,558 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 00:43:22,635 : loading BERT model bert-base-uncased
2019-02-16 00:43:22,636 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:43:22,681 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:43:22,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp38wmpgm6
2019-02-16 00:43:25,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:43:26,761 : Computing embeddings for train/dev/test
2019-02-16 00:44:51,418 : Computed embeddings
2019-02-16 00:44:51,418 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:45:46,512 : [('reg:1e-05', 51.76), ('reg:0.0001', 50.99), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-16 00:45:46,512 : Validation : best param found is reg = 1e-05 with score             51.76
2019-02-16 00:45:46,512 : Evaluating...
2019-02-16 00:46:05,537 : 
Dev acc : 51.8 Test acc : 51.9 for BIGRAMSHIFT classification

2019-02-16 00:46:05,538 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 00:46:05,967 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 00:46:06,041 : loading BERT model bert-base-uncased
2019-02-16 00:46:06,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:46:06,073 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:46:06,074 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnde827lv
2019-02-16 00:46:08,641 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:46:10,148 : Computing embeddings for train/dev/test
2019-02-16 00:47:32,462 : Computed embeddings
2019-02-16 00:47:32,462 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:48:39,549 : [('reg:1e-05', 87.51), ('reg:0.0001', 87.49), ('reg:0.001', 86.42), ('reg:0.01', 76.85)]
2019-02-16 00:48:39,549 : Validation : best param found is reg = 1e-05 with score             87.51
2019-02-16 00:48:39,549 : Evaluating...
2019-02-16 00:48:59,080 : 
Dev acc : 87.5 Test acc : 85.5 for TENSE classification

2019-02-16 00:48:59,082 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 00:48:59,544 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 00:48:59,613 : loading BERT model bert-base-uncased
2019-02-16 00:48:59,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:48:59,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:48:59,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvkzhbnqe
2019-02-16 00:49:02,333 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:49:03,772 : Computing embeddings for train/dev/test
2019-02-16 00:50:31,284 : Computed embeddings
2019-02-16 00:50:31,284 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:51:27,018 : [('reg:1e-05', 79.88), ('reg:0.0001', 80.1), ('reg:0.001', 74.87), ('reg:0.01', 63.58)]
2019-02-16 00:51:27,018 : Validation : best param found is reg = 0.0001 with score             80.1
2019-02-16 00:51:27,018 : Evaluating...
2019-02-16 00:51:47,648 : 
Dev acc : 80.1 Test acc : 79.0 for SUBJNUMBER classification

2019-02-16 00:51:47,649 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 00:51:48,129 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 00:51:48,207 : loading BERT model bert-base-uncased
2019-02-16 00:51:48,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:51:48,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:51:48,340 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr1l59dso
2019-02-16 00:51:50,878 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:51:52,393 : Computing embeddings for train/dev/test
2019-02-16 00:53:18,454 : Computed embeddings
2019-02-16 00:53:18,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:54:20,211 : [('reg:1e-05', 77.7), ('reg:0.0001', 75.06), ('reg:0.001', 70.92), ('reg:0.01', 66.23)]
2019-02-16 00:54:20,211 : Validation : best param found is reg = 1e-05 with score             77.7
2019-02-16 00:54:20,212 : Evaluating...
2019-02-16 00:54:37,248 : 
Dev acc : 77.7 Test acc : 77.4 for OBJNUMBER classification

2019-02-16 00:54:37,249 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 00:54:37,857 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 00:54:37,933 : loading BERT model bert-base-uncased
2019-02-16 00:54:37,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:54:37,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:54:37,965 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8nfhjtyd
2019-02-16 00:54:40,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:54:41,978 : Computing embeddings for train/dev/test
2019-02-16 00:56:19,915 : Computed embeddings
2019-02-16 00:56:19,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:57:05,335 : [('reg:1e-05', 54.72), ('reg:0.0001', 52.59), ('reg:0.001', 51.52), ('reg:0.01', 50.24)]
2019-02-16 00:57:05,335 : Validation : best param found is reg = 1e-05 with score             54.72
2019-02-16 00:57:05,335 : Evaluating...
2019-02-16 00:57:16,374 : 
Dev acc : 54.7 Test acc : 53.9 for ODDMANOUT classification

2019-02-16 00:57:16,375 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 00:57:16,850 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 00:57:16,938 : loading BERT model bert-base-uncased
2019-02-16 00:57:16,938 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 00:57:16,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 00:57:16,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3ipz6juz
2019-02-16 00:57:19,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 00:57:20,951 : Computing embeddings for train/dev/test
2019-02-16 00:59:03,242 : Computed embeddings
2019-02-16 00:59:03,242 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 00:59:56,683 : [('reg:1e-05', 55.02), ('reg:0.0001', 54.39), ('reg:0.001', 51.45), ('reg:0.01', 50.02)]
2019-02-16 00:59:56,684 : Validation : best param found is reg = 1e-05 with score             55.02
2019-02-16 00:59:56,684 : Evaluating...
2019-02-16 01:00:07,417 : 
Dev acc : 55.0 Test acc : 53.8 for COORDINATIONINVERSION classification

2019-02-16 01:00:07,419 : total results: {'STS12': {'MSRpar': {'pearson': (0.20945693944427735, 6.9967563097552725e-09), 'spearman': SpearmanrResult(correlation=0.25802253776381934, pvalue=7.161629692866363e-13), 'nsamples': 750}, 'MSRvid': {'pearson': (0.1817445500358232, 5.421383083994221e-07), 'spearman': SpearmanrResult(correlation=0.24568339719121218, pvalue=8.982990477854401e-12), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4043030526978324, 1.771832363667932e-19), 'spearman': SpearmanrResult(correlation=0.5703156417894253, pvalue=5.907155056396839e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.10863243859512511, 0.002893044815774046), 'spearman': SpearmanrResult(correlation=0.1730915633548561, pvalue=1.8565908224712926e-06), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.47679325179491316, 4.8633395443373256e-24), 'spearman': SpearmanrResult(correlation=0.370660312407875, pvalue=1.9318755781911735e-14), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.27618604651359424, 'wmean': 0.24153508838831877}, 'spearman': {'mean': 0.3235546905014376, 'wmean': 0.2951307811983604}}}, 'STS13': {'FNWN': {'pearson': (0.23136781735976955, 0.0013588246413546916), 'spearman': SpearmanrResult(correlation=0.24168285537444215, pvalue=0.0008070811660923214), 'nsamples': 189}, 'headlines': {'pearson': (0.0665754262361011, 0.06842099503832993), 'spearman': SpearmanrResult(correlation=0.2286868536442046, pvalue=2.3495495962033673e-10), 'nsamples': 750}, 'OnWN': {'pearson': (0.18083580077715605, 1.638472052659954e-05), 'spearman': SpearmanrResult(correlation=0.19363283738021173, pvalue=3.840309874373167e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1595930147910089, 'wmean': 0.13007264759603787}, 'spearman': {'mean': 0.22133418213295283, 'wmean': 0.2172141477794812}}}, 'STS14': {'deft-forum': {'pearson': (0.1024290570255165, 0.029816314110708477), 'spearman': SpearmanrResult(correlation=0.16306281364278286, pvalue=0.0005151851768563842), 'nsamples': 450}, 'deft-news': {'pearson': (0.3203380401976993, 1.382747451331689e-08), 'spearman': SpearmanrResult(correlation=0.5082487788114827, pvalue=4.138806076821094e-21), 'nsamples': 300}, 'headlines': {'pearson': (0.08837392296666471, 0.015481244790591158), 'spearman': SpearmanrResult(correlation=0.20091407128055813, pvalue=2.860498450911207e-08), 'nsamples': 750}, 'images': {'pearson': (0.1569568102555124, 1.574061611183727e-05), 'spearman': SpearmanrResult(correlation=0.28583209074109345, pvalue=1.4410823016756517e-15), 'nsamples': 750}, 'OnWN': {'pearson': (0.16984829366685344, 2.9002955030927377e-06), 'spearman': SpearmanrResult(correlation=0.21427743310163158, pvalue=3.077267320677454e-09), 'nsamples': 750}, 'tweet-news': {'pearson': (0.1508698303227869, 3.3448891727989087e-05), 'spearman': SpearmanrResult(correlation=0.24089438948105124, pvalue=2.3116618189585133e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.16480265907250555, 'wmean': 0.15112830150124143}, 'spearman': {'mean': 0.26887159617643336, 'wmean': 0.2486110368629194}}}, 'STS15': {'answers-forums': {'pearson': (0.16664849892153513, 0.0011993314806627536), 'spearman': SpearmanrResult(correlation=0.21994388540840118, pvalue=1.725666959297952e-05), 'nsamples': 375}, 'answers-students': {'pearson': (0.26445322500893437, 1.815491749647492e-13), 'spearman': SpearmanrResult(correlation=0.31999221058815996, pvalue=2.5624437485852926e-19), 'nsamples': 750}, 'belief': {'pearson': (0.1637190157556346, 0.0014660931610345801), 'spearman': SpearmanrResult(correlation=0.32823832330643044, pvalue=7.196240437881015e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.1780433694389158, 9.245953436045306e-07), 'spearman': SpearmanrResult(correlation=0.3409005417892076, pvalue=7.275982847614968e-22), 'nsamples': 750}, 'images': {'pearson': (0.005683500399955518, 0.8765133079693878), 'spearman': SpearmanrResult(correlation=0.33110162542256905, pvalue=1.202228490061825e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.15570952190499507, 'wmean': 0.15334096304659764}, 'spearman': {'mean': 0.30803531730295364, 'wmean': 0.3165213705393381}}}, 'STS16': {'answer-answer': {'pearson': (0.18525103206964225, 0.003041161319017776), 'spearman': SpearmanrResult(correlation=0.3210643703516975, pvalue=1.6875477355189946e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.1928456086696344, 0.002239509097837317), 'spearman': SpearmanrResult(correlation=0.41973150383902785, pvalue=4.786540731906739e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.24368686170748624, 0.0001899845723577504), 'spearman': SpearmanrResult(correlation=0.4165842485982131, pvalue=4.543234119013539e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.3142979312473921, 5.390535305317704e-07), 'spearman': SpearmanrResult(correlation=0.730258746391663, pvalue=6.318764990858222e-42), 'nsamples': 244}, 'question-question': {'pearson': (0.058583904074218776, 0.3994621369620561), 'spearman': SpearmanrResult(correlation=0.13743518679580682, pvalue=0.047212625302725374), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.19893306755367474, 'wmean': 0.20240558859445643}, 'spearman': {'mean': 0.4050148111952817, 'wmean': 0.412128971216465}}}, 'MR': {'devacc': 63.65, 'acc': 66.26, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 68.4, 'acc': 65.75, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 79.95, 'acc': 82.32, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.84, 'acc': 87.94, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.69, 'acc': 73.48, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.6, 'acc': 36.02, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 52.0, 'acc': 75.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.5, 'acc': 71.83, 'f1': 80.94, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.6, 'acc': 67.77, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7572475777026654, 'pearson': 0.7527080905415069, 'spearman': 0.6883958069583715, 'mse': 0.4448644346280351, 'yhat': array([2.056121  , 4.04908499, 1.90648232, ..., 3.55824354, 3.98396706,        4.38456622]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6342857221656193, 'pearson': 0.5413199282897899, 'spearman': 0.5347957204141336, 'mse': 1.9346301160062207, 'yhat': array([2.86092747, 2.2328785 , 2.69322221, ..., 3.73555332, 3.66448071,        3.33935188]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.11, 'acc': 56.59, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 317.676, 'acc': [(27.82, 60.699999999999996, 75.63999999999999, 3.6), (22.484, 55.712, 72.61999999999999, 4.4)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 87.25, 'acc': 87.15, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 1.99, 'acc': 1.91, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.11, 'acc': 30.22, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 47.49, 'acc': 48.05, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 51.76, 'acc': 51.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.51, 'acc': 85.46, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.1, 'acc': 79.05, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.7, 'acc': 77.42, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.72, 'acc': 53.89, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.02, 'acc': 53.75, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 01:00:07,419 : STS12 p=0.2415, STS12 s=0.2951, STS13 p=0.1301, STS13 s=0.2172, STS14 p=0.1511, STS14 s=0.2486, STS15 p=0.1533, STS15 s=0.3165, STS 16 p=0.2024, STS16 s=0.4121, STS B p=0.5413, STS B s=0.5348, STS B m=1.9346, SICK-R p=0.7527, SICK-R s=0.6884, SICK-P m=0.4449
2019-02-16 01:00:07,419 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 01:00:07,419 : 0.2415,0.2951,0.1301,0.2172,0.1511,0.2486,0.1533,0.3165,0.2024,0.4121,0.5413,0.5348,1.9346,0.7527,0.6884,0.4449
2019-02-16 01:00:07,419 : MR=66.26, CR=65.75, SUBJ=87.94, MPQA=82.32, SST-B=73.48, SST-F=36.02, TREC=75.20, SICK-E=67.77, SNLI=56.59, MRPC=71.83, MRPC f=80.94
2019-02-16 01:00:07,419 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 01:00:07,419 : 66.26,65.75,87.94,82.32,73.48,36.02,75.20,67.77,56.59,71.83,80.94
2019-02-16 01:00:07,420 : COCO r1i2t=27.82, COCO r5i2t=60.70, COCO r10i2t=75.64, COCO medr_i2t=3.60, COCO r1t2i=22.48, COCO r5t2i=55.71, COCO r10t2i=72.62, COCO medr_t2i=4.40
2019-02-16 01:00:07,420 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 01:00:07,420 : 27.82,60.70,75.64,3.60,22.48,55.71,72.62,4.40
2019-02-16 01:00:07,420 : SentLen=87.15, WC=1.91, TreeDepth=30.22, TopConst=48.05, BShift=51.89, Tense=85.46, SubjNum=79.05, ObjNum=77.42, SOMO=53.89, CoordInv=53.75, average=56.88
2019-02-16 01:00:07,420 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 01:00:07,420 : 87.15,1.91,30.22,48.05,51.89,85.46,79.05,77.42,53.89,53.75,56.88
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,420 : layer 4
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,420 : ********************************************************************************
2019-02-16 01:00:07,517 : ***** Transfer task : STS12 *****


2019-02-16 01:00:07,530 : loading BERT model bert-base-uncased
2019-02-16 01:00:07,530 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:07,550 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:07,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp25d746jp
2019-02-16 01:00:10,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:13,320 : MSRpar : pearson = 0.3329, spearman = 0.3789
2019-02-16 01:00:14,112 : MSRvid : pearson = 0.4663, spearman = 0.5942
2019-02-16 01:00:14,744 : SMTeuroparl : pearson = 0.4427, spearman = 0.5680
2019-02-16 01:00:15,912 : surprise.OnWN : pearson = 0.1711, spearman = 0.2073
2019-02-16 01:00:16,567 : surprise.SMTnews : pearson = 0.6665, spearman = 0.5084
2019-02-16 01:00:16,567 : ALL (weighted average) : Pearson = 0.3851,             Spearman = 0.4340
2019-02-16 01:00:16,568 : ALL (average) : Pearson = 0.4159,             Spearman = 0.4514

2019-02-16 01:00:16,568 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 01:00:16,578 : loading BERT model bert-base-uncased
2019-02-16 01:00:16,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:16,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:16,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzh9pw8v2
2019-02-16 01:00:19,104 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:21,230 : FNWN : pearson = 0.3230, spearman = 0.3143
2019-02-16 01:00:22,099 : headlines : pearson = 0.2994, spearman = 0.4266
2019-02-16 01:00:22,797 : OnWN : pearson = 0.2803, spearman = 0.3220
2019-02-16 01:00:22,797 : ALL (weighted average) : Pearson = 0.2953,             Spearman = 0.3733
2019-02-16 01:00:22,797 : ALL (average) : Pearson = 0.3009,             Spearman = 0.3543

2019-02-16 01:00:22,797 : ***** Transfer task : STS14 *****


2019-02-16 01:00:22,815 : loading BERT model bert-base-uncased
2019-02-16 01:00:22,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:22,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:22,836 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp94x7fjcq
2019-02-16 01:00:25,378 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:27,501 : deft-forum : pearson = 0.2759, spearman = 0.3438
2019-02-16 01:00:28,203 : deft-news : pearson = 0.5811, spearman = 0.6768
2019-02-16 01:00:29,207 : headlines : pearson = 0.2888, spearman = 0.3552
2019-02-16 01:00:30,152 : images : pearson = 0.5990, spearman = 0.5940
2019-02-16 01:00:31,133 : OnWN : pearson = 0.2934, spearman = 0.3269
2019-02-16 01:00:32,356 : tweet-news : pearson = 0.2815, spearman = 0.3298
2019-02-16 01:00:32,357 : ALL (weighted average) : Pearson = 0.3721,             Spearman = 0.4165
2019-02-16 01:00:32,357 : ALL (average) : Pearson = 0.3866,             Spearman = 0.4377

2019-02-16 01:00:32,357 : ***** Transfer task : STS15 *****


2019-02-16 01:00:32,397 : loading BERT model bert-base-uncased
2019-02-16 01:00:32,397 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:32,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:32,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy6m6bf56
2019-02-16 01:00:34,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:37,371 : answers-forums : pearson = 0.3999, spearman = 0.4372
2019-02-16 01:00:38,294 : answers-students : pearson = 0.3854, spearman = 0.4209
2019-02-16 01:00:39,171 : belief : pearson = 0.3637, spearman = 0.4716
2019-02-16 01:00:40,194 : headlines : pearson = 0.3944, spearman = 0.4949
2019-02-16 01:00:41,134 : images : pearson = 0.1379, spearman = 0.5091
2019-02-16 01:00:41,134 : ALL (weighted average) : Pearson = 0.3249,             Spearman = 0.4698
2019-02-16 01:00:41,135 : ALL (average) : Pearson = 0.3363,             Spearman = 0.4667

2019-02-16 01:00:41,135 : ***** Transfer task : STS16 *****


2019-02-16 01:00:41,182 : loading BERT model bert-base-uncased
2019-02-16 01:00:41,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:41,207 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:41,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp68wtc9_q
2019-02-16 01:00:43,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:45,639 : answer-answer : pearson = 0.3228, spearman = 0.4578
2019-02-16 01:00:45,972 : headlines : pearson = 0.3946, spearman = 0.5296
2019-02-16 01:00:46,384 : plagiarism : pearson = 0.4793, spearman = 0.6467
2019-02-16 01:00:47,018 : postediting : pearson = 0.4859, spearman = 0.7718
2019-02-16 01:00:47,312 : question-question : pearson = 0.5365, spearman = 0.5480
2019-02-16 01:00:47,312 : ALL (weighted average) : Pearson = 0.4394,             Spearman = 0.5900
2019-02-16 01:00:47,312 : ALL (average) : Pearson = 0.4438,             Spearman = 0.5908

2019-02-16 01:00:47,312 : ***** Transfer task : MR *****


2019-02-16 01:00:47,332 : loading BERT model bert-base-uncased
2019-02-16 01:00:47,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:00:47,399 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:00:47,399 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr_mxyohx
2019-02-16 01:00:49,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:00:51,474 : Generating sentence embeddings
2019-02-16 01:01:05,794 : Generated sentence embeddings
2019-02-16 01:01:05,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:01:26,269 : Best param found at split 1: l2reg = 0.01                 with score 64.0
2019-02-16 01:01:54,070 : Best param found at split 2: l2reg = 0.0001                 with score 67.85
2019-02-16 01:02:15,935 : Best param found at split 3: l2reg = 1e-05                 with score 66.94
2019-02-16 01:02:40,950 : Best param found at split 4: l2reg = 1e-05                 with score 67.44
2019-02-16 01:03:01,482 : Best param found at split 5: l2reg = 1e-05                 with score 66.37
2019-02-16 01:03:02,136 : Dev acc : 66.52 Test acc : 65.33

2019-02-16 01:03:02,137 : ***** Transfer task : CR *****


2019-02-16 01:03:02,145 : loading BERT model bert-base-uncased
2019-02-16 01:03:02,145 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:03:02,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:03:02,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpefav30xi
2019-02-16 01:03:04,690 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:03:06,203 : Generating sentence embeddings
2019-02-16 01:03:09,932 : Generated sentence embeddings
2019-02-16 01:03:09,932 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:03:16,112 : Best param found at split 1: l2reg = 0.001                 with score 68.57
2019-02-16 01:03:22,481 : Best param found at split 2: l2reg = 0.001                 with score 68.73
2019-02-16 01:03:29,798 : Best param found at split 3: l2reg = 0.001                 with score 70.89
2019-02-16 01:03:37,383 : Best param found at split 4: l2reg = 0.001                 with score 70.8
2019-02-16 01:03:44,585 : Best param found at split 5: l2reg = 1e-05                 with score 70.54
2019-02-16 01:03:44,912 : Dev acc : 69.91 Test acc : 64.03

2019-02-16 01:03:44,913 : ***** Transfer task : MPQA *****


2019-02-16 01:03:44,962 : loading BERT model bert-base-uncased
2019-02-16 01:03:44,962 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:03:44,990 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:03:44,990 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp00m_bb63
2019-02-16 01:03:47,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:03:49,077 : Generating sentence embeddings
2019-02-16 01:03:52,878 : Generated sentence embeddings
2019-02-16 01:03:52,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:04:11,570 : Best param found at split 1: l2reg = 0.0001                 with score 81.42
2019-02-16 01:04:36,672 : Best param found at split 2: l2reg = 1e-05                 with score 83.75
2019-02-16 01:04:59,260 : Best param found at split 3: l2reg = 0.0001                 with score 83.88
2019-02-16 01:05:21,783 : Best param found at split 4: l2reg = 0.001                 with score 82.84
2019-02-16 01:05:45,059 : Best param found at split 5: l2reg = 0.0001                 with score 83.05
2019-02-16 01:05:46,391 : Dev acc : 82.99 Test acc : 81.84

2019-02-16 01:05:46,392 : ***** Transfer task : SUBJ *****


2019-02-16 01:05:46,414 : loading BERT model bert-base-uncased
2019-02-16 01:05:46,414 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:05:46,476 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:05:46,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0xhg8mwf
2019-02-16 01:05:49,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:05:50,505 : Generating sentence embeddings
2019-02-16 01:06:03,614 : Generated sentence embeddings
2019-02-16 01:06:03,614 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 01:06:21,707 : Best param found at split 1: l2reg = 0.0001                 with score 90.86
2019-02-16 01:06:42,255 : Best param found at split 2: l2reg = 1e-05                 with score 91.35
2019-02-16 01:07:02,471 : Best param found at split 3: l2reg = 0.001                 with score 90.94
2019-02-16 01:07:30,713 : Best param found at split 4: l2reg = 0.0001                 with score 91.7
2019-02-16 01:08:06,032 : Best param found at split 5: l2reg = 1e-05                 with score 91.14
2019-02-16 01:08:08,160 : Dev acc : 91.2 Test acc : 91.02

2019-02-16 01:08:08,162 : ***** Transfer task : SST Binary classification *****


2019-02-16 01:08:08,320 : loading BERT model bert-base-uncased
2019-02-16 01:08:08,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:08:08,343 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:08:08,343 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa38ladcc
2019-02-16 01:08:10,890 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:08:12,444 : Computing embedding for train
2019-02-16 01:09:27,796 : Computed train embeddings
2019-02-16 01:09:27,796 : Computing embedding for dev
2019-02-16 01:09:28,846 : Computed dev embeddings
2019-02-16 01:09:28,846 : Computing embedding for test
2019-02-16 01:09:31,063 : Computed test embeddings
2019-02-16 01:09:31,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:10:15,667 : [('reg:1e-05', 77.52), ('reg:0.0001', 77.64), ('reg:0.001', 77.64), ('reg:0.01', 73.62)]
2019-02-16 01:10:15,668 : Validation : best param found is reg = 0.0001 with score             77.64
2019-02-16 01:10:15,668 : Evaluating...
2019-02-16 01:10:28,000 : 
Dev acc : 77.64 Test acc : 76.83 for             SST Binary classification

2019-02-16 01:10:28,001 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 01:10:28,063 : loading BERT model bert-base-uncased
2019-02-16 01:10:28,063 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:10:28,089 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:10:28,090 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcgzch2ul
2019-02-16 01:10:30,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:10:32,119 : Computing embedding for train
2019-02-16 01:10:42,406 : Computed train embeddings
2019-02-16 01:10:42,406 : Computing embedding for dev
2019-02-16 01:10:43,725 : Computed dev embeddings
2019-02-16 01:10:43,725 : Computing embedding for test
2019-02-16 01:10:46,517 : Computed test embeddings
2019-02-16 01:10:46,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:10:53,706 : [('reg:1e-05', 36.15), ('reg:0.0001', 37.6), ('reg:0.001', 36.06), ('reg:0.01', 27.88)]
2019-02-16 01:10:53,706 : Validation : best param found is reg = 0.0001 with score             37.6
2019-02-16 01:10:53,706 : Evaluating...
2019-02-16 01:10:56,216 : 
Dev acc : 37.6 Test acc : 37.38 for             SST Fine-Grained classification

2019-02-16 01:10:56,217 : ***** Transfer task : TREC *****


2019-02-16 01:10:56,234 : loading BERT model bert-base-uncased
2019-02-16 01:10:56,234 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:10:56,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:10:56,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyr69vcz3
2019-02-16 01:10:58,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:11:05,668 : Computed train embeddings
2019-02-16 01:11:06,122 : Computed test embeddings
2019-02-16 01:11:06,122 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 01:11:28,664 : [('reg:1e-05', 55.36), ('reg:0.0001', 54.75), ('reg:0.001', 49.97), ('reg:0.01', 44.28)]
2019-02-16 01:11:28,664 : Cross-validation : best param found is reg = 1e-05             with score 55.36
2019-02-16 01:11:28,664 : Evaluating...
2019-02-16 01:11:29,813 : 
Dev acc : 55.36 Test acc : 67.8             for TREC

2019-02-16 01:11:29,814 : ***** Transfer task : MRPC *****


2019-02-16 01:11:29,836 : loading BERT model bert-base-uncased
2019-02-16 01:11:29,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:11:29,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:11:29,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpai3flbpf
2019-02-16 01:11:32,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:11:34,024 : Computing embedding for train
2019-02-16 01:11:46,950 : Computed train embeddings
2019-02-16 01:11:46,951 : Computing embedding for test
2019-02-16 01:11:52,703 : Computed test embeddings
2019-02-16 01:11:52,720 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 01:12:08,906 : [('reg:1e-05', 69.36), ('reg:0.0001', 69.55), ('reg:0.001', 69.06), ('reg:0.01', 68.52)]
2019-02-16 01:12:08,907 : Cross-validation : best param found is reg = 0.0001             with score 69.55
2019-02-16 01:12:08,907 : Evaluating...
2019-02-16 01:12:09,887 : Dev acc : 69.55 Test acc 67.54; Test F1 80.35 for MRPC.

2019-02-16 01:12:09,887 : ***** Transfer task : SICK-Entailment*****


2019-02-16 01:12:09,915 : loading BERT model bert-base-uncased
2019-02-16 01:12:09,916 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:12:09,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:12:09,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxnimfie6
2019-02-16 01:12:12,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:12:14,074 : Computing embedding for train
2019-02-16 01:12:22,745 : Computed train embeddings
2019-02-16 01:12:22,745 : Computing embedding for dev
2019-02-16 01:12:23,983 : Computed dev embeddings
2019-02-16 01:12:23,983 : Computing embedding for test
2019-02-16 01:12:32,029 : Computed test embeddings
2019-02-16 01:12:32,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:12:35,290 : [('reg:1e-05', 72.4), ('reg:0.0001', 74.6), ('reg:0.001', 66.4), ('reg:0.01', 69.4)]
2019-02-16 01:12:35,290 : Validation : best param found is reg = 0.0001 with score             74.6
2019-02-16 01:12:35,290 : Evaluating...
2019-02-16 01:12:36,185 : 
Dev acc : 74.6 Test acc : 74.08 for                        SICK entailment

2019-02-16 01:12:36,186 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 01:12:36,266 : loading BERT model bert-base-uncased
2019-02-16 01:12:36,266 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:12:36,291 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:12:36,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxcscuax2
2019-02-16 01:12:38,886 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:12:40,377 : Computing embedding for train
2019-02-16 01:12:46,981 : Computed train embeddings
2019-02-16 01:12:46,981 : Computing embedding for dev
2019-02-16 01:12:47,813 : Computed dev embeddings
2019-02-16 01:12:47,813 : Computing embedding for test
2019-02-16 01:12:54,927 : Computed test embeddings
2019-02-16 01:13:47,226 : Dev : Pearson 0.7835423335857185
2019-02-16 01:13:47,226 : Test : Pearson 0.7722223091331447 Spearman 0.6980729292493973 MSE 0.4108660467326688                        for SICK Relatedness

2019-02-16 01:13:47,227 : 

***** Transfer task : STSBenchmark*****


2019-02-16 01:13:47,303 : loading BERT model bert-base-uncased
2019-02-16 01:13:47,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:13:47,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:13:47,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy83z2b2p
2019-02-16 01:13:49,934 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:13:51,378 : Computing embedding for train
2019-02-16 01:14:00,498 : Computed train embeddings
2019-02-16 01:14:00,498 : Computing embedding for dev
2019-02-16 01:14:03,589 : Computed dev embeddings
2019-02-16 01:14:03,589 : Computing embedding for test
2019-02-16 01:14:06,253 : Computed test embeddings
2019-02-16 01:14:57,012 : Dev : Pearson 0.6845589020484055
2019-02-16 01:14:57,013 : Test : Pearson 0.6424955470744644 Spearman 0.6384598771531589 MSE 1.6651889503720172                        for SICK Relatedness

2019-02-16 01:14:57,013 : ***** Transfer task : SNLI Entailment*****


2019-02-16 01:15:03,232 : loading BERT model bert-base-uncased
2019-02-16 01:15:03,232 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:15:03,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:15:03,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpei2ljqiq
2019-02-16 01:15:06,432 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:15:08,703 : PROGRESS (encoding): 0.00%
2019-02-16 01:16:56,564 : PROGRESS (encoding): 14.56%
2019-02-16 01:18:51,711 : PROGRESS (encoding): 29.12%
2019-02-16 01:20:52,786 : PROGRESS (encoding): 43.69%
2019-02-16 01:23:03,330 : PROGRESS (encoding): 58.25%
2019-02-16 01:25:16,127 : PROGRESS (encoding): 72.81%
2019-02-16 01:27:40,580 : PROGRESS (encoding): 87.37%
2019-02-16 01:29:47,684 : PROGRESS (encoding): 0.00%
2019-02-16 01:30:05,169 : PROGRESS (encoding): 0.00%
2019-02-16 01:30:23,790 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 01:31:40,139 : [('reg:1e-09', 58.45)]
2019-02-16 01:31:40,139 : Validation : best param found is reg = 1e-09 with score             58.45
2019-02-16 01:31:40,139 : Evaluating...
2019-02-16 01:32:14,937 : Dev acc : 58.45 Test acc : 58.35 for SNLI

2019-02-16 01:32:14,937 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 01:32:24,297 : loading BERT model bert-base-uncased
2019-02-16 01:32:24,297 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 01:32:24,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 01:32:24,361 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpplvxixji
2019-02-16 01:32:27,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 01:32:29,681 : Computing embedding for train
2019-02-16 01:42:17,505 : Computed train embeddings
2019-02-16 01:42:17,505 : Computing embedding for dev
2019-02-16 01:42:48,681 : Computed dev embeddings
2019-02-16 01:42:48,681 : Computing embedding for test
2019-02-16 01:43:23,255 : Computed test embeddings
2019-02-16 01:43:23,271 : prepare data
2019-02-16 01:43:23,336 : start epoch
2019-02-16 01:44:08,163 : samples : 64000
2019-02-16 01:44:18,527 : Image to text: 5.66, 17.32, 27.72, 31.0
2019-02-16 01:44:26,142 : Text to Image: 4.692, 15.764, 24.84, 34.0
2019-02-16 01:45:10,823 : samples : 128000
2019-02-16 01:45:21,391 : Image to text: 6.18, 20.06, 30.74, 27.0
2019-02-16 01:45:30,057 : Text to Image: 4.912, 16.672, 26.36, 32.0
2019-02-16 01:46:15,174 : samples : 192000
2019-02-16 01:46:25,803 : Image to text: 6.84, 21.16, 31.68, 25.0
2019-02-16 01:46:33,370 : Text to Image: 5.26, 17.696, 27.644, 30.0
2019-02-16 01:47:17,164 : samples : 256000
2019-02-16 01:47:27,716 : Image to text: 7.22, 22.04, 32.72, 24.0
2019-02-16 01:47:35,268 : Text to Image: 6.172, 19.032, 29.624, 28.0
2019-02-16 01:48:20,000 : samples : 320000
2019-02-16 01:48:30,641 : Image to text: 6.9, 22.56, 33.94, 23.0
2019-02-16 01:48:37,999 : Text to Image: 5.608, 18.664, 28.476, 29.0
2019-02-16 01:49:29,256 : samples : 384000
2019-02-16 01:49:41,021 : Image to text: 7.64, 23.24, 33.5, 22.0
2019-02-16 01:49:48,803 : Text to Image: 6.024, 19.324, 29.4, 28.0
2019-02-16 01:50:33,642 : samples : 448000
2019-02-16 01:50:44,197 : Image to text: 8.02, 23.36, 34.84, 21.0
2019-02-16 01:50:51,756 : Text to Image: 6.66, 20.892, 31.616, 25.0
2019-02-16 01:51:36,297 : samples : 512000
2019-02-16 01:51:46,906 : Image to text: 7.26, 22.36, 33.02, 24.0
2019-02-16 01:51:54,520 : Text to Image: 5.536, 18.128, 27.876, 30.0
2019-02-16 01:52:32,554 : Epoch 1 finished
2019-02-16 01:52:33,529 : Image to text: 23.0, 53.7, 68.7, 5.0
2019-02-16 01:52:34,270 : Text to Image: 16.74, 46.28, 64.06, 6.0
2019-02-16 01:52:35,157 : Image to text: 22.2, 52.4, 68.7, 5.0
2019-02-16 01:52:35,978 : Text to Image: 16.76, 45.48, 63.96, 6.0
2019-02-16 01:52:36,929 : Image to text: 22.8, 53.1, 69.0, 5.0
2019-02-16 01:52:37,697 : Text to Image: 17.36, 46.5, 64.12, 6.0
2019-02-16 01:52:38,688 : Image to text: 23.5, 55.0, 69.5, 4.0
2019-02-16 01:52:39,461 : Text to Image: 16.94, 45.98, 63.8, 6.0
2019-02-16 01:52:40,435 : Image to text: 24.8, 54.7, 68.4, 4.0
2019-02-16 01:52:41,195 : Text to Image: 17.42, 47.14, 64.26, 6.0
2019-02-16 01:52:41,195 : Dev mean Text to Image: 17.044, 46.275999999999996, 64.03999999999999, 6.0
2019-02-16 01:52:41,195 : Dev mean Image to text: 23.26, 53.78, 68.86, 4.6
2019-02-16 01:52:41,196 : start epoch
2019-02-16 01:53:27,009 : samples : 64000
2019-02-16 01:53:37,611 : Image to text: 8.58, 25.08, 37.08, 19.0
2019-02-16 01:53:45,223 : Text to Image: 6.796, 20.952, 31.84, 24.0
2019-02-16 01:54:29,659 : samples : 128000
2019-02-16 01:54:40,690 : Image to text: 9.0, 25.5, 38.1, 19.0
2019-02-16 01:54:50,954 : Text to Image: 7.096, 21.752, 33.108, 23.0
2019-02-16 01:55:36,342 : samples : 192000
2019-02-16 01:55:46,994 : Image to text: 7.88, 23.9, 35.36, 21.0
2019-02-16 01:55:55,523 : Text to Image: 6.404, 20.628, 31.212, 25.0
2019-02-16 01:56:40,440 : samples : 256000
2019-02-16 01:56:50,893 : Image to text: 8.58, 25.9, 37.38, 19.0
2019-02-16 01:56:59,591 : Text to Image: 7.44, 22.484, 33.432, 23.0
2019-02-16 01:57:44,332 : samples : 320000
2019-02-16 01:57:54,779 : Image to text: 9.58, 26.78, 38.6, 18.0
2019-02-16 01:58:04,800 : Text to Image: 7.384, 21.788, 32.436, 24.0
2019-02-16 01:58:51,479 : samples : 384000
2019-02-16 01:59:04,050 : Image to text: 8.74, 25.48, 37.28, 19.0
2019-02-16 01:59:14,038 : Text to Image: 7.48, 22.696, 33.96, 22.0
2019-02-16 02:00:00,910 : samples : 448000
2019-02-16 02:00:13,484 : Image to text: 8.74, 25.12, 36.46, 20.0
2019-02-16 02:00:23,517 : Text to Image: 6.632, 21.5, 32.24, 24.0
2019-02-16 02:01:10,375 : samples : 512000
2019-02-16 02:01:22,979 : Image to text: 9.06, 26.82, 38.56, 18.0
2019-02-16 02:01:32,982 : Text to Image: 7.196, 21.972, 33.004, 24.0
2019-02-16 02:02:12,200 : Epoch 2 finished
2019-02-16 02:02:13,175 : Image to text: 22.7, 55.2, 69.3, 4.0
2019-02-16 02:02:13,952 : Text to Image: 19.2, 50.22, 68.44, 5.0
2019-02-16 02:02:14,836 : Image to text: 22.2, 55.5, 70.6, 4.0
2019-02-16 02:02:15,632 : Text to Image: 19.5, 50.98, 67.22, 5.0
2019-02-16 02:02:16,569 : Image to text: 25.2, 54.8, 69.6, 5.0
2019-02-16 02:02:17,338 : Text to Image: 19.28, 50.1, 67.3, 5.0
2019-02-16 02:02:18,245 : Image to text: 25.5, 56.0, 70.3, 4.0
2019-02-16 02:02:18,990 : Text to Image: 19.5, 50.88, 68.22, 5.0
2019-02-16 02:02:19,921 : Image to text: 23.9, 56.6, 72.8, 4.0
2019-02-16 02:02:20,686 : Text to Image: 19.6, 50.96, 67.5, 5.0
2019-02-16 02:02:20,687 : Dev mean Text to Image: 19.416, 50.628, 67.73599999999999, 5.0
2019-02-16 02:02:20,687 : Dev mean Image to text: 23.9, 55.62, 70.52, 4.2
2019-02-16 02:02:20,687 : start epoch
2019-02-16 02:03:07,343 : samples : 64000
2019-02-16 02:03:19,939 : Image to text: 9.0, 26.52, 39.0, 18.0
2019-02-16 02:03:29,987 : Text to Image: 7.576, 22.552, 33.592, 22.0
2019-02-16 02:04:15,925 : samples : 128000
2019-02-16 02:04:28,507 : Image to text: 8.04, 24.18, 36.46, 20.0
2019-02-16 02:04:38,519 : Text to Image: 6.836, 21.656, 32.608, 24.0
2019-02-16 02:05:24,570 : samples : 192000
2019-02-16 02:05:37,266 : Image to text: 9.5, 27.66, 39.44, 18.0
2019-02-16 02:05:47,286 : Text to Image: 6.928, 22.288, 33.636, 23.0
2019-02-16 02:06:39,686 : samples : 256000
2019-02-16 02:06:52,622 : Image to text: 9.36, 26.72, 38.12, 18.0
2019-02-16 02:07:02,683 : Text to Image: 7.04, 22.436, 33.152, 23.0
2019-02-16 02:07:48,563 : samples : 320000
2019-02-16 02:08:01,134 : Image to text: 9.5, 27.38, 39.18, 18.0
2019-02-16 02:08:11,227 : Text to Image: 7.852, 23.36, 34.62, 21.0
2019-02-16 02:08:57,900 : samples : 384000
2019-02-16 02:09:10,074 : Image to text: 9.2, 26.88, 39.3, 18.0
2019-02-16 02:09:20,224 : Text to Image: 7.428, 22.588, 33.66, 22.0
2019-02-16 02:10:05,864 : samples : 448000
2019-02-16 02:10:16,391 : Image to text: 10.56, 28.18, 40.2, 17.0
2019-02-16 02:10:23,970 : Text to Image: 7.74, 23.852, 35.452, 21.0
2019-02-16 02:11:07,779 : samples : 512000
2019-02-16 02:11:18,110 : Image to text: 10.62, 27.58, 40.3, 17.0
2019-02-16 02:11:25,142 : Text to Image: 8.372, 24.016, 35.58, 20.0
2019-02-16 02:12:04,930 : Epoch 3 finished
2019-02-16 02:12:05,987 : Image to text: 24.0, 54.3, 71.4, 4.0
2019-02-16 02:12:06,787 : Text to Image: 19.84, 50.82, 68.62, 5.0
2019-02-16 02:12:07,919 : Image to text: 23.6, 58.0, 73.5, 4.0
2019-02-16 02:12:08,792 : Text to Image: 19.88, 50.42, 68.48, 5.0
2019-02-16 02:12:09,864 : Image to text: 25.9, 57.1, 72.9, 4.0
2019-02-16 02:12:10,679 : Text to Image: 19.04, 51.36, 68.78, 5.0
2019-02-16 02:12:11,776 : Image to text: 26.6, 56.4, 73.5, 4.0
2019-02-16 02:12:12,642 : Text to Image: 19.24, 50.98, 68.7, 5.0
2019-02-16 02:12:13,680 : Image to text: 25.5, 59.2, 73.5, 4.0
2019-02-16 02:12:14,540 : Text to Image: 20.4, 51.4, 68.9, 5.0
2019-02-16 02:12:14,540 : Dev mean Text to Image: 19.68, 50.995999999999995, 68.696, 5.0
2019-02-16 02:12:14,540 : Dev mean Image to text: 25.119999999999997, 57.0, 72.96000000000001, 4.0
2019-02-16 02:12:14,541 : start epoch
2019-02-16 02:13:01,386 : samples : 64000
2019-02-16 02:13:14,272 : Image to text: 9.78, 28.66, 40.7, 17.0
2019-02-16 02:13:24,646 : Text to Image: 7.748, 23.528, 35.18, 21.0
2019-02-16 02:14:11,315 : samples : 128000
2019-02-16 02:14:24,163 : Image to text: 9.38, 27.58, 39.74, 17.0
2019-02-16 02:14:34,687 : Text to Image: 7.936, 23.76, 35.292, 21.0
2019-02-16 02:15:21,877 : samples : 192000
2019-02-16 02:15:34,772 : Image to text: 9.54, 27.1, 38.94, 18.0
2019-02-16 02:15:45,291 : Text to Image: 6.932, 21.976, 33.048, 22.0
2019-02-16 02:16:32,128 : samples : 256000
2019-02-16 02:16:45,075 : Image to text: 10.12, 27.9, 40.46, 17.0
2019-02-16 02:16:55,579 : Text to Image: 7.604, 23.08, 34.472, 21.0
2019-02-16 02:17:42,474 : samples : 320000
2019-02-16 02:17:55,370 : Image to text: 10.6, 29.02, 40.54, 16.0
2019-02-16 02:18:05,808 : Text to Image: 8.396, 24.404, 36.152, 20.0
2019-02-16 02:18:53,437 : samples : 384000
2019-02-16 02:19:05,582 : Image to text: 9.98, 28.56, 40.88, 16.0
2019-02-16 02:19:16,053 : Text to Image: 7.916, 23.904, 35.62, 20.0
2019-02-16 02:20:01,528 : samples : 448000
2019-02-16 02:20:12,145 : Image to text: 9.88, 26.54, 39.16, 17.0
2019-02-16 02:20:19,853 : Text to Image: 8.204, 23.7, 35.512, 20.0
2019-02-16 02:21:04,371 : samples : 512000
2019-02-16 02:21:15,020 : Image to text: 10.18, 27.86, 40.46, 16.0
2019-02-16 02:21:22,683 : Text to Image: 7.816, 23.868, 35.3, 20.0
2019-02-16 02:22:00,546 : Epoch 4 finished
2019-02-16 02:22:00,991 : Image to text: 25.9, 59.4, 75.1, 4.0
2019-02-16 02:22:01,332 : Text to Image: 21.0, 53.76, 71.44, 5.0
2019-02-16 02:22:01,771 : Image to text: 24.2, 57.8, 72.6, 4.0
2019-02-16 02:22:02,131 : Text to Image: 20.94, 52.2, 69.34, 5.0
2019-02-16 02:22:02,587 : Image to text: 26.6, 58.2, 73.2, 4.0
2019-02-16 02:22:02,924 : Text to Image: 22.12, 53.98, 70.64, 5.0
2019-02-16 02:22:03,405 : Image to text: 29.5, 60.9, 74.2, 4.0
2019-02-16 02:22:03,762 : Text to Image: 21.58, 52.96, 70.84, 5.0
2019-02-16 02:22:04,238 : Image to text: 28.9, 59.1, 74.7, 4.0
2019-02-16 02:22:04,590 : Text to Image: 21.36, 53.32, 69.48, 5.0
2019-02-16 02:22:04,590 : Dev mean Text to Image: 21.4, 53.244, 70.348, 5.0
2019-02-16 02:22:04,590 : Dev mean Image to text: 27.020000000000003, 59.08, 73.96, 4.0
2019-02-16 02:22:04,590 : start epoch
2019-02-16 02:22:48,300 : samples : 64000
2019-02-16 02:22:58,904 : Image to text: 10.66, 28.3, 41.64, 15.0
2019-02-16 02:23:06,430 : Text to Image: 8.392, 24.76, 36.46, 20.0
2019-02-16 02:24:00,219 : samples : 128000
2019-02-16 02:24:11,927 : Image to text: 10.46, 27.9, 41.08, 16.0
2019-02-16 02:24:19,663 : Text to Image: 7.82, 23.712, 34.94, 21.0
2019-02-16 02:25:03,913 : samples : 192000
2019-02-16 02:25:14,567 : Image to text: 10.92, 29.44, 41.96, 15.0
2019-02-16 02:25:22,157 : Text to Image: 8.672, 25.22, 37.072, 19.0
2019-02-16 02:26:05,865 : samples : 256000
2019-02-16 02:26:16,519 : Image to text: 10.2, 27.98, 40.5, 16.0
2019-02-16 02:26:24,208 : Text to Image: 8.572, 25.08, 37.02, 19.0
2019-02-16 02:27:08,387 : samples : 320000
2019-02-16 02:27:19,009 : Image to text: 10.9, 29.2, 41.74, 16.0
2019-02-16 02:27:26,674 : Text to Image: 8.3, 24.36, 36.356, 20.0
2019-02-16 02:28:11,056 : samples : 384000
2019-02-16 02:28:21,632 : Image to text: 10.38, 28.66, 41.42, 16.0
2019-02-16 02:28:29,192 : Text to Image: 8.684, 24.98, 36.564, 19.0
2019-02-16 02:29:13,095 : samples : 448000
2019-02-16 02:29:23,817 : Image to text: 10.82, 28.92, 41.28, 16.0
2019-02-16 02:29:31,485 : Text to Image: 8.444, 24.628, 36.66, 19.0
2019-02-16 02:30:15,709 : samples : 512000
2019-02-16 02:30:26,317 : Image to text: 10.96, 29.52, 42.22, 15.0
2019-02-16 02:30:33,948 : Text to Image: 8.86, 26.0, 37.936, 18.0
2019-02-16 02:31:12,229 : Epoch 5 finished
2019-02-16 02:31:12,684 : Image to text: 26.2, 57.8, 74.9, 4.0
2019-02-16 02:31:13,040 : Text to Image: 21.26, 54.38, 71.16, 5.0
2019-02-16 02:31:13,518 : Image to text: 26.1, 58.8, 73.6, 4.0
2019-02-16 02:31:13,876 : Text to Image: 21.48, 53.82, 70.98, 5.0
2019-02-16 02:31:14,340 : Image to text: 27.6, 59.6, 74.0, 4.0
2019-02-16 02:31:14,684 : Text to Image: 22.24, 54.9, 71.6, 5.0
2019-02-16 02:31:15,152 : Image to text: 27.7, 60.5, 74.4, 4.0
2019-02-16 02:31:15,487 : Text to Image: 21.9, 54.04, 71.58, 5.0
2019-02-16 02:31:15,926 : Image to text: 30.2, 61.0, 76.3, 3.0
2019-02-16 02:31:16,259 : Text to Image: 21.92, 54.14, 71.52, 5.0
2019-02-16 02:31:16,259 : Dev mean Text to Image: 21.76, 54.256, 71.368, 5.0
2019-02-16 02:31:16,259 : Dev mean Image to text: 27.56, 59.540000000000006, 74.64, 3.8000000000000003
2019-02-16 02:31:16,260 : start epoch
2019-02-16 02:32:00,267 : samples : 64000
2019-02-16 02:32:10,847 : Image to text: 10.92, 29.54, 42.84, 15.0
2019-02-16 02:32:18,521 : Text to Image: 8.656, 25.16, 36.884, 19.0
2019-02-16 02:33:02,643 : samples : 128000
2019-02-16 02:33:13,307 : Image to text: 10.12, 29.2, 40.72, 16.0
2019-02-16 02:33:21,023 : Text to Image: 8.332, 24.792, 36.548, 20.0
2019-02-16 02:34:05,110 : samples : 192000
2019-02-16 02:34:15,748 : Image to text: 10.88, 28.42, 41.32, 16.0
2019-02-16 02:34:23,295 : Text to Image: 8.18, 24.312, 36.2, 20.0
2019-02-16 02:35:07,585 : samples : 256000
2019-02-16 02:35:18,206 : Image to text: 10.48, 29.5, 42.28, 15.0
2019-02-16 02:35:25,862 : Text to Image: 8.284, 24.192, 35.952, 20.0
2019-02-16 02:36:09,981 : samples : 320000
2019-02-16 02:36:20,675 : Image to text: 10.06, 28.36, 41.5, 16.0
2019-02-16 02:36:28,310 : Text to Image: 8.204, 24.6, 36.188, 20.0
2019-02-16 02:37:12,788 : samples : 384000
2019-02-16 02:37:23,231 : Image to text: 10.2, 28.48, 41.7, 15.0
2019-02-16 02:37:30,830 : Text to Image: 8.688, 25.424, 37.224, 19.0
2019-02-16 02:38:15,457 : samples : 448000
2019-02-16 02:38:26,113 : Image to text: 11.0, 29.46, 42.46, 15.0
2019-02-16 02:38:33,752 : Text to Image: 8.512, 24.856, 36.772, 19.0
2019-02-16 02:39:18,720 : samples : 512000
2019-02-16 02:39:29,386 : Image to text: 10.42, 29.02, 41.08, 16.0
2019-02-16 02:39:37,084 : Text to Image: 8.152, 24.356, 36.004, 20.0
2019-02-16 02:40:14,272 : Epoch 6 finished
2019-02-16 02:40:14,689 : Image to text: 25.0, 57.8, 75.0, 4.0
2019-02-16 02:40:15,015 : Text to Image: 22.52, 55.38, 73.14, 5.0
2019-02-16 02:40:15,439 : Image to text: 24.7, 58.6, 72.3, 4.0
2019-02-16 02:40:15,767 : Text to Image: 21.66, 53.58, 71.06, 5.0
2019-02-16 02:40:16,195 : Image to text: 27.7, 59.0, 74.9, 4.0
2019-02-16 02:40:16,525 : Text to Image: 23.3, 55.64, 72.34, 4.0
2019-02-16 02:40:16,951 : Image to text: 28.1, 61.3, 76.0, 4.0
2019-02-16 02:40:17,281 : Text to Image: 21.74, 54.6, 72.04, 5.0
2019-02-16 02:40:17,707 : Image to text: 27.6, 60.9, 76.7, 4.0
2019-02-16 02:40:18,037 : Text to Image: 22.24, 54.4, 70.4, 5.0
2019-02-16 02:40:18,037 : Dev mean Text to Image: 22.291999999999998, 54.72, 71.796, 4.8
2019-02-16 02:40:18,037 : Dev mean Image to text: 26.62, 59.519999999999996, 74.98, 4.0
2019-02-16 02:40:18,037 : start epoch
2019-02-16 02:41:11,538 : samples : 64000
2019-02-16 02:41:22,556 : Image to text: 10.52, 29.16, 41.52, 15.0
2019-02-16 02:41:30,257 : Text to Image: 8.884, 25.6, 37.564, 18.0
2019-02-16 02:42:14,751 : samples : 128000
2019-02-16 02:42:25,422 : Image to text: 9.98, 29.44, 41.28, 16.0
2019-02-16 02:42:33,175 : Text to Image: 8.056, 24.684, 36.304, 20.0
2019-02-16 02:43:16,399 : samples : 192000
2019-02-16 02:43:27,039 : Image to text: 10.9, 29.36, 42.72, 15.0
2019-02-16 02:43:34,743 : Text to Image: 8.772, 25.732, 37.632, 19.0
2019-02-16 02:44:19,234 : samples : 256000
2019-02-16 02:44:29,806 : Image to text: 10.74, 28.9, 41.98, 15.0
2019-02-16 02:44:37,491 : Text to Image: 8.82, 25.624, 37.7, 19.0
2019-02-16 02:45:21,951 : samples : 320000
2019-02-16 02:45:32,585 : Image to text: 10.64, 29.06, 42.7, 15.0
2019-02-16 02:45:40,295 : Text to Image: 8.796, 25.672, 37.448, 19.0
2019-02-16 02:46:24,280 : samples : 384000
2019-02-16 02:46:34,928 : Image to text: 10.46, 29.56, 42.74, 15.0
2019-02-16 02:46:42,620 : Text to Image: 9.336, 26.848, 38.648, 18.0
2019-02-16 02:47:26,744 : samples : 448000
2019-02-16 02:47:37,377 : Image to text: 10.84, 29.12, 41.76, 15.0
2019-02-16 02:47:45,025 : Text to Image: 9.38, 26.072, 38.544, 18.0
2019-02-16 02:48:29,494 : samples : 512000
2019-02-16 02:48:40,106 : Image to text: 11.24, 30.72, 43.04, 15.0
2019-02-16 02:48:47,848 : Text to Image: 9.368, 26.712, 38.584, 18.0
2019-02-16 02:49:24,774 : Epoch 7 finished
2019-02-16 02:49:25,207 : Image to text: 26.3, 58.4, 73.8, 4.0
2019-02-16 02:49:25,540 : Text to Image: 21.34, 54.78, 72.92, 5.0
2019-02-16 02:49:25,997 : Image to text: 26.7, 58.5, 72.7, 4.0
2019-02-16 02:49:26,346 : Text to Image: 22.64, 53.74, 70.86, 5.0
2019-02-16 02:49:26,794 : Image to text: 25.5, 60.5, 74.8, 4.0
2019-02-16 02:49:27,135 : Text to Image: 21.4, 54.2, 71.34, 5.0
2019-02-16 02:49:27,588 : Image to text: 28.0, 59.5, 76.0, 4.0
2019-02-16 02:49:27,934 : Text to Image: 20.82, 55.02, 72.02, 5.0
2019-02-16 02:49:28,386 : Image to text: 29.8, 59.7, 74.3, 4.0
2019-02-16 02:49:28,733 : Text to Image: 22.3, 53.86, 71.32, 5.0
2019-02-16 02:49:28,733 : Dev mean Text to Image: 21.7, 54.32, 71.692, 5.0
2019-02-16 02:49:28,733 : Dev mean Image to text: 27.259999999999998, 59.31999999999999, 74.32, 4.0
2019-02-16 02:49:28,733 : start epoch
2019-02-16 02:50:13,879 : samples : 64000
2019-02-16 02:50:24,485 : Image to text: 10.78, 29.86, 42.66, 15.0
2019-02-16 02:50:32,163 : Text to Image: 8.288, 24.78, 36.768, 19.0
2019-02-16 02:51:16,885 : samples : 128000
2019-02-16 02:51:27,521 : Image to text: 11.3, 30.96, 42.7, 15.0
2019-02-16 02:51:35,227 : Text to Image: 9.108, 26.108, 38.144, 18.0
2019-02-16 02:52:18,872 : samples : 192000
2019-02-16 02:52:29,554 : Image to text: 11.46, 30.54, 43.5, 14.0
2019-02-16 02:52:37,240 : Text to Image: 9.044, 26.052, 38.076, 18.0
2019-02-16 02:53:21,712 : samples : 256000
2019-02-16 02:53:32,407 : Image to text: 11.18, 30.36, 42.6, 15.0
2019-02-16 02:53:40,151 : Text to Image: 9.156, 26.52, 38.096, 18.0
2019-02-16 02:54:24,780 : samples : 320000
2019-02-16 02:54:35,498 : Image to text: 12.0, 31.28, 44.18, 14.0
2019-02-16 02:54:43,165 : Text to Image: 9.544, 27.06, 39.324, 17.0
2019-02-16 02:55:27,076 : samples : 384000
2019-02-16 02:55:37,748 : Image to text: 11.02, 30.78, 43.98, 14.0
2019-02-16 02:55:45,427 : Text to Image: 8.888, 25.236, 37.224, 19.0
2019-02-16 02:56:30,894 : samples : 448000
2019-02-16 02:56:41,496 : Image to text: 11.5, 31.4, 44.08, 14.0
2019-02-16 02:56:49,212 : Text to Image: 9.088, 26.376, 38.06, 18.0
2019-02-16 02:57:32,509 : samples : 512000
2019-02-16 02:57:44,278 : Image to text: 10.9, 29.82, 42.68, 15.0
2019-02-16 02:57:53,352 : Text to Image: 9.188, 26.308, 38.288, 18.0
2019-02-16 02:58:36,760 : Epoch 8 finished
2019-02-16 02:58:37,201 : Image to text: 27.8, 61.3, 76.8, 4.0
2019-02-16 02:58:37,540 : Text to Image: 22.08, 56.02, 72.84, 5.0
2019-02-16 02:58:37,987 : Image to text: 27.2, 61.1, 76.9, 4.0
2019-02-16 02:58:38,332 : Text to Image: 21.98, 54.62, 72.04, 5.0
2019-02-16 02:58:38,787 : Image to text: 28.2, 61.1, 75.9, 4.0
2019-02-16 02:58:39,136 : Text to Image: 22.18, 55.46, 72.46, 4.0
2019-02-16 02:58:39,601 : Image to text: 29.9, 61.2, 75.8, 4.0
2019-02-16 02:58:39,944 : Text to Image: 21.74, 55.14, 72.46, 5.0
2019-02-16 02:58:40,410 : Image to text: 31.4, 61.3, 75.8, 3.0
2019-02-16 02:58:40,778 : Text to Image: 21.74, 54.52, 70.98, 5.0
2019-02-16 02:58:40,778 : Dev mean Text to Image: 21.944, 55.152, 72.156, 4.8
2019-02-16 02:58:40,778 : Dev mean Image to text: 28.9, 61.2, 76.24, 3.8000000000000003
2019-02-16 02:58:40,778 : start epoch
2019-02-16 02:59:25,474 : samples : 64000
2019-02-16 02:59:36,128 : Image to text: 10.72, 30.28, 43.62, 14.0
2019-02-16 02:59:43,822 : Text to Image: 9.032, 26.4, 38.196, 18.0
2019-02-16 03:00:28,707 : samples : 128000
2019-02-16 03:00:39,337 : Image to text: 11.44, 30.4, 43.52, 14.0
2019-02-16 03:00:47,033 : Text to Image: 8.968, 25.336, 37.696, 18.0
2019-02-16 03:01:30,658 : samples : 192000
2019-02-16 03:01:41,283 : Image to text: 10.6, 30.12, 43.66, 14.0
2019-02-16 03:01:48,994 : Text to Image: 8.888, 25.956, 38.0, 18.0
2019-02-16 03:02:33,047 : samples : 256000
2019-02-16 03:02:43,776 : Image to text: 11.34, 30.7, 43.44, 14.0
2019-02-16 03:02:51,504 : Text to Image: 9.004, 26.444, 38.612, 18.0
2019-02-16 03:03:35,102 : samples : 320000
2019-02-16 03:03:45,760 : Image to text: 11.8, 30.78, 43.38, 14.0
2019-02-16 03:03:53,467 : Text to Image: 9.44, 26.624, 38.52, 18.0
2019-02-16 03:04:37,290 : samples : 384000
2019-02-16 03:04:47,828 : Image to text: 11.3, 30.28, 42.9, 15.0
2019-02-16 03:04:55,532 : Text to Image: 8.704, 25.148, 37.032, 19.0
2019-02-16 03:05:39,713 : samples : 448000
2019-02-16 03:05:50,301 : Image to text: 10.96, 29.74, 43.0, 14.0
2019-02-16 03:05:58,001 : Text to Image: 8.792, 25.812, 37.736, 18.0
2019-02-16 03:06:42,032 : samples : 512000
2019-02-16 03:06:52,649 : Image to text: 11.74, 30.52, 43.4, 14.0
2019-02-16 03:07:00,238 : Text to Image: 9.156, 26.78, 39.132, 17.0
2019-02-16 03:07:37,157 : Epoch 9 finished
2019-02-16 03:07:37,600 : Image to text: 30.3, 61.1, 77.1, 4.0
2019-02-16 03:07:37,939 : Text to Image: 24.46, 57.26, 74.78, 4.0
2019-02-16 03:07:38,388 : Image to text: 28.3, 60.8, 76.5, 4.0
2019-02-16 03:07:38,749 : Text to Image: 23.82, 56.72, 73.3, 4.0
2019-02-16 03:07:39,213 : Image to text: 29.8, 63.7, 78.3, 3.0
2019-02-16 03:07:39,567 : Text to Image: 25.08, 58.48, 74.82, 4.0
2019-02-16 03:07:40,032 : Image to text: 30.4, 61.8, 76.7, 3.0
2019-02-16 03:07:40,388 : Text to Image: 23.96, 57.92, 74.9, 4.0
2019-02-16 03:07:40,856 : Image to text: 32.2, 63.8, 76.3, 3.0
2019-02-16 03:07:41,235 : Text to Image: 25.94, 57.9, 73.44, 4.0
2019-02-16 03:07:41,235 : Dev mean Text to Image: 24.652, 57.65599999999999, 74.248, 4.0
2019-02-16 03:07:41,235 : Dev mean Image to text: 30.2, 62.24, 76.98, 3.4000000000000004
2019-02-16 03:07:41,236 : start epoch
2019-02-16 03:08:25,370 : samples : 64000
2019-02-16 03:08:35,922 : Image to text: 10.86, 29.96, 42.54, 15.0
2019-02-16 03:08:43,660 : Text to Image: 8.604, 25.36, 37.584, 19.0
2019-02-16 03:09:28,483 : samples : 128000
2019-02-16 03:09:39,080 : Image to text: 11.2, 31.38, 43.68, 14.0
2019-02-16 03:09:46,782 : Text to Image: 9.872, 27.132, 39.264, 17.0
2019-02-16 03:10:29,870 : samples : 192000
2019-02-16 03:10:40,194 : Image to text: 11.74, 31.1, 43.86, 14.0
2019-02-16 03:10:47,768 : Text to Image: 9.316, 26.524, 38.404, 18.0
2019-02-16 03:11:30,385 : samples : 256000
2019-02-16 03:11:40,629 : Image to text: 11.84, 31.9, 45.14, 13.0
2019-02-16 03:11:48,129 : Text to Image: 9.212, 26.632, 38.424, 18.0
2019-02-16 03:12:31,269 : samples : 320000
2019-02-16 03:12:41,508 : Image to text: 11.48, 31.4, 44.1, 14.0
2019-02-16 03:12:48,965 : Text to Image: 9.604, 27.344, 39.052, 17.0
2019-02-16 03:13:31,256 : samples : 384000
2019-02-16 03:13:41,473 : Image to text: 12.12, 31.54, 44.44, 13.0
2019-02-16 03:13:48,992 : Text to Image: 9.448, 27.144, 39.428, 17.0
2019-02-16 03:14:31,446 : samples : 448000
2019-02-16 03:14:43,118 : Image to text: 11.2, 30.48, 43.48, 14.0
2019-02-16 03:14:51,637 : Text to Image: 9.164, 26.548, 38.66, 17.0
2019-02-16 03:15:40,727 : samples : 512000
2019-02-16 03:15:51,181 : Image to text: 11.66, 30.78, 43.06, 14.0
2019-02-16 03:15:58,795 : Text to Image: 9.444, 26.768, 39.032, 17.0
2019-02-16 03:16:35,192 : Epoch 10 finished
2019-02-16 03:16:35,626 : Image to text: 29.5, 61.2, 76.4, 4.0
2019-02-16 03:16:35,938 : Text to Image: 23.48, 57.84, 74.18, 4.0
2019-02-16 03:16:36,378 : Image to text: 27.6, 60.0, 75.2, 4.0
2019-02-16 03:16:36,692 : Text to Image: 23.0, 56.08, 72.88, 4.0
2019-02-16 03:16:37,122 : Image to text: 31.1, 64.0, 78.0, 3.0
2019-02-16 03:16:37,448 : Text to Image: 23.82, 57.52, 73.58, 4.0
2019-02-16 03:16:37,920 : Image to text: 28.7, 61.9, 76.3, 3.0
2019-02-16 03:16:38,264 : Text to Image: 23.68, 56.48, 74.12, 4.0
2019-02-16 03:16:38,714 : Image to text: 30.5, 64.1, 76.5, 3.0
2019-02-16 03:16:39,051 : Text to Image: 24.24, 56.78, 73.2, 4.0
2019-02-16 03:16:39,052 : Dev mean Text to Image: 23.644, 56.94, 73.592, 4.0
2019-02-16 03:16:39,052 : Dev mean Image to text: 29.480000000000004, 62.24, 76.48, 3.4000000000000004
2019-02-16 03:16:39,052 : start epoch
2019-02-16 03:17:22,281 : samples : 64000
2019-02-16 03:17:32,511 : Image to text: 11.02, 30.58, 43.9, 14.0
2019-02-16 03:17:39,971 : Text to Image: 9.124, 26.66, 38.676, 18.0
2019-02-16 03:18:22,401 : samples : 128000
2019-02-16 03:18:32,624 : Image to text: 12.2, 31.28, 44.18, 14.0
2019-02-16 03:18:40,104 : Text to Image: 9.312, 27.056, 39.256, 17.0
2019-02-16 03:19:22,976 : samples : 192000
2019-02-16 03:19:33,219 : Image to text: 11.88, 31.08, 43.92, 14.0
2019-02-16 03:19:40,612 : Text to Image: 9.22, 26.656, 38.484, 18.0
2019-02-16 03:20:24,997 : samples : 256000
2019-02-16 03:20:35,262 : Image to text: 11.72, 31.12, 43.58, 14.0
2019-02-16 03:20:42,682 : Text to Image: 9.316, 26.76, 38.744, 17.0
2019-02-16 03:21:24,668 : samples : 320000
2019-02-16 03:21:34,877 : Image to text: 11.28, 30.98, 44.0, 14.0
2019-02-16 03:21:42,329 : Text to Image: 9.112, 26.456, 38.52, 18.0
2019-02-16 03:22:24,723 : samples : 384000
2019-02-16 03:22:34,939 : Image to text: 12.14, 31.46, 44.16, 14.0
2019-02-16 03:22:42,393 : Text to Image: 9.384, 27.036, 38.944, 17.0
2019-02-16 03:23:25,086 : samples : 448000
2019-02-16 03:23:35,554 : Image to text: 11.82, 31.72, 43.54, 14.0
2019-02-16 03:23:43,096 : Text to Image: 9.396, 26.9, 39.032, 17.0
2019-02-16 03:24:25,416 : samples : 512000
2019-02-16 03:24:35,890 : Image to text: 11.18, 30.88, 42.92, 14.0
2019-02-16 03:24:43,460 : Text to Image: 9.172, 26.164, 38.168, 18.0
2019-02-16 03:25:20,057 : Epoch 11 finished
2019-02-16 03:25:20,485 : Image to text: 27.3, 59.9, 76.2, 4.0
2019-02-16 03:25:20,797 : Text to Image: 23.0, 57.2, 73.78, 4.0
2019-02-16 03:25:21,220 : Image to text: 29.0, 60.6, 76.3, 4.0
2019-02-16 03:25:21,539 : Text to Image: 22.64, 55.58, 72.58, 4.0
2019-02-16 03:25:21,962 : Image to text: 29.8, 62.6, 77.7, 4.0
2019-02-16 03:25:22,289 : Text to Image: 23.78, 57.16, 73.62, 4.0
2019-02-16 03:25:22,731 : Image to text: 29.3, 62.3, 76.5, 3.0
2019-02-16 03:25:23,059 : Text to Image: 22.56, 56.18, 73.54, 4.0
2019-02-16 03:25:23,500 : Image to text: 30.6, 61.8, 75.9, 3.0
2019-02-16 03:25:23,815 : Text to Image: 23.04, 56.94, 73.16, 4.0
2019-02-16 03:25:23,815 : Dev mean Text to Image: 23.004, 56.612, 73.336, 4.0
2019-02-16 03:25:23,815 : Dev mean Image to text: 29.2, 61.440000000000005, 76.52000000000001, 3.6000000000000005
2019-02-16 03:25:23,815 : start epoch
2019-02-16 03:26:06,658 : samples : 64000
2019-02-16 03:26:16,976 : Image to text: 12.12, 31.64, 44.9, 13.0
2019-02-16 03:26:24,437 : Text to Image: 9.496, 27.012, 39.376, 17.0
2019-02-16 03:27:06,952 : samples : 128000
2019-02-16 03:27:17,159 : Image to text: 11.84, 31.76, 44.92, 13.0
2019-02-16 03:27:24,619 : Text to Image: 9.512, 26.656, 39.124, 17.0
2019-02-16 03:28:07,220 : samples : 192000
2019-02-16 03:28:17,536 : Image to text: 12.14, 32.0, 45.0, 13.0
2019-02-16 03:28:24,952 : Text to Image: 9.656, 27.468, 39.932, 17.0
2019-02-16 03:29:07,553 : samples : 256000
2019-02-16 03:29:17,837 : Image to text: 11.92, 31.58, 44.6, 13.0
2019-02-16 03:29:25,319 : Text to Image: 9.312, 27.16, 39.548, 17.0
2019-02-16 03:30:07,785 : samples : 320000
2019-02-16 03:30:18,069 : Image to text: 11.4, 30.78, 43.4, 14.0
2019-02-16 03:30:25,485 : Text to Image: 9.036, 26.336, 38.716, 18.0
2019-02-16 03:31:08,623 : samples : 384000
2019-02-16 03:31:18,685 : Image to text: 11.72, 31.64, 45.1, 13.0
2019-02-16 03:31:25,913 : Text to Image: 9.816, 27.712, 39.848, 17.0
2019-02-16 03:32:18,726 : samples : 448000
2019-02-16 03:32:29,255 : Image to text: 11.9, 32.58, 45.34, 13.0
2019-02-16 03:32:36,810 : Text to Image: 9.916, 27.544, 39.716, 17.0
2019-02-16 03:33:19,655 : samples : 512000
2019-02-16 03:33:30,170 : Image to text: 11.76, 31.28, 43.66, 14.0
2019-02-16 03:33:37,727 : Text to Image: 9.572, 27.352, 39.484, 17.0
2019-02-16 03:34:14,102 : Epoch 12 finished
2019-02-16 03:34:14,505 : Image to text: 30.7, 61.2, 77.3, 4.0
2019-02-16 03:34:14,814 : Text to Image: 23.74, 58.0, 74.9, 4.0
2019-02-16 03:34:15,236 : Image to text: 28.5, 61.8, 74.9, 3.0
2019-02-16 03:34:15,563 : Text to Image: 23.14, 56.46, 73.3, 4.0
2019-02-16 03:34:15,991 : Image to text: 28.9, 64.4, 77.1, 3.0
2019-02-16 03:34:16,320 : Text to Image: 23.46, 58.38, 75.1, 4.0
2019-02-16 03:34:16,757 : Image to text: 29.6, 63.4, 78.7, 3.0
2019-02-16 03:34:17,092 : Text to Image: 23.74, 57.46, 75.18, 4.0
2019-02-16 03:34:17,537 : Image to text: 30.7, 62.7, 77.5, 3.0
2019-02-16 03:34:17,872 : Text to Image: 24.2, 57.5, 74.1, 4.0
2019-02-16 03:34:17,873 : Dev mean Text to Image: 23.656, 57.56, 74.51599999999999, 4.0
2019-02-16 03:34:17,873 : Dev mean Image to text: 29.68, 62.7, 77.1, 3.2
2019-02-16 03:34:21,766 : 
Test scores | Image to text:             29.22, 61.74000000000001, 77.92, 3.4
2019-02-16 03:34:21,766 : Test scores | Text to image:             23.768, 57.412000000000006, 74.11600000000001, 4.0

2019-02-16 03:34:21,860 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 03:34:22,072 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 03:34:22,794 : loading BERT model bert-base-uncased
2019-02-16 03:34:22,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:34:22,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:34:22,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6edcuuhp
2019-02-16 03:34:25,378 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:34:26,849 : Computing embeddings for train/dev/test
2019-02-16 03:36:01,422 : Computed embeddings
2019-02-16 03:36:01,423 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:36:56,561 : [('reg:1e-05', 85.61), ('reg:0.0001', 75.81), ('reg:0.001', 72.43), ('reg:0.01', 59.9)]
2019-02-16 03:36:56,561 : Validation : best param found is reg = 1e-05 with score             85.61
2019-02-16 03:36:56,561 : Evaluating...
2019-02-16 03:37:09,138 : 
Dev acc : 85.6 Test acc : 85.7 for LENGTH classification

2019-02-16 03:37:09,138 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 03:37:09,501 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 03:37:09,549 : loading BERT model bert-base-uncased
2019-02-16 03:37:09,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:37:09,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:37:09,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4xllwe0l
2019-02-16 03:37:12,164 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:37:13,640 : Computing embeddings for train/dev/test
2019-02-16 03:38:42,461 : Computed embeddings
2019-02-16 03:38:42,461 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:39:44,214 : [('reg:1e-05', 11.15), ('reg:0.0001', 1.99), ('reg:0.001', 0.39), ('reg:0.01', 0.16)]
2019-02-16 03:39:44,214 : Validation : best param found is reg = 1e-05 with score             11.15
2019-02-16 03:39:44,214 : Evaluating...
2019-02-16 03:40:02,938 : 
Dev acc : 11.2 Test acc : 11.4 for WORDCONTENT classification

2019-02-16 03:40:02,940 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 03:40:03,332 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 03:40:03,405 : loading BERT model bert-base-uncased
2019-02-16 03:40:03,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:40:03,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:40:03,515 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp427v792a
2019-02-16 03:40:06,065 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:40:07,565 : Computing embeddings for train/dev/test
2019-02-16 03:41:30,948 : Computed embeddings
2019-02-16 03:41:30,948 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:42:05,265 : [('reg:1e-05', 29.02), ('reg:0.0001', 26.53), ('reg:0.001', 25.27), ('reg:0.01', 22.46)]
2019-02-16 03:42:05,266 : Validation : best param found is reg = 1e-05 with score             29.02
2019-02-16 03:42:05,266 : Evaluating...
2019-02-16 03:42:13,637 : 
Dev acc : 29.0 Test acc : 29.4 for DEPTH classification

2019-02-16 03:42:13,638 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 03:42:14,248 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 03:42:14,319 : loading BERT model bert-base-uncased
2019-02-16 03:42:14,319 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:42:14,354 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:42:14,354 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjq3a44p8
2019-02-16 03:42:16,842 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:42:18,306 : Computing embeddings for train/dev/test
2019-02-16 03:43:36,869 : Computed embeddings
2019-02-16 03:43:36,869 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:44:32,385 : [('reg:1e-05', 49.74), ('reg:0.0001', 48.02), ('reg:0.001', 33.77), ('reg:0.01', 18.0)]
2019-02-16 03:44:32,385 : Validation : best param found is reg = 1e-05 with score             49.74
2019-02-16 03:44:32,385 : Evaluating...
2019-02-16 03:44:46,972 : 
Dev acc : 49.7 Test acc : 49.7 for TOPCONSTITUENTS classification

2019-02-16 03:44:46,973 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 03:44:47,560 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 03:44:47,632 : loading BERT model bert-base-uncased
2019-02-16 03:44:47,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:44:47,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:44:47,666 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwd0zd2dz
2019-02-16 03:44:50,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:44:51,639 : Computing embeddings for train/dev/test
2019-02-16 03:46:15,197 : Computed embeddings
2019-02-16 03:46:15,197 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:46:52,691 : [('reg:1e-05', 63.24), ('reg:0.0001', 62.76), ('reg:0.001', 61.6), ('reg:0.01', 51.2)]
2019-02-16 03:46:52,692 : Validation : best param found is reg = 1e-05 with score             63.24
2019-02-16 03:46:52,692 : Evaluating...
2019-02-16 03:47:02,742 : 
Dev acc : 63.2 Test acc : 63.9 for BIGRAMSHIFT classification

2019-02-16 03:47:02,743 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 03:47:03,210 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 03:47:03,285 : loading BERT model bert-base-uncased
2019-02-16 03:47:03,286 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:47:03,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:47:03,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2hhrhp8v
2019-02-16 03:47:05,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:47:07,324 : Computing embeddings for train/dev/test
2019-02-16 03:48:30,769 : Computed embeddings
2019-02-16 03:48:30,769 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:49:02,167 : [('reg:1e-05', 88.02), ('reg:0.0001', 88.01), ('reg:0.001', 87.65), ('reg:0.01', 84.85)]
2019-02-16 03:49:02,167 : Validation : best param found is reg = 1e-05 with score             88.02
2019-02-16 03:49:02,167 : Evaluating...
2019-02-16 03:49:08,299 : 
Dev acc : 88.0 Test acc : 86.5 for TENSE classification

2019-02-16 03:49:08,301 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 03:49:08,914 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 03:49:09,012 : loading BERT model bert-base-uncased
2019-02-16 03:49:09,012 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:49:09,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:49:09,193 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvu_ovnl2
2019-02-16 03:49:11,827 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:49:13,276 : Computing embeddings for train/dev/test
2019-02-16 03:50:39,900 : Computed embeddings
2019-02-16 03:50:39,900 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:51:17,881 : [('reg:1e-05', 76.65), ('reg:0.0001', 76.74), ('reg:0.001', 76.35), ('reg:0.01', 65.59)]
2019-02-16 03:51:17,882 : Validation : best param found is reg = 0.0001 with score             76.74
2019-02-16 03:51:17,882 : Evaluating...
2019-02-16 03:51:26,724 : 
Dev acc : 76.7 Test acc : 76.0 for SUBJNUMBER classification

2019-02-16 03:51:26,725 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 03:51:27,380 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 03:51:27,454 : loading BERT model bert-base-uncased
2019-02-16 03:51:27,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:51:27,487 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:51:27,487 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_orf2zxz
2019-02-16 03:51:29,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:51:31,457 : Computing embeddings for train/dev/test
2019-02-16 03:52:57,729 : Computed embeddings
2019-02-16 03:52:57,729 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:53:51,560 : [('reg:1e-05', 76.79), ('reg:0.0001', 76.55), ('reg:0.001', 75.48), ('reg:0.01', 70.65)]
2019-02-16 03:53:51,560 : Validation : best param found is reg = 1e-05 with score             76.79
2019-02-16 03:53:51,560 : Evaluating...
2019-02-16 03:54:04,311 : 
Dev acc : 76.8 Test acc : 78.2 for OBJNUMBER classification

2019-02-16 03:54:04,312 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 03:54:04,699 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 03:54:04,773 : loading BERT model bert-base-uncased
2019-02-16 03:54:04,773 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:54:04,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:54:04,902 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3obs8003
2019-02-16 03:54:07,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:54:08,891 : Computing embeddings for train/dev/test
2019-02-16 03:55:45,973 : Computed embeddings
2019-02-16 03:55:45,973 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:56:27,657 : [('reg:1e-05', 56.21), ('reg:0.0001', 56.23), ('reg:0.001', 55.82), ('reg:0.01', 53.38)]
2019-02-16 03:56:27,657 : Validation : best param found is reg = 0.0001 with score             56.23
2019-02-16 03:56:27,657 : Evaluating...
2019-02-16 03:56:38,074 : 
Dev acc : 56.2 Test acc : 55.1 for ODDMANOUT classification

2019-02-16 03:56:38,075 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 03:56:38,736 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 03:56:38,830 : loading BERT model bert-base-uncased
2019-02-16 03:56:38,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:56:38,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:56:38,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaiwvl3be
2019-02-16 03:56:41,391 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:56:42,860 : Computing embeddings for train/dev/test
2019-02-16 03:58:21,175 : Computed embeddings
2019-02-16 03:58:21,175 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 03:58:59,115 : [('reg:1e-05', 50.51), ('reg:0.0001', 50.41), ('reg:0.001', 50.11), ('reg:0.01', 50.0)]
2019-02-16 03:58:59,115 : Validation : best param found is reg = 1e-05 with score             50.51
2019-02-16 03:58:59,115 : Evaluating...
2019-02-16 03:59:09,233 : 
Dev acc : 50.5 Test acc : 50.4 for COORDINATIONINVERSION classification

2019-02-16 03:59:09,236 : total results: {'STS12': {'MSRpar': {'pearson': (0.33292502347645775, 7.188344123227065e-21), 'spearman': SpearmanrResult(correlation=0.37886442982537843, pvalue=5.1969848823478316e-27), 'nsamples': 750}, 'MSRvid': {'pearson': (0.46629641787386994, 9.36072814445691e-42), 'spearman': SpearmanrResult(correlation=0.5941606759247019, pvalue=9.148099646369967e-73), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4426609980696041, 1.9019753251301734e-23), 'spearman': SpearmanrResult(correlation=0.5680340120737659, pvalue=1.426775373375149e-40), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.17112287023827047, 2.436352374364928e-06), 'spearman': SpearmanrResult(correlation=0.20727000211778826, pvalue=1.009149738385023e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6665441706814754, 1.351565849589579e-52), 'spearman': SpearmanrResult(correlation=0.5084412056152384, pvalue=1.284039112912398e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4159098960679355, 'wmean': 0.38509998581316135}, 'spearman': {'mean': 0.4513540651113746, 'wmean': 0.43398229841803093}}}, 'STS13': {'FNWN': {'pearson': (0.3230289067687263, 5.798967318734957e-06), 'spearman': SpearmanrResult(correlation=0.3143457619828799, pvalue=1.0575759439484824e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.2994400380572252, 5.299365499203079e-17), 'spearman': SpearmanrResult(correlation=0.42655429912751003, pvalue=1.6302399291856864e-34), 'nsamples': 750}, 'OnWN': {'pearson': (0.28029559643629076, 1.3790423321402124e-11), 'spearman': SpearmanrResult(correlation=0.32200501172566365, pvalue=5.3232705230227676e-15), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.30092151375408077, 'wmean': 0.29525221434864485}, 'spearman': {'mean': 0.3543016909453512, 'wmean': 0.3733145899589961}}}, 'STS14': {'deft-forum': {'pearson': (0.27592225785898816, 2.6349144891394305e-09), 'spearman': SpearmanrResult(correlation=0.343791101719268, pvalue=6.260185504106288e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.5810863824189059, 1.731263948305825e-28), 'spearman': SpearmanrResult(correlation=0.6767708143016062, pvalue=1.5703130579198019e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.2887631631434192, 7.181668267809188e-16), 'spearman': SpearmanrResult(correlation=0.3551768369710113, pvalue=1.0180402429064104e-23), 'nsamples': 750}, 'images': {'pearson': (0.598977393291072, 3.225416986538311e-74), 'spearman': SpearmanrResult(correlation=0.5939580428705691, pvalue=1.0517410227212211e-72), 'nsamples': 750}, 'OnWN': {'pearson': (0.293438161332521, 2.3250466987259993e-16), 'spearman': SpearmanrResult(correlation=0.32685303923385467, pvalue=3.9319924902383705e-20), 'nsamples': 750}, 'tweet-news': {'pearson': (0.2815084186947909, 3.966466450783058e-15), 'spearman': SpearmanrResult(correlation=0.3297626523716799, pvalue=1.750048522060265e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.38661596278994953, 'wmean': 0.3721350088289517}, 'spearman': {'mean': 0.43771874791133153, 'wmean': 0.4165467116398637}}}, 'STS15': {'answers-forums': {'pearson': (0.39993017222831573, 7.787513770813092e-16), 'spearman': SpearmanrResult(correlation=0.43722069904865174, pvalue=6.116577442181571e-19), 'nsamples': 375}, 'answers-students': {'pearson': (0.3854143443074299, 5.705363112896917e-28), 'spearman': SpearmanrResult(correlation=0.4208734725834483, pvalue=1.482648903059762e-33), 'nsamples': 750}, 'belief': {'pearson': (0.3637259341034387, 3.5808061869116454e-13), 'spearman': SpearmanrResult(correlation=0.4716294188116715, pvalue=3.6366561252445195e-22), 'nsamples': 375}, 'headlines': {'pearson': (0.3944149920363467, 2.523012999055229e-29), 'spearman': SpearmanrResult(correlation=0.4948542536949515, pvalue=1.4009270910846216e-47), 'nsamples': 750}, 'images': {'pearson': (0.13785510354352376, 0.00015240604801924936), 'spearman': SpearmanrResult(correlation=0.5090947908414748, pvalue=1.0702907711810444e-50), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.33626810924381095, 'wmean': 0.3248781232632944}, 'spearman': {'mean': 0.46673452699603957, 'wmean': 0.46981189401250906}}}, 'STS16': {'answer-answer': {'pearson': (0.3227857799723158, 1.4368946136760226e-07), 'spearman': SpearmanrResult(correlation=0.4577944141353731, pvalue=1.4598402819800768e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.3946195903560399, 1.0489855906865772e-10), 'spearman': SpearmanrResult(correlation=0.5295754201085991, pvalue=2.1080428555495031e-19), 'nsamples': 249}, 'plagiarism': {'pearson': (0.4792632681535299, 1.304559449358429e-14), 'spearman': SpearmanrResult(correlation=0.6466916698126703, pvalue=1.237523526088338e-28), 'nsamples': 230}, 'postediting': {'pearson': (0.4859340249532845, 7.288720182146055e-16), 'spearman': SpearmanrResult(correlation=0.7717754747804415, pvalue=1.738534894969914e-49), 'nsamples': 244}, 'question-question': {'pearson': (0.5364525615813561, 5.6804109315632e-17), 'spearman': SpearmanrResult(correlation=0.5480250331575404, pvalue=8.836394484882632e-18), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.44381104500330526, 'wmean': 0.43943078013999903}, 'spearman': {'mean': 0.5907724023989249, 'wmean': 0.5899945974963692}}}, 'MR': {'devacc': 66.52, 'acc': 65.33, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.91, 'acc': 64.03, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.99, 'acc': 81.84, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.2, 'acc': 91.02, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.64, 'acc': 76.83, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.6, 'acc': 37.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 55.36, 'acc': 67.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.55, 'acc': 67.54, 'f1': 80.35, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.6, 'acc': 74.08, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7835423335857185, 'pearson': 0.7722223091331447, 'spearman': 0.6980729292493973, 'mse': 0.4108660467326688, 'yhat': array([3.05129275, 4.29797008, 2.18803492, ..., 2.9057259 , 4.33315012,        4.27719228]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6845589020484055, 'pearson': 0.6424955470744644, 'spearman': 0.6384598771531589, 'mse': 1.6651889503720172, 'yhat': array([2.41764016, 1.33976334, 2.81588475, ..., 3.87299817, 3.7774294 ,        3.46973953]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.45, 'acc': 58.35, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 325.97599999999994, 'acc': [(29.22, 61.74000000000001, 77.92, 3.4), (23.768, 57.412000000000006, 74.11600000000001, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 85.61, 'acc': 85.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 11.15, 'acc': 11.44, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.02, 'acc': 29.42, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.74, 'acc': 49.73, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 63.24, 'acc': 63.9, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.02, 'acc': 86.49, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.74, 'acc': 76.04, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.79, 'acc': 78.23, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.23, 'acc': 55.07, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.51, 'acc': 50.41, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 03:59:09,236 : STS12 p=0.3851, STS12 s=0.4340, STS13 p=0.2953, STS13 s=0.3733, STS14 p=0.3721, STS14 s=0.4165, STS15 p=0.3249, STS15 s=0.4698, STS 16 p=0.4394, STS16 s=0.5900, STS B p=0.6425, STS B s=0.6385, STS B m=1.6652, SICK-R p=0.7722, SICK-R s=0.6981, SICK-P m=0.4109
2019-02-16 03:59:09,236 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 03:59:09,236 : 0.3851,0.4340,0.2953,0.3733,0.3721,0.4165,0.3249,0.4698,0.4394,0.5900,0.6425,0.6385,1.6652,0.7722,0.6981,0.4109
2019-02-16 03:59:09,236 : MR=65.33, CR=64.03, SUBJ=91.02, MPQA=81.84, SST-B=76.83, SST-F=37.38, TREC=67.80, SICK-E=74.08, SNLI=58.35, MRPC=67.54, MRPC f=80.35
2019-02-16 03:59:09,236 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 03:59:09,236 : 65.33,64.03,91.02,81.84,76.83,37.38,67.80,74.08,58.35,67.54,80.35
2019-02-16 03:59:09,236 : COCO r1i2t=29.22, COCO r5i2t=61.74, COCO r10i2t=77.92, COCO medr_i2t=3.40, COCO r1t2i=23.77, COCO r5t2i=57.41, COCO r10t2i=74.12, COCO medr_t2i=4.00
2019-02-16 03:59:09,236 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 03:59:09,236 : 29.22,61.74,77.92,3.40,23.77,57.41,74.12,4.00
2019-02-16 03:59:09,236 : SentLen=85.69, WC=11.44, TreeDepth=29.42, TopConst=49.73, BShift=63.90, Tense=86.49, SubjNum=76.04, ObjNum=78.23, SOMO=55.07, CoordInv=50.41, average=58.64
2019-02-16 03:59:09,236 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 03:59:09,236 : 85.69,11.44,29.42,49.73,63.90,86.49,76.04,78.23,55.07,50.41,58.64
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,236 : layer 5
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,236 : ********************************************************************************
2019-02-16 03:59:09,345 : ***** Transfer task : STS12 *****


2019-02-16 03:59:09,360 : loading BERT model bert-base-uncased
2019-02-16 03:59:09,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:09,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:09,382 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpulbh9lld
2019-02-16 03:59:11,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:15,294 : MSRpar : pearson = 0.2971, spearman = 0.3514
2019-02-16 03:59:16,138 : MSRvid : pearson = 0.3494, spearman = 0.4170
2019-02-16 03:59:16,850 : SMTeuroparl : pearson = 0.4627, spearman = 0.5757
2019-02-16 03:59:18,131 : surprise.OnWN : pearson = 0.2639, spearman = 0.2607
2019-02-16 03:59:18,839 : surprise.SMTnews : pearson = 0.6512, spearman = 0.5008
2019-02-16 03:59:18,839 : ALL (weighted average) : Pearson = 0.3716,             Spearman = 0.3977
2019-02-16 03:59:18,839 : ALL (average) : Pearson = 0.4049,             Spearman = 0.4211

2019-02-16 03:59:18,839 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 03:59:18,848 : loading BERT model bert-base-uncased
2019-02-16 03:59:18,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:18,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:18,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpblvqxk5x
2019-02-16 03:59:21,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:23,416 : FNWN : pearson = 0.2605, spearman = 0.2693
2019-02-16 03:59:24,282 : headlines : pearson = 0.4268, spearman = 0.4616
2019-02-16 03:59:24,938 : OnWN : pearson = 0.2192, spearman = 0.2325
2019-02-16 03:59:24,938 : ALL (weighted average) : Pearson = 0.3282,             Spearman = 0.3517
2019-02-16 03:59:24,938 : ALL (average) : Pearson = 0.3022,             Spearman = 0.3211

2019-02-16 03:59:24,939 : ***** Transfer task : STS14 *****


2019-02-16 03:59:24,954 : loading BERT model bert-base-uncased
2019-02-16 03:59:24,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:24,973 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:24,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvi7k6g45
2019-02-16 03:59:27,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:29,586 : deft-forum : pearson = 0.1723, spearman = 0.2090
2019-02-16 03:59:30,305 : deft-news : pearson = 0.6384, spearman = 0.6529
2019-02-16 03:59:31,307 : headlines : pearson = 0.3766, spearman = 0.3803
2019-02-16 03:59:32,271 : images : pearson = 0.4072, spearman = 0.3987
2019-02-16 03:59:33,219 : OnWN : pearson = 0.3094, spearman = 0.3178
2019-02-16 03:59:34,497 : tweet-news : pearson = 0.3350, spearman = 0.3366
2019-02-16 03:59:34,497 : ALL (weighted average) : Pearson = 0.3574,             Spearman = 0.3640
2019-02-16 03:59:34,497 : ALL (average) : Pearson = 0.3731,             Spearman = 0.3825

2019-02-16 03:59:34,497 : ***** Transfer task : STS15 *****


2019-02-16 03:59:34,565 : loading BERT model bert-base-uncased
2019-02-16 03:59:34,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:34,585 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:34,585 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5wylz2xw
2019-02-16 03:59:37,098 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:39,492 : answers-forums : pearson = 0.3616, spearman = 0.3888
2019-02-16 03:59:40,468 : answers-students : pearson = 0.4689, spearman = 0.4909
2019-02-16 03:59:41,321 : belief : pearson = 0.3829, spearman = 0.4623
2019-02-16 03:59:42,325 : headlines : pearson = 0.4983, spearman = 0.5240
2019-02-16 03:59:43,294 : images : pearson = 0.1685, spearman = 0.3588
2019-02-16 03:59:43,295 : ALL (weighted average) : Pearson = 0.3770,             Spearman = 0.4498
2019-02-16 03:59:43,295 : ALL (average) : Pearson = 0.3761,             Spearman = 0.4450

2019-02-16 03:59:43,295 : ***** Transfer task : STS16 *****


2019-02-16 03:59:43,342 : loading BERT model bert-base-uncased
2019-02-16 03:59:43,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:43,401 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:43,401 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi4ypjh3r
2019-02-16 03:59:45,924 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:47,806 : answer-answer : pearson = 0.3889, spearman = 0.4610
2019-02-16 03:59:48,117 : headlines : pearson = 0.4948, spearman = 0.5431
2019-02-16 03:59:48,493 : plagiarism : pearson = 0.6495, spearman = 0.7059
2019-02-16 03:59:49,123 : postediting : pearson = 0.5843, spearman = 0.7856
2019-02-16 03:59:49,421 : question-question : pearson = 0.4453, spearman = 0.4464
2019-02-16 03:59:49,421 : ALL (weighted average) : Pearson = 0.5118,             Spearman = 0.5899
2019-02-16 03:59:49,421 : ALL (average) : Pearson = 0.5126,             Spearman = 0.5884

2019-02-16 03:59:49,421 : ***** Transfer task : MR *****


2019-02-16 03:59:49,438 : loading BERT model bert-base-uncased
2019-02-16 03:59:49,438 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 03:59:49,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 03:59:49,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_3jxqjs2
2019-02-16 03:59:51,962 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 03:59:53,446 : Generating sentence embeddings
2019-02-16 04:00:06,905 : Generated sentence embeddings
2019-02-16 04:00:06,905 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 04:00:22,848 : Best param found at split 1: l2reg = 0.001                 with score 70.34
2019-02-16 04:00:39,823 : Best param found at split 2: l2reg = 0.0001                 with score 69.14
2019-02-16 04:01:00,651 : Best param found at split 3: l2reg = 0.0001                 with score 70.22
2019-02-16 04:01:18,854 : Best param found at split 4: l2reg = 1e-05                 with score 68.83
2019-02-16 04:01:36,431 : Best param found at split 5: l2reg = 0.001                 with score 69.11
2019-02-16 04:01:37,654 : Dev acc : 69.53 Test acc : 68.13

2019-02-16 04:01:37,655 : ***** Transfer task : CR *****


2019-02-16 04:01:37,663 : loading BERT model bert-base-uncased
2019-02-16 04:01:37,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:01:37,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:01:37,731 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbyrxdsm5
2019-02-16 04:01:40,240 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:01:41,742 : Generating sentence embeddings
2019-02-16 04:01:45,435 : Generated sentence embeddings
2019-02-16 04:01:45,435 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 04:01:50,950 : Best param found at split 1: l2reg = 0.001                 with score 72.04
2019-02-16 04:01:56,547 : Best param found at split 2: l2reg = 0.001                 with score 74.0
2019-02-16 04:02:02,998 : Best param found at split 3: l2reg = 1e-05                 with score 72.71
2019-02-16 04:02:07,612 : Best param found at split 4: l2reg = 0.001                 with score 74.64
2019-02-16 04:02:14,360 : Best param found at split 5: l2reg = 0.001                 with score 73.72
2019-02-16 04:02:14,549 : Dev acc : 73.42 Test acc : 69.38

2019-02-16 04:02:14,550 : ***** Transfer task : MPQA *****


2019-02-16 04:02:14,556 : loading BERT model bert-base-uncased
2019-02-16 04:02:14,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:02:14,577 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:02:14,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5hpg297i
2019-02-16 04:02:17,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:02:18,584 : Generating sentence embeddings
2019-02-16 04:02:22,374 : Generated sentence embeddings
2019-02-16 04:02:22,375 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 04:02:38,711 : Best param found at split 1: l2reg = 0.001                 with score 84.04
2019-02-16 04:02:54,281 : Best param found at split 2: l2reg = 1e-05                 with score 83.42
2019-02-16 04:03:11,393 : Best param found at split 3: l2reg = 1e-05                 with score 83.7
2019-02-16 04:03:30,051 : Best param found at split 4: l2reg = 0.001                 with score 83.62
2019-02-16 04:03:49,388 : Best param found at split 5: l2reg = 0.001                 with score 85.56
2019-02-16 04:03:50,118 : Dev acc : 84.07 Test acc : 84.5

2019-02-16 04:03:50,119 : ***** Transfer task : SUBJ *****


2019-02-16 04:03:50,135 : loading BERT model bert-base-uncased
2019-02-16 04:03:50,135 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:03:50,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:03:50,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp5mvgduj
2019-02-16 04:03:52,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:03:54,150 : Generating sentence embeddings
2019-02-16 04:04:08,547 : Generated sentence embeddings
2019-02-16 04:04:08,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 04:04:27,839 : Best param found at split 1: l2reg = 0.0001                 with score 92.99
2019-02-16 04:04:45,462 : Best param found at split 2: l2reg = 0.0001                 with score 93.14
2019-02-16 04:05:01,271 : Best param found at split 3: l2reg = 1e-05                 with score 92.36
2019-02-16 04:05:12,943 : Best param found at split 4: l2reg = 1e-05                 with score 93.19
2019-02-16 04:05:23,361 : Best param found at split 5: l2reg = 0.001                 with score 92.62
2019-02-16 04:05:23,993 : Dev acc : 92.86 Test acc : 92.68

2019-02-16 04:05:23,995 : ***** Transfer task : SST Binary classification *****


2019-02-16 04:05:24,178 : loading BERT model bert-base-uncased
2019-02-16 04:05:24,178 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:05:24,209 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:05:24,209 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmqjjmd0x
2019-02-16 04:05:27,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:05:29,199 : Computing embedding for train
2019-02-16 04:06:17,194 : Computed train embeddings
2019-02-16 04:06:17,194 : Computing embedding for dev
2019-02-16 04:06:18,240 : Computed dev embeddings
2019-02-16 04:06:18,240 : Computing embedding for test
2019-02-16 04:06:20,419 : Computed test embeddings
2019-02-16 04:06:20,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:06:45,931 : [('reg:1e-05', 78.33), ('reg:0.0001', 78.1), ('reg:0.001', 77.75), ('reg:0.01', 67.78)]
2019-02-16 04:06:45,931 : Validation : best param found is reg = 1e-05 with score             78.33
2019-02-16 04:06:45,931 : Evaluating...
2019-02-16 04:06:51,330 : 
Dev acc : 78.33 Test acc : 77.81 for             SST Binary classification

2019-02-16 04:06:51,330 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 04:06:51,389 : loading BERT model bert-base-uncased
2019-02-16 04:06:51,390 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:06:51,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:06:51,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6enavq0i
2019-02-16 04:06:53,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:06:55,362 : Computing embedding for train
2019-02-16 04:07:04,910 : Computed train embeddings
2019-02-16 04:07:04,910 : Computing embedding for dev
2019-02-16 04:07:06,103 : Computed dev embeddings
2019-02-16 04:07:06,104 : Computing embedding for test
2019-02-16 04:07:08,519 : Computed test embeddings
2019-02-16 04:07:08,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:07:12,548 : [('reg:1e-05', 34.33), ('reg:0.0001', 35.6), ('reg:0.001', 36.33), ('reg:0.01', 35.6)]
2019-02-16 04:07:12,549 : Validation : best param found is reg = 0.001 with score             36.33
2019-02-16 04:07:12,549 : Evaluating...
2019-02-16 04:07:13,101 : 
Dev acc : 36.33 Test acc : 38.73 for             SST Fine-Grained classification

2019-02-16 04:07:13,102 : ***** Transfer task : TREC *****


2019-02-16 04:07:13,114 : loading BERT model bert-base-uncased
2019-02-16 04:07:13,114 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:07:13,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:07:13,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg9w6xlst
2019-02-16 04:07:15,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:07:20,823 : Computed train embeddings
2019-02-16 04:07:21,116 : Computed test embeddings
2019-02-16 04:07:21,116 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 04:07:33,174 : [('reg:1e-05', 67.77), ('reg:0.0001', 66.97), ('reg:0.001', 65.33), ('reg:0.01', 61.07)]
2019-02-16 04:07:33,174 : Cross-validation : best param found is reg = 1e-05             with score 67.77
2019-02-16 04:07:33,175 : Evaluating...
2019-02-16 04:07:33,960 : 
Dev acc : 67.77 Test acc : 79.2             for TREC

2019-02-16 04:07:33,961 : ***** Transfer task : MRPC *****


2019-02-16 04:07:34,024 : loading BERT model bert-base-uncased
2019-02-16 04:07:34,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:07:34,047 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:07:34,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpab3ikqoc
2019-02-16 04:07:36,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:07:37,957 : Computing embedding for train
2019-02-16 04:07:47,648 : Computed train embeddings
2019-02-16 04:07:47,648 : Computing embedding for test
2019-02-16 04:07:51,866 : Computed test embeddings
2019-02-16 04:07:51,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 04:08:00,692 : [('reg:1e-05', 69.63), ('reg:0.0001', 70.07), ('reg:0.001', 68.06), ('reg:0.01', 69.55)]
2019-02-16 04:08:00,692 : Cross-validation : best param found is reg = 0.0001             with score 70.07
2019-02-16 04:08:00,692 : Evaluating...
2019-02-16 04:08:01,045 : Dev acc : 70.07 Test acc 67.42; Test F1 75.46 for MRPC.

2019-02-16 04:08:01,045 : ***** Transfer task : SICK-Entailment*****


2019-02-16 04:08:01,070 : loading BERT model bert-base-uncased
2019-02-16 04:08:01,070 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:08:01,131 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:08:01,131 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_3_4y9b7
2019-02-16 04:08:03,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:08:05,042 : Computing embedding for train
2019-02-16 04:08:10,212 : Computed train embeddings
2019-02-16 04:08:10,212 : Computing embedding for dev
2019-02-16 04:08:10,911 : Computed dev embeddings
2019-02-16 04:08:10,911 : Computing embedding for test
2019-02-16 04:08:16,814 : Computed test embeddings
2019-02-16 04:08:16,842 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:08:19,550 : [('reg:1e-05', 72.4), ('reg:0.0001', 73.2), ('reg:0.001', 67.8), ('reg:0.01', 62.2)]
2019-02-16 04:08:19,551 : Validation : best param found is reg = 0.0001 with score             73.2
2019-02-16 04:08:19,551 : Evaluating...
2019-02-16 04:08:20,002 : 
Dev acc : 73.2 Test acc : 70.98 for                        SICK entailment

2019-02-16 04:08:20,003 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 04:08:20,032 : loading BERT model bert-base-uncased
2019-02-16 04:08:20,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:08:20,055 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:08:20,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg32yhnor
2019-02-16 04:08:22,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:08:24,001 : Computing embedding for train
2019-02-16 04:08:29,102 : Computed train embeddings
2019-02-16 04:08:29,102 : Computing embedding for dev
2019-02-16 04:08:29,768 : Computed dev embeddings
2019-02-16 04:08:29,768 : Computing embedding for test
2019-02-16 04:08:35,193 : Computed test embeddings
2019-02-16 04:09:02,094 : Dev : Pearson 0.7498275509314094
2019-02-16 04:09:02,094 : Test : Pearson 0.7556118409882442 Spearman 0.6920325135739948 MSE 0.44186181803867103                        for SICK Relatedness

2019-02-16 04:09:02,095 : 

***** Transfer task : STSBenchmark*****


2019-02-16 04:09:02,182 : loading BERT model bert-base-uncased
2019-02-16 04:09:02,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:09:02,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:09:02,205 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcox32pc3
2019-02-16 04:09:04,729 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:09:06,207 : Computing embedding for train
2019-02-16 04:09:14,520 : Computed train embeddings
2019-02-16 04:09:14,520 : Computing embedding for dev
2019-02-16 04:09:17,025 : Computed dev embeddings
2019-02-16 04:09:17,026 : Computing embedding for test
2019-02-16 04:09:19,083 : Computed test embeddings
2019-02-16 04:09:49,811 : Dev : Pearson 0.6365482500673998
2019-02-16 04:09:49,811 : Test : Pearson 0.6203475545781522 Spearman 0.6151130448745691 MSE 1.6714750152740945                        for SICK Relatedness

2019-02-16 04:09:49,812 : ***** Transfer task : SNLI Entailment*****


2019-02-16 04:09:54,790 : loading BERT model bert-base-uncased
2019-02-16 04:09:54,791 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:09:54,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:09:54,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn5qqghbc
2019-02-16 04:09:57,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:09:59,117 : PROGRESS (encoding): 0.00%
2019-02-16 04:11:18,190 : PROGRESS (encoding): 14.56%
2019-02-16 04:12:45,403 : PROGRESS (encoding): 29.12%
2019-02-16 04:14:12,266 : PROGRESS (encoding): 43.69%
2019-02-16 04:15:46,765 : PROGRESS (encoding): 58.25%
2019-02-16 04:17:30,956 : PROGRESS (encoding): 72.81%
2019-02-16 04:19:14,952 : PROGRESS (encoding): 87.37%
2019-02-16 04:21:04,535 : PROGRESS (encoding): 0.00%
2019-02-16 04:21:18,246 : PROGRESS (encoding): 0.00%
2019-02-16 04:21:31,234 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 04:22:14,765 : [('reg:1e-09', 53.71)]
2019-02-16 04:22:14,765 : Validation : best param found is reg = 1e-09 with score             53.71
2019-02-16 04:22:14,765 : Evaluating...
2019-02-16 04:22:52,928 : Dev acc : 53.71 Test acc : 53.44 for SNLI

2019-02-16 04:22:52,929 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 04:23:04,050 : loading BERT model bert-base-uncased
2019-02-16 04:23:04,050 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 04:23:04,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 04:23:04,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpndz2xvnb
2019-02-16 04:23:06,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 04:23:08,103 : Computing embedding for train
2019-02-16 04:30:37,876 : Computed train embeddings
2019-02-16 04:30:37,876 : Computing embedding for dev
2019-02-16 04:30:57,594 : Computed dev embeddings
2019-02-16 04:30:57,595 : Computing embedding for test
2019-02-16 04:31:18,098 : Computed test embeddings
2019-02-16 04:31:18,114 : prepare data
2019-02-16 04:31:18,183 : start epoch
2019-02-16 04:32:01,125 : samples : 64000
2019-02-16 04:32:11,398 : Image to text: 4.72, 17.62, 27.68, 30.0
2019-02-16 04:32:18,809 : Text to Image: 4.736, 15.868, 24.908, 34.0
2019-02-16 04:33:02,220 : samples : 128000
2019-02-16 04:33:12,521 : Image to text: 6.28, 20.92, 31.7, 25.0
2019-02-16 04:33:19,966 : Text to Image: 5.308, 17.772, 27.196, 31.0
2019-02-16 04:34:03,237 : samples : 192000
2019-02-16 04:34:13,475 : Image to text: 6.48, 21.26, 32.16, 24.0
2019-02-16 04:34:20,898 : Text to Image: 5.336, 17.972, 28.112, 29.0
2019-02-16 04:35:04,063 : samples : 256000
2019-02-16 04:35:14,332 : Image to text: 6.62, 20.54, 31.1, 26.0
2019-02-16 04:35:21,730 : Text to Image: 5.496, 18.18, 28.16, 29.0
2019-02-16 04:36:04,946 : samples : 320000
2019-02-16 04:36:15,155 : Image to text: 6.82, 22.2, 34.06, 22.0
2019-02-16 04:36:22,632 : Text to Image: 6.228, 19.324, 29.576, 27.0
2019-02-16 04:37:05,408 : samples : 384000
2019-02-16 04:37:15,667 : Image to text: 7.2, 23.0, 34.62, 21.0
2019-02-16 04:37:23,060 : Text to Image: 6.272, 20.104, 30.612, 26.0
2019-02-16 04:38:06,456 : samples : 448000
2019-02-16 04:38:17,669 : Image to text: 7.16, 23.44, 35.24, 21.0
2019-02-16 04:38:27,693 : Text to Image: 6.632, 20.844, 31.624, 25.0
2019-02-16 04:39:13,930 : samples : 512000
2019-02-16 04:39:27,179 : Image to text: 7.7, 23.5, 34.84, 22.0
2019-02-16 04:39:37,649 : Text to Image: 6.276, 20.292, 30.556, 27.0
2019-02-16 04:40:17,423 : Epoch 1 finished
2019-02-16 04:40:17,908 : Image to text: 21.8, 52.2, 67.7, 5.0
2019-02-16 04:40:18,279 : Text to Image: 16.86, 48.04, 65.12, 6.0
2019-02-16 04:40:18,750 : Image to text: 21.9, 52.7, 69.7, 5.0
2019-02-16 04:40:19,123 : Text to Image: 17.56, 46.84, 64.94, 6.0
2019-02-16 04:40:19,575 : Image to text: 22.9, 54.1, 69.8, 5.0
2019-02-16 04:40:19,942 : Text to Image: 18.0, 46.62, 64.38, 6.0
2019-02-16 04:40:20,395 : Image to text: 22.4, 54.1, 69.8, 5.0
2019-02-16 04:40:20,779 : Text to Image: 17.38, 47.4, 64.82, 6.0
2019-02-16 04:40:21,231 : Image to text: 23.4, 55.5, 68.9, 5.0
2019-02-16 04:40:21,600 : Text to Image: 18.5, 47.72, 64.92, 6.0
2019-02-16 04:40:21,600 : Dev mean Text to Image: 17.66, 47.324, 64.836, 6.0
2019-02-16 04:40:21,601 : Dev mean Image to text: 22.48, 53.720000000000006, 69.18, 5.0
2019-02-16 04:40:21,601 : start epoch
2019-02-16 04:41:05,076 : samples : 64000
2019-02-16 04:41:17,633 : Image to text: 9.08, 25.94, 37.44, 20.0
2019-02-16 04:41:27,637 : Text to Image: 6.744, 21.384, 31.896, 24.0
2019-02-16 04:42:13,274 : samples : 128000
2019-02-16 04:42:24,665 : Image to text: 8.18, 25.62, 37.36, 19.0
2019-02-16 04:42:31,936 : Text to Image: 7.024, 22.06, 33.124, 23.0
2019-02-16 04:43:13,616 : samples : 192000
2019-02-16 04:43:23,862 : Image to text: 9.12, 25.8, 37.5, 19.0
2019-02-16 04:43:31,072 : Text to Image: 6.904, 21.984, 33.004, 23.0
2019-02-16 04:44:13,825 : samples : 256000
2019-02-16 04:44:26,403 : Image to text: 8.72, 25.56, 37.38, 19.0
2019-02-16 04:44:36,446 : Text to Image: 7.556, 23.356, 34.5, 22.0
2019-02-16 04:45:21,468 : samples : 320000
2019-02-16 04:45:31,696 : Image to text: 8.9, 26.48, 39.42, 17.0
2019-02-16 04:45:39,090 : Text to Image: 7.612, 23.16, 34.2, 22.0
2019-02-16 04:46:21,575 : samples : 384000
2019-02-16 04:46:34,067 : Image to text: 8.82, 26.04, 37.92, 19.0
2019-02-16 04:46:44,061 : Text to Image: 6.996, 22.448, 33.72, 22.0
2019-02-16 04:47:29,245 : samples : 448000
2019-02-16 04:47:41,503 : Image to text: 9.48, 26.32, 38.66, 17.0
2019-02-16 04:47:48,901 : Text to Image: 7.416, 23.34, 34.58, 22.0
2019-02-16 04:48:31,557 : samples : 512000
2019-02-16 04:48:41,675 : Image to text: 10.18, 27.92, 40.3, 16.0
2019-02-16 04:48:51,297 : Text to Image: 8.004, 23.872, 35.204, 21.0
2019-02-16 04:49:29,207 : Epoch 2 finished
2019-02-16 04:49:30,134 : Image to text: 24.4, 56.0, 72.0, 4.0
2019-02-16 04:49:30,914 : Text to Image: 20.4, 53.26, 70.24, 5.0
2019-02-16 04:49:31,893 : Image to text: 22.8, 57.5, 71.8, 4.0
2019-02-16 04:49:32,646 : Text to Image: 20.76, 52.24, 69.6, 5.0
2019-02-16 04:49:33,587 : Image to text: 25.5, 57.5, 72.2, 4.0
2019-02-16 04:49:34,401 : Text to Image: 21.38, 52.2, 69.32, 5.0
2019-02-16 04:49:35,383 : Image to text: 25.0, 58.3, 73.5, 4.0
2019-02-16 04:49:36,148 : Text to Image: 20.48, 52.26, 69.26, 5.0
2019-02-16 04:49:37,118 : Image to text: 25.9, 58.8, 73.6, 4.0
2019-02-16 04:49:37,898 : Text to Image: 21.04, 52.28, 68.96, 5.0
2019-02-16 04:49:37,898 : Dev mean Text to Image: 20.811999999999998, 52.44800000000001, 69.476, 5.0
2019-02-16 04:49:37,898 : Dev mean Image to text: 24.72, 57.62, 72.62, 4.0
2019-02-16 04:49:37,899 : start epoch
2019-02-16 04:50:22,156 : samples : 64000
2019-02-16 04:50:32,330 : Image to text: 9.04, 27.54, 39.44, 17.0
2019-02-16 04:50:39,756 : Text to Image: 7.768, 23.428, 34.644, 21.0
2019-02-16 04:51:22,011 : samples : 128000
2019-02-16 04:51:33,393 : Image to text: 7.72, 24.98, 37.4, 19.0
2019-02-16 04:51:43,424 : Text to Image: 7.028, 22.1, 32.98, 23.0
2019-02-16 04:52:28,161 : samples : 192000
2019-02-16 04:52:40,758 : Image to text: 8.98, 27.72, 39.58, 17.0
2019-02-16 04:52:50,285 : Text to Image: 7.216, 22.66, 34.236, 22.0
2019-02-16 04:53:32,836 : samples : 256000
2019-02-16 04:53:43,041 : Image to text: 9.14, 26.46, 38.82, 17.0
2019-02-16 04:53:50,402 : Text to Image: 6.992, 22.46, 33.704, 23.0
2019-02-16 04:54:33,961 : samples : 320000
2019-02-16 04:54:46,568 : Image to text: 9.48, 28.12, 41.18, 16.0
2019-02-16 04:54:56,590 : Text to Image: 8.064, 23.912, 35.156, 21.0
2019-02-16 04:55:40,516 : samples : 384000
2019-02-16 04:55:50,657 : Image to text: 9.58, 27.34, 39.86, 16.0
2019-02-16 04:55:57,873 : Text to Image: 7.636, 23.336, 34.672, 21.0
2019-02-16 04:56:49,622 : samples : 448000
2019-02-16 04:57:02,149 : Image to text: 10.1, 28.88, 40.86, 16.0
2019-02-16 04:57:12,176 : Text to Image: 8.32, 24.512, 36.244, 20.0
2019-02-16 04:57:57,248 : samples : 512000
2019-02-16 04:58:07,513 : Image to text: 10.06, 28.0, 39.46, 17.0
2019-02-16 04:58:14,851 : Text to Image: 8.216, 24.16, 35.42, 20.0
2019-02-16 04:58:50,748 : Epoch 3 finished
2019-02-16 04:58:51,215 : Image to text: 22.8, 54.7, 71.4, 5.0
2019-02-16 04:58:51,587 : Text to Image: 19.44, 50.3, 67.92, 5.0
2019-02-16 04:58:52,047 : Image to text: 25.5, 57.2, 71.7, 4.0
2019-02-16 04:58:52,421 : Text to Image: 19.4, 50.68, 67.9, 5.0
2019-02-16 04:58:52,883 : Image to text: 25.5, 57.7, 72.7, 4.0
2019-02-16 04:58:53,246 : Text to Image: 19.06, 50.08, 68.12, 5.0
2019-02-16 04:58:53,693 : Image to text: 25.8, 58.1, 74.0, 4.0
2019-02-16 04:58:54,055 : Text to Image: 19.64, 51.04, 68.88, 5.0
2019-02-16 04:58:54,527 : Image to text: 28.4, 61.6, 74.2, 3.0
2019-02-16 04:58:54,903 : Text to Image: 20.1, 50.96, 68.38, 5.0
2019-02-16 04:58:54,903 : Dev mean Text to Image: 19.528, 50.611999999999995, 68.24000000000001, 5.0
2019-02-16 04:58:54,903 : Dev mean Image to text: 25.6, 57.86000000000001, 72.80000000000001, 4.0
2019-02-16 04:58:54,903 : start epoch
2019-02-16 04:59:37,631 : samples : 64000
2019-02-16 04:59:50,196 : Image to text: 10.52, 29.72, 41.86, 15.0
2019-02-16 05:00:00,281 : Text to Image: 8.408, 24.952, 36.632, 19.0
2019-02-16 05:00:45,970 : samples : 128000
2019-02-16 05:00:58,545 : Image to text: 9.38, 28.42, 41.16, 16.0
2019-02-16 05:01:08,442 : Text to Image: 8.208, 24.296, 35.512, 20.0
2019-02-16 05:01:50,803 : samples : 192000
2019-02-16 05:02:01,010 : Image to text: 9.3, 27.12, 40.14, 16.0
2019-02-16 05:02:09,078 : Text to Image: 7.612, 23.368, 34.708, 22.0
2019-02-16 05:02:53,501 : samples : 256000
2019-02-16 05:03:06,109 : Image to text: 9.66, 28.94, 40.92, 16.0
2019-02-16 05:03:16,137 : Text to Image: 7.856, 23.764, 35.356, 21.0
2019-02-16 05:04:00,161 : samples : 320000
2019-02-16 05:04:10,376 : Image to text: 9.8, 29.28, 41.44, 15.0
2019-02-16 05:04:17,774 : Text to Image: 8.636, 25.332, 36.92, 19.0
2019-02-16 05:05:00,354 : samples : 384000
2019-02-16 05:05:11,855 : Image to text: 10.02, 29.16, 41.84, 15.0
2019-02-16 05:05:19,975 : Text to Image: 8.6, 24.828, 36.7, 19.0
2019-02-16 05:06:03,568 : samples : 448000
2019-02-16 05:06:13,768 : Image to text: 10.54, 29.08, 41.5, 16.0
2019-02-16 05:06:21,127 : Text to Image: 8.436, 25.068, 36.876, 19.0
2019-02-16 05:07:03,622 : samples : 512000
2019-02-16 05:07:13,852 : Image to text: 10.02, 28.68, 41.24, 16.0
2019-02-16 05:07:21,336 : Text to Image: 8.104, 24.244, 35.268, 21.0
2019-02-16 05:07:57,481 : Epoch 4 finished
2019-02-16 05:07:58,345 : Image to text: 25.2, 59.8, 75.6, 4.0
2019-02-16 05:07:59,053 : Text to Image: 21.38, 54.8, 71.84, 5.0
2019-02-16 05:07:59,824 : Image to text: 27.9, 58.1, 73.8, 4.0
2019-02-16 05:08:00,186 : Text to Image: 21.92, 53.22, 70.66, 5.0
2019-02-16 05:08:00,632 : Image to text: 28.7, 58.8, 74.0, 4.0
2019-02-16 05:08:00,994 : Text to Image: 22.42, 54.3, 71.02, 5.0
2019-02-16 05:08:01,440 : Image to text: 27.8, 60.6, 75.0, 3.0
2019-02-16 05:08:01,801 : Text to Image: 21.68, 54.42, 71.36, 5.0
2019-02-16 05:08:02,254 : Image to text: 29.8, 62.0, 75.8, 3.0
2019-02-16 05:08:02,619 : Text to Image: 22.34, 54.48, 70.54, 5.0
2019-02-16 05:08:02,619 : Dev mean Text to Image: 21.948, 54.244, 71.084, 5.0
2019-02-16 05:08:02,619 : Dev mean Image to text: 27.880000000000003, 59.85999999999999, 74.84, 3.6000000000000005
2019-02-16 05:08:02,620 : start epoch
2019-02-16 05:08:45,227 : samples : 64000
2019-02-16 05:08:55,501 : Image to text: 11.16, 29.0, 41.9, 16.0
2019-02-16 05:09:02,877 : Text to Image: 8.432, 25.524, 37.048, 19.0
2019-02-16 05:09:45,340 : samples : 128000
2019-02-16 05:09:57,558 : Image to text: 11.02, 30.08, 42.14, 15.0
2019-02-16 05:10:04,983 : Text to Image: 8.336, 24.912, 36.36, 20.0
2019-02-16 05:10:47,538 : samples : 192000
2019-02-16 05:10:57,735 : Image to text: 10.06, 28.94, 42.3, 15.0
2019-02-16 05:11:05,124 : Text to Image: 8.792, 25.336, 37.544, 19.0
2019-02-16 05:11:48,064 : samples : 256000
2019-02-16 05:12:00,742 : Image to text: 10.64, 29.9, 41.38, 16.0
2019-02-16 05:12:10,922 : Text to Image: 8.564, 25.548, 37.672, 19.0
2019-02-16 05:12:53,624 : samples : 320000
2019-02-16 05:13:05,477 : Image to text: 10.6, 29.94, 41.7, 15.0
2019-02-16 05:13:15,356 : Text to Image: 8.524, 25.516, 37.24, 19.0
2019-02-16 05:14:03,682 : samples : 384000
2019-02-16 05:14:15,633 : Image to text: 9.96, 28.84, 41.76, 15.0
2019-02-16 05:14:23,495 : Text to Image: 8.796, 25.284, 37.244, 19.0
2019-02-16 05:15:06,190 : samples : 448000
2019-02-16 05:15:18,828 : Image to text: 10.96, 30.2, 42.4, 15.0
2019-02-16 05:15:26,197 : Text to Image: 8.788, 25.732, 37.496, 19.0
2019-02-16 05:16:09,375 : samples : 512000
2019-02-16 05:16:21,860 : Image to text: 9.6, 29.1, 42.08, 15.0
2019-02-16 05:16:31,814 : Text to Image: 9.152, 26.376, 38.012, 18.0
2019-02-16 05:17:10,094 : Epoch 5 finished
2019-02-16 05:17:11,000 : Image to text: 25.1, 58.7, 74.9, 4.0
2019-02-16 05:17:11,732 : Text to Image: 20.9, 53.76, 70.88, 5.0
2019-02-16 05:17:12,646 : Image to text: 27.5, 58.4, 75.4, 4.0
2019-02-16 05:17:13,384 : Text to Image: 21.66, 53.1, 70.96, 5.0
2019-02-16 05:17:14,308 : Image to text: 27.4, 58.9, 74.9, 4.0
2019-02-16 05:17:15,031 : Text to Image: 22.38, 55.08, 71.82, 5.0
2019-02-16 05:17:15,912 : Image to text: 28.3, 60.6, 75.0, 4.0
2019-02-16 05:17:16,699 : Text to Image: 21.52, 54.18, 71.8, 5.0
2019-02-16 05:17:17,605 : Image to text: 29.7, 62.8, 77.9, 3.0
2019-02-16 05:17:18,335 : Text to Image: 22.0, 54.08, 70.92, 5.0
2019-02-16 05:17:18,335 : Dev mean Text to Image: 21.692, 54.03999999999999, 71.276, 5.0
2019-02-16 05:17:18,335 : Dev mean Image to text: 27.6, 59.88000000000001, 75.62, 3.8000000000000003
2019-02-16 05:17:18,336 : start epoch
2019-02-16 05:18:03,455 : samples : 64000
2019-02-16 05:18:16,019 : Image to text: 10.86, 29.84, 41.94, 15.0
2019-02-16 05:18:26,002 : Text to Image: 8.992, 25.844, 37.332, 19.0
2019-02-16 05:19:10,941 : samples : 128000
2019-02-16 05:19:23,480 : Image to text: 10.24, 29.58, 42.96, 14.0
2019-02-16 05:19:33,427 : Text to Image: 8.788, 25.868, 37.504, 19.0
2019-02-16 05:20:18,211 : samples : 192000
2019-02-16 05:20:30,839 : Image to text: 10.6, 29.3, 42.34, 15.0
2019-02-16 05:20:40,891 : Text to Image: 8.624, 25.124, 37.032, 19.0
2019-02-16 05:21:25,530 : samples : 256000
2019-02-16 05:21:38,157 : Image to text: 10.04, 29.1, 41.8, 15.0
2019-02-16 05:21:48,193 : Text to Image: 8.456, 25.396, 37.26, 19.0
2019-02-16 05:22:33,190 : samples : 320000
2019-02-16 05:22:45,757 : Image to text: 10.12, 29.28, 41.92, 15.0
2019-02-16 05:22:55,770 : Text to Image: 8.512, 25.52, 37.144, 19.0
2019-02-16 05:23:40,497 : samples : 384000
2019-02-16 05:23:53,082 : Image to text: 10.42, 29.88, 42.96, 15.0
2019-02-16 05:24:03,059 : Text to Image: 9.256, 26.568, 38.708, 18.0
2019-02-16 05:24:47,175 : samples : 448000
2019-02-16 05:24:59,731 : Image to text: 10.5, 30.86, 43.48, 14.0
2019-02-16 05:25:09,731 : Text to Image: 8.868, 26.176, 38.1, 18.0
2019-02-16 05:25:53,365 : samples : 512000
2019-02-16 05:26:05,940 : Image to text: 10.84, 30.38, 42.74, 15.0
2019-02-16 05:26:15,959 : Text to Image: 8.504, 25.288, 37.528, 18.0
2019-02-16 05:26:54,679 : Epoch 6 finished
2019-02-16 05:26:55,677 : Image to text: 25.2, 59.9, 76.5, 4.0
2019-02-16 05:26:56,496 : Text to Image: 22.72, 56.6, 73.52, 4.0
2019-02-16 05:26:57,555 : Image to text: 28.1, 60.8, 75.4, 4.0
2019-02-16 05:26:58,373 : Text to Image: 22.82, 54.96, 72.84, 5.0
2019-02-16 05:26:59,453 : Image to text: 29.1, 59.5, 75.1, 3.0
2019-02-16 05:27:00,434 : Text to Image: 24.1, 57.34, 73.4, 4.0
2019-02-16 05:27:01,237 : Image to text: 28.9, 60.9, 76.8, 4.0
2019-02-16 05:27:01,527 : Text to Image: 23.42, 56.2, 73.42, 4.0
2019-02-16 05:27:01,904 : Image to text: 31.0, 62.1, 77.8, 3.0
2019-02-16 05:27:02,186 : Text to Image: 23.18, 55.8, 72.14, 4.0
2019-02-16 05:27:02,186 : Dev mean Text to Image: 23.248, 56.18000000000001, 73.064, 4.2
2019-02-16 05:27:02,186 : Dev mean Image to text: 28.459999999999997, 60.64, 76.32000000000001, 3.6
2019-02-16 05:27:02,186 : start epoch
2019-02-16 05:27:46,379 : samples : 64000
2019-02-16 05:27:56,579 : Image to text: 10.26, 30.64, 43.52, 14.0
2019-02-16 05:28:03,865 : Text to Image: 9.316, 26.432, 38.396, 18.0
2019-02-16 05:28:46,263 : samples : 128000
2019-02-16 05:28:56,214 : Image to text: 10.86, 30.12, 43.38, 14.0
2019-02-16 05:29:04,938 : Text to Image: 8.824, 25.812, 38.04, 18.0
2019-02-16 05:29:50,173 : samples : 192000
2019-02-16 05:30:03,336 : Image to text: 10.8, 30.5, 43.38, 14.0
2019-02-16 05:30:13,967 : Text to Image: 9.088, 26.76, 38.588, 18.0
2019-02-16 05:31:02,283 : samples : 256000
2019-02-16 05:31:15,121 : Image to text: 11.34, 30.48, 43.14, 14.0
2019-02-16 05:31:25,549 : Text to Image: 9.472, 27.028, 39.068, 17.0
2019-02-16 05:32:11,419 : samples : 320000
2019-02-16 05:32:24,298 : Image to text: 11.54, 30.96, 43.76, 14.0
2019-02-16 05:32:34,767 : Text to Image: 9.716, 26.864, 38.804, 17.0
2019-02-16 05:33:20,702 : samples : 384000
2019-02-16 05:33:33,581 : Image to text: 11.92, 31.64, 44.44, 14.0
2019-02-16 05:33:44,058 : Text to Image: 9.796, 27.676, 39.644, 17.0
2019-02-16 05:34:30,111 : samples : 448000
2019-02-16 05:34:43,032 : Image to text: 10.8, 30.78, 43.86, 14.0
2019-02-16 05:34:53,431 : Text to Image: 9.456, 27.204, 39.844, 17.0
2019-02-16 05:35:38,735 : samples : 512000
2019-02-16 05:35:51,649 : Image to text: 11.5, 31.88, 45.02, 13.0
2019-02-16 05:36:02,117 : Text to Image: 9.808, 27.584, 39.712, 17.0
2019-02-16 05:36:41,259 : Epoch 7 finished
2019-02-16 05:36:42,381 : Image to text: 24.5, 59.3, 74.0, 4.0
2019-02-16 05:36:43,216 : Text to Image: 21.12, 56.22, 72.8, 4.0
2019-02-16 05:36:44,260 : Image to text: 26.0, 58.7, 73.7, 4.0
2019-02-16 05:36:45,132 : Text to Image: 22.52, 55.4, 72.0, 5.0
2019-02-16 05:36:46,123 : Image to text: 26.9, 59.1, 74.6, 4.0
2019-02-16 05:36:46,740 : Text to Image: 23.32, 56.16, 72.14, 4.0
2019-02-16 05:36:47,857 : Image to text: 27.9, 61.4, 77.3, 3.0
2019-02-16 05:36:48,753 : Text to Image: 22.48, 56.36, 73.76, 4.0
2019-02-16 05:36:49,802 : Image to text: 30.1, 59.9, 76.5, 4.0
2019-02-16 05:36:50,679 : Text to Image: 22.94, 55.8, 72.28, 4.0
2019-02-16 05:36:50,679 : Dev mean Text to Image: 22.476, 55.988, 72.596, 4.2
2019-02-16 05:36:50,679 : Dev mean Image to text: 27.080000000000002, 59.68000000000001, 75.22, 3.8000000000000007
2019-02-16 05:36:50,679 : start epoch
2019-02-16 05:37:34,852 : samples : 64000
2019-02-16 05:37:45,299 : Image to text: 11.4, 31.02, 43.2, 14.0
2019-02-16 05:37:52,964 : Text to Image: 8.544, 25.34, 37.044, 19.0
2019-02-16 05:38:35,810 : samples : 128000
2019-02-16 05:38:46,266 : Image to text: 11.58, 31.82, 44.5, 14.0
2019-02-16 05:38:53,870 : Text to Image: 9.1, 26.868, 39.016, 18.0
2019-02-16 05:39:36,869 : samples : 192000
2019-02-16 05:39:47,358 : Image to text: 11.4, 30.46, 43.86, 14.0
2019-02-16 05:39:54,933 : Text to Image: 8.964, 26.72, 38.884, 18.0
2019-02-16 05:40:37,024 : samples : 256000
2019-02-16 05:40:47,525 : Image to text: 10.4, 30.04, 43.98, 14.0
2019-02-16 05:40:55,111 : Text to Image: 9.164, 26.792, 38.992, 18.0
2019-02-16 05:41:38,073 : samples : 320000
2019-02-16 05:41:48,620 : Image to text: 12.2, 31.22, 44.9, 13.0
2019-02-16 05:41:56,207 : Text to Image: 9.912, 27.908, 40.1, 17.0
2019-02-16 05:42:38,873 : samples : 384000
2019-02-16 05:42:49,350 : Image to text: 11.04, 30.62, 43.7, 14.0
2019-02-16 05:42:56,946 : Text to Image: 9.176, 26.504, 38.392, 18.0
2019-02-16 05:43:39,851 : samples : 448000
2019-02-16 05:43:50,280 : Image to text: 10.92, 31.18, 44.08, 14.0
2019-02-16 05:43:57,845 : Text to Image: 9.512, 26.592, 38.8, 18.0
2019-02-16 05:44:40,656 : samples : 512000
2019-02-16 05:44:51,156 : Image to text: 10.82, 30.82, 44.22, 14.0
2019-02-16 05:44:58,749 : Text to Image: 9.46, 26.908, 38.616, 18.0
2019-02-16 05:45:34,908 : Epoch 8 finished
2019-02-16 05:45:35,363 : Image to text: 26.8, 61.4, 76.7, 4.0
2019-02-16 05:45:35,700 : Text to Image: 22.24, 57.4, 73.82, 4.0
2019-02-16 05:45:36,129 : Image to text: 27.8, 62.4, 78.1, 3.0
2019-02-16 05:45:36,463 : Text to Image: 23.2, 56.32, 73.28, 4.0
2019-02-16 05:45:36,910 : Image to text: 29.3, 62.1, 77.5, 3.0
2019-02-16 05:45:37,254 : Text to Image: 23.78, 57.04, 73.12, 4.0
2019-02-16 05:45:37,695 : Image to text: 28.7, 62.6, 77.1, 3.0
2019-02-16 05:45:38,032 : Text to Image: 22.86, 56.2, 73.22, 4.0
2019-02-16 05:45:38,487 : Image to text: 31.3, 64.6, 76.7, 3.0
2019-02-16 05:45:38,822 : Text to Image: 23.4, 56.2, 72.66, 4.0
2019-02-16 05:45:38,822 : Dev mean Text to Image: 23.096, 56.632000000000005, 73.22, 4.0
2019-02-16 05:45:38,822 : Dev mean Image to text: 28.78, 62.620000000000005, 77.22, 3.2
2019-02-16 05:45:38,822 : start epoch
2019-02-16 05:46:21,652 : samples : 64000
2019-02-16 05:46:32,074 : Image to text: 11.52, 31.46, 44.6, 14.0
2019-02-16 05:46:40,327 : Text to Image: 9.38, 26.58, 38.596, 17.0
2019-02-16 05:47:33,274 : samples : 128000
2019-02-16 05:47:43,729 : Image to text: 11.12, 31.54, 44.06, 14.0
2019-02-16 05:47:51,407 : Text to Image: 8.832, 25.88, 38.32, 18.0
2019-02-16 05:48:34,292 : samples : 192000
2019-02-16 05:48:44,762 : Image to text: 11.96, 31.78, 44.5, 14.0
2019-02-16 05:48:52,518 : Text to Image: 9.404, 26.736, 38.904, 18.0
2019-02-16 05:49:34,991 : samples : 256000
2019-02-16 05:49:45,476 : Image to text: 11.5, 31.58, 43.92, 14.0
2019-02-16 05:49:53,067 : Text to Image: 9.156, 26.78, 38.916, 17.0
2019-02-16 05:50:35,445 : samples : 320000
2019-02-16 05:50:45,911 : Image to text: 11.72, 31.66, 44.6, 13.0
2019-02-16 05:50:53,551 : Text to Image: 9.836, 27.476, 39.804, 17.0
2019-02-16 05:51:35,867 : samples : 384000
2019-02-16 05:51:46,318 : Image to text: 10.78, 30.06, 42.84, 15.0
2019-02-16 05:51:53,927 : Text to Image: 8.888, 25.48, 37.468, 18.0
2019-02-16 05:52:36,469 : samples : 448000
2019-02-16 05:52:46,923 : Image to text: 12.0, 31.96, 45.16, 13.0
2019-02-16 05:52:54,575 : Text to Image: 9.48, 27.068, 39.48, 17.0
2019-02-16 05:53:37,720 : samples : 512000
2019-02-16 05:53:48,242 : Image to text: 11.36, 32.1, 44.4, 14.0
2019-02-16 05:53:55,862 : Text to Image: 9.244, 27.184, 39.332, 17.0
2019-02-16 05:54:32,044 : Epoch 9 finished
2019-02-16 05:54:32,481 : Image to text: 29.3, 62.6, 77.7, 3.0
2019-02-16 05:54:32,813 : Text to Image: 24.88, 58.7, 75.66, 4.0
2019-02-16 05:54:33,242 : Image to text: 29.3, 62.2, 77.1, 3.0
2019-02-16 05:54:33,574 : Text to Image: 24.12, 56.86, 73.4, 4.0
2019-02-16 05:54:34,017 : Image to text: 30.4, 62.8, 77.8, 3.0
2019-02-16 05:54:34,355 : Text to Image: 25.34, 58.6, 74.94, 4.0
2019-02-16 05:54:34,812 : Image to text: 28.3, 64.5, 77.8, 3.0
2019-02-16 05:54:35,153 : Text to Image: 24.96, 58.58, 75.2, 4.0
2019-02-16 05:54:35,608 : Image to text: 30.1, 65.4, 78.4, 3.0
2019-02-16 05:54:35,953 : Text to Image: 24.98, 57.86, 73.9, 4.0
2019-02-16 05:54:35,953 : Dev mean Text to Image: 24.856, 58.120000000000005, 74.62, 4.0
2019-02-16 05:54:35,954 : Dev mean Image to text: 29.48, 63.5, 77.76, 3.0
2019-02-16 05:54:35,954 : start epoch
2019-02-16 05:55:18,825 : samples : 64000
2019-02-16 05:55:29,275 : Image to text: 11.28, 30.52, 43.28, 14.0
2019-02-16 05:55:36,945 : Text to Image: 8.58, 25.364, 37.388, 19.0
2019-02-16 05:56:19,478 : samples : 128000
2019-02-16 05:56:29,924 : Image to text: 11.94, 31.78, 45.12, 13.0
2019-02-16 05:56:37,581 : Text to Image: 9.788, 27.952, 39.996, 16.0
2019-02-16 05:57:19,933 : samples : 192000
2019-02-16 05:57:30,450 : Image to text: 12.18, 31.96, 44.8, 13.0
2019-02-16 05:57:38,089 : Text to Image: 9.612, 27.08, 39.128, 17.0
2019-02-16 05:58:20,599 : samples : 256000
2019-02-16 05:58:31,090 : Image to text: 11.52, 31.6, 45.12, 13.0
2019-02-16 05:58:38,757 : Text to Image: 9.5, 27.308, 39.524, 17.0
2019-02-16 05:59:21,023 : samples : 320000
2019-02-16 05:59:31,529 : Image to text: 11.28, 31.52, 44.14, 14.0
2019-02-16 05:59:39,276 : Text to Image: 9.372, 27.352, 39.304, 17.0
2019-02-16 06:00:21,422 : samples : 384000
2019-02-16 06:00:31,938 : Image to text: 11.62, 31.8, 45.02, 13.0
2019-02-16 06:00:39,632 : Text to Image: 9.712, 27.808, 40.0, 16.0
2019-02-16 06:01:22,701 : samples : 448000
2019-02-16 06:01:33,198 : Image to text: 11.06, 32.06, 45.3, 13.0
2019-02-16 06:01:40,999 : Text to Image: 9.876, 27.728, 40.256, 16.0
2019-02-16 06:02:23,447 : samples : 512000
2019-02-16 06:02:33,906 : Image to text: 12.2, 31.54, 44.6, 14.0
2019-02-16 06:02:41,508 : Text to Image: 9.852, 28.02, 40.228, 16.0
2019-02-16 06:03:18,536 : Epoch 10 finished
2019-02-16 06:03:18,975 : Image to text: 28.9, 62.1, 76.6, 3.0
2019-02-16 06:03:19,307 : Text to Image: 24.06, 59.4, 75.2, 4.0
2019-02-16 06:03:19,737 : Image to text: 29.7, 61.2, 77.6, 3.0
2019-02-16 06:03:20,068 : Text to Image: 24.56, 57.46, 74.04, 4.0
2019-02-16 06:03:20,510 : Image to text: 30.8, 62.7, 78.2, 3.0
2019-02-16 06:03:20,841 : Text to Image: 25.54, 58.4, 74.78, 4.0
2019-02-16 06:03:21,270 : Image to text: 28.3, 63.9, 77.9, 3.0
2019-02-16 06:03:21,608 : Text to Image: 24.66, 57.96, 74.34, 4.0
2019-02-16 06:03:22,052 : Image to text: 33.2, 65.2, 78.4, 3.0
2019-02-16 06:03:22,396 : Text to Image: 25.16, 58.26, 73.94, 4.0
2019-02-16 06:03:22,396 : Dev mean Text to Image: 24.796000000000003, 58.296, 74.46000000000001, 4.0
2019-02-16 06:03:22,396 : Dev mean Image to text: 30.18, 63.02, 77.74000000000001, 3.0
2019-02-16 06:03:22,397 : start epoch
2019-02-16 06:04:12,153 : samples : 64000
2019-02-16 06:04:24,963 : Image to text: 12.48, 32.24, 44.92, 13.0
2019-02-16 06:04:32,567 : Text to Image: 9.548, 27.764, 39.98, 17.0
2019-02-16 06:05:15,329 : samples : 128000
2019-02-16 06:05:25,846 : Image to text: 11.94, 31.6, 44.46, 14.0
2019-02-16 06:05:33,421 : Text to Image: 9.312, 26.848, 39.508, 17.0
2019-02-16 06:06:16,253 : samples : 192000
2019-02-16 06:06:26,737 : Image to text: 11.9, 31.76, 44.58, 13.0
2019-02-16 06:06:34,324 : Text to Image: 9.176, 27.18, 39.316, 17.0
2019-02-16 06:07:17,240 : samples : 256000
2019-02-16 06:07:27,724 : Image to text: 11.96, 31.8, 44.88, 13.0
2019-02-16 06:07:35,329 : Text to Image: 9.7, 27.54, 39.844, 17.0
2019-02-16 06:08:17,932 : samples : 320000
2019-02-16 06:08:28,506 : Image to text: 11.98, 31.4, 44.86, 14.0
2019-02-16 06:08:36,124 : Text to Image: 9.276, 26.988, 39.424, 17.0
2019-02-16 06:09:18,778 : samples : 384000
2019-02-16 06:09:29,258 : Image to text: 11.84, 32.46, 45.4, 13.0
2019-02-16 06:09:37,039 : Text to Image: 9.976, 28.428, 40.48, 16.0
2019-02-16 06:10:19,324 : samples : 448000
2019-02-16 06:10:29,793 : Image to text: 12.14, 32.62, 45.18, 13.0
2019-02-16 06:10:37,419 : Text to Image: 9.656, 27.66, 40.036, 17.0
2019-02-16 06:11:20,141 : samples : 512000
2019-02-16 06:11:30,592 : Image to text: 11.54, 31.46, 44.62, 13.0
2019-02-16 06:11:38,214 : Text to Image: 9.676, 27.372, 39.48, 17.0
2019-02-16 06:12:15,219 : Epoch 11 finished
2019-02-16 06:12:15,672 : Image to text: 26.9, 60.5, 76.7, 4.0
2019-02-16 06:12:16,019 : Text to Image: 23.42, 57.32, 73.46, 4.0
2019-02-16 06:12:16,458 : Image to text: 28.4, 61.1, 75.6, 4.0
2019-02-16 06:12:16,801 : Text to Image: 23.92, 56.2, 72.98, 4.0
2019-02-16 06:12:17,242 : Image to text: 29.3, 60.5, 76.0, 3.0
2019-02-16 06:12:17,586 : Text to Image: 23.58, 56.76, 74.0, 4.0
2019-02-16 06:12:18,036 : Image to text: 29.3, 62.6, 77.7, 3.0
2019-02-16 06:12:18,378 : Text to Image: 23.36, 56.26, 73.76, 4.0
2019-02-16 06:12:18,814 : Image to text: 31.0, 63.3, 77.3, 3.0
2019-02-16 06:12:19,147 : Text to Image: 23.14, 56.9, 73.06, 4.0
2019-02-16 06:12:19,147 : Dev mean Text to Image: 23.483999999999998, 56.68799999999999, 73.452, 4.0
2019-02-16 06:12:19,147 : Dev mean Image to text: 28.979999999999997, 61.599999999999994, 76.66, 3.4000000000000004
2019-02-16 06:12:19,147 : start epoch
2019-02-16 06:13:01,550 : samples : 64000
2019-02-16 06:13:12,021 : Image to text: 11.68, 32.18, 45.52, 13.0
2019-02-16 06:13:19,578 : Text to Image: 10.032, 28.02, 40.376, 16.0
2019-02-16 06:14:02,315 : samples : 128000
2019-02-16 06:14:12,777 : Image to text: 11.44, 32.32, 45.52, 13.0
2019-02-16 06:14:20,442 : Text to Image: 9.612, 27.284, 39.376, 17.0
2019-02-16 06:15:02,808 : samples : 192000
2019-02-16 06:15:13,298 : Image to text: 12.56, 32.26, 45.8, 13.0
2019-02-16 06:15:20,897 : Text to Image: 10.268, 28.556, 40.788, 16.0
2019-02-16 06:16:03,229 : samples : 256000
2019-02-16 06:16:13,753 : Image to text: 12.28, 32.3, 45.48, 13.0
2019-02-16 06:16:21,301 : Text to Image: 9.592, 27.784, 40.42, 16.0
2019-02-16 06:17:04,754 : samples : 320000
2019-02-16 06:17:15,290 : Image to text: 11.86, 31.82, 44.8, 13.0
2019-02-16 06:17:22,877 : Text to Image: 9.68, 27.72, 39.892, 17.0
2019-02-16 06:18:05,268 : samples : 384000
2019-02-16 06:18:15,707 : Image to text: 11.98, 32.1, 46.1, 13.0
2019-02-16 06:18:23,328 : Text to Image: 9.764, 27.652, 39.828, 17.0
2019-02-16 06:19:05,856 : samples : 448000
2019-02-16 06:19:16,371 : Image to text: 12.58, 32.7, 46.62, 12.0
2019-02-16 06:19:23,978 : Text to Image: 10.156, 28.604, 40.604, 16.0
2019-02-16 06:20:06,905 : samples : 512000
2019-02-16 06:20:17,378 : Image to text: 11.74, 30.92, 44.34, 13.0
2019-02-16 06:20:24,723 : Text to Image: 9.612, 27.844, 40.356, 16.0
2019-02-16 06:21:09,030 : Epoch 12 finished
2019-02-16 06:21:09,596 : Image to text: 27.7, 63.0, 77.0, 3.0
2019-02-16 06:21:10,034 : Text to Image: 24.02, 59.42, 75.24, 4.0
2019-02-16 06:21:10,586 : Image to text: 30.3, 62.8, 78.2, 3.0
2019-02-16 06:21:11,024 : Text to Image: 24.4, 56.98, 74.6, 4.0
2019-02-16 06:21:11,589 : Image to text: 32.1, 64.5, 78.7, 3.0
2019-02-16 06:21:11,992 : Text to Image: 25.32, 59.44, 75.24, 4.0
2019-02-16 06:21:12,556 : Image to text: 30.5, 65.3, 78.8, 3.0
2019-02-16 06:21:12,992 : Text to Image: 24.2, 58.18, 75.42, 4.0
2019-02-16 06:21:13,539 : Image to text: 31.6, 65.7, 80.3, 3.0
2019-02-16 06:21:13,976 : Text to Image: 25.0, 58.36, 74.68, 4.0
2019-02-16 06:21:13,976 : Dev mean Text to Image: 24.588, 58.476, 75.036, 4.0
2019-02-16 06:21:13,976 : Dev mean Image to text: 30.440000000000005, 64.25999999999999, 78.6, 3.0
2019-02-16 06:21:13,977 : start epoch
2019-02-16 06:21:56,512 : samples : 64000
2019-02-16 06:22:06,996 : Image to text: 12.52, 33.2, 45.48, 13.0
2019-02-16 06:22:14,607 : Text to Image: 9.8, 27.504, 39.716, 17.0
2019-02-16 06:22:56,777 : samples : 128000
2019-02-16 06:23:07,297 : Image to text: 11.82, 32.3, 45.74, 13.0
2019-02-16 06:23:14,852 : Text to Image: 9.62, 27.468, 39.456, 17.0
2019-02-16 06:23:57,331 : samples : 192000
2019-02-16 06:24:07,855 : Image to text: 12.22, 33.08, 45.5, 13.0
2019-02-16 06:24:15,532 : Text to Image: 9.524, 27.836, 40.328, 16.0
2019-02-16 06:24:57,833 : samples : 256000
2019-02-16 06:25:08,338 : Image to text: 12.24, 32.68, 45.9, 12.0
2019-02-16 06:25:16,004 : Text to Image: 9.936, 28.176, 40.76, 16.0
2019-02-16 06:25:58,631 : samples : 320000
2019-02-16 06:26:09,138 : Image to text: 12.3, 31.9, 44.32, 14.0
2019-02-16 06:26:16,741 : Text to Image: 9.552, 28.012, 40.3, 16.0
2019-02-16 06:26:58,919 : samples : 384000
2019-02-16 06:27:09,382 : Image to text: 11.68, 31.66, 45.54, 13.0
2019-02-16 06:27:17,028 : Text to Image: 9.408, 27.416, 39.772, 17.0
2019-02-16 06:27:59,339 : samples : 448000
2019-02-16 06:28:09,820 : Image to text: 12.16, 33.28, 46.26, 12.0
2019-02-16 06:28:17,451 : Text to Image: 9.672, 27.808, 40.144, 17.0
2019-02-16 06:29:01,018 : samples : 512000
2019-02-16 06:29:11,528 : Image to text: 11.72, 32.02, 45.74, 12.0
2019-02-16 06:29:19,092 : Text to Image: 9.428, 27.424, 40.0, 16.0
2019-02-16 06:29:55,779 : Epoch 13 finished
2019-02-16 06:29:56,212 : Image to text: 30.0, 62.5, 77.0, 3.0
2019-02-16 06:29:56,543 : Text to Image: 24.46, 59.02, 75.24, 4.0
2019-02-16 06:29:56,989 : Image to text: 29.4, 62.5, 77.9, 3.0
2019-02-16 06:29:57,325 : Text to Image: 24.34, 57.58, 74.28, 4.0
2019-02-16 06:29:57,771 : Image to text: 29.5, 62.5, 78.7, 3.0
2019-02-16 06:29:58,117 : Text to Image: 25.36, 58.36, 74.6, 4.0
2019-02-16 06:29:58,570 : Image to text: 31.8, 63.7, 79.2, 3.0
2019-02-16 06:29:58,913 : Text to Image: 24.66, 58.68, 75.38, 4.0
2019-02-16 06:29:59,350 : Image to text: 31.2, 64.8, 78.1, 3.0
2019-02-16 06:29:59,691 : Text to Image: 24.6, 57.7, 74.14, 4.0
2019-02-16 06:29:59,691 : Dev mean Text to Image: 24.684000000000005, 58.26800000000001, 74.728, 4.0
2019-02-16 06:29:59,691 : Dev mean Image to text: 30.380000000000003, 63.2, 78.18, 3.0
2019-02-16 06:30:03,636 : 
Test scores | Image to text:             30.439999999999998, 63.49999999999999, 78.38, 3.1999999999999997
2019-02-16 06:30:03,636 : Test scores | Text to image:             24.076, 58.368, 75.016, 4.0

2019-02-16 06:30:03,751 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 06:30:03,982 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 06:30:04,715 : loading BERT model bert-base-uncased
2019-02-16 06:30:04,716 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:30:04,749 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:30:04,749 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpni2ef9o9
2019-02-16 06:30:07,298 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:30:08,799 : Computing embeddings for train/dev/test
2019-02-16 06:31:43,499 : Computed embeddings
2019-02-16 06:31:43,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:32:32,373 : [('reg:1e-05', 71.81), ('reg:0.0001', 73.46), ('reg:0.001', 63.1), ('reg:0.01', 54.8)]
2019-02-16 06:32:32,374 : Validation : best param found is reg = 0.0001 with score             73.46
2019-02-16 06:32:32,374 : Evaluating...
2019-02-16 06:32:47,592 : 
Dev acc : 73.5 Test acc : 75.0 for LENGTH classification

2019-02-16 06:32:47,593 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 06:32:47,970 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 06:32:48,021 : loading BERT model bert-base-uncased
2019-02-16 06:32:48,021 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:32:48,053 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:32:48,053 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe2oulstf
2019-02-16 06:32:50,533 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:32:52,014 : Computing embeddings for train/dev/test
2019-02-16 06:34:20,515 : Computed embeddings
2019-02-16 06:34:20,515 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:35:10,418 : [('reg:1e-05', 10.63), ('reg:0.0001', 2.44), ('reg:0.001', 0.47), ('reg:0.01', 0.2)]
2019-02-16 06:35:10,419 : Validation : best param found is reg = 1e-05 with score             10.63
2019-02-16 06:35:10,419 : Evaluating...
2019-02-16 06:35:25,874 : 
Dev acc : 10.6 Test acc : 10.5 for WORDCONTENT classification

2019-02-16 06:35:25,875 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 06:35:26,249 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 06:35:26,319 : loading BERT model bert-base-uncased
2019-02-16 06:35:26,319 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:35:26,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:35:26,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp945qz22f
2019-02-16 06:35:28,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:35:30,335 : Computing embeddings for train/dev/test
2019-02-16 06:36:52,576 : Computed embeddings
2019-02-16 06:36:52,576 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:37:30,861 : [('reg:1e-05', 29.86), ('reg:0.0001', 27.77), ('reg:0.001', 26.97), ('reg:0.01', 25.02)]
2019-02-16 06:37:30,862 : Validation : best param found is reg = 1e-05 with score             29.86
2019-02-16 06:37:30,862 : Evaluating...
2019-02-16 06:37:37,906 : 
Dev acc : 29.9 Test acc : 29.8 for DEPTH classification

2019-02-16 06:37:37,907 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 06:37:38,401 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 06:37:38,483 : loading BERT model bert-base-uncased
2019-02-16 06:37:38,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:37:38,619 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:37:38,619 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2itdp_qw
2019-02-16 06:37:41,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:37:43,448 : Computing embeddings for train/dev/test
2019-02-16 06:39:02,566 : Computed embeddings
2019-02-16 06:39:02,566 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:39:50,646 : [('reg:1e-05', 68.04), ('reg:0.0001', 66.68), ('reg:0.001', 60.84), ('reg:0.01', 39.0)]
2019-02-16 06:39:50,646 : Validation : best param found is reg = 1e-05 with score             68.04
2019-02-16 06:39:50,646 : Evaluating...
2019-02-16 06:40:03,443 : 
Dev acc : 68.0 Test acc : 68.3 for TOPCONSTITUENTS classification

2019-02-16 06:40:03,445 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 06:40:04,025 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 06:40:04,096 : loading BERT model bert-base-uncased
2019-02-16 06:40:04,097 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:40:04,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:40:04,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpthjdwfg3
2019-02-16 06:40:06,661 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:40:08,123 : Computing embeddings for train/dev/test
2019-02-16 06:41:32,172 : Computed embeddings
2019-02-16 06:41:32,172 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:42:26,791 : [('reg:1e-05', 81.16), ('reg:0.0001', 81.92), ('reg:0.001', 80.83), ('reg:0.01', 72.26)]
2019-02-16 06:42:26,791 : Validation : best param found is reg = 0.0001 with score             81.92
2019-02-16 06:42:26,791 : Evaluating...
2019-02-16 06:42:44,442 : 
Dev acc : 81.9 Test acc : 81.1 for BIGRAMSHIFT classification

2019-02-16 06:42:44,443 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 06:42:44,850 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 06:42:44,917 : loading BERT model bert-base-uncased
2019-02-16 06:42:44,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:42:44,949 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:42:44,949 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2d0gf0kp
2019-02-16 06:42:47,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:42:48,937 : Computing embeddings for train/dev/test
2019-02-16 06:44:10,881 : Computed embeddings
2019-02-16 06:44:10,881 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:44:52,332 : [('reg:1e-05', 89.66), ('reg:0.0001', 89.7), ('reg:0.001', 89.95), ('reg:0.01', 89.9)]
2019-02-16 06:44:52,332 : Validation : best param found is reg = 0.001 with score             89.95
2019-02-16 06:44:52,332 : Evaluating...
2019-02-16 06:44:59,916 : 
Dev acc : 90.0 Test acc : 88.7 for TENSE classification

2019-02-16 06:44:59,917 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 06:45:00,363 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 06:45:00,432 : loading BERT model bert-base-uncased
2019-02-16 06:45:00,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:45:00,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:45:00,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1qy0q69i
2019-02-16 06:45:02,992 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:45:04,491 : Computing embeddings for train/dev/test
2019-02-16 06:46:31,435 : Computed embeddings
2019-02-16 06:46:31,435 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:47:17,462 : [('reg:1e-05', 84.63), ('reg:0.0001', 85.04), ('reg:0.001', 84.33), ('reg:0.01', 72.69)]
2019-02-16 06:47:17,462 : Validation : best param found is reg = 0.0001 with score             85.04
2019-02-16 06:47:17,463 : Evaluating...
2019-02-16 06:47:28,266 : 
Dev acc : 85.0 Test acc : 84.3 for SUBJNUMBER classification

2019-02-16 06:47:28,267 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 06:47:28,730 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 06:47:28,808 : loading BERT model bert-base-uncased
2019-02-16 06:47:28,808 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:47:28,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:47:28,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy1hij7tt
2019-02-16 06:47:31,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:47:32,924 : Computing embeddings for train/dev/test
2019-02-16 06:48:58,670 : Computed embeddings
2019-02-16 06:48:58,670 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:49:48,416 : [('reg:1e-05', 77.59), ('reg:0.0001', 77.43), ('reg:0.001', 76.93), ('reg:0.01', 72.88)]
2019-02-16 06:49:48,416 : Validation : best param found is reg = 1e-05 with score             77.59
2019-02-16 06:49:48,416 : Evaluating...
2019-02-16 06:49:59,153 : 
Dev acc : 77.6 Test acc : 79.2 for OBJNUMBER classification

2019-02-16 06:49:59,154 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 06:49:59,541 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 06:49:59,614 : loading BERT model bert-base-uncased
2019-02-16 06:49:59,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:49:59,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:49:59,740 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_r1j3i8b
2019-02-16 06:50:02,258 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:50:03,737 : Computing embeddings for train/dev/test
2019-02-16 06:51:41,406 : Computed embeddings
2019-02-16 06:51:41,406 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:52:24,540 : [('reg:1e-05', 56.73), ('reg:0.0001', 57.22), ('reg:0.001', 56.41), ('reg:0.01', 54.6)]
2019-02-16 06:52:24,540 : Validation : best param found is reg = 0.0001 with score             57.22
2019-02-16 06:52:24,540 : Evaluating...
2019-02-16 06:52:36,520 : 
Dev acc : 57.2 Test acc : 57.9 for ODDMANOUT classification

2019-02-16 06:52:36,521 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 06:52:37,188 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 06:52:37,277 : loading BERT model bert-base-uncased
2019-02-16 06:52:37,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:52:37,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:52:37,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvlijocao
2019-02-16 06:52:39,840 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:52:41,323 : Computing embeddings for train/dev/test
2019-02-16 06:54:20,151 : Computed embeddings
2019-02-16 06:54:20,151 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 06:54:46,413 : [('reg:1e-05', 54.88), ('reg:0.0001', 54.62), ('reg:0.001', 52.86), ('reg:0.01', 50.4)]
2019-02-16 06:54:46,413 : Validation : best param found is reg = 1e-05 with score             54.88
2019-02-16 06:54:46,413 : Evaluating...
2019-02-16 06:54:53,124 : 
Dev acc : 54.9 Test acc : 54.6 for COORDINATIONINVERSION classification

2019-02-16 06:54:53,127 : total results: {'STS12': {'MSRpar': {'pearson': (0.2971227234301257, 9.418333968579935e-17), 'spearman': SpearmanrResult(correlation=0.35142632036170107, pvalue=3.192464704960887e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.34937659623191775, 5.923635071681274e-23), 'spearman': SpearmanrResult(correlation=0.41698694697985667, pvalue=6.554150548387194e-33), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4626515966033861, 1.0029133258902804e-25), 'spearman': SpearmanrResult(correlation=0.5756765837572106, pvalue=7.238850842890226e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.26394954746824617, 2.0242578577525526e-13), 'spearman': SpearmanrResult(correlation=0.26073821390731844, pvalue=4.029961159439806e-13), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6512370926667364, 1.6387094141862897e-49), 'spearman': SpearmanrResult(correlation=0.5008252729490448, pvalue=1.0093993922350583e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4048675112800824, 'wmean': 0.3716336335787321}, 'spearman': {'mean': 0.42113066759102635, 'wmean': 0.39766037541437765}}}, 'STS13': {'FNWN': {'pearson': (0.2605235436491155, 0.00029395363582579274), 'spearman': SpearmanrResult(correlation=0.26930418586625043, pvalue=0.00017879974831908193), 'nsamples': 189}, 'headlines': {'pearson': (0.4268185667756522, 1.4696126319516573e-34), 'spearman': SpearmanrResult(correlation=0.4616479615342043, pvalue=7.386552637514035e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.2192320225526428, 1.5631373431128984e-07), 'spearman': SpearmanrResult(correlation=0.23248703927576894, pvalue=2.5359466571712093e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3021913776591368, 'wmean': 0.3282280263223031}, 'spearman': {'mean': 0.32114639555874125, 'wmean': 0.3517064608753873}}}, 'STS14': {'deft-forum': {'pearson': (0.17230108294297514, 0.00024028965360522862), 'spearman': SpearmanrResult(correlation=0.20902438845557683, pvalue=7.777132575835715e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.6384187304669929, 9.534020638482025e-36), 'spearman': SpearmanrResult(correlation=0.6528549005542249, pvalue=7.958712793391714e-38), 'nsamples': 300}, 'headlines': {'pearson': (0.37657664401592794, 1.1111439688567723e-26), 'spearman': SpearmanrResult(correlation=0.3803337333069784, pvalue=3.1798873606343923e-27), 'nsamples': 750}, 'images': {'pearson': (0.4071561798414186, 2.581716206052868e-31), 'spearman': SpearmanrResult(correlation=0.39865970418224156, pvalue=5.6044854569499715e-30), 'nsamples': 750}, 'OnWN': {'pearson': (0.3094026058937166, 4.212259948470871e-18), 'spearman': SpearmanrResult(correlation=0.31781712849714416, pvalue=4.596061873607641e-19), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3350291422258736, 3.953779379131649e-21), 'spearman': SpearmanrResult(correlation=0.3365704209581113, pvalue=2.5442787190877192e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3731473975644841, 'wmean': 0.3573825427859038}, 'spearman': {'mean': 0.38254337932571286, 'wmean': 0.3639875160479023}}}, 'STS15': {'answers-forums': {'pearson': (0.36158190409207785, 5.029002078944632e-13), 'spearman': SpearmanrResult(correlation=0.3888180721765381, pvalue=5.545363270832513e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.4688868182346517, 2.919777695546361e-42), 'spearman': SpearmanrResult(correlation=0.4909319229852269, pvalue=9.537035784989864e-47), 'nsamples': 750}, 'belief': {'pearson': (0.3829437745161806, 1.5197412304166492e-14), 'spearman': SpearmanrResult(correlation=0.46233858763569785, pvalue=2.93777533904565e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.49831736729198006, 2.5228985993672837e-48), 'spearman': SpearmanrResult(correlation=0.5239540166334941, pvalue=4.13238877345749e-54), 'nsamples': 750}, 'images': {'pearson': (0.1685303765123521, 3.4684356124511663e-06), 'spearman': SpearmanrResult(correlation=0.35881483418116455, pvalue=3.3105522623918325e-24), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3760520481294484, 'wmean': 0.3769993503357783}, 'spearman': {'mean': 0.44497148672242426, 'wmean': 0.4498197759265009}}}, 'STS16': {'answer-answer': {'pearson': (0.3889011107540634, 1.3442679928987114e-10), 'spearman': SpearmanrResult(correlation=0.46104432511878457, pvalue=8.99948857747052e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.494777084216068, 8.828006452188184e-17), 'spearman': SpearmanrResult(correlation=0.5430694695380944, pvalue=1.672342206772441e-20), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6495016721254091, 6.022522611604663e-29), 'spearman': SpearmanrResult(correlation=0.705883715572025, pvalue=5.312396291554973e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.5843303277420319, 9.759781574414356e-24), 'spearman': SpearmanrResult(correlation=0.7856024923373829, pvalue=2.274207572005856e-52), 'nsamples': 244}, 'question-question': {'pearson': (0.4452786566959501, 1.4239798826252169e-11), 'spearman': SpearmanrResult(correlation=0.446386214605813, pvalue=1.2504590024650582e-11), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5125577703067045, 'wmean': 0.5118091061624676}, 'spearman': {'mean': 0.58839724343442, 'wmean': 0.5899363727315842}}}, 'MR': {'devacc': 69.53, 'acc': 68.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 73.42, 'acc': 69.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.07, 'acc': 84.5, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.86, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.33, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.33, 'acc': 38.73, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.77, 'acc': 79.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.07, 'acc': 67.42, 'f1': 75.46, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 70.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7498275509314094, 'pearson': 0.7556118409882442, 'spearman': 0.6920325135739948, 'mse': 0.44186181803867103, 'yhat': array([3.34850412, 3.91901105, 2.24816336, ..., 2.95443197, 4.44030722,        4.58507018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6365482500673998, 'pearson': 0.6203475545781522, 'spearman': 0.6151130448745691, 'mse': 1.6714750152740945, 'yhat': array([2.41759952, 1.6228788 , 2.95518608, ..., 3.84841727, 3.63706733,        3.25899408]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 53.71, 'acc': 53.44, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 331.4, 'acc': [(30.439999999999998, 63.49999999999999, 78.38, 3.1999999999999997), (24.076, 58.368, 75.016, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 73.46, 'acc': 74.95, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 10.63, 'acc': 10.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.86, 'acc': 29.81, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.04, 'acc': 68.32, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.92, 'acc': 81.09, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.95, 'acc': 88.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.04, 'acc': 84.35, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.59, 'acc': 79.21, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.22, 'acc': 57.88, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 54.88, 'acc': 54.56, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 06:54:53,127 : STS12 p=0.3716, STS12 s=0.3977, STS13 p=0.3282, STS13 s=0.3517, STS14 p=0.3574, STS14 s=0.3640, STS15 p=0.3770, STS15 s=0.4498, STS 16 p=0.5118, STS16 s=0.5899, STS B p=0.6203, STS B s=0.6151, STS B m=1.6715, SICK-R p=0.7556, SICK-R s=0.6920, SICK-P m=0.4419
2019-02-16 06:54:53,127 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 06:54:53,127 : 0.3716,0.3977,0.3282,0.3517,0.3574,0.3640,0.3770,0.4498,0.5118,0.5899,0.6203,0.6151,1.6715,0.7556,0.6920,0.4419
2019-02-16 06:54:53,127 : MR=68.13, CR=69.38, SUBJ=92.68, MPQA=84.50, SST-B=77.81, SST-F=38.73, TREC=79.20, SICK-E=70.98, SNLI=53.44, MRPC=67.42, MRPC f=75.46
2019-02-16 06:54:53,127 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 06:54:53,127 : 68.13,69.38,92.68,84.50,77.81,38.73,79.20,70.98,53.44,67.42,75.46
2019-02-16 06:54:53,127 : COCO r1i2t=30.44, COCO r5i2t=63.50, COCO r10i2t=78.38, COCO medr_i2t=3.20, COCO r1t2i=24.08, COCO r5t2i=58.37, COCO r10t2i=75.02, COCO medr_t2i=4.00
2019-02-16 06:54:53,127 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 06:54:53,127 : 30.44,63.50,78.38,3.20,24.08,58.37,75.02,4.00
2019-02-16 06:54:53,127 : SentLen=74.95, WC=10.49, TreeDepth=29.81, TopConst=68.32, BShift=81.09, Tense=88.68, SubjNum=84.35, ObjNum=79.21, SOMO=57.88, CoordInv=54.56, average=62.93
2019-02-16 06:54:53,127 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 06:54:53,127 : 74.95,10.49,29.81,68.32,81.09,88.68,84.35,79.21,57.88,54.56,62.93
2019-02-16 06:54:53,127 : ********************************************************************************
2019-02-16 06:54:53,127 : ********************************************************************************
2019-02-16 06:54:53,127 : ********************************************************************************
2019-02-16 06:54:53,128 : layer 6
2019-02-16 06:54:53,128 : ********************************************************************************
2019-02-16 06:54:53,128 : ********************************************************************************
2019-02-16 06:54:53,128 : ********************************************************************************
2019-02-16 06:54:53,249 : ***** Transfer task : STS12 *****


2019-02-16 06:54:53,268 : loading BERT model bert-base-uncased
2019-02-16 06:54:53,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:54:53,295 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:54:53,296 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwixnil16
2019-02-16 06:54:56,323 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:00,672 : MSRpar : pearson = 0.2264, spearman = 0.2758
2019-02-16 06:55:01,527 : MSRvid : pearson = 0.1589, spearman = 0.1729
2019-02-16 06:55:02,277 : SMTeuroparl : pearson = 0.4183, spearman = 0.5189
2019-02-16 06:55:03,534 : surprise.OnWN : pearson = 0.3270, spearman = 0.3222
2019-02-16 06:55:04,246 : surprise.SMTnews : pearson = 0.5912, spearman = 0.4420
2019-02-16 06:55:04,246 : ALL (weighted average) : Pearson = 0.3096,             Spearman = 0.3194
2019-02-16 06:55:04,246 : ALL (average) : Pearson = 0.3444,             Spearman = 0.3464

2019-02-16 06:55:04,246 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 06:55:04,259 : loading BERT model bert-base-uncased
2019-02-16 06:55:04,259 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:55:04,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:55:04,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5cx6e6ca
2019-02-16 06:55:07,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:09,113 : FNWN : pearson = 0.0960, spearman = 0.1243
2019-02-16 06:55:10,010 : headlines : pearson = 0.4524, spearman = 0.4446
2019-02-16 06:55:10,676 : OnWN : pearson = 0.1898, spearman = 0.1940
2019-02-16 06:55:10,676 : ALL (weighted average) : Pearson = 0.3093,             Spearman = 0.3105
2019-02-16 06:55:10,676 : ALL (average) : Pearson = 0.2461,             Spearman = 0.2543

2019-02-16 06:55:10,676 : ***** Transfer task : STS14 *****


2019-02-16 06:55:10,694 : loading BERT model bert-base-uncased
2019-02-16 06:55:10,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:55:10,716 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:55:10,716 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn17iir4h
2019-02-16 06:55:13,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:15,388 : deft-forum : pearson = -0.0012, spearman = 0.0070
2019-02-16 06:55:16,184 : deft-news : pearson = 0.4521, spearman = 0.4936
2019-02-16 06:55:17,268 : headlines : pearson = 0.4028, spearman = 0.3796
2019-02-16 06:55:18,314 : images : pearson = 0.2372, spearman = 0.2497
2019-02-16 06:55:19,365 : OnWN : pearson = 0.3358, spearman = 0.3405
2019-02-16 06:55:20,744 : tweet-news : pearson = 0.3987, spearman = 0.3799
2019-02-16 06:55:20,744 : ALL (weighted average) : Pearson = 0.3109,             Spearman = 0.3103
2019-02-16 06:55:20,744 : ALL (average) : Pearson = 0.3042,             Spearman = 0.3084

2019-02-16 06:55:20,744 : ***** Transfer task : STS15 *****


2019-02-16 06:55:20,780 : loading BERT model bert-base-uncased
2019-02-16 06:55:20,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:55:20,799 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:55:20,799 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph8lx59dm
2019-02-16 06:55:23,355 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:25,805 : answers-forums : pearson = 0.2137, spearman = 0.2275
2019-02-16 06:55:26,846 : answers-students : pearson = 0.4765, spearman = 0.4838
2019-02-16 06:55:27,730 : belief : pearson = 0.3384, spearman = 0.4115
2019-02-16 06:55:28,723 : headlines : pearson = 0.4804, spearman = 0.4834
2019-02-16 06:55:29,677 : images : pearson = 0.1113, spearman = 0.1542
2019-02-16 06:55:29,677 : ALL (weighted average) : Pearson = 0.3361,             Spearman = 0.3602
2019-02-16 06:55:29,677 : ALL (average) : Pearson = 0.3241,             Spearman = 0.3521

2019-02-16 06:55:29,677 : ***** Transfer task : STS16 *****


2019-02-16 06:55:29,745 : loading BERT model bert-base-uncased
2019-02-16 06:55:29,745 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:55:29,763 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:55:29,763 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwwplp8zs
2019-02-16 06:55:32,260 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:34,086 : answer-answer : pearson = 0.3775, spearman = 0.4270
2019-02-16 06:55:34,388 : headlines : pearson = 0.5445, spearman = 0.5538
2019-02-16 06:55:34,773 : plagiarism : pearson = 0.6029, spearman = 0.6481
2019-02-16 06:55:35,386 : postediting : pearson = 0.5883, spearman = 0.7422
2019-02-16 06:55:35,684 : question-question : pearson = 0.1718, spearman = 0.2521
2019-02-16 06:55:35,685 : ALL (weighted average) : Pearson = 0.4634,             Spearman = 0.5306
2019-02-16 06:55:35,685 : ALL (average) : Pearson = 0.4570,             Spearman = 0.5247

2019-02-16 06:55:35,685 : ***** Transfer task : MR *****


2019-02-16 06:55:35,705 : loading BERT model bert-base-uncased
2019-02-16 06:55:35,705 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:55:35,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:55:35,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3fjq00qb
2019-02-16 06:55:38,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:55:39,737 : Generating sentence embeddings
2019-02-16 06:55:53,131 : Generated sentence embeddings
2019-02-16 06:55:53,131 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 06:56:10,850 : Best param found at split 1: l2reg = 1e-05                 with score 73.21
2019-02-16 06:56:28,259 : Best param found at split 2: l2reg = 1e-05                 with score 70.83
2019-02-16 06:56:44,005 : Best param found at split 3: l2reg = 1e-05                 with score 70.73
2019-02-16 06:57:02,554 : Best param found at split 4: l2reg = 0.001                 with score 71.27
2019-02-16 06:57:19,996 : Best param found at split 5: l2reg = 0.01                 with score 70.79
2019-02-16 06:57:21,280 : Dev acc : 71.37 Test acc : 70.79

2019-02-16 06:57:21,282 : ***** Transfer task : CR *****


2019-02-16 06:57:21,292 : loading BERT model bert-base-uncased
2019-02-16 06:57:21,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:57:21,317 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:57:21,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1yvmtv4i
2019-02-16 06:57:23,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:57:25,269 : Generating sentence embeddings
2019-02-16 06:57:29,337 : Generated sentence embeddings
2019-02-16 06:57:29,337 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 06:57:35,283 : Best param found at split 1: l2reg = 0.001                 with score 76.42
2019-02-16 06:57:40,389 : Best param found at split 2: l2reg = 1e-05                 with score 75.92
2019-02-16 06:57:46,631 : Best param found at split 3: l2reg = 0.001                 with score 74.87
2019-02-16 06:57:53,346 : Best param found at split 4: l2reg = 0.0001                 with score 74.81
2019-02-16 06:58:00,248 : Best param found at split 5: l2reg = 0.001                 with score 74.71
2019-02-16 06:58:00,446 : Dev acc : 75.35 Test acc : 70.57

2019-02-16 06:58:00,447 : ***** Transfer task : MPQA *****


2019-02-16 06:58:00,452 : loading BERT model bert-base-uncased
2019-02-16 06:58:00,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:58:00,475 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:58:00,475 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpova9sdpi
2019-02-16 06:58:02,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:58:04,470 : Generating sentence embeddings
2019-02-16 06:58:08,219 : Generated sentence embeddings
2019-02-16 06:58:08,220 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 06:58:24,376 : Best param found at split 1: l2reg = 1e-05                 with score 83.71
2019-02-16 06:58:43,426 : Best param found at split 2: l2reg = 0.001                 with score 85.93
2019-02-16 06:59:03,016 : Best param found at split 3: l2reg = 1e-05                 with score 84.97
2019-02-16 06:59:18,700 : Best param found at split 4: l2reg = 0.0001                 with score 85.55
2019-02-16 06:59:35,127 : Best param found at split 5: l2reg = 0.001                 with score 85.0
2019-02-16 06:59:35,667 : Dev acc : 85.03 Test acc : 86.19

2019-02-16 06:59:35,668 : ***** Transfer task : SUBJ *****


2019-02-16 06:59:35,681 : loading BERT model bert-base-uncased
2019-02-16 06:59:35,682 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 06:59:35,706 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 06:59:35,706 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8o_wd5bh
2019-02-16 06:59:38,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 06:59:39,783 : Generating sentence embeddings
2019-02-16 06:59:53,068 : Generated sentence embeddings
2019-02-16 06:59:53,068 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 07:00:11,283 : Best param found at split 1: l2reg = 0.0001                 with score 94.0
2019-02-16 07:00:27,502 : Best param found at split 2: l2reg = 1e-05                 with score 93.84
2019-02-16 07:00:45,713 : Best param found at split 3: l2reg = 1e-05                 with score 93.62
2019-02-16 07:01:03,623 : Best param found at split 4: l2reg = 0.001                 with score 94.05
2019-02-16 07:01:18,556 : Best param found at split 5: l2reg = 1e-05                 with score 93.68
2019-02-16 07:01:19,739 : Dev acc : 93.84 Test acc : 93.85

2019-02-16 07:01:19,740 : ***** Transfer task : SST Binary classification *****


2019-02-16 07:01:19,877 : loading BERT model bert-base-uncased
2019-02-16 07:01:19,878 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:01:19,906 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:01:19,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk101eyu6
2019-02-16 07:01:22,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:01:23,862 : Computing embedding for train
2019-02-16 07:02:08,696 : Computed train embeddings
2019-02-16 07:02:08,696 : Computing embedding for dev
2019-02-16 07:02:09,756 : Computed dev embeddings
2019-02-16 07:02:09,756 : Computing embedding for test
2019-02-16 07:02:11,937 : Computed test embeddings
2019-02-16 07:02:11,937 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:02:33,968 : [('reg:1e-05', 79.13), ('reg:0.0001', 79.13), ('reg:0.001', 79.36), ('reg:0.01', 77.18)]
2019-02-16 07:02:33,969 : Validation : best param found is reg = 0.001 with score             79.36
2019-02-16 07:02:33,969 : Evaluating...
2019-02-16 07:02:39,705 : 
Dev acc : 79.36 Test acc : 77.81 for             SST Binary classification

2019-02-16 07:02:39,705 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 07:02:39,761 : loading BERT model bert-base-uncased
2019-02-16 07:02:39,761 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:02:39,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:02:39,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcm5ftsk7
2019-02-16 07:02:42,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:02:43,781 : Computing embedding for train
2019-02-16 07:02:53,151 : Computed train embeddings
2019-02-16 07:02:53,151 : Computing embedding for dev
2019-02-16 07:02:54,364 : Computed dev embeddings
2019-02-16 07:02:54,365 : Computing embedding for test
2019-02-16 07:02:56,826 : Computed test embeddings
2019-02-16 07:02:56,826 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:03:00,027 : [('reg:1e-05', 38.33), ('reg:0.0001', 40.42), ('reg:0.001', 39.15), ('reg:0.01', 34.97)]
2019-02-16 07:03:00,027 : Validation : best param found is reg = 0.0001 with score             40.42
2019-02-16 07:03:00,027 : Evaluating...
2019-02-16 07:03:00,912 : 
Dev acc : 40.42 Test acc : 41.49 for             SST Fine-Grained classification

2019-02-16 07:03:00,913 : ***** Transfer task : TREC *****


2019-02-16 07:03:00,927 : loading BERT model bert-base-uncased
2019-02-16 07:03:00,927 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:03:00,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:03:00,948 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw15bkfo3
2019-02-16 07:03:03,417 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:03:08,520 : Computed train embeddings
2019-02-16 07:03:08,812 : Computed test embeddings
2019-02-16 07:03:08,813 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 07:03:18,999 : [('reg:1e-05', 75.73), ('reg:0.0001', 75.97), ('reg:0.001', 74.61), ('reg:0.01', 66.25)]
2019-02-16 07:03:18,999 : Cross-validation : best param found is reg = 0.0001             with score 75.97
2019-02-16 07:03:18,999 : Evaluating...
2019-02-16 07:03:19,631 : 
Dev acc : 75.97 Test acc : 86.0             for TREC

2019-02-16 07:03:19,631 : ***** Transfer task : MRPC *****


2019-02-16 07:03:19,653 : loading BERT model bert-base-uncased
2019-02-16 07:03:19,653 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:03:19,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:03:19,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj21x3cgl
2019-02-16 07:03:22,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:03:23,671 : Computing embedding for train
2019-02-16 07:03:33,266 : Computed train embeddings
2019-02-16 07:03:33,266 : Computing embedding for test
2019-02-16 07:03:37,474 : Computed test embeddings
2019-02-16 07:03:37,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 07:03:45,272 : [('reg:1e-05', 69.26), ('reg:0.0001', 70.39), ('reg:0.001', 70.05), ('reg:0.01', 69.16)]
2019-02-16 07:03:45,272 : Cross-validation : best param found is reg = 0.0001             with score 70.39
2019-02-16 07:03:45,273 : Evaluating...
2019-02-16 07:03:45,758 : Dev acc : 70.39 Test acc 71.88; Test F1 81.17 for MRPC.

2019-02-16 07:03:45,758 : ***** Transfer task : SICK-Entailment*****


2019-02-16 07:03:45,824 : loading BERT model bert-base-uncased
2019-02-16 07:03:45,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:03:45,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:03:45,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6z0rnz2f
2019-02-16 07:03:48,363 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:03:49,861 : Computing embedding for train
2019-02-16 07:03:55,018 : Computed train embeddings
2019-02-16 07:03:55,019 : Computing embedding for dev
2019-02-16 07:03:55,706 : Computed dev embeddings
2019-02-16 07:03:55,706 : Computing embedding for test
2019-02-16 07:04:01,259 : Computed test embeddings
2019-02-16 07:04:01,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:04:03,678 : [('reg:1e-05', 71.4), ('reg:0.0001', 68.2), ('reg:0.001', 73.2), ('reg:0.01', 67.0)]
2019-02-16 07:04:03,679 : Validation : best param found is reg = 0.001 with score             73.2
2019-02-16 07:04:03,679 : Evaluating...
2019-02-16 07:04:04,349 : 
Dev acc : 73.2 Test acc : 72.17 for                        SICK entailment

2019-02-16 07:04:04,350 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 07:04:04,383 : loading BERT model bert-base-uncased
2019-02-16 07:04:04,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:04:04,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:04:04,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ifqe1tf
2019-02-16 07:04:06,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:04:08,431 : Computing embedding for train
2019-02-16 07:04:14,093 : Computed train embeddings
2019-02-16 07:04:14,093 : Computing embedding for dev
2019-02-16 07:04:14,846 : Computed dev embeddings
2019-02-16 07:04:14,846 : Computing embedding for test
2019-02-16 07:04:20,959 : Computed test embeddings
2019-02-16 07:04:52,779 : Dev : Pearson 0.7423340884408229
2019-02-16 07:04:52,779 : Test : Pearson 0.7209892196358876 Spearman 0.6576542740719158 MSE 0.4989185742570675                        for SICK Relatedness

2019-02-16 07:04:52,780 : 

***** Transfer task : STSBenchmark*****


2019-02-16 07:04:52,822 : loading BERT model bert-base-uncased
2019-02-16 07:04:52,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:04:52,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:04:52,853 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6ixrdcyo
2019-02-16 07:04:55,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:04:56,889 : Computing embedding for train
2019-02-16 07:05:05,172 : Computed train embeddings
2019-02-16 07:05:05,172 : Computing embedding for dev
2019-02-16 07:05:07,601 : Computed dev embeddings
2019-02-16 07:05:07,601 : Computing embedding for test
2019-02-16 07:05:09,596 : Computed test embeddings
2019-02-16 07:05:39,868 : Dev : Pearson 0.5317967869206648
2019-02-16 07:05:39,868 : Test : Pearson 0.5121652098439453 Spearman 0.5074582556323265 MSE 1.817709283474579                        for SICK Relatedness

2019-02-16 07:05:39,868 : ***** Transfer task : SNLI Entailment*****


2019-02-16 07:05:44,938 : loading BERT model bert-base-uncased
2019-02-16 07:05:44,939 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:05:45,030 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:05:45,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpapercrkn
2019-02-16 07:05:47,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:05:49,097 : PROGRESS (encoding): 0.00%
2019-02-16 07:07:07,332 : PROGRESS (encoding): 14.56%
2019-02-16 07:08:34,241 : PROGRESS (encoding): 29.12%
2019-02-16 07:10:01,374 : PROGRESS (encoding): 43.69%
2019-02-16 07:11:38,233 : PROGRESS (encoding): 58.25%
2019-02-16 07:13:25,142 : PROGRESS (encoding): 72.81%
2019-02-16 07:15:09,855 : PROGRESS (encoding): 87.37%
2019-02-16 07:16:59,052 : PROGRESS (encoding): 0.00%
2019-02-16 07:17:12,627 : PROGRESS (encoding): 0.00%
2019-02-16 07:17:25,765 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 07:18:08,014 : [('reg:1e-09', 58.47)]
2019-02-16 07:18:08,015 : Validation : best param found is reg = 1e-09 with score             58.47
2019-02-16 07:18:08,015 : Evaluating...
2019-02-16 07:18:51,143 : Dev acc : 58.47 Test acc : 58.46 for SNLI

2019-02-16 07:18:51,143 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 07:19:00,368 : loading BERT model bert-base-uncased
2019-02-16 07:19:00,368 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 07:19:00,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 07:19:00,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ssrg3t5
2019-02-16 07:19:02,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 07:19:04,354 : Computing embedding for train
2019-02-16 07:26:32,073 : Computed train embeddings
2019-02-16 07:26:32,073 : Computing embedding for dev
2019-02-16 07:26:51,315 : Computed dev embeddings
2019-02-16 07:26:51,315 : Computing embedding for test
2019-02-16 07:27:11,516 : Computed test embeddings
2019-02-16 07:27:11,531 : prepare data
2019-02-16 07:27:11,596 : start epoch
2019-02-16 07:27:53,903 : samples : 64000
2019-02-16 07:28:03,904 : Image to text: 3.94, 14.98, 24.36, 37.0
2019-02-16 07:28:12,240 : Text to Image: 3.44, 13.044, 21.232, 42.0
2019-02-16 07:29:03,205 : samples : 128000
2019-02-16 07:29:13,480 : Image to text: 5.12, 17.68, 28.4, 30.0
2019-02-16 07:29:20,949 : Text to Image: 4.2, 14.88, 23.356, 37.0
2019-02-16 07:30:03,000 : samples : 192000
2019-02-16 07:30:13,268 : Image to text: 5.64, 18.8, 29.18, 27.0
2019-02-16 07:30:20,827 : Text to Image: 4.624, 15.644, 24.9, 34.0
2019-02-16 07:31:03,775 : samples : 256000
2019-02-16 07:31:14,041 : Image to text: 5.66, 18.76, 29.02, 29.0
2019-02-16 07:31:21,555 : Text to Image: 4.448, 15.92, 25.456, 32.0
2019-02-16 07:32:04,783 : samples : 320000
2019-02-16 07:32:15,023 : Image to text: 5.98, 19.28, 30.72, 25.0
2019-02-16 07:32:22,526 : Text to Image: 5.78, 18.432, 28.372, 29.0
2019-02-16 07:33:05,936 : samples : 384000
2019-02-16 07:33:16,191 : Image to text: 6.46, 20.48, 30.88, 25.0
2019-02-16 07:33:23,662 : Text to Image: 5.176, 17.784, 27.836, 30.0
2019-02-16 07:34:07,023 : samples : 448000
2019-02-16 07:34:17,271 : Image to text: 6.02, 20.54, 31.68, 24.0
2019-02-16 07:34:24,764 : Text to Image: 5.764, 18.948, 29.068, 28.0
2019-02-16 07:35:07,853 : samples : 512000
2019-02-16 07:35:18,107 : Image to text: 7.18, 22.06, 32.58, 24.0
2019-02-16 07:35:25,607 : Text to Image: 5.592, 18.636, 28.556, 29.0
2019-02-16 07:36:01,205 : Epoch 1 finished
2019-02-16 07:36:01,676 : Image to text: 19.9, 49.0, 66.1, 6.0
2019-02-16 07:36:02,028 : Text to Image: 17.04, 46.58, 64.14, 6.0
2019-02-16 07:36:02,449 : Image to text: 23.2, 51.8, 67.6, 5.0
2019-02-16 07:36:02,775 : Text to Image: 17.18, 45.4, 63.22, 6.0
2019-02-16 07:36:03,205 : Image to text: 22.2, 52.7, 67.3, 5.0
2019-02-16 07:36:03,532 : Text to Image: 17.62, 44.98, 62.6, 7.0
2019-02-16 07:36:03,972 : Image to text: 21.4, 51.0, 67.6, 5.0
2019-02-16 07:36:04,307 : Text to Image: 17.04, 47.0, 63.62, 6.0
2019-02-16 07:36:04,752 : Image to text: 19.4, 51.3, 66.9, 5.0
2019-02-16 07:36:05,090 : Text to Image: 17.12, 45.98, 63.38, 6.0
2019-02-16 07:36:05,091 : Dev mean Text to Image: 17.2, 45.988, 63.39200000000001, 6.2
2019-02-16 07:36:05,091 : Dev mean Image to text: 21.219999999999995, 51.160000000000004, 67.1, 5.2
2019-02-16 07:36:05,091 : start epoch
2019-02-16 07:36:48,438 : samples : 64000
2019-02-16 07:36:58,651 : Image to text: 7.26, 22.08, 34.08, 22.0
2019-02-16 07:37:06,138 : Text to Image: 5.896, 19.316, 29.948, 27.0
2019-02-16 07:37:49,038 : samples : 128000
2019-02-16 07:37:59,310 : Image to text: 7.18, 22.66, 34.5, 21.0
2019-02-16 07:38:06,768 : Text to Image: 6.292, 20.24, 31.008, 25.0
2019-02-16 07:38:49,549 : samples : 192000
2019-02-16 07:38:59,766 : Image to text: 7.44, 22.96, 33.92, 22.0
2019-02-16 07:39:07,217 : Text to Image: 6.4, 20.432, 31.02, 25.0
2019-02-16 07:39:50,239 : samples : 256000
2019-02-16 07:40:00,492 : Image to text: 7.88, 24.08, 35.7, 21.0
2019-02-16 07:40:07,953 : Text to Image: 7.212, 21.64, 32.572, 23.0
2019-02-16 07:40:50,836 : samples : 320000
2019-02-16 07:41:01,039 : Image to text: 8.44, 25.88, 37.32, 19.0
2019-02-16 07:41:08,502 : Text to Image: 6.936, 21.736, 32.604, 23.0
2019-02-16 07:41:51,221 : samples : 384000
2019-02-16 07:42:01,418 : Image to text: 7.48, 22.88, 34.84, 21.0
2019-02-16 07:42:08,795 : Text to Image: 5.94, 19.876, 30.7, 25.0
2019-02-16 07:42:50,656 : samples : 448000
2019-02-16 07:43:00,947 : Image to text: 8.36, 25.06, 36.44, 20.0
2019-02-16 07:43:08,361 : Text to Image: 6.876, 21.7, 32.656, 23.0
2019-02-16 07:43:51,188 : samples : 512000
2019-02-16 07:44:01,497 : Image to text: 8.84, 25.58, 36.86, 19.0
2019-02-16 07:44:08,892 : Text to Image: 7.396, 22.76, 33.92, 22.0
2019-02-16 07:44:45,158 : Epoch 2 finished
2019-02-16 07:44:45,607 : Image to text: 21.5, 52.7, 68.2, 5.0
2019-02-16 07:44:45,980 : Text to Image: 19.36, 50.0, 67.64, 5.0
2019-02-16 07:44:46,417 : Image to text: 22.1, 52.9, 69.0, 5.0
2019-02-16 07:44:46,899 : Text to Image: 18.34, 49.7, 67.54, 6.0
2019-02-16 07:44:47,349 : Image to text: 21.9, 53.7, 69.0, 5.0
2019-02-16 07:44:47,712 : Text to Image: 18.04, 49.94, 66.24, 6.0
2019-02-16 07:44:48,161 : Image to text: 22.2, 53.0, 68.8, 5.0
2019-02-16 07:44:48,525 : Text to Image: 18.36, 49.34, 66.1, 6.0
2019-02-16 07:44:48,926 : Image to text: 21.9, 51.1, 69.1, 5.0
2019-02-16 07:44:49,246 : Text to Image: 18.42, 48.76, 65.5, 6.0
2019-02-16 07:44:49,246 : Dev mean Text to Image: 18.504, 49.548, 66.604, 5.800000000000001
2019-02-16 07:44:49,246 : Dev mean Image to text: 21.919999999999998, 52.68, 68.82, 5.0
2019-02-16 07:44:49,246 : start epoch
2019-02-16 07:45:42,163 : samples : 64000
2019-02-16 07:45:52,384 : Image to text: 9.0, 25.08, 37.18, 19.0
2019-02-16 07:45:59,860 : Text to Image: 7.096, 22.128, 33.64, 22.0
2019-02-16 07:46:41,912 : samples : 128000
2019-02-16 07:46:52,149 : Image to text: 7.54, 23.48, 34.58, 21.0
2019-02-16 07:46:59,556 : Text to Image: 6.56, 20.692, 31.552, 24.0
2019-02-16 07:47:42,267 : samples : 192000
2019-02-16 07:47:52,527 : Image to text: 8.14, 25.74, 37.9, 19.0
2019-02-16 07:47:59,856 : Text to Image: 6.976, 21.912, 32.972, 23.0
2019-02-16 07:48:43,525 : samples : 256000
2019-02-16 07:48:53,748 : Image to text: 8.54, 25.2, 37.22, 20.0
2019-02-16 07:49:01,189 : Text to Image: 6.76, 21.688, 32.496, 23.0
2019-02-16 07:49:44,227 : samples : 320000
2019-02-16 07:49:54,435 : Image to text: 8.68, 25.72, 38.14, 19.0
2019-02-16 07:50:01,849 : Text to Image: 7.152, 22.372, 33.592, 22.0
2019-02-16 07:50:44,819 : samples : 384000
2019-02-16 07:50:55,075 : Image to text: 8.92, 26.32, 37.86, 18.0
2019-02-16 07:51:02,487 : Text to Image: 7.732, 22.776, 33.776, 22.0
2019-02-16 07:51:44,933 : samples : 448000
2019-02-16 07:51:55,175 : Image to text: 8.7, 25.8, 37.5, 19.0
2019-02-16 07:52:02,535 : Text to Image: 7.356, 22.772, 33.956, 22.0
2019-02-16 07:52:45,808 : samples : 512000
2019-02-16 07:52:56,056 : Image to text: 8.6, 24.88, 36.82, 19.0
2019-02-16 07:53:03,447 : Text to Image: 7.348, 22.696, 33.852, 22.0
2019-02-16 07:53:39,173 : Epoch 3 finished
2019-02-16 07:53:39,619 : Image to text: 22.4, 54.1, 67.2, 5.0
2019-02-16 07:53:40,063 : Text to Image: 18.06, 48.72, 66.24, 6.0
2019-02-16 07:53:40,532 : Image to text: 23.8, 54.1, 70.2, 4.0
2019-02-16 07:53:40,970 : Text to Image: 17.54, 48.76, 65.64, 6.0
2019-02-16 07:53:41,434 : Image to text: 22.7, 55.0, 71.3, 4.0
2019-02-16 07:53:41,872 : Text to Image: 16.9, 47.12, 65.5, 6.0
2019-02-16 07:53:42,334 : Image to text: 22.2, 55.0, 71.5, 4.0
2019-02-16 07:53:42,770 : Text to Image: 17.34, 48.54, 66.1, 6.0
2019-02-16 07:53:43,244 : Image to text: 25.7, 57.4, 71.2, 4.0
2019-02-16 07:53:43,681 : Text to Image: 17.94, 48.32, 65.76, 6.0
2019-02-16 07:53:43,681 : Dev mean Text to Image: 17.556, 48.292, 65.848, 6.0
2019-02-16 07:53:43,681 : Dev mean Image to text: 23.36, 55.120000000000005, 70.28, 4.2
2019-02-16 07:53:43,681 : start epoch
2019-02-16 07:54:28,208 : samples : 64000
2019-02-16 07:54:40,865 : Image to text: 9.8, 27.46, 39.52, 17.0
2019-02-16 07:54:50,929 : Text to Image: 7.54, 23.364, 35.012, 21.0
2019-02-16 07:55:35,662 : samples : 128000
2019-02-16 07:55:46,016 : Image to text: 8.82, 25.74, 37.7, 18.0
2019-02-16 07:55:53,420 : Text to Image: 7.4, 22.448, 33.82, 22.0
2019-02-16 07:56:35,972 : samples : 192000
2019-02-16 07:56:48,486 : Image to text: 8.86, 26.0, 38.0, 18.0
2019-02-16 07:56:58,474 : Text to Image: 6.96, 21.692, 32.76, 23.0
2019-02-16 07:57:43,821 : samples : 256000
2019-02-16 07:57:56,457 : Image to text: 8.62, 27.3, 39.48, 17.0
2019-02-16 07:58:05,520 : Text to Image: 7.28, 22.764, 34.104, 22.0
2019-02-16 07:58:47,150 : samples : 320000
2019-02-16 07:58:57,340 : Image to text: 9.44, 27.46, 39.54, 17.0
2019-02-16 07:59:04,679 : Text to Image: 7.792, 23.7, 35.272, 21.0
2019-02-16 07:59:47,720 : samples : 384000
2019-02-16 08:00:00,286 : Image to text: 9.08, 26.7, 38.72, 17.0
2019-02-16 08:00:10,376 : Text to Image: 7.588, 23.468, 34.964, 21.0
2019-02-16 08:00:55,330 : samples : 448000
2019-02-16 08:01:05,632 : Image to text: 8.98, 26.38, 38.22, 18.0
2019-02-16 08:01:12,969 : Text to Image: 7.98, 24.192, 35.8, 21.0
2019-02-16 08:01:57,889 : samples : 512000
2019-02-16 08:02:11,189 : Image to text: 8.86, 26.08, 38.74, 18.0
2019-02-16 08:02:21,637 : Text to Image: 7.516, 22.744, 34.208, 22.0
2019-02-16 08:02:59,958 : Epoch 4 finished
2019-02-16 08:03:00,886 : Image to text: 24.7, 58.5, 73.4, 4.0
2019-02-16 08:03:01,622 : Text to Image: 20.76, 53.8, 71.16, 5.0
2019-02-16 08:03:02,573 : Image to text: 26.6, 57.8, 73.9, 4.0
2019-02-16 08:03:03,313 : Text to Image: 20.76, 53.6, 70.36, 5.0
2019-02-16 08:03:04,221 : Image to text: 26.7, 59.2, 73.6, 4.0
2019-02-16 08:03:05,006 : Text to Image: 21.24, 53.48, 70.9, 5.0
2019-02-16 08:03:05,927 : Image to text: 26.9, 58.6, 74.2, 4.0
2019-02-16 08:03:06,691 : Text to Image: 21.78, 53.82, 70.6, 5.0
2019-02-16 08:03:07,631 : Image to text: 27.1, 59.5, 74.3, 4.0
2019-02-16 08:03:08,438 : Text to Image: 21.2, 53.0, 69.76, 5.0
2019-02-16 08:03:08,438 : Dev mean Text to Image: 21.148000000000003, 53.54, 70.556, 5.0
2019-02-16 08:03:08,438 : Dev mean Image to text: 26.4, 58.71999999999999, 73.88, 4.0
2019-02-16 08:03:08,439 : start epoch
2019-02-16 08:03:50,845 : samples : 64000
2019-02-16 08:04:01,044 : Image to text: 8.96, 26.34, 39.16, 18.0
2019-02-16 08:04:08,407 : Text to Image: 7.664, 23.336, 34.7, 21.0
2019-02-16 08:04:52,052 : samples : 128000
2019-02-16 08:05:04,623 : Image to text: 10.08, 28.26, 40.86, 16.0
2019-02-16 08:05:14,704 : Text to Image: 7.716, 23.768, 35.504, 20.0
2019-02-16 08:05:58,882 : samples : 192000
2019-02-16 08:06:09,129 : Image to text: 9.2, 26.78, 38.6, 18.0
2019-02-16 08:06:16,475 : Text to Image: 7.976, 23.884, 35.76, 20.0
2019-02-16 08:06:59,022 : samples : 256000
2019-02-16 08:07:10,253 : Image to text: 8.82, 26.26, 38.68, 18.0
2019-02-16 08:07:20,241 : Text to Image: 7.932, 24.068, 35.792, 20.0
2019-02-16 08:08:05,496 : samples : 320000
2019-02-16 08:08:18,112 : Image to text: 9.8, 27.62, 39.3, 18.0
2019-02-16 08:08:27,712 : Text to Image: 7.664, 23.476, 35.3, 20.0
2019-02-16 08:09:10,130 : samples : 384000
2019-02-16 08:09:20,412 : Image to text: 9.46, 27.52, 39.58, 18.0
2019-02-16 08:09:27,523 : Text to Image: 7.84, 23.924, 35.88, 20.0
2019-02-16 08:10:11,692 : samples : 448000
2019-02-16 08:10:24,309 : Image to text: 10.06, 27.72, 39.46, 17.0
2019-02-16 08:10:34,356 : Text to Image: 7.732, 23.508, 35.316, 20.0
2019-02-16 08:11:17,854 : samples : 512000
2019-02-16 08:11:28,126 : Image to text: 9.48, 27.62, 40.4, 16.0
2019-02-16 08:11:35,534 : Text to Image: 8.252, 24.6, 36.44, 19.0
2019-02-16 08:12:11,959 : Epoch 5 finished
2019-02-16 08:12:12,904 : Image to text: 24.9, 57.4, 71.9, 4.0
2019-02-16 08:12:13,646 : Text to Image: 20.7, 53.62, 70.46, 5.0
2019-02-16 08:12:14,586 : Image to text: 25.6, 56.5, 72.9, 4.0
2019-02-16 08:12:15,371 : Text to Image: 21.94, 53.8, 70.9, 5.0
2019-02-16 08:12:16,288 : Image to text: 26.6, 58.1, 73.8, 4.0
2019-02-16 08:12:17,026 : Text to Image: 21.7, 53.98, 71.44, 5.0
2019-02-16 08:12:17,960 : Image to text: 26.7, 58.7, 74.9, 4.0
2019-02-16 08:12:18,731 : Text to Image: 21.26, 53.84, 70.74, 5.0
2019-02-16 08:12:19,674 : Image to text: 26.9, 59.9, 75.2, 4.0
2019-02-16 08:12:20,458 : Text to Image: 21.44, 53.08, 70.34, 5.0
2019-02-16 08:12:20,458 : Dev mean Text to Image: 21.407999999999998, 53.664, 70.776, 5.0
2019-02-16 08:12:20,458 : Dev mean Image to text: 26.139999999999997, 58.120000000000005, 73.74000000000001, 4.0
2019-02-16 08:12:20,458 : start epoch
2019-02-16 08:13:04,910 : samples : 64000
2019-02-16 08:13:17,469 : Image to text: 10.58, 28.24, 40.84, 16.0
2019-02-16 08:13:27,545 : Text to Image: 8.328, 24.912, 36.58, 20.0
2019-02-16 08:14:10,327 : samples : 128000
2019-02-16 08:14:20,537 : Image to text: 9.16, 26.94, 39.72, 17.0
2019-02-16 08:14:27,912 : Text to Image: 7.796, 23.924, 35.356, 20.0
2019-02-16 08:15:11,395 : samples : 192000
2019-02-16 08:15:23,981 : Image to text: 9.68, 27.38, 39.52, 17.0
2019-02-16 08:15:34,049 : Text to Image: 7.784, 23.62, 35.472, 21.0
2019-02-16 08:16:19,279 : samples : 256000
2019-02-16 08:16:29,581 : Image to text: 9.64, 27.36, 39.72, 17.0
2019-02-16 08:16:37,006 : Text to Image: 8.22, 24.148, 35.844, 20.0
2019-02-16 08:17:19,692 : samples : 320000
2019-02-16 08:17:32,264 : Image to text: 9.0, 27.24, 39.22, 17.0
2019-02-16 08:17:42,314 : Text to Image: 8.04, 24.26, 36.072, 20.0
2019-02-16 08:18:27,814 : samples : 384000
2019-02-16 08:18:40,636 : Image to text: 10.0, 28.34, 40.8, 16.0
2019-02-16 08:18:50,929 : Text to Image: 8.396, 25.268, 37.084, 19.0
2019-02-16 08:19:38,642 : samples : 448000
2019-02-16 08:19:48,850 : Image to text: 9.52, 28.12, 40.34, 16.0
2019-02-16 08:19:56,308 : Text to Image: 7.644, 23.9, 35.736, 20.0
2019-02-16 08:20:39,179 : samples : 512000
2019-02-16 08:20:49,890 : Image to text: 10.4, 28.78, 41.34, 16.0
2019-02-16 08:20:59,542 : Text to Image: 8.564, 25.46, 37.328, 19.0
2019-02-16 08:21:36,320 : Epoch 6 finished
2019-02-16 08:21:36,804 : Image to text: 24.4, 58.1, 72.4, 4.0
2019-02-16 08:21:37,166 : Text to Image: 21.34, 55.42, 72.8, 4.0
2019-02-16 08:21:37,617 : Image to text: 25.6, 57.1, 74.1, 4.0
2019-02-16 08:21:37,979 : Text to Image: 21.78, 55.12, 71.78, 5.0
2019-02-16 08:21:38,438 : Image to text: 24.3, 57.3, 73.8, 4.0
2019-02-16 08:21:38,805 : Text to Image: 22.38, 55.6, 72.66, 4.0
2019-02-16 08:21:39,271 : Image to text: 26.7, 59.1, 74.1, 4.0
2019-02-16 08:21:39,637 : Text to Image: 22.38, 54.92, 72.3, 5.0
2019-02-16 08:21:40,105 : Image to text: 26.4, 58.6, 73.4, 4.0
2019-02-16 08:21:40,481 : Text to Image: 21.8, 54.5, 70.94, 5.0
2019-02-16 08:21:40,481 : Dev mean Text to Image: 21.935999999999996, 55.111999999999995, 72.09599999999999, 4.6
2019-02-16 08:21:40,481 : Dev mean Image to text: 25.479999999999997, 58.04, 73.56, 4.0
2019-02-16 08:21:40,481 : start epoch
2019-02-16 08:22:22,887 : samples : 64000
2019-02-16 08:22:33,093 : Image to text: 10.14, 28.62, 40.94, 16.0
2019-02-16 08:22:40,543 : Text to Image: 8.272, 25.232, 36.916, 19.0
2019-02-16 08:23:23,300 : samples : 128000
2019-02-16 08:23:34,179 : Image to text: 9.9, 28.44, 40.68, 16.0
2019-02-16 08:23:41,601 : Text to Image: 8.3, 24.88, 36.804, 19.0
2019-02-16 08:24:24,144 : samples : 192000
2019-02-16 08:24:34,417 : Image to text: 10.18, 28.8, 41.72, 15.0
2019-02-16 08:24:41,826 : Text to Image: 8.308, 25.04, 37.148, 19.0
2019-02-16 08:25:25,405 : samples : 256000
2019-02-16 08:25:35,652 : Image to text: 9.68, 27.64, 40.68, 16.0
2019-02-16 08:25:43,065 : Text to Image: 8.452, 25.436, 37.36, 19.0
2019-02-16 08:26:25,809 : samples : 320000
2019-02-16 08:26:36,129 : Image to text: 10.52, 28.84, 40.78, 16.0
2019-02-16 08:26:43,524 : Text to Image: 8.888, 25.68, 37.28, 19.0
2019-02-16 08:27:27,150 : samples : 384000
2019-02-16 08:27:39,821 : Image to text: 10.2, 28.5, 41.04, 15.0
2019-02-16 08:27:49,980 : Text to Image: 8.82, 26.088, 38.456, 18.0
2019-02-16 08:28:32,877 : samples : 448000
2019-02-16 08:28:44,258 : Image to text: 9.98, 29.02, 41.52, 15.0
2019-02-16 08:28:52,355 : Text to Image: 8.788, 26.172, 38.172, 18.0
2019-02-16 08:29:35,689 : samples : 512000
2019-02-16 08:29:48,100 : Image to text: 10.8, 29.48, 42.76, 15.0
2019-02-16 08:29:55,770 : Text to Image: 9.132, 26.48, 38.988, 18.0
2019-02-16 08:30:32,408 : Epoch 7 finished
2019-02-16 08:30:32,801 : Image to text: 26.0, 57.1, 72.7, 4.0
2019-02-16 08:30:33,090 : Text to Image: 21.24, 54.28, 71.88, 5.0
2019-02-16 08:30:33,662 : Image to text: 25.7, 57.6, 74.1, 4.0
2019-02-16 08:30:34,448 : Text to Image: 21.18, 53.54, 71.12, 5.0
2019-02-16 08:30:35,380 : Image to text: 25.0, 59.9, 73.5, 4.0
2019-02-16 08:30:36,209 : Text to Image: 21.24, 54.9, 70.54, 5.0
2019-02-16 08:30:37,205 : Image to text: 26.6, 61.9, 75.5, 4.0
2019-02-16 08:30:38,081 : Text to Image: 21.46, 54.56, 71.68, 5.0
2019-02-16 08:30:39,053 : Image to text: 27.0, 58.6, 73.0, 4.0
2019-02-16 08:30:39,826 : Text to Image: 20.32, 53.5, 70.62, 5.0
2019-02-16 08:30:39,826 : Dev mean Text to Image: 21.087999999999997, 54.15599999999999, 71.16799999999999, 5.0
2019-02-16 08:30:39,826 : Dev mean Image to text: 26.060000000000002, 59.019999999999996, 73.76, 4.0
2019-02-16 08:30:39,826 : start epoch
2019-02-16 08:31:22,495 : samples : 64000
2019-02-16 08:31:34,640 : Image to text: 10.64, 28.76, 42.18, 15.0
2019-02-16 08:31:44,601 : Text to Image: 8.336, 25.484, 37.328, 19.0
2019-02-16 08:32:29,487 : samples : 128000
2019-02-16 08:32:42,026 : Image to text: 10.64, 29.52, 41.68, 15.0
2019-02-16 08:32:52,001 : Text to Image: 8.348, 25.412, 37.476, 19.0
2019-02-16 08:33:36,999 : samples : 192000
2019-02-16 08:33:49,500 : Image to text: 10.32, 29.32, 42.22, 15.0
2019-02-16 08:33:59,469 : Text to Image: 8.536, 25.468, 37.616, 19.0
2019-02-16 08:34:44,701 : samples : 256000
2019-02-16 08:34:57,195 : Image to text: 10.58, 28.94, 41.6, 15.0
2019-02-16 08:35:07,183 : Text to Image: 8.936, 26.192, 37.916, 18.0
2019-02-16 08:35:56,600 : samples : 320000
2019-02-16 08:36:09,667 : Image to text: 11.06, 30.4, 42.76, 14.0
2019-02-16 08:36:19,912 : Text to Image: 9.204, 26.428, 38.46, 17.0
2019-02-16 08:37:04,754 : samples : 384000
2019-02-16 08:37:17,348 : Image to text: 9.82, 28.78, 41.88, 15.0
2019-02-16 08:37:27,346 : Text to Image: 8.828, 25.748, 37.492, 18.0
2019-02-16 08:38:12,479 : samples : 448000
2019-02-16 08:38:25,083 : Image to text: 10.48, 29.76, 42.04, 15.0
2019-02-16 08:38:35,128 : Text to Image: 8.924, 25.548, 37.652, 19.0
2019-02-16 08:39:19,649 : samples : 512000
2019-02-16 08:39:32,249 : Image to text: 10.2, 29.16, 42.06, 15.0
2019-02-16 08:39:42,258 : Text to Image: 8.776, 25.936, 37.748, 18.0
2019-02-16 08:40:20,435 : Epoch 8 finished
2019-02-16 08:40:21,355 : Image to text: 25.7, 59.4, 73.9, 4.0
2019-02-16 08:40:22,125 : Text to Image: 21.38, 54.58, 72.1, 5.0
2019-02-16 08:40:23,041 : Image to text: 24.8, 60.8, 76.4, 4.0
2019-02-16 08:40:23,797 : Text to Image: 21.52, 53.98, 71.98, 5.0
2019-02-16 08:40:24,739 : Image to text: 26.2, 57.8, 74.8, 4.0
2019-02-16 08:40:25,489 : Text to Image: 22.12, 54.96, 71.78, 4.0
2019-02-16 08:40:26,443 : Image to text: 26.9, 59.5, 76.2, 4.0
2019-02-16 08:40:27,199 : Text to Image: 21.4, 54.5, 71.68, 5.0
2019-02-16 08:40:28,117 : Image to text: 27.1, 60.2, 73.7, 4.0
2019-02-16 08:40:28,863 : Text to Image: 22.34, 53.24, 70.46, 5.0
2019-02-16 08:40:28,863 : Dev mean Text to Image: 21.752, 54.251999999999995, 71.6, 4.8
2019-02-16 08:40:28,863 : Dev mean Image to text: 26.14, 59.53999999999999, 75.0, 4.0
2019-02-16 08:40:28,863 : start epoch
2019-02-16 08:41:13,518 : samples : 64000
2019-02-16 08:41:26,055 : Image to text: 11.04, 30.2, 43.16, 14.0
2019-02-16 08:41:36,146 : Text to Image: 8.708, 26.056, 38.084, 18.0
2019-02-16 08:42:21,131 : samples : 128000
2019-02-16 08:42:33,875 : Image to text: 10.54, 29.36, 42.58, 15.0
2019-02-16 08:42:42,900 : Text to Image: 8.96, 25.968, 37.808, 18.0
2019-02-16 08:43:26,615 : samples : 192000
2019-02-16 08:43:36,826 : Image to text: 10.32, 29.46, 42.58, 15.0
2019-02-16 08:43:44,210 : Text to Image: 8.88, 25.944, 38.28, 18.0
2019-02-16 08:44:26,842 : samples : 256000
2019-02-16 08:44:36,914 : Image to text: 10.24, 29.94, 42.68, 15.0
2019-02-16 08:44:43,832 : Text to Image: 8.656, 25.488, 37.708, 18.0
2019-02-16 08:45:29,279 : samples : 320000
2019-02-16 08:45:42,099 : Image to text: 10.9, 29.54, 42.54, 15.0
2019-02-16 08:45:52,474 : Text to Image: 8.604, 25.796, 38.024, 18.0
2019-02-16 08:46:38,716 : samples : 384000
2019-02-16 08:46:51,549 : Image to text: 9.88, 27.96, 40.12, 16.0
2019-02-16 08:47:01,978 : Text to Image: 8.532, 25.756, 37.408, 19.0
2019-02-16 08:47:48,053 : samples : 448000
2019-02-16 08:48:00,931 : Image to text: 10.86, 30.2, 42.4, 15.0
2019-02-16 08:48:11,404 : Text to Image: 8.86, 26.24, 38.552, 18.0
2019-02-16 08:48:57,121 : samples : 512000
2019-02-16 08:49:09,965 : Image to text: 10.92, 29.06, 41.9, 15.0
2019-02-16 08:49:20,474 : Text to Image: 8.22, 25.112, 37.3, 19.0
2019-02-16 08:49:59,525 : Epoch 9 finished
2019-02-16 08:50:00,595 : Image to text: 28.6, 59.6, 76.2, 4.0
2019-02-16 08:50:01,511 : Text to Image: 23.66, 56.96, 73.86, 4.0
2019-02-16 08:50:02,554 : Image to text: 26.8, 59.5, 75.5, 3.0
2019-02-16 08:50:03,502 : Text to Image: 23.04, 56.18, 73.08, 4.0
2019-02-16 08:50:04,622 : Image to text: 28.7, 61.4, 77.8, 4.0
2019-02-16 08:50:05,508 : Text to Image: 23.38, 56.96, 73.46, 4.0
2019-02-16 08:50:06,599 : Image to text: 27.2, 62.4, 76.9, 4.0
2019-02-16 08:50:07,511 : Text to Image: 23.16, 57.0, 73.12, 4.0
2019-02-16 08:50:08,613 : Image to text: 29.6, 61.6, 75.0, 3.0
2019-02-16 08:50:09,469 : Text to Image: 23.72, 57.06, 73.04, 4.0
2019-02-16 08:50:09,469 : Dev mean Text to Image: 23.392, 56.831999999999994, 73.312, 4.0
2019-02-16 08:50:09,469 : Dev mean Image to text: 28.18, 60.9, 76.28, 3.6
2019-02-16 08:50:09,469 : start epoch
2019-02-16 08:50:55,158 : samples : 64000
2019-02-16 08:51:08,024 : Image to text: 10.38, 29.46, 41.4, 15.0
2019-02-16 08:51:18,457 : Text to Image: 8.196, 25.016, 37.176, 19.0
2019-02-16 08:52:04,425 : samples : 128000
2019-02-16 08:52:16,401 : Image to text: 11.34, 29.64, 42.16, 15.0
2019-02-16 08:52:26,810 : Text to Image: 9.116, 26.532, 38.996, 18.0
2019-02-16 08:53:19,845 : samples : 192000
2019-02-16 08:53:30,322 : Image to text: 11.06, 30.02, 43.12, 14.0
2019-02-16 08:53:37,845 : Text to Image: 8.828, 26.496, 38.684, 18.0
2019-02-16 08:54:20,397 : samples : 256000
2019-02-16 08:54:30,876 : Image to text: 10.46, 29.64, 42.86, 15.0
2019-02-16 08:54:38,440 : Text to Image: 8.644, 25.9, 37.96, 18.0
2019-02-16 08:55:20,736 : samples : 320000
2019-02-16 08:55:31,218 : Image to text: 10.64, 28.62, 41.26, 16.0
2019-02-16 08:55:38,787 : Text to Image: 8.24, 25.232, 37.22, 19.0
2019-02-16 08:56:21,203 : samples : 384000
2019-02-16 08:56:31,627 : Image to text: 10.7, 30.16, 42.48, 14.0
2019-02-16 08:56:39,169 : Text to Image: 9.052, 26.468, 38.616, 18.0
2019-02-16 08:57:21,584 : samples : 448000
2019-02-16 08:57:32,091 : Image to text: 10.7, 30.34, 43.8, 14.0
2019-02-16 08:57:39,653 : Text to Image: 9.124, 26.732, 38.884, 18.0
2019-02-16 08:58:22,238 : samples : 512000
2019-02-16 08:58:32,707 : Image to text: 10.84, 29.88, 42.58, 15.0
2019-02-16 08:58:40,249 : Text to Image: 8.824, 26.188, 38.588, 18.0
2019-02-16 08:59:16,477 : Epoch 10 finished
2019-02-16 08:59:16,901 : Image to text: 27.2, 60.7, 76.5, 4.0
2019-02-16 08:59:17,232 : Text to Image: 23.24, 57.64, 74.8, 4.0
2019-02-16 08:59:17,675 : Image to text: 28.2, 61.9, 75.6, 3.0
2019-02-16 08:59:18,006 : Text to Image: 22.2, 55.96, 73.92, 4.0
2019-02-16 08:59:18,446 : Image to text: 28.4, 61.9, 77.5, 4.0
2019-02-16 08:59:18,785 : Text to Image: 23.62, 56.92, 73.58, 4.0
2019-02-16 08:59:19,248 : Image to text: 27.9, 62.7, 77.6, 3.0
2019-02-16 08:59:19,588 : Text to Image: 24.04, 57.16, 72.78, 4.0
2019-02-16 08:59:20,041 : Image to text: 29.7, 62.5, 76.9, 3.0
2019-02-16 08:59:20,382 : Text to Image: 23.78, 56.84, 72.76, 4.0
2019-02-16 08:59:20,382 : Dev mean Text to Image: 23.375999999999998, 56.904, 73.56800000000001, 4.0
2019-02-16 08:59:20,382 : Dev mean Image to text: 28.279999999999994, 61.94, 76.82, 3.4000000000000004
2019-02-16 08:59:20,382 : start epoch
2019-02-16 09:00:02,911 : samples : 64000
2019-02-16 09:00:13,466 : Image to text: 11.18, 30.4, 42.32, 15.0
2019-02-16 09:00:21,110 : Text to Image: 8.684, 26.392, 38.348, 18.0
2019-02-16 09:01:03,404 : samples : 128000
2019-02-16 09:01:13,897 : Image to text: 11.22, 30.2, 43.08, 15.0
2019-02-16 09:01:21,404 : Text to Image: 8.896, 26.196, 38.46, 17.0
2019-02-16 09:02:05,007 : samples : 192000
2019-02-16 09:02:15,493 : Image to text: 11.54, 30.18, 43.1, 14.0
2019-02-16 09:02:23,008 : Text to Image: 8.924, 25.988, 38.412, 18.0
2019-02-16 09:03:05,587 : samples : 256000
2019-02-16 09:03:16,046 : Image to text: 11.02, 29.54, 42.5, 15.0
2019-02-16 09:03:23,569 : Text to Image: 9.016, 26.544, 38.872, 17.0
2019-02-16 09:04:05,970 : samples : 320000
2019-02-16 09:04:16,500 : Image to text: 11.06, 30.0, 43.76, 14.0
2019-02-16 09:04:24,050 : Text to Image: 8.62, 26.212, 38.26, 18.0
2019-02-16 09:05:06,475 : samples : 384000
2019-02-16 09:05:17,064 : Image to text: 10.86, 31.1, 43.28, 14.0
2019-02-16 09:05:24,648 : Text to Image: 9.132, 26.6, 38.948, 17.0
2019-02-16 09:06:07,669 : samples : 448000
2019-02-16 09:06:18,137 : Image to text: 11.22, 30.8, 43.28, 14.0
2019-02-16 09:06:25,684 : Text to Image: 9.524, 27.148, 39.508, 17.0
2019-02-16 09:07:08,101 : samples : 512000
2019-02-16 09:07:18,605 : Image to text: 10.68, 30.42, 43.44, 14.0
2019-02-16 09:07:26,156 : Text to Image: 9.032, 26.42, 38.776, 18.0
2019-02-16 09:08:02,185 : Epoch 11 finished
2019-02-16 09:08:02,626 : Image to text: 26.9, 57.9, 74.6, 4.0
2019-02-16 09:08:02,967 : Text to Image: 22.64, 56.38, 74.0, 4.0
2019-02-16 09:08:03,409 : Image to text: 27.8, 58.3, 75.9, 4.0
2019-02-16 09:08:03,764 : Text to Image: 22.96, 56.16, 72.66, 4.0
2019-02-16 09:08:04,204 : Image to text: 27.3, 61.1, 75.8, 4.0
2019-02-16 09:08:04,545 : Text to Image: 22.16, 55.94, 73.26, 4.0
2019-02-16 09:08:04,985 : Image to text: 27.8, 62.1, 76.0, 4.0
2019-02-16 09:08:05,329 : Text to Image: 22.76, 55.86, 73.38, 4.0
2019-02-16 09:08:05,792 : Image to text: 29.7, 62.0, 76.0, 3.0
2019-02-16 09:08:06,132 : Text to Image: 22.92, 55.82, 72.54, 4.0
2019-02-16 09:08:06,132 : Dev mean Text to Image: 22.688000000000002, 56.032, 73.16799999999999, 4.0
2019-02-16 09:08:06,132 : Dev mean Image to text: 27.9, 60.28, 75.66000000000001, 3.8000000000000003
2019-02-16 09:08:06,132 : start epoch
2019-02-16 09:08:49,122 : samples : 64000
2019-02-16 09:08:59,644 : Image to text: 10.76, 29.76, 42.66, 15.0
2019-02-16 09:09:07,186 : Text to Image: 8.888, 26.628, 38.752, 18.0
2019-02-16 09:09:58,093 : samples : 128000
2019-02-16 09:10:09,666 : Image to text: 10.46, 30.06, 42.98, 14.0
2019-02-16 09:10:17,201 : Text to Image: 8.916, 26.416, 38.5, 18.0
2019-02-16 09:10:59,405 : samples : 192000
2019-02-16 09:11:09,894 : Image to text: 11.18, 30.68, 44.2, 14.0
2019-02-16 09:11:17,477 : Text to Image: 9.588, 27.26, 39.612, 17.0
2019-02-16 09:12:00,677 : samples : 256000
2019-02-16 09:12:11,169 : Image to text: 11.24, 30.68, 43.54, 14.0
2019-02-16 09:12:18,715 : Text to Image: 8.928, 26.54, 38.852, 17.0
2019-02-16 09:13:02,187 : samples : 320000
2019-02-16 09:13:12,663 : Image to text: 10.8, 29.8, 42.5, 15.0
2019-02-16 09:13:20,216 : Text to Image: 8.524, 25.736, 37.6, 19.0
2019-02-16 09:14:02,992 : samples : 384000
2019-02-16 09:14:13,525 : Image to text: 11.04, 30.62, 43.26, 14.0
2019-02-16 09:14:21,128 : Text to Image: 9.536, 27.188, 39.58, 17.0
2019-02-16 09:15:04,070 : samples : 448000
2019-02-16 09:15:14,581 : Image to text: 11.78, 31.1, 44.46, 14.0
2019-02-16 09:15:22,158 : Text to Image: 9.608, 27.384, 39.968, 17.0
2019-02-16 09:16:04,623 : samples : 512000
2019-02-16 09:16:15,163 : Image to text: 11.24, 29.86, 42.92, 14.0
2019-02-16 09:16:22,713 : Text to Image: 9.016, 26.652, 38.9, 17.0
2019-02-16 09:16:58,879 : Epoch 12 finished
2019-02-16 09:16:59,329 : Image to text: 25.8, 59.1, 74.4, 4.0
2019-02-16 09:16:59,665 : Text to Image: 22.76, 56.74, 73.8, 4.0
2019-02-16 09:17:00,101 : Image to text: 28.4, 59.7, 76.6, 3.0
2019-02-16 09:17:00,436 : Text to Image: 23.16, 56.3, 73.76, 4.0
2019-02-16 09:17:00,887 : Image to text: 27.9, 62.2, 77.2, 4.0
2019-02-16 09:17:01,229 : Text to Image: 23.44, 57.12, 73.76, 4.0
2019-02-16 09:17:01,701 : Image to text: 28.1, 62.6, 77.0, 3.0
2019-02-16 09:17:02,043 : Text to Image: 23.72, 56.48, 73.3, 4.0
2019-02-16 09:17:02,499 : Image to text: 29.3, 61.0, 76.6, 3.0
2019-02-16 09:17:02,840 : Text to Image: 22.64, 56.32, 73.34, 4.0
2019-02-16 09:17:02,840 : Dev mean Text to Image: 23.144, 56.592, 73.59200000000001, 4.0
2019-02-16 09:17:02,840 : Dev mean Image to text: 27.900000000000002, 60.92, 76.36, 3.4000000000000004
2019-02-16 09:17:06,835 : 
Test scores | Image to text:             27.42, 61.02, 76.26, 3.8000000000000003
2019-02-16 09:17:06,836 : Test scores | Text to image:             22.683999999999997, 55.800000000000004, 72.892, 4.2

2019-02-16 09:17:06,950 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 09:17:07,337 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 09:17:08,046 : loading BERT model bert-base-uncased
2019-02-16 09:17:08,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:17:08,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:17:08,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp0fc45ha
2019-02-16 09:17:10,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:17:12,061 : Computing embeddings for train/dev/test
2019-02-16 09:18:48,236 : Computed embeddings
2019-02-16 09:18:48,236 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:19:39,780 : [('reg:1e-05', 74.33), ('reg:0.0001', 74.29), ('reg:0.001', 65.36), ('reg:0.01', 49.42)]
2019-02-16 09:19:39,781 : Validation : best param found is reg = 1e-05 with score             74.33
2019-02-16 09:19:39,781 : Evaluating...
2019-02-16 09:19:50,480 : 
Dev acc : 74.3 Test acc : 75.3 for LENGTH classification

2019-02-16 09:19:50,481 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 09:19:50,849 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 09:19:50,903 : loading BERT model bert-base-uncased
2019-02-16 09:19:50,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:19:50,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:19:50,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_qkj4zs_
2019-02-16 09:19:53,457 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:19:54,949 : Computing embeddings for train/dev/test
2019-02-16 09:21:23,581 : Computed embeddings
2019-02-16 09:21:23,581 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:22:20,146 : [('reg:1e-05', 15.08), ('reg:0.0001', 2.25), ('reg:0.001', 0.62), ('reg:0.01', 0.23)]
2019-02-16 09:22:20,147 : Validation : best param found is reg = 1e-05 with score             15.08
2019-02-16 09:22:20,147 : Evaluating...
2019-02-16 09:22:39,274 : 
Dev acc : 15.1 Test acc : 15.1 for WORDCONTENT classification

2019-02-16 09:22:39,276 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 09:22:39,640 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 09:22:39,706 : loading BERT model bert-base-uncased
2019-02-16 09:22:39,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:22:39,801 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:22:39,802 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv7o_yk4v
2019-02-16 09:22:42,313 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:22:43,812 : Computing embeddings for train/dev/test
2019-02-16 09:24:06,354 : Computed embeddings
2019-02-16 09:24:06,354 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:24:50,729 : [('reg:1e-05', 29.87), ('reg:0.0001', 24.71), ('reg:0.001', 27.54), ('reg:0.01', 23.65)]
2019-02-16 09:24:50,729 : Validation : best param found is reg = 1e-05 with score             29.87
2019-02-16 09:24:50,729 : Evaluating...
2019-02-16 09:25:01,303 : 
Dev acc : 29.9 Test acc : 29.3 for DEPTH classification

2019-02-16 09:25:01,304 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 09:25:01,717 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 09:25:01,787 : loading BERT model bert-base-uncased
2019-02-16 09:25:01,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:25:01,917 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:25:01,917 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvlnu9ju1
2019-02-16 09:25:04,417 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:25:05,861 : Computing embeddings for train/dev/test
2019-02-16 09:26:24,481 : Computed embeddings
2019-02-16 09:26:24,481 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:26:54,107 : [('reg:1e-05', 72.41), ('reg:0.0001', 71.15), ('reg:0.001', 63.99), ('reg:0.01', 53.5)]
2019-02-16 09:26:54,107 : Validation : best param found is reg = 1e-05 with score             72.41
2019-02-16 09:26:54,107 : Evaluating...
2019-02-16 09:27:01,196 : 
Dev acc : 72.4 Test acc : 73.0 for TOPCONSTITUENTS classification

2019-02-16 09:27:01,198 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 09:27:01,886 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 09:27:01,973 : loading BERT model bert-base-uncased
2019-02-16 09:27:01,973 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:27:02,017 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:27:02,017 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcpx9jaff
2019-02-16 09:27:05,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:27:07,023 : Computing embeddings for train/dev/test
2019-02-16 09:28:31,017 : Computed embeddings
2019-02-16 09:28:31,018 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:29:24,587 : [('reg:1e-05', 81.45), ('reg:0.0001', 81.3), ('reg:0.001', 80.92), ('reg:0.01', 77.32)]
2019-02-16 09:29:24,587 : Validation : best param found is reg = 1e-05 with score             81.45
2019-02-16 09:29:24,587 : Evaluating...
2019-02-16 09:29:39,447 : 
Dev acc : 81.5 Test acc : 80.6 for BIGRAMSHIFT classification

2019-02-16 09:29:39,448 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 09:29:39,877 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 09:29:39,945 : loading BERT model bert-base-uncased
2019-02-16 09:29:39,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:29:39,975 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:29:39,975 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp945m9lyn
2019-02-16 09:29:42,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:29:43,988 : Computing embeddings for train/dev/test
2019-02-16 09:31:06,518 : Computed embeddings
2019-02-16 09:31:06,518 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:31:51,883 : [('reg:1e-05', 90.27), ('reg:0.0001', 90.4), ('reg:0.001', 90.35), ('reg:0.01', 89.97)]
2019-02-16 09:31:51,883 : Validation : best param found is reg = 0.0001 with score             90.4
2019-02-16 09:31:51,883 : Evaluating...
2019-02-16 09:32:00,694 : 
Dev acc : 90.4 Test acc : 89.0 for TENSE classification

2019-02-16 09:32:00,696 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 09:32:01,107 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 09:32:01,176 : loading BERT model bert-base-uncased
2019-02-16 09:32:01,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:32:01,299 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:32:01,300 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkyf2gt26
2019-02-16 09:32:03,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:32:05,294 : Computing embeddings for train/dev/test
2019-02-16 09:33:32,148 : Computed embeddings
2019-02-16 09:33:32,148 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:34:15,131 : [('reg:1e-05', 87.45), ('reg:0.0001', 87.39), ('reg:0.001', 87.34), ('reg:0.01', 84.03)]
2019-02-16 09:34:15,131 : Validation : best param found is reg = 1e-05 with score             87.45
2019-02-16 09:34:15,131 : Evaluating...
2019-02-16 09:34:28,008 : 
Dev acc : 87.5 Test acc : 88.4 for SUBJNUMBER classification

2019-02-16 09:34:28,009 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 09:34:28,462 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 09:34:28,537 : loading BERT model bert-base-uncased
2019-02-16 09:34:28,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:34:28,672 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:34:28,672 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp18476sq7
2019-02-16 09:34:31,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:34:32,666 : Computing embeddings for train/dev/test
2019-02-16 09:35:58,470 : Computed embeddings
2019-02-16 09:35:58,470 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:36:46,430 : [('reg:1e-05', 76.95), ('reg:0.0001', 76.94), ('reg:0.001', 76.43), ('reg:0.01', 73.91)]
2019-02-16 09:36:46,430 : Validation : best param found is reg = 1e-05 with score             76.95
2019-02-16 09:36:46,430 : Evaluating...
2019-02-16 09:36:58,736 : 
Dev acc : 77.0 Test acc : 78.4 for OBJNUMBER classification

2019-02-16 09:36:58,737 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 09:36:59,351 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 09:36:59,425 : loading BERT model bert-base-uncased
2019-02-16 09:36:59,425 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:36:59,457 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:36:59,457 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpelma7xmc
2019-02-16 09:37:01,997 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:37:03,502 : Computing embeddings for train/dev/test
2019-02-16 09:38:41,606 : Computed embeddings
2019-02-16 09:38:41,606 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:39:17,708 : [('reg:1e-05', 56.8), ('reg:0.0001', 56.7), ('reg:0.001', 56.39), ('reg:0.01', 55.95)]
2019-02-16 09:39:17,709 : Validation : best param found is reg = 1e-05 with score             56.8
2019-02-16 09:39:17,709 : Evaluating...
2019-02-16 09:39:26,406 : 
Dev acc : 56.8 Test acc : 56.7 for ODDMANOUT classification

2019-02-16 09:39:26,407 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 09:39:26,870 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 09:39:26,954 : loading BERT model bert-base-uncased
2019-02-16 09:39:26,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:39:26,991 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:39:26,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo1e2erkw
2019-02-16 09:39:29,486 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:39:30,981 : Computing embeddings for train/dev/test
2019-02-16 09:41:08,011 : Computed embeddings
2019-02-16 09:41:08,011 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:41:56,145 : [('reg:1e-05', 66.27), ('reg:0.0001', 66.19), ('reg:0.001', 62.86), ('reg:0.01', 57.53)]
2019-02-16 09:41:56,145 : Validation : best param found is reg = 1e-05 with score             66.27
2019-02-16 09:41:56,146 : Evaluating...
2019-02-16 09:42:09,523 : 
Dev acc : 66.3 Test acc : 64.8 for COORDINATIONINVERSION classification

2019-02-16 09:42:09,525 : total results: {'STS12': {'MSRpar': {'pearson': (0.22644315917116492, 3.5480762836370453e-10), 'spearman': SpearmanrResult(correlation=0.27581296444987563, pvalue=1.4654842594850125e-14), 'nsamples': 750}, 'MSRvid': {'pearson': (0.1589473440802136, 1.2225639644218391e-05), 'spearman': SpearmanrResult(correlation=0.1728871942556177, pvalue=1.909985864879762e-06), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4183465670228753, 7.143612428553414e-21), 'spearman': SpearmanrResult(correlation=0.5189394895707813, pvalue=5.2327305455121144e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3270301022648735, 3.743922696161727e-20), 'spearman': SpearmanrResult(correlation=0.32219457352015296, pvalue=1.4112917025617514e-19), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5911874617434961, 5.7251373853490225e-39), 'spearman': SpearmanrResult(correlation=0.4420374383198061, pvalue=1.6232538320249476e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3443909268565247, 'wmean': 0.30959469936819295}, 'spearman': {'mean': 0.34637433202324674, 'wmean': 0.3194135176228526}}}, 'STS13': {'FNWN': {'pearson': (0.09604012901354853, 0.1886392033239116), 'spearman': SpearmanrResult(correlation=0.12430979654199671, pvalue=0.08833315252453024), 'nsamples': 189}, 'headlines': {'pearson': (0.4523842573878113, 4.126706886004628e-39), 'spearman': SpearmanrResult(correlation=0.4445833042451607, pvalue=1.111169606462235e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.18981638732686118, 5.980497933467232e-06), 'spearman': SpearmanrResult(correlation=0.19396663787131116, pvalue=3.692828496610767e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.24608025790940702, 'wmean': 0.30928451380985883}, 'spearman': {'mean': 0.25428657955282286, 'wmean': 0.3104982090507423}}}, 'STS14': {'deft-forum': {'pearson': (-0.0012185609080442483, 0.979434653816075), 'spearman': SpearmanrResult(correlation=0.006950917783066193, pvalue=0.883098160492497), 'nsamples': 450}, 'deft-news': {'pearson': (0.45206906923250445, 1.6219908196902618e-16), 'spearman': SpearmanrResult(correlation=0.49362929403146005, pvalue=7.846594189226638e-20), 'nsamples': 300}, 'headlines': {'pearson': (0.4028429891878525, 1.2452935043936456e-30), 'spearman': SpearmanrResult(correlation=0.37957967156308564, pvalue=4.092945164089396e-27), 'nsamples': 750}, 'images': {'pearson': (0.23719583125622956, 4.7312610827836906e-11), 'spearman': SpearmanrResult(correlation=0.24974986548316558, pvalue=3.962279770221814e-12), 'nsamples': 750}, 'OnWN': {'pearson': (0.33576753059717507, 3.2020611293876683e-21), 'spearman': SpearmanrResult(correlation=0.34049106342909974, pvalue=8.197335631888469e-22), 'nsamples': 750}, 'tweet-news': {'pearson': (0.39874764747316394, 5.431230105697154e-30), 'spearman': SpearmanrResult(correlation=0.3799073004941535, pvalue=3.668099186916723e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30423408447314687, 'wmean': 0.31093009793251924}, 'spearman': {'mean': 0.30838468546400516, 'wmean': 0.31027003385038565}}}, 'STS15': {'answers-forums': {'pearson': (0.21371064904459963, 3.0063921097385346e-05), 'spearman': SpearmanrResult(correlation=0.22751096076365193, pvalue=8.606733234190372e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.47647007399030844, 9.099191628638345e-44), 'spearman': SpearmanrResult(correlation=0.48381587845964097, pvalue=2.90636711688961e-45), 'nsamples': 750}, 'belief': {'pearson': (0.3384199633606691, 1.6822963500143915e-11), 'spearman': SpearmanrResult(correlation=0.4114679522827258, pvalue=9.369631103133765e-17), 'nsamples': 375}, 'headlines': {'pearson': (0.4803568992561015, 1.486442265486683e-44), 'spearman': SpearmanrResult(correlation=0.48338073045470814, pvalue=3.5724439238729087e-45), 'nsamples': 750}, 'images': {'pearson': (0.11131475857455615, 0.0022665048250826306), 'spearman': SpearmanrResult(correlation=0.15424074058129297, pvalue=2.2111906936073685e-05), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.324054468845247, 'wmean': 0.3360517595059001}, 'spearman': {'mean': 0.352083252508404, 'wmean': 0.3602317015047078}}}, 'STS16': {'answer-answer': {'pearson': (0.37748665111646185, 5.033922181274851e-10), 'spearman': SpearmanrResult(correlation=0.42703845188957035, pvalue=1.110939834219107e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.5445329104006258, 1.2617493103773368e-20), 'spearman': SpearmanrResult(correlation=0.5538113945152924, pvalue=2.0471728473334675e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6028783551877503, 3.758114259132284e-24), 'spearman': SpearmanrResult(correlation=0.6481181277110284, pvalue=8.593878525428387e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.5882946343439598, 4.114500728710288e-24), 'spearman': SpearmanrResult(correlation=0.7422463101674335, pvalue=5.856585065457803e-44), 'nsamples': 244}, 'question-question': {'pearson': (0.1718040766589299, 0.012869167475419817), 'spearman': SpearmanrResult(correlation=0.2521416813114171, pvalue=0.0002305110624044935), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.45699932554154554, 'wmean': 0.4633923006476916}, 'spearman': {'mean': 0.5246711931189483, 'wmean': 0.5305562263598104}}}, 'MR': {'devacc': 71.37, 'acc': 70.79, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.35, 'acc': 70.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.03, 'acc': 86.19, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.84, 'acc': 93.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.36, 'acc': 77.81, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.42, 'acc': 41.49, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.97, 'acc': 86.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.39, 'acc': 71.88, 'f1': 81.17, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 72.17, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7423340884408229, 'pearson': 0.7209892196358876, 'spearman': 0.6576542740719158, 'mse': 0.4989185742570675, 'yhat': array([2.5462301 , 3.8804357 , 2.45749664, ..., 2.97158585, 4.59071346,        4.78771255]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5317967869206648, 'pearson': 0.5121652098439453, 'spearman': 0.5074582556323265, 'mse': 1.817709283474579, 'yhat': array([2.51224466, 2.04294173, 3.26640688, ..., 3.7063257 , 3.69458564,        3.51113953]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.47, 'acc': 58.46, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 320.88800000000003, 'acc': [(27.42, 61.02, 76.26, 3.8000000000000003), (22.683999999999997, 55.800000000000004, 72.892, 4.2)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 74.33, 'acc': 75.28, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 15.08, 'acc': 15.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.87, 'acc': 29.32, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.41, 'acc': 73.05, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.45, 'acc': 80.62, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.4, 'acc': 89.0, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.45, 'acc': 88.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.95, 'acc': 78.38, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.8, 'acc': 56.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.27, 'acc': 64.81, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 09:42:09,525 : STS12 p=0.3096, STS12 s=0.3194, STS13 p=0.3093, STS13 s=0.3105, STS14 p=0.3109, STS14 s=0.3103, STS15 p=0.3361, STS15 s=0.3602, STS 16 p=0.4634, STS16 s=0.5306, STS B p=0.5122, STS B s=0.5075, STS B m=1.8177, SICK-R p=0.7210, SICK-R s=0.6577, SICK-P m=0.4989
2019-02-16 09:42:09,525 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 09:42:09,525 : 0.3096,0.3194,0.3093,0.3105,0.3109,0.3103,0.3361,0.3602,0.4634,0.5306,0.5122,0.5075,1.8177,0.7210,0.6577,0.4989
2019-02-16 09:42:09,525 : MR=70.79, CR=70.57, SUBJ=93.85, MPQA=86.19, SST-B=77.81, SST-F=41.49, TREC=86.00, SICK-E=72.17, SNLI=58.46, MRPC=71.88, MRPC f=81.17
2019-02-16 09:42:09,525 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 09:42:09,525 : 70.79,70.57,93.85,86.19,77.81,41.49,86.00,72.17,58.46,71.88,81.17
2019-02-16 09:42:09,525 : COCO r1i2t=27.42, COCO r5i2t=61.02, COCO r10i2t=76.26, COCO medr_i2t=3.80, COCO r1t2i=22.68, COCO r5t2i=55.80, COCO r10t2i=72.89, COCO medr_t2i=4.20
2019-02-16 09:42:09,525 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 09:42:09,525 : 27.42,61.02,76.26,3.80,22.68,55.80,72.89,4.20
2019-02-16 09:42:09,525 : SentLen=75.28, WC=15.06, TreeDepth=29.32, TopConst=73.05, BShift=80.62, Tense=89.00, SubjNum=88.42, ObjNum=78.38, SOMO=56.69, CoordInv=64.81, average=65.06
2019-02-16 09:42:09,525 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 09:42:09,525 : 75.28,15.06,29.32,73.05,80.62,89.00,88.42,78.38,56.69,64.81,65.06
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,525 : layer 7
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,525 : ********************************************************************************
2019-02-16 09:42:09,625 : ***** Transfer task : STS12 *****


2019-02-16 09:42:09,638 : loading BERT model bert-base-uncased
2019-02-16 09:42:09,638 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:09,658 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:09,658 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcv7nqdu0
2019-02-16 09:42:12,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:15,633 : MSRpar : pearson = 0.2264, spearman = 0.2787
2019-02-16 09:42:16,471 : MSRvid : pearson = 0.0820, spearman = 0.1063
2019-02-16 09:42:17,186 : SMTeuroparl : pearson = 0.4300, spearman = 0.5277
2019-02-16 09:42:18,480 : surprise.OnWN : pearson = 0.2695, spearman = 0.2685
2019-02-16 09:42:19,189 : surprise.SMTnews : pearson = 0.5645, spearman = 0.4436
2019-02-16 09:42:19,189 : ALL (weighted average) : Pearson = 0.2754,             Spearman = 0.2926
2019-02-16 09:42:19,189 : ALL (average) : Pearson = 0.3145,             Spearman = 0.3250

2019-02-16 09:42:19,189 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 09:42:19,200 : loading BERT model bert-base-uncased
2019-02-16 09:42:19,200 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:19,222 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:19,222 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt7m51p1y
2019-02-16 09:42:21,732 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:23,769 : FNWN : pearson = 0.1197, spearman = 0.1409
2019-02-16 09:42:24,639 : headlines : pearson = 0.3918, spearman = 0.3828
2019-02-16 09:42:25,301 : OnWN : pearson = 0.0238, spearman = 0.0344
2019-02-16 09:42:25,301 : ALL (weighted average) : Pearson = 0.2199,             Spearman = 0.2220
2019-02-16 09:42:25,301 : ALL (average) : Pearson = 0.1784,             Spearman = 0.1860

2019-02-16 09:42:25,301 : ***** Transfer task : STS14 *****


2019-02-16 09:42:25,317 : loading BERT model bert-base-uncased
2019-02-16 09:42:25,318 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:25,336 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:25,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0cohxvj3
2019-02-16 09:42:27,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:29,926 : deft-forum : pearson = -0.0438, spearman = -0.0276
2019-02-16 09:42:30,628 : deft-news : pearson = 0.3997, spearman = 0.4317
2019-02-16 09:42:31,604 : headlines : pearson = 0.3830, spearman = 0.3607
2019-02-16 09:42:32,574 : images : pearson = 0.1850, spearman = 0.1941
2019-02-16 09:42:33,540 : OnWN : pearson = 0.2458, spearman = 0.2456
2019-02-16 09:42:34,831 : tweet-news : pearson = 0.4331, spearman = 0.3943
2019-02-16 09:42:34,831 : ALL (weighted average) : Pearson = 0.2761,             Spearman = 0.2702
2019-02-16 09:42:34,831 : ALL (average) : Pearson = 0.2671,             Spearman = 0.2665

2019-02-16 09:42:34,831 : ***** Transfer task : STS15 *****


2019-02-16 09:42:34,870 : loading BERT model bert-base-uncased
2019-02-16 09:42:34,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:34,891 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:34,891 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx6vwj1yg
2019-02-16 09:42:37,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:39,788 : answers-forums : pearson = 0.2572, spearman = 0.2761
2019-02-16 09:42:40,692 : answers-students : pearson = 0.3487, spearman = 0.3548
2019-02-16 09:42:41,558 : belief : pearson = 0.3762, spearman = 0.4458
2019-02-16 09:42:42,580 : headlines : pearson = 0.4313, spearman = 0.4405
2019-02-16 09:42:43,566 : images : pearson = 0.0940, spearman = 0.1312
2019-02-16 09:42:43,566 : ALL (weighted average) : Pearson = 0.2977,             Spearman = 0.3218
2019-02-16 09:42:43,566 : ALL (average) : Pearson = 0.3015,             Spearman = 0.3297

2019-02-16 09:42:43,566 : ***** Transfer task : STS16 *****


2019-02-16 09:42:43,609 : loading BERT model bert-base-uncased
2019-02-16 09:42:43,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:43,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:43,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqg8z99iw
2019-02-16 09:42:46,128 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:47,967 : answer-answer : pearson = 0.3658, spearman = 0.3851
2019-02-16 09:42:48,262 : headlines : pearson = 0.5119, spearman = 0.5180
2019-02-16 09:42:48,658 : plagiarism : pearson = 0.5702, spearman = 0.6335
2019-02-16 09:42:49,288 : postediting : pearson = 0.6437, spearman = 0.7545
2019-02-16 09:42:49,574 : question-question : pearson = 0.0857, spearman = 0.1811
2019-02-16 09:42:49,574 : ALL (weighted average) : Pearson = 0.4439,             Spearman = 0.5012
2019-02-16 09:42:49,574 : ALL (average) : Pearson = 0.4354,             Spearman = 0.4944

2019-02-16 09:42:49,574 : ***** Transfer task : MR *****


2019-02-16 09:42:49,630 : loading BERT model bert-base-uncased
2019-02-16 09:42:49,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:42:49,652 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:42:49,652 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi8lgw90x
2019-02-16 09:42:52,174 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:42:53,684 : Generating sentence embeddings
2019-02-16 09:43:07,030 : Generated sentence embeddings
2019-02-16 09:43:07,031 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 09:43:18,627 : Best param found at split 1: l2reg = 0.001                 with score 74.18
2019-02-16 09:43:30,189 : Best param found at split 2: l2reg = 0.01                 with score 73.54
2019-02-16 09:43:40,375 : Best param found at split 3: l2reg = 0.001                 with score 72.02
2019-02-16 09:43:52,524 : Best param found at split 4: l2reg = 1e-05                 with score 73.93
2019-02-16 09:44:03,276 : Best param found at split 5: l2reg = 1e-05                 with score 71.49
2019-02-16 09:44:03,982 : Dev acc : 73.03 Test acc : 73.03

2019-02-16 09:44:03,983 : ***** Transfer task : CR *****


2019-02-16 09:44:03,991 : loading BERT model bert-base-uncased
2019-02-16 09:44:03,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:44:04,013 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:44:04,013 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptl7_i5rp
2019-02-16 09:44:06,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:44:07,943 : Generating sentence embeddings
2019-02-16 09:44:11,628 : Generated sentence embeddings
2019-02-16 09:44:11,628 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 09:44:18,261 : Best param found at split 1: l2reg = 1e-05                 with score 75.82
2019-02-16 09:44:25,168 : Best param found at split 2: l2reg = 0.0001                 with score 76.88
2019-02-16 09:44:29,954 : Best param found at split 3: l2reg = 0.0001                 with score 77.15
2019-02-16 09:44:35,696 : Best param found at split 4: l2reg = 0.0001                 with score 76.07
2019-02-16 09:44:41,077 : Best param found at split 5: l2reg = 0.001                 with score 77.89
2019-02-16 09:44:41,332 : Dev acc : 76.76 Test acc : 75.74

2019-02-16 09:44:41,333 : ***** Transfer task : MPQA *****


2019-02-16 09:44:41,339 : loading BERT model bert-base-uncased
2019-02-16 09:44:41,339 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:44:41,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:44:41,361 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0osd1kz5
2019-02-16 09:44:43,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:44:45,344 : Generating sentence embeddings
2019-02-16 09:44:49,113 : Generated sentence embeddings
2019-02-16 09:44:49,114 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 09:45:06,535 : Best param found at split 1: l2reg = 0.001                 with score 86.02
2019-02-16 09:45:23,627 : Best param found at split 2: l2reg = 1e-05                 with score 85.53
2019-02-16 09:45:42,172 : Best param found at split 3: l2reg = 0.0001                 with score 85.41
2019-02-16 09:45:59,617 : Best param found at split 4: l2reg = 0.0001                 with score 86.0
2019-02-16 09:46:19,044 : Best param found at split 5: l2reg = 0.0001                 with score 84.6
2019-02-16 09:46:20,356 : Dev acc : 85.51 Test acc : 86.0

2019-02-16 09:46:20,357 : ***** Transfer task : SUBJ *****


2019-02-16 09:46:20,375 : loading BERT model bert-base-uncased
2019-02-16 09:46:20,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:46:20,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:46:20,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8123mj39
2019-02-16 09:46:22,923 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:46:24,378 : Generating sentence embeddings
2019-02-16 09:46:38,639 : Generated sentence embeddings
2019-02-16 09:46:38,639 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 09:46:55,904 : Best param found at split 1: l2reg = 1e-05                 with score 94.26
2019-02-16 09:47:14,103 : Best param found at split 2: l2reg = 0.0001                 with score 94.21
2019-02-16 09:47:30,208 : Best param found at split 3: l2reg = 1e-05                 with score 94.04
2019-02-16 09:47:46,142 : Best param found at split 4: l2reg = 0.0001                 with score 94.62
2019-02-16 09:48:05,224 : Best param found at split 5: l2reg = 0.0001                 with score 94.25
2019-02-16 09:48:06,646 : Dev acc : 94.28 Test acc : 94.35

2019-02-16 09:48:06,648 : ***** Transfer task : SST Binary classification *****


2019-02-16 09:48:06,821 : loading BERT model bert-base-uncased
2019-02-16 09:48:06,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:48:06,847 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:48:06,847 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi5g66o5n
2019-02-16 09:48:09,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:48:10,818 : Computing embedding for train
2019-02-16 09:48:55,916 : Computed train embeddings
2019-02-16 09:48:55,917 : Computing embedding for dev
2019-02-16 09:48:56,880 : Computed dev embeddings
2019-02-16 09:48:56,880 : Computing embedding for test
2019-02-16 09:48:58,887 : Computed test embeddings
2019-02-16 09:48:58,887 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:49:22,484 : [('reg:1e-05', 77.64), ('reg:0.0001', 77.52), ('reg:0.001', 78.1), ('reg:0.01', 77.06)]
2019-02-16 09:49:22,484 : Validation : best param found is reg = 0.001 with score             78.1
2019-02-16 09:49:22,484 : Evaluating...
2019-02-16 09:49:27,966 : 
Dev acc : 78.1 Test acc : 78.14 for             SST Binary classification

2019-02-16 09:49:27,966 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 09:49:28,017 : loading BERT model bert-base-uncased
2019-02-16 09:49:28,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:49:28,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:49:28,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoljzk9g1
2019-02-16 09:49:30,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:49:31,953 : Computing embedding for train
2019-02-16 09:49:41,427 : Computed train embeddings
2019-02-16 09:49:41,428 : Computing embedding for dev
2019-02-16 09:49:42,632 : Computed dev embeddings
2019-02-16 09:49:42,632 : Computing embedding for test
2019-02-16 09:49:45,076 : Computed test embeddings
2019-02-16 09:49:45,076 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:49:48,759 : [('reg:1e-05', 40.87), ('reg:0.0001', 40.87), ('reg:0.001', 35.42), ('reg:0.01', 38.6)]
2019-02-16 09:49:48,760 : Validation : best param found is reg = 1e-05 with score             40.87
2019-02-16 09:49:48,760 : Evaluating...
2019-02-16 09:49:49,477 : 
Dev acc : 40.87 Test acc : 42.76 for             SST Fine-Grained classification

2019-02-16 09:49:49,478 : ***** Transfer task : TREC *****


2019-02-16 09:49:49,492 : loading BERT model bert-base-uncased
2019-02-16 09:49:49,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:49:49,514 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:49:49,514 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8h2yrm6n
2019-02-16 09:49:52,033 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:49:56,965 : Computed train embeddings
2019-02-16 09:49:57,237 : Computed test embeddings
2019-02-16 09:49:57,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 09:50:08,329 : [('reg:1e-05', 78.85), ('reg:0.0001', 79.44), ('reg:0.001', 79.36), ('reg:0.01', 67.43)]
2019-02-16 09:50:08,329 : Cross-validation : best param found is reg = 0.0001             with score 79.44
2019-02-16 09:50:08,329 : Evaluating...
2019-02-16 09:50:09,421 : 
Dev acc : 79.44 Test acc : 86.6             for TREC

2019-02-16 09:50:09,422 : ***** Transfer task : MRPC *****


2019-02-16 09:50:09,491 : loading BERT model bert-base-uncased
2019-02-16 09:50:09,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:50:09,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:50:09,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8y5brg_7
2019-02-16 09:50:12,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:50:13,487 : Computing embedding for train
2019-02-16 09:50:24,057 : Computed train embeddings
2019-02-16 09:50:24,057 : Computing embedding for test
2019-02-16 09:50:28,515 : Computed test embeddings
2019-02-16 09:50:28,531 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 09:50:35,557 : [('reg:1e-05', 70.49), ('reg:0.0001', 70.14), ('reg:0.001', 70.24), ('reg:0.01', 69.04)]
2019-02-16 09:50:35,557 : Cross-validation : best param found is reg = 1e-05             with score 70.49
2019-02-16 09:50:35,557 : Evaluating...
2019-02-16 09:50:36,137 : Dev acc : 70.49 Test acc 68.06; Test F1 80.54 for MRPC.

2019-02-16 09:50:36,137 : ***** Transfer task : SICK-Entailment*****


2019-02-16 09:50:36,165 : loading BERT model bert-base-uncased
2019-02-16 09:50:36,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:50:36,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:50:36,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0jue4fbf
2019-02-16 09:50:38,674 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:50:40,249 : Computing embedding for train
2019-02-16 09:50:45,375 : Computed train embeddings
2019-02-16 09:50:45,375 : Computing embedding for dev
2019-02-16 09:50:46,062 : Computed dev embeddings
2019-02-16 09:50:46,063 : Computing embedding for test
2019-02-16 09:50:51,659 : Computed test embeddings
2019-02-16 09:50:51,688 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 09:50:53,193 : [('reg:1e-05', 73.8), ('reg:0.0001', 68.8), ('reg:0.001', 73.8), ('reg:0.01', 71.2)]
2019-02-16 09:50:53,194 : Validation : best param found is reg = 1e-05 with score             73.8
2019-02-16 09:50:53,194 : Evaluating...
2019-02-16 09:50:53,685 : 
Dev acc : 73.8 Test acc : 71.48 for                        SICK entailment

2019-02-16 09:50:53,685 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 09:50:53,714 : loading BERT model bert-base-uncased
2019-02-16 09:50:53,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:50:53,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:50:53,737 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv_1rc98n
2019-02-16 09:50:56,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:50:57,782 : Computing embedding for train
2019-02-16 09:51:02,980 : Computed train embeddings
2019-02-16 09:51:02,981 : Computing embedding for dev
2019-02-16 09:51:03,667 : Computed dev embeddings
2019-02-16 09:51:03,667 : Computing embedding for test
2019-02-16 09:51:09,163 : Computed test embeddings
2019-02-16 09:51:32,396 : Dev : Pearson 0.6866385385093505
2019-02-16 09:51:32,397 : Test : Pearson 0.6971499229597029 Spearman 0.6422100419948958 MSE 0.5262799125307982                        for SICK Relatedness

2019-02-16 09:51:32,398 : 

***** Transfer task : STSBenchmark*****


2019-02-16 09:51:32,493 : loading BERT model bert-base-uncased
2019-02-16 09:51:32,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:51:32,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:51:32,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwtx1kh46
2019-02-16 09:51:34,982 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:51:36,470 : Computing embedding for train
2019-02-16 09:51:44,704 : Computed train embeddings
2019-02-16 09:51:44,704 : Computing embedding for dev
2019-02-16 09:51:47,153 : Computed dev embeddings
2019-02-16 09:51:47,154 : Computing embedding for test
2019-02-16 09:51:49,185 : Computed test embeddings
2019-02-16 09:52:18,530 : Dev : Pearson 0.48173077157097627
2019-02-16 09:52:18,530 : Test : Pearson 0.46557109337882074 Spearman 0.4628866686484039 MSE 1.9249203571621725                        for SICK Relatedness

2019-02-16 09:52:18,530 : ***** Transfer task : SNLI Entailment*****


2019-02-16 09:52:23,585 : loading BERT model bert-base-uncased
2019-02-16 09:52:23,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 09:52:23,715 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 09:52:23,715 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6ma_hkna
2019-02-16 09:52:26,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 09:52:27,814 : PROGRESS (encoding): 0.00%
2019-02-16 09:53:46,247 : PROGRESS (encoding): 14.56%
2019-02-16 09:55:13,257 : PROGRESS (encoding): 29.12%
2019-02-16 09:56:40,097 : PROGRESS (encoding): 43.69%
2019-02-16 09:58:14,485 : PROGRESS (encoding): 58.25%
2019-02-16 09:59:59,266 : PROGRESS (encoding): 72.81%
2019-02-16 10:01:47,328 : PROGRESS (encoding): 87.37%
2019-02-16 10:03:37,105 : PROGRESS (encoding): 0.00%
2019-02-16 10:03:50,519 : PROGRESS (encoding): 0.00%
2019-02-16 10:04:03,502 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 10:05:19,941 : [('reg:1e-09', 56.37)]
2019-02-16 10:05:19,941 : Validation : best param found is reg = 1e-09 with score             56.37
2019-02-16 10:05:19,941 : Evaluating...
2019-02-16 10:06:36,104 : Dev acc : 56.37 Test acc : 56.29 for SNLI

2019-02-16 10:06:36,104 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 10:06:45,365 : loading BERT model bert-base-uncased
2019-02-16 10:06:45,365 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 10:06:45,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 10:06:45,421 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg_j1ujia
2019-02-16 10:06:47,929 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 10:06:49,398 : Computing embedding for train
2019-02-16 10:14:20,816 : Computed train embeddings
2019-02-16 10:14:20,816 : Computing embedding for dev
2019-02-16 10:14:39,933 : Computed dev embeddings
2019-02-16 10:14:39,933 : Computing embedding for test
2019-02-16 10:14:59,754 : Computed test embeddings
2019-02-16 10:14:59,770 : prepare data
2019-02-16 10:14:59,835 : start epoch
2019-02-16 10:15:42,772 : samples : 64000
2019-02-16 10:15:53,232 : Image to text: 3.12, 13.2, 21.6, 40.0
2019-02-16 10:16:00,801 : Text to Image: 2.944, 11.772, 19.1, 48.0
2019-02-16 10:16:43,845 : samples : 128000
2019-02-16 10:16:53,971 : Image to text: 4.66, 16.56, 26.12, 33.0
2019-02-16 10:17:01,354 : Text to Image: 3.74, 13.62, 22.016, 40.0
2019-02-16 10:17:54,253 : samples : 192000
2019-02-16 10:18:04,627 : Image to text: 4.88, 17.12, 26.66, 32.0
2019-02-16 10:18:12,163 : Text to Image: 4.076, 13.932, 22.532, 39.0
2019-02-16 10:18:54,672 : samples : 256000
2019-02-16 10:19:04,990 : Image to text: 5.26, 17.0, 26.86, 30.0
2019-02-16 10:19:12,572 : Text to Image: 3.912, 14.34, 23.32, 36.0
2019-02-16 10:19:55,302 : samples : 320000
2019-02-16 10:20:05,667 : Image to text: 5.36, 18.18, 29.02, 29.0
2019-02-16 10:20:13,140 : Text to Image: 5.0, 16.808, 26.3, 32.0
2019-02-16 10:20:55,776 : samples : 384000
2019-02-16 10:21:06,151 : Image to text: 5.76, 19.68, 30.54, 27.0
2019-02-16 10:21:13,782 : Text to Image: 5.032, 17.164, 26.912, 31.0
2019-02-16 10:21:56,092 : samples : 448000
2019-02-16 10:22:06,478 : Image to text: 5.66, 19.26, 29.82, 27.0
2019-02-16 10:22:14,024 : Text to Image: 4.936, 17.016, 26.332, 32.0
2019-02-16 10:22:56,898 : samples : 512000
2019-02-16 10:23:07,344 : Image to text: 5.96, 19.9, 31.42, 26.0
2019-02-16 10:23:14,837 : Text to Image: 5.112, 17.348, 27.172, 30.0
2019-02-16 10:23:51,341 : Epoch 1 finished
2019-02-16 10:23:51,781 : Image to text: 18.9, 50.6, 65.4, 5.0
2019-02-16 10:23:52,117 : Text to Image: 16.3, 45.36, 63.08, 7.0
2019-02-16 10:23:52,554 : Image to text: 19.3, 49.9, 66.3, 6.0
2019-02-16 10:23:52,890 : Text to Image: 15.66, 44.68, 62.34, 7.0
2019-02-16 10:23:53,332 : Image to text: 22.0, 48.9, 64.1, 6.0
2019-02-16 10:23:53,670 : Text to Image: 15.86, 44.72, 61.56, 7.0
2019-02-16 10:23:54,110 : Image to text: 19.2, 48.1, 64.5, 6.0
2019-02-16 10:23:54,455 : Text to Image: 15.88, 44.46, 61.72, 7.0
2019-02-16 10:23:54,894 : Image to text: 19.7, 47.9, 65.1, 6.0
2019-02-16 10:23:55,231 : Text to Image: 16.02, 44.56, 62.12, 7.0
2019-02-16 10:23:55,231 : Dev mean Text to Image: 15.943999999999999, 44.75599999999999, 62.164, 7.0
2019-02-16 10:23:55,231 : Dev mean Image to text: 19.82, 49.08, 65.08, 5.800000000000001
2019-02-16 10:23:55,232 : start epoch
2019-02-16 10:24:37,872 : samples : 64000
2019-02-16 10:24:48,387 : Image to text: 6.9, 21.84, 33.4, 23.0
2019-02-16 10:24:55,896 : Text to Image: 5.268, 18.152, 28.104, 29.0
2019-02-16 10:25:40,125 : samples : 128000
2019-02-16 10:25:50,485 : Image to text: 6.14, 22.0, 33.62, 23.0
2019-02-16 10:25:58,062 : Text to Image: 5.46, 18.46, 28.584, 29.0
2019-02-16 10:26:41,194 : samples : 192000
2019-02-16 10:26:51,570 : Image to text: 6.8, 21.7, 33.06, 24.0
2019-02-16 10:26:59,037 : Text to Image: 5.692, 18.92, 29.096, 28.0
2019-02-16 10:27:41,700 : samples : 256000
2019-02-16 10:27:52,066 : Image to text: 7.18, 21.9, 33.04, 23.0
2019-02-16 10:27:59,565 : Text to Image: 6.208, 19.712, 29.924, 27.0
2019-02-16 10:28:42,530 : samples : 320000
2019-02-16 10:28:52,889 : Image to text: 8.16, 23.8, 34.72, 21.0
2019-02-16 10:29:00,395 : Text to Image: 6.008, 19.796, 30.228, 26.0
2019-02-16 10:29:44,055 : samples : 384000
2019-02-16 10:29:54,364 : Image to text: 6.36, 21.68, 32.26, 23.0
2019-02-16 10:30:01,891 : Text to Image: 5.604, 18.628, 29.036, 28.0
2019-02-16 10:30:44,277 : samples : 448000
2019-02-16 10:30:54,578 : Image to text: 7.66, 22.96, 34.54, 22.0
2019-02-16 10:31:02,051 : Text to Image: 5.844, 19.476, 30.052, 27.0
2019-02-16 10:31:44,529 : samples : 512000
2019-02-16 10:31:54,853 : Image to text: 7.44, 23.56, 34.12, 22.0
2019-02-16 10:32:02,359 : Text to Image: 6.136, 20.152, 31.008, 26.0
2019-02-16 10:32:39,181 : Epoch 2 finished
2019-02-16 10:32:39,636 : Image to text: 20.7, 52.5, 67.5, 5.0
2019-02-16 10:32:39,975 : Text to Image: 18.62, 49.76, 66.66, 6.0
2019-02-16 10:32:40,429 : Image to text: 20.8, 53.2, 68.5, 5.0
2019-02-16 10:32:40,768 : Text to Image: 18.28, 48.32, 66.4, 6.0
2019-02-16 10:32:41,226 : Image to text: 20.6, 53.2, 69.3, 5.0
2019-02-16 10:32:41,567 : Text to Image: 16.84, 48.76, 66.5, 6.0
2019-02-16 10:32:42,009 : Image to text: 18.9, 49.8, 67.4, 6.0
2019-02-16 10:32:42,338 : Text to Image: 18.36, 48.18, 64.8, 6.0
2019-02-16 10:32:42,778 : Image to text: 21.3, 52.1, 69.2, 5.0
2019-02-16 10:32:43,114 : Text to Image: 18.38, 48.2, 65.62, 6.0
2019-02-16 10:32:43,114 : Dev mean Text to Image: 18.096, 48.644, 65.99600000000001, 6.0
2019-02-16 10:32:43,114 : Dev mean Image to text: 20.46, 52.160000000000004, 68.38000000000001, 5.2
2019-02-16 10:32:43,114 : start epoch
2019-02-16 10:33:25,444 : samples : 64000
2019-02-16 10:33:35,975 : Image to text: 6.78, 22.8, 34.9, 21.0
2019-02-16 10:33:43,339 : Text to Image: 6.284, 19.92, 30.7, 25.0
2019-02-16 10:34:34,858 : samples : 128000
2019-02-16 10:34:46,261 : Image to text: 6.92, 21.5, 32.56, 23.0
2019-02-16 10:34:53,785 : Text to Image: 5.496, 18.836, 28.808, 28.0
2019-02-16 10:35:37,004 : samples : 192000
2019-02-16 10:35:47,386 : Image to text: 7.5, 23.2, 34.72, 21.0
2019-02-16 10:35:54,931 : Text to Image: 5.796, 19.132, 29.884, 27.0
2019-02-16 10:36:37,827 : samples : 256000
2019-02-16 10:36:48,081 : Image to text: 7.52, 24.36, 36.8, 20.0
2019-02-16 10:36:55,542 : Text to Image: 6.36, 20.612, 31.576, 25.0
2019-02-16 10:37:39,041 : samples : 320000
2019-02-16 10:37:49,414 : Image to text: 7.8, 24.72, 36.76, 19.0
2019-02-16 10:37:56,899 : Text to Image: 6.512, 20.508, 31.488, 24.0
2019-02-16 10:38:39,337 : samples : 384000
2019-02-16 10:38:49,738 : Image to text: 8.34, 25.08, 37.22, 19.0
2019-02-16 10:38:57,288 : Text to Image: 6.892, 21.488, 32.344, 24.0
2019-02-16 10:39:40,288 : samples : 448000
2019-02-16 10:39:50,640 : Image to text: 8.34, 25.02, 36.32, 20.0
2019-02-16 10:39:58,134 : Text to Image: 6.756, 21.916, 32.824, 23.0
2019-02-16 10:40:41,229 : samples : 512000
2019-02-16 10:40:51,517 : Image to text: 7.7, 24.04, 35.38, 21.0
2019-02-16 10:40:59,032 : Text to Image: 6.348, 21.176, 31.948, 24.0
2019-02-16 10:41:35,277 : Epoch 3 finished
2019-02-16 10:41:35,704 : Image to text: 20.1, 53.1, 69.0, 5.0
2019-02-16 10:41:36,041 : Text to Image: 17.48, 47.04, 64.76, 6.0
2019-02-16 10:41:36,489 : Image to text: 21.6, 51.6, 67.1, 5.0
2019-02-16 10:41:36,824 : Text to Image: 16.34, 46.92, 64.46, 6.0
2019-02-16 10:41:37,271 : Image to text: 21.1, 54.7, 70.6, 5.0
2019-02-16 10:41:37,606 : Text to Image: 15.84, 46.02, 63.3, 6.0
2019-02-16 10:41:38,060 : Image to text: 20.6, 54.2, 69.5, 5.0
2019-02-16 10:41:38,394 : Text to Image: 16.58, 46.62, 63.96, 6.0
2019-02-16 10:41:38,833 : Image to text: 22.0, 53.8, 70.4, 5.0
2019-02-16 10:41:39,175 : Text to Image: 17.24, 47.54, 64.64, 6.0
2019-02-16 10:41:39,175 : Dev mean Text to Image: 16.695999999999998, 46.828, 64.224, 6.0
2019-02-16 10:41:39,175 : Dev mean Image to text: 21.08, 53.48, 69.32, 5.0
2019-02-16 10:41:39,175 : start epoch
2019-02-16 10:42:21,871 : samples : 64000
2019-02-16 10:42:32,339 : Image to text: 8.28, 25.16, 36.82, 20.0
2019-02-16 10:42:39,848 : Text to Image: 6.76, 21.564, 32.352, 23.0
2019-02-16 10:43:22,783 : samples : 128000
2019-02-16 10:43:33,071 : Image to text: 7.74, 24.14, 35.56, 20.0
2019-02-16 10:43:40,602 : Text to Image: 6.6, 20.972, 32.128, 24.0
2019-02-16 10:44:22,995 : samples : 192000
2019-02-16 10:44:33,327 : Image to text: 8.2, 24.04, 36.26, 20.0
2019-02-16 10:44:40,779 : Text to Image: 6.404, 20.828, 31.78, 24.0
2019-02-16 10:45:24,519 : samples : 256000
2019-02-16 10:45:34,883 : Image to text: 8.58, 25.1, 37.22, 20.0
2019-02-16 10:45:42,317 : Text to Image: 6.812, 21.896, 33.036, 23.0
2019-02-16 10:46:24,887 : samples : 320000
2019-02-16 10:46:35,169 : Image to text: 8.38, 25.52, 37.88, 18.0
2019-02-16 10:46:42,680 : Text to Image: 6.86, 22.384, 33.464, 22.0
2019-02-16 10:47:24,544 : samples : 384000
2019-02-16 10:47:34,831 : Image to text: 7.9, 25.5, 37.26, 19.0
2019-02-16 10:47:42,306 : Text to Image: 6.888, 21.996, 33.084, 23.0
2019-02-16 10:48:25,164 : samples : 448000
2019-02-16 10:48:35,486 : Image to text: 8.04, 24.64, 36.54, 19.0
2019-02-16 10:48:42,907 : Text to Image: 6.988, 22.304, 33.452, 23.0
2019-02-16 10:49:26,274 : samples : 512000
2019-02-16 10:49:36,607 : Image to text: 8.84, 25.82, 37.46, 18.0
2019-02-16 10:49:44,069 : Text to Image: 7.388, 22.304, 33.58, 22.0
2019-02-16 10:50:20,680 : Epoch 4 finished
2019-02-16 10:50:21,120 : Image to text: 23.3, 55.2, 72.9, 4.0
2019-02-16 10:50:21,453 : Text to Image: 19.42, 51.14, 69.3, 5.0
2019-02-16 10:50:21,888 : Image to text: 24.8, 56.6, 70.2, 4.0
2019-02-16 10:50:22,227 : Text to Image: 19.6, 50.24, 67.64, 5.0
2019-02-16 10:50:22,669 : Image to text: 24.3, 56.8, 72.8, 4.0
2019-02-16 10:50:22,999 : Text to Image: 19.56, 51.1, 68.7, 5.0
2019-02-16 10:50:23,428 : Image to text: 24.9, 55.4, 72.4, 4.0
2019-02-16 10:50:23,758 : Text to Image: 19.46, 50.52, 67.96, 5.0
2019-02-16 10:50:24,195 : Image to text: 24.5, 56.3, 72.4, 4.0
2019-02-16 10:50:24,523 : Text to Image: 19.92, 51.04, 67.56, 5.0
2019-02-16 10:50:24,523 : Dev mean Text to Image: 19.592000000000002, 50.808, 68.232, 5.0
2019-02-16 10:50:24,523 : Dev mean Image to text: 24.36, 56.059999999999995, 72.14000000000001, 4.0
2019-02-16 10:50:24,523 : start epoch
2019-02-16 10:51:11,458 : samples : 64000
2019-02-16 10:51:23,423 : Image to text: 8.0, 25.9, 37.58, 19.0
2019-02-16 10:51:32,583 : Text to Image: 7.108, 22.228, 33.26, 23.0
2019-02-16 10:52:15,123 : samples : 128000
2019-02-16 10:52:25,558 : Image to text: 8.74, 26.04, 38.08, 18.0
2019-02-16 10:52:33,177 : Text to Image: 7.008, 22.456, 33.408, 23.0
2019-02-16 10:53:15,413 : samples : 192000
2019-02-16 10:53:25,817 : Image to text: 8.66, 26.34, 38.04, 18.0
2019-02-16 10:53:33,393 : Text to Image: 7.132, 22.78, 34.02, 22.0
2019-02-16 10:54:15,483 : samples : 256000
2019-02-16 10:54:25,869 : Image to text: 8.04, 25.2, 36.66, 19.0
2019-02-16 10:54:33,424 : Text to Image: 7.04, 22.18, 33.196, 23.0
2019-02-16 10:55:15,728 : samples : 320000
2019-02-16 10:55:26,107 : Image to text: 8.9, 25.74, 38.12, 18.0
2019-02-16 10:55:33,670 : Text to Image: 7.3, 22.804, 34.152, 22.0
2019-02-16 10:56:16,948 : samples : 384000
2019-02-16 10:56:27,317 : Image to text: 8.18, 25.64, 38.0, 18.0
2019-02-16 10:56:34,935 : Text to Image: 7.24, 22.264, 33.8, 22.0
2019-02-16 10:57:17,502 : samples : 448000
2019-02-16 10:57:27,869 : Image to text: 8.42, 25.4, 37.96, 18.0
2019-02-16 10:57:35,470 : Text to Image: 6.936, 22.272, 33.532, 22.0
2019-02-16 10:58:18,400 : samples : 512000
2019-02-16 10:58:28,817 : Image to text: 8.7, 25.34, 37.42, 18.0
2019-02-16 10:58:36,399 : Text to Image: 7.472, 22.792, 34.236, 22.0
2019-02-16 10:59:12,356 : Epoch 5 finished
2019-02-16 10:59:12,805 : Image to text: 23.2, 56.0, 72.5, 4.0
2019-02-16 10:59:13,142 : Text to Image: 20.04, 51.9, 69.84, 5.0
2019-02-16 10:59:13,592 : Image to text: 25.1, 55.7, 70.9, 4.0
2019-02-16 10:59:13,934 : Text to Image: 19.62, 51.58, 68.7, 5.0
2019-02-16 10:59:14,381 : Image to text: 25.5, 58.0, 72.5, 4.0
2019-02-16 10:59:14,716 : Text to Image: 20.48, 52.84, 69.76, 5.0
2019-02-16 10:59:15,166 : Image to text: 23.8, 57.8, 73.3, 4.0
2019-02-16 10:59:15,502 : Text to Image: 20.32, 52.3, 69.16, 5.0
2019-02-16 10:59:15,953 : Image to text: 27.4, 57.2, 72.5, 4.0
2019-02-16 10:59:16,297 : Text to Image: 19.74, 52.38, 68.96, 5.0
2019-02-16 10:59:16,297 : Dev mean Text to Image: 20.04, 52.199999999999996, 69.28399999999999, 5.0
2019-02-16 10:59:16,297 : Dev mean Image to text: 25.0, 56.94, 72.34, 4.0
2019-02-16 10:59:16,297 : start epoch
2019-02-16 10:59:58,754 : samples : 64000
2019-02-16 11:00:09,262 : Image to text: 9.16, 26.76, 38.8, 17.0
2019-02-16 11:00:16,753 : Text to Image: 7.488, 23.176, 34.428, 22.0
2019-02-16 11:00:59,738 : samples : 128000
2019-02-16 11:01:10,134 : Image to text: 8.36, 26.24, 39.24, 17.0
2019-02-16 11:01:17,756 : Text to Image: 7.352, 22.728, 34.216, 22.0
2019-02-16 11:02:00,063 : samples : 192000
2019-02-16 11:02:10,406 : Image to text: 8.84, 26.08, 37.62, 18.0
2019-02-16 11:02:18,039 : Text to Image: 6.94, 21.784, 32.996, 23.0
2019-02-16 11:03:00,355 : samples : 256000
2019-02-16 11:03:10,700 : Image to text: 8.38, 25.42, 37.72, 19.0
2019-02-16 11:03:18,250 : Text to Image: 7.172, 22.42, 33.616, 22.0
2019-02-16 11:04:01,112 : samples : 320000
2019-02-16 11:04:11,479 : Image to text: 8.5, 24.8, 37.18, 18.0
2019-02-16 11:04:19,063 : Text to Image: 7.136, 22.744, 33.74, 22.0
2019-02-16 11:05:02,499 : samples : 384000
2019-02-16 11:05:12,909 : Image to text: 9.04, 27.56, 39.38, 17.0
2019-02-16 11:05:20,426 : Text to Image: 7.964, 23.456, 34.808, 21.0
2019-02-16 11:06:02,504 : samples : 448000
2019-02-16 11:06:12,887 : Image to text: 8.36, 25.8, 38.24, 18.0
2019-02-16 11:06:20,448 : Text to Image: 7.276, 22.872, 34.428, 22.0
2019-02-16 11:07:02,972 : samples : 512000
2019-02-16 11:07:13,311 : Image to text: 8.98, 26.84, 39.76, 17.0
2019-02-16 11:07:20,918 : Text to Image: 7.42, 23.296, 34.224, 21.0
2019-02-16 11:07:59,797 : Epoch 6 finished
2019-02-16 11:08:00,348 : Image to text: 23.8, 56.2, 71.8, 4.0
2019-02-16 11:08:00,788 : Text to Image: 20.72, 53.48, 70.9, 5.0
2019-02-16 11:08:01,354 : Image to text: 24.4, 55.6, 71.1, 5.0
2019-02-16 11:08:01,776 : Text to Image: 20.48, 52.58, 69.78, 5.0
2019-02-16 11:08:02,345 : Image to text: 24.0, 57.0, 71.3, 4.0
2019-02-16 11:08:02,782 : Text to Image: 20.98, 53.6, 70.42, 5.0
2019-02-16 11:08:03,338 : Image to text: 23.2, 56.7, 74.9, 4.0
2019-02-16 11:08:03,770 : Text to Image: 20.86, 52.78, 70.1, 5.0
2019-02-16 11:08:04,334 : Image to text: 26.5, 58.4, 72.8, 4.0
2019-02-16 11:08:04,759 : Text to Image: 21.18, 52.08, 68.72, 5.0
2019-02-16 11:08:04,759 : Dev mean Text to Image: 20.844, 52.903999999999996, 69.984, 5.0
2019-02-16 11:08:04,759 : Dev mean Image to text: 24.380000000000003, 56.779999999999994, 72.38, 4.2
2019-02-16 11:08:04,760 : start epoch
2019-02-16 11:08:52,758 : samples : 64000
2019-02-16 11:09:03,250 : Image to text: 9.02, 26.7, 39.5, 17.0
2019-02-16 11:09:10,773 : Text to Image: 7.72, 23.32, 35.18, 21.0
2019-02-16 11:09:54,724 : samples : 128000
2019-02-16 11:10:07,321 : Image to text: 8.74, 27.0, 38.5, 17.0
2019-02-16 11:10:17,420 : Text to Image: 7.616, 23.144, 34.716, 21.0
2019-02-16 11:11:02,756 : samples : 192000
2019-02-16 11:11:13,059 : Image to text: 8.62, 26.8, 39.24, 17.0
2019-02-16 11:11:20,438 : Text to Image: 7.36, 23.22, 34.952, 21.0
2019-02-16 11:12:02,762 : samples : 256000
2019-02-16 11:12:15,322 : Image to text: 8.9, 26.54, 38.94, 17.0
2019-02-16 11:12:25,317 : Text to Image: 7.524, 23.272, 34.544, 21.0
2019-02-16 11:13:10,481 : samples : 320000
2019-02-16 11:13:23,111 : Image to text: 9.2, 27.44, 40.04, 17.0
2019-02-16 11:13:32,623 : Text to Image: 8.064, 24.108, 35.684, 20.0
2019-02-16 11:14:14,393 : samples : 384000
2019-02-16 11:14:24,646 : Image to text: 8.94, 27.12, 40.1, 17.0
2019-02-16 11:14:31,852 : Text to Image: 8.276, 24.288, 35.956, 20.0
2019-02-16 11:15:14,722 : samples : 448000
2019-02-16 11:15:27,319 : Image to text: 8.94, 26.86, 39.5, 17.0
2019-02-16 11:15:37,406 : Text to Image: 7.832, 24.1, 35.716, 20.0
2019-02-16 11:16:22,187 : samples : 512000
2019-02-16 11:16:32,419 : Image to text: 9.12, 27.3, 39.68, 16.0
2019-02-16 11:16:39,759 : Text to Image: 8.072, 24.748, 36.428, 20.0
2019-02-16 11:17:16,035 : Epoch 7 finished
2019-02-16 11:17:16,924 : Image to text: 23.9, 56.2, 71.8, 4.0
2019-02-16 11:17:17,640 : Text to Image: 20.7, 52.9, 70.74, 5.0
2019-02-16 11:17:18,536 : Image to text: 24.0, 57.2, 73.3, 4.0
2019-02-16 11:17:19,269 : Text to Image: 20.6, 52.36, 70.24, 5.0
2019-02-16 11:17:20,173 : Image to text: 23.2, 56.3, 72.4, 4.0
2019-02-16 11:17:20,941 : Text to Image: 20.54, 53.94, 70.94, 5.0
2019-02-16 11:17:21,805 : Image to text: 24.4, 57.9, 73.6, 4.0
2019-02-16 11:17:22,531 : Text to Image: 20.6, 52.7, 70.6, 5.0
2019-02-16 11:17:23,461 : Image to text: 27.1, 56.7, 71.7, 4.0
2019-02-16 11:17:24,198 : Text to Image: 20.86, 52.32, 69.62, 5.0
2019-02-16 11:17:24,198 : Dev mean Text to Image: 20.66, 52.844, 70.428, 5.0
2019-02-16 11:17:24,199 : Dev mean Image to text: 24.519999999999996, 56.86, 72.56, 4.0
2019-02-16 11:17:24,200 : start epoch
2019-02-16 11:18:09,067 : samples : 64000
2019-02-16 11:18:21,682 : Image to text: 9.36, 27.14, 39.5, 17.0
2019-02-16 11:18:31,708 : Text to Image: 8.024, 23.96, 35.6, 20.0
2019-02-16 11:19:14,698 : samples : 128000
2019-02-16 11:19:24,915 : Image to text: 9.9, 27.02, 39.42, 17.0
2019-02-16 11:19:32,213 : Text to Image: 7.524, 23.52, 35.088, 21.0
2019-02-16 11:20:16,213 : samples : 192000
2019-02-16 11:20:28,851 : Image to text: 9.54, 26.8, 39.88, 16.0
2019-02-16 11:20:38,895 : Text to Image: 7.508, 23.612, 35.164, 20.0
2019-02-16 11:21:22,496 : samples : 256000
2019-02-16 11:21:32,745 : Image to text: 8.84, 26.4, 40.0, 17.0
2019-02-16 11:21:40,144 : Text to Image: 7.808, 23.976, 35.784, 20.0
2019-02-16 11:22:22,934 : samples : 320000
2019-02-16 11:22:34,594 : Image to text: 9.58, 27.82, 40.72, 15.0
2019-02-16 11:22:44,605 : Text to Image: 8.2, 24.388, 36.032, 20.0
2019-02-16 11:23:29,545 : samples : 384000
2019-02-16 11:23:42,144 : Image to text: 9.46, 27.02, 39.52, 16.0
2019-02-16 11:23:51,266 : Text to Image: 7.708, 24.008, 35.364, 20.0
2019-02-16 11:24:33,464 : samples : 448000
2019-02-16 11:24:45,000 : Image to text: 8.98, 27.96, 40.4, 16.0
2019-02-16 11:24:55,192 : Text to Image: 7.692, 23.34, 34.732, 21.0
2019-02-16 11:25:45,177 : samples : 512000
2019-02-16 11:25:57,753 : Image to text: 9.68, 27.78, 40.9, 16.0
2019-02-16 11:26:07,778 : Text to Image: 8.064, 24.248, 36.06, 20.0
2019-02-16 11:26:44,187 : Epoch 8 finished
2019-02-16 11:26:44,656 : Image to text: 24.1, 55.7, 72.8, 4.0
2019-02-16 11:26:45,027 : Text to Image: 20.16, 54.04, 70.72, 5.0
2019-02-16 11:26:45,490 : Image to text: 25.2, 58.1, 73.8, 4.0
2019-02-16 11:26:45,859 : Text to Image: 21.06, 53.72, 71.28, 5.0
2019-02-16 11:26:46,340 : Image to text: 23.3, 58.3, 73.9, 4.0
2019-02-16 11:26:46,705 : Text to Image: 20.84, 54.12, 70.98, 5.0
2019-02-16 11:26:47,155 : Image to text: 25.4, 58.4, 75.9, 4.0
2019-02-16 11:26:47,517 : Text to Image: 20.54, 53.24, 70.48, 5.0
2019-02-16 11:26:47,956 : Image to text: 26.6, 59.1, 74.5, 4.0
2019-02-16 11:26:48,318 : Text to Image: 21.98, 53.0, 69.58, 5.0
2019-02-16 11:26:48,318 : Dev mean Text to Image: 20.916, 53.624, 70.608, 5.0
2019-02-16 11:26:48,318 : Dev mean Image to text: 24.92, 57.92, 74.18, 4.0
2019-02-16 11:26:48,319 : start epoch
2019-02-16 11:27:31,313 : samples : 64000
2019-02-16 11:27:43,904 : Image to text: 9.64, 27.66, 40.9, 16.0
2019-02-16 11:27:53,919 : Text to Image: 7.972, 24.208, 36.032, 20.0
2019-02-16 11:28:39,310 : samples : 128000
2019-02-16 11:28:49,892 : Image to text: 9.26, 27.36, 40.6, 16.0
2019-02-16 11:28:57,216 : Text to Image: 7.608, 23.248, 35.124, 21.0
2019-02-16 11:29:40,013 : samples : 192000
2019-02-16 11:29:50,352 : Image to text: 9.64, 28.06, 40.48, 16.0
2019-02-16 11:29:57,718 : Text to Image: 8.224, 24.492, 36.516, 19.0
2019-02-16 11:30:41,497 : samples : 256000
2019-02-16 11:30:54,121 : Image to text: 10.04, 27.92, 40.08, 16.0
2019-02-16 11:31:04,176 : Text to Image: 7.772, 23.992, 35.928, 20.0
2019-02-16 11:31:49,061 : samples : 320000
2019-02-16 11:31:59,278 : Image to text: 9.54, 28.08, 40.12, 16.0
2019-02-16 11:32:06,745 : Text to Image: 8.192, 24.368, 36.22, 20.0
2019-02-16 11:32:49,863 : samples : 384000
2019-02-16 11:33:02,456 : Image to text: 9.8, 27.42, 40.36, 16.0
2019-02-16 11:33:12,487 : Text to Image: 8.148, 24.46, 36.228, 20.0
2019-02-16 11:33:58,247 : samples : 448000
2019-02-16 11:34:10,856 : Image to text: 9.22, 27.68, 40.74, 16.0
2019-02-16 11:34:20,763 : Text to Image: 7.868, 23.812, 35.632, 20.0
2019-02-16 11:35:03,222 : samples : 512000
2019-02-16 11:35:13,478 : Image to text: 9.4, 28.04, 40.4, 16.0
2019-02-16 11:35:20,807 : Text to Image: 7.568, 23.624, 35.308, 20.0
2019-02-16 11:35:57,149 : Epoch 9 finished
2019-02-16 11:35:57,546 : Image to text: 26.8, 57.3, 73.2, 4.0
2019-02-16 11:35:57,836 : Text to Image: 22.12, 54.82, 71.74, 5.0
2019-02-16 11:35:58,225 : Image to text: 25.7, 57.5, 74.8, 4.0
2019-02-16 11:35:58,531 : Text to Image: 21.52, 53.8, 71.38, 5.0
2019-02-16 11:35:59,548 : Image to text: 26.0, 59.5, 74.4, 4.0
2019-02-16 11:36:00,393 : Text to Image: 21.78, 54.98, 71.14, 4.0
2019-02-16 11:36:01,294 : Image to text: 27.0, 60.1, 76.1, 4.0
2019-02-16 11:36:02,124 : Text to Image: 21.24, 53.64, 70.68, 5.0
2019-02-16 11:36:03,149 : Image to text: 27.2, 61.2, 75.4, 4.0
2019-02-16 11:36:03,744 : Text to Image: 22.2, 54.14, 70.58, 5.0
2019-02-16 11:36:03,744 : Dev mean Text to Image: 21.772, 54.275999999999996, 71.104, 4.8
2019-02-16 11:36:03,744 : Dev mean Image to text: 26.54, 59.12, 74.78, 4.0
2019-02-16 11:36:03,744 : start epoch
2019-02-16 11:36:47,308 : samples : 64000
2019-02-16 11:36:58,618 : Image to text: 9.38, 28.12, 41.38, 16.0
2019-02-16 11:37:06,014 : Text to Image: 7.968, 24.028, 35.516, 20.0
2019-02-16 11:37:48,471 : samples : 128000
2019-02-16 11:37:58,802 : Image to text: 10.06, 28.98, 41.08, 16.0
2019-02-16 11:38:06,075 : Text to Image: 8.696, 25.548, 37.548, 19.0
2019-02-16 11:38:48,739 : samples : 192000
2019-02-16 11:38:59,088 : Image to text: 9.58, 28.06, 40.62, 16.0
2019-02-16 11:39:06,433 : Text to Image: 8.04, 24.664, 36.452, 20.0
2019-02-16 11:39:48,635 : samples : 256000
2019-02-16 11:39:58,903 : Image to text: 9.54, 28.26, 40.96, 16.0
2019-02-16 11:40:06,338 : Text to Image: 7.984, 24.376, 36.136, 20.0
2019-02-16 11:40:48,554 : samples : 320000
2019-02-16 11:41:01,134 : Image to text: 9.26, 27.22, 40.1, 17.0
2019-02-16 11:41:09,208 : Text to Image: 7.456, 23.432, 35.084, 21.0
2019-02-16 11:41:55,497 : samples : 384000
2019-02-16 11:42:07,240 : Image to text: 10.12, 27.48, 40.0, 16.0
2019-02-16 11:42:15,839 : Text to Image: 8.016, 24.284, 36.264, 20.0
2019-02-16 11:43:01,605 : samples : 448000
2019-02-16 11:43:12,657 : Image to text: 9.46, 28.94, 42.14, 15.0
2019-02-16 11:43:20,051 : Text to Image: 7.972, 24.504, 36.24, 20.0
2019-02-16 11:44:03,359 : samples : 512000
2019-02-16 11:44:14,217 : Image to text: 10.18, 27.42, 40.38, 16.0
2019-02-16 11:44:21,350 : Text to Image: 7.684, 23.98, 36.188, 20.0
2019-02-16 11:44:58,905 : Epoch 10 finished
2019-02-16 11:44:59,589 : Image to text: 28.1, 59.2, 74.2, 4.0
2019-02-16 11:44:59,953 : Text to Image: 22.56, 55.34, 72.66, 4.0
2019-02-16 11:45:00,407 : Image to text: 25.4, 58.6, 73.0, 4.0
2019-02-16 11:45:00,769 : Text to Image: 21.36, 54.6, 71.3, 5.0
2019-02-16 11:45:01,245 : Image to text: 26.0, 59.2, 74.4, 4.0
2019-02-16 11:45:01,617 : Text to Image: 21.88, 55.14, 71.78, 5.0
2019-02-16 11:45:02,045 : Image to text: 25.7, 60.8, 75.7, 4.0
2019-02-16 11:45:02,329 : Text to Image: 22.06, 54.18, 71.18, 5.0
2019-02-16 11:45:02,707 : Image to text: 26.3, 59.7, 75.2, 4.0
2019-02-16 11:45:02,991 : Text to Image: 22.16, 54.48, 71.18, 5.0
2019-02-16 11:45:02,991 : Dev mean Text to Image: 22.003999999999998, 54.748, 71.62, 4.8
2019-02-16 11:45:02,991 : Dev mean Image to text: 26.299999999999997, 59.5, 74.5, 4.0
2019-02-16 11:45:02,992 : start epoch
2019-02-16 11:45:45,913 : samples : 64000
2019-02-16 11:45:58,672 : Image to text: 10.08, 28.4, 40.78, 16.0
2019-02-16 11:46:06,144 : Text to Image: 7.82, 24.076, 35.884, 20.0
2019-02-16 11:46:49,334 : samples : 128000
2019-02-16 11:47:01,850 : Image to text: 9.8, 28.14, 40.58, 16.0
2019-02-16 11:47:11,788 : Text to Image: 8.216, 24.104, 35.828, 20.0
2019-02-16 11:47:56,991 : samples : 192000
2019-02-16 11:48:09,555 : Image to text: 9.46, 27.74, 40.5, 16.0
2019-02-16 11:48:19,583 : Text to Image: 8.16, 24.576, 36.64, 19.0
2019-02-16 11:49:04,263 : samples : 256000
2019-02-16 11:49:16,829 : Image to text: 10.08, 28.82, 41.6, 16.0
2019-02-16 11:49:26,840 : Text to Image: 8.428, 24.8, 36.692, 19.0
2019-02-16 11:50:12,359 : samples : 320000
2019-02-16 11:50:24,864 : Image to text: 9.76, 29.38, 41.98, 15.0
2019-02-16 11:50:34,864 : Text to Image: 7.848, 24.632, 36.604, 19.0
2019-02-16 11:51:19,843 : samples : 384000
2019-02-16 11:51:32,416 : Image to text: 10.12, 28.94, 42.06, 15.0
2019-02-16 11:51:42,424 : Text to Image: 8.452, 25.252, 37.204, 19.0
2019-02-16 11:52:27,066 : samples : 448000
2019-02-16 11:52:39,620 : Image to text: 9.9, 28.96, 42.04, 15.0
2019-02-16 11:52:49,633 : Text to Image: 8.132, 25.176, 37.176, 19.0
2019-02-16 11:53:34,672 : samples : 512000
2019-02-16 11:53:47,292 : Image to text: 9.72, 28.44, 41.48, 16.0
2019-02-16 11:53:57,358 : Text to Image: 8.272, 24.592, 36.572, 19.0
2019-02-16 11:54:35,741 : Epoch 11 finished
2019-02-16 11:54:36,681 : Image to text: 24.2, 57.1, 74.0, 4.0
2019-02-16 11:54:37,413 : Text to Image: 21.14, 54.88, 72.54, 5.0
2019-02-16 11:54:38,334 : Image to text: 26.0, 59.7, 73.1, 4.0
2019-02-16 11:54:39,103 : Text to Image: 20.86, 54.24, 71.86, 5.0
2019-02-16 11:54:40,037 : Image to text: 25.6, 58.4, 73.1, 4.0
2019-02-16 11:54:40,784 : Text to Image: 21.52, 54.56, 71.9, 5.0
2019-02-16 11:54:41,724 : Image to text: 24.9, 61.4, 75.3, 4.0
2019-02-16 11:54:42,503 : Text to Image: 21.28, 54.46, 71.72, 5.0
2019-02-16 11:54:43,436 : Image to text: 26.4, 59.3, 74.5, 4.0
2019-02-16 11:54:44,205 : Text to Image: 22.02, 54.44, 71.84, 5.0
2019-02-16 11:54:44,205 : Dev mean Text to Image: 21.364, 54.516, 71.97200000000001, 5.0
2019-02-16 11:54:44,205 : Dev mean Image to text: 25.42, 59.18, 74.0, 4.0
2019-02-16 11:54:44,205 : start epoch
2019-02-16 11:55:29,014 : samples : 64000
2019-02-16 11:55:41,617 : Image to text: 10.06, 28.44, 41.22, 15.0
2019-02-16 11:55:51,655 : Text to Image: 8.252, 24.644, 36.848, 19.0
2019-02-16 11:56:35,648 : samples : 128000
2019-02-16 11:56:48,292 : Image to text: 10.3, 28.88, 41.54, 15.0
2019-02-16 11:56:58,309 : Text to Image: 8.172, 24.876, 36.588, 19.0
2019-02-16 11:57:43,974 : samples : 192000
2019-02-16 11:57:56,033 : Image to text: 10.46, 28.76, 42.08, 15.0
2019-02-16 11:58:06,200 : Text to Image: 8.528, 25.228, 37.324, 19.0
2019-02-16 11:58:52,732 : samples : 256000
2019-02-16 11:59:04,595 : Image to text: 10.6, 29.12, 41.66, 15.0
2019-02-16 11:59:12,768 : Text to Image: 8.276, 25.228, 37.564, 19.0
2019-02-16 11:59:58,739 : samples : 320000
2019-02-16 12:00:11,621 : Image to text: 9.02, 27.88, 40.88, 17.0
2019-02-16 12:00:22,002 : Text to Image: 7.768, 23.6, 35.068, 20.0
2019-02-16 12:01:07,620 : samples : 384000
2019-02-16 12:01:20,483 : Image to text: 10.0, 28.88, 41.02, 16.0
2019-02-16 12:01:30,878 : Text to Image: 8.564, 25.36, 37.464, 19.0
2019-02-16 12:02:17,164 : samples : 448000
2019-02-16 12:02:30,033 : Image to text: 10.18, 29.06, 42.1, 15.0
2019-02-16 12:02:40,503 : Text to Image: 8.3, 25.316, 37.152, 19.0
2019-02-16 12:03:26,569 : samples : 512000
2019-02-16 12:03:39,477 : Image to text: 9.48, 28.5, 40.6, 17.0
2019-02-16 12:03:49,898 : Text to Image: 7.988, 24.572, 36.628, 19.0
2019-02-16 12:04:29,291 : Epoch 12 finished
2019-02-16 12:04:30,363 : Image to text: 24.8, 59.2, 73.0, 4.0
2019-02-16 12:04:31,320 : Text to Image: 21.9, 55.18, 72.38, 5.0
2019-02-16 12:04:32,330 : Image to text: 26.7, 61.4, 74.9, 4.0
2019-02-16 12:04:33,181 : Text to Image: 22.14, 54.52, 72.58, 5.0
2019-02-16 12:04:34,301 : Image to text: 26.4, 60.6, 73.5, 4.0
2019-02-16 12:04:35,165 : Text to Image: 22.3, 56.16, 72.7, 4.0
2019-02-16 12:04:36,261 : Image to text: 27.4, 60.0, 76.0, 4.0
2019-02-16 12:04:37,221 : Text to Image: 22.58, 55.56, 72.06, 4.0
2019-02-16 12:04:38,298 : Image to text: 27.9, 59.6, 76.0, 4.0
2019-02-16 12:04:39,145 : Text to Image: 22.42, 54.76, 71.98, 4.0
2019-02-16 12:04:39,145 : Dev mean Text to Image: 22.268, 55.236, 72.34, 4.3999999999999995
2019-02-16 12:04:39,145 : Dev mean Image to text: 26.64, 60.16, 74.68, 4.0
2019-02-16 12:04:39,146 : start epoch
2019-02-16 12:05:25,209 : samples : 64000
2019-02-16 12:05:38,094 : Image to text: 9.92, 28.76, 41.54, 15.0
2019-02-16 12:05:48,401 : Text to Image: 8.256, 24.656, 36.74, 19.0
2019-02-16 12:06:34,126 : samples : 128000
2019-02-16 12:06:47,003 : Image to text: 9.78, 29.44, 41.82, 15.0
2019-02-16 12:06:56,602 : Text to Image: 8.308, 25.168, 37.04, 19.0
2019-02-16 12:07:42,013 : samples : 192000
2019-02-16 12:07:52,603 : Image to text: 9.64, 28.64, 41.46, 16.0
2019-02-16 12:08:00,170 : Text to Image: 7.916, 24.724, 36.612, 19.0
2019-02-16 12:08:43,019 : samples : 256000
2019-02-16 12:08:53,550 : Image to text: 9.82, 29.64, 42.6, 15.0
2019-02-16 12:09:01,108 : Text to Image: 8.444, 25.332, 37.34, 19.0
2019-02-16 12:09:43,478 : samples : 320000
2019-02-16 12:09:53,989 : Image to text: 9.74, 27.38, 40.64, 16.0
2019-02-16 12:10:01,566 : Text to Image: 8.488, 25.256, 37.2, 19.0
2019-02-16 12:10:44,146 : samples : 384000
2019-02-16 12:10:54,665 : Image to text: 10.14, 28.44, 41.82, 15.0
2019-02-16 12:11:02,250 : Text to Image: 8.164, 25.112, 37.292, 19.0
2019-02-16 12:11:44,848 : samples : 448000
2019-02-16 12:11:55,353 : Image to text: 10.34, 28.64, 42.26, 15.0
2019-02-16 12:12:02,906 : Text to Image: 8.328, 25.364, 37.204, 19.0
2019-02-16 12:12:46,252 : samples : 512000
2019-02-16 12:12:56,722 : Image to text: 9.72, 29.0, 41.98, 16.0
2019-02-16 12:13:04,238 : Text to Image: 7.948, 24.616, 36.676, 19.0
2019-02-16 12:13:40,653 : Epoch 13 finished
2019-02-16 12:13:41,092 : Image to text: 26.8, 57.5, 73.7, 4.0
2019-02-16 12:13:41,411 : Text to Image: 22.44, 56.2, 73.22, 4.0
2019-02-16 12:13:41,859 : Image to text: 26.3, 58.2, 74.8, 4.0
2019-02-16 12:13:42,196 : Text to Image: 22.28, 55.42, 72.14, 4.0
2019-02-16 12:13:42,664 : Image to text: 26.8, 59.1, 74.8, 3.0
2019-02-16 12:13:43,012 : Text to Image: 22.48, 56.98, 73.44, 4.0
2019-02-16 12:13:43,473 : Image to text: 27.6, 61.4, 76.5, 4.0
2019-02-16 12:13:43,818 : Text to Image: 22.52, 55.88, 72.9, 4.0
2019-02-16 12:13:44,258 : Image to text: 28.6, 60.5, 75.3, 3.0
2019-02-16 12:13:44,592 : Text to Image: 22.46, 55.46, 71.94, 5.0
2019-02-16 12:13:44,592 : Dev mean Text to Image: 22.436000000000003, 55.988, 72.72800000000001, 4.2
2019-02-16 12:13:44,592 : Dev mean Image to text: 27.22, 59.34, 75.02, 3.6
2019-02-16 12:13:44,592 : start epoch
2019-02-16 12:14:27,305 : samples : 64000
2019-02-16 12:14:37,554 : Image to text: 10.24, 28.5, 41.32, 15.0
2019-02-16 12:14:45,046 : Text to Image: 8.184, 25.12, 37.224, 19.0
2019-02-16 12:15:27,444 : samples : 128000
2019-02-16 12:15:37,862 : Image to text: 10.1, 29.46, 41.62, 15.0
2019-02-16 12:15:46,244 : Text to Image: 8.236, 25.252, 37.22, 19.0
2019-02-16 12:16:37,256 : samples : 192000
2019-02-16 12:16:47,807 : Image to text: 10.64, 29.22, 41.72, 15.0
2019-02-16 12:16:55,346 : Text to Image: 8.48, 25.228, 37.18, 19.0
2019-02-16 12:17:38,156 : samples : 256000
2019-02-16 12:17:48,688 : Image to text: 10.16, 29.54, 42.04, 15.0
2019-02-16 12:17:56,211 : Text to Image: 8.5, 25.524, 37.852, 18.0
2019-02-16 12:18:39,001 : samples : 320000
2019-02-16 12:18:49,539 : Image to text: 9.88, 27.86, 41.14, 15.0
2019-02-16 12:18:57,072 : Text to Image: 8.324, 25.176, 37.248, 19.0
2019-02-16 12:19:38,881 : samples : 384000
2019-02-16 12:19:49,531 : Image to text: 10.18, 29.0, 42.3, 15.0
2019-02-16 12:19:57,095 : Text to Image: 8.212, 24.988, 37.184, 19.0
2019-02-16 12:20:39,750 : samples : 448000
2019-02-16 12:20:50,290 : Image to text: 9.96, 28.54, 42.18, 15.0
2019-02-16 12:20:57,881 : Text to Image: 8.476, 25.588, 37.604, 18.0
2019-02-16 12:21:40,293 : samples : 512000
2019-02-16 12:21:50,790 : Image to text: 10.76, 30.12, 43.04, 15.0
2019-02-16 12:21:58,404 : Text to Image: 8.6, 25.648, 37.7, 18.0
2019-02-16 12:22:34,345 : Epoch 14 finished
2019-02-16 12:22:34,793 : Image to text: 26.3, 60.2, 75.4, 4.0
2019-02-16 12:22:35,121 : Text to Image: 22.78, 55.9, 73.8, 4.0
2019-02-16 12:22:35,554 : Image to text: 26.9, 59.0, 74.8, 4.0
2019-02-16 12:22:35,903 : Text to Image: 22.34, 56.2, 72.62, 5.0
2019-02-16 12:22:36,366 : Image to text: 28.0, 58.6, 73.2, 4.0
2019-02-16 12:22:36,710 : Text to Image: 22.56, 56.88, 72.56, 4.0
2019-02-16 12:22:37,151 : Image to text: 27.9, 59.6, 75.2, 4.0
2019-02-16 12:22:37,495 : Text to Image: 21.92, 55.84, 73.02, 5.0
2019-02-16 12:22:37,944 : Image to text: 29.5, 61.4, 75.8, 3.0
2019-02-16 12:22:38,277 : Text to Image: 23.44, 55.48, 72.24, 5.0
2019-02-16 12:22:38,277 : Dev mean Text to Image: 22.608000000000004, 56.06, 72.848, 4.6
2019-02-16 12:22:38,277 : Dev mean Image to text: 27.72, 59.760000000000005, 74.88, 3.8000000000000003
2019-02-16 12:22:38,278 : start epoch
2019-02-16 12:23:21,714 : samples : 64000
2019-02-16 12:23:31,935 : Image to text: 10.68, 29.82, 42.68, 15.0
2019-02-16 12:23:39,355 : Text to Image: 8.796, 25.38, 37.536, 18.0
2019-02-16 12:24:22,559 : samples : 128000
2019-02-16 12:24:32,815 : Image to text: 10.64, 29.38, 41.98, 15.0
2019-02-16 12:24:40,297 : Text to Image: 8.196, 25.356, 37.372, 18.0
2019-02-16 12:25:22,764 : samples : 192000
2019-02-16 12:25:33,293 : Image to text: 10.08, 29.1, 42.16, 15.0
2019-02-16 12:25:40,898 : Text to Image: 8.42, 25.62, 37.388, 18.0
2019-02-16 12:26:23,754 : samples : 256000
2019-02-16 12:26:34,311 : Image to text: 10.68, 29.9, 42.5, 15.0
2019-02-16 12:26:41,868 : Text to Image: 8.488, 25.448, 37.652, 18.0
2019-02-16 12:27:24,360 : samples : 320000
2019-02-16 12:27:34,850 : Image to text: 10.04, 27.9, 41.7, 15.0
2019-02-16 12:27:42,394 : Text to Image: 8.788, 25.904, 37.848, 18.0
2019-02-16 12:28:25,322 : samples : 384000
2019-02-16 12:28:35,852 : Image to text: 9.94, 28.48, 41.1, 15.0
2019-02-16 12:28:43,412 : Text to Image: 8.36, 25.18, 36.88, 19.0
2019-02-16 12:29:25,721 : samples : 448000
2019-02-16 12:29:36,220 : Image to text: 10.72, 28.88, 42.36, 15.0
2019-02-16 12:29:43,764 : Text to Image: 8.836, 25.784, 38.076, 18.0
2019-02-16 12:30:26,301 : samples : 512000
2019-02-16 12:30:36,849 : Image to text: 10.04, 29.14, 41.38, 15.0
2019-02-16 12:30:44,377 : Text to Image: 8.524, 25.328, 37.204, 19.0
2019-02-16 12:31:21,151 : Epoch 15 finished
2019-02-16 12:31:21,587 : Image to text: 28.2, 60.9, 76.5, 4.0
2019-02-16 12:31:21,904 : Text to Image: 22.84, 56.32, 73.16, 4.0
2019-02-16 12:31:22,319 : Image to text: 27.8, 62.8, 76.1, 4.0
2019-02-16 12:31:22,655 : Text to Image: 22.16, 55.84, 72.34, 4.0
2019-02-16 12:31:23,101 : Image to text: 26.4, 59.7, 74.4, 4.0
2019-02-16 12:31:23,437 : Text to Image: 22.02, 55.86, 73.02, 4.0
2019-02-16 12:31:23,899 : Image to text: 28.3, 61.9, 77.5, 3.0
2019-02-16 12:31:24,245 : Text to Image: 22.14, 55.62, 72.08, 5.0
2019-02-16 12:31:24,693 : Image to text: 28.3, 61.3, 78.2, 4.0
2019-02-16 12:31:25,044 : Text to Image: 22.58, 54.88, 71.74, 5.0
2019-02-16 12:31:25,044 : Dev mean Text to Image: 22.348, 55.70399999999999, 72.468, 4.4
2019-02-16 12:31:25,044 : Dev mean Image to text: 27.799999999999997, 61.32, 76.53999999999999, 3.8000000000000007
2019-02-16 12:31:25,045 : start epoch
2019-02-16 12:32:07,520 : samples : 64000
2019-02-16 12:32:17,654 : Image to text: 10.52, 29.66, 42.4, 15.0
2019-02-16 12:32:24,836 : Text to Image: 8.488, 25.5, 37.852, 18.0
2019-02-16 12:33:17,174 : samples : 128000
2019-02-16 12:33:27,476 : Image to text: 10.08, 28.94, 42.26, 15.0
2019-02-16 12:33:34,924 : Text to Image: 8.48, 25.328, 37.168, 19.0
2019-02-16 12:34:17,551 : samples : 192000
2019-02-16 12:34:28,069 : Image to text: 10.08, 28.34, 41.56, 15.0
2019-02-16 12:34:35,640 : Text to Image: 8.424, 25.368, 37.736, 19.0
2019-02-16 12:35:18,550 : samples : 256000
2019-02-16 12:35:29,020 : Image to text: 10.28, 28.72, 42.24, 15.0
2019-02-16 12:35:36,599 : Text to Image: 8.608, 25.676, 37.86, 18.0
2019-02-16 12:36:19,219 : samples : 320000
2019-02-16 12:36:29,757 : Image to text: 10.5, 28.92, 41.94, 15.0
2019-02-16 12:36:37,294 : Text to Image: 8.348, 25.272, 37.328, 19.0
2019-02-16 12:37:19,700 : samples : 384000
2019-02-16 12:37:30,223 : Image to text: 10.56, 28.88, 42.04, 15.0
2019-02-16 12:37:37,753 : Text to Image: 8.624, 25.504, 37.688, 18.0
2019-02-16 12:38:20,203 : samples : 448000
2019-02-16 12:38:30,741 : Image to text: 10.26, 29.98, 42.78, 14.0
2019-02-16 12:38:38,278 : Text to Image: 8.728, 25.984, 37.944, 18.0
2019-02-16 12:39:21,285 : samples : 512000
2019-02-16 12:39:31,756 : Image to text: 10.26, 30.38, 42.84, 15.0
2019-02-16 12:39:39,249 : Text to Image: 8.5, 25.816, 38.144, 18.0
2019-02-16 12:40:15,270 : Epoch 16 finished
2019-02-16 12:40:15,719 : Image to text: 26.1, 57.9, 74.7, 4.0
2019-02-16 12:40:16,035 : Text to Image: 22.48, 55.54, 73.26, 4.0
2019-02-16 12:40:16,475 : Image to text: 26.1, 59.3, 75.8, 4.0
2019-02-16 12:40:16,817 : Text to Image: 21.9, 56.22, 73.2, 4.0
2019-02-16 12:40:17,280 : Image to text: 28.0, 60.4, 74.4, 4.0
2019-02-16 12:40:17,627 : Text to Image: 21.78, 56.06, 73.12, 4.0
2019-02-16 12:40:18,100 : Image to text: 27.6, 61.0, 77.1, 4.0
2019-02-16 12:40:18,450 : Text to Image: 22.0, 56.62, 73.1, 4.0
2019-02-16 12:40:18,903 : Image to text: 29.7, 61.4, 75.5, 4.0
2019-02-16 12:40:19,249 : Text to Image: 23.34, 55.34, 72.1, 4.0
2019-02-16 12:40:19,249 : Dev mean Text to Image: 22.3, 55.956, 72.956, 4.0
2019-02-16 12:40:19,249 : Dev mean Image to text: 27.5, 60.0, 75.5, 4.0
2019-02-16 12:40:19,249 : start epoch
2019-02-16 12:41:02,430 : samples : 64000
2019-02-16 12:41:12,608 : Image to text: 10.98, 29.96, 42.16, 15.0
2019-02-16 12:41:20,044 : Text to Image: 8.888, 26.304, 38.572, 18.0
2019-02-16 12:42:03,071 : samples : 128000
2019-02-16 12:42:13,315 : Image to text: 11.34, 30.32, 42.84, 14.0
2019-02-16 12:42:20,775 : Text to Image: 8.768, 26.1, 38.356, 18.0
2019-02-16 12:43:03,561 : samples : 192000
2019-02-16 12:43:14,034 : Image to text: 10.78, 29.28, 42.14, 15.0
2019-02-16 12:43:21,591 : Text to Image: 8.58, 25.34, 37.552, 18.0
2019-02-16 12:44:04,070 : samples : 256000
2019-02-16 12:44:14,577 : Image to text: 10.48, 29.6, 42.66, 15.0
2019-02-16 12:44:22,146 : Text to Image: 8.7, 25.808, 37.936, 18.0
2019-02-16 12:45:04,846 : samples : 320000
2019-02-16 12:45:15,411 : Image to text: 10.34, 30.52, 42.66, 15.0
2019-02-16 12:45:23,017 : Text to Image: 8.356, 25.296, 37.296, 18.0
2019-02-16 12:46:05,574 : samples : 384000
2019-02-16 12:46:16,095 : Image to text: 10.28, 29.62, 42.08, 15.0
2019-02-16 12:46:23,641 : Text to Image: 8.652, 25.704, 37.832, 18.0
2019-02-16 12:47:06,102 : samples : 448000
2019-02-16 12:47:16,676 : Image to text: 10.16, 28.56, 41.18, 16.0
2019-02-16 12:47:24,241 : Text to Image: 8.532, 25.272, 37.44, 19.0
2019-02-16 12:48:07,250 : samples : 512000
2019-02-16 12:48:17,749 : Image to text: 10.94, 29.8, 43.52, 14.0
2019-02-16 12:48:25,293 : Text to Image: 8.48, 25.916, 38.068, 18.0
2019-02-16 12:49:01,991 : Epoch 17 finished
2019-02-16 12:49:02,434 : Image to text: 27.7, 58.7, 75.5, 4.0
2019-02-16 12:49:02,764 : Text to Image: 23.18, 56.6, 73.5, 4.0
2019-02-16 12:49:03,201 : Image to text: 27.3, 59.8, 75.3, 4.0
2019-02-16 12:49:03,539 : Text to Image: 22.2, 54.6, 72.72, 5.0
2019-02-16 12:49:03,991 : Image to text: 26.4, 59.9, 74.9, 4.0
2019-02-16 12:49:04,333 : Text to Image: 22.06, 56.66, 72.92, 4.0
2019-02-16 12:49:04,798 : Image to text: 28.5, 61.8, 77.5, 4.0
2019-02-16 12:49:05,140 : Text to Image: 22.54, 56.5, 73.0, 4.0
2019-02-16 12:49:05,596 : Image to text: 28.4, 61.5, 76.6, 4.0
2019-02-16 12:49:05,940 : Text to Image: 23.18, 55.54, 72.38, 4.0
2019-02-16 12:49:05,940 : Dev mean Text to Image: 22.631999999999998, 55.980000000000004, 72.904, 4.2
2019-02-16 12:49:05,940 : Dev mean Image to text: 27.66, 60.34, 75.96, 4.0
2019-02-16 12:49:09,755 : 
Test scores | Image to text:             27.400000000000006, 59.92, 74.72, 3.6
2019-02-16 12:49:09,755 : Test scores | Text to image:             21.908, 54.732, 72.072, 4.4

2019-02-16 12:49:09,873 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 12:49:10,080 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 12:49:10,748 : loading BERT model bert-base-uncased
2019-02-16 12:49:10,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 12:49:10,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 12:49:10,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3kn0kfss
2019-02-16 12:49:13,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 12:49:14,665 : Computing embeddings for train/dev/test
2019-02-16 12:50:53,667 : Computed embeddings
2019-02-16 12:50:53,667 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 12:51:46,845 : [('reg:1e-05', 67.55), ('reg:0.0001', 67.85), ('reg:0.001', 57.27), ('reg:0.01', 45.01)]
2019-02-16 12:51:46,845 : Validation : best param found is reg = 0.0001 with score             67.85
2019-02-16 12:51:46,845 : Evaluating...
2019-02-16 12:51:59,151 : 
Dev acc : 67.8 Test acc : 68.9 for LENGTH classification

2019-02-16 12:51:59,152 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 12:51:59,528 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 12:51:59,580 : loading BERT model bert-base-uncased
2019-02-16 12:51:59,580 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 12:51:59,611 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 12:51:59,611 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphrd55dnl
2019-02-16 12:52:02,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 12:52:03,619 : Computing embeddings for train/dev/test
2019-02-16 12:53:31,749 : Computed embeddings
2019-02-16 12:53:31,749 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 12:54:32,179 : [('reg:1e-05', 23.72), ('reg:0.0001', 5.3), ('reg:0.001', 0.94), ('reg:0.01', 0.47)]
2019-02-16 12:54:32,180 : Validation : best param found is reg = 1e-05 with score             23.72
2019-02-16 12:54:32,180 : Evaluating...
2019-02-16 12:54:50,894 : 
Dev acc : 23.7 Test acc : 23.0 for WORDCONTENT classification

2019-02-16 12:54:50,896 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 12:54:51,276 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 12:54:51,351 : loading BERT model bert-base-uncased
2019-02-16 12:54:51,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 12:54:51,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 12:54:51,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgz67fokr
2019-02-16 12:54:53,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 12:54:55,317 : Computing embeddings for train/dev/test
2019-02-16 12:56:18,237 : Computed embeddings
2019-02-16 12:56:18,237 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 12:57:01,687 : [('reg:1e-05', 29.11), ('reg:0.0001', 29.13), ('reg:0.001', 27.82), ('reg:0.01', 24.04)]
2019-02-16 12:57:01,687 : Validation : best param found is reg = 0.0001 with score             29.13
2019-02-16 12:57:01,687 : Evaluating...
2019-02-16 12:57:14,836 : 
Dev acc : 29.1 Test acc : 30.0 for DEPTH classification

2019-02-16 12:57:14,837 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 12:57:15,232 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 12:57:15,303 : loading BERT model bert-base-uncased
2019-02-16 12:57:15,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 12:57:15,427 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 12:57:15,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmfwae43q
2019-02-16 12:57:17,912 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 12:57:19,377 : Computing embeddings for train/dev/test
2019-02-16 12:58:36,824 : Computed embeddings
2019-02-16 12:58:36,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 12:59:22,374 : [('reg:1e-05', 76.11), ('reg:0.0001', 75.39), ('reg:0.001', 70.53), ('reg:0.01', 58.87)]
2019-02-16 12:59:22,374 : Validation : best param found is reg = 1e-05 with score             76.11
2019-02-16 12:59:22,375 : Evaluating...
2019-02-16 12:59:33,084 : 
Dev acc : 76.1 Test acc : 76.7 for TOPCONSTITUENTS classification

2019-02-16 12:59:33,085 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 12:59:33,679 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 12:59:33,751 : loading BERT model bert-base-uncased
2019-02-16 12:59:33,751 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 12:59:33,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 12:59:33,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu_vogei6
2019-02-16 12:59:36,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 12:59:37,794 : Computing embeddings for train/dev/test
2019-02-16 13:01:01,871 : Computed embeddings
2019-02-16 13:01:01,871 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:01:46,226 : [('reg:1e-05', 81.91), ('reg:0.0001', 81.91), ('reg:0.001', 81.23), ('reg:0.01', 79.04)]
2019-02-16 13:01:46,227 : Validation : best param found is reg = 1e-05 with score             81.91
2019-02-16 13:01:46,227 : Evaluating...
2019-02-16 13:01:57,595 : 
Dev acc : 81.9 Test acc : 81.2 for BIGRAMSHIFT classification

2019-02-16 13:01:57,596 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 13:01:57,986 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 13:01:58,060 : loading BERT model bert-base-uncased
2019-02-16 13:01:58,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:01:58,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:01:58,193 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsric5wk8
2019-02-16 13:02:00,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:02:02,168 : Computing embeddings for train/dev/test
2019-02-16 13:03:24,334 : Computed embeddings
2019-02-16 13:03:24,334 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:04:11,225 : [('reg:1e-05', 89.84), ('reg:0.0001', 89.68), ('reg:0.001', 90.4), ('reg:0.01', 89.74)]
2019-02-16 13:04:11,225 : Validation : best param found is reg = 0.001 with score             90.4
2019-02-16 13:04:11,226 : Evaluating...
2019-02-16 13:04:20,321 : 
Dev acc : 90.4 Test acc : 89.3 for TENSE classification

2019-02-16 13:04:20,322 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 13:04:20,920 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 13:04:20,991 : loading BERT model bert-base-uncased
2019-02-16 13:04:20,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:04:21,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:04:21,030 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxk8shp_a
2019-02-16 13:04:23,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:04:24,966 : Computing embeddings for train/dev/test
2019-02-16 13:05:51,865 : Computed embeddings
2019-02-16 13:05:51,865 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:06:22,907 : [('reg:1e-05', 86.94), ('reg:0.0001', 87.01), ('reg:0.001', 86.69), ('reg:0.01', 85.53)]
2019-02-16 13:06:22,907 : Validation : best param found is reg = 0.0001 with score             87.01
2019-02-16 13:06:22,907 : Evaluating...
2019-02-16 13:06:28,536 : 
Dev acc : 87.0 Test acc : 87.6 for SUBJNUMBER classification

2019-02-16 13:06:28,537 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 13:06:29,356 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 13:06:29,449 : loading BERT model bert-base-uncased
2019-02-16 13:06:29,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:06:29,494 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:06:29,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr9uvbed2
2019-02-16 13:06:32,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:06:35,068 : Computing embeddings for train/dev/test
2019-02-16 13:08:02,757 : Computed embeddings
2019-02-16 13:08:02,757 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:08:53,991 : [('reg:1e-05', 77.52), ('reg:0.0001', 77.57), ('reg:0.001', 77.93), ('reg:0.01', 73.52)]
2019-02-16 13:08:53,991 : Validation : best param found is reg = 0.001 with score             77.93
2019-02-16 13:08:53,991 : Evaluating...
2019-02-16 13:09:08,534 : 
Dev acc : 77.9 Test acc : 79.5 for OBJNUMBER classification

2019-02-16 13:09:08,535 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 13:09:08,991 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 13:09:09,073 : loading BERT model bert-base-uncased
2019-02-16 13:09:09,073 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:09:09,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:09:09,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_aam2avn
2019-02-16 13:09:11,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:09:13,072 : Computing embeddings for train/dev/test
2019-02-16 13:10:51,557 : Computed embeddings
2019-02-16 13:10:51,558 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:11:28,662 : [('reg:1e-05', 57.3), ('reg:0.0001', 57.3), ('reg:0.001', 57.12), ('reg:0.01', 56.23)]
2019-02-16 13:11:28,663 : Validation : best param found is reg = 1e-05 with score             57.3
2019-02-16 13:11:28,663 : Evaluating...
2019-02-16 13:11:37,384 : 
Dev acc : 57.3 Test acc : 57.6 for ODDMANOUT classification

2019-02-16 13:11:37,385 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 13:11:37,805 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 13:11:37,893 : loading BERT model bert-base-uncased
2019-02-16 13:11:37,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:11:38,027 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:11:38,027 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpos9npht9
2019-02-16 13:11:40,558 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:11:42,001 : Computing embeddings for train/dev/test
2019-02-16 13:13:18,816 : Computed embeddings
2019-02-16 13:13:18,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:14:14,551 : [('reg:1e-05', 70.07), ('reg:0.0001', 70.05), ('reg:0.001', 69.78), ('reg:0.01', 66.69)]
2019-02-16 13:14:14,551 : Validation : best param found is reg = 1e-05 with score             70.07
2019-02-16 13:14:14,551 : Evaluating...
2019-02-16 13:14:26,883 : 
Dev acc : 70.1 Test acc : 68.6 for COORDINATIONINVERSION classification

2019-02-16 13:14:26,885 : total results: {'STS12': {'MSRpar': {'pearson': (0.22640027039615485, 3.5759914582100204e-10), 'spearman': SpearmanrResult(correlation=0.2787156745428831, pvalue=7.557193941787097e-15), 'nsamples': 750}, 'MSRvid': {'pearson': (0.08201070756469313, 0.02470439543363753), 'spearman': SpearmanrResult(correlation=0.10630907637329233, pvalue=0.0035591467996128195), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4300404691811492, 4.381388110282282e-22), 'spearman': SpearmanrResult(correlation=0.5276565263889252, pvalue=2.915989012612447e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.2694738556275138, 6.057379302429383e-14), 'spearman': SpearmanrResult(correlation=0.2685286736418725, pvalue=7.461001433682861e-14), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5644637630942212, 5.973290003631731e-35), 'spearman': SpearmanrResult(correlation=0.44363999471361804, pvalue=1.139320118169869e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3144778131727464, 'wmean': 0.27542575354569276}, 'spearman': {'mean': 0.3249699891321182, 'wmean': 0.29259066020649493}}}, 'STS13': {'FNWN': {'pearson': (0.11970576959920125, 0.10086600065952624), 'spearman': SpearmanrResult(correlation=0.14087564562642999, pvalue=0.053171513685932204), 'nsamples': 189}, 'headlines': {'pearson': (0.39178474942274394, 6.339388858942566e-29), 'spearman': SpearmanrResult(correlation=0.38279641409583726, pvalue=1.3879914130863963e-27), 'nsamples': 750}, 'OnWN': {'pearson': (0.0237942994462884, 0.5738428299795575), 'spearman': SpearmanrResult(correlation=0.034403766144772724, pvalue=0.41605092592363746), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.17842827282274454, 'wmean': 0.2198743696737832}, 'spearman': {'mean': 0.18602527528901333, 'wmean': 0.22201554693499378}}}, 'STS14': {'deft-forum': {'pearson': (-0.043812419629510845, 0.35379046640623246), 'spearman': SpearmanrResult(correlation=-0.027594521456751325, pvalue=0.559320372620908), 'nsamples': 450}, 'deft-news': {'pearson': (0.3996602267280921, 6.22311942731534e-13), 'spearman': SpearmanrResult(correlation=0.4317471009117585, pvalue=4.719770747266119e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.3830414253544331, 1.277613975492409e-27), 'spearman': SpearmanrResult(correlation=0.36068517910047804, pvalue=1.847625502838219e-24), 'nsamples': 750}, 'images': {'pearson': (0.18503483116784253, 3.341974995764883e-07), 'spearman': SpearmanrResult(correlation=0.19409818927508896, pvalue=8.425744908582733e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.24583473892736435, 8.715757841315688e-12), 'spearman': SpearmanrResult(correlation=0.24560335047914267, pvalue=9.127555925816214e-12), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4330671251420096, 1.2312119500105807e-35), 'spearman': SpearmanrResult(correlation=0.3942585386682461, pvalue=2.665760983577104e-29), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2671376546150384, 'wmean': 0.276110951901036}, 'spearman': {'mean': 0.26646630616299377, 'wmean': 0.2701574770027217}}}, 'STS15': {'answers-forums': {'pearson': (0.2572124378645401, 4.4302525575503883e-07), 'spearman': SpearmanrResult(correlation=0.27612733461420474, pvalue=5.4684403927132705e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.34873437994248824, 7.182838666977683e-23), 'spearman': SpearmanrResult(correlation=0.35476428913867386, pvalue=1.1552874942537296e-23), 'nsamples': 750}, 'belief': {'pearson': (0.37615857441011763, 4.749753088003947e-14), 'spearman': SpearmanrResult(correlation=0.44578461663701086, pvalue=1.0411043328867419e-19), 'nsamples': 375}, 'headlines': {'pearson': (0.43133670498000176, 2.4592930497925e-35), 'spearman': SpearmanrResult(correlation=0.44051337945763597, pvalue=5.988678295773399e-37), 'nsamples': 750}, 'images': {'pearson': (0.09403549512372665, 0.009975301782076612), 'spearman': SpearmanrResult(correlation=0.13116563375830023, pvalue=0.0003161444792519731), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3014955184641749, 'wmean': 0.29769802154588637}, 'spearman': {'mean': 0.3296710507211651, 'wmean': 0.3218498194950544}}}, 'STS16': {'answer-answer': {'pearson': (0.36577856727399816, 1.851050060199976e-09), 'spearman': SpearmanrResult(correlation=0.38510218631668625, pvalue=2.0979316172554138e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5119087908138538, 4.925617910799115e-18), 'spearman': SpearmanrResult(correlation=0.5179913546506333, pvalue=1.6991253925464661e-18), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5701685836597956, 3.1370613451012896e-21), 'spearman': SpearmanrResult(correlation=0.63351982949809, pvalue=3.280206591987593e-27), 'nsamples': 230}, 'postediting': {'pearson': (0.6436571576573903, 6.132537588078914e-30), 'spearman': SpearmanrResult(correlation=0.7545230771407708, pvalue=3.673797839071777e-46), 'nsamples': 244}, 'question-question': {'pearson': (0.08569715166274253, 0.2172960498209676), 'spearman': SpearmanrResult(correlation=0.18107711429897685, pvalue=0.00869477491586174), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4354420502135561, 'wmean': 0.44390798516687563}, 'spearman': {'mean': 0.49444271238103144, 'wmean': 0.5012260633455657}}}, 'MR': {'devacc': 73.03, 'acc': 73.03, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.76, 'acc': 75.74, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.51, 'acc': 86.0, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.28, 'acc': 94.35, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.1, 'acc': 78.14, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.87, 'acc': 42.76, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.44, 'acc': 86.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.49, 'acc': 68.06, 'f1': 80.54, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.8, 'acc': 71.48, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6866385385093505, 'pearson': 0.6971499229597029, 'spearman': 0.6422100419948958, 'mse': 0.5262799125307982, 'yhat': array([2.89347036, 4.02375374, 2.05523302, ..., 3.2186546 , 4.44047924,        4.29090893]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.48173077157097627, 'pearson': 0.46557109337882074, 'spearman': 0.4628866686484039, 'mse': 1.9249203571621725, 'yhat': array([2.35286464, 2.04092713, 3.27200237, ..., 3.72951862, 3.6548544 ,        3.75573437]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 56.37, 'acc': 56.29, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 316.17999999999995, 'acc': [(27.400000000000006, 59.92, 74.72, 3.6), (21.908, 54.732, 72.072, 4.4)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 67.85, 'acc': 68.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 23.72, 'acc': 22.98, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.13, 'acc': 30.03, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 76.11, 'acc': 76.73, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.91, 'acc': 81.17, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.4, 'acc': 89.28, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.01, 'acc': 87.57, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.93, 'acc': 79.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.3, 'acc': 57.59, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.07, 'acc': 68.64, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 13:14:26,885 : STS12 p=0.2754, STS12 s=0.2926, STS13 p=0.2199, STS13 s=0.2220, STS14 p=0.2761, STS14 s=0.2702, STS15 p=0.2977, STS15 s=0.3218, STS 16 p=0.4439, STS16 s=0.5012, STS B p=0.4656, STS B s=0.4629, STS B m=1.9249, SICK-R p=0.6971, SICK-R s=0.6422, SICK-P m=0.5263
2019-02-16 13:14:26,885 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 13:14:26,885 : 0.2754,0.2926,0.2199,0.2220,0.2761,0.2702,0.2977,0.3218,0.4439,0.5012,0.4656,0.4629,1.9249,0.6971,0.6422,0.5263
2019-02-16 13:14:26,886 : MR=73.03, CR=75.74, SUBJ=94.35, MPQA=86.00, SST-B=78.14, SST-F=42.76, TREC=86.60, SICK-E=71.48, SNLI=56.29, MRPC=68.06, MRPC f=80.54
2019-02-16 13:14:26,886 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 13:14:26,886 : 73.03,75.74,94.35,86.00,78.14,42.76,86.60,71.48,56.29,68.06,80.54
2019-02-16 13:14:26,886 : COCO r1i2t=27.40, COCO r5i2t=59.92, COCO r10i2t=74.72, COCO medr_i2t=3.60, COCO r1t2i=21.91, COCO r5t2i=54.73, COCO r10t2i=72.07, COCO medr_t2i=4.40
2019-02-16 13:14:26,886 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 13:14:26,886 : 27.40,59.92,74.72,3.60,21.91,54.73,72.07,4.40
2019-02-16 13:14:26,886 : SentLen=68.86, WC=22.98, TreeDepth=30.03, TopConst=76.73, BShift=81.17, Tense=89.28, SubjNum=87.57, ObjNum=79.55, SOMO=57.59, CoordInv=68.64, average=66.24
2019-02-16 13:14:26,886 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 13:14:26,886 : 68.86,22.98,30.03,76.73,81.17,89.28,87.57,79.55,57.59,68.64,66.24
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,886 : layer 8
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,886 : ********************************************************************************
2019-02-16 13:14:26,988 : ***** Transfer task : STS12 *****


2019-02-16 13:14:27,001 : loading BERT model bert-base-uncased
2019-02-16 13:14:27,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:14:27,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:14:27,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuixw7vyq
2019-02-16 13:14:29,521 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:14:32,840 : MSRpar : pearson = 0.2107, spearman = 0.2657
2019-02-16 13:14:33,684 : MSRvid : pearson = 0.0238, spearman = 0.0414
2019-02-16 13:14:34,405 : SMTeuroparl : pearson = 0.4292, spearman = 0.4966
2019-02-16 13:14:35,699 : surprise.OnWN : pearson = 0.3058, spearman = 0.2957
2019-02-16 13:14:36,410 : surprise.SMTnews : pearson = 0.5400, spearman = 0.4453
2019-02-16 13:14:36,410 : ALL (weighted average) : Pearson = 0.2631,             Spearman = 0.2760
2019-02-16 13:14:36,410 : ALL (average) : Pearson = 0.3019,             Spearman = 0.3089

2019-02-16 13:14:36,410 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 13:14:36,420 : loading BERT model bert-base-uncased
2019-02-16 13:14:36,420 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:14:36,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:14:36,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjdowfv_2
2019-02-16 13:14:38,948 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:14:41,069 : FNWN : pearson = 0.0855, spearman = 0.0996
2019-02-16 13:14:42,053 : headlines : pearson = 0.3958, spearman = 0.3857
2019-02-16 13:14:42,789 : OnWN : pearson = 0.0269, spearman = 0.0235
2019-02-16 13:14:42,789 : ALL (weighted average) : Pearson = 0.2187,             Spearman = 0.2142
2019-02-16 13:14:42,790 : ALL (average) : Pearson = 0.1694,             Spearman = 0.1696

2019-02-16 13:14:42,790 : ***** Transfer task : STS14 *****


2019-02-16 13:14:42,807 : loading BERT model bert-base-uncased
2019-02-16 13:14:42,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:14:42,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:14:42,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvncioykh
2019-02-16 13:14:45,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:14:47,496 : deft-forum : pearson = -0.1157, spearman = -0.0979
2019-02-16 13:14:48,195 : deft-news : pearson = 0.3885, spearman = 0.4331
2019-02-16 13:14:49,164 : headlines : pearson = 0.3822, spearman = 0.3569
2019-02-16 13:14:50,097 : images : pearson = 0.1499, spearman = 0.1662
2019-02-16 13:14:51,041 : OnWN : pearson = 0.2438, spearman = 0.2446
2019-02-16 13:14:52,283 : tweet-news : pearson = 0.4445, spearman = 0.4074
2019-02-16 13:14:52,283 : ALL (weighted average) : Pearson = 0.2613,             Spearman = 0.2579
2019-02-16 13:14:52,283 : ALL (average) : Pearson = 0.2489,             Spearman = 0.2517

2019-02-16 13:14:52,283 : ***** Transfer task : STS15 *****


2019-02-16 13:14:52,352 : loading BERT model bert-base-uncased
2019-02-16 13:14:52,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:14:52,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:14:52,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_5orhwv0
2019-02-16 13:14:54,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:14:57,138 : answers-forums : pearson = 0.2253, spearman = 0.2562
2019-02-16 13:14:58,100 : answers-students : pearson = 0.3696, spearman = 0.3774
2019-02-16 13:14:58,975 : belief : pearson = 0.3785, spearman = 0.4281
2019-02-16 13:14:59,975 : headlines : pearson = 0.4310, spearman = 0.4397
2019-02-16 13:15:00,954 : images : pearson = 0.1207, spearman = 0.1378
2019-02-16 13:15:00,954 : ALL (weighted average) : Pearson = 0.3058,             Spearman = 0.3242
2019-02-16 13:15:00,954 : ALL (average) : Pearson = 0.3050,             Spearman = 0.3278

2019-02-16 13:15:00,954 : ***** Transfer task : STS16 *****


2019-02-16 13:15:01,002 : loading BERT model bert-base-uncased
2019-02-16 13:15:01,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:15:01,058 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:15:01,059 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpozudym2q
2019-02-16 13:15:03,613 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:15:05,510 : answer-answer : pearson = 0.3336, spearman = 0.3398
2019-02-16 13:15:05,822 : headlines : pearson = 0.5388, spearman = 0.5526
2019-02-16 13:15:06,211 : plagiarism : pearson = 0.5215, spearman = 0.6059
2019-02-16 13:15:06,830 : postediting : pearson = 0.6820, spearman = 0.7569
2019-02-16 13:15:07,116 : question-question : pearson = 0.0258, spearman = 0.1001
2019-02-16 13:15:07,116 : ALL (weighted average) : Pearson = 0.4306,             Spearman = 0.4796
2019-02-16 13:15:07,116 : ALL (average) : Pearson = 0.4204,             Spearman = 0.4711

2019-02-16 13:15:07,116 : ***** Transfer task : MR *****


2019-02-16 13:15:07,135 : loading BERT model bert-base-uncased
2019-02-16 13:15:07,135 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:15:07,158 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:15:07,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiqqcqfpl
2019-02-16 13:15:09,640 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:15:11,152 : Generating sentence embeddings
2019-02-16 13:15:24,611 : Generated sentence embeddings
2019-02-16 13:15:24,612 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 13:15:40,848 : Best param found at split 1: l2reg = 0.0001                 with score 76.08
2019-02-16 13:15:54,609 : Best param found at split 2: l2reg = 0.01                 with score 73.63
2019-02-16 13:16:10,203 : Best param found at split 3: l2reg = 0.0001                 with score 74.67
2019-02-16 13:16:29,581 : Best param found at split 4: l2reg = 0.0001                 with score 74.89
2019-02-16 13:16:46,019 : Best param found at split 5: l2reg = 1e-05                 with score 74.95
2019-02-16 13:16:46,769 : Dev acc : 74.84 Test acc : 75.52

2019-02-16 13:16:46,770 : ***** Transfer task : CR *****


2019-02-16 13:16:46,777 : loading BERT model bert-base-uncased
2019-02-16 13:16:46,778 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:16:46,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:16:46,835 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptezi4k_k
2019-02-16 13:16:49,334 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:16:50,770 : Generating sentence embeddings
2019-02-16 13:16:54,415 : Generated sentence embeddings
2019-02-16 13:16:54,416 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 13:16:59,216 : Best param found at split 1: l2reg = 0.001                 with score 79.56
2019-02-16 13:17:05,634 : Best param found at split 2: l2reg = 0.001                 with score 79.56
2019-02-16 13:17:13,053 : Best param found at split 3: l2reg = 0.0001                 with score 79.6
2019-02-16 13:17:18,610 : Best param found at split 4: l2reg = 0.0001                 with score 80.4
2019-02-16 13:17:24,739 : Best param found at split 5: l2reg = 0.0001                 with score 79.34
2019-02-16 13:17:25,166 : Dev acc : 79.69 Test acc : 75.18

2019-02-16 13:17:25,166 : ***** Transfer task : MPQA *****


2019-02-16 13:17:25,173 : loading BERT model bert-base-uncased
2019-02-16 13:17:25,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:17:25,196 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:17:25,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdencdj6y
2019-02-16 13:17:27,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:17:29,235 : Generating sentence embeddings
2019-02-16 13:17:33,026 : Generated sentence embeddings
2019-02-16 13:17:33,026 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 13:17:49,906 : Best param found at split 1: l2reg = 0.001                 with score 86.83
2019-02-16 13:18:06,590 : Best param found at split 2: l2reg = 0.01                 with score 86.53
2019-02-16 13:18:26,178 : Best param found at split 3: l2reg = 1e-05                 with score 86.48
2019-02-16 13:18:42,426 : Best param found at split 4: l2reg = 0.01                 with score 87.06
2019-02-16 13:19:02,527 : Best param found at split 5: l2reg = 0.0001                 with score 86.51
2019-02-16 13:19:03,729 : Dev acc : 86.68 Test acc : 86.61

2019-02-16 13:19:03,730 : ***** Transfer task : SUBJ *****


2019-02-16 13:19:03,755 : loading BERT model bert-base-uncased
2019-02-16 13:19:03,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:19:03,780 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:19:03,780 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxmlil5bf
2019-02-16 13:19:06,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:19:07,765 : Generating sentence embeddings
2019-02-16 13:19:21,081 : Generated sentence embeddings
2019-02-16 13:19:21,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 13:19:36,420 : Best param found at split 1: l2reg = 1e-05                 with score 94.48
2019-02-16 13:19:54,453 : Best param found at split 2: l2reg = 1e-05                 with score 94.75
2019-02-16 13:20:11,625 : Best param found at split 3: l2reg = 0.001                 with score 94.36
2019-02-16 13:20:28,640 : Best param found at split 4: l2reg = 0.001                 with score 94.96
2019-02-16 13:20:45,484 : Best param found at split 5: l2reg = 0.001                 with score 94.54
2019-02-16 13:20:46,672 : Dev acc : 94.62 Test acc : 93.8

2019-02-16 13:20:46,673 : ***** Transfer task : SST Binary classification *****


2019-02-16 13:20:46,813 : loading BERT model bert-base-uncased
2019-02-16 13:20:46,813 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:20:46,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:20:46,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmwozq4wz
2019-02-16 13:20:49,305 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:20:50,836 : Computing embedding for train
2019-02-16 13:21:36,724 : Computed train embeddings
2019-02-16 13:21:36,724 : Computing embedding for dev
2019-02-16 13:21:37,650 : Computed dev embeddings
2019-02-16 13:21:37,650 : Computing embedding for test
2019-02-16 13:21:39,586 : Computed test embeddings
2019-02-16 13:21:39,586 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:22:07,486 : [('reg:1e-05', 80.96), ('reg:0.0001', 80.85), ('reg:0.001', 78.44), ('reg:0.01', 80.05)]
2019-02-16 13:22:07,487 : Validation : best param found is reg = 1e-05 with score             80.96
2019-02-16 13:22:07,487 : Evaluating...
2019-02-16 13:22:13,739 : 
Dev acc : 80.96 Test acc : 78.69 for             SST Binary classification

2019-02-16 13:22:13,740 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 13:22:13,800 : loading BERT model bert-base-uncased
2019-02-16 13:22:13,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:22:13,866 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:22:13,866 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcnpimp4f
2019-02-16 13:22:16,382 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:22:17,845 : Computing embedding for train
2019-02-16 13:22:27,545 : Computed train embeddings
2019-02-16 13:22:27,545 : Computing embedding for dev
2019-02-16 13:22:28,903 : Computed dev embeddings
2019-02-16 13:22:28,903 : Computing embedding for test
2019-02-16 13:22:31,584 : Computed test embeddings
2019-02-16 13:22:31,584 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:22:35,986 : [('reg:1e-05', 36.06), ('reg:0.0001', 41.24), ('reg:0.001', 34.51), ('reg:0.01', 36.33)]
2019-02-16 13:22:35,986 : Validation : best param found is reg = 0.0001 with score             41.24
2019-02-16 13:22:35,986 : Evaluating...
2019-02-16 13:22:37,742 : 
Dev acc : 41.24 Test acc : 42.58 for             SST Fine-Grained classification

2019-02-16 13:22:37,743 : ***** Transfer task : TREC *****


2019-02-16 13:22:37,762 : loading BERT model bert-base-uncased
2019-02-16 13:22:37,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:22:37,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:22:37,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzhojne81
2019-02-16 13:22:40,250 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:22:45,030 : Computed train embeddings
2019-02-16 13:22:45,279 : Computed test embeddings
2019-02-16 13:22:45,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 13:22:54,836 : [('reg:1e-05', 80.13), ('reg:0.0001', 80.09), ('reg:0.001', 76.72), ('reg:0.01', 72.66)]
2019-02-16 13:22:54,836 : Cross-validation : best param found is reg = 1e-05             with score 80.13
2019-02-16 13:22:54,836 : Evaluating...
2019-02-16 13:22:55,429 : 
Dev acc : 80.13 Test acc : 87.2             for TREC

2019-02-16 13:22:55,430 : ***** Transfer task : MRPC *****


2019-02-16 13:22:55,484 : loading BERT model bert-base-uncased
2019-02-16 13:22:55,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:22:55,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:22:55,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp90v6pkip
2019-02-16 13:22:57,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:22:59,270 : Computing embedding for train
2019-02-16 13:23:08,917 : Computed train embeddings
2019-02-16 13:23:08,917 : Computing embedding for test
2019-02-16 13:23:13,570 : Computed test embeddings
2019-02-16 13:23:13,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 13:23:18,778 : [('reg:1e-05', 70.46), ('reg:0.0001', 70.51), ('reg:0.001', 69.97), ('reg:0.01', 69.33)]
2019-02-16 13:23:18,778 : Cross-validation : best param found is reg = 0.0001             with score 70.51
2019-02-16 13:23:18,779 : Evaluating...
2019-02-16 13:23:19,012 : Dev acc : 70.51 Test acc 68.17; Test F1 76.65 for MRPC.

2019-02-16 13:23:19,013 : ***** Transfer task : SICK-Entailment*****


2019-02-16 13:23:19,048 : loading BERT model bert-base-uncased
2019-02-16 13:23:19,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:23:19,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:23:19,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8soxiie1
2019-02-16 13:23:22,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:23:24,283 : Computing embedding for train
2019-02-16 13:23:30,103 : Computed train embeddings
2019-02-16 13:23:30,103 : Computing embedding for dev
2019-02-16 13:23:30,862 : Computed dev embeddings
2019-02-16 13:23:30,862 : Computing embedding for test
2019-02-16 13:23:37,424 : Computed test embeddings
2019-02-16 13:23:37,455 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:23:39,379 : [('reg:1e-05', 70.2), ('reg:0.0001', 72.2), ('reg:0.001', 73.4), ('reg:0.01', 69.0)]
2019-02-16 13:23:39,380 : Validation : best param found is reg = 0.001 with score             73.4
2019-02-16 13:23:39,380 : Evaluating...
2019-02-16 13:23:39,868 : 
Dev acc : 73.4 Test acc : 71.69 for                        SICK entailment

2019-02-16 13:23:39,869 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 13:23:39,908 : loading BERT model bert-base-uncased
2019-02-16 13:23:39,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:23:39,937 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:23:39,937 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp06knpja3
2019-02-16 13:23:42,927 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:23:44,967 : Computing embedding for train
2019-02-16 13:23:50,595 : Computed train embeddings
2019-02-16 13:23:50,595 : Computing embedding for dev
2019-02-16 13:23:51,320 : Computed dev embeddings
2019-02-16 13:23:51,320 : Computing embedding for test
2019-02-16 13:23:56,784 : Computed test embeddings
2019-02-16 13:24:20,935 : Dev : Pearson 0.6691442503501241
2019-02-16 13:24:20,935 : Test : Pearson 0.687198663860213 Spearman 0.6351076561112308 MSE 0.5385652793527482                        for SICK Relatedness

2019-02-16 13:24:20,936 : 

***** Transfer task : STSBenchmark*****


2019-02-16 13:24:21,025 : loading BERT model bert-base-uncased
2019-02-16 13:24:21,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:24:21,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:24:21,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnvf2720j
2019-02-16 13:24:23,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:24:25,046 : Computing embedding for train
2019-02-16 13:24:33,376 : Computed train embeddings
2019-02-16 13:24:33,376 : Computing embedding for dev
2019-02-16 13:24:35,821 : Computed dev embeddings
2019-02-16 13:24:35,822 : Computing embedding for test
2019-02-16 13:24:37,788 : Computed test embeddings
2019-02-16 13:25:04,777 : Dev : Pearson 0.47728406848883
2019-02-16 13:25:04,778 : Test : Pearson 0.44280712746696177 Spearman 0.44081226467020446 MSE 2.0199047320325065                        for SICK Relatedness

2019-02-16 13:25:04,778 : ***** Transfer task : SNLI Entailment*****


2019-02-16 13:25:09,871 : loading BERT model bert-base-uncased
2019-02-16 13:25:09,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:25:10,000 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:25:10,000 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps143s897
2019-02-16 13:25:12,521 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:25:14,165 : PROGRESS (encoding): 0.00%
2019-02-16 13:26:32,328 : PROGRESS (encoding): 14.56%
2019-02-16 13:27:59,960 : PROGRESS (encoding): 29.12%
2019-02-16 13:29:26,894 : PROGRESS (encoding): 43.69%
2019-02-16 13:31:01,498 : PROGRESS (encoding): 58.25%
2019-02-16 13:32:46,226 : PROGRESS (encoding): 72.81%
2019-02-16 13:34:31,764 : PROGRESS (encoding): 87.37%
2019-02-16 13:36:20,840 : PROGRESS (encoding): 0.00%
2019-02-16 13:36:34,525 : PROGRESS (encoding): 0.00%
2019-02-16 13:36:48,538 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 13:37:31,097 : [('reg:1e-09', 60.91)]
2019-02-16 13:37:31,098 : Validation : best param found is reg = 1e-09 with score             60.91
2019-02-16 13:37:31,098 : Evaluating...
2019-02-16 13:38:13,048 : Dev acc : 60.91 Test acc : 61.42 for SNLI

2019-02-16 13:38:13,048 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 13:38:22,305 : loading BERT model bert-base-uncased
2019-02-16 13:38:22,306 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 13:38:22,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 13:38:22,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6wcyzfty
2019-02-16 13:38:24,829 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 13:38:26,304 : Computing embedding for train
2019-02-16 13:46:00,086 : Computed train embeddings
2019-02-16 13:46:00,086 : Computing embedding for dev
2019-02-16 13:46:19,359 : Computed dev embeddings
2019-02-16 13:46:19,359 : Computing embedding for test
2019-02-16 13:46:40,546 : Computed test embeddings
2019-02-16 13:46:40,562 : prepare data
2019-02-16 13:46:40,624 : start epoch
2019-02-16 13:47:23,217 : samples : 64000
2019-02-16 13:47:33,535 : Image to text: 3.32, 12.82, 21.1, 43.0
2019-02-16 13:47:41,007 : Text to Image: 2.876, 11.344, 18.656, 51.0
2019-02-16 13:48:23,781 : samples : 128000
2019-02-16 13:48:34,268 : Image to text: 3.96, 14.62, 24.66, 35.0
2019-02-16 13:48:41,798 : Text to Image: 3.596, 12.76, 20.908, 43.0
2019-02-16 13:49:24,747 : samples : 192000
2019-02-16 13:49:35,207 : Image to text: 3.78, 14.7, 24.2, 35.0
2019-02-16 13:49:42,807 : Text to Image: 3.512, 12.412, 20.684, 44.0
2019-02-16 13:50:25,507 : samples : 256000
2019-02-16 13:50:36,004 : Image to text: 4.56, 16.96, 25.98, 33.0
2019-02-16 13:50:43,489 : Text to Image: 3.524, 13.0, 21.656, 40.0
2019-02-16 13:51:26,009 : samples : 320000
2019-02-16 13:51:36,474 : Image to text: 4.8, 17.42, 27.18, 31.0
2019-02-16 13:51:44,012 : Text to Image: 4.736, 16.32, 25.724, 34.0
2019-02-16 13:52:26,659 : samples : 384000
2019-02-16 13:52:37,217 : Image to text: 5.2, 18.0, 29.28, 28.0
2019-02-16 13:52:44,777 : Text to Image: 4.528, 16.008, 25.168, 34.0
2019-02-16 13:53:27,359 : samples : 448000
2019-02-16 13:53:37,808 : Image to text: 4.82, 17.42, 28.24, 29.0
2019-02-16 13:53:45,374 : Text to Image: 4.788, 16.208, 25.36, 34.0
2019-02-16 13:54:28,278 : samples : 512000
2019-02-16 13:54:38,785 : Image to text: 5.84, 19.72, 30.02, 27.0
2019-02-16 13:54:46,313 : Text to Image: 4.952, 17.068, 26.188, 33.0
2019-02-16 13:55:22,828 : Epoch 1 finished
2019-02-16 13:55:23,243 : Image to text: 18.2, 50.0, 64.5, 5.0
2019-02-16 13:55:23,553 : Text to Image: 15.74, 43.74, 60.64, 7.0
2019-02-16 13:55:23,978 : Image to text: 20.4, 48.9, 66.2, 6.0
2019-02-16 13:55:24,291 : Text to Image: 15.62, 43.56, 60.6, 7.0
2019-02-16 13:55:24,731 : Image to text: 20.0, 48.8, 65.5, 6.0
2019-02-16 13:55:25,052 : Text to Image: 15.96, 43.24, 60.56, 7.0
2019-02-16 13:55:25,498 : Image to text: 18.1, 49.5, 65.7, 6.0
2019-02-16 13:55:25,813 : Text to Image: 14.06, 43.34, 61.18, 7.0
2019-02-16 13:55:26,258 : Image to text: 19.2, 49.0, 64.0, 6.0
2019-02-16 13:55:26,574 : Text to Image: 15.98, 43.9, 60.6, 7.0
2019-02-16 13:55:26,574 : Dev mean Text to Image: 15.472, 43.556000000000004, 60.71600000000001, 7.0
2019-02-16 13:55:26,574 : Dev mean Image to text: 19.18, 49.239999999999995, 65.18, 5.800000000000001
2019-02-16 13:55:26,574 : start epoch
2019-02-16 13:56:09,450 : samples : 64000
2019-02-16 13:56:19,724 : Image to text: 6.2, 20.42, 31.8, 25.0
2019-02-16 13:56:27,208 : Text to Image: 5.016, 17.112, 26.848, 31.0
2019-02-16 13:57:13,432 : samples : 128000
2019-02-16 13:57:25,839 : Image to text: 6.54, 20.84, 31.12, 24.0
2019-02-16 13:57:36,172 : Text to Image: 4.944, 17.152, 26.748, 30.0
2019-02-16 13:58:22,778 : samples : 192000
2019-02-16 13:58:35,413 : Image to text: 6.72, 20.86, 32.22, 24.0
2019-02-16 13:58:45,472 : Text to Image: 5.412, 17.896, 28.024, 30.0
2019-02-16 13:59:29,167 : samples : 256000
2019-02-16 13:59:39,418 : Image to text: 6.24, 20.36, 31.22, 24.0
2019-02-16 13:59:46,906 : Text to Image: 5.444, 18.176, 28.172, 29.0
2019-02-16 14:00:30,449 : samples : 320000
2019-02-16 14:00:43,046 : Image to text: 6.78, 21.28, 32.68, 23.0
2019-02-16 14:00:53,142 : Text to Image: 5.308, 18.2, 28.012, 29.0
2019-02-16 14:01:38,301 : samples : 384000
2019-02-16 14:01:48,617 : Image to text: 6.0, 20.62, 31.94, 24.0
2019-02-16 14:01:55,838 : Text to Image: 5.396, 17.852, 27.86, 29.0
2019-02-16 14:02:37,762 : samples : 448000
2019-02-16 14:02:47,981 : Image to text: 6.4, 22.2, 32.42, 24.0
2019-02-16 14:02:55,886 : Text to Image: 5.1, 17.96, 27.764, 30.0
2019-02-16 14:03:40,274 : samples : 512000
2019-02-16 14:03:52,905 : Image to text: 6.72, 22.26, 35.02, 22.0
2019-02-16 14:04:02,990 : Text to Image: 5.768, 19.304, 29.292, 28.0
2019-02-16 14:04:40,106 : Epoch 2 finished
2019-02-16 14:04:40,579 : Image to text: 18.3, 49.0, 65.2, 6.0
2019-02-16 14:04:40,947 : Text to Image: 16.0, 46.5, 64.18, 6.0
2019-02-16 14:04:41,412 : Image to text: 18.1, 51.5, 66.5, 5.0
2019-02-16 14:04:41,775 : Text to Image: 16.48, 45.78, 62.84, 7.0
2019-02-16 14:04:42,225 : Image to text: 19.8, 50.4, 67.3, 5.0
2019-02-16 14:04:42,587 : Text to Image: 16.24, 45.94, 63.44, 6.0
2019-02-16 14:04:43,043 : Image to text: 17.4, 49.6, 67.2, 6.0
2019-02-16 14:04:43,411 : Text to Image: 15.66, 44.92, 61.86, 7.0
2019-02-16 14:04:43,874 : Image to text: 20.7, 52.3, 67.1, 5.0
2019-02-16 14:04:44,244 : Text to Image: 16.88, 45.3, 62.62, 7.0
2019-02-16 14:04:44,245 : Dev mean Text to Image: 16.252, 45.688, 62.988, 6.6
2019-02-16 14:04:44,245 : Dev mean Image to text: 18.86, 50.56, 66.66000000000001, 5.4
2019-02-16 14:04:44,245 : start epoch
2019-02-16 14:05:27,283 : samples : 64000
2019-02-16 14:05:39,819 : Image to text: 7.0, 21.84, 33.76, 22.0
2019-02-16 14:05:49,823 : Text to Image: 5.864, 19.52, 29.624, 27.0
2019-02-16 14:06:35,263 : samples : 128000
2019-02-16 14:06:45,486 : Image to text: 6.08, 20.66, 31.72, 25.0
2019-02-16 14:06:52,877 : Text to Image: 5.24, 17.624, 27.684, 29.0
2019-02-16 14:07:35,031 : samples : 192000
2019-02-16 14:07:45,702 : Image to text: 6.74, 22.2, 34.0, 23.0
2019-02-16 14:07:55,710 : Text to Image: 5.188, 17.796, 28.236, 29.0
2019-02-16 14:08:40,477 : samples : 256000
2019-02-16 14:08:53,064 : Image to text: 7.36, 23.34, 35.28, 21.0
2019-02-16 14:09:03,155 : Text to Image: 6.116, 19.72, 30.18, 26.0
2019-02-16 14:09:45,742 : samples : 320000
2019-02-16 14:09:56,025 : Image to text: 7.2, 23.74, 34.88, 21.0
2019-02-16 14:10:03,411 : Text to Image: 5.976, 19.344, 30.18, 26.0
2019-02-16 14:10:46,811 : samples : 384000
2019-02-16 14:10:59,400 : Image to text: 7.1, 23.72, 35.34, 21.0
2019-02-16 14:11:09,460 : Text to Image: 6.52, 20.48, 30.996, 25.0
2019-02-16 14:11:54,709 : samples : 448000
2019-02-16 14:12:04,966 : Image to text: 6.98, 23.56, 35.3, 21.0
2019-02-16 14:12:12,285 : Text to Image: 6.36, 20.62, 31.512, 25.0
2019-02-16 14:12:55,031 : samples : 512000
2019-02-16 14:13:07,599 : Image to text: 6.56, 22.14, 33.68, 23.0
2019-02-16 14:13:17,632 : Text to Image: 5.9, 19.384, 29.936, 26.0
2019-02-16 14:13:56,241 : Epoch 3 finished
2019-02-16 14:13:57,178 : Image to text: 18.2, 50.2, 67.1, 5.0
2019-02-16 14:13:57,987 : Text to Image: 16.38, 45.88, 62.84, 6.0
2019-02-16 14:13:58,916 : Image to text: 19.8, 51.3, 67.5, 5.0
2019-02-16 14:13:59,715 : Text to Image: 15.38, 44.94, 62.76, 7.0
2019-02-16 14:14:00,596 : Image to text: 21.3, 52.3, 68.8, 5.0
2019-02-16 14:14:01,317 : Text to Image: 15.9, 44.6, 61.6, 7.0
2019-02-16 14:14:02,182 : Image to text: 19.7, 50.8, 68.1, 5.0
2019-02-16 14:14:02,934 : Text to Image: 15.74, 45.34, 62.72, 7.0
2019-02-16 14:14:03,848 : Image to text: 20.4, 53.0, 68.1, 5.0
2019-02-16 14:14:04,561 : Text to Image: 16.56, 45.66, 62.88, 7.0
2019-02-16 14:14:04,561 : Dev mean Text to Image: 15.991999999999999, 45.284, 62.559999999999995, 6.799999999999999
2019-02-16 14:14:04,562 : Dev mean Image to text: 19.88, 51.52, 67.92, 5.0
2019-02-16 14:14:04,562 : start epoch
2019-02-16 14:14:55,684 : samples : 64000
2019-02-16 14:15:05,913 : Image to text: 6.9, 23.78, 35.12, 21.0
2019-02-16 14:15:13,281 : Text to Image: 5.936, 19.968, 30.636, 25.0
2019-02-16 14:15:57,698 : samples : 128000
2019-02-16 14:16:10,303 : Image to text: 6.76, 23.1, 34.44, 22.0
2019-02-16 14:16:20,352 : Text to Image: 6.172, 19.876, 30.416, 26.0
2019-02-16 14:17:04,117 : samples : 192000
2019-02-16 14:17:14,396 : Image to text: 7.0, 22.78, 33.98, 22.0
2019-02-16 14:17:21,786 : Text to Image: 5.864, 19.36, 30.104, 26.0
2019-02-16 14:18:04,140 : samples : 256000
2019-02-16 14:18:16,693 : Image to text: 7.1, 24.1, 35.76, 21.0
2019-02-16 14:18:26,752 : Text to Image: 6.088, 20.444, 31.1, 24.0
2019-02-16 14:19:12,613 : samples : 320000
2019-02-16 14:19:25,235 : Image to text: 7.84, 24.62, 36.54, 19.0
2019-02-16 14:19:35,240 : Text to Image: 6.608, 21.176, 32.068, 24.0
2019-02-16 14:20:18,166 : samples : 384000
2019-02-16 14:20:28,480 : Image to text: 7.24, 23.02, 34.74, 20.0
2019-02-16 14:20:35,906 : Text to Image: 6.344, 20.696, 31.332, 25.0
2019-02-16 14:21:19,080 : samples : 448000
2019-02-16 14:21:31,705 : Image to text: 7.64, 24.36, 36.02, 21.0
2019-02-16 14:21:41,762 : Text to Image: 6.704, 21.32, 32.264, 24.0
2019-02-16 14:22:26,507 : samples : 512000
2019-02-16 14:22:36,744 : Image to text: 7.64, 23.78, 36.44, 20.0
2019-02-16 14:22:44,121 : Text to Image: 6.58, 20.892, 31.68, 25.0
2019-02-16 14:23:20,454 : Epoch 4 finished
2019-02-16 14:23:20,821 : Image to text: 21.4, 55.4, 70.2, 4.0
2019-02-16 14:23:21,111 : Text to Image: 18.56, 49.72, 67.16, 6.0
2019-02-16 14:23:21,495 : Image to text: 22.3, 54.9, 68.3, 4.0
2019-02-16 14:23:21,772 : Text to Image: 18.54, 49.0, 66.34, 6.0
2019-02-16 14:23:22,150 : Image to text: 23.1, 54.8, 70.9, 5.0
2019-02-16 14:23:22,427 : Text to Image: 18.9, 49.44, 67.08, 6.0
2019-02-16 14:23:22,806 : Image to text: 22.8, 55.5, 72.3, 4.0
2019-02-16 14:23:23,089 : Text to Image: 18.32, 48.78, 66.1, 6.0
2019-02-16 14:23:24,049 : Image to text: 20.9, 52.5, 68.0, 5.0
2019-02-16 14:23:24,804 : Text to Image: 18.1, 48.72, 65.6, 6.0
2019-02-16 14:23:24,804 : Dev mean Text to Image: 18.483999999999998, 49.132, 66.456, 6.0
2019-02-16 14:23:24,804 : Dev mean Image to text: 22.1, 54.620000000000005, 69.94, 4.4
2019-02-16 14:23:24,805 : start epoch
2019-02-16 14:24:08,731 : samples : 64000
2019-02-16 14:24:18,983 : Image to text: 6.78, 23.4, 34.98, 21.0
2019-02-16 14:24:26,335 : Text to Image: 6.18, 20.08, 30.888, 26.0
2019-02-16 14:25:09,064 : samples : 128000
2019-02-16 14:25:19,317 : Image to text: 7.76, 24.8, 36.62, 20.0
2019-02-16 14:25:26,628 : Text to Image: 6.492, 20.644, 31.64, 24.0
2019-02-16 14:26:09,250 : samples : 192000
2019-02-16 14:26:19,481 : Image to text: 7.84, 24.24, 36.06, 20.0
2019-02-16 14:26:26,803 : Text to Image: 6.572, 21.024, 31.808, 24.0
2019-02-16 14:27:09,219 : samples : 256000
2019-02-16 14:27:19,458 : Image to text: 6.38, 22.92, 34.72, 21.0
2019-02-16 14:27:26,851 : Text to Image: 6.544, 20.62, 31.632, 25.0
2019-02-16 14:28:09,319 : samples : 320000
2019-02-16 14:28:22,040 : Image to text: 7.5, 24.62, 37.06, 19.0
2019-02-16 14:28:29,992 : Text to Image: 7.056, 22.292, 33.324, 23.0
2019-02-16 14:29:12,606 : samples : 384000
2019-02-16 14:29:22,853 : Image to text: 8.3, 24.6, 36.24, 20.0
2019-02-16 14:29:30,278 : Text to Image: 6.628, 20.76, 31.788, 24.0
2019-02-16 14:30:13,177 : samples : 448000
2019-02-16 14:30:25,891 : Image to text: 7.5, 24.24, 36.18, 20.0
2019-02-16 14:30:36,102 : Text to Image: 6.416, 21.02, 32.124, 24.0
2019-02-16 14:31:25,293 : samples : 512000
2019-02-16 14:31:38,456 : Image to text: 8.0, 24.24, 36.72, 19.0
2019-02-16 14:31:46,482 : Text to Image: 6.76, 21.868, 32.76, 23.0
2019-02-16 14:32:23,493 : Epoch 5 finished
2019-02-16 14:32:23,967 : Image to text: 24.4, 56.1, 71.1, 4.0
2019-02-16 14:32:24,343 : Text to Image: 18.74, 50.52, 68.02, 5.0
2019-02-16 14:32:24,805 : Image to text: 23.3, 56.1, 70.2, 4.0
2019-02-16 14:32:25,167 : Text to Image: 19.24, 49.38, 67.04, 6.0
2019-02-16 14:32:25,617 : Image to text: 21.3, 57.1, 70.9, 4.0
2019-02-16 14:32:25,979 : Text to Image: 19.88, 50.62, 67.7, 5.0
2019-02-16 14:32:26,375 : Image to text: 20.9, 55.5, 70.7, 5.0
2019-02-16 14:32:26,652 : Text to Image: 19.06, 50.06, 66.94, 5.0
2019-02-16 14:32:27,013 : Image to text: 23.4, 54.7, 71.0, 5.0
2019-02-16 14:32:27,296 : Text to Image: 19.46, 50.42, 67.6, 5.0
2019-02-16 14:32:27,296 : Dev mean Text to Image: 19.276, 50.2, 67.46, 5.2
2019-02-16 14:32:27,296 : Dev mean Image to text: 22.659999999999997, 55.900000000000006, 70.78, 4.4
2019-02-16 14:32:27,297 : start epoch
2019-02-16 14:33:10,759 : samples : 64000
2019-02-16 14:33:20,989 : Image to text: 7.92, 25.84, 37.92, 19.0
2019-02-16 14:33:28,707 : Text to Image: 7.164, 21.976, 32.796, 23.0
2019-02-16 14:34:11,710 : samples : 128000
2019-02-16 14:34:23,145 : Image to text: 7.42, 22.82, 35.04, 20.0
2019-02-16 14:34:33,107 : Text to Image: 6.744, 21.004, 32.048, 24.0
2019-02-16 14:35:18,148 : samples : 192000
2019-02-16 14:35:30,677 : Image to text: 7.74, 24.6, 35.9, 20.0
2019-02-16 14:35:40,650 : Text to Image: 6.304, 20.816, 31.932, 24.0
2019-02-16 14:36:25,854 : samples : 256000
2019-02-16 14:36:38,401 : Image to text: 8.1, 24.1, 36.22, 20.0
2019-02-16 14:36:48,382 : Text to Image: 6.764, 21.464, 32.496, 23.0
2019-02-16 14:37:33,049 : samples : 320000
2019-02-16 14:37:45,596 : Image to text: 7.64, 24.14, 36.78, 19.0
2019-02-16 14:37:55,545 : Text to Image: 6.844, 21.78, 33.136, 23.0
2019-02-16 14:38:40,439 : samples : 384000
2019-02-16 14:38:53,036 : Image to text: 7.9, 25.44, 37.9, 18.0
2019-02-16 14:39:03,083 : Text to Image: 7.208, 22.232, 33.528, 22.0
2019-02-16 14:39:48,046 : samples : 448000
2019-02-16 14:40:00,618 : Image to text: 8.36, 25.12, 37.48, 18.0
2019-02-16 14:40:10,650 : Text to Image: 6.78, 21.856, 33.24, 23.0
2019-02-16 14:40:55,488 : samples : 512000
2019-02-16 14:41:08,039 : Image to text: 8.32, 25.3, 37.88, 19.0
2019-02-16 14:41:18,053 : Text to Image: 6.824, 21.02, 32.404, 23.0
2019-02-16 14:41:56,215 : Epoch 6 finished
2019-02-16 14:41:57,163 : Image to text: 24.8, 56.1, 71.1, 4.0
2019-02-16 14:41:57,909 : Text to Image: 19.8, 51.62, 69.22, 5.0
2019-02-16 14:41:58,825 : Image to text: 23.4, 56.1, 71.9, 4.0
2019-02-16 14:41:59,613 : Text to Image: 19.26, 51.5, 68.88, 5.0
2019-02-16 14:42:00,522 : Image to text: 24.8, 56.0, 70.7, 4.0
2019-02-16 14:42:01,281 : Text to Image: 20.58, 52.5, 68.78, 5.0
2019-02-16 14:42:02,195 : Image to text: 22.6, 56.6, 73.8, 4.0
2019-02-16 14:42:02,939 : Text to Image: 19.38, 51.12, 67.84, 5.0
2019-02-16 14:42:03,854 : Image to text: 24.1, 56.3, 71.3, 4.0
2019-02-16 14:42:04,610 : Text to Image: 19.68, 51.56, 68.22, 5.0
2019-02-16 14:42:04,611 : Dev mean Text to Image: 19.740000000000002, 51.66, 68.58800000000001, 5.0
2019-02-16 14:42:04,611 : Dev mean Image to text: 23.94, 56.22, 71.76, 4.0
2019-02-16 14:42:04,612 : start epoch
2019-02-16 14:42:48,720 : samples : 64000
2019-02-16 14:43:01,323 : Image to text: 8.46, 25.26, 38.08, 18.0
2019-02-16 14:43:11,296 : Text to Image: 7.028, 22.416, 33.636, 22.0
2019-02-16 14:43:55,806 : samples : 128000
2019-02-16 14:44:08,453 : Image to text: 7.82, 24.74, 38.06, 18.0
2019-02-16 14:44:18,462 : Text to Image: 7.056, 21.804, 33.2, 23.0
2019-02-16 14:45:03,468 : samples : 192000
2019-02-16 14:45:16,114 : Image to text: 8.42, 25.62, 38.06, 19.0
2019-02-16 14:45:25,432 : Text to Image: 6.668, 21.98, 33.228, 23.0
2019-02-16 14:46:09,487 : samples : 256000
2019-02-16 14:46:19,740 : Image to text: 7.88, 24.94, 37.24, 19.0
2019-02-16 14:46:27,122 : Text to Image: 6.816, 21.752, 32.868, 23.0
2019-02-16 14:47:09,595 : samples : 320000
2019-02-16 14:47:19,549 : Image to text: 8.02, 25.1, 37.02, 19.0
2019-02-16 14:47:29,614 : Text to Image: 7.388, 22.18, 33.824, 22.0
2019-02-16 14:48:19,028 : samples : 384000
2019-02-16 14:48:32,134 : Image to text: 8.24, 26.06, 38.46, 18.0
2019-02-16 14:48:42,764 : Text to Image: 7.548, 22.848, 34.232, 22.0
2019-02-16 14:49:28,612 : samples : 448000
2019-02-16 14:49:41,525 : Image to text: 8.22, 25.86, 38.04, 18.0
2019-02-16 14:49:52,004 : Text to Image: 7.032, 22.528, 34.012, 22.0
2019-02-16 14:50:37,532 : samples : 512000
2019-02-16 14:50:50,435 : Image to text: 8.36, 25.38, 37.76, 18.0
2019-02-16 14:51:00,889 : Text to Image: 7.42, 22.896, 34.212, 22.0
2019-02-16 14:51:39,862 : Epoch 7 finished
2019-02-16 14:51:40,878 : Image to text: 23.6, 55.4, 71.4, 5.0
2019-02-16 14:51:41,760 : Text to Image: 19.88, 51.66, 68.8, 5.0
2019-02-16 14:51:42,821 : Image to text: 23.2, 54.9, 71.7, 4.0
2019-02-16 14:51:43,710 : Text to Image: 19.74, 51.38, 69.34, 5.0
2019-02-16 14:51:44,766 : Image to text: 25.5, 55.5, 71.2, 4.0
2019-02-16 14:51:45,672 : Text to Image: 20.14, 51.84, 69.48, 5.0
2019-02-16 14:51:46,764 : Image to text: 23.9, 54.6, 72.0, 4.0
2019-02-16 14:51:47,594 : Text to Image: 19.44, 50.28, 68.58, 5.0
2019-02-16 14:51:48,790 : Image to text: 22.8, 56.5, 70.4, 4.0
2019-02-16 14:51:49,712 : Text to Image: 19.88, 50.4, 67.96, 5.0
2019-02-16 14:51:49,712 : Dev mean Text to Image: 19.816, 51.111999999999995, 68.832, 5.0
2019-02-16 14:51:49,712 : Dev mean Image to text: 23.799999999999997, 55.38000000000001, 71.34, 4.2
2019-02-16 14:51:49,712 : start epoch
2019-02-16 14:52:35,625 : samples : 64000
2019-02-16 14:52:48,545 : Image to text: 8.48, 26.2, 38.4, 18.0
2019-02-16 14:52:58,982 : Text to Image: 7.504, 22.904, 34.332, 22.0
2019-02-16 14:53:44,315 : samples : 128000
2019-02-16 14:53:57,186 : Image to text: 8.2, 25.2, 38.12, 18.0
2019-02-16 14:54:07,630 : Text to Image: 7.176, 22.364, 33.804, 22.0
2019-02-16 14:54:53,815 : samples : 192000
2019-02-16 14:55:06,737 : Image to text: 8.1, 25.92, 37.76, 18.0
2019-02-16 14:55:17,055 : Text to Image: 6.816, 22.336, 33.624, 23.0
2019-02-16 14:56:00,877 : samples : 256000
2019-02-16 14:56:11,395 : Image to text: 8.04, 25.5, 37.98, 18.0
2019-02-16 14:56:19,143 : Text to Image: 7.464, 22.852, 34.292, 21.0
2019-02-16 14:57:02,298 : samples : 320000
2019-02-16 14:57:12,808 : Image to text: 8.16, 26.62, 39.54, 17.0
2019-02-16 14:57:20,498 : Text to Image: 7.78, 23.232, 34.76, 21.0
2019-02-16 14:58:03,407 : samples : 384000
2019-02-16 14:58:13,904 : Image to text: 7.98, 26.18, 38.0, 17.0
2019-02-16 14:58:21,601 : Text to Image: 7.228, 22.572, 33.92, 21.0
2019-02-16 14:59:04,854 : samples : 448000
2019-02-16 14:59:15,367 : Image to text: 8.08, 25.54, 37.7, 18.0
2019-02-16 14:59:23,118 : Text to Image: 7.232, 22.296, 33.844, 22.0
2019-02-16 15:00:06,387 : samples : 512000
2019-02-16 15:00:16,927 : Image to text: 8.58, 26.16, 38.42, 18.0
2019-02-16 15:00:24,633 : Text to Image: 7.428, 23.128, 34.756, 21.0
2019-02-16 15:01:01,441 : Epoch 8 finished
2019-02-16 15:01:01,877 : Image to text: 23.1, 56.7, 72.5, 4.0
2019-02-16 15:01:02,218 : Text to Image: 20.12, 52.82, 70.4, 5.0
2019-02-16 15:01:02,671 : Image to text: 23.5, 57.3, 73.3, 4.0
2019-02-16 15:01:03,000 : Text to Image: 19.96, 51.7, 69.92, 5.0
2019-02-16 15:01:03,458 : Image to text: 24.0, 58.3, 73.2, 4.0
2019-02-16 15:01:03,796 : Text to Image: 20.2, 52.8, 70.22, 5.0
2019-02-16 15:01:04,232 : Image to text: 22.1, 56.5, 74.4, 4.0
2019-02-16 15:01:04,566 : Text to Image: 19.52, 52.18, 69.68, 5.0
2019-02-16 15:01:05,001 : Image to text: 24.6, 58.3, 73.6, 4.0
2019-02-16 15:01:05,338 : Text to Image: 21.08, 52.12, 68.0, 5.0
2019-02-16 15:01:05,339 : Dev mean Text to Image: 20.176000000000002, 52.324, 69.644, 5.0
2019-02-16 15:01:05,339 : Dev mean Image to text: 23.46, 57.41999999999999, 73.4, 4.0
2019-02-16 15:01:05,339 : start epoch
2019-02-16 15:01:48,667 : samples : 64000
2019-02-16 15:01:59,204 : Image to text: 8.5, 27.2, 39.14, 17.0
2019-02-16 15:02:06,765 : Text to Image: 7.428, 23.304, 34.632, 21.0
2019-02-16 15:02:49,322 : samples : 128000
2019-02-16 15:02:59,827 : Image to text: 8.38, 25.7, 38.48, 17.0
2019-02-16 15:03:07,270 : Text to Image: 7.108, 22.344, 33.908, 22.0
2019-02-16 15:03:50,066 : samples : 192000
2019-02-16 15:04:00,506 : Image to text: 8.04, 25.42, 37.64, 18.0
2019-02-16 15:04:08,053 : Text to Image: 7.484, 23.156, 34.576, 21.0
2019-02-16 15:04:50,011 : samples : 256000
2019-02-16 15:05:01,510 : Image to text: 8.42, 26.46, 38.22, 18.0
2019-02-16 15:05:09,953 : Text to Image: 7.016, 22.724, 34.168, 22.0
2019-02-16 15:06:00,063 : samples : 320000
2019-02-16 15:06:10,539 : Image to text: 8.8, 26.02, 38.2, 18.0
2019-02-16 15:06:18,244 : Text to Image: 7.352, 23.268, 34.5, 21.0
2019-02-16 15:07:00,907 : samples : 384000
2019-02-16 15:07:11,404 : Image to text: 8.02, 25.72, 37.8, 18.0
2019-02-16 15:07:19,155 : Text to Image: 7.608, 22.784, 34.144, 21.0
2019-02-16 15:08:01,282 : samples : 448000
2019-02-16 15:08:11,839 : Image to text: 8.52, 26.36, 39.04, 17.0
2019-02-16 15:08:19,571 : Text to Image: 7.428, 23.12, 34.364, 21.0
2019-02-16 15:09:02,099 : samples : 512000
2019-02-16 15:09:12,571 : Image to text: 9.0, 26.5, 38.32, 18.0
2019-02-16 15:09:20,305 : Text to Image: 7.368, 22.992, 34.596, 21.0
2019-02-16 15:09:56,916 : Epoch 9 finished
2019-02-16 15:09:57,336 : Image to text: 26.7, 57.0, 72.0, 4.0
2019-02-16 15:09:57,663 : Text to Image: 20.58, 53.18, 70.22, 5.0
2019-02-16 15:09:58,103 : Image to text: 24.8, 57.9, 74.6, 4.0
2019-02-16 15:09:58,435 : Text to Image: 19.8, 52.12, 70.18, 5.0
2019-02-16 15:09:58,868 : Image to text: 25.3, 58.9, 73.9, 4.0
2019-02-16 15:09:59,204 : Text to Image: 21.56, 52.72, 69.48, 5.0
2019-02-16 15:09:59,648 : Image to text: 23.1, 57.5, 72.3, 4.0
2019-02-16 15:09:59,987 : Text to Image: 20.02, 52.34, 69.44, 5.0
2019-02-16 15:10:00,462 : Image to text: 24.8, 56.0, 72.2, 4.0
2019-02-16 15:10:00,800 : Text to Image: 20.72, 52.82, 69.3, 5.0
2019-02-16 15:10:00,800 : Dev mean Text to Image: 20.536, 52.636, 69.72399999999999, 5.0
2019-02-16 15:10:00,800 : Dev mean Image to text: 24.94, 57.459999999999994, 73.0, 4.0
2019-02-16 15:10:00,801 : start epoch
2019-02-16 15:10:43,587 : samples : 64000
2019-02-16 15:10:54,084 : Image to text: 8.46, 26.22, 38.78, 18.0
2019-02-16 15:11:01,641 : Text to Image: 7.196, 22.78, 34.02, 22.0
2019-02-16 15:11:44,016 : samples : 128000
2019-02-16 15:11:54,539 : Image to text: 9.04, 27.08, 39.34, 17.0
2019-02-16 15:12:02,005 : Text to Image: 7.916, 24.032, 35.632, 20.0
2019-02-16 15:12:44,537 : samples : 192000
2019-02-16 15:12:55,040 : Image to text: 8.98, 25.78, 38.52, 18.0
2019-02-16 15:13:02,616 : Text to Image: 7.272, 22.952, 34.564, 21.0
2019-02-16 15:13:45,489 : samples : 256000
2019-02-16 15:13:56,095 : Image to text: 8.46, 26.42, 39.56, 17.0
2019-02-16 15:14:03,789 : Text to Image: 7.488, 23.04, 34.688, 21.0
2019-02-16 15:14:46,274 : samples : 320000
2019-02-16 15:14:56,715 : Image to text: 8.08, 25.24, 37.44, 18.0
2019-02-16 15:15:04,354 : Text to Image: 6.956, 21.996, 33.5, 22.0
2019-02-16 15:15:47,127 : samples : 384000
2019-02-16 15:15:57,625 : Image to text: 9.5, 26.14, 37.9, 18.0
2019-02-16 15:16:05,315 : Text to Image: 7.664, 23.676, 35.152, 20.0
2019-02-16 15:16:48,049 : samples : 448000
2019-02-16 15:16:58,525 : Image to text: 8.52, 26.52, 39.38, 17.0
2019-02-16 15:17:06,191 : Text to Image: 7.672, 23.176, 34.484, 21.0
2019-02-16 15:17:49,231 : samples : 512000
2019-02-16 15:17:59,775 : Image to text: 9.26, 27.84, 39.6, 17.0
2019-02-16 15:18:07,389 : Text to Image: 7.724, 23.572, 35.068, 21.0
2019-02-16 15:18:43,838 : Epoch 10 finished
2019-02-16 15:18:44,295 : Image to text: 26.0, 57.4, 72.6, 4.0
2019-02-16 15:18:44,629 : Text to Image: 21.26, 53.84, 71.14, 5.0
2019-02-16 15:18:45,067 : Image to text: 24.1, 57.8, 73.5, 4.0
2019-02-16 15:18:45,399 : Text to Image: 21.22, 53.06, 70.2, 5.0
2019-02-16 15:18:45,844 : Image to text: 26.2, 60.2, 74.1, 4.0
2019-02-16 15:18:46,175 : Text to Image: 21.7, 53.16, 70.64, 5.0
2019-02-16 15:18:46,607 : Image to text: 23.4, 59.0, 75.4, 4.0
2019-02-16 15:18:46,938 : Text to Image: 20.68, 53.28, 70.3, 5.0
2019-02-16 15:18:47,380 : Image to text: 25.8, 57.2, 71.2, 4.0
2019-02-16 15:18:47,709 : Text to Image: 20.66, 53.4, 70.44, 5.0
2019-02-16 15:18:47,709 : Dev mean Text to Image: 21.104, 53.348, 70.544, 5.0
2019-02-16 15:18:47,709 : Dev mean Image to text: 25.099999999999998, 58.31999999999999, 73.36, 4.0
2019-02-16 15:18:47,709 : start epoch
2019-02-16 15:19:30,971 : samples : 64000
2019-02-16 15:19:41,456 : Image to text: 8.54, 26.16, 38.24, 17.0
2019-02-16 15:19:49,013 : Text to Image: 7.38, 23.048, 34.48, 21.0
2019-02-16 15:20:31,916 : samples : 128000
2019-02-16 15:20:42,385 : Image to text: 8.44, 25.78, 38.5, 18.0
2019-02-16 15:20:49,973 : Text to Image: 7.544, 23.244, 34.872, 21.0
2019-02-16 15:21:32,987 : samples : 192000
2019-02-16 15:21:43,332 : Image to text: 9.1, 26.72, 38.56, 18.0
2019-02-16 15:21:50,702 : Text to Image: 7.568, 23.668, 34.864, 21.0
2019-02-16 15:22:44,901 : samples : 256000
2019-02-16 15:22:55,409 : Image to text: 8.94, 27.44, 39.88, 17.0
2019-02-16 15:23:03,013 : Text to Image: 7.968, 23.996, 35.62, 20.0
2019-02-16 15:23:46,191 : samples : 320000
2019-02-16 15:23:56,666 : Image to text: 8.58, 26.52, 39.24, 17.0
2019-02-16 15:24:04,373 : Text to Image: 7.464, 23.296, 34.84, 21.0
2019-02-16 15:24:47,204 : samples : 384000
2019-02-16 15:24:57,712 : Image to text: 9.06, 27.76, 39.54, 17.0
2019-02-16 15:25:05,429 : Text to Image: 7.684, 23.804, 35.408, 20.0
2019-02-16 15:25:47,731 : samples : 448000
2019-02-16 15:25:58,179 : Image to text: 9.0, 27.18, 40.04, 17.0
2019-02-16 15:26:05,966 : Text to Image: 7.484, 23.824, 35.74, 20.0
2019-02-16 15:26:49,019 : samples : 512000
2019-02-16 15:26:59,514 : Image to text: 9.02, 26.6, 38.42, 17.0
2019-02-16 15:27:07,295 : Text to Image: 7.728, 23.644, 35.368, 20.0
2019-02-16 15:27:43,273 : Epoch 11 finished
2019-02-16 15:27:43,731 : Image to text: 26.0, 55.3, 71.9, 4.0
2019-02-16 15:27:44,078 : Text to Image: 20.48, 53.46, 70.72, 5.0
2019-02-16 15:27:44,532 : Image to text: 23.6, 56.4, 72.1, 4.0
2019-02-16 15:27:44,877 : Text to Image: 20.28, 52.98, 70.72, 5.0
2019-02-16 15:27:45,332 : Image to text: 25.3, 57.5, 72.4, 4.0
2019-02-16 15:27:45,675 : Text to Image: 21.94, 53.12, 71.0, 5.0
2019-02-16 15:27:46,124 : Image to text: 24.2, 56.3, 73.6, 4.0
2019-02-16 15:27:46,467 : Text to Image: 21.04, 52.58, 70.52, 5.0
2019-02-16 15:27:46,919 : Image to text: 25.4, 56.9, 72.3, 4.0
2019-02-16 15:27:47,256 : Text to Image: 21.24, 53.68, 70.58, 5.0
2019-02-16 15:27:47,256 : Dev mean Text to Image: 20.996000000000002, 53.164, 70.708, 5.0
2019-02-16 15:27:47,256 : Dev mean Image to text: 24.9, 56.47999999999999, 72.46, 4.0
2019-02-16 15:27:47,256 : start epoch
2019-02-16 15:28:30,544 : samples : 64000
2019-02-16 15:28:41,036 : Image to text: 8.76, 26.82, 39.5, 17.0
2019-02-16 15:28:48,599 : Text to Image: 7.696, 23.44, 35.368, 20.0
2019-02-16 15:29:31,322 : samples : 128000
2019-02-16 15:29:41,814 : Image to text: 9.2, 27.26, 39.8, 16.0
2019-02-16 15:29:49,347 : Text to Image: 7.576, 23.628, 35.524, 20.0
2019-02-16 15:30:31,542 : samples : 192000
2019-02-16 15:30:42,012 : Image to text: 9.2, 28.2, 40.14, 17.0
2019-02-16 15:30:49,579 : Text to Image: 7.78, 24.18, 36.148, 20.0
2019-02-16 15:31:32,627 : samples : 256000
2019-02-16 15:31:43,172 : Image to text: 9.44, 27.14, 40.3, 17.0
2019-02-16 15:31:50,966 : Text to Image: 7.616, 23.844, 35.88, 20.0
2019-02-16 15:32:33,975 : samples : 320000
2019-02-16 15:32:44,470 : Image to text: 9.3, 27.12, 39.7, 18.0
2019-02-16 15:32:52,250 : Text to Image: 7.296, 22.688, 34.144, 21.0
2019-02-16 15:33:34,781 : samples : 384000
2019-02-16 15:33:45,372 : Image to text: 9.42, 27.76, 40.04, 17.0
2019-02-16 15:33:53,122 : Text to Image: 7.888, 23.94, 35.96, 20.0
2019-02-16 15:34:36,725 : samples : 448000
2019-02-16 15:34:47,198 : Image to text: 9.42, 27.38, 39.52, 17.0
2019-02-16 15:34:54,958 : Text to Image: 7.948, 24.032, 35.732, 20.0
2019-02-16 15:35:38,161 : samples : 512000
2019-02-16 15:35:48,660 : Image to text: 8.34, 26.0, 38.3, 18.0
2019-02-16 15:35:56,386 : Text to Image: 7.384, 23.132, 35.14, 21.0
2019-02-16 15:36:33,421 : Epoch 12 finished
2019-02-16 15:36:33,858 : Image to text: 24.1, 55.5, 71.1, 4.0
2019-02-16 15:36:34,194 : Text to Image: 19.82, 53.48, 71.16, 5.0
2019-02-16 15:36:34,639 : Image to text: 24.8, 56.8, 72.0, 4.0
2019-02-16 15:36:34,975 : Text to Image: 20.32, 53.0, 70.92, 5.0
2019-02-16 15:36:35,406 : Image to text: 26.5, 58.5, 73.5, 4.0
2019-02-16 15:36:35,735 : Text to Image: 21.3, 54.16, 70.9, 5.0
2019-02-16 15:36:36,177 : Image to text: 23.3, 59.0, 74.9, 4.0
2019-02-16 15:36:36,506 : Text to Image: 20.38, 52.42, 70.36, 5.0
2019-02-16 15:36:36,934 : Image to text: 25.0, 58.8, 73.9, 4.0
2019-02-16 15:36:37,272 : Text to Image: 20.1, 52.66, 70.26, 5.0
2019-02-16 15:36:37,272 : Dev mean Text to Image: 20.384, 53.144000000000005, 70.72000000000001, 5.0
2019-02-16 15:36:37,272 : Dev mean Image to text: 24.740000000000002, 57.71999999999999, 73.08, 4.0
2019-02-16 15:36:37,272 : start epoch
2019-02-16 15:37:19,454 : samples : 64000
2019-02-16 15:37:29,988 : Image to text: 9.26, 26.26, 40.38, 17.0
2019-02-16 15:37:37,627 : Text to Image: 7.696, 23.668, 35.368, 20.0
2019-02-16 15:38:20,307 : samples : 128000
2019-02-16 15:38:30,823 : Image to text: 8.86, 26.8, 39.92, 16.0
2019-02-16 15:38:38,190 : Text to Image: 7.68, 23.32, 34.852, 21.0
2019-02-16 15:39:31,209 : samples : 192000
2019-02-16 15:39:41,769 : Image to text: 9.2, 27.02, 40.08, 17.0
2019-02-16 15:39:49,310 : Text to Image: 7.532, 23.44, 35.076, 20.0
2019-02-16 15:40:33,086 : samples : 256000
2019-02-16 15:40:43,527 : Image to text: 8.88, 27.42, 39.62, 17.0
2019-02-16 15:40:51,213 : Text to Image: 7.716, 23.788, 35.444, 20.0
2019-02-16 15:41:34,390 : samples : 320000
2019-02-16 15:41:44,803 : Image to text: 9.08, 27.12, 39.5, 18.0
2019-02-16 15:41:52,593 : Text to Image: 7.808, 23.868, 35.74, 20.0
2019-02-16 15:42:35,320 : samples : 384000
2019-02-16 15:42:45,803 : Image to text: 8.94, 27.26, 40.32, 17.0
2019-02-16 15:42:53,507 : Text to Image: 7.652, 23.588, 35.668, 20.0
2019-02-16 15:43:36,637 : samples : 448000
2019-02-16 15:43:47,068 : Image to text: 9.24, 27.52, 39.7, 16.0
2019-02-16 15:43:54,841 : Text to Image: 7.74, 24.024, 35.704, 20.0
2019-02-16 15:44:37,723 : samples : 512000
2019-02-16 15:44:48,217 : Image to text: 9.28, 27.42, 39.86, 17.0
2019-02-16 15:44:55,914 : Text to Image: 7.212, 22.86, 34.272, 21.0
2019-02-16 15:45:33,196 : Epoch 13 finished
2019-02-16 15:45:33,627 : Image to text: 25.2, 57.2, 72.0, 4.0
2019-02-16 15:45:33,956 : Text to Image: 21.26, 55.2, 71.96, 5.0
2019-02-16 15:45:34,403 : Image to text: 23.8, 57.5, 72.5, 4.0
2019-02-16 15:45:34,767 : Text to Image: 20.74, 53.28, 70.42, 5.0
2019-02-16 15:45:35,209 : Image to text: 27.3, 59.0, 74.3, 4.0
2019-02-16 15:45:35,547 : Text to Image: 22.02, 55.16, 72.38, 4.0
2019-02-16 15:45:35,994 : Image to text: 22.6, 58.7, 74.3, 4.0
2019-02-16 15:45:36,333 : Text to Image: 20.82, 53.96, 70.88, 5.0
2019-02-16 15:45:36,787 : Image to text: 24.5, 59.1, 73.9, 4.0
2019-02-16 15:45:37,121 : Text to Image: 20.94, 53.8, 70.26, 5.0
2019-02-16 15:45:37,121 : Dev mean Text to Image: 21.156, 54.28, 71.18, 4.8
2019-02-16 15:45:37,122 : Dev mean Image to text: 24.68, 58.300000000000004, 73.4, 4.0
2019-02-16 15:45:37,123 : start epoch
2019-02-16 15:46:19,828 : samples : 64000
2019-02-16 15:46:30,341 : Image to text: 9.28, 26.94, 39.94, 17.0
2019-02-16 15:46:37,866 : Text to Image: 7.868, 23.924, 36.036, 20.0
2019-02-16 15:47:20,410 : samples : 128000
2019-02-16 15:47:30,931 : Image to text: 9.12, 27.48, 40.36, 16.0
2019-02-16 15:47:38,430 : Text to Image: 7.776, 23.796, 35.564, 20.0
2019-02-16 15:48:20,858 : samples : 192000
2019-02-16 15:48:31,385 : Image to text: 9.46, 27.86, 40.08, 16.0
2019-02-16 15:48:38,925 : Text to Image: 8.044, 24.332, 36.12, 20.0
2019-02-16 15:49:22,255 : samples : 256000
2019-02-16 15:49:32,787 : Image to text: 9.04, 27.24, 40.68, 16.0
2019-02-16 15:49:40,462 : Text to Image: 7.792, 24.12, 36.508, 19.0
2019-02-16 15:50:23,975 : samples : 320000
2019-02-16 15:50:34,434 : Image to text: 9.5, 27.24, 40.3, 16.0
2019-02-16 15:50:42,140 : Text to Image: 7.888, 24.036, 35.98, 20.0
2019-02-16 15:51:24,226 : samples : 384000
2019-02-16 15:51:34,681 : Image to text: 8.92, 27.2, 38.88, 17.0
2019-02-16 15:51:42,382 : Text to Image: 7.716, 24.112, 35.82, 20.0
2019-02-16 15:52:24,968 : samples : 448000
2019-02-16 15:52:35,480 : Image to text: 8.92, 27.58, 40.2, 16.0
2019-02-16 15:52:43,201 : Text to Image: 7.924, 24.504, 36.424, 19.0
2019-02-16 15:53:25,427 : samples : 512000
2019-02-16 15:53:35,851 : Image to text: 9.0, 27.36, 40.24, 16.0
2019-02-16 15:53:43,594 : Text to Image: 8.036, 24.5, 36.468, 19.0
2019-02-16 15:54:19,859 : Epoch 14 finished
2019-02-16 15:54:20,306 : Image to text: 25.4, 56.8, 72.2, 4.0
2019-02-16 15:54:20,643 : Text to Image: 20.96, 54.1, 71.8, 5.0
2019-02-16 15:54:21,090 : Image to text: 24.1, 56.9, 71.9, 4.0
2019-02-16 15:54:21,426 : Text to Image: 20.9, 53.68, 71.44, 5.0
2019-02-16 15:54:21,884 : Image to text: 27.4, 60.3, 73.0, 4.0
2019-02-16 15:54:22,230 : Text to Image: 22.12, 54.78, 71.64, 5.0
2019-02-16 15:54:22,688 : Image to text: 23.4, 58.9, 73.4, 4.0
2019-02-16 15:54:23,037 : Text to Image: 20.96, 54.14, 71.0, 5.0
2019-02-16 15:54:23,507 : Image to text: 27.4, 57.2, 74.1, 4.0
2019-02-16 15:54:23,841 : Text to Image: 21.24, 54.28, 71.22, 5.0
2019-02-16 15:54:23,841 : Dev mean Text to Image: 21.235999999999997, 54.196000000000005, 71.42, 5.0
2019-02-16 15:54:23,841 : Dev mean Image to text: 25.54, 58.019999999999996, 72.92, 4.0
2019-02-16 15:54:23,841 : start epoch
2019-02-16 15:55:06,208 : samples : 64000
2019-02-16 15:55:16,779 : Image to text: 9.8, 27.48, 40.52, 16.0
2019-02-16 15:55:24,412 : Text to Image: 7.96, 24.324, 35.72, 20.0
2019-02-16 15:56:15,476 : samples : 128000
2019-02-16 15:56:26,899 : Image to text: 9.5, 28.12, 40.82, 16.0
2019-02-16 15:56:34,464 : Text to Image: 7.812, 24.552, 36.288, 20.0
2019-02-16 15:57:16,848 : samples : 192000
2019-02-16 15:57:27,371 : Image to text: 8.7, 27.66, 40.8, 17.0
2019-02-16 15:57:34,934 : Text to Image: 7.776, 24.168, 36.008, 20.0
2019-02-16 15:58:17,546 : samples : 256000
2019-02-16 15:58:27,983 : Image to text: 9.6, 27.84, 40.0, 16.0
2019-02-16 15:58:35,711 : Text to Image: 7.984, 24.668, 36.272, 19.0
2019-02-16 15:59:19,381 : samples : 320000
2019-02-16 15:59:29,806 : Image to text: 8.78, 26.34, 39.22, 17.0
2019-02-16 15:59:37,555 : Text to Image: 8.064, 24.476, 36.116, 20.0
2019-02-16 16:00:20,729 : samples : 384000
2019-02-16 16:00:31,156 : Image to text: 9.1, 26.7, 39.12, 17.0
2019-02-16 16:00:38,745 : Text to Image: 7.436, 23.688, 35.348, 21.0
2019-02-16 16:01:21,694 : samples : 448000
2019-02-16 16:01:32,196 : Image to text: 9.52, 28.46, 40.16, 16.0
2019-02-16 16:01:39,904 : Text to Image: 8.064, 24.456, 36.18, 19.0
2019-02-16 16:02:22,836 : samples : 512000
2019-02-16 16:02:33,220 : Image to text: 8.88, 27.74, 39.86, 16.0
2019-02-16 16:02:40,783 : Text to Image: 7.988, 24.188, 35.988, 20.0
2019-02-16 16:03:17,339 : Epoch 15 finished
2019-02-16 16:03:17,780 : Image to text: 27.5, 58.9, 74.2, 4.0
2019-02-16 16:03:18,111 : Text to Image: 22.14, 54.84, 72.54, 5.0
2019-02-16 16:03:18,559 : Image to text: 24.0, 60.1, 75.7, 4.0
2019-02-16 16:03:18,896 : Text to Image: 20.92, 54.68, 72.02, 5.0
2019-02-16 16:03:19,346 : Image to text: 26.9, 60.4, 74.7, 4.0
2019-02-16 16:03:19,674 : Text to Image: 21.8, 54.82, 72.1, 5.0
2019-02-16 16:03:20,102 : Image to text: 24.8, 59.6, 74.6, 4.0
2019-02-16 16:03:20,431 : Text to Image: 21.52, 54.16, 71.78, 5.0
2019-02-16 16:03:20,872 : Image to text: 25.5, 60.1, 75.6, 4.0
2019-02-16 16:03:21,210 : Text to Image: 22.12, 54.28, 71.78, 5.0
2019-02-16 16:03:21,210 : Dev mean Text to Image: 21.700000000000003, 54.556000000000004, 72.044, 5.0
2019-02-16 16:03:21,210 : Dev mean Image to text: 25.740000000000002, 59.81999999999999, 74.96000000000001, 4.0
2019-02-16 16:03:21,210 : start epoch
2019-02-16 16:04:03,963 : samples : 64000
2019-02-16 16:04:14,413 : Image to text: 9.3, 26.86, 40.6, 16.0
2019-02-16 16:04:21,928 : Text to Image: 7.892, 24.404, 36.196, 19.0
2019-02-16 16:05:04,553 : samples : 128000
2019-02-16 16:05:15,048 : Image to text: 8.9, 26.9, 39.68, 17.0
2019-02-16 16:05:22,552 : Text to Image: 7.892, 24.368, 36.124, 20.0
2019-02-16 16:06:04,999 : samples : 192000
2019-02-16 16:06:15,534 : Image to text: 9.6, 27.22, 40.42, 17.0
2019-02-16 16:06:23,071 : Text to Image: 7.884, 24.288, 36.156, 20.0
2019-02-16 16:07:05,806 : samples : 256000
2019-02-16 16:07:16,182 : Image to text: 9.32, 28.0, 40.62, 16.0
2019-02-16 16:07:23,893 : Text to Image: 8.152, 24.36, 36.308, 19.0
2019-02-16 16:08:06,272 : samples : 320000
2019-02-16 16:08:16,697 : Image to text: 9.8, 28.12, 40.56, 16.0
2019-02-16 16:08:24,369 : Text to Image: 7.928, 24.192, 35.892, 20.0
2019-02-16 16:09:07,172 : samples : 384000
2019-02-16 16:09:17,615 : Image to text: 9.26, 27.26, 40.14, 17.0
2019-02-16 16:09:25,287 : Text to Image: 7.88, 23.824, 35.752, 20.0
2019-02-16 16:10:08,727 : samples : 448000
2019-02-16 16:10:19,135 : Image to text: 9.5, 27.5, 40.2, 16.0
2019-02-16 16:10:26,798 : Text to Image: 8.212, 24.836, 36.552, 19.0
2019-02-16 16:11:09,731 : samples : 512000
2019-02-16 16:11:20,139 : Image to text: 9.4, 27.52, 40.62, 16.0
2019-02-16 16:11:27,743 : Text to Image: 7.816, 24.344, 36.1, 19.0
2019-02-16 16:12:04,099 : Epoch 16 finished
2019-02-16 16:12:04,540 : Image to text: 25.5, 56.4, 72.8, 4.0
2019-02-16 16:12:04,876 : Text to Image: 21.02, 54.54, 71.82, 5.0
2019-02-16 16:12:05,321 : Image to text: 24.8, 56.9, 72.5, 4.0
2019-02-16 16:12:05,659 : Text to Image: 20.34, 54.38, 71.2, 5.0
2019-02-16 16:12:06,127 : Image to text: 25.6, 58.0, 75.3, 4.0
2019-02-16 16:12:06,474 : Text to Image: 21.74, 54.48, 72.04, 5.0
2019-02-16 16:12:06,915 : Image to text: 25.3, 58.6, 75.1, 4.0
2019-02-16 16:12:07,247 : Text to Image: 21.14, 53.92, 71.36, 5.0
2019-02-16 16:12:07,682 : Image to text: 27.2, 59.0, 74.8, 4.0
2019-02-16 16:12:08,013 : Text to Image: 22.54, 53.3, 70.34, 5.0
2019-02-16 16:12:08,013 : Dev mean Text to Image: 21.355999999999998, 54.123999999999995, 71.352, 5.0
2019-02-16 16:12:08,013 : Dev mean Image to text: 25.68, 57.78, 74.1, 4.0
2019-02-16 16:12:11,945 : 
Test scores | Image to text:             25.78, 58.31999999999999, 75.06, 4.0
2019-02-16 16:12:11,946 : Test scores | Text to image:             21.204, 54.260000000000005, 71.36, 5.0

2019-02-16 16:12:12,040 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 16:12:12,253 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 16:12:12,947 : loading BERT model bert-base-uncased
2019-02-16 16:12:12,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:12:12,981 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:12:12,981 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm6ydis6j
2019-02-16 16:12:15,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:12:17,007 : Computing embeddings for train/dev/test
2019-02-16 16:13:56,206 : Computed embeddings
2019-02-16 16:13:56,206 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:14:47,536 : [('reg:1e-05', 63.13), ('reg:0.0001', 62.03), ('reg:0.001', 50.25), ('reg:0.01', 50.84)]
2019-02-16 16:14:47,536 : Validation : best param found is reg = 1e-05 with score             63.13
2019-02-16 16:14:47,536 : Evaluating...
2019-02-16 16:15:04,144 : 
Dev acc : 63.1 Test acc : 62.8 for LENGTH classification

2019-02-16 16:15:04,145 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 16:15:04,524 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 16:15:04,578 : loading BERT model bert-base-uncased
2019-02-16 16:15:04,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:15:04,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:15:04,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgozoe4vr
2019-02-16 16:15:07,111 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:15:08,605 : Computing embeddings for train/dev/test
2019-02-16 16:16:37,692 : Computed embeddings
2019-02-16 16:16:37,692 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:17:27,064 : [('reg:1e-05', 17.53), ('reg:0.0001', 3.45), ('reg:0.001', 0.8), ('reg:0.01', 0.31)]
2019-02-16 16:17:27,064 : Validation : best param found is reg = 1e-05 with score             17.53
2019-02-16 16:17:27,064 : Evaluating...
2019-02-16 16:17:42,806 : 
Dev acc : 17.5 Test acc : 17.5 for WORDCONTENT classification

2019-02-16 16:17:42,808 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 16:17:43,187 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 16:17:43,255 : loading BERT model bert-base-uncased
2019-02-16 16:17:43,256 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:17:43,354 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:17:43,354 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa2u1xp42
2019-02-16 16:17:45,853 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:17:47,359 : Computing embeddings for train/dev/test
2019-02-16 16:19:10,000 : Computed embeddings
2019-02-16 16:19:10,000 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:19:48,311 : [('reg:1e-05', 30.06), ('reg:0.0001', 27.23), ('reg:0.001', 25.58), ('reg:0.01', 24.98)]
2019-02-16 16:19:48,311 : Validation : best param found is reg = 1e-05 with score             30.06
2019-02-16 16:19:48,311 : Evaluating...
2019-02-16 16:20:00,467 : 
Dev acc : 30.1 Test acc : 30.2 for DEPTH classification

2019-02-16 16:20:00,468 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 16:20:00,887 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 16:20:00,961 : loading BERT model bert-base-uncased
2019-02-16 16:20:00,961 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:20:01,088 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:20:01,088 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgjpjypdu
2019-02-16 16:20:03,625 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:20:05,140 : Computing embeddings for train/dev/test
2019-02-16 16:21:22,984 : Computed embeddings
2019-02-16 16:21:22,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:22:08,953 : [('reg:1e-05', 74.45), ('reg:0.0001', 74.91), ('reg:0.001', 66.8), ('reg:0.01', 58.57)]
2019-02-16 16:22:08,954 : Validation : best param found is reg = 0.0001 with score             74.91
2019-02-16 16:22:08,954 : Evaluating...
2019-02-16 16:22:21,604 : 
Dev acc : 74.9 Test acc : 75.8 for TOPCONSTITUENTS classification

2019-02-16 16:22:21,606 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 16:22:22,177 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 16:22:22,250 : loading BERT model bert-base-uncased
2019-02-16 16:22:22,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:22:22,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:22:22,287 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ikzvvzt
2019-02-16 16:22:24,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:22:26,263 : Computing embeddings for train/dev/test
2019-02-16 16:23:50,567 : Computed embeddings
2019-02-16 16:23:50,567 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:24:34,270 : [('reg:1e-05', 81.85), ('reg:0.0001', 81.79), ('reg:0.001', 81.19), ('reg:0.01', 78.8)]
2019-02-16 16:24:34,270 : Validation : best param found is reg = 1e-05 with score             81.85
2019-02-16 16:24:34,270 : Evaluating...
2019-02-16 16:24:45,049 : 
Dev acc : 81.8 Test acc : 81.4 for BIGRAMSHIFT classification

2019-02-16 16:24:45,050 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 16:24:45,499 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 16:24:45,582 : loading BERT model bert-base-uncased
2019-02-16 16:24:45,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:24:45,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:24:45,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp94v99yrj
2019-02-16 16:24:48,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:24:49,598 : Computing embeddings for train/dev/test
2019-02-16 16:26:11,641 : Computed embeddings
2019-02-16 16:26:11,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:26:52,790 : [('reg:1e-05', 89.89), ('reg:0.0001', 89.56), ('reg:0.001', 90.08), ('reg:0.01', 90.58)]
2019-02-16 16:26:52,790 : Validation : best param found is reg = 0.01 with score             90.58
2019-02-16 16:26:52,790 : Evaluating...
2019-02-16 16:27:03,799 : 
Dev acc : 90.6 Test acc : 89.7 for TENSE classification

2019-02-16 16:27:03,800 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 16:27:04,252 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 16:27:04,327 : loading BERT model bert-base-uncased
2019-02-16 16:27:04,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:27:04,360 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:27:04,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0oxyh_qh
2019-02-16 16:27:06,898 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:27:08,360 : Computing embeddings for train/dev/test
2019-02-16 16:28:35,251 : Computed embeddings
2019-02-16 16:28:35,251 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:29:26,308 : [('reg:1e-05', 87.48), ('reg:0.0001', 87.46), ('reg:0.001', 86.83), ('reg:0.01', 85.53)]
2019-02-16 16:29:26,308 : Validation : best param found is reg = 1e-05 with score             87.48
2019-02-16 16:29:26,308 : Evaluating...
2019-02-16 16:29:36,260 : 
Dev acc : 87.5 Test acc : 87.4 for SUBJNUMBER classification

2019-02-16 16:29:36,262 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 16:29:36,882 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 16:29:36,977 : loading BERT model bert-base-uncased
2019-02-16 16:29:36,977 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:29:37,147 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:29:37,147 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl55h_q1q
2019-02-16 16:29:40,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:29:42,078 : Computing embeddings for train/dev/test
2019-02-16 16:31:10,763 : Computed embeddings
2019-02-16 16:31:10,764 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:32:00,064 : [('reg:1e-05', 77.11), ('reg:0.0001', 77.04), ('reg:0.001', 77.0), ('reg:0.01', 71.89)]
2019-02-16 16:32:00,065 : Validation : best param found is reg = 1e-05 with score             77.11
2019-02-16 16:32:00,065 : Evaluating...
2019-02-16 16:32:12,919 : 
Dev acc : 77.1 Test acc : 78.2 for OBJNUMBER classification

2019-02-16 16:32:12,920 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 16:32:13,330 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 16:32:13,413 : loading BERT model bert-base-uncased
2019-02-16 16:32:13,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:32:13,553 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:32:13,554 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3jf2jfxn
2019-02-16 16:32:16,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:32:17,499 : Computing embeddings for train/dev/test
2019-02-16 16:33:54,805 : Computed embeddings
2019-02-16 16:33:54,805 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:34:34,201 : [('reg:1e-05', 56.93), ('reg:0.0001', 56.94), ('reg:0.001', 57.47), ('reg:0.01', 57.48)]
2019-02-16 16:34:34,201 : Validation : best param found is reg = 0.01 with score             57.48
2019-02-16 16:34:34,201 : Evaluating...
2019-02-16 16:34:45,225 : 
Dev acc : 57.5 Test acc : 57.1 for ODDMANOUT classification

2019-02-16 16:34:45,226 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 16:34:45,888 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 16:34:45,980 : loading BERT model bert-base-uncased
2019-02-16 16:34:45,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:34:46,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:34:46,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprgsp92pu
2019-02-16 16:34:48,548 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:34:49,986 : Computing embeddings for train/dev/test
2019-02-16 16:36:28,605 : Computed embeddings
2019-02-16 16:36:28,605 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:37:13,969 : [('reg:1e-05', 70.88), ('reg:0.0001', 70.93), ('reg:0.001', 61.18), ('reg:0.01', 58.98)]
2019-02-16 16:37:13,970 : Validation : best param found is reg = 0.0001 with score             70.93
2019-02-16 16:37:13,970 : Evaluating...
2019-02-16 16:37:26,121 : 
Dev acc : 70.9 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-16 16:37:26,123 : total results: {'STS12': {'MSRpar': {'pearson': (0.21066139401165082, 5.708935805789869e-09), 'spearman': SpearmanrResult(correlation=0.2657399200763268, pvalue=1.3733389725315842e-13), 'nsamples': 750}, 'MSRvid': {'pearson': (0.023787409071418766, 0.5154012727779222), 'spearman': SpearmanrResult(correlation=0.04141954400821207, pvalue=0.2572498908787381), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.42915150589769885, 5.438010172525836e-22), 'spearman': SpearmanrResult(correlation=0.49658582602304324, pvalue=5.937320498287193e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.30582979635881036, 1.0562661475247592e-17), 'spearman': SpearmanrResult(correlation=0.29567976221913966, pvalue=1.3438413685229922e-16), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5399910819948284, 1.4084582346580594e-31), 'spearman': SpearmanrResult(correlation=0.4452592396119006, pvalue=7.951976079773241e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3018842374668814, 'wmean': 0.2630778418611294}, 'spearman': {'mean': 0.30893685838772444, 'wmean': 0.27597192743805793}}}, 'STS13': {'FNWN': {'pearson': (0.0854695565445628, 0.24225934430483406), 'spearman': SpearmanrResult(correlation=0.09956169737735293, pvalue=0.17286487605289325), 'nsamples': 189}, 'headlines': {'pearson': (0.3958044884340722, 1.5455389632954395e-29), 'spearman': SpearmanrResult(correlation=0.3857471988898105, pvalue=5.092623683399922e-28), 'nsamples': 750}, 'OnWN': {'pearson': (0.026945046532353897, 0.5241916748899758), 'spearman': SpearmanrResult(correlation=0.023469662318130143, pvalue=0.5790822443608172), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.16940636383699628, 'wmean': 0.2187488557447514}, 'spearman': {'mean': 0.16959285286176454, 'wmean': 0.21419602702143242}}}, 'STS14': {'deft-forum': {'pearson': (-0.11571490848384293, 0.01404492706163805), 'spearman': SpearmanrResult(correlation=-0.09794449526413412, pvalue=0.037807680086308484), 'nsamples': 450}, 'deft-news': {'pearson': (0.38848768968750397, 3.022961312645294e-12), 'spearman': SpearmanrResult(correlation=0.4330976738617186, pvalue=3.79876367580961e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.3821974502630231, 1.699158067732195e-27), 'spearman': SpearmanrResult(correlation=0.35687581101631866, pvalue=6.035528693948021e-24), 'nsamples': 750}, 'images': {'pearson': (0.14990440368489527, 3.7598006961529614e-05), 'spearman': SpearmanrResult(correlation=0.16621930935785292, pvalue=4.730846074334027e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.2438253434795928, 1.2993627421067837e-11), 'spearman': SpearmanrResult(correlation=0.24456748084447838, pvalue=1.1216537265570908e-11), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4445089869844141, 1.1461134153345033e-37), 'spearman': SpearmanrResult(correlation=0.40740399483466805, pvalue=2.3568978141337697e-31), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.24886816093593103, 'wmean': 0.26128046303932423}, 'spearman': {'mean': 0.2517032957751504, 'wmean': 0.257907793687905}}}, 'STS15': {'answers-forums': {'pearson': (0.2253303058054386, 1.0543154608333686e-05), 'spearman': SpearmanrResult(correlation=0.2561548808220161, pvalue=4.956443152937436e-07), 'nsamples': 375}, 'answers-students': {'pearson': (0.3695704381941862, 1.0969652682199395e-25), 'spearman': SpearmanrResult(correlation=0.37737713185033006, pvalue=8.523084111837608e-27), 'nsamples': 750}, 'belief': {'pearson': (0.37845277505163466, 3.240587662618194e-14), 'spearman': SpearmanrResult(correlation=0.42809222975576816, pvalue=3.824550087318502e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.43102937000048996, 2.779684915401729e-35), 'spearman': SpearmanrResult(correlation=0.4396747665306025, pvalue=8.449661468367129e-37), 'nsamples': 750}, 'images': {'pearson': (0.12071963728809429, 0.0009241616252749972), 'spearman': SpearmanrResult(correlation=0.13780801331719023, pvalue=0.00015320887713354112), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.30502050526796876, 'wmean': 0.3058027464778268}, 'spearman': {'mean': 0.3278214044551814, 'wmean': 0.3242458667467537}}}, 'STS16': {'answer-answer': {'pearson': (0.3335872687446605, 5.119517623486627e-08), 'spearman': SpearmanrResult(correlation=0.33981937471285095, pvalue=2.7711539446671402e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5388428371234959, 3.744246360371832e-20), 'spearman': SpearmanrResult(correlation=0.5525752093654931, pvalue=2.617013368887917e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5215463887419741, 1.9166454774640222e-17), 'spearman': SpearmanrResult(correlation=0.6059246937835111, pvalue=1.9301124751035793e-24), 'nsamples': 230}, 'postediting': {'pearson': (0.682021008151072, 9.762787756819518e-35), 'spearman': SpearmanrResult(correlation=0.7568522528853883, pvalue=1.3568113385944333e-46), 'nsamples': 244}, 'question-question': {'pearson': (0.02581113652745789, 0.7106589951969635), 'spearman': SpearmanrResult(correlation=0.10009715276898969, pvalue=0.14929302046105217), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.42036172785773207, 'wmean': 0.4305787147037509}, 'spearman': {'mean': 0.47105373670324663, 'wmean': 0.4796461066711914}}}, 'MR': {'devacc': 74.84, 'acc': 75.52, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.69, 'acc': 75.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.68, 'acc': 86.61, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.62, 'acc': 93.8, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.96, 'acc': 78.69, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.24, 'acc': 42.58, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.13, 'acc': 87.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.51, 'acc': 68.17, 'f1': 76.65, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.4, 'acc': 71.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6691442503501241, 'pearson': 0.687198663860213, 'spearman': 0.6351076561112308, 'mse': 0.5385652793527482, 'yhat': array([2.20088921, 4.1497965 , 1.54807265, ..., 3.11174169, 4.30984041,        4.32978644]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.47728406848883, 'pearson': 0.44280712746696177, 'spearman': 0.44081226467020446, 'mse': 2.0199047320325065, 'yhat': array([1.86005691, 2.18744957, 2.84396434, ..., 3.77856238, 3.66884905,        3.65768549]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.91, 'acc': 61.42, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 308.82, 'acc': [(25.78, 58.31999999999999, 75.06, 4.0), (21.204, 54.260000000000005, 71.36, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 63.13, 'acc': 62.82, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 17.53, 'acc': 17.53, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.06, 'acc': 30.18, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.91, 'acc': 75.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 81.85, 'acc': 81.39, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.58, 'acc': 89.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 87.48, 'acc': 87.43, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.11, 'acc': 78.19, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.48, 'acc': 57.14, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.93, 'acc': 69.44, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 16:37:26,123 : STS12 p=0.2631, STS12 s=0.2760, STS13 p=0.2187, STS13 s=0.2142, STS14 p=0.2613, STS14 s=0.2579, STS15 p=0.3058, STS15 s=0.3242, STS 16 p=0.4306, STS16 s=0.4796, STS B p=0.4428, STS B s=0.4408, STS B m=2.0199, SICK-R p=0.6872, SICK-R s=0.6351, SICK-P m=0.5386
2019-02-16 16:37:26,123 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 16:37:26,123 : 0.2631,0.2760,0.2187,0.2142,0.2613,0.2579,0.3058,0.3242,0.4306,0.4796,0.4428,0.4408,2.0199,0.6872,0.6351,0.5386
2019-02-16 16:37:26,123 : MR=75.52, CR=75.18, SUBJ=93.80, MPQA=86.61, SST-B=78.69, SST-F=42.58, TREC=87.20, SICK-E=71.69, SNLI=61.42, MRPC=68.17, MRPC f=76.65
2019-02-16 16:37:26,123 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 16:37:26,123 : 75.52,75.18,93.80,86.61,78.69,42.58,87.20,71.69,61.42,68.17,76.65
2019-02-16 16:37:26,123 : COCO r1i2t=25.78, COCO r5i2t=58.32, COCO r10i2t=75.06, COCO medr_i2t=4.00, COCO r1t2i=21.20, COCO r5t2i=54.26, COCO r10t2i=71.36, COCO medr_t2i=5.00
2019-02-16 16:37:26,123 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 16:37:26,123 : 25.78,58.32,75.06,4.00,21.20,54.26,71.36,5.00
2019-02-16 16:37:26,123 : SentLen=62.82, WC=17.53, TreeDepth=30.18, TopConst=75.79, BShift=81.39, Tense=89.68, SubjNum=87.43, ObjNum=78.19, SOMO=57.14, CoordInv=69.44, average=64.96
2019-02-16 16:37:26,123 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 16:37:26,123 : 62.82,17.53,30.18,75.79,81.39,89.68,87.43,78.19,57.14,69.44,64.96
2019-02-16 16:37:26,123 : ********************************************************************************
2019-02-16 16:37:26,123 : ********************************************************************************
2019-02-16 16:37:26,123 : ********************************************************************************
2019-02-16 16:37:26,123 : layer 9
2019-02-16 16:37:26,124 : ********************************************************************************
2019-02-16 16:37:26,124 : ********************************************************************************
2019-02-16 16:37:26,124 : ********************************************************************************
2019-02-16 16:37:26,218 : ***** Transfer task : STS12 *****


2019-02-16 16:37:26,231 : loading BERT model bert-base-uncased
2019-02-16 16:37:26,232 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:37:26,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:37:26,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy1377yy6
2019-02-16 16:37:28,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:37:31,972 : MSRpar : pearson = 0.2373, spearman = 0.2770
2019-02-16 16:37:32,748 : MSRvid : pearson = 0.0012, spearman = 0.0538
2019-02-16 16:37:33,375 : SMTeuroparl : pearson = 0.4065, spearman = 0.4949
2019-02-16 16:37:34,539 : surprise.OnWN : pearson = 0.3251, spearman = 0.3275
2019-02-16 16:37:35,178 : surprise.SMTnews : pearson = 0.5508, spearman = 0.4714
2019-02-16 16:37:35,178 : ALL (weighted average) : Pearson = 0.2668,             Spearman = 0.2925
2019-02-16 16:37:35,178 : ALL (average) : Pearson = 0.3042,             Spearman = 0.3249

2019-02-16 16:37:35,178 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 16:37:35,186 : loading BERT model bert-base-uncased
2019-02-16 16:37:35,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:37:35,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:37:35,205 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc7jxe6nb
2019-02-16 16:37:37,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:37:39,778 : FNWN : pearson = 0.1479, spearman = 0.1614
2019-02-16 16:37:40,659 : headlines : pearson = 0.3875, spearman = 0.3759
2019-02-16 16:37:41,334 : OnWN : pearson = 0.0838, spearman = 0.0755
2019-02-16 16:37:41,335 : ALL (weighted average) : Pearson = 0.2437,             Spearman = 0.2365
2019-02-16 16:37:41,335 : ALL (average) : Pearson = 0.2064,             Spearman = 0.2043

2019-02-16 16:37:41,335 : ***** Transfer task : STS14 *****


2019-02-16 16:37:41,351 : loading BERT model bert-base-uncased
2019-02-16 16:37:41,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:37:41,371 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:37:41,371 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjkpjr6zw
2019-02-16 16:37:43,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:37:46,078 : deft-forum : pearson = -0.1190, spearman = -0.1098
2019-02-16 16:37:46,817 : deft-news : pearson = 0.3670, spearman = 0.4103
2019-02-16 16:37:47,824 : headlines : pearson = 0.3809, spearman = 0.3560
2019-02-16 16:37:48,776 : images : pearson = 0.1516, spearman = 0.1601
2019-02-16 16:37:49,739 : OnWN : pearson = 0.2906, spearman = 0.2875
2019-02-16 16:37:50,974 : tweet-news : pearson = 0.4698, spearman = 0.4243
2019-02-16 16:37:50,974 : ALL (weighted average) : Pearson = 0.2737,             Spearman = 0.2652
2019-02-16 16:37:50,974 : ALL (average) : Pearson = 0.2568,             Spearman = 0.2547

2019-02-16 16:37:50,974 : ***** Transfer task : STS15 *****


2019-02-16 16:37:51,009 : loading BERT model bert-base-uncased
2019-02-16 16:37:51,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:37:51,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:37:51,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphtbw6hc9
2019-02-16 16:37:53,523 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:37:55,848 : answers-forums : pearson = 0.2697, spearman = 0.2785
2019-02-16 16:37:56,810 : answers-students : pearson = 0.3904, spearman = 0.3971
2019-02-16 16:37:57,683 : belief : pearson = 0.4040, spearman = 0.4296
2019-02-16 16:37:58,695 : headlines : pearson = 0.4143, spearman = 0.4185
2019-02-16 16:37:59,679 : images : pearson = 0.1693, spearman = 0.1858
2019-02-16 16:37:59,679 : ALL (weighted average) : Pearson = 0.3277,             Spearman = 0.3388
2019-02-16 16:37:59,679 : ALL (average) : Pearson = 0.3295,             Spearman = 0.3419

2019-02-16 16:37:59,679 : ***** Transfer task : STS16 *****


2019-02-16 16:37:59,758 : loading BERT model bert-base-uncased
2019-02-16 16:37:59,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:37:59,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:37:59,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphr7838n5
2019-02-16 16:38:02,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:38:04,124 : answer-answer : pearson = 0.3271, spearman = 0.3360
2019-02-16 16:38:04,433 : headlines : pearson = 0.5373, spearman = 0.5620
2019-02-16 16:38:04,826 : plagiarism : pearson = 0.5936, spearman = 0.6176
2019-02-16 16:38:05,447 : postediting : pearson = 0.7078, spearman = 0.7427
2019-02-16 16:38:05,732 : question-question : pearson = -0.0398, spearman = 0.0092
2019-02-16 16:38:05,732 : ALL (weighted average) : Pearson = 0.4366,             Spearman = 0.4641
2019-02-16 16:38:05,732 : ALL (average) : Pearson = 0.4252,             Spearman = 0.4535

2019-02-16 16:38:05,732 : ***** Transfer task : MR *****


2019-02-16 16:38:05,751 : loading BERT model bert-base-uncased
2019-02-16 16:38:05,751 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:38:05,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:38:05,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp842zsir2
2019-02-16 16:38:08,261 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:38:09,800 : Generating sentence embeddings
2019-02-16 16:38:24,367 : Generated sentence embeddings
2019-02-16 16:38:24,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 16:38:41,600 : Best param found at split 1: l2reg = 0.0001                 with score 77.16
2019-02-16 16:38:58,089 : Best param found at split 2: l2reg = 1e-05                 with score 76.4
2019-02-16 16:39:16,167 : Best param found at split 3: l2reg = 0.001                 with score 76.68
2019-02-16 16:39:34,383 : Best param found at split 4: l2reg = 0.0001                 with score 76.34
2019-02-16 16:39:50,158 : Best param found at split 5: l2reg = 0.001                 with score 76.69
2019-02-16 16:39:51,524 : Dev acc : 76.65 Test acc : 77.88

2019-02-16 16:39:51,525 : ***** Transfer task : CR *****


2019-02-16 16:39:51,533 : loading BERT model bert-base-uncased
2019-02-16 16:39:51,533 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:39:51,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:39:51,555 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4h3idx0y
2019-02-16 16:39:54,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:39:55,493 : Generating sentence embeddings
2019-02-16 16:39:59,174 : Generated sentence embeddings
2019-02-16 16:39:59,175 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 16:40:04,685 : Best param found at split 1: l2reg = 0.01                 with score 82.58
2019-02-16 16:40:10,716 : Best param found at split 2: l2reg = 0.001                 with score 83.44
2019-02-16 16:40:17,198 : Best param found at split 3: l2reg = 0.001                 with score 83.41
2019-02-16 16:40:23,535 : Best param found at split 4: l2reg = 1e-05                 with score 83.45
2019-02-16 16:40:29,679 : Best param found at split 5: l2reg = 0.0001                 with score 83.98
2019-02-16 16:40:29,979 : Dev acc : 83.37 Test acc : 81.77

2019-02-16 16:40:29,980 : ***** Transfer task : MPQA *****


2019-02-16 16:40:29,992 : loading BERT model bert-base-uncased
2019-02-16 16:40:29,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:40:30,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:40:30,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7oxpvl4u
2019-02-16 16:40:32,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:40:34,002 : Generating sentence embeddings
2019-02-16 16:40:37,726 : Generated sentence embeddings
2019-02-16 16:40:37,727 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 16:40:54,703 : Best param found at split 1: l2reg = 1e-05                 with score 87.22
2019-02-16 16:41:13,998 : Best param found at split 2: l2reg = 1e-05                 with score 86.87
2019-02-16 16:41:30,202 : Best param found at split 3: l2reg = 1e-05                 with score 86.45
2019-02-16 16:41:47,582 : Best param found at split 4: l2reg = 0.001                 with score 87.73
2019-02-16 16:42:06,224 : Best param found at split 5: l2reg = 0.001                 with score 87.05
2019-02-16 16:42:07,651 : Dev acc : 87.06 Test acc : 87.76

2019-02-16 16:42:07,652 : ***** Transfer task : SUBJ *****


2019-02-16 16:42:07,672 : loading BERT model bert-base-uncased
2019-02-16 16:42:07,672 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:42:07,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:42:07,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv3a3avzm
2019-02-16 16:42:10,238 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:42:11,734 : Generating sentence embeddings
2019-02-16 16:42:25,903 : Generated sentence embeddings
2019-02-16 16:42:25,904 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 16:42:40,771 : Best param found at split 1: l2reg = 0.001                 with score 94.82
2019-02-16 16:42:55,747 : Best param found at split 2: l2reg = 0.0001                 with score 94.82
2019-02-16 16:43:10,895 : Best param found at split 3: l2reg = 0.0001                 with score 94.62
2019-02-16 16:43:27,981 : Best param found at split 4: l2reg = 0.0001                 with score 95.15
2019-02-16 16:43:44,604 : Best param found at split 5: l2reg = 0.001                 with score 94.75
2019-02-16 16:43:45,424 : Dev acc : 94.83 Test acc : 94.58

2019-02-16 16:43:45,425 : ***** Transfer task : SST Binary classification *****


2019-02-16 16:43:45,568 : loading BERT model bert-base-uncased
2019-02-16 16:43:45,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:43:45,596 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:43:45,596 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzmyc5xi8
2019-02-16 16:43:48,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:43:49,564 : Computing embedding for train
2019-02-16 16:44:35,199 : Computed train embeddings
2019-02-16 16:44:35,199 : Computing embedding for dev
2019-02-16 16:44:36,145 : Computed dev embeddings
2019-02-16 16:44:36,145 : Computing embedding for test
2019-02-16 16:44:38,109 : Computed test embeddings
2019-02-16 16:44:38,109 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:45:04,439 : [('reg:1e-05', 82.0), ('reg:0.0001', 82.0), ('reg:0.001', 81.42), ('reg:0.01', 80.62)]
2019-02-16 16:45:04,440 : Validation : best param found is reg = 1e-05 with score             82.0
2019-02-16 16:45:04,440 : Evaluating...
2019-02-16 16:45:10,764 : 
Dev acc : 82.0 Test acc : 80.83 for             SST Binary classification

2019-02-16 16:45:10,764 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 16:45:10,819 : loading BERT model bert-base-uncased
2019-02-16 16:45:10,819 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:45:10,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:45:10,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa7b7mf69
2019-02-16 16:45:13,352 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:45:14,763 : Computing embedding for train
2019-02-16 16:45:24,295 : Computed train embeddings
2019-02-16 16:45:24,296 : Computing embedding for dev
2019-02-16 16:45:25,508 : Computed dev embeddings
2019-02-16 16:45:25,508 : Computing embedding for test
2019-02-16 16:45:27,931 : Computed test embeddings
2019-02-16 16:45:27,931 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:45:32,345 : [('reg:1e-05', 41.87), ('reg:0.0001', 39.42), ('reg:0.001', 42.69), ('reg:0.01', 41.33)]
2019-02-16 16:45:32,345 : Validation : best param found is reg = 0.001 with score             42.69
2019-02-16 16:45:32,345 : Evaluating...
2019-02-16 16:45:33,183 : 
Dev acc : 42.69 Test acc : 41.67 for             SST Fine-Grained classification

2019-02-16 16:45:33,184 : ***** Transfer task : TREC *****


2019-02-16 16:45:33,205 : loading BERT model bert-base-uncased
2019-02-16 16:45:33,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:45:33,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:45:33,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb3s0m8oi
2019-02-16 16:45:35,701 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:45:40,603 : Computed train embeddings
2019-02-16 16:45:40,875 : Computed test embeddings
2019-02-16 16:45:40,875 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 16:45:50,528 : [('reg:1e-05', 80.39), ('reg:0.0001', 80.5), ('reg:0.001', 79.23), ('reg:0.01', 75.37)]
2019-02-16 16:45:50,529 : Cross-validation : best param found is reg = 0.0001             with score 80.5
2019-02-16 16:45:50,529 : Evaluating...
2019-02-16 16:45:51,509 : 
Dev acc : 80.5 Test acc : 86.8             for TREC

2019-02-16 16:45:51,510 : ***** Transfer task : MRPC *****


2019-02-16 16:45:51,537 : loading BERT model bert-base-uncased
2019-02-16 16:45:51,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:45:51,560 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:45:51,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnr_ckxd4
2019-02-16 16:45:54,081 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:45:55,575 : Computing embedding for train
2019-02-16 16:46:05,200 : Computed train embeddings
2019-02-16 16:46:05,200 : Computing embedding for test
2019-02-16 16:46:09,357 : Computed test embeddings
2019-02-16 16:46:09,373 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 16:46:14,989 : [('reg:1e-05', 70.71), ('reg:0.0001', 70.78), ('reg:0.001', 71.22), ('reg:0.01', 70.53)]
2019-02-16 16:46:14,990 : Cross-validation : best param found is reg = 0.001             with score 71.22
2019-02-16 16:46:14,990 : Evaluating...
2019-02-16 16:46:15,322 : Dev acc : 71.22 Test acc 71.07; Test F1 81.12 for MRPC.

2019-02-16 16:46:15,322 : ***** Transfer task : SICK-Entailment*****


2019-02-16 16:46:15,405 : loading BERT model bert-base-uncased
2019-02-16 16:46:15,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:46:15,432 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:46:15,432 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7iy3r2im
2019-02-16 16:46:18,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:46:20,293 : Computing embedding for train
2019-02-16 16:46:25,918 : Computed train embeddings
2019-02-16 16:46:25,918 : Computing embedding for dev
2019-02-16 16:46:26,645 : Computed dev embeddings
2019-02-16 16:46:26,645 : Computing embedding for test
2019-02-16 16:46:32,676 : Computed test embeddings
2019-02-16 16:46:32,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 16:46:34,166 : [('reg:1e-05', 71.2), ('reg:0.0001', 73.2), ('reg:0.001', 73.8), ('reg:0.01', 73.8)]
2019-02-16 16:46:34,166 : Validation : best param found is reg = 0.001 with score             73.8
2019-02-16 16:46:34,166 : Evaluating...
2019-02-16 16:46:34,511 : 
Dev acc : 73.8 Test acc : 71.75 for                        SICK entailment

2019-02-16 16:46:34,512 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 16:46:34,553 : loading BERT model bert-base-uncased
2019-02-16 16:46:34,554 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:46:34,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:46:34,584 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6tb_5oai
2019-02-16 16:46:37,478 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:46:39,717 : Computing embedding for train
2019-02-16 16:46:45,505 : Computed train embeddings
2019-02-16 16:46:45,506 : Computing embedding for dev
2019-02-16 16:46:46,260 : Computed dev embeddings
2019-02-16 16:46:46,260 : Computing embedding for test
2019-02-16 16:46:52,356 : Computed test embeddings
2019-02-16 16:47:22,144 : Dev : Pearson 0.7166140888139473
2019-02-16 16:47:22,144 : Test : Pearson 0.6943244008528043 Spearman 0.6384538811677655 MSE 0.5348548722864234                        for SICK Relatedness

2019-02-16 16:47:22,146 : 

***** Transfer task : STSBenchmark*****


2019-02-16 16:47:22,186 : loading BERT model bert-base-uncased
2019-02-16 16:47:22,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:47:22,216 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:47:22,217 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnrvvn6yu
2019-02-16 16:47:24,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:47:26,061 : Computing embedding for train
2019-02-16 16:47:34,727 : Computed train embeddings
2019-02-16 16:47:34,727 : Computing embedding for dev
2019-02-16 16:47:37,132 : Computed dev embeddings
2019-02-16 16:47:37,132 : Computing embedding for test
2019-02-16 16:47:39,080 : Computed test embeddings
2019-02-16 16:48:08,796 : Dev : Pearson 0.4678601789119381
2019-02-16 16:48:08,796 : Test : Pearson 0.42740355164288973 Spearman 0.42865504598877424 MSE 2.0439178769739743                        for SICK Relatedness

2019-02-16 16:48:08,796 : ***** Transfer task : SNLI Entailment*****


2019-02-16 16:48:13,862 : loading BERT model bert-base-uncased
2019-02-16 16:48:13,862 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 16:48:14,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 16:48:14,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzpt8rchy
2019-02-16 16:48:16,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 16:48:18,120 : PROGRESS (encoding): 0.00%
2019-02-16 16:49:37,131 : PROGRESS (encoding): 14.56%
2019-02-16 16:51:04,363 : PROGRESS (encoding): 29.12%
2019-02-16 16:52:31,507 : PROGRESS (encoding): 43.69%
2019-02-16 16:54:06,003 : PROGRESS (encoding): 58.25%
2019-02-16 16:55:49,728 : PROGRESS (encoding): 72.81%
2019-02-16 16:57:34,124 : PROGRESS (encoding): 87.37%
2019-02-16 16:59:23,750 : PROGRESS (encoding): 0.00%
2019-02-16 16:59:37,625 : PROGRESS (encoding): 0.00%
2019-02-16 16:59:50,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 17:00:40,197 : [('reg:1e-09', 53.94)]
2019-02-16 17:00:40,197 : Validation : best param found is reg = 1e-09 with score             53.94
2019-02-16 17:00:40,197 : Evaluating...
2019-02-16 17:01:28,895 : Dev acc : 53.94 Test acc : 54.36 for SNLI

2019-02-16 17:01:28,896 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 17:01:38,031 : loading BERT model bert-base-uncased
2019-02-16 17:01:38,031 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 17:01:38,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 17:01:38,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwfl77nhv
2019-02-16 17:01:40,555 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 17:01:42,047 : Computing embedding for train
2019-02-16 17:09:14,571 : Computed train embeddings
2019-02-16 17:09:14,571 : Computing embedding for dev
2019-02-16 17:09:33,749 : Computed dev embeddings
2019-02-16 17:09:33,749 : Computing embedding for test
2019-02-16 17:09:53,595 : Computed test embeddings
2019-02-16 17:09:53,611 : prepare data
2019-02-16 17:09:53,679 : start epoch
2019-02-16 17:10:35,855 : samples : 64000
2019-02-16 17:10:46,309 : Image to text: 2.9, 12.04, 20.6, 45.0
2019-02-16 17:10:53,859 : Text to Image: 2.708, 10.596, 17.412, 54.0
2019-02-16 17:11:36,433 : samples : 128000
2019-02-16 17:11:46,886 : Image to text: 3.82, 14.26, 23.34, 37.0
2019-02-16 17:11:54,417 : Text to Image: 3.16, 12.104, 19.868, 46.0
2019-02-16 17:12:36,895 : samples : 192000
2019-02-16 17:12:47,359 : Image to text: 4.38, 14.22, 23.44, 35.0
2019-02-16 17:12:54,946 : Text to Image: 3.632, 12.748, 20.788, 44.0
2019-02-16 17:13:37,817 : samples : 256000
2019-02-16 17:13:48,247 : Image to text: 4.62, 16.6, 25.68, 33.0
2019-02-16 17:13:55,893 : Text to Image: 3.488, 13.316, 21.648, 41.0
2019-02-16 17:14:38,892 : samples : 320000
2019-02-16 17:14:49,428 : Image to text: 4.18, 15.8, 25.6, 34.0
2019-02-16 17:14:56,974 : Text to Image: 4.152, 14.5, 23.324, 37.0
2019-02-16 17:15:39,509 : samples : 384000
2019-02-16 17:15:49,971 : Image to text: 5.0, 17.56, 27.74, 30.0
2019-02-16 17:15:57,634 : Text to Image: 4.284, 15.352, 24.276, 36.0
2019-02-16 17:16:40,716 : samples : 448000
2019-02-16 17:16:51,166 : Image to text: 5.22, 17.4, 27.94, 30.0
2019-02-16 17:16:58,778 : Text to Image: 4.364, 15.536, 24.488, 36.0
2019-02-16 17:17:42,256 : samples : 512000
2019-02-16 17:17:52,775 : Image to text: 4.76, 18.2, 27.82, 30.0
2019-02-16 17:18:00,413 : Text to Image: 4.728, 16.22, 25.648, 34.0
2019-02-16 17:18:36,853 : Epoch 1 finished
2019-02-16 17:18:37,298 : Image to text: 18.3, 45.8, 62.4, 6.0
2019-02-16 17:18:37,639 : Text to Image: 14.18, 42.24, 59.34, 7.0
2019-02-16 17:18:38,091 : Image to text: 16.2, 43.3, 62.8, 7.0
2019-02-16 17:18:38,425 : Text to Image: 14.32, 41.72, 59.06, 7.0
2019-02-16 17:18:38,878 : Image to text: 19.0, 50.3, 64.2, 5.0
2019-02-16 17:18:39,224 : Text to Image: 14.42, 42.76, 59.22, 7.0
2019-02-16 17:18:39,664 : Image to text: 15.6, 45.5, 62.9, 6.0
2019-02-16 17:18:39,995 : Text to Image: 14.52, 41.0, 58.36, 8.0
2019-02-16 17:18:40,425 : Image to text: 17.3, 47.4, 62.8, 6.0
2019-02-16 17:18:40,754 : Text to Image: 15.48, 43.54, 60.32, 7.0
2019-02-16 17:18:40,754 : Dev mean Text to Image: 14.584, 42.251999999999995, 59.26, 7.199999999999999
2019-02-16 17:18:40,754 : Dev mean Image to text: 17.28, 46.459999999999994, 63.019999999999996, 6.0
2019-02-16 17:18:40,754 : start epoch
2019-02-16 17:19:23,876 : samples : 64000
2019-02-16 17:19:34,381 : Image to text: 5.6, 18.72, 30.12, 27.0
2019-02-16 17:19:41,974 : Text to Image: 4.8, 16.432, 26.18, 32.0
2019-02-16 17:20:32,404 : samples : 128000
2019-02-16 17:20:44,434 : Image to text: 5.74, 19.76, 30.46, 26.0
2019-02-16 17:20:51,986 : Text to Image: 4.736, 15.984, 25.324, 34.0
2019-02-16 17:21:34,867 : samples : 192000
2019-02-16 17:21:45,322 : Image to text: 5.98, 20.72, 31.46, 25.0
2019-02-16 17:21:52,994 : Text to Image: 5.084, 17.324, 26.8, 31.0
2019-02-16 17:22:35,947 : samples : 256000
2019-02-16 17:22:46,429 : Image to text: 5.98, 19.1, 29.78, 27.0
2019-02-16 17:22:54,095 : Text to Image: 5.148, 17.6, 27.304, 31.0
2019-02-16 17:23:37,313 : samples : 320000
2019-02-16 17:23:47,794 : Image to text: 6.42, 19.92, 30.6, 25.0
2019-02-16 17:23:55,420 : Text to Image: 5.224, 17.116, 26.972, 31.0
2019-02-16 17:24:38,202 : samples : 384000
2019-02-16 17:24:48,688 : Image to text: 5.96, 19.72, 30.02, 26.0
2019-02-16 17:24:56,291 : Text to Image: 4.92, 16.98, 26.58, 30.0
2019-02-16 17:25:39,255 : samples : 448000
2019-02-16 17:25:49,759 : Image to text: 5.8, 19.02, 29.74, 27.0
2019-02-16 17:25:57,379 : Text to Image: 5.008, 17.116, 27.076, 31.0
2019-02-16 17:26:40,264 : samples : 512000
2019-02-16 17:26:50,781 : Image to text: 6.76, 21.3, 32.44, 23.0
2019-02-16 17:26:58,391 : Text to Image: 5.564, 18.388, 28.636, 28.0
2019-02-16 17:27:35,145 : Epoch 2 finished
2019-02-16 17:27:35,608 : Image to text: 18.2, 48.4, 64.9, 6.0
2019-02-16 17:27:35,951 : Text to Image: 16.42, 45.4, 63.54, 7.0
2019-02-16 17:27:36,399 : Image to text: 16.2, 47.8, 64.9, 6.0
2019-02-16 17:27:36,731 : Text to Image: 16.08, 45.84, 62.74, 7.0
2019-02-16 17:27:37,177 : Image to text: 20.1, 49.8, 65.5, 6.0
2019-02-16 17:27:37,511 : Text to Image: 16.42, 46.04, 63.34, 6.0
2019-02-16 17:27:37,952 : Image to text: 18.1, 49.2, 65.6, 6.0
2019-02-16 17:27:38,290 : Text to Image: 15.94, 44.98, 62.06, 7.0
2019-02-16 17:27:38,741 : Image to text: 19.3, 49.4, 65.7, 6.0
2019-02-16 17:27:39,085 : Text to Image: 16.7, 45.24, 62.56, 7.0
2019-02-16 17:27:39,085 : Dev mean Text to Image: 16.312, 45.5, 62.848, 6.800000000000001
2019-02-16 17:27:39,085 : Dev mean Image to text: 18.38, 48.91999999999999, 65.32, 6.0
2019-02-16 17:27:39,086 : start epoch
2019-02-16 17:28:21,705 : samples : 64000
2019-02-16 17:28:32,176 : Image to text: 6.28, 20.54, 31.9, 25.0
2019-02-16 17:28:39,635 : Text to Image: 5.268, 17.908, 27.856, 29.0
2019-02-16 17:29:22,088 : samples : 128000
2019-02-16 17:29:32,603 : Image to text: 5.48, 19.42, 30.48, 26.0
2019-02-16 17:29:40,123 : Text to Image: 5.068, 17.16, 27.24, 30.0
2019-02-16 17:30:23,383 : samples : 192000
2019-02-16 17:30:35,966 : Image to text: 6.8, 21.38, 32.42, 24.0
2019-02-16 17:30:46,024 : Text to Image: 5.104, 17.748, 27.752, 30.0
2019-02-16 17:31:31,655 : samples : 256000
2019-02-16 17:31:43,054 : Image to text: 6.18, 20.46, 31.84, 24.0
2019-02-16 17:31:50,406 : Text to Image: 5.208, 17.872, 27.788, 29.0
2019-02-16 17:32:33,114 : samples : 320000
2019-02-16 17:32:43,414 : Image to text: 6.94, 21.46, 33.06, 23.0
2019-02-16 17:32:50,801 : Text to Image: 5.672, 19.104, 29.624, 27.0
2019-02-16 17:33:34,325 : samples : 384000
2019-02-16 17:33:46,901 : Image to text: 6.76, 22.5, 33.18, 23.0
2019-02-16 17:33:56,951 : Text to Image: 5.604, 18.864, 29.132, 28.0
2019-02-16 17:34:41,347 : samples : 448000
2019-02-16 17:34:51,579 : Image to text: 6.32, 21.22, 31.92, 25.0
2019-02-16 17:34:58,797 : Text to Image: 5.732, 18.988, 29.5, 27.0
2019-02-16 17:35:40,543 : samples : 512000
2019-02-16 17:35:50,815 : Image to text: 6.16, 21.36, 33.3, 24.0
2019-02-16 17:35:58,206 : Text to Image: 5.408, 19.0, 29.104, 28.0
2019-02-16 17:36:35,114 : Epoch 3 finished
2019-02-16 17:36:36,047 : Image to text: 19.6, 49.0, 65.1, 6.0
2019-02-16 17:36:36,827 : Text to Image: 15.66, 44.46, 61.8, 7.0
2019-02-16 17:36:37,773 : Image to text: 17.8, 50.2, 66.1, 5.0
2019-02-16 17:36:38,529 : Text to Image: 15.76, 44.84, 62.48, 7.0
2019-02-16 17:36:39,522 : Image to text: 23.1, 51.5, 67.2, 5.0
2019-02-16 17:36:40,299 : Text to Image: 15.96, 45.2, 62.1, 7.0
2019-02-16 17:36:41,254 : Image to text: 19.0, 50.0, 65.4, 5.0
2019-02-16 17:36:42,010 : Text to Image: 14.82, 43.9, 62.0, 7.0
2019-02-16 17:36:42,954 : Image to text: 20.0, 52.4, 67.3, 5.0
2019-02-16 17:36:43,724 : Text to Image: 16.2, 45.22, 62.24, 7.0
2019-02-16 17:36:43,724 : Dev mean Text to Image: 15.680000000000001, 44.724000000000004, 62.123999999999995, 7.0
2019-02-16 17:36:43,725 : Dev mean Image to text: 19.900000000000002, 50.620000000000005, 66.22, 5.2
2019-02-16 17:36:43,725 : start epoch
2019-02-16 17:37:36,263 : samples : 64000
2019-02-16 17:37:46,523 : Image to text: 7.02, 22.2, 33.66, 23.0
2019-02-16 17:37:53,880 : Text to Image: 5.824, 19.116, 29.628, 26.0
2019-02-16 17:38:37,664 : samples : 128000
2019-02-16 17:38:50,209 : Image to text: 6.66, 21.94, 33.62, 22.0
2019-02-16 17:39:00,196 : Text to Image: 6.012, 19.652, 30.248, 26.0
2019-02-16 17:39:44,592 : samples : 192000
2019-02-16 17:39:54,843 : Image to text: 6.4, 21.38, 32.3, 24.0
2019-02-16 17:40:02,286 : Text to Image: 5.316, 18.444, 28.668, 28.0
2019-02-16 17:40:44,925 : samples : 256000
2019-02-16 17:40:57,463 : Image to text: 6.56, 21.76, 33.2, 22.0
2019-02-16 17:41:07,480 : Text to Image: 5.976, 19.504, 29.852, 26.0
2019-02-16 17:41:52,740 : samples : 320000
2019-02-16 17:42:05,178 : Image to text: 7.04, 22.88, 34.06, 22.0
2019-02-16 17:42:12,587 : Text to Image: 6.024, 20.084, 30.508, 26.0
2019-02-16 17:42:55,348 : samples : 384000
2019-02-16 17:43:05,653 : Image to text: 6.54, 21.76, 33.76, 22.0
2019-02-16 17:43:13,020 : Text to Image: 5.996, 19.58, 29.872, 27.0
2019-02-16 17:43:56,315 : samples : 448000
2019-02-16 17:44:08,892 : Image to text: 7.58, 23.2, 34.76, 22.0
2019-02-16 17:44:18,956 : Text to Image: 6.156, 19.728, 30.496, 26.0
2019-02-16 17:45:03,243 : samples : 512000
2019-02-16 17:45:13,535 : Image to text: 7.0, 23.44, 34.92, 21.0
2019-02-16 17:45:20,966 : Text to Image: 6.36, 20.244, 31.136, 25.0
2019-02-16 17:45:57,547 : Epoch 4 finished
2019-02-16 17:45:58,487 : Image to text: 21.0, 51.7, 68.5, 5.0
2019-02-16 17:45:59,302 : Text to Image: 17.88, 48.22, 65.46, 6.0
2019-02-16 17:46:00,211 : Image to text: 21.9, 50.5, 66.6, 5.0
2019-02-16 17:46:00,972 : Text to Image: 17.3, 47.7, 64.44, 6.0
2019-02-16 17:46:01,902 : Image to text: 23.1, 56.1, 71.6, 5.0
2019-02-16 17:46:02,686 : Text to Image: 18.46, 48.42, 65.76, 6.0
2019-02-16 17:46:03,647 : Image to text: 18.6, 53.2, 70.3, 5.0
2019-02-16 17:46:04,432 : Text to Image: 16.74, 46.68, 64.98, 6.0
2019-02-16 17:46:05,350 : Image to text: 20.4, 51.6, 69.0, 5.0
2019-02-16 17:46:06,103 : Text to Image: 17.6, 47.98, 65.06, 6.0
2019-02-16 17:46:06,103 : Dev mean Text to Image: 17.596, 47.8, 65.14, 6.0
2019-02-16 17:46:06,103 : Dev mean Image to text: 21.0, 52.62, 69.19999999999999, 5.0
2019-02-16 17:46:06,105 : start epoch
2019-02-16 17:46:50,994 : samples : 64000
2019-02-16 17:47:03,576 : Image to text: 6.64, 23.12, 33.24, 22.0
2019-02-16 17:47:13,207 : Text to Image: 6.036, 20.288, 30.92, 25.0
2019-02-16 17:47:55,788 : samples : 128000
2019-02-16 17:48:05,917 : Image to text: 7.32, 22.84, 34.34, 21.0
2019-02-16 17:48:14,843 : Text to Image: 6.12, 19.804, 30.384, 26.0
2019-02-16 17:48:59,133 : samples : 192000
2019-02-16 17:49:11,720 : Image to text: 7.1, 22.38, 33.92, 22.0
2019-02-16 17:49:21,753 : Text to Image: 6.136, 19.5, 29.804, 26.0
2019-02-16 17:50:05,089 : samples : 256000
2019-02-16 17:50:15,340 : Image to text: 5.94, 21.58, 33.44, 22.0
2019-02-16 17:50:22,673 : Text to Image: 5.98, 19.344, 30.1, 26.0
2019-02-16 17:51:05,689 : samples : 320000
2019-02-16 17:51:18,275 : Image to text: 7.08, 22.24, 33.98, 21.0
2019-02-16 17:51:28,378 : Text to Image: 5.948, 19.744, 30.476, 26.0
2019-02-16 17:52:14,230 : samples : 384000
2019-02-16 17:52:26,879 : Image to text: 7.04, 22.7, 33.92, 21.0
2019-02-16 17:52:34,582 : Text to Image: 6.18, 20.308, 31.316, 25.0
2019-02-16 17:53:17,358 : samples : 448000
2019-02-16 17:53:27,444 : Image to text: 7.64, 22.72, 34.92, 22.0
2019-02-16 17:53:34,586 : Text to Image: 6.332, 20.384, 31.092, 25.0
2019-02-16 17:54:27,252 : samples : 512000
2019-02-16 17:54:39,896 : Image to text: 7.24, 23.7, 35.72, 21.0
2019-02-16 17:54:49,924 : Text to Image: 6.532, 20.64, 31.492, 24.0
2019-02-16 17:55:27,736 : Epoch 5 finished
2019-02-16 17:55:28,203 : Image to text: 21.0, 53.5, 70.1, 5.0
2019-02-16 17:55:28,586 : Text to Image: 19.18, 50.08, 67.12, 5.0
2019-02-16 17:55:29,042 : Image to text: 21.2, 52.2, 67.7, 5.0
2019-02-16 17:55:29,416 : Text to Image: 18.36, 49.2, 66.98, 6.0
2019-02-16 17:55:29,875 : Image to text: 23.7, 53.0, 68.9, 5.0
2019-02-16 17:55:30,238 : Text to Image: 19.1, 50.02, 67.12, 5.0
2019-02-16 17:55:30,688 : Image to text: 20.0, 54.7, 71.8, 5.0
2019-02-16 17:55:31,051 : Text to Image: 17.84, 48.74, 66.36, 6.0
2019-02-16 17:55:31,500 : Image to text: 22.8, 53.1, 70.4, 5.0
2019-02-16 17:55:31,861 : Text to Image: 19.28, 50.44, 66.46, 5.0
2019-02-16 17:55:31,862 : Dev mean Text to Image: 18.752, 49.696000000000005, 66.80799999999999, 5.4
2019-02-16 17:55:31,862 : Dev mean Image to text: 21.740000000000002, 53.30000000000001, 69.78, 5.0
2019-02-16 17:55:31,862 : start epoch
2019-02-16 17:56:14,151 : samples : 64000
2019-02-16 17:56:25,513 : Image to text: 6.96, 23.62, 35.7, 21.0
2019-02-16 17:56:33,705 : Text to Image: 6.6, 21.216, 32.124, 24.0
2019-02-16 17:57:17,520 : samples : 128000
2019-02-16 17:57:27,840 : Image to text: 6.88, 23.72, 35.76, 21.0
2019-02-16 17:57:35,163 : Text to Image: 6.32, 20.612, 31.332, 24.0
2019-02-16 17:58:17,975 : samples : 192000
2019-02-16 17:58:28,220 : Image to text: 7.34, 22.8, 34.26, 21.0
2019-02-16 17:58:35,628 : Text to Image: 5.964, 19.66, 30.212, 26.0
2019-02-16 17:59:18,389 : samples : 256000
2019-02-16 17:59:28,590 : Image to text: 7.16, 22.74, 34.54, 21.0
2019-02-16 17:59:36,065 : Text to Image: 6.42, 20.56, 31.464, 25.0
2019-02-16 18:00:18,926 : samples : 320000
2019-02-16 18:00:29,184 : Image to text: 7.5, 22.78, 33.98, 22.0
2019-02-16 18:00:36,603 : Text to Image: 6.416, 20.248, 31.392, 25.0
2019-02-16 18:01:19,726 : samples : 384000
2019-02-16 18:01:31,432 : Image to text: 8.06, 24.42, 36.14, 20.0
2019-02-16 18:01:38,848 : Text to Image: 6.54, 20.872, 31.64, 24.0
2019-02-16 18:02:21,398 : samples : 448000
2019-02-16 18:02:31,589 : Image to text: 7.8, 22.72, 34.98, 21.0
2019-02-16 18:02:39,004 : Text to Image: 6.06, 19.916, 30.936, 25.0
2019-02-16 18:03:22,542 : samples : 512000
2019-02-16 18:03:35,258 : Image to text: 7.48, 23.36, 35.34, 20.0
2019-02-16 18:03:45,453 : Text to Image: 6.084, 20.276, 30.84, 25.0
2019-02-16 18:04:22,047 : Epoch 6 finished
2019-02-16 18:04:22,527 : Image to text: 23.4, 54.2, 70.4, 5.0
2019-02-16 18:04:22,902 : Text to Image: 18.8, 50.96, 67.86, 5.0
2019-02-16 18:04:23,363 : Image to text: 22.5, 54.4, 70.6, 5.0
2019-02-16 18:04:23,736 : Text to Image: 18.2, 50.22, 67.66, 5.0
2019-02-16 18:04:24,199 : Image to text: 23.4, 53.9, 68.1, 5.0
2019-02-16 18:04:24,588 : Text to Image: 18.9, 50.62, 68.46, 5.0
2019-02-16 18:04:25,050 : Image to text: 20.6, 53.5, 71.1, 5.0
2019-02-16 18:04:25,418 : Text to Image: 18.26, 49.94, 67.52, 6.0
2019-02-16 18:04:25,890 : Image to text: 22.3, 53.8, 68.6, 5.0
2019-02-16 18:04:26,256 : Text to Image: 18.64, 50.44, 67.08, 5.0
2019-02-16 18:04:26,256 : Dev mean Text to Image: 18.560000000000002, 50.436, 67.716, 5.2
2019-02-16 18:04:26,256 : Dev mean Image to text: 22.44, 53.96, 69.75999999999999, 5.0
2019-02-16 18:04:26,256 : start epoch
2019-02-16 18:05:09,679 : samples : 64000
2019-02-16 18:05:20,660 : Image to text: 7.68, 23.52, 36.1, 20.0
2019-02-16 18:05:30,781 : Text to Image: 6.8, 21.372, 33.028, 23.0
2019-02-16 18:06:13,992 : samples : 128000
2019-02-16 18:06:24,227 : Image to text: 7.74, 23.7, 35.82, 20.0
2019-02-16 18:06:31,684 : Text to Image: 6.728, 21.156, 32.24, 24.0
2019-02-16 18:07:14,857 : samples : 192000
2019-02-16 18:07:25,080 : Image to text: 7.86, 24.62, 36.28, 20.0
2019-02-16 18:07:31,959 : Text to Image: 6.532, 20.876, 32.156, 24.0
2019-02-16 18:08:17,151 : samples : 256000
2019-02-16 18:08:29,717 : Image to text: 7.7, 24.18, 35.42, 20.0
2019-02-16 18:08:39,618 : Text to Image: 6.664, 21.14, 32.34, 23.0
2019-02-16 18:09:24,605 : samples : 320000
2019-02-16 18:09:37,128 : Image to text: 7.68, 24.16, 34.78, 21.0
2019-02-16 18:09:47,052 : Text to Image: 6.788, 21.572, 32.732, 23.0
2019-02-16 18:10:31,380 : samples : 384000
2019-02-16 18:10:44,395 : Image to text: 7.34, 24.06, 36.38, 20.0
2019-02-16 18:10:54,768 : Text to Image: 7.068, 21.724, 32.832, 23.0
2019-02-16 18:11:42,616 : samples : 448000
2019-02-16 18:11:55,215 : Image to text: 7.78, 24.5, 36.02, 20.0
2019-02-16 18:12:05,229 : Text to Image: 6.612, 21.236, 32.732, 23.0
2019-02-16 18:12:50,473 : samples : 512000
2019-02-16 18:13:03,057 : Image to text: 8.32, 24.9, 36.62, 20.0
2019-02-16 18:13:13,084 : Text to Image: 6.98, 22.024, 33.296, 23.0
2019-02-16 18:13:50,961 : Epoch 7 finished
2019-02-16 18:13:51,870 : Image to text: 22.0, 54.0, 71.2, 5.0
2019-02-16 18:13:52,685 : Text to Image: 18.22, 51.36, 68.38, 5.0
2019-02-16 18:13:53,649 : Image to text: 20.3, 53.0, 69.2, 5.0
2019-02-16 18:13:54,423 : Text to Image: 19.24, 51.06, 68.62, 5.0
2019-02-16 18:13:55,453 : Image to text: 24.6, 55.0, 70.5, 4.0
2019-02-16 18:13:56,379 : Text to Image: 19.6, 51.4, 68.72, 5.0
2019-02-16 18:13:57,297 : Image to text: 20.2, 53.7, 71.4, 4.0
2019-02-16 18:13:58,050 : Text to Image: 18.58, 50.18, 67.84, 5.0
2019-02-16 18:13:58,922 : Image to text: 20.6, 53.9, 70.3, 5.0
2019-02-16 18:13:59,657 : Text to Image: 18.74, 50.04, 68.02, 5.0
2019-02-16 18:13:59,657 : Dev mean Text to Image: 18.875999999999998, 50.80800000000001, 68.316, 5.0
2019-02-16 18:13:59,657 : Dev mean Image to text: 21.540000000000003, 53.92, 70.52, 4.6
2019-02-16 18:13:59,658 : start epoch
2019-02-16 18:14:43,677 : samples : 64000
2019-02-16 18:14:56,312 : Image to text: 7.94, 24.82, 36.7, 19.0
2019-02-16 18:15:06,314 : Text to Image: 7.008, 21.988, 33.304, 23.0
2019-02-16 18:15:50,664 : samples : 128000
2019-02-16 18:16:03,272 : Image to text: 7.26, 23.92, 36.4, 20.0
2019-02-16 18:16:13,340 : Text to Image: 6.564, 20.984, 32.232, 23.0
2019-02-16 18:16:57,699 : samples : 192000
2019-02-16 18:17:10,275 : Image to text: 7.82, 24.22, 35.52, 20.0
2019-02-16 18:17:20,303 : Text to Image: 6.676, 21.088, 32.452, 24.0
2019-02-16 18:18:04,757 : samples : 256000
2019-02-16 18:18:17,419 : Image to text: 7.76, 24.14, 36.18, 19.0
2019-02-16 18:18:27,502 : Text to Image: 6.968, 21.836, 32.868, 23.0
2019-02-16 18:19:12,716 : samples : 320000
2019-02-16 18:19:23,000 : Image to text: 8.18, 24.8, 36.6, 19.0
2019-02-16 18:19:30,382 : Text to Image: 6.932, 21.776, 33.108, 22.0
2019-02-16 18:20:12,896 : samples : 384000
2019-02-16 18:20:23,170 : Image to text: 7.8, 23.98, 36.04, 20.0
2019-02-16 18:20:30,537 : Text to Image: 6.916, 21.508, 32.648, 23.0
2019-02-16 18:21:12,884 : samples : 448000
2019-02-16 18:21:22,892 : Image to text: 7.92, 24.56, 36.5, 20.0
2019-02-16 18:21:33,279 : Text to Image: 6.688, 21.236, 32.424, 24.0
2019-02-16 18:22:18,700 : samples : 512000
2019-02-16 18:22:31,519 : Image to text: 8.12, 24.46, 36.22, 19.0
2019-02-16 18:22:42,010 : Text to Image: 7.044, 21.924, 32.98, 23.0
2019-02-16 18:23:20,706 : Epoch 8 finished
2019-02-16 18:23:21,774 : Image to text: 23.3, 55.9, 72.4, 4.0
2019-02-16 18:23:22,629 : Text to Image: 19.52, 51.28, 69.18, 5.0
2019-02-16 18:23:23,697 : Image to text: 22.4, 53.0, 70.1, 5.0
2019-02-16 18:23:24,550 : Text to Image: 19.06, 50.86, 68.54, 5.0
2019-02-16 18:23:25,582 : Image to text: 25.3, 56.2, 69.0, 4.0
2019-02-16 18:23:26,517 : Text to Image: 19.84, 52.36, 69.5, 5.0
2019-02-16 18:23:27,595 : Image to text: 21.7, 55.7, 71.8, 5.0
2019-02-16 18:23:28,501 : Text to Image: 18.6, 49.64, 68.3, 6.0
2019-02-16 18:23:29,561 : Image to text: 23.7, 56.3, 71.9, 5.0
2019-02-16 18:23:30,480 : Text to Image: 19.78, 50.46, 67.4, 5.0
2019-02-16 18:23:30,480 : Dev mean Text to Image: 19.36, 50.92, 68.584, 5.2
2019-02-16 18:23:30,480 : Dev mean Image to text: 23.28, 55.42, 71.03999999999999, 4.6
2019-02-16 18:23:30,480 : start epoch
2019-02-16 18:24:16,959 : samples : 64000
2019-02-16 18:24:29,840 : Image to text: 7.68, 24.52, 36.26, 20.0
2019-02-16 18:24:40,247 : Text to Image: 6.736, 21.108, 32.324, 23.0
2019-02-16 18:25:26,108 : samples : 128000
2019-02-16 18:25:38,991 : Image to text: 7.82, 24.4, 36.24, 19.0
2019-02-16 18:25:49,398 : Text to Image: 6.588, 21.072, 32.08, 24.0
2019-02-16 18:26:35,616 : samples : 192000
2019-02-16 18:26:48,495 : Image to text: 7.04, 24.66, 36.66, 19.0
2019-02-16 18:26:58,924 : Text to Image: 7.22, 22.104, 33.412, 22.0
2019-02-16 18:27:49,789 : samples : 256000
2019-02-16 18:28:03,110 : Image to text: 8.0, 24.88, 37.12, 19.0
2019-02-16 18:28:13,537 : Text to Image: 6.772, 21.324, 32.744, 23.0
2019-02-16 18:28:59,145 : samples : 320000
2019-02-16 18:29:11,967 : Image to text: 7.4, 24.02, 35.64, 20.0
2019-02-16 18:29:22,433 : Text to Image: 6.612, 21.2, 32.184, 23.0
2019-02-16 18:30:05,837 : samples : 384000
2019-02-16 18:30:16,273 : Image to text: 8.12, 24.38, 36.36, 19.0
2019-02-16 18:30:23,966 : Text to Image: 7.08, 21.652, 32.832, 23.0
2019-02-16 18:31:06,639 : samples : 448000
2019-02-16 18:31:17,073 : Image to text: 7.96, 24.74, 36.44, 19.0
2019-02-16 18:31:24,783 : Text to Image: 6.692, 20.9, 32.156, 24.0
2019-02-16 18:32:07,690 : samples : 512000
2019-02-16 18:32:18,119 : Image to text: 8.32, 24.88, 37.06, 18.0
2019-02-16 18:32:25,736 : Text to Image: 6.816, 22.028, 33.356, 22.0
2019-02-16 18:33:02,426 : Epoch 9 finished
2019-02-16 18:33:02,868 : Image to text: 23.6, 53.0, 70.3, 5.0
2019-02-16 18:33:03,198 : Text to Image: 19.08, 51.52, 68.5, 5.0
2019-02-16 18:33:03,617 : Image to text: 22.6, 54.5, 69.9, 5.0
2019-02-16 18:33:03,950 : Text to Image: 19.3, 50.48, 67.7, 5.0
2019-02-16 18:33:04,389 : Image to text: 23.6, 56.1, 71.3, 4.0
2019-02-16 18:33:04,714 : Text to Image: 20.12, 51.76, 68.82, 5.0
2019-02-16 18:33:05,165 : Image to text: 21.9, 55.9, 72.2, 4.0
2019-02-16 18:33:05,504 : Text to Image: 19.16, 49.72, 67.46, 6.0
2019-02-16 18:33:05,944 : Image to text: 24.0, 55.7, 69.7, 5.0
2019-02-16 18:33:06,280 : Text to Image: 19.68, 50.96, 67.12, 5.0
2019-02-16 18:33:06,281 : Dev mean Text to Image: 19.468, 50.888, 67.92, 5.2
2019-02-16 18:33:06,281 : Dev mean Image to text: 23.140000000000004, 55.04, 70.67999999999999, 4.6
2019-02-16 18:33:06,281 : start epoch
2019-02-16 18:33:48,982 : samples : 64000
2019-02-16 18:33:59,470 : Image to text: 7.96, 24.64, 35.84, 20.0
2019-02-16 18:34:06,950 : Text to Image: 6.544, 21.188, 32.12, 24.0
2019-02-16 18:34:49,336 : samples : 128000
2019-02-16 18:34:59,841 : Image to text: 8.38, 25.9, 37.92, 19.0
2019-02-16 18:35:07,415 : Text to Image: 7.28, 22.352, 34.04, 22.0
2019-02-16 18:35:49,889 : samples : 192000
2019-02-16 18:36:00,396 : Image to text: 8.24, 25.54, 38.24, 18.0
2019-02-16 18:36:07,904 : Text to Image: 7.228, 22.372, 34.328, 22.0
2019-02-16 18:36:50,413 : samples : 256000
2019-02-16 18:37:00,878 : Image to text: 8.44, 24.1, 36.72, 19.0
2019-02-16 18:37:08,448 : Text to Image: 6.74, 21.772, 32.888, 23.0
2019-02-16 18:37:51,659 : samples : 320000
2019-02-16 18:38:02,163 : Image to text: 7.26, 24.06, 37.08, 19.0
2019-02-16 18:38:09,726 : Text to Image: 6.688, 21.268, 32.684, 23.0
2019-02-16 18:38:52,288 : samples : 384000
2019-02-16 18:39:02,683 : Image to text: 8.12, 24.8, 36.52, 19.0
2019-02-16 18:39:10,388 : Text to Image: 7.104, 22.296, 33.876, 22.0
2019-02-16 18:39:53,017 : samples : 448000
2019-02-16 18:40:03,418 : Image to text: 8.52, 25.48, 38.2, 18.0
2019-02-16 18:40:11,074 : Text to Image: 7.152, 22.564, 33.74, 22.0
2019-02-16 18:40:53,489 : samples : 512000
2019-02-16 18:41:03,939 : Image to text: 8.86, 25.62, 37.94, 18.0
2019-02-16 18:41:11,658 : Text to Image: 6.772, 21.772, 33.292, 23.0
2019-02-16 18:41:49,136 : Epoch 10 finished
2019-02-16 18:41:49,584 : Image to text: 24.3, 58.2, 72.8, 4.0
2019-02-16 18:41:49,927 : Text to Image: 18.74, 52.06, 69.82, 5.0
2019-02-16 18:41:50,371 : Image to text: 22.8, 54.4, 70.5, 5.0
2019-02-16 18:41:50,711 : Text to Image: 18.88, 50.88, 68.9, 5.0
2019-02-16 18:41:51,150 : Image to text: 24.8, 57.2, 72.5, 4.0
2019-02-16 18:41:51,495 : Text to Image: 20.52, 52.32, 68.44, 5.0
2019-02-16 18:41:51,961 : Image to text: 22.7, 56.6, 72.0, 4.0
2019-02-16 18:41:52,305 : Text to Image: 19.5, 49.8, 67.9, 6.0
2019-02-16 18:41:52,746 : Image to text: 23.3, 55.6, 70.2, 5.0
2019-02-16 18:41:53,077 : Text to Image: 19.26, 51.04, 68.02, 5.0
2019-02-16 18:41:53,077 : Dev mean Text to Image: 19.380000000000003, 51.22, 68.616, 5.2
2019-02-16 18:41:53,077 : Dev mean Image to text: 23.580000000000002, 56.400000000000006, 71.6, 4.4
2019-02-16 18:41:53,078 : start epoch
2019-02-16 18:42:35,415 : samples : 64000
2019-02-16 18:42:45,962 : Image to text: 7.8, 25.2, 36.92, 19.0
2019-02-16 18:42:53,512 : Text to Image: 6.952, 22.316, 33.732, 22.0
2019-02-16 18:43:36,151 : samples : 128000
2019-02-16 18:43:46,622 : Image to text: 7.52, 24.78, 36.88, 19.0
2019-02-16 18:43:54,181 : Text to Image: 6.696, 21.52, 33.2, 22.0
2019-02-16 18:44:43,075 : samples : 192000
2019-02-16 18:44:55,581 : Image to text: 8.4, 24.8, 36.54, 19.0
2019-02-16 18:45:03,042 : Text to Image: 6.7, 21.492, 32.952, 22.0
2019-02-16 18:45:45,836 : samples : 256000
2019-02-16 18:45:56,293 : Image to text: 8.56, 25.6, 37.62, 19.0
2019-02-16 18:46:03,867 : Text to Image: 7.352, 22.584, 34.092, 22.0
2019-02-16 18:46:45,892 : samples : 320000
2019-02-16 18:46:56,338 : Image to text: 8.7, 25.86, 38.6, 18.0
2019-02-16 18:47:03,897 : Text to Image: 7.116, 22.66, 34.144, 22.0
2019-02-16 18:47:46,540 : samples : 384000
2019-02-16 18:47:57,002 : Image to text: 8.76, 26.08, 38.64, 18.0
2019-02-16 18:48:04,805 : Text to Image: 6.972, 22.164, 33.784, 22.0
2019-02-16 18:48:47,428 : samples : 448000
2019-02-16 18:48:57,854 : Image to text: 8.14, 25.92, 37.98, 19.0
2019-02-16 18:49:05,582 : Text to Image: 6.996, 22.072, 33.708, 22.0
2019-02-16 18:49:48,628 : samples : 512000
2019-02-16 18:49:59,092 : Image to text: 8.36, 25.12, 37.58, 19.0
2019-02-16 18:50:06,835 : Text to Image: 7.108, 22.204, 33.716, 22.0
2019-02-16 18:50:43,339 : Epoch 11 finished
2019-02-16 18:50:43,768 : Image to text: 22.0, 54.7, 71.3, 5.0
2019-02-16 18:50:44,104 : Text to Image: 19.3, 51.64, 69.78, 5.0
2019-02-16 18:50:44,540 : Image to text: 22.6, 53.5, 67.7, 5.0
2019-02-16 18:50:44,870 : Text to Image: 18.58, 50.86, 69.1, 5.0
2019-02-16 18:50:45,309 : Image to text: 24.4, 57.4, 71.8, 4.0
2019-02-16 18:50:45,648 : Text to Image: 20.4, 52.02, 69.1, 5.0
2019-02-16 18:50:46,087 : Image to text: 22.3, 55.6, 72.1, 4.0
2019-02-16 18:50:46,427 : Text to Image: 19.08, 50.56, 67.88, 5.0
2019-02-16 18:50:46,882 : Image to text: 23.9, 54.1, 69.2, 5.0
2019-02-16 18:50:47,220 : Text to Image: 19.86, 51.5, 68.16, 5.0
2019-02-16 18:50:47,220 : Dev mean Text to Image: 19.444000000000003, 51.316, 68.804, 5.0
2019-02-16 18:50:47,221 : Dev mean Image to text: 23.04, 55.06000000000001, 70.42, 4.6
2019-02-16 18:50:47,221 : start epoch
2019-02-16 18:51:29,812 : samples : 64000
2019-02-16 18:51:40,283 : Image to text: 8.44, 26.14, 38.0, 18.0
2019-02-16 18:51:47,821 : Text to Image: 7.152, 22.46, 34.072, 22.0
2019-02-16 18:52:30,607 : samples : 128000
2019-02-16 18:52:41,136 : Image to text: 8.26, 25.94, 38.1, 18.0
2019-02-16 18:52:48,677 : Text to Image: 7.096, 22.392, 34.172, 22.0
2019-02-16 18:53:31,974 : samples : 192000
2019-02-16 18:53:42,462 : Image to text: 8.5, 25.72, 37.28, 19.0
2019-02-16 18:53:50,062 : Text to Image: 6.74, 21.792, 33.424, 22.0
2019-02-16 18:54:32,138 : samples : 256000
2019-02-16 18:54:42,657 : Image to text: 8.8, 25.86, 38.02, 18.0
2019-02-16 18:54:50,222 : Text to Image: 7.284, 22.668, 34.516, 21.0
2019-02-16 18:55:33,090 : samples : 320000
2019-02-16 18:55:43,553 : Image to text: 8.48, 25.76, 37.98, 18.0
2019-02-16 18:55:51,141 : Text to Image: 7.024, 21.828, 33.236, 22.0
2019-02-16 18:56:33,681 : samples : 384000
2019-02-16 18:56:44,127 : Image to text: 9.08, 26.44, 38.84, 18.0
2019-02-16 18:56:51,839 : Text to Image: 7.048, 22.84, 34.344, 22.0
2019-02-16 18:57:34,939 : samples : 448000
2019-02-16 18:57:45,312 : Image to text: 9.0, 26.86, 38.2, 18.0
2019-02-16 18:57:53,030 : Text to Image: 7.24, 22.564, 34.688, 21.0
2019-02-16 18:58:35,547 : samples : 512000
2019-02-16 18:58:46,009 : Image to text: 8.3, 24.76, 36.96, 19.0
2019-02-16 18:58:53,727 : Text to Image: 7.064, 21.848, 33.516, 22.0
2019-02-16 18:59:30,813 : Epoch 12 finished
2019-02-16 18:59:31,251 : Image to text: 23.3, 56.0, 71.6, 4.0
2019-02-16 18:59:31,596 : Text to Image: 18.74, 51.5, 69.12, 5.0
2019-02-16 18:59:32,028 : Image to text: 23.2, 53.3, 70.0, 5.0
2019-02-16 18:59:32,371 : Text to Image: 19.08, 51.54, 68.48, 5.0
2019-02-16 18:59:32,838 : Image to text: 25.0, 56.9, 73.8, 4.0
2019-02-16 18:59:33,197 : Text to Image: 20.08, 52.6, 69.58, 5.0
2019-02-16 18:59:33,646 : Image to text: 22.2, 56.2, 71.9, 4.0
2019-02-16 18:59:33,983 : Text to Image: 19.08, 51.4, 68.74, 5.0
2019-02-16 18:59:34,435 : Image to text: 23.3, 55.8, 72.2, 4.0
2019-02-16 18:59:34,765 : Text to Image: 19.36, 51.72, 68.56, 5.0
2019-02-16 18:59:34,765 : Dev mean Text to Image: 19.268, 51.752, 68.896, 5.0
2019-02-16 18:59:34,765 : Dev mean Image to text: 23.400000000000002, 55.64, 71.9, 4.2
2019-02-16 18:59:34,766 : start epoch
2019-02-16 19:00:17,536 : samples : 64000
2019-02-16 19:00:28,063 : Image to text: 8.18, 25.56, 38.14, 18.0
2019-02-16 19:00:35,610 : Text to Image: 7.292, 22.536, 34.068, 21.0
2019-02-16 19:01:21,620 : samples : 128000
2019-02-16 19:01:33,860 : Image to text: 8.14, 25.78, 36.9, 19.0
2019-02-16 19:01:42,588 : Text to Image: 6.816, 22.168, 33.836, 21.0
2019-02-16 19:02:26,226 : samples : 192000
2019-02-16 19:02:36,761 : Image to text: 8.88, 25.52, 37.68, 18.0
2019-02-16 19:02:44,330 : Text to Image: 6.996, 22.416, 33.724, 22.0
2019-02-16 19:03:27,206 : samples : 256000
2019-02-16 19:03:37,696 : Image to text: 8.64, 26.3, 38.66, 18.0
2019-02-16 19:03:45,268 : Text to Image: 7.252, 22.484, 34.112, 22.0
2019-02-16 19:04:28,388 : samples : 320000
2019-02-16 19:04:38,896 : Image to text: 8.66, 25.26, 37.24, 19.0
2019-02-16 19:04:46,389 : Text to Image: 7.176, 22.664, 34.404, 21.0
2019-02-16 19:05:29,199 : samples : 384000
2019-02-16 19:05:39,697 : Image to text: 8.04, 25.16, 37.64, 18.0
2019-02-16 19:05:47,393 : Text to Image: 7.036, 22.192, 34.08, 22.0
2019-02-16 19:06:30,702 : samples : 448000
2019-02-16 19:06:41,197 : Image to text: 9.02, 26.6, 38.04, 18.0
2019-02-16 19:06:48,917 : Text to Image: 7.3, 22.656, 34.508, 21.0
2019-02-16 19:07:31,623 : samples : 512000
2019-02-16 19:07:42,126 : Image to text: 8.58, 25.72, 38.48, 18.0
2019-02-16 19:07:49,853 : Text to Image: 7.172, 22.444, 34.088, 22.0
2019-02-16 19:08:26,192 : Epoch 13 finished
2019-02-16 19:08:26,636 : Image to text: 22.2, 56.5, 72.6, 4.0
2019-02-16 19:08:26,975 : Text to Image: 20.14, 52.86, 70.14, 5.0
2019-02-16 19:08:27,438 : Image to text: 23.8, 55.1, 69.9, 4.0
2019-02-16 19:08:27,780 : Text to Image: 20.32, 52.36, 69.56, 5.0
2019-02-16 19:08:28,228 : Image to text: 23.1, 56.6, 72.9, 4.0
2019-02-16 19:08:28,565 : Text to Image: 21.28, 53.86, 70.06, 5.0
2019-02-16 19:08:29,022 : Image to text: 21.8, 55.8, 72.6, 4.0
2019-02-16 19:08:29,362 : Text to Image: 20.02, 51.94, 69.28, 5.0
2019-02-16 19:08:29,805 : Image to text: 23.7, 57.9, 71.3, 4.0
2019-02-16 19:08:30,138 : Text to Image: 20.36, 52.48, 68.92, 5.0
2019-02-16 19:08:30,138 : Dev mean Text to Image: 20.424, 52.69999999999999, 69.592, 5.0
2019-02-16 19:08:30,138 : Dev mean Image to text: 22.92, 56.379999999999995, 71.86, 4.0
2019-02-16 19:08:30,138 : start epoch
2019-02-16 19:09:12,862 : samples : 64000
2019-02-16 19:09:23,367 : Image to text: 8.62, 25.7, 38.06, 18.0
2019-02-16 19:09:30,914 : Text to Image: 7.32, 22.608, 34.284, 22.0
2019-02-16 19:10:13,587 : samples : 128000
2019-02-16 19:10:24,145 : Image to text: 8.66, 26.26, 38.8, 18.0
2019-02-16 19:10:31,690 : Text to Image: 7.228, 22.584, 34.324, 21.0
2019-02-16 19:11:14,211 : samples : 192000
2019-02-16 19:11:24,706 : Image to text: 9.12, 26.5, 38.28, 18.0
2019-02-16 19:11:32,257 : Text to Image: 7.204, 22.452, 34.088, 21.0
2019-02-16 19:12:14,648 : samples : 256000
2019-02-16 19:12:25,127 : Image to text: 8.16, 25.84, 38.18, 18.0
2019-02-16 19:12:32,665 : Text to Image: 7.004, 22.692, 34.356, 21.0
2019-02-16 19:13:14,610 : samples : 320000
2019-02-16 19:13:25,184 : Image to text: 8.58, 25.44, 37.78, 19.0
2019-02-16 19:13:32,722 : Text to Image: 7.204, 22.716, 34.508, 21.0
2019-02-16 19:14:15,057 : samples : 384000
2019-02-16 19:14:25,536 : Image to text: 8.76, 25.4, 37.78, 18.0
2019-02-16 19:14:33,264 : Text to Image: 7.276, 22.6, 34.312, 21.0
2019-02-16 19:15:16,497 : samples : 448000
2019-02-16 19:15:26,989 : Image to text: 8.58, 26.0, 38.0, 18.0
2019-02-16 19:15:34,650 : Text to Image: 7.208, 22.624, 34.512, 21.0
2019-02-16 19:16:17,236 : samples : 512000
2019-02-16 19:16:27,713 : Image to text: 8.72, 26.54, 38.66, 17.0
2019-02-16 19:16:35,393 : Text to Image: 7.7, 23.272, 35.0, 21.0
2019-02-16 19:17:12,368 : Epoch 14 finished
2019-02-16 19:17:12,815 : Image to text: 24.8, 55.9, 71.6, 4.0
2019-02-16 19:17:13,152 : Text to Image: 19.32, 51.92, 70.12, 5.0
2019-02-16 19:17:13,610 : Image to text: 22.0, 55.9, 69.9, 4.0
2019-02-16 19:17:13,936 : Text to Image: 20.14, 52.2, 69.52, 5.0
2019-02-16 19:17:14,386 : Image to text: 25.3, 57.4, 73.3, 4.0
2019-02-16 19:17:14,724 : Text to Image: 20.42, 53.56, 70.46, 5.0
2019-02-16 19:17:15,162 : Image to text: 22.7, 57.1, 71.4, 4.0
2019-02-16 19:17:15,505 : Text to Image: 19.76, 51.6, 69.5, 5.0
2019-02-16 19:17:15,955 : Image to text: 23.6, 58.2, 71.7, 4.0
2019-02-16 19:17:16,300 : Text to Image: 20.28, 52.04, 69.4, 5.0
2019-02-16 19:17:16,300 : Dev mean Text to Image: 19.984, 52.264, 69.8, 5.0
2019-02-16 19:17:16,300 : Dev mean Image to text: 23.68, 56.900000000000006, 71.58, 4.0
2019-02-16 19:17:16,301 : start epoch
2019-02-16 19:17:59,740 : samples : 64000
2019-02-16 19:18:12,674 : Image to text: 8.58, 26.42, 38.9, 17.0
2019-02-16 19:18:21,778 : Text to Image: 7.12, 22.404, 34.064, 22.0
2019-02-16 19:19:07,028 : samples : 128000
2019-02-16 19:19:17,506 : Image to text: 8.72, 25.96, 38.16, 18.0
2019-02-16 19:19:25,080 : Text to Image: 7.264, 22.628, 34.424, 21.0
2019-02-16 19:20:07,784 : samples : 192000
2019-02-16 19:20:18,334 : Image to text: 8.88, 26.6, 38.42, 18.0
2019-02-16 19:20:25,900 : Text to Image: 7.12, 22.996, 34.612, 21.0
2019-02-16 19:21:08,103 : samples : 256000
2019-02-16 19:21:18,623 : Image to text: 9.16, 26.38, 38.8, 18.0
2019-02-16 19:21:26,114 : Text to Image: 7.316, 23.1, 35.136, 21.0
2019-02-16 19:22:08,846 : samples : 320000
2019-02-16 19:22:19,343 : Image to text: 9.16, 25.48, 37.02, 19.0
2019-02-16 19:22:26,873 : Text to Image: 7.7, 23.32, 34.844, 21.0
2019-02-16 19:23:09,766 : samples : 384000
2019-02-16 19:23:20,255 : Image to text: 8.94, 26.44, 38.54, 18.0
2019-02-16 19:23:27,891 : Text to Image: 7.324, 22.776, 34.492, 21.0
2019-02-16 19:24:11,057 : samples : 448000
2019-02-16 19:24:21,545 : Image to text: 8.8, 26.24, 38.34, 18.0
2019-02-16 19:24:29,184 : Text to Image: 7.092, 22.536, 34.288, 21.0
2019-02-16 19:25:12,000 : samples : 512000
2019-02-16 19:25:22,511 : Image to text: 8.64, 26.08, 38.94, 18.0
2019-02-16 19:25:30,170 : Text to Image: 7.324, 22.956, 34.768, 21.0
2019-02-16 19:26:06,631 : Epoch 15 finished
2019-02-16 19:26:07,087 : Image to text: 24.7, 57.2, 73.3, 4.0
2019-02-16 19:26:07,415 : Text to Image: 19.82, 53.56, 70.6, 5.0
2019-02-16 19:26:07,842 : Image to text: 23.9, 57.0, 71.8, 4.0
2019-02-16 19:26:08,170 : Text to Image: 20.24, 52.94, 70.26, 5.0
2019-02-16 19:26:08,621 : Image to text: 26.1, 57.1, 72.3, 4.0
2019-02-16 19:26:08,956 : Text to Image: 21.24, 53.42, 70.9, 5.0
2019-02-16 19:26:09,402 : Image to text: 22.4, 57.7, 73.7, 4.0
2019-02-16 19:26:09,745 : Text to Image: 20.62, 52.16, 69.88, 5.0
2019-02-16 19:26:10,198 : Image to text: 23.3, 57.7, 73.6, 4.0
2019-02-16 19:26:10,539 : Text to Image: 21.12, 53.08, 69.56, 5.0
2019-02-16 19:26:10,539 : Dev mean Text to Image: 20.608, 53.032, 70.24000000000001, 5.0
2019-02-16 19:26:10,539 : Dev mean Image to text: 24.08, 57.34, 72.94, 4.0
2019-02-16 19:26:10,539 : start epoch
2019-02-16 19:26:52,954 : samples : 64000
2019-02-16 19:27:03,461 : Image to text: 8.38, 25.56, 37.78, 18.0
2019-02-16 19:27:10,990 : Text to Image: 7.072, 22.668, 34.484, 21.0
2019-02-16 19:27:53,346 : samples : 128000
2019-02-16 19:28:03,846 : Image to text: 8.44, 25.54, 38.14, 18.0
2019-02-16 19:28:11,392 : Text to Image: 7.144, 22.836, 34.464, 21.0
2019-02-16 19:28:53,837 : samples : 192000
2019-02-16 19:29:04,383 : Image to text: 9.08, 27.66, 39.8, 17.0
2019-02-16 19:29:11,917 : Text to Image: 7.78, 23.36, 35.38, 21.0
2019-02-16 19:29:55,063 : samples : 256000
2019-02-16 19:30:05,550 : Image to text: 8.66, 26.68, 38.8, 17.0
2019-02-16 19:30:13,071 : Text to Image: 7.616, 23.172, 35.188, 20.0
2019-02-16 19:30:55,827 : samples : 320000
2019-02-16 19:31:06,424 : Image to text: 9.28, 26.92, 39.34, 17.0
2019-02-16 19:31:13,952 : Text to Image: 7.352, 23.14, 34.864, 21.0
2019-02-16 19:31:56,483 : samples : 384000
2019-02-16 19:32:06,953 : Image to text: 8.5, 26.66, 39.06, 18.0
2019-02-16 19:32:14,689 : Text to Image: 7.5, 22.776, 34.416, 21.0
2019-02-16 19:32:57,293 : samples : 448000
2019-02-16 19:33:07,781 : Image to text: 8.24, 25.36, 37.22, 18.0
2019-02-16 19:33:15,491 : Text to Image: 7.48, 22.904, 34.832, 21.0
2019-02-16 19:33:58,349 : samples : 512000
2019-02-16 19:34:08,920 : Image to text: 8.98, 26.32, 38.4, 18.0
2019-02-16 19:34:16,619 : Text to Image: 7.572, 23.108, 34.972, 21.0
2019-02-16 19:34:54,427 : Epoch 16 finished
2019-02-16 19:34:54,995 : Image to text: 23.8, 54.7, 71.4, 5.0
2019-02-16 19:34:55,421 : Text to Image: 19.96, 52.72, 70.58, 5.0
2019-02-16 19:34:55,992 : Image to text: 22.8, 55.3, 73.2, 4.0
2019-02-16 19:34:56,430 : Text to Image: 20.26, 52.78, 70.14, 5.0
2019-02-16 19:34:56,997 : Image to text: 25.6, 57.0, 74.2, 4.0
2019-02-16 19:34:57,421 : Text to Image: 21.94, 53.54, 70.68, 5.0
2019-02-16 19:34:57,964 : Image to text: 22.7, 56.1, 73.0, 4.0
2019-02-16 19:34:58,379 : Text to Image: 19.5, 51.4, 69.72, 5.0
2019-02-16 19:34:58,905 : Image to text: 23.2, 56.5, 71.6, 4.0
2019-02-16 19:34:59,307 : Text to Image: 20.42, 52.24, 69.24, 5.0
2019-02-16 19:34:59,308 : Dev mean Text to Image: 20.416, 52.536, 70.072, 5.0
2019-02-16 19:34:59,308 : Dev mean Image to text: 23.62, 55.92, 72.68, 4.2
2019-02-16 19:34:59,308 : start epoch
2019-02-16 19:35:49,203 : samples : 64000
2019-02-16 19:35:59,719 : Image to text: 9.32, 26.64, 38.78, 18.0
2019-02-16 19:36:07,288 : Text to Image: 7.612, 23.328, 34.828, 21.0
2019-02-16 19:36:49,703 : samples : 128000
2019-02-16 19:37:00,168 : Image to text: 9.28, 27.28, 39.5, 17.0
2019-02-16 19:37:07,737 : Text to Image: 7.548, 23.652, 35.312, 21.0
2019-02-16 19:37:50,919 : samples : 192000
2019-02-16 19:38:01,477 : Image to text: 9.06, 26.36, 38.26, 17.0
2019-02-16 19:38:09,020 : Text to Image: 7.6, 23.156, 34.748, 21.0
2019-02-16 19:38:51,762 : samples : 256000
2019-02-16 19:39:02,261 : Image to text: 8.86, 26.22, 38.52, 18.0
2019-02-16 19:39:09,779 : Text to Image: 7.56, 23.212, 35.024, 21.0
2019-02-16 19:39:52,740 : samples : 320000
2019-02-16 19:40:03,231 : Image to text: 8.86, 26.06, 38.76, 18.0
2019-02-16 19:40:10,827 : Text to Image: 7.304, 22.772, 34.44, 21.0
2019-02-16 19:40:53,114 : samples : 384000
2019-02-16 19:41:03,593 : Image to text: 8.24, 25.9, 38.5, 18.0
2019-02-16 19:41:11,325 : Text to Image: 7.192, 22.776, 34.596, 21.0
2019-02-16 19:41:54,950 : samples : 448000
2019-02-16 19:42:05,458 : Image to text: 8.8, 26.78, 38.08, 18.0
2019-02-16 19:42:13,177 : Text to Image: 7.412, 23.08, 34.712, 21.0
2019-02-16 19:42:56,303 : samples : 512000
2019-02-16 19:43:06,744 : Image to text: 8.7, 26.28, 38.28, 18.0
2019-02-16 19:43:14,520 : Text to Image: 7.352, 23.084, 34.764, 21.0
2019-02-16 19:43:51,262 : Epoch 17 finished
2019-02-16 19:43:51,711 : Image to text: 24.4, 57.4, 73.7, 4.0
2019-02-16 19:43:52,049 : Text to Image: 19.88, 53.0, 70.9, 5.0
2019-02-16 19:43:52,496 : Image to text: 22.6, 56.0, 72.9, 4.0
2019-02-16 19:43:52,828 : Text to Image: 19.26, 52.8, 69.36, 5.0
2019-02-16 19:43:53,280 : Image to text: 24.8, 58.4, 74.1, 4.0
2019-02-16 19:43:53,629 : Text to Image: 21.52, 53.8, 70.7, 5.0
2019-02-16 19:43:54,085 : Image to text: 22.9, 58.2, 73.5, 4.0
2019-02-16 19:43:54,432 : Text to Image: 19.6, 52.6, 69.1, 5.0
2019-02-16 19:43:54,868 : Image to text: 24.5, 56.0, 71.6, 4.0
2019-02-16 19:43:55,200 : Text to Image: 20.42, 52.76, 69.16, 5.0
2019-02-16 19:43:55,200 : Dev mean Text to Image: 20.136000000000003, 52.992, 69.844, 5.0
2019-02-16 19:43:55,200 : Dev mean Image to text: 23.839999999999996, 57.2, 73.16, 4.0
2019-02-16 19:43:59,118 : 
Test scores | Image to text:             24.68, 57.7, 73.82, 4.0
2019-02-16 19:43:59,119 : Test scores | Text to image:             20.492, 52.74399999999999, 70.232, 5.0

2019-02-16 19:43:59,225 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 19:43:59,454 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 19:44:00,149 : loading BERT model bert-base-uncased
2019-02-16 19:44:00,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:44:00,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:44:00,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiugdqbpv
2019-02-16 19:44:02,725 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:44:04,221 : Computing embeddings for train/dev/test
2019-02-16 19:45:40,238 : Computed embeddings
2019-02-16 19:45:40,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:46:26,644 : [('reg:1e-05', 59.82), ('reg:0.0001', 59.45), ('reg:0.001', 57.62), ('reg:0.01', 48.55)]
2019-02-16 19:46:26,644 : Validation : best param found is reg = 1e-05 with score             59.82
2019-02-16 19:46:26,644 : Evaluating...
2019-02-16 19:46:36,198 : 
Dev acc : 59.8 Test acc : 61.1 for LENGTH classification

2019-02-16 19:46:36,199 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 19:46:36,532 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 19:46:36,581 : loading BERT model bert-base-uncased
2019-02-16 19:46:36,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:46:36,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:46:36,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbc9gsh8z
2019-02-16 19:46:39,198 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:46:40,654 : Computing embeddings for train/dev/test
2019-02-16 19:48:09,121 : Computed embeddings
2019-02-16 19:48:09,122 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:48:58,351 : [('reg:1e-05', 21.26), ('reg:0.0001', 5.69), ('reg:0.001', 1.1), ('reg:0.01', 0.51)]
2019-02-16 19:48:58,351 : Validation : best param found is reg = 1e-05 with score             21.26
2019-02-16 19:48:58,351 : Evaluating...
2019-02-16 19:49:12,154 : 
Dev acc : 21.3 Test acc : 21.1 for WORDCONTENT classification

2019-02-16 19:49:12,156 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 19:49:12,545 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 19:49:12,622 : loading BERT model bert-base-uncased
2019-02-16 19:49:12,622 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:49:12,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:49:12,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5agbllsu
2019-02-16 19:49:15,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:49:16,737 : Computing embeddings for train/dev/test
2019-02-16 19:50:40,156 : Computed embeddings
2019-02-16 19:50:40,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:51:21,888 : [('reg:1e-05', 29.37), ('reg:0.0001', 27.49), ('reg:0.001', 25.64), ('reg:0.01', 22.61)]
2019-02-16 19:51:21,888 : Validation : best param found is reg = 1e-05 with score             29.37
2019-02-16 19:51:21,888 : Evaluating...
2019-02-16 19:51:35,184 : 
Dev acc : 29.4 Test acc : 29.2 for DEPTH classification

2019-02-16 19:51:35,185 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 19:51:35,758 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 19:51:35,823 : loading BERT model bert-base-uncased
2019-02-16 19:51:35,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:51:35,851 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:51:35,851 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsckbnzlo
2019-02-16 19:51:38,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:51:39,731 : Computing embeddings for train/dev/test
2019-02-16 19:53:01,767 : Computed embeddings
2019-02-16 19:53:01,767 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:53:50,095 : [('reg:1e-05', 72.46), ('reg:0.0001', 70.48), ('reg:0.001', 69.08), ('reg:0.01', 59.62)]
2019-02-16 19:53:50,095 : Validation : best param found is reg = 1e-05 with score             72.46
2019-02-16 19:53:50,096 : Evaluating...
2019-02-16 19:54:01,400 : 
Dev acc : 72.5 Test acc : 72.6 for TOPCONSTITUENTS classification

2019-02-16 19:54:01,401 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 19:54:01,990 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 19:54:02,065 : loading BERT model bert-base-uncased
2019-02-16 19:54:02,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:54:02,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:54:02,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph0mmn489
2019-02-16 19:54:04,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:54:06,112 : Computing embeddings for train/dev/test
2019-02-16 19:55:30,204 : Computed embeddings
2019-02-16 19:55:30,204 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:56:23,687 : [('reg:1e-05', 85.38), ('reg:0.0001', 85.29), ('reg:0.001', 84.97), ('reg:0.01', 83.44)]
2019-02-16 19:56:23,687 : Validation : best param found is reg = 1e-05 with score             85.38
2019-02-16 19:56:23,687 : Evaluating...
2019-02-16 19:56:32,879 : 
Dev acc : 85.4 Test acc : 84.4 for BIGRAMSHIFT classification

2019-02-16 19:56:32,881 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 19:56:33,336 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 19:56:33,409 : loading BERT model bert-base-uncased
2019-02-16 19:56:33,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:56:33,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:56:33,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi80p8vfg
2019-02-16 19:56:36,009 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:56:37,523 : Computing embeddings for train/dev/test
2019-02-16 19:58:00,029 : Computed embeddings
2019-02-16 19:58:00,030 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 19:58:38,572 : [('reg:1e-05', 89.58), ('reg:0.0001', 89.59), ('reg:0.001', 89.9), ('reg:0.01', 89.9)]
2019-02-16 19:58:38,573 : Validation : best param found is reg = 0.001 with score             89.9
2019-02-16 19:58:38,573 : Evaluating...
2019-02-16 19:58:48,636 : 
Dev acc : 89.9 Test acc : 89.2 for TENSE classification

2019-02-16 19:58:48,637 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 19:58:49,097 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 19:58:49,171 : loading BERT model bert-base-uncased
2019-02-16 19:58:49,171 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 19:58:49,208 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 19:58:49,208 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf93w82ks
2019-02-16 19:58:51,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 19:58:53,202 : Computing embeddings for train/dev/test
2019-02-16 20:00:20,786 : Computed embeddings
2019-02-16 20:00:20,787 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:01:06,625 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.14), ('reg:0.001', 86.38), ('reg:0.01', 83.2)]
2019-02-16 20:01:06,625 : Validation : best param found is reg = 0.001 with score             86.38
2019-02-16 20:01:06,625 : Evaluating...
2019-02-16 20:01:16,186 : 
Dev acc : 86.4 Test acc : 85.8 for SUBJNUMBER classification

2019-02-16 20:01:16,187 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 20:01:16,666 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 20:01:16,740 : loading BERT model bert-base-uncased
2019-02-16 20:01:16,740 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:01:16,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:01:16,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5bpvuv6l
2019-02-16 20:01:19,365 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:01:20,831 : Computing embeddings for train/dev/test
2019-02-16 20:02:46,065 : Computed embeddings
2019-02-16 20:02:46,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:03:29,441 : [('reg:1e-05', 74.55), ('reg:0.0001', 76.08), ('reg:0.001', 75.86), ('reg:0.01', 71.12)]
2019-02-16 20:03:29,441 : Validation : best param found is reg = 0.0001 with score             76.08
2019-02-16 20:03:29,441 : Evaluating...
2019-02-16 20:03:41,454 : 
Dev acc : 76.1 Test acc : 77.3 for OBJNUMBER classification

2019-02-16 20:03:41,455 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 20:03:41,872 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 20:03:41,952 : loading BERT model bert-base-uncased
2019-02-16 20:03:41,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:03:42,105 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:03:42,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ogree8o
2019-02-16 20:03:44,627 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:03:46,105 : Computing embeddings for train/dev/test
2019-02-16 20:05:25,366 : Computed embeddings
2019-02-16 20:05:25,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:06:08,598 : [('reg:1e-05', 58.86), ('reg:0.0001', 58.76), ('reg:0.001', 57.85), ('reg:0.01', 57.38)]
2019-02-16 20:06:08,598 : Validation : best param found is reg = 1e-05 with score             58.86
2019-02-16 20:06:08,598 : Evaluating...
2019-02-16 20:06:20,560 : 
Dev acc : 58.9 Test acc : 58.6 for ODDMANOUT classification

2019-02-16 20:06:20,561 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 20:06:21,198 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 20:06:21,278 : loading BERT model bert-base-uncased
2019-02-16 20:06:21,279 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:06:21,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:06:21,313 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptl7wj0e6
2019-02-16 20:06:23,805 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:06:25,286 : Computing embeddings for train/dev/test
2019-02-16 20:08:01,889 : Computed embeddings
2019-02-16 20:08:01,889 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:08:45,333 : [('reg:1e-05', 71.69), ('reg:0.0001', 71.62), ('reg:0.001', 70.99), ('reg:0.01', 67.67)]
2019-02-16 20:08:45,333 : Validation : best param found is reg = 1e-05 with score             71.69
2019-02-16 20:08:45,333 : Evaluating...
2019-02-16 20:08:52,909 : 
Dev acc : 71.7 Test acc : 70.5 for COORDINATIONINVERSION classification

2019-02-16 20:08:52,911 : total results: {'STS12': {'MSRpar': {'pearson': (0.23732150353416157, 4.618414291200209e-11), 'spearman': SpearmanrResult(correlation=0.2769972218674011, pvalue=1.1195646451082229e-14), 'nsamples': 750}, 'MSRvid': {'pearson': (0.0011992695963712332, 0.9738431840627108), 'spearman': SpearmanrResult(correlation=0.05382554179196728, pvalue=0.1408367546110975), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.40651989750203854, 1.0780762905422355e-19), 'spearman': SpearmanrResult(correlation=0.4949458700028933, pvalue=9.74884781211349e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3251174712003722, 6.346214206110203e-20), 'spearman': SpearmanrResult(correlation=0.3274539569459294, pvalue=3.329015495702171e-20), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.550842359321709, 4.871017156990426e-33), 'spearman': SpearmanrResult(correlation=0.4713873457009087, pvalue=1.827177357304411e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.30420010023093047, 'wmean': 0.26676557836903997}, 'spearman': {'mean': 0.32492198726182, 'wmean': 0.29246179077218915}}}, 'STS13': {'FNWN': {'pearson': (0.14794580970723736, 0.04219219761728595), 'spearman': SpearmanrResult(correlation=0.1613749261246404, pvalue=0.02652885637573362), 'nsamples': 189}, 'headlines': {'pearson': (0.38752522619478624, 2.7695408757452145e-28), 'spearman': SpearmanrResult(correlation=0.3758632003441952, pvalue=1.4065319656358656e-26), 'nsamples': 750}, 'OnWN': {'pearson': (0.08375267396959814, 0.047392127360468), 'spearman': SpearmanrResult(correlation=0.0755315118871364, pvalue=0.07384688465601386), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2064079032905406, 'wmean': 0.2437272851851347}, 'spearman': {'mean': 0.20425654611865732, 'wmean': 0.23651362630959127}}}, 'STS14': {'deft-forum': {'pearson': (-0.11902251841562451, 0.01151023402159769), 'spearman': SpearmanrResult(correlation=-0.10978313840546783, pvalue=0.01983684707464685), 'nsamples': 450}, 'deft-news': {'pearson': (0.3669793271229437, 5.379843687864704e-11), 'spearman': SpearmanrResult(correlation=0.4102703646460207, pvalue=1.311777822707954e-13), 'nsamples': 300}, 'headlines': {'pearson': (0.3809099840849702, 2.6208592996949945e-27), 'spearman': SpearmanrResult(correlation=0.35595298187419916, pvalue=8.020699661417406e-24), 'nsamples': 750}, 'images': {'pearson': (0.1516413455823836, 3.0449158876302396e-05), 'spearman': SpearmanrResult(correlation=0.1600627945056886, pvalue=1.0597077923749478e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.290570470674278, 4.6553631706246e-16), 'spearman': SpearmanrResult(correlation=0.28748754978136143, pvalue=9.734073760387232e-16), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4697584655268745, 1.9684343874804547e-42), 'spearman': SpearmanrResult(correlation=0.4242889700554511, pvalue=3.951653521045761e-34), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.25680617909597087, 'wmean': 0.27365169713366183}, 'spearman': {'mean': 0.2547132537428755, 'wmean': 0.26520611180636555}}}, 'STS15': {'answers-forums': {'pearson': (0.2697440348009397, 1.128075770017156e-07), 'spearman': SpearmanrResult(correlation=0.27851904240016523, pvalue=4.149029714873457e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.3904189193038716, 1.019539838783138e-28), 'spearman': SpearmanrResult(correlation=0.3970746153317555, pvalue=9.854562840586162e-30), 'nsamples': 750}, 'belief': {'pearson': (0.40403726181142596, 3.6991747569357865e-16), 'spearman': SpearmanrResult(correlation=0.42956337761334984, pvalue=2.8569840882849715e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.4142802971294049, 1.8241888613465465e-32), 'spearman': SpearmanrResult(correlation=0.4184726547258298, pvalue=3.721909085328712e-33), 'nsamples': 750}, 'images': {'pearson': (0.1692525856167979, 3.145088753069313e-06), 'spearman': SpearmanrResult(correlation=0.18577469711184116, pvalue=2.993896667279881e-07), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.329546619732488, 'wmean': 0.32771061258906437}, 'spearman': {'mean': 0.3418808774365883, 'wmean': 0.33884079429404595}}}, 'STS16': {'answer-answer': {'pearson': (0.3270564242805792, 9.600512187421546e-08), 'spearman': SpearmanrResult(correlation=0.336043174393608, pvalue=4.026105750754545e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5373200251072341, 4.992045394929496e-20), 'spearman': SpearmanrResult(correlation=0.562003829527071, pvalue=3.917736770588799e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5936044050350544, 2.7362731945726126e-23), 'spearman': SpearmanrResult(correlation=0.617585068928873, pvalue=1.4080352052918652e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.7077820764781809, 2.1517119515754857e-38), 'spearman': SpearmanrResult(correlation=0.7427221993123767, pvalue=4.837380175938314e-44), 'nsamples': 244}, 'question-question': {'pearson': (-0.03977361688840963, 0.5674721612690782), 'spearman': SpearmanrResult(correlation=0.00918238364820357, pvalue=0.8950200279578715), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.4251978628025278, 'wmean': 0.43657687344690504}, 'spearman': {'mean': 0.45350733116202635, 'wmean': 0.464149932981916}}}, 'MR': {'devacc': 76.65, 'acc': 77.88, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.37, 'acc': 81.77, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.06, 'acc': 87.76, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.83, 'acc': 94.58, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.0, 'acc': 80.83, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.69, 'acc': 41.67, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.5, 'acc': 86.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.22, 'acc': 71.07, 'f1': 81.12, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.8, 'acc': 71.75, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7166140888139473, 'pearson': 0.6943244008528043, 'spearman': 0.6384538811677655, 'mse': 0.5348548722864234, 'yhat': array([1.82579456, 4.48271422, 1.28874683, ..., 2.80794991, 4.10339894,        4.49364018]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4678601789119381, 'pearson': 0.42740355164288973, 'spearman': 0.42865504598877424, 'mse': 2.0439178769739743, 'yhat': array([3.70161049, 1.9628877 , 2.56194076, ..., 3.79305473, 3.66688984,        3.52281093]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 53.94, 'acc': 54.36, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 298.24, 'acc': [(24.68, 57.7, 73.82, 4.0), (20.492, 52.74399999999999, 70.232, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 59.82, 'acc': 61.15, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 21.26, 'acc': 21.07, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.37, 'acc': 29.22, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.46, 'acc': 72.64, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.38, 'acc': 84.39, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.9, 'acc': 89.22, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.38, 'acc': 85.85, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.08, 'acc': 77.32, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.86, 'acc': 58.6, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.69, 'acc': 70.48, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 20:08:52,911 : STS12 p=0.2668, STS12 s=0.2925, STS13 p=0.2437, STS13 s=0.2365, STS14 p=0.2737, STS14 s=0.2652, STS15 p=0.3277, STS15 s=0.3388, STS 16 p=0.4366, STS16 s=0.4641, STS B p=0.4274, STS B s=0.4287, STS B m=2.0439, SICK-R p=0.6943, SICK-R s=0.6385, SICK-P m=0.5349
2019-02-16 20:08:52,911 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 20:08:52,911 : 0.2668,0.2925,0.2437,0.2365,0.2737,0.2652,0.3277,0.3388,0.4366,0.4641,0.4274,0.4287,2.0439,0.6943,0.6385,0.5349
2019-02-16 20:08:52,911 : MR=77.88, CR=81.77, SUBJ=94.58, MPQA=87.76, SST-B=80.83, SST-F=41.67, TREC=86.80, SICK-E=71.75, SNLI=54.36, MRPC=71.07, MRPC f=81.12
2019-02-16 20:08:52,911 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 20:08:52,911 : 77.88,81.77,94.58,87.76,80.83,41.67,86.80,71.75,54.36,71.07,81.12
2019-02-16 20:08:52,911 : COCO r1i2t=24.68, COCO r5i2t=57.70, COCO r10i2t=73.82, COCO medr_i2t=4.00, COCO r1t2i=20.49, COCO r5t2i=52.74, COCO r10t2i=70.23, COCO medr_t2i=5.00
2019-02-16 20:08:52,911 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 20:08:52,911 : 24.68,57.70,73.82,4.00,20.49,52.74,70.23,5.00
2019-02-16 20:08:52,911 : SentLen=61.15, WC=21.07, TreeDepth=29.22, TopConst=72.64, BShift=84.39, Tense=89.22, SubjNum=85.85, ObjNum=77.32, SOMO=58.60, CoordInv=70.48, average=64.99
2019-02-16 20:08:52,911 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 20:08:52,911 : 61.15,21.07,29.22,72.64,84.39,89.22,85.85,77.32,58.60,70.48,64.99
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:52,911 : layer 10
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:52,911 : ********************************************************************************
2019-02-16 20:08:53,003 : ***** Transfer task : STS12 *****


2019-02-16 20:08:53,016 : loading BERT model bert-base-uncased
2019-02-16 20:08:53,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:08:53,035 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:08:53,035 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmunqo294
2019-02-16 20:08:55,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:08:58,706 : MSRpar : pearson = 0.2409, spearman = 0.2847
2019-02-16 20:08:59,514 : MSRvid : pearson = -0.0263, spearman = 0.0278
2019-02-16 20:09:00,229 : SMTeuroparl : pearson = 0.4152, spearman = 0.4968
2019-02-16 20:09:01,428 : surprise.OnWN : pearson = 0.3317, spearman = 0.3377
2019-02-16 20:09:02,185 : surprise.SMTnews : pearson = 0.5211, spearman = 0.4446
2019-02-16 20:09:02,186 : ALL (weighted average) : Pearson = 0.2601,             Spearman = 0.2874
2019-02-16 20:09:02,186 : ALL (average) : Pearson = 0.2965,             Spearman = 0.3183

2019-02-16 20:09:02,186 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 20:09:02,199 : loading BERT model bert-base-uncased
2019-02-16 20:09:02,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:09:02,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:09:02,229 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0d9ndtjk
2019-02-16 20:09:05,298 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:09:08,055 : FNWN : pearson = 0.0803, spearman = 0.0936
2019-02-16 20:09:09,022 : headlines : pearson = 0.4338, spearman = 0.4200
2019-02-16 20:09:09,730 : OnWN : pearson = 0.1325, spearman = 0.1288
2019-02-16 20:09:09,731 : ALL (weighted average) : Pearson = 0.2766,             Spearman = 0.2699
2019-02-16 20:09:09,731 : ALL (average) : Pearson = 0.2155,             Spearman = 0.2141

2019-02-16 20:09:09,731 : ***** Transfer task : STS14 *****


2019-02-16 20:09:09,755 : loading BERT model bert-base-uncased
2019-02-16 20:09:09,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:09:09,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:09:09,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfr8j40yp
2019-02-16 20:09:12,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:09:15,410 : deft-forum : pearson = -0.1069, spearman = -0.1025
2019-02-16 20:09:16,267 : deft-news : pearson = 0.3967, spearman = 0.4350
2019-02-16 20:09:17,450 : headlines : pearson = 0.4195, spearman = 0.3922
2019-02-16 20:09:18,614 : images : pearson = 0.1291, spearman = 0.1345
2019-02-16 20:09:19,770 : OnWN : pearson = 0.3353, spearman = 0.3326
2019-02-16 20:09:21,287 : tweet-news : pearson = 0.4993, spearman = 0.4554
2019-02-16 20:09:21,288 : ALL (weighted average) : Pearson = 0.2956,             Spearman = 0.2855
2019-02-16 20:09:21,288 : ALL (average) : Pearson = 0.2789,             Spearman = 0.2745

2019-02-16 20:09:21,288 : ***** Transfer task : STS15 *****


2019-02-16 20:09:21,335 : loading BERT model bert-base-uncased
2019-02-16 20:09:21,336 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:09:21,361 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:09:21,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmper5oxp16
2019-02-16 20:09:24,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:09:27,238 : answers-forums : pearson = 0.2934, spearman = 0.2895
2019-02-16 20:09:28,286 : answers-students : pearson = 0.4098, spearman = 0.4134
2019-02-16 20:09:29,220 : belief : pearson = 0.4061, spearman = 0.4135
2019-02-16 20:09:30,310 : headlines : pearson = 0.4736, spearman = 0.4661
2019-02-16 20:09:31,402 : images : pearson = 0.1876, spearman = 0.1945
2019-02-16 20:09:31,402 : ALL (weighted average) : Pearson = 0.3552,             Spearman = 0.3564
2019-02-16 20:09:31,402 : ALL (average) : Pearson = 0.3541,             Spearman = 0.3554

2019-02-16 20:09:31,402 : ***** Transfer task : STS16 *****


2019-02-16 20:09:31,495 : loading BERT model bert-base-uncased
2019-02-16 20:09:31,495 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:09:31,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:09:31,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9ods2e_e
2019-02-16 20:09:34,569 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:09:37,185 : answer-answer : pearson = 0.2953, spearman = 0.3040
2019-02-16 20:09:37,549 : headlines : pearson = 0.5584, spearman = 0.5771
2019-02-16 20:09:38,001 : plagiarism : pearson = 0.6252, spearman = 0.6505
2019-02-16 20:09:38,642 : postediting : pearson = 0.7214, spearman = 0.7442
2019-02-16 20:09:38,977 : question-question : pearson = -0.0076, spearman = 0.0378
2019-02-16 20:09:38,978 : ALL (weighted average) : Pearson = 0.4488,             Spearman = 0.4722
2019-02-16 20:09:38,978 : ALL (average) : Pearson = 0.4385,             Spearman = 0.4627

2019-02-16 20:09:38,978 : ***** Transfer task : MR *****


2019-02-16 20:09:39,004 : loading BERT model bert-base-uncased
2019-02-16 20:09:39,004 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:09:39,038 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:09:39,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe0mkjlc9
2019-02-16 20:09:41,971 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:09:43,654 : Generating sentence embeddings
2019-02-16 20:09:57,199 : Generated sentence embeddings
2019-02-16 20:09:57,200 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 20:10:12,215 : Best param found at split 1: l2reg = 0.001                 with score 78.08
2019-02-16 20:10:27,384 : Best param found at split 2: l2reg = 0.0001                 with score 78.76
2019-02-16 20:10:42,310 : Best param found at split 3: l2reg = 0.001                 with score 78.34
2019-02-16 20:11:01,728 : Best param found at split 4: l2reg = 0.001                 with score 79.07
2019-02-16 20:11:18,703 : Best param found at split 5: l2reg = 0.001                 with score 78.9
2019-02-16 20:11:19,648 : Dev acc : 78.63 Test acc : 79.3

2019-02-16 20:11:19,649 : ***** Transfer task : CR *****


2019-02-16 20:11:19,657 : loading BERT model bert-base-uncased
2019-02-16 20:11:19,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:11:19,678 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:11:19,678 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4_cgy5x6
2019-02-16 20:11:22,139 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:11:23,616 : Generating sentence embeddings
2019-02-16 20:11:27,672 : Generated sentence embeddings
2019-02-16 20:11:27,672 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 20:11:32,106 : Best param found at split 1: l2reg = 0.001                 with score 86.15
2019-02-16 20:11:38,450 : Best param found at split 2: l2reg = 0.01                 with score 85.23
2019-02-16 20:11:44,023 : Best param found at split 3: l2reg = 1e-05                 with score 85.99
2019-02-16 20:11:50,615 : Best param found at split 4: l2reg = 0.01                 with score 85.47
2019-02-16 20:11:56,362 : Best param found at split 5: l2reg = 1e-05                 with score 85.6
2019-02-16 20:11:56,598 : Dev acc : 85.69 Test acc : 84.21

2019-02-16 20:11:56,598 : ***** Transfer task : MPQA *****


2019-02-16 20:11:56,604 : loading BERT model bert-base-uncased
2019-02-16 20:11:56,604 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:11:56,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:11:56,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp54y5_0y6
2019-02-16 20:11:59,144 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:12:00,684 : Generating sentence embeddings
2019-02-16 20:12:04,429 : Generated sentence embeddings
2019-02-16 20:12:04,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 20:12:21,240 : Best param found at split 1: l2reg = 0.001                 with score 87.13
2019-02-16 20:12:35,586 : Best param found at split 2: l2reg = 0.001                 with score 86.83
2019-02-16 20:12:53,229 : Best param found at split 3: l2reg = 0.001                 with score 86.67
2019-02-16 20:13:08,303 : Best param found at split 4: l2reg = 0.01                 with score 87.75
2019-02-16 20:13:27,011 : Best param found at split 5: l2reg = 1e-05                 with score 86.91
2019-02-16 20:13:28,212 : Dev acc : 87.06 Test acc : 87.27

2019-02-16 20:13:28,213 : ***** Transfer task : SUBJ *****


2019-02-16 20:13:28,230 : loading BERT model bert-base-uncased
2019-02-16 20:13:28,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:13:28,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:13:28,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ffbyaaq
2019-02-16 20:13:30,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:13:32,265 : Generating sentence embeddings
2019-02-16 20:13:45,428 : Generated sentence embeddings
2019-02-16 20:13:45,428 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 20:14:01,920 : Best param found at split 1: l2reg = 0.001                 with score 95.01
2019-02-16 20:14:17,805 : Best param found at split 2: l2reg = 1e-05                 with score 95.11
2019-02-16 20:14:33,615 : Best param found at split 3: l2reg = 0.001                 with score 95.04
2019-02-16 20:14:50,200 : Best param found at split 4: l2reg = 0.001                 with score 95.26
2019-02-16 20:15:06,605 : Best param found at split 5: l2reg = 0.001                 with score 94.96
2019-02-16 20:15:07,094 : Dev acc : 95.08 Test acc : 94.85

2019-02-16 20:15:07,095 : ***** Transfer task : SST Binary classification *****


2019-02-16 20:15:07,228 : loading BERT model bert-base-uncased
2019-02-16 20:15:07,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:15:07,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:15:07,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2m3xfc3a
2019-02-16 20:15:09,772 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:15:11,227 : Computing embedding for train
2019-02-16 20:15:56,880 : Computed train embeddings
2019-02-16 20:15:56,880 : Computing embedding for dev
2019-02-16 20:15:57,855 : Computed dev embeddings
2019-02-16 20:15:57,856 : Computing embedding for test
2019-02-16 20:15:59,826 : Computed test embeddings
2019-02-16 20:15:59,826 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:16:23,102 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.86), ('reg:0.001', 84.29), ('reg:0.01', 79.93)]
2019-02-16 20:16:23,102 : Validation : best param found is reg = 0.0001 with score             84.86
2019-02-16 20:16:23,102 : Evaluating...
2019-02-16 20:16:28,939 : 
Dev acc : 84.86 Test acc : 82.04 for             SST Binary classification

2019-02-16 20:16:28,940 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 20:16:28,995 : loading BERT model bert-base-uncased
2019-02-16 20:16:28,995 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:16:29,019 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:16:29,019 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqte572du
2019-02-16 20:16:31,516 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:16:32,988 : Computing embedding for train
2019-02-16 20:16:43,449 : Computed train embeddings
2019-02-16 20:16:43,449 : Computing embedding for dev
2019-02-16 20:16:44,709 : Computed dev embeddings
2019-02-16 20:16:44,709 : Computing embedding for test
2019-02-16 20:16:47,147 : Computed test embeddings
2019-02-16 20:16:47,147 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:16:51,155 : [('reg:1e-05', 42.05), ('reg:0.0001', 39.33), ('reg:0.001', 42.14), ('reg:0.01', 35.97)]
2019-02-16 20:16:51,155 : Validation : best param found is reg = 0.001 with score             42.14
2019-02-16 20:16:51,156 : Evaluating...
2019-02-16 20:16:51,869 : 
Dev acc : 42.14 Test acc : 42.04 for             SST Fine-Grained classification

2019-02-16 20:16:51,869 : ***** Transfer task : TREC *****


2019-02-16 20:16:51,883 : loading BERT model bert-base-uncased
2019-02-16 20:16:51,883 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:16:51,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:16:51,904 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8igcw5o1
2019-02-16 20:16:54,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:16:59,335 : Computed train embeddings
2019-02-16 20:16:59,594 : Computed test embeddings
2019-02-16 20:16:59,594 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 20:17:10,162 : [('reg:1e-05', 79.77), ('reg:0.0001', 79.69), ('reg:0.001', 78.1), ('reg:0.01', 71.66)]
2019-02-16 20:17:10,163 : Cross-validation : best param found is reg = 1e-05             with score 79.77
2019-02-16 20:17:10,163 : Evaluating...
2019-02-16 20:17:10,809 : 
Dev acc : 79.77 Test acc : 88.4             for TREC

2019-02-16 20:17:10,810 : ***** Transfer task : MRPC *****


2019-02-16 20:17:10,832 : loading BERT model bert-base-uncased
2019-02-16 20:17:10,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:17:10,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:17:10,855 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptw0hfnnj
2019-02-16 20:17:13,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:17:14,874 : Computing embedding for train
2019-02-16 20:17:24,501 : Computed train embeddings
2019-02-16 20:17:24,501 : Computing embedding for test
2019-02-16 20:17:28,668 : Computed test embeddings
2019-02-16 20:17:28,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 20:17:36,334 : [('reg:1e-05', 70.98), ('reg:0.0001', 70.73), ('reg:0.001', 70.22), ('reg:0.01', 70.54)]
2019-02-16 20:17:36,334 : Cross-validation : best param found is reg = 1e-05             with score 70.98
2019-02-16 20:17:36,335 : Evaluating...
2019-02-16 20:17:36,796 : Dev acc : 70.98 Test acc 69.68; Test F1 77.75 for MRPC.

2019-02-16 20:17:36,796 : ***** Transfer task : SICK-Entailment*****


2019-02-16 20:17:36,866 : loading BERT model bert-base-uncased
2019-02-16 20:17:36,866 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:17:36,891 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:17:36,891 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwvfk1wou
2019-02-16 20:17:39,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:17:40,920 : Computing embedding for train
2019-02-16 20:17:46,059 : Computed train embeddings
2019-02-16 20:17:46,060 : Computing embedding for dev
2019-02-16 20:17:46,744 : Computed dev embeddings
2019-02-16 20:17:46,744 : Computing embedding for test
2019-02-16 20:17:52,257 : Computed test embeddings
2019-02-16 20:17:52,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:17:54,997 : [('reg:1e-05', 72.2), ('reg:0.0001', 70.8), ('reg:0.001', 72.4), ('reg:0.01', 73.2)]
2019-02-16 20:17:54,997 : Validation : best param found is reg = 0.01 with score             73.2
2019-02-16 20:17:54,997 : Evaluating...
2019-02-16 20:17:55,344 : 
Dev acc : 73.2 Test acc : 69.92 for                        SICK entailment

2019-02-16 20:17:55,345 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 20:17:55,375 : loading BERT model bert-base-uncased
2019-02-16 20:17:55,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:17:55,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:17:55,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9k2r6l0u
2019-02-16 20:17:57,963 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:17:59,457 : Computing embedding for train
2019-02-16 20:18:04,638 : Computed train embeddings
2019-02-16 20:18:04,638 : Computing embedding for dev
2019-02-16 20:18:05,312 : Computed dev embeddings
2019-02-16 20:18:05,312 : Computing embedding for test
2019-02-16 20:18:10,843 : Computed test embeddings
2019-02-16 20:18:33,182 : Dev : Pearson 0.6613608896247289
2019-02-16 20:18:33,182 : Test : Pearson 0.6786826123360956 Spearman 0.6358473936506438 MSE 0.5519720149382482                        for SICK Relatedness

2019-02-16 20:18:33,183 : 

***** Transfer task : STSBenchmark*****


2019-02-16 20:18:33,256 : loading BERT model bert-base-uncased
2019-02-16 20:18:33,256 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:18:33,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:18:33,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9ab07wd2
2019-02-16 20:18:35,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:18:37,313 : Computing embedding for train
2019-02-16 20:18:45,656 : Computed train embeddings
2019-02-16 20:18:45,656 : Computing embedding for dev
2019-02-16 20:18:48,131 : Computed dev embeddings
2019-02-16 20:18:48,132 : Computing embedding for test
2019-02-16 20:18:50,112 : Computed test embeddings
2019-02-16 20:19:18,306 : Dev : Pearson 0.4242663873189794
2019-02-16 20:19:18,306 : Test : Pearson 0.4290266265485483 Spearman 0.4301109170742621 MSE 1.9940691059024633                        for SICK Relatedness

2019-02-16 20:19:18,306 : ***** Transfer task : SNLI Entailment*****


2019-02-16 20:19:23,084 : loading BERT model bert-base-uncased
2019-02-16 20:19:23,084 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:19:23,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:19:23,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbe9nw692
2019-02-16 20:19:25,676 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:19:27,343 : PROGRESS (encoding): 0.00%
2019-02-16 20:20:45,986 : PROGRESS (encoding): 14.56%
2019-02-16 20:22:13,278 : PROGRESS (encoding): 29.12%
2019-02-16 20:23:40,806 : PROGRESS (encoding): 43.69%
2019-02-16 20:25:16,459 : PROGRESS (encoding): 58.25%
2019-02-16 20:27:05,470 : PROGRESS (encoding): 72.81%
2019-02-16 20:28:49,162 : PROGRESS (encoding): 87.37%
2019-02-16 20:30:39,139 : PROGRESS (encoding): 0.00%
2019-02-16 20:30:52,565 : PROGRESS (encoding): 0.00%
2019-02-16 20:31:05,583 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 20:31:53,044 : [('reg:1e-09', 59.45)]
2019-02-16 20:31:53,044 : Validation : best param found is reg = 1e-09 with score             59.45
2019-02-16 20:31:53,044 : Evaluating...
2019-02-16 20:32:41,413 : Dev acc : 59.45 Test acc : 59.06 for SNLI

2019-02-16 20:32:41,413 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 20:32:50,774 : loading BERT model bert-base-uncased
2019-02-16 20:32:50,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 20:32:50,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 20:32:50,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdv5ugaeh
2019-02-16 20:32:53,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 20:32:54,814 : Computing embedding for train
2019-02-16 20:41:29,232 : Computed train embeddings
2019-02-16 20:41:29,233 : Computing embedding for dev
2019-02-16 20:41:52,542 : Computed dev embeddings
2019-02-16 20:41:52,542 : Computing embedding for test
2019-02-16 20:42:20,129 : Computed test embeddings
2019-02-16 20:42:20,145 : prepare data
2019-02-16 20:42:20,207 : start epoch
2019-02-16 20:43:06,064 : samples : 64000
2019-02-16 20:43:18,759 : Image to text: 3.3, 12.48, 20.06, 46.0
2019-02-16 20:43:28,183 : Text to Image: 2.728, 10.336, 17.676, 55.0
2019-02-16 20:44:18,510 : samples : 128000
2019-02-16 20:44:31,061 : Image to text: 3.98, 14.94, 23.54, 39.0
2019-02-16 20:44:41,119 : Text to Image: 3.452, 12.36, 19.972, 47.0
2019-02-16 20:45:26,482 : samples : 192000
2019-02-16 20:45:39,107 : Image to text: 4.42, 15.72, 25.42, 36.0
2019-02-16 20:45:47,010 : Text to Image: 3.584, 13.264, 21.344, 43.0
2019-02-16 20:46:29,782 : samples : 256000
2019-02-16 20:46:40,078 : Image to text: 4.28, 16.24, 25.94, 33.0
2019-02-16 20:46:49,994 : Text to Image: 3.684, 13.852, 22.264, 40.0
2019-02-16 20:47:35,692 : samples : 320000
2019-02-16 20:47:48,273 : Image to text: 4.24, 16.06, 25.66, 32.0
2019-02-16 20:47:58,022 : Text to Image: 4.236, 14.44, 23.2, 38.0
2019-02-16 20:48:40,979 : samples : 384000
2019-02-16 20:48:51,186 : Image to text: 4.72, 16.96, 26.68, 31.0
2019-02-16 20:48:59,059 : Text to Image: 4.172, 14.96, 23.748, 37.0
2019-02-16 20:49:44,391 : samples : 448000
2019-02-16 20:49:56,994 : Image to text: 4.98, 17.7, 27.98, 29.0
2019-02-16 20:50:07,009 : Text to Image: 4.34, 15.56, 24.688, 36.0
2019-02-16 20:50:50,515 : samples : 512000
2019-02-16 20:51:00,807 : Image to text: 5.16, 18.52, 28.48, 29.0
2019-02-16 20:51:08,220 : Text to Image: 4.864, 16.58, 25.92, 33.0
2019-02-16 20:51:45,656 : Epoch 1 finished
2019-02-16 20:51:46,590 : Image to text: 17.0, 45.4, 62.7, 7.0
2019-02-16 20:51:47,387 : Text to Image: 14.22, 42.26, 59.0, 7.0
2019-02-16 20:51:48,346 : Image to text: 15.6, 45.4, 61.8, 7.0
2019-02-16 20:51:49,134 : Text to Image: 13.82, 41.78, 58.96, 8.0
2019-02-16 20:51:50,095 : Image to text: 14.9, 47.1, 63.1, 6.0
2019-02-16 20:51:50,840 : Text to Image: 14.02, 41.12, 57.44, 8.0
2019-02-16 20:51:51,769 : Image to text: 16.3, 46.3, 62.7, 6.0
2019-02-16 20:51:52,528 : Text to Image: 14.88, 41.18, 58.18, 8.0
2019-02-16 20:51:53,497 : Image to text: 18.7, 48.0, 63.7, 6.0
2019-02-16 20:51:54,273 : Text to Image: 14.94, 44.06, 60.02, 7.0
2019-02-16 20:51:54,273 : Dev mean Text to Image: 14.376000000000001, 42.08, 58.72, 7.6
2019-02-16 20:51:54,274 : Dev mean Image to text: 16.5, 46.44, 62.8, 6.4
2019-02-16 20:51:54,274 : start epoch
2019-02-16 20:52:40,320 : samples : 64000
2019-02-16 20:52:52,455 : Image to text: 6.3, 19.1, 29.24, 27.0
2019-02-16 20:52:59,752 : Text to Image: 4.856, 16.5, 25.996, 33.0
2019-02-16 20:53:42,708 : samples : 128000
2019-02-16 20:53:54,413 : Image to text: 5.9, 19.74, 29.8, 26.0
2019-02-16 20:54:04,404 : Text to Image: 4.604, 16.576, 25.996, 32.0
2019-02-16 20:54:49,787 : samples : 192000
2019-02-16 20:55:02,379 : Image to text: 6.02, 19.92, 30.26, 26.0
2019-02-16 20:55:12,439 : Text to Image: 5.008, 17.18, 27.004, 32.0
2019-02-16 20:55:55,204 : samples : 256000
2019-02-16 20:56:05,463 : Image to text: 6.28, 20.0, 30.74, 27.0
2019-02-16 20:56:12,541 : Text to Image: 5.388, 17.972, 27.68, 30.0
2019-02-16 20:56:57,466 : samples : 320000
2019-02-16 20:57:10,085 : Image to text: 5.84, 20.54, 31.08, 26.0
2019-02-16 20:57:20,136 : Text to Image: 4.872, 17.204, 26.636, 31.0
2019-02-16 20:58:04,042 : samples : 384000
2019-02-16 20:58:14,325 : Image to text: 5.92, 18.94, 30.32, 26.0
2019-02-16 20:58:21,648 : Text to Image: 5.04, 17.448, 27.264, 30.0
2019-02-16 20:59:04,870 : samples : 448000
2019-02-16 20:59:17,534 : Image to text: 6.06, 19.34, 29.68, 27.0
2019-02-16 20:59:27,623 : Text to Image: 5.14, 17.34, 26.996, 31.0
2019-02-16 21:00:13,699 : samples : 512000
2019-02-16 21:00:26,296 : Image to text: 6.28, 20.98, 31.8, 24.0
2019-02-16 21:00:34,677 : Text to Image: 5.356, 18.252, 28.348, 29.0
2019-02-16 21:01:20,236 : Epoch 2 finished
2019-02-16 21:01:20,799 : Image to text: 20.0, 49.8, 66.7, 6.0
2019-02-16 21:01:21,229 : Text to Image: 16.56, 45.48, 62.98, 7.0
2019-02-16 21:01:21,756 : Image to text: 17.8, 47.9, 65.9, 6.0
2019-02-16 21:01:22,195 : Text to Image: 15.26, 44.78, 61.96, 7.0
2019-02-16 21:01:22,735 : Image to text: 18.0, 48.5, 65.9, 6.0
2019-02-16 21:01:23,189 : Text to Image: 15.72, 44.64, 62.24, 7.0
2019-02-16 21:01:23,720 : Image to text: 19.0, 50.0, 65.9, 5.0
2019-02-16 21:01:24,143 : Text to Image: 16.16, 45.48, 61.5, 7.0
2019-02-16 21:01:24,671 : Image to text: 20.6, 51.2, 66.8, 5.0
2019-02-16 21:01:25,077 : Text to Image: 16.44, 46.02, 62.62, 7.0
2019-02-16 21:01:25,077 : Dev mean Text to Image: 16.028, 45.28, 62.260000000000005, 7.0
2019-02-16 21:01:25,077 : Dev mean Image to text: 19.080000000000002, 49.48, 66.24000000000001, 5.6
2019-02-16 21:01:25,077 : start epoch
2019-02-16 21:02:09,375 : samples : 64000
2019-02-16 21:02:22,021 : Image to text: 5.48, 19.12, 30.5, 26.0
2019-02-16 21:02:32,075 : Text to Image: 5.08, 17.308, 27.268, 31.0
2019-02-16 21:03:17,016 : samples : 128000
2019-02-16 21:03:27,314 : Image to text: 5.64, 19.8, 30.46, 25.0
2019-02-16 21:03:34,764 : Text to Image: 5.048, 17.42, 27.18, 30.0
2019-02-16 21:04:17,663 : samples : 192000
2019-02-16 21:04:28,156 : Image to text: 6.6, 21.44, 32.64, 24.0
2019-02-16 21:04:36,781 : Text to Image: 5.296, 18.272, 28.448, 29.0
2019-02-16 21:05:20,648 : samples : 256000
2019-02-16 21:05:31,024 : Image to text: 6.4, 20.66, 32.26, 24.0
2019-02-16 21:05:38,410 : Text to Image: 5.396, 18.568, 28.552, 29.0
2019-02-16 21:06:21,064 : samples : 320000
2019-02-16 21:06:31,351 : Image to text: 6.22, 21.42, 32.52, 24.0
2019-02-16 21:06:38,768 : Text to Image: 5.528, 18.82, 29.188, 28.0
2019-02-16 21:07:21,797 : samples : 384000
2019-02-16 21:07:32,196 : Image to text: 7.18, 21.2, 32.4, 24.0
2019-02-16 21:07:40,335 : Text to Image: 5.62, 18.86, 29.172, 28.0
2019-02-16 21:08:22,959 : samples : 448000
2019-02-16 21:08:33,246 : Image to text: 6.44, 20.52, 31.12, 24.0
2019-02-16 21:08:40,665 : Text to Image: 5.856, 19.368, 29.732, 27.0
2019-02-16 21:09:24,153 : samples : 512000
2019-02-16 21:09:34,396 : Image to text: 6.92, 21.42, 32.74, 23.0
2019-02-16 21:09:41,840 : Text to Image: 5.344, 18.548, 28.964, 28.0
2019-02-16 21:10:18,292 : Epoch 3 finished
2019-02-16 21:10:18,768 : Image to text: 17.6, 48.1, 65.9, 6.0
2019-02-16 21:10:19,133 : Text to Image: 15.04, 43.26, 60.48, 7.0
2019-02-16 21:10:19,599 : Image to text: 16.9, 46.8, 64.1, 6.0
2019-02-16 21:10:19,965 : Text to Image: 14.12, 42.92, 60.2, 7.0
2019-02-16 21:10:20,431 : Image to text: 19.0, 49.5, 65.3, 6.0
2019-02-16 21:10:20,803 : Text to Image: 13.46, 41.68, 59.0, 7.0
2019-02-16 21:10:21,251 : Image to text: 17.9, 48.6, 64.4, 6.0
2019-02-16 21:10:21,614 : Text to Image: 15.04, 43.04, 59.58, 7.0
2019-02-16 21:10:22,065 : Image to text: 19.1, 49.5, 65.5, 6.0
2019-02-16 21:10:22,427 : Text to Image: 14.88, 44.76, 61.54, 7.0
2019-02-16 21:10:22,428 : Dev mean Text to Image: 14.508, 43.13199999999999, 60.160000000000004, 7.0
2019-02-16 21:10:22,428 : Dev mean Image to text: 18.1, 48.5, 65.04, 6.0
2019-02-16 21:10:22,428 : start epoch
2019-02-16 21:11:05,310 : samples : 64000
2019-02-16 21:11:17,956 : Image to text: 6.72, 21.32, 32.74, 23.0
2019-02-16 21:11:28,142 : Text to Image: 5.552, 18.716, 29.072, 27.0
2019-02-16 21:12:12,501 : samples : 128000
2019-02-16 21:12:22,796 : Image to text: 6.44, 21.88, 33.46, 22.0
2019-02-16 21:12:29,890 : Text to Image: 5.76, 19.288, 30.04, 27.0
2019-02-16 21:13:13,623 : samples : 192000
2019-02-16 21:13:26,364 : Image to text: 5.96, 20.58, 31.58, 24.0
2019-02-16 21:13:33,758 : Text to Image: 5.576, 18.568, 28.98, 28.0
2019-02-16 21:14:17,672 : samples : 256000
2019-02-16 21:14:27,912 : Image to text: 6.52, 21.14, 32.74, 23.0
2019-02-16 21:14:35,448 : Text to Image: 5.852, 19.628, 30.188, 26.0
2019-02-16 21:15:18,483 : samples : 320000
2019-02-16 21:15:29,634 : Image to text: 7.02, 21.88, 33.22, 22.0
2019-02-16 21:15:39,606 : Text to Image: 5.716, 19.64, 30.204, 27.0
2019-02-16 21:16:24,850 : samples : 384000
2019-02-16 21:16:37,373 : Image to text: 6.72, 21.88, 32.72, 22.0
2019-02-16 21:16:47,312 : Text to Image: 6.224, 19.584, 30.28, 27.0
2019-02-16 21:17:32,641 : samples : 448000
2019-02-16 21:17:45,125 : Image to text: 7.0, 22.52, 34.4, 22.0
2019-02-16 21:17:55,361 : Text to Image: 6.192, 20.08, 30.52, 26.0
2019-02-16 21:18:48,151 : samples : 512000
2019-02-16 21:19:00,708 : Image to text: 7.08, 22.8, 35.48, 21.0
2019-02-16 21:19:10,633 : Text to Image: 6.164, 20.088, 30.988, 26.0
2019-02-16 21:19:49,234 : Epoch 4 finished
2019-02-16 21:19:50,135 : Image to text: 22.2, 52.9, 68.7, 5.0
2019-02-16 21:19:50,887 : Text to Image: 18.0, 47.9, 65.7, 6.0
2019-02-16 21:19:51,822 : Image to text: 20.2, 50.7, 67.9, 5.0
2019-02-16 21:19:52,613 : Text to Image: 17.24, 47.02, 64.92, 6.0
2019-02-16 21:19:53,527 : Image to text: 20.7, 53.7, 69.9, 5.0
2019-02-16 21:19:54,296 : Text to Image: 16.78, 47.5, 64.74, 6.0
2019-02-16 21:19:55,232 : Image to text: 19.9, 53.9, 70.1, 5.0
2019-02-16 21:19:56,022 : Text to Image: 17.08, 46.74, 64.5, 6.0
2019-02-16 21:19:56,938 : Image to text: 21.0, 55.1, 69.8, 5.0
2019-02-16 21:19:57,712 : Text to Image: 18.0, 48.38, 65.38, 6.0
2019-02-16 21:19:57,712 : Dev mean Text to Image: 17.42, 47.508, 65.048, 6.0
2019-02-16 21:19:57,712 : Dev mean Image to text: 20.8, 53.260000000000005, 69.27999999999999, 5.0
2019-02-16 21:19:57,712 : start epoch
2019-02-16 21:20:43,103 : samples : 64000
2019-02-16 21:20:55,649 : Image to text: 7.34, 22.98, 34.68, 22.0
2019-02-16 21:21:05,632 : Text to Image: 6.224, 20.224, 31.276, 25.0
2019-02-16 21:21:50,589 : samples : 128000
2019-02-16 21:22:03,174 : Image to text: 7.16, 22.62, 34.02, 22.0
2019-02-16 21:22:13,229 : Text to Image: 6.104, 19.732, 30.492, 26.0
2019-02-16 21:22:57,425 : samples : 192000
2019-02-16 21:23:10,119 : Image to text: 6.48, 22.24, 33.22, 22.0
2019-02-16 21:23:20,105 : Text to Image: 6.052, 19.548, 30.404, 26.0
2019-02-16 21:24:04,695 : samples : 256000
2019-02-16 21:24:17,257 : Image to text: 6.6, 21.78, 33.64, 21.0
2019-02-16 21:24:27,273 : Text to Image: 6.084, 19.78, 30.6, 26.0
2019-02-16 21:25:11,923 : samples : 320000
2019-02-16 21:25:24,523 : Image to text: 6.9, 22.46, 33.7, 22.0
2019-02-16 21:25:34,526 : Text to Image: 6.148, 19.792, 30.732, 26.0
2019-02-16 21:26:20,014 : samples : 384000
2019-02-16 21:26:32,723 : Image to text: 6.9, 21.94, 33.6, 22.0
2019-02-16 21:26:41,778 : Text to Image: 6.128, 19.872, 30.556, 26.0
2019-02-16 21:27:25,640 : samples : 448000
2019-02-16 21:27:35,943 : Image to text: 7.06, 22.38, 34.52, 21.0
2019-02-16 21:27:43,367 : Text to Image: 6.072, 19.64, 30.284, 26.0
2019-02-16 21:28:26,245 : samples : 512000
2019-02-16 21:28:36,250 : Image to text: 7.24, 23.36, 34.86, 20.0
2019-02-16 21:28:43,078 : Text to Image: 6.464, 20.604, 31.256, 25.0
2019-02-16 21:29:21,837 : Epoch 5 finished
2019-02-16 21:29:22,853 : Image to text: 22.3, 55.4, 71.7, 5.0
2019-02-16 21:29:23,703 : Text to Image: 18.48, 49.24, 66.84, 6.0
2019-02-16 21:29:24,787 : Image to text: 19.1, 52.0, 68.5, 5.0
2019-02-16 21:29:25,677 : Text to Image: 18.26, 48.86, 66.74, 6.0
2019-02-16 21:29:26,700 : Image to text: 23.0, 54.3, 70.0, 5.0
2019-02-16 21:29:27,571 : Text to Image: 18.4, 48.5, 66.24, 6.0
2019-02-16 21:29:28,663 : Image to text: 21.0, 54.6, 70.9, 5.0
2019-02-16 21:29:29,536 : Text to Image: 18.0, 49.16, 66.52, 6.0
2019-02-16 21:29:30,577 : Image to text: 22.4, 55.8, 71.8, 4.0
2019-02-16 21:29:31,422 : Text to Image: 19.04, 50.42, 66.64, 5.0
2019-02-16 21:29:31,422 : Dev mean Text to Image: 18.436, 49.236000000000004, 66.596, 5.8
2019-02-16 21:29:31,422 : Dev mean Image to text: 21.560000000000002, 54.42, 70.58, 4.8
2019-02-16 21:29:31,423 : start epoch
2019-02-16 21:30:17,388 : samples : 64000
2019-02-16 21:30:30,239 : Image to text: 7.34, 23.74, 35.44, 20.0
2019-02-16 21:30:40,575 : Text to Image: 6.64, 21.312, 32.148, 24.0
2019-02-16 21:31:26,809 : samples : 128000
2019-02-16 21:31:39,619 : Image to text: 7.12, 22.68, 34.46, 21.0
2019-02-16 21:31:50,110 : Text to Image: 6.248, 20.068, 30.772, 25.0
2019-02-16 21:32:36,243 : samples : 192000
2019-02-16 21:32:49,157 : Image to text: 7.12, 22.34, 34.36, 21.0
2019-02-16 21:32:59,690 : Text to Image: 6.308, 20.268, 31.18, 25.0
2019-02-16 21:33:45,748 : samples : 256000
2019-02-16 21:33:58,654 : Image to text: 6.74, 21.98, 33.92, 21.0
2019-02-16 21:34:09,153 : Text to Image: 6.032, 20.112, 30.78, 26.0
2019-02-16 21:34:54,610 : samples : 320000
2019-02-16 21:35:07,554 : Image to text: 7.4, 22.94, 35.34, 21.0
2019-02-16 21:35:18,071 : Text to Image: 6.564, 20.768, 31.528, 25.0
2019-02-16 21:36:09,977 : samples : 384000
2019-02-16 21:36:22,065 : Image to text: 7.78, 24.14, 35.96, 20.0
2019-02-16 21:36:32,451 : Text to Image: 6.544, 20.72, 31.58, 25.0
2019-02-16 21:37:17,349 : samples : 448000
2019-02-16 21:37:27,933 : Image to text: 7.18, 22.08, 34.5, 21.0
2019-02-16 21:37:35,458 : Text to Image: 6.232, 20.52, 31.22, 25.0
2019-02-16 21:38:18,234 : samples : 512000
2019-02-16 21:38:28,766 : Image to text: 6.9, 22.6, 34.64, 21.0
2019-02-16 21:38:36,291 : Text to Image: 5.712, 18.984, 29.604, 27.0
2019-02-16 21:39:13,358 : Epoch 6 finished
2019-02-16 21:39:13,785 : Image to text: 22.1, 54.5, 70.6, 5.0
2019-02-16 21:39:14,125 : Text to Image: 18.92, 49.92, 67.96, 6.0
2019-02-16 21:39:14,564 : Image to text: 20.3, 53.0, 71.5, 5.0
2019-02-16 21:39:14,897 : Text to Image: 17.88, 49.72, 67.32, 6.0
2019-02-16 21:39:15,353 : Image to text: 21.8, 53.8, 71.3, 5.0
2019-02-16 21:39:15,692 : Text to Image: 18.66, 49.62, 67.62, 6.0
2019-02-16 21:39:16,142 : Image to text: 21.9, 53.8, 70.2, 5.0
2019-02-16 21:39:16,485 : Text to Image: 18.64, 49.4, 67.48, 6.0
2019-02-16 21:39:16,941 : Image to text: 22.0, 54.5, 72.0, 5.0
2019-02-16 21:39:17,291 : Text to Image: 18.84, 50.3, 66.6, 5.0
2019-02-16 21:39:17,291 : Dev mean Text to Image: 18.587999999999997, 49.792, 67.396, 5.8
2019-02-16 21:39:17,291 : Dev mean Image to text: 21.619999999999997, 53.919999999999995, 71.12, 5.0
2019-02-16 21:39:17,292 : start epoch
2019-02-16 21:40:00,283 : samples : 64000
2019-02-16 21:40:10,803 : Image to text: 7.76, 23.96, 35.92, 19.0
2019-02-16 21:40:18,349 : Text to Image: 6.932, 21.756, 32.832, 23.0
2019-02-16 21:41:01,494 : samples : 128000
2019-02-16 21:41:12,002 : Image to text: 7.68, 23.8, 35.38, 21.0
2019-02-16 21:41:19,538 : Text to Image: 6.624, 21.04, 32.2, 24.0
2019-02-16 21:42:02,226 : samples : 192000
2019-02-16 21:42:12,747 : Image to text: 7.94, 24.68, 36.46, 20.0
2019-02-16 21:42:20,302 : Text to Image: 6.624, 21.168, 32.544, 23.0
2019-02-16 21:43:03,047 : samples : 256000
2019-02-16 21:43:13,597 : Image to text: 7.06, 23.04, 34.72, 21.0
2019-02-16 21:43:21,169 : Text to Image: 6.536, 20.864, 31.732, 24.0
2019-02-16 21:44:03,697 : samples : 320000
2019-02-16 21:44:14,184 : Image to text: 7.76, 22.86, 34.76, 21.0
2019-02-16 21:44:21,762 : Text to Image: 6.504, 21.084, 31.96, 24.0
2019-02-16 21:45:04,256 : samples : 384000
2019-02-16 21:45:14,752 : Image to text: 7.36, 23.46, 35.54, 20.0
2019-02-16 21:45:22,283 : Text to Image: 6.784, 21.148, 32.396, 23.0
2019-02-16 21:46:04,434 : samples : 448000
2019-02-16 21:46:14,933 : Image to text: 7.78, 23.8, 35.7, 20.0
2019-02-16 21:46:22,469 : Text to Image: 6.516, 21.532, 32.464, 24.0
2019-02-16 21:47:05,115 : samples : 512000
2019-02-16 21:47:15,552 : Image to text: 8.02, 24.9, 36.22, 20.0
2019-02-16 21:47:23,092 : Text to Image: 7.056, 21.752, 33.328, 23.0
2019-02-16 21:47:59,310 : Epoch 7 finished
2019-02-16 21:47:59,748 : Image to text: 21.8, 54.3, 71.7, 5.0
2019-02-16 21:48:00,091 : Text to Image: 19.06, 50.76, 68.14, 5.0
2019-02-16 21:48:00,540 : Image to text: 20.2, 53.4, 71.3, 5.0
2019-02-16 21:48:00,887 : Text to Image: 18.88, 50.74, 68.18, 5.0
2019-02-16 21:48:01,353 : Image to text: 24.0, 54.1, 70.1, 4.0
2019-02-16 21:48:01,711 : Text to Image: 19.26, 49.64, 66.76, 6.0
2019-02-16 21:48:02,170 : Image to text: 22.1, 55.1, 71.8, 4.0
2019-02-16 21:48:02,514 : Text to Image: 18.52, 50.02, 67.58, 5.0
2019-02-16 21:48:02,982 : Image to text: 22.8, 55.4, 72.1, 5.0
2019-02-16 21:48:03,321 : Text to Image: 19.1, 49.96, 68.18, 6.0
2019-02-16 21:48:03,321 : Dev mean Text to Image: 18.964, 50.224000000000004, 67.768, 5.4
2019-02-16 21:48:03,321 : Dev mean Image to text: 22.18, 54.459999999999994, 71.4, 4.6
2019-02-16 21:48:03,322 : start epoch
2019-02-16 21:48:45,858 : samples : 64000
2019-02-16 21:48:56,392 : Image to text: 7.02, 23.32, 35.56, 19.0
2019-02-16 21:49:03,936 : Text to Image: 6.732, 21.436, 32.44, 24.0
2019-02-16 21:49:46,543 : samples : 128000
2019-02-16 21:49:57,108 : Image to text: 7.58, 24.0, 35.88, 20.0
2019-02-16 21:50:04,680 : Text to Image: 6.632, 20.988, 31.996, 24.0
2019-02-16 21:50:47,148 : samples : 192000
2019-02-16 21:50:57,674 : Image to text: 7.66, 23.44, 36.66, 19.0
2019-02-16 21:51:05,169 : Text to Image: 6.616, 20.98, 32.004, 24.0
2019-02-16 21:51:48,054 : samples : 256000
2019-02-16 21:51:58,688 : Image to text: 7.84, 23.86, 36.02, 20.0
2019-02-16 21:52:06,122 : Text to Image: 6.936, 21.864, 33.012, 23.0
2019-02-16 21:52:58,593 : samples : 320000
2019-02-16 21:53:09,523 : Image to text: 8.1, 24.34, 36.88, 19.0
2019-02-16 21:53:17,147 : Text to Image: 6.952, 22.256, 33.384, 23.0
2019-02-16 21:53:59,956 : samples : 384000
2019-02-16 21:54:10,439 : Image to text: 7.9, 24.18, 35.78, 20.0
2019-02-16 21:54:17,953 : Text to Image: 7.1, 21.812, 32.66, 23.0
2019-02-16 21:55:00,772 : samples : 448000
2019-02-16 21:55:11,233 : Image to text: 7.54, 24.0, 36.5, 19.0
2019-02-16 21:55:18,814 : Text to Image: 6.588, 21.332, 32.424, 24.0
2019-02-16 21:56:01,274 : samples : 512000
2019-02-16 21:56:11,772 : Image to text: 7.64, 23.62, 35.5, 21.0
2019-02-16 21:56:19,267 : Text to Image: 6.82, 21.056, 32.364, 23.0
2019-02-16 21:56:55,691 : Epoch 8 finished
2019-02-16 21:56:56,140 : Image to text: 22.8, 57.0, 73.2, 5.0
2019-02-16 21:56:56,473 : Text to Image: 19.06, 50.88, 68.66, 5.0
2019-02-16 21:56:56,922 : Image to text: 22.6, 54.1, 72.6, 5.0
2019-02-16 21:56:57,268 : Text to Image: 19.14, 50.6, 68.34, 5.0
2019-02-16 21:56:57,706 : Image to text: 23.7, 55.4, 70.5, 4.0
2019-02-16 21:56:58,050 : Text to Image: 19.26, 50.44, 68.08, 5.0
2019-02-16 21:56:58,495 : Image to text: 21.5, 56.5, 73.9, 4.0
2019-02-16 21:56:58,838 : Text to Image: 18.96, 51.12, 68.06, 5.0
2019-02-16 21:56:59,289 : Image to text: 23.6, 58.0, 73.1, 4.0
2019-02-16 21:56:59,621 : Text to Image: 19.76, 51.28, 67.4, 5.0
2019-02-16 21:56:59,621 : Dev mean Text to Image: 19.236, 50.864000000000004, 68.108, 5.0
2019-02-16 21:56:59,621 : Dev mean Image to text: 22.840000000000003, 56.199999999999996, 72.66, 4.3999999999999995
2019-02-16 21:56:59,621 : start epoch
2019-02-16 21:57:42,441 : samples : 64000
2019-02-16 21:57:52,943 : Image to text: 7.8, 23.74, 36.3, 20.0
2019-02-16 21:58:00,547 : Text to Image: 6.792, 21.048, 32.164, 24.0
2019-02-16 21:58:43,595 : samples : 128000
2019-02-16 21:58:54,078 : Image to text: 7.98, 23.04, 35.5, 20.0
2019-02-16 21:59:01,606 : Text to Image: 6.556, 21.056, 32.024, 24.0
2019-02-16 21:59:44,327 : samples : 192000
2019-02-16 21:59:54,791 : Image to text: 7.6, 24.2, 36.92, 19.0
2019-02-16 22:00:02,300 : Text to Image: 7.072, 21.952, 33.24, 23.0
2019-02-16 22:00:45,472 : samples : 256000
2019-02-16 22:00:55,976 : Image to text: 7.92, 24.52, 36.4, 19.0
2019-02-16 22:01:03,584 : Text to Image: 6.716, 21.572, 33.004, 23.0
2019-02-16 22:01:46,713 : samples : 320000
2019-02-16 22:01:57,235 : Image to text: 7.12, 23.08, 35.0, 21.0
2019-02-16 22:02:04,800 : Text to Image: 6.3, 20.844, 31.692, 24.0
2019-02-16 22:02:47,939 : samples : 384000
2019-02-16 22:02:58,465 : Image to text: 7.46, 24.48, 36.58, 19.0
2019-02-16 22:03:06,016 : Text to Image: 7.0, 21.72, 32.752, 23.0
2019-02-16 22:03:48,845 : samples : 448000
2019-02-16 22:03:59,320 : Image to text: 7.92, 24.42, 36.78, 19.0
2019-02-16 22:04:06,872 : Text to Image: 6.676, 20.78, 31.832, 24.0
2019-02-16 22:04:49,478 : samples : 512000
2019-02-16 22:04:59,960 : Image to text: 7.78, 24.3, 36.64, 19.0
2019-02-16 22:05:07,436 : Text to Image: 6.76, 21.872, 32.992, 23.0
2019-02-16 22:05:43,876 : Epoch 9 finished
2019-02-16 22:05:44,317 : Image to text: 23.2, 54.4, 72.1, 5.0
2019-02-16 22:05:44,661 : Text to Image: 19.4, 50.74, 68.46, 5.0
2019-02-16 22:05:45,099 : Image to text: 21.6, 53.9, 70.2, 5.0
2019-02-16 22:05:45,442 : Text to Image: 19.2, 50.44, 67.52, 5.0
2019-02-16 22:05:45,906 : Image to text: 22.2, 56.2, 70.8, 4.0
2019-02-16 22:05:46,250 : Text to Image: 19.2, 50.2, 67.76, 5.0
2019-02-16 22:05:46,697 : Image to text: 23.7, 56.2, 73.2, 4.0
2019-02-16 22:05:47,079 : Text to Image: 18.56, 50.16, 67.2, 5.0
2019-02-16 22:05:47,528 : Image to text: 21.4, 54.8, 72.5, 5.0
2019-02-16 22:05:47,865 : Text to Image: 19.58, 50.78, 67.14, 5.0
2019-02-16 22:05:47,865 : Dev mean Text to Image: 19.188, 50.46399999999999, 67.616, 5.0
2019-02-16 22:05:47,865 : Dev mean Image to text: 22.42, 55.1, 71.76, 4.6
2019-02-16 22:05:47,865 : start epoch
2019-02-16 22:06:31,119 : samples : 64000
2019-02-16 22:06:41,596 : Image to text: 7.56, 25.12, 37.22, 19.0
2019-02-16 22:06:49,182 : Text to Image: 6.824, 22.072, 32.9, 23.0
2019-02-16 22:07:31,616 : samples : 128000
2019-02-16 22:07:42,137 : Image to text: 7.96, 24.1, 36.66, 19.0
2019-02-16 22:07:49,684 : Text to Image: 7.016, 22.236, 33.54, 23.0
2019-02-16 22:08:32,219 : samples : 192000
2019-02-16 22:08:42,715 : Image to text: 7.66, 24.94, 36.68, 19.0
2019-02-16 22:08:50,256 : Text to Image: 6.892, 22.004, 33.232, 22.0
2019-02-16 22:09:35,266 : samples : 256000
2019-02-16 22:09:47,711 : Image to text: 7.82, 24.12, 36.46, 19.0
2019-02-16 22:09:56,306 : Text to Image: 6.708, 21.132, 32.42, 23.0
2019-02-16 22:10:42,086 : samples : 320000
2019-02-16 22:10:52,671 : Image to text: 7.9, 23.32, 36.08, 19.0
2019-02-16 22:11:00,196 : Text to Image: 6.804, 21.236, 32.184, 23.0
2019-02-16 22:11:43,359 : samples : 384000
2019-02-16 22:11:53,954 : Image to text: 7.86, 24.5, 37.7, 18.0
2019-02-16 22:12:01,531 : Text to Image: 7.072, 21.864, 33.184, 23.0
2019-02-16 22:12:44,690 : samples : 448000
2019-02-16 22:12:55,300 : Image to text: 8.12, 24.1, 36.66, 19.0
2019-02-16 22:13:02,837 : Text to Image: 6.848, 21.496, 32.508, 24.0
2019-02-16 22:13:45,286 : samples : 512000
2019-02-16 22:13:55,779 : Image to text: 8.18, 24.14, 36.8, 19.0
2019-02-16 22:14:03,329 : Text to Image: 6.856, 21.604, 32.836, 23.0
2019-02-16 22:14:39,933 : Epoch 10 finished
2019-02-16 22:14:40,392 : Image to text: 24.5, 56.5, 73.0, 4.0
2019-02-16 22:14:40,735 : Text to Image: 19.74, 51.16, 69.14, 5.0
2019-02-16 22:14:41,179 : Image to text: 20.2, 53.3, 70.8, 5.0
2019-02-16 22:14:41,522 : Text to Image: 18.64, 49.52, 67.22, 6.0
2019-02-16 22:14:41,962 : Image to text: 24.0, 57.4, 71.9, 4.0
2019-02-16 22:14:42,307 : Text to Image: 19.2, 50.48, 67.76, 5.0
2019-02-16 22:14:42,749 : Image to text: 23.3, 56.5, 74.3, 4.0
2019-02-16 22:14:43,093 : Text to Image: 18.68, 50.28, 67.7, 5.0
2019-02-16 22:14:43,540 : Image to text: 21.1, 56.6, 72.4, 4.0
2019-02-16 22:14:43,871 : Text to Image: 19.04, 50.66, 67.22, 5.0
2019-02-16 22:14:43,872 : Dev mean Text to Image: 19.06, 50.419999999999995, 67.80799999999999, 5.2
2019-02-16 22:14:43,872 : Dev mean Image to text: 22.620000000000005, 56.059999999999995, 72.48, 4.2
2019-02-16 22:14:43,872 : start epoch
2019-02-16 22:15:26,676 : samples : 64000
2019-02-16 22:15:37,110 : Image to text: 8.86, 25.16, 37.64, 18.0
2019-02-16 22:15:44,667 : Text to Image: 7.196, 22.392, 33.716, 22.0
2019-02-16 22:16:27,744 : samples : 128000
2019-02-16 22:16:38,234 : Image to text: 7.6, 24.34, 36.58, 19.0
2019-02-16 22:16:45,726 : Text to Image: 6.72, 21.524, 32.976, 23.0
2019-02-16 22:17:28,729 : samples : 192000
2019-02-16 22:17:39,258 : Image to text: 8.02, 25.34, 37.56, 18.0
2019-02-16 22:17:46,817 : Text to Image: 7.012, 21.7, 33.344, 23.0
2019-02-16 22:18:29,384 : samples : 256000
2019-02-16 22:18:39,858 : Image to text: 8.52, 25.8, 38.32, 18.0
2019-02-16 22:18:47,363 : Text to Image: 7.256, 22.596, 34.08, 22.0
2019-02-16 22:19:30,339 : samples : 320000
2019-02-16 22:19:40,932 : Image to text: 8.64, 25.78, 38.12, 18.0
2019-02-16 22:19:48,455 : Text to Image: 7.16, 22.292, 33.7, 22.0
2019-02-16 22:20:31,467 : samples : 384000
2019-02-16 22:20:41,995 : Image to text: 9.08, 26.06, 38.84, 17.0
2019-02-16 22:20:49,526 : Text to Image: 7.116, 22.372, 33.42, 22.0
2019-02-16 22:21:32,666 : samples : 448000
2019-02-16 22:21:43,169 : Image to text: 8.06, 24.9, 37.62, 18.0
2019-02-16 22:21:50,762 : Text to Image: 6.748, 22.024, 33.148, 23.0
2019-02-16 22:22:33,017 : samples : 512000
2019-02-16 22:22:43,540 : Image to text: 7.8, 25.5, 37.54, 19.0
2019-02-16 22:22:51,061 : Text to Image: 7.152, 22.144, 33.28, 23.0
2019-02-16 22:23:27,630 : Epoch 11 finished
2019-02-16 22:23:28,079 : Image to text: 24.5, 57.1, 71.6, 4.0
2019-02-16 22:23:28,424 : Text to Image: 18.92, 51.42, 69.5, 5.0
2019-02-16 22:23:28,867 : Image to text: 19.8, 54.3, 71.7, 5.0
2019-02-16 22:23:29,209 : Text to Image: 18.8, 50.52, 68.2, 5.0
2019-02-16 22:23:29,652 : Image to text: 23.5, 54.8, 70.7, 5.0
2019-02-16 22:23:29,999 : Text to Image: 19.8, 50.56, 68.1, 5.0
2019-02-16 22:23:30,440 : Image to text: 23.0, 53.7, 72.2, 5.0
2019-02-16 22:23:30,772 : Text to Image: 18.64, 50.92, 67.62, 5.0
2019-02-16 22:23:31,219 : Image to text: 22.4, 56.9, 71.9, 5.0
2019-02-16 22:23:31,549 : Text to Image: 19.28, 51.22, 67.28, 5.0
2019-02-16 22:23:31,549 : Dev mean Text to Image: 19.088, 50.928, 68.14, 5.0
2019-02-16 22:23:31,549 : Dev mean Image to text: 22.639999999999997, 55.36, 71.61999999999999, 4.8
2019-02-16 22:23:35,498 : 
Test scores | Image to text:             23.22, 55.6, 70.96000000000001, 4.6
2019-02-16 22:23:35,499 : Test scores | Text to image:             19.18, 50.664, 68.06400000000001, 5.2

2019-02-16 22:23:35,638 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-16 22:23:36,021 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-16 22:23:36,756 : loading BERT model bert-base-uncased
2019-02-16 22:23:36,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:23:36,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:23:36,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpx5d4u5x8
2019-02-16 22:23:39,315 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:23:40,829 : Computing embeddings for train/dev/test
2019-02-16 22:25:16,184 : Computed embeddings
2019-02-16 22:25:16,184 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:26:11,963 : [('reg:1e-05', 58.81), ('reg:0.0001', 57.44), ('reg:0.001', 52.55), ('reg:0.01', 44.88)]
2019-02-16 22:26:11,963 : Validation : best param found is reg = 1e-05 with score             58.81
2019-02-16 22:26:11,963 : Evaluating...
2019-02-16 22:26:24,906 : 
Dev acc : 58.8 Test acc : 57.9 for LENGTH classification

2019-02-16 22:26:24,907 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-16 22:26:25,385 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-16 22:26:25,431 : loading BERT model bert-base-uncased
2019-02-16 22:26:25,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:26:25,462 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:26:25,462 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpebt61rrn
2019-02-16 22:26:27,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:26:29,362 : Computing embeddings for train/dev/test
2019-02-16 22:28:02,290 : Computed embeddings
2019-02-16 22:28:02,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:28:49,109 : [('reg:1e-05', 26.4), ('reg:0.0001', 8.7), ('reg:0.001', 1.44), ('reg:0.01', 0.57)]
2019-02-16 22:28:49,110 : Validation : best param found is reg = 1e-05 with score             26.4
2019-02-16 22:28:49,110 : Evaluating...
2019-02-16 22:29:03,894 : 
Dev acc : 26.4 Test acc : 25.8 for WORDCONTENT classification

2019-02-16 22:29:03,896 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-16 22:29:04,330 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-16 22:29:04,404 : loading BERT model bert-base-uncased
2019-02-16 22:29:04,404 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:29:04,433 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:29:04,433 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9dyz8usy
2019-02-16 22:29:06,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:29:08,414 : Computing embeddings for train/dev/test
2019-02-16 22:30:31,921 : Computed embeddings
2019-02-16 22:30:31,921 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:31:09,880 : [('reg:1e-05', 25.97), ('reg:0.0001', 25.45), ('reg:0.001', 26.01), ('reg:0.01', 22.19)]
2019-02-16 22:31:09,880 : Validation : best param found is reg = 0.001 with score             26.01
2019-02-16 22:31:09,880 : Evaluating...
2019-02-16 22:31:21,828 : 
Dev acc : 26.0 Test acc : 25.6 for DEPTH classification

2019-02-16 22:31:21,829 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-16 22:31:22,234 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-16 22:31:22,303 : loading BERT model bert-base-uncased
2019-02-16 22:31:22,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:31:22,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:31:22,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpphuwum9p
2019-02-16 22:31:24,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:31:26,327 : Computing embeddings for train/dev/test
2019-02-16 22:32:44,346 : Computed embeddings
2019-02-16 22:32:44,347 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:33:30,628 : [('reg:1e-05', 74.5), ('reg:0.0001', 73.63), ('reg:0.001', 69.38), ('reg:0.01', 58.32)]
2019-02-16 22:33:30,628 : Validation : best param found is reg = 1e-05 with score             74.5
2019-02-16 22:33:30,628 : Evaluating...
2019-02-16 22:33:40,439 : 
Dev acc : 74.5 Test acc : 74.4 for TOPCONSTITUENTS classification

2019-02-16 22:33:40,440 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-16 22:33:40,837 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-16 22:33:40,906 : loading BERT model bert-base-uncased
2019-02-16 22:33:40,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:33:40,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:33:40,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2uibjbmy
2019-02-16 22:33:43,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:33:44,938 : Computing embeddings for train/dev/test
2019-02-16 22:35:08,794 : Computed embeddings
2019-02-16 22:35:08,794 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:35:55,986 : [('reg:1e-05', 85.53), ('reg:0.0001', 85.45), ('reg:0.001', 85.35), ('reg:0.01', 84.28)]
2019-02-16 22:35:55,986 : Validation : best param found is reg = 1e-05 with score             85.53
2019-02-16 22:35:55,987 : Evaluating...
2019-02-16 22:36:09,604 : 
Dev acc : 85.5 Test acc : 84.8 for BIGRAMSHIFT classification

2019-02-16 22:36:09,605 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-16 22:36:10,045 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-16 22:36:10,122 : loading BERT model bert-base-uncased
2019-02-16 22:36:10,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:36:10,265 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:36:10,265 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyj4ie_b3
2019-02-16 22:36:12,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:36:14,287 : Computing embeddings for train/dev/test
2019-02-16 22:37:36,754 : Computed embeddings
2019-02-16 22:37:36,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:38:19,381 : [('reg:1e-05', 89.16), ('reg:0.0001', 88.98), ('reg:0.001', 88.3), ('reg:0.01', 88.15)]
2019-02-16 22:38:19,381 : Validation : best param found is reg = 1e-05 with score             89.16
2019-02-16 22:38:19,382 : Evaluating...
2019-02-16 22:38:29,528 : 
Dev acc : 89.2 Test acc : 88.7 for TENSE classification

2019-02-16 22:38:29,529 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-16 22:38:30,177 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-16 22:38:30,250 : loading BERT model bert-base-uncased
2019-02-16 22:38:30,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:38:30,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:38:30,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzyp_ogfg
2019-02-16 22:38:32,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:38:34,302 : Computing embeddings for train/dev/test
2019-02-16 22:40:01,745 : Computed embeddings
2019-02-16 22:40:01,745 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:40:42,865 : [('reg:1e-05', 84.89), ('reg:0.0001', 84.94), ('reg:0.001', 85.01), ('reg:0.01', 81.39)]
2019-02-16 22:40:42,865 : Validation : best param found is reg = 0.001 with score             85.01
2019-02-16 22:40:42,865 : Evaluating...
2019-02-16 22:40:53,085 : 
Dev acc : 85.0 Test acc : 85.4 for SUBJNUMBER classification

2019-02-16 22:40:53,086 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-16 22:40:53,513 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-16 22:40:53,582 : loading BERT model bert-base-uncased
2019-02-16 22:40:53,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:40:53,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:40:53,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwjufe8fn
2019-02-16 22:40:56,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:40:57,681 : Computing embeddings for train/dev/test
2019-02-16 22:42:23,396 : Computed embeddings
2019-02-16 22:42:23,396 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:43:02,627 : [('reg:1e-05', 72.96), ('reg:0.0001', 72.83), ('reg:0.001', 72.28), ('reg:0.01', 62.65)]
2019-02-16 22:43:02,627 : Validation : best param found is reg = 1e-05 with score             72.96
2019-02-16 22:43:02,627 : Evaluating...
2019-02-16 22:43:12,106 : 
Dev acc : 73.0 Test acc : 73.5 for OBJNUMBER classification

2019-02-16 22:43:12,107 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-16 22:43:12,729 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-16 22:43:12,807 : loading BERT model bert-base-uncased
2019-02-16 22:43:12,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:43:12,841 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:43:12,841 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpud3btlj1
2019-02-16 22:43:15,311 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:43:16,794 : Computing embeddings for train/dev/test
2019-02-16 22:44:59,606 : Computed embeddings
2019-02-16 22:44:59,606 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:45:56,021 : [('reg:1e-05', 60.7), ('reg:0.0001', 60.78), ('reg:0.001', 60.92), ('reg:0.01', 57.89)]
2019-02-16 22:45:56,021 : Validation : best param found is reg = 0.001 with score             60.92
2019-02-16 22:45:56,021 : Evaluating...
2019-02-16 22:46:09,681 : 
Dev acc : 60.9 Test acc : 60.3 for ODDMANOUT classification

2019-02-16 22:46:09,682 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-16 22:46:10,275 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-16 22:46:10,353 : loading BERT model bert-base-uncased
2019-02-16 22:46:10,353 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:46:10,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:46:10,385 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptv46x9ur
2019-02-16 22:46:12,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:46:14,374 : Computing embeddings for train/dev/test
2019-02-16 22:47:51,024 : Computed embeddings
2019-02-16 22:47:51,024 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:48:39,572 : [('reg:1e-05', 71.03), ('reg:0.0001', 71.01), ('reg:0.001', 70.8), ('reg:0.01', 61.52)]
2019-02-16 22:48:39,572 : Validation : best param found is reg = 1e-05 with score             71.03
2019-02-16 22:48:39,572 : Evaluating...
2019-02-16 22:48:53,290 : 
Dev acc : 71.0 Test acc : 70.5 for COORDINATIONINVERSION classification

2019-02-16 22:48:53,292 : total results: {'STS12': {'MSRpar': {'pearson': (0.2409490836819592, 2.2871015986600694e-11), 'spearman': SpearmanrResult(correlation=0.2846612115178702, pvalue=1.898996325176434e-15), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.02628816737023693, 0.47223054890475913), 'spearman': SpearmanrResult(correlation=0.027820780704932366, pvalue=0.44678853962086973), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4151831368788611, 1.4921344239578373e-20), 'spearman': SpearmanrResult(correlation=0.4968180476299375, pvalue=5.533505946043748e-30), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3317446544465511, 1.0032063632058529e-20), 'spearman': SpearmanrResult(correlation=0.33772936481588484, pvalue=1.82345157177117e-21), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5211017454362185, 3.7083237693002093e-29), 'spearman': SpearmanrResult(correlation=0.4445808091407956, pvalue=9.247175079631886e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29653809061467057, 'wmean': 0.26006847951259765}, 'spearman': {'mean': 0.31832204276188414, 'wmean': 0.2873506256397472}}}, 'STS13': {'FNWN': {'pearson': (0.08032809067351498, 0.27186438641853555), 'spearman': SpearmanrResult(correlation=0.09358448529240816, pvalue=0.20024574568481457), 'nsamples': 189}, 'headlines': {'pearson': (0.43380046472807676, 9.172066104274262e-36), 'spearman': SpearmanrResult(correlation=0.4199776149214178, pvalue=2.0920561011328347e-33), 'nsamples': 750}, 'OnWN': {'pearson': (0.13251575039350869, 0.0016576021076611669), 'spearman': SpearmanrResult(correlation=0.12878020164661208, pvalue=0.002242013210975531), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.21554810193170015, 'wmean': 0.27658246243607354}, 'spearman': {'mean': 0.21411410062014602, 'wmean': 0.26994424802338524}}}, 'STS14': {'deft-forum': {'pearson': (-0.10685918748572, 0.023389280452958412), 'spearman': SpearmanrResult(correlation=-0.10249423678121661, pvalue=0.029711762598810927), 'nsamples': 450}, 'deft-news': {'pearson': (0.39667887058499035, 9.543606762988496e-13), 'spearman': SpearmanrResult(correlation=0.43498954434205156, pvalue=2.798140144940878e-15), 'nsamples': 300}, 'headlines': {'pearson': (0.4195496061355736, 2.465226360079837e-33), 'spearman': SpearmanrResult(correlation=0.39224966580162096, pvalue=5.389942702717733e-29), 'nsamples': 750}, 'images': {'pearson': (0.1291165177557772, 0.0003926841573323477), 'spearman': SpearmanrResult(correlation=0.13453411132166, pvalue=0.00021985775769272592), 'nsamples': 750}, 'OnWN': {'pearson': (0.335345402595391, 3.612574624985909e-21), 'spearman': SpearmanrResult(correlation=0.3326309377683195, pvalue=7.811864719090602e-21), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4992692162421108, 1.5695201451939058e-48), 'spearman': SpearmanrResult(correlation=0.45535959493842076, pvalue=1.1489133012034523e-39), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.27885007097135384, 'wmean': 0.29556735569428333}, 'spearman': {'mean': 0.27454493623180937, 'wmean': 0.28545471709962233}}}, 'STS15': {'answers-forums': {'pearson': (0.29337201699588356, 7.03652495617096e-09), 'spearman': SpearmanrResult(correlation=0.28949153937233374, pvalue=1.1298507649042734e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.4097966082498831, 9.74103594340724e-32), 'spearman': SpearmanrResult(correlation=0.4134229297555132, pvalue=2.517925299989329e-32), 'nsamples': 750}, 'belief': {'pearson': (0.4060567288081899, 2.5556036195261646e-16), 'spearman': SpearmanrResult(correlation=0.4135104980540808, pvalue=6.385118508323894e-17), 'nsamples': 375}, 'headlines': {'pearson': (0.4735922859510122, 3.4284973795316332e-43), 'spearman': SpearmanrResult(correlation=0.46613796485881576, pvalue=1.0048894052293287e-41), 'nsamples': 750}, 'images': {'pearson': (0.18761054963263032, 2.274490856210665e-07), 'spearman': SpearmanrResult(correlation=0.194526897078301, pvalue=7.881074129186982e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.35408563792751985, 'wmean': 0.3551784541838906}, 'spearman': {'mean': 0.3554179658238089, 'wmean': 0.35639720260145924}}}, 'STS16': {'answer-answer': {'pearson': (0.29529295482966733, 1.6659059872639162e-06), 'spearman': SpearmanrResult(correlation=0.3039807806397207, pvalue=7.888277338873299e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.5583644502771391, 8.212922828870873e-22), 'spearman': SpearmanrResult(correlation=0.5771491099812783, pvalue=1.6287315421830653e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6251662040481382, 2.4186294868600877e-26), 'spearman': SpearmanrResult(correlation=0.6505106618527603, pvalue=4.6415499502531147e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.7213867140765519, 1.7230696958821009e-40), 'spearman': SpearmanrResult(correlation=0.7442319743813697, pvalue=2.630044927844188e-44), 'nsamples': 244}, 'question-question': {'pearson': (-0.007623951381694058, 0.9127593208688329), 'spearman': SpearmanrResult(correlation=0.03777650171578507, pvalue=0.5870973739543479), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.43851727436996046, 'wmean': 0.4487776880039794}, 'spearman': {'mean': 0.46272980571418276, 'wmean': 0.4721977989052407}}}, 'MR': {'devacc': 78.63, 'acc': 79.3, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.69, 'acc': 84.21, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.06, 'acc': 87.27, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.08, 'acc': 94.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.86, 'acc': 82.04, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.14, 'acc': 42.04, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.77, 'acc': 88.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.98, 'acc': 69.68, 'f1': 77.75, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 69.92, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6613608896247289, 'pearson': 0.6786826123360956, 'spearman': 0.6358473936506438, 'mse': 0.5519720149382482, 'yhat': array([2.51058448, 4.01781434, 1.87656338, ..., 2.70772311, 4.41204237,        4.08646382]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4242663873189794, 'pearson': 0.4290266265485483, 'spearman': 0.4301109170742621, 'mse': 1.9940691059024633, 'yhat': array([4.38696312, 2.19003039, 2.31827114, ..., 3.49858737, 3.29488146,        3.35040229]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.45, 'acc': 59.06, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 289.908, 'acc': [(23.22, 55.6, 70.96000000000001, 4.6), (19.18, 50.664, 68.06400000000001, 5.2)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 58.81, 'acc': 57.91, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 26.4, 'acc': 25.76, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.01, 'acc': 25.56, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 74.5, 'acc': 74.39, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.53, 'acc': 84.84, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.16, 'acc': 88.69, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.01, 'acc': 85.38, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.96, 'acc': 73.55, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.92, 'acc': 60.29, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.03, 'acc': 70.49, 'ndev': 10002, 'ntest': 10002}}
2019-02-16 22:48:53,293 : STS12 p=0.2601, STS12 s=0.2874, STS13 p=0.2766, STS13 s=0.2699, STS14 p=0.2956, STS14 s=0.2855, STS15 p=0.3552, STS15 s=0.3564, STS 16 p=0.4488, STS16 s=0.4722, STS B p=0.4290, STS B s=0.4301, STS B m=1.9941, SICK-R p=0.6787, SICK-R s=0.6358, SICK-P m=0.5520
2019-02-16 22:48:53,293 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-16 22:48:53,293 : 0.2601,0.2874,0.2766,0.2699,0.2956,0.2855,0.3552,0.3564,0.4488,0.4722,0.4290,0.4301,1.9941,0.6787,0.6358,0.5520
2019-02-16 22:48:53,293 : MR=79.30, CR=84.21, SUBJ=94.85, MPQA=87.27, SST-B=82.04, SST-F=42.04, TREC=88.40, SICK-E=69.92, SNLI=59.06, MRPC=69.68, MRPC f=77.75
2019-02-16 22:48:53,293 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-16 22:48:53,293 : 79.30,84.21,94.85,87.27,82.04,42.04,88.40,69.92,59.06,69.68,77.75
2019-02-16 22:48:53,293 : COCO r1i2t=23.22, COCO r5i2t=55.60, COCO r10i2t=70.96, COCO medr_i2t=4.60, COCO r1t2i=19.18, COCO r5t2i=50.66, COCO r10t2i=68.06, COCO medr_t2i=5.20
2019-02-16 22:48:53,293 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-16 22:48:53,293 : 23.22,55.60,70.96,4.60,19.18,50.66,68.06,5.20
2019-02-16 22:48:53,293 : SentLen=57.91, WC=25.76, TreeDepth=25.56, TopConst=74.39, BShift=84.84, Tense=88.69, SubjNum=85.38, ObjNum=73.55, SOMO=60.29, CoordInv=70.49, average=64.69
2019-02-16 22:48:53,293 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-16 22:48:53,293 : 57.91,25.76,25.56,74.39,84.84,88.69,85.38,73.55,60.29,70.49,64.69
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,293 : layer 11
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,293 : ********************************************************************************
2019-02-16 22:48:53,393 : ***** Transfer task : STS12 *****


2019-02-16 22:48:53,406 : loading BERT model bert-base-uncased
2019-02-16 22:48:53,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:48:53,427 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:48:53,427 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprfvx1i8i
2019-02-16 22:48:55,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:48:59,256 : MSRpar : pearson = 0.2490, spearman = 0.2924
2019-02-16 22:49:00,031 : MSRvid : pearson = -0.0172, spearman = 0.0323
2019-02-16 22:49:00,664 : SMTeuroparl : pearson = 0.4101, spearman = 0.4875
2019-02-16 22:49:01,845 : surprise.OnWN : pearson = 0.3413, spearman = 0.3420
2019-02-16 22:49:02,487 : surprise.SMTnews : pearson = 0.5047, spearman = 0.4478
2019-02-16 22:49:02,487 : ALL (weighted average) : Pearson = 0.2636,             Spearman = 0.2904
2019-02-16 22:49:02,487 : ALL (average) : Pearson = 0.2976,             Spearman = 0.3204

2019-02-16 22:49:02,487 : ***** Transfer task : STS13 (-SMT) *****


2019-02-16 22:49:02,499 : loading BERT model bert-base-uncased
2019-02-16 22:49:02,499 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:49:02,519 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:49:02,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptggyt_cc
2019-02-16 22:49:05,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:49:07,188 : FNWN : pearson = 0.0624, spearman = 0.0766
2019-02-16 22:49:08,054 : headlines : pearson = 0.4510, spearman = 0.4318
2019-02-16 22:49:08,743 : OnWN : pearson = 0.1385, spearman = 0.1465
2019-02-16 22:49:08,743 : ALL (weighted average) : Pearson = 0.2851,             Spearman = 0.2803
2019-02-16 22:49:08,743 : ALL (average) : Pearson = 0.2173,             Spearman = 0.2183

2019-02-16 22:49:08,743 : ***** Transfer task : STS14 *****


2019-02-16 22:49:08,762 : loading BERT model bert-base-uncased
2019-02-16 22:49:08,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:49:08,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:49:08,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpke2yovxa
2019-02-16 22:49:11,305 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:49:13,407 : deft-forum : pearson = -0.0824, spearman = -0.0821
2019-02-16 22:49:14,123 : deft-news : pearson = 0.4267, spearman = 0.4553
2019-02-16 22:49:15,160 : headlines : pearson = 0.4271, spearman = 0.4005
2019-02-16 22:49:16,132 : images : pearson = 0.1442, spearman = 0.1449
2019-02-16 22:49:17,119 : OnWN : pearson = 0.3534, spearman = 0.3530
2019-02-16 22:49:18,398 : tweet-news : pearson = 0.4855, spearman = 0.4533
2019-02-16 22:49:18,398 : ALL (weighted average) : Pearson = 0.3063,             Spearman = 0.2969
2019-02-16 22:49:18,398 : ALL (average) : Pearson = 0.2924,             Spearman = 0.2875

2019-02-16 22:49:18,398 : ***** Transfer task : STS15 *****


2019-02-16 22:49:18,432 : loading BERT model bert-base-uncased
2019-02-16 22:49:18,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:49:18,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:49:18,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8pdu8kgf
2019-02-16 22:49:20,896 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:49:23,244 : answers-forums : pearson = 0.3020, spearman = 0.2921
2019-02-16 22:49:24,162 : answers-students : pearson = 0.4124, spearman = 0.4142
2019-02-16 22:49:25,041 : belief : pearson = 0.4281, spearman = 0.4267
2019-02-16 22:49:26,059 : headlines : pearson = 0.4904, spearman = 0.4813
2019-02-16 22:49:27,010 : images : pearson = 0.2079, spearman = 0.2104
2019-02-16 22:49:27,010 : ALL (weighted average) : Pearson = 0.3689,             Spearman = 0.3663
2019-02-16 22:49:27,010 : ALL (average) : Pearson = 0.3682,             Spearman = 0.3649

2019-02-16 22:49:27,010 : ***** Transfer task : STS16 *****


2019-02-16 22:49:27,086 : loading BERT model bert-base-uncased
2019-02-16 22:49:27,086 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:49:27,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:49:27,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplyssuv0k
2019-02-16 22:49:29,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:49:31,516 : answer-answer : pearson = 0.3400, spearman = 0.3416
2019-02-16 22:49:31,823 : headlines : pearson = 0.5665, spearman = 0.5728
2019-02-16 22:49:32,183 : plagiarism : pearson = 0.6225, spearman = 0.6526
2019-02-16 22:49:32,818 : postediting : pearson = 0.7281, spearman = 0.7504
2019-02-16 22:49:33,107 : question-question : pearson = 0.0320, spearman = 0.0830
2019-02-16 22:49:33,107 : ALL (weighted average) : Pearson = 0.4679,             Spearman = 0.4890
2019-02-16 22:49:33,107 : ALL (average) : Pearson = 0.4578,             Spearman = 0.4801

2019-02-16 22:49:33,107 : ***** Transfer task : MR *****


2019-02-16 22:49:33,126 : loading BERT model bert-base-uncased
2019-02-16 22:49:33,126 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:49:33,153 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:49:33,154 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7xvxufql
2019-02-16 22:49:35,703 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:49:37,247 : Generating sentence embeddings
2019-02-16 22:49:50,759 : Generated sentence embeddings
2019-02-16 22:49:50,760 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 22:50:07,060 : Best param found at split 1: l2reg = 1e-05                 with score 78.93
2019-02-16 22:50:24,299 : Best param found at split 2: l2reg = 0.01                 with score 78.71
2019-02-16 22:50:41,614 : Best param found at split 3: l2reg = 0.001                 with score 78.83
2019-02-16 22:50:59,727 : Best param found at split 4: l2reg = 0.001                 with score 78.55
2019-02-16 22:51:15,816 : Best param found at split 5: l2reg = 0.0001                 with score 79.52
2019-02-16 22:51:16,848 : Dev acc : 78.91 Test acc : 78.31

2019-02-16 22:51:16,850 : ***** Transfer task : CR *****


2019-02-16 22:51:16,861 : loading BERT model bert-base-uncased
2019-02-16 22:51:16,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:51:16,887 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:51:16,887 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr8aldoji
2019-02-16 22:51:19,398 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:51:20,808 : Generating sentence embeddings
2019-02-16 22:51:24,538 : Generated sentence embeddings
2019-02-16 22:51:24,539 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 22:51:30,702 : Best param found at split 1: l2reg = 1e-05                 with score 85.49
2019-02-16 22:51:36,799 : Best param found at split 2: l2reg = 1e-05                 with score 85.33
2019-02-16 22:51:43,652 : Best param found at split 3: l2reg = 0.001                 with score 86.03
2019-02-16 22:51:49,781 : Best param found at split 4: l2reg = 0.01                 with score 84.74
2019-02-16 22:51:56,169 : Best param found at split 5: l2reg = 1e-05                 with score 85.63
2019-02-16 22:51:56,579 : Dev acc : 85.44 Test acc : 85.09

2019-02-16 22:51:56,579 : ***** Transfer task : MPQA *****


2019-02-16 22:51:56,584 : loading BERT model bert-base-uncased
2019-02-16 22:51:56,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:51:56,607 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:51:56,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptjipz_dd
2019-02-16 22:51:59,091 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:52:00,668 : Generating sentence embeddings
2019-02-16 22:52:04,578 : Generated sentence embeddings
2019-02-16 22:52:04,578 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 22:52:20,658 : Best param found at split 1: l2reg = 0.001                 with score 86.94
2019-02-16 22:52:36,930 : Best param found at split 2: l2reg = 0.01                 with score 87.09
2019-02-16 22:52:54,380 : Best param found at split 3: l2reg = 1e-05                 with score 86.65
2019-02-16 22:53:11,170 : Best param found at split 4: l2reg = 0.01                 with score 87.26
2019-02-16 22:53:28,022 : Best param found at split 5: l2reg = 0.01                 with score 86.95
2019-02-16 22:53:29,178 : Dev acc : 86.98 Test acc : 86.14

2019-02-16 22:53:29,179 : ***** Transfer task : SUBJ *****


2019-02-16 22:53:29,205 : loading BERT model bert-base-uncased
2019-02-16 22:53:29,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:53:29,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:53:29,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpubgyg09q
2019-02-16 22:53:31,748 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:53:33,206 : Generating sentence embeddings
2019-02-16 22:53:46,570 : Generated sentence embeddings
2019-02-16 22:53:46,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-16 22:54:02,144 : Best param found at split 1: l2reg = 0.001                 with score 94.98
2019-02-16 22:54:17,143 : Best param found at split 2: l2reg = 1e-05                 with score 95.06
2019-02-16 22:54:33,153 : Best param found at split 3: l2reg = 0.001                 with score 94.96
2019-02-16 22:54:50,213 : Best param found at split 4: l2reg = 0.001                 with score 95.55
2019-02-16 22:55:05,890 : Best param found at split 5: l2reg = 1e-05                 with score 95.05
2019-02-16 22:55:06,348 : Dev acc : 95.12 Test acc : 94.1

2019-02-16 22:55:06,349 : ***** Transfer task : SST Binary classification *****


2019-02-16 22:55:06,482 : loading BERT model bert-base-uncased
2019-02-16 22:55:06,482 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:55:06,508 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:55:06,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpua1ay3gm
2019-02-16 22:55:09,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:55:10,495 : Computing embedding for train
2019-02-16 22:55:55,923 : Computed train embeddings
2019-02-16 22:55:55,923 : Computing embedding for dev
2019-02-16 22:55:56,851 : Computed dev embeddings
2019-02-16 22:55:56,851 : Computing embedding for test
2019-02-16 22:55:58,895 : Computed test embeddings
2019-02-16 22:55:58,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:56:26,693 : [('reg:1e-05', 83.37), ('reg:0.0001', 83.03), ('reg:0.001', 82.68), ('reg:0.01', 83.03)]
2019-02-16 22:56:26,693 : Validation : best param found is reg = 1e-05 with score             83.37
2019-02-16 22:56:26,693 : Evaluating...
2019-02-16 22:56:34,492 : 
Dev acc : 83.37 Test acc : 83.8 for             SST Binary classification

2019-02-16 22:56:34,493 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-16 22:56:34,554 : loading BERT model bert-base-uncased
2019-02-16 22:56:34,554 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:56:34,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:56:34,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptr5aarci
2019-02-16 22:56:37,093 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:56:38,574 : Computing embedding for train
2019-02-16 22:56:48,131 : Computed train embeddings
2019-02-16 22:56:48,131 : Computing embedding for dev
2019-02-16 22:56:49,370 : Computed dev embeddings
2019-02-16 22:56:49,370 : Computing embedding for test
2019-02-16 22:56:51,773 : Computed test embeddings
2019-02-16 22:56:51,774 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:56:55,921 : [('reg:1e-05', 40.69), ('reg:0.0001', 41.87), ('reg:0.001', 39.6), ('reg:0.01', 38.69)]
2019-02-16 22:56:55,921 : Validation : best param found is reg = 0.0001 with score             41.87
2019-02-16 22:56:55,922 : Evaluating...
2019-02-16 22:56:56,845 : 
Dev acc : 41.87 Test acc : 43.71 for             SST Fine-Grained classification

2019-02-16 22:56:56,846 : ***** Transfer task : TREC *****


2019-02-16 22:56:56,860 : loading BERT model bert-base-uncased
2019-02-16 22:56:56,860 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:56:56,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:56:56,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsg7qh6ib
2019-02-16 22:56:59,372 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:57:04,385 : Computed train embeddings
2019-02-16 22:57:04,648 : Computed test embeddings
2019-02-16 22:57:04,648 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 22:57:15,605 : [('reg:1e-05', 80.77), ('reg:0.0001', 80.65), ('reg:0.001', 79.56), ('reg:0.01', 72.03)]
2019-02-16 22:57:15,606 : Cross-validation : best param found is reg = 1e-05             with score 80.77
2019-02-16 22:57:15,606 : Evaluating...
2019-02-16 22:57:16,228 : 
Dev acc : 80.77 Test acc : 87.0             for TREC

2019-02-16 22:57:16,229 : ***** Transfer task : MRPC *****


2019-02-16 22:57:16,254 : loading BERT model bert-base-uncased
2019-02-16 22:57:16,254 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:57:16,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:57:16,279 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcf_t44ju
2019-02-16 22:57:18,773 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:57:20,256 : Computing embedding for train
2019-02-16 22:57:30,523 : Computed train embeddings
2019-02-16 22:57:30,524 : Computing embedding for test
2019-02-16 22:57:34,701 : Computed test embeddings
2019-02-16 22:57:34,719 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-16 22:57:41,903 : [('reg:1e-05', 70.27), ('reg:0.0001', 70.76), ('reg:0.001', 69.87), ('reg:0.01', 70.61)]
2019-02-16 22:57:41,903 : Cross-validation : best param found is reg = 0.0001             with score 70.76
2019-02-16 22:57:41,903 : Evaluating...
2019-02-16 22:57:42,516 : Dev acc : 70.76 Test acc 66.55; Test F1 79.9 for MRPC.

2019-02-16 22:57:42,516 : ***** Transfer task : SICK-Entailment*****


2019-02-16 22:57:42,589 : loading BERT model bert-base-uncased
2019-02-16 22:57:42,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:57:42,611 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:57:42,611 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzl9vs85u
2019-02-16 22:57:45,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:57:46,622 : Computing embedding for train
2019-02-16 22:57:51,786 : Computed train embeddings
2019-02-16 22:57:51,786 : Computing embedding for dev
2019-02-16 22:57:52,468 : Computed dev embeddings
2019-02-16 22:57:52,468 : Computing embedding for test
2019-02-16 22:57:57,978 : Computed test embeddings
2019-02-16 22:57:58,008 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 22:58:00,201 : [('reg:1e-05', 71.4), ('reg:0.0001', 72.2), ('reg:0.001', 71.2), ('reg:0.01', 64.4)]
2019-02-16 22:58:00,201 : Validation : best param found is reg = 0.0001 with score             72.2
2019-02-16 22:58:00,201 : Evaluating...
2019-02-16 22:58:00,678 : 
Dev acc : 72.2 Test acc : 71.4 for                        SICK entailment

2019-02-16 22:58:00,678 : ***** Transfer task : SICK-Relatedness*****


2019-02-16 22:58:00,706 : loading BERT model bert-base-uncased
2019-02-16 22:58:00,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:58:00,767 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:58:00,768 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa03slcwr
2019-02-16 22:58:03,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:58:04,716 : Computing embedding for train
2019-02-16 22:58:09,890 : Computed train embeddings
2019-02-16 22:58:09,890 : Computing embedding for dev
2019-02-16 22:58:10,585 : Computed dev embeddings
2019-02-16 22:58:10,585 : Computing embedding for test
2019-02-16 22:58:16,239 : Computed test embeddings
2019-02-16 22:58:38,906 : Dev : Pearson 0.6614904315726032
2019-02-16 22:58:38,907 : Test : Pearson 0.6806312749941023 Spearman 0.6284273875003842 MSE 0.5579650792507292                        for SICK Relatedness

2019-02-16 22:58:38,908 : 

***** Transfer task : STSBenchmark*****


2019-02-16 22:58:38,951 : loading BERT model bert-base-uncased
2019-02-16 22:58:38,951 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:58:38,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:58:38,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr9elsm2h
2019-02-16 22:58:41,493 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:58:43,016 : Computing embedding for train
2019-02-16 22:58:51,365 : Computed train embeddings
2019-02-16 22:58:51,365 : Computing embedding for dev
2019-02-16 22:58:53,822 : Computed dev embeddings
2019-02-16 22:58:53,823 : Computing embedding for test
2019-02-16 22:58:55,831 : Computed test embeddings
2019-02-16 22:59:23,291 : Dev : Pearson 0.43988369008862993
2019-02-16 22:59:23,292 : Test : Pearson 0.41027518369570704 Spearman 0.4093809362927808 MSE 2.0883983920712836                        for SICK Relatedness

2019-02-16 22:59:23,292 : ***** Transfer task : SNLI Entailment*****


2019-02-16 22:59:28,282 : loading BERT model bert-base-uncased
2019-02-16 22:59:28,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 22:59:28,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 22:59:28,415 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpajrn3oia
2019-02-16 22:59:30,919 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 22:59:32,551 : PROGRESS (encoding): 0.00%
2019-02-16 23:00:50,975 : PROGRESS (encoding): 14.56%
2019-02-16 23:02:21,911 : PROGRESS (encoding): 29.12%
2019-02-16 23:03:50,283 : PROGRESS (encoding): 43.69%
2019-02-16 23:05:24,771 : PROGRESS (encoding): 58.25%
2019-02-16 23:07:10,082 : PROGRESS (encoding): 72.81%
2019-02-16 23:08:53,362 : PROGRESS (encoding): 87.37%
2019-02-16 23:10:43,943 : PROGRESS (encoding): 0.00%
2019-02-16 23:10:57,317 : PROGRESS (encoding): 0.00%
2019-02-16 23:11:10,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-16 23:11:52,338 : [('reg:1e-09', 62.7)]
2019-02-16 23:11:52,338 : Validation : best param found is reg = 1e-09 with score             62.7
2019-02-16 23:11:52,338 : Evaluating...
2019-02-16 23:12:33,733 : Dev acc : 62.7 Test acc : 62.05 for SNLI

2019-02-16 23:12:33,733 : ***** Transfer task: Image Caption Retrieval *****


2019-02-16 23:12:43,111 : loading BERT model bert-base-uncased
2019-02-16 23:12:43,112 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-16 23:12:43,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-16 23:12:43,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfx6zfb12
2019-02-16 23:12:45,617 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-16 23:12:47,060 : Computing embedding for train
2019-02-16 23:20:21,180 : Computed train embeddings
2019-02-16 23:20:21,180 : Computing embedding for dev
2019-02-16 23:20:40,567 : Computed dev embeddings
2019-02-16 23:20:40,567 : Computing embedding for test
2019-02-16 23:21:00,498 : Computed test embeddings
2019-02-16 23:21:00,514 : prepare data
2019-02-16 23:21:00,578 : start epoch
2019-02-16 23:21:43,880 : samples : 64000
2019-02-16 23:21:54,429 : Image to text: 3.34, 12.24, 20.54, 44.0
2019-02-16 23:22:01,968 : Text to Image: 2.78, 10.724, 18.04, 54.0
2019-02-16 23:22:44,358 : samples : 128000
2019-02-16 23:22:54,827 : Image to text: 4.14, 14.48, 23.2, 41.0
2019-02-16 23:23:02,351 : Text to Image: 3.308, 12.216, 19.84, 49.0
2019-02-16 23:23:45,654 : samples : 192000
2019-02-16 23:23:56,111 : Image to text: 4.56, 16.44, 25.86, 34.0
2019-02-16 23:24:03,658 : Text to Image: 3.756, 13.432, 21.844, 42.0
2019-02-16 23:24:46,754 : samples : 256000
2019-02-16 23:24:57,221 : Image to text: 4.88, 16.68, 26.68, 32.0
2019-02-16 23:25:04,772 : Text to Image: 3.904, 13.868, 22.592, 40.0
2019-02-16 23:25:47,502 : samples : 320000
2019-02-16 23:25:57,994 : Image to text: 5.0, 16.7, 26.54, 32.0
2019-02-16 23:26:05,554 : Text to Image: 4.156, 14.944, 23.74, 38.0
2019-02-16 23:26:48,734 : samples : 384000
2019-02-16 23:26:59,298 : Image to text: 5.28, 17.32, 27.64, 29.0
2019-02-16 23:27:06,759 : Text to Image: 4.268, 15.24, 24.64, 36.0
2019-02-16 23:27:50,022 : samples : 448000
2019-02-16 23:28:00,278 : Image to text: 5.62, 18.52, 28.74, 29.0
2019-02-16 23:28:07,698 : Text to Image: 4.444, 15.908, 25.072, 35.0
2019-02-16 23:28:50,570 : samples : 512000
2019-02-16 23:29:00,845 : Image to text: 5.74, 19.0, 28.68, 28.0
2019-02-16 23:29:08,291 : Text to Image: 5.208, 17.104, 26.48, 33.0
2019-02-16 23:29:44,710 : Epoch 1 finished
2019-02-16 23:29:45,148 : Image to text: 16.6, 48.1, 63.7, 6.0
2019-02-16 23:29:45,480 : Text to Image: 14.9, 41.96, 58.82, 7.0
2019-02-16 23:29:45,907 : Image to text: 15.5, 45.2, 62.2, 7.0
2019-02-16 23:29:46,245 : Text to Image: 13.84, 41.44, 59.14, 8.0
2019-02-16 23:29:46,679 : Image to text: 17.3, 47.1, 65.4, 6.0
2019-02-16 23:29:47,010 : Text to Image: 13.98, 41.56, 57.86, 8.0
2019-02-16 23:29:47,451 : Image to text: 17.6, 46.8, 62.4, 6.0
2019-02-16 23:29:47,804 : Text to Image: 14.64, 41.28, 58.26, 8.0
2019-02-16 23:29:48,248 : Image to text: 17.6, 46.9, 64.7, 6.0
2019-02-16 23:29:48,603 : Text to Image: 15.98, 43.3, 59.32, 7.0
2019-02-16 23:29:48,603 : Dev mean Text to Image: 14.668000000000001, 41.908, 58.68000000000001, 7.6
2019-02-16 23:29:48,603 : Dev mean Image to text: 16.919999999999998, 46.82000000000001, 63.68000000000001, 6.2
2019-02-16 23:29:48,603 : start epoch
2019-02-16 23:30:31,904 : samples : 64000
2019-02-16 23:30:42,466 : Image to text: 5.92, 19.88, 29.96, 27.0
2019-02-16 23:30:49,982 : Text to Image: 5.18, 16.584, 26.16, 33.0
2019-02-16 23:31:33,704 : samples : 128000
2019-02-16 23:31:44,189 : Image to text: 6.22, 20.34, 30.24, 25.0
2019-02-16 23:31:51,699 : Text to Image: 5.008, 17.12, 26.976, 31.0
2019-02-16 23:32:34,343 : samples : 192000
2019-02-16 23:32:44,854 : Image to text: 6.12, 20.48, 30.9, 25.0
2019-02-16 23:32:52,369 : Text to Image: 5.2, 17.468, 27.42, 31.0
2019-02-16 23:33:35,736 : samples : 256000
2019-02-16 23:33:46,256 : Image to text: 6.24, 20.16, 30.54, 26.0
2019-02-16 23:33:53,785 : Text to Image: 5.552, 18.336, 28.124, 30.0
2019-02-16 23:34:36,613 : samples : 320000
2019-02-16 23:34:47,100 : Image to text: 6.18, 20.42, 31.4, 25.0
2019-02-16 23:34:54,633 : Text to Image: 5.068, 17.5, 27.496, 31.0
2019-02-16 23:35:43,088 : samples : 384000
2019-02-16 23:35:55,959 : Image to text: 5.46, 19.34, 30.56, 26.0
2019-02-16 23:36:04,621 : Text to Image: 5.232, 17.684, 27.384, 30.0
2019-02-16 23:36:48,168 : samples : 448000
2019-02-16 23:36:58,518 : Image to text: 5.78, 20.24, 31.02, 25.0
2019-02-16 23:37:05,973 : Text to Image: 5.324, 17.848, 27.54, 30.0
2019-02-16 23:37:49,279 : samples : 512000
2019-02-16 23:37:59,592 : Image to text: 6.56, 21.22, 32.5, 23.0
2019-02-16 23:38:07,110 : Text to Image: 5.628, 18.696, 28.544, 29.0
2019-02-16 23:38:44,444 : Epoch 2 finished
2019-02-16 23:38:44,895 : Image to text: 20.1, 49.6, 66.2, 6.0
2019-02-16 23:38:45,227 : Text to Image: 17.04, 44.88, 61.6, 7.0
2019-02-16 23:38:45,657 : Image to text: 16.8, 47.8, 65.4, 6.0
2019-02-16 23:38:45,983 : Text to Image: 15.48, 44.7, 62.26, 7.0
2019-02-16 23:38:46,425 : Image to text: 18.9, 50.2, 66.1, 5.0
2019-02-16 23:38:46,758 : Text to Image: 15.4, 45.3, 61.58, 7.0
2019-02-16 23:38:47,201 : Image to text: 17.1, 49.5, 67.1, 6.0
2019-02-16 23:38:47,536 : Text to Image: 16.44, 44.64, 61.02, 7.0
2019-02-16 23:38:47,992 : Image to text: 20.6, 50.0, 66.0, 5.0
2019-02-16 23:38:48,332 : Text to Image: 16.34, 45.14, 60.92, 7.0
2019-02-16 23:38:48,332 : Dev mean Text to Image: 16.14, 44.932, 61.476, 7.0
2019-02-16 23:38:48,332 : Dev mean Image to text: 18.7, 49.419999999999995, 66.16, 5.6
2019-02-16 23:38:48,333 : start epoch
2019-02-16 23:39:32,158 : samples : 64000
2019-02-16 23:39:42,642 : Image to text: 5.78, 19.72, 30.6, 26.0
2019-02-16 23:39:50,180 : Text to Image: 5.108, 17.46, 27.152, 32.0
2019-02-16 23:40:33,178 : samples : 128000
2019-02-16 23:40:43,685 : Image to text: 5.7, 19.9, 30.96, 25.0
2019-02-16 23:40:51,271 : Text to Image: 5.116, 17.808, 27.536, 30.0
2019-02-16 23:41:34,241 : samples : 192000
2019-02-16 23:41:44,809 : Image to text: 6.48, 22.76, 35.0, 21.0
2019-02-16 23:41:52,304 : Text to Image: 5.724, 19.304, 29.72, 28.0
2019-02-16 23:42:35,149 : samples : 256000
2019-02-16 23:42:45,674 : Image to text: 5.78, 20.92, 31.5, 25.0
2019-02-16 23:42:53,227 : Text to Image: 5.448, 17.896, 27.72, 30.0
2019-02-16 23:43:35,373 : samples : 320000
2019-02-16 23:43:45,882 : Image to text: 6.56, 21.8, 32.84, 23.0
2019-02-16 23:43:53,426 : Text to Image: 5.66, 19.048, 29.308, 28.0
2019-02-16 23:44:35,978 : samples : 384000
2019-02-16 23:44:46,567 : Image to text: 6.5, 22.24, 33.22, 24.0
2019-02-16 23:44:54,076 : Text to Image: 5.888, 18.696, 28.988, 28.0
2019-02-16 23:45:36,378 : samples : 448000
2019-02-16 23:45:46,650 : Image to text: 6.38, 20.68, 32.12, 23.0
2019-02-16 23:45:54,153 : Text to Image: 5.708, 18.92, 29.392, 28.0
2019-02-16 23:46:37,323 : samples : 512000
2019-02-16 23:46:47,668 : Image to text: 6.7, 21.2, 32.06, 24.0
2019-02-16 23:46:55,197 : Text to Image: 5.508, 18.472, 28.688, 29.0
2019-02-16 23:47:31,818 : Epoch 3 finished
2019-02-16 23:47:32,247 : Image to text: 19.6, 48.9, 67.1, 6.0
2019-02-16 23:47:32,576 : Text to Image: 15.86, 44.9, 62.08, 7.0
2019-02-16 23:47:33,018 : Image to text: 18.9, 50.6, 67.1, 5.0
2019-02-16 23:47:33,353 : Text to Image: 15.7, 44.94, 62.64, 7.0
2019-02-16 23:47:33,809 : Image to text: 21.3, 50.7, 66.6, 5.0
2019-02-16 23:47:34,152 : Text to Image: 15.1, 43.9, 61.2, 7.0
2019-02-16 23:47:34,608 : Image to text: 18.4, 49.9, 66.9, 6.0
2019-02-16 23:47:34,952 : Text to Image: 15.84, 45.08, 61.86, 7.0
2019-02-16 23:47:35,401 : Image to text: 21.1, 51.4, 67.0, 5.0
2019-02-16 23:47:35,739 : Text to Image: 16.4, 45.68, 62.46, 6.0
2019-02-16 23:47:35,739 : Dev mean Text to Image: 15.78, 44.89999999999999, 62.048, 6.8
2019-02-16 23:47:35,739 : Dev mean Image to text: 19.86, 50.3, 66.94, 5.4
2019-02-16 23:47:35,740 : start epoch
2019-02-16 23:48:18,726 : samples : 64000
2019-02-16 23:48:29,237 : Image to text: 6.54, 21.78, 33.34, 22.0
2019-02-16 23:48:36,778 : Text to Image: 5.944, 18.96, 29.164, 27.0
2019-02-16 23:49:19,156 : samples : 128000
2019-02-16 23:49:29,683 : Image to text: 6.9, 21.84, 33.58, 22.0
2019-02-16 23:49:37,211 : Text to Image: 5.848, 19.572, 30.132, 27.0
2019-02-16 23:50:19,814 : samples : 192000
2019-02-16 23:50:30,331 : Image to text: 6.62, 20.8, 32.52, 24.0
2019-02-16 23:50:37,914 : Text to Image: 5.684, 18.924, 29.408, 29.0
2019-02-16 23:51:20,821 : samples : 256000
2019-02-16 23:51:31,425 : Image to text: 6.44, 21.66, 33.0, 23.0
2019-02-16 23:51:39,023 : Text to Image: 6.26, 19.672, 30.292, 27.0
2019-02-16 23:52:22,174 : samples : 320000
2019-02-16 23:52:32,552 : Image to text: 6.5, 22.78, 34.22, 22.0
2019-02-16 23:52:40,922 : Text to Image: 6.06, 19.624, 30.12, 27.0
2019-02-16 23:53:33,184 : samples : 384000
2019-02-16 23:53:43,715 : Image to text: 5.98, 21.58, 32.88, 24.0
2019-02-16 23:53:51,282 : Text to Image: 6.064, 19.476, 29.76, 27.0
2019-02-16 23:54:34,309 : samples : 448000
2019-02-16 23:54:44,647 : Image to text: 6.58, 22.76, 34.12, 22.0
2019-02-16 23:54:52,137 : Text to Image: 6.052, 19.812, 30.312, 27.0
2019-02-16 23:55:34,348 : samples : 512000
2019-02-16 23:55:44,726 : Image to text: 7.2, 22.62, 34.42, 22.0
2019-02-16 23:55:52,249 : Text to Image: 6.184, 20.06, 30.852, 26.0
2019-02-16 23:56:29,740 : Epoch 4 finished
2019-02-16 23:56:30,189 : Image to text: 22.4, 52.3, 69.0, 5.0
2019-02-16 23:56:30,521 : Text to Image: 18.16, 47.94, 65.46, 6.0
2019-02-16 23:56:30,973 : Image to text: 20.4, 51.7, 70.0, 5.0
2019-02-16 23:56:31,309 : Text to Image: 17.76, 47.82, 65.58, 6.0
2019-02-16 23:56:31,776 : Image to text: 22.7, 53.3, 69.3, 5.0
2019-02-16 23:56:32,111 : Text to Image: 17.28, 47.06, 64.7, 6.0
2019-02-16 23:56:32,557 : Image to text: 19.6, 53.6, 70.1, 5.0
2019-02-16 23:56:32,883 : Text to Image: 17.46, 46.76, 64.74, 6.0
2019-02-16 23:56:33,330 : Image to text: 22.5, 53.3, 70.0, 5.0
2019-02-16 23:56:33,661 : Text to Image: 18.22, 48.56, 64.02, 6.0
2019-02-16 23:56:33,661 : Dev mean Text to Image: 17.776, 47.628, 64.89999999999999, 6.0
2019-02-16 23:56:33,662 : Dev mean Image to text: 21.52, 52.84, 69.67999999999999, 5.0
2019-02-16 23:56:33,662 : start epoch
2019-02-16 23:57:16,269 : samples : 64000
2019-02-16 23:57:26,756 : Image to text: 7.14, 23.06, 34.8, 21.0
2019-02-16 23:57:34,295 : Text to Image: 6.396, 20.736, 31.372, 25.0
2019-02-16 23:58:17,121 : samples : 128000
2019-02-16 23:58:27,545 : Image to text: 7.0, 22.96, 35.1, 22.0
2019-02-16 23:58:35,095 : Text to Image: 6.116, 20.28, 30.692, 26.0
2019-02-16 23:59:18,610 : samples : 192000
2019-02-16 23:59:29,161 : Image to text: 7.12, 22.24, 34.26, 21.0
2019-02-16 23:59:36,717 : Text to Image: 6.232, 20.448, 30.952, 26.0
2019-02-17 00:00:19,224 : samples : 256000
2019-02-17 00:00:29,711 : Image to text: 6.72, 22.6, 33.4, 23.0
2019-02-17 00:00:37,237 : Text to Image: 6.056, 19.568, 30.268, 27.0
2019-02-17 00:01:20,249 : samples : 320000
2019-02-17 00:01:30,725 : Image to text: 7.54, 23.48, 35.04, 21.0
2019-02-17 00:01:38,195 : Text to Image: 6.424, 20.38, 30.944, 26.0
2019-02-17 00:02:21,276 : samples : 384000
2019-02-17 00:02:31,788 : Image to text: 6.76, 22.06, 33.86, 21.0
2019-02-17 00:02:39,339 : Text to Image: 6.104, 19.832, 30.232, 26.0
2019-02-17 00:03:22,247 : samples : 448000
2019-02-17 00:03:32,613 : Image to text: 6.8, 22.3, 34.32, 22.0
2019-02-17 00:03:40,201 : Text to Image: 6.104, 19.652, 30.6, 26.0
2019-02-17 00:04:22,716 : samples : 512000
2019-02-17 00:04:33,079 : Image to text: 7.18, 23.36, 35.16, 21.0
2019-02-17 00:04:40,584 : Text to Image: 6.452, 20.82, 31.328, 25.0
2019-02-17 00:05:17,147 : Epoch 5 finished
2019-02-17 00:05:17,600 : Image to text: 20.9, 54.6, 69.3, 4.0
2019-02-17 00:05:17,942 : Text to Image: 18.48, 48.0, 65.14, 6.0
2019-02-17 00:05:18,396 : Image to text: 20.8, 53.3, 68.3, 5.0
2019-02-17 00:05:18,738 : Text to Image: 17.3, 48.54, 65.84, 6.0
2019-02-17 00:05:19,192 : Image to text: 22.1, 53.9, 68.9, 5.0
2019-02-17 00:05:19,528 : Text to Image: 18.2, 47.6, 64.82, 6.0
2019-02-17 00:05:19,979 : Image to text: 19.5, 55.0, 71.6, 5.0
2019-02-17 00:05:20,324 : Text to Image: 18.08, 47.86, 64.94, 6.0
2019-02-17 00:05:20,773 : Image to text: 23.9, 55.7, 70.2, 4.0
2019-02-17 00:05:21,114 : Text to Image: 19.06, 49.52, 65.58, 6.0
2019-02-17 00:05:21,114 : Dev mean Text to Image: 18.224, 48.304, 65.264, 6.0
2019-02-17 00:05:21,114 : Dev mean Image to text: 21.439999999999998, 54.5, 69.66, 4.6
2019-02-17 00:05:21,114 : start epoch
2019-02-17 00:06:03,932 : samples : 64000
2019-02-17 00:06:14,409 : Image to text: 6.66, 22.68, 35.0, 21.0
2019-02-17 00:06:21,884 : Text to Image: 6.484, 20.756, 31.204, 25.0
2019-02-17 00:07:04,860 : samples : 128000
2019-02-17 00:07:15,476 : Image to text: 7.04, 23.28, 34.88, 21.0
2019-02-17 00:07:23,031 : Text to Image: 6.504, 20.228, 31.248, 25.0
2019-02-17 00:08:05,316 : samples : 192000
2019-02-17 00:08:15,827 : Image to text: 6.84, 22.32, 34.8, 21.0
2019-02-17 00:08:23,358 : Text to Image: 6.232, 20.284, 31.284, 25.0
2019-02-17 00:09:05,798 : samples : 256000
2019-02-17 00:09:16,250 : Image to text: 6.26, 22.32, 34.0, 21.0
2019-02-17 00:09:23,837 : Text to Image: 6.168, 19.932, 30.776, 26.0
2019-02-17 00:10:10,420 : samples : 320000
2019-02-17 00:10:22,532 : Image to text: 6.8, 22.72, 34.56, 21.0
2019-02-17 00:10:30,903 : Text to Image: 6.556, 20.952, 31.408, 25.0
2019-02-17 00:11:14,794 : samples : 384000
2019-02-17 00:11:25,316 : Image to text: 7.54, 23.36, 35.32, 21.0
2019-02-17 00:11:32,838 : Text to Image: 6.568, 20.684, 31.364, 25.0
2019-02-17 00:12:16,132 : samples : 448000
2019-02-17 00:12:28,757 : Image to text: 7.28, 22.78, 35.44, 20.0
2019-02-17 00:12:38,822 : Text to Image: 6.492, 20.636, 31.128, 25.0
2019-02-17 00:13:24,595 : samples : 512000
2019-02-17 00:13:35,098 : Image to text: 7.1, 23.14, 35.08, 21.0
2019-02-17 00:13:42,505 : Text to Image: 6.216, 19.716, 30.416, 26.0
2019-02-17 00:14:18,980 : Epoch 6 finished
2019-02-17 00:14:19,455 : Image to text: 21.0, 54.0, 70.9, 5.0
2019-02-17 00:14:19,831 : Text to Image: 19.3, 50.06, 67.12, 5.0
2019-02-17 00:14:20,295 : Image to text: 19.1, 52.9, 69.8, 5.0
2019-02-17 00:14:20,674 : Text to Image: 19.02, 49.9, 67.28, 6.0
2019-02-17 00:14:21,067 : Image to text: 22.9, 54.1, 71.0, 5.0
2019-02-17 00:14:21,358 : Text to Image: 18.74, 49.38, 67.12, 6.0
2019-02-17 00:14:21,724 : Image to text: 19.7, 55.1, 69.7, 5.0
2019-02-17 00:14:22,001 : Text to Image: 18.96, 50.26, 66.92, 5.0
2019-02-17 00:14:22,371 : Image to text: 23.9, 53.2, 69.7, 5.0
2019-02-17 00:14:22,655 : Text to Image: 19.46, 50.06, 65.64, 5.0
2019-02-17 00:14:22,655 : Dev mean Text to Image: 19.096, 49.932, 66.816, 5.4
2019-02-17 00:14:22,655 : Dev mean Image to text: 21.32, 53.86, 70.22, 5.0
2019-02-17 00:14:22,656 : start epoch
2019-02-17 00:15:07,738 : samples : 64000
2019-02-17 00:15:20,389 : Image to text: 7.82, 24.64, 36.48, 19.0
2019-02-17 00:15:30,420 : Text to Image: 6.988, 21.888, 32.548, 24.0
2019-02-17 00:16:14,046 : samples : 128000
2019-02-17 00:16:24,317 : Image to text: 7.82, 24.48, 35.74, 20.0
2019-02-17 00:16:31,627 : Text to Image: 6.832, 21.152, 32.188, 24.0
2019-02-17 00:17:13,598 : samples : 192000
2019-02-17 00:17:23,761 : Image to text: 7.04, 24.08, 36.62, 20.0
2019-02-17 00:17:33,850 : Text to Image: 6.72, 21.664, 32.536, 24.0
2019-02-17 00:18:18,620 : samples : 256000
2019-02-17 00:18:31,312 : Image to text: 6.84, 23.3, 34.5, 21.0
2019-02-17 00:18:41,376 : Text to Image: 6.772, 20.772, 32.052, 24.0
2019-02-17 00:19:24,576 : samples : 320000
2019-02-17 00:19:34,833 : Image to text: 7.4, 23.98, 35.16, 20.0
2019-02-17 00:19:42,270 : Text to Image: 6.612, 21.144, 32.06, 24.0
2019-02-17 00:20:26,716 : samples : 384000
2019-02-17 00:20:39,322 : Image to text: 7.28, 24.16, 36.1, 20.0
2019-02-17 00:20:49,363 : Text to Image: 6.828, 21.472, 32.536, 24.0
2019-02-17 00:21:33,014 : samples : 448000
2019-02-17 00:21:43,439 : Image to text: 7.74, 23.98, 35.88, 20.0
2019-02-17 00:21:50,796 : Text to Image: 7.072, 21.572, 32.848, 24.0
2019-02-17 00:22:34,662 : samples : 512000
2019-02-17 00:22:47,210 : Image to text: 7.68, 24.2, 36.1, 19.0
2019-02-17 00:22:57,239 : Text to Image: 6.892, 21.84, 32.568, 24.0
2019-02-17 00:23:35,993 : Epoch 7 finished
2019-02-17 00:23:36,967 : Image to text: 21.3, 52.6, 70.6, 5.0
2019-02-17 00:23:37,710 : Text to Image: 18.68, 49.68, 67.0, 6.0
2019-02-17 00:23:38,710 : Image to text: 20.2, 54.8, 71.2, 4.0
2019-02-17 00:23:39,474 : Text to Image: 18.94, 50.38, 67.76, 5.0
2019-02-17 00:23:39,927 : Image to text: 22.5, 54.6, 69.5, 5.0
2019-02-17 00:23:40,290 : Text to Image: 19.22, 48.92, 66.76, 6.0
2019-02-17 00:23:40,752 : Image to text: 20.3, 54.5, 71.2, 5.0
2019-02-17 00:23:41,118 : Text to Image: 18.86, 49.42, 67.14, 6.0
2019-02-17 00:23:41,592 : Image to text: 23.9, 55.2, 69.8, 5.0
2019-02-17 00:23:41,961 : Text to Image: 19.38, 50.06, 66.56, 5.0
2019-02-17 00:23:41,962 : Dev mean Text to Image: 19.016000000000002, 49.692, 67.044, 5.6000000000000005
2019-02-17 00:23:41,962 : Dev mean Image to text: 21.64, 54.339999999999996, 70.46, 4.8
2019-02-17 00:23:41,962 : start epoch
2019-02-17 00:24:24,523 : samples : 64000
2019-02-17 00:24:34,774 : Image to text: 7.68, 23.04, 35.68, 20.0
2019-02-17 00:24:42,129 : Text to Image: 6.936, 21.328, 32.236, 24.0
2019-02-17 00:25:26,008 : samples : 128000
2019-02-17 00:25:38,568 : Image to text: 6.98, 24.02, 35.88, 20.0
2019-02-17 00:25:48,614 : Text to Image: 6.824, 21.284, 32.032, 24.0
2019-02-17 00:26:33,754 : samples : 192000
2019-02-17 00:26:44,020 : Image to text: 7.56, 24.24, 36.44, 19.0
2019-02-17 00:26:51,381 : Text to Image: 6.688, 21.3, 32.16, 24.0
2019-02-17 00:27:41,333 : samples : 256000
2019-02-17 00:27:54,191 : Image to text: 7.82, 24.72, 36.44, 19.0
2019-02-17 00:28:04,219 : Text to Image: 7.052, 21.92, 32.844, 23.0
2019-02-17 00:28:49,849 : samples : 320000
2019-02-17 00:29:00,157 : Image to text: 7.54, 25.14, 37.28, 19.0
2019-02-17 00:29:07,577 : Text to Image: 7.004, 22.368, 33.42, 23.0
2019-02-17 00:29:50,500 : samples : 384000
2019-02-17 00:30:03,114 : Image to text: 7.08, 23.58, 35.96, 20.0
2019-02-17 00:30:13,096 : Text to Image: 6.852, 21.496, 32.264, 24.0
2019-02-17 00:30:58,979 : samples : 448000
2019-02-17 00:31:11,252 : Image to text: 7.7, 23.92, 36.8, 19.0
2019-02-17 00:31:18,676 : Text to Image: 6.632, 21.608, 32.464, 24.0
2019-02-17 00:32:01,396 : samples : 512000
2019-02-17 00:32:11,697 : Image to text: 7.28, 23.06, 35.66, 20.0
2019-02-17 00:32:18,729 : Text to Image: 6.68, 21.428, 32.492, 24.0
2019-02-17 00:32:57,118 : Epoch 8 finished
2019-02-17 00:32:58,094 : Image to text: 21.1, 54.9, 71.0, 5.0
2019-02-17 00:32:58,876 : Text to Image: 19.28, 50.08, 67.08, 5.0
2019-02-17 00:32:59,849 : Image to text: 21.6, 55.7, 71.2, 4.0
2019-02-17 00:33:00,619 : Text to Image: 19.42, 51.06, 67.9, 5.0
2019-02-17 00:33:01,613 : Image to text: 24.0, 54.7, 71.8, 5.0
2019-02-17 00:33:02,359 : Text to Image: 18.94, 50.58, 68.12, 5.0
2019-02-17 00:33:03,327 : Image to text: 21.6, 55.0, 72.9, 5.0
2019-02-17 00:33:04,127 : Text to Image: 19.6, 50.16, 67.72, 5.0
2019-02-17 00:33:05,107 : Image to text: 23.3, 55.6, 71.7, 4.0
2019-02-17 00:33:05,912 : Text to Image: 19.54, 49.98, 66.42, 6.0
2019-02-17 00:33:05,912 : Dev mean Text to Image: 19.356, 50.372, 67.44800000000001, 5.2
2019-02-17 00:33:05,912 : Dev mean Image to text: 22.32, 55.18000000000001, 71.72, 4.6
2019-02-17 00:33:05,913 : start epoch
2019-02-17 00:33:51,879 : samples : 64000
2019-02-17 00:34:03,128 : Image to text: 7.22, 23.96, 36.16, 19.0
2019-02-17 00:34:10,510 : Text to Image: 6.856, 21.404, 32.184, 24.0
2019-02-17 00:34:53,480 : samples : 128000
2019-02-17 00:35:06,098 : Image to text: 7.94, 24.0, 36.4, 20.0
2019-02-17 00:35:16,133 : Text to Image: 6.696, 21.16, 32.272, 24.0
2019-02-17 00:36:02,354 : samples : 192000
2019-02-17 00:36:14,917 : Image to text: 7.24, 24.22, 36.96, 20.0
2019-02-17 00:36:24,099 : Text to Image: 6.82, 21.816, 32.9, 24.0
2019-02-17 00:37:07,015 : samples : 256000
2019-02-17 00:37:17,387 : Image to text: 8.08, 24.16, 36.7, 19.0
2019-02-17 00:37:25,076 : Text to Image: 6.952, 21.84, 32.924, 23.0
2019-02-17 00:38:08,652 : samples : 320000
2019-02-17 00:38:20,562 : Image to text: 7.28, 23.16, 35.0, 21.0
2019-02-17 00:38:27,966 : Text to Image: 6.416, 20.956, 31.892, 24.0
2019-02-17 00:39:10,421 : samples : 384000
2019-02-17 00:39:20,683 : Image to text: 7.06, 23.92, 35.48, 20.0
2019-02-17 00:39:28,170 : Text to Image: 6.752, 21.648, 32.764, 24.0
2019-02-17 00:40:11,053 : samples : 448000
2019-02-17 00:40:21,391 : Image to text: 8.16, 24.62, 35.66, 19.0
2019-02-17 00:40:28,814 : Text to Image: 6.824, 21.352, 32.344, 24.0
2019-02-17 00:41:11,515 : samples : 512000
2019-02-17 00:41:21,765 : Image to text: 7.4, 24.98, 37.4, 19.0
2019-02-17 00:41:29,160 : Text to Image: 7.18, 22.2, 33.236, 23.0
2019-02-17 00:42:05,647 : Epoch 9 finished
2019-02-17 00:42:06,108 : Image to text: 22.0, 55.4, 71.2, 5.0
2019-02-17 00:42:06,470 : Text to Image: 19.36, 50.28, 67.82, 5.0
2019-02-17 00:42:06,917 : Image to text: 20.5, 54.1, 72.7, 5.0
2019-02-17 00:42:07,276 : Text to Image: 19.58, 49.84, 67.86, 6.0
2019-02-17 00:42:07,725 : Image to text: 24.7, 56.5, 70.6, 4.0
2019-02-17 00:42:08,085 : Text to Image: 19.36, 50.12, 66.94, 5.0
2019-02-17 00:42:08,532 : Image to text: 20.9, 56.0, 72.0, 4.0
2019-02-17 00:42:08,894 : Text to Image: 18.9, 49.52, 67.18, 6.0
2019-02-17 00:42:09,277 : Image to text: 22.2, 54.7, 69.5, 5.0
2019-02-17 00:42:09,557 : Text to Image: 19.82, 50.64, 66.12, 5.0
2019-02-17 00:42:09,557 : Dev mean Text to Image: 19.404, 50.08, 67.184, 5.4
2019-02-17 00:42:09,557 : Dev mean Image to text: 22.059999999999995, 55.34, 71.2, 4.6
2019-02-17 00:42:09,557 : start epoch
2019-02-17 00:42:53,359 : samples : 64000
2019-02-17 00:43:03,649 : Image to text: 7.32, 25.2, 36.78, 19.0
2019-02-17 00:43:11,074 : Text to Image: 6.94, 21.928, 33.032, 23.0
2019-02-17 00:43:53,828 : samples : 128000
2019-02-17 00:44:04,302 : Image to text: 7.8, 24.94, 37.38, 19.0
2019-02-17 00:44:14,443 : Text to Image: 7.088, 22.272, 33.5, 23.0
2019-02-17 00:45:08,981 : samples : 192000
2019-02-17 00:45:19,560 : Image to text: 8.12, 24.64, 37.46, 19.0
2019-02-17 00:45:29,131 : Text to Image: 7.02, 22.016, 33.472, 23.0
2019-02-17 00:46:12,421 : samples : 256000
2019-02-17 00:46:23,924 : Image to text: 7.54, 23.8, 35.68, 20.0
2019-02-17 00:46:31,155 : Text to Image: 6.712, 21.34, 32.7, 23.0
2019-02-17 00:47:14,581 : samples : 320000
2019-02-17 00:47:24,703 : Image to text: 7.44, 23.92, 36.82, 19.0
2019-02-17 00:47:34,528 : Text to Image: 6.62, 21.648, 32.588, 24.0
2019-02-17 00:48:17,163 : samples : 384000
2019-02-17 00:48:29,681 : Image to text: 7.62, 24.56, 36.62, 19.0
2019-02-17 00:48:39,635 : Text to Image: 7.02, 21.752, 32.916, 23.0
2019-02-17 00:49:24,837 : samples : 448000
2019-02-17 00:49:37,418 : Image to text: 8.08, 25.92, 38.26, 18.0
2019-02-17 00:49:47,425 : Text to Image: 7.088, 22.084, 33.412, 23.0
2019-02-17 00:50:32,715 : samples : 512000
2019-02-17 00:50:45,305 : Image to text: 7.74, 24.32, 36.88, 19.0
2019-02-17 00:50:55,295 : Text to Image: 7.016, 22.04, 33.324, 23.0
2019-02-17 00:51:33,762 : Epoch 10 finished
2019-02-17 00:51:34,669 : Image to text: 22.6, 56.6, 72.2, 4.0
2019-02-17 00:51:35,399 : Text to Image: 20.22, 50.74, 68.84, 5.0
2019-02-17 00:51:36,298 : Image to text: 21.8, 55.1, 70.9, 5.0
2019-02-17 00:51:37,047 : Text to Image: 18.74, 49.68, 68.22, 6.0
2019-02-17 00:51:37,969 : Image to text: 23.1, 54.6, 71.5, 4.0
2019-02-17 00:51:38,689 : Text to Image: 19.36, 49.46, 67.48, 6.0
2019-02-17 00:51:39,591 : Image to text: 20.4, 54.9, 72.9, 5.0
2019-02-17 00:51:40,357 : Text to Image: 18.98, 49.5, 67.56, 6.0
2019-02-17 00:51:41,246 : Image to text: 22.2, 54.2, 71.4, 5.0
2019-02-17 00:51:41,990 : Text to Image: 19.3, 50.68, 66.96, 5.0
2019-02-17 00:51:41,990 : Dev mean Text to Image: 19.32, 50.012, 67.812, 5.6000000000000005
2019-02-17 00:51:41,990 : Dev mean Image to text: 22.019999999999996, 55.08, 71.78, 4.6
2019-02-17 00:51:41,990 : start epoch
2019-02-17 00:52:27,178 : samples : 64000
2019-02-17 00:52:39,775 : Image to text: 8.3, 25.62, 37.7, 18.0
2019-02-17 00:52:49,804 : Text to Image: 7.14, 22.172, 33.432, 23.0
2019-02-17 00:53:35,249 : samples : 128000
2019-02-17 00:53:47,823 : Image to text: 7.54, 24.38, 36.3, 20.0
2019-02-17 00:53:57,829 : Text to Image: 6.8, 21.472, 32.592, 24.0
2019-02-17 00:54:42,958 : samples : 192000
2019-02-17 00:54:55,576 : Image to text: 8.1, 24.7, 37.08, 19.0
2019-02-17 00:55:05,621 : Text to Image: 6.92, 21.688, 33.052, 23.0
2019-02-17 00:55:50,065 : samples : 256000
2019-02-17 00:56:02,681 : Image to text: 8.18, 25.68, 37.48, 19.0
2019-02-17 00:56:12,690 : Text to Image: 7.104, 22.72, 33.916, 22.0
2019-02-17 00:56:57,341 : samples : 320000
2019-02-17 00:57:09,891 : Image to text: 7.94, 24.92, 37.62, 18.0
2019-02-17 00:57:19,882 : Text to Image: 7.112, 22.188, 33.54, 22.0
2019-02-17 00:58:04,325 : samples : 384000
2019-02-17 00:58:16,946 : Image to text: 8.46, 25.76, 38.24, 18.0
2019-02-17 00:58:27,068 : Text to Image: 7.248, 22.604, 33.648, 22.0
2019-02-17 00:59:12,397 : samples : 448000
2019-02-17 00:59:24,947 : Image to text: 7.44, 24.24, 36.6, 19.0
2019-02-17 00:59:34,378 : Text to Image: 6.852, 21.728, 32.828, 23.0
2019-02-17 01:00:17,736 : samples : 512000
2019-02-17 01:00:28,078 : Image to text: 8.36, 25.46, 38.42, 18.0
2019-02-17 01:00:35,417 : Text to Image: 7.208, 22.536, 33.62, 22.0
2019-02-17 01:01:12,002 : Epoch 11 finished
2019-02-17 01:01:12,472 : Image to text: 24.3, 55.5, 70.3, 5.0
2019-02-17 01:01:12,836 : Text to Image: 18.94, 51.06, 68.76, 5.0
2019-02-17 01:01:13,300 : Image to text: 21.6, 53.7, 69.8, 5.0
2019-02-17 01:01:13,663 : Text to Image: 19.14, 51.26, 68.88, 5.0
2019-02-17 01:01:14,118 : Image to text: 24.1, 55.2, 70.9, 4.0
2019-02-17 01:01:14,486 : Text to Image: 19.58, 50.58, 67.4, 5.0
2019-02-17 01:01:14,942 : Image to text: 20.6, 53.5, 71.5, 5.0
2019-02-17 01:01:15,310 : Text to Image: 18.32, 49.66, 68.32, 6.0
2019-02-17 01:01:15,762 : Image to text: 22.8, 55.0, 70.7, 4.0
2019-02-17 01:01:16,047 : Text to Image: 20.54, 51.34, 67.6, 5.0
2019-02-17 01:01:16,047 : Dev mean Text to Image: 19.304, 50.78, 68.192, 5.2
2019-02-17 01:01:16,047 : Dev mean Image to text: 22.68, 54.58, 70.64, 4.6
2019-02-17 01:01:16,047 : start epoch
2019-02-17 01:02:03,351 : samples : 64000
2019-02-17 01:02:16,563 : Image to text: 7.86, 25.14, 37.72, 18.0
2019-02-17 01:02:27,058 : Text to Image: 7.276, 22.572, 33.828, 22.0
2019-02-17 01:03:12,759 : samples : 128000
2019-02-17 01:03:25,641 : Image to text: 7.64, 24.88, 37.12, 19.0
2019-02-17 01:03:36,056 : Text to Image: 6.996, 22.032, 33.364, 23.0
2019-02-17 01:04:22,091 : samples : 192000
2019-02-17 01:04:34,977 : Image to text: 8.16, 25.2, 37.7, 18.0
2019-02-17 01:04:45,406 : Text to Image: 7.12, 22.544, 33.716, 22.0
2019-02-17 01:05:31,554 : samples : 256000
2019-02-17 01:05:44,497 : Image to text: 8.08, 25.64, 37.86, 18.0
2019-02-17 01:05:54,994 : Text to Image: 7.388, 22.912, 34.256, 22.0
2019-02-17 01:06:41,897 : samples : 320000
2019-02-17 01:06:54,781 : Image to text: 7.82, 24.32, 36.52, 19.0
2019-02-17 01:07:05,265 : Text to Image: 6.608, 21.3, 31.612, 24.0
2019-02-17 01:07:51,152 : samples : 384000
2019-02-17 01:08:04,030 : Image to text: 8.14, 25.52, 38.58, 18.0
2019-02-17 01:08:14,490 : Text to Image: 7.176, 22.4, 33.428, 23.0
2019-02-17 01:09:00,588 : samples : 448000
2019-02-17 01:09:12,663 : Image to text: 8.36, 25.98, 37.78, 18.0
2019-02-17 01:09:23,069 : Text to Image: 7.248, 22.572, 33.764, 22.0
2019-02-17 01:10:07,879 : samples : 512000
2019-02-17 01:10:18,462 : Image to text: 7.24, 24.66, 36.22, 19.0
2019-02-17 01:10:25,966 : Text to Image: 7.16, 22.26, 33.352, 22.0
2019-02-17 01:11:02,521 : Epoch 12 finished
2019-02-17 01:11:02,958 : Image to text: 22.2, 55.8, 70.7, 5.0
2019-02-17 01:11:03,287 : Text to Image: 18.86, 50.7, 68.9, 5.0
2019-02-17 01:11:03,724 : Image to text: 22.5, 55.5, 72.4, 4.0
2019-02-17 01:11:04,065 : Text to Image: 19.72, 51.1, 69.54, 5.0
2019-02-17 01:11:04,510 : Image to text: 23.7, 58.0, 71.5, 4.0
2019-02-17 01:11:04,852 : Text to Image: 19.14, 51.2, 68.7, 5.0
2019-02-17 01:11:05,315 : Image to text: 22.8, 54.2, 71.8, 5.0
2019-02-17 01:11:05,646 : Text to Image: 18.58, 51.0, 68.44, 5.0
2019-02-17 01:11:06,094 : Image to text: 22.9, 55.6, 71.4, 5.0
2019-02-17 01:11:06,426 : Text to Image: 19.72, 51.14, 67.94, 5.0
2019-02-17 01:11:06,426 : Dev mean Text to Image: 19.204, 51.028, 68.704, 5.0
2019-02-17 01:11:06,426 : Dev mean Image to text: 22.82, 55.82000000000001, 71.56, 4.6
2019-02-17 01:11:06,427 : start epoch
2019-02-17 01:11:49,225 : samples : 64000
2019-02-17 01:11:59,691 : Image to text: 8.12, 25.74, 38.34, 18.0
2019-02-17 01:12:07,228 : Text to Image: 7.568, 22.708, 34.016, 22.0
2019-02-17 01:12:49,958 : samples : 128000
2019-02-17 01:13:00,458 : Image to text: 7.6, 24.54, 37.06, 19.0
2019-02-17 01:13:07,972 : Text to Image: 6.976, 22.184, 33.204, 23.0
2019-02-17 01:13:51,124 : samples : 192000
2019-02-17 01:14:01,660 : Image to text: 8.12, 24.88, 36.7, 19.0
2019-02-17 01:14:09,241 : Text to Image: 6.82, 22.18, 33.268, 23.0
2019-02-17 01:14:52,291 : samples : 256000
2019-02-17 01:15:02,841 : Image to text: 8.4, 25.16, 38.22, 18.0
2019-02-17 01:15:10,369 : Text to Image: 7.504, 22.72, 34.072, 22.0
2019-02-17 01:15:53,043 : samples : 320000
2019-02-17 01:16:03,504 : Image to text: 7.94, 24.5, 36.2, 19.0
2019-02-17 01:16:11,003 : Text to Image: 7.252, 22.572, 33.92, 22.0
2019-02-17 01:16:53,622 : samples : 384000
2019-02-17 01:17:04,092 : Image to text: 7.64, 24.24, 36.78, 19.0
2019-02-17 01:17:11,622 : Text to Image: 6.868, 22.1, 33.34, 23.0
2019-02-17 01:17:54,737 : samples : 448000
2019-02-17 01:18:05,248 : Image to text: 8.72, 26.14, 38.38, 18.0
2019-02-17 01:18:12,801 : Text to Image: 7.276, 22.764, 34.42, 22.0
2019-02-17 01:18:56,927 : samples : 512000
2019-02-17 01:19:09,045 : Image to text: 7.72, 25.24, 37.64, 19.0
2019-02-17 01:19:17,652 : Text to Image: 7.04, 21.864, 33.196, 23.0
2019-02-17 01:20:00,812 : Epoch 13 finished
2019-02-17 01:20:01,249 : Image to text: 23.0, 55.2, 71.5, 5.0
2019-02-17 01:20:01,585 : Text to Image: 19.88, 51.68, 69.56, 5.0
2019-02-17 01:20:02,021 : Image to text: 20.1, 54.9, 71.3, 5.0
2019-02-17 01:20:02,348 : Text to Image: 20.2, 51.84, 69.18, 5.0
2019-02-17 01:20:02,794 : Image to text: 24.4, 56.1, 71.2, 4.0
2019-02-17 01:20:03,127 : Text to Image: 19.68, 51.34, 68.62, 5.0
2019-02-17 01:20:03,572 : Image to text: 23.5, 55.2, 71.1, 5.0
2019-02-17 01:20:03,896 : Text to Image: 18.92, 51.62, 69.04, 5.0
2019-02-17 01:20:04,357 : Image to text: 23.6, 55.8, 70.3, 4.0
2019-02-17 01:20:04,692 : Text to Image: 20.26, 51.8, 67.58, 5.0
2019-02-17 01:20:04,692 : Dev mean Text to Image: 19.788, 51.656, 68.796, 5.0
2019-02-17 01:20:04,692 : Dev mean Image to text: 22.92, 55.44, 71.08, 4.6
2019-02-17 01:20:04,693 : start epoch
2019-02-17 01:20:48,046 : samples : 64000
2019-02-17 01:20:58,647 : Image to text: 7.86, 25.74, 37.7, 18.0
2019-02-17 01:21:06,202 : Text to Image: 7.336, 22.792, 33.944, 22.0
2019-02-17 01:21:49,288 : samples : 128000
2019-02-17 01:21:59,740 : Image to text: 7.96, 25.3, 37.32, 19.0
2019-02-17 01:22:07,312 : Text to Image: 7.348, 22.692, 33.82, 22.0
2019-02-17 01:22:49,822 : samples : 192000
2019-02-17 01:23:00,372 : Image to text: 8.32, 25.7, 37.62, 18.0
2019-02-17 01:23:07,909 : Text to Image: 7.228, 22.544, 33.604, 22.0
2019-02-17 01:23:50,929 : samples : 256000
2019-02-17 01:24:01,458 : Image to text: 8.08, 24.94, 37.14, 19.0
2019-02-17 01:24:09,007 : Text to Image: 7.368, 22.416, 33.576, 22.0
2019-02-17 01:24:51,795 : samples : 320000
2019-02-17 01:25:02,345 : Image to text: 8.2, 25.36, 38.0, 18.0
2019-02-17 01:25:09,911 : Text to Image: 7.54, 22.956, 34.5, 22.0
2019-02-17 01:25:52,554 : samples : 384000
2019-02-17 01:26:03,008 : Image to text: 7.94, 25.0, 37.8, 18.0
2019-02-17 01:26:10,539 : Text to Image: 7.176, 22.404, 33.704, 23.0
2019-02-17 01:26:53,353 : samples : 448000
2019-02-17 01:27:03,900 : Image to text: 7.82, 24.86, 37.76, 18.0
2019-02-17 01:27:11,418 : Text to Image: 7.28, 22.68, 34.08, 22.0
2019-02-17 01:27:54,088 : samples : 512000
2019-02-17 01:28:04,631 : Image to text: 8.04, 26.18, 38.16, 18.0
2019-02-17 01:28:12,179 : Text to Image: 7.34, 23.072, 34.324, 22.0
2019-02-17 01:28:48,366 : Epoch 14 finished
2019-02-17 01:28:48,811 : Image to text: 23.1, 56.9, 72.8, 4.0
2019-02-17 01:28:49,138 : Text to Image: 20.62, 51.92, 69.24, 5.0
2019-02-17 01:28:49,587 : Image to text: 24.2, 55.5, 72.4, 4.0
2019-02-17 01:28:49,928 : Text to Image: 19.84, 52.38, 70.0, 5.0
2019-02-17 01:28:50,363 : Image to text: 24.2, 56.0, 72.4, 4.0
2019-02-17 01:28:50,702 : Text to Image: 20.1, 51.38, 68.98, 5.0
2019-02-17 01:28:51,147 : Image to text: 21.8, 55.2, 72.0, 4.0
2019-02-17 01:28:51,491 : Text to Image: 19.82, 51.78, 68.84, 5.0
2019-02-17 01:28:51,939 : Image to text: 23.5, 56.2, 70.9, 4.0
2019-02-17 01:28:52,269 : Text to Image: 20.22, 51.74, 68.3, 5.0
2019-02-17 01:28:52,269 : Dev mean Text to Image: 20.12, 51.839999999999996, 69.072, 5.0
2019-02-17 01:28:52,269 : Dev mean Image to text: 23.36, 55.959999999999994, 72.1, 4.0
2019-02-17 01:28:52,270 : start epoch
2019-02-17 01:29:34,782 : samples : 64000
2019-02-17 01:29:45,299 : Image to text: 7.86, 25.14, 38.04, 18.0
2019-02-17 01:29:52,859 : Text to Image: 7.12, 22.184, 33.456, 23.0
2019-02-17 01:30:35,593 : samples : 128000
2019-02-17 01:30:46,163 : Image to text: 7.88, 24.56, 37.02, 19.0
2019-02-17 01:30:53,761 : Text to Image: 7.276, 22.316, 33.932, 22.0
2019-02-17 01:31:35,884 : samples : 192000
2019-02-17 01:31:46,322 : Image to text: 8.32, 25.04, 37.84, 18.0
2019-02-17 01:31:53,876 : Text to Image: 7.116, 22.748, 33.984, 22.0
2019-02-17 01:32:36,302 : samples : 256000
2019-02-17 01:32:46,779 : Image to text: 8.28, 26.02, 37.92, 18.0
2019-02-17 01:32:54,425 : Text to Image: 7.628, 23.128, 34.4, 22.0
2019-02-17 01:33:36,758 : samples : 320000
2019-02-17 01:33:47,218 : Image to text: 7.7, 25.84, 37.84, 19.0
2019-02-17 01:33:54,815 : Text to Image: 7.468, 22.82, 34.212, 22.0
2019-02-17 01:34:37,469 : samples : 384000
2019-02-17 01:34:48,038 : Image to text: 7.92, 25.16, 37.12, 19.0
2019-02-17 01:34:55,585 : Text to Image: 7.408, 22.916, 34.22, 22.0
2019-02-17 01:35:38,202 : samples : 448000
2019-02-17 01:35:48,716 : Image to text: 8.32, 25.44, 37.5, 18.0
2019-02-17 01:35:56,230 : Text to Image: 7.388, 22.684, 34.016, 22.0
2019-02-17 01:36:46,545 : samples : 512000
2019-02-17 01:36:59,106 : Image to text: 8.26, 24.98, 37.86, 18.0
2019-02-17 01:37:07,150 : Text to Image: 7.408, 22.808, 34.064, 22.0
2019-02-17 01:37:43,665 : Epoch 15 finished
2019-02-17 01:37:44,109 : Image to text: 22.6, 57.2, 73.0, 4.0
2019-02-17 01:37:44,456 : Text to Image: 20.44, 52.84, 70.34, 5.0
2019-02-17 01:37:44,916 : Image to text: 22.0, 57.7, 72.8, 4.0
2019-02-17 01:37:45,262 : Text to Image: 20.46, 52.02, 69.74, 5.0
2019-02-17 01:37:45,724 : Image to text: 25.4, 58.5, 72.5, 4.0
2019-02-17 01:37:46,071 : Text to Image: 20.18, 51.4, 68.92, 5.0
2019-02-17 01:37:46,544 : Image to text: 23.7, 57.9, 73.4, 4.0
2019-02-17 01:37:46,883 : Text to Image: 19.28, 51.24, 69.38, 5.0
2019-02-17 01:37:47,324 : Image to text: 24.5, 56.9, 72.8, 4.0
2019-02-17 01:37:47,665 : Text to Image: 20.78, 52.22, 68.52, 5.0
2019-02-17 01:37:47,665 : Dev mean Text to Image: 20.228, 51.944, 69.38, 5.0
2019-02-17 01:37:47,665 : Dev mean Image to text: 23.64, 57.64, 72.89999999999999, 4.0
2019-02-17 01:37:47,665 : start epoch
2019-02-17 01:38:30,028 : samples : 64000
2019-02-17 01:38:40,408 : Image to text: 8.04, 24.88, 37.66, 18.0
2019-02-17 01:38:47,929 : Text to Image: 7.124, 22.304, 33.636, 22.0
2019-02-17 01:39:30,779 : samples : 128000
2019-02-17 01:39:41,227 : Image to text: 8.04, 25.42, 37.94, 18.0
2019-02-17 01:39:48,793 : Text to Image: 7.312, 22.552, 34.172, 22.0
2019-02-17 01:40:31,756 : samples : 192000
2019-02-17 01:40:42,187 : Image to text: 8.32, 26.4, 39.06, 17.0
2019-02-17 01:40:49,720 : Text to Image: 7.692, 23.104, 34.568, 21.0
2019-02-17 01:41:32,647 : samples : 256000
2019-02-17 01:41:43,067 : Image to text: 8.1, 25.98, 38.24, 18.0
2019-02-17 01:41:50,622 : Text to Image: 7.332, 22.852, 34.52, 22.0
2019-02-17 01:42:33,630 : samples : 320000
2019-02-17 01:42:44,034 : Image to text: 8.34, 26.3, 38.42, 18.0
2019-02-17 01:42:51,559 : Text to Image: 7.504, 23.276, 34.568, 21.0
2019-02-17 01:43:34,382 : samples : 384000
2019-02-17 01:43:44,767 : Image to text: 8.02, 25.74, 38.12, 18.0
2019-02-17 01:43:52,294 : Text to Image: 7.12, 22.424, 33.476, 22.0
2019-02-17 01:44:34,898 : samples : 448000
2019-02-17 01:44:45,289 : Image to text: 7.5, 25.68, 38.14, 18.0
2019-02-17 01:44:52,874 : Text to Image: 7.54, 22.96, 34.208, 22.0
2019-02-17 01:45:35,514 : samples : 512000
2019-02-17 01:45:46,046 : Image to text: 8.0, 26.38, 38.36, 17.0
2019-02-17 01:45:53,530 : Text to Image: 7.444, 23.508, 34.772, 21.0
2019-02-17 01:46:29,927 : Epoch 16 finished
2019-02-17 01:46:30,357 : Image to text: 23.0, 56.4, 72.0, 4.0
2019-02-17 01:46:30,686 : Text to Image: 20.12, 51.98, 69.54, 5.0
2019-02-17 01:46:31,140 : Image to text: 22.6, 57.0, 73.5, 4.0
2019-02-17 01:46:31,477 : Text to Image: 20.66, 52.36, 69.8, 5.0
2019-02-17 01:46:31,935 : Image to text: 26.3, 57.5, 73.2, 4.0
2019-02-17 01:46:32,283 : Text to Image: 20.58, 51.84, 68.42, 5.0
2019-02-17 01:46:32,731 : Image to text: 21.6, 56.8, 73.3, 4.0
2019-02-17 01:46:33,075 : Text to Image: 19.52, 51.98, 68.6, 5.0
2019-02-17 01:46:33,513 : Image to text: 24.0, 58.1, 72.9, 4.0
2019-02-17 01:46:33,856 : Text to Image: 20.68, 51.98, 68.6, 5.0
2019-02-17 01:46:33,856 : Dev mean Text to Image: 20.311999999999998, 52.028, 68.992, 5.0
2019-02-17 01:46:33,856 : Dev mean Image to text: 23.500000000000004, 57.16, 72.98, 4.0
2019-02-17 01:46:37,873 : 
Test scores | Image to text:             23.28, 56.459999999999994, 72.16, 4.2
2019-02-17 01:46:37,873 : Test scores | Text to image:             19.968, 52.012, 68.768, 5.0

2019-02-17 01:46:37,974 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-17 01:46:38,344 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-17 01:46:39,079 : loading BERT model bert-base-uncased
2019-02-17 01:46:39,080 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:46:39,114 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:46:39,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpshik8kzv
2019-02-17 01:46:41,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:46:43,120 : Computing embeddings for train/dev/test
2019-02-17 01:48:18,153 : Computed embeddings
2019-02-17 01:48:18,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:49:05,468 : [('reg:1e-05', 56.28), ('reg:0.0001', 52.9), ('reg:0.001', 50.1), ('reg:0.01', 45.43)]
2019-02-17 01:49:05,468 : Validation : best param found is reg = 1e-05 with score             56.28
2019-02-17 01:49:05,469 : Evaluating...
2019-02-17 01:49:18,139 : 
Dev acc : 56.3 Test acc : 56.0 for LENGTH classification

2019-02-17 01:49:18,139 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-17 01:49:18,486 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-17 01:49:18,536 : loading BERT model bert-base-uncased
2019-02-17 01:49:18,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:49:18,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:49:18,646 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_kbznrwp
2019-02-17 01:49:21,149 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:49:22,613 : Computing embeddings for train/dev/test
2019-02-17 01:50:51,729 : Computed embeddings
2019-02-17 01:50:51,729 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:51:43,432 : [('reg:1e-05', 25.15), ('reg:0.0001', 8.71), ('reg:0.001', 1.61), ('reg:0.01', 0.57)]
2019-02-17 01:51:43,432 : Validation : best param found is reg = 1e-05 with score             25.15
2019-02-17 01:51:43,432 : Evaluating...
2019-02-17 01:52:00,914 : 
Dev acc : 25.1 Test acc : 24.7 for WORDCONTENT classification

2019-02-17 01:52:00,915 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-17 01:52:01,302 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-17 01:52:01,381 : loading BERT model bert-base-uncased
2019-02-17 01:52:01,381 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:52:01,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:52:01,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoc6se_wp
2019-02-17 01:52:04,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:52:05,531 : Computing embeddings for train/dev/test
2019-02-17 01:53:28,837 : Computed embeddings
2019-02-17 01:53:28,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:53:52,858 : [('reg:1e-05', 27.29), ('reg:0.0001', 27.23), ('reg:0.001', 26.26), ('reg:0.01', 23.53)]
2019-02-17 01:53:52,860 : Validation : best param found is reg = 1e-05 with score             27.29
2019-02-17 01:53:52,860 : Evaluating...
2019-02-17 01:53:58,624 : 
Dev acc : 27.3 Test acc : 26.9 for DEPTH classification

2019-02-17 01:53:58,625 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-17 01:53:59,385 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-17 01:53:59,473 : loading BERT model bert-base-uncased
2019-02-17 01:53:59,473 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:53:59,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:53:59,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpu5p1ewy3
2019-02-17 01:54:02,588 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:54:04,680 : Computing embeddings for train/dev/test
2019-02-17 01:55:23,441 : Computed embeddings
2019-02-17 01:55:23,441 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:56:10,180 : [('reg:1e-05', 72.64), ('reg:0.0001', 72.16), ('reg:0.001', 70.24), ('reg:0.01', 58.78)]
2019-02-17 01:56:10,180 : Validation : best param found is reg = 1e-05 with score             72.64
2019-02-17 01:56:10,180 : Evaluating...
2019-02-17 01:56:22,386 : 
Dev acc : 72.6 Test acc : 73.1 for TOPCONSTITUENTS classification

2019-02-17 01:56:22,387 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-17 01:56:22,789 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-17 01:56:22,866 : loading BERT model bert-base-uncased
2019-02-17 01:56:22,866 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:56:22,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:56:22,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcg2rblkg
2019-02-17 01:56:25,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:56:26,911 : Computing embeddings for train/dev/test
2019-02-17 01:57:51,464 : Computed embeddings
2019-02-17 01:57:51,464 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 01:58:37,801 : [('reg:1e-05', 84.73), ('reg:0.0001', 84.68), ('reg:0.001', 84.7), ('reg:0.01', 83.59)]
2019-02-17 01:58:37,801 : Validation : best param found is reg = 1e-05 with score             84.73
2019-02-17 01:58:37,801 : Evaluating...
2019-02-17 01:58:49,778 : 
Dev acc : 84.7 Test acc : 84.0 for BIGRAMSHIFT classification

2019-02-17 01:58:49,779 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-17 01:58:50,188 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-17 01:58:50,260 : loading BERT model bert-base-uncased
2019-02-17 01:58:50,260 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 01:58:50,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 01:58:50,385 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4zq1s0_2
2019-02-17 01:58:52,890 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 01:58:54,340 : Computing embeddings for train/dev/test
2019-02-17 02:00:16,760 : Computed embeddings
2019-02-17 02:00:16,760 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:01:02,684 : [('reg:1e-05', 88.78), ('reg:0.0001', 88.76), ('reg:0.001', 89.24), ('reg:0.01', 89.59)]
2019-02-17 02:01:02,684 : Validation : best param found is reg = 0.01 with score             89.59
2019-02-17 02:01:02,684 : Evaluating...
2019-02-17 02:01:12,856 : 
Dev acc : 89.6 Test acc : 88.7 for TENSE classification

2019-02-17 02:01:12,857 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-17 02:01:13,274 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-17 02:01:13,345 : loading BERT model bert-base-uncased
2019-02-17 02:01:13,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:01:13,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:01:13,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjyqvw743
2019-02-17 02:01:15,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:01:17,410 : Computing embeddings for train/dev/test
2019-02-17 02:02:44,618 : Computed embeddings
2019-02-17 02:02:44,619 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:03:24,610 : [('reg:1e-05', 82.26), ('reg:0.0001', 82.34), ('reg:0.001', 81.45), ('reg:0.01', 81.93)]
2019-02-17 02:03:24,610 : Validation : best param found is reg = 0.0001 with score             82.34
2019-02-17 02:03:24,610 : Evaluating...
2019-02-17 02:03:34,514 : 
Dev acc : 82.3 Test acc : 82.1 for SUBJNUMBER classification

2019-02-17 02:03:34,516 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-17 02:03:35,190 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-17 02:03:35,267 : loading BERT model bert-base-uncased
2019-02-17 02:03:35,267 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:03:35,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:03:35,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0yca79uf
2019-02-17 02:03:37,792 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:03:39,296 : Computing embeddings for train/dev/test
2019-02-17 02:05:05,121 : Computed embeddings
2019-02-17 02:05:05,121 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:05:39,634 : [('reg:1e-05', 70.09), ('reg:0.0001', 70.03), ('reg:0.001', 69.61), ('reg:0.01', 63.73)]
2019-02-17 02:05:39,635 : Validation : best param found is reg = 1e-05 with score             70.09
2019-02-17 02:05:39,635 : Evaluating...
2019-02-17 02:05:47,166 : 
Dev acc : 70.1 Test acc : 71.2 for OBJNUMBER classification

2019-02-17 02:05:47,168 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-17 02:05:47,639 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-17 02:05:47,719 : loading BERT model bert-base-uncased
2019-02-17 02:05:47,720 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:05:47,757 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:05:47,758 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvob4bfcj
2019-02-17 02:05:50,365 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:05:51,825 : Computing embeddings for train/dev/test
2019-02-17 02:07:31,410 : Computed embeddings
2019-02-17 02:07:31,410 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:08:11,447 : [('reg:1e-05', 60.46), ('reg:0.0001', 60.48), ('reg:0.001', 60.42), ('reg:0.01', 59.56)]
2019-02-17 02:08:11,447 : Validation : best param found is reg = 0.0001 with score             60.48
2019-02-17 02:08:11,447 : Evaluating...
2019-02-17 02:08:22,457 : 
Dev acc : 60.5 Test acc : 59.8 for ODDMANOUT classification

2019-02-17 02:08:22,458 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-17 02:08:22,893 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-17 02:08:22,981 : loading BERT model bert-base-uncased
2019-02-17 02:08:22,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:08:23,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:08:23,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgyigoure
2019-02-17 02:08:25,469 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:08:26,880 : Computing embeddings for train/dev/test
2019-02-17 02:10:03,890 : Computed embeddings
2019-02-17 02:10:03,890 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:10:44,848 : [('reg:1e-05', 69.54), ('reg:0.0001', 69.67), ('reg:0.001', 69.47), ('reg:0.01', 68.05)]
2019-02-17 02:10:44,848 : Validation : best param found is reg = 0.0001 with score             69.67
2019-02-17 02:10:44,848 : Evaluating...
2019-02-17 02:10:54,039 : 
Dev acc : 69.7 Test acc : 69.2 for COORDINATIONINVERSION classification

2019-02-17 02:10:54,042 : total results: {'STS12': {'MSRpar': {'pearson': (0.24897112093145368, 4.6399511802918166e-12), 'spearman': SpearmanrResult(correlation=0.29235046206098525, pvalue=3.0283127158216606e-16), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.017167280811295028, 0.6387867772162574), 'spearman': SpearmanrResult(correlation=0.03231417013984031, pvalue=0.3768502316159331), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41006766245448883, 4.8301450062286876e-20), 'spearman': SpearmanrResult(correlation=0.4874623054038768, pvalue=9.058794315610078e-29), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.34126362016771117, 6.545058953716754e-22), 'spearman': SpearmanrResult(correlation=0.34203476102777663, pvalue=5.22467309092397e-22), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5047334432986987, 3.526538125366472e-27), 'spearman': SpearmanrResult(correlation=0.44782320244588064, pvalue=4.482034295560564e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29757371320821147, 'wmean': 0.2636455264345861}, 'spearman': {'mean': 0.3203969802156719, 'wmean': 0.2903639642463762}}}, 'STS13': {'FNWN': {'pearson': (0.062393810423665194, 0.3937069793271627), 'spearman': SpearmanrResult(correlation=0.0766435570176691, pvalue=0.2945338692242989), 'nsamples': 189}, 'headlines': {'pearson': (0.45097597585047977, 7.52555822637532e-39), 'spearman': SpearmanrResult(correlation=0.4317594600014223, pvalue=2.077599243950802e-35), 'nsamples': 750}, 'OnWN': {'pearson': (0.13848929883743816, 0.0010064272334124704), 'spearman': SpearmanrResult(correlation=0.14652312636206116, pvalue=0.0004985555978027848), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.21728636170386104, 'wmean': 0.28514460580382356}, 'spearman': {'mean': 0.21830871446038422, 'wmean': 0.2803364674443483}}}, 'STS14': {'deft-forum': {'pearson': (-0.08240180485577009, 0.08079097812224358), 'spearman': SpearmanrResult(correlation=-0.08208005108896498, pvalue=0.08198661916363537), 'nsamples': 450}, 'deft-news': {'pearson': (0.42667966903744536, 1.056661043108385e-14), 'spearman': SpearmanrResult(correlation=0.4553462932834323, pvalue=9.219146902058772e-17), 'nsamples': 300}, 'headlines': {'pearson': (0.4271260026948364, 1.3024025662667834e-34), 'spearman': SpearmanrResult(correlation=0.4005398662349647, pvalue=2.8581261792905352e-30), 'nsamples': 750}, 'images': {'pearson': (0.1441775587302927, 7.41377465769117e-05), 'spearman': SpearmanrResult(correlation=0.1449269765366459, pvalue=6.793130348316458e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.35343496101313754, 1.734291164230278e-23), 'spearman': SpearmanrResult(correlation=0.3529623868229145, pvalue=2.0028175449672187e-23), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4854584202913478, 1.330205326396094e-45), 'spearman': SpearmanrResult(correlation=0.45334046816547474, pvalue=2.739897181289704e-39), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2924124678185483, 'wmean': 0.3062855454862261}, 'spearman': {'mean': 0.28750598999241117, 'wmean': 0.2969320368839988}}}, 'STS15': {'answers-forums': {'pearson': (0.302048270758448, 2.378186981588214e-09), 'spearman': SpearmanrResult(correlation=0.29208193831968227, pvalue=8.242808190641836e-09), 'nsamples': 375}, 'answers-students': {'pearson': (0.4124173737599264, 3.6702281462709573e-32), 'spearman': SpearmanrResult(correlation=0.4141890594389426, pvalue=1.887923830991198e-32), 'nsamples': 750}, 'belief': {'pearson': (0.42808240579073176, 3.8319882056659656e-18), 'spearman': SpearmanrResult(correlation=0.42667596062314, pvalue=5.0575998960079974e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.4903708963830672, 1.252215604125707e-46), 'spearman': SpearmanrResult(correlation=0.4812824612827008, pvalue=9.622326598162417e-45), 'nsamples': 750}, 'images': {'pearson': (0.20788168877003324, 9.112575618196672e-09), 'spearman': SpearmanrResult(correlation=0.21040724778533623, pvalue=5.959907508250104e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3681601270924413, 'wmean': 0.36893382429690413}, 'spearman': {'mean': 0.3649273334899604, 'wmean': 0.3663144294945977}}}, 'STS16': {'answer-answer': {'pearson': (0.33998258746252147, 2.726465844426062e-08), 'spearman': SpearmanrResult(correlation=0.34164614878461075, pvalue=2.308881551231742e-08), 'nsamples': 254}, 'headlines': {'pearson': (0.5665374640923926, 1.5382029615977046e-22), 'spearman': SpearmanrResult(correlation=0.5728356290984573, pvalue=4.0973055034003894e-23), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6224669658796115, 4.553631292240472e-26), 'spearman': SpearmanrResult(correlation=0.6525676973134201, pvalue=2.720938271395754e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.728111483233633, 1.423310098294675e-41), 'spearman': SpearmanrResult(correlation=0.7503892091105759, pvalue=2.0945117265875948e-45), 'nsamples': 244}, 'question-question': {'pearson': (0.031990282591026376, 0.6456438393521091), 'spearman': SpearmanrResult(correlation=0.08295607850377859, pvalue=0.23241970450147348), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.457817756651837, 'wmean': 0.46790554713096777}, 'spearman': {'mean': 0.4800789525621686, 'wmean': 0.4889861309014873}}}, 'MR': {'devacc': 78.91, 'acc': 78.31, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.44, 'acc': 85.09, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.98, 'acc': 86.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.12, 'acc': 94.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.37, 'acc': 83.8, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.87, 'acc': 43.71, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 80.77, 'acc': 87.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.76, 'acc': 66.55, 'f1': 79.9, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 71.4, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6614904315726032, 'pearson': 0.6806312749941023, 'spearman': 0.6284273875003842, 'mse': 0.5579650792507292, 'yhat': array([2.63094702, 4.23390679, 2.10152186, ..., 3.05305561, 4.67553791,        4.41310092]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.43988369008862993, 'pearson': 0.41027518369570704, 'spearman': 0.4093809362927808, 'mse': 2.0883983920712836, 'yhat': array([4.12377873, 1.73844707, 2.54401346, ..., 3.80695643, 2.88646699,        3.6986703 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.7, 'acc': 62.05, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 295.73199999999997, 'acc': [(23.28, 56.459999999999994, 72.16, 4.2), (19.968, 52.012, 68.768, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 56.28, 'acc': 56.04, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.15, 'acc': 24.7, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.29, 'acc': 26.95, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.64, 'acc': 73.11, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.73, 'acc': 84.01, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.59, 'acc': 88.73, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.34, 'acc': 82.14, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 70.09, 'acc': 71.25, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.48, 'acc': 59.84, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.67, 'acc': 69.2, 'ndev': 10002, 'ntest': 10002}}
2019-02-17 02:10:54,042 : STS12 p=0.2636, STS12 s=0.2904, STS13 p=0.2851, STS13 s=0.2803, STS14 p=0.3063, STS14 s=0.2969, STS15 p=0.3689, STS15 s=0.3663, STS 16 p=0.4679, STS16 s=0.4890, STS B p=0.4103, STS B s=0.4094, STS B m=2.0884, SICK-R p=0.6806, SICK-R s=0.6284, SICK-P m=0.5580
2019-02-17 02:10:54,042 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-17 02:10:54,042 : 0.2636,0.2904,0.2851,0.2803,0.3063,0.2969,0.3689,0.3663,0.4679,0.4890,0.4103,0.4094,2.0884,0.6806,0.6284,0.5580
2019-02-17 02:10:54,042 : MR=78.31, CR=85.09, SUBJ=94.10, MPQA=86.14, SST-B=83.80, SST-F=43.71, TREC=87.00, SICK-E=71.40, SNLI=62.05, MRPC=66.55, MRPC f=79.90
2019-02-17 02:10:54,042 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-17 02:10:54,042 : 78.31,85.09,94.10,86.14,83.80,43.71,87.00,71.40,62.05,66.55,79.90
2019-02-17 02:10:54,042 : COCO r1i2t=23.28, COCO r5i2t=56.46, COCO r10i2t=72.16, COCO medr_i2t=4.20, COCO r1t2i=19.97, COCO r5t2i=52.01, COCO r10t2i=68.77, COCO medr_t2i=5.00
2019-02-17 02:10:54,042 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-17 02:10:54,042 : 23.28,56.46,72.16,4.20,19.97,52.01,68.77,5.00
2019-02-17 02:10:54,043 : SentLen=56.04, WC=24.70, TreeDepth=26.95, TopConst=73.11, BShift=84.01, Tense=88.73, SubjNum=82.14, ObjNum=71.25, SOMO=59.84, CoordInv=69.20, average=63.60
2019-02-17 02:10:54,043 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-17 02:10:54,043 : 56.04,24.70,26.95,73.11,84.01,88.73,82.14,71.25,59.84,69.20,63.60
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,043 : layer 12
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,043 : ********************************************************************************
2019-02-17 02:10:54,174 : ***** Transfer task : STS12 *****


2019-02-17 02:10:54,191 : loading BERT model bert-base-uncased
2019-02-17 02:10:54,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:10:54,213 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:10:54,213 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp05bxios0
2019-02-17 02:10:57,068 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:01,313 : MSRpar : pearson = 0.2970, spearman = 0.3495
2019-02-17 02:11:02,144 : MSRvid : pearson = -0.0315, spearman = 0.0273
2019-02-17 02:11:02,870 : SMTeuroparl : pearson = 0.3484, spearman = 0.4575
2019-02-17 02:11:04,151 : surprise.OnWN : pearson = 0.4068, spearman = 0.3982
2019-02-17 02:11:04,862 : surprise.SMTnews : pearson = 0.4595, spearman = 0.4577
2019-02-17 02:11:04,862 : ALL (weighted average) : Pearson = 0.2727,             Spearman = 0.3133
2019-02-17 02:11:04,862 : ALL (average) : Pearson = 0.2961,             Spearman = 0.3380

2019-02-17 02:11:04,863 : ***** Transfer task : STS13 (-SMT) *****


2019-02-17 02:11:04,877 : loading BERT model bert-base-uncased
2019-02-17 02:11:04,877 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:11:04,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:11:04,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsfz9x5i3
2019-02-17 02:11:07,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:10,803 : FNWN : pearson = 0.0313, spearman = 0.0416
2019-02-17 02:11:11,779 : headlines : pearson = 0.4751, spearman = 0.4636
2019-02-17 02:11:12,538 : OnWN : pearson = 0.0347, spearman = 0.0616
2019-02-17 02:11:12,539 : ALL (weighted average) : Pearson = 0.2545,             Spearman = 0.2601
2019-02-17 02:11:12,539 : ALL (average) : Pearson = 0.1804,             Spearman = 0.1890

2019-02-17 02:11:12,539 : ***** Transfer task : STS14 *****


2019-02-17 02:11:12,564 : loading BERT model bert-base-uncased
2019-02-17 02:11:12,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:11:12,593 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:11:12,593 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpabl3_jb9
2019-02-17 02:11:15,720 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:18,721 : deft-forum : pearson = -0.0140, spearman = -0.0155
2019-02-17 02:11:19,517 : deft-news : pearson = 0.4460, spearman = 0.4791
2019-02-17 02:11:20,625 : headlines : pearson = 0.4369, spearman = 0.4114
2019-02-17 02:11:21,634 : images : pearson = 0.1312, spearman = 0.1509
2019-02-17 02:11:22,686 : OnWN : pearson = 0.2723, spearman = 0.2851
2019-02-17 02:11:24,074 : tweet-news : pearson = 0.3765, spearman = 0.3784
2019-02-17 02:11:24,075 : ALL (weighted average) : Pearson = 0.2774,             Spearman = 0.2816
2019-02-17 02:11:24,076 : ALL (average) : Pearson = 0.2748,             Spearman = 0.2816

2019-02-17 02:11:24,076 : ***** Transfer task : STS15 *****


2019-02-17 02:11:24,125 : loading BERT model bert-base-uncased
2019-02-17 02:11:24,125 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:11:24,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:11:24,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt97h1ld6
2019-02-17 02:11:27,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:30,175 : answers-forums : pearson = 0.2080, spearman = 0.2363
2019-02-17 02:11:31,229 : answers-students : pearson = 0.2880, spearman = 0.3260
2019-02-17 02:11:32,185 : belief : pearson = 0.4258, spearman = 0.4645
2019-02-17 02:11:33,297 : headlines : pearson = 0.5052, spearman = 0.5057
2019-02-17 02:11:34,321 : images : pearson = 0.1966, spearman = 0.2060
2019-02-17 02:11:34,322 : ALL (weighted average) : Pearson = 0.3267,             Spearman = 0.3470
2019-02-17 02:11:34,322 : ALL (average) : Pearson = 0.3247,             Spearman = 0.3477

2019-02-17 02:11:34,322 : ***** Transfer task : STS16 *****


2019-02-17 02:11:34,397 : loading BERT model bert-base-uncased
2019-02-17 02:11:34,397 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:11:34,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:11:34,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2jk6djwr
2019-02-17 02:11:36,864 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:38,709 : answer-answer : pearson = 0.3938, spearman = 0.4327
2019-02-17 02:11:39,062 : headlines : pearson = 0.5661, spearman = 0.5892
2019-02-17 02:11:39,499 : plagiarism : pearson = 0.6415, spearman = 0.6915
2019-02-17 02:11:40,194 : postediting : pearson = 0.6503, spearman = 0.6819
2019-02-17 02:11:40,514 : question-question : pearson = 0.1060, spearman = 0.2086
2019-02-17 02:11:40,515 : ALL (weighted average) : Pearson = 0.4801,             Spearman = 0.5275
2019-02-17 02:11:40,515 : ALL (average) : Pearson = 0.4715,             Spearman = 0.5208

2019-02-17 02:11:40,515 : ***** Transfer task : MR *****


2019-02-17 02:11:40,534 : loading BERT model bert-base-uncased
2019-02-17 02:11:40,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:11:40,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:11:40,555 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmd4xgdhx
2019-02-17 02:11:43,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:11:44,535 : Generating sentence embeddings
2019-02-17 02:11:58,422 : Generated sentence embeddings
2019-02-17 02:11:58,422 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 02:12:13,381 : Best param found at split 1: l2reg = 0.0001                 with score 78.67
2019-02-17 02:12:30,093 : Best param found at split 2: l2reg = 0.0001                 with score 79.39
2019-02-17 02:12:46,471 : Best param found at split 3: l2reg = 0.01                 with score 79.61
2019-02-17 02:13:03,125 : Best param found at split 4: l2reg = 0.01                 with score 79.24
2019-02-17 02:13:19,480 : Best param found at split 5: l2reg = 0.01                 with score 79.32
2019-02-17 02:13:20,721 : Dev acc : 79.25 Test acc : 79.23

2019-02-17 02:13:20,722 : ***** Transfer task : CR *****


2019-02-17 02:13:20,730 : loading BERT model bert-base-uncased
2019-02-17 02:13:20,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:13:20,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:13:20,752 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp16jwela
2019-02-17 02:13:23,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:13:24,631 : Generating sentence embeddings
2019-02-17 02:13:28,362 : Generated sentence embeddings
2019-02-17 02:13:28,363 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 02:13:35,659 : Best param found at split 1: l2reg = 0.0001                 with score 86.16
2019-02-17 02:13:41,009 : Best param found at split 2: l2reg = 0.0001                 with score 85.62
2019-02-17 02:13:47,481 : Best param found at split 3: l2reg = 1e-05                 with score 86.59
2019-02-17 02:13:52,886 : Best param found at split 4: l2reg = 0.0001                 with score 85.5
2019-02-17 02:13:59,410 : Best param found at split 5: l2reg = 1e-05                 with score 85.53
2019-02-17 02:13:59,672 : Dev acc : 85.88 Test acc : 84.74

2019-02-17 02:13:59,673 : ***** Transfer task : MPQA *****


2019-02-17 02:13:59,680 : loading BERT model bert-base-uncased
2019-02-17 02:13:59,680 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:13:59,704 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:13:59,704 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoiczoium
2019-02-17 02:14:02,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:14:03,779 : Generating sentence embeddings
2019-02-17 02:14:07,547 : Generated sentence embeddings
2019-02-17 02:14:07,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 02:14:24,745 : Best param found at split 1: l2reg = 1e-05                 with score 86.69
2019-02-17 02:14:40,174 : Best param found at split 2: l2reg = 0.0001                 with score 86.53
2019-02-17 02:14:59,411 : Best param found at split 3: l2reg = 1e-05                 with score 86.56
2019-02-17 02:15:16,891 : Best param found at split 4: l2reg = 0.0001                 with score 87.14
2019-02-17 02:15:35,940 : Best param found at split 5: l2reg = 0.001                 with score 86.6
2019-02-17 02:15:36,702 : Dev acc : 86.7 Test acc : 87.57

2019-02-17 02:15:36,703 : ***** Transfer task : SUBJ *****


2019-02-17 02:15:36,721 : loading BERT model bert-base-uncased
2019-02-17 02:15:36,721 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:15:36,741 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:15:36,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmc4l87ed
2019-02-17 02:15:39,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:15:40,696 : Generating sentence embeddings
2019-02-17 02:15:53,917 : Generated sentence embeddings
2019-02-17 02:15:53,917 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-17 02:16:10,745 : Best param found at split 1: l2reg = 0.001                 with score 95.35
2019-02-17 02:16:25,453 : Best param found at split 2: l2reg = 1e-05                 with score 95.14
2019-02-17 02:16:41,662 : Best param found at split 3: l2reg = 0.001                 with score 95.43
2019-02-17 02:16:57,392 : Best param found at split 4: l2reg = 1e-05                 with score 95.6
2019-02-17 02:17:13,537 : Best param found at split 5: l2reg = 0.001                 with score 95.1
2019-02-17 02:17:14,476 : Dev acc : 95.32 Test acc : 94.7

2019-02-17 02:17:14,477 : ***** Transfer task : SST Binary classification *****


2019-02-17 02:17:14,623 : loading BERT model bert-base-uncased
2019-02-17 02:17:14,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:17:14,650 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:17:14,650 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfog7o3ut
2019-02-17 02:17:17,121 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:17:18,607 : Computing embedding for train
2019-02-17 02:18:04,483 : Computed train embeddings
2019-02-17 02:18:04,483 : Computing embedding for dev
2019-02-17 02:18:05,422 : Computed dev embeddings
2019-02-17 02:18:05,422 : Computing embedding for test
2019-02-17 02:18:07,398 : Computed test embeddings
2019-02-17 02:18:07,398 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:18:36,993 : [('reg:1e-05', 82.45), ('reg:0.0001', 82.68), ('reg:0.001', 81.88), ('reg:0.01', 83.14)]
2019-02-17 02:18:36,993 : Validation : best param found is reg = 0.01 with score             83.14
2019-02-17 02:18:36,993 : Evaluating...
2019-02-17 02:18:42,177 : 
Dev acc : 83.14 Test acc : 83.2 for             SST Binary classification

2019-02-17 02:18:42,178 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-17 02:18:42,229 : loading BERT model bert-base-uncased
2019-02-17 02:18:42,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:18:42,252 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:18:42,253 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpks9hhe68
2019-02-17 02:18:44,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:18:46,218 : Computing embedding for train
2019-02-17 02:18:55,613 : Computed train embeddings
2019-02-17 02:18:55,614 : Computing embedding for dev
2019-02-17 02:18:56,837 : Computed dev embeddings
2019-02-17 02:18:56,837 : Computing embedding for test
2019-02-17 02:18:59,238 : Computed test embeddings
2019-02-17 02:18:59,238 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:19:03,678 : [('reg:1e-05', 41.05), ('reg:0.0001', 41.51), ('reg:0.001', 41.51), ('reg:0.01', 43.69)]
2019-02-17 02:19:03,678 : Validation : best param found is reg = 0.01 with score             43.69
2019-02-17 02:19:03,678 : Evaluating...
2019-02-17 02:19:04,458 : 
Dev acc : 43.69 Test acc : 43.21 for             SST Fine-Grained classification

2019-02-17 02:19:04,459 : ***** Transfer task : TREC *****


2019-02-17 02:19:04,473 : loading BERT model bert-base-uncased
2019-02-17 02:19:04,473 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:19:04,494 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:19:04,494 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyij00nvf
2019-02-17 02:19:07,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:19:11,945 : Computed train embeddings
2019-02-17 02:19:12,203 : Computed test embeddings
2019-02-17 02:19:12,203 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 02:19:23,661 : [('reg:1e-05', 81.44), ('reg:0.0001', 81.33), ('reg:0.001', 79.23), ('reg:0.01', 69.3)]
2019-02-17 02:19:23,662 : Cross-validation : best param found is reg = 1e-05             with score 81.44
2019-02-17 02:19:23,662 : Evaluating...
2019-02-17 02:19:24,206 : 
Dev acc : 81.44 Test acc : 87.0             for TREC

2019-02-17 02:19:24,206 : ***** Transfer task : MRPC *****


2019-02-17 02:19:24,228 : loading BERT model bert-base-uncased
2019-02-17 02:19:24,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:19:24,253 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:19:24,253 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppyatj1m1
2019-02-17 02:19:26,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:19:28,247 : Computing embedding for train
2019-02-17 02:19:38,088 : Computed train embeddings
2019-02-17 02:19:38,089 : Computing embedding for test
2019-02-17 02:19:42,699 : Computed test embeddings
2019-02-17 02:19:42,715 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-17 02:19:51,547 : [('reg:1e-05', 69.95), ('reg:0.0001', 70.24), ('reg:0.001', 70.41), ('reg:0.01', 70.78)]
2019-02-17 02:19:51,547 : Cross-validation : best param found is reg = 0.01             with score 70.78
2019-02-17 02:19:51,547 : Evaluating...
2019-02-17 02:19:51,997 : Dev acc : 70.78 Test acc 69.74; Test F1 78.52 for MRPC.

2019-02-17 02:19:51,998 : ***** Transfer task : SICK-Entailment*****


2019-02-17 02:19:52,061 : loading BERT model bert-base-uncased
2019-02-17 02:19:52,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:19:52,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:19:52,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbpkavwuh
2019-02-17 02:19:54,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:19:55,970 : Computing embedding for train
2019-02-17 02:20:01,151 : Computed train embeddings
2019-02-17 02:20:01,151 : Computing embedding for dev
2019-02-17 02:20:01,866 : Computed dev embeddings
2019-02-17 02:20:01,866 : Computing embedding for test
2019-02-17 02:20:07,435 : Computed test embeddings
2019-02-17 02:20:07,463 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:20:09,412 : [('reg:1e-05', 71.6), ('reg:0.0001', 72.0), ('reg:0.001', 72.2), ('reg:0.01', 66.8)]
2019-02-17 02:20:09,412 : Validation : best param found is reg = 0.001 with score             72.2
2019-02-17 02:20:09,412 : Evaluating...
2019-02-17 02:20:09,749 : 
Dev acc : 72.2 Test acc : 68.56 for                        SICK entailment

2019-02-17 02:20:09,749 : ***** Transfer task : SICK-Relatedness*****


2019-02-17 02:20:09,781 : loading BERT model bert-base-uncased
2019-02-17 02:20:09,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:20:09,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:20:09,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpisn1uruc
2019-02-17 02:20:12,409 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:20:13,919 : Computing embedding for train
2019-02-17 02:20:19,038 : Computed train embeddings
2019-02-17 02:20:19,038 : Computing embedding for dev
2019-02-17 02:20:19,725 : Computed dev embeddings
2019-02-17 02:20:19,725 : Computing embedding for test
2019-02-17 02:20:25,299 : Computed test embeddings
2019-02-17 02:20:47,445 : Dev : Pearson 0.6809848077441052
2019-02-17 02:20:47,446 : Test : Pearson 0.6945593971919223 Spearman 0.6415164105592593 MSE 0.5267593878003589                        for SICK Relatedness

2019-02-17 02:20:47,446 : 

***** Transfer task : STSBenchmark*****


2019-02-17 02:20:47,487 : loading BERT model bert-base-uncased
2019-02-17 02:20:47,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:20:47,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:20:47,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpobpvrhp4
2019-02-17 02:20:50,039 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:20:51,485 : Computing embedding for train
2019-02-17 02:20:59,673 : Computed train embeddings
2019-02-17 02:20:59,673 : Computing embedding for dev
2019-02-17 02:21:02,201 : Computed dev embeddings
2019-02-17 02:21:02,201 : Computing embedding for test
2019-02-17 02:21:04,176 : Computed test embeddings
2019-02-17 02:21:33,810 : Dev : Pearson 0.5469116428416618
2019-02-17 02:21:33,810 : Test : Pearson 0.4650535822098155 Spearman 0.46686339421803735 MSE 2.0349728254746275                        for SICK Relatedness

2019-02-17 02:21:33,810 : ***** Transfer task : SNLI Entailment*****


2019-02-17 02:21:38,852 : loading BERT model bert-base-uncased
2019-02-17 02:21:38,852 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:21:38,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:21:38,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyl01skev
2019-02-17 02:21:41,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:21:43,161 : PROGRESS (encoding): 0.00%
2019-02-17 02:23:01,884 : PROGRESS (encoding): 14.56%
2019-02-17 02:24:29,300 : PROGRESS (encoding): 29.12%
2019-02-17 02:25:57,023 : PROGRESS (encoding): 43.69%
2019-02-17 02:27:32,046 : PROGRESS (encoding): 58.25%
2019-02-17 02:29:20,655 : PROGRESS (encoding): 72.81%
2019-02-17 02:31:05,839 : PROGRESS (encoding): 87.37%
2019-02-17 02:32:55,778 : PROGRESS (encoding): 0.00%
2019-02-17 02:33:09,291 : PROGRESS (encoding): 0.00%
2019-02-17 02:33:22,372 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 02:34:09,184 : [('reg:1e-09', 54.03)]
2019-02-17 02:34:09,184 : Validation : best param found is reg = 1e-09 with score             54.03
2019-02-17 02:34:09,185 : Evaluating...
2019-02-17 02:34:57,445 : Dev acc : 54.03 Test acc : 54.58 for SNLI

2019-02-17 02:34:57,445 : ***** Transfer task: Image Caption Retrieval *****


2019-02-17 02:35:06,717 : loading BERT model bert-base-uncased
2019-02-17 02:35:06,717 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 02:35:06,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 02:35:06,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt8rycs9g
2019-02-17 02:35:09,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 02:35:10,864 : Computing embedding for train
2019-02-17 02:42:42,797 : Computed train embeddings
2019-02-17 02:42:42,797 : Computing embedding for dev
2019-02-17 02:43:02,051 : Computed dev embeddings
2019-02-17 02:43:02,051 : Computing embedding for test
2019-02-17 02:43:22,171 : Computed test embeddings
2019-02-17 02:43:22,187 : prepare data
2019-02-17 02:43:22,253 : start epoch
2019-02-17 02:44:06,031 : samples : 64000
2019-02-17 02:44:16,479 : Image to text: 4.4, 15.44, 24.66, 37.0
2019-02-17 02:44:24,060 : Text to Image: 3.868, 13.008, 21.292, 43.0
2019-02-17 02:45:06,929 : samples : 128000
2019-02-17 02:45:17,196 : Image to text: 5.02, 16.52, 25.92, 34.0
2019-02-17 02:45:25,201 : Text to Image: 4.012, 13.952, 22.516, 40.0
2019-02-17 02:46:17,066 : samples : 192000
2019-02-17 02:46:27,590 : Image to text: 5.88, 18.54, 28.5, 31.0
2019-02-17 02:46:35,197 : Text to Image: 4.608, 15.52, 24.324, 36.0
2019-02-17 02:47:18,183 : samples : 256000
2019-02-17 02:47:28,641 : Image to text: 5.92, 19.14, 28.96, 28.0
2019-02-17 02:47:36,267 : Text to Image: 5.248, 16.624, 26.484, 32.0
2019-02-17 02:48:19,202 : samples : 320000
2019-02-17 02:48:29,746 : Image to text: 6.14, 19.48, 29.62, 27.0
2019-02-17 02:48:37,391 : Text to Image: 5.196, 16.964, 26.416, 32.0
2019-02-17 02:49:20,683 : samples : 384000
2019-02-17 02:49:31,118 : Image to text: 6.56, 20.76, 31.18, 25.0
2019-02-17 02:49:38,686 : Text to Image: 5.52, 18.208, 28.344, 30.0
2019-02-17 02:50:21,522 : samples : 448000
2019-02-17 02:50:32,053 : Image to text: 6.48, 20.84, 31.46, 25.0
2019-02-17 02:50:39,763 : Text to Image: 5.456, 18.2, 27.98, 30.0
2019-02-17 02:51:22,847 : samples : 512000
2019-02-17 02:51:33,310 : Image to text: 6.46, 21.34, 32.7, 24.0
2019-02-17 02:51:40,763 : Text to Image: 5.776, 18.304, 28.504, 29.0
2019-02-17 02:52:17,165 : Epoch 1 finished
2019-02-17 02:52:17,625 : Image to text: 19.8, 49.9, 66.4, 6.0
2019-02-17 02:52:17,975 : Text to Image: 16.66, 44.8, 62.16, 7.0
2019-02-17 02:52:18,435 : Image to text: 19.6, 46.2, 64.1, 6.0
2019-02-17 02:52:18,793 : Text to Image: 15.84, 44.16, 62.54, 7.0
2019-02-17 02:52:19,253 : Image to text: 20.4, 49.1, 65.8, 6.0
2019-02-17 02:52:19,608 : Text to Image: 15.52, 44.3, 61.38, 7.0
2019-02-17 02:52:20,065 : Image to text: 19.6, 49.7, 66.8, 6.0
2019-02-17 02:52:20,426 : Text to Image: 16.78, 44.6, 62.56, 7.0
2019-02-17 02:52:20,880 : Image to text: 20.0, 48.7, 66.8, 6.0
2019-02-17 02:52:21,231 : Text to Image: 17.1, 45.34, 61.82, 7.0
2019-02-17 02:52:21,231 : Dev mean Text to Image: 16.38, 44.64, 62.092, 7.0
2019-02-17 02:52:21,231 : Dev mean Image to text: 19.880000000000003, 48.720000000000006, 65.98, 6.0
2019-02-17 02:52:21,231 : start epoch
2019-02-17 02:53:04,097 : samples : 64000
2019-02-17 02:53:14,682 : Image to text: 7.04, 21.72, 33.24, 23.0
2019-02-17 02:53:22,291 : Text to Image: 6.06, 18.924, 29.176, 28.0
2019-02-17 02:54:05,353 : samples : 128000
2019-02-17 02:54:15,801 : Image to text: 7.24, 22.36, 33.66, 23.0
2019-02-17 02:54:23,424 : Text to Image: 5.952, 18.792, 29.044, 28.0
2019-02-17 02:55:06,563 : samples : 192000
2019-02-17 02:55:17,057 : Image to text: 6.64, 22.2, 33.58, 23.0
2019-02-17 02:55:24,725 : Text to Image: 6.1, 19.54, 29.808, 27.0
2019-02-17 02:56:07,689 : samples : 256000
2019-02-17 02:56:18,177 : Image to text: 8.12, 22.82, 33.82, 22.0
2019-02-17 02:56:25,817 : Text to Image: 6.708, 20.6, 30.924, 26.0
2019-02-17 02:57:09,210 : samples : 320000
2019-02-17 02:57:19,687 : Image to text: 7.94, 24.5, 35.74, 21.0
2019-02-17 02:57:27,348 : Text to Image: 6.52, 20.524, 30.752, 26.0
2019-02-17 02:58:10,542 : samples : 384000
2019-02-17 02:58:21,048 : Image to text: 7.42, 22.28, 33.54, 23.0
2019-02-17 02:58:28,731 : Text to Image: 6.36, 19.952, 30.652, 26.0
2019-02-17 02:59:11,961 : samples : 448000
2019-02-17 02:59:22,516 : Image to text: 7.32, 23.04, 34.38, 21.0
2019-02-17 02:59:30,420 : Text to Image: 6.484, 20.436, 31.056, 26.0
2019-02-17 03:00:13,803 : samples : 512000
2019-02-17 03:00:24,226 : Image to text: 7.66, 23.7, 35.68, 20.0
2019-02-17 03:00:31,712 : Text to Image: 6.54, 20.672, 31.268, 25.0
2019-02-17 03:01:08,380 : Epoch 2 finished
2019-02-17 03:01:08,841 : Image to text: 21.8, 53.8, 69.1, 5.0
2019-02-17 03:01:09,200 : Text to Image: 18.4, 48.26, 66.1, 6.0
2019-02-17 03:01:09,670 : Image to text: 19.3, 51.1, 68.7, 5.0
2019-02-17 03:01:10,029 : Text to Image: 17.6, 47.48, 64.76, 6.0
2019-02-17 03:01:10,479 : Image to text: 22.1, 55.8, 70.0, 4.0
2019-02-17 03:01:10,824 : Text to Image: 17.54, 47.52, 64.9, 6.0
2019-02-17 03:01:11,311 : Image to text: 21.1, 53.4, 69.3, 5.0
2019-02-17 03:01:11,653 : Text to Image: 18.12, 47.56, 64.58, 6.0
2019-02-17 03:01:12,107 : Image to text: 23.0, 54.3, 69.6, 5.0
2019-02-17 03:01:12,459 : Text to Image: 18.64, 47.76, 64.24, 6.0
2019-02-17 03:01:12,459 : Dev mean Text to Image: 18.060000000000002, 47.716, 64.916, 6.0
2019-02-17 03:01:12,459 : Dev mean Image to text: 21.46, 53.68, 69.34, 4.8
2019-02-17 03:01:12,460 : start epoch
2019-02-17 03:01:55,402 : samples : 64000
2019-02-17 03:02:05,902 : Image to text: 7.38, 22.54, 33.5, 22.0
2019-02-17 03:02:13,539 : Text to Image: 5.856, 19.3, 29.584, 27.0
2019-02-17 03:03:04,032 : samples : 128000
2019-02-17 03:03:16,362 : Image to text: 7.24, 23.34, 35.36, 21.0
2019-02-17 03:03:24,259 : Text to Image: 6.452, 20.304, 31.08, 25.0
2019-02-17 03:04:07,714 : samples : 192000
2019-02-17 03:04:18,173 : Image to text: 7.98, 23.58, 36.0, 20.0
2019-02-17 03:04:25,802 : Text to Image: 6.612, 21.204, 31.92, 25.0
2019-02-17 03:05:08,462 : samples : 256000
2019-02-17 03:05:18,990 : Image to text: 8.06, 24.86, 37.24, 19.0
2019-02-17 03:05:26,679 : Text to Image: 7.008, 21.356, 31.748, 25.0
2019-02-17 03:06:08,991 : samples : 320000
2019-02-17 03:06:19,440 : Image to text: 7.42, 23.56, 35.44, 20.0
2019-02-17 03:06:27,163 : Text to Image: 6.784, 21.372, 32.04, 24.0
2019-02-17 03:07:09,818 : samples : 384000
2019-02-17 03:07:20,311 : Image to text: 7.84, 23.74, 35.58, 21.0
2019-02-17 03:07:27,985 : Text to Image: 6.704, 20.536, 31.136, 25.0
2019-02-17 03:08:10,879 : samples : 448000
2019-02-17 03:08:21,385 : Image to text: 8.28, 24.42, 36.44, 20.0
2019-02-17 03:08:29,049 : Text to Image: 7.204, 21.94, 32.888, 23.0
2019-02-17 03:09:11,530 : samples : 512000
2019-02-17 03:09:21,936 : Image to text: 7.86, 24.4, 35.94, 20.0
2019-02-17 03:09:29,390 : Text to Image: 6.76, 21.256, 32.056, 24.0
2019-02-17 03:10:05,793 : Epoch 3 finished
2019-02-17 03:10:06,281 : Image to text: 21.6, 53.4, 69.6, 5.0
2019-02-17 03:10:06,652 : Text to Image: 18.7, 48.54, 67.18, 6.0
2019-02-17 03:10:07,110 : Image to text: 20.4, 52.4, 67.6, 5.0
2019-02-17 03:10:07,457 : Text to Image: 17.6, 49.84, 66.84, 6.0
2019-02-17 03:10:07,899 : Image to text: 21.5, 52.8, 68.5, 5.0
2019-02-17 03:10:08,240 : Text to Image: 18.1, 47.88, 65.78, 6.0
2019-02-17 03:10:08,686 : Image to text: 21.2, 54.8, 70.9, 5.0
2019-02-17 03:10:09,028 : Text to Image: 18.5, 48.66, 66.14, 6.0
2019-02-17 03:10:09,466 : Image to text: 23.4, 52.5, 69.1, 5.0
2019-02-17 03:10:09,807 : Text to Image: 18.78, 49.58, 66.46, 6.0
2019-02-17 03:10:09,808 : Dev mean Text to Image: 18.336, 48.900000000000006, 66.48, 6.0
2019-02-17 03:10:09,808 : Dev mean Image to text: 21.619999999999997, 53.18, 69.14, 5.0
2019-02-17 03:10:09,808 : start epoch
2019-02-17 03:10:52,214 : samples : 64000
2019-02-17 03:11:02,715 : Image to text: 7.94, 23.86, 34.6, 20.0
2019-02-17 03:11:10,388 : Text to Image: 6.676, 20.42, 31.236, 25.0
2019-02-17 03:11:53,275 : samples : 128000
2019-02-17 03:12:03,745 : Image to text: 8.34, 25.48, 37.72, 18.0
2019-02-17 03:12:11,406 : Text to Image: 7.284, 21.94, 32.836, 23.0
2019-02-17 03:12:54,889 : samples : 192000
2019-02-17 03:13:05,374 : Image to text: 7.84, 23.8, 35.82, 20.0
2019-02-17 03:13:13,092 : Text to Image: 6.624, 20.848, 31.868, 24.0
2019-02-17 03:13:56,073 : samples : 256000
2019-02-17 03:14:06,576 : Image to text: 8.34, 24.24, 36.86, 19.0
2019-02-17 03:14:14,172 : Text to Image: 7.292, 22.156, 33.524, 23.0
2019-02-17 03:14:56,317 : samples : 320000
2019-02-17 03:15:06,821 : Image to text: 7.82, 24.2, 36.92, 19.0
2019-02-17 03:15:14,490 : Text to Image: 6.82, 21.652, 32.3, 24.0
2019-02-17 03:15:57,346 : samples : 384000
2019-02-17 03:16:07,940 : Image to text: 7.88, 24.16, 36.18, 19.0
2019-02-17 03:16:15,619 : Text to Image: 7.14, 21.716, 32.756, 24.0
2019-02-17 03:16:58,024 : samples : 448000
2019-02-17 03:17:08,490 : Image to text: 8.2, 25.14, 37.18, 19.0
2019-02-17 03:17:16,104 : Text to Image: 7.192, 21.924, 33.232, 23.0
2019-02-17 03:17:58,626 : samples : 512000
2019-02-17 03:18:09,076 : Image to text: 8.24, 25.74, 37.56, 17.0
2019-02-17 03:18:16,621 : Text to Image: 7.02, 22.084, 33.24, 23.0
2019-02-17 03:18:52,634 : Epoch 4 finished
2019-02-17 03:18:53,104 : Image to text: 24.7, 57.1, 71.4, 4.0
2019-02-17 03:18:53,457 : Text to Image: 20.28, 51.94, 69.26, 5.0
2019-02-17 03:18:53,914 : Image to text: 21.7, 54.2, 70.9, 5.0
2019-02-17 03:18:54,272 : Text to Image: 18.98, 51.0, 68.94, 5.0
2019-02-17 03:18:54,733 : Image to text: 24.0, 55.0, 72.2, 4.0
2019-02-17 03:18:55,089 : Text to Image: 19.42, 50.44, 68.48, 5.0
2019-02-17 03:18:55,551 : Image to text: 23.3, 56.6, 70.9, 4.0
2019-02-17 03:18:55,910 : Text to Image: 19.16, 51.24, 68.84, 5.0
2019-02-17 03:18:56,371 : Image to text: 24.8, 56.1, 71.8, 4.0
2019-02-17 03:18:56,727 : Text to Image: 20.26, 50.56, 67.34, 5.0
2019-02-17 03:18:56,728 : Dev mean Text to Image: 19.62, 51.03600000000001, 68.572, 5.0
2019-02-17 03:18:56,728 : Dev mean Image to text: 23.7, 55.8, 71.44, 4.2
2019-02-17 03:18:56,728 : start epoch
2019-02-17 03:19:38,814 : samples : 64000
2019-02-17 03:19:49,433 : Image to text: 8.46, 25.62, 38.4, 18.0
2019-02-17 03:19:58,142 : Text to Image: 7.644, 23.292, 34.344, 22.0
2019-02-17 03:20:49,208 : samples : 128000
2019-02-17 03:20:59,699 : Image to text: 7.9, 25.44, 37.58, 18.0
2019-02-17 03:21:07,296 : Text to Image: 7.192, 22.04, 33.248, 22.0
2019-02-17 03:21:50,289 : samples : 192000
2019-02-17 03:22:00,792 : Image to text: 8.34, 25.74, 37.98, 18.0
2019-02-17 03:22:08,553 : Text to Image: 7.372, 22.5, 33.768, 22.0
2019-02-17 03:22:50,737 : samples : 256000
2019-02-17 03:23:01,280 : Image to text: 8.84, 25.98, 37.78, 19.0
2019-02-17 03:23:08,950 : Text to Image: 7.224, 22.324, 33.548, 23.0
2019-02-17 03:23:51,909 : samples : 320000
2019-02-17 03:24:02,415 : Image to text: 8.36, 26.06, 37.96, 19.0
2019-02-17 03:24:10,079 : Text to Image: 7.492, 22.256, 33.548, 22.0
2019-02-17 03:24:53,091 : samples : 384000
2019-02-17 03:25:03,605 : Image to text: 7.56, 25.2, 36.82, 19.0
2019-02-17 03:25:11,324 : Text to Image: 7.332, 22.472, 33.616, 22.0
2019-02-17 03:25:54,576 : samples : 448000
2019-02-17 03:26:05,076 : Image to text: 8.08, 25.78, 37.66, 18.0
2019-02-17 03:26:12,851 : Text to Image: 7.312, 22.364, 33.508, 22.0
2019-02-17 03:26:55,594 : samples : 512000
2019-02-17 03:27:05,960 : Image to text: 8.84, 26.18, 38.4, 18.0
2019-02-17 03:27:13,508 : Text to Image: 7.716, 22.828, 34.248, 22.0
2019-02-17 03:27:50,129 : Epoch 5 finished
2019-02-17 03:27:50,612 : Image to text: 25.0, 58.2, 73.7, 4.0
2019-02-17 03:27:50,965 : Text to Image: 20.68, 52.1, 69.08, 5.0
2019-02-17 03:27:51,431 : Image to text: 23.1, 55.2, 71.5, 4.0
2019-02-17 03:27:51,784 : Text to Image: 18.94, 51.38, 68.68, 5.0
2019-02-17 03:27:52,251 : Image to text: 24.6, 55.4, 71.2, 4.0
2019-02-17 03:27:52,600 : Text to Image: 19.26, 50.8, 68.36, 5.0
2019-02-17 03:27:53,076 : Image to text: 25.3, 57.4, 72.9, 4.0
2019-02-17 03:27:53,425 : Text to Image: 20.22, 51.38, 68.38, 5.0
2019-02-17 03:27:53,889 : Image to text: 26.3, 56.5, 73.7, 4.0
2019-02-17 03:27:54,245 : Text to Image: 20.66, 51.84, 69.06, 5.0
2019-02-17 03:27:54,245 : Dev mean Text to Image: 19.951999999999998, 51.5, 68.712, 5.0
2019-02-17 03:27:54,245 : Dev mean Image to text: 24.86, 56.53999999999999, 72.6, 4.0
2019-02-17 03:27:54,246 : start epoch
2019-02-17 03:28:36,693 : samples : 64000
2019-02-17 03:28:47,259 : Image to text: 8.3, 26.54, 38.32, 18.0
2019-02-17 03:28:54,919 : Text to Image: 7.76, 23.204, 34.612, 21.0
2019-02-17 03:29:37,953 : samples : 128000
2019-02-17 03:29:48,540 : Image to text: 8.42, 26.0, 38.3, 18.0
2019-02-17 03:29:56,277 : Text to Image: 7.564, 22.412, 33.82, 22.0
2019-02-17 03:30:39,574 : samples : 192000
2019-02-17 03:30:50,086 : Image to text: 8.24, 26.28, 39.04, 18.0
2019-02-17 03:30:57,774 : Text to Image: 7.556, 22.788, 34.136, 22.0
2019-02-17 03:31:40,462 : samples : 256000
2019-02-17 03:31:50,969 : Image to text: 8.28, 24.72, 36.84, 19.0
2019-02-17 03:31:58,703 : Text to Image: 7.204, 22.288, 33.496, 22.0
2019-02-17 03:32:41,410 : samples : 320000
2019-02-17 03:32:52,018 : Image to text: 8.34, 25.6, 38.18, 18.0
2019-02-17 03:32:59,808 : Text to Image: 7.44, 23.116, 34.424, 22.0
2019-02-17 03:33:42,079 : samples : 384000
2019-02-17 03:33:52,551 : Image to text: 8.46, 25.64, 37.02, 18.0
2019-02-17 03:34:00,269 : Text to Image: 7.544, 22.804, 33.928, 22.0
2019-02-17 03:34:42,685 : samples : 448000
2019-02-17 03:34:53,150 : Image to text: 8.72, 26.52, 38.88, 18.0
2019-02-17 03:35:00,932 : Text to Image: 7.5, 22.712, 34.276, 22.0
2019-02-17 03:35:44,114 : samples : 512000
2019-02-17 03:35:54,578 : Image to text: 9.14, 26.12, 38.88, 17.0
2019-02-17 03:36:02,184 : Text to Image: 7.716, 23.032, 34.592, 21.0
2019-02-17 03:36:38,436 : Epoch 6 finished
2019-02-17 03:36:38,898 : Image to text: 25.2, 57.6, 72.1, 4.0
2019-02-17 03:36:39,246 : Text to Image: 21.34, 53.52, 71.36, 5.0
2019-02-17 03:36:39,703 : Image to text: 22.3, 55.2, 73.9, 4.0
2019-02-17 03:36:40,051 : Text to Image: 20.44, 53.4, 70.66, 5.0
2019-02-17 03:36:40,508 : Image to text: 25.5, 57.9, 73.8, 4.0
2019-02-17 03:36:40,864 : Text to Image: 21.36, 52.9, 70.12, 5.0
2019-02-17 03:36:41,328 : Image to text: 23.1, 57.5, 73.2, 4.0
2019-02-17 03:36:41,669 : Text to Image: 20.44, 52.36, 70.28, 5.0
2019-02-17 03:36:42,129 : Image to text: 24.2, 59.2, 73.1, 4.0
2019-02-17 03:36:42,479 : Text to Image: 21.48, 53.2, 69.8, 5.0
2019-02-17 03:36:42,479 : Dev mean Text to Image: 21.012, 53.076, 70.44399999999999, 5.0
2019-02-17 03:36:42,479 : Dev mean Image to text: 24.06, 57.480000000000004, 73.22, 4.0
2019-02-17 03:36:42,479 : start epoch
2019-02-17 03:37:29,850 : samples : 64000
2019-02-17 03:37:42,456 : Image to text: 9.3, 26.64, 39.1, 17.0
2019-02-17 03:37:51,308 : Text to Image: 7.78, 23.648, 35.068, 21.0
2019-02-17 03:38:34,092 : samples : 128000
2019-02-17 03:38:44,631 : Image to text: 9.34, 28.02, 39.48, 17.0
2019-02-17 03:38:52,488 : Text to Image: 8.024, 23.904, 35.244, 21.0
2019-02-17 03:39:34,978 : samples : 192000
2019-02-17 03:39:45,497 : Image to text: 9.06, 27.02, 39.48, 17.0
2019-02-17 03:39:53,140 : Text to Image: 7.78, 23.624, 35.468, 21.0
2019-02-17 03:40:36,089 : samples : 256000
2019-02-17 03:40:46,656 : Image to text: 8.7, 25.48, 37.96, 18.0
2019-02-17 03:40:54,377 : Text to Image: 7.776, 23.26, 34.808, 21.0
2019-02-17 03:41:37,542 : samples : 320000
2019-02-17 03:41:48,301 : Image to text: 8.12, 25.7, 38.48, 18.0
2019-02-17 03:41:56,019 : Text to Image: 7.48, 23.112, 34.48, 21.0
2019-02-17 03:42:38,390 : samples : 384000
2019-02-17 03:42:48,857 : Image to text: 8.66, 25.74, 39.0, 17.0
2019-02-17 03:42:56,580 : Text to Image: 7.984, 23.756, 35.428, 21.0
2019-02-17 03:43:39,042 : samples : 448000
2019-02-17 03:43:49,625 : Image to text: 8.98, 26.98, 39.12, 17.0
2019-02-17 03:43:57,340 : Text to Image: 8.064, 23.588, 35.048, 21.0
2019-02-17 03:44:39,996 : samples : 512000
2019-02-17 03:44:50,380 : Image to text: 9.16, 27.7, 40.14, 17.0
2019-02-17 03:44:57,380 : Text to Image: 8.052, 24.088, 35.724, 20.0
2019-02-17 03:45:35,584 : Epoch 7 finished
2019-02-17 03:45:36,538 : Image to text: 24.1, 57.2, 73.5, 4.0
2019-02-17 03:45:37,317 : Text to Image: 21.12, 53.68, 70.94, 5.0
2019-02-17 03:45:38,263 : Image to text: 25.1, 57.2, 73.2, 4.0
2019-02-17 03:45:39,044 : Text to Image: 20.9, 53.7, 70.58, 5.0
2019-02-17 03:45:40,020 : Image to text: 24.9, 55.7, 71.9, 4.0
2019-02-17 03:45:40,766 : Text to Image: 20.52, 52.7, 70.2, 5.0
2019-02-17 03:45:41,708 : Image to text: 22.3, 56.7, 72.6, 4.0
2019-02-17 03:45:42,534 : Text to Image: 20.86, 53.4, 70.46, 5.0
2019-02-17 03:45:43,471 : Image to text: 26.4, 58.1, 71.8, 4.0
2019-02-17 03:45:44,281 : Text to Image: 22.36, 53.02, 69.78, 5.0
2019-02-17 03:45:44,282 : Dev mean Text to Image: 21.152, 53.3, 70.392, 5.0
2019-02-17 03:45:44,282 : Dev mean Image to text: 24.560000000000002, 56.980000000000004, 72.6, 4.0
2019-02-17 03:45:44,282 : start epoch
2019-02-17 03:46:29,781 : samples : 64000
2019-02-17 03:46:40,695 : Image to text: 8.98, 26.88, 39.44, 17.0
2019-02-17 03:46:48,009 : Text to Image: 8.044, 23.824, 35.42, 21.0
2019-02-17 03:47:30,513 : samples : 128000
2019-02-17 03:47:42,537 : Image to text: 9.38, 26.76, 39.6, 17.0
2019-02-17 03:47:52,564 : Text to Image: 7.992, 23.936, 35.032, 21.0
2019-02-17 03:48:37,895 : samples : 192000
2019-02-17 03:48:50,520 : Image to text: 9.74, 26.94, 39.46, 18.0
2019-02-17 03:49:00,546 : Text to Image: 7.84, 23.432, 34.82, 21.0
2019-02-17 03:49:42,373 : samples : 256000
2019-02-17 03:49:52,602 : Image to text: 8.84, 27.36, 39.3, 18.0
2019-02-17 03:49:59,948 : Text to Image: 8.028, 23.952, 35.48, 20.0
2019-02-17 03:50:43,160 : samples : 320000
2019-02-17 03:50:55,774 : Image to text: 9.6, 26.32, 38.84, 17.0
2019-02-17 03:51:05,862 : Text to Image: 8.096, 24.276, 35.968, 20.0
2019-02-17 03:51:51,083 : samples : 384000
2019-02-17 03:52:01,321 : Image to text: 8.18, 26.38, 39.3, 17.0
2019-02-17 03:52:08,745 : Text to Image: 7.972, 23.88, 35.36, 21.0
2019-02-17 03:52:51,571 : samples : 448000
2019-02-17 03:53:04,101 : Image to text: 9.16, 27.34, 39.48, 17.0
2019-02-17 03:53:14,100 : Text to Image: 7.928, 23.948, 35.312, 21.0
2019-02-17 03:53:59,178 : samples : 512000
2019-02-17 03:54:10,328 : Image to text: 8.8, 26.56, 38.8, 17.0
2019-02-17 03:54:17,591 : Text to Image: 7.808, 23.376, 34.792, 21.0
2019-02-17 03:55:03,302 : Epoch 8 finished
2019-02-17 03:55:04,248 : Image to text: 25.6, 57.8, 73.5, 4.0
2019-02-17 03:55:05,048 : Text to Image: 20.98, 53.94, 71.36, 5.0
2019-02-17 03:55:05,968 : Image to text: 24.2, 57.5, 74.3, 4.0
2019-02-17 03:55:06,784 : Text to Image: 20.78, 53.48, 70.86, 5.0
2019-02-17 03:55:07,723 : Image to text: 24.9, 57.3, 72.3, 4.0
2019-02-17 03:55:08,480 : Text to Image: 20.84, 52.76, 70.76, 5.0
2019-02-17 03:55:09,422 : Image to text: 22.8, 57.1, 73.2, 4.0
2019-02-17 03:55:10,187 : Text to Image: 20.08, 53.18, 70.86, 5.0
2019-02-17 03:55:11,119 : Image to text: 25.2, 58.2, 73.6, 4.0
2019-02-17 03:55:11,875 : Text to Image: 21.62, 52.76, 69.18, 5.0
2019-02-17 03:55:11,875 : Dev mean Text to Image: 20.86, 53.224, 70.604, 5.0
2019-02-17 03:55:11,875 : Dev mean Image to text: 24.54, 57.58, 73.38, 4.0
2019-02-17 03:55:11,876 : start epoch
2019-02-17 03:55:56,571 : samples : 64000
2019-02-17 03:56:09,203 : Image to text: 8.3, 27.02, 39.3, 17.0
2019-02-17 03:56:19,250 : Text to Image: 7.764, 23.064, 34.596, 21.0
2019-02-17 03:57:01,827 : samples : 128000
2019-02-17 03:57:12,133 : Image to text: 8.94, 27.16, 38.9, 17.0
2019-02-17 03:57:19,536 : Text to Image: 7.756, 23.416, 35.104, 21.0
2019-02-17 03:58:03,377 : samples : 192000
2019-02-17 03:58:15,952 : Image to text: 9.14, 26.96, 39.94, 16.0
2019-02-17 03:58:25,975 : Text to Image: 7.856, 24.02, 35.892, 20.0
2019-02-17 03:59:11,449 : samples : 256000
2019-02-17 03:59:21,697 : Image to text: 9.34, 27.7, 40.04, 17.0
2019-02-17 03:59:29,097 : Text to Image: 7.912, 24.044, 35.664, 20.0
2019-02-17 04:00:12,548 : samples : 320000
2019-02-17 04:00:25,142 : Image to text: 9.12, 26.52, 38.64, 18.0
2019-02-17 04:00:35,158 : Text to Image: 7.904, 23.648, 35.3, 21.0
2019-02-17 04:01:20,823 : samples : 384000
2019-02-17 04:01:32,534 : Image to text: 8.58, 26.76, 39.22, 17.0
2019-02-17 04:01:40,014 : Text to Image: 7.868, 23.736, 35.4, 21.0
2019-02-17 04:02:22,641 : samples : 448000
2019-02-17 04:02:33,793 : Image to text: 9.56, 26.6, 39.32, 17.0
2019-02-17 04:02:43,858 : Text to Image: 8.088, 23.748, 35.096, 21.0
2019-02-17 04:03:28,875 : samples : 512000
2019-02-17 04:03:41,448 : Image to text: 9.32, 27.6, 40.08, 16.0
2019-02-17 04:03:51,513 : Text to Image: 8.08, 24.436, 35.788, 20.0
2019-02-17 04:04:28,033 : Epoch 9 finished
2019-02-17 04:04:28,495 : Image to text: 26.3, 58.1, 74.3, 4.0
2019-02-17 04:04:28,866 : Text to Image: 21.5, 54.62, 71.72, 5.0
2019-02-17 04:04:29,324 : Image to text: 24.8, 57.6, 73.2, 4.0
2019-02-17 04:04:29,701 : Text to Image: 20.88, 53.5, 70.42, 5.0
2019-02-17 04:04:30,165 : Image to text: 25.6, 59.5, 73.6, 4.0
2019-02-17 04:04:30,543 : Text to Image: 21.12, 53.22, 70.62, 5.0
2019-02-17 04:04:31,007 : Image to text: 24.4, 59.2, 74.4, 4.0
2019-02-17 04:04:31,381 : Text to Image: 20.9, 53.92, 70.42, 5.0
2019-02-17 04:04:31,840 : Image to text: 25.1, 59.4, 73.3, 4.0
2019-02-17 04:04:32,201 : Text to Image: 22.16, 53.68, 69.86, 5.0
2019-02-17 04:04:32,201 : Dev mean Text to Image: 21.311999999999998, 53.788, 70.60799999999999, 5.0
2019-02-17 04:04:32,201 : Dev mean Image to text: 25.24, 58.75999999999999, 73.76, 4.0
2019-02-17 04:04:32,202 : start epoch
2019-02-17 04:05:15,079 : samples : 64000
2019-02-17 04:05:27,681 : Image to text: 9.14, 26.9, 38.84, 17.0
2019-02-17 04:05:37,718 : Text to Image: 7.756, 23.688, 35.628, 20.0
2019-02-17 04:06:23,931 : samples : 128000
2019-02-17 04:06:36,627 : Image to text: 9.22, 27.18, 39.48, 16.0
2019-02-17 04:06:46,753 : Text to Image: 8.1, 24.012, 35.792, 20.0
2019-02-17 04:07:29,928 : samples : 192000
2019-02-17 04:07:40,622 : Image to text: 9.46, 27.82, 40.24, 17.0
2019-02-17 04:07:50,606 : Text to Image: 8.156, 24.036, 36.024, 20.0
2019-02-17 04:08:36,255 : samples : 256000
2019-02-17 04:08:48,923 : Image to text: 8.9, 27.38, 39.32, 17.0
2019-02-17 04:08:58,949 : Text to Image: 7.832, 23.712, 35.076, 20.0
2019-02-17 04:09:42,396 : samples : 320000
2019-02-17 04:09:52,714 : Image to text: 9.34, 26.78, 39.28, 17.0
2019-02-17 04:10:00,156 : Text to Image: 7.976, 24.036, 35.48, 21.0
2019-02-17 04:10:43,712 : samples : 384000
2019-02-17 04:10:54,346 : Image to text: 9.1, 27.46, 40.12, 17.0
2019-02-17 04:11:04,248 : Text to Image: 7.984, 23.868, 35.292, 21.0
2019-02-17 04:11:49,781 : samples : 448000
2019-02-17 04:12:01,639 : Image to text: 8.86, 27.74, 40.1, 17.0
2019-02-17 04:12:10,243 : Text to Image: 8.024, 23.988, 35.504, 20.0
2019-02-17 04:12:56,503 : samples : 512000
2019-02-17 04:13:06,789 : Image to text: 9.2, 26.68, 39.7, 17.0
2019-02-17 04:13:14,984 : Text to Image: 8.18, 24.12, 36.0, 20.0
2019-02-17 04:13:51,703 : Epoch 10 finished
2019-02-17 04:13:52,168 : Image to text: 27.3, 59.0, 72.9, 4.0
2019-02-17 04:13:52,529 : Text to Image: 21.58, 54.74, 72.22, 5.0
2019-02-17 04:13:52,989 : Image to text: 23.2, 55.5, 72.5, 5.0
2019-02-17 04:13:53,355 : Text to Image: 20.72, 53.9, 70.96, 5.0
2019-02-17 04:13:53,831 : Image to text: 25.8, 58.3, 74.2, 4.0
2019-02-17 04:13:54,202 : Text to Image: 21.3, 53.14, 70.76, 5.0
2019-02-17 04:13:54,671 : Image to text: 23.1, 56.3, 73.7, 4.0
2019-02-17 04:13:55,046 : Text to Image: 20.92, 52.56, 70.44, 5.0
2019-02-17 04:13:55,508 : Image to text: 26.3, 58.1, 72.7, 4.0
2019-02-17 04:13:55,884 : Text to Image: 21.4, 53.6, 69.88, 5.0
2019-02-17 04:13:55,884 : Dev mean Text to Image: 21.183999999999997, 53.588, 70.852, 5.0
2019-02-17 04:13:55,884 : Dev mean Image to text: 25.14, 57.44, 73.2, 4.2
2019-02-17 04:13:55,884 : start epoch
2019-02-17 04:14:38,645 : samples : 64000
2019-02-17 04:14:50,123 : Image to text: 9.1, 27.36, 38.94, 17.0
2019-02-17 04:14:59,425 : Text to Image: 8.012, 24.056, 35.408, 20.0
2019-02-17 04:15:42,131 : samples : 128000
2019-02-17 04:15:52,451 : Image to text: 8.86, 26.82, 39.4, 17.0
2019-02-17 04:15:59,885 : Text to Image: 8.152, 23.7, 35.648, 20.0
2019-02-17 04:16:43,044 : samples : 192000
2019-02-17 04:16:55,715 : Image to text: 9.7, 27.84, 40.26, 16.0
2019-02-17 04:17:05,928 : Text to Image: 8.332, 24.596, 35.976, 20.0
2019-02-17 04:17:49,809 : samples : 256000
2019-02-17 04:18:00,006 : Image to text: 9.42, 27.52, 40.26, 17.0
2019-02-17 04:18:10,167 : Text to Image: 8.204, 24.272, 36.328, 20.0
2019-02-17 04:18:54,038 : samples : 320000
2019-02-17 04:19:05,163 : Image to text: 9.72, 27.96, 41.06, 16.0
2019-02-17 04:19:12,722 : Text to Image: 8.228, 24.716, 36.296, 20.0
2019-02-17 04:19:56,459 : samples : 384000
2019-02-17 04:20:06,713 : Image to text: 9.58, 28.3, 40.82, 16.0
2019-02-17 04:20:15,901 : Text to Image: 8.432, 24.624, 36.42, 20.0
2019-02-17 04:20:58,621 : samples : 448000
2019-02-17 04:21:11,211 : Image to text: 8.86, 27.0, 39.38, 17.0
2019-02-17 04:21:21,170 : Text to Image: 7.836, 24.092, 35.76, 20.0
2019-02-17 04:22:06,422 : samples : 512000
2019-02-17 04:22:19,005 : Image to text: 9.54, 27.74, 40.08, 17.0
2019-02-17 04:22:29,032 : Text to Image: 8.44, 24.436, 36.196, 19.0
2019-02-17 04:23:07,587 : Epoch 11 finished
2019-02-17 04:23:08,458 : Image to text: 24.7, 59.9, 74.5, 4.0
2019-02-17 04:23:09,217 : Text to Image: 21.54, 54.66, 72.32, 5.0
2019-02-17 04:23:10,096 : Image to text: 24.3, 58.1, 74.0, 4.0
2019-02-17 04:23:10,852 : Text to Image: 20.74, 53.72, 71.04, 5.0
2019-02-17 04:23:11,743 : Image to text: 25.1, 58.7, 73.7, 4.0
2019-02-17 04:23:12,492 : Text to Image: 21.34, 53.74, 71.22, 5.0
2019-02-17 04:23:13,379 : Image to text: 23.9, 59.4, 74.3, 4.0
2019-02-17 04:23:14,130 : Text to Image: 20.48, 53.88, 70.48, 5.0
2019-02-17 04:23:15,017 : Image to text: 27.2, 58.3, 72.6, 4.0
2019-02-17 04:23:15,781 : Text to Image: 22.22, 53.84, 70.16, 5.0
2019-02-17 04:23:15,781 : Dev mean Text to Image: 21.264, 53.968, 71.044, 5.0
2019-02-17 04:23:15,781 : Dev mean Image to text: 25.04, 58.879999999999995, 73.82000000000001, 4.0
2019-02-17 04:23:15,782 : start epoch
2019-02-17 04:24:00,884 : samples : 64000
2019-02-17 04:24:13,465 : Image to text: 9.74, 28.02, 40.22, 16.0
2019-02-17 04:24:23,467 : Text to Image: 8.456, 24.296, 36.344, 20.0
2019-02-17 04:25:08,469 : samples : 128000
2019-02-17 04:25:21,036 : Image to text: 8.74, 26.94, 39.5, 17.0
2019-02-17 04:25:31,059 : Text to Image: 7.688, 23.64, 35.316, 21.0
2019-02-17 04:26:16,764 : samples : 192000
2019-02-17 04:26:29,329 : Image to text: 9.52, 27.92, 40.84, 16.0
2019-02-17 04:26:39,348 : Text to Image: 8.228, 24.548, 36.26, 19.0
2019-02-17 04:27:24,510 : samples : 256000
2019-02-17 04:27:37,049 : Image to text: 9.66, 27.9, 40.66, 16.0
2019-02-17 04:27:47,083 : Text to Image: 8.26, 24.564, 36.58, 19.0
2019-02-17 04:28:31,216 : samples : 320000
2019-02-17 04:28:43,845 : Image to text: 10.04, 28.2, 40.52, 16.0
2019-02-17 04:28:53,796 : Text to Image: 8.092, 24.344, 36.144, 20.0
2019-02-17 04:29:47,272 : samples : 384000
2019-02-17 04:29:59,859 : Image to text: 9.66, 28.16, 40.96, 16.0
2019-02-17 04:30:09,868 : Text to Image: 8.44, 24.74, 36.512, 20.0
2019-02-17 04:30:54,595 : samples : 448000
2019-02-17 04:31:07,211 : Image to text: 9.74, 28.4, 40.86, 16.0
2019-02-17 04:31:17,275 : Text to Image: 8.376, 24.532, 36.344, 20.0
2019-02-17 04:32:03,359 : samples : 512000
2019-02-17 04:32:15,420 : Image to text: 8.9, 27.74, 40.36, 16.0
2019-02-17 04:32:25,580 : Text to Image: 8.448, 24.684, 36.536, 19.0
2019-02-17 04:33:02,773 : Epoch 12 finished
2019-02-17 04:33:03,237 : Image to text: 26.3, 59.8, 74.7, 3.0
2019-02-17 04:33:03,600 : Text to Image: 21.2, 54.36, 71.96, 5.0
2019-02-17 04:33:04,059 : Image to text: 25.0, 58.3, 73.8, 4.0
2019-02-17 04:33:04,423 : Text to Image: 21.54, 54.54, 71.48, 5.0
2019-02-17 04:33:04,877 : Image to text: 26.7, 60.0, 74.3, 4.0
2019-02-17 04:33:05,238 : Text to Image: 21.36, 53.56, 71.78, 5.0
2019-02-17 04:33:05,693 : Image to text: 24.2, 58.7, 73.7, 4.0
2019-02-17 04:33:06,059 : Text to Image: 20.86, 54.3, 71.46, 5.0
2019-02-17 04:33:06,516 : Image to text: 25.5, 59.2, 75.0, 4.0
2019-02-17 04:33:06,877 : Text to Image: 22.3, 54.64, 71.3, 5.0
2019-02-17 04:33:06,877 : Dev mean Text to Image: 21.452, 54.28, 71.596, 5.0
2019-02-17 04:33:06,877 : Dev mean Image to text: 25.54, 59.2, 74.30000000000001, 3.8
2019-02-17 04:33:06,877 : start epoch
2019-02-17 04:33:49,409 : samples : 64000
2019-02-17 04:33:59,422 : Image to text: 9.24, 28.14, 41.0, 16.0
2019-02-17 04:34:06,348 : Text to Image: 8.376, 24.436, 36.316, 20.0
2019-02-17 04:34:52,159 : samples : 128000
2019-02-17 04:35:05,034 : Image to text: 9.3, 27.54, 39.7, 17.0
2019-02-17 04:35:15,353 : Text to Image: 8.096, 23.892, 35.644, 20.0
2019-02-17 04:36:01,738 : samples : 192000
2019-02-17 04:36:14,580 : Image to text: 8.78, 26.94, 40.3, 17.0
2019-02-17 04:36:24,999 : Text to Image: 7.932, 24.144, 35.932, 20.0
2019-02-17 04:37:11,209 : samples : 256000
2019-02-17 04:37:24,063 : Image to text: 10.02, 28.38, 41.12, 16.0
2019-02-17 04:37:34,475 : Text to Image: 8.316, 24.632, 36.608, 19.0
2019-02-17 04:38:20,595 : samples : 320000
2019-02-17 04:38:33,477 : Image to text: 9.92, 27.8, 40.14, 16.0
2019-02-17 04:38:43,983 : Text to Image: 8.208, 24.424, 36.528, 19.0
2019-02-17 04:39:30,448 : samples : 384000
2019-02-17 04:39:43,480 : Image to text: 9.14, 28.1, 40.9, 16.0
2019-02-17 04:39:53,904 : Text to Image: 8.088, 24.728, 36.288, 20.0
2019-02-17 04:40:40,244 : samples : 448000
2019-02-17 04:40:53,091 : Image to text: 10.26, 28.34, 41.1, 16.0
2019-02-17 04:41:03,616 : Text to Image: 8.32, 24.552, 36.296, 20.0
2019-02-17 04:41:49,853 : samples : 512000
2019-02-17 04:42:02,657 : Image to text: 9.12, 27.14, 39.18, 16.0
2019-02-17 04:42:13,170 : Text to Image: 8.036, 23.972, 35.588, 20.0
2019-02-17 04:42:50,240 : Epoch 13 finished
2019-02-17 04:42:50,674 : Image to text: 26.5, 60.8, 75.5, 4.0
2019-02-17 04:42:51,007 : Text to Image: 22.44, 55.38, 72.92, 5.0
2019-02-17 04:42:51,457 : Image to text: 25.0, 58.9, 73.4, 4.0
2019-02-17 04:42:51,794 : Text to Image: 22.24, 55.68, 71.86, 4.0
2019-02-17 04:42:52,237 : Image to text: 25.0, 60.8, 74.7, 4.0
2019-02-17 04:42:52,588 : Text to Image: 21.74, 55.08, 72.32, 5.0
2019-02-17 04:42:53,050 : Image to text: 24.6, 58.9, 75.4, 4.0
2019-02-17 04:42:53,383 : Text to Image: 21.34, 54.36, 72.28, 5.0
2019-02-17 04:42:53,835 : Image to text: 26.0, 58.1, 73.4, 4.0
2019-02-17 04:42:54,161 : Text to Image: 22.58, 54.54, 70.98, 5.0
2019-02-17 04:42:54,162 : Dev mean Text to Image: 22.067999999999998, 55.008, 72.072, 4.8
2019-02-17 04:42:54,162 : Dev mean Image to text: 25.419999999999998, 59.5, 74.48, 4.0
2019-02-17 04:42:54,162 : start epoch
2019-02-17 04:43:36,907 : samples : 64000
2019-02-17 04:43:47,451 : Image to text: 9.76, 27.82, 40.36, 16.0
2019-02-17 04:43:55,035 : Text to Image: 8.616, 24.712, 36.524, 19.0
2019-02-17 04:44:37,805 : samples : 128000
2019-02-17 04:44:48,322 : Image to text: 10.18, 27.88, 40.46, 16.0
2019-02-17 04:44:55,882 : Text to Image: 8.344, 24.44, 36.316, 19.0
2019-02-17 04:45:38,770 : samples : 192000
2019-02-17 04:45:49,305 : Image to text: 9.5, 28.64, 41.36, 16.0
2019-02-17 04:45:56,764 : Text to Image: 8.448, 24.4, 36.192, 19.0
2019-02-17 04:46:48,086 : samples : 256000
2019-02-17 04:46:59,795 : Image to text: 9.68, 27.52, 40.1, 16.0
2019-02-17 04:47:07,354 : Text to Image: 8.372, 24.724, 36.44, 19.0
2019-02-17 04:47:49,756 : samples : 320000
2019-02-17 04:48:00,291 : Image to text: 9.5, 27.96, 40.36, 16.0
2019-02-17 04:48:07,834 : Text to Image: 8.42, 24.652, 36.504, 19.0
2019-02-17 04:48:50,161 : samples : 384000
2019-02-17 04:49:00,653 : Image to text: 9.1, 27.64, 40.2, 17.0
2019-02-17 04:49:08,156 : Text to Image: 8.376, 24.3, 36.348, 19.0
2019-02-17 04:49:50,939 : samples : 448000
2019-02-17 04:50:01,460 : Image to text: 9.42, 28.38, 40.86, 16.0
2019-02-17 04:50:09,137 : Text to Image: 8.428, 24.696, 36.648, 19.0
2019-02-17 04:50:51,641 : samples : 512000
2019-02-17 04:51:02,121 : Image to text: 9.48, 28.08, 40.88, 15.0
2019-02-17 04:51:09,632 : Text to Image: 8.388, 25.128, 37.048, 19.0
2019-02-17 04:51:46,154 : Epoch 14 finished
2019-02-17 04:51:46,617 : Image to text: 25.1, 60.0, 74.8, 4.0
2019-02-17 04:51:46,961 : Text to Image: 21.54, 56.0, 73.1, 4.0
2019-02-17 04:51:47,400 : Image to text: 24.7, 59.4, 74.4, 4.0
2019-02-17 04:51:47,744 : Text to Image: 21.74, 55.2, 72.24, 5.0
2019-02-17 04:51:48,184 : Image to text: 27.5, 60.6, 74.7, 4.0
2019-02-17 04:51:48,519 : Text to Image: 21.34, 54.8, 71.78, 5.0
2019-02-17 04:51:48,961 : Image to text: 23.7, 57.8, 74.9, 4.0
2019-02-17 04:51:49,296 : Text to Image: 21.56, 54.86, 72.08, 5.0
2019-02-17 04:51:49,749 : Image to text: 28.2, 59.2, 75.0, 4.0
2019-02-17 04:51:50,082 : Text to Image: 22.06, 54.38, 70.6, 5.0
2019-02-17 04:51:50,082 : Dev mean Text to Image: 21.647999999999996, 55.048, 71.96000000000001, 4.8
2019-02-17 04:51:50,082 : Dev mean Image to text: 25.840000000000003, 59.400000000000006, 74.76, 4.0
2019-02-17 04:51:50,082 : start epoch
2019-02-17 04:52:32,561 : samples : 64000
2019-02-17 04:52:43,089 : Image to text: 9.48, 28.82, 40.98, 16.0
2019-02-17 04:52:50,714 : Text to Image: 8.432, 24.432, 36.328, 20.0
2019-02-17 04:53:33,591 : samples : 128000
2019-02-17 04:53:44,182 : Image to text: 10.0, 28.0, 40.84, 16.0
2019-02-17 04:53:51,713 : Text to Image: 8.308, 24.572, 36.536, 19.0
2019-02-17 04:54:34,012 : samples : 192000
2019-02-17 04:54:44,552 : Image to text: 10.06, 28.64, 41.82, 16.0
2019-02-17 04:54:52,081 : Text to Image: 8.376, 24.876, 36.496, 19.0
2019-02-17 04:55:34,849 : samples : 256000
2019-02-17 04:55:45,342 : Image to text: 9.82, 28.18, 41.24, 15.0
2019-02-17 04:55:52,895 : Text to Image: 8.68, 25.24, 37.116, 19.0
2019-02-17 04:56:35,670 : samples : 320000
2019-02-17 04:56:46,159 : Image to text: 9.74, 28.44, 40.68, 16.0
2019-02-17 04:56:53,673 : Text to Image: 8.776, 25.188, 37.204, 19.0
2019-02-17 04:57:36,681 : samples : 384000
2019-02-17 04:57:47,205 : Image to text: 9.32, 28.42, 40.38, 16.0
2019-02-17 04:57:54,799 : Text to Image: 8.728, 25.044, 36.964, 19.0
2019-02-17 04:58:37,706 : samples : 448000
2019-02-17 04:58:48,331 : Image to text: 9.84, 27.94, 41.02, 16.0
2019-02-17 04:58:55,841 : Text to Image: 8.444, 24.768, 36.676, 19.0
2019-02-17 04:59:39,001 : samples : 512000
2019-02-17 04:59:49,498 : Image to text: 9.8, 28.44, 40.46, 16.0
2019-02-17 04:59:57,062 : Text to Image: 8.484, 24.772, 36.808, 19.0
2019-02-17 05:00:32,837 : Epoch 15 finished
2019-02-17 05:00:33,288 : Image to text: 28.8, 60.1, 76.5, 4.0
2019-02-17 05:00:33,618 : Text to Image: 22.14, 56.66, 73.44, 4.0
2019-02-17 05:00:34,067 : Image to text: 25.5, 60.1, 74.8, 4.0
2019-02-17 05:00:34,405 : Text to Image: 21.88, 54.96, 72.62, 5.0
2019-02-17 05:00:34,860 : Image to text: 26.4, 60.6, 74.8, 3.0
2019-02-17 05:00:35,215 : Text to Image: 22.2, 55.32, 72.24, 5.0
2019-02-17 05:00:35,678 : Image to text: 26.5, 59.7, 75.0, 4.0
2019-02-17 05:00:36,027 : Text to Image: 21.7, 54.86, 72.02, 5.0
2019-02-17 05:00:36,483 : Image to text: 27.5, 60.8, 74.2, 3.0
2019-02-17 05:00:36,834 : Text to Image: 22.9, 55.5, 71.24, 5.0
2019-02-17 05:00:36,834 : Dev mean Text to Image: 22.163999999999994, 55.46, 72.312, 4.8
2019-02-17 05:00:36,834 : Dev mean Image to text: 26.94, 60.25999999999999, 75.06, 3.6
2019-02-17 05:00:36,835 : start epoch
2019-02-17 05:01:20,135 : samples : 64000
2019-02-17 05:01:30,667 : Image to text: 10.34, 28.64, 40.9, 16.0
2019-02-17 05:01:38,167 : Text to Image: 8.308, 24.344, 36.324, 20.0
2019-02-17 05:02:20,710 : samples : 128000
2019-02-17 05:02:31,207 : Image to text: 9.1, 27.26, 39.52, 16.0
2019-02-17 05:02:38,773 : Text to Image: 8.12, 24.708, 36.664, 19.0
2019-02-17 05:03:20,989 : samples : 192000
2019-02-17 05:03:32,157 : Image to text: 9.4, 28.34, 40.94, 16.0
2019-02-17 05:03:40,674 : Text to Image: 8.468, 25.16, 37.072, 19.0
2019-02-17 05:04:31,082 : samples : 256000
2019-02-17 05:04:41,586 : Image to text: 9.96, 28.54, 41.24, 15.0
2019-02-17 05:04:49,178 : Text to Image: 8.48, 24.992, 36.992, 19.0
2019-02-17 05:05:32,472 : samples : 320000
2019-02-17 05:05:43,062 : Image to text: 9.72, 28.34, 40.7, 16.0
2019-02-17 05:05:50,626 : Text to Image: 8.46, 25.196, 37.208, 19.0
2019-02-17 05:06:33,042 : samples : 384000
2019-02-17 05:06:43,552 : Image to text: 9.62, 27.84, 40.1, 16.0
2019-02-17 05:06:51,081 : Text to Image: 8.34, 24.576, 36.384, 20.0
2019-02-17 05:07:34,177 : samples : 448000
2019-02-17 05:07:44,674 : Image to text: 9.48, 27.18, 39.8, 16.0
2019-02-17 05:07:52,172 : Text to Image: 8.488, 24.96, 37.14, 19.0
2019-02-17 05:08:34,725 : samples : 512000
2019-02-17 05:08:45,376 : Image to text: 9.68, 28.16, 40.66, 16.0
2019-02-17 05:08:52,897 : Text to Image: 8.576, 25.208, 37.152, 19.0
2019-02-17 05:09:29,397 : Epoch 16 finished
2019-02-17 05:09:29,830 : Image to text: 25.9, 58.9, 75.8, 4.0
2019-02-17 05:09:30,168 : Text to Image: 22.24, 56.72, 72.96, 4.0
2019-02-17 05:09:30,621 : Image to text: 25.7, 60.5, 76.1, 4.0
2019-02-17 05:09:30,968 : Text to Image: 21.68, 55.94, 72.88, 4.0
2019-02-17 05:09:31,416 : Image to text: 27.0, 60.5, 76.2, 3.0
2019-02-17 05:09:31,764 : Text to Image: 22.48, 54.98, 72.02, 5.0
2019-02-17 05:09:32,222 : Image to text: 25.3, 60.1, 75.7, 4.0
2019-02-17 05:09:32,572 : Text to Image: 22.0, 54.66, 71.9, 5.0
2019-02-17 05:09:33,048 : Image to text: 26.8, 60.8, 76.3, 4.0
2019-02-17 05:09:33,379 : Text to Image: 23.3, 55.46, 71.2, 4.0
2019-02-17 05:09:33,379 : Dev mean Text to Image: 22.34, 55.55199999999999, 72.19200000000001, 4.4
2019-02-17 05:09:33,379 : Dev mean Image to text: 26.14, 60.16, 76.02, 3.8
2019-02-17 05:09:33,379 : start epoch
2019-02-17 05:10:16,019 : samples : 64000
2019-02-17 05:10:26,561 : Image to text: 9.86, 28.56, 40.88, 16.0
2019-02-17 05:10:34,085 : Text to Image: 8.624, 25.504, 37.352, 19.0
2019-02-17 05:11:17,001 : samples : 128000
2019-02-17 05:11:27,577 : Image to text: 10.16, 28.26, 41.56, 15.0
2019-02-17 05:11:35,163 : Text to Image: 8.552, 25.152, 36.904, 19.0
2019-02-17 05:12:18,462 : samples : 192000
2019-02-17 05:12:28,988 : Image to text: 9.8, 28.6, 41.3, 15.0
2019-02-17 05:12:36,574 : Text to Image: 8.456, 25.476, 37.472, 19.0
2019-02-17 05:13:19,002 : samples : 256000
2019-02-17 05:13:29,573 : Image to text: 9.5, 28.44, 41.28, 15.0
2019-02-17 05:13:37,129 : Text to Image: 8.52, 24.72, 36.772, 19.0
2019-02-17 05:14:19,799 : samples : 320000
2019-02-17 05:14:30,317 : Image to text: 9.58, 28.06, 40.56, 16.0
2019-02-17 05:14:37,918 : Text to Image: 8.4, 24.864, 36.804, 19.0
2019-02-17 05:15:20,445 : samples : 384000
2019-02-17 05:15:31,022 : Image to text: 8.98, 27.1, 40.28, 16.0
2019-02-17 05:15:38,496 : Text to Image: 8.004, 24.348, 36.536, 19.0
2019-02-17 05:16:20,449 : samples : 448000
2019-02-17 05:16:30,999 : Image to text: 9.44, 28.08, 41.44, 16.0
2019-02-17 05:16:38,538 : Text to Image: 8.568, 25.116, 37.048, 19.0
2019-02-17 05:17:21,535 : samples : 512000
2019-02-17 05:17:32,043 : Image to text: 9.86, 27.74, 40.82, 16.0
2019-02-17 05:17:39,583 : Text to Image: 8.344, 24.9, 36.864, 19.0
2019-02-17 05:18:16,041 : Epoch 17 finished
2019-02-17 05:18:16,481 : Image to text: 26.6, 60.5, 74.7, 4.0
2019-02-17 05:18:16,811 : Text to Image: 21.18, 55.44, 72.78, 4.0
2019-02-17 05:18:17,256 : Image to text: 25.2, 58.2, 73.7, 4.0
2019-02-17 05:18:17,589 : Text to Image: 21.0, 54.94, 71.48, 5.0
2019-02-17 05:18:18,064 : Image to text: 25.3, 58.4, 74.2, 4.0
2019-02-17 05:18:18,423 : Text to Image: 22.1, 54.02, 71.26, 5.0
2019-02-17 05:18:18,890 : Image to text: 24.9, 58.5, 74.4, 4.0
2019-02-17 05:18:19,234 : Text to Image: 21.44, 54.62, 71.04, 5.0
2019-02-17 05:18:19,682 : Image to text: 26.0, 57.6, 74.0, 4.0
2019-02-17 05:18:20,015 : Text to Image: 22.1, 54.3, 70.68, 5.0
2019-02-17 05:18:20,015 : Dev mean Text to Image: 21.564, 54.664, 71.44800000000001, 4.8
2019-02-17 05:18:20,015 : Dev mean Image to text: 25.599999999999998, 58.64, 74.2, 4.0
2019-02-17 05:18:20,015 : start epoch
2019-02-17 05:19:03,310 : samples : 64000
2019-02-17 05:19:13,855 : Image to text: 9.96, 28.88, 42.06, 15.0
2019-02-17 05:19:21,438 : Text to Image: 8.828, 25.728, 37.8, 18.0
2019-02-17 05:20:03,942 : samples : 128000
2019-02-17 05:20:14,470 : Image to text: 9.5, 28.6, 41.32, 16.0
2019-02-17 05:20:22,028 : Text to Image: 8.464, 25.008, 36.864, 19.0
2019-02-17 05:21:11,029 : samples : 192000
2019-02-17 05:21:23,315 : Image to text: 9.3, 28.82, 40.72, 16.0
2019-02-17 05:21:31,156 : Text to Image: 8.452, 24.8, 36.92, 19.0
2019-02-17 05:22:14,026 : samples : 256000
2019-02-17 05:22:24,542 : Image to text: 9.56, 29.04, 41.5, 16.0
2019-02-17 05:22:32,067 : Text to Image: 8.804, 25.36, 37.52, 19.0
2019-02-17 05:23:14,547 : samples : 320000
2019-02-17 05:23:25,101 : Image to text: 10.02, 28.7, 42.36, 15.0
2019-02-17 05:23:32,599 : Text to Image: 8.684, 25.388, 37.528, 19.0
2019-02-17 05:24:15,685 : samples : 384000
2019-02-17 05:24:26,181 : Image to text: 9.44, 27.78, 40.3, 16.0
2019-02-17 05:24:33,735 : Text to Image: 8.308, 24.696, 36.564, 19.0
2019-02-17 05:25:17,179 : samples : 448000
2019-02-17 05:25:27,697 : Image to text: 9.46, 28.1, 40.24, 16.0
2019-02-17 05:25:35,208 : Text to Image: 8.368, 25.168, 37.3, 19.0
2019-02-17 05:26:18,225 : samples : 512000
2019-02-17 05:26:28,709 : Image to text: 9.96, 28.18, 40.46, 16.0
2019-02-17 05:26:36,293 : Text to Image: 8.396, 25.028, 36.724, 19.0
2019-02-17 05:27:12,341 : Epoch 18 finished
2019-02-17 05:27:12,780 : Image to text: 26.2, 60.7, 75.0, 4.0
2019-02-17 05:27:13,110 : Text to Image: 22.3, 56.82, 73.26, 4.0
2019-02-17 05:27:13,550 : Image to text: 24.5, 60.5, 75.9, 4.0
2019-02-17 05:27:13,882 : Text to Image: 22.06, 55.42, 72.36, 5.0
2019-02-17 05:27:14,326 : Image to text: 26.3, 59.4, 74.2, 4.0
2019-02-17 05:27:14,666 : Text to Image: 21.08, 54.9, 72.5, 5.0
2019-02-17 05:27:15,116 : Image to text: 23.9, 58.9, 75.1, 4.0
2019-02-17 05:27:15,444 : Text to Image: 21.76, 54.66, 72.82, 5.0
2019-02-17 05:27:15,893 : Image to text: 27.6, 61.6, 75.4, 3.0
2019-02-17 05:27:16,233 : Text to Image: 22.58, 55.06, 72.16, 5.0
2019-02-17 05:27:16,233 : Dev mean Text to Image: 21.955999999999996, 55.372, 72.62, 4.8
2019-02-17 05:27:16,233 : Dev mean Image to text: 25.7, 60.220000000000006, 75.11999999999999, 3.8000000000000003
2019-02-17 05:27:20,153 : 
Test scores | Image to text:             25.759999999999998, 59.06, 74.89999999999999, 3.8
2019-02-17 05:27:20,153 : Test scores | Text to image:             22.375999999999998, 54.908, 72.14399999999999, 4.8

2019-02-17 05:27:20,276 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-17 05:27:20,653 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-17 05:27:21,407 : loading BERT model bert-base-uncased
2019-02-17 05:27:21,407 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:27:21,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:27:21,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw6eu8sdp
2019-02-17 05:27:23,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:27:25,459 : Computing embeddings for train/dev/test
2019-02-17 05:29:01,404 : Computed embeddings
2019-02-17 05:29:01,405 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:29:46,708 : [('reg:1e-05', 53.52), ('reg:0.0001', 53.35), ('reg:0.001', 47.9), ('reg:0.01', 40.88)]
2019-02-17 05:29:46,709 : Validation : best param found is reg = 1e-05 with score             53.52
2019-02-17 05:29:46,709 : Evaluating...
2019-02-17 05:30:00,145 : 
Dev acc : 53.5 Test acc : 54.9 for LENGTH classification

2019-02-17 05:30:00,145 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-17 05:30:00,533 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-17 05:30:00,585 : loading BERT model bert-base-uncased
2019-02-17 05:30:00,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:30:00,624 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:30:00,624 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaxilw3kv
2019-02-17 05:30:03,115 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:30:04,711 : Computing embeddings for train/dev/test
2019-02-17 05:31:33,085 : Computed embeddings
2019-02-17 05:31:33,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:32:35,626 : [('reg:1e-05', 28.89), ('reg:0.0001', 8.82), ('reg:0.001', 1.36), ('reg:0.01', 0.54)]
2019-02-17 05:32:35,626 : Validation : best param found is reg = 1e-05 with score             28.89
2019-02-17 05:32:35,626 : Evaluating...
2019-02-17 05:32:53,658 : 
Dev acc : 28.9 Test acc : 28.5 for WORDCONTENT classification

2019-02-17 05:32:53,659 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-17 05:32:53,981 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-17 05:32:54,056 : loading BERT model bert-base-uncased
2019-02-17 05:32:54,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:32:54,082 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:32:54,082 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp32u72fl3
2019-02-17 05:32:56,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:32:58,055 : Computing embeddings for train/dev/test
2019-02-17 05:34:20,969 : Computed embeddings
2019-02-17 05:34:20,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:34:52,272 : [('reg:1e-05', 27.16), ('reg:0.0001', 26.92), ('reg:0.001', 25.68), ('reg:0.01', 22.78)]
2019-02-17 05:34:52,272 : Validation : best param found is reg = 1e-05 with score             27.16
2019-02-17 05:34:52,272 : Evaluating...
2019-02-17 05:35:00,880 : 
Dev acc : 27.2 Test acc : 27.5 for DEPTH classification

2019-02-17 05:35:00,881 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-17 05:35:01,304 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-17 05:35:01,369 : loading BERT model bert-base-uncased
2019-02-17 05:35:01,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:35:01,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:35:01,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuqf4idj2
2019-02-17 05:35:03,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:35:05,354 : Computing embeddings for train/dev/test
2019-02-17 05:36:22,804 : Computed embeddings
2019-02-17 05:36:22,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:37:08,522 : [('reg:1e-05', 69.81), ('reg:0.0001', 69.85), ('reg:0.001', 64.36), ('reg:0.01', 53.66)]
2019-02-17 05:37:08,522 : Validation : best param found is reg = 0.0001 with score             69.85
2019-02-17 05:37:08,522 : Evaluating...
2019-02-17 05:37:22,446 : 
Dev acc : 69.8 Test acc : 69.1 for TOPCONSTITUENTS classification

2019-02-17 05:37:22,448 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-17 05:37:22,830 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-17 05:37:22,904 : loading BERT model bert-base-uncased
2019-02-17 05:37:22,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:37:23,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:37:23,048 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjjrn4aqq
2019-02-17 05:37:25,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:37:27,121 : Computing embeddings for train/dev/test
2019-02-17 05:38:55,581 : Computed embeddings
2019-02-17 05:38:55,581 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:39:41,567 : [('reg:1e-05', 85.46), ('reg:0.0001', 85.49), ('reg:0.001', 84.95), ('reg:0.01', 83.17)]
2019-02-17 05:39:41,567 : Validation : best param found is reg = 0.0001 with score             85.49
2019-02-17 05:39:41,567 : Evaluating...
2019-02-17 05:39:53,533 : 
Dev acc : 85.5 Test acc : 84.8 for BIGRAMSHIFT classification

2019-02-17 05:39:53,535 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-17 05:39:54,211 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-17 05:39:54,286 : loading BERT model bert-base-uncased
2019-02-17 05:39:54,287 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:39:54,324 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:39:54,324 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp71fn7361
2019-02-17 05:39:56,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:39:58,349 : Computing embeddings for train/dev/test
2019-02-17 05:41:21,185 : Computed embeddings
2019-02-17 05:41:21,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:42:02,670 : [('reg:1e-05', 90.24), ('reg:0.0001', 90.29), ('reg:0.001', 90.32), ('reg:0.01', 90.19)]
2019-02-17 05:42:02,671 : Validation : best param found is reg = 0.001 with score             90.32
2019-02-17 05:42:02,671 : Evaluating...
2019-02-17 05:42:12,939 : 
Dev acc : 90.3 Test acc : 89.0 for TENSE classification

2019-02-17 05:42:12,940 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-17 05:42:13,376 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-17 05:42:13,450 : loading BERT model bert-base-uncased
2019-02-17 05:42:13,450 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:42:13,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:42:13,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2om4gbqi
2019-02-17 05:42:16,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:42:17,591 : Computing embeddings for train/dev/test
2019-02-17 05:43:45,264 : Computed embeddings
2019-02-17 05:43:45,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:44:21,324 : [('reg:1e-05', 80.68), ('reg:0.0001', 80.74), ('reg:0.001', 81.43), ('reg:0.01', 79.85)]
2019-02-17 05:44:21,324 : Validation : best param found is reg = 0.001 with score             81.43
2019-02-17 05:44:21,324 : Evaluating...
2019-02-17 05:44:31,836 : 
Dev acc : 81.4 Test acc : 81.3 for SUBJNUMBER classification

2019-02-17 05:44:31,837 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-17 05:44:32,470 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-17 05:44:32,540 : loading BERT model bert-base-uncased
2019-02-17 05:44:32,540 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:44:32,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:44:32,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_fl8m4gi
2019-02-17 05:44:35,053 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:44:36,540 : Computing embeddings for train/dev/test
2019-02-17 05:46:01,668 : Computed embeddings
2019-02-17 05:46:01,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:46:43,381 : [('reg:1e-05', 72.84), ('reg:0.0001', 72.84), ('reg:0.001', 73.08), ('reg:0.01', 66.8)]
2019-02-17 05:46:43,381 : Validation : best param found is reg = 0.001 with score             73.08
2019-02-17 05:46:43,381 : Evaluating...
2019-02-17 05:46:53,463 : 
Dev acc : 73.1 Test acc : 74.4 for OBJNUMBER classification

2019-02-17 05:46:53,464 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-17 05:46:54,076 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-17 05:46:54,156 : loading BERT model bert-base-uncased
2019-02-17 05:46:54,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:46:54,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:46:54,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqf46pezh
2019-02-17 05:46:56,702 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:46:58,212 : Computing embeddings for train/dev/test
2019-02-17 05:48:36,994 : Computed embeddings
2019-02-17 05:48:36,994 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:49:28,170 : [('reg:1e-05', 62.11), ('reg:0.0001', 62.12), ('reg:0.001', 61.66), ('reg:0.01', 60.82)]
2019-02-17 05:49:28,170 : Validation : best param found is reg = 0.0001 with score             62.12
2019-02-17 05:49:28,171 : Evaluating...
2019-02-17 05:49:41,718 : 
Dev acc : 62.1 Test acc : 62.2 for ODDMANOUT classification

2019-02-17 05:49:41,719 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-17 05:49:42,178 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-17 05:49:42,266 : loading BERT model bert-base-uncased
2019-02-17 05:49:42,267 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-17 05:49:42,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-17 05:49:42,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy919jtsx
2019-02-17 05:49:44,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-17 05:49:46,272 : Computing embeddings for train/dev/test
2019-02-17 05:51:23,307 : Computed embeddings
2019-02-17 05:51:23,307 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-17 05:52:12,291 : [('reg:1e-05', 69.45), ('reg:0.0001', 69.34), ('reg:0.001', 69.31), ('reg:0.01', 60.04)]
2019-02-17 05:52:12,291 : Validation : best param found is reg = 1e-05 with score             69.45
2019-02-17 05:52:12,291 : Evaluating...
2019-02-17 05:52:26,444 : 
Dev acc : 69.5 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-02-17 05:52:26,446 : total results: {'STS12': {'MSRpar': {'pearson': (0.29696611181983773, 9.789745050444839e-17), 'spearman': SpearmanrResult(correlation=0.34952268284469634, pvalue=5.669177216148451e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.03147126639491536, 0.3894298724184839), 'spearman': SpearmanrResult(correlation=0.027273368876600593, pvalue=0.45578528347759606), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.34842014570280677, 1.512572831761387e-14), 'spearman': SpearmanrResult(correlation=0.45746496632984246, pvalue=4.0439218050821923e-25), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4068001994080315, 2.9423100526868423e-31), 'spearman': SpearmanrResult(correlation=0.3982428548476992, pvalue=6.503119130486331e-30), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4595496294903302, 3.05801078360474e-22), 'spearman': SpearmanrResult(correlation=0.4576865015715348, pvalue=4.717647094321368e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29605296400521813, 'wmean': 0.27268546739670063}, 'spearman': {'mean': 0.33803807489407467, 'wmean': 0.3133437945943331}}}, 'STS13': {'FNWN': {'pearson': (0.03131713509440084, 0.6688053291486091), 'spearman': SpearmanrResult(correlation=0.041586571128491734, pvalue=0.5699168710967685), 'nsamples': 189}, 'headlines': {'pearson': (0.47512121071712954, 1.6971218780118689e-43), 'spearman': SpearmanrResult(correlation=0.4636290347968956, pvalue=3.0746186358902073e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.03467525115630963, 0.41237963035070524), 'spearman': SpearmanrResult(correlation=0.061638402826436185, pvalue=0.14482319515855174), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.18037119898928, 'wmean': 0.2544751083129191}, 'spearman': {'mean': 0.18895133625060787, 'wmean': 0.2601071880177249}}}, 'STS14': {'deft-forum': {'pearson': (-0.014009338782729584, 0.7669468206482611), 'spearman': SpearmanrResult(correlation=-0.015481846691131099, pvalue=0.7432702707074741), 'nsamples': 450}, 'deft-news': {'pearson': (0.4460124349488527, 4.535321266677203e-16), 'spearman': SpearmanrResult(correlation=0.4790685685228266, pvalue=1.2820343303813202e-18), 'nsamples': 300}, 'headlines': {'pearson': (0.4368649427931535, 2.6591299551602905e-36), 'spearman': SpearmanrResult(correlation=0.41135166249681226, pvalue=5.464204430758958e-32), 'nsamples': 750}, 'images': {'pearson': (0.1311918517659196, 0.0003152623145532143), 'spearman': SpearmanrResult(correlation=0.1508690600162816, pvalue=3.3452022175758596e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.27229924964290747, 3.2328696544609047e-14), 'spearman': SpearmanrResult(correlation=0.28509114387847023, pvalue=1.7162778233833135e-15), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3764645614485085, 1.153110859594805e-26), 'spearman': SpearmanrResult(correlation=0.37843615396517494, pvalue=5.994202282581746e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2748039503027687, 'wmean': 0.27736399527207845}, 'spearman': {'mean': 0.28155579036473904, 'wmean': 0.28161726795023817}}}, 'STS15': {'answers-forums': {'pearson': (0.20796222365792594, 4.945652961217275e-05), 'spearman': SpearmanrResult(correlation=0.23625017820655794, pvalue=3.740037018000821e-06), 'nsamples': 375}, 'answers-students': {'pearson': (0.28804021378631395, 8.534100857034731e-16), 'spearman': SpearmanrResult(correlation=0.3260434103215193, pvalue=4.917728877651064e-20), 'nsamples': 750}, 'belief': {'pearson': (0.4257999694257162, 6.00793665059707e-18), 'spearman': SpearmanrResult(correlation=0.46447796639784555, pvalue=1.826169334774916e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.5052043117790275, 7.864880530700966e-50), 'spearman': SpearmanrResult(correlation=0.5057454645278043, pvalue=5.968650296485303e-50), 'nsamples': 750}, 'images': {'pearson': (0.19656566244645005, 5.723626765434872e-08), 'spearman': SpearmanrResult(correlation=0.20602717287533573, pvalue=1.2404458117127073e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.32471447621908667, 'wmean': 0.3266728211384031}, 'spearman': {'mean': 0.3477088384658126, 'wmean': 0.34704503000671527}}}, 'STS16': {'answer-answer': {'pearson': (0.39376091541705005, 7.543406124119522e-11), 'spearman': SpearmanrResult(correlation=0.432681566992628, pvalue=5.182696900660818e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.5660811728348878, 1.691066671302775e-22), 'spearman': SpearmanrResult(correlation=0.5891884647535829, pvalue=1.1526673810240406e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6414668442853152, 4.6287733863116365e-28), 'spearman': SpearmanrResult(correlation=0.6915045830995717, pvalue=4.766071043217876e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.6503123854886803, 1.0113027053858838e-30), 'spearman': SpearmanrResult(correlation=0.6819206583818435, pvalue=1.0071235594923312e-34), 'nsamples': 244}, 'question-question': {'pearson': (0.10602713138871137, 0.12652902144955758), 'spearman': SpearmanrResult(correlation=0.20856669544015055, pvalue=0.0024412575976705347), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.47152968988292904, 'wmean': 0.4800529099974022}, 'spearman': {'mean': 0.5207723937335553, 'wmean': 0.5275161718759127}}}, 'MR': {'devacc': 79.25, 'acc': 79.23, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.88, 'acc': 84.74, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.7, 'acc': 87.57, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.32, 'acc': 94.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.14, 'acc': 83.2, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.69, 'acc': 43.21, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 81.44, 'acc': 87.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.78, 'acc': 69.74, 'f1': 78.52, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 68.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6809848077441052, 'pearson': 0.6945593971919223, 'spearman': 0.6415164105592593, 'mse': 0.5267593878003589, 'yhat': array([3.63630735, 4.11016887, 2.73366217, ..., 3.20377216, 4.51483387,        4.22536561]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5469116428416618, 'pearson': 0.4650535822098155, 'spearman': 0.46686339421803735, 'mse': 2.0349728254746275, 'yhat': array([3.01103473, 1.81525694, 3.21587664, ..., 3.84357727, 2.82794652,        4.51361995]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 54.03, 'acc': 54.58, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 312.404, 'acc': [(25.759999999999998, 59.06, 74.89999999999999, 3.8), (22.375999999999998, 54.908, 72.14399999999999, 4.8)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 53.52, 'acc': 54.9, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.89, 'acc': 28.52, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.16, 'acc': 27.54, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 69.85, 'acc': 69.12, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.49, 'acc': 84.82, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.32, 'acc': 89.01, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.43, 'acc': 81.33, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.08, 'acc': 74.38, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.12, 'acc': 62.2, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.45, 'acc': 69.44, 'ndev': 10002, 'ntest': 10002}}
2019-02-17 05:52:26,446 : STS12 p=0.2727, STS12 s=0.3133, STS13 p=0.2545, STS13 s=0.2601, STS14 p=0.2774, STS14 s=0.2816, STS15 p=0.3267, STS15 s=0.3470, STS 16 p=0.4801, STS16 s=0.5275, STS B p=0.4651, STS B s=0.4669, STS B m=2.0350, SICK-R p=0.6946, SICK-R s=0.6415, SICK-P m=0.5268
2019-02-17 05:52:26,446 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-17 05:52:26,446 : 0.2727,0.3133,0.2545,0.2601,0.2774,0.2816,0.3267,0.3470,0.4801,0.5275,0.4651,0.4669,2.0350,0.6946,0.6415,0.5268
2019-02-17 05:52:26,446 : MR=79.23, CR=84.74, SUBJ=94.70, MPQA=87.57, SST-B=83.20, SST-F=43.21, TREC=87.00, SICK-E=68.56, SNLI=54.58, MRPC=69.74, MRPC f=78.52
2019-02-17 05:52:26,446 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-17 05:52:26,446 : 79.23,84.74,94.70,87.57,83.20,43.21,87.00,68.56,54.58,69.74,78.52
2019-02-17 05:52:26,446 : COCO r1i2t=25.76, COCO r5i2t=59.06, COCO r10i2t=74.90, COCO medr_i2t=3.80, COCO r1t2i=22.38, COCO r5t2i=54.91, COCO r10t2i=72.14, COCO medr_t2i=4.80
2019-02-17 05:52:26,446 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-17 05:52:26,446 : 25.76,59.06,74.90,3.80,22.38,54.91,72.14,4.80
2019-02-17 05:52:26,446 : SentLen=54.90, WC=28.52, TreeDepth=27.54, TopConst=69.12, BShift=84.82, Tense=89.01, SubjNum=81.33, ObjNum=74.38, SOMO=62.20, CoordInv=69.44, average=64.13
2019-02-17 05:52:26,446 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-17 05:52:26,446 : 54.90,28.52,27.54,69.12,84.82,89.01,81.33,74.38,62.20,69.44,64.13
