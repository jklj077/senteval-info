2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : layer 0
2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : ********************************************************************************
2019-03-12 11:31:33,827 : ***** Transfer task : STS12 *****


2019-03-12 11:31:33,863 : loading BERT model bert-large-uncased
2019-03-12 11:31:33,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:31:33,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:31:33,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy2aboqgm
2019-03-12 11:31:41,417 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:32:00,003 : MSRpar : pearson = 0.3754, spearman = 0.3972
2019-03-12 11:32:04,082 : MSRvid : pearson = 0.7008, spearman = 0.7024
2019-03-12 11:32:07,480 : SMTeuroparl : pearson = 0.5003, spearman = 0.5960
2019-03-12 11:32:13,825 : surprise.OnWN : pearson = 0.6066, spearman = 0.6484
2019-03-12 11:32:15,465 : surprise.SMTnews : pearson = 0.5141, spearman = 0.4294
2019-03-12 11:32:15,465 : ALL (weighted average) : Pearson = 0.5460,             Spearman = 0.5650
2019-03-12 11:32:15,465 : ALL (average) : Pearson = 0.5394,             Spearman = 0.5547

2019-03-12 11:32:15,465 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 11:32:15,474 : loading BERT model bert-large-uncased
2019-03-12 11:32:15,474 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:15,493 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:15,493 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6_t34835
2019-03-12 11:32:22,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:32:30,034 : FNWN : pearson = 0.2066, spearman = 0.2086
2019-03-12 11:32:31,919 : headlines : pearson = 0.6623, spearman = 0.6619
2019-03-12 11:32:33,378 : OnWN : pearson = 0.5571, spearman = 0.5554
2019-03-12 11:32:33,378 : ALL (weighted average) : Pearson = 0.5655,             Spearman = 0.5649
2019-03-12 11:32:33,378 : ALL (average) : Pearson = 0.4753,             Spearman = 0.4753

2019-03-12 11:32:33,378 : ***** Transfer task : STS14 *****


2019-03-12 11:32:33,497 : loading BERT model bert-large-uncased
2019-03-12 11:32:33,497 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:33,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:33,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6_2psf4w
2019-03-12 11:32:40,934 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:32:48,240 : deft-forum : pearson = 0.3509, spearman = 0.3614
2019-03-12 11:32:49,866 : deft-news : pearson = 0.6909, spearman = 0.6503
2019-03-12 11:32:52,022 : headlines : pearson = 0.6305, spearman = 0.6132
2019-03-12 11:32:54,084 : images : pearson = 0.7056, spearman = 0.6964
2019-03-12 11:32:56,195 : OnWN : pearson = 0.6475, spearman = 0.6781
2019-03-12 11:32:59,034 : tweet-news : pearson = 0.5984, spearman = 0.5957
2019-03-12 11:32:59,034 : ALL (weighted average) : Pearson = 0.6138,             Spearman = 0.6121
2019-03-12 11:32:59,034 : ALL (average) : Pearson = 0.6040,             Spearman = 0.5992

2019-03-12 11:32:59,034 : ***** Transfer task : STS15 *****


2019-03-12 11:32:59,065 : loading BERT model bert-large-uncased
2019-03-12 11:32:59,066 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:32:59,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:32:59,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo9i0k2s0
2019-03-12 11:33:06,533 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:33:14,355 : answers-forums : pearson = 0.5576, spearman = 0.5378
2019-03-12 11:33:16,423 : answers-students : pearson = 0.6886, spearman = 0.6995
2019-03-12 11:33:18,454 : belief : pearson = 0.5571, spearman = 0.6020
2019-03-12 11:33:20,687 : headlines : pearson = 0.6540, spearman = 0.6575
2019-03-12 11:33:22,804 : images : pearson = 0.7707, spearman = 0.7789
2019-03-12 11:33:22,804 : ALL (weighted average) : Pearson = 0.6676,             Spearman = 0.6765
2019-03-12 11:33:22,804 : ALL (average) : Pearson = 0.6456,             Spearman = 0.6552

2019-03-12 11:33:22,804 : ***** Transfer task : STS16 *****


2019-03-12 11:33:22,956 : loading BERT model bert-large-uncased
2019-03-12 11:33:22,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:33:22,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:33:22,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9ql2k0nu
2019-03-12 11:33:30,485 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:33:37,305 : answer-answer : pearson = 0.4225, spearman = 0.4251
2019-03-12 11:33:37,959 : headlines : pearson = 0.6597, spearman = 0.6787
2019-03-12 11:33:38,833 : plagiarism : pearson = 0.7115, spearman = 0.7349
2019-03-12 11:33:40,312 : postediting : pearson = 0.7805, spearman = 0.8262
2019-03-12 11:33:40,912 : question-question : pearson = 0.6406, spearman = 0.6450
2019-03-12 11:33:40,913 : ALL (weighted average) : Pearson = 0.6404,             Spearman = 0.6597
2019-03-12 11:33:40,913 : ALL (average) : Pearson = 0.6430,             Spearman = 0.6620

2019-03-12 11:33:40,913 : ***** Transfer task : MR *****


2019-03-12 11:33:40,929 : loading BERT model bert-large-uncased
2019-03-12 11:33:40,929 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:33:41,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:33:41,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjqxjsvfj
2019-03-12 11:33:48,529 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:33:54,119 : Generating sentence embeddings
2019-03-12 11:34:25,284 : Generated sentence embeddings
2019-03-12 11:34:25,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:34:34,356 : Best param found at split 1: l2reg = 0.0001                 with score 52.97
2019-03-12 11:34:43,106 : Best param found at split 2: l2reg = 0.0001                 with score 50.11
2019-03-12 11:34:53,224 : Best param found at split 3: l2reg = 0.01                 with score 52.42
2019-03-12 11:35:02,746 : Best param found at split 4: l2reg = 1e-05                 with score 52.47
2019-03-12 11:35:12,217 : Best param found at split 5: l2reg = 0.0001                 with score 54.22
2019-03-12 11:35:12,674 : Dev acc : 52.44 Test acc : 50.21

2019-03-12 11:35:12,675 : ***** Transfer task : CR *****


2019-03-12 11:35:12,683 : loading BERT model bert-large-uncased
2019-03-12 11:35:12,683 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:35:12,702 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:35:12,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd11v7u22
2019-03-12 11:35:20,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:35:26,134 : Generating sentence embeddings
2019-03-12 11:35:34,378 : Generated sentence embeddings
2019-03-12 11:35:34,379 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:35:37,564 : Best param found at split 1: l2reg = 0.0001                 with score 65.79
2019-03-12 11:35:40,612 : Best param found at split 2: l2reg = 0.0001                 with score 65.68
2019-03-12 11:35:43,827 : Best param found at split 3: l2reg = 1e-05                 with score 64.84
2019-03-12 11:35:47,203 : Best param found at split 4: l2reg = 0.01                 with score 66.63
2019-03-12 11:35:50,090 : Best param found at split 5: l2reg = 0.001                 with score 65.05
2019-03-12 11:35:50,252 : Dev acc : 65.6 Test acc : 63.76

2019-03-12 11:35:50,252 : ***** Transfer task : MPQA *****


2019-03-12 11:35:50,376 : loading BERT model bert-large-uncased
2019-03-12 11:35:50,376 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:35:50,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:35:50,395 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6k6tybne
2019-03-12 11:35:57,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:36:03,802 : Generating sentence embeddings
2019-03-12 11:36:11,305 : Generated sentence embeddings
2019-03-12 11:36:11,305 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:36:21,772 : Best param found at split 1: l2reg = 0.001                 with score 85.54
2019-03-12 11:36:34,736 : Best param found at split 2: l2reg = 0.001                 with score 85.82
2019-03-12 11:36:46,317 : Best param found at split 3: l2reg = 0.001                 with score 85.99
2019-03-12 11:36:58,641 : Best param found at split 4: l2reg = 1e-05                 with score 86.38
2019-03-12 11:37:11,143 : Best param found at split 5: l2reg = 0.0001                 with score 86.33
2019-03-12 11:37:11,754 : Dev acc : 86.01 Test acc : 84.56

2019-03-12 11:37:11,755 : ***** Transfer task : SUBJ *****


2019-03-12 11:37:11,772 : loading BERT model bert-large-uncased
2019-03-12 11:37:11,773 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:37:11,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:37:11,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkzhn8v5f
2019-03-12 11:37:19,282 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:37:24,918 : Generating sentence embeddings
2019-03-12 11:37:55,443 : Generated sentence embeddings
2019-03-12 11:37:55,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:38:07,833 : Best param found at split 1: l2reg = 1e-05                 with score 58.17
2019-03-12 11:38:16,911 : Best param found at split 2: l2reg = 1e-05                 with score 53.68
2019-03-12 11:38:27,990 : Best param found at split 3: l2reg = 0.001                 with score 56.16
2019-03-12 11:38:43,996 : Best param found at split 4: l2reg = 1e-05                 with score 53.89
2019-03-12 11:39:07,037 : Best param found at split 5: l2reg = 0.0001                 with score 54.09
2019-03-12 11:39:09,439 : Dev acc : 55.2 Test acc : 56.61

2019-03-12 11:39:09,440 : ***** Transfer task : SST Binary classification *****


2019-03-12 11:39:09,596 : loading BERT model bert-large-uncased
2019-03-12 11:39:09,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:39:09,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:39:09,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg8gq7bu4
2019-03-12 11:39:17,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:39:23,005 : Computing embedding for train
2019-03-12 11:42:35,057 : Computed train embeddings
2019-03-12 11:42:35,057 : Computing embedding for dev
2019-03-12 11:42:39,229 : Computed dev embeddings
2019-03-12 11:42:39,229 : Computing embedding for test
2019-03-12 11:42:47,572 : Computed test embeddings
2019-03-12 11:42:47,573 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:43:28,903 : [('reg:1e-05', 73.85), ('reg:0.0001', 73.74), ('reg:0.001', 73.51), ('reg:0.01', 63.07)]
2019-03-12 11:43:28,904 : Validation : best param found is reg = 1e-05 with score             73.85
2019-03-12 11:43:28,904 : Evaluating...
2019-03-12 11:43:39,128 : 
Dev acc : 73.85 Test acc : 71.22 for             SST Binary classification

2019-03-12 11:43:39,129 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 11:43:39,180 : loading BERT model bert-large-uncased
2019-03-12 11:43:39,181 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:43:39,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:43:39,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm6ev3hf0
2019-03-12 11:43:46,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:43:52,608 : Computing embedding for train
2019-03-12 11:44:31,849 : Computed train embeddings
2019-03-12 11:44:31,849 : Computing embedding for dev
2019-03-12 11:44:36,845 : Computed dev embeddings
2019-03-12 11:44:36,845 : Computing embedding for test
2019-03-12 11:44:46,924 : Computed test embeddings
2019-03-12 11:44:46,924 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:44:51,454 : [('reg:1e-05', 31.34), ('reg:0.0001', 29.16), ('reg:0.001', 26.25), ('reg:0.01', 25.89)]
2019-03-12 11:44:51,454 : Validation : best param found is reg = 1e-05 with score             31.34
2019-03-12 11:44:51,455 : Evaluating...
2019-03-12 11:44:52,582 : 
Dev acc : 31.34 Test acc : 31.9 for             SST Fine-Grained classification

2019-03-12 11:44:52,583 : ***** Transfer task : TREC *****


2019-03-12 11:44:52,595 : loading BERT model bert-large-uncased
2019-03-12 11:44:52,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:44:52,615 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:44:52,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpniof10on
2019-03-12 11:45:00,054 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:45:25,458 : Computed train embeddings
2019-03-12 11:45:26,964 : Computed test embeddings
2019-03-12 11:45:26,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:45:50,636 : [('reg:1e-05', 53.56), ('reg:0.0001', 50.43), ('reg:0.001', 51.09), ('reg:0.01', 43.91)]
2019-03-12 11:45:50,636 : Cross-validation : best param found is reg = 1e-05             with score 53.56
2019-03-12 11:45:50,636 : Evaluating...
2019-03-12 11:45:52,819 : 
Dev acc : 53.56 Test acc : 63.0             for TREC

2019-03-12 11:45:52,819 : ***** Transfer task : MRPC *****


2019-03-12 11:45:52,874 : loading BERT model bert-large-uncased
2019-03-12 11:45:52,874 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:45:52,899 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:45:52,899 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzvblzog2
2019-03-12 11:46:00,347 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:46:06,272 : Computing embedding for train
2019-03-12 11:46:47,601 : Computed train embeddings
2019-03-12 11:46:47,602 : Computing embedding for test
2019-03-12 11:47:04,883 : Computed test embeddings
2019-03-12 11:47:04,905 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:47:16,564 : [('reg:1e-05', 70.95), ('reg:0.0001', 70.95), ('reg:0.001', 71.39), ('reg:0.01', 70.68)]
2019-03-12 11:47:16,564 : Cross-validation : best param found is reg = 0.001             with score 71.39
2019-03-12 11:47:16,564 : Evaluating...
2019-03-12 11:47:17,528 : Dev acc : 71.39 Test acc 69.91; Test F1 78.17 for MRPC.

2019-03-12 11:47:17,529 : ***** Transfer task : SICK-Entailment*****


2019-03-12 11:47:17,553 : loading BERT model bert-large-uncased
2019-03-12 11:47:17,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:47:17,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:47:17,614 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph42lkvst
2019-03-12 11:47:25,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:47:30,746 : Computing embedding for train
2019-03-12 11:47:57,585 : Computed train embeddings
2019-03-12 11:47:57,586 : Computing embedding for dev
2019-03-12 11:48:01,277 : Computed dev embeddings
2019-03-12 11:48:01,277 : Computing embedding for test
2019-03-12 11:48:25,303 : Computed test embeddings
2019-03-12 11:48:25,341 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:48:27,814 : [('reg:1e-05', 72.0), ('reg:0.0001', 73.8), ('reg:0.001', 78.2), ('reg:0.01', 76.2)]
2019-03-12 11:48:27,814 : Validation : best param found is reg = 0.001 with score             78.2
2019-03-12 11:48:27,815 : Evaluating...
2019-03-12 11:48:28,725 : 
Dev acc : 78.2 Test acc : 76.52 for                        SICK entailment

2019-03-12 11:48:28,726 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 11:48:28,753 : loading BERT model bert-large-uncased
2019-03-12 11:48:28,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:48:28,772 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:48:28,772 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuni4zzjk
2019-03-12 11:48:36,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:48:41,771 : Computing embedding for train
2019-03-12 11:49:04,644 : Computed train embeddings
2019-03-12 11:49:04,644 : Computing embedding for dev
2019-03-12 11:49:08,278 : Computed dev embeddings
2019-03-12 11:49:08,278 : Computing embedding for test
2019-03-12 11:49:37,060 : Computed test embeddings
2019-03-12 11:50:06,307 : Dev : Pearson 0.7933951921676722
2019-03-12 11:50:06,307 : Test : Pearson 0.8018032890996528 Spearman 0.7363788888729816 MSE 0.36384180920057785                        for SICK Relatedness

2019-03-12 11:50:06,308 : 

***** Transfer task : STSBenchmark*****


2019-03-12 11:50:06,348 : loading BERT model bert-large-uncased
2019-03-12 11:50:06,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:50:06,378 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:50:06,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1rzpl366
2019-03-12 11:50:13,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:50:19,681 : Computing embedding for train
2019-03-12 11:50:56,987 : Computed train embeddings
2019-03-12 11:50:56,987 : Computing embedding for dev
2019-03-12 11:51:07,779 : Computed dev embeddings
2019-03-12 11:51:07,779 : Computing embedding for test
2019-03-12 11:51:15,903 : Computed test embeddings
2019-03-12 11:51:57,744 : Dev : Pearson 0.650466984101275
2019-03-12 11:51:57,744 : Test : Pearson 0.6472718084924654 Spearman 0.6412769142746935 MSE 1.4673280215601412                        for SICK Relatedness

2019-03-12 11:51:57,745 : ***** Transfer task : SNLI Entailment*****


2019-03-12 11:52:02,603 : loading BERT model bert-large-uncased
2019-03-12 11:52:02,603 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:52:02,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:52:02,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzng54yiy
2019-03-12 11:52:10,171 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:52:16,396 : PROGRESS (encoding): 0.00%
2019-03-12 11:58:04,220 : PROGRESS (encoding): 14.56%
2019-03-12 12:04:29,728 : PROGRESS (encoding): 29.12%
2019-03-12 12:10:50,141 : PROGRESS (encoding): 43.69%
2019-03-12 12:17:30,051 : PROGRESS (encoding): 58.25%
2019-03-12 12:24:44,479 : PROGRESS (encoding): 72.81%
2019-03-12 12:32:00,497 : PROGRESS (encoding): 87.37%
2019-03-12 12:39:25,166 : PROGRESS (encoding): 0.00%
2019-03-12 12:40:27,563 : PROGRESS (encoding): 0.00%
2019-03-12 12:41:34,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:43:33,902 : [('reg:1e-09', 61.51)]
2019-03-12 12:43:33,902 : Validation : best param found is reg = 1e-09 with score             61.51
2019-03-12 12:43:33,902 : Evaluating...
2019-03-12 12:45:18,766 : Dev acc : 61.51 Test acc : 61.69 for SNLI

2019-03-12 12:45:18,766 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 12:45:18,981 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 12:45:20,022 : loading BERT model bert-large-uncased
2019-03-12 12:45:20,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:45:20,047 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:45:20,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy67ryp83
2019-03-12 12:45:27,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:45:33,324 : Computing embeddings for train/dev/test
2019-03-12 12:52:13,321 : Computed embeddings
2019-03-12 12:52:13,321 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:53:07,668 : [('reg:1e-05', 82.03), ('reg:0.0001', 72.0), ('reg:0.001', 72.56), ('reg:0.01', 75.22)]
2019-03-12 12:53:07,668 : Validation : best param found is reg = 1e-05 with score             82.03
2019-03-12 12:53:07,668 : Evaluating...
2019-03-12 12:53:21,159 : 
Dev acc : 82.0 Test acc : 81.7 for LENGTH classification

2019-03-12 12:53:21,160 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 12:53:21,510 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 12:53:21,558 : loading BERT model bert-large-uncased
2019-03-12 12:53:21,558 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:53:21,588 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:53:21,589 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptjpv2ckv
2019-03-12 12:53:29,188 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:53:34,812 : Computing embeddings for train/dev/test
2019-03-12 13:00:02,449 : Computed embeddings
2019-03-12 13:00:02,449 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:01:18,141 : [('reg:1e-05', 39.88), ('reg:0.0001', 10.6), ('reg:0.001', 0.42), ('reg:0.01', 0.2)]
2019-03-12 13:01:18,142 : Validation : best param found is reg = 1e-05 with score             39.88
2019-03-12 13:01:18,142 : Evaluating...
2019-03-12 13:01:38,728 : 
Dev acc : 39.9 Test acc : 39.5 for WORDCONTENT classification

2019-03-12 13:01:38,729 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 13:01:39,104 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 13:01:39,168 : loading BERT model bert-large-uncased
2019-03-12 13:01:39,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:01:39,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:01:39,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppn1gp84q
2019-03-12 13:01:46,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:01:52,465 : Computing embeddings for train/dev/test
2019-03-12 13:07:54,353 : Computed embeddings
2019-03-12 13:07:54,353 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:08:52,574 : [('reg:1e-05', 24.76), ('reg:0.0001', 23.01), ('reg:0.001', 22.43), ('reg:0.01', 23.73)]
2019-03-12 13:08:52,574 : Validation : best param found is reg = 1e-05 with score             24.76
2019-03-12 13:08:52,574 : Evaluating...
2019-03-12 13:09:11,618 : 
Dev acc : 24.8 Test acc : 24.3 for DEPTH classification

2019-03-12 13:09:11,619 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 13:09:12,016 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 13:09:12,080 : loading BERT model bert-large-uncased
2019-03-12 13:09:12,080 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:09:12,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:09:12,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph3qnm4vw
2019-03-12 13:09:19,632 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:09:25,418 : Computing embeddings for train/dev/test
2019-03-12 13:12:46,741 : Computed embeddings
2019-03-12 13:12:46,741 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:13:15,113 : [('reg:1e-05', 46.78), ('reg:0.0001', 53.08), ('reg:0.001', 40.97), ('reg:0.01', 23.22)]
2019-03-12 13:13:15,113 : Validation : best param found is reg = 0.0001 with score             53.08
2019-03-12 13:13:15,113 : Evaluating...
2019-03-12 13:13:25,617 : 
Dev acc : 53.1 Test acc : 54.1 for TOPCONSTITUENTS classification

2019-03-12 13:13:25,618 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 13:13:25,965 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 13:13:26,031 : loading BERT model bert-large-uncased
2019-03-12 13:13:26,032 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:13:26,150 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:13:26,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2l1p85tx
2019-03-12 13:13:33,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:13:39,074 : Computing embeddings for train/dev/test
2019-03-12 13:16:40,946 : Computed embeddings
2019-03-12 13:16:40,947 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:16:59,117 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:16:59,118 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:16:59,118 : Evaluating...
2019-03-12 13:17:04,710 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-03-12 13:17:04,711 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 13:17:05,279 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 13:17:05,345 : loading BERT model bert-large-uncased
2019-03-12 13:17:05,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:17:05,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:17:05,376 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwv03kv4l
2019-03-12 13:17:12,889 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:17:18,462 : Computing embeddings for train/dev/test
2019-03-12 13:21:06,473 : Computed embeddings
2019-03-12 13:21:06,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:22:01,732 : [('reg:1e-05', 78.45), ('reg:0.0001', 78.43), ('reg:0.001', 78.35), ('reg:0.01', 77.5)]
2019-03-12 13:22:01,732 : Validation : best param found is reg = 1e-05 with score             78.45
2019-03-12 13:22:01,732 : Evaluating...
2019-03-12 13:22:13,872 : 
Dev acc : 78.5 Test acc : 75.6 for TENSE classification

2019-03-12 13:22:13,873 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 13:22:14,273 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 13:22:14,340 : loading BERT model bert-large-uncased
2019-03-12 13:22:14,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:22:14,366 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:22:14,366 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr3rfugvm
2019-03-12 13:22:21,785 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:22:27,637 : Computing embeddings for train/dev/test
2019-03-12 13:28:38,580 : Computed embeddings
2019-03-12 13:28:38,580 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:29:37,288 : [('reg:1e-05', 74.2), ('reg:0.0001', 74.18), ('reg:0.001', 73.87), ('reg:0.01', 64.91)]
2019-03-12 13:29:37,289 : Validation : best param found is reg = 1e-05 with score             74.2
2019-03-12 13:29:37,289 : Evaluating...
2019-03-12 13:29:50,718 : 
Dev acc : 74.2 Test acc : 72.5 for SUBJNUMBER classification

2019-03-12 13:29:50,719 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 13:29:51,132 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 13:29:51,198 : loading BERT model bert-large-uncased
2019-03-12 13:29:51,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:29:51,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:29:51,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsps08ma4
2019-03-12 13:29:58,739 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:30:04,526 : Computing embeddings for train/dev/test
2019-03-12 13:36:06,176 : Computed embeddings
2019-03-12 13:36:06,176 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:37:01,887 : [('reg:1e-05', 71.81), ('reg:0.0001', 71.81), ('reg:0.001', 71.4), ('reg:0.01', 68.48)]
2019-03-12 13:37:01,887 : Validation : best param found is reg = 1e-05 with score             71.81
2019-03-12 13:37:01,887 : Evaluating...
2019-03-12 13:37:15,902 : 
Dev acc : 71.8 Test acc : 72.2 for OBJNUMBER classification

2019-03-12 13:37:15,903 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 13:37:16,299 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 13:37:16,369 : loading BERT model bert-large-uncased
2019-03-12 13:37:16,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:37:16,490 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:37:16,491 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxvr86_29
2019-03-12 13:37:23,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:37:29,601 : Computing embeddings for train/dev/test
2019-03-12 13:44:24,222 : Computed embeddings
2019-03-12 13:44:24,223 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:45:14,260 : [('reg:1e-05', 49.82), ('reg:0.0001', 49.82), ('reg:0.001', 49.81), ('reg:0.01', 50.19)]
2019-03-12 13:45:14,260 : Validation : best param found is reg = 0.01 with score             50.19
2019-03-12 13:45:14,261 : Evaluating...
2019-03-12 13:45:27,076 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-03-12 13:45:27,077 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 13:45:27,480 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 13:45:27,557 : loading BERT model bert-large-uncased
2019-03-12 13:45:27,557 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:45:27,683 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:45:27,683 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnimmdgai
2019-03-12 13:45:35,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:45:41,144 : Computing embeddings for train/dev/test
2019-03-12 13:52:29,753 : Computed embeddings
2019-03-12 13:52:29,753 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:53:14,992 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:53:14,992 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:53:14,992 : Evaluating...
2019-03-12 13:53:27,332 : 
Dev acc : 50.0 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-03-12 13:53:27,348 : total results: {'STS12': {'MSRpar': {'pearson': (0.3754279377848696, 1.6236217470010875e-26), 'spearman': SpearmanrResult(correlation=0.3972419144696747, pvalue=9.28604135910459e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7008122039848839, 7.7114054429856995e-112), 'spearman': SpearmanrResult(correlation=0.7024309569589406, pvalue=1.443649539505406e-112), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.500280807491322, 1.923388845087843e-30), 'spearman': SpearmanrResult(correlation=0.5959935285793415, pvalue=1.7666303574152915e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6065778131449191, 1.4636518444928943e-76), 'spearman': SpearmanrResult(correlation=0.6484341362808737, pvalue=1.1052677805722217e-90), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5140885861964642, 2.6907219644438678e-28), 'spearman': SpearmanrResult(correlation=0.4293948349099956, pvalue=2.4835628395306634e-19), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5394374697204918, 'wmean': 0.5459664423156082}, 'spearman': {'mean': 0.5546990742397653, 'wmean': 0.5649838560261012}}}, 'STS13': {'FNWN': {'pearson': (0.2065811328209463, 0.004344377714171777), 'spearman': SpearmanrResult(correlation=0.2085511572055756, pvalue=0.003979043332202871), 'nsamples': 189}, 'headlines': {'pearson': (0.6623226959974053, 7.083888967575324e-96), 'spearman': SpearmanrResult(correlation=0.661893995634262, pvalue=1.0345124573050169e-95), 'nsamples': 750}, 'OnWN': {'pearson': (0.5571008689887436, 4.732959979349441e-47), 'spearman': SpearmanrResult(correlation=0.5553643089314748, pvalue=1.0376019567900207e-46), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4753348992690318, 'wmean': 0.565546295735932}, 'spearman': {'mean': 0.4752698205904375, 'wmean': 0.5649306951654051}}}, 'STS14': {'deft-forum': {'pearson': (0.35088035851035515, 1.7506703761184345e-14), 'spearman': SpearmanrResult(correlation=0.3614466492045888, pvalue=2.4667418337442096e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.690900136376406, 6.883042591026727e-44), 'spearman': SpearmanrResult(correlation=0.6502605896931941, pvalue=1.917317963777522e-37), 'nsamples': 300}, 'headlines': {'pearson': (0.6304902041203789, 2.3088976420519807e-84), 'spearman': SpearmanrResult(correlation=0.6132013475693031, pvalue=1.1781076634015746e-78), 'nsamples': 750}, 'images': {'pearson': (0.7055896809868517, 5.311627128489563e-114), 'spearman': SpearmanrResult(correlation=0.6963904494353323, pvalue=7.079541162918369e-110), 'nsamples': 750}, 'OnWN': {'pearson': (0.647513423228003, 2.3886619659477457e-90), 'spearman': SpearmanrResult(correlation=0.6780525768830012, pvalue=4.185587669178096e-102), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5983820363272155, 4.89198079533496e-74), 'spearman': SpearmanrResult(correlation=0.5956526312546903, pvalue=3.265702503383479e-73), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.603959306591535, 'wmean': 0.6137727228638449}, 'spearman': {'mean': 0.599167374006685, 'wmean': 0.6120538461084716}}}, 'STS15': {'answers-forums': {'pearson': (0.557589885622089, 5.079574736067702e-32), 'spearman': SpearmanrResult(correlation=0.5378371275727962, pvalue=1.6803488588396024e-29), 'nsamples': 375}, 'answers-students': {'pearson': (0.688598494439508, 1.6713051922194463e-106), 'spearman': SpearmanrResult(correlation=0.6994963202288422, pvalue=2.985691814363935e-111), 'nsamples': 750}, 'belief': {'pearson': (0.5570685560550975, 5.949780381751243e-32), 'spearman': SpearmanrResult(correlation=0.6020200443671878, pvalue=2.392516981726682e-38), 'nsamples': 375}, 'headlines': {'pearson': (0.6539742723506831, 1.0111194833503432e-92), 'spearman': SpearmanrResult(correlation=0.6574883588153051, pvalue=4.889415146516708e-94), 'nsamples': 750}, 'images': {'pearson': (0.7706820707486816, 1.5345136075878336e-148), 'spearman': SpearmanrResult(correlation=0.7789417096508955, pvalue=9.527806012141102e-154), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6455826558432118, 'wmean': 0.6676460145943665}, 'spearman': {'mean': 0.6551567121270053, 'wmean': 0.6764637436662588}}}, 'STS16': {'answer-answer': {'pearson': (0.42249789533222587, 2.0309384537661757e-12), 'spearman': SpearmanrResult(correlation=0.42506672524650113, pvalue=1.4452666435259878e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6597212923174455, 1.735778788691107e-32), 'spearman': SpearmanrResult(correlation=0.6786548151104488, pvalue=5.824161873704933e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.71147047934741, 8.59643642469606e-37), 'spearman': SpearmanrResult(correlation=0.7349002159874942, pvalue=2.515569350699104e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7805398756292393, 2.734265791780512e-51), 'spearman': SpearmanrResult(correlation=0.8262323659199278, pvalue=2.9934618867108695e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.640594956700347, 1.5470826717189415e-25), 'spearman': SpearmanrResult(correlation=0.6450348378631405, pvalue=5.614664888542931e-26), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6429648998653336, 'wmean': 0.6404379030819903}, 'spearman': {'mean': 0.6619777920255026, 'wmean': 0.6596897346122221}}}, 'MR': {'devacc': 52.44, 'acc': 50.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 65.6, 'acc': 63.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.01, 'acc': 84.56, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 55.2, 'acc': 56.61, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 73.85, 'acc': 71.22, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 31.34, 'acc': 31.9, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 53.56, 'acc': 63.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.39, 'acc': 69.91, 'f1': 78.17, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 76.52, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7933951921676722, 'pearson': 0.8018032890996528, 'spearman': 0.7363788888729816, 'mse': 0.36384180920057785, 'yhat': array([3.71356808, 4.44389743, 1.14725483, ..., 3.10315726, 4.6753382 ,        4.56584003]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.650466984101275, 'pearson': 0.6472718084924654, 'spearman': 0.6412769142746935, 'mse': 1.4673280215601412, 'yhat': array([1.61470937, 1.35659739, 2.9592024 , ..., 3.86994985, 3.98558125,        3.56398153]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.51, 'acc': 61.69, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 82.03, 'acc': 81.66, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 39.88, 'acc': 39.51, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 24.76, 'acc': 24.26, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.08, 'acc': 54.06, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 78.45, 'acc': 75.6, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.2, 'acc': 72.49, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 71.81, 'acc': 72.17, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.89, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 13:53:27,348 : STS12 p=0.5460, STS12 s=0.5650, STS13 p=0.5655, STS13 s=0.5649, STS14 p=0.6138, STS14 s=0.6121, STS15 p=0.6676, STS15 s=0.6765, STS 16 p=0.6404, STS16 s=0.6597, STS B p=0.6473, STS B s=0.6413, STS B m=1.4673, SICK-R p=0.8018, SICK-R s=0.7364, SICK-P m=0.3638
2019-03-12 13:53:27,348 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 13:53:27,348 : 0.5460,0.5650,0.5655,0.5649,0.6138,0.6121,0.6676,0.6765,0.6404,0.6597,0.6473,0.6413,1.4673,0.8018,0.7364,0.3638
2019-03-12 13:53:27,348 : MR=50.21, CR=63.76, SUBJ=56.61, MPQA=84.56, SST-B=71.22, SST-F=31.90, TREC=63.00, SICK-E=76.52, SNLI=61.69, MRPC=69.91, MRPC f=78.17
2019-03-12 13:53:27,348 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 13:53:27,348 : 50.21,63.76,56.61,84.56,71.22,31.90,63.00,76.52,61.69,69.91,78.17
2019-03-12 13:53:27,348 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 13:53:27,349 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 13:53:27,349 : na,na,na,na,na,na,na,na,na,na
2019-03-12 13:53:27,349 : SentLen=81.66, WC=39.51, TreeDepth=24.26, TopConst=54.06, BShift=50.00, Tense=75.60, SubjNum=72.49, ObjNum=72.17, SOMO=49.89, CoordInv=50.00, average=56.96
2019-03-12 13:53:27,349 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 13:53:27,349 : 81.66,39.51,24.26,54.06,50.00,75.60,72.49,72.17,49.89,50.00,56.96
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,349 : layer 1
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,349 : ********************************************************************************
2019-03-12 13:53:27,445 : ***** Transfer task : STS12 *****


2019-03-12 13:53:27,481 : loading BERT model bert-large-uncased
2019-03-12 13:53:27,482 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:53:27,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:53:27,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgbgnuna4
2019-03-12 13:53:34,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:53:47,844 : MSRpar : pearson = 0.3766, spearman = 0.4085
2019-03-12 13:53:51,715 : MSRvid : pearson = 0.7877, spearman = 0.7851
2019-03-12 13:53:54,845 : SMTeuroparl : pearson = 0.5145, spearman = 0.6089
2019-03-12 13:53:59,417 : surprise.OnWN : pearson = 0.6579, spearman = 0.6652
2019-03-12 13:54:01,799 : surprise.SMTnews : pearson = 0.5528, spearman = 0.4416
2019-03-12 13:54:01,799 : ALL (weighted average) : Pearson = 0.5867,             Spearman = 0.5952
2019-03-12 13:54:01,799 : ALL (average) : Pearson = 0.5779,             Spearman = 0.5819

2019-03-12 13:54:01,800 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 13:54:01,809 : loading BERT model bert-large-uncased
2019-03-12 13:54:01,809 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:54:01,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:54:01,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaf50xlhs
2019-03-12 13:54:09,342 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:54:17,816 : FNWN : pearson = 0.2030, spearman = 0.1993
2019-03-12 13:54:21,554 : headlines : pearson = 0.7100, spearman = 0.6987
2019-03-12 13:54:24,729 : OnWN : pearson = 0.6965, spearman = 0.6921
2019-03-12 13:54:24,729 : ALL (weighted average) : Pearson = 0.6410,             Spearman = 0.6333
2019-03-12 13:54:24,729 : ALL (average) : Pearson = 0.5365,             Spearman = 0.5300

2019-03-12 13:54:24,729 : ***** Transfer task : STS14 *****


2019-03-12 13:54:24,774 : loading BERT model bert-large-uncased
2019-03-12 13:54:24,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:54:24,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:54:24,793 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5hkrqdx8
2019-03-12 13:54:32,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:54:41,066 : deft-forum : pearson = 0.4312, spearman = 0.4330
2019-03-12 13:54:43,888 : deft-news : pearson = 0.7045, spearman = 0.6531
2019-03-12 13:54:47,813 : headlines : pearson = 0.6765, spearman = 0.6510
2019-03-12 13:54:51,878 : images : pearson = 0.7592, spearman = 0.7452
2019-03-12 13:54:55,868 : OnWN : pearson = 0.7414, spearman = 0.7592
2019-03-12 13:55:01,261 : tweet-news : pearson = 0.6478, spearman = 0.6273
2019-03-12 13:55:01,261 : ALL (weighted average) : Pearson = 0.6731,             Spearman = 0.6608
2019-03-12 13:55:01,262 : ALL (average) : Pearson = 0.6601,             Spearman = 0.6448

2019-03-12 13:55:01,262 : ***** Transfer task : STS15 *****


2019-03-12 13:55:01,310 : loading BERT model bert-large-uncased
2019-03-12 13:55:01,310 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:55:01,364 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:55:01,364 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyuch2siy
2019-03-12 13:55:08,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:55:17,786 : answers-forums : pearson = 0.5881, spearman = 0.5779
2019-03-12 13:55:21,733 : answers-students : pearson = 0.7218, spearman = 0.7294
2019-03-12 13:55:25,562 : belief : pearson = 0.5938, spearman = 0.6224
2019-03-12 13:55:29,684 : headlines : pearson = 0.7023, spearman = 0.7023
2019-03-12 13:55:33,762 : images : pearson = 0.8121, spearman = 0.8215
2019-03-12 13:55:33,762 : ALL (weighted average) : Pearson = 0.7068,             Spearman = 0.7134
2019-03-12 13:55:33,762 : ALL (average) : Pearson = 0.6836,             Spearman = 0.6907

2019-03-12 13:55:33,762 : ***** Transfer task : STS16 *****


2019-03-12 13:55:33,800 : loading BERT model bert-large-uncased
2019-03-12 13:55:33,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:55:33,818 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:55:33,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18foe9sn
2019-03-12 13:55:41,330 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:55:48,527 : answer-answer : pearson = 0.4885, spearman = 0.4809
2019-03-12 13:55:50,007 : headlines : pearson = 0.7010, spearman = 0.7093
2019-03-12 13:55:52,236 : plagiarism : pearson = 0.7574, spearman = 0.7683
2019-03-12 13:55:55,137 : postediting : pearson = 0.8268, spearman = 0.8464
2019-03-12 13:55:56,451 : question-question : pearson = 0.6849, spearman = 0.6944
2019-03-12 13:55:56,451 : ALL (weighted average) : Pearson = 0.6894,             Spearman = 0.6974
2019-03-12 13:55:56,452 : ALL (average) : Pearson = 0.6917,             Spearman = 0.6999

2019-03-12 13:55:56,452 : ***** Transfer task : MR *****


2019-03-12 13:55:56,498 : loading BERT model bert-large-uncased
2019-03-12 13:55:56,498 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:55:56,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:55:56,519 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_qw5t3nm
2019-03-12 13:56:03,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:56:09,644 : Generating sentence embeddings
2019-03-12 13:57:11,152 : Generated sentence embeddings
2019-03-12 13:57:11,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 13:57:41,958 : Best param found at split 1: l2reg = 0.0001                 with score 62.38
2019-03-12 13:58:13,008 : Best param found at split 2: l2reg = 1e-05                 with score 59.38
2019-03-12 13:58:38,019 : Best param found at split 3: l2reg = 0.0001                 with score 61.75
2019-03-12 13:59:05,069 : Best param found at split 4: l2reg = 0.0001                 with score 57.58
2019-03-12 13:59:27,020 : Best param found at split 5: l2reg = 1e-05                 with score 59.94
2019-03-12 13:59:28,045 : Dev acc : 60.21 Test acc : 60.17

2019-03-12 13:59:28,046 : ***** Transfer task : CR *****


2019-03-12 13:59:28,053 : loading BERT model bert-large-uncased
2019-03-12 13:59:28,053 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:59:28,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:59:28,108 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_cctyrvf
2019-03-12 13:59:35,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:59:41,266 : Generating sentence embeddings
2019-03-12 13:59:55,328 : Generated sentence embeddings
2019-03-12 13:59:55,329 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:00:00,622 : Best param found at split 1: l2reg = 1e-05                 with score 71.41
2019-03-12 14:00:08,260 : Best param found at split 2: l2reg = 0.0001                 with score 70.25
2019-03-12 14:00:15,846 : Best param found at split 3: l2reg = 1e-05                 with score 69.11
2019-03-12 14:00:22,392 : Best param found at split 4: l2reg = 0.001                 with score 73.42
2019-03-12 14:00:29,893 : Best param found at split 5: l2reg = 1e-05                 with score 70.87
2019-03-12 14:00:30,210 : Dev acc : 71.01 Test acc : 67.26

2019-03-12 14:00:30,210 : ***** Transfer task : MPQA *****


2019-03-12 14:00:30,215 : loading BERT model bert-large-uncased
2019-03-12 14:00:30,215 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:00:30,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:00:30,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv0opzsc8
2019-03-12 14:00:37,743 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:00:43,570 : Generating sentence embeddings
2019-03-12 14:01:00,781 : Generated sentence embeddings
2019-03-12 14:01:00,782 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:01:26,064 : Best param found at split 1: l2reg = 0.001                 with score 86.54
2019-03-12 14:01:49,025 : Best param found at split 2: l2reg = 0.0001                 with score 86.78
2019-03-12 14:02:12,746 : Best param found at split 3: l2reg = 0.01                 with score 85.93
2019-03-12 14:02:37,203 : Best param found at split 4: l2reg = 1e-05                 with score 86.68
2019-03-12 14:02:59,745 : Best param found at split 5: l2reg = 1e-05                 with score 87.04
2019-03-12 14:03:01,088 : Dev acc : 86.59 Test acc : 86.02

2019-03-12 14:03:01,089 : ***** Transfer task : SUBJ *****


2019-03-12 14:03:01,105 : loading BERT model bert-large-uncased
2019-03-12 14:03:01,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:03:01,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:03:01,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpygfu_ywb
2019-03-12 14:03:08,600 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:03:14,435 : Generating sentence embeddings
2019-03-12 14:04:16,131 : Generated sentence embeddings
2019-03-12 14:04:16,132 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:04:40,958 : Best param found at split 1: l2reg = 0.01                 with score 82.85
2019-03-12 14:05:06,525 : Best param found at split 2: l2reg = 0.0001                 with score 83.55
2019-03-12 14:05:34,982 : Best param found at split 3: l2reg = 0.001                 with score 80.94
2019-03-12 14:06:07,753 : Best param found at split 4: l2reg = 0.001                 with score 82.32
2019-03-12 14:06:54,093 : Best param found at split 5: l2reg = 0.01                 with score 84.59
2019-03-12 14:06:55,892 : Dev acc : 82.85 Test acc : 83.03

2019-03-12 14:06:55,893 : ***** Transfer task : SST Binary classification *****


2019-03-12 14:06:56,037 : loading BERT model bert-large-uncased
2019-03-12 14:06:56,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:06:56,059 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:06:56,060 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpve4n19uq
2019-03-12 14:07:03,534 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:07:09,421 : Computing embedding for train
2019-03-12 14:10:27,293 : Computed train embeddings
2019-03-12 14:10:27,293 : Computing embedding for dev
2019-03-12 14:10:31,018 : Computed dev embeddings
2019-03-12 14:10:31,018 : Computing embedding for test
2019-03-12 14:10:38,728 : Computed test embeddings
2019-03-12 14:10:38,728 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:11:13,031 : [('reg:1e-05', 74.66), ('reg:0.0001', 75.0), ('reg:0.001', 76.83), ('reg:0.01', 75.11)]
2019-03-12 14:11:13,031 : Validation : best param found is reg = 0.001 with score             76.83
2019-03-12 14:11:13,032 : Evaluating...
2019-03-12 14:11:26,222 : 
Dev acc : 76.83 Test acc : 77.65 for             SST Binary classification

2019-03-12 14:11:26,222 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 14:11:26,275 : loading BERT model bert-large-uncased
2019-03-12 14:11:26,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:11:26,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:11:26,298 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeqbzr6c2
2019-03-12 14:11:33,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:11:39,676 : Computing embedding for train
2019-03-12 14:12:19,216 : Computed train embeddings
2019-03-12 14:12:19,217 : Computing embedding for dev
2019-03-12 14:12:24,299 : Computed dev embeddings
2019-03-12 14:12:24,300 : Computing embedding for test
2019-03-12 14:12:36,201 : Computed test embeddings
2019-03-12 14:12:36,201 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:12:45,082 : [('reg:1e-05', 32.24), ('reg:0.0001', 30.7), ('reg:0.001', 29.79), ('reg:0.01', 29.52)]
2019-03-12 14:12:45,082 : Validation : best param found is reg = 1e-05 with score             32.24
2019-03-12 14:12:45,082 : Evaluating...
2019-03-12 14:12:46,487 : 
Dev acc : 32.24 Test acc : 35.2 for             SST Fine-Grained classification

2019-03-12 14:12:46,487 : ***** Transfer task : TREC *****


2019-03-12 14:12:46,501 : loading BERT model bert-large-uncased
2019-03-12 14:12:46,501 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:12:46,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:12:46,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcxouvw19
2019-03-12 14:12:53,962 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:13:17,270 : Computed train embeddings
2019-03-12 14:13:18,595 : Computed test embeddings
2019-03-12 14:13:18,595 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:13:32,452 : [('reg:1e-05', 56.55), ('reg:0.0001', 57.94), ('reg:0.001', 54.59), ('reg:0.01', 61.5)]
2019-03-12 14:13:32,452 : Cross-validation : best param found is reg = 0.01             with score 61.5
2019-03-12 14:13:32,453 : Evaluating...
2019-03-12 14:13:33,259 : 
Dev acc : 61.5 Test acc : 72.6             for TREC

2019-03-12 14:13:33,259 : ***** Transfer task : MRPC *****


2019-03-12 14:13:33,280 : loading BERT model bert-large-uncased
2019-03-12 14:13:33,280 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:13:33,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:13:33,340 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeoom6j99
2019-03-12 14:13:40,793 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:13:46,447 : Computing embedding for train
2019-03-12 14:14:28,220 : Computed train embeddings
2019-03-12 14:14:28,220 : Computing embedding for test
2019-03-12 14:14:47,567 : Computed test embeddings
2019-03-12 14:14:47,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:14:59,378 : [('reg:1e-05', 71.56), ('reg:0.0001', 69.92), ('reg:0.001', 70.78), ('reg:0.01', 72.45)]
2019-03-12 14:14:59,379 : Cross-validation : best param found is reg = 0.01             with score 72.45
2019-03-12 14:14:59,379 : Evaluating...
2019-03-12 14:14:59,938 : Dev acc : 72.45 Test acc 66.61; Test F1 79.93 for MRPC.

2019-03-12 14:14:59,938 : ***** Transfer task : SICK-Entailment*****


2019-03-12 14:14:59,964 : loading BERT model bert-large-uncased
2019-03-12 14:14:59,965 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:14:59,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:14:59,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprax1btls
2019-03-12 14:15:07,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:15:13,253 : Computing embedding for train
2019-03-12 14:15:37,395 : Computed train embeddings
2019-03-12 14:15:37,395 : Computing embedding for dev
2019-03-12 14:15:41,088 : Computed dev embeddings
2019-03-12 14:15:41,088 : Computing embedding for test
2019-03-12 14:16:08,884 : Computed test embeddings
2019-03-12 14:16:08,920 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:16:12,055 : [('reg:1e-05', 76.6), ('reg:0.0001', 76.2), ('reg:0.001', 76.8), ('reg:0.01', 78.0)]
2019-03-12 14:16:12,055 : Validation : best param found is reg = 0.01 with score             78.0
2019-03-12 14:16:12,055 : Evaluating...
2019-03-12 14:16:12,874 : 
Dev acc : 78.0 Test acc : 75.42 for                        SICK entailment

2019-03-12 14:16:12,875 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 14:16:12,903 : loading BERT model bert-large-uncased
2019-03-12 14:16:12,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:16:12,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:16:12,924 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl05bqvfc
2019-03-12 14:16:20,415 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:16:26,216 : Computing embedding for train
2019-03-12 14:16:49,434 : Computed train embeddings
2019-03-12 14:16:49,434 : Computing embedding for dev
2019-03-12 14:16:52,788 : Computed dev embeddings
2019-03-12 14:16:52,788 : Computing embedding for test
2019-03-12 14:17:16,928 : Computed test embeddings
2019-03-12 14:17:43,741 : Dev : Pearson 0.8059141224024381
2019-03-12 14:17:43,741 : Test : Pearson 0.8068411579950716 Spearman 0.7338772307960827 MSE 0.35638962162577853                        for SICK Relatedness

2019-03-12 14:17:43,744 : 

***** Transfer task : STSBenchmark*****


2019-03-12 14:17:43,840 : loading BERT model bert-large-uncased
2019-03-12 14:17:43,840 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:17:43,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:17:43,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcr6fafhw
2019-03-12 14:17:51,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:17:57,144 : Computing embedding for train
2019-03-12 14:18:35,895 : Computed train embeddings
2019-03-12 14:18:35,895 : Computing embedding for dev
2019-03-12 14:18:49,270 : Computed dev embeddings
2019-03-12 14:18:49,270 : Computing embedding for test
2019-03-12 14:18:59,862 : Computed test embeddings
2019-03-12 14:19:39,604 : Dev : Pearson 0.6795249982080018
2019-03-12 14:19:39,604 : Test : Pearson 0.6617954226375278 Spearman 0.6541893387180728 MSE 1.4072259809146932                        for SICK Relatedness

2019-03-12 14:19:39,605 : ***** Transfer task : SNLI Entailment*****


2019-03-12 14:19:44,652 : loading BERT model bert-large-uncased
2019-03-12 14:19:44,652 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:19:44,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:19:44,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi22_d832
2019-03-12 14:19:52,262 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:19:58,712 : PROGRESS (encoding): 0.00%
2019-03-12 14:25:35,230 : PROGRESS (encoding): 14.56%
2019-03-12 14:31:59,158 : PROGRESS (encoding): 29.12%
2019-03-12 14:38:24,744 : PROGRESS (encoding): 43.69%
2019-03-12 14:45:08,105 : PROGRESS (encoding): 58.25%
2019-03-12 14:51:11,079 : PROGRESS (encoding): 72.81%
2019-03-12 14:54:51,916 : PROGRESS (encoding): 87.37%
2019-03-12 14:58:51,014 : PROGRESS (encoding): 0.00%
2019-03-12 14:59:21,046 : PROGRESS (encoding): 0.00%
2019-03-12 14:59:57,718 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:01:20,110 : [('reg:1e-09', 58.55)]
2019-03-12 15:01:20,110 : Validation : best param found is reg = 1e-09 with score             58.55
2019-03-12 15:01:20,110 : Evaluating...
2019-03-12 15:02:44,083 : Dev acc : 58.55 Test acc : 58.88 for SNLI

2019-03-12 15:02:44,084 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 15:02:44,296 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 15:02:45,346 : loading BERT model bert-large-uncased
2019-03-12 15:02:45,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:02:45,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:02:45,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoxqmei26
2019-03-12 15:02:52,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:02:58,539 : Computing embeddings for train/dev/test
2019-03-12 15:09:48,940 : Computed embeddings
2019-03-12 15:09:48,941 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:11:00,109 : [('reg:1e-05', 75.16), ('reg:0.0001', 71.18), ('reg:0.001', 69.8), ('reg:0.01', 66.88)]
2019-03-12 15:11:00,109 : Validation : best param found is reg = 1e-05 with score             75.16
2019-03-12 15:11:00,109 : Evaluating...
2019-03-12 15:11:14,985 : 
Dev acc : 75.2 Test acc : 74.3 for LENGTH classification

2019-03-12 15:11:14,986 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 15:11:15,354 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 15:11:15,399 : loading BERT model bert-large-uncased
2019-03-12 15:11:15,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:11:15,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:11:15,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp965p23yq
2019-03-12 15:11:22,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:11:28,683 : Computing embeddings for train/dev/test
2019-03-12 15:17:38,044 : Computed embeddings
2019-03-12 15:17:38,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:18:52,067 : [('reg:1e-05', 56.83), ('reg:0.0001', 26.49), ('reg:0.001', 0.86), ('reg:0.01', 0.1)]
2019-03-12 15:18:52,067 : Validation : best param found is reg = 1e-05 with score             56.83
2019-03-12 15:18:52,067 : Evaluating...
2019-03-12 15:19:15,979 : 
Dev acc : 56.8 Test acc : 57.1 for WORDCONTENT classification

2019-03-12 15:19:15,980 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 15:19:16,522 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 15:19:16,589 : loading BERT model bert-large-uncased
2019-03-12 15:19:16,589 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:19:16,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:19:16,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx6_imllg
2019-03-12 15:19:24,116 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:19:29,770 : Computing embeddings for train/dev/test
2019-03-12 15:25:18,061 : Computed embeddings
2019-03-12 15:25:18,061 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:26:14,223 : [('reg:1e-05', 25.28), ('reg:0.0001', 23.31), ('reg:0.001', 23.97), ('reg:0.01', 24.76)]
2019-03-12 15:26:14,224 : Validation : best param found is reg = 1e-05 with score             25.28
2019-03-12 15:26:14,224 : Evaluating...
2019-03-12 15:26:27,007 : 
Dev acc : 25.3 Test acc : 24.3 for DEPTH classification

2019-03-12 15:26:27,008 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 15:26:27,568 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 15:26:27,631 : loading BERT model bert-large-uncased
2019-03-12 15:26:27,631 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:26:27,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:26:27,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp28031s1p
2019-03-12 15:26:35,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:26:41,033 : Computing embeddings for train/dev/test
2019-03-12 15:32:09,128 : Computed embeddings
2019-03-12 15:32:09,128 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:33:11,222 : [('reg:1e-05', 43.38), ('reg:0.0001', 47.49), ('reg:0.001', 45.58), ('reg:0.01', 26.21)]
2019-03-12 15:33:11,222 : Validation : best param found is reg = 0.0001 with score             47.49
2019-03-12 15:33:11,222 : Evaluating...
2019-03-12 15:33:30,053 : 
Dev acc : 47.5 Test acc : 47.3 for TOPCONSTITUENTS classification

2019-03-12 15:33:30,054 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 15:33:30,587 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 15:33:30,658 : loading BERT model bert-large-uncased
2019-03-12 15:33:30,658 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:33:30,695 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:33:30,695 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9rgaczx_
2019-03-12 15:33:38,166 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:33:43,758 : Computing embeddings for train/dev/test
2019-03-12 15:39:49,236 : Computed embeddings
2019-03-12 15:39:49,236 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:40:44,987 : [('reg:1e-05', 53.34), ('reg:0.0001', 53.08), ('reg:0.001', 53.16), ('reg:0.01', 50.0)]
2019-03-12 15:40:44,987 : Validation : best param found is reg = 1e-05 with score             53.34
2019-03-12 15:40:44,987 : Evaluating...
2019-03-12 15:40:59,460 : 
Dev acc : 53.3 Test acc : 53.9 for BIGRAMSHIFT classification

2019-03-12 15:40:59,461 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 15:41:00,050 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 15:41:00,120 : loading BERT model bert-large-uncased
2019-03-12 15:41:00,120 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:41:00,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:41:00,151 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9_wn67et
2019-03-12 15:41:07,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:41:13,365 : Computing embeddings for train/dev/test
2019-03-12 15:47:08,422 : Computed embeddings
2019-03-12 15:47:08,422 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:48:02,805 : [('reg:1e-05', 82.87), ('reg:0.0001', 82.76), ('reg:0.001', 82.68), ('reg:0.01', 83.15)]
2019-03-12 15:48:02,805 : Validation : best param found is reg = 0.01 with score             83.15
2019-03-12 15:48:02,805 : Evaluating...
2019-03-12 15:48:18,250 : 
Dev acc : 83.2 Test acc : 81.8 for TENSE classification

2019-03-12 15:48:18,251 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 15:48:18,648 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 15:48:18,711 : loading BERT model bert-large-uncased
2019-03-12 15:48:18,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:48:18,839 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:48:18,839 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz5er17gt
2019-03-12 15:48:26,336 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:48:32,058 : Computing embeddings for train/dev/test
2019-03-12 15:54:38,864 : Computed embeddings
2019-03-12 15:54:38,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:55:48,646 : [('reg:1e-05', 75.31), ('reg:0.0001', 75.28), ('reg:0.001', 75.41), ('reg:0.01', 73.6)]
2019-03-12 15:55:48,646 : Validation : best param found is reg = 0.001 with score             75.41
2019-03-12 15:55:48,646 : Evaluating...
2019-03-12 15:56:04,310 : 
Dev acc : 75.4 Test acc : 74.2 for SUBJNUMBER classification

2019-03-12 15:56:04,311 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 15:56:04,908 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 15:56:04,978 : loading BERT model bert-large-uncased
2019-03-12 15:56:04,978 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:56:05,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:56:05,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_40vmxp
2019-03-12 15:56:12,539 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:56:18,276 : Computing embeddings for train/dev/test
2019-03-12 16:02:25,266 : Computed embeddings
2019-03-12 16:02:25,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:03:33,416 : [('reg:1e-05', 72.92), ('reg:0.0001', 72.85), ('reg:0.001', 70.54), ('reg:0.01', 72.18)]
2019-03-12 16:03:33,416 : Validation : best param found is reg = 1e-05 with score             72.92
2019-03-12 16:03:33,416 : Evaluating...
2019-03-12 16:03:51,381 : 
Dev acc : 72.9 Test acc : 75.1 for OBJNUMBER classification

2019-03-12 16:03:51,383 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 16:03:51,801 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 16:03:51,876 : loading BERT model bert-large-uncased
2019-03-12 16:03:51,876 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:03:51,906 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:03:51,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ytsezyd
2019-03-12 16:03:59,388 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:04:05,230 : Computing embeddings for train/dev/test
2019-03-12 16:10:58,884 : Computed embeddings
2019-03-12 16:10:58,884 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:11:49,761 : [('reg:1e-05', 50.98), ('reg:0.0001', 51.0), ('reg:0.001', 50.92), ('reg:0.01', 50.16)]
2019-03-12 16:11:49,761 : Validation : best param found is reg = 0.0001 with score             51.0
2019-03-12 16:11:49,761 : Evaluating...
2019-03-12 16:12:02,720 : 
Dev acc : 51.0 Test acc : 51.1 for ODDMANOUT classification

2019-03-12 16:12:02,721 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 16:12:03,156 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 16:12:03,235 : loading BERT model bert-large-uncased
2019-03-12 16:12:03,235 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:12:03,363 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:12:03,364 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7nem45jx
2019-03-12 16:12:10,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:12:17,090 : Computing embeddings for train/dev/test
2019-03-12 16:19:11,918 : Computed embeddings
2019-03-12 16:19:11,918 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:19:58,727 : [('reg:1e-05', 50.05), ('reg:0.0001', 50.05), ('reg:0.001', 50.04), ('reg:0.01', 50.0)]
2019-03-12 16:19:58,727 : Validation : best param found is reg = 1e-05 with score             50.05
2019-03-12 16:19:58,727 : Evaluating...
2019-03-12 16:20:10,949 : 
Dev acc : 50.0 Test acc : 50.1 for COORDINATIONINVERSION classification

2019-03-12 16:20:10,951 : total results: {'STS12': {'MSRpar': {'pearson': (0.37663414356137925, 1.090204693791009e-26), 'spearman': SpearmanrResult(correlation=0.40849923479517913, pvalue=1.574231822830345e-31), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7876937995386395, 1.6210888920131757e-159), 'spearman': SpearmanrResult(correlation=0.7850942967071823, pvalue=8.944733198100556e-158), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5145397167923402, 2.177660316856962e-32), 'spearman': SpearmanrResult(correlation=0.6088800500457522, pvalue=6.593360213195535e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6578722174496835, 3.5033309602992047e-94), 'spearman': SpearmanrResult(correlation=0.6651635300889128, pvalue=5.669128280556041e-97), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5528394185272021, 2.5873802049046616e-33), 'spearman': SpearmanrResult(correlation=0.44164747150179023, pvalue=1.768783710242354e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5779158591738488, 'wmean': 0.5866817176358798}, 'spearman': {'mean': 0.5818569166277634, 'wmean': 0.5951612227457111}}}, 'STS13': {'FNWN': {'pearson': (0.2030466869012116, 0.005076134520328024), 'spearman': SpearmanrResult(correlation=0.1992730700610512, pvalue=0.005977674904608385), 'nsamples': 189}, 'headlines': {'pearson': (0.7099800937095634, 5.008234881960658e-116), 'spearman': SpearmanrResult(correlation=0.6987257998964989, pvalue=6.573649164462226e-111), 'nsamples': 750}, 'OnWN': {'pearson': (0.6964553512388628, 1.326326117647225e-82), 'spearman': SpearmanrResult(correlation=0.6921006127989217, pvalue=3.4867214118049046e-81), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5364940439498792, 'wmean': 0.641048230767669}, 'spearman': {'mean': 0.5300331609188239, 'wmean': 0.6333169359627386}}}, 'STS14': {'deft-forum': {'pearson': (0.4312038649430657, 8.407581251970095e-22), 'spearman': SpearmanrResult(correlation=0.43296228710073514, pvalue=5.51039868964351e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.704516265990159, 2.7114041888076047e-46), 'spearman': SpearmanrResult(correlation=0.6531242138814253, pvalue=7.26086312480531e-38), 'nsamples': 300}, 'headlines': {'pearson': (0.6764933790563019, 1.8050656454048078e-101), 'spearman': SpearmanrResult(correlation=0.6510411534970864, pvalue=1.2287447119863706e-91), 'nsamples': 750}, 'images': {'pearson': (0.7591737320833986, 1.2211973646781395e-141), 'spearman': SpearmanrResult(correlation=0.7452358958333684, pvalue=8.734452631546931e-134), 'nsamples': 750}, 'OnWN': {'pearson': (0.7414000826592028, 1.0313233405092919e-131), 'spearman': SpearmanrResult(correlation=0.7592178450576039, pvalue=1.1510092197483885e-141), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6477535772954851, 1.954197609581912e-90), 'spearman': SpearmanrResult(correlation=0.6273310959260396, pvalue=2.7121593932518623e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6600901503379355, 'wmean': 0.6730699192912583}, 'spearman': {'mean': 0.6448187485493765, 'wmean': 0.6607706096254219}}}, 'STS15': {'answers-forums': {'pearson': (0.588107000104569, 2.9210745266207333e-36), 'spearman': SpearmanrResult(correlation=0.5778590251307307, pvalue=8.694340835677567e-35), 'nsamples': 375}, 'answers-students': {'pearson': (0.7217584510081363, 1.1840070395316525e-121), 'spearman': SpearmanrResult(correlation=0.7294384531594519, pvalue=1.7594243988897508e-125), 'nsamples': 750}, 'belief': {'pearson': (0.5937573298860636, 4.269000607826738e-37), 'spearman': SpearmanrResult(correlation=0.6223714444246079, pvalue=1.3663187589883963e-41), 'nsamples': 375}, 'headlines': {'pearson': (0.7022608834613423, 1.7224480507868476e-112), 'spearman': SpearmanrResult(correlation=0.7023301036576072, pvalue=1.603031526851119e-112), 'nsamples': 750}, 'images': {'pearson': (0.8121364494213524, 3.4309672717649722e-177), 'spearman': SpearmanrResult(correlation=0.8215493892255682, pvalue=1.0534021755719367e-184), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6836040227762926, 'wmean': 0.7067719872215369}, 'spearman': {'mean': 0.6907096831195932, 'wmean': 0.7133582952050741}}}, 'STS16': {'answer-answer': {'pearson': (0.4884763999703267, 1.2280918168537676e-16), 'spearman': SpearmanrResult(correlation=0.4808585982702769, pvalue=4.207137030647368e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.700967758015361, 3.9793106631842215e-38), 'spearman': SpearmanrResult(correlation=0.7093110571568744, pvalue=2.184926178318295e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7573508172338272, 4.398037097808646e-44), 'spearman': SpearmanrResult(correlation=0.7682701806730975, pvalue=4.6149129950112765e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8267776278029025, 2.1204325998340854e-62), 'spearman': SpearmanrResult(correlation=0.8464258169259594, pvalue=3.5641219510348787e-68), 'nsamples': 244}, 'question-question': {'pearson': (0.6849000020659677, 2.7881365444258445e-30), 'spearman': SpearmanrResult(correlation=0.6944163408857169, pvalue=2.0613823601339457e-31), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.691694521017677, 'wmean': 0.6894456213471869}, 'spearman': {'mean': 0.699856398782385, 'wmean': 0.6974026925148173}}}, 'MR': {'devacc': 60.21, 'acc': 60.17, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.01, 'acc': 67.26, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.59, 'acc': 86.02, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 82.85, 'acc': 83.03, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.83, 'acc': 77.65, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.24, 'acc': 35.2, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 61.5, 'acc': 72.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.45, 'acc': 66.61, 'f1': 79.93, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 75.42, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8059141224024381, 'pearson': 0.8068411579950716, 'spearman': 0.7338772307960827, 'mse': 0.35638962162577853, 'yhat': array([2.78700352, 4.27068546, 1.22033249, ..., 3.05794424, 4.62851815,        3.97835272]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6795249982080018, 'pearson': 0.6617954226375278, 'spearman': 0.6541893387180728, 'mse': 1.4072259809146932, 'yhat': array([1.89312158, 1.29523279, 3.32732064, ..., 3.16780902, 4.07862085,        2.24030045]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.55, 'acc': 58.88, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 75.16, 'acc': 74.33, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 56.83, 'acc': 57.08, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.28, 'acc': 24.3, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 47.49, 'acc': 47.26, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 53.34, 'acc': 53.92, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 83.15, 'acc': 81.8, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.41, 'acc': 74.17, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.92, 'acc': 75.14, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.0, 'acc': 51.06, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.05, 'acc': 50.06, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 16:20:10,952 : STS12 p=0.5867, STS12 s=0.5952, STS13 p=0.6410, STS13 s=0.6333, STS14 p=0.6731, STS14 s=0.6608, STS15 p=0.7068, STS15 s=0.7134, STS 16 p=0.6894, STS16 s=0.6974, STS B p=0.6618, STS B s=0.6542, STS B m=1.4072, SICK-R p=0.8068, SICK-R s=0.7339, SICK-P m=0.3564
2019-03-12 16:20:10,952 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 16:20:10,952 : 0.5867,0.5952,0.6410,0.6333,0.6731,0.6608,0.7068,0.7134,0.6894,0.6974,0.6618,0.6542,1.4072,0.8068,0.7339,0.3564
2019-03-12 16:20:10,952 : MR=60.17, CR=67.26, SUBJ=83.03, MPQA=86.02, SST-B=77.65, SST-F=35.20, TREC=72.60, SICK-E=75.42, SNLI=58.88, MRPC=66.61, MRPC f=79.93
2019-03-12 16:20:10,952 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 16:20:10,952 : 60.17,67.26,83.03,86.02,77.65,35.20,72.60,75.42,58.88,66.61,79.93
2019-03-12 16:20:10,952 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 16:20:10,952 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 16:20:10,952 : na,na,na,na,na,na,na,na,na,na
2019-03-12 16:20:10,952 : SentLen=74.33, WC=57.08, TreeDepth=24.30, TopConst=47.26, BShift=53.92, Tense=81.80, SubjNum=74.17, ObjNum=75.14, SOMO=51.06, CoordInv=50.06, average=58.91
2019-03-12 16:20:10,952 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 16:20:10,952 : 74.33,57.08,24.30,47.26,53.92,81.80,74.17,75.14,51.06,50.06,58.91
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:10,952 : layer 2
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:10,952 : ********************************************************************************
2019-03-12 16:20:11,046 : ***** Transfer task : STS12 *****


2019-03-12 16:20:11,058 : loading BERT model bert-large-uncased
2019-03-12 16:20:11,058 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:20:11,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:20:11,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdir28g8p
2019-03-12 16:20:18,507 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:20:32,115 : MSRpar : pearson = 0.3855, spearman = 0.4174
2019-03-12 16:20:35,294 : MSRvid : pearson = 0.8004, spearman = 0.7992
2019-03-12 16:20:37,700 : SMTeuroparl : pearson = 0.4956, spearman = 0.5986
2019-03-12 16:20:42,550 : surprise.OnWN : pearson = 0.6581, spearman = 0.6653
2019-03-12 16:20:44,973 : surprise.SMTnews : pearson = 0.5691, spearman = 0.4557
2019-03-12 16:20:44,973 : ALL (weighted average) : Pearson = 0.5912,             Spearman = 0.6010
2019-03-12 16:20:44,973 : ALL (average) : Pearson = 0.5818,             Spearman = 0.5872

2019-03-12 16:20:44,974 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 16:20:44,984 : loading BERT model bert-large-uncased
2019-03-12 16:20:44,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:20:45,002 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:20:45,002 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmoxxjudx
2019-03-12 16:20:52,460 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:21:00,596 : FNWN : pearson = 0.2193, spearman = 0.2205
2019-03-12 16:21:04,399 : headlines : pearson = 0.7098, spearman = 0.6976
2019-03-12 16:21:07,736 : OnWN : pearson = 0.7030, spearman = 0.6984
2019-03-12 16:21:07,736 : ALL (weighted average) : Pearson = 0.6455,             Spearman = 0.6378
2019-03-12 16:21:07,736 : ALL (average) : Pearson = 0.5441,             Spearman = 0.5388

2019-03-12 16:21:07,736 : ***** Transfer task : STS14 *****


2019-03-12 16:21:07,784 : loading BERT model bert-large-uncased
2019-03-12 16:21:07,784 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:21:07,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:21:07,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3jgpwu_p
2019-03-12 16:21:15,247 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:21:24,723 : deft-forum : pearson = 0.4391, spearman = 0.4345
2019-03-12 16:21:28,440 : deft-news : pearson = 0.7168, spearman = 0.6672
2019-03-12 16:21:33,237 : headlines : pearson = 0.6683, spearman = 0.6385
2019-03-12 16:21:37,874 : images : pearson = 0.7612, spearman = 0.7483
2019-03-12 16:21:42,925 : OnWN : pearson = 0.7522, spearman = 0.7682
2019-03-12 16:21:49,192 : tweet-news : pearson = 0.6571, spearman = 0.6302
2019-03-12 16:21:49,192 : ALL (weighted average) : Pearson = 0.6778,             Spearman = 0.6625
2019-03-12 16:21:49,192 : ALL (average) : Pearson = 0.6658,             Spearman = 0.6478

2019-03-12 16:21:49,192 : ***** Transfer task : STS15 *****


2019-03-12 16:21:49,224 : loading BERT model bert-large-uncased
2019-03-12 16:21:49,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:21:49,241 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:21:49,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6_stgx8f
2019-03-12 16:21:56,694 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:05,812 : answers-forums : pearson = 0.5933, spearman = 0.5882
2019-03-12 16:22:10,423 : answers-students : pearson = 0.7227, spearman = 0.7302
2019-03-12 16:22:14,132 : belief : pearson = 0.6237, spearman = 0.6456
2019-03-12 16:22:18,265 : headlines : pearson = 0.7052, spearman = 0.7033
2019-03-12 16:22:22,981 : images : pearson = 0.8194, spearman = 0.8269
2019-03-12 16:22:22,981 : ALL (weighted average) : Pearson = 0.7139,             Spearman = 0.7193
2019-03-12 16:22:22,981 : ALL (average) : Pearson = 0.6929,             Spearman = 0.6988

2019-03-12 16:22:22,981 : ***** Transfer task : STS16 *****


2019-03-12 16:22:23,057 : loading BERT model bert-large-uncased
2019-03-12 16:22:23,058 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:22:23,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:22:23,076 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcs154jt7
2019-03-12 16:22:30,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:37,948 : answer-answer : pearson = 0.5230, spearman = 0.5107
2019-03-12 16:22:39,295 : headlines : pearson = 0.7053, spearman = 0.7114
2019-03-12 16:22:41,091 : plagiarism : pearson = 0.7598, spearman = 0.7662
2019-03-12 16:22:44,054 : postediting : pearson = 0.8354, spearman = 0.8493
2019-03-12 16:22:45,715 : question-question : pearson = 0.6839, spearman = 0.6901
2019-03-12 16:22:45,715 : ALL (weighted average) : Pearson = 0.6998,             Spearman = 0.7037
2019-03-12 16:22:45,715 : ALL (average) : Pearson = 0.7015,             Spearman = 0.7056

2019-03-12 16:22:45,715 : ***** Transfer task : MR *****


2019-03-12 16:22:45,732 : loading BERT model bert-large-uncased
2019-03-12 16:22:45,732 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:22:45,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:22:45,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6b96dzm8
2019-03-12 16:22:53,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:22:58,915 : Generating sentence embeddings
2019-03-12 16:23:57,328 : Generated sentence embeddings
2019-03-12 16:23:57,328 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:24:19,709 : Best param found at split 1: l2reg = 0.001                 with score 61.52
2019-03-12 16:24:42,088 : Best param found at split 2: l2reg = 1e-05                 with score 59.3
2019-03-12 16:25:03,402 : Best param found at split 3: l2reg = 1e-05                 with score 58.26
2019-03-12 16:25:24,878 : Best param found at split 4: l2reg = 0.0001                 with score 56.24
2019-03-12 16:25:45,999 : Best param found at split 5: l2reg = 0.0001                 with score 62.67
2019-03-12 16:25:47,376 : Dev acc : 59.6 Test acc : 59.73

2019-03-12 16:25:47,377 : ***** Transfer task : CR *****


2019-03-12 16:25:47,388 : loading BERT model bert-large-uncased
2019-03-12 16:25:47,388 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:25:47,414 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:25:47,414 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6moo5onc
2019-03-12 16:25:54,875 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:26:00,650 : Generating sentence embeddings
2019-03-12 16:26:16,065 : Generated sentence embeddings
2019-03-12 16:26:16,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:26:22,437 : Best param found at split 1: l2reg = 1e-05                 with score 70.78
2019-03-12 16:26:29,642 : Best param found at split 2: l2reg = 0.0001                 with score 71.68
2019-03-12 16:26:36,675 : Best param found at split 3: l2reg = 0.0001                 with score 67.71
2019-03-12 16:26:43,672 : Best param found at split 4: l2reg = 0.0001                 with score 73.25
2019-03-12 16:26:53,625 : Best param found at split 5: l2reg = 0.001                 with score 68.92
2019-03-12 16:26:54,061 : Dev acc : 70.47 Test acc : 66.42

2019-03-12 16:26:54,061 : ***** Transfer task : MPQA *****


2019-03-12 16:26:54,099 : loading BERT model bert-large-uncased
2019-03-12 16:26:54,099 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:26:54,118 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:26:54,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn76a0nz_
2019-03-12 16:27:01,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:27:07,290 : Generating sentence embeddings
2019-03-12 16:27:23,909 : Generated sentence embeddings
2019-03-12 16:27:23,910 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:27:54,899 : Best param found at split 1: l2reg = 0.01                 with score 86.54
2019-03-12 16:28:14,036 : Best param found at split 2: l2reg = 0.0001                 with score 86.72
2019-03-12 16:28:36,007 : Best param found at split 3: l2reg = 0.01                 with score 86.66
2019-03-12 16:29:05,039 : Best param found at split 4: l2reg = 0.01                 with score 86.81
2019-03-12 16:29:37,924 : Best param found at split 5: l2reg = 0.0001                 with score 87.05
2019-03-12 16:29:39,727 : Dev acc : 86.76 Test acc : 83.03

2019-03-12 16:29:39,728 : ***** Transfer task : SUBJ *****


2019-03-12 16:29:39,747 : loading BERT model bert-large-uncased
2019-03-12 16:29:39,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:29:39,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:29:39,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphcvlie7f
2019-03-12 16:29:47,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:29:53,127 : Generating sentence embeddings
2019-03-12 16:30:35,517 : Generated sentence embeddings
2019-03-12 16:30:35,518 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:30:49,511 : Best param found at split 1: l2reg = 0.01                 with score 86.85
2019-03-12 16:31:03,141 : Best param found at split 2: l2reg = 0.0001                 with score 86.56
2019-03-12 16:31:16,539 : Best param found at split 3: l2reg = 1e-05                 with score 83.41
2019-03-12 16:31:31,203 : Best param found at split 4: l2reg = 0.01                 with score 87.15
2019-03-12 16:31:46,211 : Best param found at split 5: l2reg = 1e-05                 with score 88.35
2019-03-12 16:31:46,878 : Dev acc : 86.46 Test acc : 88.01

2019-03-12 16:31:46,879 : ***** Transfer task : SST Binary classification *****


2019-03-12 16:31:47,024 : loading BERT model bert-large-uncased
2019-03-12 16:31:47,024 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:31:47,047 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:31:47,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpatk_52q_
2019-03-12 16:31:54,489 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:31:59,972 : Computing embedding for train
2019-03-12 16:33:39,253 : Computed train embeddings
2019-03-12 16:33:39,253 : Computing embedding for dev
2019-03-12 16:33:41,417 : Computed dev embeddings
2019-03-12 16:33:41,417 : Computing embedding for test
2019-03-12 16:33:45,961 : Computed test embeddings
2019-03-12 16:33:45,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:34:00,745 : [('reg:1e-05', 73.39), ('reg:0.0001', 73.62), ('reg:0.001', 74.2), ('reg:0.01', 74.54)]
2019-03-12 16:34:00,745 : Validation : best param found is reg = 0.01 with score             74.54
2019-03-12 16:34:00,745 : Evaluating...
2019-03-12 16:34:03,976 : 
Dev acc : 74.54 Test acc : 75.78 for             SST Binary classification

2019-03-12 16:34:03,976 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 16:34:04,027 : loading BERT model bert-large-uncased
2019-03-12 16:34:04,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:34:04,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:34:04,048 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmparitaef1
2019-03-12 16:34:11,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:34:17,047 : Computing embedding for train
2019-03-12 16:34:38,738 : Computed train embeddings
2019-03-12 16:34:38,738 : Computing embedding for dev
2019-03-12 16:34:41,569 : Computed dev embeddings
2019-03-12 16:34:41,569 : Computing embedding for test
2019-03-12 16:34:47,152 : Computed test embeddings
2019-03-12 16:34:47,152 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:34:49,682 : [('reg:1e-05', 28.25), ('reg:0.0001', 29.61), ('reg:0.001', 32.97), ('reg:0.01', 28.43)]
2019-03-12 16:34:49,682 : Validation : best param found is reg = 0.001 with score             32.97
2019-03-12 16:34:49,682 : Evaluating...
2019-03-12 16:34:50,502 : 
Dev acc : 32.97 Test acc : 37.01 for             SST Fine-Grained classification

2019-03-12 16:34:50,502 : ***** Transfer task : TREC *****


2019-03-12 16:34:50,515 : loading BERT model bert-large-uncased
2019-03-12 16:34:50,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:34:50,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:34:50,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppqm8ifwb
2019-03-12 16:34:58,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:10,988 : Computed train embeddings
2019-03-12 16:35:11,571 : Computed test embeddings
2019-03-12 16:35:11,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:35:18,981 : [('reg:1e-05', 53.78), ('reg:0.0001', 63.98), ('reg:0.001', 54.84), ('reg:0.01', 56.0)]
2019-03-12 16:35:18,981 : Cross-validation : best param found is reg = 0.0001             with score 63.98
2019-03-12 16:35:18,981 : Evaluating...
2019-03-12 16:35:19,535 : 
Dev acc : 63.98 Test acc : 65.8             for TREC

2019-03-12 16:35:19,536 : ***** Transfer task : MRPC *****


2019-03-12 16:35:19,594 : loading BERT model bert-large-uncased
2019-03-12 16:35:19,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:35:19,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:35:19,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppz9axc8i
2019-03-12 16:35:27,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:32,751 : Computing embedding for train
2019-03-12 16:35:54,790 : Computed train embeddings
2019-03-12 16:35:54,790 : Computing embedding for test
2019-03-12 16:36:04,436 : Computed test embeddings
2019-03-12 16:36:04,457 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:36:09,524 : [('reg:1e-05', 70.29), ('reg:0.0001', 71.32), ('reg:0.001', 72.13), ('reg:0.01', 70.41)]
2019-03-12 16:36:09,524 : Cross-validation : best param found is reg = 0.001             with score 72.13
2019-03-12 16:36:09,525 : Evaluating...
2019-03-12 16:36:09,922 : Dev acc : 72.13 Test acc 62.2; Test F1 65.76 for MRPC.

2019-03-12 16:36:09,923 : ***** Transfer task : SICK-Entailment*****


2019-03-12 16:36:09,946 : loading BERT model bert-large-uncased
2019-03-12 16:36:09,946 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:36:10,002 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:36:10,002 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjd2twgqx
2019-03-12 16:36:17,479 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:36:22,875 : Computing embedding for train
2019-03-12 16:36:34,054 : Computed train embeddings
2019-03-12 16:36:34,054 : Computing embedding for dev
2019-03-12 16:36:35,580 : Computed dev embeddings
2019-03-12 16:36:35,581 : Computing embedding for test
2019-03-12 16:36:47,574 : Computed test embeddings
2019-03-12 16:36:47,610 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:36:48,649 : [('reg:1e-05', 77.2), ('reg:0.0001', 72.6), ('reg:0.001', 68.0), ('reg:0.01', 74.6)]
2019-03-12 16:36:48,650 : Validation : best param found is reg = 1e-05 with score             77.2
2019-03-12 16:36:48,650 : Evaluating...
2019-03-12 16:36:48,887 : 
Dev acc : 77.2 Test acc : 75.81 for                        SICK entailment

2019-03-12 16:36:48,888 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 16:36:48,915 : loading BERT model bert-large-uncased
2019-03-12 16:36:48,915 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:36:48,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:36:48,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjax98jiw
2019-03-12 16:36:56,424 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:37:01,946 : Computing embedding for train
2019-03-12 16:37:13,140 : Computed train embeddings
2019-03-12 16:37:13,140 : Computing embedding for dev
2019-03-12 16:37:14,669 : Computed dev embeddings
2019-03-12 16:37:14,669 : Computing embedding for test
2019-03-12 16:37:26,689 : Computed test embeddings
2019-03-12 16:37:41,771 : Dev : Pearson 0.7995990549544443
2019-03-12 16:37:41,772 : Test : Pearson 0.8034333701694516 Spearman 0.7335687777630575 MSE 0.362727659281624                        for SICK Relatedness

2019-03-12 16:37:41,773 : 

***** Transfer task : STSBenchmark*****


2019-03-12 16:37:41,859 : loading BERT model bert-large-uncased
2019-03-12 16:37:41,859 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:37:41,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:37:41,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb6dsg03_
2019-03-12 16:37:49,353 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:37:54,939 : Computing embedding for train
2019-03-12 16:38:13,358 : Computed train embeddings
2019-03-12 16:38:13,358 : Computing embedding for dev
2019-03-12 16:38:18,950 : Computed dev embeddings
2019-03-12 16:38:18,950 : Computing embedding for test
2019-03-12 16:38:23,514 : Computed test embeddings
2019-03-12 16:38:41,835 : Dev : Pearson 0.6739641047045161
2019-03-12 16:38:41,835 : Test : Pearson 0.6493834286545799 Spearman 0.6444159233164553 MSE 1.4495010667916834                        for SICK Relatedness

2019-03-12 16:38:41,836 : ***** Transfer task : SNLI Entailment*****


2019-03-12 16:38:46,684 : loading BERT model bert-large-uncased
2019-03-12 16:38:46,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:38:46,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:38:46,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7ohmtg_w
2019-03-12 16:38:54,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:39:00,282 : PROGRESS (encoding): 0.00%
2019-03-12 16:41:44,318 : PROGRESS (encoding): 14.56%
2019-03-12 16:44:50,169 : PROGRESS (encoding): 29.12%
2019-03-12 16:47:56,703 : PROGRESS (encoding): 43.69%
2019-03-12 16:51:15,505 : PROGRESS (encoding): 58.25%
2019-03-12 16:54:56,667 : PROGRESS (encoding): 72.81%
2019-03-12 16:58:36,754 : PROGRESS (encoding): 87.37%
2019-03-12 17:02:35,052 : PROGRESS (encoding): 0.00%
2019-03-12 17:03:05,058 : PROGRESS (encoding): 0.00%
2019-03-12 17:03:33,889 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:04:20,721 : [('reg:1e-09', 57.87)]
2019-03-12 17:04:20,721 : Validation : best param found is reg = 1e-09 with score             57.87
2019-03-12 17:04:20,721 : Evaluating...
2019-03-12 17:05:07,421 : Dev acc : 57.87 Test acc : 57.94 for SNLI

2019-03-12 17:05:07,421 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 17:05:07,629 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 17:05:08,703 : loading BERT model bert-large-uncased
2019-03-12 17:05:08,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:05:08,729 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:05:08,729 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnc3k33sl
2019-03-12 17:05:16,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:05:21,719 : Computing embeddings for train/dev/test
2019-03-12 17:08:50,229 : Computed embeddings
2019-03-12 17:08:50,229 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:09:18,903 : [('reg:1e-05', 74.73), ('reg:0.0001', 62.03), ('reg:0.001', 69.65), ('reg:0.01', 58.71)]
2019-03-12 17:09:18,903 : Validation : best param found is reg = 1e-05 with score             74.73
2019-03-12 17:09:18,904 : Evaluating...
2019-03-12 17:09:25,482 : 
Dev acc : 74.7 Test acc : 75.2 for LENGTH classification

2019-03-12 17:09:25,483 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 17:09:25,866 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 17:09:25,912 : loading BERT model bert-large-uncased
2019-03-12 17:09:25,913 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:09:25,944 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:09:25,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmpk8uuw6
2019-03-12 17:09:33,404 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:09:38,942 : Computing embeddings for train/dev/test
2019-03-12 17:12:51,134 : Computed embeddings
2019-03-12 17:12:51,134 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:13:24,069 : [('reg:1e-05', 51.07), ('reg:0.0001', 48.52), ('reg:0.001', 1.29), ('reg:0.01', 0.2)]
2019-03-12 17:13:24,070 : Validation : best param found is reg = 1e-05 with score             51.07
2019-03-12 17:13:24,070 : Evaluating...
2019-03-12 17:13:34,962 : 
Dev acc : 51.1 Test acc : 51.2 for WORDCONTENT classification

2019-03-12 17:13:34,963 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 17:13:35,318 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 17:13:35,384 : loading BERT model bert-large-uncased
2019-03-12 17:13:35,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:13:35,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:13:35,481 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi5qqizy4
2019-03-12 17:13:42,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:13:48,395 : Computing embeddings for train/dev/test
2019-03-12 17:16:49,039 : Computed embeddings
2019-03-12 17:16:49,039 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:17:18,672 : [('reg:1e-05', 24.13), ('reg:0.0001', 22.88), ('reg:0.001', 24.92), ('reg:0.01', 19.86)]
2019-03-12 17:17:18,672 : Validation : best param found is reg = 0.001 with score             24.92
2019-03-12 17:17:18,673 : Evaluating...
2019-03-12 17:17:24,860 : 
Dev acc : 24.9 Test acc : 24.2 for DEPTH classification

2019-03-12 17:17:24,861 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 17:17:25,232 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 17:17:25,296 : loading BERT model bert-large-uncased
2019-03-12 17:17:25,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:17:25,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:17:25,407 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj5e8_i9b
2019-03-12 17:17:32,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:17:38,507 : Computing embeddings for train/dev/test
2019-03-12 17:20:25,637 : Computed embeddings
2019-03-12 17:20:25,637 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:20:57,616 : [('reg:1e-05', 45.44), ('reg:0.0001', 43.3), ('reg:0.001', 43.34), ('reg:0.01', 28.4)]
2019-03-12 17:20:57,616 : Validation : best param found is reg = 1e-05 with score             45.44
2019-03-12 17:20:57,616 : Evaluating...
2019-03-12 17:21:07,606 : 
Dev acc : 45.4 Test acc : 44.9 for TOPCONSTITUENTS classification

2019-03-12 17:21:07,607 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 17:21:08,120 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 17:21:08,186 : loading BERT model bert-large-uncased
2019-03-12 17:21:08,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:21:08,214 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:21:08,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk_7m3sxk
2019-03-12 17:21:15,665 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:21:21,198 : Computing embeddings for train/dev/test
2019-03-12 17:24:23,045 : Computed embeddings
2019-03-12 17:24:23,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:24:51,340 : [('reg:1e-05', 53.81), ('reg:0.0001', 53.78), ('reg:0.001', 53.5), ('reg:0.01', 50.0)]
2019-03-12 17:24:51,341 : Validation : best param found is reg = 1e-05 with score             53.81
2019-03-12 17:24:51,341 : Evaluating...
2019-03-12 17:24:58,991 : 
Dev acc : 53.8 Test acc : 54.3 for BIGRAMSHIFT classification

2019-03-12 17:24:58,992 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 17:24:59,391 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 17:24:59,456 : loading BERT model bert-large-uncased
2019-03-12 17:24:59,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:24:59,483 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:24:59,484 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcdt3wstb
2019-03-12 17:25:06,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:25:12,461 : Computing embeddings for train/dev/test
2019-03-12 17:28:10,179 : Computed embeddings
2019-03-12 17:28:10,179 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:28:46,411 : [('reg:1e-05', 81.9), ('reg:0.0001', 82.1), ('reg:0.001', 82.4), ('reg:0.01', 82.68)]
2019-03-12 17:28:46,411 : Validation : best param found is reg = 0.01 with score             82.68
2019-03-12 17:28:46,411 : Evaluating...
2019-03-12 17:28:55,098 : 
Dev acc : 82.7 Test acc : 81.8 for TENSE classification

2019-03-12 17:28:55,099 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 17:28:55,495 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 17:28:55,558 : loading BERT model bert-large-uncased
2019-03-12 17:28:55,558 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:28:55,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:28:55,671 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpftsvoxni
2019-03-12 17:29:03,161 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:29:08,777 : Computing embeddings for train/dev/test
2019-03-12 17:32:16,888 : Computed embeddings
2019-03-12 17:32:16,888 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:32:53,507 : [('reg:1e-05', 75.79), ('reg:0.0001', 75.78), ('reg:0.001', 75.77), ('reg:0.01', 72.57)]
2019-03-12 17:32:53,507 : Validation : best param found is reg = 1e-05 with score             75.79
2019-03-12 17:32:53,507 : Evaluating...
2019-03-12 17:33:03,340 : 
Dev acc : 75.8 Test acc : 74.2 for SUBJNUMBER classification

2019-03-12 17:33:03,341 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 17:33:03,934 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 17:33:04,000 : loading BERT model bert-large-uncased
2019-03-12 17:33:04,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:33:04,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:33:04,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9vczh88l
2019-03-12 17:33:11,476 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:33:16,951 : Computing embeddings for train/dev/test
2019-03-12 17:36:21,554 : Computed embeddings
2019-03-12 17:36:21,554 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:36:54,387 : [('reg:1e-05', 73.86), ('reg:0.0001', 73.78), ('reg:0.001', 73.76), ('reg:0.01', 71.4)]
2019-03-12 17:36:54,387 : Validation : best param found is reg = 1e-05 with score             73.86
2019-03-12 17:36:54,387 : Evaluating...
2019-03-12 17:37:02,791 : 
Dev acc : 73.9 Test acc : 75.3 for OBJNUMBER classification

2019-03-12 17:37:02,792 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 17:37:03,162 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 17:37:03,230 : loading BERT model bert-large-uncased
2019-03-12 17:37:03,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:37:03,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:37:03,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa1bukyie
2019-03-12 17:37:10,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:37:16,351 : Computing embeddings for train/dev/test
2019-03-12 17:40:50,581 : Computed embeddings
2019-03-12 17:40:50,581 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:41:16,905 : [('reg:1e-05', 51.52), ('reg:0.0001', 51.54), ('reg:0.001', 51.38), ('reg:0.01', 51.49)]
2019-03-12 17:41:16,905 : Validation : best param found is reg = 0.0001 with score             51.54
2019-03-12 17:41:16,905 : Evaluating...
2019-03-12 17:41:23,344 : 
Dev acc : 51.5 Test acc : 52.3 for ODDMANOUT classification

2019-03-12 17:41:23,345 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 17:41:23,784 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 17:41:23,859 : loading BERT model bert-large-uncased
2019-03-12 17:41:23,860 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:41:23,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:41:23,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkky15e2t
2019-03-12 17:41:31,360 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:41:36,986 : Computing embeddings for train/dev/test
2019-03-12 17:45:09,639 : Computed embeddings
2019-03-12 17:45:09,639 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:45:35,075 : [('reg:1e-05', 50.07), ('reg:0.0001', 50.07), ('reg:0.001', 50.02), ('reg:0.01', 50.0)]
2019-03-12 17:45:35,076 : Validation : best param found is reg = 1e-05 with score             50.07
2019-03-12 17:45:35,076 : Evaluating...
2019-03-12 17:45:41,802 : 
Dev acc : 50.1 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-03-12 17:45:41,804 : total results: {'STS12': {'MSRpar': {'pearson': (0.38551589978099354, 5.511057236994927e-28), 'spearman': SpearmanrResult(correlation=0.41735663371691367, pvalue=5.6948404914480795e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.800430045603883, 2.0300367191568103e-168), 'spearman': SpearmanrResult(correlation=0.7991959876725296, pvalue=1.5779223316366255e-167), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4955906762620525, 8.024440098500992e-30), 'spearman': SpearmanrResult(correlation=0.5985824771649941, pvalue=5.863556171190069e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6581219484986242, 2.8195415493807087e-94), 'spearman': SpearmanrResult(correlation=0.6653485715130293, pvalue=4.804609454452968e-97), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5691075525832335, 1.2709256825822022e-35), 'spearman': SpearmanrResult(correlation=0.45571216276934035, pvalue=7.447219624197e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5817532245457573, 'wmean': 0.5912486982939569}, 'spearman': {'mean': 0.5872391665673614, 'wmean': 0.6010310182241163}}}, 'STS13': {'FNWN': {'pearson': (0.21934909944366918, 0.0024248432356583118), 'spearman': SpearmanrResult(correlation=0.22050647229990764, pvalue=0.00229626458150785), 'nsamples': 189}, 'headlines': {'pearson': (0.7097973761439766, 6.091695599785226e-116), 'spearman': SpearmanrResult(correlation=0.6975692138474435, pvalue=2.1392071225576085e-110), 'nsamples': 750}, 'OnWN': {'pearson': (0.7030372380334006, 8.465032285813529e-85), 'spearman': SpearmanrResult(correlation=0.6983977566998045, pvalue=3.027637394171874e-83), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5440612378736821, 'wmean': 0.6454726016263824}, 'spearman': {'mean': 0.5388244809490519, 'wmean': 0.637769183439237}}}, 'STS14': {'deft-forum': {'pearson': (0.43912373092220125, 1.2294933960307491e-22), 'spearman': SpearmanrResult(correlation=0.4344714632776609, pvalue=3.8268904809409774e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7168163150320053, 1.3794867690052903e-48), 'spearman': SpearmanrResult(correlation=0.6671801438313063, pvalue=5.284575005897093e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.6683361189742775, 3.268150495129233e-98), 'spearman': SpearmanrResult(correlation=0.6384592138442348, pvalue=4.060976066310215e-87), 'nsamples': 750}, 'images': {'pearson': (0.7612278461755637, 7.65276896049338e-143), 'spearman': SpearmanrResult(correlation=0.7483394613790719, pvalue=1.7265592730725114e-135), 'nsamples': 750}, 'OnWN': {'pearson': (0.7521669842701479, 1.2613837620905602e-137), 'spearman': SpearmanrResult(correlation=0.7682172990938073, pvalue=4.984109332604177e-147), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6570766165867757, 6.987356847907098e-94), 'spearman': SpearmanrResult(correlation=0.6301662853531402, pvalue=2.9763106979688796e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6657912686601619, 'wmean': 0.6778016661145777}, 'spearman': {'mean': 0.6478056444632035, 'wmean': 0.6625474390338746}}}, 'STS15': {'answers-forums': {'pearson': (0.5933087397496513, 4.98010441951813e-37), 'spearman': SpearmanrResult(correlation=0.5881546946909154, pvalue=2.8744916714907984e-36), 'nsamples': 375}, 'answers-students': {'pearson': (0.722713633037696, 4.0207896967417695e-122), 'spearman': SpearmanrResult(correlation=0.7301829977142743, pvalue=7.366357754254691e-126), 'nsamples': 750}, 'belief': {'pearson': (0.6236750703424646, 8.310349790366222e-42), 'spearman': SpearmanrResult(correlation=0.645566166196888, pvalue=1.3657465405774895e-45), 'nsamples': 375}, 'headlines': {'pearson': (0.7052243563012534, 7.799768924024689e-114), 'spearman': SpearmanrResult(correlation=0.7032971908929558, pvalue=5.86187573161819e-113), 'nsamples': 750}, 'images': {'pearson': (0.8193631475837386, 6.404945207691583e-183), 'spearman': SpearmanrResult(correlation=0.8269028735513861, pvalue=3.5408770468449975e-189), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6928569894029607, 'wmean': 0.7139482604921865}, 'spearman': {'mean': 0.698820784609284, 'wmean': 0.7193108731506295}}}, 'STS16': {'answer-answer': {'pearson': (0.5230434806061472, 3.075534069127532e-19), 'spearman': SpearmanrResult(correlation=0.5107448450882224, pvalue=2.80175580589503e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.7052738240495385, 9.012274651261903e-39), 'spearman': SpearmanrResult(correlation=0.7113703733910735, pvalue=1.0506479913220911e-39), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7598452144916513, 1.586687955870182e-44), 'spearman': SpearmanrResult(correlation=0.7662106402983355, pvalue=1.1109518615902946e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.835390322916957, 7.74759586919806e-65), 'spearman': SpearmanrResult(correlation=0.8493263947777095, pvalue=4.275282052337982e-69), 'nsamples': 244}, 'question-question': {'pearson': (0.6839050638779003, 3.639887692531494e-30), 'spearman': SpearmanrResult(correlation=0.6901474730192306, pvalue=6.713913786588856e-31), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.7014915811884389, 'wmean': 0.6998330714482252}, 'spearman': {'mean': 0.7055599453149143, 'wmean': 0.7036807108618746}}}, 'MR': {'devacc': 59.6, 'acc': 59.73, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.47, 'acc': 66.42, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.76, 'acc': 83.03, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.46, 'acc': 88.01, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 74.54, 'acc': 75.78, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 32.97, 'acc': 37.01, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 63.98, 'acc': 65.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.13, 'acc': 62.2, 'f1': 65.76, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.2, 'acc': 75.81, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7995990549544443, 'pearson': 0.8034333701694516, 'spearman': 0.7335687777630575, 'mse': 0.362727659281624, 'yhat': array([2.50097307, 4.57883501, 1.28875799, ..., 3.0275644 , 4.789232  ,        4.73337024]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6739641047045161, 'pearson': 0.6493834286545799, 'spearman': 0.6444159233164553, 'mse': 1.4495010667916834, 'yhat': array([1.48373518, 1.93168444, 3.10039376, ..., 3.16467767, 3.53850946,        3.07079312]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.87, 'acc': 57.94, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 74.73, 'acc': 75.17, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 51.07, 'acc': 51.18, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 24.92, 'acc': 24.21, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 45.44, 'acc': 44.85, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 53.81, 'acc': 54.34, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.68, 'acc': 81.78, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.79, 'acc': 74.17, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.86, 'acc': 75.35, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.54, 'acc': 52.33, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.07, 'acc': 50.05, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 17:45:41,805 : STS12 p=0.5912, STS12 s=0.6010, STS13 p=0.6455, STS13 s=0.6378, STS14 p=0.6778, STS14 s=0.6625, STS15 p=0.7139, STS15 s=0.7193, STS 16 p=0.6998, STS16 s=0.7037, STS B p=0.6494, STS B s=0.6444, STS B m=1.4495, SICK-R p=0.8034, SICK-R s=0.7336, SICK-P m=0.3627
2019-03-12 17:45:41,805 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 17:45:41,805 : 0.5912,0.6010,0.6455,0.6378,0.6778,0.6625,0.7139,0.7193,0.6998,0.7037,0.6494,0.6444,1.4495,0.8034,0.7336,0.3627
2019-03-12 17:45:41,805 : MR=59.73, CR=66.42, SUBJ=88.01, MPQA=83.03, SST-B=75.78, SST-F=37.01, TREC=65.80, SICK-E=75.81, SNLI=57.94, MRPC=62.20, MRPC f=65.76
2019-03-12 17:45:41,805 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 17:45:41,805 : 59.73,66.42,88.01,83.03,75.78,37.01,65.80,75.81,57.94,62.20,65.76
2019-03-12 17:45:41,805 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 17:45:41,805 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 17:45:41,805 : na,na,na,na,na,na,na,na,na,na
2019-03-12 17:45:41,805 : SentLen=75.17, WC=51.18, TreeDepth=24.21, TopConst=44.85, BShift=54.34, Tense=81.78, SubjNum=74.17, ObjNum=75.35, SOMO=52.33, CoordInv=50.05, average=58.34
2019-03-12 17:45:41,805 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 17:45:41,805 : 75.17,51.18,24.21,44.85,54.34,81.78,74.17,75.35,52.33,50.05,58.34
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,805 : layer 3
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,805 : ********************************************************************************
2019-03-12 17:45:41,896 : ***** Transfer task : STS12 *****


2019-03-12 17:45:41,908 : loading BERT model bert-large-uncased
2019-03-12 17:45:41,908 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:45:41,925 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:45:41,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfa48wsqi
2019-03-12 17:45:49,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:45:58,941 : MSRpar : pearson = 0.3905, spearman = 0.4202
2019-03-12 17:46:00,572 : MSRvid : pearson = 0.7853, spearman = 0.7850
2019-03-12 17:46:01,989 : SMTeuroparl : pearson = 0.4924, spearman = 0.5893
2019-03-12 17:46:04,663 : surprise.OnWN : pearson = 0.6553, spearman = 0.6659
2019-03-12 17:46:06,081 : surprise.SMTnews : pearson = 0.5657, spearman = 0.4590
2019-03-12 17:46:06,081 : ALL (weighted average) : Pearson = 0.5872,             Spearman = 0.5975
2019-03-12 17:46:06,081 : ALL (average) : Pearson = 0.5778,             Spearman = 0.5839

2019-03-12 17:46:06,081 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 17:46:06,089 : loading BERT model bert-large-uncased
2019-03-12 17:46:06,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:46:06,107 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:46:06,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjunnddk0
2019-03-12 17:46:13,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:46:20,470 : FNWN : pearson = 0.2277, spearman = 0.2259
2019-03-12 17:46:22,350 : headlines : pearson = 0.6956, spearman = 0.6854
2019-03-12 17:46:23,807 : OnWN : pearson = 0.6779, spearman = 0.6758
2019-03-12 17:46:23,807 : ALL (weighted average) : Pearson = 0.6300,             Spearman = 0.6239
2019-03-12 17:46:23,808 : ALL (average) : Pearson = 0.5337,             Spearman = 0.5291

2019-03-12 17:46:23,808 : ***** Transfer task : STS14 *****


2019-03-12 17:46:23,823 : loading BERT model bert-large-uncased
2019-03-12 17:46:23,823 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:46:23,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:46:23,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbi2hsxw0
2019-03-12 17:46:31,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:46:37,983 : deft-forum : pearson = 0.4275, spearman = 0.4243
2019-03-12 17:46:39,607 : deft-news : pearson = 0.7089, spearman = 0.6640
2019-03-12 17:46:41,764 : headlines : pearson = 0.6586, spearman = 0.6286
2019-03-12 17:46:43,827 : images : pearson = 0.7424, spearman = 0.7327
2019-03-12 17:46:45,941 : OnWN : pearson = 0.7375, spearman = 0.7563
2019-03-12 17:46:48,778 : tweet-news : pearson = 0.6443, spearman = 0.6211
2019-03-12 17:46:48,778 : ALL (weighted average) : Pearson = 0.6646,             Spearman = 0.6518
2019-03-12 17:46:48,778 : ALL (average) : Pearson = 0.6532,             Spearman = 0.6378

2019-03-12 17:46:48,778 : ***** Transfer task : STS15 *****


2019-03-12 17:46:48,810 : loading BERT model bert-large-uncased
2019-03-12 17:46:48,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:46:48,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:46:48,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpznazuasi
2019-03-12 17:46:56,369 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:47:03,923 : answers-forums : pearson = 0.5736, spearman = 0.5652
2019-03-12 17:47:05,990 : answers-students : pearson = 0.7234, spearman = 0.7261
2019-03-12 17:47:08,021 : belief : pearson = 0.6178, spearman = 0.6393
2019-03-12 17:47:10,252 : headlines : pearson = 0.6959, spearman = 0.6955
2019-03-12 17:47:12,368 : images : pearson = 0.8132, spearman = 0.8196
2019-03-12 17:47:12,368 : ALL (weighted average) : Pearson = 0.7071,             Spearman = 0.7109
2019-03-12 17:47:12,368 : ALL (average) : Pearson = 0.6848,             Spearman = 0.6892

2019-03-12 17:47:12,369 : ***** Transfer task : STS16 *****


2019-03-12 17:47:12,437 : loading BERT model bert-large-uncased
2019-03-12 17:47:12,437 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:47:12,455 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:47:12,455 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw1gynaee
2019-03-12 17:47:19,951 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:47:26,349 : answer-answer : pearson = 0.5204, spearman = 0.5154
2019-03-12 17:47:27,005 : headlines : pearson = 0.6980, spearman = 0.7016
2019-03-12 17:47:27,881 : plagiarism : pearson = 0.7538, spearman = 0.7582
2019-03-12 17:47:29,360 : postediting : pearson = 0.8375, spearman = 0.8464
2019-03-12 17:47:29,965 : question-question : pearson = 0.6640, spearman = 0.6695
2019-03-12 17:47:29,965 : ALL (weighted average) : Pearson = 0.6935,             Spearman = 0.6969
2019-03-12 17:47:29,965 : ALL (average) : Pearson = 0.6948,             Spearman = 0.6982

2019-03-12 17:47:29,965 : ***** Transfer task : MR *****


2019-03-12 17:47:29,980 : loading BERT model bert-large-uncased
2019-03-12 17:47:29,980 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:47:30,001 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:47:30,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplox6qln1
2019-03-12 17:47:37,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:47:44,239 : Generating sentence embeddings
2019-03-12 17:48:15,664 : Generated sentence embeddings
2019-03-12 17:48:15,665 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:48:24,034 : Best param found at split 1: l2reg = 1e-05                 with score 61.89
2019-03-12 17:48:35,568 : Best param found at split 2: l2reg = 1e-05                 with score 58.64
2019-03-12 17:48:46,998 : Best param found at split 3: l2reg = 0.001                 with score 57.6
2019-03-12 17:48:57,173 : Best param found at split 4: l2reg = 0.001                 with score 57.84
2019-03-12 17:49:08,840 : Best param found at split 5: l2reg = 1e-05                 with score 61.63
2019-03-12 17:49:09,551 : Dev acc : 59.52 Test acc : 60.1

2019-03-12 17:49:09,552 : ***** Transfer task : CR *****


2019-03-12 17:49:09,559 : loading BERT model bert-large-uncased
2019-03-12 17:49:09,559 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:49:09,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:49:09,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7jmxg0_9
2019-03-12 17:49:17,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:49:22,658 : Generating sentence embeddings
2019-03-12 17:49:30,894 : Generated sentence embeddings
2019-03-12 17:49:30,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:49:33,839 : Best param found at split 1: l2reg = 1e-05                 with score 68.37
2019-03-12 17:49:37,305 : Best param found at split 2: l2reg = 1e-05                 with score 68.1
2019-03-12 17:49:40,543 : Best param found at split 3: l2reg = 1e-05                 with score 70.23
2019-03-12 17:49:44,320 : Best param found at split 4: l2reg = 1e-05                 with score 72.92
2019-03-12 17:49:47,956 : Best param found at split 5: l2reg = 0.0001                 with score 72.73
2019-03-12 17:49:48,150 : Dev acc : 70.47 Test acc : 67.16

2019-03-12 17:49:48,150 : ***** Transfer task : MPQA *****


2019-03-12 17:49:48,156 : loading BERT model bert-large-uncased
2019-03-12 17:49:48,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:49:48,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:49:48,207 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptaaxh7x9
2019-03-12 17:49:55,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:50:01,510 : Generating sentence embeddings
2019-03-12 17:50:09,020 : Generated sentence embeddings
2019-03-12 17:50:09,021 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:50:21,219 : Best param found at split 1: l2reg = 1e-05                 with score 86.42
2019-03-12 17:50:32,949 : Best param found at split 2: l2reg = 0.001                 with score 85.69
2019-03-12 17:50:45,672 : Best param found at split 3: l2reg = 0.001                 with score 86.31
2019-03-12 17:50:54,368 : Best param found at split 4: l2reg = 0.0001                 with score 86.1
2019-03-12 17:51:04,820 : Best param found at split 5: l2reg = 0.001                 with score 86.21
2019-03-12 17:51:05,399 : Dev acc : 86.15 Test acc : 85.26

2019-03-12 17:51:05,400 : ***** Transfer task : SUBJ *****


2019-03-12 17:51:05,418 : loading BERT model bert-large-uncased
2019-03-12 17:51:05,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:05,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:05,437 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpogj7w72f
2019-03-12 17:51:13,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:51:18,861 : Generating sentence embeddings
2019-03-12 17:51:49,447 : Generated sentence embeddings
2019-03-12 17:51:49,448 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:52:00,046 : Best param found at split 1: l2reg = 0.001                 with score 87.4
2019-03-12 17:52:14,307 : Best param found at split 2: l2reg = 0.001                 with score 87.54
2019-03-12 17:52:28,545 : Best param found at split 3: l2reg = 0.001                 with score 87.52
2019-03-12 17:52:39,569 : Best param found at split 4: l2reg = 1e-05                 with score 87.21
2019-03-12 17:52:49,720 : Best param found at split 5: l2reg = 0.01                 with score 87.26
2019-03-12 17:52:50,145 : Dev acc : 87.39 Test acc : 88.25

2019-03-12 17:52:50,146 : ***** Transfer task : SST Binary classification *****


2019-03-12 17:52:50,236 : loading BERT model bert-large-uncased
2019-03-12 17:52:50,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:52:50,308 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:52:50,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpijy09odz
2019-03-12 17:52:57,742 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:53:03,303 : Computing embedding for train
2019-03-12 17:54:42,362 : Computed train embeddings
2019-03-12 17:54:42,362 : Computing embedding for dev
2019-03-12 17:54:44,516 : Computed dev embeddings
2019-03-12 17:54:44,517 : Computing embedding for test
2019-03-12 17:54:49,046 : Computed test embeddings
2019-03-12 17:54:49,046 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:55:06,532 : [('reg:1e-05', 77.06), ('reg:0.0001', 77.06), ('reg:0.001', 77.06), ('reg:0.01', 74.08)]
2019-03-12 17:55:06,532 : Validation : best param found is reg = 1e-05 with score             77.06
2019-03-12 17:55:06,532 : Evaluating...
2019-03-12 17:55:10,969 : 
Dev acc : 77.06 Test acc : 76.33 for             SST Binary classification

2019-03-12 17:55:10,969 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 17:55:11,023 : loading BERT model bert-large-uncased
2019-03-12 17:55:11,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:55:11,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:55:11,044 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2iatsau0
2019-03-12 17:55:18,526 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:55:24,075 : Computing embedding for train
2019-03-12 17:55:45,733 : Computed train embeddings
2019-03-12 17:55:45,733 : Computing embedding for dev
2019-03-12 17:55:48,561 : Computed dev embeddings
2019-03-12 17:55:48,562 : Computing embedding for test
2019-03-12 17:55:54,135 : Computed test embeddings
2019-03-12 17:55:54,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:55:56,736 : [('reg:1e-05', 35.33), ('reg:0.0001', 32.7), ('reg:0.001', 29.79), ('reg:0.01', 33.51)]
2019-03-12 17:55:56,736 : Validation : best param found is reg = 1e-05 with score             35.33
2019-03-12 17:55:56,736 : Evaluating...
2019-03-12 17:55:57,391 : 
Dev acc : 35.33 Test acc : 38.51 for             SST Fine-Grained classification

2019-03-12 17:55:57,391 : ***** Transfer task : TREC *****


2019-03-12 17:55:57,405 : loading BERT model bert-large-uncased
2019-03-12 17:55:57,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:55:57,424 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:55:57,424 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvua6a28u
2019-03-12 17:56:04,944 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:17,868 : Computed train embeddings
2019-03-12 17:56:18,449 : Computed test embeddings
2019-03-12 17:56:18,450 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 17:56:25,983 : [('reg:1e-05', 54.84), ('reg:0.0001', 58.86), ('reg:0.001', 49.85), ('reg:0.01', 58.57)]
2019-03-12 17:56:25,983 : Cross-validation : best param found is reg = 0.0001             with score 58.86
2019-03-12 17:56:25,983 : Evaluating...
2019-03-12 17:56:26,330 : 
Dev acc : 58.86 Test acc : 71.0             for TREC

2019-03-12 17:56:26,331 : ***** Transfer task : MRPC *****


2019-03-12 17:56:26,351 : loading BERT model bert-large-uncased
2019-03-12 17:56:26,351 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:56:26,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:56:26,374 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf3bn07bs
2019-03-12 17:56:33,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:39,460 : Computing embedding for train
2019-03-12 17:57:01,454 : Computed train embeddings
2019-03-12 17:57:01,454 : Computing embedding for test
2019-03-12 17:57:11,082 : Computed test embeddings
2019-03-12 17:57:11,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 17:57:16,131 : [('reg:1e-05', 71.59), ('reg:0.0001', 70.46), ('reg:0.001', 70.95), ('reg:0.01', 71.1)]
2019-03-12 17:57:16,131 : Cross-validation : best param found is reg = 1e-05             with score 71.59
2019-03-12 17:57:16,131 : Evaluating...
2019-03-12 17:57:16,436 : Dev acc : 71.59 Test acc 69.1; Test F1 76.0 for MRPC.

2019-03-12 17:57:16,437 : ***** Transfer task : SICK-Entailment*****


2019-03-12 17:57:16,500 : loading BERT model bert-large-uncased
2019-03-12 17:57:16,500 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:16,519 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:16,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvcyz4c9s
2019-03-12 17:57:23,992 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:57:29,524 : Computing embedding for train
2019-03-12 17:57:40,718 : Computed train embeddings
2019-03-12 17:57:40,718 : Computing embedding for dev
2019-03-12 17:57:42,246 : Computed dev embeddings
2019-03-12 17:57:42,247 : Computing embedding for test
2019-03-12 17:57:54,240 : Computed test embeddings
2019-03-12 17:57:54,278 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:57:55,640 : [('reg:1e-05', 79.4), ('reg:0.0001', 71.2), ('reg:0.001', 76.2), ('reg:0.01', 74.6)]
2019-03-12 17:57:55,640 : Validation : best param found is reg = 1e-05 with score             79.4
2019-03-12 17:57:55,640 : Evaluating...
2019-03-12 17:57:56,007 : 
Dev acc : 79.4 Test acc : 78.4 for                        SICK entailment

2019-03-12 17:57:56,008 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 17:57:56,035 : loading BERT model bert-large-uncased
2019-03-12 17:57:56,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:56,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:56,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc8gdd25a
2019-03-12 17:58:03,563 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:58:09,051 : Computing embedding for train
2019-03-12 17:58:20,228 : Computed train embeddings
2019-03-12 17:58:20,228 : Computing embedding for dev
2019-03-12 17:58:21,753 : Computed dev embeddings
2019-03-12 17:58:21,753 : Computing embedding for test
2019-03-12 17:58:33,747 : Computed test embeddings
2019-03-12 17:58:48,581 : Dev : Pearson 0.8080129119531342
2019-03-12 17:58:48,581 : Test : Pearson 0.8018362843308445 Spearman 0.7273575363642109 MSE 0.36430491652080277                        for SICK Relatedness

2019-03-12 17:58:48,584 : 

***** Transfer task : STSBenchmark*****


2019-03-12 17:58:48,653 : loading BERT model bert-large-uncased
2019-03-12 17:58:48,653 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:58:48,672 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:58:48,673 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_tjrq_6w
2019-03-12 17:58:56,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:59:01,540 : Computing embedding for train
2019-03-12 17:59:19,959 : Computed train embeddings
2019-03-12 17:59:19,960 : Computing embedding for dev
2019-03-12 17:59:25,551 : Computed dev embeddings
2019-03-12 17:59:25,551 : Computing embedding for test
2019-03-12 17:59:30,105 : Computed test embeddings
2019-03-12 17:59:48,152 : Dev : Pearson 0.6662936608225253
2019-03-12 17:59:48,152 : Test : Pearson 0.6518251394263531 Spearman 0.648808161759135 MSE 1.4281527585358353                        for SICK Relatedness

2019-03-12 17:59:48,153 : ***** Transfer task : SNLI Entailment*****


2019-03-12 17:59:53,248 : loading BERT model bert-large-uncased
2019-03-12 17:59:53,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:59:53,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:59:53,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp24n5bfma
2019-03-12 18:00:00,956 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:00:06,925 : PROGRESS (encoding): 0.00%
2019-03-12 18:02:50,932 : PROGRESS (encoding): 14.56%
2019-03-12 18:05:56,796 : PROGRESS (encoding): 29.12%
2019-03-12 18:09:03,251 : PROGRESS (encoding): 43.69%
2019-03-12 18:12:22,157 : PROGRESS (encoding): 58.25%
2019-03-12 18:16:03,562 : PROGRESS (encoding): 72.81%
2019-03-12 18:19:43,934 : PROGRESS (encoding): 87.37%
2019-03-12 18:23:42,390 : PROGRESS (encoding): 0.00%
2019-03-12 18:24:12,418 : PROGRESS (encoding): 0.00%
2019-03-12 18:24:41,261 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:25:20,859 : [('reg:1e-09', 63.53)]
2019-03-12 18:25:20,859 : Validation : best param found is reg = 1e-09 with score             63.53
2019-03-12 18:25:20,859 : Evaluating...
2019-03-12 18:25:55,149 : Dev acc : 63.53 Test acc : 64.17 for SNLI

2019-03-12 18:25:55,149 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 18:25:55,349 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 18:25:56,410 : loading BERT model bert-large-uncased
2019-03-12 18:25:56,411 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:25:56,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:25:56,437 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa4s0wqk0
2019-03-12 18:26:03,912 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:26:09,428 : Computing embeddings for train/dev/test
2019-03-12 18:29:37,754 : Computed embeddings
2019-03-12 18:29:37,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:30:05,100 : [('reg:1e-05', 74.23), ('reg:0.0001', 65.04), ('reg:0.001', 71.4), ('reg:0.01', 62.96)]
2019-03-12 18:30:05,100 : Validation : best param found is reg = 1e-05 with score             74.23
2019-03-12 18:30:05,100 : Evaluating...
2019-03-12 18:30:12,736 : 
Dev acc : 74.2 Test acc : 74.6 for LENGTH classification

2019-03-12 18:30:12,737 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 18:30:13,110 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 18:30:13,154 : loading BERT model bert-large-uncased
2019-03-12 18:30:13,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:30:13,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:30:13,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxra7cf2g
2019-03-12 18:30:20,582 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:30:26,006 : Computing embeddings for train/dev/test
2019-03-12 18:33:38,160 : Computed embeddings
2019-03-12 18:33:38,160 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:34:15,295 : [('reg:1e-05', 81.21), ('reg:0.0001', 16.34), ('reg:0.001', 0.41), ('reg:0.01', 0.11)]
2019-03-12 18:34:15,295 : Validation : best param found is reg = 1e-05 with score             81.21
2019-03-12 18:34:15,295 : Evaluating...
2019-03-12 18:34:30,796 : 
Dev acc : 81.2 Test acc : 81.9 for WORDCONTENT classification

2019-03-12 18:34:30,798 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 18:34:31,177 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 18:34:31,243 : loading BERT model bert-large-uncased
2019-03-12 18:34:31,243 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:34:31,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:34:31,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0k8ef0rt
2019-03-12 18:34:38,722 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:34:44,219 : Computing embeddings for train/dev/test
2019-03-12 18:37:44,817 : Computed embeddings
2019-03-12 18:37:44,818 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:38:11,008 : [('reg:1e-05', 25.7), ('reg:0.0001', 27.68), ('reg:0.001', 22.65), ('reg:0.01', 23.07)]
2019-03-12 18:38:11,009 : Validation : best param found is reg = 0.0001 with score             27.68
2019-03-12 18:38:11,009 : Evaluating...
2019-03-12 18:38:18,726 : 
Dev acc : 27.7 Test acc : 27.1 for DEPTH classification

2019-03-12 18:38:18,727 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 18:38:19,143 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 18:38:19,211 : loading BERT model bert-large-uncased
2019-03-12 18:38:19,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:38:19,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:38:19,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpueyu95wp
2019-03-12 18:38:26,812 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:38:32,274 : Computing embeddings for train/dev/test
2019-03-12 18:41:19,560 : Computed embeddings
2019-03-12 18:41:19,560 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:41:48,763 : [('reg:1e-05', 48.84), ('reg:0.0001', 45.56), ('reg:0.001', 32.67), ('reg:0.01', 27.48)]
2019-03-12 18:41:48,763 : Validation : best param found is reg = 1e-05 with score             48.84
2019-03-12 18:41:48,763 : Evaluating...
2019-03-12 18:41:55,819 : 
Dev acc : 48.8 Test acc : 48.6 for TOPCONSTITUENTS classification

2019-03-12 18:41:55,821 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 18:41:56,174 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 18:41:56,240 : loading BERT model bert-large-uncased
2019-03-12 18:41:56,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:41:56,267 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:41:56,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuw1t885a
2019-03-12 18:42:03,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:42:09,201 : Computing embeddings for train/dev/test
2019-03-12 18:45:10,948 : Computed embeddings
2019-03-12 18:45:10,948 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:45:45,147 : [('reg:1e-05', 56.21), ('reg:0.0001', 56.21), ('reg:0.001', 56.2), ('reg:0.01', 50.5)]
2019-03-12 18:45:45,147 : Validation : best param found is reg = 1e-05 with score             56.21
2019-03-12 18:45:45,147 : Evaluating...
2019-03-12 18:45:53,856 : 
Dev acc : 56.2 Test acc : 56.7 for BIGRAMSHIFT classification

2019-03-12 18:45:53,857 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 18:45:54,241 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 18:45:54,306 : loading BERT model bert-large-uncased
2019-03-12 18:45:54,307 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:45:54,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:45:54,422 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5s929idg
2019-03-12 18:46:01,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:46:07,514 : Computing embeddings for train/dev/test
2019-03-12 18:49:05,137 : Computed embeddings
2019-03-12 18:49:05,138 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:49:33,637 : [('reg:1e-05', 82.31), ('reg:0.0001', 82.29), ('reg:0.001', 82.38), ('reg:0.01', 81.09)]
2019-03-12 18:49:33,637 : Validation : best param found is reg = 0.001 with score             82.38
2019-03-12 18:49:33,637 : Evaluating...
2019-03-12 18:49:40,242 : 
Dev acc : 82.4 Test acc : 80.9 for TENSE classification

2019-03-12 18:49:40,243 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 18:49:40,831 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 18:49:40,895 : loading BERT model bert-large-uncased
2019-03-12 18:49:40,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:49:40,921 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:49:40,921 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwagi5b_c
2019-03-12 18:49:48,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:49:54,022 : Computing embeddings for train/dev/test
2019-03-12 18:53:02,187 : Computed embeddings
2019-03-12 18:53:02,187 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:53:37,090 : [('reg:1e-05', 74.51), ('reg:0.0001', 74.5), ('reg:0.001', 74.75), ('reg:0.01', 72.85)]
2019-03-12 18:53:37,090 : Validation : best param found is reg = 0.001 with score             74.75
2019-03-12 18:53:37,090 : Evaluating...
2019-03-12 18:53:45,704 : 
Dev acc : 74.8 Test acc : 73.8 for SUBJNUMBER classification

2019-03-12 18:53:45,705 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 18:53:46,097 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 18:53:46,163 : loading BERT model bert-large-uncased
2019-03-12 18:53:46,163 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:53:46,276 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:53:46,276 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy4ku26e5
2019-03-12 18:53:53,721 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:53:59,312 : Computing embeddings for train/dev/test
2019-03-12 18:57:04,148 : Computed embeddings
2019-03-12 18:57:04,148 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:57:30,433 : [('reg:1e-05', 72.64), ('reg:0.0001', 72.37), ('reg:0.001', 70.55), ('reg:0.01', 72.25)]
2019-03-12 18:57:30,433 : Validation : best param found is reg = 1e-05 with score             72.64
2019-03-12 18:57:30,433 : Evaluating...
2019-03-12 18:57:36,910 : 
Dev acc : 72.6 Test acc : 73.9 for OBJNUMBER classification

2019-03-12 18:57:36,911 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 18:57:37,336 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 18:57:37,405 : loading BERT model bert-large-uncased
2019-03-12 18:57:37,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:57:37,431 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:57:37,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp38f9lpwl
2019-03-12 18:57:44,866 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:57:50,359 : Computing embeddings for train/dev/test
2019-03-12 19:01:24,855 : Computed embeddings
2019-03-12 19:01:24,855 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:01:51,252 : [('reg:1e-05', 52.39), ('reg:0.0001', 52.35), ('reg:0.001', 52.51), ('reg:0.01', 52.99)]
2019-03-12 19:01:51,253 : Validation : best param found is reg = 0.01 with score             52.99
2019-03-12 19:01:51,253 : Evaluating...
2019-03-12 19:01:57,660 : 
Dev acc : 53.0 Test acc : 52.8 for ODDMANOUT classification

2019-03-12 19:01:57,661 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 19:01:58,031 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 19:01:58,112 : loading BERT model bert-large-uncased
2019-03-12 19:01:58,112 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:01:58,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:01:58,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpno1sd2if
2019-03-12 19:02:05,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:02:11,173 : Computing embeddings for train/dev/test
2019-03-12 19:05:43,461 : Computed embeddings
2019-03-12 19:05:43,461 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:06:07,640 : [('reg:1e-05', 50.05), ('reg:0.0001', 50.03), ('reg:0.001', 50.04), ('reg:0.01', 50.01)]
2019-03-12 19:06:07,641 : Validation : best param found is reg = 1e-05 with score             50.05
2019-03-12 19:06:07,641 : Evaluating...
2019-03-12 19:06:12,598 : 
Dev acc : 50.0 Test acc : 50.1 for COORDINATIONINVERSION classification

2019-03-12 19:06:12,600 : total results: {'STS12': {'MSRpar': {'pearson': (0.3904781505890793, 9.987925887842857e-29), 'spearman': SpearmanrResult(correlation=0.42023558819451345, pvalue=1.8947812703473974e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7853346574574762, 6.187814039080941e-158), 'spearman': SpearmanrResult(correlation=0.784981740483386, pvalue=1.0627607037268337e-157), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4924004358908507, 2.0937653964383875e-29), 'spearman': SpearmanrResult(correlation=0.5893166023770922, pvalue=2.901343553815668e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6553177333635578, 3.1911069436752573e-93), 'spearman': SpearmanrResult(correlation=0.6659036957853787, pvalue=2.922627725691777e-97), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5657142326466515, 3.9471033677001166e-35), 'spearman': SpearmanrResult(correlation=0.4590330749380548, pvalue=3.449510171309597e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5778490419895231, 'wmean': 0.5872199758550513}, 'spearman': {'mean': 0.583894140355685, 'wmean': 0.5974875436740115}}}, 'STS13': {'FNWN': {'pearson': (0.22768400095908228, 0.0016278991975794226), 'spearman': SpearmanrResult(correlation=0.22593932955046553, pvalue=0.0017715919433812964), 'nsamples': 189}, 'headlines': {'pearson': (0.6956347642136702, 1.5201255245898967e-109), 'spearman': SpearmanrResult(correlation=0.6854381431307972, pvalue=3.638188662372138e-105), 'nsamples': 750}, 'OnWN': {'pearson': (0.6779160074030361, 9.922224908815717e-77), 'spearman': SpearmanrResult(correlation=0.6758019865186112, pvalue=4.3555391968333e-76), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5337449241919295, 'wmean': 0.630046152996415}, 'spearman': {'mean': 0.5290598197332913, 'wmean': 0.6239373700467179}}}, 'STS14': {'deft-forum': {'pearson': (0.42746433290950964, 2.0480195825916715e-21), 'spearman': SpearmanrResult(correlation=0.4242875502466377, pvalue=4.325606613525885e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7088701076890452, 4.315799542985484e-47), 'spearman': SpearmanrResult(correlation=0.6639680441037661, pvalue=1.6665249941583e-39), 'nsamples': 300}, 'headlines': {'pearson': (0.6585893661946964, 1.8768558584878368e-94), 'spearman': SpearmanrResult(correlation=0.6285783930972154, pvalue=1.0288788014908716e-83), 'nsamples': 750}, 'images': {'pearson': (0.7424397097543001, 2.853814559208213e-132), 'spearman': SpearmanrResult(correlation=0.7326725296951673, pvalue=3.92378702027276e-127), 'nsamples': 750}, 'OnWN': {'pearson': (0.7375164007642334, 1.1863535352492183e-129), 'spearman': SpearmanrResult(correlation=0.756256810701492, pvalue=5.948583074252332e-140), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6443294010326898, 3.362661124581723e-89), 'spearman': SpearmanrResult(correlation=0.6211241288780883, pvalue=3.161492141081029e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6532015530574125, 'wmean': 0.6645803041134487}, 'spearman': {'mean': 0.6378145761203944, 'wmean': 0.6517583220322903}}}, 'STS15': {'answers-forums': {'pearson': (0.5735894412631878, 3.451655783612858e-34), 'spearman': SpearmanrResult(correlation=0.5652276508616473, pvalue=4.849437409672294e-33), 'nsamples': 375}, 'answers-students': {'pearson': (0.7234229995990188, 1.797620536151591e-122), 'spearman': SpearmanrResult(correlation=0.7261136741580532, pvalue=8.290595260685311e-124), 'nsamples': 750}, 'belief': {'pearson': (0.6178487457636428, 7.5311154640703e-41), 'spearman': SpearmanrResult(correlation=0.639321462696256, pvalue=1.7628319488670168e-44), 'nsamples': 375}, 'headlines': {'pearson': (0.6959206506063795, 1.1388025943605732e-109), 'spearman': SpearmanrResult(correlation=0.6955149065369345, pvalue=1.715624791332626e-109), 'nsamples': 750}, 'images': {'pearson': (0.813186375545385, 5.231539226366345e-178), 'spearman': SpearmanrResult(correlation=0.819598976015198, pvalue=4.123360426649639e-183), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6847936425555228, 'wmean': 0.7070622798160496}, 'spearman': {'mean': 0.6891553340536178, 'wmean': 0.7108755283722844}}}, 'STS16': {'answer-answer': {'pearson': (0.5203767450070407, 5.003622948167949e-19), 'spearman': SpearmanrResult(correlation=0.5154498568836255, pvalue=1.2160052483010527e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6980438779050357, 1.0745017398717485e-37), 'spearman': SpearmanrResult(correlation=0.701561557606215, pvalue=3.2475430107553664e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7538341663884176, 1.813242830694711e-43), 'spearman': SpearmanrResult(correlation=0.7582084417077075, pvalue=3.1019362325129373e-44), 'nsamples': 230}, 'postediting': {'pearson': (0.8375174558460716, 1.8427048128108618e-65), 'spearman': SpearmanrResult(correlation=0.8464450870849977, pvalue=3.5147705694712684e-68), 'nsamples': 244}, 'question-question': {'pearson': (0.6640320865099721, 6.035359869277625e-28), 'spearman': SpearmanrResult(correlation=0.6695261033113563, pvalue=1.5283938728856665e-28), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6947608663313076, 'wmean': 0.6935138637491601}, 'spearman': {'mean': 0.6982382093187803, 'wmean': 0.6968504130910406}}}, 'MR': {'devacc': 59.52, 'acc': 60.1, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.47, 'acc': 67.16, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.15, 'acc': 85.26, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 87.39, 'acc': 88.25, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.06, 'acc': 76.33, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.33, 'acc': 38.51, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 58.86, 'acc': 71.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.59, 'acc': 69.1, 'f1': 76.0, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.4, 'acc': 78.4, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8080129119531342, 'pearson': 0.8018362843308445, 'spearman': 0.7273575363642109, 'mse': 0.36430491652080277, 'yhat': array([2.62233604, 4.35488992, 1.40049278, ..., 3.02235531, 4.4044525 ,        4.32111525]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6662936608225253, 'pearson': 0.6518251394263531, 'spearman': 0.648808161759135, 'mse': 1.4281527585358353, 'yhat': array([1.42008776, 1.11761365, 2.79747096, ..., 3.32137466, 4.33210557,        3.05713135]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.53, 'acc': 64.17, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 74.23, 'acc': 74.57, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 81.21, 'acc': 81.93, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.68, 'acc': 27.08, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 48.84, 'acc': 48.56, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 56.21, 'acc': 56.67, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.38, 'acc': 80.89, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.75, 'acc': 73.84, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.64, 'acc': 73.87, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 52.99, 'acc': 52.79, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.05, 'acc': 50.07, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 19:06:12,600 : STS12 p=0.5872, STS12 s=0.5975, STS13 p=0.6300, STS13 s=0.6239, STS14 p=0.6646, STS14 s=0.6518, STS15 p=0.7071, STS15 s=0.7109, STS 16 p=0.6935, STS16 s=0.6969, STS B p=0.6518, STS B s=0.6488, STS B m=1.4282, SICK-R p=0.8018, SICK-R s=0.7274, SICK-P m=0.3643
2019-03-12 19:06:12,600 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 19:06:12,600 : 0.5872,0.5975,0.6300,0.6239,0.6646,0.6518,0.7071,0.7109,0.6935,0.6969,0.6518,0.6488,1.4282,0.8018,0.7274,0.3643
2019-03-12 19:06:12,600 : MR=60.10, CR=67.16, SUBJ=88.25, MPQA=85.26, SST-B=76.33, SST-F=38.51, TREC=71.00, SICK-E=78.40, SNLI=64.17, MRPC=69.10, MRPC f=76.00
2019-03-12 19:06:12,600 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 19:06:12,600 : 60.10,67.16,88.25,85.26,76.33,38.51,71.00,78.40,64.17,69.10,76.00
2019-03-12 19:06:12,600 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 19:06:12,600 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 19:06:12,600 : na,na,na,na,na,na,na,na,na,na
2019-03-12 19:06:12,600 : SentLen=74.57, WC=81.93, TreeDepth=27.08, TopConst=48.56, BShift=56.67, Tense=80.89, SubjNum=73.84, ObjNum=73.87, SOMO=52.79, CoordInv=50.07, average=62.03
2019-03-12 19:06:12,600 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 19:06:12,600 : 74.57,81.93,27.08,48.56,56.67,80.89,73.84,73.87,52.79,50.07,62.03
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,600 : layer 4
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,600 : ********************************************************************************
2019-03-12 19:06:12,691 : ***** Transfer task : STS12 *****


2019-03-12 19:06:12,704 : loading BERT model bert-large-uncased
2019-03-12 19:06:12,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:06:12,721 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:06:12,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgivkmdwr
2019-03-12 19:06:20,189 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:06:29,831 : MSRpar : pearson = 0.3713, spearman = 0.4047
2019-03-12 19:06:31,464 : MSRvid : pearson = 0.7510, spearman = 0.7542
2019-03-12 19:06:32,867 : SMTeuroparl : pearson = 0.5014, spearman = 0.5999
2019-03-12 19:06:35,542 : surprise.OnWN : pearson = 0.6401, spearman = 0.6529
2019-03-12 19:06:36,957 : surprise.SMTnews : pearson = 0.5603, spearman = 0.4609
2019-03-12 19:06:36,958 : ALL (weighted average) : Pearson = 0.5713,             Spearman = 0.5850
2019-03-12 19:06:36,958 : ALL (average) : Pearson = 0.5648,             Spearman = 0.5745

2019-03-12 19:06:36,958 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 19:06:36,977 : loading BERT model bert-large-uncased
2019-03-12 19:06:36,977 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:06:37,000 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:06:37,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpugsi1rpd
2019-03-12 19:06:44,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:06:51,218 : FNWN : pearson = 0.2177, spearman = 0.2062
2019-03-12 19:06:53,099 : headlines : pearson = 0.6779, spearman = 0.6668
2019-03-12 19:06:54,556 : OnWN : pearson = 0.6619, spearman = 0.6597
2019-03-12 19:06:54,556 : ALL (weighted average) : Pearson = 0.6139,             Spearman = 0.6061
2019-03-12 19:06:54,556 : ALL (average) : Pearson = 0.5191,             Spearman = 0.5109

2019-03-12 19:06:54,556 : ***** Transfer task : STS14 *****


2019-03-12 19:06:54,572 : loading BERT model bert-large-uncased
2019-03-12 19:06:54,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:06:54,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:06:54,590 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl5cwua22
2019-03-12 19:07:02,088 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:07:09,019 : deft-forum : pearson = 0.4252, spearman = 0.4328
2019-03-12 19:07:10,644 : deft-news : pearson = 0.7079, spearman = 0.6650
2019-03-12 19:07:12,798 : headlines : pearson = 0.6391, spearman = 0.6089
2019-03-12 19:07:14,866 : images : pearson = 0.7058, spearman = 0.6992
2019-03-12 19:07:16,980 : OnWN : pearson = 0.7253, spearman = 0.7420
2019-03-12 19:07:19,819 : tweet-news : pearson = 0.6288, spearman = 0.6062
2019-03-12 19:07:19,819 : ALL (weighted average) : Pearson = 0.6475,             Spearman = 0.6364
2019-03-12 19:07:19,819 : ALL (average) : Pearson = 0.6387,             Spearman = 0.6257

2019-03-12 19:07:19,820 : ***** Transfer task : STS15 *****


2019-03-12 19:07:19,868 : loading BERT model bert-large-uncased
2019-03-12 19:07:19,868 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:07:19,886 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:07:19,886 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnzt12pf1
2019-03-12 19:07:27,441 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:07:34,803 : answers-forums : pearson = 0.5622, spearman = 0.5510
2019-03-12 19:07:36,874 : answers-students : pearson = 0.7180, spearman = 0.7223
2019-03-12 19:07:38,905 : belief : pearson = 0.5884, spearman = 0.6161
2019-03-12 19:07:41,139 : headlines : pearson = 0.6836, spearman = 0.6820
2019-03-12 19:07:43,253 : images : pearson = 0.7930, spearman = 0.8002
2019-03-12 19:07:43,253 : ALL (weighted average) : Pearson = 0.6925,             Spearman = 0.6970
2019-03-12 19:07:43,253 : ALL (average) : Pearson = 0.6691,             Spearman = 0.6743

2019-03-12 19:07:43,253 : ***** Transfer task : STS16 *****


2019-03-12 19:07:43,322 : loading BERT model bert-large-uncased
2019-03-12 19:07:43,322 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:07:43,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:07:43,340 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppgwk13po
2019-03-12 19:07:50,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:07:57,292 : answer-answer : pearson = 0.5110, spearman = 0.5102
2019-03-12 19:07:57,948 : headlines : pearson = 0.6827, spearman = 0.6909
2019-03-12 19:07:58,823 : plagiarism : pearson = 0.7420, spearman = 0.7510
2019-03-12 19:08:00,304 : postediting : pearson = 0.8260, spearman = 0.8395
2019-03-12 19:08:00,905 : question-question : pearson = 0.6126, spearman = 0.6242
2019-03-12 19:08:00,905 : ALL (weighted average) : Pearson = 0.6746,             Spearman = 0.6827
2019-03-12 19:08:00,905 : ALL (average) : Pearson = 0.6749,             Spearman = 0.6832

2019-03-12 19:08:00,905 : ***** Transfer task : MR *****


2019-03-12 19:08:00,921 : loading BERT model bert-large-uncased
2019-03-12 19:08:00,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:08:00,974 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:08:00,974 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmizruy_w
2019-03-12 19:08:08,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:08:14,267 : Generating sentence embeddings
2019-03-12 19:08:45,562 : Generated sentence embeddings
2019-03-12 19:08:45,562 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:08:55,913 : Best param found at split 1: l2reg = 0.01                 with score 59.32
2019-03-12 19:09:05,552 : Best param found at split 2: l2reg = 1e-05                 with score 59.09
2019-03-12 19:09:18,528 : Best param found at split 3: l2reg = 1e-05                 with score 61.15
2019-03-12 19:09:28,975 : Best param found at split 4: l2reg = 0.0001                 with score 61.62
2019-03-12 19:09:38,148 : Best param found at split 5: l2reg = 0.0001                 with score 58.22
2019-03-12 19:09:38,720 : Dev acc : 59.88 Test acc : 58.2

2019-03-12 19:09:38,721 : ***** Transfer task : CR *****


2019-03-12 19:09:38,729 : loading BERT model bert-large-uncased
2019-03-12 19:09:38,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:09:38,748 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:09:38,748 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy3algcgq
2019-03-12 19:09:46,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:09:51,642 : Generating sentence embeddings
2019-03-12 19:09:59,881 : Generated sentence embeddings
2019-03-12 19:09:59,881 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:10:02,563 : Best param found at split 1: l2reg = 0.0001                 with score 70.56
2019-03-12 19:10:06,003 : Best param found at split 2: l2reg = 0.01                 with score 70.39
2019-03-12 19:10:09,276 : Best param found at split 3: l2reg = 0.001                 with score 67.15
2019-03-12 19:10:13,223 : Best param found at split 4: l2reg = 1e-05                 with score 74.38
2019-03-12 19:10:16,950 : Best param found at split 5: l2reg = 1e-05                 with score 74.15
2019-03-12 19:10:17,108 : Dev acc : 71.33 Test acc : 66.49

2019-03-12 19:10:17,109 : ***** Transfer task : MPQA *****


2019-03-12 19:10:17,146 : loading BERT model bert-large-uncased
2019-03-12 19:10:17,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:10:17,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:10:17,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdfvv228x
2019-03-12 19:10:24,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:10:30,153 : Generating sentence embeddings
2019-03-12 19:10:37,670 : Generated sentence embeddings
2019-03-12 19:10:37,671 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:10:48,738 : Best param found at split 1: l2reg = 0.0001                 with score 86.41
2019-03-12 19:11:00,509 : Best param found at split 2: l2reg = 0.001                 with score 86.2
2019-03-12 19:11:12,886 : Best param found at split 3: l2reg = 0.001                 with score 86.69
2019-03-12 19:11:23,649 : Best param found at split 4: l2reg = 1e-05                 with score 85.4
2019-03-12 19:11:36,354 : Best param found at split 5: l2reg = 0.01                 with score 86.01
2019-03-12 19:11:36,884 : Dev acc : 86.14 Test acc : 85.19

2019-03-12 19:11:36,885 : ***** Transfer task : SUBJ *****


2019-03-12 19:11:36,899 : loading BERT model bert-large-uncased
2019-03-12 19:11:36,899 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:11:36,919 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:11:36,920 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyem0zb77
2019-03-12 19:11:44,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:11:49,971 : Generating sentence embeddings
2019-03-12 19:12:20,536 : Generated sentence embeddings
2019-03-12 19:12:20,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:12:32,197 : Best param found at split 1: l2reg = 0.0001                 with score 86.3
2019-03-12 19:12:45,993 : Best param found at split 2: l2reg = 1e-05                 with score 88.29
2019-03-12 19:12:58,460 : Best param found at split 3: l2reg = 0.01                 with score 88.85
2019-03-12 19:13:10,437 : Best param found at split 4: l2reg = 0.001                 with score 89.04
2019-03-12 19:13:22,717 : Best param found at split 5: l2reg = 0.001                 with score 87.28
2019-03-12 19:13:23,372 : Dev acc : 87.95 Test acc : 85.16

2019-03-12 19:13:23,373 : ***** Transfer task : SST Binary classification *****


2019-03-12 19:13:23,464 : loading BERT model bert-large-uncased
2019-03-12 19:13:23,465 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:23,540 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:23,540 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpupx0rgby
2019-03-12 19:13:31,069 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:36,637 : Computing embedding for train
2019-03-12 19:15:15,574 : Computed train embeddings
2019-03-12 19:15:15,574 : Computing embedding for dev
2019-03-12 19:15:17,729 : Computed dev embeddings
2019-03-12 19:15:17,729 : Computing embedding for test
2019-03-12 19:15:22,256 : Computed test embeddings
2019-03-12 19:15:22,256 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:15:39,841 : [('reg:1e-05', 75.57), ('reg:0.0001', 75.8), ('reg:0.001', 75.46), ('reg:0.01', 69.95)]
2019-03-12 19:15:39,841 : Validation : best param found is reg = 0.0001 with score             75.8
2019-03-12 19:15:39,841 : Evaluating...
2019-03-12 19:15:44,174 : 
Dev acc : 75.8 Test acc : 76.88 for             SST Binary classification

2019-03-12 19:15:44,175 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 19:15:44,230 : loading BERT model bert-large-uncased
2019-03-12 19:15:44,230 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:15:44,250 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:15:44,250 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjlssmpi7
2019-03-12 19:15:51,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:15:57,217 : Computing embedding for train
2019-03-12 19:16:18,866 : Computed train embeddings
2019-03-12 19:16:18,866 : Computing embedding for dev
2019-03-12 19:16:21,695 : Computed dev embeddings
2019-03-12 19:16:21,695 : Computing embedding for test
2019-03-12 19:16:27,298 : Computed test embeddings
2019-03-12 19:16:27,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:16:29,733 : [('reg:1e-05', 27.07), ('reg:0.0001', 32.88), ('reg:0.001', 33.7), ('reg:0.01', 28.7)]
2019-03-12 19:16:29,733 : Validation : best param found is reg = 0.001 with score             33.7
2019-03-12 19:16:29,733 : Evaluating...
2019-03-12 19:16:30,300 : 
Dev acc : 33.7 Test acc : 34.12 for             SST Fine-Grained classification

2019-03-12 19:16:30,300 : ***** Transfer task : TREC *****


2019-03-12 19:16:30,315 : loading BERT model bert-large-uncased
2019-03-12 19:16:30,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:16:30,336 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:16:30,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj7v4vgv8
2019-03-12 19:16:37,804 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:16:50,788 : Computed train embeddings
2019-03-12 19:16:51,371 : Computed test embeddings
2019-03-12 19:16:51,371 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:16:59,367 : [('reg:1e-05', 61.39), ('reg:0.0001', 60.15), ('reg:0.001', 56.63), ('reg:0.01', 60.72)]
2019-03-12 19:16:59,367 : Cross-validation : best param found is reg = 1e-05             with score 61.39
2019-03-12 19:16:59,367 : Evaluating...
2019-03-12 19:16:59,831 : 
Dev acc : 61.39 Test acc : 74.0             for TREC

2019-03-12 19:16:59,832 : ***** Transfer task : MRPC *****


2019-03-12 19:16:59,853 : loading BERT model bert-large-uncased
2019-03-12 19:16:59,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:16:59,875 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:16:59,875 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmsbazmoh
2019-03-12 19:17:07,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:17:12,754 : Computing embedding for train
2019-03-12 19:17:34,880 : Computed train embeddings
2019-03-12 19:17:34,880 : Computing embedding for test
2019-03-12 19:17:44,606 : Computed test embeddings
2019-03-12 19:17:44,628 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:17:48,799 : [('reg:1e-05', 70.71), ('reg:0.0001', 70.8), ('reg:0.001', 71.81), ('reg:0.01', 71.47)]
2019-03-12 19:17:48,799 : Cross-validation : best param found is reg = 0.001             with score 71.81
2019-03-12 19:17:48,799 : Evaluating...
2019-03-12 19:17:49,069 : Dev acc : 71.81 Test acc 70.61; Test F1 81.28 for MRPC.

2019-03-12 19:17:49,069 : ***** Transfer task : SICK-Entailment*****


2019-03-12 19:17:49,094 : loading BERT model bert-large-uncased
2019-03-12 19:17:49,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:17:49,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:17:49,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmqtrwpn0
2019-03-12 19:17:56,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:02,160 : Computing embedding for train
2019-03-12 19:18:13,349 : Computed train embeddings
2019-03-12 19:18:13,349 : Computing embedding for dev
2019-03-12 19:18:14,873 : Computed dev embeddings
2019-03-12 19:18:14,873 : Computing embedding for test
2019-03-12 19:18:26,888 : Computed test embeddings
2019-03-12 19:18:26,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:18:28,095 : [('reg:1e-05', 76.4), ('reg:0.0001', 78.6), ('reg:0.001', 71.2), ('reg:0.01', 74.8)]
2019-03-12 19:18:28,095 : Validation : best param found is reg = 0.0001 with score             78.6
2019-03-12 19:18:28,095 : Evaluating...
2019-03-12 19:18:28,398 : 
Dev acc : 78.6 Test acc : 77.8 for                        SICK entailment

2019-03-12 19:18:28,398 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 19:18:28,465 : loading BERT model bert-large-uncased
2019-03-12 19:18:28,465 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:18:28,484 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:18:28,484 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8czzlbjj
2019-03-12 19:18:35,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:41,565 : Computing embedding for train
2019-03-12 19:18:52,748 : Computed train embeddings
2019-03-12 19:18:52,749 : Computing embedding for dev
2019-03-12 19:18:54,273 : Computed dev embeddings
2019-03-12 19:18:54,273 : Computing embedding for test
2019-03-12 19:19:06,281 : Computed test embeddings
2019-03-12 19:19:19,239 : Dev : Pearson 0.7948110354854676
2019-03-12 19:19:19,239 : Test : Pearson 0.8004321738005828 Spearman 0.7290589509397607 MSE 0.36657739181776217                        for SICK Relatedness

2019-03-12 19:19:19,240 : 

***** Transfer task : STSBenchmark*****


2019-03-12 19:19:19,278 : loading BERT model bert-large-uncased
2019-03-12 19:19:19,279 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:19:19,307 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:19:19,308 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1gzz6yeu
2019-03-12 19:19:26,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:19:32,405 : Computing embedding for train
2019-03-12 19:19:50,819 : Computed train embeddings
2019-03-12 19:19:50,819 : Computing embedding for dev
2019-03-12 19:19:56,402 : Computed dev embeddings
2019-03-12 19:19:56,402 : Computing embedding for test
2019-03-12 19:20:00,964 : Computed test embeddings
2019-03-12 19:20:20,270 : Dev : Pearson 0.6757871514541792
2019-03-12 19:20:20,270 : Test : Pearson 0.639463525192992 Spearman 0.6381497834160653 MSE 1.4327672966536247                        for SICK Relatedness

2019-03-12 19:20:20,270 : ***** Transfer task : SNLI Entailment*****


2019-03-12 19:20:24,943 : loading BERT model bert-large-uncased
2019-03-12 19:20:24,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:20:25,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:20:25,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkgpfx1w9
2019-03-12 19:20:32,488 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:20:39,394 : PROGRESS (encoding): 0.00%
2019-03-12 19:23:23,367 : PROGRESS (encoding): 14.56%
2019-03-12 19:26:29,255 : PROGRESS (encoding): 29.12%
2019-03-12 19:29:35,763 : PROGRESS (encoding): 43.69%
2019-03-12 19:32:54,641 : PROGRESS (encoding): 58.25%
2019-03-12 19:36:36,068 : PROGRESS (encoding): 72.81%
2019-03-12 19:40:16,614 : PROGRESS (encoding): 87.37%
2019-03-12 19:44:15,145 : PROGRESS (encoding): 0.00%
2019-03-12 19:44:45,177 : PROGRESS (encoding): 0.00%
2019-03-12 19:45:14,023 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:46:13,192 : [('reg:1e-09', 64.68)]
2019-03-12 19:46:13,192 : Validation : best param found is reg = 1e-09 with score             64.68
2019-03-12 19:46:13,192 : Evaluating...
2019-03-12 19:47:15,968 : Dev acc : 64.68 Test acc : 65.7 for SNLI

2019-03-12 19:47:15,969 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 19:47:16,179 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 19:47:17,162 : loading BERT model bert-large-uncased
2019-03-12 19:47:17,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:47:17,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:47:17,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxzgefm92
2019-03-12 19:47:24,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:47:30,180 : Computing embeddings for train/dev/test
2019-03-12 19:50:58,306 : Computed embeddings
2019-03-12 19:50:58,306 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:51:23,508 : [('reg:1e-05', 69.78), ('reg:0.0001', 61.31), ('reg:0.001', 76.2), ('reg:0.01', 48.62)]
2019-03-12 19:51:23,508 : Validation : best param found is reg = 0.001 with score             76.2
2019-03-12 19:51:23,508 : Evaluating...
2019-03-12 19:51:30,044 : 
Dev acc : 76.2 Test acc : 76.5 for LENGTH classification

2019-03-12 19:51:30,045 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 19:51:30,416 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 19:51:30,463 : loading BERT model bert-large-uncased
2019-03-12 19:51:30,464 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:51:30,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:51:30,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpas411is0
2019-03-12 19:51:37,951 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:51:43,508 : Computing embeddings for train/dev/test
2019-03-12 19:54:55,647 : Computed embeddings
2019-03-12 19:54:55,647 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:55:26,427 : [('reg:1e-05', 66.84), ('reg:0.0001', 18.2), ('reg:0.001', 0.64), ('reg:0.01', 0.1)]
2019-03-12 19:55:26,427 : Validation : best param found is reg = 1e-05 with score             66.84
2019-03-12 19:55:26,427 : Evaluating...
2019-03-12 19:55:34,646 : 
Dev acc : 66.8 Test acc : 67.0 for WORDCONTENT classification

2019-03-12 19:55:34,647 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 19:55:35,002 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 19:55:35,067 : loading BERT model bert-large-uncased
2019-03-12 19:55:35,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:55:35,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:55:35,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqiy0gv4g
2019-03-12 19:55:42,523 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:55:48,066 : Computing embeddings for train/dev/test
2019-03-12 19:58:48,602 : Computed embeddings
2019-03-12 19:58:48,602 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:59:10,865 : [('reg:1e-05', 27.4), ('reg:0.0001', 29.54), ('reg:0.001', 25.58), ('reg:0.01', 24.62)]
2019-03-12 19:59:10,865 : Validation : best param found is reg = 0.0001 with score             29.54
2019-03-12 19:59:10,865 : Evaluating...
2019-03-12 19:59:15,643 : 
Dev acc : 29.5 Test acc : 28.9 for DEPTH classification

2019-03-12 19:59:15,644 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 19:59:16,043 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 19:59:16,109 : loading BERT model bert-large-uncased
2019-03-12 19:59:16,110 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:59:16,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:59:16,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps8oibldq
2019-03-12 19:59:23,674 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:59:29,167 : Computing embeddings for train/dev/test
2019-03-12 20:02:16,356 : Computed embeddings
2019-03-12 20:02:16,356 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:02:43,718 : [('reg:1e-05', 54.82), ('reg:0.0001', 44.39), ('reg:0.001', 32.85), ('reg:0.01', 26.25)]
2019-03-12 20:02:43,718 : Validation : best param found is reg = 1e-05 with score             54.82
2019-03-12 20:02:43,718 : Evaluating...
2019-03-12 20:02:53,737 : 
Dev acc : 54.8 Test acc : 55.0 for TOPCONSTITUENTS classification

2019-03-12 20:02:53,738 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 20:02:54,082 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 20:02:54,148 : loading BERT model bert-large-uncased
2019-03-12 20:02:54,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:02:54,266 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:02:54,266 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb9oc_2jz
2019-03-12 20:03:01,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:03:07,342 : Computing embeddings for train/dev/test
2019-03-12 20:06:08,809 : Computed embeddings
2019-03-12 20:06:08,809 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:06:42,426 : [('reg:1e-05', 63.67), ('reg:0.0001', 63.63), ('reg:0.001', 63.01), ('reg:0.01', 58.28)]
2019-03-12 20:06:42,426 : Validation : best param found is reg = 1e-05 with score             63.67
2019-03-12 20:06:42,427 : Evaluating...
2019-03-12 20:06:51,261 : 
Dev acc : 63.7 Test acc : 63.2 for BIGRAMSHIFT classification

2019-03-12 20:06:51,262 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 20:06:51,838 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 20:06:51,905 : loading BERT model bert-large-uncased
2019-03-12 20:06:51,905 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:06:51,935 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:06:51,935 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp79zzqum3
2019-03-12 20:06:59,359 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:07:04,931 : Computing embeddings for train/dev/test
2019-03-12 20:10:02,704 : Computed embeddings
2019-03-12 20:10:02,704 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:10:25,538 : [('reg:1e-05', 82.28), ('reg:0.0001', 82.3), ('reg:0.001', 82.36), ('reg:0.01', 81.76)]
2019-03-12 20:10:25,538 : Validation : best param found is reg = 0.001 with score             82.36
2019-03-12 20:10:25,538 : Evaluating...
2019-03-12 20:10:31,628 : 
Dev acc : 82.4 Test acc : 81.7 for TENSE classification

2019-03-12 20:10:31,629 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 20:10:32,010 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 20:10:32,076 : loading BERT model bert-large-uncased
2019-03-12 20:10:32,076 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:10:32,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:10:32,103 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqzswdpys
2019-03-12 20:10:39,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:10:45,096 : Computing embeddings for train/dev/test
2019-03-12 20:13:53,423 : Computed embeddings
2019-03-12 20:13:53,424 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:14:22,291 : [('reg:1e-05', 73.02), ('reg:0.0001', 73.23), ('reg:0.001', 73.09), ('reg:0.01', 65.91)]
2019-03-12 20:14:22,291 : Validation : best param found is reg = 0.0001 with score             73.23
2019-03-12 20:14:22,291 : Evaluating...
2019-03-12 20:14:29,855 : 
Dev acc : 73.2 Test acc : 71.7 for SUBJNUMBER classification

2019-03-12 20:14:29,856 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 20:14:30,270 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 20:14:30,336 : loading BERT model bert-large-uncased
2019-03-12 20:14:30,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:14:30,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:14:30,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp10m54avu
2019-03-12 20:14:37,805 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:14:43,367 : Computing embeddings for train/dev/test
2019-03-12 20:17:48,102 : Computed embeddings
2019-03-12 20:17:48,102 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:18:23,931 : [('reg:1e-05', 72.5), ('reg:0.0001', 72.5), ('reg:0.001', 73.59), ('reg:0.01', 70.94)]
2019-03-12 20:18:23,931 : Validation : best param found is reg = 0.001 with score             73.59
2019-03-12 20:18:23,931 : Evaluating...
2019-03-12 20:18:30,065 : 
Dev acc : 73.6 Test acc : 73.7 for OBJNUMBER classification

2019-03-12 20:18:30,066 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 20:18:30,461 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 20:18:30,531 : loading BERT model bert-large-uncased
2019-03-12 20:18:30,531 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:18:30,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:18:30,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbnu2hv1s
2019-03-12 20:18:38,070 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:18:43,650 : Computing embeddings for train/dev/test
2019-03-12 20:22:17,637 : Computed embeddings
2019-03-12 20:22:17,638 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:22:38,952 : [('reg:1e-05', 53.15), ('reg:0.0001', 53.16), ('reg:0.001', 52.83), ('reg:0.01', 53.56)]
2019-03-12 20:22:38,952 : Validation : best param found is reg = 0.01 with score             53.56
2019-03-12 20:22:38,952 : Evaluating...
2019-03-12 20:22:45,492 : 
Dev acc : 53.6 Test acc : 52.8 for ODDMANOUT classification

2019-03-12 20:22:45,493 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 20:22:45,888 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 20:22:45,965 : loading BERT model bert-large-uncased
2019-03-12 20:22:45,965 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:22:46,088 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:22:46,089 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp01an1a5d
2019-03-12 20:22:53,570 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:22:59,003 : Computing embeddings for train/dev/test
2019-03-12 20:26:31,160 : Computed embeddings
2019-03-12 20:26:31,160 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:27:00,848 : [('reg:1e-05', 50.24), ('reg:0.0001', 50.23), ('reg:0.001', 50.17), ('reg:0.01', 50.01)]
2019-03-12 20:27:00,848 : Validation : best param found is reg = 1e-05 with score             50.24
2019-03-12 20:27:00,848 : Evaluating...
2019-03-12 20:27:08,466 : 
Dev acc : 50.2 Test acc : 50.1 for COORDINATIONINVERSION classification

2019-03-12 20:27:08,468 : total results: {'STS12': {'MSRpar': {'pearson': (0.371274556875998, 6.317702983946702e-26), 'spearman': SpearmanrResult(correlation=0.40466054739973967, pvalue=6.434676962110556e-31), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7510159978271688, 5.589474292579384e-137), 'spearman': SpearmanrResult(correlation=0.7541707205706395, pvalue=9.263621040640141e-139), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5014036997251549, 1.3618096866238215e-30), 'spearman': SpearmanrResult(correlation=0.5998639967866622, pvalue=3.384137842952933e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6400694065656339, 1.1017343105736593e-87), 'spearman': SpearmanrResult(correlation=0.6528903968401137, pvalue=2.5528440367945484e-92), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.560250002323852, 2.382040893872217e-34), 'spearman': SpearmanrResult(correlation=0.4609078316469009, pvalue=2.225616987983218e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5648027326635615, 'wmean': 0.5712529022048467}, 'spearman': {'mean': 0.5744986986488112, 'wmean': 0.5849520746332243}}}, 'STS13': {'FNWN': {'pearson': (0.2177038046994188, 0.002618890183249691), 'spearman': SpearmanrResult(correlation=0.2061991166772921, pvalue=0.004418617724719321), 'nsamples': 189}, 'headlines': {'pearson': (0.6778824409821183, 4.9114370763515346e-102), 'spearman': SpearmanrResult(correlation=0.6667716491474096, pvalue=1.340592251197477e-97), 'nsamples': 750}, 'OnWN': {'pearson': (0.6618600985685168, 5.557606973236321e-72), 'spearman': SpearmanrResult(correlation=0.6596711388128227, pvalue=2.3419799999759828e-71), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5191487814166846, 'wmean': 0.6139075767478112}, 'spearman': {'mean': 0.5108806348791748, 'wmean': 0.6060839191910393}}}, 'STS14': {'deft-forum': {'pearson': (0.42520993733616835, 3.4843345161496336e-21), 'spearman': SpearmanrResult(correlation=0.4327837766117168, pvalue=5.752534151101817e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7078690686926842, 6.604960796025629e-47), 'spearman': SpearmanrResult(correlation=0.6649593104603931, pvalue=1.1709555931916264e-39), 'nsamples': 300}, 'headlines': {'pearson': (0.6390509687208903, 2.516591269193067e-87), 'spearman': SpearmanrResult(correlation=0.6089172206753141, pvalue=2.7002479931894315e-77), 'nsamples': 750}, 'images': {'pearson': (0.705815508112024, 4.187523026879987e-114), 'spearman': SpearmanrResult(correlation=0.6991923413094565, pvalue=4.0775511898640296e-111), 'nsamples': 750}, 'OnWN': {'pearson': (0.7253425203452973, 2.0097761994020504e-123), 'spearman': SpearmanrResult(correlation=0.7420157677942323, pvalue=4.822598722584436e-132), 'nsamples': 750}, 'tweet-news': {'pearson': (0.628815459881365, 8.553405269382316e-84), 'spearman': SpearmanrResult(correlation=0.6061671354683199, pvalue=1.9663753190674413e-76), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6386839105147382, 'wmean': 0.6474596093876702}, 'spearman': {'mean': 0.6256725920532388, 'wmean': 0.636389291079702}}}, 'STS15': {'answers-forums': {'pearson': (0.5622378306682217, 1.225115251665887e-32), 'spearman': SpearmanrResult(correlation=0.5510433603111076, pvalue=3.6264114712450794e-31), 'nsamples': 375}, 'answers-students': {'pearson': (0.718027761842935, 7.702332866765367e-120), 'spearman': SpearmanrResult(correlation=0.7222950143149743, pvalue=6.458281284068224e-122), 'nsamples': 750}, 'belief': {'pearson': (0.5884263920751798, 2.622814905618958e-36), 'spearman': SpearmanrResult(correlation=0.6160566166152289, pvalue=1.4698796188704419e-40), 'nsamples': 375}, 'headlines': {'pearson': (0.6836271391369741, 2.0885720789276067e-104), 'spearman': SpearmanrResult(correlation=0.6820103144521918, pvalue=9.835152762202923e-104), 'nsamples': 750}, 'images': {'pearson': (0.7929544955493265, 4.0622983357455325e-163), 'spearman': SpearmanrResult(correlation=0.8001947778257941, pvalue=3.0044988990090854e-168), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6690547238545274, 'wmean': 0.6924853769752342}, 'spearman': {'mean': 0.6743200167038594, 'wmean': 0.6970125237640321}}}, 'STS16': {'answer-answer': {'pearson': (0.5110127359839579, 2.672642656706762e-18), 'spearman': SpearmanrResult(correlation=0.5101776716938164, pvalue=3.0956409063395134e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.6826526831494134, 1.6551833756288897e-35), 'spearman': SpearmanrResult(correlation=0.6909157191340528, pvalue=1.1518593069723363e-36), 'nsamples': 249}, 'plagiarism': {'pearson': (0.741971547473525, 1.8170526042578225e-41), 'spearman': SpearmanrResult(correlation=0.7510323637879582, pvalue=5.510819611442793e-43), 'nsamples': 230}, 'postediting': {'pearson': (0.8260135666034367, 3.4365073016356067e-62), 'spearman': SpearmanrResult(correlation=0.8395237486582767, pvalue=4.6651428543375403e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.6126467127438183, 6.34217737660599e-23), 'spearman': SpearmanrResult(correlation=0.6241711394477756, pvalue=5.71212263825796e-24), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6748594491908303, 'wmean': 0.6745545380925266}, 'spearman': {'mean': 0.683164128544376, 'wmean': 0.6826780347074566}}}, 'MR': {'devacc': 59.88, 'acc': 58.2, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.33, 'acc': 66.49, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.14, 'acc': 85.19, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 87.95, 'acc': 85.16, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.8, 'acc': 76.88, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.7, 'acc': 34.12, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 61.39, 'acc': 74.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.81, 'acc': 70.61, 'f1': 81.28, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.6, 'acc': 77.8, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7948110354854676, 'pearson': 0.8004321738005828, 'spearman': 0.7290589509397607, 'mse': 0.36657739181776217, 'yhat': array([2.33871916, 4.35141535, 1.78777291, ..., 3.06786476, 4.23169439,        4.72436258]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6757871514541792, 'pearson': 0.639463525192992, 'spearman': 0.6381497834160653, 'mse': 1.4327672966536247, 'yhat': array([1.45696962, 1.39792513, 2.40103499, ..., 3.35678629, 3.28229211,        3.48766197]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.68, 'acc': 65.7, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 76.2, 'acc': 76.55, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 66.84, 'acc': 67.05, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.54, 'acc': 28.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.82, 'acc': 55.02, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 63.67, 'acc': 63.21, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.36, 'acc': 81.73, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.23, 'acc': 71.66, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.59, 'acc': 73.65, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 53.56, 'acc': 52.82, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.24, 'acc': 50.13, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 20:27:08,468 : STS12 p=0.5713, STS12 s=0.5850, STS13 p=0.6139, STS13 s=0.6061, STS14 p=0.6475, STS14 s=0.6364, STS15 p=0.6925, STS15 s=0.6970, STS 16 p=0.6746, STS16 s=0.6827, STS B p=0.6395, STS B s=0.6381, STS B m=1.4328, SICK-R p=0.8004, SICK-R s=0.7291, SICK-P m=0.3666
2019-03-12 20:27:08,468 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 20:27:08,468 : 0.5713,0.5850,0.6139,0.6061,0.6475,0.6364,0.6925,0.6970,0.6746,0.6827,0.6395,0.6381,1.4328,0.8004,0.7291,0.3666
2019-03-12 20:27:08,468 : MR=58.20, CR=66.49, SUBJ=85.16, MPQA=85.19, SST-B=76.88, SST-F=34.12, TREC=74.00, SICK-E=77.80, SNLI=65.70, MRPC=70.61, MRPC f=81.28
2019-03-12 20:27:08,468 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 20:27:08,468 : 58.20,66.49,85.16,85.19,76.88,34.12,74.00,77.80,65.70,70.61,81.28
2019-03-12 20:27:08,468 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 20:27:08,468 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 20:27:08,468 : na,na,na,na,na,na,na,na,na,na
2019-03-12 20:27:08,468 : SentLen=76.55, WC=67.05, TreeDepth=28.88, TopConst=55.02, BShift=63.21, Tense=81.73, SubjNum=71.66, ObjNum=73.65, SOMO=52.82, CoordInv=50.13, average=62.07
2019-03-12 20:27:08,468 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 20:27:08,468 : 76.55,67.05,28.88,55.02,63.21,81.73,71.66,73.65,52.82,50.13,62.07
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,468 : layer 5
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,468 : ********************************************************************************
2019-03-12 20:27:08,560 : ***** Transfer task : STS12 *****


2019-03-12 20:27:08,596 : loading BERT model bert-large-uncased
2019-03-12 20:27:08,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:27:08,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:27:08,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprhx97jp0
2019-03-12 20:27:16,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:27:25,496 : MSRpar : pearson = 0.3684, spearman = 0.4024
2019-03-12 20:27:27,124 : MSRvid : pearson = 0.7478, spearman = 0.7502
2019-03-12 20:27:28,524 : SMTeuroparl : pearson = 0.4959, spearman = 0.5953
2019-03-12 20:27:31,197 : surprise.OnWN : pearson = 0.6231, spearman = 0.6430
2019-03-12 20:27:32,613 : surprise.SMTnews : pearson = 0.5418, spearman = 0.4547
2019-03-12 20:27:32,614 : ALL (weighted average) : Pearson = 0.5625,             Spearman = 0.5796
2019-03-12 20:27:32,614 : ALL (average) : Pearson = 0.5554,             Spearman = 0.5691

2019-03-12 20:27:32,614 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 20:27:32,623 : loading BERT model bert-large-uncased
2019-03-12 20:27:32,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:27:32,640 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:27:32,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx4gu05bp
2019-03-12 20:27:40,049 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:27:46,912 : FNWN : pearson = 0.2187, spearman = 0.2107
2019-03-12 20:27:48,790 : headlines : pearson = 0.6709, spearman = 0.6613
2019-03-12 20:27:50,244 : OnWN : pearson = 0.6592, spearman = 0.6562
2019-03-12 20:27:50,245 : ALL (weighted average) : Pearson = 0.6095,             Spearman = 0.6026
2019-03-12 20:27:50,245 : ALL (average) : Pearson = 0.5162,             Spearman = 0.5094

2019-03-12 20:27:50,245 : ***** Transfer task : STS14 *****


2019-03-12 20:27:50,293 : loading BERT model bert-large-uncased
2019-03-12 20:27:50,294 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:27:50,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:27:50,311 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv79282zm
2019-03-12 20:27:57,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:28:04,722 : deft-forum : pearson = 0.4262, spearman = 0.4313
2019-03-12 20:28:06,344 : deft-news : pearson = 0.7084, spearman = 0.6663
2019-03-12 20:28:08,495 : headlines : pearson = 0.6408, spearman = 0.6121
2019-03-12 20:28:10,554 : images : pearson = 0.6994, spearman = 0.6912
2019-03-12 20:28:12,667 : OnWN : pearson = 0.7209, spearman = 0.7404
2019-03-12 20:28:15,505 : tweet-news : pearson = 0.6278, spearman = 0.6044
2019-03-12 20:28:15,505 : ALL (weighted average) : Pearson = 0.6456,             Spearman = 0.6347
2019-03-12 20:28:15,505 : ALL (average) : Pearson = 0.6373,             Spearman = 0.6243

2019-03-12 20:28:15,505 : ***** Transfer task : STS15 *****


2019-03-12 20:28:15,537 : loading BERT model bert-large-uncased
2019-03-12 20:28:15,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:28:15,585 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:28:15,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3mqk4pcz
2019-03-12 20:28:23,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:28:30,477 : answers-forums : pearson = 0.5526, spearman = 0.5413
2019-03-12 20:28:32,544 : answers-students : pearson = 0.7203, spearman = 0.7253
2019-03-12 20:28:34,571 : belief : pearson = 0.5685, spearman = 0.5957
2019-03-12 20:28:36,801 : headlines : pearson = 0.6771, spearman = 0.6770
2019-03-12 20:28:38,918 : images : pearson = 0.7831, spearman = 0.7910
2019-03-12 20:28:38,918 : ALL (weighted average) : Pearson = 0.6853,             Spearman = 0.6904
2019-03-12 20:28:38,918 : ALL (average) : Pearson = 0.6603,             Spearman = 0.6661

2019-03-12 20:28:38,918 : ***** Transfer task : STS16 *****


2019-03-12 20:28:38,955 : loading BERT model bert-large-uncased
2019-03-12 20:28:38,955 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:28:38,973 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:28:38,973 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoizn9u6v
2019-03-12 20:28:46,445 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:28:52,875 : answer-answer : pearson = 0.4882, spearman = 0.4920
2019-03-12 20:28:53,529 : headlines : pearson = 0.6793, spearman = 0.6883
2019-03-12 20:28:54,403 : plagiarism : pearson = 0.7293, spearman = 0.7407
2019-03-12 20:28:55,882 : postediting : pearson = 0.8181, spearman = 0.8415
2019-03-12 20:28:56,483 : question-question : pearson = 0.5727, spearman = 0.5844
2019-03-12 20:28:56,483 : ALL (weighted average) : Pearson = 0.6578,             Spearman = 0.6696
2019-03-12 20:28:56,483 : ALL (average) : Pearson = 0.6575,             Spearman = 0.6694

2019-03-12 20:28:56,483 : ***** Transfer task : MR *****


2019-03-12 20:28:56,528 : loading BERT model bert-large-uncased
2019-03-12 20:28:56,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:28:56,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:28:56,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpibnqmbbf
2019-03-12 20:29:04,025 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:29:09,574 : Generating sentence embeddings
2019-03-12 20:29:40,754 : Generated sentence embeddings
2019-03-12 20:29:40,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:29:51,978 : Best param found at split 1: l2reg = 1e-05                 with score 62.78
2019-03-12 20:30:05,056 : Best param found at split 2: l2reg = 1e-05                 with score 60.94
2019-03-12 20:30:17,477 : Best param found at split 3: l2reg = 1e-05                 with score 60.05
2019-03-12 20:30:28,954 : Best param found at split 4: l2reg = 1e-05                 with score 60.59
2019-03-12 20:30:40,823 : Best param found at split 5: l2reg = 1e-05                 with score 58.87
2019-03-12 20:30:41,564 : Dev acc : 60.65 Test acc : 57.36

2019-03-12 20:30:41,565 : ***** Transfer task : CR *****


2019-03-12 20:30:41,573 : loading BERT model bert-large-uncased
2019-03-12 20:30:41,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:30:41,622 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:30:41,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjnr3ypma
2019-03-12 20:30:49,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:30:54,632 : Generating sentence embeddings
2019-03-12 20:31:02,864 : Generated sentence embeddings
2019-03-12 20:31:02,865 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:31:05,879 : Best param found at split 1: l2reg = 0.001                 with score 69.86
2019-03-12 20:31:09,301 : Best param found at split 2: l2reg = 0.001                 with score 69.76
2019-03-12 20:31:12,682 : Best param found at split 3: l2reg = 1e-05                 with score 67.65
2019-03-12 20:31:16,244 : Best param found at split 4: l2reg = 1e-05                 with score 73.45
2019-03-12 20:31:19,067 : Best param found at split 5: l2reg = 0.001                 with score 72.56
2019-03-12 20:31:19,187 : Dev acc : 70.66 Test acc : 64.45

2019-03-12 20:31:19,187 : ***** Transfer task : MPQA *****


2019-03-12 20:31:19,192 : loading BERT model bert-large-uncased
2019-03-12 20:31:19,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:31:19,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:31:19,211 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7hsaut53
2019-03-12 20:31:26,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:31:32,194 : Generating sentence embeddings
2019-03-12 20:31:39,693 : Generated sentence embeddings
2019-03-12 20:31:39,693 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:31:50,520 : Best param found at split 1: l2reg = 0.001                 with score 86.24
2019-03-12 20:31:59,403 : Best param found at split 2: l2reg = 0.0001                 with score 86.07
2019-03-12 20:32:09,358 : Best param found at split 3: l2reg = 0.0001                 with score 86.48
2019-03-12 20:32:20,987 : Best param found at split 4: l2reg = 0.001                 with score 86.41
2019-03-12 20:32:33,563 : Best param found at split 5: l2reg = 0.0001                 with score 85.27
2019-03-12 20:32:34,093 : Dev acc : 86.09 Test acc : 86.06

2019-03-12 20:32:34,094 : ***** Transfer task : SUBJ *****


2019-03-12 20:32:34,109 : loading BERT model bert-large-uncased
2019-03-12 20:32:34,109 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:32:34,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:32:34,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphe0gowpo
2019-03-12 20:32:41,658 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:32:47,305 : Generating sentence embeddings
2019-03-12 20:33:17,893 : Generated sentence embeddings
2019-03-12 20:33:17,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:33:29,385 : Best param found at split 1: l2reg = 1e-05                 with score 89.38
2019-03-12 20:33:41,280 : Best param found at split 2: l2reg = 0.001                 with score 88.99
2019-03-12 20:33:52,834 : Best param found at split 3: l2reg = 0.01                 with score 88.89
2019-03-12 20:34:03,182 : Best param found at split 4: l2reg = 1e-05                 with score 89.09
2019-03-12 20:34:15,821 : Best param found at split 5: l2reg = 0.0001                 with score 89.61
2019-03-12 20:34:16,522 : Dev acc : 89.19 Test acc : 88.49

2019-03-12 20:34:16,523 : ***** Transfer task : SST Binary classification *****


2019-03-12 20:34:16,661 : loading BERT model bert-large-uncased
2019-03-12 20:34:16,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:34:16,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:34:16,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj2nl5osj
2019-03-12 20:34:24,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:29,613 : Computing embedding for train
2019-03-12 20:36:08,559 : Computed train embeddings
2019-03-12 20:36:08,559 : Computing embedding for dev
2019-03-12 20:36:10,712 : Computed dev embeddings
2019-03-12 20:36:10,712 : Computing embedding for test
2019-03-12 20:36:15,244 : Computed test embeddings
2019-03-12 20:36:15,244 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:36:32,205 : [('reg:1e-05', 76.49), ('reg:0.0001', 76.49), ('reg:0.001', 76.38), ('reg:0.01', 70.3)]
2019-03-12 20:36:32,205 : Validation : best param found is reg = 1e-05 with score             76.49
2019-03-12 20:36:32,205 : Evaluating...
2019-03-12 20:36:36,544 : 
Dev acc : 76.49 Test acc : 76.0 for             SST Binary classification

2019-03-12 20:36:36,545 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 20:36:36,594 : loading BERT model bert-large-uncased
2019-03-12 20:36:36,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:36:36,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:36:36,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbekuwvjz
2019-03-12 20:36:44,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:36:49,621 : Computing embedding for train
2019-03-12 20:37:11,310 : Computed train embeddings
2019-03-12 20:37:11,311 : Computing embedding for dev
2019-03-12 20:37:14,140 : Computed dev embeddings
2019-03-12 20:37:14,140 : Computing embedding for test
2019-03-12 20:37:19,715 : Computed test embeddings
2019-03-12 20:37:19,715 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:37:22,332 : [('reg:1e-05', 36.69), ('reg:0.0001', 32.33), ('reg:0.001', 35.06), ('reg:0.01', 27.61)]
2019-03-12 20:37:22,332 : Validation : best param found is reg = 1e-05 with score             36.69
2019-03-12 20:37:22,332 : Evaluating...
2019-03-12 20:37:23,161 : 
Dev acc : 36.69 Test acc : 37.56 for             SST Fine-Grained classification

2019-03-12 20:37:23,162 : ***** Transfer task : TREC *****


2019-03-12 20:37:23,174 : loading BERT model bert-large-uncased
2019-03-12 20:37:23,174 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:37:23,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:37:23,193 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1r68u55n
2019-03-12 20:37:30,696 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:37:43,712 : Computed train embeddings
2019-03-12 20:37:44,295 : Computed test embeddings
2019-03-12 20:37:44,296 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:37:52,233 : [('reg:1e-05', 58.3), ('reg:0.0001', 60.32), ('reg:0.001', 63.98), ('reg:0.01', 56.48)]
2019-03-12 20:37:52,233 : Cross-validation : best param found is reg = 0.001             with score 63.98
2019-03-12 20:37:52,233 : Evaluating...
2019-03-12 20:37:52,567 : 
Dev acc : 63.98 Test acc : 80.4             for TREC

2019-03-12 20:37:52,567 : ***** Transfer task : MRPC *****


2019-03-12 20:37:52,588 : loading BERT model bert-large-uncased
2019-03-12 20:37:52,588 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:37:52,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:37:52,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgbg3g4kc
2019-03-12 20:38:00,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:38:05,518 : Computing embedding for train
2019-03-12 20:38:27,511 : Computed train embeddings
2019-03-12 20:38:27,511 : Computing embedding for test
2019-03-12 20:38:37,149 : Computed test embeddings
2019-03-12 20:38:37,171 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:38:42,256 : [('reg:1e-05', 71.22), ('reg:0.0001', 71.76), ('reg:0.001', 71.15), ('reg:0.01', 71.74)]
2019-03-12 20:38:42,256 : Cross-validation : best param found is reg = 0.0001             with score 71.76
2019-03-12 20:38:42,256 : Evaluating...
2019-03-12 20:38:42,507 : Dev acc : 71.76 Test acc 70.32; Test F1 81.31 for MRPC.

2019-03-12 20:38:42,507 : ***** Transfer task : SICK-Entailment*****


2019-03-12 20:38:42,537 : loading BERT model bert-large-uncased
2019-03-12 20:38:42,537 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:38:42,556 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:38:42,557 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_49hf8sk
2019-03-12 20:38:50,144 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:38:55,607 : Computing embedding for train
2019-03-12 20:39:06,793 : Computed train embeddings
2019-03-12 20:39:06,793 : Computing embedding for dev
2019-03-12 20:39:08,319 : Computed dev embeddings
2019-03-12 20:39:08,319 : Computing embedding for test
2019-03-12 20:39:20,323 : Computed test embeddings
2019-03-12 20:39:20,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:39:21,822 : [('reg:1e-05', 67.0), ('reg:0.0001', 80.6), ('reg:0.001', 77.2), ('reg:0.01', 74.8)]
2019-03-12 20:39:21,823 : Validation : best param found is reg = 0.0001 with score             80.6
2019-03-12 20:39:21,823 : Evaluating...
2019-03-12 20:39:22,287 : 
Dev acc : 80.6 Test acc : 78.26 for                        SICK entailment

2019-03-12 20:39:22,288 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 20:39:22,314 : loading BERT model bert-large-uncased
2019-03-12 20:39:22,314 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:39:22,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:39:22,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpell7hmnf
2019-03-12 20:39:29,822 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:39:35,285 : Computing embedding for train
2019-03-12 20:39:46,473 : Computed train embeddings
2019-03-12 20:39:46,473 : Computing embedding for dev
2019-03-12 20:39:48,001 : Computed dev embeddings
2019-03-12 20:39:48,001 : Computing embedding for test
2019-03-12 20:39:59,997 : Computed test embeddings
2019-03-12 20:40:15,308 : Dev : Pearson 0.7771852305189285
2019-03-12 20:40:15,309 : Test : Pearson 0.7956149361331633 Spearman 0.7278095562523037 MSE 0.3754558547476309                        for SICK Relatedness

2019-03-12 20:40:15,309 : 

***** Transfer task : STSBenchmark*****


2019-03-12 20:40:15,350 : loading BERT model bert-large-uncased
2019-03-12 20:40:15,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:40:15,412 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:40:15,413 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt6hdoabm
2019-03-12 20:40:22,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:40:28,510 : Computing embedding for train
2019-03-12 20:40:46,922 : Computed train embeddings
2019-03-12 20:40:46,923 : Computing embedding for dev
2019-03-12 20:40:52,511 : Computed dev embeddings
2019-03-12 20:40:52,511 : Computing embedding for test
2019-03-12 20:40:57,071 : Computed test embeddings
2019-03-12 20:41:16,068 : Dev : Pearson 0.6774393370700892
2019-03-12 20:41:16,068 : Test : Pearson 0.6550486649344721 Spearman 0.655639414291777 MSE 1.3737904463578485                        for SICK Relatedness

2019-03-12 20:41:16,069 : ***** Transfer task : SNLI Entailment*****


2019-03-12 20:41:20,977 : loading BERT model bert-large-uncased
2019-03-12 20:41:20,978 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:41:21,096 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:41:21,097 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6jirdnmf
2019-03-12 20:41:28,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:41:34,418 : PROGRESS (encoding): 0.00%
2019-03-12 20:44:18,205 : PROGRESS (encoding): 14.56%
2019-03-12 20:47:23,918 : PROGRESS (encoding): 29.12%
2019-03-12 20:50:30,403 : PROGRESS (encoding): 43.69%
2019-03-12 20:53:49,342 : PROGRESS (encoding): 58.25%
2019-03-12 20:57:30,838 : PROGRESS (encoding): 72.81%
2019-03-12 21:01:11,247 : PROGRESS (encoding): 87.37%
2019-03-12 21:05:09,740 : PROGRESS (encoding): 0.00%
2019-03-12 21:05:39,710 : PROGRESS (encoding): 0.00%
2019-03-12 21:06:08,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:06:47,081 : [('reg:1e-09', 57.74)]
2019-03-12 21:06:47,081 : Validation : best param found is reg = 1e-09 with score             57.74
2019-03-12 21:06:47,081 : Evaluating...
2019-03-12 21:07:26,435 : Dev acc : 57.74 Test acc : 58.5 for SNLI

2019-03-12 21:07:26,435 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 21:07:26,646 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 21:07:27,700 : loading BERT model bert-large-uncased
2019-03-12 21:07:27,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:07:27,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:07:27,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeovxe6pd
2019-03-12 21:07:35,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:07:40,822 : Computing embeddings for train/dev/test
2019-03-12 21:11:08,981 : Computed embeddings
2019-03-12 21:11:08,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:11:34,461 : [('reg:1e-05', 68.42), ('reg:0.0001', 65.34), ('reg:0.001', 64.65), ('reg:0.01', 50.26)]
2019-03-12 21:11:34,461 : Validation : best param found is reg = 1e-05 with score             68.42
2019-03-12 21:11:34,461 : Evaluating...
2019-03-12 21:11:42,062 : 
Dev acc : 68.4 Test acc : 68.9 for LENGTH classification

2019-03-12 21:11:42,063 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 21:11:42,424 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 21:11:42,470 : loading BERT model bert-large-uncased
2019-03-12 21:11:42,470 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:11:42,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:11:42,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_0ktrem
2019-03-12 21:11:49,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:11:55,593 : Computing embeddings for train/dev/test
2019-03-12 21:15:07,713 : Computed embeddings
2019-03-12 21:15:07,713 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:15:47,224 : [('reg:1e-05', 45.1), ('reg:0.0001', 24.4), ('reg:0.001', 0.7), ('reg:0.01', 0.22)]
2019-03-12 21:15:47,224 : Validation : best param found is reg = 1e-05 with score             45.1
2019-03-12 21:15:47,224 : Evaluating...
2019-03-12 21:16:01,447 : 
Dev acc : 45.1 Test acc : 45.1 for WORDCONTENT classification

2019-03-12 21:16:01,449 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 21:16:01,834 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 21:16:01,905 : loading BERT model bert-large-uncased
2019-03-12 21:16:01,905 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:16:02,008 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:16:02,008 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp10uedf8s
2019-03-12 21:16:09,532 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:16:15,165 : Computing embeddings for train/dev/test
2019-03-12 21:19:15,242 : Computed embeddings
2019-03-12 21:19:15,242 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:19:43,013 : [('reg:1e-05', 24.77), ('reg:0.0001', 29.08), ('reg:0.001', 26.99), ('reg:0.01', 25.07)]
2019-03-12 21:19:43,013 : Validation : best param found is reg = 0.0001 with score             29.08
2019-03-12 21:19:43,013 : Evaluating...
2019-03-12 21:19:50,265 : 
Dev acc : 29.1 Test acc : 28.9 for DEPTH classification

2019-03-12 21:19:50,266 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 21:19:50,656 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 21:19:50,721 : loading BERT model bert-large-uncased
2019-03-12 21:19:50,721 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:19:50,833 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:19:50,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbhondww7
2019-03-12 21:19:58,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:20:03,899 : Computing embeddings for train/dev/test
2019-03-12 21:22:51,152 : Computed embeddings
2019-03-12 21:22:51,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:23:19,511 : [('reg:1e-05', 50.73), ('reg:0.0001', 46.69), ('reg:0.001', 44.16), ('reg:0.01', 33.46)]
2019-03-12 21:23:19,512 : Validation : best param found is reg = 1e-05 with score             50.73
2019-03-12 21:23:19,512 : Evaluating...
2019-03-12 21:23:26,875 : 
Dev acc : 50.7 Test acc : 50.9 for TOPCONSTITUENTS classification

2019-03-12 21:23:26,876 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 21:23:27,255 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 21:23:27,328 : loading BERT model bert-large-uncased
2019-03-12 21:23:27,328 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:23:27,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:23:27,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphjcg5bv7
2019-03-12 21:23:35,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:23:40,564 : Computing embeddings for train/dev/test
2019-03-12 21:26:42,128 : Computed embeddings
2019-03-12 21:26:42,128 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:27:17,631 : [('reg:1e-05', 67.76), ('reg:0.0001', 67.8), ('reg:0.001', 67.01), ('reg:0.01', 63.51)]
2019-03-12 21:27:17,631 : Validation : best param found is reg = 0.0001 with score             67.8
2019-03-12 21:27:17,631 : Evaluating...
2019-03-12 21:27:26,611 : 
Dev acc : 67.8 Test acc : 67.2 for BIGRAMSHIFT classification

2019-03-12 21:27:26,612 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 21:27:27,024 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 21:27:27,090 : loading BERT model bert-large-uncased
2019-03-12 21:27:27,090 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:27:27,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:27:27,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8_dfjdsn
2019-03-12 21:27:34,580 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:27:40,041 : Computing embeddings for train/dev/test
2019-03-12 21:30:37,366 : Computed embeddings
2019-03-12 21:30:37,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:31:04,204 : [('reg:1e-05', 83.42), ('reg:0.0001', 83.38), ('reg:0.001', 83.35), ('reg:0.01', 83.06)]
2019-03-12 21:31:04,205 : Validation : best param found is reg = 1e-05 with score             83.42
2019-03-12 21:31:04,205 : Evaluating...
2019-03-12 21:31:10,871 : 
Dev acc : 83.4 Test acc : 82.1 for TENSE classification

2019-03-12 21:31:10,873 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 21:31:11,273 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 21:31:11,335 : loading BERT model bert-large-uncased
2019-03-12 21:31:11,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:31:11,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:31:11,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmb880mjo
2019-03-12 21:31:18,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:31:24,542 : Computing embeddings for train/dev/test
2019-03-12 21:34:32,448 : Computed embeddings
2019-03-12 21:34:32,448 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:34:59,954 : [('reg:1e-05', 73.62), ('reg:0.0001', 73.57), ('reg:0.001', 73.67), ('reg:0.01', 68.35)]
2019-03-12 21:34:59,955 : Validation : best param found is reg = 0.001 with score             73.67
2019-03-12 21:34:59,955 : Evaluating...
2019-03-12 21:35:07,697 : 
Dev acc : 73.7 Test acc : 71.8 for SUBJNUMBER classification

2019-03-12 21:35:07,698 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 21:35:08,097 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 21:35:08,164 : loading BERT model bert-large-uncased
2019-03-12 21:35:08,164 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:35:08,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:35:08,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6vzxn39d
2019-03-12 21:35:15,759 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:35:21,305 : Computing embeddings for train/dev/test
2019-03-12 21:38:25,938 : Computed embeddings
2019-03-12 21:38:25,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:38:56,246 : [('reg:1e-05', 73.51), ('reg:0.0001', 73.32), ('reg:0.001', 72.33), ('reg:0.01', 70.6)]
2019-03-12 21:38:56,246 : Validation : best param found is reg = 1e-05 with score             73.51
2019-03-12 21:38:56,246 : Evaluating...
2019-03-12 21:39:02,969 : 
Dev acc : 73.5 Test acc : 74.1 for OBJNUMBER classification

2019-03-12 21:39:02,970 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 21:39:03,531 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 21:39:03,599 : loading BERT model bert-large-uncased
2019-03-12 21:39:03,600 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:39:03,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:39:03,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcxse3m89
2019-03-12 21:39:11,179 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:39:16,727 : Computing embeddings for train/dev/test
2019-03-12 21:42:50,829 : Computed embeddings
2019-03-12 21:42:50,829 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:43:16,790 : [('reg:1e-05', 54.56), ('reg:0.0001', 54.56), ('reg:0.001', 54.63), ('reg:0.01', 54.82)]
2019-03-12 21:43:16,790 : Validation : best param found is reg = 0.01 with score             54.82
2019-03-12 21:43:16,790 : Evaluating...
2019-03-12 21:43:23,334 : 
Dev acc : 54.8 Test acc : 53.8 for ODDMANOUT classification

2019-03-12 21:43:23,335 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 21:43:23,709 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 21:43:23,789 : loading BERT model bert-large-uncased
2019-03-12 21:43:23,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:43:23,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:43:23,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdrq3y3e1
2019-03-12 21:43:31,287 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:43:36,856 : Computing embeddings for train/dev/test
2019-03-12 21:47:09,082 : Computed embeddings
2019-03-12 21:47:09,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:47:40,968 : [('reg:1e-05', 50.16), ('reg:0.0001', 54.08), ('reg:0.001', 55.92), ('reg:0.01', 50.01)]
2019-03-12 21:47:40,969 : Validation : best param found is reg = 0.001 with score             55.92
2019-03-12 21:47:40,969 : Evaluating...
2019-03-12 21:47:50,754 : 
Dev acc : 55.9 Test acc : 56.1 for COORDINATIONINVERSION classification

2019-03-12 21:47:50,756 : total results: {'STS12': {'MSRpar': {'pearson': (0.3683774798716827, 1.6110017715436018e-25), 'spearman': SpearmanrResult(correlation=0.40242756709280725, pvalue=1.4473102613598371e-30), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7477844252201801, 3.4975591982475365e-135), 'spearman': SpearmanrResult(correlation=0.750200058340132, pvalue=1.5978951815402901e-136), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4958814014483724, 7.349196142460858e-30), 'spearman': SpearmanrResult(correlation=0.5952928654657, pvalue=2.3770037395305563e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.623051500222921, 7.297394311927298e-82), 'spearman': SpearmanrResult(correlation=0.6430085599627909, pvalue=9.97982431922024e-89), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5418375881215886, 8.014036313639399e-32), 'spearman': SpearmanrResult(correlation=0.45469490103006194, pvalue=9.411270586921559e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5553864789769489, 'wmean': 0.5624880356857801}, 'spearman': {'mean': 0.5691247903782984, 'wmean': 0.5795977573380144}}}, 'STS13': {'FNWN': {'pearson': (0.21873539412779536, 0.002495643284434074), 'spearman': SpearmanrResult(correlation=0.210730358392008, pvalue=0.0036073468097605737), 'nsamples': 189}, 'headlines': {'pearson': (0.6708638881911931, 3.2803190060479508e-99), 'spearman': SpearmanrResult(correlation=0.661323928543572, pvalue=1.7100622601522564e-95), 'nsamples': 750}, 'OnWN': {'pearson': (0.6591500525593444, 3.29245869000459e-71), 'spearman': SpearmanrResult(correlation=0.6561541212995821, pvalue=2.3033727022604523e-70), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5162497782927776, 'wmean': 0.6095147234128936}, 'spearman': {'mean': 0.509402802745054, 'wmean': 0.6026156307952227}}}, 'STS14': {'deft-forum': {'pearson': (0.4262315244508522, 2.7400174384467913e-21), 'spearman': SpearmanrResult(correlation=0.4313248673537462, pvalue=8.167309958720587e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7084141695803656, 5.240014340340422e-47), 'spearman': SpearmanrResult(correlation=0.66631210914033, pvalue=7.218019476109821e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.6407904093202199, 6.127551613068878e-88), 'spearman': SpearmanrResult(correlation=0.6120919550867141, pvalue=2.663309432385111e-78), 'nsamples': 750}, 'images': {'pearson': (0.6994018498091447, 3.2894909644360766e-111), 'spearman': SpearmanrResult(correlation=0.6912096911540931, pvalue=1.2725143585023138e-107), 'nsamples': 750}, 'OnWN': {'pearson': (0.7209381979900029, 2.9824469115357937e-121), 'spearman': SpearmanrResult(correlation=0.7404186209293866, pvalue=3.448933675937288e-131), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6278311538895639, 1.839843224943871e-83), 'spearman': SpearmanrResult(correlation=0.6044087982758326, pvalue=6.927450554817349e-76), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6372678841733582, 'wmean': 0.6456132387023179}, 'spearman': {'mean': 0.6242943403233504, 'wmean': 0.6346897659028813}}}, 'STS15': {'answers-forums': {'pearson': (0.5526450806465049, 2.2508952684622007e-31), 'spearman': SpearmanrResult(correlation=0.541324031990012, pvalue=6.2040397267028034e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.7202663224809142, 6.3407534212347965e-121), 'spearman': SpearmanrResult(correlation=0.7253473787741209, pvalue=1.998614317696114e-123), 'nsamples': 750}, 'belief': {'pearson': (0.5684872086278046, 1.746644170080584e-33), 'spearman': SpearmanrResult(correlation=0.5957000762839197, pvalue=2.184387816886911e-37), 'nsamples': 375}, 'headlines': {'pearson': (0.6771351180270797, 9.901980177938292e-102), 'spearman': SpearmanrResult(correlation=0.6769603381321277, pvalue=1.1662985790185731e-101), 'nsamples': 750}, 'images': {'pearson': (0.7831320100824534, 1.7798043670040275e-156), 'spearman': SpearmanrResult(correlation=0.7909589002055626, pvalue=9.70543476250388e-162), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6603331479729514, 'wmean': 0.6852748988069005}, 'spearman': {'mean': 0.6660581450771487, 'wmean': 0.6904446678121943}}}, 'STS16': {'answer-answer': {'pearson': (0.48815500715401294, 1.294380102423019e-16), 'spearman': SpearmanrResult(correlation=0.4919924424398874, pvalue=6.884738011687985e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6792703087313715, 4.8048391667932407e-35), 'spearman': SpearmanrResult(correlation=0.6882946545951181, pvalue=2.7085341627418636e-36), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7293474618639328, 1.8684160465722552e-39), 'spearman': SpearmanrResult(correlation=0.7407091117903072, pvalue=2.923096258211032e-41), 'nsamples': 230}, 'postediting': {'pearson': (0.8180511825676492, 4.5932944387631893e-60), 'spearman': SpearmanrResult(correlation=0.8414943318781832, pvalue=1.188061992274122e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.5726885775790208, 1.3059645898755576e-19), 'spearman': SpearmanrResult(correlation=0.5843629592037193, pvalue=1.5657115118225937e-20), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6575025075791975, 'wmean': 0.657821244671549}, 'spearman': {'mean': 0.669370699981443, 'wmean': 0.6696214338427828}}}, 'MR': {'devacc': 60.65, 'acc': 57.36, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.66, 'acc': 64.45, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.09, 'acc': 86.06, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.19, 'acc': 88.49, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.49, 'acc': 76.0, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.69, 'acc': 37.56, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 63.98, 'acc': 80.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.76, 'acc': 70.32, 'f1': 81.31, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.6, 'acc': 78.26, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7771852305189285, 'pearson': 0.7956149361331633, 'spearman': 0.7278095562523037, 'mse': 0.3754558547476309, 'yhat': array([2.10593975, 4.29175175, 2.03481095, ..., 3.05934793, 4.44247937,        4.73396147]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6774393370700892, 'pearson': 0.6550486649344721, 'spearman': 0.655639414291777, 'mse': 1.3737904463578485, 'yhat': array([1.47637628, 1.55744925, 2.60878965, ..., 2.65854282, 4.11084032,        3.20839162]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.74, 'acc': 58.5, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 68.42, 'acc': 68.87, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 45.1, 'acc': 45.11, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.08, 'acc': 28.9, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 50.73, 'acc': 50.85, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 67.8, 'acc': 67.22, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 83.42, 'acc': 82.14, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.67, 'acc': 71.79, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 73.51, 'acc': 74.08, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.82, 'acc': 53.8, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.92, 'acc': 56.14, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 21:47:50,756 : STS12 p=0.5625, STS12 s=0.5796, STS13 p=0.6095, STS13 s=0.6026, STS14 p=0.6456, STS14 s=0.6347, STS15 p=0.6853, STS15 s=0.6904, STS 16 p=0.6578, STS16 s=0.6696, STS B p=0.6550, STS B s=0.6556, STS B m=1.3738, SICK-R p=0.7956, SICK-R s=0.7278, SICK-P m=0.3755
2019-03-12 21:47:50,756 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 21:47:50,756 : 0.5625,0.5796,0.6095,0.6026,0.6456,0.6347,0.6853,0.6904,0.6578,0.6696,0.6550,0.6556,1.3738,0.7956,0.7278,0.3755
2019-03-12 21:47:50,756 : MR=57.36, CR=64.45, SUBJ=88.49, MPQA=86.06, SST-B=76.00, SST-F=37.56, TREC=80.40, SICK-E=78.26, SNLI=58.50, MRPC=70.32, MRPC f=81.31
2019-03-12 21:47:50,756 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 21:47:50,756 : 57.36,64.45,88.49,86.06,76.00,37.56,80.40,78.26,58.50,70.32,81.31
2019-03-12 21:47:50,756 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 21:47:50,756 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 21:47:50,757 : na,na,na,na,na,na,na,na,na,na
2019-03-12 21:47:50,757 : SentLen=68.87, WC=45.11, TreeDepth=28.90, TopConst=50.85, BShift=67.22, Tense=82.14, SubjNum=71.79, ObjNum=74.08, SOMO=53.80, CoordInv=56.14, average=59.89
2019-03-12 21:47:50,757 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 21:47:50,757 : 68.87,45.11,28.90,50.85,67.22,82.14,71.79,74.08,53.80,56.14,59.89
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,757 : layer 6
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,757 : ********************************************************************************
2019-03-12 21:47:50,849 : ***** Transfer task : STS12 *****


2019-03-12 21:47:50,862 : loading BERT model bert-large-uncased
2019-03-12 21:47:50,862 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:47:50,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:47:50,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppnai451j
2019-03-12 21:47:58,281 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:48:07,855 : MSRpar : pearson = 0.3804, spearman = 0.4169
2019-03-12 21:48:09,482 : MSRvid : pearson = 0.7505, spearman = 0.7541
2019-03-12 21:48:10,881 : SMTeuroparl : pearson = 0.4942, spearman = 0.6074
2019-03-12 21:48:13,548 : surprise.OnWN : pearson = 0.6121, spearman = 0.6367
2019-03-12 21:48:14,960 : surprise.SMTnews : pearson = 0.5303, spearman = 0.4545
2019-03-12 21:48:14,960 : ALL (weighted average) : Pearson = 0.5617,             Spearman = 0.5843
2019-03-12 21:48:14,960 : ALL (average) : Pearson = 0.5535,             Spearman = 0.5739

2019-03-12 21:48:14,960 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 21:48:14,969 : loading BERT model bert-large-uncased
2019-03-12 21:48:14,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:48:14,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:48:14,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxuxitkqk
2019-03-12 21:48:22,421 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:48:29,172 : FNWN : pearson = 0.2160, spearman = 0.2044
2019-03-12 21:48:31,050 : headlines : pearson = 0.6835, spearman = 0.6761
2019-03-12 21:48:32,503 : OnWN : pearson = 0.6585, spearman = 0.6561
2019-03-12 21:48:32,503 : ALL (weighted average) : Pearson = 0.6153,             Spearman = 0.6092
2019-03-12 21:48:32,503 : ALL (average) : Pearson = 0.5193,             Spearman = 0.5122

2019-03-12 21:48:32,503 : ***** Transfer task : STS14 *****


2019-03-12 21:48:32,518 : loading BERT model bert-large-uncased
2019-03-12 21:48:32,519 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:48:32,536 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:48:32,536 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpah_8jqjx
2019-03-12 21:48:40,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:48:46,758 : deft-forum : pearson = 0.4190, spearman = 0.4315
2019-03-12 21:48:48,381 : deft-news : pearson = 0.7087, spearman = 0.6658
2019-03-12 21:48:50,531 : headlines : pearson = 0.6479, spearman = 0.6204
2019-03-12 21:48:52,588 : images : pearson = 0.6961, spearman = 0.6895
2019-03-12 21:48:54,700 : OnWN : pearson = 0.7176, spearman = 0.7389
2019-03-12 21:48:57,529 : tweet-news : pearson = 0.6372, spearman = 0.6106
2019-03-12 21:48:57,529 : ALL (weighted average) : Pearson = 0.6467,             Spearman = 0.6369
2019-03-12 21:48:57,529 : ALL (average) : Pearson = 0.6378,             Spearman = 0.6261

2019-03-12 21:48:57,529 : ***** Transfer task : STS15 *****


2019-03-12 21:48:57,561 : loading BERT model bert-large-uncased
2019-03-12 21:48:57,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:48:57,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:48:57,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4f6sxr4t
2019-03-12 21:49:05,068 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:49:12,352 : answers-forums : pearson = 0.5364, spearman = 0.5198
2019-03-12 21:49:14,419 : answers-students : pearson = 0.7267, spearman = 0.7313
2019-03-12 21:49:16,447 : belief : pearson = 0.5770, spearman = 0.6093
2019-03-12 21:49:18,675 : headlines : pearson = 0.6875, spearman = 0.6875
2019-03-12 21:49:20,785 : images : pearson = 0.7886, spearman = 0.7959
2019-03-12 21:49:20,786 : ALL (weighted average) : Pearson = 0.6899,             Spearman = 0.6948
2019-03-12 21:49:20,786 : ALL (average) : Pearson = 0.6632,             Spearman = 0.6688

2019-03-12 21:49:20,786 : ***** Transfer task : STS16 *****


2019-03-12 21:49:20,855 : loading BERT model bert-large-uncased
2019-03-12 21:49:20,855 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:49:20,873 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:49:20,873 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiwneg_v9
2019-03-12 21:49:28,366 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:49:34,766 : answer-answer : pearson = 0.4928, spearman = 0.4906
2019-03-12 21:49:35,423 : headlines : pearson = 0.6778, spearman = 0.6833
2019-03-12 21:49:36,298 : plagiarism : pearson = 0.7289, spearman = 0.7416
2019-03-12 21:49:37,778 : postediting : pearson = 0.8035, spearman = 0.8372
2019-03-12 21:49:38,379 : question-question : pearson = 0.5604, spearman = 0.5715
2019-03-12 21:49:38,379 : ALL (weighted average) : Pearson = 0.6532,             Spearman = 0.6653
2019-03-12 21:49:38,379 : ALL (average) : Pearson = 0.6527,             Spearman = 0.6649

2019-03-12 21:49:38,379 : ***** Transfer task : MR *****


2019-03-12 21:49:38,395 : loading BERT model bert-large-uncased
2019-03-12 21:49:38,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:49:38,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:49:38,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg68vczgk
2019-03-12 21:49:45,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:49:51,437 : Generating sentence embeddings
2019-03-12 21:50:22,569 : Generated sentence embeddings
2019-03-12 21:50:22,569 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:50:34,374 : Best param found at split 1: l2reg = 1e-05                 with score 62.47
2019-03-12 21:50:46,792 : Best param found at split 2: l2reg = 1e-05                 with score 60.68
2019-03-12 21:50:59,878 : Best param found at split 3: l2reg = 0.0001                 with score 59.12
2019-03-12 21:51:11,305 : Best param found at split 4: l2reg = 0.01                 with score 57.91
2019-03-12 21:51:21,224 : Best param found at split 5: l2reg = 1e-05                 with score 60.15
2019-03-12 21:51:21,741 : Dev acc : 60.07 Test acc : 62.08

2019-03-12 21:51:21,742 : ***** Transfer task : CR *****


2019-03-12 21:51:21,749 : loading BERT model bert-large-uncased
2019-03-12 21:51:21,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:51:21,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:51:21,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj5fnx239
2019-03-12 21:51:29,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:51:34,851 : Generating sentence embeddings
2019-03-12 21:51:43,081 : Generated sentence embeddings
2019-03-12 21:51:43,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:51:46,266 : Best param found at split 1: l2reg = 0.001                 with score 69.66
2019-03-12 21:51:49,763 : Best param found at split 2: l2reg = 0.0001                 with score 74.96
2019-03-12 21:51:53,245 : Best param found at split 3: l2reg = 0.0001                 with score 70.34
2019-03-12 21:51:56,861 : Best param found at split 4: l2reg = 0.001                 with score 73.72
2019-03-12 21:52:00,585 : Best param found at split 5: l2reg = 1e-05                 with score 75.04
2019-03-12 21:52:00,746 : Dev acc : 72.74 Test acc : 66.44

2019-03-12 21:52:00,746 : ***** Transfer task : MPQA *****


2019-03-12 21:52:00,752 : loading BERT model bert-large-uncased
2019-03-12 21:52:00,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:52:00,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:52:00,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3hto8jkv
2019-03-12 21:52:08,308 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:52:13,900 : Generating sentence embeddings
2019-03-12 21:52:21,411 : Generated sentence embeddings
2019-03-12 21:52:21,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:52:32,448 : Best param found at split 1: l2reg = 0.0001                 with score 86.96
2019-03-12 21:52:43,893 : Best param found at split 2: l2reg = 1e-05                 with score 86.48
2019-03-12 21:52:56,433 : Best param found at split 3: l2reg = 1e-05                 with score 86.51
2019-03-12 21:53:07,540 : Best param found at split 4: l2reg = 0.001                 with score 86.49
2019-03-12 21:53:19,910 : Best param found at split 5: l2reg = 0.001                 with score 86.5
2019-03-12 21:53:20,533 : Dev acc : 86.59 Test acc : 85.4

2019-03-12 21:53:20,534 : ***** Transfer task : SUBJ *****


2019-03-12 21:53:20,550 : loading BERT model bert-large-uncased
2019-03-12 21:53:20,550 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:53:20,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:53:20,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjmq51c1h
2019-03-12 21:53:28,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:53:33,584 : Generating sentence embeddings
2019-03-12 21:54:04,115 : Generated sentence embeddings
2019-03-12 21:54:04,115 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 21:54:15,314 : Best param found at split 1: l2reg = 0.001                 with score 89.88
2019-03-12 21:54:26,554 : Best param found at split 2: l2reg = 1e-05                 with score 90.71
2019-03-12 21:54:37,527 : Best param found at split 3: l2reg = 0.0001                 with score 90.06
2019-03-12 21:54:46,967 : Best param found at split 4: l2reg = 1e-05                 with score 91.1
2019-03-12 21:54:58,482 : Best param found at split 5: l2reg = 0.001                 with score 90.95
2019-03-12 21:54:59,064 : Dev acc : 90.54 Test acc : 90.88

2019-03-12 21:54:59,065 : ***** Transfer task : SST Binary classification *****


2019-03-12 21:54:59,155 : loading BERT model bert-large-uncased
2019-03-12 21:54:59,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:54:59,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:54:59,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0znb5dyj
2019-03-12 21:55:06,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:55:12,130 : Computing embedding for train
2019-03-12 21:56:50,999 : Computed train embeddings
2019-03-12 21:56:50,999 : Computing embedding for dev
2019-03-12 21:56:53,151 : Computed dev embeddings
2019-03-12 21:56:53,151 : Computing embedding for test
2019-03-12 21:56:57,679 : Computed test embeddings
2019-03-12 21:56:57,679 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:57:13,454 : [('reg:1e-05', 76.61), ('reg:0.0001', 76.61), ('reg:0.001', 76.49), ('reg:0.01', 75.0)]
2019-03-12 21:57:13,454 : Validation : best param found is reg = 1e-05 with score             76.61
2019-03-12 21:57:13,454 : Evaluating...
2019-03-12 21:57:17,257 : 
Dev acc : 76.61 Test acc : 77.7 for             SST Binary classification

2019-03-12 21:57:17,257 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 21:57:17,313 : loading BERT model bert-large-uncased
2019-03-12 21:57:17,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:57:17,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:57:17,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeyf7osic
2019-03-12 21:57:24,787 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:57:30,234 : Computing embedding for train
2019-03-12 21:57:51,886 : Computed train embeddings
2019-03-12 21:57:51,886 : Computing embedding for dev
2019-03-12 21:57:54,715 : Computed dev embeddings
2019-03-12 21:57:54,715 : Computing embedding for test
2019-03-12 21:58:00,283 : Computed test embeddings
2019-03-12 21:58:00,283 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:58:02,784 : [('reg:1e-05', 32.7), ('reg:0.0001', 27.07), ('reg:0.001', 33.7), ('reg:0.01', 33.88)]
2019-03-12 21:58:02,784 : Validation : best param found is reg = 0.01 with score             33.88
2019-03-12 21:58:02,784 : Evaluating...
2019-03-12 21:58:03,194 : 
Dev acc : 33.88 Test acc : 35.34 for             SST Fine-Grained classification

2019-03-12 21:58:03,194 : ***** Transfer task : TREC *****


2019-03-12 21:58:03,207 : loading BERT model bert-large-uncased
2019-03-12 21:58:03,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:58:03,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:58:03,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3memc5hy
2019-03-12 21:58:10,677 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:58:23,647 : Computed train embeddings
2019-03-12 21:58:24,228 : Computed test embeddings
2019-03-12 21:58:24,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 21:58:32,328 : [('reg:1e-05', 64.66), ('reg:0.0001', 64.29), ('reg:0.001', 67.52), ('reg:0.01', 60.34)]
2019-03-12 21:58:32,328 : Cross-validation : best param found is reg = 0.001             with score 67.52
2019-03-12 21:58:32,328 : Evaluating...
2019-03-12 21:58:32,788 : 
Dev acc : 67.52 Test acc : 77.0             for TREC

2019-03-12 21:58:32,789 : ***** Transfer task : MRPC *****


2019-03-12 21:58:32,810 : loading BERT model bert-large-uncased
2019-03-12 21:58:32,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:58:32,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:58:32,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplnbadnc_
2019-03-12 21:58:40,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:58:45,665 : Computing embedding for train
2019-03-12 21:59:07,676 : Computed train embeddings
2019-03-12 21:59:07,676 : Computing embedding for test
2019-03-12 21:59:17,318 : Computed test embeddings
2019-03-12 21:59:17,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 21:59:22,493 : [('reg:1e-05', 70.54), ('reg:0.0001', 72.11), ('reg:0.001', 72.08), ('reg:0.01', 72.74)]
2019-03-12 21:59:22,493 : Cross-validation : best param found is reg = 0.01             with score 72.74
2019-03-12 21:59:22,493 : Evaluating...
2019-03-12 21:59:22,732 : Dev acc : 72.74 Test acc 69.74; Test F1 81.21 for MRPC.

2019-03-12 21:59:22,732 : ***** Transfer task : SICK-Entailment*****


2019-03-12 21:59:22,795 : loading BERT model bert-large-uncased
2019-03-12 21:59:22,795 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:59:22,814 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:59:22,814 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvn7quvdx
2019-03-12 21:59:30,237 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:59:35,697 : Computing embedding for train
2019-03-12 21:59:46,856 : Computed train embeddings
2019-03-12 21:59:46,856 : Computing embedding for dev
2019-03-12 21:59:48,379 : Computed dev embeddings
2019-03-12 21:59:48,380 : Computing embedding for test
2019-03-12 22:00:00,347 : Computed test embeddings
2019-03-12 22:00:00,385 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:00:01,803 : [('reg:1e-05', 71.6), ('reg:0.0001', 77.6), ('reg:0.001', 73.4), ('reg:0.01', 78.0)]
2019-03-12 22:00:01,804 : Validation : best param found is reg = 0.01 with score             78.0
2019-03-12 22:00:01,804 : Evaluating...
2019-03-12 22:00:02,171 : 
Dev acc : 78.0 Test acc : 76.74 for                        SICK entailment

2019-03-12 22:00:02,171 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 22:00:02,198 : loading BERT model bert-large-uncased
2019-03-12 22:00:02,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:00:02,256 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:00:02,256 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmxcavqxk
2019-03-12 22:00:09,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:00:15,269 : Computing embedding for train
2019-03-12 22:00:26,449 : Computed train embeddings
2019-03-12 22:00:26,449 : Computing embedding for dev
2019-03-12 22:00:27,976 : Computed dev embeddings
2019-03-12 22:00:27,976 : Computing embedding for test
2019-03-12 22:00:39,979 : Computed test embeddings
2019-03-12 22:00:54,959 : Dev : Pearson 0.7809094524856198
2019-03-12 22:00:54,959 : Test : Pearson 0.801570019875196 Spearman 0.7372594878922833 MSE 0.3670247691246824                        for SICK Relatedness

2019-03-12 22:00:54,960 : 

***** Transfer task : STSBenchmark*****


2019-03-12 22:00:54,999 : loading BERT model bert-large-uncased
2019-03-12 22:00:55,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:00:55,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:00:55,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwjm0aftv
2019-03-12 22:01:02,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:01:07,963 : Computing embedding for train
2019-03-12 22:01:26,362 : Computed train embeddings
2019-03-12 22:01:26,362 : Computing embedding for dev
2019-03-12 22:01:31,934 : Computed dev embeddings
2019-03-12 22:01:31,935 : Computing embedding for test
2019-03-12 22:01:36,483 : Computed test embeddings
2019-03-12 22:01:55,439 : Dev : Pearson 0.6643194553741282
2019-03-12 22:01:55,439 : Test : Pearson 0.6462940939548273 Spearman 0.6425477005423746 MSE 1.4219766188382121                        for SICK Relatedness

2019-03-12 22:01:55,439 : ***** Transfer task : SNLI Entailment*****


2019-03-12 22:02:00,089 : loading BERT model bert-large-uncased
2019-03-12 22:02:00,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:02:01,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:02:01,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm1_j104p
2019-03-12 22:02:08,543 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:02:14,606 : PROGRESS (encoding): 0.00%
2019-03-12 22:04:58,260 : PROGRESS (encoding): 14.56%
2019-03-12 22:08:03,821 : PROGRESS (encoding): 29.12%
2019-03-12 22:11:09,921 : PROGRESS (encoding): 43.69%
2019-03-12 22:14:28,413 : PROGRESS (encoding): 58.25%
2019-03-12 22:18:09,378 : PROGRESS (encoding): 72.81%
2019-03-12 22:21:49,245 : PROGRESS (encoding): 87.37%
2019-03-12 22:25:47,243 : PROGRESS (encoding): 0.00%
2019-03-12 22:26:17,214 : PROGRESS (encoding): 0.00%
2019-03-12 22:26:45,990 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:27:39,291 : [('reg:1e-09', 63.4)]
2019-03-12 22:27:39,291 : Validation : best param found is reg = 1e-09 with score             63.4
2019-03-12 22:27:39,291 : Evaluating...
2019-03-12 22:28:32,583 : Dev acc : 63.4 Test acc : 64.09 for SNLI

2019-03-12 22:28:32,583 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 22:28:32,794 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 22:28:33,796 : loading BERT model bert-large-uncased
2019-03-12 22:28:33,796 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:28:33,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:28:33,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf82xw8dz
2019-03-12 22:28:41,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:28:46,861 : Computing embeddings for train/dev/test
2019-03-12 22:32:14,875 : Computed embeddings
2019-03-12 22:32:14,875 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:32:42,786 : [('reg:1e-05', 63.98), ('reg:0.0001', 59.44), ('reg:0.001', 68.24), ('reg:0.01', 50.07)]
2019-03-12 22:32:42,786 : Validation : best param found is reg = 0.001 with score             68.24
2019-03-12 22:32:42,786 : Evaluating...
2019-03-12 22:32:50,072 : 
Dev acc : 68.2 Test acc : 68.1 for LENGTH classification

2019-03-12 22:32:50,073 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 22:32:50,438 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 22:32:50,482 : loading BERT model bert-large-uncased
2019-03-12 22:32:50,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:32:50,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:32:50,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwbqg8n55
2019-03-12 22:32:57,893 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:33:03,393 : Computing embeddings for train/dev/test
2019-03-12 22:36:15,279 : Computed embeddings
2019-03-12 22:36:15,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:36:51,080 : [('reg:1e-05', 40.2), ('reg:0.0001', 15.08), ('reg:0.001', 0.81), ('reg:0.01', 0.2)]
2019-03-12 22:36:51,081 : Validation : best param found is reg = 1e-05 with score             40.2
2019-03-12 22:36:51,081 : Evaluating...
2019-03-12 22:37:02,337 : 
Dev acc : 40.2 Test acc : 40.0 for WORDCONTENT classification

2019-03-12 22:37:02,338 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 22:37:02,692 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 22:37:02,758 : loading BERT model bert-large-uncased
2019-03-12 22:37:02,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:37:02,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:37:02,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq47eg_u4
2019-03-12 22:37:10,243 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:37:15,735 : Computing embeddings for train/dev/test
2019-03-12 22:40:15,989 : Computed embeddings
2019-03-12 22:40:15,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:40:41,840 : [('reg:1e-05', 27.54), ('reg:0.0001', 25.8), ('reg:0.001', 25.21), ('reg:0.01', 24.93)]
2019-03-12 22:40:41,841 : Validation : best param found is reg = 1e-05 with score             27.54
2019-03-12 22:40:41,841 : Evaluating...
2019-03-12 22:40:50,268 : 
Dev acc : 27.5 Test acc : 27.2 for DEPTH classification

2019-03-12 22:40:50,269 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 22:40:50,661 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 22:40:50,726 : loading BERT model bert-large-uncased
2019-03-12 22:40:50,726 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:40:50,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:40:50,843 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp57y7gg6m
2019-03-12 22:40:58,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:41:03,850 : Computing embeddings for train/dev/test
2019-03-12 22:43:50,957 : Computed embeddings
2019-03-12 22:43:50,958 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:44:21,366 : [('reg:1e-05', 52.39), ('reg:0.0001', 47.18), ('reg:0.001', 49.01), ('reg:0.01', 33.39)]
2019-03-12 22:44:21,366 : Validation : best param found is reg = 1e-05 with score             52.39
2019-03-12 22:44:21,366 : Evaluating...
2019-03-12 22:44:30,303 : 
Dev acc : 52.4 Test acc : 52.1 for TOPCONSTITUENTS classification

2019-03-12 22:44:30,304 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 22:44:30,643 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 22:44:30,709 : loading BERT model bert-large-uncased
2019-03-12 22:44:30,709 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:44:30,830 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:44:30,830 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph9_zqffa
2019-03-12 22:44:38,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:44:43,843 : Computing embeddings for train/dev/test
2019-03-12 22:47:45,405 : Computed embeddings
2019-03-12 22:47:45,405 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:48:21,611 : [('reg:1e-05', 68.63), ('reg:0.0001', 68.67), ('reg:0.001', 68.51), ('reg:0.01', 64.69)]
2019-03-12 22:48:21,611 : Validation : best param found is reg = 0.0001 with score             68.67
2019-03-12 22:48:21,611 : Evaluating...
2019-03-12 22:48:31,312 : 
Dev acc : 68.7 Test acc : 68.7 for BIGRAMSHIFT classification

2019-03-12 22:48:31,313 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 22:48:31,715 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 22:48:31,780 : loading BERT model bert-large-uncased
2019-03-12 22:48:31,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:48:31,808 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:48:31,808 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpykgpnj7e
2019-03-12 22:48:39,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:48:44,829 : Computing embeddings for train/dev/test
2019-03-12 22:51:42,420 : Computed embeddings
2019-03-12 22:51:42,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:52:12,800 : [('reg:1e-05', 85.19), ('reg:0.0001', 85.16), ('reg:0.001', 85.23), ('reg:0.01', 84.9)]
2019-03-12 22:52:12,800 : Validation : best param found is reg = 0.001 with score             85.23
2019-03-12 22:52:12,800 : Evaluating...
2019-03-12 22:52:20,415 : 
Dev acc : 85.2 Test acc : 83.8 for TENSE classification

2019-03-12 22:52:20,416 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 22:52:20,823 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 22:52:20,886 : loading BERT model bert-large-uncased
2019-03-12 22:52:20,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:52:20,914 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:52:20,914 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl3b4baej
2019-03-12 22:52:28,379 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:52:33,920 : Computing embeddings for train/dev/test
2019-03-12 22:55:42,135 : Computed embeddings
2019-03-12 22:55:42,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:56:14,772 : [('reg:1e-05', 76.33), ('reg:0.0001', 76.35), ('reg:0.001', 76.12), ('reg:0.01', 62.69)]
2019-03-12 22:56:14,772 : Validation : best param found is reg = 0.0001 with score             76.35
2019-03-12 22:56:14,772 : Evaluating...
2019-03-12 22:56:21,293 : 
Dev acc : 76.3 Test acc : 74.8 for SUBJNUMBER classification

2019-03-12 22:56:21,294 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 22:56:21,699 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 22:56:21,766 : loading BERT model bert-large-uncased
2019-03-12 22:56:21,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:56:21,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:56:21,879 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmx3sia_w
2019-03-12 22:56:29,337 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:56:34,882 : Computing embeddings for train/dev/test
2019-03-12 22:59:39,559 : Computed embeddings
2019-03-12 22:59:39,559 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:00:04,259 : [('reg:1e-05', 75.91), ('reg:0.0001', 76.5), ('reg:0.001', 75.2), ('reg:0.01', 72.45)]
2019-03-12 23:00:04,259 : Validation : best param found is reg = 0.0001 with score             76.5
2019-03-12 23:00:04,259 : Evaluating...
2019-03-12 23:00:10,377 : 
Dev acc : 76.5 Test acc : 76.2 for OBJNUMBER classification

2019-03-12 23:00:10,378 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 23:00:10,756 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 23:00:10,825 : loading BERT model bert-large-uncased
2019-03-12 23:00:10,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:00:10,946 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:00:10,946 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx2syfz40
2019-03-12 23:00:18,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:00:23,950 : Computing embeddings for train/dev/test
2019-03-12 23:03:58,163 : Computed embeddings
2019-03-12 23:03:58,163 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:04:25,837 : [('reg:1e-05', 54.89), ('reg:0.0001', 54.85), ('reg:0.001', 54.91), ('reg:0.01', 55.37)]
2019-03-12 23:04:25,837 : Validation : best param found is reg = 0.01 with score             55.37
2019-03-12 23:04:25,837 : Evaluating...
2019-03-12 23:04:32,547 : 
Dev acc : 55.4 Test acc : 54.2 for ODDMANOUT classification

2019-03-12 23:04:32,548 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 23:04:33,133 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 23:04:33,208 : loading BERT model bert-large-uncased
2019-03-12 23:04:33,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:04:33,239 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:04:33,239 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyeh70a4r
2019-03-12 23:04:40,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:04:46,325 : Computing embeddings for train/dev/test
2019-03-12 23:08:18,136 : Computed embeddings
2019-03-12 23:08:18,137 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:08:49,315 : [('reg:1e-05', 50.18), ('reg:0.0001', 50.16), ('reg:0.001', 56.78), ('reg:0.01', 50.23)]
2019-03-12 23:08:49,315 : Validation : best param found is reg = 0.001 with score             56.78
2019-03-12 23:08:49,315 : Evaluating...
2019-03-12 23:08:57,765 : 
Dev acc : 56.8 Test acc : 55.9 for COORDINATIONINVERSION classification

2019-03-12 23:08:57,767 : total results: {'STS12': {'MSRpar': {'pearson': (0.3803914276735831, 3.118978645686538e-27), 'spearman': SpearmanrResult(correlation=0.41685151969245193, pvalue=6.900114005996225e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7505285875673893, 1.0473374629596709e-136), 'spearman': SpearmanrResult(correlation=0.754117467553008, pvalue=9.93258344973974e-139), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49417675383530063, 1.2290088183520024e-29), 'spearman': SpearmanrResult(correlation=0.6073975541662207, pvalue=1.2708757213377778e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6121278215263476, 2.5941269730813153e-78), 'spearman': SpearmanrResult(correlation=0.6366568736354217, pvalue=1.7329617054829028e-86), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5302664273208507, 2.5932233837681248e-30), 'spearman': SpearmanrResult(correlation=0.45449592725386245, pvalue=9.851258951530325e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5534982035846943, 'wmean': 0.5616760978400619}, 'spearman': {'mean': 0.5739038684601929, 'wmean': 0.5842531364212508}}}, 'STS13': {'FNWN': {'pearson': (0.21601996640197182, 0.0028319630775442395), 'spearman': SpearmanrResult(correlation=0.20444666829883235, pvalue=0.004774008497612067), 'nsamples': 189}, 'headlines': {'pearson': (0.6835177666979021, 2.3201114260760644e-104), 'spearman': SpearmanrResult(correlation=0.6760881688184862, pvalue=2.6351719346777327e-101), 'nsamples': 750}, 'OnWN': {'pearson': (0.6584954784500123, 5.045873690756555e-71), 'spearman': SpearmanrResult(correlation=0.6561137768168693, pvalue=2.3641517473723243e-70), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.519344403849962, 'wmean': 0.6152547080559041}, 'spearman': {'mean': 0.5122162046447293, 'wmean': 0.6091909171444051}}}, 'STS14': {'deft-forum': {'pearson': (0.4189534315460649, 1.4913732136994008e-20), 'spearman': SpearmanrResult(correlation=0.4314871977371402, pvalue=7.855580342654314e-22), 'nsamples': 450}, 'deft-news': {'pearson': (0.7087493414958412, 4.543582189359859e-47), 'spearman': SpearmanrResult(correlation=0.6657550089271214, pvalue=8.812162053785778e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.6479116229463914, 1.7121747888399527e-90), 'spearman': SpearmanrResult(correlation=0.6203623378994805, pvalue=5.627726750083711e-81), 'nsamples': 750}, 'images': {'pearson': (0.6960985763089148, 9.512831722977576e-110), 'spearman': SpearmanrResult(correlation=0.689468998843606, pvalue=7.10450584042808e-107), 'nsamples': 750}, 'OnWN': {'pearson': (0.7176163035425274, 1.2156459464899261e-119), 'spearman': SpearmanrResult(correlation=0.7389434547315693, pvalue=2.0953195676991714e-130), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6372428589492185, 1.0823568671929309e-86), 'spearman': SpearmanrResult(correlation=0.610594166160125, pvalue=7.969845073799291e-78), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6377620224648265, 'wmean': 0.6467482314546054}, 'spearman': {'mean': 0.6261018607165071, 'wmean': 0.6369126559695827}}}, 'STS15': {'answers-forums': {'pearson': (0.5363517665915385, 2.559807179378855e-29), 'spearman': SpearmanrResult(correlation=0.5198136036741985, pvalue=2.4193941103637757e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.7267069001302039, 4.186545765753711e-124), 'spearman': SpearmanrResult(correlation=0.7312799787990303, pvalue=2.0316093857169283e-126), 'nsamples': 750}, 'belief': {'pearson': (0.5770139954703508, 1.1440531266783295e-34), 'spearman': SpearmanrResult(correlation=0.6093009682834842, pvalue=1.7598255343761106e-39), 'nsamples': 375}, 'headlines': {'pearson': (0.6875233835943181, 4.787312169190998e-106), 'spearman': SpearmanrResult(correlation=0.6874559883438569, pvalue=5.112996851750405e-106), 'nsamples': 750}, 'images': {'pearson': (0.7886195156486753, 3.833650975464401e-160), 'spearman': SpearmanrResult(correlation=0.7959135986832088, pvalue=3.4423839312424645e-165), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6632431122870173, 'wmean': 0.6898831701010355}, 'spearman': {'mean': 0.6687528275567558, 'wmean': 0.6948017129512343}}}, 'STS16': {'answer-answer': {'pearson': (0.492776173039322, 6.045997080969934e-17), 'spearman': SpearmanrResult(correlation=0.4906269148834997, pvalue=8.626704495088771e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6777519379534077, 7.716719266471881e-35), 'spearman': SpearmanrResult(correlation=0.6832807862427739, pvalue=1.35584144666088e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7288820813400747, 2.2053552357703018e-39), 'spearman': SpearmanrResult(correlation=0.7416294172314256, pvalue=2.0674884626273998e-41), 'nsamples': 230}, 'postediting': {'pearson': (0.8035125541467926, 1.9388791321435248e-56), 'spearman': SpearmanrResult(correlation=0.8372289240667274, pvalue=2.241738265960166e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.5603759069649367, 1.1186211039753851e-18), 'spearman': SpearmanrResult(correlation=0.5715389507799651, pvalue=1.6020583966706765e-19), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6526597306889067, 'wmean': 0.653241051414918}, 'spearman': {'mean': 0.6648609986408784, 'wmean': 0.6653173830551279}}}, 'MR': {'devacc': 60.07, 'acc': 62.08, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 72.74, 'acc': 66.44, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.59, 'acc': 85.4, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.54, 'acc': 90.88, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.61, 'acc': 77.7, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.88, 'acc': 35.34, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.52, 'acc': 77.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.74, 'acc': 69.74, 'f1': 81.21, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 76.74, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7809094524856198, 'pearson': 0.801570019875196, 'spearman': 0.7372594878922833, 'mse': 0.3670247691246824, 'yhat': array([2.71744826, 4.60563547, 1.26147089, ..., 3.09577034, 4.76731521,        4.89730248]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6643194553741282, 'pearson': 0.6462940939548273, 'spearman': 0.6425477005423746, 'mse': 1.4219766188382121, 'yhat': array([1.64601339, 1.19767969, 1.5443621 , ..., 4.05112043, 3.97373461,        3.22510844]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.4, 'acc': 64.09, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 68.24, 'acc': 68.12, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 40.2, 'acc': 39.99, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.54, 'acc': 27.22, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.39, 'acc': 52.13, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 68.67, 'acc': 68.66, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.23, 'acc': 83.85, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.35, 'acc': 74.85, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.5, 'acc': 76.22, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.37, 'acc': 54.21, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.78, 'acc': 55.89, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 23:08:57,767 : STS12 p=0.5617, STS12 s=0.5843, STS13 p=0.6153, STS13 s=0.6092, STS14 p=0.6467, STS14 s=0.6369, STS15 p=0.6899, STS15 s=0.6948, STS 16 p=0.6532, STS16 s=0.6653, STS B p=0.6463, STS B s=0.6425, STS B m=1.4220, SICK-R p=0.8016, SICK-R s=0.7373, SICK-P m=0.3670
2019-03-12 23:08:57,767 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 23:08:57,767 : 0.5617,0.5843,0.6153,0.6092,0.6467,0.6369,0.6899,0.6948,0.6532,0.6653,0.6463,0.6425,1.4220,0.8016,0.7373,0.3670
2019-03-12 23:08:57,767 : MR=62.08, CR=66.44, SUBJ=90.88, MPQA=85.40, SST-B=77.70, SST-F=35.34, TREC=77.00, SICK-E=76.74, SNLI=64.09, MRPC=69.74, MRPC f=81.21
2019-03-12 23:08:57,768 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 23:08:57,768 : 62.08,66.44,90.88,85.40,77.70,35.34,77.00,76.74,64.09,69.74,81.21
2019-03-12 23:08:57,768 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 23:08:57,768 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 23:08:57,768 : na,na,na,na,na,na,na,na,na,na
2019-03-12 23:08:57,768 : SentLen=68.12, WC=39.99, TreeDepth=27.22, TopConst=52.13, BShift=68.66, Tense=83.85, SubjNum=74.85, ObjNum=76.22, SOMO=54.21, CoordInv=55.89, average=60.11
2019-03-12 23:08:57,768 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 23:08:57,768 : 68.12,39.99,27.22,52.13,68.66,83.85,74.85,76.22,54.21,55.89,60.11
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,768 : layer 7
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,768 : ********************************************************************************
2019-03-12 23:08:57,860 : ***** Transfer task : STS12 *****


2019-03-12 23:08:57,872 : loading BERT model bert-large-uncased
2019-03-12 23:08:57,872 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:08:57,890 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:08:57,890 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwrzp2x38
2019-03-12 23:09:05,377 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:09:14,898 : MSRpar : pearson = 0.3782, spearman = 0.4156
2019-03-12 23:09:16,535 : MSRvid : pearson = 0.7059, spearman = 0.7112
2019-03-12 23:09:17,936 : SMTeuroparl : pearson = 0.4915, spearman = 0.6053
2019-03-12 23:09:20,608 : surprise.OnWN : pearson = 0.5917, spearman = 0.6268
2019-03-12 23:09:22,022 : surprise.SMTnews : pearson = 0.5354, spearman = 0.4608
2019-03-12 23:09:22,022 : ALL (weighted average) : Pearson = 0.5457,             Spearman = 0.5717
2019-03-12 23:09:22,023 : ALL (average) : Pearson = 0.5405,             Spearman = 0.5639

2019-03-12 23:09:22,023 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 23:09:22,033 : loading BERT model bert-large-uncased
2019-03-12 23:09:22,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:09:22,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:09:22,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5gt_4dlo
2019-03-12 23:09:29,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:09:36,285 : FNWN : pearson = 0.2059, spearman = 0.2016
2019-03-12 23:09:38,162 : headlines : pearson = 0.6794, spearman = 0.6762
2019-03-12 23:09:39,613 : OnWN : pearson = 0.6418, spearman = 0.6424
2019-03-12 23:09:39,614 : ALL (weighted average) : Pearson = 0.6056,             Spearman = 0.6038
2019-03-12 23:09:39,614 : ALL (average) : Pearson = 0.5090,             Spearman = 0.5068

2019-03-12 23:09:39,614 : ***** Transfer task : STS14 *****


2019-03-12 23:09:39,629 : loading BERT model bert-large-uncased
2019-03-12 23:09:39,629 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:09:39,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:09:39,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplq_a2zay
2019-03-12 23:09:47,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:09:54,014 : deft-forum : pearson = 0.4161, spearman = 0.4294
2019-03-12 23:09:55,633 : deft-news : pearson = 0.7198, spearman = 0.6772
2019-03-12 23:09:57,777 : headlines : pearson = 0.6466, spearman = 0.6187
2019-03-12 23:09:59,835 : images : pearson = 0.6649, spearman = 0.6580
2019-03-12 23:10:01,946 : OnWN : pearson = 0.7058, spearman = 0.7303
2019-03-12 23:10:04,777 : tweet-news : pearson = 0.6314, spearman = 0.6083
2019-03-12 23:10:04,778 : ALL (weighted average) : Pearson = 0.6373,             Spearman = 0.6287
2019-03-12 23:10:04,778 : ALL (average) : Pearson = 0.6308,             Spearman = 0.6203

2019-03-12 23:10:04,778 : ***** Transfer task : STS15 *****


2019-03-12 23:10:04,812 : loading BERT model bert-large-uncased
2019-03-12 23:10:04,813 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:10:04,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:10:04,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppxg0vo3m
2019-03-12 23:10:12,296 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:19,887 : answers-forums : pearson = 0.5279, spearman = 0.5036
2019-03-12 23:10:21,952 : answers-students : pearson = 0.7252, spearman = 0.7309
2019-03-12 23:10:23,980 : belief : pearson = 0.5728, spearman = 0.5998
2019-03-12 23:10:26,208 : headlines : pearson = 0.6892, spearman = 0.6913
2019-03-12 23:10:28,322 : images : pearson = 0.7740, spearman = 0.7806
2019-03-12 23:10:28,322 : ALL (weighted average) : Pearson = 0.6847,             Spearman = 0.6886
2019-03-12 23:10:28,322 : ALL (average) : Pearson = 0.6578,             Spearman = 0.6612

2019-03-12 23:10:28,323 : ***** Transfer task : STS16 *****


2019-03-12 23:10:28,392 : loading BERT model bert-large-uncased
2019-03-12 23:10:28,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:10:28,409 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:10:28,410 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq2q846zk
2019-03-12 23:10:35,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:42,317 : answer-answer : pearson = 0.4689, spearman = 0.4727
2019-03-12 23:10:42,970 : headlines : pearson = 0.6755, spearman = 0.6872
2019-03-12 23:10:43,843 : plagiarism : pearson = 0.7111, spearman = 0.7204
2019-03-12 23:10:45,318 : postediting : pearson = 0.7981, spearman = 0.8353
2019-03-12 23:10:45,919 : question-question : pearson = 0.5187, spearman = 0.5256
2019-03-12 23:10:45,919 : ALL (weighted average) : Pearson = 0.6357,             Spearman = 0.6497
2019-03-12 23:10:45,919 : ALL (average) : Pearson = 0.6345,             Spearman = 0.6483

2019-03-12 23:10:45,919 : ***** Transfer task : MR *****


2019-03-12 23:10:45,938 : loading BERT model bert-large-uncased
2019-03-12 23:10:45,938 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:10:45,956 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:10:45,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4pa6308i
2019-03-12 23:10:53,402 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:10:59,078 : Generating sentence embeddings
2019-03-12 23:11:30,201 : Generated sentence embeddings
2019-03-12 23:11:30,201 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:11:42,804 : Best param found at split 1: l2reg = 1e-05                 with score 65.99
2019-03-12 23:11:54,869 : Best param found at split 2: l2reg = 0.0001                 with score 61.34
2019-03-12 23:12:07,947 : Best param found at split 3: l2reg = 0.001                 with score 62.65
2019-03-12 23:12:19,510 : Best param found at split 4: l2reg = 0.0001                 with score 61.37
2019-03-12 23:12:32,064 : Best param found at split 5: l2reg = 1e-05                 with score 61.03
2019-03-12 23:12:32,779 : Dev acc : 62.48 Test acc : 62.19

2019-03-12 23:12:32,780 : ***** Transfer task : CR *****


2019-03-12 23:12:32,788 : loading BERT model bert-large-uncased
2019-03-12 23:12:32,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:12:32,808 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:12:32,808 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0_v1xb6w
2019-03-12 23:12:40,270 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:12:45,827 : Generating sentence embeddings
2019-03-12 23:12:54,054 : Generated sentence embeddings
2019-03-12 23:12:54,054 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:12:57,391 : Best param found at split 1: l2reg = 1e-05                 with score 70.85
2019-03-12 23:13:00,973 : Best param found at split 2: l2reg = 0.0001                 with score 74.16
2019-03-12 23:13:04,310 : Best param found at split 3: l2reg = 0.01                 with score 68.08
2019-03-12 23:13:07,916 : Best param found at split 4: l2reg = 0.001                 with score 71.76
2019-03-12 23:13:11,303 : Best param found at split 5: l2reg = 1e-05                 with score 69.22
2019-03-12 23:13:11,495 : Dev acc : 70.81 Test acc : 68.13

2019-03-12 23:13:11,495 : ***** Transfer task : MPQA *****


2019-03-12 23:13:11,501 : loading BERT model bert-large-uncased
2019-03-12 23:13:11,501 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:13:11,520 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:13:11,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6v7efkac
2019-03-12 23:13:18,994 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:13:24,517 : Generating sentence embeddings
2019-03-12 23:13:32,004 : Generated sentence embeddings
2019-03-12 23:13:32,005 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:13:43,859 : Best param found at split 1: l2reg = 0.001                 with score 86.39
2019-03-12 23:13:55,577 : Best param found at split 2: l2reg = 0.01                 with score 86.1
2019-03-12 23:14:06,862 : Best param found at split 3: l2reg = 1e-05                 with score 86.15
2019-03-12 23:14:17,390 : Best param found at split 4: l2reg = 0.001                 with score 85.87
2019-03-12 23:14:30,449 : Best param found at split 5: l2reg = 0.0001                 with score 86.52
2019-03-12 23:14:31,049 : Dev acc : 86.21 Test acc : 84.72

2019-03-12 23:14:31,050 : ***** Transfer task : SUBJ *****


2019-03-12 23:14:31,067 : loading BERT model bert-large-uncased
2019-03-12 23:14:31,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:14:31,086 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:14:31,087 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn5urt4tp
2019-03-12 23:14:38,558 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:14:44,150 : Generating sentence embeddings
2019-03-12 23:15:14,680 : Generated sentence embeddings
2019-03-12 23:15:14,681 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:15:25,619 : Best param found at split 1: l2reg = 1e-05                 with score 90.12
2019-03-12 23:15:36,912 : Best param found at split 2: l2reg = 1e-05                 with score 91.01
2019-03-12 23:15:49,083 : Best param found at split 3: l2reg = 0.01                 with score 90.76
2019-03-12 23:16:01,008 : Best param found at split 4: l2reg = 0.001                 with score 91.15
2019-03-12 23:16:12,646 : Best param found at split 5: l2reg = 0.001                 with score 91.11
2019-03-12 23:16:13,234 : Dev acc : 90.83 Test acc : 89.47

2019-03-12 23:16:13,235 : ***** Transfer task : SST Binary classification *****


2019-03-12 23:16:13,364 : loading BERT model bert-large-uncased
2019-03-12 23:16:13,364 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:16:13,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:16:13,389 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaz5olibv
2019-03-12 23:16:20,782 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:16:26,392 : Computing embedding for train
2019-03-12 23:18:05,218 : Computed train embeddings
2019-03-12 23:18:05,218 : Computing embedding for dev
2019-03-12 23:18:07,370 : Computed dev embeddings
2019-03-12 23:18:07,370 : Computing embedding for test
2019-03-12 23:18:11,893 : Computed test embeddings
2019-03-12 23:18:11,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:18:29,503 : [('reg:1e-05', 77.64), ('reg:0.0001', 77.75), ('reg:0.001', 76.95), ('reg:0.01', 75.46)]
2019-03-12 23:18:29,503 : Validation : best param found is reg = 0.0001 with score             77.75
2019-03-12 23:18:29,503 : Evaluating...
2019-03-12 23:18:32,926 : 
Dev acc : 77.75 Test acc : 78.91 for             SST Binary classification

2019-03-12 23:18:32,927 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 23:18:32,976 : loading BERT model bert-large-uncased
2019-03-12 23:18:32,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:18:32,998 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:18:32,998 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpomuh5av1
2019-03-12 23:18:40,458 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:18:46,020 : Computing embedding for train
2019-03-12 23:19:07,658 : Computed train embeddings
2019-03-12 23:19:07,658 : Computing embedding for dev
2019-03-12 23:19:10,479 : Computed dev embeddings
2019-03-12 23:19:10,480 : Computing embedding for test
2019-03-12 23:19:16,042 : Computed test embeddings
2019-03-12 23:19:16,043 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:19:18,145 : [('reg:1e-05', 37.33), ('reg:0.0001', 33.51), ('reg:0.001', 26.79), ('reg:0.01', 33.88)]
2019-03-12 23:19:18,145 : Validation : best param found is reg = 1e-05 with score             37.33
2019-03-12 23:19:18,145 : Evaluating...
2019-03-12 23:19:18,746 : 
Dev acc : 37.33 Test acc : 38.55 for             SST Fine-Grained classification

2019-03-12 23:19:18,746 : ***** Transfer task : TREC *****


2019-03-12 23:19:18,760 : loading BERT model bert-large-uncased
2019-03-12 23:19:18,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:19:18,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:19:18,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppb03dytl
2019-03-12 23:19:26,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:19:39,129 : Computed train embeddings
2019-03-12 23:19:39,711 : Computed test embeddings
2019-03-12 23:19:39,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:19:46,903 : [('reg:1e-05', 59.1), ('reg:0.0001', 59.3), ('reg:0.001', 58.73), ('reg:0.01', 55.21)]
2019-03-12 23:19:46,904 : Cross-validation : best param found is reg = 0.0001             with score 59.3
2019-03-12 23:19:46,904 : Evaluating...
2019-03-12 23:19:47,246 : 
Dev acc : 59.3 Test acc : 79.4             for TREC

2019-03-12 23:19:47,247 : ***** Transfer task : MRPC *****


2019-03-12 23:19:47,267 : loading BERT model bert-large-uncased
2019-03-12 23:19:47,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:19:47,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:19:47,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpicip8vao
2019-03-12 23:19:54,776 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:20:00,118 : Computing embedding for train
2019-03-12 23:20:22,102 : Computed train embeddings
2019-03-12 23:20:22,102 : Computing embedding for test
2019-03-12 23:20:31,722 : Computed test embeddings
2019-03-12 23:20:31,743 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:20:36,872 : [('reg:1e-05', 70.83), ('reg:0.0001', 72.03), ('reg:0.001', 71.2), ('reg:0.01', 71.54)]
2019-03-12 23:20:36,872 : Cross-validation : best param found is reg = 0.0001             with score 72.03
2019-03-12 23:20:36,872 : Evaluating...
2019-03-12 23:20:37,152 : Dev acc : 72.03 Test acc 72.0; Test F1 81.59 for MRPC.

2019-03-12 23:20:37,152 : ***** Transfer task : SICK-Entailment*****


2019-03-12 23:20:37,213 : loading BERT model bert-large-uncased
2019-03-12 23:20:37,214 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:20:37,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:20:37,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp73h_d70l
2019-03-12 23:20:44,705 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:20:50,167 : Computing embedding for train
2019-03-12 23:21:01,344 : Computed train embeddings
2019-03-12 23:21:01,344 : Computing embedding for dev
2019-03-12 23:21:02,868 : Computed dev embeddings
2019-03-12 23:21:02,868 : Computing embedding for test
2019-03-12 23:21:14,849 : Computed test embeddings
2019-03-12 23:21:14,886 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:21:16,308 : [('reg:1e-05', 75.6), ('reg:0.0001', 76.0), ('reg:0.001', 70.6), ('reg:0.01', 77.2)]
2019-03-12 23:21:16,309 : Validation : best param found is reg = 0.01 with score             77.2
2019-03-12 23:21:16,309 : Evaluating...
2019-03-12 23:21:16,629 : 
Dev acc : 77.2 Test acc : 75.5 for                        SICK entailment

2019-03-12 23:21:16,629 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 23:21:16,656 : loading BERT model bert-large-uncased
2019-03-12 23:21:16,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:21:16,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:21:16,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzxuwz7we
2019-03-12 23:21:24,168 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:21:29,628 : Computing embedding for train
2019-03-12 23:21:40,799 : Computed train embeddings
2019-03-12 23:21:40,799 : Computing embedding for dev
2019-03-12 23:21:42,323 : Computed dev embeddings
2019-03-12 23:21:42,323 : Computing embedding for test
2019-03-12 23:21:54,322 : Computed test embeddings
2019-03-12 23:22:09,354 : Dev : Pearson 0.7921241827152568
2019-03-12 23:22:09,354 : Test : Pearson 0.7910273793002863 Spearman 0.7267509338930332 MSE 0.3858494033417923                        for SICK Relatedness

2019-03-12 23:22:09,355 : 

***** Transfer task : STSBenchmark*****


2019-03-12 23:22:09,426 : loading BERT model bert-large-uncased
2019-03-12 23:22:09,426 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:22:09,446 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:22:09,446 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8weix15c
2019-03-12 23:22:16,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:22:22,340 : Computing embedding for train
2019-03-12 23:22:40,721 : Computed train embeddings
2019-03-12 23:22:40,721 : Computing embedding for dev
2019-03-12 23:22:46,292 : Computed dev embeddings
2019-03-12 23:22:46,292 : Computing embedding for test
2019-03-12 23:22:50,842 : Computed test embeddings
2019-03-12 23:23:09,642 : Dev : Pearson 0.6668681238231111
2019-03-12 23:23:09,642 : Test : Pearson 0.6566793636640941 Spearman 0.6551008647960186 MSE 1.4226105627481729                        for SICK Relatedness

2019-03-12 23:23:09,643 : ***** Transfer task : SNLI Entailment*****


2019-03-12 23:23:14,676 : loading BERT model bert-large-uncased
2019-03-12 23:23:14,677 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:23:14,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:23:14,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp637fs0pp
2019-03-12 23:23:22,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:23:28,117 : PROGRESS (encoding): 0.00%
2019-03-12 23:26:11,752 : PROGRESS (encoding): 14.56%
2019-03-12 23:29:17,406 : PROGRESS (encoding): 29.12%
2019-03-12 23:32:23,522 : PROGRESS (encoding): 43.69%
2019-03-12 23:35:42,050 : PROGRESS (encoding): 58.25%
2019-03-12 23:39:23,073 : PROGRESS (encoding): 72.81%
2019-03-12 23:43:02,903 : PROGRESS (encoding): 87.37%
2019-03-12 23:47:00,944 : PROGRESS (encoding): 0.00%
2019-03-12 23:47:30,879 : PROGRESS (encoding): 0.00%
2019-03-12 23:47:59,680 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:48:54,302 : [('reg:1e-09', 62.87)]
2019-03-12 23:48:54,302 : Validation : best param found is reg = 1e-09 with score             62.87
2019-03-12 23:48:54,302 : Evaluating...
2019-03-12 23:49:49,174 : Dev acc : 62.87 Test acc : 62.85 for SNLI

2019-03-12 23:49:49,175 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 23:49:49,383 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 23:49:50,433 : loading BERT model bert-large-uncased
2019-03-12 23:49:50,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:49:50,459 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:49:50,459 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpppt_t0vt
2019-03-12 23:49:57,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:50:03,506 : Computing embeddings for train/dev/test
2019-03-12 23:53:31,208 : Computed embeddings
2019-03-12 23:53:31,208 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:54:04,066 : [('reg:1e-05', 67.94), ('reg:0.0001', 69.78), ('reg:0.001', 62.58), ('reg:0.01', 53.74)]
2019-03-12 23:54:04,066 : Validation : best param found is reg = 0.0001 with score             69.78
2019-03-12 23:54:04,066 : Evaluating...
2019-03-12 23:54:10,986 : 
Dev acc : 69.8 Test acc : 69.3 for LENGTH classification

2019-03-12 23:54:10,987 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 23:54:11,240 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 23:54:11,288 : loading BERT model bert-large-uncased
2019-03-12 23:54:11,288 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:54:11,317 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:54:11,317 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfwvwqs2m
2019-03-12 23:54:18,775 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:54:24,330 : Computing embeddings for train/dev/test
2019-03-12 23:57:36,009 : Computed embeddings
2019-03-12 23:57:36,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:58:12,738 : [('reg:1e-05', 32.3), ('reg:0.0001', 11.35), ('reg:0.001', 0.51), ('reg:0.01', 0.16)]
2019-03-12 23:58:12,738 : Validation : best param found is reg = 1e-05 with score             32.3
2019-03-12 23:58:12,738 : Evaluating...
2019-03-12 23:58:27,030 : 
Dev acc : 32.3 Test acc : 31.8 for WORDCONTENT classification

2019-03-12 23:58:27,031 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 23:58:27,542 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 23:58:27,607 : loading BERT model bert-large-uncased
2019-03-12 23:58:27,608 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:58:27,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:58:27,631 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppde5joki
2019-03-12 23:58:35,074 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:58:40,606 : Computing embeddings for train/dev/test
2019-03-13 00:01:40,531 : Computed embeddings
2019-03-13 00:01:40,531 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:01:58,318 : [('reg:1e-05', 23.28), ('reg:0.0001', 29.68), ('reg:0.001', 28.69), ('reg:0.01', 25.12)]
2019-03-13 00:01:58,318 : Validation : best param found is reg = 0.0001 with score             29.68
2019-03-13 00:01:58,318 : Evaluating...
2019-03-13 00:02:04,276 : 
Dev acc : 29.7 Test acc : 29.5 for DEPTH classification

2019-03-13 00:02:04,277 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 00:02:04,689 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 00:02:04,754 : loading BERT model bert-large-uncased
2019-03-13 00:02:04,754 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:02:04,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:02:04,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ja1r60d
2019-03-13 00:02:12,229 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:02:17,704 : Computing embeddings for train/dev/test
2019-03-13 00:05:04,864 : Computed embeddings
2019-03-13 00:05:04,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:05:38,226 : [('reg:1e-05', 55.67), ('reg:0.0001', 55.33), ('reg:0.001', 59.37), ('reg:0.01', 31.81)]
2019-03-13 00:05:38,226 : Validation : best param found is reg = 0.001 with score             59.37
2019-03-13 00:05:38,226 : Evaluating...
2019-03-13 00:05:48,063 : 
Dev acc : 59.4 Test acc : 59.6 for TOPCONSTITUENTS classification

2019-03-13 00:05:48,064 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 00:05:48,417 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 00:05:48,482 : loading BERT model bert-large-uncased
2019-03-13 00:05:48,482 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:05:48,511 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:05:48,511 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp9je3_jn
2019-03-13 00:05:55,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:06:01,489 : Computing embeddings for train/dev/test
2019-03-13 00:09:02,699 : Computed embeddings
2019-03-13 00:09:02,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:09:33,215 : [('reg:1e-05', 68.21), ('reg:0.0001', 68.13), ('reg:0.001', 67.7), ('reg:0.01', 67.94)]
2019-03-13 00:09:33,215 : Validation : best param found is reg = 1e-05 with score             68.21
2019-03-13 00:09:33,215 : Evaluating...
2019-03-13 00:09:40,838 : 
Dev acc : 68.2 Test acc : 67.4 for BIGRAMSHIFT classification

2019-03-13 00:09:40,839 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 00:09:41,227 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 00:09:41,291 : loading BERT model bert-large-uncased
2019-03-13 00:09:41,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:09:41,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:09:41,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaj_9n201
2019-03-13 00:09:48,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:09:54,291 : Computing embeddings for train/dev/test
2019-03-13 00:12:51,664 : Computed embeddings
2019-03-13 00:12:51,664 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:13:19,464 : [('reg:1e-05', 86.35), ('reg:0.0001', 86.37), ('reg:0.001', 86.35), ('reg:0.01', 85.82)]
2019-03-13 00:13:19,464 : Validation : best param found is reg = 0.0001 with score             86.37
2019-03-13 00:13:19,464 : Evaluating...
2019-03-13 00:13:27,028 : 
Dev acc : 86.4 Test acc : 84.7 for TENSE classification

2019-03-13 00:13:27,029 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 00:13:27,430 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 00:13:27,492 : loading BERT model bert-large-uncased
2019-03-13 00:13:27,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:13:27,604 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:13:27,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp__h7slqo
2019-03-13 00:13:35,041 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:13:40,538 : Computing embeddings for train/dev/test
2019-03-13 00:16:48,314 : Computed embeddings
2019-03-13 00:16:48,314 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:17:17,570 : [('reg:1e-05', 73.29), ('reg:0.0001', 73.26), ('reg:0.001', 72.65), ('reg:0.01', 61.84)]
2019-03-13 00:17:17,571 : Validation : best param found is reg = 1e-05 with score             73.29
2019-03-13 00:17:17,571 : Evaluating...
2019-03-13 00:17:25,150 : 
Dev acc : 73.3 Test acc : 71.0 for SUBJNUMBER classification

2019-03-13 00:17:25,151 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 00:17:25,743 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 00:17:25,809 : loading BERT model bert-large-uncased
2019-03-13 00:17:25,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:17:25,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:17:25,836 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbe63krab
2019-03-13 00:17:33,285 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:17:38,777 : Computing embeddings for train/dev/test
2019-03-13 00:20:43,377 : Computed embeddings
2019-03-13 00:20:43,377 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:21:14,868 : [('reg:1e-05', 72.77), ('reg:0.0001', 72.57), ('reg:0.001', 72.68), ('reg:0.01', 70.7)]
2019-03-13 00:21:14,868 : Validation : best param found is reg = 1e-05 with score             72.77
2019-03-13 00:21:14,868 : Evaluating...
2019-03-13 00:21:22,567 : 
Dev acc : 72.8 Test acc : 72.4 for OBJNUMBER classification

2019-03-13 00:21:22,568 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 00:21:22,944 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 00:21:23,011 : loading BERT model bert-large-uncased
2019-03-13 00:21:23,011 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:21:23,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:21:23,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp99wmxg1h
2019-03-13 00:21:30,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:21:36,199 : Computing embeddings for train/dev/test
2019-03-13 00:25:10,053 : Computed embeddings
2019-03-13 00:25:10,053 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:25:41,007 : [('reg:1e-05', 55.45), ('reg:0.0001', 55.47), ('reg:0.001', 55.45), ('reg:0.01', 50.07)]
2019-03-13 00:25:41,008 : Validation : best param found is reg = 0.0001 with score             55.47
2019-03-13 00:25:41,008 : Evaluating...
2019-03-13 00:25:49,662 : 
Dev acc : 55.5 Test acc : 54.9 for ODDMANOUT classification

2019-03-13 00:25:49,663 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 00:25:50,106 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 00:25:50,182 : loading BERT model bert-large-uncased
2019-03-13 00:25:50,182 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:25:50,211 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:25:50,212 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpixeg775n
2019-03-13 00:25:57,714 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:26:03,266 : Computing embeddings for train/dev/test
2019-03-13 00:29:34,906 : Computed embeddings
2019-03-13 00:29:34,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:30:12,599 : [('reg:1e-05', 55.23), ('reg:0.0001', 55.23), ('reg:0.001', 55.09), ('reg:0.01', 50.23)]
2019-03-13 00:30:12,599 : Validation : best param found is reg = 1e-05 with score             55.23
2019-03-13 00:30:12,599 : Evaluating...
2019-03-13 00:30:22,474 : 
Dev acc : 55.2 Test acc : 55.0 for COORDINATIONINVERSION classification

2019-03-13 00:30:22,476 : total results: {'STS12': {'MSRpar': {'pearson': (0.37820500514558836, 6.473584662624685e-27), 'spearman': SpearmanrResult(correlation=0.41563038073268643, pvalue=1.0960568110597702e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7058875805668222, 3.8813058126419045e-114), 'spearman': SpearmanrResult(correlation=0.7111869965331549, pvalue=1.3683980826911655e-116), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4915105882321337, 2.731028647428774e-29), 'spearman': SpearmanrResult(correlation=0.6053185611638527, pvalue=3.17140225947688e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.591721211382017, 4.872680309434287e-72), 'spearman': SpearmanrResult(correlation=0.626761254152589, pvalue=4.2169065837010235e-83), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5354045748778752, 5.6302362116578494e-31), 'spearman': SpearmanrResult(correlation=0.4607532227370166, pvalue=2.3077527779044986e-22), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5405457920408874, 'wmean': 0.545717546073244}, 'spearman': {'mean': 0.5639300830638598, 'wmean': 0.5717071200161199}}}, 'STS13': {'FNWN': {'pearson': (0.20585724980562453, 0.004486020140474202), 'spearman': SpearmanrResult(correlation=0.2015859099138633, pvalue=0.0054095527169125725), 'nsamples': 189}, 'headlines': {'pearson': (0.6793859342113319, 1.1907694546569814e-102), 'spearman': SpearmanrResult(correlation=0.6762413910753402, pvalue=2.2840580549824727e-101), 'nsamples': 750}, 'OnWN': {'pearson': (0.6417604696041355, 1.950523261592251e-66), 'spearman': SpearmanrResult(correlation=0.642428962326689, pvalue=1.2953761648967465e-66), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5090012178736972, 'wmean': 0.6056493962131212}, 'spearman': {'mean': 0.5067520877719641, 'wmean': 0.6037889520969986}}}, 'STS14': {'deft-forum': {'pearson': (0.4160973656484918, 2.8673816198530566e-20), 'spearman': SpearmanrResult(correlation=0.4294060517614608, pvalue=1.2917053840466644e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7197615122221714, 3.735488040851157e-49), 'spearman': SpearmanrResult(correlation=0.6771643233615156, pvalue=1.3554831777892257e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.6466147672704802, 5.05483633747229e-90), 'spearman': SpearmanrResult(correlation=0.6186635891172089, pvalue=2.0245238694766203e-80), 'nsamples': 750}, 'images': {'pearson': (0.6648745586811834, 7.338901417901935e-97), 'spearman': SpearmanrResult(correlation=0.6579867652599479, pvalue=3.171298905774294e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.7058130163548937, 4.198529624427057e-114), 'spearman': SpearmanrResult(correlation=0.7303048953035314, pvalue=6.385964697213494e-126), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6314085111883588, 1.12219023695934e-84), 'spearman': SpearmanrResult(correlation=0.6082569970270216, pvalue=4.356938165226132e-77), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6307616218942632, 'wmean': 0.637254775554576}, 'spearman': {'mean': 0.6202971036384478, 'wmean': 0.6287443214218386}}}, 'STS15': {'answers-forums': {'pearson': (0.5278732049682796, 2.719811812534544e-28), 'spearman': SpearmanrResult(correlation=0.5036000855391283, pvalue=1.6525623524669619e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.7251958336295953, 2.3776570853128326e-123), 'spearman': SpearmanrResult(correlation=0.7308822362801607, pvalue=3.243326289398351e-126), 'nsamples': 750}, 'belief': {'pearson': (0.5727772096343406, 4.476620837296939e-34), 'spearman': SpearmanrResult(correlation=0.5997822637838478, pvalue=5.264812549024798e-38), 'nsamples': 375}, 'headlines': {'pearson': (0.6892449890185532, 8.856550262617847e-107), 'spearman': SpearmanrResult(correlation=0.6913415789366903, pvalue=1.1164972351243579e-107), 'nsamples': 750}, 'images': {'pearson': (0.7740332480012704, 1.2590801901493346e-150), 'spearman': SpearmanrResult(correlation=0.7806385781161802, pvalue=7.608503522920693e-155), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6578248970504078, 'wmean': 0.6846998194876823}, 'spearman': {'mean': 0.6612489485312014, 'wmean': 0.6886383919986299}}}, 'STS16': {'answer-answer': {'pearson': (0.46885125506949293, 2.756409439667934e-15), 'spearman': SpearmanrResult(correlation=0.4727330774652287, pvalue=1.5132701129598105e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.675517445060586, 1.5416627623210626e-34), 'spearman': SpearmanrResult(correlation=0.687248833953655, pvalue=3.800167262890894e-36), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7111451434868798, 9.5696340475014e-37), 'spearman': SpearmanrResult(correlation=0.720444356704168, pvalue=4.203174277602423e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.7980650845629724, 3.703158643723191e-55), 'spearman': SpearmanrResult(correlation=0.8352993946639024, pvalue=8.234361781009978e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.5187168968633679, 8.596882284081318e-16), 'spearman': SpearmanrResult(correlation=0.5256163218778427, pvalue=3.04436320791541e-16), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6344591650086598, 'wmean': 0.6357463386910022}, 'spearman': {'mean': 0.6482683969329595, 'wmean': 0.6497202588052682}}}, 'MR': {'devacc': 62.48, 'acc': 62.19, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.81, 'acc': 68.13, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.21, 'acc': 84.72, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.83, 'acc': 89.47, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.75, 'acc': 78.91, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.33, 'acc': 38.55, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 59.3, 'acc': 79.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.03, 'acc': 72.0, 'f1': 81.59, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.2, 'acc': 75.5, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7921241827152568, 'pearson': 0.7910273793002863, 'spearman': 0.7267509338930332, 'mse': 0.3858494033417923, 'yhat': array([2.82130382, 4.89404705, 1.30654192, ..., 3.00790228, 4.73616448,        4.91614694]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6668681238231111, 'pearson': 0.6566793636640941, 'spearman': 0.6551008647960186, 'mse': 1.4226105627481729, 'yhat': array([1.48333743, 1.68876012, 2.06087949, ..., 4.61859061, 3.99808016,        3.73755553]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.87, 'acc': 62.85, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 69.78, 'acc': 69.34, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 32.3, 'acc': 31.81, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.68, 'acc': 29.51, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.37, 'acc': 59.63, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 68.21, 'acc': 67.4, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.37, 'acc': 84.72, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.29, 'acc': 71.04, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.77, 'acc': 72.37, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.47, 'acc': 54.89, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.23, 'acc': 55.03, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 00:30:22,476 : STS12 p=0.5457, STS12 s=0.5717, STS13 p=0.6056, STS13 s=0.6038, STS14 p=0.6373, STS14 s=0.6287, STS15 p=0.6847, STS15 s=0.6886, STS 16 p=0.6357, STS16 s=0.6497, STS B p=0.6567, STS B s=0.6551, STS B m=1.4226, SICK-R p=0.7910, SICK-R s=0.7268, SICK-P m=0.3858
2019-03-13 00:30:22,476 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 00:30:22,476 : 0.5457,0.5717,0.6056,0.6038,0.6373,0.6287,0.6847,0.6886,0.6357,0.6497,0.6567,0.6551,1.4226,0.7910,0.7268,0.3858
2019-03-13 00:30:22,476 : MR=62.19, CR=68.13, SUBJ=89.47, MPQA=84.72, SST-B=78.91, SST-F=38.55, TREC=79.40, SICK-E=75.50, SNLI=62.85, MRPC=72.00, MRPC f=81.59
2019-03-13 00:30:22,476 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 00:30:22,476 : 62.19,68.13,89.47,84.72,78.91,38.55,79.40,75.50,62.85,72.00,81.59
2019-03-13 00:30:22,476 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 00:30:22,476 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 00:30:22,476 : na,na,na,na,na,na,na,na,na,na
2019-03-13 00:30:22,476 : SentLen=69.34, WC=31.81, TreeDepth=29.51, TopConst=59.63, BShift=67.40, Tense=84.72, SubjNum=71.04, ObjNum=72.37, SOMO=54.89, CoordInv=55.03, average=59.57
2019-03-13 00:30:22,476 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 00:30:22,476 : 69.34,31.81,29.51,59.63,67.40,84.72,71.04,72.37,54.89,55.03,59.57
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,476 : layer 8
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,476 : ********************************************************************************
2019-03-13 00:30:22,569 : ***** Transfer task : STS12 *****


2019-03-13 00:30:22,581 : loading BERT model bert-large-uncased
2019-03-13 00:30:22,581 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:30:22,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:30:22,599 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptzz8u5fo
2019-03-13 00:30:30,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:30:39,728 : MSRpar : pearson = 0.3869, spearman = 0.4202
2019-03-13 00:30:41,349 : MSRvid : pearson = 0.7497, spearman = 0.7517
2019-03-13 00:30:42,744 : SMTeuroparl : pearson = 0.5022, spearman = 0.6125
2019-03-13 00:30:45,408 : surprise.OnWN : pearson = 0.6148, spearman = 0.6462
2019-03-13 00:30:46,820 : surprise.SMTnews : pearson = 0.5368, spearman = 0.4732
2019-03-13 00:30:46,820 : ALL (weighted average) : Pearson = 0.5657,             Spearman = 0.5899
2019-03-13 00:30:46,820 : ALL (average) : Pearson = 0.5581,             Spearman = 0.5808

2019-03-13 00:30:46,820 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 00:30:46,829 : loading BERT model bert-large-uncased
2019-03-13 00:30:46,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:30:46,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:30:46,846 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpir8j_r4n
2019-03-13 00:30:54,284 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:31:01,099 : FNWN : pearson = 0.2362, spearman = 0.2234
2019-03-13 00:31:02,970 : headlines : pearson = 0.6916, spearman = 0.6853
2019-03-13 00:31:04,418 : OnWN : pearson = 0.6605, spearman = 0.6596
2019-03-13 00:31:04,418 : ALL (weighted average) : Pearson = 0.6226,             Spearman = 0.6175
2019-03-13 00:31:04,418 : ALL (average) : Pearson = 0.5295,             Spearman = 0.5228

2019-03-13 00:31:04,418 : ***** Transfer task : STS14 *****


2019-03-13 00:31:04,433 : loading BERT model bert-large-uncased
2019-03-13 00:31:04,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:31:04,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:31:04,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnkg4fcpd
2019-03-13 00:31:11,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:31:18,736 : deft-forum : pearson = 0.4341, spearman = 0.4476
2019-03-13 00:31:20,354 : deft-news : pearson = 0.7301, spearman = 0.6820
2019-03-13 00:31:22,499 : headlines : pearson = 0.6500, spearman = 0.6186
2019-03-13 00:31:24,552 : images : pearson = 0.7043, spearman = 0.6957
2019-03-13 00:31:26,658 : OnWN : pearson = 0.7304, spearman = 0.7526
2019-03-13 00:31:29,484 : tweet-news : pearson = 0.6490, spearman = 0.6231
2019-03-13 00:31:29,484 : ALL (weighted average) : Pearson = 0.6572,             Spearman = 0.6463
2019-03-13 00:31:29,484 : ALL (average) : Pearson = 0.6497,             Spearman = 0.6366

2019-03-13 00:31:29,484 : ***** Transfer task : STS15 *****


2019-03-13 00:31:29,516 : loading BERT model bert-large-uncased
2019-03-13 00:31:29,516 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:31:29,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:31:29,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ko8i0xi
2019-03-13 00:31:36,968 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:31:44,242 : answers-forums : pearson = 0.5827, spearman = 0.5648
2019-03-13 00:31:46,302 : answers-students : pearson = 0.7259, spearman = 0.7283
2019-03-13 00:31:48,324 : belief : pearson = 0.6067, spearman = 0.6327
2019-03-13 00:31:50,545 : headlines : pearson = 0.6981, spearman = 0.6994
2019-03-13 00:31:52,655 : images : pearson = 0.7978, spearman = 0.8034
2019-03-13 00:31:52,655 : ALL (weighted average) : Pearson = 0.7041,             Spearman = 0.7075
2019-03-13 00:31:52,655 : ALL (average) : Pearson = 0.6822,             Spearman = 0.6857

2019-03-13 00:31:52,655 : ***** Transfer task : STS16 *****


2019-03-13 00:31:52,724 : loading BERT model bert-large-uncased
2019-03-13 00:31:52,724 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:31:52,741 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:31:52,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6upgh1n9
2019-03-13 00:32:00,229 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:32:06,599 : answer-answer : pearson = 0.5195, spearman = 0.5210
2019-03-13 00:32:07,253 : headlines : pearson = 0.6897, spearman = 0.7008
2019-03-13 00:32:08,124 : plagiarism : pearson = 0.7513, spearman = 0.7683
2019-03-13 00:32:09,599 : postediting : pearson = 0.8126, spearman = 0.8412
2019-03-13 00:32:10,198 : question-question : pearson = 0.5912, spearman = 0.5993
2019-03-13 00:32:10,198 : ALL (weighted average) : Pearson = 0.6731,             Spearman = 0.6864
2019-03-13 00:32:10,198 : ALL (average) : Pearson = 0.6729,             Spearman = 0.6861

2019-03-13 00:32:10,198 : ***** Transfer task : MR *****


2019-03-13 00:32:10,213 : loading BERT model bert-large-uncased
2019-03-13 00:32:10,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:32:10,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:32:10,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps800z2b7
2019-03-13 00:32:17,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:32:23,303 : Generating sentence embeddings
2019-03-13 00:32:54,476 : Generated sentence embeddings
2019-03-13 00:32:54,477 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:33:05,389 : Best param found at split 1: l2reg = 1e-05                 with score 61.88
2019-03-13 00:33:19,829 : Best param found at split 2: l2reg = 1e-05                 with score 63.89
2019-03-13 00:33:33,329 : Best param found at split 3: l2reg = 1e-05                 with score 64.5
2019-03-13 00:33:45,472 : Best param found at split 4: l2reg = 1e-05                 with score 62.84
2019-03-13 00:33:56,830 : Best param found at split 5: l2reg = 0.0001                 with score 60.7
2019-03-13 00:33:57,344 : Dev acc : 62.76 Test acc : 61.84

2019-03-13 00:33:57,345 : ***** Transfer task : CR *****


2019-03-13 00:33:57,352 : loading BERT model bert-large-uncased
2019-03-13 00:33:57,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:33:57,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:33:57,373 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv9jk8qej
2019-03-13 00:34:04,765 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:34:10,275 : Generating sentence embeddings
2019-03-13 00:34:18,490 : Generated sentence embeddings
2019-03-13 00:34:18,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:34:20,920 : Best param found at split 1: l2reg = 0.0001                 with score 72.9
2019-03-13 00:34:23,626 : Best param found at split 2: l2reg = 0.001                 with score 71.61
2019-03-13 00:34:26,269 : Best param found at split 3: l2reg = 0.001                 with score 70.6
2019-03-13 00:34:29,319 : Best param found at split 4: l2reg = 0.001                 with score 72.66
2019-03-13 00:34:33,019 : Best param found at split 5: l2reg = 0.01                 with score 74.28
2019-03-13 00:34:33,307 : Dev acc : 72.41 Test acc : 64.88

2019-03-13 00:34:33,307 : ***** Transfer task : MPQA *****


2019-03-13 00:34:33,315 : loading BERT model bert-large-uncased
2019-03-13 00:34:33,315 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:34:33,377 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:34:33,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo1ogblf3
2019-03-13 00:34:40,792 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:34:46,270 : Generating sentence embeddings
2019-03-13 00:34:53,768 : Generated sentence embeddings
2019-03-13 00:34:53,768 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:35:04,511 : Best param found at split 1: l2reg = 0.01                 with score 84.9
2019-03-13 00:35:15,761 : Best param found at split 2: l2reg = 0.001                 with score 85.44
2019-03-13 00:35:28,531 : Best param found at split 3: l2reg = 1e-05                 with score 85.49
2019-03-13 00:35:39,566 : Best param found at split 4: l2reg = 1e-05                 with score 85.62
2019-03-13 00:35:52,254 : Best param found at split 5: l2reg = 0.01                 with score 86.17
2019-03-13 00:35:52,697 : Dev acc : 85.52 Test acc : 82.66

2019-03-13 00:35:52,698 : ***** Transfer task : SUBJ *****


2019-03-13 00:35:52,714 : loading BERT model bert-large-uncased
2019-03-13 00:35:52,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:35:52,733 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:35:52,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpljo5cvt3
2019-03-13 00:36:00,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:36:05,835 : Generating sentence embeddings
2019-03-13 00:36:36,319 : Generated sentence embeddings
2019-03-13 00:36:36,320 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:36:47,266 : Best param found at split 1: l2reg = 0.0001                 with score 89.92
2019-03-13 00:36:58,628 : Best param found at split 2: l2reg = 0.001                 with score 91.76
2019-03-13 00:37:09,824 : Best param found at split 3: l2reg = 0.001                 with score 91.07
2019-03-13 00:37:20,099 : Best param found at split 4: l2reg = 1e-05                 with score 91.48
2019-03-13 00:37:31,395 : Best param found at split 5: l2reg = 1e-05                 with score 91.34
2019-03-13 00:37:32,245 : Dev acc : 91.11 Test acc : 91.52

2019-03-13 00:37:32,246 : ***** Transfer task : SST Binary classification *****


2019-03-13 00:37:32,340 : loading BERT model bert-large-uncased
2019-03-13 00:37:32,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:37:32,417 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:37:32,417 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphv2mcdcn
2019-03-13 00:37:39,833 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:37:45,381 : Computing embedding for train
2019-03-13 00:39:24,101 : Computed train embeddings
2019-03-13 00:39:24,101 : Computing embedding for dev
2019-03-13 00:39:26,250 : Computed dev embeddings
2019-03-13 00:39:26,250 : Computing embedding for test
2019-03-13 00:39:30,770 : Computed test embeddings
2019-03-13 00:39:30,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:39:45,402 : [('reg:1e-05', 76.26), ('reg:0.0001', 76.26), ('reg:0.001', 75.92), ('reg:0.01', 75.92)]
2019-03-13 00:39:45,402 : Validation : best param found is reg = 1e-05 with score             76.26
2019-03-13 00:39:45,402 : Evaluating...
2019-03-13 00:39:49,028 : 
Dev acc : 76.26 Test acc : 76.77 for             SST Binary classification

2019-03-13 00:39:49,029 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 00:39:49,082 : loading BERT model bert-large-uncased
2019-03-13 00:39:49,082 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:39:49,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:39:49,103 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyhjkgtsv
2019-03-13 00:39:56,574 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:40:02,000 : Computing embedding for train
2019-03-13 00:40:23,645 : Computed train embeddings
2019-03-13 00:40:23,645 : Computing embedding for dev
2019-03-13 00:40:26,467 : Computed dev embeddings
2019-03-13 00:40:26,468 : Computing embedding for test
2019-03-13 00:40:32,034 : Computed test embeddings
2019-03-13 00:40:32,034 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:40:34,785 : [('reg:1e-05', 35.79), ('reg:0.0001', 27.97), ('reg:0.001', 29.43), ('reg:0.01', 35.24)]
2019-03-13 00:40:34,785 : Validation : best param found is reg = 1e-05 with score             35.79
2019-03-13 00:40:34,785 : Evaluating...
2019-03-13 00:40:35,530 : 
Dev acc : 35.79 Test acc : 38.14 for             SST Fine-Grained classification

2019-03-13 00:40:35,531 : ***** Transfer task : TREC *****


2019-03-13 00:40:35,545 : loading BERT model bert-large-uncased
2019-03-13 00:40:35,545 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:40:35,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:40:35,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo8gjf08e
2019-03-13 00:40:43,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:40:56,056 : Computed train embeddings
2019-03-13 00:40:56,638 : Computed test embeddings
2019-03-13 00:40:56,639 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:41:03,758 : [('reg:1e-05', 59.72), ('reg:0.0001', 61.49), ('reg:0.001', 58.09), ('reg:0.01', 59.21)]
2019-03-13 00:41:03,758 : Cross-validation : best param found is reg = 0.0001             with score 61.49
2019-03-13 00:41:03,759 : Evaluating...
2019-03-13 00:41:04,217 : 
Dev acc : 61.49 Test acc : 75.6             for TREC

2019-03-13 00:41:04,218 : ***** Transfer task : MRPC *****


2019-03-13 00:41:04,238 : loading BERT model bert-large-uncased
2019-03-13 00:41:04,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:41:04,261 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:41:04,261 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphw8if5o7
2019-03-13 00:41:11,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:41:17,013 : Computing embedding for train
2019-03-13 00:41:38,993 : Computed train embeddings
2019-03-13 00:41:38,993 : Computing embedding for test
2019-03-13 00:41:48,616 : Computed test embeddings
2019-03-13 00:41:48,637 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:41:53,673 : [('reg:1e-05', 71.69), ('reg:0.0001', 70.04), ('reg:0.001', 69.5), ('reg:0.01', 69.75)]
2019-03-13 00:41:53,673 : Cross-validation : best param found is reg = 1e-05             with score 71.69
2019-03-13 00:41:53,673 : Evaluating...
2019-03-13 00:41:53,857 : Dev acc : 71.69 Test acc 71.83; Test F1 80.76 for MRPC.

2019-03-13 00:41:53,857 : ***** Transfer task : SICK-Entailment*****


2019-03-13 00:41:53,918 : loading BERT model bert-large-uncased
2019-03-13 00:41:53,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:41:53,937 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:41:53,937 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpslka4use
2019-03-13 00:42:01,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:42:06,875 : Computing embedding for train
2019-03-13 00:42:18,025 : Computed train embeddings
2019-03-13 00:42:18,025 : Computing embedding for dev
2019-03-13 00:42:19,547 : Computed dev embeddings
2019-03-13 00:42:19,547 : Computing embedding for test
2019-03-13 00:42:31,502 : Computed test embeddings
2019-03-13 00:42:31,538 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:42:32,895 : [('reg:1e-05', 76.2), ('reg:0.0001', 76.8), ('reg:0.001', 71.8), ('reg:0.01', 77.6)]
2019-03-13 00:42:32,895 : Validation : best param found is reg = 0.01 with score             77.6
2019-03-13 00:42:32,895 : Evaluating...
2019-03-13 00:42:33,212 : 
Dev acc : 77.6 Test acc : 76.92 for                        SICK entailment

2019-03-13 00:42:33,213 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 00:42:33,239 : loading BERT model bert-large-uncased
2019-03-13 00:42:33,239 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:42:33,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:42:33,295 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplig743_h
2019-03-13 00:42:40,745 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:42:46,199 : Computing embedding for train
2019-03-13 00:42:57,362 : Computed train embeddings
2019-03-13 00:42:57,363 : Computing embedding for dev
2019-03-13 00:42:58,884 : Computed dev embeddings
2019-03-13 00:42:58,884 : Computing embedding for test
2019-03-13 00:43:10,871 : Computed test embeddings
2019-03-13 00:43:25,791 : Dev : Pearson 0.7955123572523262
2019-03-13 00:43:25,791 : Test : Pearson 0.7977424438428916 Spearman 0.7266337060261728 MSE 0.37233594371221534                        for SICK Relatedness

2019-03-13 00:43:25,792 : 

***** Transfer task : STSBenchmark*****


2019-03-13 00:43:25,830 : loading BERT model bert-large-uncased
2019-03-13 00:43:25,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:43:25,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:43:25,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1torehu0
2019-03-13 00:43:33,337 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:43:38,823 : Computing embedding for train
2019-03-13 00:43:57,198 : Computed train embeddings
2019-03-13 00:43:57,198 : Computing embedding for dev
2019-03-13 00:44:02,760 : Computed dev embeddings
2019-03-13 00:44:02,760 : Computing embedding for test
2019-03-13 00:44:07,301 : Computed test embeddings
2019-03-13 00:44:22,460 : Dev : Pearson 0.6676441963305915
2019-03-13 00:44:22,461 : Test : Pearson 0.6491384366119634 Spearman 0.6472458640349628 MSE 1.4399800524888857                        for SICK Relatedness

2019-03-13 00:44:22,461 : ***** Transfer task : SNLI Entailment*****


2019-03-13 00:44:27,625 : loading BERT model bert-large-uncased
2019-03-13 00:44:27,625 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:44:27,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:44:27,744 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw_sxb7pj
2019-03-13 00:44:35,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:44:41,043 : PROGRESS (encoding): 0.00%
2019-03-13 00:47:24,903 : PROGRESS (encoding): 14.56%
2019-03-13 00:50:30,588 : PROGRESS (encoding): 29.12%
2019-03-13 00:53:36,852 : PROGRESS (encoding): 43.69%
2019-03-13 00:56:55,469 : PROGRESS (encoding): 58.25%
2019-03-13 01:00:36,761 : PROGRESS (encoding): 72.81%
2019-03-13 01:04:16,819 : PROGRESS (encoding): 87.37%
2019-03-13 01:08:15,142 : PROGRESS (encoding): 0.00%
2019-03-13 01:08:45,094 : PROGRESS (encoding): 0.00%
2019-03-13 01:09:13,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:10:07,924 : [('reg:1e-09', 67.06)]
2019-03-13 01:10:07,924 : Validation : best param found is reg = 1e-09 with score             67.06
2019-03-13 01:10:07,924 : Evaluating...
2019-03-13 01:10:56,110 : Dev acc : 67.06 Test acc : 66.87 for SNLI

2019-03-13 01:10:56,110 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 01:10:56,312 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 01:10:57,393 : loading BERT model bert-large-uncased
2019-03-13 01:10:57,393 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:10:57,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:10:57,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp132d1fch
2019-03-13 01:11:04,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:11:10,468 : Computing embeddings for train/dev/test
2019-03-13 01:14:38,230 : Computed embeddings
2019-03-13 01:14:38,230 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:15:02,265 : [('reg:1e-05', 61.48), ('reg:0.0001', 66.13), ('reg:0.001', 58.64), ('reg:0.01', 59.29)]
2019-03-13 01:15:02,265 : Validation : best param found is reg = 0.0001 with score             66.13
2019-03-13 01:15:02,265 : Evaluating...
2019-03-13 01:15:08,921 : 
Dev acc : 66.1 Test acc : 66.9 for LENGTH classification

2019-03-13 01:15:08,922 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 01:15:09,298 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 01:15:09,342 : loading BERT model bert-large-uncased
2019-03-13 01:15:09,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:15:09,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:15:09,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb3uwo2dg
2019-03-13 01:15:16,837 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:15:22,370 : Computing embeddings for train/dev/test
2019-03-13 01:18:34,109 : Computed embeddings
2019-03-13 01:18:34,109 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:19:12,017 : [('reg:1e-05', 47.61), ('reg:0.0001', 12.31), ('reg:0.001', 0.83), ('reg:0.01', 0.21)]
2019-03-13 01:19:12,017 : Validation : best param found is reg = 1e-05 with score             47.61
2019-03-13 01:19:12,017 : Evaluating...
2019-03-13 01:19:25,794 : 
Dev acc : 47.6 Test acc : 47.9 for WORDCONTENT classification

2019-03-13 01:19:25,795 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 01:19:26,177 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 01:19:26,242 : loading BERT model bert-large-uncased
2019-03-13 01:19:26,243 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:19:26,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:19:26,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphk56yk7v
2019-03-13 01:19:33,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:19:39,395 : Computing embeddings for train/dev/test
2019-03-13 01:22:39,391 : Computed embeddings
2019-03-13 01:22:39,391 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:23:12,396 : [('reg:1e-05', 26.35), ('reg:0.0001', 25.73), ('reg:0.001', 26.73), ('reg:0.01', 26.11)]
2019-03-13 01:23:12,396 : Validation : best param found is reg = 0.001 with score             26.73
2019-03-13 01:23:12,396 : Evaluating...
2019-03-13 01:23:21,279 : 
Dev acc : 26.7 Test acc : 26.8 for DEPTH classification

2019-03-13 01:23:21,280 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 01:23:21,698 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 01:23:21,766 : loading BERT model bert-large-uncased
2019-03-13 01:23:21,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:23:21,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:23:21,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn7u58fc2
2019-03-13 01:23:29,562 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:23:35,033 : Computing embeddings for train/dev/test
2019-03-13 01:26:21,719 : Computed embeddings
2019-03-13 01:26:21,719 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:26:56,401 : [('reg:1e-05', 55.13), ('reg:0.0001', 55.52), ('reg:0.001', 49.55), ('reg:0.01', 31.79)]
2019-03-13 01:26:56,401 : Validation : best param found is reg = 0.0001 with score             55.52
2019-03-13 01:26:56,401 : Evaluating...
2019-03-13 01:27:05,752 : 
Dev acc : 55.5 Test acc : 55.8 for TOPCONSTITUENTS classification

2019-03-13 01:27:05,753 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 01:27:06,095 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 01:27:06,161 : loading BERT model bert-large-uncased
2019-03-13 01:27:06,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:27:06,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:27:06,281 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkymuwn3e
2019-03-13 01:27:13,942 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:27:19,502 : Computing embeddings for train/dev/test
2019-03-13 01:30:20,612 : Computed embeddings
2019-03-13 01:30:20,612 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:30:43,903 : [('reg:1e-05', 54.99), ('reg:0.0001', 54.99), ('reg:0.001', 55.11), ('reg:0.01', 62.53)]
2019-03-13 01:30:43,903 : Validation : best param found is reg = 0.01 with score             62.53
2019-03-13 01:30:43,903 : Evaluating...
2019-03-13 01:30:52,207 : 
Dev acc : 62.5 Test acc : 62.2 for BIGRAMSHIFT classification

2019-03-13 01:30:52,208 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 01:30:52,763 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 01:30:52,828 : loading BERT model bert-large-uncased
2019-03-13 01:30:52,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:30:52,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:30:52,858 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppmqo4f5f
2019-03-13 01:31:00,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:31:05,903 : Computing embeddings for train/dev/test
2019-03-13 01:34:03,233 : Computed embeddings
2019-03-13 01:34:03,233 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:34:28,124 : [('reg:1e-05', 85.57), ('reg:0.0001', 85.6), ('reg:0.001', 86.06), ('reg:0.01', 85.6)]
2019-03-13 01:34:28,125 : Validation : best param found is reg = 0.001 with score             86.06
2019-03-13 01:34:28,125 : Evaluating...
2019-03-13 01:34:34,489 : 
Dev acc : 86.1 Test acc : 84.8 for TENSE classification

2019-03-13 01:34:34,490 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 01:34:34,908 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 01:34:34,971 : loading BERT model bert-large-uncased
2019-03-13 01:34:34,971 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:34:34,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:34:34,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4iiwww1i
2019-03-13 01:34:42,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:34:47,899 : Computing embeddings for train/dev/test
2019-03-13 01:37:55,643 : Computed embeddings
2019-03-13 01:37:55,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:38:25,983 : [('reg:1e-05', 77.98), ('reg:0.0001', 77.88), ('reg:0.001', 77.64), ('reg:0.01', 77.2)]
2019-03-13 01:38:25,983 : Validation : best param found is reg = 1e-05 with score             77.98
2019-03-13 01:38:25,983 : Evaluating...
2019-03-13 01:38:34,043 : 
Dev acc : 78.0 Test acc : 76.1 for SUBJNUMBER classification

2019-03-13 01:38:34,044 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 01:38:34,449 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 01:38:34,515 : loading BERT model bert-large-uncased
2019-03-13 01:38:34,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:38:34,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:38:34,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppvjui6hy
2019-03-13 01:38:42,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:38:47,520 : Computing embeddings for train/dev/test
2019-03-13 01:41:51,852 : Computed embeddings
2019-03-13 01:41:51,852 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:42:23,983 : [('reg:1e-05', 75.64), ('reg:0.0001', 75.62), ('reg:0.001', 73.32), ('reg:0.01', 74.49)]
2019-03-13 01:42:23,984 : Validation : best param found is reg = 1e-05 with score             75.64
2019-03-13 01:42:23,984 : Evaluating...
2019-03-13 01:42:32,796 : 
Dev acc : 75.6 Test acc : 76.4 for OBJNUMBER classification

2019-03-13 01:42:32,797 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 01:42:33,385 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 01:42:33,453 : loading BERT model bert-large-uncased
2019-03-13 01:42:33,453 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:42:33,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:42:33,481 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7cipl4sd
2019-03-13 01:42:40,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:42:46,409 : Computing embeddings for train/dev/test
2019-03-13 01:46:20,196 : Computed embeddings
2019-03-13 01:46:20,196 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:46:54,755 : [('reg:1e-05', 58.7), ('reg:0.0001', 58.63), ('reg:0.001', 58.62), ('reg:0.01', 55.56)]
2019-03-13 01:46:54,755 : Validation : best param found is reg = 1e-05 with score             58.7
2019-03-13 01:46:54,755 : Evaluating...
2019-03-13 01:47:04,438 : 
Dev acc : 58.7 Test acc : 58.7 for ODDMANOUT classification

2019-03-13 01:47:04,439 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 01:47:04,837 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 01:47:04,914 : loading BERT model bert-large-uncased
2019-03-13 01:47:04,914 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:47:05,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:47:05,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjjwgfy91
2019-03-13 01:47:12,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:47:17,974 : Computing embeddings for train/dev/test
2019-03-13 01:50:49,479 : Computed embeddings
2019-03-13 01:50:49,479 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:51:23,374 : [('reg:1e-05', 56.19), ('reg:0.0001', 56.21), ('reg:0.001', 50.1), ('reg:0.01', 50.06)]
2019-03-13 01:51:23,374 : Validation : best param found is reg = 0.0001 with score             56.21
2019-03-13 01:51:23,374 : Evaluating...
2019-03-13 01:51:33,070 : 
Dev acc : 56.2 Test acc : 55.9 for COORDINATIONINVERSION classification

2019-03-13 01:51:33,072 : total results: {'STS12': {'MSRpar': {'pearson': (0.38693244355869294, 3.394548623614756e-28), 'spearman': SpearmanrResult(correlation=0.4201811160879592, pvalue=1.9348389716443938e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7497296070975882, 2.9225709648739746e-136), 'spearman': SpearmanrResult(correlation=0.7517069168975955, pvalue=2.289284182617514e-137), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5021520900288177, 1.0811042030019906e-30), 'spearman': SpearmanrResult(correlation=0.6124674285650444, pvalue=1.3280286299541436e-48), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6148330999164388, 3.5284831391775865e-79), 'spearman': SpearmanrResult(correlation=0.6462011328772154, pvalue=7.131541741944499e-90), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5367501784347832, 3.757755495328512e-31), 'spearman': SpearmanrResult(correlation=0.47322522913256465, pvalue=1.1681303995733167e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5580794838072641, 'wmean': 0.5657247404917136}, 'spearman': {'mean': 0.5807563647120759, 'wmean': 0.5899312389100149}}}, 'STS13': {'FNWN': {'pearson': (0.23622424397944605, 0.001066230793988312), 'spearman': SpearmanrResult(correlation=0.22344028648916434, pvalue=0.0019976103924811716), 'nsamples': 189}, 'headlines': {'pearson': (0.6916263716811398, 8.41571096720826e-108), 'spearman': SpearmanrResult(correlation=0.6852536065082638, pvalue=4.349861885222431e-105), 'nsamples': 750}, 'OnWN': {'pearson': (0.6605144854169646, 1.3474966622790924e-71), 'spearman': SpearmanrResult(correlation=0.6595946034565878, pvalue=2.462239098184903e-71), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5294550336925168, 'wmean': 0.6226098581279248}, 'spearman': {'mean': 0.5227628321513387, 'wmean': 0.6174686610445306}}}, 'STS14': {'deft-forum': {'pearson': (0.4341449895162709, 4.1415768774203377e-22), 'spearman': SpearmanrResult(correlation=0.44764387703621755, pvalue=1.467845392639604e-23), 'nsamples': 450}, 'deft-news': {'pearson': (0.7301323345313253, 3.2741849262714734e-51), 'spearman': SpearmanrResult(correlation=0.6820319948209134, pvalue=2.155000927190214e-42), 'nsamples': 300}, 'headlines': {'pearson': (0.6499576375851039, 3.0697470735641025e-91), 'spearman': SpearmanrResult(correlation=0.6185698540497649, pvalue=2.172213930673072e-80), 'nsamples': 750}, 'images': {'pearson': (0.7043001166257128, 2.0561883586542585e-113), 'spearman': SpearmanrResult(correlation=0.6956554292853602, pvalue=1.48873613903278e-109), 'nsamples': 750}, 'OnWN': {'pearson': (0.730440044857467, 5.45026199907956e-126), 'spearman': SpearmanrResult(correlation=0.7525932466100238, pvalue=7.252796336351915e-138), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6489792376839152, 6.994719485068105e-91), 'spearman': SpearmanrResult(correlation=0.6230628817759938, pvalue=7.234267744299593e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6496590601332991, 'wmean': 0.6572433928548983}, 'spearman': {'mean': 0.6365928805963789, 'wmean': 0.6462561071742476}}}, 'STS15': {'answers-forums': {'pearson': (0.5826524380286506, 1.80476263609252e-35), 'spearman': SpearmanrResult(correlation=0.5647780294160726, pvalue=5.5780238972147184e-33), 'nsamples': 375}, 'answers-students': {'pearson': (0.7258725224679986, 1.0939202049575019e-123), 'spearman': SpearmanrResult(correlation=0.728317379101985, pvalue=6.491352536665235e-125), 'nsamples': 750}, 'belief': {'pearson': (0.6067078944348324, 4.4923760934763214e-39), 'spearman': SpearmanrResult(correlation=0.6327330056612946, pvalue=2.458444524300689e-43), 'nsamples': 375}, 'headlines': {'pearson': (0.6981299438009708, 1.2081430817486724e-110), 'spearman': SpearmanrResult(correlation=0.699434256905572, pvalue=3.181956062510825e-111), 'nsamples': 750}, 'images': {'pearson': (0.7977864996392209, 1.6131591326853957e-166), 'spearman': SpearmanrResult(correlation=0.8033901565465363, pvalue=1.3980162371795166e-170), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6822298596743347, 'wmean': 0.704117283034983}, 'spearman': {'mean': 0.6857305655262922, 'wmean': 0.7074743275231943}}}, 'STS16': {'answer-answer': {'pearson': (0.519535180741948, 5.82910269105924e-19), 'spearman': SpearmanrResult(correlation=0.5209740365435575, pvalue=4.48853850703256e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.6897187856740672, 1.7039734479777994e-36), 'spearman': SpearmanrResult(correlation=0.7007849776349816, pvalue=4.2357425144956855e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7513372506306567, 4.8864739038344915e-43), 'spearman': SpearmanrResult(correlation=0.7683182750238581, pvalue=4.520711246356382e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.8126241964378887, 1.128127793420556e-58), 'spearman': SpearmanrResult(correlation=0.8412471711426931, pvalue=1.4118528286177415e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.5911975528868852, 4.345127871740597e-21), 'spearman': SpearmanrResult(correlation=0.5992675135470341, pvalue=9.1930168516528e-22), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6728825932742891, 'wmean': 0.6731450874119329}, 'spearman': {'mean': 0.6861183947784248, 'wmean': 0.6863805126971405}}}, 'MR': {'devacc': 62.76, 'acc': 61.84, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 72.41, 'acc': 64.88, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.52, 'acc': 82.66, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.11, 'acc': 91.52, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.26, 'acc': 76.77, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.79, 'acc': 38.14, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 61.49, 'acc': 75.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.69, 'acc': 71.83, 'f1': 80.76, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.6, 'acc': 76.92, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7955123572523262, 'pearson': 0.7977424438428916, 'spearman': 0.7266337060261728, 'mse': 0.37233594371221534, 'yhat': array([2.96332536, 4.26420344, 1.57771385, ..., 3.04077447, 4.34393033,        4.45708468]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6676441963305915, 'pearson': 0.6491384366119634, 'spearman': 0.6472458640349628, 'mse': 1.4399800524888857, 'yhat': array([1.1474373 , 1.78506386, 1.78382642, ..., 4.04417686, 3.68509739,        3.58065762]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.06, 'acc': 66.87, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 66.13, 'acc': 66.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 47.61, 'acc': 47.88, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.73, 'acc': 26.84, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 55.52, 'acc': 55.76, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 62.53, 'acc': 62.23, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.06, 'acc': 84.76, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.98, 'acc': 76.07, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.64, 'acc': 76.36, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.7, 'acc': 58.7, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.21, 'acc': 55.89, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 01:51:33,072 : STS12 p=0.5657, STS12 s=0.5899, STS13 p=0.6226, STS13 s=0.6175, STS14 p=0.6572, STS14 s=0.6463, STS15 p=0.7041, STS15 s=0.7075, STS 16 p=0.6731, STS16 s=0.6864, STS B p=0.6491, STS B s=0.6472, STS B m=1.4400, SICK-R p=0.7977, SICK-R s=0.7266, SICK-P m=0.3723
2019-03-13 01:51:33,072 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 01:51:33,072 : 0.5657,0.5899,0.6226,0.6175,0.6572,0.6463,0.7041,0.7075,0.6731,0.6864,0.6491,0.6472,1.4400,0.7977,0.7266,0.3723
2019-03-13 01:51:33,072 : MR=61.84, CR=64.88, SUBJ=91.52, MPQA=82.66, SST-B=76.77, SST-F=38.14, TREC=75.60, SICK-E=76.92, SNLI=66.87, MRPC=71.83, MRPC f=80.76
2019-03-13 01:51:33,072 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 01:51:33,072 : 61.84,64.88,91.52,82.66,76.77,38.14,75.60,76.92,66.87,71.83,80.76
2019-03-13 01:51:33,072 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 01:51:33,072 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 01:51:33,072 : na,na,na,na,na,na,na,na,na,na
2019-03-13 01:51:33,072 : SentLen=66.86, WC=47.88, TreeDepth=26.84, TopConst=55.76, BShift=62.23, Tense=84.76, SubjNum=76.07, ObjNum=76.36, SOMO=58.70, CoordInv=55.89, average=61.14
2019-03-13 01:51:33,072 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 01:51:33,072 : 66.86,47.88,26.84,55.76,62.23,84.76,76.07,76.36,58.70,55.89,61.14
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,073 : layer 9
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,073 : ********************************************************************************
2019-03-13 01:51:33,164 : ***** Transfer task : STS12 *****


2019-03-13 01:51:33,176 : loading BERT model bert-large-uncased
2019-03-13 01:51:33,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:51:33,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:51:33,201 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdnmst9v1
2019-03-13 01:51:40,679 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:51:50,181 : MSRpar : pearson = 0.3824, spearman = 0.4187
2019-03-13 01:51:51,807 : MSRvid : pearson = 0.7585, spearman = 0.7593
2019-03-13 01:51:53,205 : SMTeuroparl : pearson = 0.5048, spearman = 0.6204
2019-03-13 01:51:55,869 : surprise.OnWN : pearson = 0.6247, spearman = 0.6560
2019-03-13 01:51:57,282 : surprise.SMTnews : pearson = 0.5445, spearman = 0.4748
2019-03-13 01:51:57,282 : ALL (weighted average) : Pearson = 0.5705,             Spearman = 0.5952
2019-03-13 01:51:57,282 : ALL (average) : Pearson = 0.5630,             Spearman = 0.5859

2019-03-13 01:51:57,282 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 01:51:57,290 : loading BERT model bert-large-uncased
2019-03-13 01:51:57,290 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:51:57,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:51:57,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp70z95ap6
2019-03-13 01:52:04,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:52:11,708 : FNWN : pearson = 0.2600, spearman = 0.2574
2019-03-13 01:52:13,580 : headlines : pearson = 0.6958, spearman = 0.6875
2019-03-13 01:52:15,033 : OnWN : pearson = 0.6838, spearman = 0.6840
2019-03-13 01:52:15,034 : ALL (weighted average) : Pearson = 0.6364,             Spearman = 0.6320
2019-03-13 01:52:15,034 : ALL (average) : Pearson = 0.5465,             Spearman = 0.5430

2019-03-13 01:52:15,034 : ***** Transfer task : STS14 *****


2019-03-13 01:52:15,050 : loading BERT model bert-large-uncased
2019-03-13 01:52:15,051 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:52:15,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:52:15,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvaxadnj_
2019-03-13 01:52:22,511 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:52:29,593 : deft-forum : pearson = 0.4313, spearman = 0.4418
2019-03-13 01:52:31,212 : deft-news : pearson = 0.7401, spearman = 0.6904
2019-03-13 01:52:33,359 : headlines : pearson = 0.6516, spearman = 0.6187
2019-03-13 01:52:35,412 : images : pearson = 0.7182, spearman = 0.7089
2019-03-13 01:52:37,518 : OnWN : pearson = 0.7457, spearman = 0.7674
2019-03-13 01:52:40,341 : tweet-news : pearson = 0.6520, spearman = 0.6213
2019-03-13 01:52:40,341 : ALL (weighted average) : Pearson = 0.6644,             Spearman = 0.6515
2019-03-13 01:52:40,341 : ALL (average) : Pearson = 0.6565,             Spearman = 0.6414

2019-03-13 01:52:40,341 : ***** Transfer task : STS15 *****


2019-03-13 01:52:40,374 : loading BERT model bert-large-uncased
2019-03-13 01:52:40,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:52:40,391 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:52:40,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpom9ggq0v
2019-03-13 01:52:47,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:52:55,272 : answers-forums : pearson = 0.5899, spearman = 0.5747
2019-03-13 01:52:57,334 : answers-students : pearson = 0.7181, spearman = 0.7213
2019-03-13 01:52:59,356 : belief : pearson = 0.6146, spearman = 0.6371
2019-03-13 01:53:01,579 : headlines : pearson = 0.6999, spearman = 0.6994
2019-03-13 01:53:03,687 : images : pearson = 0.8074, spearman = 0.8119
2019-03-13 01:53:03,688 : ALL (weighted average) : Pearson = 0.7069,             Spearman = 0.7096
2019-03-13 01:53:03,688 : ALL (average) : Pearson = 0.6860,             Spearman = 0.6889

2019-03-13 01:53:03,688 : ***** Transfer task : STS16 *****


2019-03-13 01:53:03,755 : loading BERT model bert-large-uncased
2019-03-13 01:53:03,756 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:53:03,773 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:53:03,773 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpptkh7f3c
2019-03-13 01:53:11,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:53:17,544 : answer-answer : pearson = 0.5586, spearman = 0.5470
2019-03-13 01:53:18,198 : headlines : pearson = 0.6917, spearman = 0.7013
2019-03-13 01:53:19,072 : plagiarism : pearson = 0.7593, spearman = 0.7653
2019-03-13 01:53:20,549 : postediting : pearson = 0.8269, spearman = 0.8466
2019-03-13 01:53:21,149 : question-question : pearson = 0.6076, spearman = 0.6137
2019-03-13 01:53:21,149 : ALL (weighted average) : Pearson = 0.6893,             Spearman = 0.6951
2019-03-13 01:53:21,149 : ALL (average) : Pearson = 0.6888,             Spearman = 0.6948

2019-03-13 01:53:21,149 : ***** Transfer task : MR *****


2019-03-13 01:53:21,168 : loading BERT model bert-large-uncased
2019-03-13 01:53:21,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:53:21,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:53:21,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6opevcvu
2019-03-13 01:53:28,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:53:34,293 : Generating sentence embeddings
2019-03-13 01:54:05,379 : Generated sentence embeddings
2019-03-13 01:54:05,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:54:15,100 : Best param found at split 1: l2reg = 0.0001                 with score 66.65
2019-03-13 01:54:27,795 : Best param found at split 2: l2reg = 0.01                 with score 64.92
2019-03-13 01:54:41,334 : Best param found at split 3: l2reg = 1e-05                 with score 64.88
2019-03-13 01:54:53,417 : Best param found at split 4: l2reg = 0.001                 with score 63.04
2019-03-13 01:55:05,235 : Best param found at split 5: l2reg = 1e-05                 with score 59.5
2019-03-13 01:55:06,036 : Dev acc : 63.8 Test acc : 62.45

2019-03-13 01:55:06,038 : ***** Transfer task : CR *****


2019-03-13 01:55:06,046 : loading BERT model bert-large-uncased
2019-03-13 01:55:06,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:55:06,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:55:06,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj5yoe89a
2019-03-13 01:55:13,577 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:55:19,031 : Generating sentence embeddings
2019-03-13 01:55:27,245 : Generated sentence embeddings
2019-03-13 01:55:27,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:55:30,520 : Best param found at split 1: l2reg = 0.0001                 with score 68.4
2019-03-13 01:55:33,954 : Best param found at split 2: l2reg = 0.0001                 with score 71.94
2019-03-13 01:55:37,492 : Best param found at split 3: l2reg = 0.01                 with score 69.34
2019-03-13 01:55:41,194 : Best param found at split 4: l2reg = 1e-05                 with score 75.7
2019-03-13 01:55:45,011 : Best param found at split 5: l2reg = 0.001                 with score 73.75
2019-03-13 01:55:45,172 : Dev acc : 71.83 Test acc : 68.66

2019-03-13 01:55:45,173 : ***** Transfer task : MPQA *****


2019-03-13 01:55:45,179 : loading BERT model bert-large-uncased
2019-03-13 01:55:45,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:55:45,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:55:45,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiaqy5rgn
2019-03-13 01:55:52,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:55:58,297 : Generating sentence embeddings
2019-03-13 01:56:05,800 : Generated sentence embeddings
2019-03-13 01:56:05,800 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:56:16,827 : Best param found at split 1: l2reg = 0.0001                 with score 85.2
2019-03-13 01:56:27,507 : Best param found at split 2: l2reg = 0.01                 with score 85.48
2019-03-13 01:56:39,295 : Best param found at split 3: l2reg = 1e-05                 with score 84.57
2019-03-13 01:56:51,724 : Best param found at split 4: l2reg = 1e-05                 with score 85.54
2019-03-13 01:57:03,249 : Best param found at split 5: l2reg = 0.0001                 with score 85.62
2019-03-13 01:57:03,778 : Dev acc : 85.28 Test acc : 83.51

2019-03-13 01:57:03,779 : ***** Transfer task : SUBJ *****


2019-03-13 01:57:03,794 : loading BERT model bert-large-uncased
2019-03-13 01:57:03,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:57:03,814 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:57:03,815 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4xt5j6t_
2019-03-13 01:57:11,284 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:57:16,916 : Generating sentence embeddings
2019-03-13 01:57:47,398 : Generated sentence embeddings
2019-03-13 01:57:47,399 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 01:57:57,088 : Best param found at split 1: l2reg = 0.001                 with score 91.12
2019-03-13 01:58:06,818 : Best param found at split 2: l2reg = 0.001                 with score 91.16
2019-03-13 01:58:18,177 : Best param found at split 3: l2reg = 0.01                 with score 91.11
2019-03-13 01:58:25,980 : Best param found at split 4: l2reg = 0.01                 with score 91.68
2019-03-13 01:58:36,699 : Best param found at split 5: l2reg = 0.0001                 with score 91.36
2019-03-13 01:58:37,392 : Dev acc : 91.29 Test acc : 91.22

2019-03-13 01:58:37,393 : ***** Transfer task : SST Binary classification *****


2019-03-13 01:58:37,485 : loading BERT model bert-large-uncased
2019-03-13 01:58:37,486 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:58:37,562 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:58:37,562 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_frita7a
2019-03-13 01:58:45,055 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:58:50,543 : Computing embedding for train
2019-03-13 02:00:29,359 : Computed train embeddings
2019-03-13 02:00:29,359 : Computing embedding for dev
2019-03-13 02:00:31,516 : Computed dev embeddings
2019-03-13 02:00:31,516 : Computing embedding for test
2019-03-13 02:00:36,040 : Computed test embeddings
2019-03-13 02:00:36,040 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:00:51,314 : [('reg:1e-05', 76.03), ('reg:0.0001', 75.92), ('reg:0.001', 75.46), ('reg:0.01', 70.3)]
2019-03-13 02:00:51,315 : Validation : best param found is reg = 1e-05 with score             76.03
2019-03-13 02:00:51,315 : Evaluating...
2019-03-13 02:00:54,990 : 
Dev acc : 76.03 Test acc : 74.85 for             SST Binary classification

2019-03-13 02:00:54,990 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 02:00:55,041 : loading BERT model bert-large-uncased
2019-03-13 02:00:55,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:00:55,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:00:55,064 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm8hoppzm
2019-03-13 02:01:02,506 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:01:07,968 : Computing embedding for train
2019-03-13 02:01:29,588 : Computed train embeddings
2019-03-13 02:01:29,588 : Computing embedding for dev
2019-03-13 02:01:32,415 : Computed dev embeddings
2019-03-13 02:01:32,415 : Computing embedding for test
2019-03-13 02:01:37,981 : Computed test embeddings
2019-03-13 02:01:37,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:01:40,968 : [('reg:1e-05', 36.24), ('reg:0.0001', 35.88), ('reg:0.001', 38.06), ('reg:0.01', 31.97)]
2019-03-13 02:01:40,969 : Validation : best param found is reg = 0.001 with score             38.06
2019-03-13 02:01:40,969 : Evaluating...
2019-03-13 02:01:41,638 : 
Dev acc : 38.06 Test acc : 40.27 for             SST Fine-Grained classification

2019-03-13 02:01:41,638 : ***** Transfer task : TREC *****


2019-03-13 02:01:41,651 : loading BERT model bert-large-uncased
2019-03-13 02:01:41,651 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:01:41,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:01:41,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3zucdddu
2019-03-13 02:01:49,203 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:02:02,216 : Computed train embeddings
2019-03-13 02:02:02,798 : Computed test embeddings
2019-03-13 02:02:02,799 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:02:09,737 : [('reg:1e-05', 58.25), ('reg:0.0001', 66.62), ('reg:0.001', 65.96), ('reg:0.01', 57.97)]
2019-03-13 02:02:09,737 : Cross-validation : best param found is reg = 0.0001             with score 66.62
2019-03-13 02:02:09,737 : Evaluating...
2019-03-13 02:02:10,248 : 
Dev acc : 66.62 Test acc : 81.8             for TREC

2019-03-13 02:02:10,249 : ***** Transfer task : MRPC *****


2019-03-13 02:02:10,270 : loading BERT model bert-large-uncased
2019-03-13 02:02:10,270 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:02:10,291 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:02:10,291 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvz2q66sw
2019-03-13 02:02:17,785 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:02:23,348 : Computing embedding for train
2019-03-13 02:02:45,310 : Computed train embeddings
2019-03-13 02:02:45,310 : Computing embedding for test
2019-03-13 02:02:54,929 : Computed test embeddings
2019-03-13 02:02:54,950 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:02:59,542 : [('reg:1e-05', 72.57), ('reg:0.0001', 70.88), ('reg:0.001', 71.79), ('reg:0.01', 71.71)]
2019-03-13 02:02:59,542 : Cross-validation : best param found is reg = 1e-05             with score 72.57
2019-03-13 02:02:59,542 : Evaluating...
2019-03-13 02:02:59,946 : Dev acc : 72.57 Test acc 74.03; Test F1 81.67 for MRPC.

2019-03-13 02:02:59,946 : ***** Transfer task : SICK-Entailment*****


2019-03-13 02:03:00,008 : loading BERT model bert-large-uncased
2019-03-13 02:03:00,008 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:03:00,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:03:00,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptqs4qkqs
2019-03-13 02:03:07,473 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:03:12,973 : Computing embedding for train
2019-03-13 02:03:24,114 : Computed train embeddings
2019-03-13 02:03:24,114 : Computing embedding for dev
2019-03-13 02:03:25,637 : Computed dev embeddings
2019-03-13 02:03:25,637 : Computing embedding for test
2019-03-13 02:03:37,611 : Computed test embeddings
2019-03-13 02:03:37,648 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:03:38,957 : [('reg:1e-05', 77.8), ('reg:0.0001', 77.2), ('reg:0.001', 68.4), ('reg:0.01', 74.8)]
2019-03-13 02:03:38,958 : Validation : best param found is reg = 1e-05 with score             77.8
2019-03-13 02:03:38,958 : Evaluating...
2019-03-13 02:03:39,322 : 
Dev acc : 77.8 Test acc : 76.96 for                        SICK entailment

2019-03-13 02:03:39,323 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 02:03:39,349 : loading BERT model bert-large-uncased
2019-03-13 02:03:39,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:03:39,405 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:03:39,405 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe4x0v8k7
2019-03-13 02:03:46,852 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:03:52,375 : Computing embedding for train
2019-03-13 02:04:03,534 : Computed train embeddings
2019-03-13 02:04:03,534 : Computing embedding for dev
2019-03-13 02:04:05,057 : Computed dev embeddings
2019-03-13 02:04:05,057 : Computing embedding for test
2019-03-13 02:04:17,033 : Computed test embeddings
2019-03-13 02:04:31,949 : Dev : Pearson 0.7923198923731497
2019-03-13 02:04:31,950 : Test : Pearson 0.7996379356482479 Spearman 0.7271073314857683 MSE 0.36944600310330505                        for SICK Relatedness

2019-03-13 02:04:31,951 : 

***** Transfer task : STSBenchmark*****


2019-03-13 02:04:32,035 : loading BERT model bert-large-uncased
2019-03-13 02:04:32,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:04:32,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:04:32,054 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8dayvo8g
2019-03-13 02:04:39,538 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:04:45,092 : Computing embedding for train
2019-03-13 02:05:03,474 : Computed train embeddings
2019-03-13 02:05:03,474 : Computing embedding for dev
2019-03-13 02:05:09,046 : Computed dev embeddings
2019-03-13 02:05:09,046 : Computing embedding for test
2019-03-13 02:05:13,598 : Computed test embeddings
2019-03-13 02:05:32,157 : Dev : Pearson 0.673029672236768
2019-03-13 02:05:32,157 : Test : Pearson 0.65988140491912 Spearman 0.6620977268936789 MSE 1.4246986376986892                        for SICK Relatedness

2019-03-13 02:05:32,157 : ***** Transfer task : SNLI Entailment*****


2019-03-13 02:05:37,348 : loading BERT model bert-large-uncased
2019-03-13 02:05:37,348 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:37,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:37,420 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpif1iybta
2019-03-13 02:05:44,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:05:50,865 : PROGRESS (encoding): 0.00%
2019-03-13 02:08:34,530 : PROGRESS (encoding): 14.56%
2019-03-13 02:11:39,966 : PROGRESS (encoding): 29.12%
2019-03-13 02:14:46,069 : PROGRESS (encoding): 43.69%
2019-03-13 02:18:04,634 : PROGRESS (encoding): 58.25%
2019-03-13 02:21:45,773 : PROGRESS (encoding): 72.81%
2019-03-13 02:25:25,729 : PROGRESS (encoding): 87.37%
2019-03-13 02:29:23,652 : PROGRESS (encoding): 0.00%
2019-03-13 02:29:53,628 : PROGRESS (encoding): 0.00%
2019-03-13 02:30:22,424 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:31:15,273 : [('reg:1e-09', 66.15)]
2019-03-13 02:31:15,274 : Validation : best param found is reg = 1e-09 with score             66.15
2019-03-13 02:31:15,274 : Evaluating...
2019-03-13 02:32:08,769 : Dev acc : 66.15 Test acc : 66.96 for SNLI

2019-03-13 02:32:08,769 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 02:32:08,976 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 02:32:10,029 : loading BERT model bert-large-uncased
2019-03-13 02:32:10,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:32:10,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:32:10,056 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkqyw91b9
2019-03-13 02:32:17,454 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:32:22,881 : Computing embeddings for train/dev/test
2019-03-13 02:35:50,588 : Computed embeddings
2019-03-13 02:35:50,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:36:23,361 : [('reg:1e-05', 59.67), ('reg:0.0001', 68.93), ('reg:0.001', 66.39), ('reg:0.01', 57.92)]
2019-03-13 02:36:23,361 : Validation : best param found is reg = 0.0001 with score             68.93
2019-03-13 02:36:23,361 : Evaluating...
2019-03-13 02:36:35,298 : 
Dev acc : 68.9 Test acc : 70.1 for LENGTH classification

2019-03-13 02:36:35,299 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 02:36:35,549 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 02:36:35,595 : loading BERT model bert-large-uncased
2019-03-13 02:36:35,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:36:35,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:36:35,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppr5f2haq
2019-03-13 02:36:43,089 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:36:48,557 : Computing embeddings for train/dev/test
2019-03-13 02:40:00,185 : Computed embeddings
2019-03-13 02:40:00,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:40:31,814 : [('reg:1e-05', 38.32), ('reg:0.0001', 13.88), ('reg:0.001', 0.96), ('reg:0.01', 0.25)]
2019-03-13 02:40:31,814 : Validation : best param found is reg = 1e-05 with score             38.32
2019-03-13 02:40:31,814 : Evaluating...
2019-03-13 02:40:43,339 : 
Dev acc : 38.3 Test acc : 38.6 for WORDCONTENT classification

2019-03-13 02:40:43,341 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 02:40:43,872 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 02:40:43,938 : loading BERT model bert-large-uncased
2019-03-13 02:40:43,938 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:40:43,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:40:43,962 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3u6kuoay
2019-03-13 02:40:51,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:40:56,831 : Computing embeddings for train/dev/test
2019-03-13 02:43:56,668 : Computed embeddings
2019-03-13 02:43:56,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:44:25,442 : [('reg:1e-05', 28.43), ('reg:0.0001', 25.08), ('reg:0.001', 25.79), ('reg:0.01', 26.13)]
2019-03-13 02:44:25,442 : Validation : best param found is reg = 1e-05 with score             28.43
2019-03-13 02:44:25,443 : Evaluating...
2019-03-13 02:44:34,190 : 
Dev acc : 28.4 Test acc : 27.7 for DEPTH classification

2019-03-13 02:44:34,191 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 02:44:34,568 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 02:44:34,633 : loading BERT model bert-large-uncased
2019-03-13 02:44:34,633 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:44:34,749 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:44:34,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0j5gvqe6
2019-03-13 02:44:42,183 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:44:47,679 : Computing embeddings for train/dev/test
2019-03-13 02:47:34,674 : Computed embeddings
2019-03-13 02:47:34,674 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:48:06,110 : [('reg:1e-05', 51.79), ('reg:0.0001', 49.64), ('reg:0.001', 40.69), ('reg:0.01', 31.5)]
2019-03-13 02:48:06,110 : Validation : best param found is reg = 1e-05 with score             51.79
2019-03-13 02:48:06,111 : Evaluating...
2019-03-13 02:48:13,851 : 
Dev acc : 51.8 Test acc : 51.7 for TOPCONSTITUENTS classification

2019-03-13 02:48:13,852 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 02:48:14,225 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 02:48:14,290 : loading BERT model bert-large-uncased
2019-03-13 02:48:14,291 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:48:14,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:48:14,320 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3zdy02f5
2019-03-13 02:48:21,796 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:48:27,349 : Computing embeddings for train/dev/test
2019-03-13 02:51:28,548 : Computed embeddings
2019-03-13 02:51:28,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:51:56,999 : [('reg:1e-05', 64.32), ('reg:0.0001', 64.31), ('reg:0.001', 64.15), ('reg:0.01', 60.5)]
2019-03-13 02:51:56,999 : Validation : best param found is reg = 1e-05 with score             64.32
2019-03-13 02:51:56,999 : Evaluating...
2019-03-13 02:52:03,067 : 
Dev acc : 64.3 Test acc : 63.3 for BIGRAMSHIFT classification

2019-03-13 02:52:03,068 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 02:52:03,456 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 02:52:03,521 : loading BERT model bert-large-uncased
2019-03-13 02:52:03,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:52:03,551 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:52:03,551 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppfotiion
2019-03-13 02:52:11,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:52:16,634 : Computing embeddings for train/dev/test
2019-03-13 02:55:13,857 : Computed embeddings
2019-03-13 02:55:13,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:55:42,217 : [('reg:1e-05', 86.74), ('reg:0.0001', 86.76), ('reg:0.001', 86.96), ('reg:0.01', 86.94)]
2019-03-13 02:55:42,218 : Validation : best param found is reg = 0.001 with score             86.96
2019-03-13 02:55:42,218 : Evaluating...
2019-03-13 02:55:48,545 : 
Dev acc : 87.0 Test acc : 86.0 for TENSE classification

2019-03-13 02:55:48,547 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 02:55:48,950 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 02:55:49,013 : loading BERT model bert-large-uncased
2019-03-13 02:55:49,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:55:49,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:55:49,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxvm8lfm2
2019-03-13 02:55:56,585 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:56:02,097 : Computing embeddings for train/dev/test
2019-03-13 02:59:09,730 : Computed embeddings
2019-03-13 02:59:09,731 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:59:37,189 : [('reg:1e-05', 74.38), ('reg:0.0001', 74.58), ('reg:0.001', 79.04), ('reg:0.01', 76.63)]
2019-03-13 02:59:37,189 : Validation : best param found is reg = 0.001 with score             79.04
2019-03-13 02:59:37,189 : Evaluating...
2019-03-13 02:59:43,927 : 
Dev acc : 79.0 Test acc : 77.1 for SUBJNUMBER classification

2019-03-13 02:59:43,928 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 02:59:44,328 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 02:59:44,394 : loading BERT model bert-large-uncased
2019-03-13 02:59:44,394 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:59:44,508 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:59:44,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpno5l2w7s
2019-03-13 02:59:51,974 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:59:57,468 : Computing embeddings for train/dev/test
2019-03-13 03:03:02,027 : Computed embeddings
2019-03-13 03:03:02,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:03:30,972 : [('reg:1e-05', 77.19), ('reg:0.0001', 77.21), ('reg:0.001', 77.07), ('reg:0.01', 72.93)]
2019-03-13 03:03:30,972 : Validation : best param found is reg = 0.0001 with score             77.21
2019-03-13 03:03:30,972 : Evaluating...
2019-03-13 03:03:38,510 : 
Dev acc : 77.2 Test acc : 77.9 for OBJNUMBER classification

2019-03-13 03:03:38,511 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 03:03:39,070 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 03:03:39,140 : loading BERT model bert-large-uncased
2019-03-13 03:03:39,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:03:39,168 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:03:39,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmvrkj7vh
2019-03-13 03:03:46,647 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:03:52,132 : Computing embeddings for train/dev/test
2019-03-13 03:07:25,768 : Computed embeddings
2019-03-13 03:07:25,768 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:07:55,299 : [('reg:1e-05', 56.47), ('reg:0.0001', 56.5), ('reg:0.001', 56.53), ('reg:0.01', 56.21)]
2019-03-13 03:07:55,299 : Validation : best param found is reg = 0.001 with score             56.53
2019-03-13 03:07:55,299 : Evaluating...
2019-03-13 03:08:02,906 : 
Dev acc : 56.5 Test acc : 56.3 for ODDMANOUT classification

2019-03-13 03:08:02,907 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 03:08:03,289 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 03:08:03,371 : loading BERT model bert-large-uncased
2019-03-13 03:08:03,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:08:03,401 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:08:03,402 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvy0ilgxc
2019-03-13 03:08:10,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:08:16,518 : Computing embeddings for train/dev/test
2019-03-13 03:11:48,129 : Computed embeddings
2019-03-13 03:11:48,129 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:12:22,446 : [('reg:1e-05', 56.06), ('reg:0.0001', 55.95), ('reg:0.001', 55.91), ('reg:0.01', 50.07)]
2019-03-13 03:12:22,446 : Validation : best param found is reg = 1e-05 with score             56.06
2019-03-13 03:12:22,446 : Evaluating...
2019-03-13 03:12:32,192 : 
Dev acc : 56.1 Test acc : 56.1 for COORDINATIONINVERSION classification

2019-03-13 03:12:32,194 : total results: {'STS12': {'MSRpar': {'pearson': (0.38236611210192145, 1.6051493166481736e-27), 'spearman': SpearmanrResult(correlation=0.4187058885011392, pvalue=3.404667818851674e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7585314975789417, 2.8868458495094065e-141), 'spearman': SpearmanrResult(correlation=0.7593222884455099, pvalue=1.000441634287589e-141), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5047839118237258, 4.7791914021609275e-31), 'spearman': SpearmanrResult(correlation=0.6203974113659904, pvalue=3.5711781286876647e-50), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6247346167803797, 2.011311777144688e-82), 'spearman': SpearmanrResult(correlation=0.6560461851589836, pvalue=1.7032947829474703e-93), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.544493837530545, 3.539152311116746e-32), 'spearman': SpearmanrResult(correlation=0.47484154999407824, pvalue=7.8641415289497e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5629819951631028, 'wmean': 0.5705189918107175}, 'spearman': {'mean': 0.5858626646931403, 'wmean': 0.5951672978905571}}}, 'STS13': {'FNWN': {'pearson': (0.25996275493471005, 0.0003032615517930958), 'spearman': SpearmanrResult(correlation=0.2573791622029645, pvalue=0.00034978725448095134), 'nsamples': 189}, 'headlines': {'pearson': (0.695834191271012, 1.242789550752721e-109), 'spearman': SpearmanrResult(correlation=0.6875258742291057, pvalue=4.775680522487291e-106), 'nsamples': 750}, 'OnWN': {'pearson': (0.6837885750426751, 1.5252491228791876e-78), 'spearman': SpearmanrResult(correlation=0.6839845966242458, pvalue=1.3245758477719306e-78), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5465285070827991, 'wmean': 0.6364093298232398}, 'spearman': {'mean': 0.542963211018772, 'wmean': 0.6320029506895944}}}, 'STS14': {'deft-forum': {'pearson': (0.43130653370477295, 8.203274467453246e-22), 'spearman': SpearmanrResult(correlation=0.4417798326777581, pvalue=6.379122114705296e-23), 'nsamples': 450}, 'deft-news': {'pearson': (0.740118477749997, 2.7654011494527026e-53), 'spearman': SpearmanrResult(correlation=0.690382933694101, pvalue=8.442881846338628e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.6515577579362545, 7.930442040832234e-92), 'spearman': SpearmanrResult(correlation=0.6187363676203639, pvalue=1.9167825369298605e-80), 'nsamples': 750}, 'images': {'pearson': (0.7181626480786317, 6.630940950191553e-120), 'spearman': SpearmanrResult(correlation=0.7089155917669446, pvalue=1.5641271982359277e-115), 'nsamples': 750}, 'OnWN': {'pearson': (0.7456512928282666, 5.183175814832958e-134), 'spearman': SpearmanrResult(correlation=0.7673863181910935, pvalue=1.5958387365303122e-146), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6519711064082361, 5.583078824782465e-92), 'spearman': SpearmanrResult(correlation=0.621256815560909, pvalue=2.8589088926029123e-81), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6564613027843598, 'wmean': 0.6644348233148503}, 'spearman': {'mean': 0.6414096432518617, 'wmean': 0.6515032332447214}}}, 'STS15': {'answers-forums': {'pearson': (0.5899350413558361, 1.5744270935548092e-36), 'spearman': SpearmanrResult(correlation=0.5747054017546029, pvalue=2.4119291233451527e-34), 'nsamples': 375}, 'answers-students': {'pearson': (0.7181061175597105, 7.060588655407159e-120), 'spearman': SpearmanrResult(correlation=0.7213121363381528, pvalue=1.9581394867346694e-121), 'nsamples': 750}, 'belief': {'pearson': (0.6145668922971604, 2.5543546933757008e-40), 'spearman': SpearmanrResult(correlation=0.6370881657280065, pvalue=4.337781189221163e-44), 'nsamples': 375}, 'headlines': {'pearson': (0.6998734339726002, 2.027156250680806e-111), 'spearman': SpearmanrResult(correlation=0.6993984278968061, pvalue=3.3010548916582537e-111), 'nsamples': 750}, 'images': {'pearson': (0.8073507335898445, 1.5643648035292697e-173), 'spearman': SpearmanrResult(correlation=0.8119405660567182, pvalue=4.866696631050687e-177), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6859664437550304, 'wmean': 0.7068953129871633}, 'spearman': {'mean': 0.6888889395548573, 'wmean': 0.7096369785082455}}}, 'STS16': {'answer-answer': {'pearson': (0.5586424800907513, 3.0175241785344784e-22), 'spearman': SpearmanrResult(correlation=0.5470134651229042, pvalue=3.1761196394008703e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6916560383556429, 9.032291330253245e-37), 'spearman': SpearmanrResult(correlation=0.7012615690995609, pvalue=3.598882779097898e-38), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7592806685214912, 2.0006592691155978e-44), 'spearman': SpearmanrResult(correlation=0.765337427242337, pvalue=1.607961059955712e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.8268894943197271, 1.975317600780329e-62), 'spearman': SpearmanrResult(correlation=0.8466210320153463, pvalue=3.09434262710343e-68), 'nsamples': 244}, 'question-question': {'pearson': (0.6076452173958207, 1.7488716693552322e-22), 'spearman': SpearmanrResult(correlation=0.6136661908154083, pvalue=5.146147218715133e-23), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6888227797366866, 'wmean': 0.6893009985693834}, 'spearman': {'mean': 0.6947799368591113, 'wmean': 0.6951221962941911}}}, 'MR': {'devacc': 63.8, 'acc': 62.45, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.83, 'acc': 68.66, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.28, 'acc': 83.51, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.29, 'acc': 91.22, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.03, 'acc': 74.85, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.06, 'acc': 40.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.62, 'acc': 81.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.57, 'acc': 74.03, 'f1': 81.67, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.8, 'acc': 76.96, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7923198923731497, 'pearson': 0.7996379356482479, 'spearman': 0.7271073314857683, 'mse': 0.36944600310330505, 'yhat': array([3.3771746 , 4.49628649, 1.16440576, ..., 3.04817079, 3.99201083,        4.88616598]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.673029672236768, 'pearson': 0.65988140491912, 'spearman': 0.6620977268936789, 'mse': 1.4246986376986892, 'yhat': array([1.34699608, 1.82087285, 2.00338964, ..., 4.04769315, 3.89220669,        3.83475629]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.15, 'acc': 66.96, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 68.93, 'acc': 70.11, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 38.32, 'acc': 38.64, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.43, 'acc': 27.69, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 51.79, 'acc': 51.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 64.32, 'acc': 63.28, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.96, 'acc': 86.04, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.04, 'acc': 77.07, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.21, 'acc': 77.89, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.53, 'acc': 56.31, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.06, 'acc': 56.08, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 03:12:32,194 : STS12 p=0.5705, STS12 s=0.5952, STS13 p=0.6364, STS13 s=0.6320, STS14 p=0.6644, STS14 s=0.6515, STS15 p=0.7069, STS15 s=0.7096, STS 16 p=0.6893, STS16 s=0.6951, STS B p=0.6599, STS B s=0.6621, STS B m=1.4247, SICK-R p=0.7996, SICK-R s=0.7271, SICK-P m=0.3694
2019-03-13 03:12:32,194 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 03:12:32,194 : 0.5705,0.5952,0.6364,0.6320,0.6644,0.6515,0.7069,0.7096,0.6893,0.6951,0.6599,0.6621,1.4247,0.7996,0.7271,0.3694
2019-03-13 03:12:32,194 : MR=62.45, CR=68.66, SUBJ=91.22, MPQA=83.51, SST-B=74.85, SST-F=40.27, TREC=81.80, SICK-E=76.96, SNLI=66.96, MRPC=74.03, MRPC f=81.67
2019-03-13 03:12:32,194 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 03:12:32,194 : 62.45,68.66,91.22,83.51,74.85,40.27,81.80,76.96,66.96,74.03,81.67
2019-03-13 03:12:32,194 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 03:12:32,194 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 03:12:32,194 : na,na,na,na,na,na,na,na,na,na
2019-03-13 03:12:32,194 : SentLen=70.11, WC=38.64, TreeDepth=27.69, TopConst=51.70, BShift=63.28, Tense=86.04, SubjNum=77.07, ObjNum=77.89, SOMO=56.31, CoordInv=56.08, average=60.48
2019-03-13 03:12:32,194 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 03:12:32,194 : 70.11,38.64,27.69,51.70,63.28,86.04,77.07,77.89,56.31,56.08,60.48
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,194 : layer 10
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,194 : ********************************************************************************
2019-03-13 03:12:32,287 : ***** Transfer task : STS12 *****


2019-03-13 03:12:32,299 : loading BERT model bert-large-uncased
2019-03-13 03:12:32,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:12:32,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:12:32,316 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqkpisumh
2019-03-13 03:12:39,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:12:49,355 : MSRpar : pearson = 0.3811, spearman = 0.4099
2019-03-13 03:12:50,980 : MSRvid : pearson = 0.7267, spearman = 0.7296
2019-03-13 03:12:52,378 : SMTeuroparl : pearson = 0.5016, spearman = 0.6170
2019-03-13 03:12:55,044 : surprise.OnWN : pearson = 0.6127, spearman = 0.6522
2019-03-13 03:12:56,455 : surprise.SMTnews : pearson = 0.5438, spearman = 0.4786
2019-03-13 03:12:56,455 : ALL (weighted average) : Pearson = 0.5591,             Spearman = 0.5849
2019-03-13 03:12:56,455 : ALL (average) : Pearson = 0.5532,             Spearman = 0.5775

2019-03-13 03:12:56,455 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 03:12:56,464 : loading BERT model bert-large-uncased
2019-03-13 03:12:56,464 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:12:56,481 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:12:56,482 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8raiybxv
2019-03-13 03:13:04,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:13:10,891 : FNWN : pearson = 0.2462, spearman = 0.2421
2019-03-13 03:13:12,762 : headlines : pearson = 0.6842, spearman = 0.6748
2019-03-13 03:13:14,213 : OnWN : pearson = 0.6308, spearman = 0.6315
2019-03-13 03:13:14,214 : ALL (weighted average) : Pearson = 0.6091,             Spearman = 0.6041
2019-03-13 03:13:14,214 : ALL (average) : Pearson = 0.5204,             Spearman = 0.5161

2019-03-13 03:13:14,214 : ***** Transfer task : STS14 *****


2019-03-13 03:13:14,229 : loading BERT model bert-large-uncased
2019-03-13 03:13:14,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:13:14,246 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:13:14,246 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps40tecex
2019-03-13 03:13:21,723 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:13:28,553 : deft-forum : pearson = 0.4184, spearman = 0.4293
2019-03-13 03:13:30,169 : deft-news : pearson = 0.7402, spearman = 0.6889
2019-03-13 03:13:32,311 : headlines : pearson = 0.6288, spearman = 0.5965
2019-03-13 03:13:34,365 : images : pearson = 0.7008, spearman = 0.6936
2019-03-13 03:13:36,472 : OnWN : pearson = 0.7113, spearman = 0.7354
2019-03-13 03:13:39,296 : tweet-news : pearson = 0.6320, spearman = 0.6056
2019-03-13 03:13:39,297 : ALL (weighted average) : Pearson = 0.6440,             Spearman = 0.6329
2019-03-13 03:13:39,297 : ALL (average) : Pearson = 0.6386,             Spearman = 0.6249

2019-03-13 03:13:39,297 : ***** Transfer task : STS15 *****


2019-03-13 03:13:39,329 : loading BERT model bert-large-uncased
2019-03-13 03:13:39,329 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:13:39,346 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:13:39,346 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptzc05x1a
2019-03-13 03:13:46,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:13:54,123 : answers-forums : pearson = 0.5880, spearman = 0.5705
2019-03-13 03:13:56,180 : answers-students : pearson = 0.7119, spearman = 0.7144
2019-03-13 03:13:58,201 : belief : pearson = 0.6067, spearman = 0.6265
2019-03-13 03:14:00,424 : headlines : pearson = 0.6836, spearman = 0.6851
2019-03-13 03:14:02,533 : images : pearson = 0.7986, spearman = 0.8037
2019-03-13 03:14:02,534 : ALL (weighted average) : Pearson = 0.6979,             Spearman = 0.7004
2019-03-13 03:14:02,534 : ALL (average) : Pearson = 0.6778,             Spearman = 0.6800

2019-03-13 03:14:02,534 : ***** Transfer task : STS16 *****


2019-03-13 03:14:02,604 : loading BERT model bert-large-uncased
2019-03-13 03:14:02,605 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:14:02,623 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:14:02,623 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppqmkxwzp
2019-03-13 03:14:10,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:14:16,482 : answer-answer : pearson = 0.5825, spearman = 0.5839
2019-03-13 03:14:17,136 : headlines : pearson = 0.6727, spearman = 0.6800
2019-03-13 03:14:18,007 : plagiarism : pearson = 0.7522, spearman = 0.7635
2019-03-13 03:14:19,482 : postediting : pearson = 0.8219, spearman = 0.8356
2019-03-13 03:14:20,081 : question-question : pearson = 0.5594, spearman = 0.5643
2019-03-13 03:14:20,081 : ALL (weighted average) : Pearson = 0.6795,             Spearman = 0.6872
2019-03-13 03:14:20,081 : ALL (average) : Pearson = 0.6778,             Spearman = 0.6855

2019-03-13 03:14:20,081 : ***** Transfer task : MR *****


2019-03-13 03:14:20,096 : loading BERT model bert-large-uncased
2019-03-13 03:14:20,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:14:20,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:14:20,116 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0rbanlmm
2019-03-13 03:14:27,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:14:33,122 : Generating sentence embeddings
2019-03-13 03:15:04,168 : Generated sentence embeddings
2019-03-13 03:15:04,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:15:17,083 : Best param found at split 1: l2reg = 1e-05                 with score 67.79
2019-03-13 03:15:30,262 : Best param found at split 2: l2reg = 0.0001                 with score 62.09
2019-03-13 03:15:44,720 : Best param found at split 3: l2reg = 0.0001                 with score 65.08
2019-03-13 03:15:54,232 : Best param found at split 4: l2reg = 0.001                 with score 62.51
2019-03-13 03:16:03,379 : Best param found at split 5: l2reg = 0.001                 with score 59.02
2019-03-13 03:16:03,966 : Dev acc : 63.3 Test acc : 60.36

2019-03-13 03:16:03,967 : ***** Transfer task : CR *****


2019-03-13 03:16:03,975 : loading BERT model bert-large-uncased
2019-03-13 03:16:03,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:16:03,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:16:03,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg_p8mvv9
2019-03-13 03:16:11,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:16:16,966 : Generating sentence embeddings
2019-03-13 03:16:25,169 : Generated sentence embeddings
2019-03-13 03:16:25,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:16:28,104 : Best param found at split 1: l2reg = 0.0001                 with score 67.11
2019-03-13 03:16:31,537 : Best param found at split 2: l2reg = 1e-05                 with score 70.45
2019-03-13 03:16:34,968 : Best param found at split 3: l2reg = 0.001                 with score 68.22
2019-03-13 03:16:38,441 : Best param found at split 4: l2reg = 1e-05                 with score 72.0
2019-03-13 03:16:41,970 : Best param found at split 5: l2reg = 0.001                 with score 71.34
2019-03-13 03:16:42,165 : Dev acc : 69.82 Test acc : 66.94

2019-03-13 03:16:42,165 : ***** Transfer task : MPQA *****


2019-03-13 03:16:42,171 : loading BERT model bert-large-uncased
2019-03-13 03:16:42,171 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:16:42,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:16:42,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps4t4k_3a
2019-03-13 03:16:49,754 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:16:55,176 : Generating sentence embeddings
2019-03-13 03:17:02,675 : Generated sentence embeddings
2019-03-13 03:17:02,676 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:17:14,082 : Best param found at split 1: l2reg = 0.001                 with score 84.89
2019-03-13 03:17:25,505 : Best param found at split 2: l2reg = 0.0001                 with score 84.58
2019-03-13 03:17:37,018 : Best param found at split 3: l2reg = 0.001                 with score 84.82
2019-03-13 03:17:46,737 : Best param found at split 4: l2reg = 0.0001                 with score 83.98
2019-03-13 03:17:55,916 : Best param found at split 5: l2reg = 0.0001                 with score 85.15
2019-03-13 03:17:56,248 : Dev acc : 84.68 Test acc : 84.6

2019-03-13 03:17:56,249 : ***** Transfer task : SUBJ *****


2019-03-13 03:17:56,265 : loading BERT model bert-large-uncased
2019-03-13 03:17:56,265 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:17:56,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:17:56,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1jcgui2u
2019-03-13 03:18:03,749 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:18:09,507 : Generating sentence embeddings
2019-03-13 03:18:39,970 : Generated sentence embeddings
2019-03-13 03:18:39,970 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:18:49,031 : Best param found at split 1: l2reg = 0.01                 with score 88.54
2019-03-13 03:18:59,177 : Best param found at split 2: l2reg = 0.01                 with score 90.95
2019-03-13 03:19:10,207 : Best param found at split 3: l2reg = 0.0001                 with score 90.57
2019-03-13 03:19:21,694 : Best param found at split 4: l2reg = 0.001                 with score 91.41
2019-03-13 03:19:33,920 : Best param found at split 5: l2reg = 0.01                 with score 91.16
2019-03-13 03:19:34,585 : Dev acc : 90.53 Test acc : 90.65

2019-03-13 03:19:34,586 : ***** Transfer task : SST Binary classification *****


2019-03-13 03:19:34,678 : loading BERT model bert-large-uncased
2019-03-13 03:19:34,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:19:34,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:19:34,753 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmkzi1gh3
2019-03-13 03:19:42,188 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:19:47,780 : Computing embedding for train
2019-03-13 03:21:26,380 : Computed train embeddings
2019-03-13 03:21:26,380 : Computing embedding for dev
2019-03-13 03:21:28,529 : Computed dev embeddings
2019-03-13 03:21:28,529 : Computing embedding for test
2019-03-13 03:21:33,044 : Computed test embeddings
2019-03-13 03:21:33,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:21:51,833 : [('reg:1e-05', 77.29), ('reg:0.0001', 77.52), ('reg:0.001', 76.83), ('reg:0.01', 76.38)]
2019-03-13 03:21:51,833 : Validation : best param found is reg = 0.0001 with score             77.52
2019-03-13 03:21:51,833 : Evaluating...
2019-03-13 03:21:56,162 : 
Dev acc : 77.52 Test acc : 77.7 for             SST Binary classification

2019-03-13 03:21:56,162 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 03:21:56,216 : loading BERT model bert-large-uncased
2019-03-13 03:21:56,216 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:21:56,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:21:56,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2bmhc6u4
2019-03-13 03:22:03,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:22:09,057 : Computing embedding for train
2019-03-13 03:22:30,639 : Computed train embeddings
2019-03-13 03:22:30,639 : Computing embedding for dev
2019-03-13 03:22:33,455 : Computed dev embeddings
2019-03-13 03:22:33,455 : Computing embedding for test
2019-03-13 03:22:39,008 : Computed test embeddings
2019-03-13 03:22:39,008 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:22:41,090 : [('reg:1e-05', 35.33), ('reg:0.0001', 34.6), ('reg:0.001', 35.33), ('reg:0.01', 30.7)]
2019-03-13 03:22:41,090 : Validation : best param found is reg = 1e-05 with score             35.33
2019-03-13 03:22:41,090 : Evaluating...
2019-03-13 03:22:41,677 : 
Dev acc : 35.33 Test acc : 36.97 for             SST Fine-Grained classification

2019-03-13 03:22:41,677 : ***** Transfer task : TREC *****


2019-03-13 03:22:41,691 : loading BERT model bert-large-uncased
2019-03-13 03:22:41,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:22:41,709 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:22:41,710 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzjl01vwf
2019-03-13 03:22:49,183 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:23:02,170 : Computed train embeddings
2019-03-13 03:23:02,751 : Computed test embeddings
2019-03-13 03:23:02,751 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:23:08,293 : [('reg:1e-05', 62.05), ('reg:0.0001', 66.2), ('reg:0.001', 67.87), ('reg:0.01', 54.78)]
2019-03-13 03:23:08,293 : Cross-validation : best param found is reg = 0.001             with score 67.87
2019-03-13 03:23:08,293 : Evaluating...
2019-03-13 03:23:08,666 : 
Dev acc : 67.87 Test acc : 75.0             for TREC

2019-03-13 03:23:08,667 : ***** Transfer task : MRPC *****


2019-03-13 03:23:08,687 : loading BERT model bert-large-uncased
2019-03-13 03:23:08,687 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:23:08,709 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:23:08,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwg50dyu6
2019-03-13 03:23:16,199 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:23:21,674 : Computing embedding for train
2019-03-13 03:23:43,616 : Computed train embeddings
2019-03-13 03:23:43,616 : Computing embedding for test
2019-03-13 03:23:53,215 : Computed test embeddings
2019-03-13 03:23:53,237 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:23:57,845 : [('reg:1e-05', 71.64), ('reg:0.0001', 70.93), ('reg:0.001', 71.61), ('reg:0.01', 71.27)]
2019-03-13 03:23:57,845 : Cross-validation : best param found is reg = 1e-05             with score 71.64
2019-03-13 03:23:57,845 : Evaluating...
2019-03-13 03:23:58,087 : Dev acc : 71.64 Test acc 69.33; Test F1 81.15 for MRPC.

2019-03-13 03:23:58,087 : ***** Transfer task : SICK-Entailment*****


2019-03-13 03:23:58,149 : loading BERT model bert-large-uncased
2019-03-13 03:23:58,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:23:58,167 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:23:58,168 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkjg8shno
2019-03-13 03:24:05,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:24:11,177 : Computing embedding for train
2019-03-13 03:24:22,313 : Computed train embeddings
2019-03-13 03:24:22,313 : Computing embedding for dev
2019-03-13 03:24:23,831 : Computed dev embeddings
2019-03-13 03:24:23,831 : Computing embedding for test
2019-03-13 03:24:35,768 : Computed test embeddings
2019-03-13 03:24:35,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:24:36,867 : [('reg:1e-05', 71.4), ('reg:0.0001', 75.8), ('reg:0.001', 75.8), ('reg:0.01', 79.6)]
2019-03-13 03:24:36,867 : Validation : best param found is reg = 0.01 with score             79.6
2019-03-13 03:24:36,868 : Evaluating...
2019-03-13 03:24:37,141 : 
Dev acc : 79.6 Test acc : 75.73 for                        SICK entailment

2019-03-13 03:24:37,141 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 03:24:37,167 : loading BERT model bert-large-uncased
2019-03-13 03:24:37,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:24:37,223 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:24:37,223 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwzzst__w
2019-03-13 03:24:44,693 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:24:50,085 : Computing embedding for train
2019-03-13 03:25:01,242 : Computed train embeddings
2019-03-13 03:25:01,243 : Computing embedding for dev
2019-03-13 03:25:02,764 : Computed dev embeddings
2019-03-13 03:25:02,764 : Computing embedding for test
2019-03-13 03:25:14,723 : Computed test embeddings
2019-03-13 03:25:29,621 : Dev : Pearson 0.813336532148404
2019-03-13 03:25:29,621 : Test : Pearson 0.7956620316462304 Spearman 0.7300819687498927 MSE 0.37605250499358267                        for SICK Relatedness

2019-03-13 03:25:29,624 : 

***** Transfer task : STSBenchmark*****


2019-03-13 03:25:29,663 : loading BERT model bert-large-uncased
2019-03-13 03:25:29,663 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:25:29,691 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:25:29,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp25hckayi
2019-03-13 03:25:37,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:25:42,607 : Computing embedding for train
2019-03-13 03:26:00,987 : Computed train embeddings
2019-03-13 03:26:00,987 : Computing embedding for dev
2019-03-13 03:26:06,556 : Computed dev embeddings
2019-03-13 03:26:06,556 : Computing embedding for test
2019-03-13 03:26:11,089 : Computed test embeddings
2019-03-13 03:26:30,348 : Dev : Pearson 0.7023353470669242
2019-03-13 03:26:30,348 : Test : Pearson 0.6541282496883349 Spearman 0.6554524483865094 MSE 1.4088021018081418                        for SICK Relatedness

2019-03-13 03:26:30,348 : ***** Transfer task : SNLI Entailment*****


2019-03-13 03:26:35,129 : loading BERT model bert-large-uncased
2019-03-13 03:26:35,130 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:26:36,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:26:36,161 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo4k_fkld
2019-03-13 03:26:43,593 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:49,551 : PROGRESS (encoding): 0.00%
2019-03-13 03:29:32,869 : PROGRESS (encoding): 14.56%
2019-03-13 03:32:38,193 : PROGRESS (encoding): 29.12%
2019-03-13 03:35:44,115 : PROGRESS (encoding): 43.69%
2019-03-13 03:39:02,538 : PROGRESS (encoding): 58.25%
2019-03-13 03:42:43,316 : PROGRESS (encoding): 72.81%
2019-03-13 03:46:23,050 : PROGRESS (encoding): 87.37%
2019-03-13 03:50:20,819 : PROGRESS (encoding): 0.00%
2019-03-13 03:50:50,720 : PROGRESS (encoding): 0.00%
2019-03-13 03:51:19,523 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:52:21,300 : [('reg:1e-09', 69.08)]
2019-03-13 03:52:21,300 : Validation : best param found is reg = 1e-09 with score             69.08
2019-03-13 03:52:21,300 : Evaluating...
2019-03-13 03:53:18,466 : Dev acc : 69.08 Test acc : 69.07 for SNLI

2019-03-13 03:53:18,466 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 03:53:18,675 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 03:53:19,641 : loading BERT model bert-large-uncased
2019-03-13 03:53:19,642 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:53:19,667 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:53:19,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuxuwjmco
2019-03-13 03:53:27,102 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:53:32,648 : Computing embeddings for train/dev/test
2019-03-13 03:57:00,125 : Computed embeddings
2019-03-13 03:57:00,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:57:28,935 : [('reg:1e-05', 65.92), ('reg:0.0001', 67.68), ('reg:0.001', 60.25), ('reg:0.01', 62.83)]
2019-03-13 03:57:28,936 : Validation : best param found is reg = 0.0001 with score             67.68
2019-03-13 03:57:28,936 : Evaluating...
2019-03-13 03:57:37,028 : 
Dev acc : 67.7 Test acc : 68.0 for LENGTH classification

2019-03-13 03:57:37,028 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 03:57:37,396 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 03:57:37,441 : loading BERT model bert-large-uncased
2019-03-13 03:57:37,442 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:57:37,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:57:37,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps20yb0sq
2019-03-13 03:57:44,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:57:50,473 : Computing embeddings for train/dev/test
2019-03-13 04:01:01,916 : Computed embeddings
2019-03-13 04:01:01,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:01:36,045 : [('reg:1e-05', 25.15), ('reg:0.0001', 9.09), ('reg:0.001', 0.81), ('reg:0.01', 0.2)]
2019-03-13 04:01:36,045 : Validation : best param found is reg = 1e-05 with score             25.15
2019-03-13 04:01:36,045 : Evaluating...
2019-03-13 04:01:45,798 : 
Dev acc : 25.1 Test acc : 25.2 for WORDCONTENT classification

2019-03-13 04:01:45,800 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 04:01:46,159 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 04:01:46,225 : loading BERT model bert-large-uncased
2019-03-13 04:01:46,225 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:01:46,250 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:01:46,250 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgxk6ytqk
2019-03-13 04:01:53,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:01:59,182 : Computing embeddings for train/dev/test
2019-03-13 04:04:59,137 : Computed embeddings
2019-03-13 04:04:59,138 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:05:24,536 : [('reg:1e-05', 24.45), ('reg:0.0001', 28.03), ('reg:0.001', 26.77), ('reg:0.01', 25.91)]
2019-03-13 04:05:24,536 : Validation : best param found is reg = 0.0001 with score             28.03
2019-03-13 04:05:24,536 : Evaluating...
2019-03-13 04:05:31,104 : 
Dev acc : 28.0 Test acc : 27.2 for DEPTH classification

2019-03-13 04:05:31,105 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 04:05:31,495 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 04:05:31,560 : loading BERT model bert-large-uncased
2019-03-13 04:05:31,560 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:05:31,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:05:31,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7vsksm7k
2019-03-13 04:05:39,165 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:05:44,683 : Computing embeddings for train/dev/test
2019-03-13 04:08:31,628 : Computed embeddings
2019-03-13 04:08:31,629 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:09:02,584 : [('reg:1e-05', 54.81), ('reg:0.0001', 45.86), ('reg:0.001', 42.38), ('reg:0.01', 34.13)]
2019-03-13 04:09:02,585 : Validation : best param found is reg = 1e-05 with score             54.81
2019-03-13 04:09:02,585 : Evaluating...
2019-03-13 04:09:11,487 : 
Dev acc : 54.8 Test acc : 54.9 for TOPCONSTITUENTS classification

2019-03-13 04:09:11,488 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 04:09:11,823 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 04:09:11,889 : loading BERT model bert-large-uncased
2019-03-13 04:09:11,889 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:09:12,008 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:09:12,016 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4kp43gwd
2019-03-13 04:09:19,479 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:09:25,021 : Computing embeddings for train/dev/test
2019-03-13 04:12:26,198 : Computed embeddings
2019-03-13 04:12:26,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:12:52,488 : [('reg:1e-05', 62.29), ('reg:0.0001', 62.28), ('reg:0.001', 62.19), ('reg:0.01', 66.83)]
2019-03-13 04:12:52,488 : Validation : best param found is reg = 0.01 with score             66.83
2019-03-13 04:12:52,488 : Evaluating...
2019-03-13 04:13:02,361 : 
Dev acc : 66.8 Test acc : 66.2 for BIGRAMSHIFT classification

2019-03-13 04:13:02,362 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 04:13:02,764 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 04:13:02,829 : loading BERT model bert-large-uncased
2019-03-13 04:13:02,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:13:02,858 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:13:02,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4x_d3p1s
2019-03-13 04:13:10,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:13:15,841 : Computing embeddings for train/dev/test
2019-03-13 04:16:13,007 : Computed embeddings
2019-03-13 04:16:13,007 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:16:39,244 : [('reg:1e-05', 85.68), ('reg:0.0001', 85.59), ('reg:0.001', 85.34), ('reg:0.01', 81.56)]
2019-03-13 04:16:39,244 : Validation : best param found is reg = 1e-05 with score             85.68
2019-03-13 04:16:39,244 : Evaluating...
2019-03-13 04:16:45,863 : 
Dev acc : 85.7 Test acc : 84.2 for TENSE classification

2019-03-13 04:16:45,864 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 04:16:46,270 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 04:16:46,333 : loading BERT model bert-large-uncased
2019-03-13 04:16:46,333 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:16:46,360 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:16:46,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd8nhgzlf
2019-03-13 04:16:53,765 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:16:59,788 : Computing embeddings for train/dev/test
2019-03-13 04:20:07,586 : Computed embeddings
2019-03-13 04:20:07,587 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:20:34,306 : [('reg:1e-05', 78.58), ('reg:0.0001', 78.57), ('reg:0.001', 78.73), ('reg:0.01', 78.46)]
2019-03-13 04:20:34,307 : Validation : best param found is reg = 0.001 with score             78.73
2019-03-13 04:20:34,307 : Evaluating...
2019-03-13 04:20:39,734 : 
Dev acc : 78.7 Test acc : 77.3 for SUBJNUMBER classification

2019-03-13 04:20:39,735 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 04:20:40,144 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 04:20:40,210 : loading BERT model bert-large-uncased
2019-03-13 04:20:40,210 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:20:40,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:20:40,326 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2lsegqk4
2019-03-13 04:20:47,805 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:20:53,455 : Computing embeddings for train/dev/test
2019-03-13 04:23:57,870 : Computed embeddings
2019-03-13 04:23:57,870 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:24:30,629 : [('reg:1e-05', 78.77), ('reg:0.0001', 78.8), ('reg:0.001', 78.75), ('reg:0.01', 73.2)]
2019-03-13 04:24:30,629 : Validation : best param found is reg = 0.0001 with score             78.8
2019-03-13 04:24:30,629 : Evaluating...
2019-03-13 04:24:39,343 : 
Dev acc : 78.8 Test acc : 79.2 for OBJNUMBER classification

2019-03-13 04:24:39,344 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 04:24:39,728 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 04:24:39,798 : loading BERT model bert-large-uncased
2019-03-13 04:24:39,798 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:24:39,922 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:24:39,922 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5ajmouoc
2019-03-13 04:24:47,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:24:53,056 : Computing embeddings for train/dev/test
2019-03-13 04:28:26,510 : Computed embeddings
2019-03-13 04:28:26,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:28:57,215 : [('reg:1e-05', 57.3), ('reg:0.0001', 57.26), ('reg:0.001', 57.09), ('reg:0.01', 59.81)]
2019-03-13 04:28:57,216 : Validation : best param found is reg = 0.01 with score             59.81
2019-03-13 04:28:57,216 : Evaluating...
2019-03-13 04:29:06,840 : 
Dev acc : 59.8 Test acc : 60.4 for ODDMANOUT classification

2019-03-13 04:29:06,841 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 04:29:07,427 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 04:29:07,503 : loading BERT model bert-large-uncased
2019-03-13 04:29:07,503 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:29:07,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:29:07,534 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp02i_jwee
2019-03-13 04:29:15,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:29:20,545 : Computing embeddings for train/dev/test
2019-03-13 04:32:52,058 : Computed embeddings
2019-03-13 04:32:52,059 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:33:18,809 : [('reg:1e-05', 50.09), ('reg:0.0001', 50.09), ('reg:0.001', 50.04), ('reg:0.01', 50.02)]
2019-03-13 04:33:18,809 : Validation : best param found is reg = 1e-05 with score             50.09
2019-03-13 04:33:18,809 : Evaluating...
2019-03-13 04:33:24,312 : 
Dev acc : 50.1 Test acc : 50.1 for COORDINATIONINVERSION classification

2019-03-13 04:33:24,314 : total results: {'STS12': {'MSRpar': {'pearson': (0.3811000900166462, 2.4587007867905443e-27), 'spearman': SpearmanrResult(correlation=0.4098655158827688, pvalue=9.495270287633528e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7267437325876399, 4.0124213711526215e-124), 'spearman': SpearmanrResult(correlation=0.7295872989233391, pvalue=1.4787000098206666e-125), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5016168689244701, 1.2752257044060712e-30), 'spearman': SpearmanrResult(correlation=0.6170146241611559, pvalue=1.6911837069950965e-49), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.6127092496362601, 1.6923453246298929e-78), 'spearman': SpearmanrResult(correlation=0.6522244917167976, pvalue=4.501102413483607e-92), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5438006285721408, 4.3836622728594966e-32), 'spearman': SpearmanrResult(correlation=0.4785754471616369, pvalue=3.127316748629105e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5531941139474315, 'wmean': 0.5590841048317329}, 'spearman': {'mean': 0.5774534755691396, 'wmean': 0.5849161183396534}}}, 'STS13': {'FNWN': {'pearson': (0.2462140171773147, 0.0006374566808316188), 'spearman': SpearmanrResult(correlation=0.24210515356020212, pvalue=0.0007896733710968245), 'nsamples': 189}, 'headlines': {'pearson': (0.6842363666039215, 1.1618477525716669e-104), 'spearman': SpearmanrResult(correlation=0.6748090056978536, pvalue=8.665500145328599e-101), 'nsamples': 750}, 'OnWN': {'pearson': (0.6308127667000535, 1.3783772916726623e-63), 'spearman': SpearmanrResult(correlation=0.6315142987528979, pvalue=9.124698940003725e-64), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.5204210501604299, 'wmean': 0.6090651242121224}, 'spearman': {'mean': 0.5161428193369845, 'wmean': 0.604096099931096}}}, 'STS14': {'deft-forum': {'pearson': (0.41838169838903794, 1.7007300046004532e-20), 'spearman': SpearmanrResult(correlation=0.4293149820359844, pvalue=1.3200207050967028e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7402035989625823, 2.652623865013464e-53), 'spearman': SpearmanrResult(correlation=0.6889293094462122, pvalue=1.4956128315092224e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.6288256633768027, 8.485638931384905e-84), 'spearman': SpearmanrResult(correlation=0.5965081442042522, pvalue=1.8046648408383932e-73), 'nsamples': 750}, 'images': {'pearson': (0.700808968071615, 7.737189630588173e-112), 'spearman': SpearmanrResult(correlation=0.6935967256529497, pvalue=1.1797474523759913e-108), 'nsamples': 750}, 'OnWN': {'pearson': (0.7112689430946625, 1.2527033553330965e-116), 'spearman': SpearmanrResult(correlation=0.7354447663888753, pvalue=1.440652724555564e-128), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6320150809516899, 6.958517843491713e-85), 'spearman': SpearmanrResult(correlation=0.6056441251129387, pvalue=2.862191537416529e-76), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.638583992141065, 'wmean': 0.6440058228226452}, 'spearman': {'mean': 0.6249063421402021, 'wmean': 0.6328708948718182}}}, 'STS15': {'answers-forums': {'pearson': (0.5879812162982644, 3.0475402869434095e-36), 'spearman': SpearmanrResult(correlation=0.570482131012519, pvalue=9.296921442505606e-34), 'nsamples': 375}, 'answers-students': {'pearson': (0.7119154324592928, 6.233251580994431e-117), 'spearman': SpearmanrResult(correlation=0.7144096934612214, pvalue=4.142509598000487e-118), 'nsamples': 750}, 'belief': {'pearson': (0.6067043686794356, 4.498077780380293e-39), 'spearman': SpearmanrResult(correlation=0.6264555395066104, pvalue=2.8552096737236914e-42), 'nsamples': 375}, 'headlines': {'pearson': (0.6835982867121271, 2.147317931128963e-104), 'spearman': SpearmanrResult(correlation=0.6850552384626043, pvalue=5.2700759506072716e-105), 'nsamples': 750}, 'images': {'pearson': (0.7986072765929602, 4.175878188321244e-167), 'spearman': SpearmanrResult(correlation=0.803682168086908, pvalue=8.515957139365913e-171), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.677761316148416, 'wmean': 0.6978659470633076}, 'spearman': {'mean': 0.6800169541059727, 'wmean': 0.7004039838175746}}}, 'STS16': {'answer-answer': {'pearson': (0.5825241840810087, 1.7779454027047785e-24), 'spearman': SpearmanrResult(correlation=0.5838564272329096, pvalue=1.318534215990087e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6727031315033032, 3.654199226945848e-34), 'spearman': SpearmanrResult(correlation=0.6800148141302516, pvalue=3.8048490601536113e-35), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7522336674390391, 3.4278231493089504e-43), 'spearman': SpearmanrResult(correlation=0.763485794738051, pvalue=3.503345278150766e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.8219152128019531, 4.4020910321488525e-61), 'spearman': SpearmanrResult(correlation=0.8355809065524604, pvalue=6.817869292314636e-65), 'nsamples': 244}, 'question-question': {'pearson': (0.5593857563625091, 1.3244054506688272e-18), 'spearman': SpearmanrResult(correlation=0.564334022938191, pvalue=5.6631658498094385e-19), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6777523904375625, 'wmean': 0.6795420750550748}, 'spearman': {'mean': 0.6854543931183728, 'wmean': 0.6872280826460587}}}, 'MR': {'devacc': 63.3, 'acc': 60.36, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.82, 'acc': 66.94, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.68, 'acc': 84.6, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.53, 'acc': 90.65, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.52, 'acc': 77.7, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.33, 'acc': 36.97, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.87, 'acc': 75.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.64, 'acc': 69.33, 'f1': 81.15, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.6, 'acc': 75.73, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.813336532148404, 'pearson': 0.7956620316462304, 'spearman': 0.7300819687498927, 'mse': 0.37605250499358267, 'yhat': array([3.28225057, 4.49146313, 1.39381431, ..., 3.03024652, 4.15090807,        4.84897021]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.7023353470669242, 'pearson': 0.6541282496883349, 'spearman': 0.6554524483865094, 'mse': 1.4088021018081418, 'yhat': array([1.67549229, 1.3040822 , 1.97283913, ..., 3.41452528, 3.97707576,        3.92976106]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 69.08, 'acc': 69.07, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 67.68, 'acc': 67.99, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.15, 'acc': 25.24, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.03, 'acc': 27.24, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.81, 'acc': 54.92, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 66.83, 'acc': 66.23, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.68, 'acc': 84.23, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.73, 'acc': 77.26, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.8, 'acc': 79.19, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.81, 'acc': 60.36, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.09, 'acc': 50.07, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 04:33:24,314 : STS12 p=0.5591, STS12 s=0.5849, STS13 p=0.6091, STS13 s=0.6041, STS14 p=0.6440, STS14 s=0.6329, STS15 p=0.6979, STS15 s=0.7004, STS 16 p=0.6795, STS16 s=0.6872, STS B p=0.6541, STS B s=0.6555, STS B m=1.4088, SICK-R p=0.7957, SICK-R s=0.7301, SICK-P m=0.3761
2019-03-13 04:33:24,314 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 04:33:24,314 : 0.5591,0.5849,0.6091,0.6041,0.6440,0.6329,0.6979,0.7004,0.6795,0.6872,0.6541,0.6555,1.4088,0.7957,0.7301,0.3761
2019-03-13 04:33:24,314 : MR=60.36, CR=66.94, SUBJ=90.65, MPQA=84.60, SST-B=77.70, SST-F=36.97, TREC=75.00, SICK-E=75.73, SNLI=69.07, MRPC=69.33, MRPC f=81.15
2019-03-13 04:33:24,314 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 04:33:24,314 : 60.36,66.94,90.65,84.60,77.70,36.97,75.00,75.73,69.07,69.33,81.15
2019-03-13 04:33:24,314 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 04:33:24,314 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 04:33:24,314 : na,na,na,na,na,na,na,na,na,na
2019-03-13 04:33:24,314 : SentLen=67.99, WC=25.24, TreeDepth=27.24, TopConst=54.92, BShift=66.23, Tense=84.23, SubjNum=77.26, ObjNum=79.19, SOMO=60.36, CoordInv=50.07, average=59.27
2019-03-13 04:33:24,314 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 04:33:24,315 : 67.99,25.24,27.24,54.92,66.23,84.23,77.26,79.19,60.36,50.07,59.27
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,315 : layer 11
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,315 : ********************************************************************************
2019-03-13 04:33:24,409 : ***** Transfer task : STS12 *****


2019-03-13 04:33:24,421 : loading BERT model bert-large-uncased
2019-03-13 04:33:24,422 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:33:24,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:33:24,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ijb4m15
2019-03-13 04:33:31,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:33:41,357 : MSRpar : pearson = 0.3465, spearman = 0.3752
2019-03-13 04:33:42,980 : MSRvid : pearson = 0.7005, spearman = 0.7068
2019-03-13 04:33:44,379 : SMTeuroparl : pearson = 0.4998, spearman = 0.6030
2019-03-13 04:33:47,045 : surprise.OnWN : pearson = 0.5898, spearman = 0.6301
2019-03-13 04:33:48,457 : surprise.SMTnews : pearson = 0.5390, spearman = 0.4671
2019-03-13 04:33:48,458 : ALL (weighted average) : Pearson = 0.5380,             Spearman = 0.5622
2019-03-13 04:33:48,458 : ALL (average) : Pearson = 0.5351,             Spearman = 0.5564

2019-03-13 04:33:48,458 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 04:33:48,468 : loading BERT model bert-large-uncased
2019-03-13 04:33:48,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:33:48,486 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:33:48,486 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa3kwfhrl
2019-03-13 04:33:55,901 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:34:02,791 : FNWN : pearson = 0.2288, spearman = 0.2279
2019-03-13 04:34:04,668 : headlines : pearson = 0.6678, spearman = 0.6582
2019-03-13 04:34:06,122 : OnWN : pearson = 0.5679, spearman = 0.5726
2019-03-13 04:34:06,122 : ALL (weighted average) : Pearson = 0.5751,             Spearman = 0.5720
2019-03-13 04:34:06,122 : ALL (average) : Pearson = 0.4882,             Spearman = 0.4862

2019-03-13 04:34:06,122 : ***** Transfer task : STS14 *****


2019-03-13 04:34:06,137 : loading BERT model bert-large-uncased
2019-03-13 04:34:06,137 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:34:06,155 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:34:06,155 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppp5cyrtt
2019-03-13 04:34:13,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:34:20,541 : deft-forum : pearson = 0.4095, spearman = 0.4251
2019-03-13 04:34:22,156 : deft-news : pearson = 0.7291, spearman = 0.6796
2019-03-13 04:34:24,298 : headlines : pearson = 0.6070, spearman = 0.5775
2019-03-13 04:34:26,355 : images : pearson = 0.6524, spearman = 0.6479
2019-03-13 04:34:28,460 : OnWN : pearson = 0.6771, spearman = 0.7057
2019-03-13 04:34:31,286 : tweet-news : pearson = 0.6178, spearman = 0.5939
2019-03-13 04:34:31,287 : ALL (weighted average) : Pearson = 0.6183,             Spearman = 0.6104
2019-03-13 04:34:31,287 : ALL (average) : Pearson = 0.6155,             Spearman = 0.6049

2019-03-13 04:34:31,287 : ***** Transfer task : STS15 *****


2019-03-13 04:34:31,320 : loading BERT model bert-large-uncased
2019-03-13 04:34:31,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:34:31,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:34:31,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprmfi59fg
2019-03-13 04:34:38,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:34:46,103 : answers-forums : pearson = 0.5729, spearman = 0.5412
2019-03-13 04:34:48,161 : answers-students : pearson = 0.7022, spearman = 0.7067
2019-03-13 04:34:50,187 : belief : pearson = 0.5973, spearman = 0.6197
2019-03-13 04:34:52,412 : headlines : pearson = 0.6548, spearman = 0.6590
2019-03-13 04:34:54,523 : images : pearson = 0.7766, spearman = 0.7820
2019-03-13 04:34:54,523 : ALL (weighted average) : Pearson = 0.6797,             Spearman = 0.6820
2019-03-13 04:34:54,523 : ALL (average) : Pearson = 0.6608,             Spearman = 0.6617

2019-03-13 04:34:54,523 : ***** Transfer task : STS16 *****


2019-03-13 04:34:54,592 : loading BERT model bert-large-uncased
2019-03-13 04:34:54,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:34:54,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:34:54,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkjtcc36q
2019-03-13 04:35:02,076 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:35:08,512 : answer-answer : pearson = 0.5690, spearman = 0.5656
2019-03-13 04:35:09,165 : headlines : pearson = 0.6594, spearman = 0.6622
2019-03-13 04:35:10,035 : plagiarism : pearson = 0.7508, spearman = 0.7660
2019-03-13 04:35:11,509 : postediting : pearson = 0.8094, spearman = 0.8349
2019-03-13 04:35:12,109 : question-question : pearson = 0.4663, spearman = 0.4718
2019-03-13 04:35:12,109 : ALL (weighted average) : Pearson = 0.6546,             Spearman = 0.6636
2019-03-13 04:35:12,109 : ALL (average) : Pearson = 0.6510,             Spearman = 0.6601

2019-03-13 04:35:12,109 : ***** Transfer task : MR *****


2019-03-13 04:35:12,129 : loading BERT model bert-large-uncased
2019-03-13 04:35:12,129 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:35:12,148 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:35:12,148 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppvv2lmsl
2019-03-13 04:35:19,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:35:25,140 : Generating sentence embeddings
2019-03-13 04:35:56,172 : Generated sentence embeddings
2019-03-13 04:35:56,173 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:36:06,907 : Best param found at split 1: l2reg = 0.001                 with score 63.42
2019-03-13 04:36:19,996 : Best param found at split 2: l2reg = 0.0001                 with score 62.44
2019-03-13 04:36:34,573 : Best param found at split 3: l2reg = 0.01                 with score 62.33
2019-03-13 04:36:45,776 : Best param found at split 4: l2reg = 1e-05                 with score 60.5
2019-03-13 04:36:57,869 : Best param found at split 5: l2reg = 0.0001                 with score 57.3
2019-03-13 04:36:58,668 : Dev acc : 61.2 Test acc : 59.07

2019-03-13 04:36:58,669 : ***** Transfer task : CR *****


2019-03-13 04:36:58,676 : loading BERT model bert-large-uncased
2019-03-13 04:36:58,676 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:36:58,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:36:58,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplds9pcb3
2019-03-13 04:37:06,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:37:11,637 : Generating sentence embeddings
2019-03-13 04:37:19,833 : Generated sentence embeddings
2019-03-13 04:37:19,833 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:37:22,941 : Best param found at split 1: l2reg = 0.0001                 with score 67.31
2019-03-13 04:37:26,175 : Best param found at split 2: l2reg = 1e-05                 with score 70.15
2019-03-13 04:37:29,423 : Best param found at split 3: l2reg = 0.001                 with score 68.81
2019-03-13 04:37:32,917 : Best param found at split 4: l2reg = 0.0001                 with score 71.63
2019-03-13 04:37:36,245 : Best param found at split 5: l2reg = 0.01                 with score 71.17
2019-03-13 04:37:36,450 : Dev acc : 69.81 Test acc : 66.57

2019-03-13 04:37:36,450 : ***** Transfer task : MPQA *****


2019-03-13 04:37:36,456 : loading BERT model bert-large-uncased
2019-03-13 04:37:36,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:37:36,476 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:37:36,476 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp13rs7mda
2019-03-13 04:37:43,911 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:37:49,416 : Generating sentence embeddings
2019-03-13 04:37:56,882 : Generated sentence embeddings
2019-03-13 04:37:56,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:38:08,878 : Best param found at split 1: l2reg = 0.001                 with score 84.43
2019-03-13 04:38:19,858 : Best param found at split 2: l2reg = 0.001                 with score 84.23
2019-03-13 04:38:31,711 : Best param found at split 3: l2reg = 0.0001                 with score 85.22
2019-03-13 04:38:42,679 : Best param found at split 4: l2reg = 0.001                 with score 85.14
2019-03-13 04:38:55,015 : Best param found at split 5: l2reg = 0.01                 with score 85.48
2019-03-13 04:38:55,909 : Dev acc : 84.9 Test acc : 83.71

2019-03-13 04:38:55,910 : ***** Transfer task : SUBJ *****


2019-03-13 04:38:55,926 : loading BERT model bert-large-uncased
2019-03-13 04:38:55,926 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:38:55,946 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:38:55,946 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuqujz71u
2019-03-13 04:39:03,444 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:39:08,943 : Generating sentence embeddings
2019-03-13 04:39:39,370 : Generated sentence embeddings
2019-03-13 04:39:39,371 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:39:49,604 : Best param found at split 1: l2reg = 0.0001                 with score 90.9
2019-03-13 04:39:57,194 : Best param found at split 2: l2reg = 0.001                 with score 89.52
2019-03-13 04:40:06,378 : Best param found at split 3: l2reg = 0.001                 with score 90.41
2019-03-13 04:40:17,804 : Best param found at split 4: l2reg = 0.0001                 with score 91.02
2019-03-13 04:40:30,138 : Best param found at split 5: l2reg = 0.001                 with score 91.02
2019-03-13 04:40:30,652 : Dev acc : 90.57 Test acc : 91.22

2019-03-13 04:40:30,653 : ***** Transfer task : SST Binary classification *****


2019-03-13 04:40:30,783 : loading BERT model bert-large-uncased
2019-03-13 04:40:30,783 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:40:30,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:40:30,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwpz3ps60
2019-03-13 04:40:38,232 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:40:43,913 : Computing embedding for train
2019-03-13 04:42:22,450 : Computed train embeddings
2019-03-13 04:42:22,450 : Computing embedding for dev
2019-03-13 04:42:24,593 : Computed dev embeddings
2019-03-13 04:42:24,593 : Computing embedding for test
2019-03-13 04:42:29,104 : Computed test embeddings
2019-03-13 04:42:29,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:42:46,696 : [('reg:1e-05', 77.29), ('reg:0.0001', 77.41), ('reg:0.001', 76.49), ('reg:0.01', 75.69)]
2019-03-13 04:42:46,696 : Validation : best param found is reg = 0.0001 with score             77.41
2019-03-13 04:42:46,697 : Evaluating...
2019-03-13 04:42:51,114 : 
Dev acc : 77.41 Test acc : 76.94 for             SST Binary classification

2019-03-13 04:42:51,114 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 04:42:51,168 : loading BERT model bert-large-uncased
2019-03-13 04:42:51,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:42:51,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:42:51,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyn0rjasw
2019-03-13 04:42:58,643 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:43:04,181 : Computing embedding for train
2019-03-13 04:43:25,765 : Computed train embeddings
2019-03-13 04:43:25,765 : Computing embedding for dev
2019-03-13 04:43:28,580 : Computed dev embeddings
2019-03-13 04:43:28,580 : Computing embedding for test
2019-03-13 04:43:34,133 : Computed test embeddings
2019-03-13 04:43:34,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:43:37,647 : [('reg:1e-05', 29.25), ('reg:0.0001', 38.15), ('reg:0.001', 33.06), ('reg:0.01', 31.43)]
2019-03-13 04:43:37,647 : Validation : best param found is reg = 0.0001 with score             38.15
2019-03-13 04:43:37,647 : Evaluating...
2019-03-13 04:43:38,470 : 
Dev acc : 38.15 Test acc : 38.01 for             SST Fine-Grained classification

2019-03-13 04:43:38,471 : ***** Transfer task : TREC *****


2019-03-13 04:43:38,485 : loading BERT model bert-large-uncased
2019-03-13 04:43:38,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:43:38,504 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:43:38,504 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkvp6ppim
2019-03-13 04:43:45,967 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:43:59,096 : Computed train embeddings
2019-03-13 04:43:59,676 : Computed test embeddings
2019-03-13 04:43:59,676 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:44:07,730 : [('reg:1e-05', 67.06), ('reg:0.0001', 68.25), ('reg:0.001', 63.22), ('reg:0.01', 57.02)]
2019-03-13 04:44:07,730 : Cross-validation : best param found is reg = 0.0001             with score 68.25
2019-03-13 04:44:07,730 : Evaluating...
2019-03-13 04:44:08,355 : 
Dev acc : 68.25 Test acc : 85.2             for TREC

2019-03-13 04:44:08,355 : ***** Transfer task : MRPC *****


2019-03-13 04:44:08,376 : loading BERT model bert-large-uncased
2019-03-13 04:44:08,376 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:44:08,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:44:08,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj0cnd36u
2019-03-13 04:44:15,832 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:44:21,199 : Computing embedding for train
2019-03-13 04:44:43,171 : Computed train embeddings
2019-03-13 04:44:43,171 : Computing embedding for test
2019-03-13 04:44:52,769 : Computed test embeddings
2019-03-13 04:44:52,791 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:44:57,229 : [('reg:1e-05', 71.29), ('reg:0.0001', 72.13), ('reg:0.001', 72.1), ('reg:0.01', 70.17)]
2019-03-13 04:44:57,230 : Cross-validation : best param found is reg = 0.0001             with score 72.13
2019-03-13 04:44:57,230 : Evaluating...
2019-03-13 04:44:57,499 : Dev acc : 72.13 Test acc 72.81; Test F1 79.81 for MRPC.

2019-03-13 04:44:57,499 : ***** Transfer task : SICK-Entailment*****


2019-03-13 04:44:57,524 : loading BERT model bert-large-uncased
2019-03-13 04:44:57,524 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:44:57,542 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:44:57,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp781nwbet
2019-03-13 04:45:05,022 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:45:10,558 : Computing embedding for train
2019-03-13 04:45:21,680 : Computed train embeddings
2019-03-13 04:45:21,680 : Computing embedding for dev
2019-03-13 04:45:23,198 : Computed dev embeddings
2019-03-13 04:45:23,198 : Computing embedding for test
2019-03-13 04:45:35,121 : Computed test embeddings
2019-03-13 04:45:35,158 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:45:36,481 : [('reg:1e-05', 71.0), ('reg:0.0001', 73.4), ('reg:0.001', 75.2), ('reg:0.01', 69.4)]
2019-03-13 04:45:36,481 : Validation : best param found is reg = 0.001 with score             75.2
2019-03-13 04:45:36,481 : Evaluating...
2019-03-13 04:45:36,802 : 
Dev acc : 75.2 Test acc : 76.44 for                        SICK entailment

2019-03-13 04:45:36,803 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 04:45:36,868 : loading BERT model bert-large-uncased
2019-03-13 04:45:36,868 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:45:36,887 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:45:36,887 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpccdo7dft
2019-03-13 04:45:44,337 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:45:49,899 : Computing embedding for train
2019-03-13 04:46:01,045 : Computed train embeddings
2019-03-13 04:46:01,045 : Computing embedding for dev
2019-03-13 04:46:02,565 : Computed dev embeddings
2019-03-13 04:46:02,565 : Computing embedding for test
2019-03-13 04:46:14,527 : Computed test embeddings
2019-03-13 04:46:29,140 : Dev : Pearson 0.8001557573642852
2019-03-13 04:46:29,141 : Test : Pearson 0.7976650600746085 Spearman 0.7234854887173041 MSE 0.37146539568381876                        for SICK Relatedness

2019-03-13 04:46:29,141 : 

***** Transfer task : STSBenchmark*****


2019-03-13 04:46:29,209 : loading BERT model bert-large-uncased
2019-03-13 04:46:29,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:46:29,229 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:46:29,229 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnbd2pl2e
2019-03-13 04:46:36,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:46:42,214 : Computing embedding for train
2019-03-13 04:47:00,573 : Computed train embeddings
2019-03-13 04:47:00,573 : Computing embedding for dev
2019-03-13 04:47:06,130 : Computed dev embeddings
2019-03-13 04:47:06,130 : Computing embedding for test
2019-03-13 04:47:10,663 : Computed test embeddings
2019-03-13 04:47:29,851 : Dev : Pearson 0.705869900567329
2019-03-13 04:47:29,851 : Test : Pearson 0.6512970919223321 Spearman 0.6515105783478904 MSE 1.393495654411708                        for SICK Relatedness

2019-03-13 04:47:29,851 : ***** Transfer task : SNLI Entailment*****


2019-03-13 04:47:34,510 : loading BERT model bert-large-uncased
2019-03-13 04:47:34,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:47:34,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:47:34,579 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn5rybche
2019-03-13 04:47:42,055 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:47:48,906 : PROGRESS (encoding): 0.00%
2019-03-13 04:50:32,139 : PROGRESS (encoding): 14.56%
2019-03-13 04:53:37,385 : PROGRESS (encoding): 29.12%
2019-03-13 04:56:43,215 : PROGRESS (encoding): 43.69%
2019-03-13 05:00:01,480 : PROGRESS (encoding): 58.25%
2019-03-13 05:03:42,327 : PROGRESS (encoding): 72.81%
2019-03-13 05:07:21,960 : PROGRESS (encoding): 87.37%
2019-03-13 05:11:19,711 : PROGRESS (encoding): 0.00%
2019-03-13 05:11:49,631 : PROGRESS (encoding): 0.00%
2019-03-13 05:12:18,381 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:13:19,486 : [('reg:1e-09', 67.1)]
2019-03-13 05:13:19,486 : Validation : best param found is reg = 1e-09 with score             67.1
2019-03-13 05:13:19,486 : Evaluating...
2019-03-13 05:14:17,028 : Dev acc : 67.1 Test acc : 68.04 for SNLI

2019-03-13 05:14:17,028 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 05:14:17,237 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 05:14:18,209 : loading BERT model bert-large-uncased
2019-03-13 05:14:18,209 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:14:18,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:14:18,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3ofc43f6
2019-03-13 05:14:25,663 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:14:31,179 : Computing embeddings for train/dev/test
2019-03-13 05:17:58,806 : Computed embeddings
2019-03-13 05:17:58,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:18:22,412 : [('reg:1e-05', 58.32), ('reg:0.0001', 63.78), ('reg:0.001', 58.22), ('reg:0.01', 60.61)]
2019-03-13 05:18:22,412 : Validation : best param found is reg = 0.0001 with score             63.78
2019-03-13 05:18:22,412 : Evaluating...
2019-03-13 05:18:27,856 : 
Dev acc : 63.8 Test acc : 63.4 for LENGTH classification

2019-03-13 05:18:27,856 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 05:18:28,223 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 05:18:28,268 : loading BERT model bert-large-uncased
2019-03-13 05:18:28,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:18:28,296 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:18:28,296 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpae9fs91d
2019-03-13 05:18:35,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:18:41,420 : Computing embeddings for train/dev/test
2019-03-13 05:21:52,963 : Computed embeddings
2019-03-13 05:21:52,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:22:34,672 : [('reg:1e-05', 24.3), ('reg:0.0001', 8.62), ('reg:0.001', 0.71), ('reg:0.01', 0.2)]
2019-03-13 05:22:34,672 : Validation : best param found is reg = 1e-05 with score             24.3
2019-03-13 05:22:34,672 : Evaluating...
2019-03-13 05:22:48,977 : 
Dev acc : 24.3 Test acc : 24.0 for WORDCONTENT classification

2019-03-13 05:22:48,978 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 05:22:49,333 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 05:22:49,399 : loading BERT model bert-large-uncased
2019-03-13 05:22:49,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:22:49,423 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:22:49,424 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv777wg9t
2019-03-13 05:22:56,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:23:02,443 : Computing embeddings for train/dev/test
2019-03-13 05:26:02,265 : Computed embeddings
2019-03-13 05:26:02,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:26:31,224 : [('reg:1e-05', 25.27), ('reg:0.0001', 27.04), ('reg:0.001', 25.95), ('reg:0.01', 25.74)]
2019-03-13 05:26:31,225 : Validation : best param found is reg = 0.0001 with score             27.04
2019-03-13 05:26:31,225 : Evaluating...
2019-03-13 05:26:36,698 : 
Dev acc : 27.0 Test acc : 27.8 for DEPTH classification

2019-03-13 05:26:36,699 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 05:26:37,076 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 05:26:37,139 : loading BERT model bert-large-uncased
2019-03-13 05:26:37,139 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:26:37,248 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:26:37,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpii121nld
2019-03-13 05:26:44,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:26:50,128 : Computing embeddings for train/dev/test
2019-03-13 05:29:36,924 : Computed embeddings
2019-03-13 05:29:36,925 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:30:14,133 : [('reg:1e-05', 51.05), ('reg:0.0001', 56.35), ('reg:0.001', 48.78), ('reg:0.01', 28.16)]
2019-03-13 05:30:14,133 : Validation : best param found is reg = 0.0001 with score             56.35
2019-03-13 05:30:14,133 : Evaluating...
2019-03-13 05:30:24,416 : 
Dev acc : 56.4 Test acc : 56.3 for TOPCONSTITUENTS classification

2019-03-13 05:30:24,417 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 05:30:24,756 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 05:30:24,822 : loading BERT model bert-large-uncased
2019-03-13 05:30:24,822 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:30:24,938 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:30:24,938 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnd3yy3pl
2019-03-13 05:30:32,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:30:37,899 : Computing embeddings for train/dev/test
2019-03-13 05:33:38,821 : Computed embeddings
2019-03-13 05:33:38,821 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:34:18,821 : [('reg:1e-05', 82.17), ('reg:0.0001', 82.3), ('reg:0.001', 82.14), ('reg:0.01', 77.11)]
2019-03-13 05:34:18,821 : Validation : best param found is reg = 0.0001 with score             82.3
2019-03-13 05:34:18,821 : Evaluating...
2019-03-13 05:34:29,568 : 
Dev acc : 82.3 Test acc : 81.5 for BIGRAMSHIFT classification

2019-03-13 05:34:29,569 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 05:34:30,147 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 05:34:30,214 : loading BERT model bert-large-uncased
2019-03-13 05:34:30,215 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:34:30,245 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:34:30,245 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpufbq0a4x
2019-03-13 05:34:37,664 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:34:43,245 : Computing embeddings for train/dev/test
2019-03-13 05:37:40,388 : Computed embeddings
2019-03-13 05:37:40,389 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:38:09,098 : [('reg:1e-05', 86.18), ('reg:0.0001', 86.25), ('reg:0.001', 86.26), ('reg:0.01', 83.96)]
2019-03-13 05:38:09,098 : Validation : best param found is reg = 0.001 with score             86.26
2019-03-13 05:38:09,098 : Evaluating...
2019-03-13 05:38:16,291 : 
Dev acc : 86.3 Test acc : 85.5 for TENSE classification

2019-03-13 05:38:16,292 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 05:38:16,672 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 05:38:16,738 : loading BERT model bert-large-uncased
2019-03-13 05:38:16,738 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:38:16,764 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:38:16,764 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8n85t7pm
2019-03-13 05:38:24,242 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:38:29,726 : Computing embeddings for train/dev/test
2019-03-13 05:41:37,476 : Computed embeddings
2019-03-13 05:41:37,476 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:42:05,354 : [('reg:1e-05', 80.93), ('reg:0.0001', 80.95), ('reg:0.001', 80.86), ('reg:0.01', 79.18)]
2019-03-13 05:42:05,354 : Validation : best param found is reg = 0.0001 with score             80.95
2019-03-13 05:42:05,354 : Evaluating...
2019-03-13 05:42:12,607 : 
Dev acc : 81.0 Test acc : 79.8 for SUBJNUMBER classification

2019-03-13 05:42:12,608 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 05:42:13,019 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 05:42:13,085 : loading BERT model bert-large-uncased
2019-03-13 05:42:13,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:42:13,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:42:13,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0eoxk7k5
2019-03-13 05:42:20,569 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:42:26,101 : Computing embeddings for train/dev/test
2019-03-13 05:45:30,492 : Computed embeddings
2019-03-13 05:45:30,492 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:46:05,206 : [('reg:1e-05', 78.64), ('reg:0.0001', 78.67), ('reg:0.001', 78.29), ('reg:0.01', 75.38)]
2019-03-13 05:46:05,207 : Validation : best param found is reg = 0.0001 with score             78.67
2019-03-13 05:46:05,207 : Evaluating...
2019-03-13 05:46:14,948 : 
Dev acc : 78.7 Test acc : 79.0 for OBJNUMBER classification

2019-03-13 05:46:14,949 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 05:46:15,336 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 05:46:15,404 : loading BERT model bert-large-uncased
2019-03-13 05:46:15,404 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:46:15,523 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:46:15,523 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpja0kq_st
2019-03-13 05:46:22,996 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:46:28,599 : Computing embeddings for train/dev/test
2019-03-13 05:50:01,912 : Computed embeddings
2019-03-13 05:50:01,913 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:50:30,221 : [('reg:1e-05', 56.31), ('reg:0.0001', 56.33), ('reg:0.001', 56.37), ('reg:0.01', 57.73)]
2019-03-13 05:50:30,221 : Validation : best param found is reg = 0.01 with score             57.73
2019-03-13 05:50:30,221 : Evaluating...
2019-03-13 05:50:37,780 : 
Dev acc : 57.7 Test acc : 57.7 for ODDMANOUT classification

2019-03-13 05:50:37,781 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 05:50:38,174 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 05:50:38,250 : loading BERT model bert-large-uncased
2019-03-13 05:50:38,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:50:38,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:50:38,374 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyvq_9j_c
2019-03-13 05:50:45,853 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:50:51,414 : Computing embeddings for train/dev/test
2019-03-13 05:54:22,909 : Computed embeddings
2019-03-13 05:54:22,909 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:55:00,048 : [('reg:1e-05', 56.84), ('reg:0.0001', 56.82), ('reg:0.001', 56.71), ('reg:0.01', 50.11)]
2019-03-13 05:55:00,049 : Validation : best param found is reg = 1e-05 with score             56.84
2019-03-13 05:55:00,049 : Evaluating...
2019-03-13 05:55:10,988 : 
Dev acc : 56.8 Test acc : 57.2 for COORDINATIONINVERSION classification

2019-03-13 05:55:10,990 : total results: {'STS12': {'MSRpar': {'pearson': (0.34654398493178956, 1.3814776903426432e-22), 'spearman': SpearmanrResult(correlation=0.37520587221901375, pvalue=1.746831779659952e-26), 'nsamples': 750}, 'MSRvid': {'pearson': (0.7005176774510349, 1.0447264002496527e-111), 'spearman': SpearmanrResult(correlation=0.7067522591654187, pvalue=1.557861940622299e-114), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4998040020849636, 2.226238405499508e-30), 'spearman': SpearmanrResult(correlation=0.6029872713170985, pvalue=8.7726315079644e-47), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.589782396927522, 1.822688991638697e-71), 'spearman': SpearmanrResult(correlation=0.6300820539351772, pvalue=3.1793051395111972e-84), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5390379386018969, 1.881757487793577e-31), 'spearman': SpearmanrResult(correlation=0.4670930772591656, pvalue=5.1427252964868913e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5351371999994414, 'wmean': 0.5380048966994578}, 'spearman': {'mean': 0.5564241067791748, 'wmean': 0.5621529711552968}}}, 'STS13': {'FNWN': {'pearson': (0.22875605578527064, 0.001544965824729856), 'spearman': SpearmanrResult(correlation=0.22790025424847768, pvalue=0.0016108498260760241), 'nsamples': 189}, 'headlines': {'pearson': (0.6677746934111853, 5.4288950264857046e-98), 'spearman': SpearmanrResult(correlation=0.658177835143104, pvalue=2.685730289158536e-94), 'nsamples': 750}, 'OnWN': {'pearson': (0.567938439069778, 3.1738679215703507e-49), 'spearman': SpearmanrResult(correlation=0.5726155416474832, pvalue=3.454969930611807e-50), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.48815639608874467, 'wmean': 0.5751195859466337}, 'spearman': {'mean': 0.4862312103463549, 'wmean': 0.5719625621830189}}}, 'STS14': {'deft-forum': {'pearson': (0.4094820391643703, 1.2726626488588143e-19), 'spearman': SpearmanrResult(correlation=0.4250971606793232, pvalue=3.57782803437052e-21), 'nsamples': 450}, 'deft-news': {'pearson': (0.7290500735807961, 5.422980256217783e-51), 'spearman': SpearmanrResult(correlation=0.6796434924433605, pvalue=5.3366000958732346e-42), 'nsamples': 300}, 'headlines': {'pearson': (0.6069773694261259, 1.0977458486916494e-76), 'spearman': SpearmanrResult(correlation=0.5774528613631951, pvalue=6.534003391934136e-68), 'nsamples': 750}, 'images': {'pearson': (0.6523648056635839, 3.994601959329002e-92), 'spearman': SpearmanrResult(correlation=0.6479010903184218, pvalue=1.7273320230313565e-90), 'nsamples': 750}, 'OnWN': {'pearson': (0.6771283045769405, 9.965387488014047e-102), 'spearman': SpearmanrResult(correlation=0.7057485015835576, pvalue=4.4937556682476984e-114), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6178448144296348, 3.741855939529946e-80), 'spearman': SpearmanrResult(correlation=0.5938555124504089, pvalue=1.1286110008723877e-72), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6154745678069086, 'wmean': 0.618324909405445}, 'spearman': {'mean': 0.6049497698063778, 'wmean': 0.6103747318201044}}}, 'STS15': {'answers-forums': {'pearson': (0.5728565228173277, 4.364529163750152e-34), 'spearman': SpearmanrResult(correlation=0.5412437650900485, pvalue=6.3488129748522e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.702221391710442, 1.7945052616529258e-112), 'spearman': SpearmanrResult(correlation=0.7066776327158621, pvalue=1.6857836484346654e-114), 'nsamples': 750}, 'belief': {'pearson': (0.5972627031199076, 1.2700760168583824e-37), 'spearman': SpearmanrResult(correlation=0.619711854911615, pvalue=3.7406219660516447e-41), 'nsamples': 375}, 'headlines': {'pearson': (0.6548334800635391, 4.839197380049587e-93), 'spearman': SpearmanrResult(correlation=0.6589971045757164, pvalue=1.315198492623263e-94), 'nsamples': 750}, 'images': {'pearson': (0.7765805305815959, 3.0923550457323036e-152), 'spearman': SpearmanrResult(correlation=0.7819597651327981, pvalue=1.0466672446188548e-155), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6607509256585625, 'wmean': 0.6796737538310487}, 'spearman': {'mean': 0.661718024485208, 'wmean': 0.6820280781063021}}}, 'STS16': {'answer-answer': {'pearson': (0.5690057658938324, 3.4220553620150313e-23), 'spearman': SpearmanrResult(correlation=0.565578916121576, pvalue=7.088772405499301e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6594213205857414, 1.8934545586989355e-32), 'spearman': SpearmanrResult(correlation=0.6622019617647061, pvalue=8.424785405920182e-33), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7508340119479766, 5.95866526206953e-43), 'spearman': SpearmanrResult(correlation=0.7660337933627259, pvalue=1.1974936181927202e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.8094177945941048, 7.122704457673499e-58), 'spearman': SpearmanrResult(correlation=0.8349458291372014, pvalue=1.043218799548777e-64), 'nsamples': 244}, 'question-question': {'pearson': (0.4662858005040528, 1.116035453775893e-12), 'spearman': SpearmanrResult(correlation=0.47184545605135353, pvalue=5.522777657522394e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6509929387051416, 'wmean': 0.6546095027801234}, 'spearman': {'mean': 0.6601211912875126, 'wmean': 0.6636387759459773}}}, 'MR': {'devacc': 61.2, 'acc': 59.07, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.81, 'acc': 66.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.9, 'acc': 83.71, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.57, 'acc': 91.22, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.41, 'acc': 76.94, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.15, 'acc': 38.01, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.25, 'acc': 85.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.13, 'acc': 72.81, 'f1': 79.81, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.2, 'acc': 76.44, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.8001557573642852, 'pearson': 0.7976650600746085, 'spearman': 0.7234854887173041, 'mse': 0.37146539568381876, 'yhat': array([3.46085409, 4.80143062, 1.65530347, ..., 3.01084118, 4.19994332,        4.57825787]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.705869900567329, 'pearson': 0.6512970919223321, 'spearman': 0.6515105783478904, 'mse': 1.393495654411708, 'yhat': array([2.189122  , 2.00368375, 1.41262254, ..., 4.31809741, 3.80796599,        3.53379693]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.1, 'acc': 68.04, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 63.78, 'acc': 63.35, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 24.3, 'acc': 24.0, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.04, 'acc': 27.78, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 56.35, 'acc': 56.3, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 82.3, 'acc': 81.46, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.26, 'acc': 85.53, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.95, 'acc': 79.79, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.67, 'acc': 78.96, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.73, 'acc': 57.71, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.84, 'acc': 57.18, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 05:55:10,990 : STS12 p=0.5380, STS12 s=0.5622, STS13 p=0.5751, STS13 s=0.5720, STS14 p=0.6183, STS14 s=0.6104, STS15 p=0.6797, STS15 s=0.6820, STS 16 p=0.6546, STS16 s=0.6636, STS B p=0.6513, STS B s=0.6515, STS B m=1.3935, SICK-R p=0.7977, SICK-R s=0.7235, SICK-P m=0.3715
2019-03-13 05:55:10,990 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 05:55:10,990 : 0.5380,0.5622,0.5751,0.5720,0.6183,0.6104,0.6797,0.6820,0.6546,0.6636,0.6513,0.6515,1.3935,0.7977,0.7235,0.3715
2019-03-13 05:55:10,990 : MR=59.07, CR=66.57, SUBJ=91.22, MPQA=83.71, SST-B=76.94, SST-F=38.01, TREC=85.20, SICK-E=76.44, SNLI=68.04, MRPC=72.81, MRPC f=79.81
2019-03-13 05:55:10,990 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 05:55:10,990 : 59.07,66.57,91.22,83.71,76.94,38.01,85.20,76.44,68.04,72.81,79.81
2019-03-13 05:55:10,990 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 05:55:10,990 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 05:55:10,990 : na,na,na,na,na,na,na,na,na,na
2019-03-13 05:55:10,990 : SentLen=63.35, WC=24.00, TreeDepth=27.78, TopConst=56.30, BShift=81.46, Tense=85.53, SubjNum=79.79, ObjNum=78.96, SOMO=57.71, CoordInv=57.18, average=61.21
2019-03-13 05:55:10,990 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 05:55:10,990 : 63.35,24.00,27.78,56.30,81.46,85.53,79.79,78.96,57.71,57.18,61.21
2019-03-13 05:55:10,990 : ********************************************************************************
2019-03-13 05:55:10,990 : ********************************************************************************
2019-03-13 05:55:10,990 : ********************************************************************************
2019-03-13 05:55:10,991 : layer 12
2019-03-13 05:55:10,991 : ********************************************************************************
2019-03-13 05:55:10,991 : ********************************************************************************
2019-03-13 05:55:10,991 : ********************************************************************************
2019-03-13 05:55:11,084 : ***** Transfer task : STS12 *****


2019-03-13 05:55:11,122 : loading BERT model bert-large-uncased
2019-03-13 05:55:11,122 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:55:11,138 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:55:11,138 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp631kbq5l
2019-03-13 05:55:18,593 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:55:28,181 : MSRpar : pearson = 0.3224, spearman = 0.3550
2019-03-13 05:55:29,800 : MSRvid : pearson = 0.6695, spearman = 0.6763
2019-03-13 05:55:31,193 : SMTeuroparl : pearson = 0.4846, spearman = 0.5769
2019-03-13 05:55:33,856 : surprise.OnWN : pearson = 0.5689, spearman = 0.6085
2019-03-13 05:55:35,271 : surprise.SMTnews : pearson = 0.5395, spearman = 0.4697
2019-03-13 05:55:35,271 : ALL (weighted average) : Pearson = 0.5174,             Spearman = 0.5412
2019-03-13 05:55:35,271 : ALL (average) : Pearson = 0.5170,             Spearman = 0.5373

2019-03-13 05:55:35,271 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 05:55:35,281 : loading BERT model bert-large-uncased
2019-03-13 05:55:35,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:55:35,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:55:35,298 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx3yp2f2h
2019-03-13 05:55:42,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:55:49,696 : FNWN : pearson = 0.1995, spearman = 0.2000
2019-03-13 05:55:51,566 : headlines : pearson = 0.6464, spearman = 0.6394
2019-03-13 05:55:53,014 : OnWN : pearson = 0.5233, spearman = 0.5249
2019-03-13 05:55:53,014 : ALL (weighted average) : Pearson = 0.5441,             Spearman = 0.5412
2019-03-13 05:55:53,014 : ALL (average) : Pearson = 0.4564,             Spearman = 0.4548

2019-03-13 05:55:53,015 : ***** Transfer task : STS14 *****


2019-03-13 05:55:53,057 : loading BERT model bert-large-uncased
2019-03-13 05:55:53,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:55:53,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:55:53,075 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqchzx4_k
2019-03-13 05:56:00,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:56:07,439 : deft-forum : pearson = 0.3896, spearman = 0.4015
2019-03-13 05:56:09,051 : deft-news : pearson = 0.7166, spearman = 0.6702
2019-03-13 05:56:11,191 : headlines : pearson = 0.5809, spearman = 0.5543
2019-03-13 05:56:13,241 : images : pearson = 0.6055, spearman = 0.6024
2019-03-13 05:56:15,343 : OnWN : pearson = 0.6399, spearman = 0.6707
2019-03-13 05:56:18,161 : tweet-news : pearson = 0.5944, spearman = 0.5687
2019-03-13 05:56:18,162 : ALL (weighted average) : Pearson = 0.5882,             Spearman = 0.5810
2019-03-13 05:56:18,162 : ALL (average) : Pearson = 0.5878,             Spearman = 0.5780

2019-03-13 05:56:18,162 : ***** Transfer task : STS15 *****


2019-03-13 05:56:18,193 : loading BERT model bert-large-uncased
2019-03-13 05:56:18,193 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:56:18,242 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:56:18,242 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5jfvr9ph
2019-03-13 05:56:25,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:56:33,012 : answers-forums : pearson = 0.5416, spearman = 0.5043
2019-03-13 05:56:35,068 : answers-students : pearson = 0.6852, spearman = 0.6901
2019-03-13 05:56:37,087 : belief : pearson = 0.5646, spearman = 0.5938
2019-03-13 05:56:39,310 : headlines : pearson = 0.6346, spearman = 0.6399
2019-03-13 05:56:41,422 : images : pearson = 0.7469, spearman = 0.7537
2019-03-13 05:56:41,422 : ALL (weighted average) : Pearson = 0.6550,             Spearman = 0.6582
2019-03-13 05:56:41,422 : ALL (average) : Pearson = 0.6346,             Spearman = 0.6364

2019-03-13 05:56:41,422 : ***** Transfer task : STS16 *****


2019-03-13 05:56:41,460 : loading BERT model bert-large-uncased
2019-03-13 05:56:41,460 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:56:41,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:56:41,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppf34bikp
2019-03-13 05:56:48,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:56:55,279 : answer-answer : pearson = 0.5459, spearman = 0.5490
2019-03-13 05:56:55,932 : headlines : pearson = 0.6355, spearman = 0.6354
2019-03-13 05:56:56,802 : plagiarism : pearson = 0.7271, spearman = 0.7364
2019-03-13 05:56:58,276 : postediting : pearson = 0.7986, spearman = 0.8311
2019-03-13 05:56:58,875 : question-question : pearson = 0.3528, spearman = 0.3639
2019-03-13 05:56:58,875 : ALL (weighted average) : Pearson = 0.6178,             Spearman = 0.6289
2019-03-13 05:56:58,875 : ALL (average) : Pearson = 0.6120,             Spearman = 0.6231

2019-03-13 05:56:58,875 : ***** Transfer task : MR *****


2019-03-13 05:56:58,919 : loading BERT model bert-large-uncased
2019-03-13 05:56:58,920 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:56:58,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:56:58,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp74pgltoj
2019-03-13 05:57:06,439 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:57:11,976 : Generating sentence embeddings
2019-03-13 05:57:42,984 : Generated sentence embeddings
2019-03-13 05:57:42,985 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:57:50,992 : Best param found at split 1: l2reg = 0.0001                 with score 58.88
2019-03-13 05:58:03,357 : Best param found at split 2: l2reg = 1e-05                 with score 58.22
2019-03-13 05:58:17,549 : Best param found at split 3: l2reg = 1e-05                 with score 61.35
2019-03-13 05:58:28,628 : Best param found at split 4: l2reg = 0.01                 with score 61.23
2019-03-13 05:58:41,533 : Best param found at split 5: l2reg = 1e-05                 with score 56.67
2019-03-13 05:58:42,329 : Dev acc : 59.27 Test acc : 56.85

2019-03-13 05:58:42,330 : ***** Transfer task : CR *****


2019-03-13 05:58:42,338 : loading BERT model bert-large-uncased
2019-03-13 05:58:42,338 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:58:42,388 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:58:42,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr_rkdwad
2019-03-13 05:58:49,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:58:55,366 : Generating sentence embeddings
2019-03-13 05:59:03,563 : Generated sentence embeddings
2019-03-13 05:59:03,563 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:59:06,538 : Best param found at split 1: l2reg = 0.01                 with score 69.06
2019-03-13 05:59:09,813 : Best param found at split 2: l2reg = 1e-05                 with score 69.76
2019-03-13 05:59:12,932 : Best param found at split 3: l2reg = 1e-05                 with score 66.89
2019-03-13 05:59:16,545 : Best param found at split 4: l2reg = 0.0001                 with score 74.61
2019-03-13 05:59:19,517 : Best param found at split 5: l2reg = 0.001                 with score 66.73
2019-03-13 05:59:19,708 : Dev acc : 69.41 Test acc : 66.18

2019-03-13 05:59:19,709 : ***** Transfer task : MPQA *****


2019-03-13 05:59:19,714 : loading BERT model bert-large-uncased
2019-03-13 05:59:19,715 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:59:19,734 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:59:19,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy77qb14l
2019-03-13 05:59:27,229 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:59:32,636 : Generating sentence embeddings
2019-03-13 05:59:40,105 : Generated sentence embeddings
2019-03-13 05:59:40,105 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 05:59:49,794 : Best param found at split 1: l2reg = 0.0001                 with score 83.69
2019-03-13 05:59:59,935 : Best param found at split 2: l2reg = 1e-05                 with score 83.45
2019-03-13 06:00:10,111 : Best param found at split 3: l2reg = 0.001                 with score 84.12
2019-03-13 06:00:19,024 : Best param found at split 4: l2reg = 0.001                 with score 84.4
2019-03-13 06:00:31,222 : Best param found at split 5: l2reg = 0.01                 with score 85.05
2019-03-13 06:00:32,096 : Dev acc : 84.14 Test acc : 83.09

2019-03-13 06:00:32,097 : ***** Transfer task : SUBJ *****


2019-03-13 06:00:32,113 : loading BERT model bert-large-uncased
2019-03-13 06:00:32,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:00:32,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:00:32,169 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppxhs3gl6
2019-03-13 06:00:39,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:00:45,062 : Generating sentence embeddings
2019-03-13 06:01:15,499 : Generated sentence embeddings
2019-03-13 06:01:15,500 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:01:25,907 : Best param found at split 1: l2reg = 0.001                 with score 87.46
2019-03-13 06:01:35,518 : Best param found at split 2: l2reg = 0.01                 with score 90.61
2019-03-13 06:01:45,723 : Best param found at split 3: l2reg = 1e-05                 with score 89.7
2019-03-13 06:01:56,486 : Best param found at split 4: l2reg = 0.01                 with score 90.69
2019-03-13 06:02:07,399 : Best param found at split 5: l2reg = 1e-05                 with score 90.44
2019-03-13 06:02:07,900 : Dev acc : 89.78 Test acc : 89.53

2019-03-13 06:02:07,901 : ***** Transfer task : SST Binary classification *****


2019-03-13 06:02:08,039 : loading BERT model bert-large-uncased
2019-03-13 06:02:08,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:02:08,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:02:08,064 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps6u4qm3n
2019-03-13 06:02:15,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:02:21,086 : Computing embedding for train
2019-03-13 06:03:59,509 : Computed train embeddings
2019-03-13 06:03:59,509 : Computing embedding for dev
2019-03-13 06:04:01,657 : Computed dev embeddings
2019-03-13 06:04:01,658 : Computing embedding for test
2019-03-13 06:04:06,170 : Computed test embeddings
2019-03-13 06:04:06,170 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:04:18,108 : [('reg:1e-05', 71.33), ('reg:0.0001', 71.22), ('reg:0.001', 70.53), ('reg:0.01', 66.63)]
2019-03-13 06:04:18,109 : Validation : best param found is reg = 1e-05 with score             71.33
2019-03-13 06:04:18,109 : Evaluating...
2019-03-13 06:04:22,086 : 
Dev acc : 71.33 Test acc : 71.77 for             SST Binary classification

2019-03-13 06:04:22,087 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 06:04:22,138 : loading BERT model bert-large-uncased
2019-03-13 06:04:22,138 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:04:22,160 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:04:22,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4vqvu1ng
2019-03-13 06:04:29,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:04:35,243 : Computing embedding for train
2019-03-13 06:04:56,882 : Computed train embeddings
2019-03-13 06:04:56,882 : Computing embedding for dev
2019-03-13 06:04:59,697 : Computed dev embeddings
2019-03-13 06:04:59,697 : Computing embedding for test
2019-03-13 06:05:05,249 : Computed test embeddings
2019-03-13 06:05:05,250 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:05:08,069 : [('reg:1e-05', 33.42), ('reg:0.0001', 29.88), ('reg:0.001', 26.07), ('reg:0.01', 31.24)]
2019-03-13 06:05:08,069 : Validation : best param found is reg = 1e-05 with score             33.42
2019-03-13 06:05:08,069 : Evaluating...
2019-03-13 06:05:08,631 : 
Dev acc : 33.42 Test acc : 37.15 for             SST Fine-Grained classification

2019-03-13 06:05:08,632 : ***** Transfer task : TREC *****


2019-03-13 06:05:08,645 : loading BERT model bert-large-uncased
2019-03-13 06:05:08,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:05:08,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:05:08,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp33be_hnx
2019-03-13 06:05:16,110 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:05:29,136 : Computed train embeddings
2019-03-13 06:05:29,717 : Computed test embeddings
2019-03-13 06:05:29,718 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:05:36,401 : [('reg:1e-05', 62.51), ('reg:0.0001', 56.52), ('reg:0.001', 61.61), ('reg:0.01', 52.93)]
2019-03-13 06:05:36,401 : Cross-validation : best param found is reg = 1e-05             with score 62.51
2019-03-13 06:05:36,401 : Evaluating...
2019-03-13 06:05:36,814 : 
Dev acc : 62.51 Test acc : 84.8             for TREC

2019-03-13 06:05:36,815 : ***** Transfer task : MRPC *****


2019-03-13 06:05:36,836 : loading BERT model bert-large-uncased
2019-03-13 06:05:36,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:05:36,894 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:05:36,894 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwl5zf5km
2019-03-13 06:05:44,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:05:49,934 : Computing embedding for train
2019-03-13 06:06:11,895 : Computed train embeddings
2019-03-13 06:06:11,895 : Computing embedding for test
2019-03-13 06:06:21,500 : Computed test embeddings
2019-03-13 06:06:21,521 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:06:26,710 : [('reg:1e-05', 72.1), ('reg:0.0001', 72.08), ('reg:0.001', 71.0), ('reg:0.01', 70.66)]
2019-03-13 06:06:26,710 : Cross-validation : best param found is reg = 1e-05             with score 72.1
2019-03-13 06:06:26,711 : Evaluating...
2019-03-13 06:06:27,041 : Dev acc : 72.1 Test acc 74.38; Test F1 82.39 for MRPC.

2019-03-13 06:06:27,041 : ***** Transfer task : SICK-Entailment*****


2019-03-13 06:06:27,065 : loading BERT model bert-large-uncased
2019-03-13 06:06:27,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:06:27,084 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:06:27,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqdwrtivv
2019-03-13 06:06:34,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:06:39,973 : Computing embedding for train
2019-03-13 06:06:51,098 : Computed train embeddings
2019-03-13 06:06:51,098 : Computing embedding for dev
2019-03-13 06:06:52,615 : Computed dev embeddings
2019-03-13 06:06:52,615 : Computing embedding for test
2019-03-13 06:07:04,590 : Computed test embeddings
2019-03-13 06:07:04,628 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:07:06,214 : [('reg:1e-05', 75.6), ('reg:0.0001', 77.2), ('reg:0.001', 78.4), ('reg:0.01', 73.6)]
2019-03-13 06:07:06,215 : Validation : best param found is reg = 0.001 with score             78.4
2019-03-13 06:07:06,215 : Evaluating...
2019-03-13 06:07:06,781 : 
Dev acc : 78.4 Test acc : 78.53 for                        SICK entailment

2019-03-13 06:07:06,782 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 06:07:06,809 : loading BERT model bert-large-uncased
2019-03-13 06:07:06,809 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:07:06,829 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:07:06,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc7ls622c
2019-03-13 06:07:14,276 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:07:19,945 : Computing embedding for train
2019-03-13 06:07:31,091 : Computed train embeddings
2019-03-13 06:07:31,092 : Computing embedding for dev
2019-03-13 06:07:32,612 : Computed dev embeddings
2019-03-13 06:07:32,612 : Computing embedding for test
2019-03-13 06:07:44,564 : Computed test embeddings
2019-03-13 06:07:59,535 : Dev : Pearson 0.790226898922276
2019-03-13 06:07:59,536 : Test : Pearson 0.7843655554634926 Spearman 0.7138129172426517 MSE 0.3920113947726751                        for SICK Relatedness

2019-03-13 06:07:59,536 : 

***** Transfer task : STSBenchmark*****


2019-03-13 06:07:59,577 : loading BERT model bert-large-uncased
2019-03-13 06:07:59,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:07:59,638 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:07:59,638 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6vpz2mvx
2019-03-13 06:08:07,094 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:08:12,732 : Computing embedding for train
2019-03-13 06:08:31,070 : Computed train embeddings
2019-03-13 06:08:31,070 : Computing embedding for dev
2019-03-13 06:08:36,631 : Computed dev embeddings
2019-03-13 06:08:36,631 : Computing embedding for test
2019-03-13 06:08:41,165 : Computed test embeddings
2019-03-13 06:08:57,747 : Dev : Pearson 0.6844962973773476
2019-03-13 06:08:57,747 : Test : Pearson 0.6364558465070517 Spearman 0.6351878127304925 MSE 1.4565020940890194                        for SICK Relatedness

2019-03-13 06:08:57,747 : ***** Transfer task : SNLI Entailment*****


2019-03-13 06:09:02,747 : loading BERT model bert-large-uncased
2019-03-13 06:09:02,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:09:02,874 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:09:02,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz2337r_m
2019-03-13 06:09:10,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:16,310 : PROGRESS (encoding): 0.00%
2019-03-13 06:11:59,609 : PROGRESS (encoding): 14.56%
2019-03-13 06:15:04,704 : PROGRESS (encoding): 29.12%
2019-03-13 06:18:10,696 : PROGRESS (encoding): 43.69%
2019-03-13 06:21:29,076 : PROGRESS (encoding): 58.25%
2019-03-13 06:25:09,896 : PROGRESS (encoding): 72.81%
2019-03-13 06:28:49,603 : PROGRESS (encoding): 87.37%
2019-03-13 06:32:47,350 : PROGRESS (encoding): 0.00%
2019-03-13 06:33:17,244 : PROGRESS (encoding): 0.00%
2019-03-13 06:33:46,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:34:53,582 : [('reg:1e-09', 60.75)]
2019-03-13 06:34:53,582 : Validation : best param found is reg = 1e-09 with score             60.75
2019-03-13 06:34:53,582 : Evaluating...
2019-03-13 06:36:03,567 : Dev acc : 60.75 Test acc : 61.84 for SNLI

2019-03-13 06:36:03,568 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 06:36:03,779 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 06:36:04,838 : loading BERT model bert-large-uncased
2019-03-13 06:36:04,838 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:36:04,865 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:36:04,866 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjrrffqdb
2019-03-13 06:36:12,301 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:36:17,890 : Computing embeddings for train/dev/test
2019-03-13 06:39:45,003 : Computed embeddings
2019-03-13 06:39:45,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:40:12,035 : [('reg:1e-05', 55.52), ('reg:0.0001', 66.88), ('reg:0.001', 51.62), ('reg:0.01', 57.85)]
2019-03-13 06:40:12,036 : Validation : best param found is reg = 0.0001 with score             66.88
2019-03-13 06:40:12,036 : Evaluating...
2019-03-13 06:40:18,760 : 
Dev acc : 66.9 Test acc : 68.3 for LENGTH classification

2019-03-13 06:40:18,761 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 06:40:19,125 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 06:40:19,169 : loading BERT model bert-large-uncased
2019-03-13 06:40:19,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:40:19,198 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:40:19,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpddryiagb
2019-03-13 06:40:26,694 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:40:32,315 : Computing embeddings for train/dev/test
2019-03-13 06:43:43,776 : Computed embeddings
2019-03-13 06:43:43,776 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:44:13,438 : [('reg:1e-05', 16.24), ('reg:0.0001', 7.48), ('reg:0.001', 0.51), ('reg:0.01', 0.1)]
2019-03-13 06:44:13,438 : Validation : best param found is reg = 1e-05 with score             16.24
2019-03-13 06:44:13,438 : Evaluating...
2019-03-13 06:44:24,894 : 
Dev acc : 16.2 Test acc : 16.3 for WORDCONTENT classification

2019-03-13 06:44:24,895 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 06:44:25,252 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 06:44:25,319 : loading BERT model bert-large-uncased
2019-03-13 06:44:25,319 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:44:25,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:44:25,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi22nk176
2019-03-13 06:44:32,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:44:38,424 : Computing embeddings for train/dev/test
2019-03-13 06:47:38,142 : Computed embeddings
2019-03-13 06:47:38,142 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:48:10,122 : [('reg:1e-05', 26.77), ('reg:0.0001', 24.95), ('reg:0.001', 28.51), ('reg:0.01', 25.91)]
2019-03-13 06:48:10,122 : Validation : best param found is reg = 0.001 with score             28.51
2019-03-13 06:48:10,122 : Evaluating...
2019-03-13 06:48:18,866 : 
Dev acc : 28.5 Test acc : 29.1 for DEPTH classification

2019-03-13 06:48:18,867 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 06:48:19,232 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 06:48:19,295 : loading BERT model bert-large-uncased
2019-03-13 06:48:19,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:48:19,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:48:19,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi1me77zj
2019-03-13 06:48:26,823 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:48:32,400 : Computing embeddings for train/dev/test
2019-03-13 06:51:19,201 : Computed embeddings
2019-03-13 06:51:19,201 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:51:47,432 : [('reg:1e-05', 57.51), ('reg:0.0001', 58.57), ('reg:0.001', 42.69), ('reg:0.01', 35.28)]
2019-03-13 06:51:47,432 : Validation : best param found is reg = 0.0001 with score             58.57
2019-03-13 06:51:47,432 : Evaluating...
2019-03-13 06:51:55,591 : 
Dev acc : 58.6 Test acc : 59.2 for TOPCONSTITUENTS classification

2019-03-13 06:51:55,593 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 06:51:55,973 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 06:51:56,039 : loading BERT model bert-large-uncased
2019-03-13 06:51:56,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:51:56,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:51:56,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr1guemaf
2019-03-13 06:52:03,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:52:09,085 : Computing embeddings for train/dev/test
2019-03-13 06:55:10,131 : Computed embeddings
2019-03-13 06:55:10,131 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:55:47,697 : [('reg:1e-05', 86.19), ('reg:0.0001', 86.11), ('reg:0.001', 85.01), ('reg:0.01', 72.48)]
2019-03-13 06:55:47,698 : Validation : best param found is reg = 1e-05 with score             86.19
2019-03-13 06:55:47,698 : Evaluating...
2019-03-13 06:55:56,328 : 
Dev acc : 86.2 Test acc : 85.6 for BIGRAMSHIFT classification

2019-03-13 06:55:56,329 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 06:55:56,721 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 06:55:56,788 : loading BERT model bert-large-uncased
2019-03-13 06:55:56,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:55:56,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:55:56,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdp0cbcls
2019-03-13 06:56:04,216 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:56:09,879 : Computing embeddings for train/dev/test
2019-03-13 06:59:07,008 : Computed embeddings
2019-03-13 06:59:07,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:59:33,332 : [('reg:1e-05', 84.1), ('reg:0.0001', 83.83), ('reg:0.001', 84.8), ('reg:0.01', 84.56)]
2019-03-13 06:59:33,332 : Validation : best param found is reg = 0.001 with score             84.8
2019-03-13 06:59:33,333 : Evaluating...
2019-03-13 06:59:39,960 : 
Dev acc : 84.8 Test acc : 83.0 for TENSE classification

2019-03-13 06:59:39,961 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 06:59:40,359 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 06:59:40,421 : loading BERT model bert-large-uncased
2019-03-13 06:59:40,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:59:40,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:59:40,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt31a0m_2
2019-03-13 06:59:48,015 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:59:53,585 : Computing embeddings for train/dev/test
2019-03-13 07:03:01,209 : Computed embeddings
2019-03-13 07:03:01,209 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:03:21,124 : [('reg:1e-05', 80.59), ('reg:0.0001', 80.53), ('reg:0.001', 80.56), ('reg:0.01', 79.75)]
2019-03-13 07:03:21,124 : Validation : best param found is reg = 1e-05 with score             80.59
2019-03-13 07:03:21,124 : Evaluating...
2019-03-13 07:03:25,748 : 
Dev acc : 80.6 Test acc : 79.7 for SUBJNUMBER classification

2019-03-13 07:03:25,749 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 07:03:26,148 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 07:03:26,214 : loading BERT model bert-large-uncased
2019-03-13 07:03:26,214 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:03:26,328 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:03:26,329 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoa7cy0da
2019-03-13 07:03:33,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:03:39,350 : Computing embeddings for train/dev/test
2019-03-13 07:06:43,783 : Computed embeddings
2019-03-13 07:06:43,783 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:07:19,999 : [('reg:1e-05', 78.77), ('reg:0.0001', 78.98), ('reg:0.001', 78.6), ('reg:0.01', 77.75)]
2019-03-13 07:07:19,999 : Validation : best param found is reg = 0.0001 with score             78.98
2019-03-13 07:07:19,999 : Evaluating...
2019-03-13 07:07:30,018 : 
Dev acc : 79.0 Test acc : 79.9 for OBJNUMBER classification

2019-03-13 07:07:30,019 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 07:07:30,588 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 07:07:30,657 : loading BERT model bert-large-uncased
2019-03-13 07:07:30,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:07:30,685 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:07:30,685 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiirs_6y6
2019-03-13 07:07:38,128 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:07:43,687 : Computing embeddings for train/dev/test
2019-03-13 07:11:17,231 : Computed embeddings
2019-03-13 07:11:17,231 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:11:44,747 : [('reg:1e-05', 59.23), ('reg:0.0001', 59.21), ('reg:0.001', 59.12), ('reg:0.01', 59.2)]
2019-03-13 07:11:44,748 : Validation : best param found is reg = 1e-05 with score             59.23
2019-03-13 07:11:44,748 : Evaluating...
2019-03-13 07:11:51,292 : 
Dev acc : 59.2 Test acc : 59.0 for ODDMANOUT classification

2019-03-13 07:11:51,293 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 07:11:51,675 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 07:11:51,757 : loading BERT model bert-large-uncased
2019-03-13 07:11:51,757 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:11:51,787 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:11:51,787 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyy70d_wn
2019-03-13 07:11:59,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:12:04,794 : Computing embeddings for train/dev/test
2019-03-13 07:15:36,333 : Computed embeddings
2019-03-13 07:15:36,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:16:11,453 : [('reg:1e-05', 59.27), ('reg:0.0001', 59.22), ('reg:0.001', 58.56), ('reg:0.01', 50.03)]
2019-03-13 07:16:11,453 : Validation : best param found is reg = 1e-05 with score             59.27
2019-03-13 07:16:11,453 : Evaluating...
2019-03-13 07:16:21,270 : 
Dev acc : 59.3 Test acc : 59.5 for COORDINATIONINVERSION classification

2019-03-13 07:16:21,272 : total results: {'STS12': {'MSRpar': {'pearson': (0.32235319095102233, 1.351692082245762e-19), 'spearman': SpearmanrResult(correlation=0.3550189062872414, pvalue=1.0685647142484999e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.669498969718852, 1.1382566180472466e-98), 'spearman': SpearmanrResult(correlation=0.6762641938543091, pvalue=2.2359486436103845e-101), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.48455130973587296, 2.1244805073201376e-28), 'spearman': SpearmanrResult(correlation=0.5769308774518023, pvalue=4.404790709049362e-42), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5688815600576678, 1.5793109695579079e-65), 'spearman': SpearmanrResult(correlation=0.6085487281083769, pvalue=3.5272118931558835e-77), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5394978224943117, 1.6364651498678426e-31), 'spearman': SpearmanrResult(correlation=0.46969421159448455, pvalue=2.752530697226562e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5169565705915453, 'wmean': 0.5174449719722177}, 'spearman': {'mean': 0.5372913834592429, 'wmean': 0.5412140071956314}}}, 'STS13': {'FNWN': {'pearson': (0.19948101711027694, 0.005924496807160163), 'spearman': SpearmanrResult(correlation=0.1999929370106167, pvalue=0.0057953817715836985), 'nsamples': 189}, 'headlines': {'pearson': (0.646425695971426, 5.916450086463854e-90), 'spearman': SpearmanrResult(correlation=0.6394484891422483, pvalue=1.823690734062712e-87), 'nsamples': 750}, 'OnWN': {'pearson': (0.5232742850333421, 9.328906702096402e-41), 'spearman': SpearmanrResult(correlation=0.52490844618451, pvalue=4.806575740769265e-41), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4563936660383483, 'wmean': 0.5440520387440779}, 'spearman': {'mean': 0.454783290779125, 'wmean': 0.5412391135074686}}}, 'STS14': {'deft-forum': {'pearson': (0.3895813145280724, 9.265395492666368e-18), 'spearman': SpearmanrResult(correlation=0.40146731067917285, pvalue=7.4095592532127355e-19), 'nsamples': 450}, 'deft-news': {'pearson': (0.7166303135566088, 1.497292545914078e-48), 'spearman': SpearmanrResult(correlation=0.6701824309970961, pvalue=1.7829272240769421e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.5808983459178347, 6.872836141531792e-69), 'spearman': SpearmanrResult(correlation=0.5542705448000874, pvalue=1.2689950612902279e-61), 'nsamples': 750}, 'images': {'pearson': (0.6054895538201486, 3.197595617264427e-76), 'spearman': SpearmanrResult(correlation=0.602434532923563, pvalue=2.822219764357469e-75), 'nsamples': 750}, 'OnWN': {'pearson': (0.6399117956149333, 1.252226377202115e-87), 'spearman': SpearmanrResult(correlation=0.6707345367345233, pvalue=3.6918656344458496e-99), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5943815011594495, 7.857154551365289e-73), 'spearman': SpearmanrResult(correlation=0.5686821003824684, pvalue=1.791007737277569e-65), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5878154707661746, 'wmean': 0.5882164221303706}, 'spearman': {'mean': 0.5779619094194851, 'wmean': 0.5810150147293969}}}, 'STS15': {'answers-forums': {'pearson': (0.5416415418492766, 5.662658437906825e-30), 'spearman': SpearmanrResult(correlation=0.5042740614706099, pvalue=1.3926476014200854e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.6852414026120281, 4.401540275609204e-105), 'spearman': SpearmanrResult(correlation=0.69012945651753, pvalue=3.7049141625288296e-107), 'nsamples': 750}, 'belief': {'pearson': (0.5645683232094082, 5.953892243822415e-33), 'spearman': SpearmanrResult(correlation=0.5938365499324544, pvalue=4.154308697145341e-37), 'nsamples': 375}, 'headlines': {'pearson': (0.6346161192489747, 8.856228565730438e-86), 'spearman': SpearmanrResult(correlation=0.6399219594506943, pvalue=1.2419324543799957e-87), 'nsamples': 750}, 'images': {'pearson': (0.7469478826306684, 1.0100151076179656e-134), 'spearman': SpearmanrResult(correlation=0.7536724004277119, pvalue=1.7776197072421246e-138), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6346030539100711, 'wmean': 0.6549775842552534}, 'spearman': {'mean': 0.6363668855598, 'wmean': 0.6581947805243672}}}, 'STS16': {'answer-answer': {'pearson': (0.5459078514494972, 3.953918558059812e-21), 'spearman': SpearmanrResult(correlation=0.5489803600135965, pvalue=2.1467749044744467e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6355384210184918, 1.414383045992794e-29), 'spearman': SpearmanrResult(correlation=0.635386694778319, pvalue=1.472369195598596e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.727104329553909, 4.1415898932096934e-39), 'spearman': SpearmanrResult(correlation=0.7363645887715927, pvalue=1.4699933834139513e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7986498376645601, 2.709904047579213e-55), 'spearman': SpearmanrResult(correlation=0.831070013671518, pvalue=1.3457610709297175e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.3528145390056364, 1.6104912983227718e-07), 'spearman': SpearmanrResult(correlation=0.3638652567094072, pvalue=6.109082670423999e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.612002995738419, 'wmean': 0.6178351230535467}, 'spearman': {'mean': 0.6231333827888867, 'wmean': 0.6288744315757485}}}, 'MR': {'devacc': 59.27, 'acc': 56.85, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.41, 'acc': 66.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.14, 'acc': 83.09, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.78, 'acc': 89.53, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 71.33, 'acc': 71.77, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.42, 'acc': 37.15, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 62.51, 'acc': 84.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.1, 'acc': 74.38, 'f1': 82.39, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.4, 'acc': 78.53, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.790226898922276, 'pearson': 0.7843655554634926, 'spearman': 0.7138129172426517, 'mse': 0.3920113947726751, 'yhat': array([3.1731141 , 4.50836624, 1.64937224, ..., 2.98231823, 4.62847143,        4.52005906]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6844962973773476, 'pearson': 0.6364558465070517, 'spearman': 0.6351878127304925, 'mse': 1.4565020940890194, 'yhat': array([1.57436162, 1.41126985, 1.40992999, ..., 4.36675524, 4.50209248,        3.54584274]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.75, 'acc': 61.84, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 66.88, 'acc': 68.31, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 16.24, 'acc': 16.26, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.51, 'acc': 29.1, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.57, 'acc': 59.23, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 86.19, 'acc': 85.64, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.8, 'acc': 83.03, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.59, 'acc': 79.73, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.98, 'acc': 79.93, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.23, 'acc': 59.04, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 59.27, 'acc': 59.5, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 07:16:21,272 : STS12 p=0.5174, STS12 s=0.5412, STS13 p=0.5441, STS13 s=0.5412, STS14 p=0.5882, STS14 s=0.5810, STS15 p=0.6550, STS15 s=0.6582, STS 16 p=0.6178, STS16 s=0.6289, STS B p=0.6365, STS B s=0.6352, STS B m=1.4565, SICK-R p=0.7844, SICK-R s=0.7138, SICK-P m=0.3920
2019-03-13 07:16:21,272 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 07:16:21,272 : 0.5174,0.5412,0.5441,0.5412,0.5882,0.5810,0.6550,0.6582,0.6178,0.6289,0.6365,0.6352,1.4565,0.7844,0.7138,0.3920
2019-03-13 07:16:21,272 : MR=56.85, CR=66.18, SUBJ=89.53, MPQA=83.09, SST-B=71.77, SST-F=37.15, TREC=84.80, SICK-E=78.53, SNLI=61.84, MRPC=74.38, MRPC f=82.39
2019-03-13 07:16:21,272 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 07:16:21,272 : 56.85,66.18,89.53,83.09,71.77,37.15,84.80,78.53,61.84,74.38,82.39
2019-03-13 07:16:21,272 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 07:16:21,272 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 07:16:21,272 : na,na,na,na,na,na,na,na,na,na
2019-03-13 07:16:21,272 : SentLen=68.31, WC=16.26, TreeDepth=29.10, TopConst=59.23, BShift=85.64, Tense=83.03, SubjNum=79.73, ObjNum=79.93, SOMO=59.04, CoordInv=59.50, average=61.98
2019-03-13 07:16:21,272 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 07:16:21,272 : 68.31,16.26,29.10,59.23,85.64,83.03,79.73,79.93,59.04,59.50,61.98
2019-03-13 07:16:21,272 : ********************************************************************************
2019-03-13 07:16:21,272 : ********************************************************************************
2019-03-13 07:16:21,272 : ********************************************************************************
2019-03-13 07:16:21,272 : layer 13
2019-03-13 07:16:21,272 : ********************************************************************************
2019-03-13 07:16:21,272 : ********************************************************************************
2019-03-13 07:16:21,273 : ********************************************************************************
2019-03-13 07:16:21,364 : ***** Transfer task : STS12 *****


2019-03-13 07:16:21,377 : loading BERT model bert-large-uncased
2019-03-13 07:16:21,377 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:16:21,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:16:21,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfz7ahd8z
2019-03-13 07:16:28,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:16:38,299 : MSRpar : pearson = 0.2967, spearman = 0.3271
2019-03-13 07:16:39,921 : MSRvid : pearson = 0.6288, spearman = 0.6380
2019-03-13 07:16:41,315 : SMTeuroparl : pearson = 0.4974, spearman = 0.5876
2019-03-13 07:16:44,014 : surprise.OnWN : pearson = 0.5588, spearman = 0.5976
2019-03-13 07:16:45,422 : surprise.SMTnews : pearson = 0.5513, spearman = 0.4779
2019-03-13 07:16:45,422 : ALL (weighted average) : Pearson = 0.5024,             Spearman = 0.5252
2019-03-13 07:16:45,422 : ALL (average) : Pearson = 0.5066,             Spearman = 0.5256

2019-03-13 07:16:45,422 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 07:16:45,431 : loading BERT model bert-large-uncased
2019-03-13 07:16:45,431 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:16:45,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:16:45,449 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkakf8yrq
2019-03-13 07:16:52,896 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:16:59,581 : FNWN : pearson = 0.1997, spearman = 0.2005
2019-03-13 07:17:01,452 : headlines : pearson = 0.6253, spearman = 0.6171
2019-03-13 07:17:02,903 : OnWN : pearson = 0.4649, spearman = 0.4717
2019-03-13 07:17:02,903 : ALL (weighted average) : Pearson = 0.5117,             Spearman = 0.5102
2019-03-13 07:17:02,903 : ALL (average) : Pearson = 0.4299,             Spearman = 0.4298

2019-03-13 07:17:02,903 : ***** Transfer task : STS14 *****


2019-03-13 07:17:02,918 : loading BERT model bert-large-uncased
2019-03-13 07:17:02,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:17:02,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:17:02,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1qnvg52w
2019-03-13 07:17:10,415 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:17:17,202 : deft-forum : pearson = 0.3487, spearman = 0.3596
2019-03-13 07:17:18,818 : deft-news : pearson = 0.7136, spearman = 0.6680
2019-03-13 07:17:20,962 : headlines : pearson = 0.5632, spearman = 0.5364
2019-03-13 07:17:23,013 : images : pearson = 0.5844, spearman = 0.5836
2019-03-13 07:17:25,117 : OnWN : pearson = 0.6034, spearman = 0.6335
2019-03-13 07:17:27,938 : tweet-news : pearson = 0.5879, spearman = 0.5642
2019-03-13 07:17:27,939 : ALL (weighted average) : Pearson = 0.5667,             Spearman = 0.5601
2019-03-13 07:17:27,939 : ALL (average) : Pearson = 0.5669,             Spearman = 0.5576

2019-03-13 07:17:27,939 : ***** Transfer task : STS15 *****


2019-03-13 07:17:27,970 : loading BERT model bert-large-uncased
2019-03-13 07:17:27,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:17:27,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:17:27,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppxnizycy
2019-03-13 07:17:35,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:17:42,686 : answers-forums : pearson = 0.5133, spearman = 0.4746
2019-03-13 07:17:44,744 : answers-students : pearson = 0.6861, spearman = 0.6925
2019-03-13 07:17:46,765 : belief : pearson = 0.5504, spearman = 0.5753
2019-03-13 07:17:48,986 : headlines : pearson = 0.6186, spearman = 0.6280
2019-03-13 07:17:51,097 : images : pearson = 0.7275, spearman = 0.7361
2019-03-13 07:17:51,097 : ALL (weighted average) : Pearson = 0.6410,             Spearman = 0.6454
2019-03-13 07:17:51,097 : ALL (average) : Pearson = 0.6192,             Spearman = 0.6213

2019-03-13 07:17:51,097 : ***** Transfer task : STS16 *****


2019-03-13 07:17:51,167 : loading BERT model bert-large-uncased
2019-03-13 07:17:51,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:17:51,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:17:51,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxjfvdihs
2019-03-13 07:17:58,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:18:04,919 : answer-answer : pearson = 0.5202, spearman = 0.5330
2019-03-13 07:18:05,571 : headlines : pearson = 0.6272, spearman = 0.6296
2019-03-13 07:18:06,442 : plagiarism : pearson = 0.7009, spearman = 0.7063
2019-03-13 07:18:07,914 : postediting : pearson = 0.7935, spearman = 0.8274
2019-03-13 07:18:08,513 : question-question : pearson = 0.2435, spearman = 0.2694
2019-03-13 07:18:08,514 : ALL (weighted average) : Pearson = 0.5851,             Spearman = 0.6010
2019-03-13 07:18:08,514 : ALL (average) : Pearson = 0.5770,             Spearman = 0.5931

2019-03-13 07:18:08,514 : ***** Transfer task : MR *****


2019-03-13 07:18:08,529 : loading BERT model bert-large-uncased
2019-03-13 07:18:08,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:18:08,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:18:08,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqzfgvjjb
2019-03-13 07:18:15,998 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:18:21,446 : Generating sentence embeddings
2019-03-13 07:18:52,475 : Generated sentence embeddings
2019-03-13 07:18:52,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:19:03,095 : Best param found at split 1: l2reg = 0.001                 with score 63.29
2019-03-13 07:19:13,666 : Best param found at split 2: l2reg = 0.01                 with score 59.67
2019-03-13 07:19:26,349 : Best param found at split 3: l2reg = 1e-05                 with score 59.89
2019-03-13 07:19:36,759 : Best param found at split 4: l2reg = 0.01                 with score 59.66
2019-03-13 07:19:48,534 : Best param found at split 5: l2reg = 0.01                 with score 54.85
2019-03-13 07:19:49,341 : Dev acc : 59.47 Test acc : 56.71

2019-03-13 07:19:49,342 : ***** Transfer task : CR *****


2019-03-13 07:19:49,350 : loading BERT model bert-large-uncased
2019-03-13 07:19:49,350 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:19:49,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:19:49,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbkp70ku6
2019-03-13 07:19:56,824 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:20:02,241 : Generating sentence embeddings
2019-03-13 07:20:10,439 : Generated sentence embeddings
2019-03-13 07:20:10,439 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:20:13,515 : Best param found at split 1: l2reg = 1e-05                 with score 68.07
2019-03-13 07:20:16,695 : Best param found at split 2: l2reg = 0.001                 with score 68.07
2019-03-13 07:20:20,190 : Best param found at split 3: l2reg = 0.001                 with score 69.31
2019-03-13 07:20:23,931 : Best param found at split 4: l2reg = 0.0001                 with score 72.76
2019-03-13 07:20:27,481 : Best param found at split 5: l2reg = 0.001                 with score 69.48
2019-03-13 07:20:27,607 : Dev acc : 69.54 Test acc : 68.5

2019-03-13 07:20:27,607 : ***** Transfer task : MPQA *****


2019-03-13 07:20:27,613 : loading BERT model bert-large-uncased
2019-03-13 07:20:27,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:20:27,662 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:20:27,662 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp07czee20
2019-03-13 07:20:35,070 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:20:40,489 : Generating sentence embeddings
2019-03-13 07:20:47,962 : Generated sentence embeddings
2019-03-13 07:20:47,963 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:20:58,478 : Best param found at split 1: l2reg = 1e-05                 with score 84.72
2019-03-13 07:21:10,073 : Best param found at split 2: l2reg = 0.001                 with score 84.43
2019-03-13 07:21:21,861 : Best param found at split 3: l2reg = 0.01                 with score 85.07
2019-03-13 07:21:33,672 : Best param found at split 4: l2reg = 0.001                 with score 84.89
2019-03-13 07:21:46,329 : Best param found at split 5: l2reg = 0.001                 with score 85.09
2019-03-13 07:21:47,016 : Dev acc : 84.84 Test acc : 82.53

2019-03-13 07:21:47,017 : ***** Transfer task : SUBJ *****


2019-03-13 07:21:47,035 : loading BERT model bert-large-uncased
2019-03-13 07:21:47,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:21:47,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:21:47,054 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps7gkslt0
2019-03-13 07:21:54,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:21:59,995 : Generating sentence embeddings
2019-03-13 07:22:30,488 : Generated sentence embeddings
2019-03-13 07:22:30,489 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:22:41,508 : Best param found at split 1: l2reg = 0.01                 with score 90.01
2019-03-13 07:22:52,154 : Best param found at split 2: l2reg = 0.001                 with score 90.9
2019-03-13 07:23:03,982 : Best param found at split 3: l2reg = 0.001                 with score 90.48
2019-03-13 07:23:16,312 : Best param found at split 4: l2reg = 0.01                 with score 91.28
2019-03-13 07:23:25,430 : Best param found at split 5: l2reg = 0.01                 with score 90.42
2019-03-13 07:23:25,795 : Dev acc : 90.62 Test acc : 90.23

2019-03-13 07:23:25,796 : ***** Transfer task : SST Binary classification *****


2019-03-13 07:23:25,888 : loading BERT model bert-large-uncased
2019-03-13 07:23:25,889 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:23:25,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:23:25,964 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5fvdmdgu
2019-03-13 07:23:33,374 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:23:38,779 : Computing embedding for train
2019-03-13 07:25:17,280 : Computed train embeddings
2019-03-13 07:25:17,280 : Computing embedding for dev
2019-03-13 07:25:19,425 : Computed dev embeddings
2019-03-13 07:25:19,425 : Computing embedding for test
2019-03-13 07:25:23,935 : Computed test embeddings
2019-03-13 07:25:23,935 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:25:42,896 : [('reg:1e-05', 76.83), ('reg:0.0001', 76.38), ('reg:0.001', 74.66), ('reg:0.01', 76.49)]
2019-03-13 07:25:42,896 : Validation : best param found is reg = 1e-05 with score             76.83
2019-03-13 07:25:42,896 : Evaluating...
2019-03-13 07:25:47,346 : 
Dev acc : 76.83 Test acc : 76.33 for             SST Binary classification

2019-03-13 07:25:47,347 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 07:25:47,401 : loading BERT model bert-large-uncased
2019-03-13 07:25:47,401 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:25:47,421 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:25:47,422 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5odsr1av
2019-03-13 07:25:54,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:26:00,324 : Computing embedding for train
2019-03-13 07:26:21,897 : Computed train embeddings
2019-03-13 07:26:21,897 : Computing embedding for dev
2019-03-13 07:26:24,714 : Computed dev embeddings
2019-03-13 07:26:24,714 : Computing embedding for test
2019-03-13 07:26:30,266 : Computed test embeddings
2019-03-13 07:26:30,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:26:32,904 : [('reg:1e-05', 31.7), ('reg:0.0001', 30.15), ('reg:0.001', 34.97), ('reg:0.01', 31.15)]
2019-03-13 07:26:32,904 : Validation : best param found is reg = 0.001 with score             34.97
2019-03-13 07:26:32,904 : Evaluating...
2019-03-13 07:26:33,675 : 
Dev acc : 34.97 Test acc : 35.25 for             SST Fine-Grained classification

2019-03-13 07:26:33,676 : ***** Transfer task : TREC *****


2019-03-13 07:26:33,690 : loading BERT model bert-large-uncased
2019-03-13 07:26:33,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:26:33,709 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:26:33,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0ybj5rkt
2019-03-13 07:26:41,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:26:54,178 : Computed train embeddings
2019-03-13 07:26:54,758 : Computed test embeddings
2019-03-13 07:26:54,758 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:27:01,484 : [('reg:1e-05', 67.78), ('reg:0.0001', 72.3), ('reg:0.001', 70.18), ('reg:0.01', 63.17)]
2019-03-13 07:27:01,484 : Cross-validation : best param found is reg = 0.0001             with score 72.3
2019-03-13 07:27:01,485 : Evaluating...
2019-03-13 07:27:01,937 : 
Dev acc : 72.3 Test acc : 81.0             for TREC

2019-03-13 07:27:01,938 : ***** Transfer task : MRPC *****


2019-03-13 07:27:01,958 : loading BERT model bert-large-uncased
2019-03-13 07:27:01,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:27:01,979 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:27:01,979 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_pu77f2z
2019-03-13 07:27:09,477 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:27:14,929 : Computing embedding for train
2019-03-13 07:27:36,843 : Computed train embeddings
2019-03-13 07:27:36,843 : Computing embedding for test
2019-03-13 07:27:46,435 : Computed test embeddings
2019-03-13 07:27:46,456 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:27:51,791 : [('reg:1e-05', 71.71), ('reg:0.0001', 72.52), ('reg:0.001', 71.59), ('reg:0.01', 71.02)]
2019-03-13 07:27:51,791 : Cross-validation : best param found is reg = 0.0001             with score 72.52
2019-03-13 07:27:51,791 : Evaluating...
2019-03-13 07:27:52,112 : Dev acc : 72.52 Test acc 71.54; Test F1 82.03 for MRPC.

2019-03-13 07:27:52,112 : ***** Transfer task : SICK-Entailment*****


2019-03-13 07:27:52,174 : loading BERT model bert-large-uncased
2019-03-13 07:27:52,174 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:27:52,193 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:27:52,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm0ei_2hj
2019-03-13 07:27:59,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:28:05,091 : Computing embedding for train
2019-03-13 07:28:16,233 : Computed train embeddings
2019-03-13 07:28:16,233 : Computing embedding for dev
2019-03-13 07:28:17,751 : Computed dev embeddings
2019-03-13 07:28:17,751 : Computing embedding for test
2019-03-13 07:28:29,701 : Computed test embeddings
2019-03-13 07:28:29,738 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:28:31,398 : [('reg:1e-05', 75.6), ('reg:0.0001', 70.2), ('reg:0.001', 79.2), ('reg:0.01', 75.0)]
2019-03-13 07:28:31,398 : Validation : best param found is reg = 0.001 with score             79.2
2019-03-13 07:28:31,398 : Evaluating...
2019-03-13 07:28:31,913 : 
Dev acc : 79.2 Test acc : 78.22 for                        SICK entailment

2019-03-13 07:28:31,914 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 07:28:31,940 : loading BERT model bert-large-uncased
2019-03-13 07:28:31,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:28:31,996 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:28:31,996 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxs55pcxm
2019-03-13 07:28:39,480 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:28:45,017 : Computing embedding for train
2019-03-13 07:28:56,148 : Computed train embeddings
2019-03-13 07:28:56,148 : Computing embedding for dev
2019-03-13 07:28:57,665 : Computed dev embeddings
2019-03-13 07:28:57,666 : Computing embedding for test
2019-03-13 07:29:09,605 : Computed test embeddings
2019-03-13 07:29:22,896 : Dev : Pearson 0.7943353501826419
2019-03-13 07:29:22,896 : Test : Pearson 0.7908820634758885 Spearman 0.7225625169306588 MSE 0.3823136155200254                        for SICK Relatedness

2019-03-13 07:29:22,897 : 

***** Transfer task : STSBenchmark*****


2019-03-13 07:29:22,964 : loading BERT model bert-large-uncased
2019-03-13 07:29:22,964 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:29:22,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:29:22,985 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm0to27l0
2019-03-13 07:29:30,410 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:29:35,907 : Computing embedding for train
2019-03-13 07:29:54,236 : Computed train embeddings
2019-03-13 07:29:54,236 : Computing embedding for dev
2019-03-13 07:29:59,785 : Computed dev embeddings
2019-03-13 07:29:59,786 : Computing embedding for test
2019-03-13 07:30:04,322 : Computed test embeddings
2019-03-13 07:30:20,682 : Dev : Pearson 0.6687927135538325
2019-03-13 07:30:20,682 : Test : Pearson 0.6249189205512907 Spearman 0.6217535751702504 MSE 1.47624835225584                        for SICK Relatedness

2019-03-13 07:30:20,683 : ***** Transfer task : SNLI Entailment*****


2019-03-13 07:30:25,327 : loading BERT model bert-large-uncased
2019-03-13 07:30:25,328 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:30:26,307 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:30:26,307 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqevws44g
2019-03-13 07:30:33,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:30:39,574 : PROGRESS (encoding): 0.00%
2019-03-13 07:33:22,739 : PROGRESS (encoding): 14.56%
2019-03-13 07:36:27,988 : PROGRESS (encoding): 29.12%
2019-03-13 07:39:33,922 : PROGRESS (encoding): 43.69%
2019-03-13 07:42:52,068 : PROGRESS (encoding): 58.25%
2019-03-13 07:46:32,834 : PROGRESS (encoding): 72.81%
2019-03-13 07:50:12,416 : PROGRESS (encoding): 87.37%
2019-03-13 07:54:10,034 : PROGRESS (encoding): 0.00%
2019-03-13 07:54:39,924 : PROGRESS (encoding): 0.00%
2019-03-13 07:55:08,666 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:55:47,949 : [('reg:1e-09', 59.7)]
2019-03-13 07:55:47,950 : Validation : best param found is reg = 1e-09 with score             59.7
2019-03-13 07:55:47,950 : Evaluating...
2019-03-13 07:56:26,026 : Dev acc : 59.7 Test acc : 60.36 for SNLI

2019-03-13 07:56:26,026 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 07:56:26,252 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 07:56:27,240 : loading BERT model bert-large-uncased
2019-03-13 07:56:27,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:56:27,270 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:56:27,270 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk9g8tahs
2019-03-13 07:56:34,775 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:56:40,353 : Computing embeddings for train/dev/test
2019-03-13 08:00:07,824 : Computed embeddings
2019-03-13 08:00:07,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:00:39,535 : [('reg:1e-05', 59.04), ('reg:0.0001', 65.77), ('reg:0.001', 65.59), ('reg:0.01', 56.95)]
2019-03-13 08:00:39,535 : Validation : best param found is reg = 0.0001 with score             65.77
2019-03-13 08:00:39,535 : Evaluating...
2019-03-13 08:00:47,191 : 
Dev acc : 65.8 Test acc : 65.7 for LENGTH classification

2019-03-13 08:00:47,191 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 08:00:47,559 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 08:00:47,604 : loading BERT model bert-large-uncased
2019-03-13 08:00:47,604 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:00:47,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:00:47,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpel5_wvdh
2019-03-13 08:00:55,030 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:01:00,557 : Computing embeddings for train/dev/test
2019-03-13 08:04:11,844 : Computed embeddings
2019-03-13 08:04:11,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:04:49,551 : [('reg:1e-05', 17.73), ('reg:0.0001', 5.76), ('reg:0.001', 0.37), ('reg:0.01', 0.2)]
2019-03-13 08:04:49,551 : Validation : best param found is reg = 1e-05 with score             17.73
2019-03-13 08:04:49,551 : Evaluating...
2019-03-13 08:05:03,020 : 
Dev acc : 17.7 Test acc : 17.4 for WORDCONTENT classification

2019-03-13 08:05:03,021 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 08:05:03,378 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 08:05:03,444 : loading BERT model bert-large-uncased
2019-03-13 08:05:03,444 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:05:03,468 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:05:03,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyhfq1gfd
2019-03-13 08:05:10,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:05:16,367 : Computing embeddings for train/dev/test
2019-03-13 08:08:15,910 : Computed embeddings
2019-03-13 08:08:15,910 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:08:46,997 : [('reg:1e-05', 25.5), ('reg:0.0001', 30.09), ('reg:0.001', 27.04), ('reg:0.01', 26.1)]
2019-03-13 08:08:46,998 : Validation : best param found is reg = 0.0001 with score             30.09
2019-03-13 08:08:46,998 : Evaluating...
2019-03-13 08:08:55,849 : 
Dev acc : 30.1 Test acc : 29.5 for DEPTH classification

2019-03-13 08:08:55,850 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 08:08:56,227 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 08:08:56,289 : loading BERT model bert-large-uncased
2019-03-13 08:08:56,289 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:08:56,396 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:08:56,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxpjx841h
2019-03-13 08:09:03,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:09:09,338 : Computing embeddings for train/dev/test
2019-03-13 08:11:55,826 : Computed embeddings
2019-03-13 08:11:55,826 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:12:27,083 : [('reg:1e-05', 59.58), ('reg:0.0001', 52.94), ('reg:0.001', 59.8), ('reg:0.01', 37.47)]
2019-03-13 08:12:27,083 : Validation : best param found is reg = 0.001 with score             59.8
2019-03-13 08:12:27,083 : Evaluating...
2019-03-13 08:12:34,638 : 
Dev acc : 59.8 Test acc : 59.8 for TOPCONSTITUENTS classification

2019-03-13 08:12:34,639 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 08:12:34,988 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 08:12:35,056 : loading BERT model bert-large-uncased
2019-03-13 08:12:35,056 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:12:35,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:12:35,184 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18o64si2
2019-03-13 08:12:42,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:12:48,104 : Computing embeddings for train/dev/test
2019-03-13 08:15:49,140 : Computed embeddings
2019-03-13 08:15:49,140 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:16:22,815 : [('reg:1e-05', 88.95), ('reg:0.0001', 88.99), ('reg:0.001', 88.99), ('reg:0.01', 84.93)]
2019-03-13 08:16:22,815 : Validation : best param found is reg = 0.0001 with score             88.99
2019-03-13 08:16:22,815 : Evaluating...
2019-03-13 08:16:32,472 : 
Dev acc : 89.0 Test acc : 88.9 for BIGRAMSHIFT classification

2019-03-13 08:16:32,473 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 08:16:32,875 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 08:16:32,939 : loading BERT model bert-large-uncased
2019-03-13 08:16:32,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:16:32,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:16:32,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8vq52c9l
2019-03-13 08:16:40,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:16:45,946 : Computing embeddings for train/dev/test
2019-03-13 08:19:42,967 : Computed embeddings
2019-03-13 08:19:42,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:20:09,143 : [('reg:1e-05', 86.08), ('reg:0.0001', 86.07), ('reg:0.001', 86.19), ('reg:0.01', 85.59)]
2019-03-13 08:20:09,143 : Validation : best param found is reg = 0.001 with score             86.19
2019-03-13 08:20:09,143 : Evaluating...
2019-03-13 08:20:15,836 : 
Dev acc : 86.2 Test acc : 84.9 for TENSE classification

2019-03-13 08:20:15,837 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 08:20:16,242 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 08:20:16,304 : loading BERT model bert-large-uncased
2019-03-13 08:20:16,304 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:20:16,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:20:16,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfy40q6ix
2019-03-13 08:20:23,791 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:20:29,264 : Computing embeddings for train/dev/test
2019-03-13 08:23:36,836 : Computed embeddings
2019-03-13 08:23:36,836 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:24:13,599 : [('reg:1e-05', 82.26), ('reg:0.0001', 82.25), ('reg:0.001', 78.57), ('reg:0.01', 80.52)]
2019-03-13 08:24:13,599 : Validation : best param found is reg = 1e-05 with score             82.26
2019-03-13 08:24:13,599 : Evaluating...
2019-03-13 08:24:24,470 : 
Dev acc : 82.3 Test acc : 81.9 for SUBJNUMBER classification

2019-03-13 08:24:24,471 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 08:24:24,871 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 08:24:24,937 : loading BERT model bert-large-uncased
2019-03-13 08:24:24,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:24:25,049 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:24:25,050 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiwn2gqlk
2019-03-13 08:24:32,625 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:24:38,166 : Computing embeddings for train/dev/test
2019-03-13 08:27:42,033 : Computed embeddings
2019-03-13 08:27:42,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:28:12,571 : [('reg:1e-05', 78.88), ('reg:0.0001', 79.91), ('reg:0.001', 76.64), ('reg:0.01', 71.78)]
2019-03-13 08:28:12,571 : Validation : best param found is reg = 0.0001 with score             79.91
2019-03-13 08:28:12,571 : Evaluating...
2019-03-13 08:28:22,083 : 
Dev acc : 79.9 Test acc : 80.1 for OBJNUMBER classification

2019-03-13 08:28:22,085 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 08:28:22,486 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 08:28:22,556 : loading BERT model bert-large-uncased
2019-03-13 08:28:22,556 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:28:22,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:28:22,680 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbmqvjbg7
2019-03-13 08:28:30,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:28:35,736 : Computing embeddings for train/dev/test
2019-03-13 08:32:08,794 : Computed embeddings
2019-03-13 08:32:08,794 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:32:35,978 : [('reg:1e-05', 59.73), ('reg:0.0001', 59.61), ('reg:0.001', 59.52), ('reg:0.01', 59.87)]
2019-03-13 08:32:35,978 : Validation : best param found is reg = 0.01 with score             59.87
2019-03-13 08:32:35,978 : Evaluating...
2019-03-13 08:32:42,224 : 
Dev acc : 59.9 Test acc : 60.0 for ODDMANOUT classification

2019-03-13 08:32:42,225 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 08:32:42,825 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 08:32:42,902 : loading BERT model bert-large-uncased
2019-03-13 08:32:42,903 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:32:42,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:32:42,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv9qxyr8y
2019-03-13 08:32:50,384 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:32:55,914 : Computing embeddings for train/dev/test
2019-03-13 08:36:27,684 : Computed embeddings
2019-03-13 08:36:27,684 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:37:04,953 : [('reg:1e-05', 59.57), ('reg:0.0001', 59.56), ('reg:0.001', 59.46), ('reg:0.01', 55.66)]
2019-03-13 08:37:04,953 : Validation : best param found is reg = 1e-05 with score             59.57
2019-03-13 08:37:04,953 : Evaluating...
2019-03-13 08:37:14,809 : 
Dev acc : 59.6 Test acc : 60.6 for COORDINATIONINVERSION classification

2019-03-13 08:37:14,811 : total results: {'STS12': {'MSRpar': {'pearson': (0.2967091188874101, 1.0430681099456417e-16), 'spearman': SpearmanrResult(correlation=0.3270971901228513, pvalue=3.675007067531817e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6288440294451522, 8.365004833054507e-84), 'spearman': SpearmanrResult(correlation=0.6380349985739963, pvalue=5.719123153827183e-87), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49736479252909155, 4.686904877339897e-30), 'spearman': SpearmanrResult(correlation=0.5875698864034782, pvalue=5.969312160035415e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.558826762897362, 8.063499237232868e-63), 'spearman': SpearmanrResult(correlation=0.597607476917956, pvalue=8.399761976650529e-74), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5513433552794372, 4.157816097083518e-33), 'spearman': SpearmanrResult(correlation=0.4779352511093118, pvalue=3.6659433864929405e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5066176118076906, 'wmean': 0.5024328738577193}, 'spearman': {'mean': 0.5256489606255187, 'wmean': 0.5252398623754552}}}, 'STS13': {'FNWN': {'pearson': (0.19965128089421105, 0.0058812707406238045), 'spearman': SpearmanrResult(correlation=0.20052660075169318, pvalue=0.005663467303774206), 'nsamples': 189}, 'headlines': {'pearson': (0.6252654412073408, 1.3373710879275598e-82), 'spearman': SpearmanrResult(correlation=0.6171171744537859, pvalue=6.449092361473169e-80), 'nsamples': 750}, 'OnWN': {'pearson': (0.46492962032086427, 1.9702584019271594e-31), 'spearman': SpearmanrResult(correlation=0.47171034746341534, pvalue=1.9989337351503147e-32), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.429948780807472, 'wmean': 0.5116724599963443}, 'spearman': {'mean': 0.42978470755629816, 'wmean': 0.5102446088729236}}}, 'STS14': {'deft-forum': {'pearson': (0.34872058104007475, 2.5898850964601262e-14), 'spearman': SpearmanrResult(correlation=0.35964459131743914, pvalue=3.4635884446176374e-15), 'nsamples': 450}, 'deft-news': {'pearson': (0.7135517803501255, 5.757289751233839e-48), 'spearman': SpearmanrResult(correlation=0.6679765100170277, pvalue=3.966249251468551e-40), 'nsamples': 300}, 'headlines': {'pearson': (0.5632402887700391, 5.361184211115247e-64), 'spearman': SpearmanrResult(correlation=0.5363919616544092, pvalue=4.228202951910084e-57), 'nsamples': 750}, 'images': {'pearson': (0.5843919973659198, 6.814546418003424e-70), 'spearman': SpearmanrResult(correlation=0.583630573129122, pvalue=1.130383689374739e-69), 'nsamples': 750}, 'OnWN': {'pearson': (0.6034439596518664, 1.3779287147115246e-75), 'spearman': SpearmanrResult(correlation=0.6334912252193149, pvalue=2.1651410824414265e-85), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5878713114774345, 6.6325420870225264e-71), 'spearman': SpearmanrResult(correlation=0.5641852855853852, pvalue=2.984577440478731e-64), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5668699864425766, 'wmean': 0.5667201236058709}, 'spearman': {'mean': 0.5575533578204497, 'wmean': 0.5601352808771011}}}, 'STS15': {'answers-forums': {'pearson': (0.5132617495909443, 1.3704181357394162e-26), 'spearman': SpearmanrResult(correlation=0.4746293858152208, pvalue=1.8271298558665463e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.6860857443232322, 1.9414981643774817e-105), 'spearman': SpearmanrResult(correlation=0.6925407545848711, pvalue=3.3880941644031975e-108), 'nsamples': 750}, 'belief': {'pearson': (0.5503615845270163, 4.439157533290119e-31), 'spearman': SpearmanrResult(correlation=0.5752530591041567, pvalue=2.021877632007194e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.618623511153863, 2.0864069288576174e-80), 'spearman': SpearmanrResult(correlation=0.6280162096696597, pvalue=1.593431534248587e-83), 'nsamples': 750}, 'images': {'pearson': (0.7274972786468893, 1.6799325453489637e-124), 'spearman': SpearmanrResult(correlation=0.7361088832540548, pvalue=6.487254563967318e-129), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.619165973648389, 'wmean': 0.6410045502957412}, 'spearman': {'mean': 0.6213096584855926, 'wmean': 0.6454017674920686}}}, 'STS16': {'answer-answer': {'pearson': (0.520161192142963, 5.203411153740569e-19), 'spearman': SpearmanrResult(correlation=0.5329634725571538, pvalue=4.8430346371388066e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6271716004251823, 1.254570198555572e-28), 'spearman': SpearmanrResult(correlation=0.6295815385986889, pvalue=6.736275298451952e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7008729767106332, 2.624588374613709e-35), 'spearman': SpearmanrResult(correlation=0.7062650399377155, pvalue=4.6978003373471624e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.7935019226236986, 4.090307303612029e-54), 'spearman': SpearmanrResult(correlation=0.8274363319433145, pvalue=1.395786469237803e-62), 'nsamples': 244}, 'question-question': {'pearson': (0.24346252527294213, 0.00038204233387083335), 'spearman': SpearmanrResult(correlation=0.2694072670119687, pvalue=7.993373714957034e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5770340434350838, 'wmean': 0.5851480546845329}, 'spearman': {'mean': 0.5931307300097683, 'wmean': 0.6009949984198443}}}, 'MR': {'devacc': 59.47, 'acc': 56.71, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 69.54, 'acc': 68.5, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.84, 'acc': 82.53, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.62, 'acc': 90.23, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.83, 'acc': 76.33, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 34.97, 'acc': 35.25, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 72.3, 'acc': 81.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.52, 'acc': 71.54, 'f1': 82.03, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 79.2, 'acc': 78.22, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7943353501826419, 'pearson': 0.7908820634758885, 'spearman': 0.7225625169306588, 'mse': 0.3823136155200254, 'yhat': array([3.08523142, 4.52244381, 1.5894401 , ..., 3.01016429, 4.46903976,        4.67901853]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6687927135538325, 'pearson': 0.6249189205512907, 'spearman': 0.6217535751702504, 'mse': 1.47624835225584, 'yhat': array([1.83195723, 2.45877445, 2.47284569, ..., 4.07660494, 3.61582325,        2.86735695]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.7, 'acc': 60.36, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 65.77, 'acc': 65.72, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 17.73, 'acc': 17.38, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.09, 'acc': 29.52, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.8, 'acc': 59.79, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.99, 'acc': 88.89, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.19, 'acc': 84.88, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.26, 'acc': 81.91, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.91, 'acc': 80.08, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.87, 'acc': 60.05, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 59.57, 'acc': 60.61, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 08:37:14,811 : STS12 p=0.5024, STS12 s=0.5252, STS13 p=0.5117, STS13 s=0.5102, STS14 p=0.5667, STS14 s=0.5601, STS15 p=0.6410, STS15 s=0.6454, STS 16 p=0.5851, STS16 s=0.6010, STS B p=0.6249, STS B s=0.6218, STS B m=1.4762, SICK-R p=0.7909, SICK-R s=0.7226, SICK-P m=0.3823
2019-03-13 08:37:14,811 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 08:37:14,811 : 0.5024,0.5252,0.5117,0.5102,0.5667,0.5601,0.6410,0.6454,0.5851,0.6010,0.6249,0.6218,1.4762,0.7909,0.7226,0.3823
2019-03-13 08:37:14,811 : MR=56.71, CR=68.50, SUBJ=90.23, MPQA=82.53, SST-B=76.33, SST-F=35.25, TREC=81.00, SICK-E=78.22, SNLI=60.36, MRPC=71.54, MRPC f=82.03
2019-03-13 08:37:14,811 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 08:37:14,811 : 56.71,68.50,90.23,82.53,76.33,35.25,81.00,78.22,60.36,71.54,82.03
2019-03-13 08:37:14,811 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 08:37:14,811 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 08:37:14,811 : na,na,na,na,na,na,na,na,na,na
2019-03-13 08:37:14,811 : SentLen=65.72, WC=17.38, TreeDepth=29.52, TopConst=59.79, BShift=88.89, Tense=84.88, SubjNum=81.91, ObjNum=80.08, SOMO=60.05, CoordInv=60.61, average=62.88
2019-03-13 08:37:14,811 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 08:37:14,811 : 65.72,17.38,29.52,59.79,88.89,84.88,81.91,80.08,60.05,60.61,62.88
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,812 : layer 14
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,812 : ********************************************************************************
2019-03-13 08:37:14,905 : ***** Transfer task : STS12 *****


2019-03-13 08:37:14,918 : loading BERT model bert-large-uncased
2019-03-13 08:37:14,918 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:37:14,935 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:37:14,935 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4fqswz3c
2019-03-13 08:37:22,354 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:37:31,744 : MSRpar : pearson = 0.2948, spearman = 0.3237
2019-03-13 08:37:33,367 : MSRvid : pearson = 0.5977, spearman = 0.6069
2019-03-13 08:37:34,764 : SMTeuroparl : pearson = 0.4941, spearman = 0.5953
2019-03-13 08:37:37,426 : surprise.OnWN : pearson = 0.5477, spearman = 0.5846
2019-03-13 08:37:38,835 : surprise.SMTnews : pearson = 0.5802, spearman = 0.4855
2019-03-13 08:37:38,835 : ALL (weighted average) : Pearson = 0.4950,             Spearman = 0.5159
2019-03-13 08:37:38,835 : ALL (average) : Pearson = 0.5029,             Spearman = 0.5192

2019-03-13 08:37:38,835 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 08:37:38,845 : loading BERT model bert-large-uncased
2019-03-13 08:37:38,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:37:38,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:37:38,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa7i1icr_
2019-03-13 08:37:46,310 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:37:53,188 : FNWN : pearson = 0.1893, spearman = 0.1861
2019-03-13 08:37:55,058 : headlines : pearson = 0.6155, spearman = 0.6108
2019-03-13 08:37:56,505 : OnWN : pearson = 0.4647, spearman = 0.4680
2019-03-13 08:37:56,506 : ALL (weighted average) : Pearson = 0.5054,             Spearman = 0.5039
2019-03-13 08:37:56,506 : ALL (average) : Pearson = 0.4232,             Spearman = 0.4216

2019-03-13 08:37:56,506 : ***** Transfer task : STS14 *****


2019-03-13 08:37:56,521 : loading BERT model bert-large-uncased
2019-03-13 08:37:56,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:37:56,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:37:56,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkxfehk0t
2019-03-13 08:38:04,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:10,977 : deft-forum : pearson = 0.3347, spearman = 0.3416
2019-03-13 08:38:12,593 : deft-news : pearson = 0.7222, spearman = 0.6783
2019-03-13 08:38:14,735 : headlines : pearson = 0.5481, spearman = 0.5249
2019-03-13 08:38:16,785 : images : pearson = 0.5629, spearman = 0.5605
2019-03-13 08:38:18,885 : OnWN : pearson = 0.6058, spearman = 0.6348
2019-03-13 08:38:21,700 : tweet-news : pearson = 0.5791, spearman = 0.5543
2019-03-13 08:38:21,700 : ALL (weighted average) : Pearson = 0.5571,             Spearman = 0.5502
2019-03-13 08:38:21,700 : ALL (average) : Pearson = 0.5588,             Spearman = 0.5491

2019-03-13 08:38:21,700 : ***** Transfer task : STS15 *****


2019-03-13 08:38:21,734 : loading BERT model bert-large-uncased
2019-03-13 08:38:21,734 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:38:21,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:38:21,752 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp626ifdvh
2019-03-13 08:38:29,165 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:36,612 : answers-forums : pearson = 0.5083, spearman = 0.4785
2019-03-13 08:38:38,670 : answers-students : pearson = 0.6813, spearman = 0.6877
2019-03-13 08:38:40,689 : belief : pearson = 0.5408, spearman = 0.5697
2019-03-13 08:38:42,909 : headlines : pearson = 0.6050, spearman = 0.6157
2019-03-13 08:38:45,015 : images : pearson = 0.7150, spearman = 0.7250
2019-03-13 08:38:45,015 : ALL (weighted average) : Pearson = 0.6315,             Spearman = 0.6381
2019-03-13 08:38:45,015 : ALL (average) : Pearson = 0.6101,             Spearman = 0.6153

2019-03-13 08:38:45,016 : ***** Transfer task : STS16 *****


2019-03-13 08:38:45,085 : loading BERT model bert-large-uncased
2019-03-13 08:38:45,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:38:45,103 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:38:45,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk_yy7jpu
2019-03-13 08:38:52,552 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:38:58,891 : answer-answer : pearson = 0.5361, spearman = 0.5537
2019-03-13 08:38:59,545 : headlines : pearson = 0.6189, spearman = 0.6212
2019-03-13 08:39:00,416 : plagiarism : pearson = 0.6849, spearman = 0.6897
2019-03-13 08:39:01,889 : postediting : pearson = 0.7824, spearman = 0.8206
2019-03-13 08:39:02,489 : question-question : pearson = 0.2120, spearman = 0.2334
2019-03-13 08:39:02,489 : ALL (weighted average) : Pearson = 0.5759,             Spearman = 0.5927
2019-03-13 08:39:02,489 : ALL (average) : Pearson = 0.5669,             Spearman = 0.5837

2019-03-13 08:39:02,489 : ***** Transfer task : MR *****


2019-03-13 08:39:02,508 : loading BERT model bert-large-uncased
2019-03-13 08:39:02,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:39:02,527 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:39:02,528 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptn3jld08
2019-03-13 08:39:10,004 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:39:15,600 : Generating sentence embeddings
2019-03-13 08:39:46,587 : Generated sentence embeddings
2019-03-13 08:39:46,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:39:57,360 : Best param found at split 1: l2reg = 0.01                 with score 64.61
2019-03-13 08:40:10,229 : Best param found at split 2: l2reg = 0.001                 with score 63.82
2019-03-13 08:40:23,996 : Best param found at split 3: l2reg = 0.01                 with score 63.25
2019-03-13 08:40:36,127 : Best param found at split 4: l2reg = 0.0001                 with score 62.05
2019-03-13 08:40:49,737 : Best param found at split 5: l2reg = 1e-05                 with score 66.21
2019-03-13 08:40:50,541 : Dev acc : 63.99 Test acc : 63.56

2019-03-13 08:40:50,542 : ***** Transfer task : CR *****


2019-03-13 08:40:50,550 : loading BERT model bert-large-uncased
2019-03-13 08:40:50,550 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:40:50,571 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:40:50,571 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsbps3xm4
2019-03-13 08:40:58,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:41:03,472 : Generating sentence embeddings
2019-03-13 08:41:11,671 : Generated sentence embeddings
2019-03-13 08:41:11,671 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:41:14,814 : Best param found at split 1: l2reg = 0.0001                 with score 72.47
2019-03-13 08:41:18,632 : Best param found at split 2: l2reg = 1e-05                 with score 73.34
2019-03-13 08:41:22,029 : Best param found at split 3: l2reg = 1e-05                 with score 70.96
2019-03-13 08:41:25,624 : Best param found at split 4: l2reg = 1e-05                 with score 71.07
2019-03-13 08:41:29,236 : Best param found at split 5: l2reg = 0.0001                 with score 70.14
2019-03-13 08:41:29,395 : Dev acc : 71.6 Test acc : 70.51

2019-03-13 08:41:29,395 : ***** Transfer task : MPQA *****


2019-03-13 08:41:29,401 : loading BERT model bert-large-uncased
2019-03-13 08:41:29,401 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:41:29,422 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:41:29,425 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppjhge0bv
2019-03-13 08:41:36,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:41:42,438 : Generating sentence embeddings
2019-03-13 08:41:49,894 : Generated sentence embeddings
2019-03-13 08:41:49,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:41:59,138 : Best param found at split 1: l2reg = 0.01                 with score 85.27
2019-03-13 08:42:10,397 : Best param found at split 2: l2reg = 0.01                 with score 85.11
2019-03-13 08:42:21,841 : Best param found at split 3: l2reg = 0.001                 with score 85.22
2019-03-13 08:42:34,479 : Best param found at split 4: l2reg = 0.01                 with score 85.16
2019-03-13 08:42:46,814 : Best param found at split 5: l2reg = 0.001                 with score 85.22
2019-03-13 08:42:47,360 : Dev acc : 85.2 Test acc : 83.04

2019-03-13 08:42:47,361 : ***** Transfer task : SUBJ *****


2019-03-13 08:42:47,378 : loading BERT model bert-large-uncased
2019-03-13 08:42:47,378 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:42:47,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:42:47,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiil_sith
2019-03-13 08:42:54,827 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:43:00,373 : Generating sentence embeddings
2019-03-13 08:43:30,788 : Generated sentence embeddings
2019-03-13 08:43:30,788 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:43:38,836 : Best param found at split 1: l2reg = 0.01                 with score 91.34
2019-03-13 08:43:47,792 : Best param found at split 2: l2reg = 0.01                 with score 92.04
2019-03-13 08:43:56,317 : Best param found at split 3: l2reg = 0.001                 with score 90.62
2019-03-13 08:44:07,784 : Best param found at split 4: l2reg = 1e-05                 with score 92.3
2019-03-13 08:44:19,491 : Best param found at split 5: l2reg = 1e-05                 with score 91.76
2019-03-13 08:44:20,288 : Dev acc : 91.61 Test acc : 90.45

2019-03-13 08:44:20,289 : ***** Transfer task : SST Binary classification *****


2019-03-13 08:44:20,416 : loading BERT model bert-large-uncased
2019-03-13 08:44:20,416 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:44:20,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:44:20,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiju7zxes
2019-03-13 08:44:27,967 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:44:33,528 : Computing embedding for train
2019-03-13 08:46:12,027 : Computed train embeddings
2019-03-13 08:46:12,028 : Computing embedding for dev
2019-03-13 08:46:14,172 : Computed dev embeddings
2019-03-13 08:46:14,172 : Computing embedding for test
2019-03-13 08:46:18,677 : Computed test embeddings
2019-03-13 08:46:18,677 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:46:37,528 : [('reg:1e-05', 79.36), ('reg:0.0001', 80.5), ('reg:0.001', 78.9), ('reg:0.01', 72.94)]
2019-03-13 08:46:37,529 : Validation : best param found is reg = 0.0001 with score             80.5
2019-03-13 08:46:37,529 : Evaluating...
2019-03-13 08:46:42,650 : 
Dev acc : 80.5 Test acc : 79.52 for             SST Binary classification

2019-03-13 08:46:42,650 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 08:46:42,700 : loading BERT model bert-large-uncased
2019-03-13 08:46:42,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:46:42,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:46:42,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkr5lvcew
2019-03-13 08:46:50,214 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:46:55,707 : Computing embedding for train
2019-03-13 08:47:17,261 : Computed train embeddings
2019-03-13 08:47:17,261 : Computing embedding for dev
2019-03-13 08:47:20,076 : Computed dev embeddings
2019-03-13 08:47:20,076 : Computing embedding for test
2019-03-13 08:47:25,629 : Computed test embeddings
2019-03-13 08:47:25,629 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:47:27,918 : [('reg:1e-05', 31.88), ('reg:0.0001', 31.88), ('reg:0.001', 36.51), ('reg:0.01', 36.51)]
2019-03-13 08:47:27,918 : Validation : best param found is reg = 0.001 with score             36.51
2019-03-13 08:47:27,918 : Evaluating...
2019-03-13 08:47:28,489 : 
Dev acc : 36.51 Test acc : 37.15 for             SST Fine-Grained classification

2019-03-13 08:47:28,489 : ***** Transfer task : TREC *****


2019-03-13 08:47:28,503 : loading BERT model bert-large-uncased
2019-03-13 08:47:28,503 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:47:28,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:47:28,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8bj3runb
2019-03-13 08:47:36,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:47:49,060 : Computed train embeddings
2019-03-13 08:47:49,640 : Computed test embeddings
2019-03-13 08:47:49,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:47:57,537 : [('reg:1e-05', 67.22), ('reg:0.0001', 70.27), ('reg:0.001', 72.01), ('reg:0.01', 69.76)]
2019-03-13 08:47:57,537 : Cross-validation : best param found is reg = 0.001             with score 72.01
2019-03-13 08:47:57,538 : Evaluating...
2019-03-13 08:47:57,963 : 
Dev acc : 72.01 Test acc : 79.0             for TREC

2019-03-13 08:47:57,964 : ***** Transfer task : MRPC *****


2019-03-13 08:47:57,985 : loading BERT model bert-large-uncased
2019-03-13 08:47:57,986 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:47:58,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:47:58,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2kqnpq6k
2019-03-13 08:48:05,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:48:10,767 : Computing embedding for train
2019-03-13 08:48:32,662 : Computed train embeddings
2019-03-13 08:48:32,662 : Computing embedding for test
2019-03-13 08:48:42,250 : Computed test embeddings
2019-03-13 08:48:42,270 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 08:48:47,341 : [('reg:1e-05', 72.2), ('reg:0.0001', 71.29), ('reg:0.001', 73.04), ('reg:0.01', 71.56)]
2019-03-13 08:48:47,341 : Cross-validation : best param found is reg = 0.001             with score 73.04
2019-03-13 08:48:47,341 : Evaluating...
2019-03-13 08:48:47,611 : Dev acc : 73.04 Test acc 71.71; Test F1 77.88 for MRPC.

2019-03-13 08:48:47,611 : ***** Transfer task : SICK-Entailment*****


2019-03-13 08:48:47,670 : loading BERT model bert-large-uncased
2019-03-13 08:48:47,671 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:48:47,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:48:47,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnkv1b3zw
2019-03-13 08:48:55,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:49:00,648 : Computing embedding for train
2019-03-13 08:49:11,773 : Computed train embeddings
2019-03-13 08:49:11,773 : Computing embedding for dev
2019-03-13 08:49:13,289 : Computed dev embeddings
2019-03-13 08:49:13,289 : Computing embedding for test
2019-03-13 08:49:25,224 : Computed test embeddings
2019-03-13 08:49:25,261 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:49:26,512 : [('reg:1e-05', 76.0), ('reg:0.0001', 76.6), ('reg:0.001', 78.8), ('reg:0.01', 76.6)]
2019-03-13 08:49:26,512 : Validation : best param found is reg = 0.001 with score             78.8
2019-03-13 08:49:26,512 : Evaluating...
2019-03-13 08:49:26,885 : 
Dev acc : 78.8 Test acc : 78.2 for                        SICK entailment

2019-03-13 08:49:26,885 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 08:49:26,912 : loading BERT model bert-large-uncased
2019-03-13 08:49:26,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:49:26,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:49:26,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprlde3dek
2019-03-13 08:49:34,412 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:49:39,882 : Computing embedding for train
2019-03-13 08:49:51,014 : Computed train embeddings
2019-03-13 08:49:51,014 : Computing embedding for dev
2019-03-13 08:49:52,530 : Computed dev embeddings
2019-03-13 08:49:52,531 : Computing embedding for test
2019-03-13 08:50:04,493 : Computed test embeddings
2019-03-13 08:50:18,305 : Dev : Pearson 0.7869077916426472
2019-03-13 08:50:18,305 : Test : Pearson 0.7887765160635547 Spearman 0.7294464893499722 MSE 0.38505076728749543                        for SICK Relatedness

2019-03-13 08:50:18,306 : 

***** Transfer task : STSBenchmark*****


2019-03-13 08:50:18,372 : loading BERT model bert-large-uncased
2019-03-13 08:50:18,373 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:50:18,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:50:18,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_ib4d80g
2019-03-13 08:50:25,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:50:31,438 : Computing embedding for train
2019-03-13 08:50:49,739 : Computed train embeddings
2019-03-13 08:50:49,739 : Computing embedding for dev
2019-03-13 08:50:55,292 : Computed dev embeddings
2019-03-13 08:50:55,292 : Computing embedding for test
2019-03-13 08:50:59,829 : Computed test embeddings
2019-03-13 08:51:13,253 : Dev : Pearson 0.6403413695983213
2019-03-13 08:51:13,253 : Test : Pearson 0.6028076065512473 Spearman 0.5997443535257666 MSE 1.5500003523662484                        for SICK Relatedness

2019-03-13 08:51:13,254 : ***** Transfer task : SNLI Entailment*****


2019-03-13 08:51:18,423 : loading BERT model bert-large-uncased
2019-03-13 08:51:18,424 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:51:18,545 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:51:18,545 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbb5f4ty9
2019-03-13 08:51:25,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:51:31,941 : PROGRESS (encoding): 0.00%
2019-03-13 08:54:15,340 : PROGRESS (encoding): 14.56%
2019-03-13 08:57:20,590 : PROGRESS (encoding): 29.12%
2019-03-13 09:00:26,619 : PROGRESS (encoding): 43.69%
2019-03-13 09:03:44,853 : PROGRESS (encoding): 58.25%
2019-03-13 09:07:25,641 : PROGRESS (encoding): 72.81%
2019-03-13 09:11:05,408 : PROGRESS (encoding): 87.37%
2019-03-13 09:15:03,214 : PROGRESS (encoding): 0.00%
2019-03-13 09:15:33,167 : PROGRESS (encoding): 0.00%
2019-03-13 09:16:01,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:16:48,482 : [('reg:1e-09', 66.01)]
2019-03-13 09:16:48,482 : Validation : best param found is reg = 1e-09 with score             66.01
2019-03-13 09:16:48,482 : Evaluating...
2019-03-13 09:17:30,408 : Dev acc : 66.01 Test acc : 66.55 for SNLI

2019-03-13 09:17:30,408 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 09:17:30,618 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 09:17:31,683 : loading BERT model bert-large-uncased
2019-03-13 09:17:31,683 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:17:31,711 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:17:31,711 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0prgrj1z
2019-03-13 09:17:39,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:17:44,644 : Computing embeddings for train/dev/test
2019-03-13 09:21:12,577 : Computed embeddings
2019-03-13 09:21:12,577 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:21:41,194 : [('reg:1e-05', 57.6), ('reg:0.0001', 62.15), ('reg:0.001', 62.11), ('reg:0.01', 46.0)]
2019-03-13 09:21:41,194 : Validation : best param found is reg = 0.0001 with score             62.15
2019-03-13 09:21:41,194 : Evaluating...
2019-03-13 09:21:48,699 : 
Dev acc : 62.1 Test acc : 62.1 for LENGTH classification

2019-03-13 09:21:48,699 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 09:21:48,953 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 09:21:48,998 : loading BERT model bert-large-uncased
2019-03-13 09:21:48,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:21:49,027 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:21:49,027 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjfjodkm8
2019-03-13 09:21:56,528 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:22:02,035 : Computing embeddings for train/dev/test
2019-03-13 09:25:13,399 : Computed embeddings
2019-03-13 09:25:13,400 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:25:45,632 : [('reg:1e-05', 11.15), ('reg:0.0001', 4.36), ('reg:0.001', 0.42), ('reg:0.01', 0.12)]
2019-03-13 09:25:45,632 : Validation : best param found is reg = 1e-05 with score             11.15
2019-03-13 09:25:45,632 : Evaluating...
2019-03-13 09:25:55,781 : 
Dev acc : 11.2 Test acc : 11.1 for WORDCONTENT classification

2019-03-13 09:25:55,783 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 09:25:56,313 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 09:25:56,380 : loading BERT model bert-large-uncased
2019-03-13 09:25:56,381 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:25:56,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:25:56,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp120ozflq
2019-03-13 09:26:03,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:26:09,526 : Computing embeddings for train/dev/test
2019-03-13 09:29:09,154 : Computed embeddings
2019-03-13 09:29:09,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:29:38,085 : [('reg:1e-05', 29.85), ('reg:0.0001', 29.74), ('reg:0.001', 27.68), ('reg:0.01', 27.12)]
2019-03-13 09:29:38,085 : Validation : best param found is reg = 1e-05 with score             29.85
2019-03-13 09:29:38,085 : Evaluating...
2019-03-13 09:29:47,399 : 
Dev acc : 29.9 Test acc : 30.4 for DEPTH classification

2019-03-13 09:29:47,400 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 09:29:47,806 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 09:29:47,868 : loading BERT model bert-large-uncased
2019-03-13 09:29:47,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:29:47,897 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:29:47,897 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmwsf5y5o
2019-03-13 09:29:55,366 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:30:00,871 : Computing embeddings for train/dev/test
2019-03-13 09:32:47,698 : Computed embeddings
2019-03-13 09:32:47,698 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:33:26,282 : [('reg:1e-05', 58.9), ('reg:0.0001', 58.86), ('reg:0.001', 53.09), ('reg:0.01', 49.38)]
2019-03-13 09:33:26,282 : Validation : best param found is reg = 1e-05 with score             58.9
2019-03-13 09:33:26,282 : Evaluating...
2019-03-13 09:33:33,173 : 
Dev acc : 58.9 Test acc : 58.7 for TOPCONSTITUENTS classification

2019-03-13 09:33:33,174 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 09:33:33,534 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 09:33:33,601 : loading BERT model bert-large-uncased
2019-03-13 09:33:33,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:33:33,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:33:33,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_nl_lw7j
2019-03-13 09:33:41,096 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:33:46,600 : Computing embeddings for train/dev/test
2019-03-13 09:36:47,536 : Computed embeddings
2019-03-13 09:36:47,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:37:14,085 : [('reg:1e-05', 90.55), ('reg:0.0001', 90.63), ('reg:0.001', 90.6), ('reg:0.01', 90.22)]
2019-03-13 09:37:14,085 : Validation : best param found is reg = 0.0001 with score             90.63
2019-03-13 09:37:14,085 : Evaluating...
2019-03-13 09:37:21,321 : 
Dev acc : 90.6 Test acc : 90.2 for BIGRAMSHIFT classification

2019-03-13 09:37:21,322 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 09:37:21,719 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 09:37:21,785 : loading BERT model bert-large-uncased
2019-03-13 09:37:21,785 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:37:21,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:37:21,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu91g39nf
2019-03-13 09:37:29,295 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:37:34,812 : Computing embeddings for train/dev/test
2019-03-13 09:40:32,021 : Computed embeddings
2019-03-13 09:40:32,021 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:41:01,486 : [('reg:1e-05', 85.37), ('reg:0.0001', 85.33), ('reg:0.001', 85.36), ('reg:0.01', 84.94)]
2019-03-13 09:41:01,487 : Validation : best param found is reg = 1e-05 with score             85.37
2019-03-13 09:41:01,487 : Evaluating...
2019-03-13 09:41:09,067 : 
Dev acc : 85.4 Test acc : 84.8 for TENSE classification

2019-03-13 09:41:09,069 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 09:41:09,472 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 09:41:09,536 : loading BERT model bert-large-uncased
2019-03-13 09:41:09,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:41:09,653 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:41:09,653 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfkbgljvu
2019-03-13 09:41:17,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:41:22,715 : Computing embeddings for train/dev/test
2019-03-13 09:44:30,311 : Computed embeddings
2019-03-13 09:44:30,311 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:44:57,318 : [('reg:1e-05', 82.25), ('reg:0.0001', 82.38), ('reg:0.001', 82.23), ('reg:0.01', 78.59)]
2019-03-13 09:44:57,318 : Validation : best param found is reg = 0.0001 with score             82.38
2019-03-13 09:44:57,318 : Evaluating...
2019-03-13 09:45:03,267 : 
Dev acc : 82.4 Test acc : 81.8 for SUBJNUMBER classification

2019-03-13 09:45:03,268 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 09:45:03,864 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 09:45:03,930 : loading BERT model bert-large-uncased
2019-03-13 09:45:03,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:45:03,957 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:45:03,957 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp60nmzgp_
2019-03-13 09:45:11,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:45:16,983 : Computing embeddings for train/dev/test
2019-03-13 09:48:21,264 : Computed embeddings
2019-03-13 09:48:21,264 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:48:57,184 : [('reg:1e-05', 79.8), ('reg:0.0001', 79.79), ('reg:0.001', 79.77), ('reg:0.01', 76.87)]
2019-03-13 09:48:57,184 : Validation : best param found is reg = 1e-05 with score             79.8
2019-03-13 09:48:57,184 : Evaluating...
2019-03-13 09:49:06,935 : 
Dev acc : 79.8 Test acc : 80.5 for OBJNUMBER classification

2019-03-13 09:49:06,936 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 09:49:07,306 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 09:49:07,375 : loading BERT model bert-large-uncased
2019-03-13 09:49:07,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:49:07,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:49:07,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp805t6ybr
2019-03-13 09:49:14,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:49:20,490 : Computing embeddings for train/dev/test
2019-03-13 09:52:53,956 : Computed embeddings
2019-03-13 09:52:53,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:53:20,147 : [('reg:1e-05', 62.31), ('reg:0.0001', 63.16), ('reg:0.001', 63.06), ('reg:0.01', 62.05)]
2019-03-13 09:53:20,147 : Validation : best param found is reg = 0.0001 with score             63.16
2019-03-13 09:53:20,147 : Evaluating...
2019-03-13 09:53:26,690 : 
Dev acc : 63.2 Test acc : 63.4 for ODDMANOUT classification

2019-03-13 09:53:26,691 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 09:53:27,130 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 09:53:27,206 : loading BERT model bert-large-uncased
2019-03-13 09:53:27,206 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:53:27,235 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:53:27,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi33d8_lx
2019-03-13 09:53:34,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:53:40,153 : Computing embeddings for train/dev/test
2019-03-13 09:57:11,664 : Computed embeddings
2019-03-13 09:57:11,664 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:57:45,935 : [('reg:1e-05', 64.45), ('reg:0.0001', 64.5), ('reg:0.001', 63.64), ('reg:0.01', 59.79)]
2019-03-13 09:57:45,936 : Validation : best param found is reg = 0.0001 with score             64.5
2019-03-13 09:57:45,936 : Evaluating...
2019-03-13 09:57:54,670 : 
Dev acc : 64.5 Test acc : 64.7 for COORDINATIONINVERSION classification

2019-03-13 09:57:54,672 : total results: {'STS12': {'MSRpar': {'pearson': (0.2947836805233246, 1.6740737934305648e-16), 'spearman': SpearmanrResult(correlation=0.32365910519404817, pvalue=9.466157711745482e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.597716202224235, 7.78662209643997e-74), 'spearman': SpearmanrResult(correlation=0.6069251771543865, pvalue=1.1398080583970845e-76), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49408291004692706, 1.26419036728928e-29), 'spearman': SpearmanrResult(correlation=0.5953461064704698, pvalue=2.324060887580629e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5476695285091709, 6.384534377918726e-60), 'spearman': SpearmanrResult(correlation=0.5845966883728168, pvalue=5.946380489513216e-70), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5802017199126326, 2.8482648293549532e-37), 'spearman': SpearmanrResult(correlation=0.4855177901215425, pvalue=5.4623218557499875e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5028908082432582, 'wmean': 0.49498442741287896}, 'spearman': {'mean': 0.5192089734626528, 'wmean': 0.5158851960004439}}}, 'STS13': {'FNWN': {'pearson': (0.18933444283865633, 0.00907291071078821), 'spearman': SpearmanrResult(correlation=0.1860535331827667, pvalue=0.010369276619851115), 'nsamples': 189}, 'headlines': {'pearson': (0.6155135339080079, 2.1298170649144194e-79), 'spearman': SpearmanrResult(correlation=0.6107577461114723, pvalue=7.072691567003895e-78), 'nsamples': 750}, 'OnWN': {'pearson': (0.46472960861441376, 2.1061955718616673e-31), 'spearman': SpearmanrResult(correlation=0.46804134870896824, pvalue=6.93835055632576e-32), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42319252845369265, 'wmean': 0.5054217803734653}, 'spearman': {'mean': 0.42161754266773577, 'wmean': 0.5038690826539189}}}, 'STS14': {'deft-forum': {'pearson': (0.33465927424507597, 3.082938719073208e-13), 'spearman': SpearmanrResult(correlation=0.3415784095740875, pvalue=9.256624895559875e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7222260134949384, 1.235812092107307e-49), 'spearman': SpearmanrResult(correlation=0.6782716240250045, pvalue=8.948952541113721e-42), 'nsamples': 300}, 'headlines': {'pearson': (0.5481236936500022, 4.889479479304458e-60), 'spearman': SpearmanrResult(correlation=0.5249245211335005, pvalue=2.439708729668371e-54), 'nsamples': 750}, 'images': {'pearson': (0.5629409023018792, 6.451784258654517e-64), 'spearman': SpearmanrResult(correlation=0.5604936007394677, pvalue=2.910644740095352e-63), 'nsamples': 750}, 'OnWN': {'pearson': (0.605829692890131, 2.505465123775398e-76), 'spearman': SpearmanrResult(correlation=0.6348083067015181, pvalue=7.59901701621419e-86), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5791464243142748, 2.1671846303397408e-68), 'spearman': SpearmanrResult(correlation=0.5542987670819122, pvalue=1.2476783123168847e-61), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5588210001493836, 'wmean': 0.5571453366202617}, 'spearman': {'mean': 0.5490625382092484, 'wmean': 0.5501561782021706}}}, 'STS15': {'answers-forums': {'pearson': (0.5083152824696626, 4.9516758997225855e-26), 'spearman': SpearmanrResult(correlation=0.47845071186572563, pvalue=7.528379542152602e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.6812965697482647, 1.94294368520372e-103), 'spearman': SpearmanrResult(correlation=0.6876908495605663, pvalue=4.0647327742567347e-106), 'nsamples': 750}, 'belief': {'pearson': (0.5407741316841582, 7.265286137144994e-30), 'spearman': SpearmanrResult(correlation=0.5696508037188707, pvalue=1.2097383475587577e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.6049774339609231, 4.614029649575799e-76), 'spearman': SpearmanrResult(correlation=0.6157054856572671, pvalue=1.8466968891269365e-79), 'nsamples': 750}, 'images': {'pearson': (0.7150111834656704, 2.144988648516826e-118), 'spearman': SpearmanrResult(correlation=0.7250362853031652, pvalue=2.8542875044702003e-123), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6100749202657358, 'wmean': 0.6314574735629421}, 'spearman': {'mean': 0.615306827221119, 'wmean': 0.6381208445783242}}}, 'STS16': {'answer-answer': {'pearson': (0.5361206577494284, 2.654795319915869e-20), 'spearman': SpearmanrResult(correlation=0.5537128756302225, pvalue=8.276840406246678e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6189009234833907, 1.017768165593027e-27), 'spearman': SpearmanrResult(correlation=0.6211543885440999, pvalue=5.789198451722511e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6849068094501095, 3.439159547377819e-33), 'spearman': SpearmanrResult(correlation=0.6896965361006625, pvalue=8.234735896896631e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7823742555198842, 1.1190720575370053e-51), 'spearman': SpearmanrResult(correlation=0.8205677769956983, pvalue=1.0036723837381083e-60), 'nsamples': 244}, 'question-question': {'pearson': (0.21199209448394768, 0.00205992617239335), 'spearman': SpearmanrResult(correlation=0.2333993632110742, pvalue=0.0006712147889262454), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5668589481373522, 'wmean': 0.5758981528526484}, 'spearman': {'mean': 0.5837061880963514, 'wmean': 0.5926979097460158}}}, 'MR': {'devacc': 63.99, 'acc': 63.56, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 71.6, 'acc': 70.51, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.2, 'acc': 83.04, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.61, 'acc': 90.45, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.5, 'acc': 79.52, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.51, 'acc': 37.15, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 72.01, 'acc': 79.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.04, 'acc': 71.71, 'f1': 77.88, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.8, 'acc': 78.2, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7869077916426472, 'pearson': 0.7887765160635547, 'spearman': 0.7294464893499722, 'mse': 0.38505076728749543, 'yhat': array([3.03980287, 4.52043248, 1.17231591, ..., 2.99839356, 4.15756128,        4.28020938]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6403413695983213, 'pearson': 0.6028076065512473, 'spearman': 0.5997443535257666, 'mse': 1.5500003523662484, 'yhat': array([1.51801343, 1.58917007, 3.06648144, ..., 3.57442276, 3.62006296,        3.0903089 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.01, 'acc': 66.55, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 62.15, 'acc': 62.1, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 11.15, 'acc': 11.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.85, 'acc': 30.44, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.9, 'acc': 58.7, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.63, 'acc': 90.24, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.37, 'acc': 84.81, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.38, 'acc': 81.78, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.8, 'acc': 80.46, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.16, 'acc': 63.38, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 64.5, 'acc': 64.65, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 09:57:54,672 : STS12 p=0.4950, STS12 s=0.5159, STS13 p=0.5054, STS13 s=0.5039, STS14 p=0.5571, STS14 s=0.5502, STS15 p=0.6315, STS15 s=0.6381, STS 16 p=0.5759, STS16 s=0.5927, STS B p=0.6028, STS B s=0.5997, STS B m=1.5500, SICK-R p=0.7888, SICK-R s=0.7294, SICK-P m=0.3851
2019-03-13 09:57:54,672 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 09:57:54,672 : 0.4950,0.5159,0.5054,0.5039,0.5571,0.5502,0.6315,0.6381,0.5759,0.5927,0.6028,0.5997,1.5500,0.7888,0.7294,0.3851
2019-03-13 09:57:54,672 : MR=63.56, CR=70.51, SUBJ=90.45, MPQA=83.04, SST-B=79.52, SST-F=37.15, TREC=79.00, SICK-E=78.20, SNLI=66.55, MRPC=71.71, MRPC f=77.88
2019-03-13 09:57:54,672 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 09:57:54,672 : 63.56,70.51,90.45,83.04,79.52,37.15,79.00,78.20,66.55,71.71,77.88
2019-03-13 09:57:54,672 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 09:57:54,672 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 09:57:54,672 : na,na,na,na,na,na,na,na,na,na
2019-03-13 09:57:54,672 : SentLen=62.10, WC=11.06, TreeDepth=30.44, TopConst=58.70, BShift=90.24, Tense=84.81, SubjNum=81.78, ObjNum=80.46, SOMO=63.38, CoordInv=64.65, average=62.76
2019-03-13 09:57:54,672 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 09:57:54,672 : 62.10,11.06,30.44,58.70,90.24,84.81,81.78,80.46,63.38,64.65,62.76
2019-03-13 09:57:54,672 : ********************************************************************************
2019-03-13 09:57:54,672 : ********************************************************************************
2019-03-13 09:57:54,673 : ********************************************************************************
2019-03-13 09:57:54,673 : layer 15
2019-03-13 09:57:54,673 : ********************************************************************************
2019-03-13 09:57:54,673 : ********************************************************************************
2019-03-13 09:57:54,673 : ********************************************************************************
2019-03-13 09:57:54,764 : ***** Transfer task : STS12 *****


2019-03-13 09:57:54,776 : loading BERT model bert-large-uncased
2019-03-13 09:57:54,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:57:54,794 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:57:54,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3kyk1vnd
2019-03-13 09:58:02,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:58:11,772 : MSRpar : pearson = 0.2961, spearman = 0.3270
2019-03-13 09:58:13,397 : MSRvid : pearson = 0.6119, spearman = 0.6230
2019-03-13 09:58:14,797 : SMTeuroparl : pearson = 0.5026, spearman = 0.5929
2019-03-13 09:58:17,462 : surprise.OnWN : pearson = 0.5422, spearman = 0.5826
2019-03-13 09:58:18,873 : surprise.SMTnews : pearson = 0.5904, spearman = 0.4960
2019-03-13 09:58:18,874 : ALL (weighted average) : Pearson = 0.5000,             Spearman = 0.5211
2019-03-13 09:58:18,874 : ALL (average) : Pearson = 0.5087,             Spearman = 0.5243

2019-03-13 09:58:18,874 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 09:58:18,882 : loading BERT model bert-large-uncased
2019-03-13 09:58:18,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:58:18,900 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:58:18,900 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp48pjspvv
2019-03-13 09:58:26,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:58:33,121 : FNWN : pearson = 0.1937, spearman = 0.1840
2019-03-13 09:58:34,991 : headlines : pearson = 0.6183, spearman = 0.6145
2019-03-13 09:58:36,441 : OnWN : pearson = 0.5108, spearman = 0.5089
2019-03-13 09:58:36,441 : ALL (weighted average) : Pearson = 0.5246,             Spearman = 0.5208
2019-03-13 09:58:36,441 : ALL (average) : Pearson = 0.4410,             Spearman = 0.4358

2019-03-13 09:58:36,441 : ***** Transfer task : STS14 *****


2019-03-13 09:58:36,456 : loading BERT model bert-large-uncased
2019-03-13 09:58:36,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:58:36,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:58:36,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbitzvthk
2019-03-13 09:58:43,993 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:58:50,839 : deft-forum : pearson = 0.3468, spearman = 0.3521
2019-03-13 09:58:52,455 : deft-news : pearson = 0.7337, spearman = 0.6890
2019-03-13 09:58:54,595 : headlines : pearson = 0.5514, spearman = 0.5281
2019-03-13 09:58:56,647 : images : pearson = 0.5712, spearman = 0.5678
2019-03-13 09:58:58,757 : OnWN : pearson = 0.6403, spearman = 0.6682
2019-03-13 09:59:01,582 : tweet-news : pearson = 0.5726, spearman = 0.5454
2019-03-13 09:59:01,582 : ALL (weighted average) : Pearson = 0.5674,             Spearman = 0.5593
2019-03-13 09:59:01,582 : ALL (average) : Pearson = 0.5693,             Spearman = 0.5584

2019-03-13 09:59:01,583 : ***** Transfer task : STS15 *****


2019-03-13 09:59:01,614 : loading BERT model bert-large-uncased
2019-03-13 09:59:01,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:01,632 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:01,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp754bt8ck
2019-03-13 09:59:09,065 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:16,374 : answers-forums : pearson = 0.5159, spearman = 0.4858
2019-03-13 09:59:18,428 : answers-students : pearson = 0.6715, spearman = 0.6794
2019-03-13 09:59:20,449 : belief : pearson = 0.5494, spearman = 0.5787
2019-03-13 09:59:22,672 : headlines : pearson = 0.6055, spearman = 0.6171
2019-03-13 09:59:24,781 : images : pearson = 0.7163, spearman = 0.7260
2019-03-13 09:59:24,781 : ALL (weighted average) : Pearson = 0.6315,             Spearman = 0.6387
2019-03-13 09:59:24,781 : ALL (average) : Pearson = 0.6117,             Spearman = 0.6174

2019-03-13 09:59:24,781 : ***** Transfer task : STS16 *****


2019-03-13 09:59:24,851 : loading BERT model bert-large-uncased
2019-03-13 09:59:24,852 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:24,869 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:24,870 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoahxe451
2019-03-13 09:59:32,391 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:38,702 : answer-answer : pearson = 0.5659, spearman = 0.5822
2019-03-13 09:59:39,353 : headlines : pearson = 0.6273, spearman = 0.6298
2019-03-13 09:59:40,223 : plagiarism : pearson = 0.7063, spearman = 0.7210
2019-03-13 09:59:41,696 : postediting : pearson = 0.7794, spearman = 0.8164
2019-03-13 09:59:42,294 : question-question : pearson = 0.2690, spearman = 0.2817
2019-03-13 09:59:42,294 : ALL (weighted average) : Pearson = 0.5976,             Spearman = 0.6143
2019-03-13 09:59:42,294 : ALL (average) : Pearson = 0.5896,             Spearman = 0.6062

2019-03-13 09:59:42,294 : ***** Transfer task : MR *****


2019-03-13 09:59:42,309 : loading BERT model bert-large-uncased
2019-03-13 09:59:42,309 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:59:42,330 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:59:42,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmputdx30a4
2019-03-13 09:59:49,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:55,359 : Generating sentence embeddings
2019-03-13 10:00:26,399 : Generated sentence embeddings
2019-03-13 10:00:26,399 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:00:38,595 : Best param found at split 1: l2reg = 1e-05                 with score 66.1
2019-03-13 10:00:51,790 : Best param found at split 2: l2reg = 1e-05                 with score 68.18
2019-03-13 10:01:04,285 : Best param found at split 3: l2reg = 1e-05                 with score 65.59
2019-03-13 10:01:16,701 : Best param found at split 4: l2reg = 1e-05                 with score 65.52
2019-03-13 10:01:31,348 : Best param found at split 5: l2reg = 1e-05                 with score 68.85
2019-03-13 10:01:31,972 : Dev acc : 66.85 Test acc : 66.07

2019-03-13 10:01:31,973 : ***** Transfer task : CR *****


2019-03-13 10:01:31,981 : loading BERT model bert-large-uncased
2019-03-13 10:01:31,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:01:32,001 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:01:32,001 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnngyjo43
2019-03-13 10:01:39,437 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:01:44,840 : Generating sentence embeddings
2019-03-13 10:01:53,042 : Generated sentence embeddings
2019-03-13 10:01:53,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:01:55,825 : Best param found at split 1: l2reg = 0.001                 with score 71.88
2019-03-13 10:01:59,581 : Best param found at split 2: l2reg = 0.001                 with score 74.89
2019-03-13 10:02:03,343 : Best param found at split 3: l2reg = 0.0001                 with score 71.89
2019-03-13 10:02:06,861 : Best param found at split 4: l2reg = 0.01                 with score 73.29
2019-03-13 10:02:10,370 : Best param found at split 5: l2reg = 1e-05                 with score 77.13
2019-03-13 10:02:10,561 : Dev acc : 73.82 Test acc : 67.61

2019-03-13 10:02:10,562 : ***** Transfer task : MPQA *****


2019-03-13 10:02:10,568 : loading BERT model bert-large-uncased
2019-03-13 10:02:10,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:02:10,616 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:02:10,616 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbzi59xv8
2019-03-13 10:02:18,076 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:02:23,549 : Generating sentence embeddings
2019-03-13 10:02:31,026 : Generated sentence embeddings
2019-03-13 10:02:31,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:02:42,421 : Best param found at split 1: l2reg = 0.001                 with score 85.15
2019-03-13 10:02:54,687 : Best param found at split 2: l2reg = 0.001                 with score 84.54
2019-03-13 10:03:05,318 : Best param found at split 3: l2reg = 0.0001                 with score 85.39
2019-03-13 10:03:18,460 : Best param found at split 4: l2reg = 0.001                 with score 85.59
2019-03-13 10:03:30,055 : Best param found at split 5: l2reg = 0.0001                 with score 84.96
2019-03-13 10:03:30,631 : Dev acc : 85.13 Test acc : 84.98

2019-03-13 10:03:30,632 : ***** Transfer task : SUBJ *****


2019-03-13 10:03:30,649 : loading BERT model bert-large-uncased
2019-03-13 10:03:30,649 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:03:30,668 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:03:30,668 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptfhm1i4w
2019-03-13 10:03:38,153 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:03:43,551 : Generating sentence embeddings
2019-03-13 10:04:13,970 : Generated sentence embeddings
2019-03-13 10:04:13,970 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:04:25,193 : Best param found at split 1: l2reg = 0.01                 with score 92.5
2019-03-13 10:04:36,820 : Best param found at split 2: l2reg = 1e-05                 with score 92.85
2019-03-13 10:04:48,526 : Best param found at split 3: l2reg = 0.0001                 with score 92.04
2019-03-13 10:04:59,320 : Best param found at split 4: l2reg = 1e-05                 with score 92.89
2019-03-13 10:05:09,736 : Best param found at split 5: l2reg = 0.01                 with score 92.61
2019-03-13 10:05:10,104 : Dev acc : 92.58 Test acc : 91.34

2019-03-13 10:05:10,105 : ***** Transfer task : SST Binary classification *****


2019-03-13 10:05:10,197 : loading BERT model bert-large-uncased
2019-03-13 10:05:10,197 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:05:10,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:05:10,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvgufpc_l
2019-03-13 10:05:17,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:05:23,216 : Computing embedding for train
2019-03-13 10:07:01,669 : Computed train embeddings
2019-03-13 10:07:01,669 : Computing embedding for dev
2019-03-13 10:07:03,816 : Computed dev embeddings
2019-03-13 10:07:03,817 : Computing embedding for test
2019-03-13 10:07:08,323 : Computed test embeddings
2019-03-13 10:07:08,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:07:24,864 : [('reg:1e-05', 82.0), ('reg:0.0001', 82.11), ('reg:0.001', 81.77), ('reg:0.01', 79.93)]
2019-03-13 10:07:24,864 : Validation : best param found is reg = 0.0001 with score             82.11
2019-03-13 10:07:24,864 : Evaluating...
2019-03-13 10:07:28,910 : 
Dev acc : 82.11 Test acc : 80.67 for             SST Binary classification

2019-03-13 10:07:28,911 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 10:07:28,965 : loading BERT model bert-large-uncased
2019-03-13 10:07:28,965 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:07:28,985 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:07:28,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4d3t1d4q
2019-03-13 10:07:36,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:07:41,957 : Computing embedding for train
2019-03-13 10:08:03,538 : Computed train embeddings
2019-03-13 10:08:03,538 : Computing embedding for dev
2019-03-13 10:08:06,356 : Computed dev embeddings
2019-03-13 10:08:06,356 : Computing embedding for test
2019-03-13 10:08:11,904 : Computed test embeddings
2019-03-13 10:08:11,904 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:08:14,508 : [('reg:1e-05', 39.33), ('reg:0.0001', 32.06), ('reg:0.001', 37.24), ('reg:0.01', 29.88)]
2019-03-13 10:08:14,508 : Validation : best param found is reg = 1e-05 with score             39.33
2019-03-13 10:08:14,508 : Evaluating...
2019-03-13 10:08:15,163 : 
Dev acc : 39.33 Test acc : 39.28 for             SST Fine-Grained classification

2019-03-13 10:08:15,163 : ***** Transfer task : TREC *****


2019-03-13 10:08:15,177 : loading BERT model bert-large-uncased
2019-03-13 10:08:15,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:08:15,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:08:15,196 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpit5x9pux
2019-03-13 10:08:22,637 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:08:35,529 : Computed train embeddings
2019-03-13 10:08:36,109 : Computed test embeddings
2019-03-13 10:08:36,110 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:08:43,127 : [('reg:1e-05', 66.09), ('reg:0.0001', 70.92), ('reg:0.001', 67.15), ('reg:0.01', 70.82)]
2019-03-13 10:08:43,127 : Cross-validation : best param found is reg = 0.0001             with score 70.92
2019-03-13 10:08:43,127 : Evaluating...
2019-03-13 10:08:43,470 : 
Dev acc : 70.92 Test acc : 73.2             for TREC

2019-03-13 10:08:43,470 : ***** Transfer task : MRPC *****


2019-03-13 10:08:43,491 : loading BERT model bert-large-uncased
2019-03-13 10:08:43,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:08:43,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:08:43,514 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2flr2ncd
2019-03-13 10:08:50,989 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:08:56,378 : Computing embedding for train
2019-03-13 10:09:18,306 : Computed train embeddings
2019-03-13 10:09:18,306 : Computing embedding for test
2019-03-13 10:09:27,898 : Computed test embeddings
2019-03-13 10:09:27,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:09:32,959 : [('reg:1e-05', 70.73), ('reg:0.0001', 71.83), ('reg:0.001', 72.01), ('reg:0.01', 71.52)]
2019-03-13 10:09:32,959 : Cross-validation : best param found is reg = 0.001             with score 72.01
2019-03-13 10:09:32,960 : Evaluating...
2019-03-13 10:09:33,296 : Dev acc : 72.01 Test acc 72.58; Test F1 79.15 for MRPC.

2019-03-13 10:09:33,296 : ***** Transfer task : SICK-Entailment*****


2019-03-13 10:09:33,359 : loading BERT model bert-large-uncased
2019-03-13 10:09:33,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:09:33,377 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:09:33,377 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaeclkqaf
2019-03-13 10:09:40,809 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:09:46,297 : Computing embedding for train
2019-03-13 10:09:57,433 : Computed train embeddings
2019-03-13 10:09:57,433 : Computing embedding for dev
2019-03-13 10:09:58,951 : Computed dev embeddings
2019-03-13 10:09:58,951 : Computing embedding for test
2019-03-13 10:10:10,889 : Computed test embeddings
2019-03-13 10:10:10,924 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:10:12,559 : [('reg:1e-05', 75.6), ('reg:0.0001', 79.0), ('reg:0.001', 73.6), ('reg:0.01', 80.2)]
2019-03-13 10:10:12,559 : Validation : best param found is reg = 0.01 with score             80.2
2019-03-13 10:10:12,559 : Evaluating...
2019-03-13 10:10:12,929 : 
Dev acc : 80.2 Test acc : 77.43 for                        SICK entailment

2019-03-13 10:10:12,930 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 10:10:12,956 : loading BERT model bert-large-uncased
2019-03-13 10:10:12,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:10:13,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:10:13,013 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz9y02ej7
2019-03-13 10:10:20,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:10:25,853 : Computing embedding for train
2019-03-13 10:10:37,001 : Computed train embeddings
2019-03-13 10:10:37,001 : Computing embedding for dev
2019-03-13 10:10:38,521 : Computed dev embeddings
2019-03-13 10:10:38,521 : Computing embedding for test
2019-03-13 10:10:50,464 : Computed test embeddings
2019-03-13 10:11:03,036 : Dev : Pearson 0.7750585307654541
2019-03-13 10:11:03,036 : Test : Pearson 0.7856839333412707 Spearman 0.7251446127832097 MSE 0.3904801199423053                        for SICK Relatedness

2019-03-13 10:11:03,037 : 

***** Transfer task : STSBenchmark*****


2019-03-13 10:11:03,104 : loading BERT model bert-large-uncased
2019-03-13 10:11:03,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:11:03,125 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:11:03,125 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmoannbz1
2019-03-13 10:11:10,586 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:11:15,991 : Computing embedding for train
2019-03-13 10:11:34,358 : Computed train embeddings
2019-03-13 10:11:34,359 : Computing embedding for dev
2019-03-13 10:11:39,918 : Computed dev embeddings
2019-03-13 10:11:39,919 : Computing embedding for test
2019-03-13 10:11:44,452 : Computed test embeddings
2019-03-13 10:12:03,536 : Dev : Pearson 0.650454470507307
2019-03-13 10:12:03,536 : Test : Pearson 0.63459297776243 Spearman 0.6319223314091079 MSE 1.4587997690389265                        for SICK Relatedness

2019-03-13 10:12:03,536 : ***** Transfer task : SNLI Entailment*****


2019-03-13 10:12:08,633 : loading BERT model bert-large-uncased
2019-03-13 10:12:08,633 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:12:08,753 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:12:08,753 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_abmcsls
2019-03-13 10:12:16,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:12:22,028 : PROGRESS (encoding): 0.00%
2019-03-13 10:15:05,187 : PROGRESS (encoding): 14.56%
2019-03-13 10:18:10,535 : PROGRESS (encoding): 29.12%
2019-03-13 10:21:16,511 : PROGRESS (encoding): 43.69%
2019-03-13 10:24:34,867 : PROGRESS (encoding): 58.25%
2019-03-13 10:28:15,942 : PROGRESS (encoding): 72.81%
2019-03-13 10:31:55,812 : PROGRESS (encoding): 87.37%
2019-03-13 10:35:53,661 : PROGRESS (encoding): 0.00%
2019-03-13 10:36:23,630 : PROGRESS (encoding): 0.00%
2019-03-13 10:36:52,403 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:37:31,598 : [('reg:1e-09', 65.06)]
2019-03-13 10:37:31,598 : Validation : best param found is reg = 1e-09 with score             65.06
2019-03-13 10:37:31,598 : Evaluating...
2019-03-13 10:38:10,540 : Dev acc : 65.06 Test acc : 65.28 for SNLI

2019-03-13 10:38:10,540 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 10:38:10,744 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 10:38:11,832 : loading BERT model bert-large-uncased
2019-03-13 10:38:11,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:38:11,861 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:38:11,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0b0k2npu
2019-03-13 10:38:19,359 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:38:24,950 : Computing embeddings for train/dev/test
2019-03-13 10:41:52,714 : Computed embeddings
2019-03-13 10:41:52,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:42:28,561 : [('reg:1e-05', 62.87), ('reg:0.0001', 63.17), ('reg:0.001', 55.94), ('reg:0.01', 52.97)]
2019-03-13 10:42:28,562 : Validation : best param found is reg = 0.0001 with score             63.17
2019-03-13 10:42:28,562 : Evaluating...
2019-03-13 10:42:39,225 : 
Dev acc : 63.2 Test acc : 62.5 for LENGTH classification

2019-03-13 10:42:39,226 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 10:42:39,602 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 10:42:39,645 : loading BERT model bert-large-uncased
2019-03-13 10:42:39,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:42:39,676 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:42:39,676 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi4iw7jge
2019-03-13 10:42:47,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:42:52,594 : Computing embeddings for train/dev/test
2019-03-13 10:46:04,009 : Computed embeddings
2019-03-13 10:46:04,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:46:28,969 : [('reg:1e-05', 11.0), ('reg:0.0001', 8.16), ('reg:0.001', 0.53), ('reg:0.01', 0.18)]
2019-03-13 10:46:28,970 : Validation : best param found is reg = 1e-05 with score             11.0
2019-03-13 10:46:28,970 : Evaluating...
2019-03-13 10:46:35,199 : 
Dev acc : 11.0 Test acc : 11.3 for WORDCONTENT classification

2019-03-13 10:46:35,201 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 10:46:35,579 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 10:46:35,645 : loading BERT model bert-large-uncased
2019-03-13 10:46:35,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:46:35,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:46:35,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj3fawrnt
2019-03-13 10:46:43,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:46:48,693 : Computing embeddings for train/dev/test
2019-03-13 10:49:48,644 : Computed embeddings
2019-03-13 10:49:48,644 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:50:22,340 : [('reg:1e-05', 34.4), ('reg:0.0001', 26.72), ('reg:0.001', 25.5), ('reg:0.01', 26.71)]
2019-03-13 10:50:22,340 : Validation : best param found is reg = 1e-05 with score             34.4
2019-03-13 10:50:22,341 : Evaluating...
2019-03-13 10:50:30,150 : 
Dev acc : 34.4 Test acc : 34.6 for DEPTH classification

2019-03-13 10:50:30,151 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 10:50:30,532 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 10:50:30,595 : loading BERT model bert-large-uncased
2019-03-13 10:50:30,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:50:30,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:50:30,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp14o_4_4l
2019-03-13 10:50:38,159 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:50:43,667 : Computing embeddings for train/dev/test
2019-03-13 10:53:30,355 : Computed embeddings
2019-03-13 10:53:30,355 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:54:02,357 : [('reg:1e-05', 62.95), ('reg:0.0001', 58.24), ('reg:0.001', 47.77), ('reg:0.01', 37.01)]
2019-03-13 10:54:02,357 : Validation : best param found is reg = 1e-05 with score             62.95
2019-03-13 10:54:02,357 : Evaluating...
2019-03-13 10:54:10,496 : 
Dev acc : 63.0 Test acc : 62.2 for TOPCONSTITUENTS classification

2019-03-13 10:54:10,497 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 10:54:10,844 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 10:54:10,912 : loading BERT model bert-large-uncased
2019-03-13 10:54:10,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:54:11,034 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:54:11,034 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpugvyu8jo
2019-03-13 10:54:18,495 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:54:23,967 : Computing embeddings for train/dev/test
2019-03-13 10:57:25,194 : Computed embeddings
2019-03-13 10:57:25,194 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:57:56,103 : [('reg:1e-05', 90.49), ('reg:0.0001', 90.59), ('reg:0.001', 90.42), ('reg:0.01', 90.16)]
2019-03-13 10:57:56,103 : Validation : best param found is reg = 0.0001 with score             90.59
2019-03-13 10:57:56,103 : Evaluating...
2019-03-13 10:58:04,693 : 
Dev acc : 90.6 Test acc : 90.5 for BIGRAMSHIFT classification

2019-03-13 10:58:04,694 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 10:58:05,279 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 10:58:05,347 : loading BERT model bert-large-uncased
2019-03-13 10:58:05,347 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:58:05,378 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:58:05,378 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9t7vec8y
2019-03-13 10:58:12,811 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:58:18,291 : Computing embeddings for train/dev/test
2019-03-13 11:01:15,505 : Computed embeddings
2019-03-13 11:01:15,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:01:43,850 : [('reg:1e-05', 85.99), ('reg:0.0001', 85.94), ('reg:0.001', 86.06), ('reg:0.01', 85.68)]
2019-03-13 11:01:43,850 : Validation : best param found is reg = 0.001 with score             86.06
2019-03-13 11:01:43,850 : Evaluating...
2019-03-13 11:01:50,401 : 
Dev acc : 86.1 Test acc : 85.4 for TENSE classification

2019-03-13 11:01:50,402 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 11:01:50,835 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 11:01:50,900 : loading BERT model bert-large-uncased
2019-03-13 11:01:50,900 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:01:50,926 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:01:50,926 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8k1wfaln
2019-03-13 11:01:58,423 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:02:04,065 : Computing embeddings for train/dev/test
2019-03-13 11:05:11,984 : Computed embeddings
2019-03-13 11:05:11,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:05:48,570 : [('reg:1e-05', 82.09), ('reg:0.0001', 81.78), ('reg:0.001', 83.05), ('reg:0.01', 81.87)]
2019-03-13 11:05:48,570 : Validation : best param found is reg = 0.001 with score             83.05
2019-03-13 11:05:48,570 : Evaluating...
2019-03-13 11:05:58,438 : 
Dev acc : 83.0 Test acc : 82.4 for SUBJNUMBER classification

2019-03-13 11:05:58,439 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 11:05:58,846 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 11:05:58,912 : loading BERT model bert-large-uncased
2019-03-13 11:05:58,912 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:05:59,028 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:05:59,028 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy4cpmmp1
2019-03-13 11:06:06,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:06:11,955 : Computing embeddings for train/dev/test
2019-03-13 11:09:16,125 : Computed embeddings
2019-03-13 11:09:16,126 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:09:56,745 : [('reg:1e-05', 79.88), ('reg:0.0001', 80.05), ('reg:0.001', 79.7), ('reg:0.01', 78.36)]
2019-03-13 11:09:56,745 : Validation : best param found is reg = 0.0001 with score             80.05
2019-03-13 11:09:56,746 : Evaluating...
2019-03-13 11:10:06,040 : 
Dev acc : 80.0 Test acc : 80.6 for OBJNUMBER classification

2019-03-13 11:10:06,041 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 11:10:06,622 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 11:10:06,690 : loading BERT model bert-large-uncased
2019-03-13 11:10:06,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:10:06,716 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:10:06,716 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdqg9wtec
2019-03-13 11:10:14,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:10:19,737 : Computing embeddings for train/dev/test
2019-03-13 11:13:53,239 : Computed embeddings
2019-03-13 11:13:53,239 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:14:21,314 : [('reg:1e-05', 63.67), ('reg:0.0001', 63.75), ('reg:0.001', 63.86), ('reg:0.01', 62.6)]
2019-03-13 11:14:21,314 : Validation : best param found is reg = 0.001 with score             63.86
2019-03-13 11:14:21,314 : Evaluating...
2019-03-13 11:14:28,687 : 
Dev acc : 63.9 Test acc : 63.4 for ODDMANOUT classification

2019-03-13 11:14:28,688 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 11:14:29,081 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 11:14:29,158 : loading BERT model bert-large-uncased
2019-03-13 11:14:29,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:14:29,283 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:14:29,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj2ycrf17
2019-03-13 11:14:36,778 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:14:42,186 : Computing embeddings for train/dev/test
2019-03-13 11:18:13,915 : Computed embeddings
2019-03-13 11:18:13,915 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:18:48,743 : [('reg:1e-05', 68.01), ('reg:0.0001', 68.0), ('reg:0.001', 67.05), ('reg:0.01', 61.45)]
2019-03-13 11:18:48,743 : Validation : best param found is reg = 1e-05 with score             68.01
2019-03-13 11:18:48,743 : Evaluating...
2019-03-13 11:18:56,995 : 
Dev acc : 68.0 Test acc : 67.7 for COORDINATIONINVERSION classification

2019-03-13 11:18:56,997 : total results: {'STS12': {'MSRpar': {'pearson': (0.29609047058351895, 1.2147804487768641e-16), 'spearman': SpearmanrResult(correlation=0.32703735802291983, pvalue=3.736408194571958e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.6119279757636799, 3.0037381292109904e-78), 'spearman': SpearmanrResult(correlation=0.6229599852119588, pvalue=7.825307093039781e-82), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5026094082823037, 9.38612964153324e-31), 'spearman': SpearmanrResult(correlation=0.5928917570119802, pvalue=6.536019398493962e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5422106692902876, 1.5275688748537464e-58), 'spearman': SpearmanrResult(correlation=0.5825741508824833, pvalue=2.2761834509112983e-69), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5904416126857427, 7.49970778888595e-39), 'spearman': SpearmanrResult(correlation=0.4960480467899544, pvalue=3.584319986212982e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.5086560273211066, 'wmean': 0.4999857653125172}, 'spearman': {'mean': 0.5243022595838592, 'wmean': 0.5210711414818894}}}, 'STS13': {'FNWN': {'pearson': (0.19372653799268802, 0.007562946988564937), 'spearman': SpearmanrResult(correlation=0.18401242528492673, pvalue=0.011255909973622717), 'nsamples': 189}, 'headlines': {'pearson': (0.6183174173929146, 2.6254571229219558e-80), 'spearman': SpearmanrResult(correlation=0.6144918250419115, pvalue=4.5430773266791284e-79), 'nsamples': 750}, 'OnWN': {'pearson': (0.5108416346189809, 1.289478617831798e-38), 'spearman': SpearmanrResult(correlation=0.5089123524555796, pvalue=2.721101331216629e-38), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.44096186333486115, 'wmean': 0.5246230238310349}, 'spearman': {'mean': 0.435805534260806, 'wmean': 0.5207646979252433}}}, 'STS14': {'deft-forum': {'pearson': (0.3467791902809567, 3.673212195924046e-14), 'spearman': SpearmanrResult(correlation=0.35210889733988465, pvalue=1.3991882152006393e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7336612577274526, 6.21052826087859e-52), 'spearman': SpearmanrResult(correlation=0.6889664791368257, pvalue=1.4739663510884662e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.5514036122659707, 7.033837664501656e-61), 'spearman': SpearmanrResult(correlation=0.5281477175998183, pvalue=4.187039384953339e-55), 'nsamples': 750}, 'images': {'pearson': (0.5712454856517167, 3.533149218696388e-66), 'spearman': SpearmanrResult(correlation=0.5678499788229552, pvalue=3.0241564793825577e-65), 'nsamples': 750}, 'OnWN': {'pearson': (0.6403184246513249, 8.998196868597264e-88), 'spearman': SpearmanrResult(correlation=0.668216521534316, pvalue=3.64162431988461e-98), 'nsamples': 750}, 'tweet-news': {'pearson': (0.572591398248752, 1.4981102793412084e-66), 'spearman': SpearmanrResult(correlation=0.5453779990464213, pvalue=2.4380603233674657e-59), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5693332281376956, 'wmean': 0.5674181876154638}, 'spearman': {'mean': 0.5584445989133702, 'wmean': 0.5592888294124344}}}, 'STS15': {'answers-forums': {'pearson': (0.5158587929188356, 6.922606022820404e-27), 'spearman': SpearmanrResult(correlation=0.48577308567673677, pvalue=1.3341306495764714e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.6715059370786837, 1.8228444102988746e-99), 'spearman': SpearmanrResult(correlation=0.6794129527398287, pvalue=1.1607407695392116e-102), 'nsamples': 750}, 'belief': {'pearson': (0.5494441713031771, 5.823181865716286e-31), 'spearman': SpearmanrResult(correlation=0.5787418733363725, pvalue=6.521108944204647e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.6054814213280583, 3.2162871172642506e-76), 'spearman': SpearmanrResult(correlation=0.6170667696889565, pvalue=6.696567463524065e-80), 'nsamples': 750}, 'images': {'pearson': (0.7162753659932236, 5.348564797015782e-119), 'spearman': SpearmanrResult(correlation=0.7259624843066066, pvalue=9.864735074560415e-124), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6117131377243957, 'wmean': 0.6314785516277429}, 'spearman': {'mean': 0.6173914331497002, 'wmean': 0.6386749215604867}}}, 'STS16': {'answer-answer': {'pearson': (0.5658951458234955, 6.630375479889998e-23), 'spearman': SpearmanrResult(correlation=0.5821837906749715, pvalue=1.918643725064817e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6273184902966806, 1.2080948778213547e-28), 'spearman': SpearmanrResult(correlation=0.6297594465096097, pvalue=6.432623560087669e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.706316255159535, 4.6207926771400285e-36), 'spearman': SpearmanrResult(correlation=0.7210445341230356, pvalue=3.420454731676519e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.7793970541487303, 4.7497133147231614e-51), 'spearman': SpearmanrResult(correlation=0.8164196658044751, pvalue=1.2159306590012405e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.26900510109635045, 8.199834474438375e-05), 'spearman': SpearmanrResult(correlation=0.28172118020061815, pvalue=3.590260278439884e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5895864093049583, 'wmean': 0.5976284630279611}, 'spearman': {'mean': 0.606225723462542, 'wmean': 0.6143432993076348}}}, 'MR': {'devacc': 66.85, 'acc': 66.07, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 73.82, 'acc': 67.61, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.13, 'acc': 84.98, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.58, 'acc': 91.34, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.11, 'acc': 80.67, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.33, 'acc': 39.28, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.92, 'acc': 73.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.01, 'acc': 72.58, 'f1': 79.15, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 80.2, 'acc': 77.43, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7750585307654541, 'pearson': 0.7856839333412707, 'spearman': 0.7251446127832097, 'mse': 0.3904801199423053, 'yhat': array([2.93675441, 4.4737032 , 1.13201871, ..., 3.02098931, 4.06121885,        4.85764716]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.650454470507307, 'pearson': 0.63459297776243, 'spearman': 0.6319223314091079, 'mse': 1.4587997690389265, 'yhat': array([1.47882636, 1.26169606, 2.50919002, ..., 3.58096767, 3.96959246,        3.3293978 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.06, 'acc': 65.28, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 63.17, 'acc': 62.54, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 11.0, 'acc': 11.32, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 34.4, 'acc': 34.57, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.95, 'acc': 62.24, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.59, 'acc': 90.51, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.06, 'acc': 85.38, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.05, 'acc': 82.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.05, 'acc': 80.61, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.86, 'acc': 63.38, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.01, 'acc': 67.66, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 11:18:56,997 : STS12 p=0.5000, STS12 s=0.5211, STS13 p=0.5246, STS13 s=0.5208, STS14 p=0.5674, STS14 s=0.5593, STS15 p=0.6315, STS15 s=0.6387, STS 16 p=0.5976, STS16 s=0.6143, STS B p=0.6346, STS B s=0.6319, STS B m=1.4588, SICK-R p=0.7857, SICK-R s=0.7251, SICK-P m=0.3905
2019-03-13 11:18:56,997 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 11:18:56,998 : 0.5000,0.5211,0.5246,0.5208,0.5674,0.5593,0.6315,0.6387,0.5976,0.6143,0.6346,0.6319,1.4588,0.7857,0.7251,0.3905
2019-03-13 11:18:56,998 : MR=66.07, CR=67.61, SUBJ=91.34, MPQA=84.98, SST-B=80.67, SST-F=39.28, TREC=73.20, SICK-E=77.43, SNLI=65.28, MRPC=72.58, MRPC f=79.15
2019-03-13 11:18:56,998 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 11:18:56,998 : 66.07,67.61,91.34,84.98,80.67,39.28,73.20,77.43,65.28,72.58,79.15
2019-03-13 11:18:56,998 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 11:18:56,998 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 11:18:56,998 : na,na,na,na,na,na,na,na,na,na
2019-03-13 11:18:56,998 : SentLen=62.54, WC=11.32, TreeDepth=34.57, TopConst=62.24, BShift=90.51, Tense=85.38, SubjNum=82.42, ObjNum=80.61, SOMO=63.38, CoordInv=67.66, average=64.06
2019-03-13 11:18:56,998 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 11:18:56,998 : 62.54,11.32,34.57,62.24,90.51,85.38,82.42,80.61,63.38,67.66,64.06
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:56,998 : layer 16
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:56,998 : ********************************************************************************
2019-03-13 11:18:57,085 : ***** Transfer task : STS12 *****


2019-03-13 11:18:57,098 : loading BERT model bert-large-uncased
2019-03-13 11:18:57,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:18:57,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:18:57,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcrt4pk64
2019-03-13 11:19:04,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:19:13,933 : MSRpar : pearson = 0.2972, spearman = 0.3297
2019-03-13 11:19:15,554 : MSRvid : pearson = 0.5700, spearman = 0.5824
2019-03-13 11:19:16,951 : SMTeuroparl : pearson = 0.5148, spearman = 0.6015
2019-03-13 11:19:19,620 : surprise.OnWN : pearson = 0.5184, spearman = 0.5629
2019-03-13 11:19:21,031 : surprise.SMTnews : pearson = 0.5982, spearman = 0.5157
2019-03-13 11:19:21,031 : ALL (weighted average) : Pearson = 0.4872,             Spearman = 0.5110
2019-03-13 11:19:21,031 : ALL (average) : Pearson = 0.4997,             Spearman = 0.5184

2019-03-13 11:19:21,031 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 11:19:21,039 : loading BERT model bert-large-uncased
2019-03-13 11:19:21,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:19:21,057 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:19:21,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz3agaleq
2019-03-13 11:19:28,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:19:35,252 : FNWN : pearson = 0.1969, spearman = 0.1972
2019-03-13 11:19:37,126 : headlines : pearson = 0.6151, spearman = 0.6091
2019-03-13 11:19:38,577 : OnWN : pearson = 0.5243, spearman = 0.5211
2019-03-13 11:19:38,577 : ALL (weighted average) : Pearson = 0.5284,             Spearman = 0.5243
2019-03-13 11:19:38,577 : ALL (average) : Pearson = 0.4454,             Spearman = 0.4425

2019-03-13 11:19:38,577 : ***** Transfer task : STS14 *****


2019-03-13 11:19:38,594 : loading BERT model bert-large-uncased
2019-03-13 11:19:38,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:19:38,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:19:38,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl55p9i01
2019-03-13 11:19:46,092 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:19:53,009 : deft-forum : pearson = 0.3469, spearman = 0.3466
2019-03-13 11:19:54,626 : deft-news : pearson = 0.7356, spearman = 0.6939
2019-03-13 11:19:56,770 : headlines : pearson = 0.5399, spearman = 0.5188
2019-03-13 11:19:58,825 : images : pearson = 0.5505, spearman = 0.5491
2019-03-13 11:20:00,937 : OnWN : pearson = 0.6359, spearman = 0.6610
2019-03-13 11:20:03,768 : tweet-news : pearson = 0.5632, spearman = 0.5376
2019-03-13 11:20:03,768 : ALL (weighted average) : Pearson = 0.5584,             Spearman = 0.5504
2019-03-13 11:20:03,768 : ALL (average) : Pearson = 0.5620,             Spearman = 0.5512

2019-03-13 11:20:03,768 : ***** Transfer task : STS15 *****


2019-03-13 11:20:03,801 : loading BERT model bert-large-uncased
2019-03-13 11:20:03,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:03,819 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:03,819 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2noi41cg
2019-03-13 11:20:11,317 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:18,788 : answers-forums : pearson = 0.5364, spearman = 0.5008
2019-03-13 11:20:20,851 : answers-students : pearson = 0.6657, spearman = 0.6748
2019-03-13 11:20:22,883 : belief : pearson = 0.5475, spearman = 0.5748
2019-03-13 11:20:25,117 : headlines : pearson = 0.6020, spearman = 0.6151
2019-03-13 11:20:27,232 : images : pearson = 0.6743, spearman = 0.6854
2019-03-13 11:20:27,232 : ALL (weighted average) : Pearson = 0.6210,             Spearman = 0.6283
2019-03-13 11:20:27,232 : ALL (average) : Pearson = 0.6052,             Spearman = 0.6102

2019-03-13 11:20:27,232 : ***** Transfer task : STS16 *****


2019-03-13 11:20:27,300 : loading BERT model bert-large-uncased
2019-03-13 11:20:27,300 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:27,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:27,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp75mole2c
2019-03-13 11:20:34,800 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:41,154 : answer-answer : pearson = 0.5513, spearman = 0.5648
2019-03-13 11:20:41,806 : headlines : pearson = 0.6275, spearman = 0.6294
2019-03-13 11:20:42,679 : plagiarism : pearson = 0.7028, spearman = 0.7166
2019-03-13 11:20:44,154 : postediting : pearson = 0.7755, spearman = 0.8061
2019-03-13 11:20:44,754 : question-question : pearson = 0.3313, spearman = 0.3339
2019-03-13 11:20:44,755 : ALL (weighted average) : Pearson = 0.6040,             Spearman = 0.6168
2019-03-13 11:20:44,755 : ALL (average) : Pearson = 0.5977,             Spearman = 0.6102

2019-03-13 11:20:44,755 : ***** Transfer task : MR *****


2019-03-13 11:20:44,774 : loading BERT model bert-large-uncased
2019-03-13 11:20:44,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:44,793 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:44,793 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo92t11vj
2019-03-13 11:20:52,288 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:20:57,847 : Generating sentence embeddings
2019-03-13 11:21:28,940 : Generated sentence embeddings
2019-03-13 11:21:28,941 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:21:40,693 : Best param found at split 1: l2reg = 0.01                 with score 70.43
2019-03-13 11:21:52,518 : Best param found at split 2: l2reg = 1e-05                 with score 70.6
2019-03-13 11:22:04,138 : Best param found at split 3: l2reg = 0.01                 with score 66.83
2019-03-13 11:22:13,655 : Best param found at split 4: l2reg = 1e-05                 with score 70.47
2019-03-13 11:22:23,841 : Best param found at split 5: l2reg = 0.001                 with score 69.2
2019-03-13 11:22:24,354 : Dev acc : 69.51 Test acc : 69.58

2019-03-13 11:22:24,355 : ***** Transfer task : CR *****


2019-03-13 11:22:24,363 : loading BERT model bert-large-uncased
2019-03-13 11:22:24,363 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:22:24,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:22:24,382 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl6p5_jy7
2019-03-13 11:22:31,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:22:37,231 : Generating sentence embeddings
2019-03-13 11:22:45,443 : Generated sentence embeddings
2019-03-13 11:22:45,443 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:22:48,416 : Best param found at split 1: l2reg = 0.001                 with score 71.88
2019-03-13 11:22:51,940 : Best param found at split 2: l2reg = 0.001                 with score 75.59
2019-03-13 11:22:55,703 : Best param found at split 3: l2reg = 0.001                 with score 79.37
2019-03-13 11:22:59,052 : Best param found at split 4: l2reg = 0.0001                 with score 72.2
2019-03-13 11:23:02,775 : Best param found at split 5: l2reg = 0.001                 with score 75.34
2019-03-13 11:23:02,967 : Dev acc : 74.88 Test acc : 67.37

2019-03-13 11:23:02,967 : ***** Transfer task : MPQA *****


2019-03-13 11:23:02,974 : loading BERT model bert-large-uncased
2019-03-13 11:23:02,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:23:03,025 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:23:03,025 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxijrsfh5
2019-03-13 11:23:10,452 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:23:15,939 : Generating sentence embeddings
2019-03-13 11:23:23,437 : Generated sentence embeddings
2019-03-13 11:23:23,438 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:23:35,403 : Best param found at split 1: l2reg = 0.001                 with score 83.3
2019-03-13 11:23:45,294 : Best param found at split 2: l2reg = 0.01                 with score 84.69
2019-03-13 11:23:57,069 : Best param found at split 3: l2reg = 0.001                 with score 83.97
2019-03-13 11:24:09,662 : Best param found at split 4: l2reg = 0.01                 with score 84.95
2019-03-13 11:24:21,370 : Best param found at split 5: l2reg = 0.0001                 with score 84.95
2019-03-13 11:24:21,813 : Dev acc : 84.37 Test acc : 83.89

2019-03-13 11:24:21,814 : ***** Transfer task : SUBJ *****


2019-03-13 11:24:21,828 : loading BERT model bert-large-uncased
2019-03-13 11:24:21,829 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:24:21,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:24:21,849 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd6l35jo2
2019-03-13 11:24:29,534 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:24:35,210 : Generating sentence embeddings
2019-03-13 11:25:05,680 : Generated sentence embeddings
2019-03-13 11:25:05,680 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:25:17,392 : Best param found at split 1: l2reg = 0.0001                 with score 92.91
2019-03-13 11:25:28,679 : Best param found at split 2: l2reg = 0.01                 with score 93.1
2019-03-13 11:25:39,889 : Best param found at split 3: l2reg = 1e-05                 with score 92.56
2019-03-13 11:25:51,171 : Best param found at split 4: l2reg = 0.001                 with score 92.92
2019-03-13 11:26:03,361 : Best param found at split 5: l2reg = 0.0001                 with score 92.82
2019-03-13 11:26:03,946 : Dev acc : 92.86 Test acc : 93.17

2019-03-13 11:26:03,947 : ***** Transfer task : SST Binary classification *****


2019-03-13 11:26:04,038 : loading BERT model bert-large-uncased
2019-03-13 11:26:04,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:26:04,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:26:04,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiqp14dpu
2019-03-13 11:26:11,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:26:17,111 : Computing embedding for train
2019-03-13 11:27:55,714 : Computed train embeddings
2019-03-13 11:27:55,714 : Computing embedding for dev
2019-03-13 11:27:57,864 : Computed dev embeddings
2019-03-13 11:27:57,864 : Computing embedding for test
2019-03-13 11:28:02,380 : Computed test embeddings
2019-03-13 11:28:02,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:28:22,231 : [('reg:1e-05', 82.8), ('reg:0.0001', 83.26), ('reg:0.001', 83.03), ('reg:0.01', 80.96)]
2019-03-13 11:28:22,232 : Validation : best param found is reg = 0.0001 with score             83.26
2019-03-13 11:28:22,232 : Evaluating...
2019-03-13 11:28:27,355 : 
Dev acc : 83.26 Test acc : 83.58 for             SST Binary classification

2019-03-13 11:28:27,355 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 11:28:27,406 : loading BERT model bert-large-uncased
2019-03-13 11:28:27,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:28:27,428 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:28:27,428 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8b8ccxcz
2019-03-13 11:28:34,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:28:40,345 : Computing embedding for train
2019-03-13 11:29:01,956 : Computed train embeddings
2019-03-13 11:29:01,956 : Computing embedding for dev
2019-03-13 11:29:04,777 : Computed dev embeddings
2019-03-13 11:29:04,777 : Computing embedding for test
2019-03-13 11:29:10,333 : Computed test embeddings
2019-03-13 11:29:10,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:29:12,480 : [('reg:1e-05', 32.15), ('reg:0.0001', 39.69), ('reg:0.001', 42.69), ('reg:0.01', 32.15)]
2019-03-13 11:29:12,480 : Validation : best param found is reg = 0.001 with score             42.69
2019-03-13 11:29:12,480 : Evaluating...
2019-03-13 11:29:13,082 : 
Dev acc : 42.69 Test acc : 41.4 for             SST Fine-Grained classification

2019-03-13 11:29:13,083 : ***** Transfer task : TREC *****


2019-03-13 11:29:13,096 : loading BERT model bert-large-uncased
2019-03-13 11:29:13,096 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:29:13,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:29:13,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpulzzdtsu
2019-03-13 11:29:20,586 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:29:33,500 : Computed train embeddings
2019-03-13 11:29:34,081 : Computed test embeddings
2019-03-13 11:29:34,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:29:41,909 : [('reg:1e-05', 64.82), ('reg:0.0001', 69.15), ('reg:0.001', 75.06), ('reg:0.01', 67.13)]
2019-03-13 11:29:41,909 : Cross-validation : best param found is reg = 0.001             with score 75.06
2019-03-13 11:29:41,909 : Evaluating...
2019-03-13 11:29:42,529 : 
Dev acc : 75.06 Test acc : 86.8             for TREC

2019-03-13 11:29:42,530 : ***** Transfer task : MRPC *****


2019-03-13 11:29:42,552 : loading BERT model bert-large-uncased
2019-03-13 11:29:42,552 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:29:42,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:29:42,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu3wyjfy2
2019-03-13 11:29:50,034 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:29:55,589 : Computing embedding for train
2019-03-13 11:30:17,552 : Computed train embeddings
2019-03-13 11:30:17,552 : Computing embedding for test
2019-03-13 11:30:27,171 : Computed test embeddings
2019-03-13 11:30:27,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:30:32,137 : [('reg:1e-05', 73.14), ('reg:0.0001', 71.27), ('reg:0.001', 71.32), ('reg:0.01', 72.89)]
2019-03-13 11:30:32,137 : Cross-validation : best param found is reg = 1e-05             with score 73.14
2019-03-13 11:30:32,137 : Evaluating...
2019-03-13 11:30:32,375 : Dev acc : 73.14 Test acc 71.07; Test F1 81.82 for MRPC.

2019-03-13 11:30:32,375 : ***** Transfer task : SICK-Entailment*****


2019-03-13 11:30:32,435 : loading BERT model bert-large-uncased
2019-03-13 11:30:32,435 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:30:32,455 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:30:32,455 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpff8zmmnw
2019-03-13 11:30:39,911 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:30:45,405 : Computing embedding for train
2019-03-13 11:30:56,551 : Computed train embeddings
2019-03-13 11:30:56,552 : Computing embedding for dev
2019-03-13 11:30:58,070 : Computed dev embeddings
2019-03-13 11:30:58,070 : Computing embedding for test
2019-03-13 11:31:10,024 : Computed test embeddings
2019-03-13 11:31:10,061 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:31:11,180 : [('reg:1e-05', 75.4), ('reg:0.0001', 78.0), ('reg:0.001', 72.0), ('reg:0.01', 77.8)]
2019-03-13 11:31:11,180 : Validation : best param found is reg = 0.0001 with score             78.0
2019-03-13 11:31:11,180 : Evaluating...
2019-03-13 11:31:11,420 : 
Dev acc : 78.0 Test acc : 76.84 for                        SICK entailment

2019-03-13 11:31:11,420 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 11:31:11,448 : loading BERT model bert-large-uncased
2019-03-13 11:31:11,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:31:11,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:31:11,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmwkaw6rz
2019-03-13 11:31:18,962 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:31:24,518 : Computing embedding for train
2019-03-13 11:31:35,679 : Computed train embeddings
2019-03-13 11:31:35,679 : Computing embedding for dev
2019-03-13 11:31:37,200 : Computed dev embeddings
2019-03-13 11:31:37,200 : Computing embedding for test
2019-03-13 11:31:49,216 : Computed test embeddings
2019-03-13 11:32:04,362 : Dev : Pearson 0.7793908408195486
2019-03-13 11:32:04,362 : Test : Pearson 0.7815898810680457 Spearman 0.7214730037634406 MSE 0.3971393050933147                        for SICK Relatedness

2019-03-13 11:32:04,363 : 

***** Transfer task : STSBenchmark*****


2019-03-13 11:32:04,402 : loading BERT model bert-large-uncased
2019-03-13 11:32:04,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:32:04,430 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:32:04,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0y7vwipz
2019-03-13 11:32:11,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:32:17,467 : Computing embedding for train
2019-03-13 11:32:35,822 : Computed train embeddings
2019-03-13 11:32:35,822 : Computing embedding for dev
2019-03-13 11:32:41,394 : Computed dev embeddings
2019-03-13 11:32:41,394 : Computing embedding for test
2019-03-13 11:32:45,948 : Computed test embeddings
2019-03-13 11:33:06,167 : Dev : Pearson 0.6553589559071555
2019-03-13 11:33:06,167 : Test : Pearson 0.6292569054262962 Spearman 0.6260663967985889 MSE 1.4691225103816796                        for SICK Relatedness

2019-03-13 11:33:06,167 : ***** Transfer task : SNLI Entailment*****


2019-03-13 11:33:11,199 : loading BERT model bert-large-uncased
2019-03-13 11:33:11,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:33:11,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:33:11,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppzc_hilg
2019-03-13 11:33:18,676 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:33:24,608 : PROGRESS (encoding): 0.00%
2019-03-13 11:36:08,191 : PROGRESS (encoding): 14.56%
2019-03-13 11:39:13,875 : PROGRESS (encoding): 29.12%
2019-03-13 11:42:20,305 : PROGRESS (encoding): 43.69%
2019-03-13 11:45:39,072 : PROGRESS (encoding): 58.25%
2019-03-13 11:49:20,435 : PROGRESS (encoding): 72.81%
2019-03-13 11:53:00,789 : PROGRESS (encoding): 87.37%
2019-03-13 11:56:59,052 : PROGRESS (encoding): 0.00%
2019-03-13 11:57:29,092 : PROGRESS (encoding): 0.00%
2019-03-13 11:57:57,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:58:52,321 : [('reg:1e-09', 66.06)]
2019-03-13 11:58:52,322 : Validation : best param found is reg = 1e-09 with score             66.06
2019-03-13 11:58:52,322 : Evaluating...
2019-03-13 11:59:46,046 : Dev acc : 66.06 Test acc : 66.86 for SNLI

2019-03-13 11:59:46,046 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 11:59:46,259 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 11:59:47,303 : loading BERT model bert-large-uncased
2019-03-13 11:59:47,304 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:59:47,330 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:59:47,330 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk5lm4_ro
2019-03-13 11:59:54,732 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:00:00,197 : Computing embeddings for train/dev/test
2019-03-13 12:03:28,104 : Computed embeddings
2019-03-13 12:03:28,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:03:56,827 : [('reg:1e-05', 62.81), ('reg:0.0001', 61.53), ('reg:0.001', 64.27), ('reg:0.01', 49.12)]
2019-03-13 12:03:56,827 : Validation : best param found is reg = 0.001 with score             64.27
2019-03-13 12:03:56,827 : Evaluating...
2019-03-13 12:04:04,573 : 
Dev acc : 64.3 Test acc : 64.5 for LENGTH classification

2019-03-13 12:04:04,574 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 12:04:04,829 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 12:04:04,878 : loading BERT model bert-large-uncased
2019-03-13 12:04:04,878 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:04:04,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:04:04,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyi90q5ki
2019-03-13 12:04:12,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:04:17,763 : Computing embeddings for train/dev/test
2019-03-13 12:07:29,650 : Computed embeddings
2019-03-13 12:07:29,650 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:08:06,553 : [('reg:1e-05', 20.91), ('reg:0.0001', 8.13), ('reg:0.001', 0.37), ('reg:0.01', 0.2)]
2019-03-13 12:08:06,554 : Validation : best param found is reg = 1e-05 with score             20.91
2019-03-13 12:08:06,554 : Evaluating...
2019-03-13 12:08:18,782 : 
Dev acc : 20.9 Test acc : 21.2 for WORDCONTENT classification

2019-03-13 12:08:18,783 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 12:08:19,313 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 12:08:19,379 : loading BERT model bert-large-uncased
2019-03-13 12:08:19,379 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:08:19,403 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:08:19,403 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx8kt9q3m
2019-03-13 12:08:26,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:08:32,417 : Computing embeddings for train/dev/test
2019-03-13 12:11:32,590 : Computed embeddings
2019-03-13 12:11:32,591 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:11:58,620 : [('reg:1e-05', 31.3), ('reg:0.0001', 27.92), ('reg:0.001', 26.1), ('reg:0.01', 28.08)]
2019-03-13 12:11:58,621 : Validation : best param found is reg = 1e-05 with score             31.3
2019-03-13 12:11:58,621 : Evaluating...
2019-03-13 12:12:06,342 : 
Dev acc : 31.3 Test acc : 31.4 for DEPTH classification

2019-03-13 12:12:06,343 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 12:12:06,710 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 12:12:06,772 : loading BERT model bert-large-uncased
2019-03-13 12:12:06,773 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:12:06,883 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:12:06,884 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp50j4zaja
2019-03-13 12:12:14,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:12:19,776 : Computing embeddings for train/dev/test
2019-03-13 12:15:06,998 : Computed embeddings
2019-03-13 12:15:06,998 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:15:40,625 : [('reg:1e-05', 58.62), ('reg:0.0001', 56.12), ('reg:0.001', 55.27), ('reg:0.01', 31.43)]
2019-03-13 12:15:40,626 : Validation : best param found is reg = 1e-05 with score             58.62
2019-03-13 12:15:40,626 : Evaluating...
2019-03-13 12:15:47,798 : 
Dev acc : 58.6 Test acc : 58.3 for TOPCONSTITUENTS classification

2019-03-13 12:15:47,799 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 12:15:48,175 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 12:15:48,241 : loading BERT model bert-large-uncased
2019-03-13 12:15:48,241 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:15:48,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:15:48,272 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv2ea4mo9
2019-03-13 12:15:55,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:16:01,184 : Computing embeddings for train/dev/test
2019-03-13 12:19:02,886 : Computed embeddings
2019-03-13 12:19:02,886 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:19:33,931 : [('reg:1e-05', 90.51), ('reg:0.0001', 90.5), ('reg:0.001', 90.47), ('reg:0.01', 90.19)]
2019-03-13 12:19:33,931 : Validation : best param found is reg = 1e-05 with score             90.51
2019-03-13 12:19:33,931 : Evaluating...
2019-03-13 12:19:41,630 : 
Dev acc : 90.5 Test acc : 90.4 for BIGRAMSHIFT classification

2019-03-13 12:19:41,631 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 12:19:42,020 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 12:19:42,085 : loading BERT model bert-large-uncased
2019-03-13 12:19:42,085 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:19:42,115 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:19:42,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpldxx5mlf
2019-03-13 12:19:49,595 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:19:55,135 : Computing embeddings for train/dev/test
2019-03-13 12:22:52,960 : Computed embeddings
2019-03-13 12:22:52,960 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:23:16,490 : [('reg:1e-05', 86.85), ('reg:0.0001', 86.84), ('reg:0.001', 86.8), ('reg:0.01', 86.71)]
2019-03-13 12:23:16,490 : Validation : best param found is reg = 1e-05 with score             86.85
2019-03-13 12:23:16,490 : Evaluating...
2019-03-13 12:23:22,938 : 
Dev acc : 86.8 Test acc : 86.1 for TENSE classification

2019-03-13 12:23:22,939 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 12:23:23,340 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 12:23:23,403 : loading BERT model bert-large-uncased
2019-03-13 12:23:23,403 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:23:23,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:23:23,519 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprwmcn7al
2019-03-13 12:23:30,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:23:36,467 : Computing embeddings for train/dev/test
2019-03-13 12:26:44,631 : Computed embeddings
2019-03-13 12:26:44,631 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:27:13,155 : [('reg:1e-05', 81.6), ('reg:0.0001', 76.99), ('reg:0.001', 77.04), ('reg:0.01', 80.95)]
2019-03-13 12:27:13,155 : Validation : best param found is reg = 1e-05 with score             81.6
2019-03-13 12:27:13,155 : Evaluating...
2019-03-13 12:27:18,946 : 
Dev acc : 81.6 Test acc : 81.5 for SUBJNUMBER classification

2019-03-13 12:27:18,947 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 12:27:19,351 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 12:27:19,418 : loading BERT model bert-large-uncased
2019-03-13 12:27:19,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:27:19,533 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:27:19,533 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpch4djbm0
2019-03-13 12:27:27,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:27:32,580 : Computing embeddings for train/dev/test
2019-03-13 12:30:37,526 : Computed embeddings
2019-03-13 12:30:37,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:31:08,647 : [('reg:1e-05', 79.44), ('reg:0.0001', 79.37), ('reg:0.001', 79.34), ('reg:0.01', 75.66)]
2019-03-13 12:31:08,647 : Validation : best param found is reg = 1e-05 with score             79.44
2019-03-13 12:31:08,647 : Evaluating...
2019-03-13 12:31:17,490 : 
Dev acc : 79.4 Test acc : 79.4 for OBJNUMBER classification

2019-03-13 12:31:17,491 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 12:31:18,054 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 12:31:18,123 : loading BERT model bert-large-uncased
2019-03-13 12:31:18,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:31:18,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:31:18,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphv5l477f
2019-03-13 12:31:25,666 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:31:31,237 : Computing embeddings for train/dev/test
2019-03-13 12:35:05,526 : Computed embeddings
2019-03-13 12:35:05,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:35:35,259 : [('reg:1e-05', 66.86), ('reg:0.0001', 66.77), ('reg:0.001', 66.02), ('reg:0.01', 64.19)]
2019-03-13 12:35:35,259 : Validation : best param found is reg = 1e-05 with score             66.86
2019-03-13 12:35:35,259 : Evaluating...
2019-03-13 12:35:42,719 : 
Dev acc : 66.9 Test acc : 66.1 for ODDMANOUT classification

2019-03-13 12:35:42,720 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 12:35:43,095 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 12:35:43,175 : loading BERT model bert-large-uncased
2019-03-13 12:35:43,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:35:43,205 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:35:43,205 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1faigtxc
2019-03-13 12:35:50,665 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:35:56,266 : Computing embeddings for train/dev/test
2019-03-13 12:39:28,227 : Computed embeddings
2019-03-13 12:39:28,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:39:53,890 : [('reg:1e-05', 69.4), ('reg:0.0001', 69.39), ('reg:0.001', 69.05), ('reg:0.01', 59.19)]
2019-03-13 12:39:53,891 : Validation : best param found is reg = 1e-05 with score             69.4
2019-03-13 12:39:53,891 : Evaluating...
2019-03-13 12:40:00,516 : 
Dev acc : 69.4 Test acc : 69.4 for COORDINATIONINVERSION classification

2019-03-13 12:40:00,518 : total results: {'STS12': {'MSRpar': {'pearson': (0.29723198050797883, 9.167473528489352e-17), 'spearman': SpearmanrResult(correlation=0.3296580665130065, pvalue=1.8019917904314677e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.570040558296995, 7.590858744209315e-66), 'spearman': SpearmanrResult(correlation=0.5824141535305171, pvalue=2.5301650144386757e-69), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5148005866726798, 2.0022777799488785e-32), 'spearman': SpearmanrResult(correlation=0.6015332885087096, pvalue=1.6477193925975567e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.518427080242247, 8.045224544032838e-53), 'spearman': SpearmanrResult(correlation=0.5629322230642714, pvalue=6.486493220847109e-64), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5982438738589735, 4.296991732169268e-40), 'spearman': SpearmanrResult(correlation=0.5156517384013249, pvalue=1.7368306520716058e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4997488159157749, 'wmean': 0.48721605187834816}, 'spearman': {'mean': 0.518437894003566, 'wmean': 0.510972379465403}}}, 'STS13': {'FNWN': {'pearson': (0.19688948735078032, 0.0066184082149720845), 'spearman': SpearmanrResult(correlation=0.19717939762110184, pvalue=0.006537335378622698), 'nsamples': 189}, 'headlines': {'pearson': (0.6150725267505223, 2.9546249120933016e-79), 'spearman': SpearmanrResult(correlation=0.6090651449003889, pvalue=2.42540122306869e-77), 'nsamples': 750}, 'OnWN': {'pearson': (0.5242982186679253, 6.159735125169646e-41), 'spearman': SpearmanrResult(correlation=0.5211172904646847, pvalue=2.226264934095518e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4454200775897426, 'wmean': 0.5284318725632635}, 'spearman': {'mean': 0.44245394432872515, 'wmean': 0.5242750431842453}}}, 'STS14': {'deft-forum': {'pearson': (0.34693378649257745, 3.5727216489179254e-14), 'spearman': SpearmanrResult(correlation=0.3466017814856052, pvalue=3.791947497904781e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7355737090069475, 2.494759635553277e-52), 'spearman': SpearmanrResult(correlation=0.6938953581705875, pvalue=2.090960365647595e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5399489453330171, 5.5964887993692876e-58), 'spearman': SpearmanrResult(correlation=0.5188312257960503, pvalue=6.487244505727716e-53), 'nsamples': 750}, 'images': {'pearson': (0.5504699584971243, 1.22417953483981e-60), 'spearman': SpearmanrResult(correlation=0.5491214102373707, pvalue=2.717068184109091e-60), 'nsamples': 750}, 'OnWN': {'pearson': (0.6358890114138211, 3.206152481364743e-86), 'spearman': SpearmanrResult(correlation=0.6609516872388452, pvalue=2.3729188648565032e-95), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5631741849459827, 5.58501240771223e-64), 'spearman': SpearmanrResult(correlation=0.5376176698480327, pvalue=2.1120512972117777e-57), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5619982659482451, 'wmean': 0.5583743711376541}, 'spearman': {'mean': 0.5511698554627485, 'wmean': 0.5504082410559794}}}, 'STS15': {'answers-forums': {'pearson': (0.536409242916123, 2.518548475891492e-29), 'spearman': SpearmanrResult(correlation=0.5007633186763963, pvalue=3.381848641919292e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.6656942680874539, 3.525927597953823e-97), 'spearman': SpearmanrResult(correlation=0.6748292595556048, pvalue=8.504097379885446e-101), 'nsamples': 750}, 'belief': {'pearson': (0.5475455819718033, 1.0184035929036872e-30), 'spearman': SpearmanrResult(correlation=0.5747642845197259, pvalue=2.366652861128588e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.6019740781491065, 3.9106769421461435e-75), 'spearman': SpearmanrResult(correlation=0.6151231384033351, pvalue=2.845764595018458e-79), 'nsamples': 750}, 'images': {'pearson': (0.6742529759494036, 1.4511219439886251e-100), 'spearman': SpearmanrResult(correlation=0.6854119542203498, pvalue=3.731643283574407e-105), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.6051752294147781, 'wmean': 0.6209746836574819}, 'spearman': {'mean': 0.6101783910750823, 'wmean': 0.6282820384443377}}}, 'STS16': {'answer-answer': {'pearson': (0.5512995308658045, 1.3482318419012949e-21), 'spearman': SpearmanrResult(correlation=0.5648337112357411, pvalue=8.295929144719464e-23), 'nsamples': 254}, 'headlines': {'pearson': (0.6275167239479251, 1.1480556237916005e-28), 'spearman': SpearmanrResult(correlation=0.6294368661143951, pvalue=6.993583722737082e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7027873668265096, 1.4312471749783145e-35), 'spearman': SpearmanrResult(correlation=0.7166413961143315, pvalue=1.5319560824179412e-37), 'nsamples': 230}, 'postediting': {'pearson': (0.7754650353386358, 3.097261437438567e-50), 'spearman': SpearmanrResult(correlation=0.8060565094069311, pvalue=4.736477984744907e-57), 'nsamples': 244}, 'question-question': {'pearson': (0.3313244575157821, 9.569360201628325e-07), 'spearman': SpearmanrResult(correlation=0.33390820701627044, pvalue=7.778348084277411e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5976786228989314, 'wmean': 0.6040329845838706}, 'spearman': {'mean': 0.6101753379775339, 'wmean': 0.616770376883938}}}, 'MR': {'devacc': 69.51, 'acc': 69.58, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 74.88, 'acc': 67.37, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.37, 'acc': 83.89, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.86, 'acc': 93.17, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.26, 'acc': 83.58, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.69, 'acc': 41.4, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.06, 'acc': 86.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.14, 'acc': 71.07, 'f1': 81.82, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 76.84, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7793908408195486, 'pearson': 0.7815898810680457, 'spearman': 0.7214730037634406, 'mse': 0.3971393050933147, 'yhat': array([2.95512435, 4.45061759, 1.03900804, ..., 3.02569715, 3.95628255,        4.13478854]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6553589559071555, 'pearson': 0.6292569054262962, 'spearman': 0.6260663967985889, 'mse': 1.4691225103816796, 'yhat': array([1.75671347, 1.35270014, 2.28290699, ..., 3.33767087, 3.984619  ,        3.37680906]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.06, 'acc': 66.86, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 64.27, 'acc': 64.5, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 20.91, 'acc': 21.17, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.3, 'acc': 31.38, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.62, 'acc': 58.32, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.51, 'acc': 90.37, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.85, 'acc': 86.14, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.6, 'acc': 81.45, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.44, 'acc': 79.4, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.86, 'acc': 66.14, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.4, 'acc': 69.42, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 12:40:00,518 : STS12 p=0.4872, STS12 s=0.5110, STS13 p=0.5284, STS13 s=0.5243, STS14 p=0.5584, STS14 s=0.5504, STS15 p=0.6210, STS15 s=0.6283, STS 16 p=0.6040, STS16 s=0.6168, STS B p=0.6293, STS B s=0.6261, STS B m=1.4691, SICK-R p=0.7816, SICK-R s=0.7215, SICK-P m=0.3971
2019-03-13 12:40:00,518 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 12:40:00,518 : 0.4872,0.5110,0.5284,0.5243,0.5584,0.5504,0.6210,0.6283,0.6040,0.6168,0.6293,0.6261,1.4691,0.7816,0.7215,0.3971
2019-03-13 12:40:00,518 : MR=69.58, CR=67.37, SUBJ=93.17, MPQA=83.89, SST-B=83.58, SST-F=41.40, TREC=86.80, SICK-E=76.84, SNLI=66.86, MRPC=71.07, MRPC f=81.82
2019-03-13 12:40:00,518 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 12:40:00,518 : 69.58,67.37,93.17,83.89,83.58,41.40,86.80,76.84,66.86,71.07,81.82
2019-03-13 12:40:00,518 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 12:40:00,518 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 12:40:00,518 : na,na,na,na,na,na,na,na,na,na
2019-03-13 12:40:00,518 : SentLen=64.50, WC=21.17, TreeDepth=31.38, TopConst=58.32, BShift=90.37, Tense=86.14, SubjNum=81.45, ObjNum=79.40, SOMO=66.14, CoordInv=69.42, average=64.83
2019-03-13 12:40:00,519 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 12:40:00,519 : 64.50,21.17,31.38,58.32,90.37,86.14,81.45,79.40,66.14,69.42,64.83
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,519 : layer 17
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,519 : ********************************************************************************
2019-03-13 12:40:00,615 : ***** Transfer task : STS12 *****


2019-03-13 12:40:00,627 : loading BERT model bert-large-uncased
2019-03-13 12:40:00,628 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:00,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:00,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpryrtufzd
2019-03-13 12:40:08,085 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:17,577 : MSRpar : pearson = 0.2963, spearman = 0.3265
2019-03-13 12:40:19,206 : MSRvid : pearson = 0.5517, spearman = 0.5654
2019-03-13 12:40:20,610 : SMTeuroparl : pearson = 0.5052, spearman = 0.5886
2019-03-13 12:40:23,287 : surprise.OnWN : pearson = 0.5150, spearman = 0.5575
2019-03-13 12:40:24,705 : surprise.SMTnews : pearson = 0.5943, spearman = 0.5177
2019-03-13 12:40:24,705 : ALL (weighted average) : Pearson = 0.4799,             Spearman = 0.5032
2019-03-13 12:40:24,705 : ALL (average) : Pearson = 0.4925,             Spearman = 0.5112

2019-03-13 12:40:24,705 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 12:40:24,714 : loading BERT model bert-large-uncased
2019-03-13 12:40:24,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:24,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:24,732 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx9wp8pc8
2019-03-13 12:40:32,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:38,868 : FNWN : pearson = 0.1963, spearman = 0.1984
2019-03-13 12:40:40,747 : headlines : pearson = 0.5991, spearman = 0.5943
2019-03-13 12:40:42,204 : OnWN : pearson = 0.5223, spearman = 0.5198
2019-03-13 12:40:42,205 : ALL (weighted average) : Pearson = 0.5196,             Spearman = 0.5165
2019-03-13 12:40:42,205 : ALL (average) : Pearson = 0.4392,             Spearman = 0.4375

2019-03-13 12:40:42,205 : ***** Transfer task : STS14 *****


2019-03-13 12:40:42,220 : loading BERT model bert-large-uncased
2019-03-13 12:40:42,220 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:40:42,237 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:40:42,237 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3wr_80lp
2019-03-13 12:40:49,736 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:40:56,541 : deft-forum : pearson = 0.3492, spearman = 0.3503
2019-03-13 12:40:58,167 : deft-news : pearson = 0.7385, spearman = 0.6993
2019-03-13 12:41:00,320 : headlines : pearson = 0.5255, spearman = 0.5030
2019-03-13 12:41:02,382 : images : pearson = 0.5047, spearman = 0.5049
2019-03-13 12:41:04,498 : OnWN : pearson = 0.6406, spearman = 0.6687
2019-03-13 12:41:07,334 : tweet-news : pearson = 0.5566, spearman = 0.5262
2019-03-13 12:41:07,334 : ALL (weighted average) : Pearson = 0.5465,             Spearman = 0.5385
2019-03-13 12:41:07,335 : ALL (average) : Pearson = 0.5525,             Spearman = 0.5421

2019-03-13 12:41:07,335 : ***** Transfer task : STS15 *****


2019-03-13 12:41:07,383 : loading BERT model bert-large-uncased
2019-03-13 12:41:07,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:41:07,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:41:07,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcvwclqw4
2019-03-13 12:41:14,826 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:41:22,046 : answers-forums : pearson = 0.5462, spearman = 0.5096
2019-03-13 12:41:24,114 : answers-students : pearson = 0.6674, spearman = 0.6784
2019-03-13 12:41:26,143 : belief : pearson = 0.5501, spearman = 0.5741
2019-03-13 12:41:28,374 : headlines : pearson = 0.5911, spearman = 0.6055
2019-03-13 12:41:30,489 : images : pearson = 0.6398, spearman = 0.6563
2019-03-13 12:41:30,489 : ALL (weighted average) : Pearson = 0.6116,             Spearman = 0.6205
2019-03-13 12:41:30,489 : ALL (average) : Pearson = 0.5989,             Spearman = 0.6048

2019-03-13 12:41:30,489 : ***** Transfer task : STS16 *****


2019-03-13 12:41:30,558 : loading BERT model bert-large-uncased
2019-03-13 12:41:30,558 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:41:30,576 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:41:30,576 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpot2dlh4x
2019-03-13 12:41:38,022 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:41:44,383 : answer-answer : pearson = 0.5396, spearman = 0.5554
2019-03-13 12:41:45,043 : headlines : pearson = 0.6288, spearman = 0.6319
2019-03-13 12:41:45,916 : plagiarism : pearson = 0.7016, spearman = 0.7099
2019-03-13 12:41:47,399 : postediting : pearson = 0.7666, spearman = 0.7994
2019-03-13 12:41:48,001 : question-question : pearson = 0.2977, spearman = 0.3129
2019-03-13 12:41:48,001 : ALL (weighted average) : Pearson = 0.5938,             Spearman = 0.6089
2019-03-13 12:41:48,001 : ALL (average) : Pearson = 0.5869,             Spearman = 0.6019

2019-03-13 12:41:48,001 : ***** Transfer task : MR *****


2019-03-13 12:41:48,016 : loading BERT model bert-large-uncased
2019-03-13 12:41:48,016 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:41:48,036 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:41:48,037 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp70zn4k3f
2019-03-13 12:41:55,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:42:01,064 : Generating sentence embeddings
2019-03-13 12:42:32,243 : Generated sentence embeddings
2019-03-13 12:42:32,244 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:42:44,424 : Best param found at split 1: l2reg = 0.0001                 with score 71.62
2019-03-13 12:42:58,472 : Best param found at split 2: l2reg = 1e-05                 with score 74.91
2019-03-13 12:43:13,932 : Best param found at split 3: l2reg = 1e-05                 with score 72.7
2019-03-13 12:43:26,521 : Best param found at split 4: l2reg = 1e-05                 with score 75.18
2019-03-13 12:43:41,550 : Best param found at split 5: l2reg = 0.0001                 with score 75.02
2019-03-13 12:43:42,259 : Dev acc : 73.89 Test acc : 72.98

2019-03-13 12:43:42,260 : ***** Transfer task : CR *****


2019-03-13 12:43:42,268 : loading BERT model bert-large-uncased
2019-03-13 12:43:42,268 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:43:42,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:43:42,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp49dadmdh
2019-03-13 12:43:49,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:43:55,826 : Generating sentence embeddings
2019-03-13 12:44:04,056 : Generated sentence embeddings
2019-03-13 12:44:04,057 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:44:07,529 : Best param found at split 1: l2reg = 0.0001                 with score 78.8
2019-03-13 12:44:11,282 : Best param found at split 2: l2reg = 1e-05                 with score 79.0
2019-03-13 12:44:14,677 : Best param found at split 3: l2reg = 0.001                 with score 75.3
2019-03-13 12:44:18,353 : Best param found at split 4: l2reg = 0.0001                 with score 79.94
2019-03-13 12:44:22,350 : Best param found at split 5: l2reg = 0.001                 with score 81.26
2019-03-13 12:44:22,577 : Dev acc : 78.86 Test acc : 78.39

2019-03-13 12:44:22,577 : ***** Transfer task : MPQA *****


2019-03-13 12:44:22,583 : loading BERT model bert-large-uncased
2019-03-13 12:44:22,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:44:22,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:44:22,634 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf0yhcg8e
2019-03-13 12:44:30,091 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:44:35,545 : Generating sentence embeddings
2019-03-13 12:44:43,017 : Generated sentence embeddings
2019-03-13 12:44:43,017 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:44:53,533 : Best param found at split 1: l2reg = 0.0001                 with score 85.79
2019-03-13 12:45:04,801 : Best param found at split 2: l2reg = 0.0001                 with score 85.76
2019-03-13 12:45:16,540 : Best param found at split 3: l2reg = 0.01                 with score 85.36
2019-03-13 12:45:27,725 : Best param found at split 4: l2reg = 1e-05                 with score 86.78
2019-03-13 12:45:39,751 : Best param found at split 5: l2reg = 1e-05                 with score 84.6
2019-03-13 12:45:40,469 : Dev acc : 85.66 Test acc : 85.66

2019-03-13 12:45:40,470 : ***** Transfer task : SUBJ *****


2019-03-13 12:45:40,487 : loading BERT model bert-large-uncased
2019-03-13 12:45:40,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:45:40,506 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:45:40,507 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe41yrnhk
2019-03-13 12:45:47,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:45:53,578 : Generating sentence embeddings
2019-03-13 12:46:24,124 : Generated sentence embeddings
2019-03-13 12:46:24,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 12:46:33,899 : Best param found at split 1: l2reg = 0.0001                 with score 93.29
2019-03-13 12:46:45,873 : Best param found at split 2: l2reg = 0.01                 with score 93.6
2019-03-13 12:46:54,319 : Best param found at split 3: l2reg = 0.01                 with score 92.89
2019-03-13 12:47:02,278 : Best param found at split 4: l2reg = 1e-05                 with score 93.26
2019-03-13 12:47:14,345 : Best param found at split 5: l2reg = 1e-05                 with score 93.14
2019-03-13 12:47:14,777 : Dev acc : 93.24 Test acc : 91.46

2019-03-13 12:47:14,778 : ***** Transfer task : SST Binary classification *****


2019-03-13 12:47:14,870 : loading BERT model bert-large-uncased
2019-03-13 12:47:14,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:47:14,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:47:14,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_20p05m4
2019-03-13 12:47:22,414 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:47:27,916 : Computing embedding for train
2019-03-13 12:49:06,788 : Computed train embeddings
2019-03-13 12:49:06,788 : Computing embedding for dev
2019-03-13 12:49:08,943 : Computed dev embeddings
2019-03-13 12:49:08,943 : Computing embedding for test
2019-03-13 12:49:13,464 : Computed test embeddings
2019-03-13 12:49:13,465 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:49:32,593 : [('reg:1e-05', 84.86), ('reg:0.0001', 84.86), ('reg:0.001', 83.72), ('reg:0.01', 83.94)]
2019-03-13 12:49:32,593 : Validation : best param found is reg = 1e-05 with score             84.86
2019-03-13 12:49:32,593 : Evaluating...
2019-03-13 12:49:37,045 : 
Dev acc : 84.86 Test acc : 85.17 for             SST Binary classification

2019-03-13 12:49:37,045 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 12:49:37,100 : loading BERT model bert-large-uncased
2019-03-13 12:49:37,100 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:49:37,120 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:49:37,120 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3gyfppvr
2019-03-13 12:49:44,524 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:49:50,009 : Computing embedding for train
2019-03-13 12:50:11,653 : Computed train embeddings
2019-03-13 12:50:11,653 : Computing embedding for dev
2019-03-13 12:50:14,481 : Computed dev embeddings
2019-03-13 12:50:14,481 : Computing embedding for test
2019-03-13 12:50:20,054 : Computed test embeddings
2019-03-13 12:50:20,054 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:50:22,380 : [('reg:1e-05', 37.06), ('reg:0.0001', 38.06), ('reg:0.001', 31.24), ('reg:0.01', 37.87)]
2019-03-13 12:50:22,381 : Validation : best param found is reg = 0.0001 with score             38.06
2019-03-13 12:50:22,381 : Evaluating...
2019-03-13 12:50:22,948 : 
Dev acc : 38.06 Test acc : 40.0 for             SST Fine-Grained classification

2019-03-13 12:50:22,949 : ***** Transfer task : TREC *****


2019-03-13 12:50:22,963 : loading BERT model bert-large-uncased
2019-03-13 12:50:22,963 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:50:22,984 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:50:22,984 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmporalg9q0
2019-03-13 12:50:30,383 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:50:43,535 : Computed train embeddings
2019-03-13 12:50:44,116 : Computed test embeddings
2019-03-13 12:50:44,117 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:50:51,904 : [('reg:1e-05', 74.3), ('reg:0.0001', 66.01), ('reg:0.001', 73.11), ('reg:0.01', 67.54)]
2019-03-13 12:50:51,904 : Cross-validation : best param found is reg = 1e-05             with score 74.3
2019-03-13 12:50:51,905 : Evaluating...
2019-03-13 12:50:52,299 : 
Dev acc : 74.3 Test acc : 92.6             for TREC

2019-03-13 12:50:52,300 : ***** Transfer task : MRPC *****


2019-03-13 12:50:52,320 : loading BERT model bert-large-uncased
2019-03-13 12:50:52,320 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:50:52,341 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:50:52,342 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptpdx56_0
2019-03-13 12:50:59,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:51:05,372 : Computing embedding for train
2019-03-13 12:51:27,381 : Computed train embeddings
2019-03-13 12:51:27,381 : Computing embedding for test
2019-03-13 12:51:37,015 : Computed test embeddings
2019-03-13 12:51:37,036 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 12:51:41,358 : [('reg:1e-05', 71.42), ('reg:0.0001', 72.79), ('reg:0.001', 72.94), ('reg:0.01', 71.44)]
2019-03-13 12:51:41,358 : Cross-validation : best param found is reg = 0.001             with score 72.94
2019-03-13 12:51:41,359 : Evaluating...
2019-03-13 12:51:41,723 : Dev acc : 72.94 Test acc 74.14; Test F1 81.62 for MRPC.

2019-03-13 12:51:41,724 : ***** Transfer task : SICK-Entailment*****


2019-03-13 12:51:41,786 : loading BERT model bert-large-uncased
2019-03-13 12:51:41,786 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:51:41,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:51:41,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqsce8zjn
2019-03-13 12:51:49,281 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:51:54,810 : Computing embedding for train
2019-03-13 12:52:05,999 : Computed train embeddings
2019-03-13 12:52:05,999 : Computing embedding for dev
2019-03-13 12:52:07,524 : Computed dev embeddings
2019-03-13 12:52:07,525 : Computing embedding for test
2019-03-13 12:52:19,496 : Computed test embeddings
2019-03-13 12:52:19,533 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:52:21,312 : [('reg:1e-05', 77.0), ('reg:0.0001', 78.2), ('reg:0.001', 75.0), ('reg:0.01', 78.0)]
2019-03-13 12:52:21,312 : Validation : best param found is reg = 0.0001 with score             78.2
2019-03-13 12:52:21,312 : Evaluating...
2019-03-13 12:52:21,679 : 
Dev acc : 78.2 Test acc : 77.76 for                        SICK entailment

2019-03-13 12:52:21,680 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 12:52:21,706 : loading BERT model bert-large-uncased
2019-03-13 12:52:21,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:52:21,761 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:52:21,761 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfex4jvaa
2019-03-13 12:52:29,242 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:52:34,800 : Computing embedding for train
2019-03-13 12:52:45,961 : Computed train embeddings
2019-03-13 12:52:45,961 : Computing embedding for dev
2019-03-13 12:52:47,486 : Computed dev embeddings
2019-03-13 12:52:47,487 : Computing embedding for test
2019-03-13 12:52:59,467 : Computed test embeddings
2019-03-13 12:53:14,523 : Dev : Pearson 0.7787692944492686
2019-03-13 12:53:14,523 : Test : Pearson 0.7807017907400123 Spearman 0.7171486253854353 MSE 0.39751808783835024                        for SICK Relatedness

2019-03-13 12:53:14,524 : 

***** Transfer task : STSBenchmark*****


2019-03-13 12:53:14,592 : loading BERT model bert-large-uncased
2019-03-13 12:53:14,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:53:14,611 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:53:14,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw2yi78zu
2019-03-13 12:53:22,042 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:53:27,637 : Computing embedding for train
2019-03-13 12:53:46,033 : Computed train embeddings
2019-03-13 12:53:46,033 : Computing embedding for dev
2019-03-13 12:53:51,606 : Computed dev embeddings
2019-03-13 12:53:51,606 : Computing embedding for test
2019-03-13 12:53:56,154 : Computed test embeddings
2019-03-13 12:54:15,125 : Dev : Pearson 0.6324647291727744
2019-03-13 12:54:15,125 : Test : Pearson 0.6203636548653669 Spearman 0.6147934401135875 MSE 1.5454934247704615                        for SICK Relatedness

2019-03-13 12:54:15,126 : ***** Transfer task : SNLI Entailment*****


2019-03-13 12:54:19,750 : loading BERT model bert-large-uncased
2019-03-13 12:54:19,750 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:54:20,730 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:54:20,730 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7bpzd2j6
2019-03-13 12:54:28,133 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:54:34,113 : PROGRESS (encoding): 0.00%
2019-03-13 12:57:17,756 : PROGRESS (encoding): 14.56%
2019-03-13 13:00:23,487 : PROGRESS (encoding): 29.12%
2019-03-13 13:03:30,032 : PROGRESS (encoding): 43.69%
2019-03-13 13:06:48,875 : PROGRESS (encoding): 58.25%
2019-03-13 13:10:30,267 : PROGRESS (encoding): 72.81%
2019-03-13 13:14:10,554 : PROGRESS (encoding): 87.37%
2019-03-13 13:18:08,929 : PROGRESS (encoding): 0.00%
2019-03-13 13:18:38,946 : PROGRESS (encoding): 0.00%
2019-03-13 13:19:07,830 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:20:09,516 : [('reg:1e-09', 69.21)]
2019-03-13 13:20:09,516 : Validation : best param found is reg = 1e-09 with score             69.21
2019-03-13 13:20:09,516 : Evaluating...
2019-03-13 13:21:12,167 : Dev acc : 69.21 Test acc : 69.2 for SNLI

2019-03-13 13:21:12,167 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 13:21:12,379 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 13:21:13,291 : loading BERT model bert-large-uncased
2019-03-13 13:21:13,291 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:21:13,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:21:13,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7hoi54yx
2019-03-13 13:21:20,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:21:26,347 : Computing embeddings for train/dev/test
2019-03-13 13:24:54,475 : Computed embeddings
2019-03-13 13:24:54,475 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:25:26,255 : [('reg:1e-05', 53.8), ('reg:0.0001', 64.61), ('reg:0.001', 55.85), ('reg:0.01', 54.19)]
2019-03-13 13:25:26,256 : Validation : best param found is reg = 0.0001 with score             64.61
2019-03-13 13:25:26,256 : Evaluating...
2019-03-13 13:25:35,742 : 
Dev acc : 64.6 Test acc : 63.9 for LENGTH classification

2019-03-13 13:25:35,743 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 13:25:36,112 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 13:25:36,160 : loading BERT model bert-large-uncased
2019-03-13 13:25:36,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:25:36,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:25:36,188 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj74hbiqz
2019-03-13 13:25:43,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:25:49,242 : Computing embeddings for train/dev/test
2019-03-13 13:29:01,198 : Computed embeddings
2019-03-13 13:29:01,198 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:29:36,151 : [('reg:1e-05', 15.72), ('reg:0.0001', 7.87), ('reg:0.001', 0.58), ('reg:0.01', 0.1)]
2019-03-13 13:29:36,151 : Validation : best param found is reg = 1e-05 with score             15.72
2019-03-13 13:29:36,152 : Evaluating...
2019-03-13 13:29:46,064 : 
Dev acc : 15.7 Test acc : 15.8 for WORDCONTENT classification

2019-03-13 13:29:46,065 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 13:29:46,425 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 13:29:46,491 : loading BERT model bert-large-uncased
2019-03-13 13:29:46,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:29:46,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:29:46,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfi6xicaz
2019-03-13 13:29:53,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:29:59,521 : Computing embeddings for train/dev/test
2019-03-13 13:32:59,932 : Computed embeddings
2019-03-13 13:32:59,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:33:26,638 : [('reg:1e-05', 31.99), ('reg:0.0001', 25.98), ('reg:0.001', 25.78), ('reg:0.01', 27.82)]
2019-03-13 13:33:26,638 : Validation : best param found is reg = 1e-05 with score             31.99
2019-03-13 13:33:26,638 : Evaluating...
2019-03-13 13:33:33,871 : 
Dev acc : 32.0 Test acc : 32.0 for DEPTH classification

2019-03-13 13:33:33,872 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 13:33:34,253 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 13:33:34,316 : loading BERT model bert-large-uncased
2019-03-13 13:33:34,316 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:33:34,424 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:33:34,424 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaqovzcl1
2019-03-13 13:33:41,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:33:47,366 : Computing embeddings for train/dev/test
2019-03-13 13:36:34,509 : Computed embeddings
2019-03-13 13:36:34,509 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:37:06,897 : [('reg:1e-05', 55.12), ('reg:0.0001', 59.32), ('reg:0.001', 49.93), ('reg:0.01', 36.09)]
2019-03-13 13:37:06,897 : Validation : best param found is reg = 0.0001 with score             59.32
2019-03-13 13:37:06,897 : Evaluating...
2019-03-13 13:37:15,842 : 
Dev acc : 59.3 Test acc : 59.8 for TOPCONSTITUENTS classification

2019-03-13 13:37:15,843 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 13:37:16,181 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 13:37:16,247 : loading BERT model bert-large-uncased
2019-03-13 13:37:16,247 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:37:16,364 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:37:16,364 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1sjmrpxt
2019-03-13 13:37:23,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:37:29,339 : Computing embeddings for train/dev/test
2019-03-13 13:40:31,154 : Computed embeddings
2019-03-13 13:40:31,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:40:51,207 : [('reg:1e-05', 90.2), ('reg:0.0001', 89.99), ('reg:0.001', 90.12), ('reg:0.01', 89.45)]
2019-03-13 13:40:51,208 : Validation : best param found is reg = 1e-05 with score             90.2
2019-03-13 13:40:51,208 : Evaluating...
2019-03-13 13:40:56,374 : 
Dev acc : 90.2 Test acc : 90.3 for BIGRAMSHIFT classification

2019-03-13 13:40:56,375 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 13:40:56,794 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 13:40:56,861 : loading BERT model bert-large-uncased
2019-03-13 13:40:56,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:40:56,893 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:40:56,893 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3xwskjgs
2019-03-13 13:41:04,419 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:41:10,077 : Computing embeddings for train/dev/test
2019-03-13 13:44:08,007 : Computed embeddings
2019-03-13 13:44:08,007 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:44:32,110 : [('reg:1e-05', 88.56), ('reg:0.0001', 88.56), ('reg:0.001', 88.4), ('reg:0.01', 88.76)]
2019-03-13 13:44:32,110 : Validation : best param found is reg = 0.01 with score             88.76
2019-03-13 13:44:32,110 : Evaluating...
2019-03-13 13:44:37,836 : 
Dev acc : 88.8 Test acc : 87.0 for TENSE classification

2019-03-13 13:44:37,837 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 13:44:38,254 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 13:44:38,318 : loading BERT model bert-large-uncased
2019-03-13 13:44:38,318 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:44:38,347 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:44:38,347 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1rltyts9
2019-03-13 13:44:45,848 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:44:51,292 : Computing embeddings for train/dev/test
2019-03-13 13:47:59,654 : Computed embeddings
2019-03-13 13:47:59,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:48:31,644 : [('reg:1e-05', 82.51), ('reg:0.0001', 82.48), ('reg:0.001', 82.5), ('reg:0.01', 80.2)]
2019-03-13 13:48:31,644 : Validation : best param found is reg = 1e-05 with score             82.51
2019-03-13 13:48:31,644 : Evaluating...
2019-03-13 13:48:36,368 : 
Dev acc : 82.5 Test acc : 81.3 for SUBJNUMBER classification

2019-03-13 13:48:36,369 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 13:48:36,772 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 13:48:36,839 : loading BERT model bert-large-uncased
2019-03-13 13:48:36,839 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:48:36,952 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:48:36,952 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbjg1ru5x
2019-03-13 13:48:44,372 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:48:49,874 : Computing embeddings for train/dev/test
2019-03-13 13:51:54,678 : Computed embeddings
2019-03-13 13:51:54,679 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:52:27,300 : [('reg:1e-05', 79.19), ('reg:0.0001', 79.32), ('reg:0.001', 79.5), ('reg:0.01', 73.75)]
2019-03-13 13:52:27,300 : Validation : best param found is reg = 0.001 with score             79.5
2019-03-13 13:52:27,300 : Evaluating...
2019-03-13 13:52:34,716 : 
Dev acc : 79.5 Test acc : 80.2 for OBJNUMBER classification

2019-03-13 13:52:34,717 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 13:52:35,098 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 13:52:35,166 : loading BERT model bert-large-uncased
2019-03-13 13:52:35,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:52:35,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:52:35,287 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjd3c5ngd
2019-03-13 13:52:42,743 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:52:48,199 : Computing embeddings for train/dev/test
2019-03-13 13:56:22,181 : Computed embeddings
2019-03-13 13:56:22,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:56:51,880 : [('reg:1e-05', 65.5), ('reg:0.0001', 65.41), ('reg:0.001', 65.08), ('reg:0.01', 61.96)]
2019-03-13 13:56:51,881 : Validation : best param found is reg = 1e-05 with score             65.5
2019-03-13 13:56:51,881 : Evaluating...
2019-03-13 13:56:59,652 : 
Dev acc : 65.5 Test acc : 65.2 for ODDMANOUT classification

2019-03-13 13:56:59,653 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 13:57:00,244 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 13:57:00,320 : loading BERT model bert-large-uncased
2019-03-13 13:57:00,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:57:00,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:57:00,352 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsk51pql1
2019-03-13 13:57:07,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:57:13,497 : Computing embeddings for train/dev/test
2019-03-13 14:00:45,755 : Computed embeddings
2019-03-13 14:00:45,755 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:01:18,710 : [('reg:1e-05', 72.0), ('reg:0.0001', 71.92), ('reg:0.001', 71.86), ('reg:0.01', 70.49)]
2019-03-13 14:01:18,710 : Validation : best param found is reg = 1e-05 with score             72.0
2019-03-13 14:01:18,710 : Evaluating...
2019-03-13 14:01:26,457 : 
Dev acc : 72.0 Test acc : 72.0 for COORDINATIONINVERSION classification

2019-03-13 14:01:26,460 : total results: {'STS12': {'MSRpar': {'pearson': (0.2963348257163791, 1.1438657468676276e-16), 'spearman': SpearmanrResult(correlation=0.3265331915682161, pvalue=4.295640904091064e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5517429969296411, 5.748124659150887e-61), 'spearman': SpearmanrResult(correlation=0.5654197630879637, pvalue=1.384659559431257e-64), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5052423292216763, 4.142749982375633e-31), 'spearman': SpearmanrResult(correlation=0.5885907966227912, pvalue=3.917645276082599e-44), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5150481922700462, 4.810256138046579e-52), 'spearman': SpearmanrResult(correlation=0.5575336405874239, pvalue=1.7705801328221372e-62), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5943056455548579, 1.8375715867307077e-39), 'spearman': SpearmanrResult(correlation=0.5176886358034175, pvalue=9.784730663509305e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49253479793852006, 'wmean': 0.4798515742844876}, 'spearman': {'mean': 0.5111532055339625, 'wmean': 0.5031647000540951}}}, 'STS13': {'FNWN': {'pearson': (0.19633703805872676, 0.006775385896435672), 'spearman': SpearmanrResult(correlation=0.19836432712966887, pvalue=0.0062151068029943175), 'nsamples': 189}, 'headlines': {'pearson': (0.5990572097014104, 3.0500350266704298e-74), 'spearman': SpearmanrResult(correlation=0.5942675548384868, pvalue=8.49887966603922e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.5223006992508834, 1.3825049411551024e-40), 'spearman': SpearmanrResult(correlation=0.51976910382254, pvalue=3.822145272976887e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.43923164900367356, 'wmean': 0.5196075331659352}, 'spearman': {'mean': 0.43746699526356525, 'wmean': 0.5165213274672117}}}, 'STS14': {'deft-forum': {'pearson': (0.3492280005282262, 2.3628718099641415e-14), 'spearman': SpearmanrResult(correlation=0.35026159960243414, pvalue=1.9591311320413318e-14), 'nsamples': 450}, 'deft-news': {'pearson': (0.7384769649218146, 6.152991531990872e-53), 'spearman': SpearmanrResult(correlation=0.6992796215576151, pvalue=2.366166515117583e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5255135588942361, 1.7703944934789613e-54), 'spearman': SpearmanrResult(correlation=0.5029936189732361, pvalue=2.415084514359342e-49), 'nsamples': 750}, 'images': {'pearson': (0.5046907733350434, 1.021399007732073e-49), 'spearman': SpearmanrResult(correlation=0.5049245277018458, pvalue=9.068898333564059e-50), 'nsamples': 750}, 'OnWN': {'pearson': (0.640555777946029, 7.41782749186269e-88), 'spearman': SpearmanrResult(correlation=0.6687379960275877, pvalue=2.2710817904514242e-98), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5565723735835079, 3.170107857723377e-62), 'spearman': SpearmanrResult(correlation=0.5262245559901362, pvalue=1.2011628248603418e-54), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5525062415348095, 'wmean': 0.5464520140088956}, 'spearman': {'mean': 0.5420703199754758, 'wmean': 0.5385499014154624}}}, 'STS15': {'answers-forums': {'pearson': (0.5462380469950552, 1.4935074920645098e-30), 'spearman': SpearmanrResult(correlation=0.5095992583379344, pvalue=3.5547591233655674e-26), 'nsamples': 375}, 'answers-students': {'pearson': (0.6673604945116349, 7.888824006585464e-98), 'spearman': SpearmanrResult(correlation=0.6783829462830014, pvalue=3.067327741165379e-102), 'nsamples': 750}, 'belief': {'pearson': (0.5500935198813003, 4.8059180667998294e-31), 'spearman': SpearmanrResult(correlation=0.5741280890295397, pvalue=2.9038018973616798e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.591101336265168, 7.43654844228853e-72), 'spearman': SpearmanrResult(correlation=0.6054910922809295, pvalue=3.1940718376820674e-76), 'nsamples': 750}, 'images': {'pearson': (0.6398321711404339, 1.3358644350498103e-87), 'spearman': SpearmanrResult(correlation=0.6563029296532757, pvalue=1.3646218194426671e-93), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5989251137587185, 'wmean': 0.6116149463388536}, 'spearman': {'mean': 0.6047808631169362, 'wmean': 0.620510160475236}}}, 'STS16': {'answer-answer': {'pearson': (0.5395549774707434, 1.3705674493244944e-20), 'spearman': SpearmanrResult(correlation=0.55535121135928, pvalue=5.929681054123804e-22), 'nsamples': 254}, 'headlines': {'pearson': (0.6287694195741622, 8.311824754954156e-29), 'spearman': SpearmanrResult(correlation=0.6318952385320172, pvalue=3.6888360179051406e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7015934589101276, 2.0902435818908453e-35), 'spearman': SpearmanrResult(correlation=0.7099272243550129, pvalue=1.4278668747065459e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.7666013675400091, 1.8511231104557467e-48), 'spearman': SpearmanrResult(correlation=0.7993781425506917, pvalue=1.8340655369902012e-55), 'nsamples': 244}, 'question-question': {'pearson': (0.29774641669462426, 1.195707854706623e-05), 'spearman': SpearmanrResult(correlation=0.31286577444372315, pvalue=3.983556175493239e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5868531280379333, 'wmean': 0.5938084148986537}, 'spearman': {'mean': 0.601883518248145, 'wmean': 0.6088714985855728}}}, 'MR': {'devacc': 73.89, 'acc': 72.98, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.86, 'acc': 78.39, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.66, 'acc': 85.66, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.24, 'acc': 91.46, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.86, 'acc': 85.17, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.06, 'acc': 40.0, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 74.3, 'acc': 92.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.94, 'acc': 74.14, 'f1': 81.62, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.2, 'acc': 77.76, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7787692944492686, 'pearson': 0.7807017907400123, 'spearman': 0.7171486253854353, 'mse': 0.39751808783835024, 'yhat': array([2.89989777, 4.48427875, 1.17802159, ..., 3.00651681, 4.43655789,        4.89735711]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6324647291727744, 'pearson': 0.6203636548653669, 'spearman': 0.6147934401135875, 'mse': 1.5454934247704615, 'yhat': array([1.98771103, 1.05991187, 2.9869382 , ..., 3.96045947, 3.08470723,        3.11473199]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 69.21, 'acc': 69.2, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 64.61, 'acc': 63.93, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 15.72, 'acc': 15.78, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.99, 'acc': 32.03, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.32, 'acc': 59.82, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.2, 'acc': 90.28, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.76, 'acc': 86.95, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.51, 'acc': 81.29, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.5, 'acc': 80.16, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.5, 'acc': 65.25, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.0, 'acc': 72.02, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 14:01:26,460 : STS12 p=0.4799, STS12 s=0.5032, STS13 p=0.5196, STS13 s=0.5165, STS14 p=0.5465, STS14 s=0.5385, STS15 p=0.6116, STS15 s=0.6205, STS 16 p=0.5938, STS16 s=0.6089, STS B p=0.6204, STS B s=0.6148, STS B m=1.5455, SICK-R p=0.7807, SICK-R s=0.7171, SICK-P m=0.3975
2019-03-13 14:01:26,460 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 14:01:26,460 : 0.4799,0.5032,0.5196,0.5165,0.5465,0.5385,0.6116,0.6205,0.5938,0.6089,0.6204,0.6148,1.5455,0.7807,0.7171,0.3975
2019-03-13 14:01:26,460 : MR=72.98, CR=78.39, SUBJ=91.46, MPQA=85.66, SST-B=85.17, SST-F=40.00, TREC=92.60, SICK-E=77.76, SNLI=69.20, MRPC=74.14, MRPC f=81.62
2019-03-13 14:01:26,460 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 14:01:26,460 : 72.98,78.39,91.46,85.66,85.17,40.00,92.60,77.76,69.20,74.14,81.62
2019-03-13 14:01:26,460 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 14:01:26,460 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 14:01:26,460 : na,na,na,na,na,na,na,na,na,na
2019-03-13 14:01:26,460 : SentLen=63.93, WC=15.78, TreeDepth=32.03, TopConst=59.82, BShift=90.28, Tense=86.95, SubjNum=81.29, ObjNum=80.16, SOMO=65.25, CoordInv=72.02, average=64.75
2019-03-13 14:01:26,460 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 14:01:26,460 : 63.93,15.78,32.03,59.82,90.28,86.95,81.29,80.16,65.25,72.02,64.75
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,460 : layer 18
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,460 : ********************************************************************************
2019-03-13 14:01:26,553 : ***** Transfer task : STS12 *****


2019-03-13 14:01:26,565 : loading BERT model bert-large-uncased
2019-03-13 14:01:26,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:01:26,582 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:01:26,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuhv46jo8
2019-03-13 14:01:34,015 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:01:43,576 : MSRpar : pearson = 0.3004, spearman = 0.3291
2019-03-13 14:01:45,206 : MSRvid : pearson = 0.5217, spearman = 0.5356
2019-03-13 14:01:46,607 : SMTeuroparl : pearson = 0.5015, spearman = 0.5832
2019-03-13 14:01:49,277 : surprise.OnWN : pearson = 0.5015, spearman = 0.5429
2019-03-13 14:01:50,691 : surprise.SMTnews : pearson = 0.5880, spearman = 0.5212
2019-03-13 14:01:50,691 : ALL (weighted average) : Pearson = 0.4690,             Spearman = 0.4927
2019-03-13 14:01:50,691 : ALL (average) : Pearson = 0.4826,             Spearman = 0.5024

2019-03-13 14:01:50,691 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 14:01:50,702 : loading BERT model bert-large-uncased
2019-03-13 14:01:50,702 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:01:50,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:01:50,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi7hw43nw
2019-03-13 14:01:58,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:02:04,926 : FNWN : pearson = 0.1679, spearman = 0.1664
2019-03-13 14:02:06,804 : headlines : pearson = 0.5891, spearman = 0.5842
2019-03-13 14:02:08,260 : OnWN : pearson = 0.5264, spearman = 0.5199
2019-03-13 14:02:08,261 : ALL (weighted average) : Pearson = 0.5126,             Spearman = 0.5075
2019-03-13 14:02:08,261 : ALL (average) : Pearson = 0.4278,             Spearman = 0.4235

2019-03-13 14:02:08,261 : ***** Transfer task : STS14 *****


2019-03-13 14:02:08,276 : loading BERT model bert-large-uncased
2019-03-13 14:02:08,276 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:02:08,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:02:08,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn_xjb7xo
2019-03-13 14:02:15,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:02:22,667 : deft-forum : pearson = 0.3177, spearman = 0.3283
2019-03-13 14:02:24,287 : deft-news : pearson = 0.7298, spearman = 0.6965
2019-03-13 14:02:26,438 : headlines : pearson = 0.5217, spearman = 0.5021
2019-03-13 14:02:28,495 : images : pearson = 0.5129, spearman = 0.5046
2019-03-13 14:02:30,607 : OnWN : pearson = 0.6417, spearman = 0.6694
2019-03-13 14:02:33,441 : tweet-news : pearson = 0.5667, spearman = 0.5346
2019-03-13 14:02:33,442 : ALL (weighted average) : Pearson = 0.5451,             Spearman = 0.5372
2019-03-13 14:02:33,442 : ALL (average) : Pearson = 0.5484,             Spearman = 0.5392

2019-03-13 14:02:33,442 : ***** Transfer task : STS15 *****


2019-03-13 14:02:33,475 : loading BERT model bert-large-uncased
2019-03-13 14:02:33,475 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:02:33,493 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:02:33,493 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfsqs1fsb
2019-03-13 14:02:40,908 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:02:48,328 : answers-forums : pearson = 0.5254, spearman = 0.4935
2019-03-13 14:02:50,398 : answers-students : pearson = 0.6581, spearman = 0.6764
2019-03-13 14:02:52,431 : belief : pearson = 0.5301, spearman = 0.5535
2019-03-13 14:02:54,666 : headlines : pearson = 0.5913, spearman = 0.6063
2019-03-13 14:02:56,785 : images : pearson = 0.6325, spearman = 0.6435
2019-03-13 14:02:56,785 : ALL (weighted average) : Pearson = 0.6024,             Spearman = 0.6124
2019-03-13 14:02:56,785 : ALL (average) : Pearson = 0.5875,             Spearman = 0.5946

2019-03-13 14:02:56,785 : ***** Transfer task : STS16 *****


2019-03-13 14:02:56,856 : loading BERT model bert-large-uncased
2019-03-13 14:02:56,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:02:56,874 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:02:56,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0w4n229f
2019-03-13 14:03:04,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:03:10,730 : answer-answer : pearson = 0.5187, spearman = 0.5211
2019-03-13 14:03:11,387 : headlines : pearson = 0.6251, spearman = 0.6277
2019-03-13 14:03:12,262 : plagiarism : pearson = 0.6860, spearman = 0.6842
2019-03-13 14:03:13,744 : postediting : pearson = 0.7615, spearman = 0.7903
2019-03-13 14:03:14,346 : question-question : pearson = 0.3002, spearman = 0.3136
2019-03-13 14:03:14,346 : ALL (weighted average) : Pearson = 0.5849,             Spearman = 0.5939
2019-03-13 14:03:14,346 : ALL (average) : Pearson = 0.5783,             Spearman = 0.5874

2019-03-13 14:03:14,346 : ***** Transfer task : MR *****


2019-03-13 14:03:14,365 : loading BERT model bert-large-uncased
2019-03-13 14:03:14,365 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:03:14,384 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:03:14,384 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1zf7ob_c
2019-03-13 14:03:21,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:03:27,357 : Generating sentence embeddings
2019-03-13 14:03:58,528 : Generated sentence embeddings
2019-03-13 14:03:58,528 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:04:08,509 : Best param found at split 1: l2reg = 0.01                 with score 72.5
2019-03-13 14:04:21,032 : Best param found at split 2: l2reg = 0.001                 with score 73.21
2019-03-13 14:04:35,336 : Best param found at split 3: l2reg = 1e-05                 with score 76.39
2019-03-13 14:04:46,140 : Best param found at split 4: l2reg = 1e-05                 with score 76.25
2019-03-13 14:04:56,479 : Best param found at split 5: l2reg = 0.01                 with score 73.6
2019-03-13 14:04:57,033 : Dev acc : 74.39 Test acc : 70.19

2019-03-13 14:04:57,034 : ***** Transfer task : CR *****


2019-03-13 14:04:57,042 : loading BERT model bert-large-uncased
2019-03-13 14:04:57,042 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:04:57,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:04:57,063 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3up94l9b
2019-03-13 14:05:04,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:05:10,087 : Generating sentence embeddings
2019-03-13 14:05:18,326 : Generated sentence embeddings
2019-03-13 14:05:18,326 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:05:21,693 : Best param found at split 1: l2reg = 0.0001                 with score 82.61
2019-03-13 14:05:25,420 : Best param found at split 2: l2reg = 0.01                 with score 82.74
2019-03-13 14:05:29,143 : Best param found at split 3: l2reg = 0.001                 with score 83.15
2019-03-13 14:05:32,822 : Best param found at split 4: l2reg = 1e-05                 with score 77.93
2019-03-13 14:05:36,401 : Best param found at split 5: l2reg = 1e-05                 with score 81.17
2019-03-13 14:05:36,596 : Dev acc : 81.52 Test acc : 79.2

2019-03-13 14:05:36,597 : ***** Transfer task : MPQA *****


2019-03-13 14:05:36,603 : loading BERT model bert-large-uncased
2019-03-13 14:05:36,603 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:05:36,622 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:05:36,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr7bc83z8
2019-03-13 14:05:44,110 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:05:49,692 : Generating sentence embeddings
2019-03-13 14:05:57,182 : Generated sentence embeddings
2019-03-13 14:05:57,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:06:06,090 : Best param found at split 1: l2reg = 0.01                 with score 85.6
2019-03-13 14:06:15,152 : Best param found at split 2: l2reg = 0.01                 with score 85.42
2019-03-13 14:06:25,821 : Best param found at split 3: l2reg = 1e-05                 with score 85.79
2019-03-13 14:06:37,890 : Best param found at split 4: l2reg = 0.0001                 with score 86.12
2019-03-13 14:06:49,477 : Best param found at split 5: l2reg = 1e-05                 with score 85.85
2019-03-13 14:06:50,010 : Dev acc : 85.76 Test acc : 85.05

2019-03-13 14:06:50,011 : ***** Transfer task : SUBJ *****


2019-03-13 14:06:50,027 : loading BERT model bert-large-uncased
2019-03-13 14:06:50,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:06:50,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:06:50,046 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3qu5hzrr
2019-03-13 14:06:57,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:07:03,107 : Generating sentence embeddings
2019-03-13 14:07:33,658 : Generated sentence embeddings
2019-03-13 14:07:33,659 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:07:44,012 : Best param found at split 1: l2reg = 0.001                 with score 94.18
2019-03-13 14:07:55,375 : Best param found at split 2: l2reg = 0.0001                 with score 93.96
2019-03-13 14:08:05,796 : Best param found at split 3: l2reg = 0.001                 with score 93.5
2019-03-13 14:08:16,117 : Best param found at split 4: l2reg = 1e-05                 with score 93.94
2019-03-13 14:08:25,951 : Best param found at split 5: l2reg = 0.0001                 with score 93.64
2019-03-13 14:08:26,378 : Dev acc : 93.84 Test acc : 93.13

2019-03-13 14:08:26,379 : ***** Transfer task : SST Binary classification *****


2019-03-13 14:08:26,505 : loading BERT model bert-large-uncased
2019-03-13 14:08:26,506 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:08:26,529 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:08:26,529 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpceco1brq
2019-03-13 14:08:33,978 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:08:39,574 : Computing embedding for train
2019-03-13 14:10:18,586 : Computed train embeddings
2019-03-13 14:10:18,586 : Computing embedding for dev
2019-03-13 14:10:20,744 : Computed dev embeddings
2019-03-13 14:10:20,744 : Computing embedding for test
2019-03-13 14:10:25,271 : Computed test embeddings
2019-03-13 14:10:25,272 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:10:44,207 : [('reg:1e-05', 85.44), ('reg:0.0001', 85.32), ('reg:0.001', 84.75), ('reg:0.01', 84.06)]
2019-03-13 14:10:44,207 : Validation : best param found is reg = 1e-05 with score             85.44
2019-03-13 14:10:44,207 : Evaluating...
2019-03-13 14:10:48,606 : 
Dev acc : 85.44 Test acc : 86.38 for             SST Binary classification

2019-03-13 14:10:48,606 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 14:10:48,656 : loading BERT model bert-large-uncased
2019-03-13 14:10:48,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:10:48,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:10:48,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa2h7262a
2019-03-13 14:10:56,135 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:11:01,754 : Computing embedding for train
2019-03-13 14:11:23,413 : Computed train embeddings
2019-03-13 14:11:23,413 : Computing embedding for dev
2019-03-13 14:11:26,238 : Computed dev embeddings
2019-03-13 14:11:26,238 : Computing embedding for test
2019-03-13 14:11:31,812 : Computed test embeddings
2019-03-13 14:11:31,812 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:11:34,463 : [('reg:1e-05', 35.69), ('reg:0.0001', 45.23), ('reg:0.001', 41.69), ('reg:0.01', 35.33)]
2019-03-13 14:11:34,463 : Validation : best param found is reg = 0.0001 with score             45.23
2019-03-13 14:11:34,463 : Evaluating...
2019-03-13 14:11:35,278 : 
Dev acc : 45.23 Test acc : 47.38 for             SST Fine-Grained classification

2019-03-13 14:11:35,279 : ***** Transfer task : TREC *****


2019-03-13 14:11:35,293 : loading BERT model bert-large-uncased
2019-03-13 14:11:35,293 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:11:35,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:11:35,312 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd1m4kjll
2019-03-13 14:11:42,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:11:55,725 : Computed train embeddings
2019-03-13 14:11:56,307 : Computed test embeddings
2019-03-13 14:11:56,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:12:04,097 : [('reg:1e-05', 74.78), ('reg:0.0001', 75.33), ('reg:0.001', 73.71), ('reg:0.01', 67.73)]
2019-03-13 14:12:04,097 : Cross-validation : best param found is reg = 0.0001             with score 75.33
2019-03-13 14:12:04,098 : Evaluating...
2019-03-13 14:12:04,612 : 
Dev acc : 75.33 Test acc : 86.0             for TREC

2019-03-13 14:12:04,612 : ***** Transfer task : MRPC *****


2019-03-13 14:12:04,634 : loading BERT model bert-large-uncased
2019-03-13 14:12:04,634 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:12:04,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:12:04,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9adwz_p_
2019-03-13 14:12:12,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:12:17,535 : Computing embedding for train
2019-03-13 14:12:39,552 : Computed train embeddings
2019-03-13 14:12:39,553 : Computing embedding for test
2019-03-13 14:12:49,186 : Computed test embeddings
2019-03-13 14:12:49,207 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:12:54,068 : [('reg:1e-05', 70.19), ('reg:0.0001', 72.84), ('reg:0.001', 72.35), ('reg:0.01', 72.5)]
2019-03-13 14:12:54,068 : Cross-validation : best param found is reg = 0.0001             with score 72.84
2019-03-13 14:12:54,068 : Evaluating...
2019-03-13 14:12:54,358 : Dev acc : 72.84 Test acc 70.84; Test F1 81.76 for MRPC.

2019-03-13 14:12:54,358 : ***** Transfer task : SICK-Entailment*****


2019-03-13 14:12:54,436 : loading BERT model bert-large-uncased
2019-03-13 14:12:54,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:12:54,457 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:12:54,457 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3c81itwj
2019-03-13 14:13:01,953 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:13:07,424 : Computing embedding for train
2019-03-13 14:13:18,601 : Computed train embeddings
2019-03-13 14:13:18,602 : Computing embedding for dev
2019-03-13 14:13:20,125 : Computed dev embeddings
2019-03-13 14:13:20,125 : Computing embedding for test
2019-03-13 14:13:32,111 : Computed test embeddings
2019-03-13 14:13:32,146 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:13:33,473 : [('reg:1e-05', 77.0), ('reg:0.0001', 78.4), ('reg:0.001', 77.6), ('reg:0.01', 71.8)]
2019-03-13 14:13:33,473 : Validation : best param found is reg = 0.0001 with score             78.4
2019-03-13 14:13:33,473 : Evaluating...
2019-03-13 14:13:33,841 : 
Dev acc : 78.4 Test acc : 78.08 for                        SICK entailment

2019-03-13 14:13:33,842 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 14:13:33,868 : loading BERT model bert-large-uncased
2019-03-13 14:13:33,869 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:13:33,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:13:33,924 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm9ql20lj
2019-03-13 14:13:41,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:13:46,882 : Computing embedding for train
2019-03-13 14:13:58,075 : Computed train embeddings
2019-03-13 14:13:58,076 : Computing embedding for dev
2019-03-13 14:13:59,601 : Computed dev embeddings
2019-03-13 14:13:59,601 : Computing embedding for test
2019-03-13 14:14:11,611 : Computed test embeddings
2019-03-13 14:14:26,545 : Dev : Pearson 0.774046475281272
2019-03-13 14:14:26,545 : Test : Pearson 0.7798982009267592 Spearman 0.7213342860191072 MSE 0.3987747474643172                        for SICK Relatedness

2019-03-13 14:14:26,546 : 

***** Transfer task : STSBenchmark*****


2019-03-13 14:14:26,585 : loading BERT model bert-large-uncased
2019-03-13 14:14:26,585 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:14:26,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:14:26,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgr_i9z14
2019-03-13 14:14:33,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:14:39,491 : Computing embedding for train
2019-03-13 14:14:57,890 : Computed train embeddings
2019-03-13 14:14:57,890 : Computing embedding for dev
2019-03-13 14:15:03,473 : Computed dev embeddings
2019-03-13 14:15:03,473 : Computing embedding for test
2019-03-13 14:15:08,028 : Computed test embeddings
2019-03-13 14:15:26,765 : Dev : Pearson 0.6629899630368937
2019-03-13 14:15:26,765 : Test : Pearson 0.6287932630696689 Spearman 0.6250936238613439 MSE 1.5051968035613963                        for SICK Relatedness

2019-03-13 14:15:26,765 : ***** Transfer task : SNLI Entailment*****


2019-03-13 14:15:31,799 : loading BERT model bert-large-uncased
2019-03-13 14:15:31,799 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:15:31,933 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:15:31,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzcgmw3hn
2019-03-13 14:15:39,399 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:15:45,313 : PROGRESS (encoding): 0.00%
2019-03-13 14:18:29,213 : PROGRESS (encoding): 14.56%
2019-03-13 14:21:34,861 : PROGRESS (encoding): 29.12%
2019-03-13 14:24:41,380 : PROGRESS (encoding): 43.69%
2019-03-13 14:28:00,260 : PROGRESS (encoding): 58.25%
2019-03-13 14:31:41,657 : PROGRESS (encoding): 72.81%
2019-03-13 14:35:21,950 : PROGRESS (encoding): 87.37%
2019-03-13 14:39:20,283 : PROGRESS (encoding): 0.00%
2019-03-13 14:39:50,335 : PROGRESS (encoding): 0.00%
2019-03-13 14:40:19,174 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:41:21,342 : [('reg:1e-09', 66.21)]
2019-03-13 14:41:21,343 : Validation : best param found is reg = 1e-09 with score             66.21
2019-03-13 14:41:21,343 : Evaluating...
2019-03-13 14:42:22,997 : Dev acc : 66.21 Test acc : 66.12 for SNLI

2019-03-13 14:42:22,998 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 14:42:23,209 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 14:42:24,201 : loading BERT model bert-large-uncased
2019-03-13 14:42:24,201 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:42:24,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:42:24,227 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6v9ezxv3
2019-03-13 14:42:31,629 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:42:37,113 : Computing embeddings for train/dev/test
2019-03-13 14:46:05,282 : Computed embeddings
2019-03-13 14:46:05,282 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:46:39,483 : [('reg:1e-05', 60.51), ('reg:0.0001', 57.84), ('reg:0.001', 58.14), ('reg:0.01', 57.43)]
2019-03-13 14:46:39,484 : Validation : best param found is reg = 1e-05 with score             60.51
2019-03-13 14:46:39,484 : Evaluating...
2019-03-13 14:46:49,332 : 
Dev acc : 60.5 Test acc : 59.5 for LENGTH classification

2019-03-13 14:46:49,332 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 14:46:49,584 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 14:46:49,632 : loading BERT model bert-large-uncased
2019-03-13 14:46:49,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:46:49,659 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:46:49,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplw26k8le
2019-03-13 14:46:57,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:47:02,546 : Computing embeddings for train/dev/test
2019-03-13 14:50:14,300 : Computed embeddings
2019-03-13 14:50:14,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:50:48,215 : [('reg:1e-05', 13.94), ('reg:0.0001', 6.2), ('reg:0.001', 0.44), ('reg:0.01', 0.1)]
2019-03-13 14:50:48,215 : Validation : best param found is reg = 1e-05 with score             13.94
2019-03-13 14:50:48,215 : Evaluating...
2019-03-13 14:51:01,555 : 
Dev acc : 13.9 Test acc : 13.5 for WORDCONTENT classification

2019-03-13 14:51:01,556 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 14:51:02,072 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 14:51:02,138 : loading BERT model bert-large-uncased
2019-03-13 14:51:02,138 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:51:02,162 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:51:02,162 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo16tpx9d
2019-03-13 14:51:09,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:51:15,086 : Computing embeddings for train/dev/test
2019-03-13 14:54:15,519 : Computed embeddings
2019-03-13 14:54:15,519 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:54:46,104 : [('reg:1e-05', 25.49), ('reg:0.0001', 30.8), ('reg:0.001', 29.91), ('reg:0.01', 26.88)]
2019-03-13 14:54:46,105 : Validation : best param found is reg = 0.0001 with score             30.8
2019-03-13 14:54:46,105 : Evaluating...
2019-03-13 14:54:54,966 : 
Dev acc : 30.8 Test acc : 30.8 for DEPTH classification

2019-03-13 14:54:54,967 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 14:54:55,369 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 14:54:55,432 : loading BERT model bert-large-uncased
2019-03-13 14:54:55,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:54:55,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:54:55,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1x5p9ebp
2019-03-13 14:55:02,908 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:55:08,370 : Computing embeddings for train/dev/test
2019-03-13 14:57:55,522 : Computed embeddings
2019-03-13 14:57:55,522 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:58:25,491 : [('reg:1e-05', 61.76), ('reg:0.0001', 57.98), ('reg:0.001', 52.48), ('reg:0.01', 40.72)]
2019-03-13 14:58:25,491 : Validation : best param found is reg = 1e-05 with score             61.76
2019-03-13 14:58:25,491 : Evaluating...
2019-03-13 14:58:35,558 : 
Dev acc : 61.8 Test acc : 61.6 for TOPCONSTITUENTS classification

2019-03-13 14:58:35,560 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 14:58:35,915 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 14:58:35,981 : loading BERT model bert-large-uncased
2019-03-13 14:58:35,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:58:36,011 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:58:36,011 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyqbuk3z6
2019-03-13 14:58:43,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:58:48,949 : Computing embeddings for train/dev/test
2019-03-13 15:01:50,861 : Computed embeddings
2019-03-13 15:01:50,861 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:02:24,061 : [('reg:1e-05', 89.0), ('reg:0.0001', 89.69), ('reg:0.001', 89.65), ('reg:0.01', 88.83)]
2019-03-13 15:02:24,061 : Validation : best param found is reg = 0.0001 with score             89.69
2019-03-13 15:02:24,061 : Evaluating...
2019-03-13 15:02:31,865 : 
Dev acc : 89.7 Test acc : 90.0 for BIGRAMSHIFT classification

2019-03-13 15:02:31,866 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 15:02:32,270 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 15:02:32,337 : loading BERT model bert-large-uncased
2019-03-13 15:02:32,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:02:32,367 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:02:32,367 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt005ecjp
2019-03-13 15:02:39,889 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:02:45,422 : Computing embeddings for train/dev/test
2019-03-13 15:05:43,117 : Computed embeddings
2019-03-13 15:05:43,117 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:06:07,955 : [('reg:1e-05', 88.66), ('reg:0.0001', 88.64), ('reg:0.001', 88.91), ('reg:0.01', 89.18)]
2019-03-13 15:06:07,955 : Validation : best param found is reg = 0.01 with score             89.18
2019-03-13 15:06:07,955 : Evaluating...
2019-03-13 15:06:14,484 : 
Dev acc : 89.2 Test acc : 87.3 for TENSE classification

2019-03-13 15:06:14,485 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 15:06:14,885 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 15:06:14,948 : loading BERT model bert-large-uncased
2019-03-13 15:06:14,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:06:15,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:06:15,063 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpncwj6pr1
2019-03-13 15:06:22,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:06:28,058 : Computing embeddings for train/dev/test
2019-03-13 15:09:36,068 : Computed embeddings
2019-03-13 15:09:36,068 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:10:14,877 : [('reg:1e-05', 82.01), ('reg:0.0001', 82.02), ('reg:0.001', 82.06), ('reg:0.01', 81.76)]
2019-03-13 15:10:14,877 : Validation : best param found is reg = 0.001 with score             82.06
2019-03-13 15:10:14,878 : Evaluating...
2019-03-13 15:10:25,621 : 
Dev acc : 82.1 Test acc : 80.5 for SUBJNUMBER classification

2019-03-13 15:10:25,622 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 15:10:26,217 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 15:10:26,283 : loading BERT model bert-large-uncased
2019-03-13 15:10:26,283 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:10:26,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:10:26,310 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp61sm6egw
2019-03-13 15:10:33,771 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:10:39,297 : Computing embeddings for train/dev/test
2019-03-13 15:13:43,932 : Computed embeddings
2019-03-13 15:13:43,932 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:14:20,822 : [('reg:1e-05', 80.13), ('reg:0.0001', 80.14), ('reg:0.001', 80.28), ('reg:0.01', 79.05)]
2019-03-13 15:14:20,822 : Validation : best param found is reg = 0.001 with score             80.28
2019-03-13 15:14:20,822 : Evaluating...
2019-03-13 15:14:30,558 : 
Dev acc : 80.3 Test acc : 80.7 for OBJNUMBER classification

2019-03-13 15:14:30,560 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 15:14:30,934 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 15:14:31,002 : loading BERT model bert-large-uncased
2019-03-13 15:14:31,002 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:14:31,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:14:31,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8rvbzbms
2019-03-13 15:14:38,547 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:14:44,038 : Computing embeddings for train/dev/test
2019-03-13 15:18:18,044 : Computed embeddings
2019-03-13 15:18:18,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:18:49,072 : [('reg:1e-05', 67.1), ('reg:0.0001', 60.75), ('reg:0.001', 64.08), ('reg:0.01', 61.15)]
2019-03-13 15:18:49,073 : Validation : best param found is reg = 1e-05 with score             67.1
2019-03-13 15:18:49,073 : Evaluating...
2019-03-13 15:18:59,636 : 
Dev acc : 67.1 Test acc : 66.9 for ODDMANOUT classification

2019-03-13 15:18:59,637 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 15:19:00,092 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 15:19:00,169 : loading BERT model bert-large-uncased
2019-03-13 15:19:00,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:19:00,199 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:19:00,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_oih27s7
2019-03-13 15:19:07,689 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:19:13,219 : Computing embeddings for train/dev/test
2019-03-13 15:22:45,235 : Computed embeddings
2019-03-13 15:22:45,235 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:23:19,901 : [('reg:1e-05', 72.35), ('reg:0.0001', 72.35), ('reg:0.001', 72.12), ('reg:0.01', 68.86)]
2019-03-13 15:23:19,901 : Validation : best param found is reg = 1e-05 with score             72.35
2019-03-13 15:23:19,901 : Evaluating...
2019-03-13 15:23:26,811 : 
Dev acc : 72.3 Test acc : 72.5 for COORDINATIONINVERSION classification

2019-03-13 15:23:26,813 : total results: {'STS12': {'MSRpar': {'pearson': (0.30037882979132896, 4.1917895647395294e-17), 'spearman': SpearmanrResult(correlation=0.3291085068226007, pvalue=2.100971035144926e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.5217417983863811, 1.3649728634225568e-53), 'spearman': SpearmanrResult(correlation=0.5355774035431168, pvalue=6.695830062552261e-57), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5015131512758269, 1.3166506240314905e-30), 'spearman': SpearmanrResult(correlation=0.5832169308245463, pvalue=3.536189376584416e-43), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5015353565262233, 5.039494868605273e-49), 'spearman': SpearmanrResult(correlation=0.5429130816795754, pvalue=1.0185771160066764e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5880054913179409, 1.8027037745962065e-38), 'spearman': SpearmanrResult(correlation=0.5211953164511129, pvalue=3.6104257620796087e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.48263492545954023, 'wmean': 0.468967411840223}, 'spearman': {'mean': 0.5024022478641904, 'wmean': 0.4927132389145529}}}, 'STS13': {'FNWN': {'pearson': (0.16793685860144955, 0.020895718849268732), 'spearman': SpearmanrResult(correlation=0.1663774668694555, pvalue=0.022131147490491557), 'nsamples': 189}, 'headlines': {'pearson': (0.5891461777624514, 2.804655861068925e-71), 'spearman': SpearmanrResult(correlation=0.584210965713629, pvalue=7.68679520920844e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.5264468301243083, 2.566162547056784e-41), 'spearman': SpearmanrResult(correlation=0.5198971847879117, pvalue=3.6312189629914036e-40), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4278432888294031, 'wmean': 0.5126242475314996}, 'spearman': {'mean': 0.42349520579033206, 'wmean': 0.5075105907930448}}}, 'STS14': {'deft-forum': {'pearson': (0.31767674676105545, 5.210153886788407e-12), 'spearman': SpearmanrResult(correlation=0.32833709894455926, pvalue=9.016796373028054e-13), 'nsamples': 450}, 'deft-news': {'pearson': (0.7298116970897958, 3.803085827558715e-51), 'spearman': SpearmanrResult(correlation=0.6965399366368041, pvalue=7.214026981629381e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5217464017038558, 1.361595724398528e-53), 'spearman': SpearmanrResult(correlation=0.5020700886799778, pvalue=3.849682958565366e-49), 'nsamples': 750}, 'images': {'pearson': (0.512939070516528, 1.4539215142486637e-51), 'spearman': SpearmanrResult(correlation=0.504583676921704, pvalue=1.0785536494543068e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.6416935176121841, 2.932089083182116e-88), 'spearman': SpearmanrResult(correlation=0.6693581263132675, pvalue=1.2936925623585678e-98), 'nsamples': 750}, 'tweet-news': {'pearson': (0.566741352017932, 6.063232835449532e-65), 'spearman': SpearmanrResult(correlation=0.5346028981710373, pvalue=1.1586201972885813e-56), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5484347976168918, 'wmean': 0.5451302137486103}, 'spearman': {'mean': 0.539248637611225, 'wmean': 0.5372466048214887}}}, 'STS15': {'answers-forums': {'pearson': (0.5253623192125976, 5.407153433270785e-28), 'spearman': SpearmanrResult(correlation=0.49347790005269265, pvalue=2.063849796208057e-24), 'nsamples': 375}, 'answers-students': {'pearson': (0.6581343822070052, 2.7892083818621866e-94), 'spearman': SpearmanrResult(correlation=0.6763545421988373, pvalue=2.0550489073723617e-101), 'nsamples': 750}, 'belief': {'pearson': (0.5300707660826227, 1.483535842284273e-28), 'spearman': SpearmanrResult(correlation=0.5535047555755388, pvalue=1.7407126272260817e-31), 'nsamples': 375}, 'headlines': {'pearson': (0.5912806252957773, 6.581242928450204e-72), 'spearman': SpearmanrResult(correlation=0.6063465013142902, pvalue=1.728561547034345e-76), 'nsamples': 750}, 'images': {'pearson': (0.6325348891874586, 4.616209662742702e-85), 'spearman': SpearmanrResult(correlation=0.6434891858337163, pvalue=6.721748847984852e-89), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5874765963970923, 'wmean': 0.6024166098344628}, 'spearman': {'mean': 0.5946345769950151, 'wmean': 0.6124203892902399}}}, 'STS16': {'answer-answer': {'pearson': (0.5186622302468942, 6.82650056545659e-19), 'spearman': SpearmanrResult(correlation=0.5210706715910972, pvalue=4.410248793846332e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.62511231167233, 2.1252429971628183e-28), 'spearman': SpearmanrResult(correlation=0.6276609818044664, pvalue=1.1062228926806636e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6860108402274709, 2.4798103541939765e-33), 'spearman': SpearmanrResult(correlation=0.6842197919077905, pvalue=4.212307362586288e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.7615167295754847, 1.7837411891916254e-47), 'spearman': SpearmanrResult(correlation=0.7903328974647605, pvalue=2.0934607178997286e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.30021202818714277, 1.003648187544837e-05), 'spearman': SpearmanrResult(correlation=0.31357401690926223, pvalue=3.777952914092034e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5783028279818645, 'wmean': 0.5849325980176819}, 'spearman': {'mean': 0.5873716719354753, 'wmean': 0.5939192948631365}}}, 'MR': {'devacc': 74.39, 'acc': 70.19, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.52, 'acc': 79.2, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.76, 'acc': 85.05, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.84, 'acc': 93.13, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.44, 'acc': 86.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.23, 'acc': 47.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 75.33, 'acc': 86.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.84, 'acc': 70.84, 'f1': 81.76, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.4, 'acc': 78.08, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.774046475281272, 'pearson': 0.7798982009267592, 'spearman': 0.7213342860191072, 'mse': 0.3987747474643172, 'yhat': array([3.18940892, 4.58426227, 1.03331627, ..., 3.19477946, 4.63514156,        4.16299862]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6629899630368937, 'pearson': 0.6287932630696689, 'spearman': 0.6250936238613439, 'mse': 1.5051968035613963, 'yhat': array([1.42702921, 1.61854267, 2.75158178, ..., 3.929321  , 3.47379699,        3.32967299]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.21, 'acc': 66.12, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 60.51, 'acc': 59.53, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 13.94, 'acc': 13.5, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.8, 'acc': 30.83, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 61.76, 'acc': 61.63, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.69, 'acc': 90.05, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.18, 'acc': 87.32, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.06, 'acc': 80.53, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.28, 'acc': 80.68, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.1, 'acc': 66.93, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.35, 'acc': 72.54, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 15:23:26,813 : STS12 p=0.4690, STS12 s=0.4927, STS13 p=0.5126, STS13 s=0.5075, STS14 p=0.5451, STS14 s=0.5372, STS15 p=0.6024, STS15 s=0.6124, STS 16 p=0.5849, STS16 s=0.5939, STS B p=0.6288, STS B s=0.6251, STS B m=1.5052, SICK-R p=0.7799, SICK-R s=0.7213, SICK-P m=0.3988
2019-03-13 15:23:26,813 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 15:23:26,813 : 0.4690,0.4927,0.5126,0.5075,0.5451,0.5372,0.6024,0.6124,0.5849,0.5939,0.6288,0.6251,1.5052,0.7799,0.7213,0.3988
2019-03-13 15:23:26,814 : MR=70.19, CR=79.20, SUBJ=93.13, MPQA=85.05, SST-B=86.38, SST-F=47.38, TREC=86.00, SICK-E=78.08, SNLI=66.12, MRPC=70.84, MRPC f=81.76
2019-03-13 15:23:26,814 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 15:23:26,814 : 70.19,79.20,93.13,85.05,86.38,47.38,86.00,78.08,66.12,70.84,81.76
2019-03-13 15:23:26,814 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 15:23:26,814 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 15:23:26,814 : na,na,na,na,na,na,na,na,na,na
2019-03-13 15:23:26,814 : SentLen=59.53, WC=13.50, TreeDepth=30.83, TopConst=61.63, BShift=90.05, Tense=87.32, SubjNum=80.53, ObjNum=80.68, SOMO=66.93, CoordInv=72.54, average=64.35
2019-03-13 15:23:26,814 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 15:23:26,814 : 59.53,13.50,30.83,61.63,90.05,87.32,80.53,80.68,66.93,72.54,64.35
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,814 : layer 19
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,814 : ********************************************************************************
2019-03-13 15:23:26,908 : ***** Transfer task : STS12 *****


2019-03-13 15:23:26,920 : loading BERT model bert-large-uncased
2019-03-13 15:23:26,920 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:23:26,938 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:23:26,938 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpli7rw_73
2019-03-13 15:23:34,398 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:23:43,908 : MSRpar : pearson = 0.3076, spearman = 0.3348
2019-03-13 15:23:45,533 : MSRvid : pearson = 0.4844, spearman = 0.5033
2019-03-13 15:23:46,932 : SMTeuroparl : pearson = 0.4911, spearman = 0.5682
2019-03-13 15:23:49,605 : surprise.OnWN : pearson = 0.4777, spearman = 0.5210
2019-03-13 15:23:51,021 : surprise.SMTnews : pearson = 0.5790, spearman = 0.5206
2019-03-13 15:23:51,022 : ALL (weighted average) : Pearson = 0.4532,             Spearman = 0.4787
2019-03-13 15:23:51,022 : ALL (average) : Pearson = 0.4679,             Spearman = 0.4896

2019-03-13 15:23:51,022 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 15:23:51,030 : loading BERT model bert-large-uncased
2019-03-13 15:23:51,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:23:51,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:23:51,048 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptcn9cmji
2019-03-13 15:23:58,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:24:05,240 : FNWN : pearson = 0.1938, spearman = 0.1883
2019-03-13 15:24:07,117 : headlines : pearson = 0.5892, spearman = 0.5896
2019-03-13 15:24:08,569 : OnWN : pearson = 0.5018, spearman = 0.4894
2019-03-13 15:24:08,569 : ALL (weighted average) : Pearson = 0.5067,             Spearman = 0.5015
2019-03-13 15:24:08,569 : ALL (average) : Pearson = 0.4283,             Spearman = 0.4224

2019-03-13 15:24:08,569 : ***** Transfer task : STS14 *****


2019-03-13 15:24:08,584 : loading BERT model bert-large-uncased
2019-03-13 15:24:08,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:24:08,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:24:08,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp47etn6tp
2019-03-13 15:24:16,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:24:22,914 : deft-forum : pearson = 0.2975, spearman = 0.3146
2019-03-13 15:24:24,536 : deft-news : pearson = 0.7228, spearman = 0.6946
2019-03-13 15:24:26,688 : headlines : pearson = 0.5212, spearman = 0.5037
2019-03-13 15:24:28,748 : images : pearson = 0.5250, spearman = 0.5175
2019-03-13 15:24:30,857 : OnWN : pearson = 0.6113, spearman = 0.6334
2019-03-13 15:24:33,692 : tweet-news : pearson = 0.5470, spearman = 0.5120
2019-03-13 15:24:33,692 : ALL (weighted average) : Pearson = 0.5344,             Spearman = 0.5266
2019-03-13 15:24:33,692 : ALL (average) : Pearson = 0.5375,             Spearman = 0.5293

2019-03-13 15:24:33,692 : ***** Transfer task : STS15 *****


2019-03-13 15:24:33,724 : loading BERT model bert-large-uncased
2019-03-13 15:24:33,724 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:24:33,741 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:24:33,741 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa2y27fb4
2019-03-13 15:24:41,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:24:48,465 : answers-forums : pearson = 0.5323, spearman = 0.5033
2019-03-13 15:24:50,530 : answers-students : pearson = 0.6395, spearman = 0.6604
2019-03-13 15:24:52,558 : belief : pearson = 0.5274, spearman = 0.5527
2019-03-13 15:24:54,790 : headlines : pearson = 0.5895, spearman = 0.6049
2019-03-13 15:24:56,906 : images : pearson = 0.6213, spearman = 0.6388
2019-03-13 15:24:56,906 : ALL (weighted average) : Pearson = 0.5950,             Spearman = 0.6080
2019-03-13 15:24:56,906 : ALL (average) : Pearson = 0.5820,             Spearman = 0.5920

2019-03-13 15:24:56,906 : ***** Transfer task : STS16 *****


2019-03-13 15:24:56,975 : loading BERT model bert-large-uncased
2019-03-13 15:24:56,975 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:24:56,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:24:56,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpag01rziv
2019-03-13 15:25:04,427 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:25:10,740 : answer-answer : pearson = 0.4900, spearman = 0.4821
2019-03-13 15:25:11,394 : headlines : pearson = 0.6170, spearman = 0.6215
2019-03-13 15:25:12,268 : plagiarism : pearson = 0.6772, spearman = 0.6821
2019-03-13 15:25:13,747 : postediting : pearson = 0.7666, spearman = 0.7876
2019-03-13 15:25:14,347 : question-question : pearson = 0.2897, spearman = 0.3058
2019-03-13 15:25:14,347 : ALL (weighted average) : Pearson = 0.5746,             Spearman = 0.5819
2019-03-13 15:25:14,347 : ALL (average) : Pearson = 0.5681,             Spearman = 0.5758

2019-03-13 15:25:14,347 : ***** Transfer task : MR *****


2019-03-13 15:25:14,362 : loading BERT model bert-large-uncased
2019-03-13 15:25:14,362 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:25:14,382 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:25:14,382 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxskt1y9v
2019-03-13 15:25:21,805 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:25:27,249 : Generating sentence embeddings
2019-03-13 15:25:58,382 : Generated sentence embeddings
2019-03-13 15:25:58,382 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:26:10,893 : Best param found at split 1: l2reg = 1e-05                 with score 79.89
2019-03-13 15:26:24,532 : Best param found at split 2: l2reg = 0.01                 with score 75.59
2019-03-13 15:26:38,064 : Best param found at split 3: l2reg = 0.0001                 with score 76.91
2019-03-13 15:26:51,693 : Best param found at split 4: l2reg = 0.01                 with score 78.29
2019-03-13 15:27:04,450 : Best param found at split 5: l2reg = 1e-05                 with score 75.08
2019-03-13 15:27:05,078 : Dev acc : 77.15 Test acc : 73.13

2019-03-13 15:27:05,079 : ***** Transfer task : CR *****


2019-03-13 15:27:05,086 : loading BERT model bert-large-uncased
2019-03-13 15:27:05,086 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:27:05,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:27:05,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpazi9dtuh
2019-03-13 15:27:12,563 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:27:18,067 : Generating sentence embeddings
2019-03-13 15:27:26,290 : Generated sentence embeddings
2019-03-13 15:27:26,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:27:29,814 : Best param found at split 1: l2reg = 0.001                 with score 79.76
2019-03-13 15:27:33,490 : Best param found at split 2: l2reg = 0.01                 with score 82.67
2019-03-13 15:27:37,067 : Best param found at split 3: l2reg = 0.01                 with score 79.14
2019-03-13 15:27:40,682 : Best param found at split 4: l2reg = 0.001                 with score 81.47
2019-03-13 15:27:44,491 : Best param found at split 5: l2reg = 0.01                 with score 83.12
2019-03-13 15:27:44,611 : Dev acc : 81.23 Test acc : 82.12

2019-03-13 15:27:44,611 : ***** Transfer task : MPQA *****


2019-03-13 15:27:44,617 : loading BERT model bert-large-uncased
2019-03-13 15:27:44,617 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:27:44,667 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:27:44,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplzxqahc1
2019-03-13 15:27:52,107 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:27:57,471 : Generating sentence embeddings
2019-03-13 15:28:04,963 : Generated sentence embeddings
2019-03-13 15:28:04,964 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:28:15,746 : Best param found at split 1: l2reg = 1e-05                 with score 83.98
2019-03-13 15:28:23,520 : Best param found at split 2: l2reg = 1e-05                 with score 84.88
2019-03-13 15:28:32,030 : Best param found at split 3: l2reg = 0.001                 with score 84.97
2019-03-13 15:28:42,177 : Best param found at split 4: l2reg = 0.001                 with score 85.09
2019-03-13 15:28:53,942 : Best param found at split 5: l2reg = 0.001                 with score 85.94
2019-03-13 15:28:54,476 : Dev acc : 84.97 Test acc : 84.84

2019-03-13 15:28:54,477 : ***** Transfer task : SUBJ *****


2019-03-13 15:28:54,494 : loading BERT model bert-large-uncased
2019-03-13 15:28:54,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:28:54,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:28:54,513 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgvrrr646
2019-03-13 15:29:01,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:29:07,398 : Generating sentence embeddings
2019-03-13 15:29:37,936 : Generated sentence embeddings
2019-03-13 15:29:37,937 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:29:48,868 : Best param found at split 1: l2reg = 0.0001                 with score 94.31
2019-03-13 15:29:59,690 : Best param found at split 2: l2reg = 0.001                 with score 93.91
2019-03-13 15:30:11,552 : Best param found at split 3: l2reg = 0.01                 with score 93.97
2019-03-13 15:30:20,637 : Best param found at split 4: l2reg = 0.01                 with score 94.33
2019-03-13 15:30:28,538 : Best param found at split 5: l2reg = 1e-05                 with score 93.98
2019-03-13 15:30:28,908 : Dev acc : 94.1 Test acc : 93.2

2019-03-13 15:30:28,909 : ***** Transfer task : SST Binary classification *****


2019-03-13 15:30:28,999 : loading BERT model bert-large-uncased
2019-03-13 15:30:28,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:30:29,074 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:30:29,074 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps6b6_2gs
2019-03-13 15:30:36,569 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:30:42,148 : Computing embedding for train
2019-03-13 15:32:21,181 : Computed train embeddings
2019-03-13 15:32:21,181 : Computing embedding for dev
2019-03-13 15:32:23,333 : Computed dev embeddings
2019-03-13 15:32:23,333 : Computing embedding for test
2019-03-13 15:32:27,856 : Computed test embeddings
2019-03-13 15:32:27,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:32:47,070 : [('reg:1e-05', 86.7), ('reg:0.0001', 86.93), ('reg:0.001', 86.58), ('reg:0.01', 86.01)]
2019-03-13 15:32:47,070 : Validation : best param found is reg = 0.0001 with score             86.93
2019-03-13 15:32:47,070 : Evaluating...
2019-03-13 15:32:52,180 : 
Dev acc : 86.93 Test acc : 86.6 for             SST Binary classification

2019-03-13 15:32:52,180 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 15:32:52,234 : loading BERT model bert-large-uncased
2019-03-13 15:32:52,235 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:32:52,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:32:52,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1wttzybv
2019-03-13 15:32:59,756 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:33:05,294 : Computing embedding for train
2019-03-13 15:33:26,957 : Computed train embeddings
2019-03-13 15:33:26,957 : Computing embedding for dev
2019-03-13 15:33:29,787 : Computed dev embeddings
2019-03-13 15:33:29,787 : Computing embedding for test
2019-03-13 15:33:35,361 : Computed test embeddings
2019-03-13 15:33:35,361 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:33:38,195 : [('reg:1e-05', 41.05), ('reg:0.0001', 37.6), ('reg:0.001', 40.42), ('reg:0.01', 36.33)]
2019-03-13 15:33:38,195 : Validation : best param found is reg = 1e-05 with score             41.05
2019-03-13 15:33:38,196 : Evaluating...
2019-03-13 15:33:38,734 : 
Dev acc : 41.05 Test acc : 40.27 for             SST Fine-Grained classification

2019-03-13 15:33:38,734 : ***** Transfer task : TREC *****


2019-03-13 15:33:38,748 : loading BERT model bert-large-uncased
2019-03-13 15:33:38,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:33:38,766 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:33:38,767 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp90i8em_k
2019-03-13 15:33:46,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:33:59,206 : Computed train embeddings
2019-03-13 15:33:59,791 : Computed test embeddings
2019-03-13 15:33:59,791 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:34:07,151 : [('reg:1e-05', 59.75), ('reg:0.0001', 71.21), ('reg:0.001', 71.68), ('reg:0.01', 60.93)]
2019-03-13 15:34:07,151 : Cross-validation : best param found is reg = 0.001             with score 71.68
2019-03-13 15:34:07,151 : Evaluating...
2019-03-13 15:34:07,614 : 
Dev acc : 71.68 Test acc : 91.6             for TREC

2019-03-13 15:34:07,615 : ***** Transfer task : MRPC *****


2019-03-13 15:34:07,635 : loading BERT model bert-large-uncased
2019-03-13 15:34:07,636 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:34:07,659 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:34:07,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5keu7zoa
2019-03-13 15:34:15,113 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:34:20,421 : Computing embedding for train
2019-03-13 15:34:42,431 : Computed train embeddings
2019-03-13 15:34:42,432 : Computing embedding for test
2019-03-13 15:34:52,066 : Computed test embeddings
2019-03-13 15:34:52,086 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:34:55,871 : [('reg:1e-05', 72.69), ('reg:0.0001', 72.28), ('reg:0.001', 72.5), ('reg:0.01', 71.96)]
2019-03-13 15:34:55,871 : Cross-validation : best param found is reg = 1e-05             with score 72.69
2019-03-13 15:34:55,871 : Evaluating...
2019-03-13 15:34:56,113 : Dev acc : 72.69 Test acc 72.06; Test F1 78.41 for MRPC.

2019-03-13 15:34:56,113 : ***** Transfer task : SICK-Entailment*****


2019-03-13 15:34:56,176 : loading BERT model bert-large-uncased
2019-03-13 15:34:56,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:34:56,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:34:56,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7b1wdpxu
2019-03-13 15:35:03,667 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:35:09,224 : Computing embedding for train
2019-03-13 15:35:20,410 : Computed train embeddings
2019-03-13 15:35:20,410 : Computing embedding for dev
2019-03-13 15:35:21,936 : Computed dev embeddings
2019-03-13 15:35:21,936 : Computing embedding for test
2019-03-13 15:35:33,915 : Computed test embeddings
2019-03-13 15:35:33,951 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:35:35,519 : [('reg:1e-05', 76.2), ('reg:0.0001', 78.0), ('reg:0.001', 74.6), ('reg:0.01', 76.0)]
2019-03-13 15:35:35,519 : Validation : best param found is reg = 0.0001 with score             78.0
2019-03-13 15:35:35,519 : Evaluating...
2019-03-13 15:35:35,886 : 
Dev acc : 78.0 Test acc : 77.65 for                        SICK entailment

2019-03-13 15:35:35,886 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 15:35:35,913 : loading BERT model bert-large-uncased
2019-03-13 15:35:35,913 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:35:35,969 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:35:35,969 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa_ouo31d
2019-03-13 15:35:43,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:35:49,008 : Computing embedding for train
2019-03-13 15:36:00,218 : Computed train embeddings
2019-03-13 15:36:00,218 : Computing embedding for dev
2019-03-13 15:36:01,745 : Computed dev embeddings
2019-03-13 15:36:01,745 : Computing embedding for test
2019-03-13 15:36:13,750 : Computed test embeddings
2019-03-13 15:36:28,876 : Dev : Pearson 0.772616229515505
2019-03-13 15:36:28,877 : Test : Pearson 0.7756346146275249 Spearman 0.7142627148517076 MSE 0.40573352918185907                        for SICK Relatedness

2019-03-13 15:36:28,882 : 

***** Transfer task : STSBenchmark*****


2019-03-13 15:36:28,953 : loading BERT model bert-large-uncased
2019-03-13 15:36:28,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:36:28,982 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:36:28,983 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfarpcqz_
2019-03-13 15:36:36,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:36:41,852 : Computing embedding for train
2019-03-13 15:37:00,260 : Computed train embeddings
2019-03-13 15:37:00,260 : Computing embedding for dev
2019-03-13 15:37:05,839 : Computed dev embeddings
2019-03-13 15:37:05,840 : Computing embedding for test
2019-03-13 15:37:10,404 : Computed test embeddings
2019-03-13 15:37:26,781 : Dev : Pearson 0.6548945580884485
2019-03-13 15:37:26,782 : Test : Pearson 0.6192004649084838 Spearman 0.613394988103528 MSE 1.5141976882194208                        for SICK Relatedness

2019-03-13 15:37:26,782 : ***** Transfer task : SNLI Entailment*****


2019-03-13 15:37:31,882 : loading BERT model bert-large-uncased
2019-03-13 15:37:31,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:37:32,006 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:37:32,006 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprubvuvpn
2019-03-13 15:37:39,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:37:45,256 : PROGRESS (encoding): 0.00%
2019-03-13 15:40:29,022 : PROGRESS (encoding): 14.56%
2019-03-13 15:43:34,786 : PROGRESS (encoding): 29.12%
2019-03-13 15:46:41,365 : PROGRESS (encoding): 43.69%
2019-03-13 15:50:00,326 : PROGRESS (encoding): 58.25%
2019-03-13 15:53:41,724 : PROGRESS (encoding): 72.81%
2019-03-13 15:57:22,182 : PROGRESS (encoding): 87.37%
2019-03-13 16:01:20,816 : PROGRESS (encoding): 0.00%
2019-03-13 16:01:50,804 : PROGRESS (encoding): 0.00%
2019-03-13 16:02:19,623 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:03:14,670 : [('reg:1e-09', 65.72)]
2019-03-13 16:03:14,671 : Validation : best param found is reg = 1e-09 with score             65.72
2019-03-13 16:03:14,671 : Evaluating...
2019-03-13 16:04:09,773 : Dev acc : 65.72 Test acc : 66.11 for SNLI

2019-03-13 16:04:09,773 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 16:04:09,975 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 16:04:11,022 : loading BERT model bert-large-uncased
2019-03-13 16:04:11,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:04:11,048 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:04:11,049 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpotfryd15
2019-03-13 16:04:18,484 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:04:24,046 : Computing embeddings for train/dev/test
2019-03-13 16:07:52,010 : Computed embeddings
2019-03-13 16:07:52,010 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:08:19,989 : [('reg:1e-05', 59.22), ('reg:0.0001', 61.53), ('reg:0.001', 53.14), ('reg:0.01', 36.01)]
2019-03-13 16:08:19,990 : Validation : best param found is reg = 0.0001 with score             61.53
2019-03-13 16:08:19,990 : Evaluating...
2019-03-13 16:08:26,910 : 
Dev acc : 61.5 Test acc : 61.7 for LENGTH classification

2019-03-13 16:08:26,911 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 16:08:27,288 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 16:08:27,332 : loading BERT model bert-large-uncased
2019-03-13 16:08:27,333 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:08:27,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:08:27,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgne_o6xq
2019-03-13 16:08:34,813 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:08:40,285 : Computing embeddings for train/dev/test
2019-03-13 16:11:51,874 : Computed embeddings
2019-03-13 16:11:51,874 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:12:31,444 : [('reg:1e-05', 14.67), ('reg:0.0001', 4.44), ('reg:0.001', 0.6), ('reg:0.01', 0.1)]
2019-03-13 16:12:31,444 : Validation : best param found is reg = 1e-05 with score             14.67
2019-03-13 16:12:31,445 : Evaluating...
2019-03-13 16:12:46,696 : 
Dev acc : 14.7 Test acc : 14.5 for WORDCONTENT classification

2019-03-13 16:12:46,697 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 16:12:47,081 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 16:12:47,147 : loading BERT model bert-large-uncased
2019-03-13 16:12:47,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:12:47,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:12:47,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphdgpi6m6
2019-03-13 16:12:54,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:13:00,078 : Computing embeddings for train/dev/test
2019-03-13 16:16:00,008 : Computed embeddings
2019-03-13 16:16:00,008 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:16:26,131 : [('reg:1e-05', 28.33), ('reg:0.0001', 27.39), ('reg:0.001', 26.06), ('reg:0.01', 24.27)]
2019-03-13 16:16:26,131 : Validation : best param found is reg = 1e-05 with score             28.33
2019-03-13 16:16:26,131 : Evaluating...
2019-03-13 16:16:33,046 : 
Dev acc : 28.3 Test acc : 29.0 for DEPTH classification

2019-03-13 16:16:33,047 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 16:16:33,428 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 16:16:33,492 : loading BERT model bert-large-uncased
2019-03-13 16:16:33,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:16:33,601 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:16:33,601 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp81p1wwvg
2019-03-13 16:16:41,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:16:46,994 : Computing embeddings for train/dev/test
2019-03-13 16:19:34,372 : Computed embeddings
2019-03-13 16:19:34,373 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:20:01,088 : [('reg:1e-05', 54.53), ('reg:0.0001', 57.17), ('reg:0.001', 58.52), ('reg:0.01', 40.82)]
2019-03-13 16:20:01,089 : Validation : best param found is reg = 0.001 with score             58.52
2019-03-13 16:20:01,089 : Evaluating...
2019-03-13 16:20:07,544 : 
Dev acc : 58.5 Test acc : 59.1 for TOPCONSTITUENTS classification

2019-03-13 16:20:07,545 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 16:20:07,887 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 16:20:07,952 : loading BERT model bert-large-uncased
2019-03-13 16:20:07,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:20:08,071 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:20:08,071 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzjdrx3kq
2019-03-13 16:20:15,531 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:20:21,034 : Computing embeddings for train/dev/test
2019-03-13 16:23:22,451 : Computed embeddings
2019-03-13 16:23:22,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:23:56,523 : [('reg:1e-05', 90.38), ('reg:0.0001', 90.5), ('reg:0.001', 90.46), ('reg:0.01', 89.57)]
2019-03-13 16:23:56,524 : Validation : best param found is reg = 0.0001 with score             90.5
2019-03-13 16:23:56,524 : Evaluating...
2019-03-13 16:24:05,956 : 
Dev acc : 90.5 Test acc : 90.8 for BIGRAMSHIFT classification

2019-03-13 16:24:05,957 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 16:24:06,553 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 16:24:06,622 : loading BERT model bert-large-uncased
2019-03-13 16:24:06,622 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:24:06,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:24:06,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeyuv7ar2
2019-03-13 16:24:14,100 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:24:19,783 : Computing embeddings for train/dev/test
2019-03-13 16:27:17,230 : Computed embeddings
2019-03-13 16:27:17,230 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:27:43,355 : [('reg:1e-05', 89.08), ('reg:0.0001', 89.11), ('reg:0.001', 89.13), ('reg:0.01', 88.82)]
2019-03-13 16:27:43,355 : Validation : best param found is reg = 0.001 with score             89.13
2019-03-13 16:27:43,355 : Evaluating...
2019-03-13 16:27:49,208 : 
Dev acc : 89.1 Test acc : 88.1 for TENSE classification

2019-03-13 16:27:49,209 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 16:27:49,653 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 16:27:49,719 : loading BERT model bert-large-uncased
2019-03-13 16:27:49,720 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:27:49,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:27:49,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf0xw92n7
2019-03-13 16:27:57,233 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:28:02,785 : Computing embeddings for train/dev/test
2019-03-13 16:31:11,384 : Computed embeddings
2019-03-13 16:31:11,384 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:31:48,058 : [('reg:1e-05', 80.37), ('reg:0.0001', 80.82), ('reg:0.001', 80.13), ('reg:0.01', 78.04)]
2019-03-13 16:31:48,059 : Validation : best param found is reg = 0.0001 with score             80.82
2019-03-13 16:31:48,059 : Evaluating...
2019-03-13 16:31:57,799 : 
Dev acc : 80.8 Test acc : 80.4 for SUBJNUMBER classification

2019-03-13 16:31:57,800 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 16:31:58,207 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 16:31:58,273 : loading BERT model bert-large-uncased
2019-03-13 16:31:58,274 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:31:58,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:31:58,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp07pyais2
2019-03-13 16:32:05,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:32:11,283 : Computing embeddings for train/dev/test
2019-03-13 16:35:17,571 : Computed embeddings
2019-03-13 16:35:17,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:35:48,491 : [('reg:1e-05', 75.75), ('reg:0.0001', 75.82), ('reg:0.001', 78.84), ('reg:0.01', 74.91)]
2019-03-13 16:35:48,491 : Validation : best param found is reg = 0.001 with score             78.84
2019-03-13 16:35:48,491 : Evaluating...
2019-03-13 16:35:57,172 : 
Dev acc : 78.8 Test acc : 79.3 for OBJNUMBER classification

2019-03-13 16:35:57,173 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 16:35:57,763 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 16:35:57,832 : loading BERT model bert-large-uncased
2019-03-13 16:35:57,833 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:35:57,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:35:57,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxxxc9z_a
2019-03-13 16:36:05,309 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:36:10,878 : Computing embeddings for train/dev/test
2019-03-13 16:39:45,160 : Computed embeddings
2019-03-13 16:39:45,160 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:40:20,538 : [('reg:1e-05', 67.61), ('reg:0.0001', 67.62), ('reg:0.001', 67.8), ('reg:0.01', 67.51)]
2019-03-13 16:40:20,538 : Validation : best param found is reg = 0.001 with score             67.8
2019-03-13 16:40:20,538 : Evaluating...
2019-03-13 16:40:29,686 : 
Dev acc : 67.8 Test acc : 67.8 for ODDMANOUT classification

2019-03-13 16:40:29,687 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 16:40:30,081 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 16:40:30,157 : loading BERT model bert-large-uncased
2019-03-13 16:40:30,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:40:30,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:40:30,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpat044s4c
2019-03-13 16:40:37,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:40:43,214 : Computing embeddings for train/dev/test
2019-03-13 16:44:15,512 : Computed embeddings
2019-03-13 16:44:15,512 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:44:40,182 : [('reg:1e-05', 68.83), ('reg:0.0001', 72.32), ('reg:0.001', 68.54), ('reg:0.01', 70.79)]
2019-03-13 16:44:40,182 : Validation : best param found is reg = 0.0001 with score             72.32
2019-03-13 16:44:40,182 : Evaluating...
2019-03-13 16:44:47,485 : 
Dev acc : 72.3 Test acc : 72.8 for COORDINATIONINVERSION classification

2019-03-13 16:44:47,487 : total results: {'STS12': {'MSRpar': {'pearson': (0.3075735918445172, 6.754463093356492e-18), 'spearman': SpearmanrResult(correlation=0.3347726188619322, pvalue=4.253741924864781e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4843900799080197, 2.2125909491656002e-45), 'spearman': SpearmanrResult(correlation=0.5032854046756052, pvalue=2.083659016071766e-49), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.49105521474309366, 3.1278776716634476e-29), 'spearman': SpearmanrResult(correlation=0.5681629674635272, pvalue=1.3576556507669803e-40), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4777360598718918, 5.056204370085737e-44), 'spearman': SpearmanrResult(correlation=0.5210130168684114, pvalue=2.0194781880361854e-53), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5789773132890542, 4.362473801233042e-37), 'spearman': SpearmanrResult(correlation=0.5205721953640976, pvalue=4.3138684842809546e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4679464519313153, 'wmean': 0.4532435940436726}, 'spearman': {'mean': 0.4895612406467148, 'wmean': 0.4786996101417296}}}, 'STS13': {'FNWN': {'pearson': (0.19375591951228144, 0.00755364803079648), 'spearman': SpearmanrResult(correlation=0.18826926227134289, pvalue=0.009477109827913588), 'nsamples': 189}, 'headlines': {'pearson': (0.5892491589517014, 2.61584556562208e-71), 'spearman': SpearmanrResult(correlation=0.5895584510662808, pvalue=2.1214939551406887e-71), 'nsamples': 750}, 'OnWN': {'pearson': (0.5018220270311509, 4.064955264524544e-37), 'spearman': SpearmanrResult(correlation=0.48939015093755683, pvalue=4.0064311551619584e-35), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4282757018317112, 'wmean': 0.5067192634440486}, 'spearman': {'mean': 0.42240595475839354, 'wmean': 0.5015330690299759}}}, 'STS14': {'deft-forum': {'pearson': (0.29752602103465503, 1.1920167953144615e-10), 'spearman': SpearmanrResult(correlation=0.31458347219126015, pvalue=8.556837762840367e-12), 'nsamples': 450}, 'deft-news': {'pearson': (0.7228358723698051, 9.381510077741294e-50), 'spearman': SpearmanrResult(correlation=0.6946432034428132, pvalue=1.5493899711951176e-44), 'nsamples': 300}, 'headlines': {'pearson': (0.5212096695402274, 1.8170897559091142e-53), 'spearman': SpearmanrResult(correlation=0.5036809715030345, pvalue=1.7053796182873608e-49), 'nsamples': 750}, 'images': {'pearson': (0.5249797049039858, 2.367567312757741e-54), 'spearman': SpearmanrResult(correlation=0.5174634757445384, pvalue=1.3424898499963287e-52), 'nsamples': 750}, 'OnWN': {'pearson': (0.6113165738437242, 4.700803994741184e-78), 'spearman': SpearmanrResult(correlation=0.633379071566211, pvalue=2.3664965671163463e-85), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5470471279649161, 9.196612710513757e-60), 'spearman': SpearmanrResult(correlation=0.5120166802756356, pvalue=2.3527103278098394e-51), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5374858282762189, 'wmean': 0.5344406075643138}, 'spearman': {'mean': 0.529294479120582, 'wmean': 0.5266295127562602}}}, 'STS15': {'answers-forums': {'pearson': (0.5323316034367377, 7.915199293485831e-29), 'spearman': SpearmanrResult(correlation=0.5032890460459567, pvalue=1.7881309163044842e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.6394848970603376, 1.770644149518707e-87), 'spearman': SpearmanrResult(correlation=0.6604349687992288, pvalue=3.7362456593232824e-95), 'nsamples': 750}, 'belief': {'pearson': (0.52736385756551, 3.1280841687852626e-28), 'spearman': SpearmanrResult(correlation=0.5526662335863114, pvalue=2.2367243888881144e-31), 'nsamples': 375}, 'headlines': {'pearson': (0.5894546574941599, 2.2760455561020256e-71), 'spearman': SpearmanrResult(correlation=0.604898637416726, pvalue=4.8815585310870003e-76), 'nsamples': 750}, 'images': {'pearson': (0.6212863580629281, 2.7955646604042174e-81), 'spearman': SpearmanrResult(correlation=0.6387698327220217, pvalue=3.1593771330367443e-87), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5819842747239348, 'wmean': 0.5950184107796374}, 'spearman': {'mean': 0.5920117437140489, 'wmean': 0.6080202696885276}}}, 'STS16': {'answer-answer': {'pearson': (0.490021330256342, 9.531196773539737e-17), 'spearman': SpearmanrResult(correlation=0.4820944118590055, pvalue=3.4525187152994252e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.616961143260843, 1.648121125234496e-27), 'spearman': SpearmanrResult(correlation=0.621530064032722, pvalue=5.267152229531394e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6772335814272377, 3.209907912071519e-32), 'spearman': SpearmanrResult(correlation=0.6820866070584262, pvalue=7.87827132103035e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.7665524939903998, 1.8923937393280465e-48), 'spearman': SpearmanrResult(correlation=0.7875948592153326, pvalue=8.38931797707996e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.2897406196329189, 2.0882094436269352e-05), 'spearman': SpearmanrResult(correlation=0.30583489484489823, pvalue=6.691902915917212e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5681018337135483, 'wmean': 0.5745759395634595}, 'spearman': {'mean': 0.5758281674020769, 'wmean': 0.5819439501272328}}}, 'MR': {'devacc': 77.15, 'acc': 73.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.23, 'acc': 82.12, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.97, 'acc': 84.84, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.1, 'acc': 93.2, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.93, 'acc': 86.6, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.05, 'acc': 40.27, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 71.68, 'acc': 91.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.69, 'acc': 72.06, 'f1': 78.41, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 78.0, 'acc': 77.65, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.772616229515505, 'pearson': 0.7756346146275249, 'spearman': 0.7142627148517076, 'mse': 0.40573352918185907, 'yhat': array([3.13794685, 4.49406897, 1.07332379, ..., 3.14478229, 4.01064693,        4.48833883]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6548945580884485, 'pearson': 0.6192004649084838, 'spearman': 0.613394988103528, 'mse': 1.5141976882194208, 'yhat': array([1.62916612, 1.05347372, 3.21434561, ..., 3.93438483, 3.57453886,        2.96404464]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.72, 'acc': 66.11, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 61.53, 'acc': 61.73, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 14.67, 'acc': 14.49, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.33, 'acc': 28.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.52, 'acc': 59.1, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.5, 'acc': 90.77, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.13, 'acc': 88.08, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.82, 'acc': 80.41, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.84, 'acc': 79.28, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.8, 'acc': 67.78, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.32, 'acc': 72.75, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 16:44:47,487 : STS12 p=0.4532, STS12 s=0.4787, STS13 p=0.5067, STS13 s=0.5015, STS14 p=0.5344, STS14 s=0.5266, STS15 p=0.5950, STS15 s=0.6080, STS 16 p=0.5746, STS16 s=0.5819, STS B p=0.6192, STS B s=0.6134, STS B m=1.5142, SICK-R p=0.7756, SICK-R s=0.7143, SICK-P m=0.4057
2019-03-13 16:44:47,487 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 16:44:47,487 : 0.4532,0.4787,0.5067,0.5015,0.5344,0.5266,0.5950,0.6080,0.5746,0.5819,0.6192,0.6134,1.5142,0.7756,0.7143,0.4057
2019-03-13 16:44:47,487 : MR=73.13, CR=82.12, SUBJ=93.20, MPQA=84.84, SST-B=86.60, SST-F=40.27, TREC=91.60, SICK-E=77.65, SNLI=66.11, MRPC=72.06, MRPC f=78.41
2019-03-13 16:44:47,487 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 16:44:47,487 : 73.13,82.12,93.20,84.84,86.60,40.27,91.60,77.65,66.11,72.06,78.41
2019-03-13 16:44:47,487 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 16:44:47,487 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 16:44:47,487 : na,na,na,na,na,na,na,na,na,na
2019-03-13 16:44:47,487 : SentLen=61.73, WC=14.49, TreeDepth=28.96, TopConst=59.10, BShift=90.77, Tense=88.08, SubjNum=80.41, ObjNum=79.28, SOMO=67.78, CoordInv=72.75, average=64.33
2019-03-13 16:44:47,487 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 16:44:47,487 : 61.73,14.49,28.96,59.10,90.77,88.08,80.41,79.28,67.78,72.75,64.33
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,487 : layer 20
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,487 : ********************************************************************************
2019-03-13 16:44:47,583 : ***** Transfer task : STS12 *****


2019-03-13 16:44:47,595 : loading BERT model bert-large-uncased
2019-03-13 16:44:47,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:44:47,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:44:47,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp75qenvto
2019-03-13 16:44:55,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:45:04,583 : MSRpar : pearson = 0.3160, spearman = 0.3421
2019-03-13 16:45:06,218 : MSRvid : pearson = 0.4555, spearman = 0.4755
2019-03-13 16:45:07,621 : SMTeuroparl : pearson = 0.4829, spearman = 0.5550
2019-03-13 16:45:10,299 : surprise.OnWN : pearson = 0.4706, spearman = 0.5131
2019-03-13 16:45:11,716 : surprise.SMTnews : pearson = 0.5840, spearman = 0.5170
2019-03-13 16:45:11,716 : ALL (weighted average) : Pearson = 0.4460,             Spearman = 0.4695
2019-03-13 16:45:11,716 : ALL (average) : Pearson = 0.4618,             Spearman = 0.4806

2019-03-13 16:45:11,716 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 16:45:11,724 : loading BERT model bert-large-uncased
2019-03-13 16:45:11,724 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:45:11,742 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:45:11,742 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2d_ttzpz
2019-03-13 16:45:19,141 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:45:25,992 : FNWN : pearson = 0.1771, spearman = 0.1815
2019-03-13 16:45:27,874 : headlines : pearson = 0.5765, spearman = 0.5852
2019-03-13 16:45:29,335 : OnWN : pearson = 0.4624, spearman = 0.4523
2019-03-13 16:45:29,335 : ALL (weighted average) : Pearson = 0.4835,             Spearman = 0.4846
2019-03-13 16:45:29,335 : ALL (average) : Pearson = 0.4053,             Spearman = 0.4063

2019-03-13 16:45:29,335 : ***** Transfer task : STS14 *****


2019-03-13 16:45:29,352 : loading BERT model bert-large-uncased
2019-03-13 16:45:29,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:45:29,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:45:29,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi87p1vx5
2019-03-13 16:45:36,842 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:45:43,802 : deft-forum : pearson = 0.2683, spearman = 0.2818
2019-03-13 16:45:45,426 : deft-news : pearson = 0.7317, spearman = 0.6968
2019-03-13 16:45:47,578 : headlines : pearson = 0.5147, spearman = 0.5053
2019-03-13 16:45:49,639 : images : pearson = 0.5102, spearman = 0.5029
2019-03-13 16:45:51,756 : OnWN : pearson = 0.5865, spearman = 0.6154
2019-03-13 16:45:54,597 : tweet-news : pearson = 0.5452, spearman = 0.5160
2019-03-13 16:45:54,597 : ALL (weighted average) : Pearson = 0.5221,             Spearman = 0.5175
2019-03-13 16:45:54,597 : ALL (average) : Pearson = 0.5261,             Spearman = 0.5197

2019-03-13 16:45:54,597 : ***** Transfer task : STS15 *****


2019-03-13 16:45:54,630 : loading BERT model bert-large-uncased
2019-03-13 16:45:54,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:45:54,648 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:45:54,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpohv9dd72
2019-03-13 16:46:02,104 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:46:09,499 : answers-forums : pearson = 0.5207, spearman = 0.4971
2019-03-13 16:46:11,568 : answers-students : pearson = 0.6289, spearman = 0.6467
2019-03-13 16:46:13,603 : belief : pearson = 0.5303, spearman = 0.5528
2019-03-13 16:46:15,838 : headlines : pearson = 0.5903, spearman = 0.6075
2019-03-13 16:46:17,960 : images : pearson = 0.6131, spearman = 0.6305
2019-03-13 16:46:17,960 : ALL (weighted average) : Pearson = 0.5895,             Spearman = 0.6024
2019-03-13 16:46:17,960 : ALL (average) : Pearson = 0.5767,             Spearman = 0.5869

2019-03-13 16:46:17,960 : ***** Transfer task : STS16 *****


2019-03-13 16:46:18,029 : loading BERT model bert-large-uncased
2019-03-13 16:46:18,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:46:18,047 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:46:18,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdt0_l5oi
2019-03-13 16:46:25,507 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:46:31,850 : answer-answer : pearson = 0.4791, spearman = 0.4696
2019-03-13 16:46:32,506 : headlines : pearson = 0.6217, spearman = 0.6291
2019-03-13 16:46:33,380 : plagiarism : pearson = 0.6655, spearman = 0.6662
2019-03-13 16:46:34,860 : postediting : pearson = 0.7617, spearman = 0.7937
2019-03-13 16:46:35,461 : question-question : pearson = 0.2846, spearman = 0.3114
2019-03-13 16:46:35,462 : ALL (weighted average) : Pearson = 0.5691,             Spearman = 0.5800
2019-03-13 16:46:35,462 : ALL (average) : Pearson = 0.5625,             Spearman = 0.5740

2019-03-13 16:46:35,462 : ***** Transfer task : MR *****


2019-03-13 16:46:35,480 : loading BERT model bert-large-uncased
2019-03-13 16:46:35,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:46:35,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:46:35,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp30n32cxf
2019-03-13 16:46:42,955 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:46:48,603 : Generating sentence embeddings
2019-03-13 16:47:19,809 : Generated sentence embeddings
2019-03-13 16:47:19,809 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:47:32,381 : Best param found at split 1: l2reg = 1e-05                 with score 78.6
2019-03-13 16:47:44,831 : Best param found at split 2: l2reg = 0.01                 with score 77.32
2019-03-13 16:47:59,182 : Best param found at split 3: l2reg = 0.0001                 with score 80.56
2019-03-13 16:48:12,303 : Best param found at split 4: l2reg = 0.01                 with score 80.27
2019-03-13 16:48:23,940 : Best param found at split 5: l2reg = 0.001                 with score 76.19
2019-03-13 16:48:24,566 : Dev acc : 78.59 Test acc : 73.52

2019-03-13 16:48:24,567 : ***** Transfer task : CR *****


2019-03-13 16:48:24,575 : loading BERT model bert-large-uncased
2019-03-13 16:48:24,575 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:48:24,594 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:48:24,595 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprkqnqt92
2019-03-13 16:48:32,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:48:37,612 : Generating sentence embeddings
2019-03-13 16:48:45,844 : Generated sentence embeddings
2019-03-13 16:48:45,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:48:48,635 : Best param found at split 1: l2reg = 0.0001                 with score 78.37
2019-03-13 16:48:51,498 : Best param found at split 2: l2reg = 1e-05                 with score 79.92
2019-03-13 16:48:54,455 : Best param found at split 3: l2reg = 0.01                 with score 80.76
2019-03-13 16:48:57,598 : Best param found at split 4: l2reg = 0.001                 with score 82.49
2019-03-13 16:49:01,011 : Best param found at split 5: l2reg = 0.001                 with score 79.18
2019-03-13 16:49:01,207 : Dev acc : 80.14 Test acc : 77.91

2019-03-13 16:49:01,207 : ***** Transfer task : MPQA *****


2019-03-13 16:49:01,213 : loading BERT model bert-large-uncased
2019-03-13 16:49:01,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:49:01,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:49:01,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp472pjbor
2019-03-13 16:49:08,704 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:49:14,206 : Generating sentence embeddings
2019-03-13 16:49:21,709 : Generated sentence embeddings
2019-03-13 16:49:21,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:49:32,355 : Best param found at split 1: l2reg = 0.01                 with score 85.6
2019-03-13 16:49:43,513 : Best param found at split 2: l2reg = 0.01                 with score 85.48
2019-03-13 16:49:54,793 : Best param found at split 3: l2reg = 1e-05                 with score 85.03
2019-03-13 16:50:06,642 : Best param found at split 4: l2reg = 1e-05                 with score 84.81
2019-03-13 16:50:19,261 : Best param found at split 5: l2reg = 0.0001                 with score 85.42
2019-03-13 16:50:19,614 : Dev acc : 85.27 Test acc : 83.27

2019-03-13 16:50:19,615 : ***** Transfer task : SUBJ *****


2019-03-13 16:50:19,630 : loading BERT model bert-large-uncased
2019-03-13 16:50:19,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:50:19,651 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:50:19,651 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0sky262z
2019-03-13 16:50:27,106 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:50:32,662 : Generating sentence embeddings
2019-03-13 16:51:03,230 : Generated sentence embeddings
2019-03-13 16:51:03,231 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 16:51:14,135 : Best param found at split 1: l2reg = 0.01                 with score 94.12
2019-03-13 16:51:25,869 : Best param found at split 2: l2reg = 0.0001                 with score 94.29
2019-03-13 16:51:35,612 : Best param found at split 3: l2reg = 0.001                 with score 94.29
2019-03-13 16:51:46,216 : Best param found at split 4: l2reg = 1e-05                 with score 94.55
2019-03-13 16:51:54,661 : Best param found at split 5: l2reg = 1e-05                 with score 94.45
2019-03-13 16:51:55,262 : Dev acc : 94.34 Test acc : 93.86

2019-03-13 16:51:55,263 : ***** Transfer task : SST Binary classification *****


2019-03-13 16:51:55,354 : loading BERT model bert-large-uncased
2019-03-13 16:51:55,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:51:55,430 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:51:55,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptnmvg9rr
2019-03-13 16:52:02,891 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:52:08,704 : Computing embedding for train
2019-03-13 16:53:47,862 : Computed train embeddings
2019-03-13 16:53:47,863 : Computing embedding for dev
2019-03-13 16:53:50,023 : Computed dev embeddings
2019-03-13 16:53:50,023 : Computing embedding for test
2019-03-13 16:53:54,557 : Computed test embeddings
2019-03-13 16:53:54,557 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:54:12,580 : [('reg:1e-05', 86.24), ('reg:0.0001', 86.24), ('reg:0.001', 86.35), ('reg:0.01', 85.89)]
2019-03-13 16:54:12,580 : Validation : best param found is reg = 0.001 with score             86.35
2019-03-13 16:54:12,580 : Evaluating...
2019-03-13 16:54:17,009 : 
Dev acc : 86.35 Test acc : 88.41 for             SST Binary classification

2019-03-13 16:54:17,010 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 16:54:17,059 : loading BERT model bert-large-uncased
2019-03-13 16:54:17,059 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:54:17,081 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:54:17,081 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuknvykar
2019-03-13 16:54:24,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:54:30,143 : Computing embedding for train
2019-03-13 16:54:51,803 : Computed train embeddings
2019-03-13 16:54:51,804 : Computing embedding for dev
2019-03-13 16:54:54,633 : Computed dev embeddings
2019-03-13 16:54:54,634 : Computing embedding for test
2019-03-13 16:55:00,212 : Computed test embeddings
2019-03-13 16:55:00,212 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:55:03,199 : [('reg:1e-05', 34.33), ('reg:0.0001', 42.33), ('reg:0.001', 39.96), ('reg:0.01', 34.88)]
2019-03-13 16:55:03,200 : Validation : best param found is reg = 0.0001 with score             42.33
2019-03-13 16:55:03,200 : Evaluating...
2019-03-13 16:55:03,893 : 
Dev acc : 42.33 Test acc : 48.51 for             SST Fine-Grained classification

2019-03-13 16:55:03,893 : ***** Transfer task : TREC *****


2019-03-13 16:55:03,906 : loading BERT model bert-large-uncased
2019-03-13 16:55:03,906 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:55:03,925 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:55:03,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl7_oqgd3
2019-03-13 16:55:11,396 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:55:24,452 : Computed train embeddings
2019-03-13 16:55:25,037 : Computed test embeddings
2019-03-13 16:55:25,037 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:55:31,892 : [('reg:1e-05', 69.96), ('reg:0.0001', 71.04), ('reg:0.001', 64.09), ('reg:0.01', 66.06)]
2019-03-13 16:55:31,892 : Cross-validation : best param found is reg = 0.0001             with score 71.04
2019-03-13 16:55:31,892 : Evaluating...
2019-03-13 16:55:32,335 : 
Dev acc : 71.04 Test acc : 92.8             for TREC

2019-03-13 16:55:32,335 : ***** Transfer task : MRPC *****


2019-03-13 16:55:32,357 : loading BERT model bert-large-uncased
2019-03-13 16:55:32,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:55:32,378 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:55:32,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm6u3nlsh
2019-03-13 16:55:39,832 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:55:45,202 : Computing embedding for train
2019-03-13 16:56:07,232 : Computed train embeddings
2019-03-13 16:56:07,232 : Computing embedding for test
2019-03-13 16:56:16,883 : Computed test embeddings
2019-03-13 16:56:16,905 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 16:56:21,556 : [('reg:1e-05', 72.45), ('reg:0.0001', 70.51), ('reg:0.001', 71.93), ('reg:0.01', 70.07)]
2019-03-13 16:56:21,557 : Cross-validation : best param found is reg = 1e-05             with score 72.45
2019-03-13 16:56:21,557 : Evaluating...
2019-03-13 16:56:21,844 : Dev acc : 72.45 Test acc 73.68; Test F1 81.38 for MRPC.

2019-03-13 16:56:21,844 : ***** Transfer task : SICK-Entailment*****


2019-03-13 16:56:21,905 : loading BERT model bert-large-uncased
2019-03-13 16:56:21,905 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:56:21,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:56:21,925 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp50j36wt3
2019-03-13 16:56:29,380 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:56:34,978 : Computing embedding for train
2019-03-13 16:56:46,170 : Computed train embeddings
2019-03-13 16:56:46,170 : Computing embedding for dev
2019-03-13 16:56:47,696 : Computed dev embeddings
2019-03-13 16:56:47,697 : Computing embedding for test
2019-03-13 16:56:59,700 : Computed test embeddings
2019-03-13 16:56:59,736 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:57:01,215 : [('reg:1e-05', 76.2), ('reg:0.0001', 71.6), ('reg:0.001', 74.2), ('reg:0.01', 76.0)]
2019-03-13 16:57:01,215 : Validation : best param found is reg = 1e-05 with score             76.2
2019-03-13 16:57:01,215 : Evaluating...
2019-03-13 16:57:01,626 : 
Dev acc : 76.2 Test acc : 76.44 for                        SICK entailment

2019-03-13 16:57:01,627 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 16:57:01,655 : loading BERT model bert-large-uncased
2019-03-13 16:57:01,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:57:01,714 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:57:01,714 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwwhpocd2
2019-03-13 16:57:09,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:57:14,676 : Computing embedding for train
2019-03-13 16:57:25,884 : Computed train embeddings
2019-03-13 16:57:25,884 : Computing embedding for dev
2019-03-13 16:57:27,412 : Computed dev embeddings
2019-03-13 16:57:27,412 : Computing embedding for test
2019-03-13 16:57:39,427 : Computed test embeddings
2019-03-13 16:57:54,238 : Dev : Pearson 0.7713320743398902
2019-03-13 16:57:54,238 : Test : Pearson 0.7698864184660172 Spearman 0.7178182498329732 MSE 0.418181533729441                        for SICK Relatedness

2019-03-13 16:57:54,239 : 

***** Transfer task : STSBenchmark*****


2019-03-13 16:57:54,278 : loading BERT model bert-large-uncased
2019-03-13 16:57:54,278 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:57:54,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:57:54,306 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp26okltkr
2019-03-13 16:58:01,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:58:07,373 : Computing embedding for train
2019-03-13 16:58:25,776 : Computed train embeddings
2019-03-13 16:58:25,776 : Computing embedding for dev
2019-03-13 16:58:31,361 : Computed dev embeddings
2019-03-13 16:58:31,361 : Computing embedding for test
2019-03-13 16:58:35,915 : Computed test embeddings
2019-03-13 16:58:54,873 : Dev : Pearson 0.6275222247124536
2019-03-13 16:58:54,873 : Test : Pearson 0.6149408862262047 Spearman 0.6139125546456572 MSE 1.5150035544601526                        for SICK Relatedness

2019-03-13 16:58:54,873 : ***** Transfer task : SNLI Entailment*****


2019-03-13 16:58:59,960 : loading BERT model bert-large-uncased
2019-03-13 16:58:59,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:59:00,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:59:00,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkgx8fly3
2019-03-13 16:59:07,494 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:59:13,468 : PROGRESS (encoding): 0.00%
2019-03-13 17:01:57,417 : PROGRESS (encoding): 14.56%
2019-03-13 17:05:03,511 : PROGRESS (encoding): 29.12%
2019-03-13 17:08:10,592 : PROGRESS (encoding): 43.69%
2019-03-13 17:11:30,064 : PROGRESS (encoding): 58.25%
2019-03-13 17:15:12,033 : PROGRESS (encoding): 72.81%
2019-03-13 17:18:52,991 : PROGRESS (encoding): 87.37%
2019-03-13 17:22:52,108 : PROGRESS (encoding): 0.00%
2019-03-13 17:23:22,140 : PROGRESS (encoding): 0.00%
2019-03-13 17:23:51,028 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:24:44,668 : [('reg:1e-09', 65.88)]
2019-03-13 17:24:44,668 : Validation : best param found is reg = 1e-09 with score             65.88
2019-03-13 17:24:44,668 : Evaluating...
2019-03-13 17:25:38,650 : Dev acc : 65.88 Test acc : 65.8 for SNLI

2019-03-13 17:25:38,650 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 17:25:38,858 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 17:25:39,921 : loading BERT model bert-large-uncased
2019-03-13 17:25:39,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:25:39,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:25:39,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbr50dbnc
2019-03-13 17:25:47,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:25:52,920 : Computing embeddings for train/dev/test
2019-03-13 17:29:21,358 : Computed embeddings
2019-03-13 17:29:21,358 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:29:53,995 : [('reg:1e-05', 53.5), ('reg:0.0001', 53.43), ('reg:0.001', 56.98), ('reg:0.01', 55.01)]
2019-03-13 17:29:53,995 : Validation : best param found is reg = 0.001 with score             56.98
2019-03-13 17:29:53,995 : Evaluating...
2019-03-13 17:30:02,514 : 
Dev acc : 57.0 Test acc : 57.8 for LENGTH classification

2019-03-13 17:30:02,515 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 17:30:02,764 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 17:30:02,810 : loading BERT model bert-large-uncased
2019-03-13 17:30:02,811 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:30:02,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:30:02,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkmqumut9
2019-03-13 17:30:10,325 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:30:15,678 : Computing embeddings for train/dev/test
2019-03-13 17:33:27,765 : Computed embeddings
2019-03-13 17:33:27,765 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:34:07,394 : [('reg:1e-05', 11.23), ('reg:0.0001', 3.85), ('reg:0.001', 0.37), ('reg:0.01', 0.16)]
2019-03-13 17:34:07,394 : Validation : best param found is reg = 1e-05 with score             11.23
2019-03-13 17:34:07,394 : Evaluating...
2019-03-13 17:34:19,646 : 
Dev acc : 11.2 Test acc : 11.4 for WORDCONTENT classification

2019-03-13 17:34:19,648 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 17:34:20,179 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 17:34:20,244 : loading BERT model bert-large-uncased
2019-03-13 17:34:20,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:34:20,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:34:20,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe3_j6k70
2019-03-13 17:34:27,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:34:33,145 : Computing embeddings for train/dev/test
2019-03-13 17:37:33,419 : Computed embeddings
2019-03-13 17:37:33,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:38:03,286 : [('reg:1e-05', 28.83), ('reg:0.0001', 27.65), ('reg:0.001', 26.63), ('reg:0.01', 26.0)]
2019-03-13 17:38:03,286 : Validation : best param found is reg = 1e-05 with score             28.83
2019-03-13 17:38:03,286 : Evaluating...
2019-03-13 17:38:08,848 : 
Dev acc : 28.8 Test acc : 28.9 for DEPTH classification

2019-03-13 17:38:08,849 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 17:38:09,218 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 17:38:09,281 : loading BERT model bert-large-uncased
2019-03-13 17:38:09,281 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:38:09,391 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:38:09,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_ckt6xvv
2019-03-13 17:38:16,902 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:38:22,521 : Computing embeddings for train/dev/test
2019-03-13 17:41:09,958 : Computed embeddings
2019-03-13 17:41:09,958 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:41:41,449 : [('reg:1e-05', 59.38), ('reg:0.0001', 53.86), ('reg:0.001', 51.71), ('reg:0.01', 41.01)]
2019-03-13 17:41:41,450 : Validation : best param found is reg = 1e-05 with score             59.38
2019-03-13 17:41:41,450 : Evaluating...
2019-03-13 17:41:49,383 : 
Dev acc : 59.4 Test acc : 59.6 for TOPCONSTITUENTS classification

2019-03-13 17:41:49,384 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 17:41:49,765 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 17:41:49,832 : loading BERT model bert-large-uncased
2019-03-13 17:41:49,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:41:49,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:41:49,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpipdvmk68
2019-03-13 17:41:57,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:42:02,845 : Computing embeddings for train/dev/test
2019-03-13 17:45:04,573 : Computed embeddings
2019-03-13 17:45:04,574 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:45:29,857 : [('reg:1e-05', 89.94), ('reg:0.0001', 88.93), ('reg:0.001', 89.26), ('reg:0.01', 89.64)]
2019-03-13 17:45:29,857 : Validation : best param found is reg = 1e-05 with score             89.94
2019-03-13 17:45:29,857 : Evaluating...
2019-03-13 17:45:37,711 : 
Dev acc : 89.9 Test acc : 89.8 for BIGRAMSHIFT classification

2019-03-13 17:45:37,712 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 17:45:38,118 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 17:45:38,186 : loading BERT model bert-large-uncased
2019-03-13 17:45:38,186 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:45:38,218 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:45:38,219 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgytdfh8y
2019-03-13 17:45:45,658 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:45:51,281 : Computing embeddings for train/dev/test
2019-03-13 17:48:49,047 : Computed embeddings
2019-03-13 17:48:49,047 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:49:14,513 : [('reg:1e-05', 89.31), ('reg:0.0001', 89.36), ('reg:0.001', 89.42), ('reg:0.01', 89.62)]
2019-03-13 17:49:14,514 : Validation : best param found is reg = 0.01 with score             89.62
2019-03-13 17:49:14,514 : Evaluating...
2019-03-13 17:49:19,270 : 
Dev acc : 89.6 Test acc : 87.7 for TENSE classification

2019-03-13 17:49:19,271 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 17:49:19,683 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 17:49:19,746 : loading BERT model bert-large-uncased
2019-03-13 17:49:19,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:49:19,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:49:19,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpduy80ktw
2019-03-13 17:49:27,327 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:49:32,995 : Computing embeddings for train/dev/test
2019-03-13 17:52:41,338 : Computed embeddings
2019-03-13 17:52:41,338 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:53:19,938 : [('reg:1e-05', 81.75), ('reg:0.0001', 81.77), ('reg:0.001', 81.59), ('reg:0.01', 80.49)]
2019-03-13 17:53:19,939 : Validation : best param found is reg = 0.0001 with score             81.77
2019-03-13 17:53:19,939 : Evaluating...
2019-03-13 17:53:29,771 : 
Dev acc : 81.8 Test acc : 80.3 for SUBJNUMBER classification

2019-03-13 17:53:29,772 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 17:53:30,172 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 17:53:30,238 : loading BERT model bert-large-uncased
2019-03-13 17:53:30,238 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:53:30,354 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:53:30,354 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1e8glfm8
2019-03-13 17:53:37,993 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:53:43,605 : Computing embeddings for train/dev/test
2019-03-13 17:56:48,485 : Computed embeddings
2019-03-13 17:56:48,486 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:57:16,451 : [('reg:1e-05', 76.21), ('reg:0.0001', 75.96), ('reg:0.001', 74.22), ('reg:0.01', 74.9)]
2019-03-13 17:57:16,451 : Validation : best param found is reg = 1e-05 with score             76.21
2019-03-13 17:57:16,451 : Evaluating...
2019-03-13 17:57:23,744 : 
Dev acc : 76.2 Test acc : 76.7 for OBJNUMBER classification

2019-03-13 17:57:23,746 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 17:57:24,305 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 17:57:24,373 : loading BERT model bert-large-uncased
2019-03-13 17:57:24,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:57:24,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:57:24,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzuwm85os
2019-03-13 17:57:31,798 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:57:37,375 : Computing embeddings for train/dev/test
2019-03-13 18:01:11,720 : Computed embeddings
2019-03-13 18:01:11,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:01:41,880 : [('reg:1e-05', 67.59), ('reg:0.0001', 63.87), ('reg:0.001', 64.58), ('reg:0.01', 63.87)]
2019-03-13 18:01:41,881 : Validation : best param found is reg = 1e-05 with score             67.59
2019-03-13 18:01:41,881 : Evaluating...
2019-03-13 18:01:52,292 : 
Dev acc : 67.6 Test acc : 67.4 for ODDMANOUT classification

2019-03-13 18:01:52,293 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 18:01:52,666 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 18:01:52,746 : loading BERT model bert-large-uncased
2019-03-13 18:01:52,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:01:52,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:01:52,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpamcnkrf9
2019-03-13 18:02:00,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:02:05,840 : Computing embeddings for train/dev/test
2019-03-13 18:05:37,968 : Computed embeddings
2019-03-13 18:05:37,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:06:01,838 : [('reg:1e-05', 73.07), ('reg:0.0001', 73.06), ('reg:0.001', 72.74), ('reg:0.01', 66.41)]
2019-03-13 18:06:01,838 : Validation : best param found is reg = 1e-05 with score             73.07
2019-03-13 18:06:01,838 : Evaluating...
2019-03-13 18:06:09,465 : 
Dev acc : 73.1 Test acc : 72.9 for COORDINATIONINVERSION classification

2019-03-13 18:06:09,467 : total results: {'STS12': {'MSRpar': {'pearson': (0.31598211299053736, 7.496065321793901e-19), 'spearman': SpearmanrResult(correlation=0.34213131106668504, pvalue=5.079114474069318e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.45548854519518023, 1.0866653798697165e-39), 'spearman': SpearmanrResult(correlation=0.4755482470530777, pvalue=1.3936045725246611e-43), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4828769982103195, 3.455951954092802e-28), 'spearman': SpearmanrResult(correlation=0.5550100764366189, pvalue=1.9248490128598967e-38), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4705527529092156, 1.3729610669151226e-42), 'spearman': SpearmanrResult(correlation=0.5130526202572188, pvalue=1.3701388430684095e-51), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5839797872647926, 7.556355248809084e-38), 'spearman': SpearmanrResult(correlation=0.5170373461288826, pvalue=1.176021120611778e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.46177603931400907, 'wmean': 0.44599936795958456}, 'spearman': {'mean': 0.48055592018849663, 'wmean': 0.4694648198109937}}}, 'STS13': {'FNWN': {'pearson': (0.17706761633317183, 0.014792500251193589), 'spearman': SpearmanrResult(correlation=0.18145725398374604, pvalue=0.012459504860817396), 'nsamples': 189}, 'headlines': {'pearson': (0.5764577674326757, 1.245919699094588e-67), 'spearman': SpearmanrResult(correlation=0.5852159389819175, pvalue=3.9350150455548167e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.4624399151948346, 4.506548344221645e-31), 'spearman': SpearmanrResult(correlation=0.45228381514068594, pvalue=1.228415935078933e-29), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4053217663202274, 'wmean': 0.48349193165718574}, 'spearman': {'mean': 0.40631900270211646, 'wmean': 0.4846257303555273}}}, 'STS14': {'deft-forum': {'pearson': (0.26831628065976243, 7.364926852983842e-09), 'spearman': SpearmanrResult(correlation=0.2818399555972234, pvalue=1.1583527259860693e-09), 'nsamples': 450}, 'deft-news': {'pearson': (0.7316638966853658, 1.5965123378303813e-51), 'spearman': SpearmanrResult(correlation=0.6968001583633905, pvalue=6.492753074790748e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.5147413343613683, 5.652850961350756e-52), 'spearman': SpearmanrResult(correlation=0.5052609711666017, pvalue=7.641125205506627e-50), 'nsamples': 750}, 'images': {'pearson': (0.5101957313502817, 6.058257632825197e-51), 'spearman': SpearmanrResult(correlation=0.5028512489852031, pvalue=2.595306777950729e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.5865029329115854, 1.663625197695137e-70), 'spearman': SpearmanrResult(correlation=0.6154436998951464, pvalue=2.2432027928608175e-79), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5452253300725397, 2.6647347557383176e-59), 'spearman': SpearmanrResult(correlation=0.5160015692485558, pvalue=2.910179238894131e-52), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5261075843401505, 'wmean': 0.5220641311531558}, 'spearman': {'mean': 0.5196996005426868, 'wmean': 0.5174763051998394}}}, 'STS15': {'answers-forums': {'pearson': (0.5206936579327429, 1.9111429487175984e-27), 'spearman': SpearmanrResult(correlation=0.49707316072898927, pvalue=8.499344020315735e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.6289226603732375, 7.867536099466943e-84), 'spearman': SpearmanrResult(correlation=0.6466679043335981, pvalue=4.83601841571731e-90), 'nsamples': 750}, 'belief': {'pearson': (0.5303497388095603, 1.373228249324763e-28), 'spearman': SpearmanrResult(correlation=0.5527743108035859, pvalue=2.1656860112368932e-31), 'nsamples': 375}, 'headlines': {'pearson': (0.5902756891682722, 1.3040862371913487e-71), 'spearman': SpearmanrResult(correlation=0.6074834600957196, pvalue=7.620768210098282e-77), 'nsamples': 750}, 'images': {'pearson': (0.613121866097179, 1.2491391346032388e-78), 'spearman': SpearmanrResult(correlation=0.630499799294326, pvalue=2.2915858919800782e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5766727224761984, 'wmean': 0.5894604785024601}, 'spearman': {'mean': 0.5868997270512437, 'wmean': 0.6023937248724829}}}, 'STS16': {'answer-answer': {'pearson': (0.479143677499656, 5.527746132792633e-16), 'spearman': SpearmanrResult(correlation=0.46958091654913614, pvalue=2.4640127449946897e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.6216915884852519, 5.057224542902244e-28), 'spearman': SpearmanrResult(correlation=0.6290513356249144, pvalue=7.727545033416447e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6655221297062986, 8.543172282983785e-31), 'spearman': SpearmanrResult(correlation=0.6662234870325296, pvalue=7.048399270054564e-31), 'nsamples': 230}, 'postediting': {'pearson': (0.7617280198395308, 1.62531509754797e-47), 'spearman': SpearmanrResult(correlation=0.793714391459464, pvalue=3.662429104390838e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.2846050535694989, 2.9601547569420517e-05), 'spearman': SpearmanrResult(correlation=0.31142251285988365, pvalue=4.435954342343632e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5625380938200473, 'wmean': 0.5690707272234905}, 'spearman': {'mean': 0.5739985287051856, 'wmean': 0.5800104166065692}}}, 'MR': {'devacc': 78.59, 'acc': 73.52, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 80.14, 'acc': 77.91, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.27, 'acc': 83.27, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.34, 'acc': 93.86, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.35, 'acc': 88.41, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.33, 'acc': 48.51, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 71.04, 'acc': 92.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.45, 'acc': 73.68, 'f1': 81.38, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.2, 'acc': 76.44, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7713320743398902, 'pearson': 0.7698864184660172, 'spearman': 0.7178182498329732, 'mse': 0.418181533729441, 'yhat': array([2.99566756, 4.81814357, 1.61661272, ..., 3.04528453, 4.17205056,        4.9056353 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6275222247124536, 'pearson': 0.6149408862262047, 'spearman': 0.6139125546456572, 'mse': 1.5150035544601526, 'yhat': array([1.55769093, 1.58410135, 3.51408734, ..., 3.88799711, 3.76204365,        3.07514913]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.88, 'acc': 65.8, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 56.98, 'acc': 57.83, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 11.23, 'acc': 11.4, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.83, 'acc': 28.86, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.38, 'acc': 59.62, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.94, 'acc': 89.75, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.62, 'acc': 87.68, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.77, 'acc': 80.33, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.21, 'acc': 76.65, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.59, 'acc': 67.38, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.07, 'acc': 72.9, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 18:06:09,467 : STS12 p=0.4460, STS12 s=0.4695, STS13 p=0.4835, STS13 s=0.4846, STS14 p=0.5221, STS14 s=0.5175, STS15 p=0.5895, STS15 s=0.6024, STS 16 p=0.5691, STS16 s=0.5800, STS B p=0.6149, STS B s=0.6139, STS B m=1.5150, SICK-R p=0.7699, SICK-R s=0.7178, SICK-P m=0.4182
2019-03-13 18:06:09,467 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 18:06:09,467 : 0.4460,0.4695,0.4835,0.4846,0.5221,0.5175,0.5895,0.6024,0.5691,0.5800,0.6149,0.6139,1.5150,0.7699,0.7178,0.4182
2019-03-13 18:06:09,468 : MR=73.52, CR=77.91, SUBJ=93.86, MPQA=83.27, SST-B=88.41, SST-F=48.51, TREC=92.80, SICK-E=76.44, SNLI=65.80, MRPC=73.68, MRPC f=81.38
2019-03-13 18:06:09,468 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 18:06:09,468 : 73.52,77.91,93.86,83.27,88.41,48.51,92.80,76.44,65.80,73.68,81.38
2019-03-13 18:06:09,468 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 18:06:09,468 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 18:06:09,468 : na,na,na,na,na,na,na,na,na,na
2019-03-13 18:06:09,468 : SentLen=57.83, WC=11.40, TreeDepth=28.86, TopConst=59.62, BShift=89.75, Tense=87.68, SubjNum=80.33, ObjNum=76.65, SOMO=67.38, CoordInv=72.90, average=63.24
2019-03-13 18:06:09,468 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 18:06:09,468 : 57.83,11.40,28.86,59.62,89.75,87.68,80.33,76.65,67.38,72.90,63.24
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,468 : layer 21
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,468 : ********************************************************************************
2019-03-13 18:06:09,571 : ***** Transfer task : STS12 *****


2019-03-13 18:06:09,584 : loading BERT model bert-large-uncased
2019-03-13 18:06:09,584 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:06:09,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:06:09,603 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3ohc_svl
2019-03-13 18:06:17,032 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:06:26,556 : MSRpar : pearson = 0.3257, spearman = 0.3550
2019-03-13 18:06:28,186 : MSRvid : pearson = 0.4612, spearman = 0.4870
2019-03-13 18:06:29,588 : SMTeuroparl : pearson = 0.5102, spearman = 0.5721
2019-03-13 18:06:32,265 : surprise.OnWN : pearson = 0.4629, spearman = 0.5100
2019-03-13 18:06:33,681 : surprise.SMTnews : pearson = 0.5898, spearman = 0.5493
2019-03-13 18:06:33,681 : ALL (weighted average) : Pearson = 0.4526,             Spearman = 0.4812
2019-03-13 18:06:33,681 : ALL (average) : Pearson = 0.4700,             Spearman = 0.4946

2019-03-13 18:06:33,681 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 18:06:33,690 : loading BERT model bert-large-uncased
2019-03-13 18:06:33,690 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:06:33,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:06:33,708 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm57stow2
2019-03-13 18:06:41,090 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:06:47,857 : FNWN : pearson = 0.2291, spearman = 0.2332
2019-03-13 18:06:49,738 : headlines : pearson = 0.5713, spearman = 0.5926
2019-03-13 18:06:51,197 : OnWN : pearson = 0.4546, spearman = 0.4433
2019-03-13 18:06:51,197 : ALL (weighted average) : Pearson = 0.4846,             Spearman = 0.4915
2019-03-13 18:06:51,197 : ALL (average) : Pearson = 0.4183,             Spearman = 0.4230

2019-03-13 18:06:51,197 : ***** Transfer task : STS14 *****


2019-03-13 18:06:51,212 : loading BERT model bert-large-uncased
2019-03-13 18:06:51,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:06:51,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:06:51,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqofs77e3
2019-03-13 18:06:58,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:07:05,528 : deft-forum : pearson = 0.2386, spearman = 0.2441
2019-03-13 18:07:07,153 : deft-news : pearson = 0.7412, spearman = 0.7001
2019-03-13 18:07:09,308 : headlines : pearson = 0.4946, spearman = 0.5030
2019-03-13 18:07:11,370 : images : pearson = 0.5086, spearman = 0.5013
2019-03-13 18:07:13,487 : OnWN : pearson = 0.5840, spearman = 0.6204
2019-03-13 18:07:16,322 : tweet-news : pearson = 0.5216, spearman = 0.5106
2019-03-13 18:07:16,322 : ALL (weighted average) : Pearson = 0.5097,             Spearman = 0.5124
2019-03-13 18:07:16,322 : ALL (average) : Pearson = 0.5147,             Spearman = 0.5132

2019-03-13 18:07:16,322 : ***** Transfer task : STS15 *****


2019-03-13 18:07:16,355 : loading BERT model bert-large-uncased
2019-03-13 18:07:16,355 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:07:16,373 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:07:16,373 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3tgq1rda
2019-03-13 18:07:23,806 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:07:31,051 : answers-forums : pearson = 0.5457, spearman = 0.5172
2019-03-13 18:07:33,121 : answers-students : pearson = 0.6074, spearman = 0.6283
2019-03-13 18:07:35,152 : belief : pearson = 0.5555, spearman = 0.5822
2019-03-13 18:07:37,388 : headlines : pearson = 0.5915, spearman = 0.6151
2019-03-13 18:07:39,507 : images : pearson = 0.6098, spearman = 0.6307
2019-03-13 18:07:39,507 : ALL (weighted average) : Pearson = 0.5898,             Spearman = 0.6059
2019-03-13 18:07:39,507 : ALL (average) : Pearson = 0.5820,             Spearman = 0.5947

2019-03-13 18:07:39,508 : ***** Transfer task : STS16 *****


2019-03-13 18:07:39,578 : loading BERT model bert-large-uncased
2019-03-13 18:07:39,578 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:07:39,596 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:07:39,596 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyfb8qguq
2019-03-13 18:07:47,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:07:53,433 : answer-answer : pearson = 0.4753, spearman = 0.4655
2019-03-13 18:07:54,090 : headlines : pearson = 0.6114, spearman = 0.6304
2019-03-13 18:07:54,966 : plagiarism : pearson = 0.6753, spearman = 0.6938
2019-03-13 18:07:56,448 : postediting : pearson = 0.7684, spearman = 0.7895
2019-03-13 18:07:57,051 : question-question : pearson = 0.3168, spearman = 0.3413
2019-03-13 18:07:57,052 : ALL (weighted average) : Pearson = 0.5751,             Spearman = 0.5892
2019-03-13 18:07:57,052 : ALL (average) : Pearson = 0.5695,             Spearman = 0.5841

2019-03-13 18:07:57,052 : ***** Transfer task : MR *****


2019-03-13 18:07:57,067 : loading BERT model bert-large-uncased
2019-03-13 18:07:57,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:07:57,088 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:07:57,088 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc1e7e_vu
2019-03-13 18:08:04,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:08:10,042 : Generating sentence embeddings
2019-03-13 18:08:41,248 : Generated sentence embeddings
2019-03-13 18:08:41,249 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:08:52,544 : Best param found at split 1: l2reg = 1e-05                 with score 80.9
2019-03-13 18:09:05,396 : Best param found at split 2: l2reg = 0.01                 with score 76.79
2019-03-13 18:09:19,583 : Best param found at split 3: l2reg = 0.001                 with score 81.3
2019-03-13 18:09:31,897 : Best param found at split 4: l2reg = 0.0001                 with score 80.79
2019-03-13 18:09:40,271 : Best param found at split 5: l2reg = 0.01                 with score 75.56
2019-03-13 18:09:40,721 : Dev acc : 79.07 Test acc : 74.69

2019-03-13 18:09:40,722 : ***** Transfer task : CR *****


2019-03-13 18:09:40,729 : loading BERT model bert-large-uncased
2019-03-13 18:09:40,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:09:40,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:09:40,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpthphab7q
2019-03-13 18:09:48,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:09:53,788 : Generating sentence embeddings
2019-03-13 18:10:02,041 : Generated sentence embeddings
2019-03-13 18:10:02,041 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:10:04,550 : Best param found at split 1: l2reg = 0.0001                 with score 81.02
2019-03-13 18:10:07,446 : Best param found at split 2: l2reg = 0.0001                 with score 78.01
2019-03-13 18:10:10,214 : Best param found at split 3: l2reg = 1e-05                 with score 81.85
2019-03-13 18:10:12,994 : Best param found at split 4: l2reg = 0.01                 with score 84.81
2019-03-13 18:10:16,586 : Best param found at split 5: l2reg = 1e-05                 with score 83.88
2019-03-13 18:10:16,777 : Dev acc : 81.91 Test acc : 82.41

2019-03-13 18:10:16,777 : ***** Transfer task : MPQA *****


2019-03-13 18:10:16,783 : loading BERT model bert-large-uncased
2019-03-13 18:10:16,783 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:10:16,832 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:10:16,832 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjsfxi78a
2019-03-13 18:10:24,214 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:10:29,733 : Generating sentence embeddings
2019-03-13 18:10:37,237 : Generated sentence embeddings
2019-03-13 18:10:37,237 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:10:48,130 : Best param found at split 1: l2reg = 0.001                 with score 84.85
2019-03-13 18:10:59,599 : Best param found at split 2: l2reg = 1e-05                 with score 84.87
2019-03-13 18:11:08,086 : Best param found at split 3: l2reg = 0.01                 with score 85.01
2019-03-13 18:11:17,213 : Best param found at split 4: l2reg = 1e-05                 with score 85.72
2019-03-13 18:11:29,014 : Best param found at split 5: l2reg = 1e-05                 with score 85.2
2019-03-13 18:11:29,545 : Dev acc : 85.13 Test acc : 85.56

2019-03-13 18:11:29,546 : ***** Transfer task : SUBJ *****


2019-03-13 18:11:29,563 : loading BERT model bert-large-uncased
2019-03-13 18:11:29,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:11:29,581 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:11:29,581 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkul_liu8
2019-03-13 18:11:37,020 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:11:42,525 : Generating sentence embeddings
2019-03-13 18:12:13,119 : Generated sentence embeddings
2019-03-13 18:12:13,119 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:12:23,430 : Best param found at split 1: l2reg = 0.001                 with score 94.64
2019-03-13 18:12:34,202 : Best param found at split 2: l2reg = 1e-05                 with score 94.45
2019-03-13 18:12:42,601 : Best param found at split 3: l2reg = 1e-05                 with score 94.22
2019-03-13 18:12:54,130 : Best param found at split 4: l2reg = 0.0001                 with score 94.8
2019-03-13 18:13:04,589 : Best param found at split 5: l2reg = 1e-05                 with score 94.45
2019-03-13 18:13:05,073 : Dev acc : 94.51 Test acc : 93.9

2019-03-13 18:13:05,074 : ***** Transfer task : SST Binary classification *****


2019-03-13 18:13:05,166 : loading BERT model bert-large-uncased
2019-03-13 18:13:05,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:13:05,240 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:13:05,240 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp312ed0d9
2019-03-13 18:13:12,668 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:13:18,107 : Computing embedding for train
2019-03-13 18:14:57,207 : Computed train embeddings
2019-03-13 18:14:57,207 : Computing embedding for dev
2019-03-13 18:14:59,361 : Computed dev embeddings
2019-03-13 18:14:59,361 : Computing embedding for test
2019-03-13 18:15:03,886 : Computed test embeddings
2019-03-13 18:15:03,887 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:15:20,964 : [('reg:1e-05', 85.89), ('reg:0.0001', 85.89), ('reg:0.001', 86.24), ('reg:0.01', 85.67)]
2019-03-13 18:15:20,964 : Validation : best param found is reg = 0.001 with score             86.24
2019-03-13 18:15:20,965 : Evaluating...
2019-03-13 18:15:25,294 : 
Dev acc : 86.24 Test acc : 87.7 for             SST Binary classification

2019-03-13 18:15:25,294 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 18:15:25,349 : loading BERT model bert-large-uncased
2019-03-13 18:15:25,349 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:15:25,369 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:15:25,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps16ly15b
2019-03-13 18:15:32,818 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:15:38,194 : Computing embedding for train
2019-03-13 18:15:59,901 : Computed train embeddings
2019-03-13 18:15:59,902 : Computing embedding for dev
2019-03-13 18:16:02,731 : Computed dev embeddings
2019-03-13 18:16:02,732 : Computing embedding for test
2019-03-13 18:16:08,314 : Computed test embeddings
2019-03-13 18:16:08,315 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:16:11,094 : [('reg:1e-05', 33.79), ('reg:0.0001', 43.78), ('reg:0.001', 44.41), ('reg:0.01', 41.05)]
2019-03-13 18:16:11,095 : Validation : best param found is reg = 0.001 with score             44.41
2019-03-13 18:16:11,095 : Evaluating...
2019-03-13 18:16:12,125 : 
Dev acc : 44.41 Test acc : 47.38 for             SST Fine-Grained classification

2019-03-13 18:16:12,126 : ***** Transfer task : TREC *****


2019-03-13 18:16:12,140 : loading BERT model bert-large-uncased
2019-03-13 18:16:12,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:16:12,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:16:12,160 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn22ivfqq
2019-03-13 18:16:19,579 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:16:32,480 : Computed train embeddings
2019-03-13 18:16:33,063 : Computed test embeddings
2019-03-13 18:16:33,063 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:16:40,185 : [('reg:1e-05', 67.85), ('reg:0.0001', 67.5), ('reg:0.001', 65.55), ('reg:0.01', 72.49)]
2019-03-13 18:16:40,186 : Cross-validation : best param found is reg = 0.01             with score 72.49
2019-03-13 18:16:40,186 : Evaluating...
2019-03-13 18:16:40,523 : 
Dev acc : 72.49 Test acc : 83.4             for TREC

2019-03-13 18:16:40,524 : ***** Transfer task : MRPC *****


2019-03-13 18:16:40,544 : loading BERT model bert-large-uncased
2019-03-13 18:16:40,545 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:16:40,568 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:16:40,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9c9gf3nl
2019-03-13 18:16:48,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:16:53,650 : Computing embedding for train
2019-03-13 18:17:15,700 : Computed train embeddings
2019-03-13 18:17:15,700 : Computing embedding for test
2019-03-13 18:17:25,346 : Computed test embeddings
2019-03-13 18:17:25,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:17:30,066 : [('reg:1e-05', 71.81), ('reg:0.0001', 71.88), ('reg:0.001', 72.64), ('reg:0.01', 71.49)]
2019-03-13 18:17:30,066 : Cross-validation : best param found is reg = 0.001             with score 72.64
2019-03-13 18:17:30,066 : Evaluating...
2019-03-13 18:17:30,392 : Dev acc : 72.64 Test acc 72.12; Test F1 78.44 for MRPC.

2019-03-13 18:17:30,392 : ***** Transfer task : SICK-Entailment*****


2019-03-13 18:17:30,454 : loading BERT model bert-large-uncased
2019-03-13 18:17:30,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:17:30,473 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:17:30,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi98qru44
2019-03-13 18:17:37,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:17:43,604 : Computing embedding for train
2019-03-13 18:17:54,790 : Computed train embeddings
2019-03-13 18:17:54,790 : Computing embedding for dev
2019-03-13 18:17:56,316 : Computed dev embeddings
2019-03-13 18:17:56,316 : Computing embedding for test
2019-03-13 18:18:08,321 : Computed test embeddings
2019-03-13 18:18:08,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:18:10,134 : [('reg:1e-05', 76.4), ('reg:0.0001', 77.2), ('reg:0.001', 77.6), ('reg:0.01', 74.6)]
2019-03-13 18:18:10,134 : Validation : best param found is reg = 0.001 with score             77.6
2019-03-13 18:18:10,134 : Evaluating...
2019-03-13 18:18:10,563 : 
Dev acc : 77.6 Test acc : 75.99 for                        SICK entailment

2019-03-13 18:18:10,564 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 18:18:10,590 : loading BERT model bert-large-uncased
2019-03-13 18:18:10,590 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:18:10,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:18:10,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpysvve5be
2019-03-13 18:18:18,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:18:23,548 : Computing embedding for train
2019-03-13 18:18:34,722 : Computed train embeddings
2019-03-13 18:18:34,722 : Computing embedding for dev
2019-03-13 18:18:36,246 : Computed dev embeddings
2019-03-13 18:18:36,246 : Computing embedding for test
2019-03-13 18:18:48,246 : Computed test embeddings
2019-03-13 18:19:01,413 : Dev : Pearson 0.6890957692519417
2019-03-13 18:19:01,414 : Test : Pearson 0.6818836145739536 Spearman 0.6867785329271796 MSE 0.573511531439032                        for SICK Relatedness

2019-03-13 18:19:01,414 : 

***** Transfer task : STSBenchmark*****


2019-03-13 18:19:01,453 : loading BERT model bert-large-uncased
2019-03-13 18:19:01,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:19:01,483 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:19:01,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2nhksq42
2019-03-13 18:19:08,927 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:19:14,425 : Computing embedding for train
2019-03-13 18:19:32,847 : Computed train embeddings
2019-03-13 18:19:32,847 : Computing embedding for dev
2019-03-13 18:19:38,425 : Computed dev embeddings
2019-03-13 18:19:38,426 : Computing embedding for test
2019-03-13 18:19:42,988 : Computed test embeddings
2019-03-13 18:20:01,651 : Dev : Pearson 0.6286972000206921
2019-03-13 18:20:01,652 : Test : Pearson 0.6257328389275618 Spearman 0.6203234700539413 MSE 1.4842437089718032                        for SICK Relatedness

2019-03-13 18:20:01,652 : ***** Transfer task : SNLI Entailment*****


2019-03-13 18:20:06,311 : loading BERT model bert-large-uncased
2019-03-13 18:20:06,311 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:20:07,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:20:07,303 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptr_2redr
2019-03-13 18:20:14,690 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:20:20,603 : PROGRESS (encoding): 0.00%
2019-03-13 18:23:04,501 : PROGRESS (encoding): 14.56%
2019-03-13 18:26:10,335 : PROGRESS (encoding): 29.12%
2019-03-13 18:29:16,885 : PROGRESS (encoding): 43.69%
2019-03-13 18:32:35,876 : PROGRESS (encoding): 58.25%
2019-03-13 18:36:17,592 : PROGRESS (encoding): 72.81%
2019-03-13 18:39:58,116 : PROGRESS (encoding): 87.37%
2019-03-13 18:43:56,939 : PROGRESS (encoding): 0.00%
2019-03-13 18:44:26,995 : PROGRESS (encoding): 0.00%
2019-03-13 18:44:55,854 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:46:13,592 : [('reg:1e-09', 68.1)]
2019-03-13 18:46:13,592 : Validation : best param found is reg = 1e-09 with score             68.1
2019-03-13 18:46:13,592 : Evaluating...
2019-03-13 18:47:24,974 : Dev acc : 68.1 Test acc : 68.41 for SNLI

2019-03-13 18:47:24,974 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 18:47:25,185 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 18:47:26,179 : loading BERT model bert-large-uncased
2019-03-13 18:47:26,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:47:26,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:47:26,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7evd_txc
2019-03-13 18:47:33,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:47:39,073 : Computing embeddings for train/dev/test
2019-03-13 18:51:07,288 : Computed embeddings
2019-03-13 18:51:07,288 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:51:29,166 : [('reg:1e-05', 58.07), ('reg:0.0001', 52.3), ('reg:0.001', 53.23), ('reg:0.01', 50.41)]
2019-03-13 18:51:29,166 : Validation : best param found is reg = 1e-05 with score             58.07
2019-03-13 18:51:29,166 : Evaluating...
2019-03-13 18:51:36,063 : 
Dev acc : 58.1 Test acc : 58.0 for LENGTH classification

2019-03-13 18:51:36,064 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 18:51:36,433 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 18:51:36,479 : loading BERT model bert-large-uncased
2019-03-13 18:51:36,479 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:51:36,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:51:36,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9adj7oa4
2019-03-13 18:51:43,996 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:51:49,437 : Computing embeddings for train/dev/test
2019-03-13 18:55:01,762 : Computed embeddings
2019-03-13 18:55:01,762 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:55:41,571 : [('reg:1e-05', 16.99), ('reg:0.0001', 5.37), ('reg:0.001', 0.55), ('reg:0.01', 0.21)]
2019-03-13 18:55:41,572 : Validation : best param found is reg = 1e-05 with score             16.99
2019-03-13 18:55:41,572 : Evaluating...
2019-03-13 18:55:54,245 : 
Dev acc : 17.0 Test acc : 16.7 for WORDCONTENT classification

2019-03-13 18:55:54,246 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 18:55:54,602 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 18:55:54,668 : loading BERT model bert-large-uncased
2019-03-13 18:55:54,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:55:54,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:55:54,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9qyummsj
2019-03-13 18:56:02,095 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:56:07,505 : Computing embeddings for train/dev/test
2019-03-13 18:59:07,945 : Computed embeddings
2019-03-13 18:59:07,945 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:59:33,357 : [('reg:1e-05', 25.99), ('reg:0.0001', 27.31), ('reg:0.001', 23.67), ('reg:0.01', 24.92)]
2019-03-13 18:59:33,357 : Validation : best param found is reg = 0.0001 with score             27.31
2019-03-13 18:59:33,358 : Evaluating...
2019-03-13 18:59:41,625 : 
Dev acc : 27.3 Test acc : 28.0 for DEPTH classification

2019-03-13 18:59:41,626 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 18:59:42,004 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 18:59:42,067 : loading BERT model bert-large-uncased
2019-03-13 18:59:42,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:59:42,175 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:59:42,175 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1v4op683
2019-03-13 18:59:49,639 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:59:55,026 : Computing embeddings for train/dev/test
2019-03-13 19:02:42,298 : Computed embeddings
2019-03-13 19:02:42,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:03:08,453 : [('reg:1e-05', 55.07), ('reg:0.0001', 55.16), ('reg:0.001', 55.78), ('reg:0.01', 43.1)]
2019-03-13 19:03:08,453 : Validation : best param found is reg = 0.001 with score             55.78
2019-03-13 19:03:08,453 : Evaluating...
2019-03-13 19:03:15,798 : 
Dev acc : 55.8 Test acc : 55.6 for TOPCONSTITUENTS classification

2019-03-13 19:03:15,799 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 19:03:16,146 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 19:03:16,212 : loading BERT model bert-large-uncased
2019-03-13 19:03:16,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:03:16,333 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:03:16,333 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpem73u90h
2019-03-13 19:03:23,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:03:29,298 : Computing embeddings for train/dev/test
2019-03-13 19:06:31,090 : Computed embeddings
2019-03-13 19:06:31,090 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:07:00,458 : [('reg:1e-05', 89.7), ('reg:0.0001', 89.59), ('reg:0.001', 89.95), ('reg:0.01', 89.57)]
2019-03-13 19:07:00,458 : Validation : best param found is reg = 0.001 with score             89.95
2019-03-13 19:07:00,458 : Evaluating...
2019-03-13 19:07:09,309 : 
Dev acc : 90.0 Test acc : 89.8 for BIGRAMSHIFT classification

2019-03-13 19:07:09,310 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 19:07:09,735 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 19:07:09,803 : loading BERT model bert-large-uncased
2019-03-13 19:07:09,803 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:07:09,836 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:07:09,836 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdwv2xne1
2019-03-13 19:07:17,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:07:22,770 : Computing embeddings for train/dev/test
2019-03-13 19:10:20,795 : Computed embeddings
2019-03-13 19:10:20,795 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:10:45,102 : [('reg:1e-05', 89.54), ('reg:0.0001', 89.52), ('reg:0.001', 89.72), ('reg:0.01', 89.84)]
2019-03-13 19:10:45,103 : Validation : best param found is reg = 0.01 with score             89.84
2019-03-13 19:10:45,103 : Evaluating...
2019-03-13 19:10:51,189 : 
Dev acc : 89.8 Test acc : 88.2 for TENSE classification

2019-03-13 19:10:51,191 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 19:10:51,599 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 19:10:51,662 : loading BERT model bert-large-uncased
2019-03-13 19:10:51,662 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:10:51,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:10:51,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpupx4ne51
2019-03-13 19:10:59,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:11:04,549 : Computing embeddings for train/dev/test
2019-03-13 19:14:12,993 : Computed embeddings
2019-03-13 19:14:12,993 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:14:45,526 : [('reg:1e-05', 80.69), ('reg:0.0001', 80.45), ('reg:0.001', 80.87), ('reg:0.01', 79.21)]
2019-03-13 19:14:45,526 : Validation : best param found is reg = 0.001 with score             80.87
2019-03-13 19:14:45,526 : Evaluating...
2019-03-13 19:14:53,209 : 
Dev acc : 80.9 Test acc : 79.5 for SUBJNUMBER classification

2019-03-13 19:14:53,210 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 19:14:53,618 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 19:14:53,685 : loading BERT model bert-large-uncased
2019-03-13 19:14:53,685 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:14:53,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:14:53,800 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4v2rmb27
2019-03-13 19:15:01,272 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:15:06,759 : Computing embeddings for train/dev/test
2019-03-13 19:18:11,803 : Computed embeddings
2019-03-13 19:18:11,803 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:18:38,716 : [('reg:1e-05', 75.11), ('reg:0.0001', 74.96), ('reg:0.001', 74.35), ('reg:0.01', 73.34)]
2019-03-13 19:18:38,717 : Validation : best param found is reg = 1e-05 with score             75.11
2019-03-13 19:18:38,717 : Evaluating...
2019-03-13 19:18:45,496 : 
Dev acc : 75.1 Test acc : 75.0 for OBJNUMBER classification

2019-03-13 19:18:45,497 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 19:18:45,876 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 19:18:45,944 : loading BERT model bert-large-uncased
2019-03-13 19:18:45,944 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:18:46,064 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:18:46,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpynfsm_fg
2019-03-13 19:18:53,510 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:18:58,830 : Computing embeddings for train/dev/test
2019-03-13 19:22:33,271 : Computed embeddings
2019-03-13 19:22:33,271 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:23:03,007 : [('reg:1e-05', 62.72), ('reg:0.0001', 62.51), ('reg:0.001', 61.58), ('reg:0.01', 66.86)]
2019-03-13 19:23:03,007 : Validation : best param found is reg = 0.01 with score             66.86
2019-03-13 19:23:03,007 : Evaluating...
2019-03-13 19:23:10,645 : 
Dev acc : 66.9 Test acc : 66.1 for ODDMANOUT classification

2019-03-13 19:23:10,646 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 19:23:11,236 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 19:23:11,312 : loading BERT model bert-large-uncased
2019-03-13 19:23:11,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:23:11,342 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:23:11,343 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpei2jqypl
2019-03-13 19:23:18,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:23:24,158 : Computing embeddings for train/dev/test
2019-03-13 19:26:56,567 : Computed embeddings
2019-03-13 19:26:56,567 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:27:29,071 : [('reg:1e-05', 68.46), ('reg:0.0001', 68.47), ('reg:0.001', 68.38), ('reg:0.01', 60.9)]
2019-03-13 19:27:29,071 : Validation : best param found is reg = 0.0001 with score             68.47
2019-03-13 19:27:29,071 : Evaluating...
2019-03-13 19:27:37,721 : 
Dev acc : 68.5 Test acc : 68.5 for COORDINATIONINVERSION classification

2019-03-13 19:27:37,723 : total results: {'STS12': {'MSRpar': {'pearson': (0.3256784805720273, 5.438246561939497e-20), 'spearman': SpearmanrResult(correlation=0.35496552236022355, pvalue=1.0861973058920258e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.461197503976637, 9.008371621416036e-41), 'spearman': SpearmanrResult(correlation=0.48699429431677665, pvalue=6.380537390029335e-46), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.5102116176407697, 8.678049729827421e-32), 'spearman': SpearmanrResult(correlation=0.5720645131515868, pvalue=2.990858155693151e-41), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4628598396572103, 4.3239821614305753e-41), 'spearman': SpearmanrResult(correlation=0.5099510436182996, pvalue=6.876328966717302e-51), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5898157756085676, 9.4015696988963e-39), 'spearman': SpearmanrResult(correlation=0.5492522609315226, pvalue=8.036571871085586e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4699526434910425, 'wmean': 0.45264655570120266}, 'spearman': {'mean': 0.49464552687568186, 'wmean': 0.4812298612836971}}}, 'STS13': {'FNWN': {'pearson': (0.22911257509201383, 0.0015182528810068093), 'spearman': SpearmanrResult(correlation=0.23320125468154093, pvalue=0.0012406694970208197), 'nsamples': 189}, 'headlines': {'pearson': (0.5713214116650984, 3.3665691113041173e-66), 'spearman': SpearmanrResult(correlation=0.592647878821317, pvalue=2.585536282696761e-72), 'nsamples': 750}, 'OnWN': {'pearson': (0.45461161777124376, 5.815297305319824e-30), 'spearman': SpearmanrResult(correlation=0.44326970013944555, pvalue=2.106804000660989e-28), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.41834853484278534, 'wmean': 0.4845536353405881}, 'spearman': {'mean': 0.4230396112141011, 'wmean': 0.4914901653526853}}}, 'STS14': {'deft-forum': {'pearson': (0.23855853097637986, 3.04515308421969e-07), 'spearman': SpearmanrResult(correlation=0.24413371739176717, pvalue=1.571239186536126e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.7412318266488078, 1.6020842369051018e-53), 'spearman': SpearmanrResult(correlation=0.7001218377808599, pvalue=1.675363603822094e-45), 'nsamples': 300}, 'headlines': {'pearson': (0.49457050997460805, 1.6107886760918342e-47), 'spearman': SpearmanrResult(correlation=0.5030199712833574, pvalue=2.383116337906945e-49), 'nsamples': 750}, 'images': {'pearson': (0.5085589442495009, 1.4108132435310974e-50), 'spearman': SpearmanrResult(correlation=0.5012535656147465, pvalue=5.8068398532407106e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.5839756240862956, 8.988705976927622e-70), 'spearman': SpearmanrResult(correlation=0.6203779035771481, pvalue=5.5618932570799225e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5215965095474401, 1.4759474729226678e-53), 'spearman': SpearmanrResult(correlation=0.5105898816968322, pvalue=4.9390533946338887e-51), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5147486575805054, 'wmean': 0.5096658874206391}, 'spearman': {'mean': 0.5132494795574519, 'wmean': 0.5123540575438977}}}, 'STS15': {'answers-forums': {'pearson': (0.5456959746186566, 1.749574591874347e-30), 'spearman': SpearmanrResult(correlation=0.5171577347345212, pvalue=4.908759295975242e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.6074259937515496, 7.943484773896443e-77), 'spearman': SpearmanrResult(correlation=0.6282968195588875, pvalue=1.2810333905320314e-83), 'nsamples': 750}, 'belief': {'pearson': (0.5555075174044946, 9.536981738527969e-32), 'spearman': SpearmanrResult(correlation=0.5822034405515488, pvalue=2.0934350218288601e-35), 'nsamples': 375}, 'headlines': {'pearson': (0.5914630726465331, 5.811329687396943e-72), 'spearman': SpearmanrResult(correlation=0.615098580779842, pvalue=2.89807764907919e-79), 'nsamples': 750}, 'images': {'pearson': (0.6098370775682005, 1.3838860832295445e-77), 'spearman': SpearmanrResult(correlation=0.6306865078045812, pvalue=1.9793019858904237e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5819859271978869, 'wmean': 0.5898319724944647}, 'spearman': {'mean': 0.5946886166858761, 'wmean': 0.6059406239465864}}}, 'STS16': {'answer-answer': {'pearson': (0.4753142513012475, 1.011377574940652e-15), 'spearman': SpearmanrResult(correlation=0.4654590600983089, pvalue=4.626334838523873e-15), 'nsamples': 254}, 'headlines': {'pearson': (0.611417562035878, 6.416682458682756e-27), 'spearman': SpearmanrResult(correlation=0.6303523278683585, pvalue=5.514908540641802e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6753308417899009, 5.527141594687715e-32), 'spearman': SpearmanrResult(correlation=0.6938091040735207, pvalue=2.3600057744226857e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7684329029193946, 8.069933748440691e-49), 'spearman': SpearmanrResult(correlation=0.7895143346221378, pvalue=3.1771456107095763e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.31683574947909415, 2.954615372524956e-06), 'spearman': SpearmanrResult(correlation=0.341293877264184, pvalue=4.2567003711566134e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5694662615051029, 'wmean': 0.5750549631893681}, 'spearman': {'mean': 0.584085740785302, 'wmean': 0.5891503733871143}}}, 'MR': {'devacc': 79.07, 'acc': 74.69, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.91, 'acc': 82.41, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.13, 'acc': 85.56, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.51, 'acc': 93.9, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.24, 'acc': 87.7, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.41, 'acc': 47.38, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 72.49, 'acc': 83.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.64, 'acc': 72.12, 'f1': 78.44, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.6, 'acc': 75.99, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6890957692519417, 'pearson': 0.6818836145739536, 'spearman': 0.6867785329271796, 'mse': 0.573511531439032, 'yhat': array([2.99963139, 4.58791155, 3.27468761, ..., 3.03035481, 4.6733492 ,        4.66885017]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6286972000206921, 'pearson': 0.6257328389275618, 'spearman': 0.6203234700539413, 'mse': 1.4842437089718032, 'yhat': array([1.45565865, 1.53217711, 2.71522203, ..., 3.68973447, 3.98032412,        3.42854649]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 68.1, 'acc': 68.41, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 58.07, 'acc': 57.97, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 16.99, 'acc': 16.71, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.31, 'acc': 27.96, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 55.78, 'acc': 55.62, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.95, 'acc': 89.82, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.84, 'acc': 88.18, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.87, 'acc': 79.45, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.11, 'acc': 75.05, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.86, 'acc': 66.1, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.47, 'acc': 68.53, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 19:27:37,723 : STS12 p=0.4526, STS12 s=0.4812, STS13 p=0.4846, STS13 s=0.4915, STS14 p=0.5097, STS14 s=0.5124, STS15 p=0.5898, STS15 s=0.6059, STS 16 p=0.5751, STS16 s=0.5892, STS B p=0.6257, STS B s=0.6203, STS B m=1.4842, SICK-R p=0.6819, SICK-R s=0.6868, SICK-P m=0.5735
2019-03-13 19:27:37,723 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 19:27:37,723 : 0.4526,0.4812,0.4846,0.4915,0.5097,0.5124,0.5898,0.6059,0.5751,0.5892,0.6257,0.6203,1.4842,0.6819,0.6868,0.5735
2019-03-13 19:27:37,723 : MR=74.69, CR=82.41, SUBJ=93.90, MPQA=85.56, SST-B=87.70, SST-F=47.38, TREC=83.40, SICK-E=75.99, SNLI=68.41, MRPC=72.12, MRPC f=78.44
2019-03-13 19:27:37,723 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 19:27:37,723 : 74.69,82.41,93.90,85.56,87.70,47.38,83.40,75.99,68.41,72.12,78.44
2019-03-13 19:27:37,723 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 19:27:37,723 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 19:27:37,723 : na,na,na,na,na,na,na,na,na,na
2019-03-13 19:27:37,723 : SentLen=57.97, WC=16.71, TreeDepth=27.96, TopConst=55.62, BShift=89.82, Tense=88.18, SubjNum=79.45, ObjNum=75.05, SOMO=66.10, CoordInv=68.53, average=62.54
2019-03-13 19:27:37,723 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 19:27:37,723 : 57.97,16.71,27.96,55.62,89.82,88.18,79.45,75.05,66.10,68.53,62.54
2019-03-13 19:27:37,723 : ********************************************************************************
2019-03-13 19:27:37,723 : ********************************************************************************
2019-03-13 19:27:37,723 : ********************************************************************************
2019-03-13 19:27:37,723 : layer 22
2019-03-13 19:27:37,723 : ********************************************************************************
2019-03-13 19:27:37,723 : ********************************************************************************
2019-03-13 19:27:37,724 : ********************************************************************************
2019-03-13 19:27:37,819 : ***** Transfer task : STS12 *****


2019-03-13 19:27:37,831 : loading BERT model bert-large-uncased
2019-03-13 19:27:37,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:27:37,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:27:37,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpay_l2ayn
2019-03-13 19:27:45,304 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:27:54,818 : MSRpar : pearson = 0.3246, spearman = 0.3630
2019-03-13 19:27:56,446 : MSRvid : pearson = 0.4600, spearman = 0.4930
2019-03-13 19:27:57,851 : SMTeuroparl : pearson = 0.4973, spearman = 0.5599
2019-03-13 19:28:00,529 : surprise.OnWN : pearson = 0.4490, spearman = 0.5089
2019-03-13 19:28:01,947 : surprise.SMTnews : pearson = 0.5816, spearman = 0.5587
2019-03-13 19:28:01,948 : ALL (weighted average) : Pearson = 0.4458,             Spearman = 0.4838
2019-03-13 19:28:01,948 : ALL (average) : Pearson = 0.4625,             Spearman = 0.4967

2019-03-13 19:28:01,948 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 19:28:01,958 : loading BERT model bert-large-uncased
2019-03-13 19:28:01,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:28:01,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:28:01,976 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwjazjucm
2019-03-13 19:28:09,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:28:16,142 : FNWN : pearson = 0.2275, spearman = 0.2277
2019-03-13 19:28:18,021 : headlines : pearson = 0.5487, spearman = 0.5945
2019-03-13 19:28:19,478 : OnWN : pearson = 0.4526, spearman = 0.4551
2019-03-13 19:28:19,478 : ALL (weighted average) : Pearson = 0.4723,             Spearman = 0.4962
2019-03-13 19:28:19,478 : ALL (average) : Pearson = 0.4096,             Spearman = 0.4258

2019-03-13 19:28:19,479 : ***** Transfer task : STS14 *****


2019-03-13 19:28:19,494 : loading BERT model bert-large-uncased
2019-03-13 19:28:19,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:28:19,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:28:19,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbclwyk0p
2019-03-13 19:28:26,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:28:34,011 : deft-forum : pearson = 0.2019, spearman = 0.2134
2019-03-13 19:28:35,635 : deft-news : pearson = 0.7387, spearman = 0.6877
2019-03-13 19:28:37,787 : headlines : pearson = 0.4779, spearman = 0.5134
2019-03-13 19:28:39,847 : images : pearson = 0.5015, spearman = 0.5094
2019-03-13 19:28:41,962 : OnWN : pearson = 0.5636, spearman = 0.6175
2019-03-13 19:28:44,800 : tweet-news : pearson = 0.4876, spearman = 0.4998
2019-03-13 19:28:44,800 : ALL (weighted average) : Pearson = 0.4894,             Spearman = 0.5086
2019-03-13 19:28:44,800 : ALL (average) : Pearson = 0.4952,             Spearman = 0.5069

2019-03-13 19:28:44,800 : ***** Transfer task : STS15 *****


2019-03-13 19:28:44,834 : loading BERT model bert-large-uncased
2019-03-13 19:28:44,834 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:28:44,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:28:44,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp496o80f0
2019-03-13 19:28:52,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:28:59,575 : answers-forums : pearson = 0.5648, spearman = 0.5304
2019-03-13 19:29:01,643 : answers-students : pearson = 0.5647, spearman = 0.6128
2019-03-13 19:29:03,680 : belief : pearson = 0.5626, spearman = 0.5906
2019-03-13 19:29:05,919 : headlines : pearson = 0.5809, spearman = 0.6110
2019-03-13 19:29:08,040 : images : pearson = 0.6126, spearman = 0.6284
2019-03-13 19:29:08,040 : ALL (weighted average) : Pearson = 0.5805,             Spearman = 0.6032
2019-03-13 19:29:08,040 : ALL (average) : Pearson = 0.5771,             Spearman = 0.5946

2019-03-13 19:29:08,040 : ***** Transfer task : STS16 *****


2019-03-13 19:29:08,113 : loading BERT model bert-large-uncased
2019-03-13 19:29:08,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:29:08,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:29:08,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp24kpvkh2
2019-03-13 19:29:15,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:29:21,957 : answer-answer : pearson = 0.4910, spearman = 0.4778
2019-03-13 19:29:22,611 : headlines : pearson = 0.5896, spearman = 0.6191
2019-03-13 19:29:23,485 : plagiarism : pearson = 0.6985, spearman = 0.7159
2019-03-13 19:29:24,969 : postediting : pearson = 0.7727, spearman = 0.7962
2019-03-13 19:29:25,572 : question-question : pearson = 0.3429, spearman = 0.3716
2019-03-13 19:29:25,572 : ALL (weighted average) : Pearson = 0.5838,             Spearman = 0.6004
2019-03-13 19:29:25,572 : ALL (average) : Pearson = 0.5790,             Spearman = 0.5961

2019-03-13 19:29:25,572 : ***** Transfer task : MR *****


2019-03-13 19:29:25,591 : loading BERT model bert-large-uncased
2019-03-13 19:29:25,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:29:25,609 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:29:25,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwess__b_
2019-03-13 19:29:33,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:29:38,445 : Generating sentence embeddings
2019-03-13 19:30:09,648 : Generated sentence embeddings
2019-03-13 19:30:09,648 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:30:20,464 : Best param found at split 1: l2reg = 1e-05                 with score 76.74
2019-03-13 19:30:33,838 : Best param found at split 2: l2reg = 0.0001                 with score 76.23
2019-03-13 19:30:43,540 : Best param found at split 3: l2reg = 0.001                 with score 77.09
2019-03-13 19:30:56,486 : Best param found at split 4: l2reg = 0.0001                 with score 79.31
2019-03-13 19:31:09,890 : Best param found at split 5: l2reg = 0.001                 with score 79.19
2019-03-13 19:31:10,515 : Dev acc : 77.71 Test acc : 76.21

2019-03-13 19:31:10,517 : ***** Transfer task : CR *****


2019-03-13 19:31:10,524 : loading BERT model bert-large-uncased
2019-03-13 19:31:10,524 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:31:10,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:31:10,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxcb3zqo5
2019-03-13 19:31:18,017 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:31:23,350 : Generating sentence embeddings
2019-03-13 19:31:31,591 : Generated sentence embeddings
2019-03-13 19:31:31,592 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:31:34,920 : Best param found at split 1: l2reg = 1e-05                 with score 82.71
2019-03-13 19:31:38,783 : Best param found at split 2: l2reg = 0.01                 with score 79.89
2019-03-13 19:31:42,341 : Best param found at split 3: l2reg = 0.01                 with score 86.09
2019-03-13 19:31:46,116 : Best param found at split 4: l2reg = 0.001                 with score 82.22
2019-03-13 19:31:50,140 : Best param found at split 5: l2reg = 1e-05                 with score 85.14
2019-03-13 19:31:50,301 : Dev acc : 83.21 Test acc : 82.35

2019-03-13 19:31:50,302 : ***** Transfer task : MPQA *****


2019-03-13 19:31:50,307 : loading BERT model bert-large-uncased
2019-03-13 19:31:50,307 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:31:50,327 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:31:50,327 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm33q7lmd
2019-03-13 19:31:57,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:32:03,092 : Generating sentence embeddings
2019-03-13 19:32:10,594 : Generated sentence embeddings
2019-03-13 19:32:10,594 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:32:21,148 : Best param found at split 1: l2reg = 0.01                 with score 84.9
2019-03-13 19:32:31,813 : Best param found at split 2: l2reg = 0.0001                 with score 84.04
2019-03-13 19:32:43,154 : Best param found at split 3: l2reg = 1e-05                 with score 84.87
2019-03-13 19:32:54,217 : Best param found at split 4: l2reg = 1e-05                 with score 84.16
2019-03-13 19:33:06,392 : Best param found at split 5: l2reg = 0.01                 with score 85.32
2019-03-13 19:33:07,100 : Dev acc : 84.66 Test acc : 83.34

2019-03-13 19:33:07,101 : ***** Transfer task : SUBJ *****


2019-03-13 19:33:07,117 : loading BERT model bert-large-uncased
2019-03-13 19:33:07,117 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:33:07,136 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:33:07,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpydvs31ha
2019-03-13 19:33:14,536 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:33:19,912 : Generating sentence embeddings
2019-03-13 19:33:50,521 : Generated sentence embeddings
2019-03-13 19:33:50,522 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:34:01,112 : Best param found at split 1: l2reg = 1e-05                 with score 94.14
2019-03-13 19:34:12,478 : Best param found at split 2: l2reg = 0.001                 with score 94.2
2019-03-13 19:34:25,279 : Best param found at split 3: l2reg = 0.0001                 with score 94.1
2019-03-13 19:34:36,130 : Best param found at split 4: l2reg = 0.001                 with score 94.68
2019-03-13 19:34:47,376 : Best param found at split 5: l2reg = 0.01                 with score 94.24
2019-03-13 19:34:48,207 : Dev acc : 94.27 Test acc : 93.44

2019-03-13 19:34:48,208 : ***** Transfer task : SST Binary classification *****


2019-03-13 19:34:48,335 : loading BERT model bert-large-uncased
2019-03-13 19:34:48,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:34:48,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:34:48,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg70zk8_2
2019-03-13 19:34:55,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:35:01,246 : Computing embedding for train
2019-03-13 19:36:40,480 : Computed train embeddings
2019-03-13 19:36:40,480 : Computing embedding for dev
2019-03-13 19:36:42,641 : Computed dev embeddings
2019-03-13 19:36:42,641 : Computing embedding for test
2019-03-13 19:36:47,178 : Computed test embeddings
2019-03-13 19:36:47,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:37:04,819 : [('reg:1e-05', 87.27), ('reg:0.0001', 87.73), ('reg:0.001', 87.61), ('reg:0.01', 87.04)]
2019-03-13 19:37:04,819 : Validation : best param found is reg = 0.0001 with score             87.73
2019-03-13 19:37:04,820 : Evaluating...
2019-03-13 19:37:09,179 : 
Dev acc : 87.73 Test acc : 88.41 for             SST Binary classification

2019-03-13 19:37:09,179 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 19:37:09,229 : loading BERT model bert-large-uncased
2019-03-13 19:37:09,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:37:09,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:37:09,252 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzv2xd9r8
2019-03-13 19:37:16,760 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:37:22,239 : Computing embedding for train
2019-03-13 19:37:43,934 : Computed train embeddings
2019-03-13 19:37:43,934 : Computing embedding for dev
2019-03-13 19:37:46,767 : Computed dev embeddings
2019-03-13 19:37:46,767 : Computing embedding for test
2019-03-13 19:37:52,349 : Computed test embeddings
2019-03-13 19:37:52,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:37:54,766 : [('reg:1e-05', 37.97), ('reg:0.0001', 37.6), ('reg:0.001', 39.42), ('reg:0.01', 37.69)]
2019-03-13 19:37:54,766 : Validation : best param found is reg = 0.001 with score             39.42
2019-03-13 19:37:54,766 : Evaluating...
2019-03-13 19:37:55,322 : 
Dev acc : 39.42 Test acc : 38.42 for             SST Fine-Grained classification

2019-03-13 19:37:55,323 : ***** Transfer task : TREC *****


2019-03-13 19:37:55,336 : loading BERT model bert-large-uncased
2019-03-13 19:37:55,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:37:55,355 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:37:55,355 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1inescp6
2019-03-13 19:38:02,779 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:38:15,596 : Computed train embeddings
2019-03-13 19:38:16,181 : Computed test embeddings
2019-03-13 19:38:16,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:38:21,886 : [('reg:1e-05', 61.17), ('reg:0.0001', 68.0), ('reg:0.001', 70.62), ('reg:0.01', 65.31)]
2019-03-13 19:38:21,886 : Cross-validation : best param found is reg = 0.001             with score 70.62
2019-03-13 19:38:21,886 : Evaluating...
2019-03-13 19:38:22,543 : 
Dev acc : 70.62 Test acc : 87.4             for TREC

2019-03-13 19:38:22,544 : ***** Transfer task : MRPC *****


2019-03-13 19:38:22,565 : loading BERT model bert-large-uncased
2019-03-13 19:38:22,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:38:22,586 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:38:22,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyzygi3np
2019-03-13 19:38:30,053 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:38:35,577 : Computing embedding for train
2019-03-13 19:38:57,625 : Computed train embeddings
2019-03-13 19:38:57,625 : Computing embedding for test
2019-03-13 19:39:07,281 : Computed test embeddings
2019-03-13 19:39:07,301 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:39:12,585 : [('reg:1e-05', 71.39), ('reg:0.0001', 72.2), ('reg:0.001', 71.27), ('reg:0.01', 71.79)]
2019-03-13 19:39:12,585 : Cross-validation : best param found is reg = 0.0001             with score 72.2
2019-03-13 19:39:12,585 : Evaluating...
2019-03-13 19:39:12,907 : Dev acc : 72.2 Test acc 72.35; Test F1 82.29 for MRPC.

2019-03-13 19:39:12,907 : ***** Transfer task : SICK-Entailment*****


2019-03-13 19:39:12,967 : loading BERT model bert-large-uncased
2019-03-13 19:39:12,968 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:39:12,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:39:12,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyp7mdb_v
2019-03-13 19:39:20,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:39:25,951 : Computing embedding for train
2019-03-13 19:39:37,145 : Computed train embeddings
2019-03-13 19:39:37,145 : Computing embedding for dev
2019-03-13 19:39:38,672 : Computed dev embeddings
2019-03-13 19:39:38,672 : Computing embedding for test
2019-03-13 19:39:50,676 : Computed test embeddings
2019-03-13 19:39:50,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:39:52,371 : [('reg:1e-05', 77.6), ('reg:0.0001', 75.6), ('reg:0.001', 76.2), ('reg:0.01', 74.2)]
2019-03-13 19:39:52,371 : Validation : best param found is reg = 1e-05 with score             77.6
2019-03-13 19:39:52,371 : Evaluating...
2019-03-13 19:39:52,788 : 
Dev acc : 77.6 Test acc : 76.76 for                        SICK entailment

2019-03-13 19:39:52,788 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 19:39:52,815 : loading BERT model bert-large-uncased
2019-03-13 19:39:52,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:39:52,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:39:52,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi1far4nj
2019-03-13 19:40:00,332 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:40:05,835 : Computing embedding for train
2019-03-13 19:40:17,027 : Computed train embeddings
2019-03-13 19:40:17,027 : Computing embedding for dev
2019-03-13 19:40:18,553 : Computed dev embeddings
2019-03-13 19:40:18,554 : Computing embedding for test
2019-03-13 19:40:30,553 : Computed test embeddings
2019-03-13 19:40:45,900 : Dev : Pearson 0.7617062224122393
2019-03-13 19:40:45,900 : Test : Pearson 0.7552263341052994 Spearman 0.6982933521908191 MSE 0.4386842381446104                        for SICK Relatedness

2019-03-13 19:40:45,901 : 

***** Transfer task : STSBenchmark*****


2019-03-13 19:40:45,940 : loading BERT model bert-large-uncased
2019-03-13 19:40:45,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:40:45,968 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:40:45,968 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_bnoqv18
2019-03-13 19:40:53,425 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:40:59,062 : Computing embedding for train
2019-03-13 19:41:17,471 : Computed train embeddings
2019-03-13 19:41:17,471 : Computing embedding for dev
2019-03-13 19:41:23,060 : Computed dev embeddings
2019-03-13 19:41:23,060 : Computing embedding for test
2019-03-13 19:41:27,619 : Computed test embeddings
2019-03-13 19:41:46,433 : Dev : Pearson 0.6298940796852758
2019-03-13 19:41:46,434 : Test : Pearson 0.6149009683339367 Spearman 0.6128604885927059 MSE 1.4848065160572221                        for SICK Relatedness

2019-03-13 19:41:46,434 : ***** Transfer task : SNLI Entailment*****


2019-03-13 19:41:51,586 : loading BERT model bert-large-uncased
2019-03-13 19:41:51,587 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:41:51,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:41:51,708 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuojc7u11
2019-03-13 19:41:59,166 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:42:05,133 : PROGRESS (encoding): 0.00%
2019-03-13 19:44:49,014 : PROGRESS (encoding): 14.56%
2019-03-13 19:47:55,032 : PROGRESS (encoding): 29.12%
2019-03-13 19:51:02,073 : PROGRESS (encoding): 43.69%
2019-03-13 19:54:21,555 : PROGRESS (encoding): 58.25%
2019-03-13 19:58:03,586 : PROGRESS (encoding): 72.81%
2019-03-13 20:01:44,459 : PROGRESS (encoding): 87.37%
2019-03-13 20:05:43,602 : PROGRESS (encoding): 0.00%
2019-03-13 20:06:13,627 : PROGRESS (encoding): 0.00%
2019-03-13 20:06:42,466 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:07:45,040 : [('reg:1e-09', 60.3)]
2019-03-13 20:07:45,040 : Validation : best param found is reg = 1e-09 with score             60.3
2019-03-13 20:07:45,041 : Evaluating...
2019-03-13 20:08:45,028 : Dev acc : 60.3 Test acc : 60.21 for SNLI

2019-03-13 20:08:45,029 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 20:08:45,237 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 20:08:46,301 : loading BERT model bert-large-uncased
2019-03-13 20:08:46,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:08:46,327 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:08:46,327 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm9836y52
2019-03-13 20:08:53,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:08:59,090 : Computing embeddings for train/dev/test
2019-03-13 20:12:27,478 : Computed embeddings
2019-03-13 20:12:27,478 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:12:53,573 : [('reg:1e-05', 52.72), ('reg:0.0001', 55.27), ('reg:0.001', 47.2), ('reg:0.01', 54.5)]
2019-03-13 20:12:53,573 : Validation : best param found is reg = 0.0001 with score             55.27
2019-03-13 20:12:53,573 : Evaluating...
2019-03-13 20:13:01,222 : 
Dev acc : 55.3 Test acc : 56.0 for LENGTH classification

2019-03-13 20:13:01,223 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 20:13:01,477 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 20:13:01,523 : loading BERT model bert-large-uncased
2019-03-13 20:13:01,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:13:01,550 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:13:01,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmqnd94x4
2019-03-13 20:13:08,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:13:14,341 : Computing embeddings for train/dev/test
2019-03-13 20:16:26,589 : Computed embeddings
2019-03-13 20:16:26,589 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:17:02,213 : [('reg:1e-05', 14.82), ('reg:0.0001', 6.44), ('reg:0.001', 0.54), ('reg:0.01', 0.1)]
2019-03-13 20:17:02,213 : Validation : best param found is reg = 1e-05 with score             14.82
2019-03-13 20:17:02,213 : Evaluating...
2019-03-13 20:17:14,320 : 
Dev acc : 14.8 Test acc : 14.9 for WORDCONTENT classification

2019-03-13 20:17:14,322 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 20:17:14,863 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 20:17:14,932 : loading BERT model bert-large-uncased
2019-03-13 20:17:14,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:17:14,958 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:17:14,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiu4djvh8
2019-03-13 20:17:22,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:17:27,879 : Computing embeddings for train/dev/test
2019-03-13 20:20:28,705 : Computed embeddings
2019-03-13 20:20:28,706 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:20:52,392 : [('reg:1e-05', 21.66), ('reg:0.0001', 26.54), ('reg:0.001', 27.35), ('reg:0.01', 24.19)]
2019-03-13 20:20:52,392 : Validation : best param found is reg = 0.001 with score             27.35
2019-03-13 20:20:52,392 : Evaluating...
2019-03-13 20:21:01,128 : 
Dev acc : 27.4 Test acc : 26.3 for DEPTH classification

2019-03-13 20:21:01,129 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 20:21:01,536 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 20:21:01,598 : loading BERT model bert-large-uncased
2019-03-13 20:21:01,598 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:21:01,628 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:21:01,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbw1hmwqq
2019-03-13 20:21:09,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:21:14,651 : Computing embeddings for train/dev/test
2019-03-13 20:24:02,296 : Computed embeddings
2019-03-13 20:24:02,297 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:24:30,304 : [('reg:1e-05', 54.34), ('reg:0.0001', 48.05), ('reg:0.001', 45.98), ('reg:0.01', 34.09)]
2019-03-13 20:24:30,305 : Validation : best param found is reg = 1e-05 with score             54.34
2019-03-13 20:24:30,305 : Evaluating...
2019-03-13 20:24:35,818 : 
Dev acc : 54.3 Test acc : 54.0 for TOPCONSTITUENTS classification

2019-03-13 20:24:35,819 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 20:24:36,174 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 20:24:36,240 : loading BERT model bert-large-uncased
2019-03-13 20:24:36,240 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:24:36,270 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:24:36,270 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcxzm7sf_
2019-03-13 20:24:43,702 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:24:49,201 : Computing embeddings for train/dev/test
2019-03-13 20:27:51,213 : Computed embeddings
2019-03-13 20:27:51,213 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:28:17,015 : [('reg:1e-05', 89.2), ('reg:0.0001', 89.2), ('reg:0.001', 89.27), ('reg:0.01', 88.25)]
2019-03-13 20:28:17,016 : Validation : best param found is reg = 0.001 with score             89.27
2019-03-13 20:28:17,016 : Evaluating...
2019-03-13 20:28:22,143 : 
Dev acc : 89.3 Test acc : 88.8 for BIGRAMSHIFT classification

2019-03-13 20:28:22,144 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 20:28:22,533 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 20:28:22,598 : loading BERT model bert-large-uncased
2019-03-13 20:28:22,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:28:22,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:28:22,626 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiuit0p_t
2019-03-13 20:28:30,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:28:35,628 : Computing embeddings for train/dev/test
2019-03-13 20:31:33,863 : Computed embeddings
2019-03-13 20:31:33,863 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:31:57,197 : [('reg:1e-05', 89.66), ('reg:0.0001', 89.66), ('reg:0.001', 89.73), ('reg:0.01', 89.93)]
2019-03-13 20:31:57,197 : Validation : best param found is reg = 0.01 with score             89.93
2019-03-13 20:31:57,197 : Evaluating...
2019-03-13 20:32:03,788 : 
Dev acc : 89.9 Test acc : 88.1 for TENSE classification

2019-03-13 20:32:03,789 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 20:32:04,192 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 20:32:04,261 : loading BERT model bert-large-uncased
2019-03-13 20:32:04,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:32:04,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:32:04,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuupdq2e_
2019-03-13 20:32:11,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:32:17,253 : Computing embeddings for train/dev/test
2019-03-13 20:35:25,610 : Computed embeddings
2019-03-13 20:35:25,610 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:35:55,474 : [('reg:1e-05', 80.37), ('reg:0.0001', 80.43), ('reg:0.001', 80.04), ('reg:0.01', 76.29)]
2019-03-13 20:35:55,474 : Validation : best param found is reg = 0.0001 with score             80.43
2019-03-13 20:35:55,474 : Evaluating...
2019-03-13 20:36:03,207 : 
Dev acc : 80.4 Test acc : 78.2 for SUBJNUMBER classification

2019-03-13 20:36:03,208 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 20:36:03,831 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 20:36:03,901 : loading BERT model bert-large-uncased
2019-03-13 20:36:03,901 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:36:03,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:36:03,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxezor4ts
2019-03-13 20:36:11,368 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:36:16,849 : Computing embeddings for train/dev/test
2019-03-13 20:39:21,830 : Computed embeddings
2019-03-13 20:39:21,830 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:39:48,244 : [('reg:1e-05', 76.01), ('reg:0.0001', 76.03), ('reg:0.001', 76.04), ('reg:0.01', 74.96)]
2019-03-13 20:39:48,244 : Validation : best param found is reg = 0.001 with score             76.04
2019-03-13 20:39:48,244 : Evaluating...
2019-03-13 20:39:54,812 : 
Dev acc : 76.0 Test acc : 76.8 for OBJNUMBER classification

2019-03-13 20:39:54,813 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 20:39:55,189 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 20:39:55,257 : loading BERT model bert-large-uncased
2019-03-13 20:39:55,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:39:55,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:39:55,380 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqztp1jwh
2019-03-13 20:40:02,875 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:40:08,308 : Computing embeddings for train/dev/test
2019-03-13 20:43:42,789 : Computed embeddings
2019-03-13 20:43:42,789 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:44:02,935 : [('reg:1e-05', 62.49), ('reg:0.0001', 62.48), ('reg:0.001', 62.41), ('reg:0.01', 63.6)]
2019-03-13 20:44:02,935 : Validation : best param found is reg = 0.01 with score             63.6
2019-03-13 20:44:02,935 : Evaluating...
2019-03-13 20:44:10,489 : 
Dev acc : 63.6 Test acc : 63.1 for ODDMANOUT classification

2019-03-13 20:44:10,490 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 20:44:10,934 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 20:44:11,010 : loading BERT model bert-large-uncased
2019-03-13 20:44:11,010 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:44:11,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:44:11,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq1xuhzp3
2019-03-13 20:44:18,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:44:23,849 : Computing embeddings for train/dev/test
2019-03-13 20:47:56,308 : Computed embeddings
2019-03-13 20:47:56,309 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:48:27,301 : [('reg:1e-05', 65.24), ('reg:0.0001', 65.27), ('reg:0.001', 66.16), ('reg:0.01', 64.26)]
2019-03-13 20:48:27,302 : Validation : best param found is reg = 0.001 with score             66.16
2019-03-13 20:48:27,302 : Evaluating...
2019-03-13 20:48:33,938 : 
Dev acc : 66.2 Test acc : 66.3 for COORDINATIONINVERSION classification

2019-03-13 20:48:33,940 : total results: {'STS12': {'MSRpar': {'pearson': (0.32460186683777487, 7.311724750183184e-20), 'spearman': SpearmanrResult(correlation=0.3629597929313601, pvalue=9.042896916055453e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.46001655660991814, 1.5136847766892447e-40), 'spearman': SpearmanrResult(correlation=0.4929677793156411, pvalue=3.5351004597691923e-47), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4972734291014601, 4.8188764346262874e-30), 'spearman': SpearmanrResult(correlation=0.5599238454828692, pvalue=3.103163085714679e-39), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.44900867184120874, 1.7338660093332802e-38), 'spearman': SpearmanrResult(correlation=0.5089349377025357, pvalue=1.1622853183773492e-50), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5815805931622857, 1.7584572701988356e-37), 'spearman': SpearmanrResult(correlation=0.5586554114710761, pvalue=3.999925716099729e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.46249622351052955, 'wmean': 0.44579133915572666}, 'spearman': {'mean': 0.4966883533806965, 'wmean': 0.4837694455327379}}}, 'STS13': {'FNWN': {'pearson': (0.22753736118642467, 0.0016395538044222298), 'spearman': SpearmanrResult(correlation=0.2277274149066266, pvalue=0.0016244632909771417), 'nsamples': 189}, 'headlines': {'pearson': (0.5486673529352272, 3.550835023105488e-60), 'spearman': SpearmanrResult(correlation=0.5945483602845936, pvalue=7.003435104269339e-73), 'nsamples': 750}, 'OnWN': {'pearson': (0.4525507133794105, 1.1278072020805116e-29), 'spearman': SpearmanrResult(correlation=0.45505267230995017, pvalue=5.043753262419663e-30), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.4095851425003541, 'wmean': 0.47225735078100267}, 'spearman': {'mean': 0.4257761491670568, 'wmean': 0.4961575338644531}}}, 'STS14': {'deft-forum': {'pearson': (0.2018635462596081, 1.5977628633772593e-05), 'spearman': SpearmanrResult(correlation=0.21336369868280383, pvalue=4.96587388855349e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.7386905350925393, 5.546832383131323e-53), 'spearman': SpearmanrResult(correlation=0.6876739977392617, pvalue=2.443962415955576e-43), 'nsamples': 300}, 'headlines': {'pearson': (0.47792042134692686, 4.640605268424502e-44), 'spearman': SpearmanrResult(correlation=0.5134400818776998, pvalue=1.1187563775961149e-51), 'nsamples': 750}, 'images': {'pearson': (0.501505121911211, 5.116748150557744e-49), 'spearman': SpearmanrResult(correlation=0.5094256061891889, pvalue=9.022580519852012e-51), 'nsamples': 750}, 'OnWN': {'pearson': (0.563571752828647, 4.366458403082578e-64), 'spearman': SpearmanrResult(correlation=0.6174655080259663, pvalue=4.970519181104432e-80), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4876182428793398, 4.729061012450004e-46), 'spearman': SpearmanrResult(correlation=0.49978317256290594, pvalue=1.2139303926706075e-48), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.495194936719712, 'wmean': 0.48944197615178103}, 'spearman': {'mean': 0.5068586775129711, 'wmean': 0.5086404373922296}}}, 'STS15': {'answers-forums': {'pearson': (0.5647727048228541, 5.5872705439778894e-33), 'spearman': SpearmanrResult(correlation=0.5303760577501618, pvalue=1.3632497732987428e-28), 'nsamples': 375}, 'answers-students': {'pearson': (0.5646901235991251, 2.1809818349213886e-64), 'spearman': SpearmanrResult(correlation=0.6128316871551535, pvalue=1.5465947542024043e-78), 'nsamples': 750}, 'belief': {'pearson': (0.562608873137, 1.092570758668332e-32), 'spearman': SpearmanrResult(correlation=0.5905841140261642, pvalue=1.2630042023314533e-36), 'nsamples': 375}, 'headlines': {'pearson': (0.580946786848246, 6.657362377302193e-69), 'spearman': SpearmanrResult(correlation=0.6109635842012079, pvalue=6.085243561089289e-78), 'nsamples': 750}, 'images': {'pearson': (0.612595411142919, 1.8400901776703816e-78), 'spearman': SpearmanrResult(correlation=0.6284161123815173, pvalue=1.1674576064076142e-83), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5771227799100289, 'wmean': 0.5804807776425543}, 'spearman': {'mean': 0.5946343111028408, 'wmean': 0.6031728674065104}}}, 'STS16': {'answer-answer': {'pearson': (0.4910095706002252, 8.099137951251729e-17), 'spearman': SpearmanrResult(correlation=0.47776813511530575, pvalue=6.873344243943233e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.5896027343044592, 1.0501981898288638e-24), 'spearman': SpearmanrResult(correlation=0.6190911298794116, pvalue=9.706068605323506e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.698519259071141, 5.495205403382361e-35), 'spearman': SpearmanrResult(correlation=0.7158773973131853, pvalue=1.981398610066273e-37), 'nsamples': 230}, 'postediting': {'pearson': (0.7727137523555387, 1.1245962221895603e-49), 'spearman': SpearmanrResult(correlation=0.7961859988143637, pvalue=1.0031622338473522e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.34293677289368996, 3.714557193602757e-07), 'spearman': SpearmanrResult(correlation=0.37158965161475255, pvalue=3.0347782258031374e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5789564178450108, 'wmean': 0.58381356026152}, 'spearman': {'mean': 0.5961024625474038, 'wmean': 0.6004133389034416}}}, 'MR': {'devacc': 77.71, 'acc': 76.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 83.21, 'acc': 82.35, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.66, 'acc': 83.34, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.27, 'acc': 93.44, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.73, 'acc': 88.41, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.42, 'acc': 38.42, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.62, 'acc': 87.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.2, 'acc': 72.35, 'f1': 82.29, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.6, 'acc': 76.76, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7617062224122393, 'pearson': 0.7552263341052994, 'spearman': 0.6982933521908191, 'mse': 0.4386842381446104, 'yhat': array([3.06978678, 4.34176076, 1.31083949, ..., 2.86844096, 4.45860194,        4.15745391]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6298940796852758, 'pearson': 0.6149009683339367, 'spearman': 0.6128604885927059, 'mse': 1.4848065160572221, 'yhat': array([2.11017343, 1.50114014, 2.2512911 , ..., 3.96025346, 4.06116511,        3.25084604]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.3, 'acc': 60.21, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 55.27, 'acc': 56.02, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 14.82, 'acc': 14.9, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.35, 'acc': 26.28, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.34, 'acc': 54.0, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.27, 'acc': 88.79, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.93, 'acc': 88.14, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.43, 'acc': 78.18, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.04, 'acc': 76.76, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.6, 'acc': 63.12, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.16, 'acc': 66.35, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 20:48:33,940 : STS12 p=0.4458, STS12 s=0.4838, STS13 p=0.4723, STS13 s=0.4962, STS14 p=0.4894, STS14 s=0.5086, STS15 p=0.5805, STS15 s=0.6032, STS 16 p=0.5838, STS16 s=0.6004, STS B p=0.6149, STS B s=0.6129, STS B m=1.4848, SICK-R p=0.7552, SICK-R s=0.6983, SICK-P m=0.4387
2019-03-13 20:48:33,940 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 20:48:33,941 : 0.4458,0.4838,0.4723,0.4962,0.4894,0.5086,0.5805,0.6032,0.5838,0.6004,0.6149,0.6129,1.4848,0.7552,0.6983,0.4387
2019-03-13 20:48:33,941 : MR=76.21, CR=82.35, SUBJ=93.44, MPQA=83.34, SST-B=88.41, SST-F=38.42, TREC=87.40, SICK-E=76.76, SNLI=60.21, MRPC=72.35, MRPC f=82.29
2019-03-13 20:48:33,941 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 20:48:33,941 : 76.21,82.35,93.44,83.34,88.41,38.42,87.40,76.76,60.21,72.35,82.29
2019-03-13 20:48:33,941 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 20:48:33,941 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 20:48:33,941 : na,na,na,na,na,na,na,na,na,na
2019-03-13 20:48:33,941 : SentLen=56.02, WC=14.90, TreeDepth=26.28, TopConst=54.00, BShift=88.79, Tense=88.14, SubjNum=78.18, ObjNum=76.76, SOMO=63.12, CoordInv=66.35, average=61.25
2019-03-13 20:48:33,941 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 20:48:33,941 : 56.02,14.90,26.28,54.00,88.79,88.14,78.18,76.76,63.12,66.35,61.25
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:33,941 : layer 23
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:33,941 : ********************************************************************************
2019-03-13 20:48:34,033 : ***** Transfer task : STS12 *****


2019-03-13 20:48:34,045 : loading BERT model bert-large-uncased
2019-03-13 20:48:34,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:48:34,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:48:34,063 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7x93ci15
2019-03-13 20:48:41,511 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:48:50,989 : MSRpar : pearson = 0.3223, spearman = 0.3695
2019-03-13 20:48:52,619 : MSRvid : pearson = 0.4375, spearman = 0.4959
2019-03-13 20:48:54,020 : SMTeuroparl : pearson = 0.4749, spearman = 0.5292
2019-03-13 20:48:56,699 : surprise.OnWN : pearson = 0.4141, spearman = 0.5020
2019-03-13 20:48:58,116 : surprise.SMTnews : pearson = 0.5767, spearman = 0.5758
2019-03-13 20:48:58,116 : ALL (weighted average) : Pearson = 0.4274,             Spearman = 0.4821
2019-03-13 20:48:58,116 : ALL (average) : Pearson = 0.4451,             Spearman = 0.4945

2019-03-13 20:48:58,116 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 20:48:58,125 : loading BERT model bert-large-uncased
2019-03-13 20:48:58,125 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:48:58,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:48:58,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmlsu9dx8
2019-03-13 20:49:05,580 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:49:12,181 : FNWN : pearson = 0.2311, spearman = 0.2264
2019-03-13 20:49:14,061 : headlines : pearson = 0.5041, spearman = 0.5848
2019-03-13 20:49:15,517 : OnWN : pearson = 0.4004, spearman = 0.4223
2019-03-13 20:49:15,517 : ALL (weighted average) : Pearson = 0.4309,             Spearman = 0.4788
2019-03-13 20:49:15,517 : ALL (average) : Pearson = 0.3785,             Spearman = 0.4111

2019-03-13 20:49:15,517 : ***** Transfer task : STS14 *****


2019-03-13 20:49:15,532 : loading BERT model bert-large-uncased
2019-03-13 20:49:15,532 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:49:15,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:49:15,550 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcj95srha
2019-03-13 20:49:23,054 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:49:29,924 : deft-forum : pearson = 0.1348, spearman = 0.1724
2019-03-13 20:49:31,549 : deft-news : pearson = 0.7247, spearman = 0.6743
2019-03-13 20:49:33,702 : headlines : pearson = 0.4433, spearman = 0.5206
2019-03-13 20:49:35,763 : images : pearson = 0.4489, spearman = 0.5004
2019-03-13 20:49:37,878 : OnWN : pearson = 0.4886, spearman = 0.5883
2019-03-13 20:49:40,720 : tweet-news : pearson = 0.4424, spearman = 0.4911
2019-03-13 20:49:40,721 : ALL (weighted average) : Pearson = 0.4388,             Spearman = 0.4947
2019-03-13 20:49:40,721 : ALL (average) : Pearson = 0.4471,             Spearman = 0.4912

2019-03-13 20:49:40,721 : ***** Transfer task : STS15 *****


2019-03-13 20:49:40,752 : loading BERT model bert-large-uncased
2019-03-13 20:49:40,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:49:40,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:49:40,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpebn3rmwy
2019-03-13 20:49:48,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:49:55,551 : answers-forums : pearson = 0.5515, spearman = 0.5397
2019-03-13 20:49:57,623 : answers-students : pearson = 0.4730, spearman = 0.5696
2019-03-13 20:49:59,655 : belief : pearson = 0.5351, spearman = 0.5698
2019-03-13 20:50:01,893 : headlines : pearson = 0.5473, spearman = 0.6027
2019-03-13 20:50:04,017 : images : pearson = 0.6090, spearman = 0.6248
2019-03-13 20:50:04,017 : ALL (weighted average) : Pearson = 0.5432,             Spearman = 0.5880
2019-03-13 20:50:04,018 : ALL (average) : Pearson = 0.5432,             Spearman = 0.5813

2019-03-13 20:50:04,018 : ***** Transfer task : STS16 *****


2019-03-13 20:50:04,088 : loading BERT model bert-large-uncased
2019-03-13 20:50:04,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:50:04,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:50:04,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8d9r5ryr
2019-03-13 20:50:11,560 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:50:17,874 : answer-answer : pearson = 0.4899, spearman = 0.4877
2019-03-13 20:50:18,530 : headlines : pearson = 0.5582, spearman = 0.6169
2019-03-13 20:50:19,407 : plagiarism : pearson = 0.7140, spearman = 0.7320
2019-03-13 20:50:20,887 : postediting : pearson = 0.7738, spearman = 0.7949
2019-03-13 20:50:21,489 : question-question : pearson = 0.3059, spearman = 0.3475
2019-03-13 20:50:21,489 : ALL (weighted average) : Pearson = 0.5737,             Spearman = 0.6007
2019-03-13 20:50:21,489 : ALL (average) : Pearson = 0.5684,             Spearman = 0.5958

2019-03-13 20:50:21,489 : ***** Transfer task : MR *****


2019-03-13 20:50:21,504 : loading BERT model bert-large-uncased
2019-03-13 20:50:21,504 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:50:21,525 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:50:21,525 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9c8fv53_
2019-03-13 20:50:28,944 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:50:34,374 : Generating sentence embeddings
2019-03-13 20:51:05,593 : Generated sentence embeddings
2019-03-13 20:51:05,593 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:51:17,637 : Best param found at split 1: l2reg = 1e-05                 with score 78.72
2019-03-13 20:51:31,171 : Best param found at split 2: l2reg = 1e-05                 with score 77.09
2019-03-13 20:51:42,278 : Best param found at split 3: l2reg = 1e-05                 with score 79.91
2019-03-13 20:51:54,869 : Best param found at split 4: l2reg = 1e-05                 with score 79.24
2019-03-13 20:52:07,408 : Best param found at split 5: l2reg = 1e-05                 with score 78.07
2019-03-13 20:52:08,030 : Dev acc : 78.61 Test acc : 75.87

2019-03-13 20:52:08,031 : ***** Transfer task : CR *****


2019-03-13 20:52:08,039 : loading BERT model bert-large-uncased
2019-03-13 20:52:08,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:52:08,059 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:52:08,060 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphyvpz83_
2019-03-13 20:52:15,513 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:52:20,976 : Generating sentence embeddings
2019-03-13 20:52:29,215 : Generated sentence embeddings
2019-03-13 20:52:29,215 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:52:32,781 : Best param found at split 1: l2reg = 0.01                 with score 79.1
2019-03-13 20:52:36,614 : Best param found at split 2: l2reg = 1e-05                 with score 82.84
2019-03-13 20:52:40,450 : Best param found at split 3: l2reg = 1e-05                 with score 84.3
2019-03-13 20:52:44,125 : Best param found at split 4: l2reg = 1e-05                 with score 80.77
2019-03-13 20:52:47,668 : Best param found at split 5: l2reg = 0.01                 with score 80.77
2019-03-13 20:52:47,812 : Dev acc : 81.56 Test acc : 79.23

2019-03-13 20:52:47,812 : ***** Transfer task : MPQA *****


2019-03-13 20:52:47,817 : loading BERT model bert-large-uncased
2019-03-13 20:52:47,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:52:47,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:52:47,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0ls83cux
2019-03-13 20:52:55,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:53:00,764 : Generating sentence embeddings
2019-03-13 20:53:08,272 : Generated sentence embeddings
2019-03-13 20:53:08,273 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:53:18,500 : Best param found at split 1: l2reg = 0.0001                 with score 84.55
2019-03-13 20:53:29,418 : Best param found at split 2: l2reg = 1e-05                 with score 83.52
2019-03-13 20:53:40,621 : Best param found at split 3: l2reg = 1e-05                 with score 84.09
2019-03-13 20:53:51,822 : Best param found at split 4: l2reg = 0.0001                 with score 82.89
2019-03-13 20:54:03,165 : Best param found at split 5: l2reg = 1e-05                 with score 84.23
2019-03-13 20:54:03,695 : Dev acc : 83.86 Test acc : 83.34

2019-03-13 20:54:03,696 : ***** Transfer task : SUBJ *****


2019-03-13 20:54:03,713 : loading BERT model bert-large-uncased
2019-03-13 20:54:03,713 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:54:03,732 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:54:03,733 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa7t_yv51
2019-03-13 20:54:11,123 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:54:16,613 : Generating sentence embeddings
2019-03-13 20:54:47,228 : Generated sentence embeddings
2019-03-13 20:54:47,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 20:54:58,737 : Best param found at split 1: l2reg = 0.001                 with score 94.22
2019-03-13 20:55:09,239 : Best param found at split 2: l2reg = 0.01                 with score 93.97
2019-03-13 20:55:21,635 : Best param found at split 3: l2reg = 0.001                 with score 93.91
2019-03-13 20:55:33,703 : Best param found at split 4: l2reg = 0.01                 with score 94.46
2019-03-13 20:55:44,547 : Best param found at split 5: l2reg = 0.001                 with score 94.25
2019-03-13 20:55:45,389 : Dev acc : 94.16 Test acc : 93.54

2019-03-13 20:55:45,390 : ***** Transfer task : SST Binary classification *****


2019-03-13 20:55:45,483 : loading BERT model bert-large-uncased
2019-03-13 20:55:45,483 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:55:45,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:55:45,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe8c0tt90
2019-03-13 20:55:53,079 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:55:58,631 : Computing embedding for train
2019-03-13 20:57:37,849 : Computed train embeddings
2019-03-13 20:57:37,849 : Computing embedding for dev
2019-03-13 20:57:40,010 : Computed dev embeddings
2019-03-13 20:57:40,011 : Computing embedding for test
2019-03-13 20:57:44,548 : Computed test embeddings
2019-03-13 20:57:44,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:58:04,232 : [('reg:1e-05', 86.58), ('reg:0.0001', 86.93), ('reg:0.001', 86.24), ('reg:0.01', 85.67)]
2019-03-13 20:58:04,232 : Validation : best param found is reg = 0.0001 with score             86.93
2019-03-13 20:58:04,233 : Evaluating...
2019-03-13 20:58:09,290 : 
Dev acc : 86.93 Test acc : 86.38 for             SST Binary classification

2019-03-13 20:58:09,291 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 20:58:09,345 : loading BERT model bert-large-uncased
2019-03-13 20:58:09,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:58:09,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:58:09,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_347cy83
2019-03-13 20:58:16,877 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:58:22,285 : Computing embedding for train
2019-03-13 20:58:43,978 : Computed train embeddings
2019-03-13 20:58:43,978 : Computing embedding for dev
2019-03-13 20:58:46,813 : Computed dev embeddings
2019-03-13 20:58:46,814 : Computing embedding for test
2019-03-13 20:58:52,395 : Computed test embeddings
2019-03-13 20:58:52,395 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:58:55,192 : [('reg:1e-05', 33.51), ('reg:0.0001', 38.87), ('reg:0.001', 41.6), ('reg:0.01', 41.78)]
2019-03-13 20:58:55,192 : Validation : best param found is reg = 0.01 with score             41.78
2019-03-13 20:58:55,192 : Evaluating...
2019-03-13 20:58:55,916 : 
Dev acc : 41.78 Test acc : 42.49 for             SST Fine-Grained classification

2019-03-13 20:58:55,917 : ***** Transfer task : TREC *****


2019-03-13 20:58:55,930 : loading BERT model bert-large-uncased
2019-03-13 20:58:55,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:58:55,949 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:58:55,949 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9hndidfz
2019-03-13 20:59:03,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:59:16,322 : Computed train embeddings
2019-03-13 20:59:16,905 : Computed test embeddings
2019-03-13 20:59:16,906 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 20:59:24,025 : [('reg:1e-05', 68.9), ('reg:0.0001', 62.25), ('reg:0.001', 65.09), ('reg:0.01', 57.84)]
2019-03-13 20:59:24,025 : Cross-validation : best param found is reg = 1e-05             with score 68.9
2019-03-13 20:59:24,025 : Evaluating...
2019-03-13 20:59:24,424 : 
Dev acc : 68.9 Test acc : 67.0             for TREC

2019-03-13 20:59:24,424 : ***** Transfer task : MRPC *****


2019-03-13 20:59:24,445 : loading BERT model bert-large-uncased
2019-03-13 20:59:24,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:59:24,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:59:24,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5yy7mnr6
2019-03-13 20:59:31,877 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:59:37,268 : Computing embedding for train
2019-03-13 20:59:59,304 : Computed train embeddings
2019-03-13 20:59:59,304 : Computing embedding for test
2019-03-13 21:00:08,953 : Computed test embeddings
2019-03-13 21:00:08,974 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 21:00:13,057 : [('reg:1e-05', 72.23), ('reg:0.0001', 72.3), ('reg:0.001', 70.9), ('reg:0.01', 70.83)]
2019-03-13 21:00:13,057 : Cross-validation : best param found is reg = 0.0001             with score 72.3
2019-03-13 21:00:13,058 : Evaluating...
2019-03-13 21:00:13,298 : Dev acc : 72.3 Test acc 72.64; Test F1 79.6 for MRPC.

2019-03-13 21:00:13,298 : ***** Transfer task : SICK-Entailment*****


2019-03-13 21:00:13,361 : loading BERT model bert-large-uncased
2019-03-13 21:00:13,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:00:13,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:00:13,380 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptpli0otk
2019-03-13 21:00:20,820 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:00:26,314 : Computing embedding for train
2019-03-13 21:00:37,514 : Computed train embeddings
2019-03-13 21:00:37,514 : Computing embedding for dev
2019-03-13 21:00:39,042 : Computed dev embeddings
2019-03-13 21:00:39,042 : Computing embedding for test
2019-03-13 21:00:51,051 : Computed test embeddings
2019-03-13 21:00:51,088 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:00:52,883 : [('reg:1e-05', 77.0), ('reg:0.0001', 76.2), ('reg:0.001', 69.6), ('reg:0.01', 74.2)]
2019-03-13 21:00:52,884 : Validation : best param found is reg = 1e-05 with score             77.0
2019-03-13 21:00:52,884 : Evaluating...
2019-03-13 21:00:53,353 : 
Dev acc : 77.0 Test acc : 75.18 for                        SICK entailment

2019-03-13 21:00:53,353 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 21:00:53,380 : loading BERT model bert-large-uncased
2019-03-13 21:00:53,380 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:00:53,437 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:00:53,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpojebp93c
2019-03-13 21:01:00,845 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:01:06,372 : Computing embedding for train
2019-03-13 21:01:17,572 : Computed train embeddings
2019-03-13 21:01:17,572 : Computing embedding for dev
2019-03-13 21:01:19,098 : Computed dev embeddings
2019-03-13 21:01:19,098 : Computing embedding for test
2019-03-13 21:01:31,107 : Computed test embeddings
2019-03-13 21:01:49,182 : Dev : Pearson 0.7515010330108755
2019-03-13 21:01:49,182 : Test : Pearson 0.7459493749932584 Spearman 0.6866127502083266 MSE 0.4672831896257482                        for SICK Relatedness

2019-03-13 21:01:49,183 : 

***** Transfer task : STSBenchmark*****


2019-03-13 21:01:49,252 : loading BERT model bert-large-uncased
2019-03-13 21:01:49,252 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:01:49,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:01:49,273 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpplx7gzrt
2019-03-13 21:01:56,669 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:02:02,127 : Computing embedding for train
2019-03-13 21:02:20,564 : Computed train embeddings
2019-03-13 21:02:20,564 : Computing embedding for dev
2019-03-13 21:02:26,145 : Computed dev embeddings
2019-03-13 21:02:26,145 : Computing embedding for test
2019-03-13 21:02:30,716 : Computed test embeddings
2019-03-13 21:02:47,471 : Dev : Pearson 0.6463534216811186
2019-03-13 21:02:47,471 : Test : Pearson 0.6064549640422039 Spearman 0.605599312291499 MSE 1.5232052471760862                        for SICK Relatedness

2019-03-13 21:02:47,471 : ***** Transfer task : SNLI Entailment*****


2019-03-13 21:02:52,559 : loading BERT model bert-large-uncased
2019-03-13 21:02:52,559 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:02:52,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:02:52,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6gxxlt9s
2019-03-13 21:03:00,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:03:06,043 : PROGRESS (encoding): 0.00%
2019-03-13 21:05:50,165 : PROGRESS (encoding): 14.56%
2019-03-13 21:08:56,299 : PROGRESS (encoding): 29.12%
2019-03-13 21:12:03,131 : PROGRESS (encoding): 43.69%
2019-03-13 21:15:22,571 : PROGRESS (encoding): 58.25%
2019-03-13 21:19:04,554 : PROGRESS (encoding): 72.81%
2019-03-13 21:22:45,344 : PROGRESS (encoding): 87.37%
2019-03-13 21:26:44,446 : PROGRESS (encoding): 0.00%
2019-03-13 21:27:14,481 : PROGRESS (encoding): 0.00%
2019-03-13 21:27:43,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:28:32,215 : [('reg:1e-09', 60.75)]
2019-03-13 21:28:32,215 : Validation : best param found is reg = 1e-09 with score             60.75
2019-03-13 21:28:32,216 : Evaluating...
2019-03-13 21:29:27,319 : Dev acc : 60.75 Test acc : 61.22 for SNLI

2019-03-13 21:29:27,320 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 21:29:27,525 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 21:29:28,598 : loading BERT model bert-large-uncased
2019-03-13 21:29:28,598 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:29:28,625 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:29:28,625 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz6ih4hw_
2019-03-13 21:29:36,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:29:41,672 : Computing embeddings for train/dev/test
2019-03-13 21:33:09,984 : Computed embeddings
2019-03-13 21:33:09,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:33:38,538 : [('reg:1e-05', 57.48), ('reg:0.0001', 57.23), ('reg:0.001', 56.52), ('reg:0.01', 50.15)]
2019-03-13 21:33:38,538 : Validation : best param found is reg = 1e-05 with score             57.48
2019-03-13 21:33:38,538 : Evaluating...
2019-03-13 21:33:45,872 : 
Dev acc : 57.5 Test acc : 58.3 for LENGTH classification

2019-03-13 21:33:45,873 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 21:33:46,255 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 21:33:46,300 : loading BERT model bert-large-uncased
2019-03-13 21:33:46,300 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:33:46,331 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:33:46,331 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpifinwt45
2019-03-13 21:33:53,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:33:59,288 : Computing embeddings for train/dev/test
2019-03-13 21:37:11,465 : Computed embeddings
2019-03-13 21:37:11,465 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:37:44,987 : [('reg:1e-05', 14.71), ('reg:0.0001', 3.08), ('reg:0.001', 0.36), ('reg:0.01', 0.1)]
2019-03-13 21:37:44,987 : Validation : best param found is reg = 1e-05 with score             14.71
2019-03-13 21:37:44,987 : Evaluating...
2019-03-13 21:37:56,755 : 
Dev acc : 14.7 Test acc : 15.0 for WORDCONTENT classification

2019-03-13 21:37:56,757 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 21:37:57,133 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 21:37:57,199 : loading BERT model bert-large-uncased
2019-03-13 21:37:57,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:37:57,224 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:37:57,224 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9nt8m0y6
2019-03-13 21:38:04,712 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:38:10,402 : Computing embeddings for train/dev/test
2019-03-13 21:41:10,921 : Computed embeddings
2019-03-13 21:41:10,922 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:41:36,814 : [('reg:1e-05', 24.14), ('reg:0.0001', 22.8), ('reg:0.001', 25.14), ('reg:0.01', 24.69)]
2019-03-13 21:41:36,814 : Validation : best param found is reg = 0.001 with score             25.14
2019-03-13 21:41:36,814 : Evaluating...
2019-03-13 21:41:43,563 : 
Dev acc : 25.1 Test acc : 24.8 for DEPTH classification

2019-03-13 21:41:43,564 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 21:41:43,944 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 21:41:44,008 : loading BERT model bert-large-uncased
2019-03-13 21:41:44,008 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:41:44,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:41:44,116 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpckctaxy0
2019-03-13 21:41:51,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:41:57,233 : Computing embeddings for train/dev/test
2019-03-13 21:44:44,477 : Computed embeddings
2019-03-13 21:44:44,477 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:45:10,597 : [('reg:1e-05', 49.48), ('reg:0.0001', 47.51), ('reg:0.001', 42.06), ('reg:0.01', 30.56)]
2019-03-13 21:45:10,597 : Validation : best param found is reg = 1e-05 with score             49.48
2019-03-13 21:45:10,598 : Evaluating...
2019-03-13 21:45:17,516 : 
Dev acc : 49.5 Test acc : 49.7 for TOPCONSTITUENTS classification

2019-03-13 21:45:17,517 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 21:45:17,863 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 21:45:17,930 : loading BERT model bert-large-uncased
2019-03-13 21:45:17,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:45:18,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:45:18,052 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjgr7ae9s
2019-03-13 21:45:25,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:45:31,188 : Computing embeddings for train/dev/test
2019-03-13 21:48:33,022 : Computed embeddings
2019-03-13 21:48:33,022 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:49:00,412 : [('reg:1e-05', 87.71), ('reg:0.0001', 87.69), ('reg:0.001', 87.45), ('reg:0.01', 88.01)]
2019-03-13 21:49:00,412 : Validation : best param found is reg = 0.01 with score             88.01
2019-03-13 21:49:00,412 : Evaluating...
2019-03-13 21:49:05,388 : 
Dev acc : 88.0 Test acc : 87.9 for BIGRAMSHIFT classification

2019-03-13 21:49:05,389 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 21:49:05,951 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 21:49:06,017 : loading BERT model bert-large-uncased
2019-03-13 21:49:06,017 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:49:06,046 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:49:06,047 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsw_6n3it
2019-03-13 21:49:13,577 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:49:19,247 : Computing embeddings for train/dev/test
2019-03-13 21:52:17,200 : Computed embeddings
2019-03-13 21:52:17,200 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:52:44,238 : [('reg:1e-05', 89.43), ('reg:0.0001', 89.42), ('reg:0.001', 89.54), ('reg:0.01', 89.72)]
2019-03-13 21:52:44,238 : Validation : best param found is reg = 0.01 with score             89.72
2019-03-13 21:52:44,238 : Evaluating...
2019-03-13 21:52:51,021 : 
Dev acc : 89.7 Test acc : 88.3 for TENSE classification

2019-03-13 21:52:51,022 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 21:52:51,445 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 21:52:51,509 : loading BERT model bert-large-uncased
2019-03-13 21:52:51,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:52:51,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:52:51,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyzg855a3
2019-03-13 21:52:59,031 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:53:04,582 : Computing embeddings for train/dev/test
2019-03-13 21:56:12,983 : Computed embeddings
2019-03-13 21:56:12,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:56:46,565 : [('reg:1e-05', 79.83), ('reg:0.0001', 79.86), ('reg:0.001', 79.77), ('reg:0.01', 79.2)]
2019-03-13 21:56:46,566 : Validation : best param found is reg = 0.0001 with score             79.86
2019-03-13 21:56:46,566 : Evaluating...
2019-03-13 21:56:56,341 : 
Dev acc : 79.9 Test acc : 78.2 for SUBJNUMBER classification

2019-03-13 21:56:56,342 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 21:56:56,747 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 21:56:56,814 : loading BERT model bert-large-uncased
2019-03-13 21:56:56,814 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:56:56,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:56:56,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkaevyjbt
2019-03-13 21:57:04,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:57:09,900 : Computing embeddings for train/dev/test
2019-03-13 22:00:14,760 : Computed embeddings
2019-03-13 22:00:14,760 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:00:40,656 : [('reg:1e-05', 74.66), ('reg:0.0001', 74.62), ('reg:0.001', 74.57), ('reg:0.01', 74.61)]
2019-03-13 22:00:40,657 : Validation : best param found is reg = 1e-05 with score             74.66
2019-03-13 22:00:40,657 : Evaluating...
2019-03-13 22:00:47,242 : 
Dev acc : 74.7 Test acc : 75.9 for OBJNUMBER classification

2019-03-13 22:00:47,243 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 22:00:47,823 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 22:00:47,890 : loading BERT model bert-large-uncased
2019-03-13 22:00:47,890 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:00:47,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:00:47,917 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmdx9rfrh
2019-03-13 22:00:55,364 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:01:00,848 : Computing embeddings for train/dev/test
2019-03-13 22:04:35,209 : Computed embeddings
2019-03-13 22:04:35,209 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:05:02,790 : [('reg:1e-05', 62.9), ('reg:0.0001', 62.86), ('reg:0.001', 62.97), ('reg:0.01', 63.39)]
2019-03-13 22:05:02,790 : Validation : best param found is reg = 0.01 with score             63.39
2019-03-13 22:05:02,790 : Evaluating...
2019-03-13 22:05:10,377 : 
Dev acc : 63.4 Test acc : 63.1 for ODDMANOUT classification

2019-03-13 22:05:10,378 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 22:05:10,771 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 22:05:10,848 : loading BERT model bert-large-uncased
2019-03-13 22:05:10,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:05:10,970 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:05:10,971 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqu4ovai4
2019-03-13 22:05:18,387 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:05:23,921 : Computing embeddings for train/dev/test
2019-03-13 22:08:56,444 : Computed embeddings
2019-03-13 22:08:56,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:09:22,713 : [('reg:1e-05', 66.02), ('reg:0.0001', 66.08), ('reg:0.001', 66.12), ('reg:0.01', 65.72)]
2019-03-13 22:09:22,713 : Validation : best param found is reg = 0.001 with score             66.12
2019-03-13 22:09:22,713 : Evaluating...
2019-03-13 22:09:28,565 : 
Dev acc : 66.1 Test acc : 65.8 for COORDINATIONINVERSION classification

2019-03-13 22:09:28,566 : total results: {'STS12': {'MSRpar': {'pearson': (0.32228171428597785, 1.378235338834892e-19), 'spearman': SpearmanrResult(correlation=0.3695476360710841, pvalue=1.105069553038808e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4374827538599349, 2.068537973956276e-36), 'spearman': SpearmanrResult(correlation=0.49593720461684754, pvalue=8.213239332730845e-48), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4748573913253097, 3.4255045443317916e-27), 'spearman': SpearmanrResult(correlation=0.5292135008304912, pvalue=1.7256967322023102e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.41408847475537647, 1.9607479462081292e-32), 'spearman': SpearmanrResult(correlation=0.5019779717624402, pvalue=4.032631048849575e-49), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5766876679723533, 9.635656102352187e-37), 'spearman': SpearmanrResult(correlation=0.5757526183057295, pvalue=1.3293767052962054e-36), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4450796004397904, 'wmean': 0.42742845216063485}, 'spearman': {'mean': 0.49448578631731854, 'wmean': 0.4820564353033978}}}, 'STS13': {'FNWN': {'pearson': (0.23110688830946227, 0.0013764527259988461), 'spearman': SpearmanrResult(correlation=0.22637053698065088, pvalue=0.001735039714820793), 'nsamples': 189}, 'headlines': {'pearson': (0.5040918249480262, 1.3846254492736568e-49), 'spearman': SpearmanrResult(correlation=0.5847724911937303, pvalue=5.289170285581021e-70), 'nsamples': 750}, 'OnWN': {'pearson': (0.4004083879437729, 5.133197834767061e-23), 'spearman': SpearmanrResult(correlation=0.42228540131562825, pvalue=1.1418613977159493e-25), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.37853570040042045, 'wmean': 0.43091811749197645}, 'spearman': {'mean': 0.4111428098300032, 'wmean': 0.47884367334847217}}}, 'STS14': {'deft-forum': {'pearson': (0.13480746238527383, 0.0041723035430209195), 'spearman': SpearmanrResult(correlation=0.17243578051512978, pvalue=0.00023756282528708065), 'nsamples': 450}, 'deft-news': {'pearson': (0.7246827567670336, 4.053897899336556e-50), 'spearman': SpearmanrResult(correlation=0.6743069312172196, pvalue=3.924513252913387e-41), 'nsamples': 300}, 'headlines': {'pearson': (0.44325158451747687, 1.9330503142843614e-37), 'spearman': SpearmanrResult(correlation=0.5205561845327457, pvalue=2.5802657798668216e-53), 'nsamples': 750}, 'images': {'pearson': (0.44891761321662693, 1.801921343640371e-38), 'spearman': SpearmanrResult(correlation=0.5004336609632707, pvalue=8.7641291203861e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.4885540999624749, 3.0141238641951303e-46), 'spearman': SpearmanrResult(correlation=0.5883070436978096, pvalue=4.94432363905044e-71), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4423820876811221, 2.7712141056669187e-37), 'spearman': SpearmanrResult(correlation=0.491115970310612, pvalue=8.721033778906265e-47), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.44709926742166806, 'wmean': 0.4387725931031357}, 'spearman': {'mean': 0.4911925952061312, 'wmean': 0.49471942006008074}}}, 'STS15': {'answers-forums': {'pearson': (0.5515311527080993, 3.137024516102252e-31), 'spearman': SpearmanrResult(correlation=0.5397053035385669, pvalue=9.867023174877925e-30), 'nsamples': 375}, 'answers-students': {'pearson': (0.4729869120823135, 4.524768018794331e-43), 'spearman': SpearmanrResult(correlation=0.5695852982246036, pvalue=1.012565232130528e-65), 'nsamples': 750}, 'belief': {'pearson': (0.5350663790034049, 3.678527732952152e-29), 'spearman': SpearmanrResult(correlation=0.5698210805693197, pvalue=1.1462929931934035e-33), 'nsamples': 375}, 'headlines': {'pearson': (0.547331554836803, 7.784628526548902e-60), 'spearman': SpearmanrResult(correlation=0.6026774471346976, pvalue=2.375549305705638e-75), 'nsamples': 750}, 'images': {'pearson': (0.6090084605101106, 2.527267725898561e-77), 'spearman': SpearmanrResult(correlation=0.6247848383615089, pvalue=1.9352027611515402e-82), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5431848918281463, 'wmean': 0.5431564233212448}, 'spearman': {'mean': 0.5813147935657395, 'wmean': 0.5879526939436883}}}, 'STS16': {'answer-answer': {'pearson': (0.48994985598063895, 9.6438972559891e-17), 'spearman': SpearmanrResult(correlation=0.48774867715985615, pvalue=1.3832232206622067e-16), 'nsamples': 254}, 'headlines': {'pearson': (0.5581827270220088, 8.520125212416472e-22), 'spearman': SpearmanrResult(correlation=0.6169293204569528, pvalue=1.6611594221038402e-27), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7139983596707684, 3.717059452234737e-37), 'spearman': SpearmanrResult(correlation=0.7320140539590332, pvalue=7.177735764705413e-40), 'nsamples': 230}, 'postediting': {'pearson': (0.7737960955556502, 6.786271499141142e-50), 'spearman': SpearmanrResult(correlation=0.7948513308427178, pvalue=2.0231401895628795e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.3058806845724098, 6.669620429937549e-06), 'spearman': SpearmanrResult(correlation=0.34747660638679156, pvalue=2.5388669628654493e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5683615445602952, 'wmean': 0.5736843976079693}, 'spearman': {'mean': 0.5958039977610703, 'wmean': 0.6007024727347597}}}, 'MR': {'devacc': 78.61, 'acc': 75.87, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.56, 'acc': 79.23, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.86, 'acc': 83.34, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.16, 'acc': 93.54, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.93, 'acc': 86.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.78, 'acc': 42.49, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.9, 'acc': 67.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.3, 'acc': 72.64, 'f1': 79.6, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.0, 'acc': 75.18, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7515010330108755, 'pearson': 0.7459493749932584, 'spearman': 0.6866127502083266, 'mse': 0.4672831896257482, 'yhat': array([3.12773761, 3.97929468, 1.7969185 , ..., 2.93289214, 4.66591522,        4.22696391]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6463534216811186, 'pearson': 0.6064549640422039, 'spearman': 0.605599312291499, 'mse': 1.5232052471760862, 'yhat': array([2.77523954, 1.24864324, 2.95942873, ..., 4.0216974 , 4.21582799,        3.02315804]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.75, 'acc': 61.22, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 57.48, 'acc': 58.27, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 14.71, 'acc': 15.04, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.14, 'acc': 24.82, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.48, 'acc': 49.73, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.01, 'acc': 87.87, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.72, 'acc': 88.35, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.86, 'acc': 78.2, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.66, 'acc': 75.9, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.39, 'acc': 63.06, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.12, 'acc': 65.79, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 22:09:28,567 : STS12 p=0.4274, STS12 s=0.4821, STS13 p=0.4309, STS13 s=0.4788, STS14 p=0.4388, STS14 s=0.4947, STS15 p=0.5432, STS15 s=0.5880, STS 16 p=0.5737, STS16 s=0.6007, STS B p=0.6065, STS B s=0.6056, STS B m=1.5232, SICK-R p=0.7459, SICK-R s=0.6866, SICK-P m=0.4673
2019-03-13 22:09:28,567 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 22:09:28,567 : 0.4274,0.4821,0.4309,0.4788,0.4388,0.4947,0.5432,0.5880,0.5737,0.6007,0.6065,0.6056,1.5232,0.7459,0.6866,0.4673
2019-03-13 22:09:28,567 : MR=75.87, CR=79.23, SUBJ=93.54, MPQA=83.34, SST-B=86.38, SST-F=42.49, TREC=67.00, SICK-E=75.18, SNLI=61.22, MRPC=72.64, MRPC f=79.60
2019-03-13 22:09:28,567 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 22:09:28,567 : 75.87,79.23,93.54,83.34,86.38,42.49,67.00,75.18,61.22,72.64,79.60
2019-03-13 22:09:28,567 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 22:09:28,567 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 22:09:28,567 : na,na,na,na,na,na,na,na,na,na
2019-03-13 22:09:28,567 : SentLen=58.27, WC=15.04, TreeDepth=24.82, TopConst=49.73, BShift=87.87, Tense=88.35, SubjNum=78.20, ObjNum=75.90, SOMO=63.06, CoordInv=65.79, average=60.70
2019-03-13 22:09:28,567 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 22:09:28,567 : 58.27,15.04,24.82,49.73,87.87,88.35,78.20,75.90,63.06,65.79,60.70
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,567 : layer 24
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,567 : ********************************************************************************
2019-03-13 22:09:28,655 : ***** Transfer task : STS12 *****


2019-03-13 22:09:28,667 : loading BERT model bert-large-uncased
2019-03-13 22:09:28,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:09:28,684 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:09:28,685 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi2koc7oo
2019-03-13 22:09:36,154 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:09:45,833 : MSRpar : pearson = 0.2772, spearman = 0.3417
2019-03-13 22:09:47,467 : MSRvid : pearson = 0.3398, spearman = 0.4579
2019-03-13 22:09:48,870 : SMTeuroparl : pearson = 0.3774, spearman = 0.4937
2019-03-13 22:09:51,550 : surprise.OnWN : pearson = 0.2738, spearman = 0.4305
2019-03-13 22:09:52,968 : surprise.SMTnews : pearson = 0.4879, spearman = 0.5195
2019-03-13 22:09:52,968 : ALL (weighted average) : Pearson = 0.3333,             Spearman = 0.4364
2019-03-13 22:09:52,968 : ALL (average) : Pearson = 0.3512,             Spearman = 0.4487

2019-03-13 22:09:52,968 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 22:09:52,976 : loading BERT model bert-large-uncased
2019-03-13 22:09:52,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:09:52,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:09:52,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgtdwhjk5
2019-03-13 22:10:00,439 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:10:07,237 : FNWN : pearson = 0.1495, spearman = 0.1514
2019-03-13 22:10:09,117 : headlines : pearson = 0.4034, spearman = 0.5233
2019-03-13 22:10:10,578 : OnWN : pearson = 0.2975, spearman = 0.4025
2019-03-13 22:10:10,578 : ALL (weighted average) : Pearson = 0.3318,             Spearman = 0.4313
2019-03-13 22:10:10,578 : ALL (average) : Pearson = 0.2834,             Spearman = 0.3591

2019-03-13 22:10:10,578 : ***** Transfer task : STS14 *****


2019-03-13 22:10:10,595 : loading BERT model bert-large-uncased
2019-03-13 22:10:10,595 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:10:10,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:10:10,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt6p3ys8_
2019-03-13 22:10:18,106 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:10:25,177 : deft-forum : pearson = 0.0616, spearman = 0.1432
2019-03-13 22:10:26,802 : deft-news : pearson = 0.6587, spearman = 0.6078
2019-03-13 22:10:28,953 : headlines : pearson = 0.3313, spearman = 0.4669
2019-03-13 22:10:31,016 : images : pearson = 0.3049, spearman = 0.4609
2019-03-13 22:10:33,132 : OnWN : pearson = 0.3104, spearman = 0.4796
2019-03-13 22:10:35,973 : tweet-news : pearson = 0.2999, spearman = 0.3803
2019-03-13 22:10:35,973 : ALL (weighted average) : Pearson = 0.3094,             Spearman = 0.4234
2019-03-13 22:10:35,973 : ALL (average) : Pearson = 0.3278,             Spearman = 0.4231

2019-03-13 22:10:35,973 : ***** Transfer task : STS15 *****


2019-03-13 22:10:36,008 : loading BERT model bert-large-uncased
2019-03-13 22:10:36,008 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:10:36,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:10:36,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr2r9tysw
2019-03-13 22:10:43,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:10:50,927 : answers-forums : pearson = 0.4624, spearman = 0.4620
2019-03-13 22:10:52,996 : answers-students : pearson = 0.3134, spearman = 0.4219
2019-03-13 22:10:55,029 : belief : pearson = 0.4859, spearman = 0.5622
2019-03-13 22:10:57,268 : headlines : pearson = 0.4611, spearman = 0.5502
2019-03-13 22:10:59,389 : images : pearson = 0.4926, spearman = 0.5356
2019-03-13 22:10:59,389 : ALL (weighted average) : Pearson = 0.4353,             Spearman = 0.5049
2019-03-13 22:10:59,389 : ALL (average) : Pearson = 0.4431,             Spearman = 0.5064

2019-03-13 22:10:59,389 : ***** Transfer task : STS16 *****


2019-03-13 22:10:59,459 : loading BERT model bert-large-uncased
2019-03-13 22:10:59,459 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:10:59,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:10:59,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbnl78gfb
2019-03-13 22:11:06,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:11:13,292 : answer-answer : pearson = 0.3490, spearman = 0.3742
2019-03-13 22:11:13,949 : headlines : pearson = 0.4466, spearman = 0.5479
2019-03-13 22:11:14,825 : plagiarism : pearson = 0.6502, spearman = 0.6659
2019-03-13 22:11:16,309 : postediting : pearson = 0.7101, spearman = 0.7481
2019-03-13 22:11:16,911 : question-question : pearson = 0.3605, spearman = 0.3952
2019-03-13 22:11:16,911 : ALL (weighted average) : Pearson = 0.5042,             Spearman = 0.5479
2019-03-13 22:11:16,911 : ALL (average) : Pearson = 0.5033,             Spearman = 0.5463

2019-03-13 22:11:16,911 : ***** Transfer task : MR *****


2019-03-13 22:11:16,931 : loading BERT model bert-large-uncased
2019-03-13 22:11:16,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:11:16,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:11:16,951 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplr3xgefa
2019-03-13 22:11:24,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:11:30,060 : Generating sentence embeddings
2019-03-13 22:12:01,308 : Generated sentence embeddings
2019-03-13 22:12:01,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:12:12,399 : Best param found at split 1: l2reg = 0.001                 with score 80.17
2019-03-13 22:12:24,194 : Best param found at split 2: l2reg = 0.01                 with score 78.96
2019-03-13 22:12:35,764 : Best param found at split 3: l2reg = 0.01                 with score 79.39
2019-03-13 22:12:47,894 : Best param found at split 4: l2reg = 0.0001                 with score 80.05
2019-03-13 22:12:59,648 : Best param found at split 5: l2reg = 1e-05                 with score 80.4
2019-03-13 22:13:00,288 : Dev acc : 79.79 Test acc : 76.0

2019-03-13 22:13:00,289 : ***** Transfer task : CR *****


2019-03-13 22:13:00,299 : loading BERT model bert-large-uncased
2019-03-13 22:13:00,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:13:00,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:13:00,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpowmcaznk
2019-03-13 22:13:07,844 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:13:13,491 : Generating sentence embeddings
2019-03-13 22:13:21,729 : Generated sentence embeddings
2019-03-13 22:13:21,730 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:13:25,210 : Best param found at split 1: l2reg = 0.0001                 with score 80.96
2019-03-13 22:13:28,998 : Best param found at split 2: l2reg = 0.001                 with score 79.23
2019-03-13 22:13:32,934 : Best param found at split 3: l2reg = 1e-05                 with score 83.81
2019-03-13 22:13:36,959 : Best param found at split 4: l2reg = 0.0001                 with score 85.17
2019-03-13 22:13:40,767 : Best param found at split 5: l2reg = 1e-05                 with score 83.58
2019-03-13 22:13:40,962 : Dev acc : 82.55 Test acc : 80.46

2019-03-13 22:13:40,963 : ***** Transfer task : MPQA *****


2019-03-13 22:13:40,969 : loading BERT model bert-large-uncased
2019-03-13 22:13:40,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:13:41,020 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:13:41,020 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3uuwhatj
2019-03-13 22:13:48,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:13:54,021 : Generating sentence embeddings
2019-03-13 22:14:01,535 : Generated sentence embeddings
2019-03-13 22:14:01,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:14:12,270 : Best param found at split 1: l2reg = 0.001                 with score 83.85
2019-03-13 22:14:23,601 : Best param found at split 2: l2reg = 1e-05                 with score 84.71
2019-03-13 22:14:35,533 : Best param found at split 3: l2reg = 1e-05                 with score 82.51
2019-03-13 22:14:47,368 : Best param found at split 4: l2reg = 0.0001                 with score 84.81
2019-03-13 22:14:58,387 : Best param found at split 5: l2reg = 0.001                 with score 84.03
2019-03-13 22:14:59,087 : Dev acc : 83.98 Test acc : 83.6

2019-03-13 22:14:59,088 : ***** Transfer task : SUBJ *****


2019-03-13 22:14:59,103 : loading BERT model bert-large-uncased
2019-03-13 22:14:59,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:14:59,125 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:14:59,125 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsmm51_lb
2019-03-13 22:15:06,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:15:12,128 : Generating sentence embeddings
2019-03-13 22:15:42,791 : Generated sentence embeddings
2019-03-13 22:15:42,792 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:15:53,282 : Best param found at split 1: l2reg = 0.001                 with score 94.32
2019-03-13 22:16:04,606 : Best param found at split 2: l2reg = 0.001                 with score 94.3
2019-03-13 22:16:13,405 : Best param found at split 3: l2reg = 0.001                 with score 94.1
2019-03-13 22:16:25,411 : Best param found at split 4: l2reg = 0.001                 with score 94.5
2019-03-13 22:16:36,823 : Best param found at split 5: l2reg = 0.01                 with score 94.15
2019-03-13 22:16:37,130 : Dev acc : 94.27 Test acc : 93.67

2019-03-13 22:16:37,131 : ***** Transfer task : SST Binary classification *****


2019-03-13 22:16:37,223 : loading BERT model bert-large-uncased
2019-03-13 22:16:37,223 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:16:37,296 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:16:37,297 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptqmv36ow
2019-03-13 22:16:44,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:16:50,407 : Computing embedding for train
2019-03-13 22:18:29,452 : Computed train embeddings
2019-03-13 22:18:29,452 : Computing embedding for dev
2019-03-13 22:18:31,612 : Computed dev embeddings
2019-03-13 22:18:31,612 : Computing embedding for test
2019-03-13 22:18:36,143 : Computed test embeddings
2019-03-13 22:18:36,144 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:18:50,742 : [('reg:1e-05', 85.44), ('reg:0.0001', 86.01), ('reg:0.001', 85.78), ('reg:0.01', 84.4)]
2019-03-13 22:18:50,742 : Validation : best param found is reg = 0.0001 with score             86.01
2019-03-13 22:18:50,742 : Evaluating...
2019-03-13 22:18:55,922 : 
Dev acc : 86.01 Test acc : 87.64 for             SST Binary classification

2019-03-13 22:18:55,922 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 22:18:55,974 : loading BERT model bert-large-uncased
2019-03-13 22:18:55,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:18:55,997 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:18:55,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3kiru9fv
2019-03-13 22:19:03,480 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:19:09,143 : Computing embedding for train
2019-03-13 22:19:30,852 : Computed train embeddings
2019-03-13 22:19:30,852 : Computing embedding for dev
2019-03-13 22:19:33,682 : Computed dev embeddings
2019-03-13 22:19:33,682 : Computing embedding for test
2019-03-13 22:19:39,265 : Computed test embeddings
2019-03-13 22:19:39,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:19:41,683 : [('reg:1e-05', 40.51), ('reg:0.0001', 37.15), ('reg:0.001', 36.78), ('reg:0.01', 41.96)]
2019-03-13 22:19:41,684 : Validation : best param found is reg = 0.01 with score             41.96
2019-03-13 22:19:41,684 : Evaluating...
2019-03-13 22:19:42,327 : 
Dev acc : 41.96 Test acc : 43.35 for             SST Fine-Grained classification

2019-03-13 22:19:42,328 : ***** Transfer task : TREC *****


2019-03-13 22:19:42,341 : loading BERT model bert-large-uncased
2019-03-13 22:19:42,341 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:19:42,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:19:42,360 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp81m22dhx
2019-03-13 22:19:49,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:20:02,819 : Computed train embeddings
2019-03-13 22:20:03,403 : Computed test embeddings
2019-03-13 22:20:03,404 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:20:10,835 : [('reg:1e-05', 61.1), ('reg:0.0001', 67.6), ('reg:0.001', 63.14), ('reg:0.01', 57.4)]
2019-03-13 22:20:10,836 : Cross-validation : best param found is reg = 0.0001             with score 67.6
2019-03-13 22:20:10,836 : Evaluating...
2019-03-13 22:20:11,290 : 
Dev acc : 67.6 Test acc : 71.0             for TREC

2019-03-13 22:20:11,291 : ***** Transfer task : MRPC *****


2019-03-13 22:20:11,313 : loading BERT model bert-large-uncased
2019-03-13 22:20:11,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:20:11,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:20:11,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3gx0lc9a
2019-03-13 22:20:18,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:20:24,768 : Computing embedding for train
2019-03-13 22:20:46,818 : Computed train embeddings
2019-03-13 22:20:46,818 : Computing embedding for test
2019-03-13 22:20:56,469 : Computed test embeddings
2019-03-13 22:20:56,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:21:01,612 : [('reg:1e-05', 71.64), ('reg:0.0001', 70.93), ('reg:0.001', 71.0), ('reg:0.01', 71.2)]
2019-03-13 22:21:01,612 : Cross-validation : best param found is reg = 1e-05             with score 71.64
2019-03-13 22:21:01,612 : Evaluating...
2019-03-13 22:21:01,984 : Dev acc : 71.64 Test acc 72.0; Test F1 79.42 for MRPC.

2019-03-13 22:21:01,984 : ***** Transfer task : SICK-Entailment*****


2019-03-13 22:21:02,045 : loading BERT model bert-large-uncased
2019-03-13 22:21:02,045 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:21:02,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:21:02,065 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp87vjtvc0
2019-03-13 22:21:09,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:21:15,037 : Computing embedding for train
2019-03-13 22:21:26,223 : Computed train embeddings
2019-03-13 22:21:26,224 : Computing embedding for dev
2019-03-13 22:21:27,750 : Computed dev embeddings
2019-03-13 22:21:27,750 : Computing embedding for test
2019-03-13 22:21:39,749 : Computed test embeddings
2019-03-13 22:21:39,785 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:21:41,545 : [('reg:1e-05', 74.8), ('reg:0.0001', 73.2), ('reg:0.001', 77.0), ('reg:0.01', 74.8)]
2019-03-13 22:21:41,545 : Validation : best param found is reg = 0.001 with score             77.0
2019-03-13 22:21:41,545 : Evaluating...
2019-03-13 22:21:41,911 : 
Dev acc : 77.0 Test acc : 74.22 for                        SICK entailment

2019-03-13 22:21:41,912 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 22:21:41,939 : loading BERT model bert-large-uncased
2019-03-13 22:21:41,939 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:21:41,994 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:21:41,994 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpft35fhg7
2019-03-13 22:21:49,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:21:54,928 : Computing embedding for train
2019-03-13 22:22:06,120 : Computed train embeddings
2019-03-13 22:22:06,120 : Computing embedding for dev
2019-03-13 22:22:07,647 : Computed dev embeddings
2019-03-13 22:22:07,647 : Computing embedding for test
2019-03-13 22:22:19,653 : Computed test embeddings
2019-03-13 22:22:29,599 : Dev : Pearson 0.7559475869640457
2019-03-13 22:22:29,599 : Test : Pearson 0.7563816855984655 Spearman 0.6957083619296187 MSE 0.4370618573782994                        for SICK Relatedness

2019-03-13 22:22:29,600 : 

***** Transfer task : STSBenchmark*****


2019-03-13 22:22:29,638 : loading BERT model bert-large-uncased
2019-03-13 22:22:29,639 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:22:29,666 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:22:29,667 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl_moida8
2019-03-13 22:22:37,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:22:42,609 : Computing embedding for train
2019-03-13 22:23:01,046 : Computed train embeddings
2019-03-13 22:23:01,046 : Computing embedding for dev
2019-03-13 22:23:06,644 : Computed dev embeddings
2019-03-13 22:23:06,644 : Computing embedding for test
2019-03-13 22:23:11,210 : Computed test embeddings
2019-03-13 22:23:28,419 : Dev : Pearson 0.6555088741581299
2019-03-13 22:23:28,419 : Test : Pearson 0.6119464492789388 Spearman 0.6091410149554003 MSE 1.52161974035571                        for SICK Relatedness

2019-03-13 22:23:28,419 : ***** Transfer task : SNLI Entailment*****


2019-03-13 22:23:33,445 : loading BERT model bert-large-uncased
2019-03-13 22:23:33,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:23:33,530 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:23:33,530 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwqhvkyq1
2019-03-13 22:23:40,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:23:46,895 : PROGRESS (encoding): 0.00%
2019-03-13 22:26:30,819 : PROGRESS (encoding): 14.56%
2019-03-13 22:29:36,604 : PROGRESS (encoding): 29.12%
2019-03-13 22:32:43,064 : PROGRESS (encoding): 43.69%
2019-03-13 22:36:01,942 : PROGRESS (encoding): 58.25%
2019-03-13 22:39:43,445 : PROGRESS (encoding): 72.81%
2019-03-13 22:43:23,862 : PROGRESS (encoding): 87.37%
2019-03-13 22:47:22,468 : PROGRESS (encoding): 0.00%
2019-03-13 22:47:52,520 : PROGRESS (encoding): 0.00%
2019-03-13 22:48:21,369 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:49:16,141 : [('reg:1e-09', 66.54)]
2019-03-13 22:49:16,142 : Validation : best param found is reg = 1e-09 with score             66.54
2019-03-13 22:49:16,142 : Evaluating...
2019-03-13 22:50:10,408 : Dev acc : 66.54 Test acc : 66.69 for SNLI

2019-03-13 22:50:10,409 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 22:50:10,616 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 22:50:11,670 : loading BERT model bert-large-uncased
2019-03-13 22:50:11,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:50:11,696 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:50:11,696 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg9zfgwfw
2019-03-13 22:50:19,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:50:24,600 : Computing embeddings for train/dev/test
2019-03-13 22:53:53,207 : Computed embeddings
2019-03-13 22:53:53,207 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:54:17,622 : [('reg:1e-05', 53.71), ('reg:0.0001', 59.38), ('reg:0.001', 50.16), ('reg:0.01', 45.68)]
2019-03-13 22:54:17,622 : Validation : best param found is reg = 0.0001 with score             59.38
2019-03-13 22:54:17,622 : Evaluating...
2019-03-13 22:54:25,284 : 
Dev acc : 59.4 Test acc : 60.1 for LENGTH classification

2019-03-13 22:54:25,284 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 22:54:25,536 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 22:54:25,583 : loading BERT model bert-large-uncased
2019-03-13 22:54:25,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:54:25,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:54:25,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpceoj7qlc
2019-03-13 22:54:33,031 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:54:38,377 : Computing embeddings for train/dev/test
2019-03-13 22:57:50,418 : Computed embeddings
2019-03-13 22:57:50,419 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:58:27,835 : [('reg:1e-05', 9.25), ('reg:0.0001', 2.7), ('reg:0.001', 0.65), ('reg:0.01', 0.17)]
2019-03-13 22:58:27,836 : Validation : best param found is reg = 1e-05 with score             9.25
2019-03-13 22:58:27,836 : Evaluating...
2019-03-13 22:58:41,158 : 
Dev acc : 9.2 Test acc : 9.1 for WORDCONTENT classification

2019-03-13 22:58:41,160 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 22:58:41,693 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 22:58:41,759 : loading BERT model bert-large-uncased
2019-03-13 22:58:41,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:58:41,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:58:41,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsxttp4en
2019-03-13 22:58:49,217 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:58:54,775 : Computing embeddings for train/dev/test
2019-03-13 23:01:54,998 : Computed embeddings
2019-03-13 23:01:54,998 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:02:18,536 : [('reg:1e-05', 24.28), ('reg:0.0001', 22.45), ('reg:0.001', 22.96), ('reg:0.01', 21.23)]
2019-03-13 23:02:18,536 : Validation : best param found is reg = 1e-05 with score             24.28
2019-03-13 23:02:18,536 : Evaluating...
2019-03-13 23:02:26,231 : 
Dev acc : 24.3 Test acc : 24.1 for DEPTH classification

2019-03-13 23:02:26,232 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 23:02:26,598 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 23:02:26,661 : loading BERT model bert-large-uncased
2019-03-13 23:02:26,661 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:02:26,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:02:26,771 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu1m8oe_g
2019-03-13 23:02:34,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:02:39,802 : Computing embeddings for train/dev/test
2019-03-13 23:05:27,060 : Computed embeddings
2019-03-13 23:05:27,060 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:05:55,418 : [('reg:1e-05', 48.47), ('reg:0.0001', 47.05), ('reg:0.001', 38.92), ('reg:0.01', 30.16)]
2019-03-13 23:05:55,418 : Validation : best param found is reg = 1e-05 with score             48.47
2019-03-13 23:05:55,418 : Evaluating...
2019-03-13 23:06:04,328 : 
Dev acc : 48.5 Test acc : 47.8 for TOPCONSTITUENTS classification

2019-03-13 23:06:04,329 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 23:06:04,707 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 23:06:04,775 : loading BERT model bert-large-uncased
2019-03-13 23:06:04,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:06:04,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:06:04,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvoe77_yg
2019-03-13 23:06:12,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:06:17,861 : Computing embeddings for train/dev/test
2019-03-13 23:09:19,409 : Computed embeddings
2019-03-13 23:09:19,409 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:09:52,895 : [('reg:1e-05', 88.19), ('reg:0.0001', 88.26), ('reg:0.001', 88.18), ('reg:0.01', 86.66)]
2019-03-13 23:09:52,896 : Validation : best param found is reg = 0.0001 with score             88.26
2019-03-13 23:09:52,896 : Evaluating...
2019-03-13 23:10:00,588 : 
Dev acc : 88.3 Test acc : 88.4 for BIGRAMSHIFT classification

2019-03-13 23:10:00,589 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 23:10:00,975 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 23:10:01,040 : loading BERT model bert-large-uncased
2019-03-13 23:10:01,040 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:10:01,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:10:01,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5caqwpn9
2019-03-13 23:10:08,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:10:14,152 : Computing embeddings for train/dev/test
2019-03-13 23:13:11,925 : Computed embeddings
2019-03-13 23:13:11,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:13:39,230 : [('reg:1e-05', 89.46), ('reg:0.0001', 89.73), ('reg:0.001', 89.72), ('reg:0.01', 89.92)]
2019-03-13 23:13:39,231 : Validation : best param found is reg = 0.01 with score             89.92
2019-03-13 23:13:39,231 : Evaluating...
2019-03-13 23:13:46,889 : 
Dev acc : 89.9 Test acc : 88.0 for TENSE classification

2019-03-13 23:13:46,890 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 23:13:47,291 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 23:13:47,354 : loading BERT model bert-large-uncased
2019-03-13 23:13:47,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:13:47,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:13:47,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu34e1lcc
2019-03-13 23:13:54,949 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:14:00,558 : Computing embeddings for train/dev/test
2019-03-13 23:17:08,653 : Computed embeddings
2019-03-13 23:17:08,654 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:17:35,899 : [('reg:1e-05', 74.01), ('reg:0.0001', 73.64), ('reg:0.001', 72.39), ('reg:0.01', 75.96)]
2019-03-13 23:17:35,899 : Validation : best param found is reg = 0.01 with score             75.96
2019-03-13 23:17:35,899 : Evaluating...
2019-03-13 23:17:44,593 : 
Dev acc : 76.0 Test acc : 74.3 for SUBJNUMBER classification

2019-03-13 23:17:44,594 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 23:17:45,000 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 23:17:45,068 : loading BERT model bert-large-uncased
2019-03-13 23:17:45,068 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:17:45,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:17:45,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplj_hd5uy
2019-03-13 23:17:52,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:17:58,276 : Computing embeddings for train/dev/test
2019-03-13 23:21:03,004 : Computed embeddings
2019-03-13 23:21:03,004 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:21:32,306 : [('reg:1e-05', 75.91), ('reg:0.0001', 76.08), ('reg:0.001', 75.7), ('reg:0.01', 72.35)]
2019-03-13 23:21:32,307 : Validation : best param found is reg = 0.0001 with score             76.08
2019-03-13 23:21:32,307 : Evaluating...
2019-03-13 23:21:40,000 : 
Dev acc : 76.1 Test acc : 76.6 for OBJNUMBER classification

2019-03-13 23:21:40,001 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 23:21:40,598 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 23:21:40,670 : loading BERT model bert-large-uncased
2019-03-13 23:21:40,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:21:40,700 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:21:40,700 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph0_v8xet
2019-03-13 23:21:48,138 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:21:53,713 : Computing embeddings for train/dev/test
2019-03-13 23:25:27,855 : Computed embeddings
2019-03-13 23:25:27,855 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:25:53,034 : [('reg:1e-05', 62.6), ('reg:0.0001', 62.54), ('reg:0.001', 62.41), ('reg:0.01', 62.09)]
2019-03-13 23:25:53,034 : Validation : best param found is reg = 1e-05 with score             62.6
2019-03-13 23:25:53,034 : Evaluating...
2019-03-13 23:25:59,595 : 
Dev acc : 62.6 Test acc : 63.2 for ODDMANOUT classification

2019-03-13 23:25:59,596 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 23:25:59,967 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 23:26:00,047 : loading BERT model bert-large-uncased
2019-03-13 23:26:00,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:26:00,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:26:00,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7yd6u75w
2019-03-13 23:26:07,517 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:26:13,015 : Computing embeddings for train/dev/test
2019-03-13 23:29:44,748 : Computed embeddings
2019-03-13 23:29:44,748 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:30:14,839 : [('reg:1e-05', 62.6), ('reg:0.0001', 62.72), ('reg:0.001', 62.9), ('reg:0.01', 61.36)]
2019-03-13 23:30:14,839 : Validation : best param found is reg = 0.001 with score             62.9
2019-03-13 23:30:14,839 : Evaluating...
2019-03-13 23:30:22,421 : 
Dev acc : 62.9 Test acc : 62.7 for COORDINATIONINVERSION classification

2019-03-13 23:30:22,423 : total results: {'STS12': {'MSRpar': {'pearson': (0.2771977895697898, 1.0695205982224695e-14), 'spearman': SpearmanrResult(correlation=0.3417216521396472, pvalue=5.725644509957735e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.33975761726352194, 1.0144452058402085e-21), 'spearman': SpearmanrResult(correlation=0.4579065725791475, pvalue=3.806675318251419e-40), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3774178693576757, 5.493520977938132e-17), 'spearman': SpearmanrResult(correlation=0.49369966711602015, pvalue=1.418504448973876e-29), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.27377748445286626, 2.3207988403926075e-14), 'spearman': SpearmanrResult(correlation=0.4304649844328067, pvalue=3.479540732917967e-35), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.48790766444751504, 2.9681213996344455e-25), 'spearman': SpearmanrResult(correlation=0.5195300811486311, pvalue=5.804998765264692e-29), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3512116850182737, 'wmean': 0.3333203438270158}, 'spearman': {'mean': 0.4486645914832505, 'wmean': 0.436444837982065}}}, 'STS13': {'FNWN': {'pearson': (0.1494843458756854, 0.04007390856727995), 'spearman': SpearmanrResult(correlation=0.1514339911948724, pvalue=0.037518803823660914), 'nsamples': 189}, 'headlines': {'pearson': (0.4033827956579613, 1.0239879171106056e-30), 'spearman': SpearmanrResult(correlation=0.523321834662338, pvalue=5.819472832883447e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.2974565845623934, 6.332485868417697e-13), 'spearman': SpearmanrResult(correlation=0.40253712733520536, pvalue=2.8892614265715527e-23), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2834412420320134, 'wmean': 0.3317751880356522}, 'spearman': {'mean': 0.3590976510641386, 'wmean': 0.4312904858450897}}}, 'STS14': {'deft-forum': {'pearson': (0.06159532243692536, 0.19215224776114379), 'spearman': SpearmanrResult(correlation=0.14322809932769248, pvalue=0.0023222181799411935), 'nsamples': 450}, 'deft-news': {'pearson': (0.6587274086904145, 1.0531287927874064e-38), 'spearman': SpearmanrResult(correlation=0.6078003389245912, pvalue=1.0950392978098125e-31), 'nsamples': 300}, 'headlines': {'pearson': (0.3313294880198501, 1.1276049252539825e-20), 'spearman': SpearmanrResult(correlation=0.46688073208723313, pvalue=7.203762215356087e-42), 'nsamples': 750}, 'images': {'pearson': (0.3048844475957899, 1.3443101798887523e-17), 'spearman': SpearmanrResult(correlation=0.4609335094037944, pvalue=1.0118297061026535e-40), 'nsamples': 750}, 'OnWN': {'pearson': (0.3103794730258317, 3.2688485148037577e-18), 'spearman': SpearmanrResult(correlation=0.4795946308988378, pvalue=2.1245215997682972e-44), 'nsamples': 750}, 'tweet-news': {'pearson': (0.29993958436013896, 4.678297935746264e-17), 'spearman': SpearmanrResult(correlation=0.38030362677351176, pvalue=3.2121371213704866e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3278092873548251, 'wmean': 0.3093962299879863}, 'spearman': {'mean': 0.4231234895692768, 'wmean': 0.42335389886596586}}}, 'STS15': {'answers-forums': {'pearson': (0.4623560939994756, 2.926408014144453e-21), 'spearman': SpearmanrResult(correlation=0.4619562658715862, pvalue=3.1971635301909185e-21), 'nsamples': 375}, 'answers-students': {'pearson': (0.31341349558992093, 1.478234319067881e-18), 'spearman': SpearmanrResult(correlation=0.42187417680104544, pvalue=1.0080191148788936e-33), 'nsamples': 750}, 'belief': {'pearson': (0.4858723527478194, 1.3028240615275479e-23), 'spearman': SpearmanrResult(correlation=0.5622165621401682, pvalue=1.233177159957377e-32), 'nsamples': 375}, 'headlines': {'pearson': (0.46107539460784625, 9.505879930046398e-41), 'spearman': SpearmanrResult(correlation=0.5501825098470908, pvalue=1.4513871574863647e-60), 'nsamples': 750}, 'images': {'pearson': (0.4926428653596814, 4.1436612004738424e-47), 'spearman': SpearmanrResult(correlation=0.5356386170745558, pvalue=6.468742969807691e-57), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4430720404609487, 'wmean': 0.43531149473277403}, 'spearman': {'mean': 0.5063736263468893, 'wmean': 0.5049454294321423}}}, 'STS16': {'answer-answer': {'pearson': (0.3490079203726128, 1.0933981406077618e-08), 'spearman': SpearmanrResult(correlation=0.37415640881651835, pvalue=7.32937148220703e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.4466099772250002, 1.3069426927008111e-13), 'spearman': SpearmanrResult(correlation=0.5479417442369382, pvalue=6.510980807571254e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6502336805297767, 4.9860597088025724e-29), 'spearman': SpearmanrResult(correlation=0.6659098717869445, pvalue=7.681901425125466e-31), 'nsamples': 230}, 'postediting': {'pearson': (0.7101223405190713, 9.567336892637477e-39), 'spearman': SpearmanrResult(correlation=0.7480968980180316, pvalue=5.4189798928973835e-45), 'nsamples': 244}, 'question-question': {'pearson': (0.36045275604053734, 8.273395904735745e-08), 'spearman': SpearmanrResult(correlation=0.3951932710239278, pvalue=3.184284014132665e-09), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5032853349373997, 'wmean': 0.5042260705941342}, 'spearman': {'mean': 0.5462596387764721, 'wmean': 0.5478613232932471}}}, 'MR': {'devacc': 79.79, 'acc': 76.0, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 82.55, 'acc': 80.46, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.98, 'acc': 83.6, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.27, 'acc': 93.67, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.01, 'acc': 87.64, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.96, 'acc': 43.35, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.6, 'acc': 71.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.64, 'acc': 72.0, 'f1': 79.42, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 77.0, 'acc': 74.22, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7559475869640457, 'pearson': 0.7563816855984655, 'spearman': 0.6957083619296187, 'mse': 0.4370618573782994, 'yhat': array([3.23981334, 4.16142907, 2.43813752, ..., 3.09503576, 4.57294863,        4.63501238]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6555088741581299, 'pearson': 0.6119464492789388, 'spearman': 0.6091410149554003, 'mse': 1.52161974035571, 'yhat': array([1.70790627, 1.51968938, 2.74311396, ..., 3.68537161, 3.89022708,        3.62018413]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.54, 'acc': 66.69, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 59.38, 'acc': 60.11, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 9.25, 'acc': 9.05, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 24.28, 'acc': 24.1, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 48.47, 'acc': 47.76, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.26, 'acc': 88.36, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.92, 'acc': 87.99, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.96, 'acc': 74.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.08, 'acc': 76.59, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.6, 'acc': 63.17, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 62.9, 'acc': 62.69, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 23:30:22,424 : STS12 p=0.3333, STS12 s=0.4364, STS13 p=0.3318, STS13 s=0.4313, STS14 p=0.3094, STS14 s=0.4234, STS15 p=0.4353, STS15 s=0.5049, STS 16 p=0.5042, STS16 s=0.5479, STS B p=0.6119, STS B s=0.6091, STS B m=1.5216, SICK-R p=0.7564, SICK-R s=0.6957, SICK-P m=0.4371
2019-03-13 23:30:22,424 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 23:30:22,424 : 0.3333,0.4364,0.3318,0.4313,0.3094,0.4234,0.4353,0.5049,0.5042,0.5479,0.6119,0.6091,1.5216,0.7564,0.6957,0.4371
2019-03-13 23:30:22,424 : MR=76.00, CR=80.46, SUBJ=93.67, MPQA=83.60, SST-B=87.64, SST-F=43.35, TREC=71.00, SICK-E=74.22, SNLI=66.69, MRPC=72.00, MRPC f=79.42
2019-03-13 23:30:22,424 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 23:30:22,424 : 76.00,80.46,93.67,83.60,87.64,43.35,71.00,74.22,66.69,72.00,79.42
2019-03-13 23:30:22,424 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 23:30:22,424 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 23:30:22,424 : na,na,na,na,na,na,na,na,na,na
2019-03-13 23:30:22,424 : SentLen=60.11, WC=9.05, TreeDepth=24.10, TopConst=47.76, BShift=88.36, Tense=87.99, SubjNum=74.30, ObjNum=76.59, SOMO=63.17, CoordInv=62.69, average=59.41
2019-03-13 23:30:22,424 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 23:30:22,424 : 60.11,9.05,24.10,47.76,88.36,87.99,74.30,76.59,63.17,62.69,59.41
