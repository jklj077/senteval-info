2019-03-12 11:38:07,212 : ********************************************************************************
2019-03-12 11:38:07,212 : ********************************************************************************
2019-03-12 11:38:07,212 : ********************************************************************************
2019-03-12 11:38:07,212 : layer 0
2019-03-12 11:38:07,212 : ********************************************************************************
2019-03-12 11:38:07,213 : ********************************************************************************
2019-03-12 11:38:07,213 : ********************************************************************************
2019-03-12 11:38:07,213 : ***** Transfer task : STS12 *****


2019-03-12 11:38:07,247 : loading BERT model bert-large-uncased
2019-03-12 11:38:07,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:38:07,266 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:38:07,266 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpscsj6vrn
2019-03-12 11:38:14,696 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:38:27,266 : MSRpar : pearson = 0.0630, spearman = 0.0672
2019-03-12 11:38:28,868 : MSRvid : pearson = 0.0063, spearman = 0.0185
2019-03-12 11:38:30,247 : SMTeuroparl : pearson = 0.2667, spearman = 0.2903
2019-03-12 11:38:33,135 : surprise.OnWN : pearson = 0.2902, spearman = 0.3860
2019-03-12 11:38:35,825 : surprise.SMTnews : pearson = 0.3159, spearman = 0.3387
2019-03-12 11:38:35,826 : ALL (weighted average) : Pearson = 0.1667,             Spearman = 0.2002
2019-03-12 11:38:35,826 : ALL (average) : Pearson = 0.1884,             Spearman = 0.2201

2019-03-12 11:38:35,826 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 11:38:35,835 : loading BERT model bert-large-uncased
2019-03-12 11:38:35,835 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:38:35,853 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:38:35,853 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9g8l2wz8
2019-03-12 11:38:43,300 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:38:51,305 : FNWN : pearson = 0.0366, spearman = 0.0447
2019-03-12 11:38:55,696 : headlines : pearson = -0.0153, spearman = -0.0022
2019-03-12 11:38:58,343 : OnWN : pearson = -0.2689, spearman = -0.2175
2019-03-12 11:38:58,343 : ALL (weighted average) : Pearson = -0.1036,             Spearman = -0.0768
2019-03-12 11:38:58,343 : ALL (average) : Pearson = -0.0825,             Spearman = -0.0583

2019-03-12 11:38:58,343 : ***** Transfer task : STS14 *****


2019-03-12 11:38:58,396 : loading BERT model bert-large-uncased
2019-03-12 11:38:58,396 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:38:58,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:38:58,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqa7pft9c
2019-03-12 11:39:05,861 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:39:14,761 : deft-forum : pearson = -0.1888, spearman = -0.1827
2019-03-12 11:39:18,165 : deft-news : pearson = -0.1175, spearman = -0.0909
2019-03-12 11:39:22,887 : headlines : pearson = -0.0065, spearman = -0.0121
2019-03-12 11:39:27,007 : images : pearson = 0.0767, spearman = 0.1052
2019-03-12 11:39:31,092 : OnWN : pearson = -0.1984, spearman = -0.1899
2019-03-12 11:39:36,512 : tweet-news : pearson = 0.0419, spearman = 0.0780
2019-03-12 11:39:36,512 : ALL (weighted average) : Pearson = -0.0493,             Spearman = -0.0330
2019-03-12 11:39:36,512 : ALL (average) : Pearson = -0.0654,             Spearman = -0.0487

2019-03-12 11:39:36,512 : ***** Transfer task : STS15 *****


2019-03-12 11:39:36,546 : loading BERT model bert-large-uncased
2019-03-12 11:39:36,546 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:39:36,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:39:36,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu15rhdpr
2019-03-12 11:39:44,049 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:39:52,909 : answers-forums : pearson = -0.0914, spearman = -0.0796
2019-03-12 11:39:56,259 : answers-students : pearson = 0.2118, spearman = 0.2282
2019-03-12 11:39:59,808 : belief : pearson = -0.1281, spearman = -0.1558
2019-03-12 11:40:04,469 : headlines : pearson = 0.0675, spearman = 0.0781
2019-03-12 11:40:08,155 : images : pearson = 0.0850, spearman = 0.1197
2019-03-12 11:40:08,155 : ALL (weighted average) : Pearson = 0.0636,             Spearman = 0.0771
2019-03-12 11:40:08,155 : ALL (average) : Pearson = 0.0290,             Spearman = 0.0381

2019-03-12 11:40:08,155 : ***** Transfer task : STS16 *****


2019-03-12 11:40:08,222 : loading BERT model bert-large-uncased
2019-03-12 11:40:08,222 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:40:08,239 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:40:08,239 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk46grfjn
2019-03-12 11:40:15,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:40:22,546 : answer-answer : pearson = 0.1569, spearman = 0.1885
2019-03-12 11:40:23,863 : headlines : pearson = 0.1311, spearman = 0.1396
2019-03-12 11:40:25,516 : plagiarism : pearson = 0.0484, spearman = 0.0800
2019-03-12 11:40:28,256 : postediting : pearson = 0.2666, spearman = 0.2459
2019-03-12 11:40:29,643 : question-question : pearson = -0.0519, spearman = -0.0713
2019-03-12 11:40:29,643 : ALL (weighted average) : Pearson = 0.1162,             Spearman = 0.1232
2019-03-12 11:40:29,643 : ALL (average) : Pearson = 0.1102,             Spearman = 0.1165

2019-03-12 11:40:29,643 : ***** Transfer task : MR *****


2019-03-12 11:40:29,661 : loading BERT model bert-large-uncased
2019-03-12 11:40:29,661 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:40:29,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:40:29,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoiu_zrv5
2019-03-12 11:40:37,175 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:40:42,370 : Generating sentence embeddings
2019-03-12 11:41:44,744 : Generated sentence embeddings
2019-03-12 11:41:44,744 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:41:59,564 : Best param found at split 1: l2reg = 1e-05                 with score 50.08
2019-03-12 11:42:14,595 : Best param found at split 2: l2reg = 1e-05                 with score 50.15
2019-03-12 11:42:29,815 : Best param found at split 3: l2reg = 1e-05                 with score 50.04
2019-03-12 11:42:46,339 : Best param found at split 4: l2reg = 0.01                 with score 50.22
2019-03-12 11:43:02,911 : Best param found at split 5: l2reg = 1e-05                 with score 50.09
2019-03-12 11:43:04,075 : Dev acc : 50.12 Test acc : 50.12

2019-03-12 11:43:04,077 : ***** Transfer task : CR *****


2019-03-12 11:43:04,088 : loading BERT model bert-large-uncased
2019-03-12 11:43:04,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:43:04,116 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:43:04,116 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4x6d35dc
2019-03-12 11:43:11,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:43:17,098 : Generating sentence embeddings
2019-03-12 11:43:32,938 : Generated sentence embeddings
2019-03-12 11:43:32,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:43:38,951 : Best param found at split 1: l2reg = 0.001                 with score 64.03
2019-03-12 11:43:44,762 : Best param found at split 2: l2reg = 1e-05                 with score 63.8
2019-03-12 11:43:50,134 : Best param found at split 3: l2reg = 1e-05                 with score 63.97
2019-03-12 11:43:56,098 : Best param found at split 4: l2reg = 1e-05                 with score 63.82
2019-03-12 11:44:02,182 : Best param found at split 5: l2reg = 1e-05                 with score 63.89
2019-03-12 11:44:02,594 : Dev acc : 63.9 Test acc : 63.76

2019-03-12 11:44:02,595 : ***** Transfer task : MPQA *****


2019-03-12 11:44:02,631 : loading BERT model bert-large-uncased
2019-03-12 11:44:02,631 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:44:02,649 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:44:02,649 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt8i0u4_f
2019-03-12 11:44:10,108 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:44:15,463 : Generating sentence embeddings
2019-03-12 11:44:29,662 : Generated sentence embeddings
2019-03-12 11:44:29,662 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:44:44,572 : Best param found at split 1: l2reg = 1e-05                 with score 68.78
2019-03-12 11:45:01,284 : Best param found at split 2: l2reg = 1e-05                 with score 68.78
2019-03-12 11:45:21,434 : Best param found at split 3: l2reg = 1e-05                 with score 68.77
2019-03-12 11:45:51,501 : Best param found at split 4: l2reg = 1e-05                 with score 68.77
2019-03-12 11:46:18,006 : Best param found at split 5: l2reg = 1e-05                 with score 68.77
2019-03-12 11:46:19,004 : Dev acc : 68.77 Test acc : 68.76

2019-03-12 11:46:19,005 : ***** Transfer task : SUBJ *****


2019-03-12 11:46:19,020 : loading BERT model bert-large-uncased
2019-03-12 11:46:19,021 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:46:19,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:46:19,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphq6b5i8j
2019-03-12 11:46:26,574 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:46:32,073 : Generating sentence embeddings
2019-03-12 11:47:31,561 : Generated sentence embeddings
2019-03-12 11:47:31,561 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 11:47:56,550 : Best param found at split 1: l2reg = 0.01                 with score 51.23
2019-03-12 11:48:16,135 : Best param found at split 2: l2reg = 0.0001                 with score 51.28
2019-03-12 11:48:32,375 : Best param found at split 3: l2reg = 1e-05                 with score 51.19
2019-03-12 11:48:46,023 : Best param found at split 4: l2reg = 1e-05                 with score 51.24
2019-03-12 11:49:06,281 : Best param found at split 5: l2reg = 1e-05                 with score 51.64
2019-03-12 11:49:07,420 : Dev acc : 51.32 Test acc : 51.1

2019-03-12 11:49:07,421 : ***** Transfer task : SST Binary classification *****


2019-03-12 11:49:07,570 : loading BERT model bert-large-uncased
2019-03-12 11:49:07,571 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:49:07,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:49:07,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpipg23j8u
2019-03-12 11:49:15,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:49:20,427 : Computing embedding for train
2019-03-12 11:52:53,382 : Computed train embeddings
2019-03-12 11:52:53,382 : Computing embedding for dev
2019-03-12 11:52:57,968 : Computed dev embeddings
2019-03-12 11:52:57,968 : Computing embedding for test
2019-03-12 11:53:07,650 : Computed test embeddings
2019-03-12 11:53:07,650 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:53:48,723 : [('reg:1e-05', 50.92), ('reg:0.0001', 50.92), ('reg:0.001', 50.92), ('reg:0.01', 50.92)]
2019-03-12 11:53:48,723 : Validation : best param found is reg = 1e-05 with score             50.92
2019-03-12 11:53:48,723 : Evaluating...
2019-03-12 11:53:56,997 : 
Dev acc : 50.92 Test acc : 49.92 for             SST Binary classification

2019-03-12 11:53:56,997 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 11:53:57,050 : loading BERT model bert-large-uncased
2019-03-12 11:53:57,050 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:53:57,072 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:53:57,072 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbpav8asd
2019-03-12 11:54:04,574 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:54:09,877 : Computing embedding for train
2019-03-12 11:54:52,083 : Computed train embeddings
2019-03-12 11:54:52,083 : Computing embedding for dev
2019-03-12 11:54:57,529 : Computed dev embeddings
2019-03-12 11:54:57,530 : Computing embedding for test
2019-03-12 11:55:08,183 : Computed test embeddings
2019-03-12 11:55:08,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:55:11,605 : [('reg:1e-05', 26.25), ('reg:0.0001', 25.34), ('reg:0.001', 25.7), ('reg:0.01', 25.79)]
2019-03-12 11:55:11,606 : Validation : best param found is reg = 1e-05 with score             26.25
2019-03-12 11:55:11,606 : Evaluating...
2019-03-12 11:55:12,243 : 
Dev acc : 26.25 Test acc : 28.64 for             SST Fine-Grained classification

2019-03-12 11:55:12,243 : ***** Transfer task : TREC *****


2019-03-12 11:55:12,256 : loading BERT model bert-large-uncased
2019-03-12 11:55:12,256 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:55:12,275 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:55:12,275 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpglu9ysy_
2019-03-12 11:55:19,736 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:55:41,263 : Computed train embeddings
2019-03-12 11:55:42,608 : Computed test embeddings
2019-03-12 11:55:42,609 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:55:54,770 : [('reg:1e-05', 24.52), ('reg:0.0001', 24.78), ('reg:0.001', 25.48), ('reg:0.01', 23.04)]
2019-03-12 11:55:54,770 : Cross-validation : best param found is reg = 0.001             with score 25.48
2019-03-12 11:55:54,770 : Evaluating...
2019-03-12 11:55:55,363 : 
Dev acc : 25.48 Test acc : 22.6             for TREC

2019-03-12 11:55:55,364 : ***** Transfer task : MRPC *****


2019-03-12 11:55:55,416 : loading BERT model bert-large-uncased
2019-03-12 11:55:55,416 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:55:55,440 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:55:55,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptpq0gxad
2019-03-12 11:56:02,878 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:56:08,085 : Computing embedding for train
2019-03-12 11:56:51,896 : Computed train embeddings
2019-03-12 11:56:51,896 : Computing embedding for test
2019-03-12 11:57:11,052 : Computed test embeddings
2019-03-12 11:57:11,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 11:57:20,268 : [('reg:1e-05', 67.54), ('reg:0.0001', 67.64), ('reg:0.001', 67.52), ('reg:0.01', 67.64)]
2019-03-12 11:57:20,268 : Cross-validation : best param found is reg = 0.0001             with score 67.64
2019-03-12 11:57:20,268 : Evaluating...
2019-03-12 11:57:20,745 : Dev acc : 67.64 Test acc 66.49; Test F1 79.87 for MRPC.

2019-03-12 11:57:20,745 : ***** Transfer task : SICK-Entailment*****


2019-03-12 11:57:20,770 : loading BERT model bert-large-uncased
2019-03-12 11:57:20,770 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:57:20,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:57:20,827 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1yaq7cig
2019-03-12 11:57:28,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:57:33,627 : Computing embedding for train
2019-03-12 11:57:58,101 : Computed train embeddings
2019-03-12 11:57:58,101 : Computing embedding for dev
2019-03-12 11:58:01,706 : Computed dev embeddings
2019-03-12 11:58:01,706 : Computing embedding for test
2019-03-12 11:58:27,532 : Computed test embeddings
2019-03-12 11:58:27,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 11:58:31,010 : [('reg:1e-05', 57.2), ('reg:0.0001', 56.4), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-03-12 11:58:31,011 : Validation : best param found is reg = 1e-05 with score             57.2
2019-03-12 11:58:31,011 : Evaluating...
2019-03-12 11:58:31,843 : 
Dev acc : 57.2 Test acc : 55.98 for                        SICK entailment

2019-03-12 11:58:31,843 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 11:58:31,870 : loading BERT model bert-large-uncased
2019-03-12 11:58:31,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 11:58:31,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 11:58:31,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkt0b2x22
2019-03-12 11:58:39,323 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 11:58:44,642 : Computing embedding for train
2019-03-12 11:59:07,814 : Computed train embeddings
2019-03-12 11:59:07,815 : Computing embedding for dev
2019-03-12 11:59:11,122 : Computed dev embeddings
2019-03-12 11:59:11,122 : Computing embedding for test
2019-03-12 11:59:35,298 : Computed test embeddings
2019-03-12 12:00:19,695 : Dev : Pearson 0.26782258181476737
2019-03-12 12:00:19,695 : Test : Pearson 0.2526071595298197 Spearman 0.2563698229385226 MSE 1.0091338449418106                        for SICK Relatedness

2019-03-12 12:00:19,698 : 

***** Transfer task : STSBenchmark*****


2019-03-12 12:00:19,736 : loading BERT model bert-large-uncased
2019-03-12 12:00:19,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:00:19,767 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:00:19,767 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzjp7m_t6
2019-03-12 12:00:27,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:00:32,508 : Computing embedding for train
2019-03-12 12:01:10,525 : Computed train embeddings
2019-03-12 12:01:10,525 : Computing embedding for dev
2019-03-12 12:01:22,582 : Computed dev embeddings
2019-03-12 12:01:22,583 : Computing embedding for test
2019-03-12 12:01:32,618 : Computed test embeddings
2019-03-12 12:02:25,274 : Dev : Pearson 0.056752655937466236
2019-03-12 12:02:25,274 : Test : Pearson 0.1421899939014837 Spearman 0.14076662416766939 MSE 2.549586601428863                        for SICK Relatedness

2019-03-12 12:02:25,274 : ***** Transfer task : SNLI Entailment*****


2019-03-12 12:02:30,173 : loading BERT model bert-large-uncased
2019-03-12 12:02:30,174 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:02:30,305 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:02:30,305 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxnsofbpj
2019-03-12 12:02:37,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:02:43,797 : PROGRESS (encoding): 0.00%
2019-03-12 12:08:49,421 : PROGRESS (encoding): 14.56%
2019-03-12 12:15:26,215 : PROGRESS (encoding): 29.12%
2019-03-12 12:22:01,751 : PROGRESS (encoding): 43.69%
2019-03-12 12:28:59,674 : PROGRESS (encoding): 58.25%
2019-03-12 12:36:25,186 : PROGRESS (encoding): 72.81%
2019-03-12 12:44:16,170 : PROGRESS (encoding): 87.37%
2019-03-12 12:52:10,539 : PROGRESS (encoding): 0.00%
2019-03-12 12:53:10,212 : PROGRESS (encoding): 0.00%
2019-03-12 12:54:06,148 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 12:55:20,881 : [('reg:1e-09', 37.38)]
2019-03-12 12:55:20,881 : Validation : best param found is reg = 1e-09 with score             37.38
2019-03-12 12:55:20,881 : Evaluating...
2019-03-12 12:56:45,201 : Dev acc : 37.38 Test acc : 37.76 for SNLI

2019-03-12 12:56:45,202 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 12:56:45,414 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 12:56:46,461 : loading BERT model bert-large-uncased
2019-03-12 12:56:46,461 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 12:56:46,489 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 12:56:46,489 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdmt22vco
2019-03-12 12:56:53,944 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 12:56:59,446 : Computing embeddings for train/dev/test
2019-03-12 13:04:10,777 : Computed embeddings
2019-03-12 13:04:10,777 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:04:55,718 : [('reg:1e-05', 83.54), ('reg:0.0001', 83.54), ('reg:0.001', 86.65), ('reg:0.01', 86.65)]
2019-03-12 13:04:55,719 : Validation : best param found is reg = 0.001 with score             86.65
2019-03-12 13:04:55,719 : Evaluating...
2019-03-12 13:05:06,696 : 
Dev acc : 86.7 Test acc : 87.2 for LENGTH classification

2019-03-12 13:05:06,697 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 13:05:07,046 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 13:05:07,091 : loading BERT model bert-large-uncased
2019-03-12 13:05:07,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:05:07,119 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:05:07,119 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa21zr9uv
2019-03-12 13:05:14,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:05:19,953 : Computing embeddings for train/dev/test
2019-03-12 13:11:12,967 : Computed embeddings
2019-03-12 13:11:12,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:11:40,368 : [('reg:1e-05', 0.15), ('reg:0.0001', 0.14), ('reg:0.001', 0.14), ('reg:0.01', 0.14)]
2019-03-12 13:11:40,369 : Validation : best param found is reg = 1e-05 with score             0.15
2019-03-12 13:11:40,369 : Evaluating...
2019-03-12 13:11:44,169 : 
Dev acc : 0.1 Test acc : 0.2 for WORDCONTENT classification

2019-03-12 13:11:44,170 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 13:11:44,529 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 13:11:44,594 : loading BERT model bert-large-uncased
2019-03-12 13:11:44,594 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:11:44,619 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:11:44,619 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa_5bi45c
2019-03-12 13:11:52,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:11:57,255 : Computing embeddings for train/dev/test
2019-03-12 13:15:06,989 : Computed embeddings
2019-03-12 13:15:06,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:15:30,787 : [('reg:1e-05', 18.06), ('reg:0.0001', 18.07), ('reg:0.001', 18.07), ('reg:0.01', 18.07)]
2019-03-12 13:15:30,787 : Validation : best param found is reg = 0.0001 with score             18.07
2019-03-12 13:15:30,788 : Evaluating...
2019-03-12 13:15:38,310 : 
Dev acc : 18.1 Test acc : 17.9 for DEPTH classification

2019-03-12 13:15:38,311 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 13:15:38,697 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 13:15:38,759 : loading BERT model bert-large-uncased
2019-03-12 13:15:38,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:15:38,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:15:38,785 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaih3x2l7
2019-03-12 13:15:46,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:15:51,343 : Computing embeddings for train/dev/test
2019-03-12 13:18:47,420 : Computed embeddings
2019-03-12 13:18:47,420 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:19:12,770 : [('reg:1e-05', 7.92), ('reg:0.0001', 7.63), ('reg:0.001', 7.97), ('reg:0.01', 7.1)]
2019-03-12 13:19:12,770 : Validation : best param found is reg = 0.001 with score             7.97
2019-03-12 13:19:12,770 : Evaluating...
2019-03-12 13:19:18,803 : 
Dev acc : 8.0 Test acc : 7.7 for TOPCONSTITUENTS classification

2019-03-12 13:19:18,804 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 13:19:19,160 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 13:19:19,226 : loading BERT model bert-large-uncased
2019-03-12 13:19:19,226 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:19:19,254 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:19:19,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_m0wkn3z
2019-03-12 13:19:26,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:19:31,963 : Computing embeddings for train/dev/test
2019-03-12 13:25:44,114 : Computed embeddings
2019-03-12 13:25:44,114 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:26:32,681 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:26:32,681 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:26:32,681 : Evaluating...
2019-03-12 13:26:44,126 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-03-12 13:26:44,127 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 13:26:44,517 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 13:26:44,583 : loading BERT model bert-large-uncased
2019-03-12 13:26:44,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:26:44,698 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:26:44,698 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprmksytb2
2019-03-12 13:26:52,184 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:26:57,480 : Computing embeddings for train/dev/test
2019-03-12 13:33:00,620 : Computed embeddings
2019-03-12 13:33:00,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:34:01,474 : [('reg:1e-05', 56.26), ('reg:0.0001', 56.26), ('reg:0.001', 56.26), ('reg:0.01', 56.26)]
2019-03-12 13:34:01,474 : Validation : best param found is reg = 1e-05 with score             56.26
2019-03-12 13:34:01,474 : Evaluating...
2019-03-12 13:34:15,329 : 
Dev acc : 56.3 Test acc : 56.2 for TENSE classification

2019-03-12 13:34:15,331 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 13:34:15,916 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 13:34:15,979 : loading BERT model bert-large-uncased
2019-03-12 13:34:15,980 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:34:16,005 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:34:16,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzpt9e0s8
2019-03-12 13:34:23,425 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:34:28,843 : Computing embeddings for train/dev/test
2019-03-12 13:41:00,282 : Computed embeddings
2019-03-12 13:41:00,282 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:41:46,542 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:41:46,542 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:41:46,542 : Evaluating...
2019-03-12 13:41:58,680 : 
Dev acc : 50.0 Test acc : 50.0 for SUBJNUMBER classification

2019-03-12 13:41:58,681 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 13:41:59,089 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 13:41:59,155 : loading BERT model bert-large-uncased
2019-03-12 13:41:59,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:41:59,278 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:41:59,278 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9ksr25ee
2019-03-12 13:42:06,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:42:12,191 : Computing embeddings for train/dev/test
2019-03-12 13:48:33,892 : Computed embeddings
2019-03-12 13:48:33,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:49:17,668 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 13:49:17,669 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 13:49:17,669 : Evaluating...
2019-03-12 13:49:28,840 : 
Dev acc : 50.0 Test acc : 50.0 for OBJNUMBER classification

2019-03-12 13:49:28,841 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 13:49:29,273 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 13:49:29,341 : loading BERT model bert-large-uncased
2019-03-12 13:49:29,341 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:49:29,368 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:49:29,368 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdjpj14ym
2019-03-12 13:49:36,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:49:42,219 : Computing embeddings for train/dev/test
2019-03-12 13:57:03,120 : Computed embeddings
2019-03-12 13:57:03,120 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 13:58:04,548 : [('reg:1e-05', 50.84), ('reg:0.0001', 50.84), ('reg:0.001', 50.84), ('reg:0.01', 50.87)]
2019-03-12 13:58:04,548 : Validation : best param found is reg = 0.01 with score             50.87
2019-03-12 13:58:04,548 : Evaluating...
2019-03-12 13:58:17,847 : 
Dev acc : 50.9 Test acc : 50.7 for ODDMANOUT classification

2019-03-12 13:58:17,848 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 13:58:18,225 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 13:58:18,313 : loading BERT model bert-large-uncased
2019-03-12 13:58:18,313 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 13:58:18,345 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 13:58:18,346 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplztf83ti
2019-03-12 13:58:25,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 13:58:31,271 : Computing embeddings for train/dev/test
2019-03-12 14:05:51,987 : Computed embeddings
2019-03-12 14:05:51,987 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:07:04,143 : [('reg:1e-05', 50.55), ('reg:0.0001', 50.55), ('reg:0.001', 50.55), ('reg:0.01', 50.55)]
2019-03-12 14:07:04,143 : Validation : best param found is reg = 1e-05 with score             50.55
2019-03-12 14:07:04,143 : Evaluating...
2019-03-12 14:07:21,298 : 
Dev acc : 50.5 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-03-12 14:07:21,300 : total results: {'STS12': {'MSRpar': {'pearson': (0.06296028729094905, 0.08487545591543953), 'spearman': SpearmanrResult(correlation=0.0672370664687612, pvalue=0.06571326143359404), 'nsamples': 750}, 'MSRvid': {'pearson': (0.006331825804891061, 0.8625597002640498), 'spearman': SpearmanrResult(correlation=0.018456281707781998, pvalue=0.6138074822751423), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.26665551922468894, 6.5247252426072244e-09), 'spearman': SpearmanrResult(correlation=0.29028498928421886, pvalue=2.3096400777364808e-10), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.29020855073625756, 5.078836309580629e-16), 'spearman': SpearmanrResult(correlation=0.3859726944707595, pvalue=4.71500675145965e-28), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.3158712650606169, 1.076861382773921e-10), 'spearman': SpearmanrResult(correlation=0.33867926527097153, pvalue=3.643797700096699e-12), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.1884054896234807, 'wmean': 0.1666837245680153}, 'spearman': {'mean': 0.22012605944049862, 'wmean': 0.20016839411520307}}}, 'STS13': {'FNWN': {'pearson': (0.03657825216515479, 0.6172869704849497), 'spearman': SpearmanrResult(correlation=0.044711599473915384, pvalue=0.5412586975733622), 'nsamples': 189}, 'headlines': {'pearson': (-0.015253369196525078, 0.6766358162503494), 'spearman': SpearmanrResult(correlation=-0.0022168619419089884, pvalue=0.9516697488120842), 'nsamples': 750}, 'OnWN': {'pearson': (-0.2689358817403824, 9.460536512177592e-11), 'spearman': SpearmanrResult(correlation=-0.21753364505342404, pvalue=1.957538078229506e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': -0.08253699959058423, 'wmean': -0.10359984459635606}, 'spearman': {'mean': -0.058346302507139215, 'wmean': -0.07683235268722176}}}, 'STS14': {'deft-forum': {'pearson': (-0.1888305944400851, 5.556825872423822e-05), 'spearman': SpearmanrResult(correlation=-0.18274574924179227, pvalue=9.670253352151727e-05), 'nsamples': 450}, 'deft-news': {'pearson': (-0.11750100756183227, 0.04197871892889891), 'spearman': SpearmanrResult(correlation=-0.0909078387353825, pvalue=0.11612419594976366), 'nsamples': 300}, 'headlines': {'pearson': (-0.006483247017568659, 0.8593067219344138), 'spearman': SpearmanrResult(correlation=-0.012130694420145477, pvalue=0.7401378223745327), 'nsamples': 750}, 'images': {'pearson': (0.07670998675098957, 0.03569533466868472), 'spearman': SpearmanrResult(correlation=0.10521597854162293, pvalue=0.00391833424507196), 'nsamples': 750}, 'OnWN': {'pearson': (-0.1984327450893135, 4.257535594360085e-08), 'spearman': SpearmanrResult(correlation=-0.18994763158701858, pvalue=1.5967360835919894e-07), 'nsamples': 750}, 'tweet-news': {'pearson': (0.0418769588129321, 0.2520280278406539), 'spearman': SpearmanrResult(correlation=0.07803488911921412, pvalue=0.032616864409476545), 'nsamples': 750}, 'all': {'pearson': {'mean': -0.0654434414241463, 'wmean': -0.04932556124634889}, 'spearman': {'mean': -0.04874684105391696, 'wmean': -0.03296760867711107}}}, 'STS15': {'answers-forums': {'pearson': (-0.09137904372220053, 0.07717143157957758), 'spearman': SpearmanrResult(correlation=-0.07957440038864853, pvalue=0.12398814251730578), 'nsamples': 375}, 'answers-students': {'pearson': (0.21182834068246736, 4.682331348315223e-09), 'spearman': SpearmanrResult(correlation=0.22823476129677928, pvalue=2.553905512399912e-10), 'nsamples': 750}, 'belief': {'pearson': (-0.12814740195959157, 0.013008858239915762), 'spearman': SpearmanrResult(correlation=-0.1558344047224215, pvalue=0.0024768630138028995), 'nsamples': 375}, 'headlines': {'pearson': (0.06752843309910099, 0.06454913538947325), 'spearman': SpearmanrResult(correlation=0.07809377284096802, pvalue=0.03248548232324772), 'nsamples': 750}, 'images': {'pearson': (0.08495884136972466, 0.019964128942923067), 'spearman': SpearmanrResult(correlation=0.11969423703275457, pvalue=0.0010223298366497549), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.028957833893900185, 'wmean': 0.06363809807759924}, 'spearman': {'mean': 0.038122793211886365, 'wmean': 0.07707959215374172}}}, 'STS16': {'answer-answer': {'pearson': (0.1569440709326934, 0.012263183619438007), 'spearman': SpearmanrResult(correlation=0.18845165390383292, pvalue=0.0025640597440176687), 'nsamples': 254}, 'headlines': {'pearson': (0.1310828698957976, 0.03873680216764798), 'spearman': SpearmanrResult(correlation=0.13959124403573295, pvalue=0.02763541051364613), 'nsamples': 249}, 'plagiarism': {'pearson': (0.048378737034224316, 0.4653107084859299), 'spearman': SpearmanrResult(correlation=0.07997495205096337, pvalue=0.22696720610002305), 'nsamples': 230}, 'postediting': {'pearson': (0.2665911915204275, 2.448259441194553e-05), 'spearman': SpearmanrResult(correlation=0.245868338170457, pvalue=0.00010419876243026598), 'nsamples': 244}, 'question-question': {'pearson': (-0.05188297199263281, 0.45562653905378525), 'spearman': SpearmanrResult(correlation=-0.07134706401476744, pvalue=0.30462044165792473), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.110222779478102, 'wmean': 0.11621858998596404}, 'spearman': {'mean': 0.11650782482924378, 'wmean': 0.12318677652841294}}}, 'MR': {'devacc': 50.12, 'acc': 50.12, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 63.9, 'acc': 63.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 68.77, 'acc': 68.76, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 51.32, 'acc': 51.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 50.92, 'acc': 49.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 26.25, 'acc': 28.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 25.48, 'acc': 22.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.64, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 57.2, 'acc': 55.98, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.26782258181476737, 'pearson': 0.2526071595298197, 'spearman': 0.2563698229385226, 'mse': 1.0091338449418106, 'yhat': array([3.13835686, 2.80045309, 3.20466483, ..., 3.10261664, 3.27607261,        3.96564548]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.056752655937466236, 'pearson': 0.1421899939014837, 'spearman': 0.14076662416766939, 'mse': 2.549586601428863, 'yhat': array([2.53046431, 3.25451286, 2.40802224, ..., 2.70171478, 2.97907243,        3.33767717]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 37.38, 'acc': 37.76, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 86.65, 'acc': 87.15, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.15, 'acc': 0.16, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 18.07, 'acc': 17.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 7.97, 'acc': 7.65, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 56.26, 'acc': 56.19, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.87, 'acc': 50.67, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.55, 'acc': 50.0, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 14:07:21,300 : STS12 p=0.1667, STS12 s=0.2002, STS13 p=-0.1036, STS13 s=-0.0768, STS14 p=-0.0493, STS14 s=-0.0330, STS15 p=0.0636, STS15 s=0.0771, STS 16 p=0.1162, STS16 s=0.1232, STS B p=0.1422, STS B s=0.1408, STS B m=2.5496, SICK-R p=0.2526, SICK-R s=0.2564, SICK-P m=1.0091
2019-03-12 14:07:21,300 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 14:07:21,300 : 0.1667,0.2002,-0.1036,-0.0768,-0.0493,-0.0330,0.0636,0.0771,0.1162,0.1232,0.1422,0.1408,2.5496,0.2526,0.2564,1.0091
2019-03-12 14:07:21,300 : MR=50.12, CR=63.76, SUBJ=51.10, MPQA=68.76, SST-B=49.92, SST-F=28.64, TREC=22.60, SICK-E=55.98, SNLI=37.76, MRPC=66.49, MRPC f=79.87
2019-03-12 14:07:21,300 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 14:07:21,301 : 50.12,63.76,51.10,68.76,49.92,28.64,22.60,55.98,37.76,66.49,79.87
2019-03-12 14:07:21,301 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 14:07:21,301 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 14:07:21,301 : na,na,na,na,na,na,na,na,na,na
2019-03-12 14:07:21,301 : SentLen=87.15, WC=0.16, TreeDepth=17.88, TopConst=7.65, BShift=50.00, Tense=56.19, SubjNum=50.00, ObjNum=50.00, SOMO=50.67, CoordInv=50.00, average=41.97
2019-03-12 14:07:21,301 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 14:07:21,301 : 87.15,0.16,17.88,7.65,50.00,56.19,50.00,50.00,50.67,50.00,41.97
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,301 : layer 1
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,301 : ********************************************************************************
2019-03-12 14:07:21,395 : ***** Transfer task : STS12 *****


2019-03-12 14:07:21,408 : loading BERT model bert-large-uncased
2019-03-12 14:07:21,408 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:07:21,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:07:21,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfwfwsh_b
2019-03-12 14:07:28,881 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:07:42,120 : MSRpar : pearson = 0.0973, spearman = 0.1134
2019-03-12 14:07:45,592 : MSRvid : pearson = 0.0673, spearman = 0.1225
2019-03-12 14:07:48,418 : SMTeuroparl : pearson = 0.2873, spearman = 0.3680
2019-03-12 14:07:54,535 : surprise.OnWN : pearson = 0.3620, spearman = 0.4680
2019-03-12 14:07:57,484 : surprise.SMTnews : pearson = 0.3158, spearman = 0.3588
2019-03-12 14:07:57,485 : ALL (weighted average) : Pearson = 0.2101,             Spearman = 0.2702
2019-03-12 14:07:57,485 : ALL (average) : Pearson = 0.2260,             Spearman = 0.2861

2019-03-12 14:07:57,485 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 14:07:57,493 : loading BERT model bert-large-uncased
2019-03-12 14:07:57,493 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:07:57,511 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:07:57,511 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2jybbj1c
2019-03-12 14:08:04,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:08:12,624 : FNWN : pearson = 0.1000, spearman = 0.1174
2019-03-12 14:08:16,740 : headlines : pearson = 0.0822, spearman = 0.1042
2019-03-12 14:08:19,535 : OnWN : pearson = -0.2303, spearman = -0.1781
2019-03-12 14:08:19,535 : ALL (weighted average) : Pearson = -0.0324,             Spearman = 0.0003
2019-03-12 14:08:19,535 : ALL (average) : Pearson = -0.0160,             Spearman = 0.0145

2019-03-12 14:08:19,536 : ***** Transfer task : STS14 *****


2019-03-12 14:08:19,553 : loading BERT model bert-large-uncased
2019-03-12 14:08:19,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:08:19,570 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:08:19,570 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9coyecfa
2019-03-12 14:08:27,091 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:08:35,249 : deft-forum : pearson = -0.1604, spearman = -0.1377
2019-03-12 14:08:38,293 : deft-news : pearson = -0.0861, spearman = -0.0360
2019-03-12 14:08:42,594 : headlines : pearson = 0.0905, spearman = 0.1082
2019-03-12 14:08:46,559 : images : pearson = 0.1187, spearman = 0.1739
2019-03-12 14:08:50,359 : OnWN : pearson = -0.1436, spearman = -0.0942
2019-03-12 14:08:55,901 : tweet-news : pearson = 0.1195, spearman = 0.1540
2019-03-12 14:08:55,901 : ALL (weighted average) : Pearson = 0.0109,             Spearman = 0.0490
2019-03-12 14:08:55,902 : ALL (average) : Pearson = -0.0102,             Spearman = 0.0280

2019-03-12 14:08:55,902 : ***** Transfer task : STS15 *****


2019-03-12 14:08:55,948 : loading BERT model bert-large-uncased
2019-03-12 14:08:55,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:08:55,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:08:55,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2hsiir0w
2019-03-12 14:09:03,425 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:09:12,234 : answers-forums : pearson = -0.0589, spearman = -0.0474
2019-03-12 14:09:16,277 : answers-students : pearson = 0.2858, spearman = 0.3253
2019-03-12 14:09:20,277 : belief : pearson = -0.1002, spearman = -0.1172
2019-03-12 14:09:24,852 : headlines : pearson = 0.1774, spearman = 0.2091
2019-03-12 14:09:29,250 : images : pearson = 0.1515, spearman = 0.2076
2019-03-12 14:09:29,250 : ALL (weighted average) : Pearson = 0.1338,             Spearman = 0.1649
2019-03-12 14:09:29,250 : ALL (average) : Pearson = 0.0911,             Spearman = 0.1155

2019-03-12 14:09:29,250 : ***** Transfer task : STS16 *****


2019-03-12 14:09:29,328 : loading BERT model bert-large-uncased
2019-03-12 14:09:29,328 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:09:29,348 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:09:29,348 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7zg__2i8
2019-03-12 14:09:36,804 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:09:43,664 : answer-answer : pearson = 0.1990, spearman = 0.2346
2019-03-12 14:09:44,777 : headlines : pearson = 0.2155, spearman = 0.2645
2019-03-12 14:09:46,392 : plagiarism : pearson = 0.1051, spearman = 0.1465
2019-03-12 14:09:49,245 : postediting : pearson = 0.3280, spearman = 0.3679
2019-03-12 14:09:50,581 : question-question : pearson = -0.0311, spearman = -0.0074
2019-03-12 14:09:50,581 : ALL (weighted average) : Pearson = 0.1703,             Spearman = 0.2086
2019-03-12 14:09:50,581 : ALL (average) : Pearson = 0.1633,             Spearman = 0.2012

2019-03-12 14:09:50,581 : ***** Transfer task : MR *****


2019-03-12 14:09:50,598 : loading BERT model bert-large-uncased
2019-03-12 14:09:50,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:09:50,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:09:50,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph3b2cak1
2019-03-12 14:09:58,088 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:10:03,443 : Generating sentence embeddings
2019-03-12 14:11:01,935 : Generated sentence embeddings
2019-03-12 14:11:01,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:11:32,160 : Best param found at split 1: l2reg = 0.001                 with score 64.6
2019-03-12 14:11:54,569 : Best param found at split 2: l2reg = 1e-05                 with score 63.82
2019-03-12 14:12:10,911 : Best param found at split 3: l2reg = 1e-05                 with score 61.34
2019-03-12 14:12:29,019 : Best param found at split 4: l2reg = 0.0001                 with score 62.95
2019-03-12 14:12:52,625 : Best param found at split 5: l2reg = 0.0001                 with score 61.64
2019-03-12 14:12:54,072 : Dev acc : 62.87 Test acc : 62.36

2019-03-12 14:12:54,073 : ***** Transfer task : CR *****


2019-03-12 14:12:54,081 : loading BERT model bert-large-uncased
2019-03-12 14:12:54,081 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:12:54,100 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:12:54,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5l9egiz1
2019-03-12 14:13:01,589 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:13:07,010 : Generating sentence embeddings
2019-03-12 14:13:25,563 : Generated sentence embeddings
2019-03-12 14:13:25,563 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:13:32,148 : Best param found at split 1: l2reg = 0.001                 with score 66.35
2019-03-12 14:13:38,753 : Best param found at split 2: l2reg = 1e-05                 with score 67.81
2019-03-12 14:13:44,688 : Best param found at split 3: l2reg = 0.0001                 with score 69.44
2019-03-12 14:13:50,124 : Best param found at split 4: l2reg = 0.0001                 with score 70.31
2019-03-12 14:13:57,476 : Best param found at split 5: l2reg = 1e-05                 with score 68.75
2019-03-12 14:13:57,649 : Dev acc : 68.53 Test acc : 68.69

2019-03-12 14:13:57,649 : ***** Transfer task : MPQA *****


2019-03-12 14:13:57,691 : loading BERT model bert-large-uncased
2019-03-12 14:13:57,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:13:57,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:13:57,713 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvmoe7cdp
2019-03-12 14:14:05,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:14:10,407 : Generating sentence embeddings
2019-03-12 14:14:26,939 : Generated sentence embeddings
2019-03-12 14:14:26,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:14:46,529 : Best param found at split 1: l2reg = 0.001                 with score 78.49
2019-03-12 14:15:08,010 : Best param found at split 2: l2reg = 0.0001                 with score 79.66
2019-03-12 14:15:31,091 : Best param found at split 3: l2reg = 0.001                 with score 80.47
2019-03-12 14:15:59,034 : Best param found at split 4: l2reg = 1e-05                 with score 78.88
2019-03-12 14:16:22,527 : Best param found at split 5: l2reg = 0.0001                 with score 81.19
2019-03-12 14:16:23,716 : Dev acc : 79.74 Test acc : 79.54

2019-03-12 14:16:23,717 : ***** Transfer task : SUBJ *****


2019-03-12 14:16:23,731 : loading BERT model bert-large-uncased
2019-03-12 14:16:23,731 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:16:23,752 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:16:23,752 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeq5g03zd
2019-03-12 14:16:31,192 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:16:36,479 : Generating sentence embeddings
2019-03-12 14:17:38,428 : Generated sentence embeddings
2019-03-12 14:17:38,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 14:17:57,137 : Best param found at split 1: l2reg = 1e-05                 with score 85.51
2019-03-12 14:18:19,776 : Best param found at split 2: l2reg = 0.0001                 with score 86.37
2019-03-12 14:18:45,411 : Best param found at split 3: l2reg = 1e-05                 with score 85.82
2019-03-12 14:19:07,576 : Best param found at split 4: l2reg = 0.0001                 with score 86.61
2019-03-12 14:19:31,091 : Best param found at split 5: l2reg = 1e-05                 with score 86.02
2019-03-12 14:19:32,408 : Dev acc : 86.07 Test acc : 86.08

2019-03-12 14:19:32,409 : ***** Transfer task : SST Binary classification *****


2019-03-12 14:19:32,502 : loading BERT model bert-large-uncased
2019-03-12 14:19:32,502 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:19:32,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:19:32,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp186iul2d
2019-03-12 14:19:40,013 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:19:45,363 : Computing embedding for train
2019-03-12 14:23:18,100 : Computed train embeddings
2019-03-12 14:23:18,100 : Computing embedding for dev
2019-03-12 14:23:22,660 : Computed dev embeddings
2019-03-12 14:23:22,661 : Computing embedding for test
2019-03-12 14:23:31,390 : Computed test embeddings
2019-03-12 14:23:31,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:24:15,118 : [('reg:1e-05', 74.66), ('reg:0.0001', 75.23), ('reg:0.001', 72.25), ('reg:0.01', 67.66)]
2019-03-12 14:24:15,118 : Validation : best param found is reg = 0.0001 with score             75.23
2019-03-12 14:24:15,118 : Evaluating...
2019-03-12 14:24:28,917 : 
Dev acc : 75.23 Test acc : 74.41 for             SST Binary classification

2019-03-12 14:24:28,917 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 14:24:28,972 : loading BERT model bert-large-uncased
2019-03-12 14:24:28,972 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:24:28,992 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:24:28,992 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgrv9orue
2019-03-12 14:24:36,441 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:24:41,941 : Computing embedding for train
2019-03-12 14:25:24,527 : Computed train embeddings
2019-03-12 14:25:24,527 : Computing embedding for dev
2019-03-12 14:25:30,127 : Computed dev embeddings
2019-03-12 14:25:30,127 : Computing embedding for test
2019-03-12 14:25:40,851 : Computed test embeddings
2019-03-12 14:25:40,852 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:25:46,782 : [('reg:1e-05', 33.97), ('reg:0.0001', 31.06), ('reg:0.001', 32.43), ('reg:0.01', 33.15)]
2019-03-12 14:25:46,782 : Validation : best param found is reg = 1e-05 with score             33.97
2019-03-12 14:25:46,782 : Evaluating...
2019-03-12 14:25:48,236 : 
Dev acc : 33.97 Test acc : 35.93 for             SST Fine-Grained classification

2019-03-12 14:25:48,236 : ***** Transfer task : TREC *****


2019-03-12 14:25:48,250 : loading BERT model bert-large-uncased
2019-03-12 14:25:48,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:25:48,269 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:25:48,269 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm5uhg5n3
2019-03-12 14:25:55,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:26:17,121 : Computed train embeddings
2019-03-12 14:26:18,514 : Computed test embeddings
2019-03-12 14:26:18,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:26:33,237 : [('reg:1e-05', 51.51), ('reg:0.0001', 51.45), ('reg:0.001', 46.92), ('reg:0.01', 40.11)]
2019-03-12 14:26:33,237 : Cross-validation : best param found is reg = 1e-05             with score 51.51
2019-03-12 14:26:33,237 : Evaluating...
2019-03-12 14:26:33,976 : 
Dev acc : 51.51 Test acc : 72.4             for TREC

2019-03-12 14:26:33,976 : ***** Transfer task : MRPC *****


2019-03-12 14:26:33,998 : loading BERT model bert-large-uncased
2019-03-12 14:26:33,998 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:26:34,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:26:34,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_3f82f6
2019-03-12 14:26:41,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:26:46,649 : Computing embedding for train
2019-03-12 14:27:30,151 : Computed train embeddings
2019-03-12 14:27:30,152 : Computing embedding for test
2019-03-12 14:27:49,346 : Computed test embeddings
2019-03-12 14:27:49,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 14:27:59,521 : [('reg:1e-05', 68.52), ('reg:0.0001', 68.03), ('reg:0.001', 67.96), ('reg:0.01', 68.15)]
2019-03-12 14:27:59,522 : Cross-validation : best param found is reg = 1e-05             with score 68.52
2019-03-12 14:27:59,522 : Evaluating...
2019-03-12 14:28:00,081 : Dev acc : 68.52 Test acc 66.49; Test F1 79.87 for MRPC.

2019-03-12 14:28:00,082 : ***** Transfer task : SICK-Entailment*****


2019-03-12 14:28:00,108 : loading BERT model bert-large-uncased
2019-03-12 14:28:00,108 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:28:00,128 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:28:00,128 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplbzllrn_
2019-03-12 14:28:07,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:28:12,790 : Computing embedding for train
2019-03-12 14:28:37,183 : Computed train embeddings
2019-03-12 14:28:37,183 : Computing embedding for dev
2019-03-12 14:28:40,523 : Computed dev embeddings
2019-03-12 14:28:40,523 : Computing embedding for test
2019-03-12 14:29:07,574 : Computed test embeddings
2019-03-12 14:29:07,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 14:29:10,567 : [('reg:1e-05', 63.2), ('reg:0.0001', 61.6), ('reg:0.001', 63.4), ('reg:0.01', 64.2)]
2019-03-12 14:29:10,567 : Validation : best param found is reg = 0.01 with score             64.2
2019-03-12 14:29:10,567 : Evaluating...
2019-03-12 14:29:11,389 : 
Dev acc : 64.2 Test acc : 63.37 for                        SICK entailment

2019-03-12 14:29:11,389 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 14:29:11,454 : loading BERT model bert-large-uncased
2019-03-12 14:29:11,455 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:29:11,474 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:29:11,474 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp83yacma9
2019-03-12 14:29:18,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:29:24,350 : Computing embedding for train
2019-03-12 14:29:48,526 : Computed train embeddings
2019-03-12 14:29:48,526 : Computing embedding for dev
2019-03-12 14:29:52,173 : Computed dev embeddings
2019-03-12 14:29:52,173 : Computing embedding for test
2019-03-12 14:30:19,223 : Computed test embeddings
2019-03-12 14:31:11,182 : Dev : Pearson 0.6163631537028949
2019-03-12 14:31:11,183 : Test : Pearson 0.6735230689667705 Spearman 0.6151298327727824 MSE 0.5630886388662021                        for SICK Relatedness

2019-03-12 14:31:11,183 : 

***** Transfer task : STSBenchmark*****


2019-03-12 14:31:11,252 : loading BERT model bert-large-uncased
2019-03-12 14:31:11,252 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:31:11,272 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:31:11,272 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg3d_frin
2019-03-12 14:31:18,712 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:31:24,074 : Computing embedding for train
2019-03-12 14:32:02,790 : Computed train embeddings
2019-03-12 14:32:02,790 : Computing embedding for dev
2019-03-12 14:32:14,440 : Computed dev embeddings
2019-03-12 14:32:14,440 : Computing embedding for test
2019-03-12 14:32:24,233 : Computed test embeddings
2019-03-12 14:33:08,022 : Dev : Pearson 0.4662601196030512
2019-03-12 14:33:08,022 : Test : Pearson 0.5185587846527417 Spearman 0.5151013680687135 MSE 1.8752909682825354                        for SICK Relatedness

2019-03-12 14:33:08,022 : ***** Transfer task : SNLI Entailment*****


2019-03-12 14:33:12,787 : loading BERT model bert-large-uncased
2019-03-12 14:33:12,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 14:33:12,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 14:33:12,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1zpbkxgb
2019-03-12 14:33:20,305 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 14:33:27,386 : PROGRESS (encoding): 0.00%
2019-03-12 14:39:28,549 : PROGRESS (encoding): 14.56%
2019-03-12 14:46:07,306 : PROGRESS (encoding): 29.12%
2019-03-12 14:51:13,529 : PROGRESS (encoding): 43.69%
2019-03-12 14:54:40,962 : PROGRESS (encoding): 58.25%
2019-03-12 14:58:30,525 : PROGRESS (encoding): 72.81%
2019-03-12 15:04:51,613 : PROGRESS (encoding): 87.37%
2019-03-12 15:12:59,763 : PROGRESS (encoding): 0.00%
2019-03-12 15:13:56,409 : PROGRESS (encoding): 0.00%
2019-03-12 15:14:49,564 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:16:03,209 : [('reg:1e-09', 59.74)]
2019-03-12 15:16:03,209 : Validation : best param found is reg = 1e-09 with score             59.74
2019-03-12 15:16:03,209 : Evaluating...
2019-03-12 15:17:27,559 : Dev acc : 59.74 Test acc : 59.07 for SNLI

2019-03-12 15:17:27,559 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 15:17:27,774 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 15:17:28,746 : loading BERT model bert-large-uncased
2019-03-12 15:17:28,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:17:28,772 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:17:28,772 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0oaqv0qt
2019-03-12 15:17:36,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:17:41,658 : Computing embeddings for train/dev/test
2019-03-12 15:24:45,646 : Computed embeddings
2019-03-12 15:24:45,646 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:25:28,797 : [('reg:1e-05', 88.88), ('reg:0.0001', 88.45), ('reg:0.001', 87.03), ('reg:0.01', 86.71)]
2019-03-12 15:25:28,797 : Validation : best param found is reg = 1e-05 with score             88.88
2019-03-12 15:25:28,797 : Evaluating...
2019-03-12 15:25:38,872 : 
Dev acc : 88.9 Test acc : 89.4 for LENGTH classification

2019-03-12 15:25:38,873 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 15:25:39,240 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 15:25:39,284 : loading BERT model bert-large-uncased
2019-03-12 15:25:39,285 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:25:39,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:25:39,313 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr1t4f2rk
2019-03-12 15:25:46,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:25:52,113 : Computing embeddings for train/dev/test
2019-03-12 15:32:28,282 : Computed embeddings
2019-03-12 15:32:28,282 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:33:36,743 : [('reg:1e-05', 4.19), ('reg:0.0001', 0.57), ('reg:0.001', 0.16), ('reg:0.01', 0.14)]
2019-03-12 15:33:36,743 : Validation : best param found is reg = 1e-05 with score             4.19
2019-03-12 15:33:36,743 : Evaluating...
2019-03-12 15:34:00,991 : 
Dev acc : 4.2 Test acc : 4.6 for WORDCONTENT classification

2019-03-12 15:34:00,993 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 15:34:01,368 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 15:34:01,441 : loading BERT model bert-large-uncased
2019-03-12 15:34:01,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:34:01,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:34:01,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwidrto5l
2019-03-12 15:34:08,884 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:34:14,171 : Computing embeddings for train/dev/test
2019-03-12 15:40:34,918 : Computed embeddings
2019-03-12 15:40:34,918 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:41:19,429 : [('reg:1e-05', 26.11), ('reg:0.0001', 27.12), ('reg:0.001', 24.38), ('reg:0.01', 23.11)]
2019-03-12 15:41:19,429 : Validation : best param found is reg = 0.0001 with score             27.12
2019-03-12 15:41:19,429 : Evaluating...
2019-03-12 15:41:31,219 : 
Dev acc : 27.1 Test acc : 27.3 for DEPTH classification

2019-03-12 15:41:31,220 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 15:41:31,623 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 15:41:31,687 : loading BERT model bert-large-uncased
2019-03-12 15:41:31,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:41:31,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:41:31,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfgwxc02d
2019-03-12 15:41:39,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:41:44,620 : Computing embeddings for train/dev/test
2019-03-12 15:47:39,669 : Computed embeddings
2019-03-12 15:47:39,669 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:48:43,673 : [('reg:1e-05', 50.0), ('reg:0.0001', 47.44), ('reg:0.001', 36.2), ('reg:0.01', 16.01)]
2019-03-12 15:48:43,673 : Validation : best param found is reg = 1e-05 with score             50.0
2019-03-12 15:48:43,674 : Evaluating...
2019-03-12 15:48:58,177 : 
Dev acc : 50.0 Test acc : 50.5 for TOPCONSTITUENTS classification

2019-03-12 15:48:58,178 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 15:48:58,542 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 15:48:58,610 : loading BERT model bert-large-uncased
2019-03-12 15:48:58,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:48:58,737 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:48:58,738 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk55j7l2c
2019-03-12 15:49:06,187 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:49:11,544 : Computing embeddings for train/dev/test
2019-03-12 15:55:26,548 : Computed embeddings
2019-03-12 15:55:26,548 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 15:56:09,920 : [('reg:1e-05', 50.11), ('reg:0.0001', 50.09), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 15:56:09,920 : Validation : best param found is reg = 1e-05 with score             50.11
2019-03-12 15:56:09,920 : Evaluating...
2019-03-12 15:56:22,431 : 
Dev acc : 50.1 Test acc : 50.1 for BIGRAMSHIFT classification

2019-03-12 15:56:22,432 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 15:56:23,012 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 15:56:23,077 : loading BERT model bert-large-uncased
2019-03-12 15:56:23,077 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 15:56:23,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 15:56:23,107 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp01y0tz4_
2019-03-12 15:56:30,635 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 15:56:36,060 : Computing embeddings for train/dev/test
2019-03-12 16:02:44,467 : Computed embeddings
2019-03-12 16:02:44,467 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:03:46,023 : [('reg:1e-05', 82.52), ('reg:0.0001', 82.29), ('reg:0.001', 81.21), ('reg:0.01', 72.44)]
2019-03-12 16:03:46,023 : Validation : best param found is reg = 1e-05 with score             82.52
2019-03-12 16:03:46,023 : Evaluating...
2019-03-12 16:04:00,023 : 
Dev acc : 82.5 Test acc : 81.3 for TENSE classification

2019-03-12 16:04:00,024 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 16:04:00,423 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 16:04:00,490 : loading BERT model bert-large-uncased
2019-03-12 16:04:00,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:04:00,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:04:00,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqwdha4a0
2019-03-12 16:04:07,954 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:04:13,246 : Computing embeddings for train/dev/test
2019-03-12 16:10:43,356 : Computed embeddings
2019-03-12 16:10:43,356 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:11:42,065 : [('reg:1e-05', 73.65), ('reg:0.0001', 73.63), ('reg:0.001', 72.96), ('reg:0.01', 66.04)]
2019-03-12 16:11:42,065 : Validation : best param found is reg = 1e-05 with score             73.65
2019-03-12 16:11:42,065 : Evaluating...
2019-03-12 16:11:53,838 : 
Dev acc : 73.7 Test acc : 72.3 for SUBJNUMBER classification

2019-03-12 16:11:53,839 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 16:11:54,273 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 16:11:54,343 : loading BERT model bert-large-uncased
2019-03-12 16:11:54,343 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:11:54,372 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:11:54,372 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvpix2gmz
2019-03-12 16:12:01,823 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:12:07,380 : Computing embeddings for train/dev/test
2019-03-12 16:18:27,995 : Computed embeddings
2019-03-12 16:18:27,996 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:19:32,728 : [('reg:1e-05', 71.18), ('reg:0.0001', 71.3), ('reg:0.001', 65.74), ('reg:0.01', 59.27)]
2019-03-12 16:19:32,728 : Validation : best param found is reg = 0.0001 with score             71.3
2019-03-12 16:19:32,728 : Evaluating...
2019-03-12 16:19:48,097 : 
Dev acc : 71.3 Test acc : 72.5 for OBJNUMBER classification

2019-03-12 16:19:48,098 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 16:19:48,510 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 16:19:48,583 : loading BERT model bert-large-uncased
2019-03-12 16:19:48,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:19:48,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:19:48,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiyknlyvi
2019-03-12 16:19:56,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:20:01,716 : Computing embeddings for train/dev/test
2019-03-12 16:27:25,270 : Computed embeddings
2019-03-12 16:27:25,270 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:28:19,784 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-03-12 16:28:19,784 : Validation : best param found is reg = 1e-05 with score             50.19
2019-03-12 16:28:19,784 : Evaluating...
2019-03-12 16:28:30,903 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-03-12 16:28:30,904 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 16:28:31,330 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 16:28:31,406 : loading BERT model bert-large-uncased
2019-03-12 16:28:31,407 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:28:31,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:28:31,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpggsalb8s
2019-03-12 16:28:39,059 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:28:44,487 : Computing embeddings for train/dev/test
2019-03-12 16:33:19,629 : Computed embeddings
2019-03-12 16:33:19,629 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:33:43,781 : [('reg:1e-05', 51.6), ('reg:0.0001', 51.54), ('reg:0.001', 50.93), ('reg:0.01', 50.29)]
2019-03-12 16:33:43,781 : Validation : best param found is reg = 1e-05 with score             51.6
2019-03-12 16:33:43,781 : Evaluating...
2019-03-12 16:33:49,240 : 
Dev acc : 51.6 Test acc : 51.8 for COORDINATIONINVERSION classification

2019-03-12 16:33:49,242 : total results: {'STS12': {'MSRpar': {'pearson': (0.09731303010261835, 0.007655125761746537), 'spearman': SpearmanrResult(correlation=0.1133763399280062, pvalue=0.001872188867076769), 'nsamples': 750}, 'MSRvid': {'pearson': (0.06730120329708993, 0.06545553761274776), 'spearman': SpearmanrResult(correlation=0.12248726027892284, pvalue=0.0007751415415828801), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.287337946607766, 3.565080031270016e-10), 'spearman': SpearmanrResult(correlation=0.3680109860634945, pvalue=3.619402918555104e-16), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.36202981282306973, 1.211934439794307e-24), 'spearman': SpearmanrResult(correlation=0.4679567862555959, pvalue=4.441237723049755e-42), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.3158290903379998, 1.0833440376440777e-10), 'spearman': SpearmanrResult(correlation=0.35876146133799086, pvalue=1.4544805023304676e-13), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.22596221663370875, 'wmean': 0.21006658919076898}, 'spearman': {'mean': 0.286118566772802, 'wmean': 0.27024715428696783}}}, 'STS13': {'FNWN': {'pearson': (0.09999687965437887, 0.17098557356388702), 'spearman': SpearmanrResult(correlation=0.11741582320568691, pvalue=0.10760259212964599), 'nsamples': 189}, 'headlines': {'pearson': (0.08222218778112689, 0.024334576226857675), 'spearman': SpearmanrResult(correlation=0.10423551544548666, pvalue=0.004268130364131954), 'nsamples': 750}, 'OnWN': {'pearson': (-0.2302607418303483, 3.4689875164673923e-08), 'spearman': SpearmanrResult(correlation=-0.1780717287859438, pvalue=2.2128895809678486e-05), 'nsamples': 561}, 'all': {'pearson': {'mean': -0.016013891464947517, 'wmean': -0.03240681671753509}, 'spearman': {'mean': 0.014526536621743266, 'wmean': 0.00031332488071691007}}}, 'STS14': {'deft-forum': {'pearson': (-0.16040023018223898, 0.0006371706848422411), 'spearman': SpearmanrResult(correlation=-0.13768005535650496, pvalue=0.003428434111443272), 'nsamples': 450}, 'deft-news': {'pearson': (-0.08613867091877143, 0.13661777276856338), 'spearman': SpearmanrResult(correlation=-0.03602076879954489, pvalue=0.5342738703027545), 'nsamples': 300}, 'headlines': {'pearson': (0.09046534479949221, 0.013195579354408356), 'spearman': SpearmanrResult(correlation=0.10824129520748516, pvalue=0.0029965640188218453), 'nsamples': 750}, 'images': {'pearson': (0.11869125932733826, 0.0011275896914100592), 'spearman': SpearmanrResult(correlation=0.1739195112060209, pvalue=1.6545595656503148e-06), 'nsamples': 750}, 'OnWN': {'pearson': (-0.14358902104762508, 7.938303714514352e-05), 'spearman': SpearmanrResult(correlation=-0.09415442012574483, pvalue=0.009881247269338466), 'nsamples': 750}, 'tweet-news': {'pearson': (0.11954240945024583, 0.001037657983521409), 'spearman': SpearmanrResult(correlation=0.15398293103209837, pvalue=2.283012268805203e-05), 'nsamples': 750}, 'all': {'pearson': {'mean': -0.010238151428593198, 'wmean': 0.010882877210519849}, 'spearman': {'mean': 0.028048082193968288, 'wmean': 0.04899459531722772}}}, 'STS15': {'answers-forums': {'pearson': (-0.05885770474714729, 0.25555695289003477), 'spearman': SpearmanrResult(correlation=-0.04740130097898818, pvalue=0.359996143041754), 'nsamples': 375}, 'answers-students': {'pearson': (0.2858051543552907, 1.4502803699767719e-15), 'spearman': SpearmanrResult(correlation=0.32528818710808516, pvalue=6.05514278977449e-20), 'nsamples': 750}, 'belief': {'pearson': (-0.10019439540702006, 0.05254280296244196), 'spearman': SpearmanrResult(correlation=-0.1171735642875262, pvalue=0.02324987468462084), 'nsamples': 375}, 'headlines': {'pearson': (0.177415390992861, 1.011152628263953e-06), 'spearman': SpearmanrResult(correlation=0.2091347482600576, pvalue=7.386503393367171e-09), 'nsamples': 750}, 'images': {'pearson': (0.15149755527581005, 3.0988147403804686e-05), 'spearman': SpearmanrResult(correlation=0.20760563031437312, pvalue=9.542393460541417e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.09113320009395888, 'wmean': 0.1337980126367195}, 'spearman': {'mean': 0.11549074008320029, 'wmean': 0.16493528326231466}}}, 'STS16': {'answer-answer': {'pearson': (0.19902478321215974, 0.0014314006805168818), 'spearman': SpearmanrResult(correlation=0.2346083383772818, pvalue=0.00016103875357505236), 'nsamples': 254}, 'headlines': {'pearson': (0.21554530351607626, 0.0006159887359690719), 'spearman': SpearmanrResult(correlation=0.26449328312632786, pvalue=2.3559569990549598e-05), 'nsamples': 249}, 'plagiarism': {'pearson': (0.10507580997001063, 0.11199859623248462), 'spearman': SpearmanrResult(correlation=0.14651543172983789, pvalue=0.026289098530623153), 'nsamples': 230}, 'postediting': {'pearson': (0.3280057307620898, 1.579216629841995e-07), 'spearman': SpearmanrResult(correlation=0.3679460923414184, pvalue=3.0809528693661964e-09), 'nsamples': 244}, 'question-question': {'pearson': (-0.031128668785996027, 0.654563574009392), 'spearman': SpearmanrResult(correlation=-0.007443240051350549, pvalue=0.9148192840377101), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.16330459173486808, 'wmean': 0.17025128021430921}, 'spearman': {'mean': 0.2012239811047031, 'wmean': 0.208575973106848}}}, 'MR': {'devacc': 62.87, 'acc': 62.36, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 68.53, 'acc': 68.69, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 79.74, 'acc': 79.54, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.07, 'acc': 86.08, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 75.23, 'acc': 74.41, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.97, 'acc': 35.93, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 51.51, 'acc': 72.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.52, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 64.2, 'acc': 63.37, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6163631537028949, 'pearson': 0.6735230689667705, 'spearman': 0.6151298327727824, 'mse': 0.5630886388662021, 'yhat': array([3.47715437, 4.10468543, 1.13063593, ..., 3.21236312, 4.02127534,        4.07884447]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4662601196030512, 'pearson': 0.5185587846527417, 'spearman': 0.5151013680687135, 'mse': 1.8752909682825354, 'yhat': array([3.00045956, 2.58476099, 2.30473586, ..., 3.86726292, 3.48250979,        3.39442229]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.74, 'acc': 59.07, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 88.88, 'acc': 89.38, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 4.19, 'acc': 4.63, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.12, 'acc': 27.34, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 50.0, 'acc': 50.47, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.11, 'acc': 50.07, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 82.52, 'acc': 81.26, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.65, 'acc': 72.3, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 71.3, 'acc': 72.49, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 51.6, 'acc': 51.75, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 16:33:49,242 : STS12 p=0.2101, STS12 s=0.2702, STS13 p=-0.0324, STS13 s=0.0003, STS14 p=0.0109, STS14 s=0.0490, STS15 p=0.1338, STS15 s=0.1649, STS 16 p=0.1703, STS16 s=0.2086, STS B p=0.5186, STS B s=0.5151, STS B m=1.8753, SICK-R p=0.6735, SICK-R s=0.6151, SICK-P m=0.5631
2019-03-12 16:33:49,242 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 16:33:49,243 : 0.2101,0.2702,-0.0324,0.0003,0.0109,0.0490,0.1338,0.1649,0.1703,0.2086,0.5186,0.5151,1.8753,0.6735,0.6151,0.5631
2019-03-12 16:33:49,243 : MR=62.36, CR=68.69, SUBJ=86.08, MPQA=79.54, SST-B=74.41, SST-F=35.93, TREC=72.40, SICK-E=63.37, SNLI=59.07, MRPC=66.49, MRPC f=79.87
2019-03-12 16:33:49,243 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 16:33:49,243 : 62.36,68.69,86.08,79.54,74.41,35.93,72.40,63.37,59.07,66.49,79.87
2019-03-12 16:33:49,243 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 16:33:49,243 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 16:33:49,243 : na,na,na,na,na,na,na,na,na,na
2019-03-12 16:33:49,243 : SentLen=89.38, WC=4.63, TreeDepth=27.34, TopConst=50.47, BShift=50.07, Tense=81.26, SubjNum=72.30, ObjNum=72.49, SOMO=49.87, CoordInv=51.75, average=54.96
2019-03-12 16:33:49,243 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 16:33:49,243 : 89.38,4.63,27.34,50.47,50.07,81.26,72.30,72.49,49.87,51.75,54.96
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,243 : layer 2
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,243 : ********************************************************************************
2019-03-12 16:33:49,335 : ***** Transfer task : STS12 *****


2019-03-12 16:33:49,372 : loading BERT model bert-large-uncased
2019-03-12 16:33:49,373 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:33:49,389 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:33:49,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpczko49s9
2019-03-12 16:33:56,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:34:06,125 : MSRpar : pearson = 0.1254, spearman = 0.1383
2019-03-12 16:34:07,780 : MSRvid : pearson = 0.1765, spearman = 0.2204
2019-03-12 16:34:09,205 : SMTeuroparl : pearson = 0.3396, spearman = 0.4206
2019-03-12 16:34:11,931 : surprise.OnWN : pearson = 0.4214, spearman = 0.4967
2019-03-12 16:34:13,373 : surprise.SMTnews : pearson = 0.3865, spearman = 0.3993
2019-03-12 16:34:13,373 : ALL (weighted average) : Pearson = 0.2743,             Spearman = 0.3198
2019-03-12 16:34:13,373 : ALL (average) : Pearson = 0.2899,             Spearman = 0.3350

2019-03-12 16:34:13,373 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 16:34:13,383 : loading BERT model bert-large-uncased
2019-03-12 16:34:13,383 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:34:13,400 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:34:13,400 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph4wwc29e
2019-03-12 16:34:20,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:34:27,343 : FNWN : pearson = 0.1486, spearman = 0.1629
2019-03-12 16:34:29,250 : headlines : pearson = 0.2215, spearman = 0.2230
2019-03-12 16:34:30,735 : OnWN : pearson = -0.1432, spearman = -0.1071
2019-03-12 16:34:30,735 : ALL (weighted average) : Pearson = 0.0759,             Spearman = 0.0920
2019-03-12 16:34:30,735 : ALL (average) : Pearson = 0.0756,             Spearman = 0.0929

2019-03-12 16:34:30,735 : ***** Transfer task : STS14 *****


2019-03-12 16:34:30,778 : loading BERT model bert-large-uncased
2019-03-12 16:34:30,778 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:34:30,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:34:30,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp15xn416q
2019-03-12 16:34:38,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:34:44,790 : deft-forum : pearson = -0.0994, spearman = -0.0673
2019-03-12 16:34:46,442 : deft-news : pearson = 0.0252, spearman = 0.0648
2019-03-12 16:34:48,628 : headlines : pearson = 0.2142, spearman = 0.2113
2019-03-12 16:34:50,721 : images : pearson = 0.2110, spearman = 0.2515
2019-03-12 16:34:52,873 : OnWN : pearson = -0.0131, spearman = 0.0335
2019-03-12 16:34:55,765 : tweet-news : pearson = 0.2582, spearman = 0.2749
2019-03-12 16:34:55,765 : ALL (weighted average) : Pearson = 0.1242,             Spearman = 0.1514
2019-03-12 16:34:55,765 : ALL (average) : Pearson = 0.0994,             Spearman = 0.1281

2019-03-12 16:34:55,765 : ***** Transfer task : STS15 *****


2019-03-12 16:34:55,796 : loading BERT model bert-large-uncased
2019-03-12 16:34:55,796 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:34:55,847 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:34:55,847 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuzni4jgn
2019-03-12 16:35:03,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:10,510 : answers-forums : pearson = 0.0247, spearman = 0.0440
2019-03-12 16:35:12,614 : answers-students : pearson = 0.3777, spearman = 0.3978
2019-03-12 16:35:14,684 : belief : pearson = 0.0361, spearman = 0.0257
2019-03-12 16:35:16,964 : headlines : pearson = 0.3271, spearman = 0.3395
2019-03-12 16:35:19,117 : images : pearson = 0.2300, spearman = 0.2626
2019-03-12 16:35:19,117 : ALL (weighted average) : Pearson = 0.2413,             Spearman = 0.2587
2019-03-12 16:35:19,117 : ALL (average) : Pearson = 0.1991,             Spearman = 0.2139

2019-03-12 16:35:19,117 : ***** Transfer task : STS16 *****


2019-03-12 16:35:19,155 : loading BERT model bert-large-uncased
2019-03-12 16:35:19,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:35:19,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:35:19,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfjxziodf
2019-03-12 16:35:26,689 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:32,786 : answer-answer : pearson = 0.2222, spearman = 0.2429
2019-03-12 16:35:33,450 : headlines : pearson = 0.3595, spearman = 0.3702
2019-03-12 16:35:34,340 : plagiarism : pearson = 0.1622, spearman = 0.1857
2019-03-12 16:35:35,848 : postediting : pearson = 0.4400, spearman = 0.4692
2019-03-12 16:35:36,459 : question-question : pearson = 0.0438, spearman = 0.0529
2019-03-12 16:35:36,459 : ALL (weighted average) : Pearson = 0.2528,             Spearman = 0.2716
2019-03-12 16:35:36,460 : ALL (average) : Pearson = 0.2455,             Spearman = 0.2642

2019-03-12 16:35:36,460 : ***** Transfer task : MR *****


2019-03-12 16:35:36,505 : loading BERT model bert-large-uncased
2019-03-12 16:35:36,505 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:35:36,526 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:35:36,526 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmr735ygp
2019-03-12 16:35:43,984 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:35:49,269 : Generating sentence embeddings
2019-03-12 16:36:21,169 : Generated sentence embeddings
2019-03-12 16:36:21,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:36:31,609 : Best param found at split 1: l2reg = 0.0001                 with score 69.08
2019-03-12 16:36:43,002 : Best param found at split 2: l2reg = 0.0001                 with score 68.8
2019-03-12 16:36:52,989 : Best param found at split 3: l2reg = 0.0001                 with score 68.19
2019-03-12 16:37:04,842 : Best param found at split 4: l2reg = 1e-05                 with score 67.73
2019-03-12 16:37:14,591 : Best param found at split 5: l2reg = 1e-05                 with score 66.72
2019-03-12 16:37:15,201 : Dev acc : 68.1 Test acc : 67.83

2019-03-12 16:37:15,202 : ***** Transfer task : CR *****


2019-03-12 16:37:15,210 : loading BERT model bert-large-uncased
2019-03-12 16:37:15,210 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:37:15,260 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:37:15,261 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9vtwk809
2019-03-12 16:37:22,724 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:37:27,927 : Generating sentence embeddings
2019-03-12 16:37:36,321 : Generated sentence embeddings
2019-03-12 16:37:36,321 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:37:38,991 : Best param found at split 1: l2reg = 0.0001                 with score 73.2
2019-03-12 16:37:42,599 : Best param found at split 2: l2reg = 1e-05                 with score 72.14
2019-03-12 16:37:46,295 : Best param found at split 3: l2reg = 0.001                 with score 72.22
2019-03-12 16:37:50,234 : Best param found at split 4: l2reg = 0.001                 with score 73.39
2019-03-12 16:37:53,834 : Best param found at split 5: l2reg = 1e-05                 with score 71.4
2019-03-12 16:37:54,019 : Dev acc : 72.47 Test acc : 71.24

2019-03-12 16:37:54,020 : ***** Transfer task : MPQA *****


2019-03-12 16:37:54,026 : loading BERT model bert-large-uncased
2019-03-12 16:37:54,026 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:37:54,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:37:54,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp55s7kzht
2019-03-12 16:38:01,522 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:38:06,786 : Generating sentence embeddings
2019-03-12 16:38:14,418 : Generated sentence embeddings
2019-03-12 16:38:14,419 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:38:24,161 : Best param found at split 1: l2reg = 0.001                 with score 82.39
2019-03-12 16:38:35,035 : Best param found at split 2: l2reg = 0.0001                 with score 83.97
2019-03-12 16:38:45,493 : Best param found at split 3: l2reg = 1e-05                 with score 82.32
2019-03-12 16:38:57,424 : Best param found at split 4: l2reg = 1e-05                 with score 82.71
2019-03-12 16:39:09,187 : Best param found at split 5: l2reg = 1e-05                 with score 84.74
2019-03-12 16:39:09,954 : Dev acc : 83.23 Test acc : 83.13

2019-03-12 16:39:09,954 : ***** Transfer task : SUBJ *****


2019-03-12 16:39:09,970 : loading BERT model bert-large-uncased
2019-03-12 16:39:09,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:39:10,027 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:39:10,027 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4ofxe9cf
2019-03-12 16:39:17,438 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:39:22,703 : Generating sentence embeddings
2019-03-12 16:39:53,925 : Generated sentence embeddings
2019-03-12 16:39:53,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 16:40:04,213 : Best param found at split 1: l2reg = 1e-05                 with score 89.45
2019-03-12 16:40:15,652 : Best param found at split 2: l2reg = 0.0001                 with score 89.75
2019-03-12 16:40:25,328 : Best param found at split 3: l2reg = 1e-05                 with score 89.21
2019-03-12 16:40:35,807 : Best param found at split 4: l2reg = 1e-05                 with score 89.82
2019-03-12 16:40:45,309 : Best param found at split 5: l2reg = 1e-05                 with score 88.91
2019-03-12 16:40:45,989 : Dev acc : 89.43 Test acc : 88.7

2019-03-12 16:40:45,990 : ***** Transfer task : SST Binary classification *****


2019-03-12 16:40:46,128 : loading BERT model bert-large-uncased
2019-03-12 16:40:46,128 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:40:46,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:40:46,151 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptsszc4cq
2019-03-12 16:40:53,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:40:58,667 : Computing embedding for train
2019-03-12 16:42:39,786 : Computed train embeddings
2019-03-12 16:42:39,786 : Computing embedding for dev
2019-03-12 16:42:42,006 : Computed dev embeddings
2019-03-12 16:42:42,006 : Computing embedding for test
2019-03-12 16:42:46,707 : Computed test embeddings
2019-03-12 16:42:46,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:43:07,417 : [('reg:1e-05', 77.87), ('reg:0.0001', 77.64), ('reg:0.001', 76.49), ('reg:0.01', 72.48)]
2019-03-12 16:43:07,417 : Validation : best param found is reg = 1e-05 with score             77.87
2019-03-12 16:43:07,417 : Evaluating...
2019-03-12 16:43:13,141 : 
Dev acc : 77.87 Test acc : 78.09 for             SST Binary classification

2019-03-12 16:43:13,142 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 16:43:13,192 : loading BERT model bert-large-uncased
2019-03-12 16:43:13,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:43:13,214 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:43:13,214 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5zc2itm3
2019-03-12 16:43:20,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:43:25,909 : Computing embedding for train
2019-03-12 16:43:48,003 : Computed train embeddings
2019-03-12 16:43:48,003 : Computing embedding for dev
2019-03-12 16:43:50,886 : Computed dev embeddings
2019-03-12 16:43:50,887 : Computing embedding for test
2019-03-12 16:43:56,575 : Computed test embeddings
2019-03-12 16:43:56,575 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:43:59,158 : [('reg:1e-05', 37.24), ('reg:0.0001', 36.69), ('reg:0.001', 37.06), ('reg:0.01', 32.61)]
2019-03-12 16:43:59,159 : Validation : best param found is reg = 1e-05 with score             37.24
2019-03-12 16:43:59,159 : Evaluating...
2019-03-12 16:43:59,802 : 
Dev acc : 37.24 Test acc : 39.19 for             SST Fine-Grained classification

2019-03-12 16:43:59,803 : ***** Transfer task : TREC *****


2019-03-12 16:43:59,815 : loading BERT model bert-large-uncased
2019-03-12 16:43:59,815 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:43:59,834 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:43:59,834 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1i_mj97j
2019-03-12 16:44:07,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:44:20,045 : Computed train embeddings
2019-03-12 16:44:20,640 : Computed test embeddings
2019-03-12 16:44:20,640 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:44:28,195 : [('reg:1e-05', 60.22), ('reg:0.0001', 59.26), ('reg:0.001', 55.37), ('reg:0.01', 47.42)]
2019-03-12 16:44:28,195 : Cross-validation : best param found is reg = 1e-05             with score 60.22
2019-03-12 16:44:28,195 : Evaluating...
2019-03-12 16:44:28,639 : 
Dev acc : 60.22 Test acc : 75.4             for TREC

2019-03-12 16:44:28,640 : ***** Transfer task : MRPC *****


2019-03-12 16:44:28,661 : loading BERT model bert-large-uncased
2019-03-12 16:44:28,661 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:44:28,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:44:28,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk0egiya_
2019-03-12 16:44:36,205 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:44:41,387 : Computing embedding for train
2019-03-12 16:45:03,790 : Computed train embeddings
2019-03-12 16:45:03,791 : Computing embedding for test
2019-03-12 16:45:13,623 : Computed test embeddings
2019-03-12 16:45:13,643 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 16:45:18,717 : [('reg:1e-05', 68.62), ('reg:0.0001', 69.38), ('reg:0.001', 68.84), ('reg:0.01', 67.88)]
2019-03-12 16:45:18,717 : Cross-validation : best param found is reg = 0.0001             with score 69.38
2019-03-12 16:45:18,717 : Evaluating...
2019-03-12 16:45:19,107 : Dev acc : 69.38 Test acc 67.77; Test F1 75.27 for MRPC.

2019-03-12 16:45:19,107 : ***** Transfer task : SICK-Entailment*****


2019-03-12 16:45:19,132 : loading BERT model bert-large-uncased
2019-03-12 16:45:19,132 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:45:19,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:45:19,151 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppvck9hdy
2019-03-12 16:45:26,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:45:31,658 : Computing embedding for train
2019-03-12 16:45:43,032 : Computed train embeddings
2019-03-12 16:45:43,032 : Computing embedding for dev
2019-03-12 16:45:44,588 : Computed dev embeddings
2019-03-12 16:45:44,589 : Computing embedding for test
2019-03-12 16:45:56,853 : Computed test embeddings
2019-03-12 16:45:56,889 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 16:45:58,249 : [('reg:1e-05', 65.0), ('reg:0.0001', 67.2), ('reg:0.001', 64.2), ('reg:0.01', 59.2)]
2019-03-12 16:45:58,249 : Validation : best param found is reg = 0.0001 with score             67.2
2019-03-12 16:45:58,249 : Evaluating...
2019-03-12 16:45:58,647 : 
Dev acc : 67.2 Test acc : 66.75 for                        SICK entailment

2019-03-12 16:45:58,648 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 16:45:58,674 : loading BERT model bert-large-uncased
2019-03-12 16:45:58,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:45:58,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:45:58,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw514qk9v
2019-03-12 16:46:06,148 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:46:11,409 : Computing embedding for train
2019-03-12 16:46:22,825 : Computed train embeddings
2019-03-12 16:46:22,825 : Computing embedding for dev
2019-03-12 16:46:24,386 : Computed dev embeddings
2019-03-12 16:46:24,386 : Computing embedding for test
2019-03-12 16:46:36,637 : Computed test embeddings
2019-03-12 16:46:58,914 : Dev : Pearson 0.7274207069540313
2019-03-12 16:46:58,914 : Test : Pearson 0.7253335937842363 Spearman 0.6509156097576677 MSE 0.4851766304640681                        for SICK Relatedness

2019-03-12 16:46:58,915 : 

***** Transfer task : STSBenchmark*****


2019-03-12 16:46:58,978 : loading BERT model bert-large-uncased
2019-03-12 16:46:58,978 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:46:59,032 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:46:59,032 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn1sv9xe_
2019-03-12 16:47:06,455 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:47:11,626 : Computing embedding for train
2019-03-12 16:47:30,361 : Computed train embeddings
2019-03-12 16:47:30,361 : Computing embedding for dev
2019-03-12 16:47:36,062 : Computed dev embeddings
2019-03-12 16:47:36,062 : Computing embedding for test
2019-03-12 16:47:40,733 : Computed test embeddings
2019-03-12 16:47:59,445 : Dev : Pearson 0.591529155554082
2019-03-12 16:47:59,445 : Test : Pearson 0.577342481468638 Spearman 0.5635257872209847 MSE 1.8881113002534398                        for SICK Relatedness

2019-03-12 16:47:59,445 : ***** Transfer task : SNLI Entailment*****


2019-03-12 16:48:04,452 : loading BERT model bert-large-uncased
2019-03-12 16:48:04,452 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 16:48:04,576 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 16:48:04,576 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjafd84q6
2019-03-12 16:48:12,021 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 16:48:17,571 : PROGRESS (encoding): 0.00%
2019-03-12 16:51:08,945 : PROGRESS (encoding): 14.56%
2019-03-12 16:54:23,833 : PROGRESS (encoding): 29.12%
2019-03-12 16:57:36,811 : PROGRESS (encoding): 43.69%
2019-03-12 17:01:02,134 : PROGRESS (encoding): 58.25%
2019-03-12 17:04:50,298 : PROGRESS (encoding): 72.81%
2019-03-12 17:08:37,324 : PROGRESS (encoding): 87.37%
2019-03-12 17:12:41,659 : PROGRESS (encoding): 0.00%
2019-03-12 17:13:12,411 : PROGRESS (encoding): 0.00%
2019-03-12 17:13:42,263 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:14:28,287 : [('reg:1e-09', 64.95)]
2019-03-12 17:14:28,287 : Validation : best param found is reg = 1e-09 with score             64.95
2019-03-12 17:14:28,287 : Evaluating...
2019-03-12 17:15:15,888 : Dev acc : 64.95 Test acc : 64.91 for SNLI

2019-03-12 17:15:15,888 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 17:15:16,098 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 17:15:17,177 : loading BERT model bert-large-uncased
2019-03-12 17:15:17,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:15:17,202 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:15:17,202 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu9vl2t09
2019-03-12 17:15:24,648 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:15:29,949 : Computing embeddings for train/dev/test
2019-03-12 17:19:05,895 : Computed embeddings
2019-03-12 17:19:05,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:19:29,526 : [('reg:1e-05', 91.58), ('reg:0.0001', 88.92), ('reg:0.001', 87.07), ('reg:0.01', 86.42)]
2019-03-12 17:19:29,526 : Validation : best param found is reg = 1e-05 with score             91.58
2019-03-12 17:19:29,526 : Evaluating...
2019-03-12 17:19:36,946 : 
Dev acc : 91.6 Test acc : 92.2 for LENGTH classification

2019-03-12 17:19:36,946 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 17:19:37,308 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 17:19:37,354 : loading BERT model bert-large-uncased
2019-03-12 17:19:37,354 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:19:37,384 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:19:37,384 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvk1jywl6
2019-03-12 17:19:44,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:19:50,078 : Computing embeddings for train/dev/test
2019-03-12 17:23:10,245 : Computed embeddings
2019-03-12 17:23:10,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:23:39,988 : [('reg:1e-05', 15.92), ('reg:0.0001', 1.52), ('reg:0.001', 0.27), ('reg:0.01', 0.18)]
2019-03-12 17:23:39,988 : Validation : best param found is reg = 1e-05 with score             15.92
2019-03-12 17:23:39,989 : Evaluating...
2019-03-12 17:23:49,787 : 
Dev acc : 15.9 Test acc : 14.9 for WORDCONTENT classification

2019-03-12 17:23:49,788 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 17:23:50,150 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 17:23:50,217 : loading BERT model bert-large-uncased
2019-03-12 17:23:50,217 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:23:50,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:23:50,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyliebw9c
2019-03-12 17:23:57,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:24:02,998 : Computing embeddings for train/dev/test
2019-03-12 17:27:10,181 : Computed embeddings
2019-03-12 17:27:10,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:27:36,855 : [('reg:1e-05', 30.32), ('reg:0.0001', 28.53), ('reg:0.001', 25.97), ('reg:0.01', 21.61)]
2019-03-12 17:27:36,855 : Validation : best param found is reg = 1e-05 with score             30.32
2019-03-12 17:27:36,855 : Evaluating...
2019-03-12 17:27:44,230 : 
Dev acc : 30.3 Test acc : 31.3 for DEPTH classification

2019-03-12 17:27:44,231 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 17:27:44,603 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 17:27:44,666 : loading BERT model bert-large-uncased
2019-03-12 17:27:44,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:27:44,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:27:44,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8bc2heaw
2019-03-12 17:27:52,208 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:27:57,356 : Computing embeddings for train/dev/test
2019-03-12 17:30:51,154 : Computed embeddings
2019-03-12 17:30:51,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:31:23,764 : [('reg:1e-05', 49.18), ('reg:0.0001', 47.49), ('reg:0.001', 39.2), ('reg:0.01', 15.97)]
2019-03-12 17:31:23,764 : Validation : best param found is reg = 1e-05 with score             49.18
2019-03-12 17:31:23,765 : Evaluating...
2019-03-12 17:31:31,418 : 
Dev acc : 49.2 Test acc : 49.8 for TOPCONSTITUENTS classification

2019-03-12 17:31:31,419 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 17:31:31,791 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 17:31:31,857 : loading BERT model bert-large-uncased
2019-03-12 17:31:31,857 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:31:31,886 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:31:31,886 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvy8vzif3
2019-03-12 17:31:39,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:31:44,535 : Computing embeddings for train/dev/test
2019-03-12 17:34:53,394 : Computed embeddings
2019-03-12 17:34:53,395 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:35:21,170 : [('reg:1e-05', 51.32), ('reg:0.0001', 51.13), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-03-12 17:35:21,171 : Validation : best param found is reg = 1e-05 with score             51.32
2019-03-12 17:35:21,171 : Evaluating...
2019-03-12 17:35:29,738 : 
Dev acc : 51.3 Test acc : 50.9 for BIGRAMSHIFT classification

2019-03-12 17:35:29,739 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 17:35:30,128 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 17:35:30,192 : loading BERT model bert-large-uncased
2019-03-12 17:35:30,192 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:35:30,223 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:35:30,223 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpld69ps1v
2019-03-12 17:35:37,694 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:35:42,976 : Computing embeddings for train/dev/test
2019-03-12 17:38:48,033 : Computed embeddings
2019-03-12 17:38:48,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:39:13,627 : [('reg:1e-05', 83.14), ('reg:0.0001', 82.09), ('reg:0.001', 82.63), ('reg:0.01', 77.99)]
2019-03-12 17:39:13,627 : Validation : best param found is reg = 1e-05 with score             83.14
2019-03-12 17:39:13,627 : Evaluating...
2019-03-12 17:39:20,165 : 
Dev acc : 83.1 Test acc : 81.3 for TENSE classification

2019-03-12 17:39:20,166 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 17:39:20,570 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 17:39:20,632 : loading BERT model bert-large-uncased
2019-03-12 17:39:20,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:39:20,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:39:20,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnlupd1id
2019-03-12 17:39:28,184 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:39:33,378 : Computing embeddings for train/dev/test
2019-03-12 17:42:48,535 : Computed embeddings
2019-03-12 17:42:48,536 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:43:17,608 : [('reg:1e-05', 77.35), ('reg:0.0001', 77.17), ('reg:0.001', 75.02), ('reg:0.01', 70.44)]
2019-03-12 17:43:17,608 : Validation : best param found is reg = 1e-05 with score             77.35
2019-03-12 17:43:17,608 : Evaluating...
2019-03-12 17:43:22,254 : 
Dev acc : 77.3 Test acc : 76.3 for SUBJNUMBER classification

2019-03-12 17:43:22,255 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 17:43:22,670 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 17:43:22,746 : loading BERT model bert-large-uncased
2019-03-12 17:43:22,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:43:22,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:43:22,871 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5cz132r8
2019-03-12 17:43:30,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:43:35,547 : Computing embeddings for train/dev/test
2019-03-12 17:46:46,588 : Computed embeddings
2019-03-12 17:46:46,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:47:10,964 : [('reg:1e-05', 74.53), ('reg:0.0001', 74.62), ('reg:0.001', 72.44), ('reg:0.01', 65.72)]
2019-03-12 17:47:10,964 : Validation : best param found is reg = 0.0001 with score             74.62
2019-03-12 17:47:10,964 : Evaluating...
2019-03-12 17:47:18,448 : 
Dev acc : 74.6 Test acc : 77.1 for OBJNUMBER classification

2019-03-12 17:47:18,449 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 17:47:19,008 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 17:47:19,077 : loading BERT model bert-large-uncased
2019-03-12 17:47:19,077 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:47:19,104 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:47:19,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyja_i0t2
2019-03-12 17:47:26,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:47:31,765 : Computing embeddings for train/dev/test
2019-03-12 17:51:13,989 : Computed embeddings
2019-03-12 17:51:13,989 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:51:40,250 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-03-12 17:51:40,250 : Validation : best param found is reg = 1e-05 with score             50.19
2019-03-12 17:51:40,250 : Evaluating...
2019-03-12 17:51:46,582 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-03-12 17:51:46,583 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 17:51:46,958 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 17:51:47,037 : loading BERT model bert-large-uncased
2019-03-12 17:51:47,037 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:51:47,067 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:51:47,067 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppn9smfs2
2019-03-12 17:51:54,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:51:59,917 : Computing embeddings for train/dev/test
2019-03-12 17:55:37,267 : Computed embeddings
2019-03-12 17:55:37,268 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 17:55:56,240 : [('reg:1e-05', 52.43), ('reg:0.0001', 52.17), ('reg:0.001', 51.21), ('reg:0.01', 50.15)]
2019-03-12 17:55:56,240 : Validation : best param found is reg = 1e-05 with score             52.43
2019-03-12 17:55:56,241 : Evaluating...
2019-03-12 17:56:01,209 : 
Dev acc : 52.4 Test acc : 52.3 for COORDINATIONINVERSION classification

2019-03-12 17:56:01,211 : total results: {'STS12': {'MSRpar': {'pearson': (0.1253892510689326, 0.0005778767607092797), 'spearman': SpearmanrResult(correlation=0.13829493707048018, pvalue=0.00014509594408333623), 'nsamples': 750}, 'MSRvid': {'pearson': (0.17649554909472553, 1.152122279323163e-06), 'spearman': SpearmanrResult(correlation=0.2204042942504227, pvalue=1.0531846919903346e-09), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3396441393102525, 7.415050321694834e-14), 'spearman': SpearmanrResult(correlation=0.42056170662690445, pvalue=4.245164125311289e-21), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4213667803171265, 1.2260371203800528e-33), 'spearman': SpearmanrResult(correlation=0.49667011660057575, pvalue=5.716072538732747e-48), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.3864839034653163, 1.1553000666280478e-15), 'spearman': SpearmanrResult(correlation=0.39925046155160676, pvalue=1.0625781472926569e-16), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.28987592465127066, 'wmean': 0.2743057988374052}, 'spearman': {'mean': 0.335036303219998, 'wmean': 0.3197766307728279}}}, 'STS13': {'FNWN': {'pearson': (0.14861526068383674, 0.041259207471054056), 'spearman': SpearmanrResult(correlation=0.16286276994366825, pvalue=0.025148736066590942), 'nsamples': 189}, 'headlines': {'pearson': (0.22148351526944962, 8.690621405537229e-10), 'spearman': SpearmanrResult(correlation=0.2229742545401131, pvalue=6.653773096403036e-10), 'nsamples': 750}, 'OnWN': {'pearson': (-0.14316550934482405, 0.0006716216437396472), 'spearman': SpearmanrResult(correlation=-0.10709819144139099, pvalue=0.011138801456016987), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.07564442220282076, 'wmean': 0.07592337998592405}, 'spearman': {'mean': 0.09291294434746344, 'wmean': 0.09195311268387853}}}, 'STS14': {'deft-forum': {'pearson': (-0.09943273275546753, 0.03497495750017882), 'spearman': SpearmanrResult(correlation=-0.06730256778255252, pvalue=0.15405644156165738), 'nsamples': 450}, 'deft-news': {'pearson': (0.025223906708578995, 0.6634635356523626), 'spearman': SpearmanrResult(correlation=0.06478031612257404, pvalue=0.2633453484442692), 'nsamples': 300}, 'headlines': {'pearson': (0.21421308434307515, 3.111594335889489e-09), 'spearman': SpearmanrResult(correlation=0.21134559274138853, pvalue=5.0832039390707285e-09), 'nsamples': 750}, 'images': {'pearson': (0.21103010223718946, 5.362986589043105e-09), 'spearman': SpearmanrResult(correlation=0.25152707473407077, pvalue=2.757936557413535e-12), 'nsamples': 750}, 'OnWN': {'pearson': (-0.013052859523796506, 0.7211769654388771), 'spearman': SpearmanrResult(correlation=0.03351584514630905, pvalue=0.3593544947726486), 'nsamples': 750}, 'tweet-news': {'pearson': (0.25822631041165645, 6.860804894531595e-13), 'spearman': SpearmanrResult(correlation=0.27493592518493126, pvalue=1.7873651885592976e-14), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.09936796857020601, 'wmean': 0.12416931209965511}, 'spearman': {'mean': 0.12813369769112018, 'wmean': 0.15137100471723952}}}, 'STS15': {'answers-forums': {'pearson': (0.02471119945395268, 0.6333581229540461), 'spearman': SpearmanrResult(correlation=0.04398693822574634, pvalue=0.39567496193808116), 'nsamples': 375}, 'answers-students': {'pearson': (0.3776824552440247, 7.701625947477754e-27), 'spearman': SpearmanrResult(correlation=0.3978259472233388, pvalue=7.544400939046468e-30), 'nsamples': 750}, 'belief': {'pearson': (0.036117111067610785, 0.4856166645641353), 'spearman': SpearmanrResult(correlation=0.025693176816056186, pvalue=0.6199176387101815), 'nsamples': 375}, 'headlines': {'pearson': (0.32711683535962766, 3.6550645740190227e-20), 'spearman': SpearmanrResult(correlation=0.33954946973821826, pvalue=1.0775820069417563e-21), 'nsamples': 750}, 'images': {'pearson': (0.22997086263077898, 1.8522412336133805e-10), 'spearman': SpearmanrResult(correlation=0.2625916482712149, pvalue=2.711492448436367e-13), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.19911969275119898, 'wmean': 0.24129607712380327}, 'spearman': {'mean': 0.21392943605491493, 'wmean': 0.2587017806884183}}}, 'STS16': {'answer-answer': {'pearson': (0.22223512187066935, 0.0003581957242564949), 'spearman': SpearmanrResult(correlation=0.2429022271690049, pvalue=9.196757532115229e-05), 'nsamples': 254}, 'headlines': {'pearson': (0.35946360662985877, 5.217958844618178e-09), 'spearman': SpearmanrResult(correlation=0.3701986475256613, pvalue=1.6624936727467359e-09), 'nsamples': 249}, 'plagiarism': {'pearson': (0.16216254019078485, 0.01380736496017687), 'spearman': SpearmanrResult(correlation=0.18572585413594078, pvalue=0.00471400193244627), 'nsamples': 230}, 'postediting': {'pearson': (0.44002224345578905, 5.627603013360667e-13), 'spearman': SpearmanrResult(correlation=0.4692124537531694, pvalue=9.22074503380696e-15), 'nsamples': 244}, 'question-question': {'pearson': (0.04380764549045944, 0.5288097543441846), 'spearman': SpearmanrResult(correlation=0.05288121153849159, pvalue=0.4469903628149091), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.2455382315275123, 'wmean': 0.2527595013158381}, 'spearman': {'mean': 0.2641840788244536, 'wmean': 0.271613480028163}}}, 'MR': {'devacc': 68.1, 'acc': 67.83, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 72.47, 'acc': 71.24, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.23, 'acc': 83.13, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.43, 'acc': 88.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.87, 'acc': 78.09, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.24, 'acc': 39.19, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 60.22, 'acc': 75.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.38, 'acc': 67.77, 'f1': 75.27, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 67.2, 'acc': 66.75, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7274207069540313, 'pearson': 0.7253335937842363, 'spearman': 0.6509156097576677, 'mse': 0.4851766304640681, 'yhat': array([3.4832155 , 4.5181534 , 1.08128859, ..., 3.25798239, 4.33835361,        4.65005987]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.591529155554082, 'pearson': 0.577342481468638, 'spearman': 0.5635257872209847, 'mse': 1.8881113002534398, 'yhat': array([2.64348662, 1.5807559 , 1.67881545, ..., 3.93973147, 3.40209758,        3.14365312]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.95, 'acc': 64.91, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.58, 'acc': 92.25, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 15.92, 'acc': 14.93, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.32, 'acc': 31.28, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.18, 'acc': 49.75, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 51.32, 'acc': 50.88, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 83.14, 'acc': 81.26, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.35, 'acc': 76.35, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.62, 'acc': 77.14, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 52.43, 'acc': 52.34, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 17:56:01,211 : STS12 p=0.2743, STS12 s=0.3198, STS13 p=0.0759, STS13 s=0.0920, STS14 p=0.1242, STS14 s=0.1514, STS15 p=0.2413, STS15 s=0.2587, STS 16 p=0.2528, STS16 s=0.2716, STS B p=0.5773, STS B s=0.5635, STS B m=1.8881, SICK-R p=0.7253, SICK-R s=0.6509, SICK-P m=0.4852
2019-03-12 17:56:01,211 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 17:56:01,211 : 0.2743,0.3198,0.0759,0.0920,0.1242,0.1514,0.2413,0.2587,0.2528,0.2716,0.5773,0.5635,1.8881,0.7253,0.6509,0.4852
2019-03-12 17:56:01,211 : MR=67.83, CR=71.24, SUBJ=88.70, MPQA=83.13, SST-B=78.09, SST-F=39.19, TREC=75.40, SICK-E=66.75, SNLI=64.91, MRPC=67.77, MRPC f=75.27
2019-03-12 17:56:01,211 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 17:56:01,212 : 67.83,71.24,88.70,83.13,78.09,39.19,75.40,66.75,64.91,67.77,75.27
2019-03-12 17:56:01,212 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 17:56:01,212 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 17:56:01,212 : na,na,na,na,na,na,na,na,na,na
2019-03-12 17:56:01,212 : SentLen=92.25, WC=14.93, TreeDepth=31.28, TopConst=49.75, BShift=50.88, Tense=81.26, SubjNum=76.35, ObjNum=77.14, SOMO=49.87, CoordInv=52.34, average=57.61
2019-03-12 17:56:01,212 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 17:56:01,212 : 92.25,14.93,31.28,49.75,50.88,81.26,76.35,77.14,49.87,52.34,57.61
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,212 : layer 3
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,212 : ********************************************************************************
2019-03-12 17:56:01,306 : ***** Transfer task : STS12 *****


2019-03-12 17:56:01,318 : loading BERT model bert-large-uncased
2019-03-12 17:56:01,319 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:56:01,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:56:01,336 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6lsmrmmp
2019-03-12 17:56:08,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:18,073 : MSRpar : pearson = 0.1737, spearman = 0.1841
2019-03-12 17:56:19,726 : MSRvid : pearson = 0.2103, spearman = 0.2398
2019-03-12 17:56:21,150 : SMTeuroparl : pearson = 0.3858, spearman = 0.4573
2019-03-12 17:56:23,869 : surprise.OnWN : pearson = 0.4739, spearman = 0.5157
2019-03-12 17:56:25,308 : surprise.SMTnews : pearson = 0.4536, spearman = 0.4409
2019-03-12 17:56:25,308 : ALL (weighted average) : Pearson = 0.3223,             Spearman = 0.3509
2019-03-12 17:56:25,308 : ALL (average) : Pearson = 0.3395,             Spearman = 0.3676

2019-03-12 17:56:25,308 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 17:56:25,317 : loading BERT model bert-large-uncased
2019-03-12 17:56:25,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:56:25,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:56:25,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaeilznup
2019-03-12 17:56:32,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:39,395 : FNWN : pearson = 0.1831, spearman = 0.1884
2019-03-12 17:56:41,298 : headlines : pearson = 0.3133, spearman = 0.3070
2019-03-12 17:56:42,774 : OnWN : pearson = -0.0465, spearman = -0.0104
2019-03-12 17:56:42,774 : ALL (weighted average) : Pearson = 0.1623,             Spearman = 0.1733
2019-03-12 17:56:42,774 : ALL (average) : Pearson = 0.1500,             Spearman = 0.1616

2019-03-12 17:56:42,774 : ***** Transfer task : STS14 *****


2019-03-12 17:56:42,789 : loading BERT model bert-large-uncased
2019-03-12 17:56:42,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:56:42,807 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:56:42,807 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3rvgvnjm
2019-03-12 17:56:50,275 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:56:56,797 : deft-forum : pearson = -0.0363, spearman = -0.0053
2019-03-12 17:56:58,441 : deft-news : pearson = 0.1715, spearman = 0.1848
2019-03-12 17:57:00,620 : headlines : pearson = 0.2900, spearman = 0.2790
2019-03-12 17:57:02,707 : images : pearson = 0.2883, spearman = 0.3096
2019-03-12 17:57:04,845 : OnWN : pearson = 0.1163, spearman = 0.1749
2019-03-12 17:57:07,717 : tweet-news : pearson = 0.3846, spearman = 0.3714
2019-03-12 17:57:07,718 : ALL (weighted average) : Pearson = 0.2252,             Spearman = 0.2411
2019-03-12 17:57:07,718 : ALL (average) : Pearson = 0.2024,             Spearman = 0.2191

2019-03-12 17:57:07,718 : ***** Transfer task : STS15 *****


2019-03-12 17:57:07,770 : loading BERT model bert-large-uncased
2019-03-12 17:57:07,770 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:07,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:07,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvf2chy83
2019-03-12 17:57:15,436 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:57:22,681 : answers-forums : pearson = 0.1264, spearman = 0.1351
2019-03-12 17:57:24,773 : answers-students : pearson = 0.4277, spearman = 0.4293
2019-03-12 17:57:26,832 : belief : pearson = 0.1344, spearman = 0.1235
2019-03-12 17:57:29,097 : headlines : pearson = 0.4286, spearman = 0.4398
2019-03-12 17:57:31,240 : images : pearson = 0.2879, spearman = 0.3033
2019-03-12 17:57:31,240 : ALL (weighted average) : Pearson = 0.3187,             Spearman = 0.3254
2019-03-12 17:57:31,240 : ALL (average) : Pearson = 0.2810,             Spearman = 0.2862

2019-03-12 17:57:31,240 : ***** Transfer task : STS16 *****


2019-03-12 17:57:31,310 : loading BERT model bert-large-uncased
2019-03-12 17:57:31,310 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:31,328 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:31,328 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphxiwiynu
2019-03-12 17:57:38,752 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:57:44,833 : answer-answer : pearson = 0.2795, spearman = 0.2783
2019-03-12 17:57:45,494 : headlines : pearson = 0.4580, spearman = 0.4597
2019-03-12 17:57:46,378 : plagiarism : pearson = 0.3265, spearman = 0.3304
2019-03-12 17:57:47,880 : postediting : pearson = 0.5348, spearman = 0.5443
2019-03-12 17:57:48,489 : question-question : pearson = 0.1078, spearman = 0.1119
2019-03-12 17:57:48,489 : ALL (weighted average) : Pearson = 0.3484,             Spearman = 0.3519
2019-03-12 17:57:48,489 : ALL (average) : Pearson = 0.3413,             Spearman = 0.3449

2019-03-12 17:57:48,489 : ***** Transfer task : MR *****


2019-03-12 17:57:48,504 : loading BERT model bert-large-uncased
2019-03-12 17:57:48,505 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:57:48,525 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:57:48,525 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcn8f7bac
2019-03-12 17:57:55,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:58:01,175 : Generating sentence embeddings
2019-03-12 17:58:32,717 : Generated sentence embeddings
2019-03-12 17:58:32,718 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:58:43,163 : Best param found at split 1: l2reg = 1e-05                 with score 70.54
2019-03-12 17:58:53,160 : Best param found at split 2: l2reg = 0.0001                 with score 69.67
2019-03-12 17:59:03,468 : Best param found at split 3: l2reg = 1e-05                 with score 69.72
2019-03-12 17:59:14,041 : Best param found at split 4: l2reg = 1e-05                 with score 69.16
2019-03-12 17:59:24,001 : Best param found at split 5: l2reg = 1e-05                 with score 70.21
2019-03-12 17:59:24,598 : Dev acc : 69.86 Test acc : 68.09

2019-03-12 17:59:24,600 : ***** Transfer task : CR *****


2019-03-12 17:59:24,607 : loading BERT model bert-large-uncased
2019-03-12 17:59:24,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 17:59:24,627 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 17:59:24,627 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4fb1v6hr
2019-03-12 17:59:32,055 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 17:59:37,357 : Generating sentence embeddings
2019-03-12 17:59:45,688 : Generated sentence embeddings
2019-03-12 17:59:45,688 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 17:59:49,001 : Best param found at split 1: l2reg = 1e-05                 with score 74.76
2019-03-12 17:59:52,051 : Best param found at split 2: l2reg = 1e-05                 with score 74.33
2019-03-12 17:59:55,646 : Best param found at split 3: l2reg = 0.0001                 with score 73.81
2019-03-12 17:59:59,391 : Best param found at split 4: l2reg = 1e-05                 with score 73.68
2019-03-12 18:00:03,368 : Best param found at split 5: l2reg = 1e-05                 with score 74.21
2019-03-12 18:00:03,593 : Dev acc : 74.16 Test acc : 71.71

2019-03-12 18:00:03,593 : ***** Transfer task : MPQA *****


2019-03-12 18:00:03,600 : loading BERT model bert-large-uncased
2019-03-12 18:00:03,600 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:00:03,656 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:00:03,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpezu60mpf
2019-03-12 18:00:11,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:00:16,245 : Generating sentence embeddings
2019-03-12 18:00:23,826 : Generated sentence embeddings
2019-03-12 18:00:23,827 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 18:00:33,567 : Best param found at split 1: l2reg = 0.0001                 with score 82.56
2019-03-12 18:00:44,409 : Best param found at split 2: l2reg = 1e-05                 with score 83.78
2019-03-12 18:00:54,968 : Best param found at split 3: l2reg = 0.001                 with score 83.87
2019-03-12 18:01:04,987 : Best param found at split 4: l2reg = 0.01                 with score 82.79
2019-03-12 18:01:15,710 : Best param found at split 5: l2reg = 0.001                 with score 83.51
2019-03-12 18:01:16,480 : Dev acc : 83.3 Test acc : 84.05

2019-03-12 18:01:16,481 : ***** Transfer task : SUBJ *****


2019-03-12 18:01:16,497 : loading BERT model bert-large-uncased
2019-03-12 18:01:16,497 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:01:16,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:01:16,516 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc72_ih3g
2019-03-12 18:01:24,009 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:01:29,297 : Generating sentence embeddings
2019-03-12 18:02:00,318 : Generated sentence embeddings
2019-03-12 18:02:00,319 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 18:02:10,429 : Best param found at split 1: l2reg = 1e-05                 with score 90.76
2019-03-12 18:02:21,276 : Best param found at split 2: l2reg = 0.0001                 with score 90.9
2019-03-12 18:02:31,511 : Best param found at split 3: l2reg = 1e-05                 with score 90.21
2019-03-12 18:02:42,534 : Best param found at split 4: l2reg = 1e-05                 with score 90.57
2019-03-12 18:02:52,897 : Best param found at split 5: l2reg = 1e-05                 with score 90.22
2019-03-12 18:02:53,631 : Dev acc : 90.53 Test acc : 90.22

2019-03-12 18:02:53,632 : ***** Transfer task : SST Binary classification *****


2019-03-12 18:02:53,722 : loading BERT model bert-large-uncased
2019-03-12 18:02:53,723 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:02:53,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:02:53,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqbhu216v
2019-03-12 18:03:01,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:03:06,407 : Computing embedding for train
2019-03-12 18:04:47,338 : Computed train embeddings
2019-03-12 18:04:47,338 : Computing embedding for dev
2019-03-12 18:04:49,541 : Computed dev embeddings
2019-03-12 18:04:49,542 : Computing embedding for test
2019-03-12 18:04:54,184 : Computed test embeddings
2019-03-12 18:04:54,184 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:05:08,864 : [('reg:1e-05', 77.29), ('reg:0.0001', 77.06), ('reg:0.001', 75.46), ('reg:0.01', 74.77)]
2019-03-12 18:05:08,864 : Validation : best param found is reg = 1e-05 with score             77.29
2019-03-12 18:05:08,864 : Evaluating...
2019-03-12 18:05:13,209 : 
Dev acc : 77.29 Test acc : 78.03 for             SST Binary classification

2019-03-12 18:05:13,210 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 18:05:13,264 : loading BERT model bert-large-uncased
2019-03-12 18:05:13,264 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:05:13,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:05:13,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxx7w26cn
2019-03-12 18:05:20,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:05:25,939 : Computing embedding for train
2019-03-12 18:05:47,992 : Computed train embeddings
2019-03-12 18:05:47,992 : Computing embedding for dev
2019-03-12 18:05:50,881 : Computed dev embeddings
2019-03-12 18:05:50,881 : Computing embedding for test
2019-03-12 18:05:56,568 : Computed test embeddings
2019-03-12 18:05:56,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:05:59,518 : [('reg:1e-05', 34.06), ('reg:0.0001', 32.79), ('reg:0.001', 35.6), ('reg:0.01', 33.24)]
2019-03-12 18:05:59,518 : Validation : best param found is reg = 0.001 with score             35.6
2019-03-12 18:05:59,518 : Evaluating...
2019-03-12 18:06:00,370 : 
Dev acc : 35.6 Test acc : 37.6 for             SST Fine-Grained classification

2019-03-12 18:06:00,371 : ***** Transfer task : TREC *****


2019-03-12 18:06:00,384 : loading BERT model bert-large-uncased
2019-03-12 18:06:00,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:06:00,402 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:06:00,403 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuralernx
2019-03-12 18:06:07,838 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:06:20,533 : Computed train embeddings
2019-03-12 18:06:21,125 : Computed test embeddings
2019-03-12 18:06:21,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 18:06:28,164 : [('reg:1e-05', 60.47), ('reg:0.0001', 60.42), ('reg:0.001', 62.2), ('reg:0.01', 47.71)]
2019-03-12 18:06:28,164 : Cross-validation : best param found is reg = 0.001             with score 62.2
2019-03-12 18:06:28,164 : Evaluating...
2019-03-12 18:06:28,712 : 
Dev acc : 62.2 Test acc : 78.2             for TREC

2019-03-12 18:06:28,713 : ***** Transfer task : MRPC *****


2019-03-12 18:06:28,734 : loading BERT model bert-large-uncased
2019-03-12 18:06:28,734 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:06:28,757 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:06:28,757 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8twelwnd
2019-03-12 18:06:36,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:06:41,357 : Computing embedding for train
2019-03-12 18:07:03,736 : Computed train embeddings
2019-03-12 18:07:03,736 : Computing embedding for test
2019-03-12 18:07:13,571 : Computed test embeddings
2019-03-12 18:07:13,592 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 18:07:17,640 : [('reg:1e-05', 69.19), ('reg:0.0001', 69.26), ('reg:0.001', 68.69), ('reg:0.01', 68.87)]
2019-03-12 18:07:17,640 : Cross-validation : best param found is reg = 0.0001             with score 69.26
2019-03-12 18:07:17,640 : Evaluating...
2019-03-12 18:07:17,896 : Dev acc : 69.26 Test acc 70.49; Test F1 79.45 for MRPC.

2019-03-12 18:07:17,897 : ***** Transfer task : SICK-Entailment*****


2019-03-12 18:07:17,959 : loading BERT model bert-large-uncased
2019-03-12 18:07:17,959 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:07:17,978 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:07:17,978 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_lrjsd1j
2019-03-12 18:07:25,400 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:07:30,647 : Computing embedding for train
2019-03-12 18:07:42,013 : Computed train embeddings
2019-03-12 18:07:42,013 : Computing embedding for dev
2019-03-12 18:07:43,564 : Computed dev embeddings
2019-03-12 18:07:43,564 : Computing embedding for test
2019-03-12 18:07:55,784 : Computed test embeddings
2019-03-12 18:07:55,821 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:07:57,330 : [('reg:1e-05', 64.8), ('reg:0.0001', 71.2), ('reg:0.001', 64.2), ('reg:0.01', 68.0)]
2019-03-12 18:07:57,330 : Validation : best param found is reg = 0.0001 with score             71.2
2019-03-12 18:07:57,330 : Evaluating...
2019-03-12 18:07:57,929 : 
Dev acc : 71.2 Test acc : 71.56 for                        SICK entailment

2019-03-12 18:07:57,929 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 18:07:57,956 : loading BERT model bert-large-uncased
2019-03-12 18:07:57,956 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:07:58,013 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:07:58,013 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqnrkp6i1
2019-03-12 18:08:05,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:08:10,842 : Computing embedding for train
2019-03-12 18:08:22,255 : Computed train embeddings
2019-03-12 18:08:22,255 : Computing embedding for dev
2019-03-12 18:08:23,814 : Computed dev embeddings
2019-03-12 18:08:23,814 : Computing embedding for test
2019-03-12 18:08:36,058 : Computed test embeddings
2019-03-12 18:08:58,163 : Dev : Pearson 0.74156344298193
2019-03-12 18:08:58,163 : Test : Pearson 0.7392555626791976 Spearman 0.6667419911108008 MSE 0.46207137017112904                        for SICK Relatedness

2019-03-12 18:08:58,164 : 

***** Transfer task : STSBenchmark*****


2019-03-12 18:08:58,202 : loading BERT model bert-large-uncased
2019-03-12 18:08:58,203 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:08:58,232 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:08:58,232 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6uwh1zn5
2019-03-12 18:09:05,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:09:10,858 : Computing embedding for train
2019-03-12 18:09:29,603 : Computed train embeddings
2019-03-12 18:09:29,604 : Computing embedding for dev
2019-03-12 18:09:35,304 : Computed dev embeddings
2019-03-12 18:09:35,304 : Computing embedding for test
2019-03-12 18:09:39,958 : Computed test embeddings
2019-03-12 18:09:58,626 : Dev : Pearson 0.6329361729775463
2019-03-12 18:09:58,626 : Test : Pearson 0.5861703534356607 Spearman 0.5726333489057761 MSE 1.845550728291544                        for SICK Relatedness

2019-03-12 18:09:58,626 : ***** Transfer task : SNLI Entailment*****


2019-03-12 18:10:03,676 : loading BERT model bert-large-uncased
2019-03-12 18:10:03,676 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:10:03,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:10:03,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfdjkx9mj
2019-03-12 18:10:11,177 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:10:16,793 : PROGRESS (encoding): 0.00%
2019-03-12 18:13:08,311 : PROGRESS (encoding): 14.56%
2019-03-12 18:16:22,619 : PROGRESS (encoding): 29.12%
2019-03-12 18:19:35,629 : PROGRESS (encoding): 43.69%
2019-03-12 18:23:00,402 : PROGRESS (encoding): 58.25%
2019-03-12 18:26:49,075 : PROGRESS (encoding): 72.81%
2019-03-12 18:30:36,135 : PROGRESS (encoding): 87.37%
2019-03-12 18:34:40,517 : PROGRESS (encoding): 0.00%
2019-03-12 18:35:11,354 : PROGRESS (encoding): 0.00%
2019-03-12 18:35:41,019 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:36:07,121 : [('reg:1e-09', 57.48)]
2019-03-12 18:36:07,121 : Validation : best param found is reg = 1e-09 with score             57.48
2019-03-12 18:36:07,121 : Evaluating...
2019-03-12 18:36:33,536 : Dev acc : 57.48 Test acc : 57.15 for SNLI

2019-03-12 18:36:33,536 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 18:36:33,741 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 18:36:34,801 : loading BERT model bert-large-uncased
2019-03-12 18:36:34,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:36:34,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:36:34,828 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp57edbm97
2019-03-12 18:36:42,269 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:36:47,450 : Computing embeddings for train/dev/test
2019-03-12 18:40:23,740 : Computed embeddings
2019-03-12 18:40:23,740 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:40:47,343 : [('reg:1e-05', 90.37), ('reg:0.0001', 92.03), ('reg:0.001', 87.04), ('reg:0.01', 86.4)]
2019-03-12 18:40:47,343 : Validation : best param found is reg = 0.0001 with score             92.03
2019-03-12 18:40:47,343 : Evaluating...
2019-03-12 18:40:54,826 : 
Dev acc : 92.0 Test acc : 92.9 for LENGTH classification

2019-03-12 18:40:54,827 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 18:40:55,204 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 18:40:55,250 : loading BERT model bert-large-uncased
2019-03-12 18:40:55,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:40:55,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:40:55,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp28_ao5xq
2019-03-12 18:41:02,685 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:41:07,884 : Computing embeddings for train/dev/test
2019-03-12 18:44:28,009 : Computed embeddings
2019-03-12 18:44:28,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:45:04,273 : [('reg:1e-05', 25.74), ('reg:0.0001', 2.26), ('reg:0.001', 0.45), ('reg:0.01', 0.19)]
2019-03-12 18:45:04,274 : Validation : best param found is reg = 1e-05 with score             25.74
2019-03-12 18:45:04,274 : Evaluating...
2019-03-12 18:45:16,408 : 
Dev acc : 25.7 Test acc : 25.8 for WORDCONTENT classification

2019-03-12 18:45:16,409 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 18:45:16,789 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 18:45:16,855 : loading BERT model bert-large-uncased
2019-03-12 18:45:16,855 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:45:16,881 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:45:16,881 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4eetqczr
2019-03-12 18:45:24,339 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:45:29,515 : Computing embeddings for train/dev/test
2019-03-12 18:48:36,882 : Computed embeddings
2019-03-12 18:48:36,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:48:57,048 : [('reg:1e-05', 28.04), ('reg:0.0001', 29.37), ('reg:0.001', 25.99), ('reg:0.01', 23.09)]
2019-03-12 18:48:57,048 : Validation : best param found is reg = 0.0001 with score             29.37
2019-03-12 18:48:57,048 : Evaluating...
2019-03-12 18:49:03,518 : 
Dev acc : 29.4 Test acc : 30.2 for DEPTH classification

2019-03-12 18:49:03,519 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 18:49:03,896 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 18:49:03,960 : loading BERT model bert-large-uncased
2019-03-12 18:49:03,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:49:04,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:49:04,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfilqej8g
2019-03-12 18:49:11,535 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:49:16,756 : Computing embeddings for train/dev/test
2019-03-12 18:52:10,553 : Computed embeddings
2019-03-12 18:52:10,553 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:52:39,926 : [('reg:1e-05', 51.31), ('reg:0.0001', 49.7), ('reg:0.001', 40.6), ('reg:0.01', 18.59)]
2019-03-12 18:52:39,926 : Validation : best param found is reg = 1e-05 with score             51.31
2019-03-12 18:52:39,926 : Evaluating...
2019-03-12 18:52:48,624 : 
Dev acc : 51.3 Test acc : 51.4 for TOPCONSTITUENTS classification

2019-03-12 18:52:48,625 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 18:52:48,967 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 18:52:49,033 : loading BERT model bert-large-uncased
2019-03-12 18:52:49,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:52:49,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:52:49,153 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvc5w08yz
2019-03-12 18:52:56,606 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:53:01,896 : Computing embeddings for train/dev/test
2019-03-12 18:56:10,162 : Computed embeddings
2019-03-12 18:56:10,162 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 18:56:35,103 : [('reg:1e-05', 50.11), ('reg:0.0001', 50.03), ('reg:0.001', 50.42), ('reg:0.01', 50.0)]
2019-03-12 18:56:35,103 : Validation : best param found is reg = 0.001 with score             50.42
2019-03-12 18:56:35,103 : Evaluating...
2019-03-12 18:56:43,444 : 
Dev acc : 50.4 Test acc : 50.7 for BIGRAMSHIFT classification

2019-03-12 18:56:43,445 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 18:56:43,995 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 18:56:44,060 : loading BERT model bert-large-uncased
2019-03-12 18:56:44,060 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 18:56:44,088 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 18:56:44,089 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy4m7kew_
2019-03-12 18:56:51,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 18:56:56,906 : Computing embeddings for train/dev/test
2019-03-12 19:00:01,551 : Computed embeddings
2019-03-12 19:00:01,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:00:32,240 : [('reg:1e-05', 83.9), ('reg:0.0001', 83.85), ('reg:0.001', 83.85), ('reg:0.01', 80.46)]
2019-03-12 19:00:32,240 : Validation : best param found is reg = 1e-05 with score             83.9
2019-03-12 19:00:32,240 : Evaluating...
2019-03-12 19:00:39,699 : 
Dev acc : 83.9 Test acc : 82.2 for TENSE classification

2019-03-12 19:00:39,700 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 19:00:40,116 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 19:00:40,178 : loading BERT model bert-large-uncased
2019-03-12 19:00:40,178 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:00:40,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:00:40,203 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvtxq2rvd
2019-03-12 19:00:47,655 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:00:52,895 : Computing embeddings for train/dev/test
2019-03-12 19:04:07,369 : Computed embeddings
2019-03-12 19:04:07,369 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:04:36,793 : [('reg:1e-05', 79.59), ('reg:0.0001', 77.47), ('reg:0.001', 77.81), ('reg:0.01', 73.11)]
2019-03-12 19:04:36,793 : Validation : best param found is reg = 1e-05 with score             79.59
2019-03-12 19:04:36,793 : Evaluating...
2019-03-12 19:04:44,511 : 
Dev acc : 79.6 Test acc : 78.3 for SUBJNUMBER classification

2019-03-12 19:04:44,512 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 19:04:44,918 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 19:04:44,984 : loading BERT model bert-large-uncased
2019-03-12 19:04:44,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:04:45,099 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:04:45,100 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjyhwf0wv
2019-03-12 19:04:52,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:04:57,833 : Computing embeddings for train/dev/test
2019-03-12 19:08:08,919 : Computed embeddings
2019-03-12 19:08:08,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:08:41,908 : [('reg:1e-05', 76.14), ('reg:0.0001', 76.41), ('reg:0.001', 74.12), ('reg:0.01', 63.36)]
2019-03-12 19:08:41,908 : Validation : best param found is reg = 0.0001 with score             76.41
2019-03-12 19:08:41,908 : Evaluating...
2019-03-12 19:08:50,047 : 
Dev acc : 76.4 Test acc : 77.9 for OBJNUMBER classification

2019-03-12 19:08:50,048 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 19:08:50,626 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 19:08:50,693 : loading BERT model bert-large-uncased
2019-03-12 19:08:50,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:08:50,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:08:50,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt9e849px
2019-03-12 19:08:58,424 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:09:03,854 : Computing embeddings for train/dev/test
2019-03-12 19:12:45,792 : Computed embeddings
2019-03-12 19:12:45,792 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:13:07,330 : [('reg:1e-05', 50.78), ('reg:0.0001', 50.74), ('reg:0.001', 50.46), ('reg:0.01', 51.03)]
2019-03-12 19:13:07,330 : Validation : best param found is reg = 0.01 with score             51.03
2019-03-12 19:13:07,331 : Evaluating...
2019-03-12 19:13:13,658 : 
Dev acc : 51.0 Test acc : 51.5 for ODDMANOUT classification

2019-03-12 19:13:13,659 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 19:13:14,064 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 19:13:14,143 : loading BERT model bert-large-uncased
2019-03-12 19:13:14,143 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:13:14,277 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:13:14,277 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprw7j8m89
2019-03-12 19:13:21,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:13:27,107 : Computing embeddings for train/dev/test
2019-03-12 19:17:04,225 : Computed embeddings
2019-03-12 19:17:04,225 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:17:26,928 : [('reg:1e-05', 51.86), ('reg:0.0001', 51.57), ('reg:0.001', 50.78), ('reg:0.01', 50.39)]
2019-03-12 19:17:26,928 : Validation : best param found is reg = 1e-05 with score             51.86
2019-03-12 19:17:26,928 : Evaluating...
2019-03-12 19:17:33,311 : 
Dev acc : 51.9 Test acc : 52.0 for COORDINATIONINVERSION classification

2019-03-12 19:17:33,313 : total results: {'STS12': {'MSRpar': {'pearson': (0.17371317116786542, 1.7028399641622125e-06), 'spearman': SpearmanrResult(correlation=0.18408008329550785, pvalue=3.84910352958276e-07), 'nsamples': 750}, 'MSRvid': {'pearson': (0.21034252944043488, 6.025509103463469e-09), 'spearman': SpearmanrResult(correlation=0.23978924957906925, pvalue=2.8668959437983613e-11), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.385822190060243, 9.671156316397452e-18), 'spearman': SpearmanrResult(correlation=0.4573119708108132, pvalue=4.212192081849536e-25), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4739217822296958, 2.9472705919911986e-43), 'spearman': SpearmanrResult(correlation=0.5157364854227926, pvalue=3.347129948658634e-52), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4535988538995862, 1.2100259272516933e-21), 'spearman': SpearmanrResult(correlation=0.44087225317330125, pvalue=2.0972867418552333e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.339479705359565, 'wmean': 0.32225271559590846}, 'spearman': {'mean': 0.3675580084562968, 'wmean': 0.35087470635178175}}}, 'STS13': {'FNWN': {'pearson': (0.1831384134976711, 0.011655553349859557), 'spearman': SpearmanrResult(correlation=0.18835211824449832, pvalue=0.009445106987712113), 'nsamples': 189}, 'headlines': {'pearson': (0.3132625879511921, 1.5380840364335525e-18), 'spearman': SpearmanrResult(correlation=0.30703659679596357, pvalue=7.754031409428699e-18), 'nsamples': 750}, 'OnWN': {'pearson': (-0.04650063677880874, 0.2715386012483471), 'spearman': SpearmanrResult(correlation=-0.010441662815708502, pvalue=0.8050858687925982), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1499667882233515, 'wmean': 0.16231549592102815}, 'spearman': {'mean': 0.16164901740825113, 'wmean': 0.17334548340371358}}}, 'STS14': {'deft-forum': {'pearson': (-0.03626365076274102, 0.44285499887292445), 'spearman': SpearmanrResult(correlation=-0.005253858938565814, pvalue=0.9115037735870725), 'nsamples': 450}, 'deft-news': {'pearson': (0.17152130839244026, 0.002877285697158774), 'spearman': SpearmanrResult(correlation=0.18478633472304742, pvalue=0.001304615806152072), 'nsamples': 300}, 'headlines': {'pearson': (0.29004489816708184, 5.282548999051376e-16), 'spearman': SpearmanrResult(correlation=0.27898303773014876, pvalue=7.107165024646888e-15), 'nsamples': 750}, 'images': {'pearson': (0.2882507253065631, 8.116353256164574e-16), 'spearman': SpearmanrResult(correlation=0.30959627443793314, pvalue=4.0060466300760086e-18), 'nsamples': 750}, 'OnWN': {'pearson': (0.11634846530714699, 0.0014135712566726057), 'spearman': SpearmanrResult(correlation=0.17491207781816231, pvalue=1.4400891301497672e-06), 'nsamples': 750}, 'tweet-news': {'pearson': (0.38455786064552466, 7.638114055116002e-28), 'spearman': SpearmanrResult(correlation=0.37142765278191525, pvalue=6.011192397689565e-26), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.20240993450933598, 'wmean': 0.22521045646512963}, 'spearman': {'mean': 0.21907525309210685, 'wmean': 0.24113625225884777}}}, 'STS15': {'answers-forums': {'pearson': (0.1264222243707511, 0.014292779824912574), 'spearman': SpearmanrResult(correlation=0.13508672166288616, pvalue=0.008812954002462842), 'nsamples': 375}, 'answers-students': {'pearson': (0.42770464029955135, 1.0372079364535858e-34), 'spearman': SpearmanrResult(correlation=0.42930252225727994, pvalue=5.518446006337337e-35), 'nsamples': 750}, 'belief': {'pearson': (0.13444827629152967, 0.009141056987753022), 'spearman': SpearmanrResult(correlation=0.12351494180376607, pvalue=0.01670935231898527), 'nsamples': 375}, 'headlines': {'pearson': (0.42863026528302745, 7.199342636894779e-35), 'spearman': SpearmanrResult(correlation=0.4398121381865446, pvalue=7.9868814618627e-37), 'nsamples': 750}, 'images': {'pearson': (0.28790610978790937, 8.811171211831772e-16), 'spearman': SpearmanrResult(correlation=0.30333962921488145, pvalue=1.9898194258920355e-17), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2810223032065538, 'wmean': 0.31866906642540715}, 'spearman': {'mean': 0.28621119062507167, 'wmean': 0.325438780348008}}}, 'STS16': {'answer-answer': {'pearson': (0.27953910911546115, 6.078207655860435e-06), 'spearman': SpearmanrResult(correlation=0.27825281425205967, pvalue=6.732624732242229e-06), 'nsamples': 254}, 'headlines': {'pearson': (0.45797406577555233, 2.5855190580447787e-14), 'spearman': SpearmanrResult(correlation=0.45967687469981655, pvalue=2.0175991049097877e-14), 'nsamples': 249}, 'plagiarism': {'pearson': (0.3265485160478236, 4.086108300187304e-07), 'spearman': SpearmanrResult(correlation=0.33040668677738705, pvalue=2.9234370810047853e-07), 'nsamples': 230}, 'postediting': {'pearson': (0.5348156548121186, 1.8727623983213659e-19), 'spearman': SpearmanrResult(correlation=0.5443065878590365, pvalue=3.204563718736573e-20), 'nsamples': 244}, 'question-question': {'pearson': (0.10782355514672026, 0.12019104398005451), 'spearman': SpearmanrResult(correlation=0.11191971437946743, pvalue=0.10666335608284015), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.3413401801795352, 'wmean': 0.3483767095988706}, 'spearman': {'mean': 0.34491253559355345, 'wmean': 0.3518813847571587}}}, 'MR': {'devacc': 69.86, 'acc': 68.09, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 74.16, 'acc': 71.71, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.3, 'acc': 84.05, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.53, 'acc': 90.22, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.29, 'acc': 78.03, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 35.6, 'acc': 37.6, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 62.2, 'acc': 78.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.26, 'acc': 70.49, 'f1': 79.45, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.2, 'acc': 71.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.74156344298193, 'pearson': 0.7392555626791976, 'spearman': 0.6667419911108008, 'mse': 0.46207137017112904, 'yhat': array([2.53607169, 4.23210894, 1.58525303, ..., 2.83227692, 4.29588675,        4.78124738]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6329361729775463, 'pearson': 0.5861703534356607, 'spearman': 0.5726333489057761, 'mse': 1.845550728291544, 'yhat': array([2.27557513, 1.66934529, 2.05655638, ..., 4.12035576, 3.66722086,        3.19478765]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.48, 'acc': 57.15, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 92.03, 'acc': 92.86, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.74, 'acc': 25.75, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.37, 'acc': 30.19, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 51.31, 'acc': 51.37, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.42, 'acc': 50.74, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 83.9, 'acc': 82.2, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.59, 'acc': 78.33, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.41, 'acc': 77.91, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.03, 'acc': 51.45, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 51.86, 'acc': 51.96, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 19:17:33,313 : STS12 p=0.3223, STS12 s=0.3509, STS13 p=0.1623, STS13 s=0.1733, STS14 p=0.2252, STS14 s=0.2411, STS15 p=0.3187, STS15 s=0.3254, STS 16 p=0.3484, STS16 s=0.3519, STS B p=0.5862, STS B s=0.5726, STS B m=1.8456, SICK-R p=0.7393, SICK-R s=0.6667, SICK-P m=0.4621
2019-03-12 19:17:33,313 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 19:17:33,313 : 0.3223,0.3509,0.1623,0.1733,0.2252,0.2411,0.3187,0.3254,0.3484,0.3519,0.5862,0.5726,1.8456,0.7393,0.6667,0.4621
2019-03-12 19:17:33,313 : MR=68.09, CR=71.71, SUBJ=90.22, MPQA=84.05, SST-B=78.03, SST-F=37.60, TREC=78.20, SICK-E=71.56, SNLI=57.15, MRPC=70.49, MRPC f=79.45
2019-03-12 19:17:33,313 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 19:17:33,313 : 68.09,71.71,90.22,84.05,78.03,37.60,78.20,71.56,57.15,70.49,79.45
2019-03-12 19:17:33,313 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 19:17:33,313 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 19:17:33,313 : na,na,na,na,na,na,na,na,na,na
2019-03-12 19:17:33,313 : SentLen=92.86, WC=25.75, TreeDepth=30.19, TopConst=51.37, BShift=50.74, Tense=82.20, SubjNum=78.33, ObjNum=77.91, SOMO=51.45, CoordInv=51.96, average=59.28
2019-03-12 19:17:33,313 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 19:17:33,313 : 92.86,25.75,30.19,51.37,50.74,82.20,78.33,77.91,51.45,51.96,59.28
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,313 : layer 4
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,313 : ********************************************************************************
2019-03-12 19:17:33,401 : ***** Transfer task : STS12 *****


2019-03-12 19:17:33,414 : loading BERT model bert-large-uncased
2019-03-12 19:17:33,414 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:17:33,431 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:17:33,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3p6zfbh3
2019-03-12 19:17:40,875 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:17:50,234 : MSRpar : pearson = 0.2031, spearman = 0.2130
2019-03-12 19:17:51,895 : MSRvid : pearson = 0.2629, spearman = 0.2818
2019-03-12 19:17:53,315 : SMTeuroparl : pearson = 0.3953, spearman = 0.4663
2019-03-12 19:17:56,035 : surprise.OnWN : pearson = 0.5034, spearman = 0.5379
2019-03-12 19:17:57,471 : surprise.SMTnews : pearson = 0.4792, spearman = 0.4483
2019-03-12 19:17:57,471 : ALL (weighted average) : Pearson = 0.3538,             Spearman = 0.3756
2019-03-12 19:17:57,471 : ALL (average) : Pearson = 0.3688,             Spearman = 0.3894

2019-03-12 19:17:57,471 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 19:17:57,480 : loading BERT model bert-large-uncased
2019-03-12 19:17:57,480 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:17:57,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:17:57,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppsbrmlth
2019-03-12 19:18:04,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:11,396 : FNWN : pearson = 0.2292, spearman = 0.2388
2019-03-12 19:18:13,303 : headlines : pearson = 0.3483, spearman = 0.3355
2019-03-12 19:18:14,776 : OnWN : pearson = 0.0148, spearman = 0.0560
2019-03-12 19:18:14,776 : ALL (weighted average) : Pearson = 0.2086,             Spearman = 0.2188
2019-03-12 19:18:14,776 : ALL (average) : Pearson = 0.1974,             Spearman = 0.2101

2019-03-12 19:18:14,776 : ***** Transfer task : STS14 *****


2019-03-12 19:18:14,793 : loading BERT model bert-large-uncased
2019-03-12 19:18:14,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:18:14,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:18:14,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp313aol5w
2019-03-12 19:18:22,329 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:28,963 : deft-forum : pearson = -0.0018, spearman = 0.0306
2019-03-12 19:18:30,608 : deft-news : pearson = 0.1965, spearman = 0.2111
2019-03-12 19:18:32,786 : headlines : pearson = 0.3283, spearman = 0.3112
2019-03-12 19:18:34,873 : images : pearson = 0.3718, spearman = 0.3806
2019-03-12 19:18:37,014 : OnWN : pearson = 0.1897, spearman = 0.2584
2019-03-12 19:18:39,890 : tweet-news : pearson = 0.4355, spearman = 0.4178
2019-03-12 19:18:39,890 : ALL (weighted average) : Pearson = 0.2806,             Spearman = 0.2942
2019-03-12 19:18:39,890 : ALL (average) : Pearson = 0.2533,             Spearman = 0.2683

2019-03-12 19:18:39,891 : ***** Transfer task : STS15 *****


2019-03-12 19:18:39,925 : loading BERT model bert-large-uncased
2019-03-12 19:18:39,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:18:39,943 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:18:39,943 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgbmo7w9b
2019-03-12 19:18:47,387 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:18:54,647 : answers-forums : pearson = 0.2000, spearman = 0.1981
2019-03-12 19:18:56,738 : answers-students : pearson = 0.4888, spearman = 0.4854
2019-03-12 19:18:58,797 : belief : pearson = 0.1810, spearman = 0.1632
2019-03-12 19:19:01,054 : headlines : pearson = 0.4622, spearman = 0.4718
2019-03-12 19:19:03,193 : images : pearson = 0.3435, spearman = 0.3567
2019-03-12 19:19:03,194 : ALL (weighted average) : Pearson = 0.3712,             Spearman = 0.3737
2019-03-12 19:19:03,194 : ALL (average) : Pearson = 0.3351,             Spearman = 0.3351

2019-03-12 19:19:03,194 : ***** Transfer task : STS16 *****


2019-03-12 19:19:03,262 : loading BERT model bert-large-uncased
2019-03-12 19:19:03,262 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:19:03,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:19:03,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplzt4oezr
2019-03-12 19:19:10,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:19:16,957 : answer-answer : pearson = 0.3225, spearman = 0.3185
2019-03-12 19:19:17,630 : headlines : pearson = 0.4946, spearman = 0.4979
2019-03-12 19:19:18,530 : plagiarism : pearson = 0.4262, spearman = 0.4420
2019-03-12 19:19:20,042 : postediting : pearson = 0.5747, spearman = 0.5802
2019-03-12 19:19:20,650 : question-question : pearson = 0.1372, spearman = 0.1396
2019-03-12 19:19:20,650 : ALL (weighted average) : Pearson = 0.3980,             Spearman = 0.4025
2019-03-12 19:19:20,650 : ALL (average) : Pearson = 0.3910,             Spearman = 0.3957

2019-03-12 19:19:20,650 : ***** Transfer task : MR *****


2019-03-12 19:19:20,669 : loading BERT model bert-large-uncased
2019-03-12 19:19:20,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:19:20,688 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:19:20,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp0gnjkds
2019-03-12 19:19:28,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:19:33,548 : Generating sentence embeddings
2019-03-12 19:20:05,181 : Generated sentence embeddings
2019-03-12 19:20:05,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:20:15,348 : Best param found at split 1: l2reg = 1e-05                 with score 71.49
2019-03-12 19:20:23,560 : Best param found at split 2: l2reg = 0.001                 with score 71.49
2019-03-12 19:20:31,613 : Best param found at split 3: l2reg = 0.0001                 with score 70.96
2019-03-12 19:20:41,646 : Best param found at split 4: l2reg = 0.001                 with score 70.59
2019-03-12 19:20:52,141 : Best param found at split 5: l2reg = 1e-05                 with score 71.15
2019-03-12 19:20:52,568 : Dev acc : 71.14 Test acc : 70.56

2019-03-12 19:20:52,569 : ***** Transfer task : CR *****


2019-03-12 19:20:52,577 : loading BERT model bert-large-uncased
2019-03-12 19:20:52,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:20:52,596 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:20:52,596 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcqf_vmsp
2019-03-12 19:21:00,003 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:21:05,213 : Generating sentence embeddings
2019-03-12 19:21:13,571 : Generated sentence embeddings
2019-03-12 19:21:13,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:21:17,037 : Best param found at split 1: l2reg = 0.0001                 with score 76.35
2019-03-12 19:21:20,672 : Best param found at split 2: l2reg = 0.0001                 with score 75.49
2019-03-12 19:21:24,413 : Best param found at split 3: l2reg = 1e-05                 with score 74.87
2019-03-12 19:21:28,494 : Best param found at split 4: l2reg = 0.0001                 with score 75.44
2019-03-12 19:21:32,541 : Best param found at split 5: l2reg = 1e-05                 with score 74.81
2019-03-12 19:21:32,726 : Dev acc : 75.39 Test acc : 70.57

2019-03-12 19:21:32,727 : ***** Transfer task : MPQA *****


2019-03-12 19:21:32,733 : loading BERT model bert-large-uncased
2019-03-12 19:21:32,733 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:21:32,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:21:32,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkc21ckeg
2019-03-12 19:21:40,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:21:45,373 : Generating sentence embeddings
2019-03-12 19:21:52,961 : Generated sentence embeddings
2019-03-12 19:21:52,961 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:22:01,901 : Best param found at split 1: l2reg = 0.001                 with score 80.61
2019-03-12 19:22:10,201 : Best param found at split 2: l2reg = 0.001                 with score 83.81
2019-03-12 19:22:21,219 : Best param found at split 3: l2reg = 0.001                 with score 84.93
2019-03-12 19:22:31,911 : Best param found at split 4: l2reg = 0.0001                 with score 85.29
2019-03-12 19:22:40,181 : Best param found at split 5: l2reg = 0.001                 with score 84.52
2019-03-12 19:22:40,734 : Dev acc : 83.83 Test acc : 84.42

2019-03-12 19:22:40,735 : ***** Transfer task : SUBJ *****


2019-03-12 19:22:40,749 : loading BERT model bert-large-uncased
2019-03-12 19:22:40,750 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:22:40,770 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:22:40,770 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk1o950o8
2019-03-12 19:22:48,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:22:53,453 : Generating sentence embeddings
2019-03-12 19:23:24,522 : Generated sentence embeddings
2019-03-12 19:23:24,523 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 19:23:33,834 : Best param found at split 1: l2reg = 1e-05                 with score 91.12
2019-03-12 19:23:45,140 : Best param found at split 2: l2reg = 1e-05                 with score 91.86
2019-03-12 19:23:53,081 : Best param found at split 3: l2reg = 1e-05                 with score 91.16
2019-03-12 19:24:01,538 : Best param found at split 4: l2reg = 0.0001                 with score 91.52
2019-03-12 19:24:10,068 : Best param found at split 5: l2reg = 1e-05                 with score 91.28
2019-03-12 19:24:10,550 : Dev acc : 91.39 Test acc : 91.44

2019-03-12 19:24:10,551 : ***** Transfer task : SST Binary classification *****


2019-03-12 19:24:10,644 : loading BERT model bert-large-uncased
2019-03-12 19:24:10,644 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:24:10,720 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:24:10,720 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoh7kju3o
2019-03-12 19:24:18,174 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:24:23,747 : Computing embedding for train
2019-03-12 19:26:04,692 : Computed train embeddings
2019-03-12 19:26:04,692 : Computing embedding for dev
2019-03-12 19:26:06,896 : Computed dev embeddings
2019-03-12 19:26:06,896 : Computing embedding for test
2019-03-12 19:26:11,529 : Computed test embeddings
2019-03-12 19:26:11,529 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:26:27,791 : [('reg:1e-05', 77.29), ('reg:0.0001', 77.06), ('reg:0.001', 76.26), ('reg:0.01', 75.0)]
2019-03-12 19:26:27,792 : Validation : best param found is reg = 1e-05 with score             77.29
2019-03-12 19:26:27,792 : Evaluating...
2019-03-12 19:26:32,037 : 
Dev acc : 77.29 Test acc : 78.58 for             SST Binary classification

2019-03-12 19:26:32,038 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 19:26:32,088 : loading BERT model bert-large-uncased
2019-03-12 19:26:32,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:26:32,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:26:32,110 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpchquo415
2019-03-12 19:26:39,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:26:44,911 : Computing embedding for train
2019-03-12 19:27:06,962 : Computed train embeddings
2019-03-12 19:27:06,962 : Computing embedding for dev
2019-03-12 19:27:09,865 : Computed dev embeddings
2019-03-12 19:27:09,865 : Computing embedding for test
2019-03-12 19:27:15,551 : Computed test embeddings
2019-03-12 19:27:15,552 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:27:18,461 : [('reg:1e-05', 37.78), ('reg:0.0001', 36.15), ('reg:0.001', 35.15), ('reg:0.01', 33.15)]
2019-03-12 19:27:18,461 : Validation : best param found is reg = 1e-05 with score             37.78
2019-03-12 19:27:18,461 : Evaluating...
2019-03-12 19:27:19,100 : 
Dev acc : 37.78 Test acc : 40.9 for             SST Fine-Grained classification

2019-03-12 19:27:19,101 : ***** Transfer task : TREC *****


2019-03-12 19:27:19,114 : loading BERT model bert-large-uncased
2019-03-12 19:27:19,114 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:27:19,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:27:19,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjzqp8v__
2019-03-12 19:27:26,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:27:39,531 : Computed train embeddings
2019-03-12 19:27:40,124 : Computed test embeddings
2019-03-12 19:27:40,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:27:47,591 : [('reg:1e-05', 65.26), ('reg:0.0001', 64.84), ('reg:0.001', 62.56), ('reg:0.01', 52.65)]
2019-03-12 19:27:47,591 : Cross-validation : best param found is reg = 1e-05             with score 65.26
2019-03-12 19:27:47,591 : Evaluating...
2019-03-12 19:27:48,100 : 
Dev acc : 65.26 Test acc : 80.0             for TREC

2019-03-12 19:27:48,100 : ***** Transfer task : MRPC *****


2019-03-12 19:27:48,123 : loading BERT model bert-large-uncased
2019-03-12 19:27:48,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:27:48,143 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:27:48,143 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyf1g4bza
2019-03-12 19:27:55,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:28:01,128 : Computing embedding for train
2019-03-12 19:28:23,534 : Computed train embeddings
2019-03-12 19:28:23,534 : Computing embedding for test
2019-03-12 19:28:33,368 : Computed test embeddings
2019-03-12 19:28:33,389 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 19:28:38,038 : [('reg:1e-05', 69.14), ('reg:0.0001', 69.78), ('reg:0.001', 68.47), ('reg:0.01', 68.62)]
2019-03-12 19:28:38,038 : Cross-validation : best param found is reg = 0.0001             with score 69.78
2019-03-12 19:28:38,038 : Evaluating...
2019-03-12 19:28:38,305 : Dev acc : 69.78 Test acc 70.38; Test F1 78.39 for MRPC.

2019-03-12 19:28:38,305 : ***** Transfer task : SICK-Entailment*****


2019-03-12 19:28:38,367 : loading BERT model bert-large-uncased
2019-03-12 19:28:38,367 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:28:38,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:28:38,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8_l9lak6
2019-03-12 19:28:45,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:28:51,084 : Computing embedding for train
2019-03-12 19:29:02,463 : Computed train embeddings
2019-03-12 19:29:02,464 : Computing embedding for dev
2019-03-12 19:29:04,020 : Computed dev embeddings
2019-03-12 19:29:04,020 : Computing embedding for test
2019-03-12 19:29:16,252 : Computed test embeddings
2019-03-12 19:29:16,288 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:29:18,089 : [('reg:1e-05', 72.2), ('reg:0.0001', 68.2), ('reg:0.001', 69.6), ('reg:0.01', 70.0)]
2019-03-12 19:29:18,089 : Validation : best param found is reg = 1e-05 with score             72.2
2019-03-12 19:29:18,089 : Evaluating...
2019-03-12 19:29:18,537 : 
Dev acc : 72.2 Test acc : 73.94 for                        SICK entailment

2019-03-12 19:29:18,538 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 19:29:18,565 : loading BERT model bert-large-uncased
2019-03-12 19:29:18,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:29:18,620 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:29:18,620 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaghed0fg
2019-03-12 19:29:26,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:29:31,252 : Computing embedding for train
2019-03-12 19:29:42,664 : Computed train embeddings
2019-03-12 19:29:42,664 : Computing embedding for dev
2019-03-12 19:29:44,221 : Computed dev embeddings
2019-03-12 19:29:44,221 : Computing embedding for test
2019-03-12 19:29:56,493 : Computed test embeddings
2019-03-12 19:30:13,529 : Dev : Pearson 0.745537135201924
2019-03-12 19:30:13,529 : Test : Pearson 0.7560995823774236 Spearman 0.6818884432169 MSE 0.43660913584196875                        for SICK Relatedness

2019-03-12 19:30:13,529 : 

***** Transfer task : STSBenchmark*****


2019-03-12 19:30:13,577 : loading BERT model bert-large-uncased
2019-03-12 19:30:13,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:30:13,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:30:13,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfqtytbi_
2019-03-12 19:30:21,015 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:30:26,315 : Computing embedding for train
2019-03-12 19:30:45,064 : Computed train embeddings
2019-03-12 19:30:45,064 : Computing embedding for dev
2019-03-12 19:30:50,768 : Computed dev embeddings
2019-03-12 19:30:50,768 : Computing embedding for test
2019-03-12 19:30:55,434 : Computed test embeddings
2019-03-12 19:31:14,831 : Dev : Pearson 0.646174588692661
2019-03-12 19:31:14,831 : Test : Pearson 0.6124588490856715 Spearman 0.6003524651779499 MSE 1.7492344982119725                        for SICK Relatedness

2019-03-12 19:31:14,831 : ***** Transfer task : SNLI Entailment*****


2019-03-12 19:31:19,854 : loading BERT model bert-large-uncased
2019-03-12 19:31:19,854 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:31:19,924 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:31:19,924 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0wh3p0tk
2019-03-12 19:31:27,333 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:31:32,939 : PROGRESS (encoding): 0.00%
2019-03-12 19:34:23,955 : PROGRESS (encoding): 14.56%
2019-03-12 19:37:38,248 : PROGRESS (encoding): 29.12%
2019-03-12 19:40:50,609 : PROGRESS (encoding): 43.69%
2019-03-12 19:44:15,190 : PROGRESS (encoding): 58.25%
2019-03-12 19:48:03,766 : PROGRESS (encoding): 72.81%
2019-03-12 19:51:50,543 : PROGRESS (encoding): 87.37%
2019-03-12 19:55:54,026 : PROGRESS (encoding): 0.00%
2019-03-12 19:56:24,761 : PROGRESS (encoding): 0.00%
2019-03-12 19:56:54,427 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 19:57:31,661 : [('reg:1e-09', 62.39)]
2019-03-12 19:57:31,661 : Validation : best param found is reg = 1e-09 with score             62.39
2019-03-12 19:57:31,661 : Evaluating...
2019-03-12 19:58:07,595 : Dev acc : 62.39 Test acc : 62.52 for SNLI

2019-03-12 19:58:07,595 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 19:58:07,822 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 19:58:08,879 : loading BERT model bert-large-uncased
2019-03-12 19:58:08,880 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 19:58:08,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 19:58:08,906 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpye_h65wt
2019-03-12 19:58:16,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 19:58:21,533 : Computing embeddings for train/dev/test
2019-03-12 20:01:57,291 : Computed embeddings
2019-03-12 20:01:57,291 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:02:24,161 : [('reg:1e-05', 93.5), ('reg:0.0001', 88.99), ('reg:0.001', 86.88), ('reg:0.01', 85.51)]
2019-03-12 20:02:24,162 : Validation : best param found is reg = 1e-05 with score             93.5
2019-03-12 20:02:24,162 : Evaluating...
2019-03-12 20:02:34,230 : 
Dev acc : 93.5 Test acc : 93.5 for LENGTH classification

2019-03-12 20:02:34,231 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 20:02:34,484 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 20:02:34,530 : loading BERT model bert-large-uncased
2019-03-12 20:02:34,530 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:02:34,561 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:02:34,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp96dcoeyq
2019-03-12 20:02:42,236 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:02:47,656 : Computing embeddings for train/dev/test
2019-03-12 20:06:06,606 : Computed embeddings
2019-03-12 20:06:06,606 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:06:38,631 : [('reg:1e-05', 21.55), ('reg:0.0001', 2.53), ('reg:0.001', 0.55), ('reg:0.01', 0.22)]
2019-03-12 20:06:38,631 : Validation : best param found is reg = 1e-05 with score             21.55
2019-03-12 20:06:38,631 : Evaluating...
2019-03-12 20:06:45,933 : 
Dev acc : 21.6 Test acc : 22.1 for WORDCONTENT classification

2019-03-12 20:06:45,934 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 20:06:46,468 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 20:06:46,534 : loading BERT model bert-large-uncased
2019-03-12 20:06:46,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:06:46,558 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:06:46,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2p478v0f
2019-03-12 20:06:54,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:06:59,408 : Computing embeddings for train/dev/test
2019-03-12 20:10:07,182 : Computed embeddings
2019-03-12 20:10:07,182 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:10:32,346 : [('reg:1e-05', 29.61), ('reg:0.0001', 28.62), ('reg:0.001', 27.59), ('reg:0.01', 23.54)]
2019-03-12 20:10:32,346 : Validation : best param found is reg = 1e-05 with score             29.61
2019-03-12 20:10:32,346 : Evaluating...
2019-03-12 20:10:39,875 : 
Dev acc : 29.6 Test acc : 29.8 for DEPTH classification

2019-03-12 20:10:39,876 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 20:10:40,259 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 20:10:40,323 : loading BERT model bert-large-uncased
2019-03-12 20:10:40,323 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:10:40,435 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:10:40,436 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpecvf25ng
2019-03-12 20:10:47,924 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:10:53,236 : Computing embeddings for train/dev/test
2019-03-12 20:13:46,239 : Computed embeddings
2019-03-12 20:13:46,239 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:14:16,185 : [('reg:1e-05', 53.68), ('reg:0.0001', 52.78), ('reg:0.001', 43.8), ('reg:0.01', 19.5)]
2019-03-12 20:14:16,185 : Validation : best param found is reg = 1e-05 with score             53.68
2019-03-12 20:14:16,185 : Evaluating...
2019-03-12 20:14:22,263 : 
Dev acc : 53.7 Test acc : 54.0 for TOPCONSTITUENTS classification

2019-03-12 20:14:22,264 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 20:14:22,643 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 20:14:22,709 : loading BERT model bert-large-uncased
2019-03-12 20:14:22,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:14:22,740 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:14:22,740 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzg3t_87p
2019-03-12 20:14:30,376 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:14:35,670 : Computing embeddings for train/dev/test
2019-03-12 20:17:44,023 : Computed embeddings
2019-03-12 20:17:44,023 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:18:06,283 : [('reg:1e-05', 55.83), ('reg:0.0001', 56.01), ('reg:0.001', 55.91), ('reg:0.01', 53.24)]
2019-03-12 20:18:06,283 : Validation : best param found is reg = 0.0001 with score             56.01
2019-03-12 20:18:06,283 : Evaluating...
2019-03-12 20:18:11,677 : 
Dev acc : 56.0 Test acc : 55.4 for BIGRAMSHIFT classification

2019-03-12 20:18:11,678 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 20:18:12,069 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 20:18:12,134 : loading BERT model bert-large-uncased
2019-03-12 20:18:12,134 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:18:12,164 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:18:12,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwegwe_ze
2019-03-12 20:18:19,628 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:18:24,829 : Computing embeddings for train/dev/test
2019-03-12 20:21:29,336 : Computed embeddings
2019-03-12 20:21:29,337 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:21:51,542 : [('reg:1e-05', 85.06), ('reg:0.0001', 85.23), ('reg:0.001', 84.58), ('reg:0.01', 82.08)]
2019-03-12 20:21:51,542 : Validation : best param found is reg = 0.0001 with score             85.23
2019-03-12 20:21:51,542 : Evaluating...
2019-03-12 20:21:57,984 : 
Dev acc : 85.2 Test acc : 83.8 for TENSE classification

2019-03-12 20:21:57,985 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 20:21:58,387 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 20:21:58,449 : loading BERT model bert-large-uncased
2019-03-12 20:21:58,449 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:21:58,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:21:58,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1das1ick
2019-03-12 20:22:06,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:22:11,307 : Computing embeddings for train/dev/test
2019-03-12 20:25:26,029 : Computed embeddings
2019-03-12 20:25:26,029 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:25:57,335 : [('reg:1e-05', 78.56), ('reg:0.0001', 78.86), ('reg:0.001', 75.66), ('reg:0.01', 74.21)]
2019-03-12 20:25:57,335 : Validation : best param found is reg = 0.0001 with score             78.86
2019-03-12 20:25:57,335 : Evaluating...
2019-03-12 20:26:04,804 : 
Dev acc : 78.9 Test acc : 78.6 for SUBJNUMBER classification

2019-03-12 20:26:04,805 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 20:26:05,208 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 20:26:05,276 : loading BERT model bert-large-uncased
2019-03-12 20:26:05,276 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:26:05,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:26:05,392 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd0q9yy8q
2019-03-12 20:26:12,887 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:26:18,158 : Computing embeddings for train/dev/test
2019-03-12 20:29:28,776 : Computed embeddings
2019-03-12 20:29:28,776 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:30:01,692 : [('reg:1e-05', 76.1), ('reg:0.0001', 76.02), ('reg:0.001', 75.51), ('reg:0.01', 70.56)]
2019-03-12 20:30:01,692 : Validation : best param found is reg = 1e-05 with score             76.1
2019-03-12 20:30:01,693 : Evaluating...
2019-03-12 20:30:09,247 : 
Dev acc : 76.1 Test acc : 78.3 for OBJNUMBER classification

2019-03-12 20:30:09,248 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 20:30:09,822 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 20:30:09,891 : loading BERT model bert-large-uncased
2019-03-12 20:30:09,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:30:09,919 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:30:09,919 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp34i35_rr
2019-03-12 20:30:17,451 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:30:22,725 : Computing embeddings for train/dev/test
2019-03-12 20:34:04,427 : Computed embeddings
2019-03-12 20:34:04,427 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:34:29,521 : [('reg:1e-05', 52.3), ('reg:0.0001', 52.25), ('reg:0.001', 52.02), ('reg:0.01', 50.67)]
2019-03-12 20:34:29,521 : Validation : best param found is reg = 1e-05 with score             52.3
2019-03-12 20:34:29,521 : Evaluating...
2019-03-12 20:34:36,018 : 
Dev acc : 52.3 Test acc : 53.3 for ODDMANOUT classification

2019-03-12 20:34:36,019 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 20:34:36,396 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 20:34:36,476 : loading BERT model bert-large-uncased
2019-03-12 20:34:36,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:34:36,506 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:34:36,506 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph34xwyl6
2019-03-12 20:34:43,985 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:34:49,322 : Computing embeddings for train/dev/test
2019-03-12 20:38:26,479 : Computed embeddings
2019-03-12 20:38:26,479 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:38:49,865 : [('reg:1e-05', 56.78), ('reg:0.0001', 56.8), ('reg:0.001', 55.52), ('reg:0.01', 50.32)]
2019-03-12 20:38:49,865 : Validation : best param found is reg = 0.0001 with score             56.8
2019-03-12 20:38:49,865 : Evaluating...
2019-03-12 20:38:55,004 : 
Dev acc : 56.8 Test acc : 57.6 for COORDINATIONINVERSION classification

2019-03-12 20:38:55,006 : total results: {'STS12': {'MSRpar': {'pearson': (0.20311318185198998, 2.0022918289386686e-08), 'spearman': SpearmanrResult(correlation=0.21302139117135901, pvalue=3.818837060487997e-09), 'nsamples': 750}, 'MSRvid': {'pearson': (0.2629265609568851, 2.5232888303513983e-13), 'spearman': SpearmanrResult(correlation=0.2817901112949186, pvalue=3.7152586798119164e-15), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3953176488833714, 1.2782573014896903e-18), 'spearman': SpearmanrResult(correlation=0.4662628459855648, pvalue=3.745361469056551e-26), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5034288998241635, 1.9376611753124613e-49), 'spearman': SpearmanrResult(correlation=0.5378673482910192, pvalue=1.8329317012438648e-57), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4791554326464679, 2.7072181158270697e-24), 'spearman': SpearmanrResult(correlation=0.4482914726047152, pvalue=4.034325971802711e-21), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3687883448325756, 'wmean': 0.35384018675617346}, 'spearman': {'mean': 0.38944663386951534, 'wmean': 0.3756087779744621}}}, 'STS13': {'FNWN': {'pearson': (0.22916424992199835, 0.001514416249737087), 'spearman': SpearmanrResult(correlation=0.2388122968206052, pvalue=0.0009350900180275964), 'nsamples': 189}, 'headlines': {'pearson': (0.34834366014980633, 8.074743060443838e-23), 'spearman': SpearmanrResult(correlation=0.33545542004104684, pvalue=3.50083132827522e-21), 'nsamples': 750}, 'OnWN': {'pearson': (0.014791091048127996, 0.7266605280004861), 'spearman': SpearmanrResult(correlation=0.055993035108720036, pvalue=0.18540322363409736), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.1974330003733109, 'wmean': 0.20857839361707484}, 'spearman': {'mean': 0.21008691732345733, 'wmean': 0.21875945455058096}}}, 'STS14': {'deft-forum': {'pearson': (-0.0018268522157123576, 0.9691729189508349), 'spearman': SpearmanrResult(correlation=0.030589622868316287, pvalue=0.5174697965810374), 'nsamples': 450}, 'deft-news': {'pearson': (0.19646563614814236, 0.0006214179398441753), 'spearman': SpearmanrResult(correlation=0.21113365935671585, pvalue=0.00023025025125576743), 'nsamples': 300}, 'headlines': {'pearson': (0.32830215468675517, 2.6302282637252497e-20), 'spearman': SpearmanrResult(correlation=0.3111549373493498, pvalue=2.6710666884033898e-18), 'nsamples': 750}, 'images': {'pearson': (0.3717600188045831, 5.395490770108066e-26), 'spearman': SpearmanrResult(correlation=0.38058488683067704, pvalue=2.9230463457642655e-27), 'nsamples': 750}, 'OnWN': {'pearson': (0.1897389888660944, 1.6482703471699345e-07), 'spearman': SpearmanrResult(correlation=0.25840815863501604, pvalue=6.602823904123452e-13), 'nsamples': 750}, 'tweet-news': {'pearson': (0.43551052833815224, 4.603403984002635e-36), 'spearman': SpearmanrResult(correlation=0.4178401237189519, pvalue=4.737432654695114e-33), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2533250791046691, 'wmean': 0.2805603667650829}, 'spearman': {'mean': 0.26828523145983785, 'wmean': 0.2941590687995342}}}, 'STS15': {'answers-forums': {'pearson': (0.20001673200917058, 9.62501023596593e-05), 'spearman': SpearmanrResult(correlation=0.19806543250653105, pvalue=0.00011290815600840968), 'nsamples': 375}, 'answers-students': {'pearson': (0.4888244430581004, 2.645701709131368e-46), 'spearman': SpearmanrResult(correlation=0.4854204109879828, pvalue=1.3545473035509857e-45), 'nsamples': 750}, 'belief': {'pearson': (0.18103118601824295, 0.00042652593572169677), 'spearman': SpearmanrResult(correlation=0.1632255696500661, pvalue=0.0015160512834003304), 'nsamples': 375}, 'headlines': {'pearson': (0.46216575937603355, 5.877481333447674e-41), 'spearman': SpearmanrResult(correlation=0.4718154020532736, pvalue=7.728358885050964e-43), 'nsamples': 750}, 'images': {'pearson': (0.3434855744239661, 3.4135974762412923e-22), 'spearman': SpearmanrResult(correlation=0.3567414724910816, pvalue=6.290980719347997e-24), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.33510473897710275, 'wmean': 0.3712499339679517}, 'spearman': {'mean': 0.33505365753778704, 'wmean': 0.37365569665265913}}}, 'STS16': {'answer-answer': {'pearson': (0.32249728724477994, 1.476244729737919e-07), 'spearman': SpearmanrResult(correlation=0.31854888572958756, pvalue=2.130684070001846e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.4945955934700635, 9.094280483211632e-17), 'spearman': SpearmanrResult(correlation=0.49794534600945833, pvalue=5.2399996280379865e-17), 'nsamples': 249}, 'plagiarism': {'pearson': (0.426212473322644, 1.4433435202457458e-11), 'spearman': SpearmanrResult(correlation=0.4420401876695829, pvalue=2.0217596501775093e-12), 'nsamples': 230}, 'postediting': {'pearson': (0.5746849807909155, 7.604117679488875e-23), 'spearman': SpearmanrResult(correlation=0.5801742186601407, pvalue=2.3836813152115937e-23), 'nsamples': 244}, 'question-question': {'pearson': (0.13724230875790433, 0.047526692013466584), 'spearman': SpearmanrResult(correlation=0.13964493006494144, pvalue=0.0437364936315404), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.3910465287172614, 'wmean': 0.39797998350911756}, 'spearman': {'mean': 0.39567071362674217, 'wmean': 0.40245982380465556}}}, 'MR': {'devacc': 71.14, 'acc': 70.56, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.39, 'acc': 70.57, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.83, 'acc': 84.42, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.39, 'acc': 91.44, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.29, 'acc': 78.58, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.78, 'acc': 40.9, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 65.26, 'acc': 80.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.78, 'acc': 70.38, 'f1': 78.39, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 73.94, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.745537135201924, 'pearson': 0.7560995823774236, 'spearman': 0.6818884432169, 'mse': 0.43660913584196875, 'yhat': array([2.18801197, 4.12625898, 1.71789075, ..., 3.02964568, 4.29829607,        4.41854135]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.646174588692661, 'pearson': 0.6124588490856715, 'spearman': 0.6003524651779499, 'mse': 1.7492344982119725, 'yhat': array([2.75112487, 1.86783482, 2.38787686, ..., 4.12504453, 3.74817282,        3.11761675]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.39, 'acc': 62.52, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 93.5, 'acc': 93.47, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 21.55, 'acc': 22.11, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.61, 'acc': 29.81, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.68, 'acc': 53.98, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 56.01, 'acc': 55.36, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.23, 'acc': 83.84, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.86, 'acc': 78.61, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.1, 'acc': 78.32, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 52.3, 'acc': 53.3, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.8, 'acc': 57.57, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 20:38:55,007 : STS12 p=0.3538, STS12 s=0.3756, STS13 p=0.2086, STS13 s=0.2188, STS14 p=0.2806, STS14 s=0.2942, STS15 p=0.3712, STS15 s=0.3737, STS 16 p=0.3980, STS16 s=0.4025, STS B p=0.6125, STS B s=0.6004, STS B m=1.7492, SICK-R p=0.7561, SICK-R s=0.6819, SICK-P m=0.4366
2019-03-12 20:38:55,007 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 20:38:55,007 : 0.3538,0.3756,0.2086,0.2188,0.2806,0.2942,0.3712,0.3737,0.3980,0.4025,0.6125,0.6004,1.7492,0.7561,0.6819,0.4366
2019-03-12 20:38:55,007 : MR=70.56, CR=70.57, SUBJ=91.44, MPQA=84.42, SST-B=78.58, SST-F=40.90, TREC=80.00, SICK-E=73.94, SNLI=62.52, MRPC=70.38, MRPC f=78.39
2019-03-12 20:38:55,007 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 20:38:55,007 : 70.56,70.57,91.44,84.42,78.58,40.90,80.00,73.94,62.52,70.38,78.39
2019-03-12 20:38:55,007 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 20:38:55,007 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 20:38:55,007 : na,na,na,na,na,na,na,na,na,na
2019-03-12 20:38:55,007 : SentLen=93.47, WC=22.11, TreeDepth=29.81, TopConst=53.98, BShift=55.36, Tense=83.84, SubjNum=78.61, ObjNum=78.32, SOMO=53.30, CoordInv=57.57, average=60.64
2019-03-12 20:38:55,007 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 20:38:55,007 : 93.47,22.11,29.81,53.98,55.36,83.84,78.61,78.32,53.30,57.57,60.64
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,007 : layer 5
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,007 : ********************************************************************************
2019-03-12 20:38:55,102 : ***** Transfer task : STS12 *****


2019-03-12 20:38:55,114 : loading BERT model bert-large-uncased
2019-03-12 20:38:55,115 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:38:55,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:38:55,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt0_hrfp7
2019-03-12 20:39:02,569 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:39:11,967 : MSRpar : pearson = 0.2422, spearman = 0.2472
2019-03-12 20:39:13,617 : MSRvid : pearson = 0.3741, spearman = 0.3926
2019-03-12 20:39:15,035 : SMTeuroparl : pearson = 0.4129, spearman = 0.4864
2019-03-12 20:39:17,748 : surprise.OnWN : pearson = 0.5029, spearman = 0.5122
2019-03-12 20:39:19,182 : surprise.SMTnews : pearson = 0.5506, spearman = 0.4750
2019-03-12 20:39:19,182 : ALL (weighted average) : Pearson = 0.4017,             Spearman = 0.4108
2019-03-12 20:39:19,182 : ALL (average) : Pearson = 0.4165,             Spearman = 0.4227

2019-03-12 20:39:19,182 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 20:39:19,191 : loading BERT model bert-large-uncased
2019-03-12 20:39:19,191 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:39:19,208 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:39:19,208 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppw2647al
2019-03-12 20:39:26,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:39:33,320 : FNWN : pearson = 0.2119, spearman = 0.2179
2019-03-12 20:39:35,224 : headlines : pearson = 0.3969, spearman = 0.3898
2019-03-12 20:39:36,698 : OnWN : pearson = 0.1243, spearman = 0.1736
2019-03-12 20:39:36,698 : ALL (weighted average) : Pearson = 0.2717,             Spearman = 0.2873
2019-03-12 20:39:36,698 : ALL (average) : Pearson = 0.2444,             Spearman = 0.2605

2019-03-12 20:39:36,698 : ***** Transfer task : STS14 *****


2019-03-12 20:39:36,713 : loading BERT model bert-large-uncased
2019-03-12 20:39:36,713 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:39:36,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:39:36,731 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl34d7tkz
2019-03-12 20:39:44,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:39:50,830 : deft-forum : pearson = 0.1024, spearman = 0.1365
2019-03-12 20:39:52,475 : deft-news : pearson = 0.2710, spearman = 0.2902
2019-03-12 20:39:54,655 : headlines : pearson = 0.3757, spearman = 0.3490
2019-03-12 20:39:56,741 : images : pearson = 0.4611, spearman = 0.4535
2019-03-12 20:39:58,880 : OnWN : pearson = 0.2927, spearman = 0.3481
2019-03-12 20:40:01,755 : tweet-news : pearson = 0.5094, spearman = 0.4828
2019-03-12 20:40:01,756 : ALL (weighted average) : Pearson = 0.3617,             Spearman = 0.3662
2019-03-12 20:40:01,756 : ALL (average) : Pearson = 0.3354,             Spearman = 0.3433

2019-03-12 20:40:01,756 : ***** Transfer task : STS15 *****


2019-03-12 20:40:01,788 : loading BERT model bert-large-uncased
2019-03-12 20:40:01,788 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:40:01,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:40:01,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpca2odzih
2019-03-12 20:40:09,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:40:16,304 : answers-forums : pearson = 0.3091, spearman = 0.3020
2019-03-12 20:40:18,399 : answers-students : pearson = 0.5151, spearman = 0.5178
2019-03-12 20:40:20,457 : belief : pearson = 0.2629, spearman = 0.2490
2019-03-12 20:40:22,721 : headlines : pearson = 0.5055, spearman = 0.5157
2019-03-12 20:40:24,867 : images : pearson = 0.3654, spearman = 0.4009
2019-03-12 20:40:24,867 : ALL (weighted average) : Pearson = 0.4180,             Spearman = 0.4275
2019-03-12 20:40:24,867 : ALL (average) : Pearson = 0.3916,             Spearman = 0.3971

2019-03-12 20:40:24,867 : ***** Transfer task : STS16 *****


2019-03-12 20:40:24,940 : loading BERT model bert-large-uncased
2019-03-12 20:40:24,940 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:40:24,958 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:40:24,958 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvhuu2dyq
2019-03-12 20:40:32,446 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:40:38,573 : answer-answer : pearson = 0.4086, spearman = 0.4092
2019-03-12 20:40:39,236 : headlines : pearson = 0.5243, spearman = 0.5316
2019-03-12 20:40:40,120 : plagiarism : pearson = 0.5436, spearman = 0.5573
2019-03-12 20:40:41,622 : postediting : pearson = 0.6289, spearman = 0.6434
2019-03-12 20:40:42,228 : question-question : pearson = 0.2791, spearman = 0.2494
2019-03-12 20:40:42,229 : ALL (weighted average) : Pearson = 0.4816,             Spearman = 0.4837
2019-03-12 20:40:42,229 : ALL (average) : Pearson = 0.4769,             Spearman = 0.4782

2019-03-12 20:40:42,229 : ***** Transfer task : MR *****


2019-03-12 20:40:42,244 : loading BERT model bert-large-uncased
2019-03-12 20:40:42,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:40:42,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:40:42,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfrf0ixwb
2019-03-12 20:40:49,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:40:54,920 : Generating sentence embeddings
2019-03-12 20:41:26,502 : Generated sentence embeddings
2019-03-12 20:41:26,503 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:41:36,691 : Best param found at split 1: l2reg = 1e-05                 with score 72.46
2019-03-12 20:41:48,062 : Best param found at split 2: l2reg = 1e-05                 with score 71.61
2019-03-12 20:41:58,769 : Best param found at split 3: l2reg = 0.001                 with score 72.46
2019-03-12 20:42:09,423 : Best param found at split 4: l2reg = 1e-05                 with score 72.7
2019-03-12 20:42:20,121 : Best param found at split 5: l2reg = 0.0001                 with score 71.32
2019-03-12 20:42:20,645 : Dev acc : 72.11 Test acc : 71.21

2019-03-12 20:42:20,646 : ***** Transfer task : CR *****


2019-03-12 20:42:20,654 : loading BERT model bert-large-uncased
2019-03-12 20:42:20,654 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:42:20,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:42:20,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1n59o59_
2019-03-12 20:42:28,076 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:42:33,279 : Generating sentence embeddings
2019-03-12 20:42:41,607 : Generated sentence embeddings
2019-03-12 20:42:41,607 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:42:45,824 : Best param found at split 1: l2reg = 1e-05                 with score 77.31
2019-03-12 20:42:49,833 : Best param found at split 2: l2reg = 0.001                 with score 77.11
2019-03-12 20:42:53,631 : Best param found at split 3: l2reg = 1e-05                 with score 76.66
2019-03-12 20:42:57,647 : Best param found at split 4: l2reg = 0.001                 with score 76.17
2019-03-12 20:43:01,229 : Best param found at split 5: l2reg = 0.0001                 with score 75.74
2019-03-12 20:43:01,506 : Dev acc : 76.6 Test acc : 72.4

2019-03-12 20:43:01,507 : ***** Transfer task : MPQA *****


2019-03-12 20:43:01,513 : loading BERT model bert-large-uncased
2019-03-12 20:43:01,513 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:43:01,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:43:01,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj8pkn461
2019-03-12 20:43:09,021 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:43:14,196 : Generating sentence embeddings
2019-03-12 20:43:21,789 : Generated sentence embeddings
2019-03-12 20:43:21,790 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:43:32,102 : Best param found at split 1: l2reg = 0.0001                 with score 85.25
2019-03-12 20:43:42,462 : Best param found at split 2: l2reg = 0.0001                 with score 84.15
2019-03-12 20:43:53,352 : Best param found at split 3: l2reg = 0.0001                 with score 85.32
2019-03-12 20:44:02,982 : Best param found at split 4: l2reg = 0.0001                 with score 84.64
2019-03-12 20:44:14,291 : Best param found at split 5: l2reg = 0.001                 with score 83.31
2019-03-12 20:44:14,879 : Dev acc : 84.53 Test acc : 85.05

2019-03-12 20:44:14,880 : ***** Transfer task : SUBJ *****


2019-03-12 20:44:14,897 : loading BERT model bert-large-uncased
2019-03-12 20:44:14,897 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:44:14,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:44:14,916 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb6qjk5l4
2019-03-12 20:44:22,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:44:27,634 : Generating sentence embeddings
2019-03-12 20:44:58,660 : Generated sentence embeddings
2019-03-12 20:44:58,661 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 20:45:08,134 : Best param found at split 1: l2reg = 1e-05                 with score 92.55
2019-03-12 20:45:20,337 : Best param found at split 2: l2reg = 0.0001                 with score 93.14
2019-03-12 20:45:31,038 : Best param found at split 3: l2reg = 1e-05                 with score 92.78
2019-03-12 20:45:39,436 : Best param found at split 4: l2reg = 1e-05                 with score 92.9
2019-03-12 20:45:49,737 : Best param found at split 5: l2reg = 1e-05                 with score 92.71
2019-03-12 20:45:50,228 : Dev acc : 92.82 Test acc : 91.93

2019-03-12 20:45:50,229 : ***** Transfer task : SST Binary classification *****


2019-03-12 20:45:50,321 : loading BERT model bert-large-uncased
2019-03-12 20:45:50,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:45:50,395 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:45:50,396 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc73m4bze
2019-03-12 20:45:57,821 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:46:03,093 : Computing embedding for train
2019-03-12 20:47:43,887 : Computed train embeddings
2019-03-12 20:47:43,887 : Computing embedding for dev
2019-03-12 20:47:46,093 : Computed dev embeddings
2019-03-12 20:47:46,093 : Computing embedding for test
2019-03-12 20:47:50,728 : Computed test embeddings
2019-03-12 20:47:50,728 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:48:06,278 : [('reg:1e-05', 77.64), ('reg:0.0001', 77.75), ('reg:0.001', 77.29), ('reg:0.01', 76.38)]
2019-03-12 20:48:06,278 : Validation : best param found is reg = 0.0001 with score             77.75
2019-03-12 20:48:06,279 : Evaluating...
2019-03-12 20:48:10,581 : 
Dev acc : 77.75 Test acc : 79.24 for             SST Binary classification

2019-03-12 20:48:10,581 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 20:48:10,637 : loading BERT model bert-large-uncased
2019-03-12 20:48:10,637 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:48:10,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:48:10,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpli8g29hi
2019-03-12 20:48:18,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:48:23,337 : Computing embedding for train
2019-03-12 20:48:45,415 : Computed train embeddings
2019-03-12 20:48:45,416 : Computing embedding for dev
2019-03-12 20:48:48,306 : Computed dev embeddings
2019-03-12 20:48:48,306 : Computing embedding for test
2019-03-12 20:48:53,993 : Computed test embeddings
2019-03-12 20:48:53,993 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:48:55,841 : [('reg:1e-05', 37.24), ('reg:0.0001', 37.42), ('reg:0.001', 38.78), ('reg:0.01', 34.6)]
2019-03-12 20:48:55,842 : Validation : best param found is reg = 0.001 with score             38.78
2019-03-12 20:48:55,842 : Evaluating...
2019-03-12 20:48:56,379 : 
Dev acc : 38.78 Test acc : 38.82 for             SST Fine-Grained classification

2019-03-12 20:48:56,379 : ***** Transfer task : TREC *****


2019-03-12 20:48:56,393 : loading BERT model bert-large-uncased
2019-03-12 20:48:56,393 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:48:56,412 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:48:56,412 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyl22lxfj
2019-03-12 20:49:03,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:49:16,711 : Computed train embeddings
2019-03-12 20:49:17,305 : Computed test embeddings
2019-03-12 20:49:17,306 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:49:24,911 : [('reg:1e-05', 67.02), ('reg:0.0001', 66.67), ('reg:0.001', 63.01), ('reg:0.01', 49.53)]
2019-03-12 20:49:24,911 : Cross-validation : best param found is reg = 1e-05             with score 67.02
2019-03-12 20:49:24,911 : Evaluating...
2019-03-12 20:49:25,358 : 
Dev acc : 67.02 Test acc : 81.6             for TREC

2019-03-12 20:49:25,359 : ***** Transfer task : MRPC *****


2019-03-12 20:49:25,380 : loading BERT model bert-large-uncased
2019-03-12 20:49:25,380 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:49:25,402 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:49:25,402 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg584hfg4
2019-03-12 20:49:32,855 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:49:38,014 : Computing embedding for train
2019-03-12 20:50:00,407 : Computed train embeddings
2019-03-12 20:50:00,407 : Computing embedding for test
2019-03-12 20:50:10,244 : Computed test embeddings
2019-03-12 20:50:10,265 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 20:50:15,041 : [('reg:1e-05', 69.46), ('reg:0.0001', 70.24), ('reg:0.001', 70.41), ('reg:0.01', 68.99)]
2019-03-12 20:50:15,041 : Cross-validation : best param found is reg = 0.001             with score 70.41
2019-03-12 20:50:15,041 : Evaluating...
2019-03-12 20:50:15,349 : Dev acc : 70.41 Test acc 67.25; Test F1 72.69 for MRPC.

2019-03-12 20:50:15,349 : ***** Transfer task : SICK-Entailment*****


2019-03-12 20:50:15,425 : loading BERT model bert-large-uncased
2019-03-12 20:50:15,425 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:50:15,444 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:50:15,444 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpws5sk2x2
2019-03-12 20:50:22,863 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:50:28,124 : Computing embedding for train
2019-03-12 20:50:39,485 : Computed train embeddings
2019-03-12 20:50:39,485 : Computing embedding for dev
2019-03-12 20:50:41,041 : Computed dev embeddings
2019-03-12 20:50:41,041 : Computing embedding for test
2019-03-12 20:50:53,258 : Computed test embeddings
2019-03-12 20:50:53,295 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 20:50:55,021 : [('reg:1e-05', 71.2), ('reg:0.0001', 74.0), ('reg:0.001', 69.6), ('reg:0.01', 62.8)]
2019-03-12 20:50:55,021 : Validation : best param found is reg = 0.0001 with score             74.0
2019-03-12 20:50:55,021 : Evaluating...
2019-03-12 20:50:55,562 : 
Dev acc : 74.0 Test acc : 74.28 for                        SICK entailment

2019-03-12 20:50:55,563 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 20:50:55,589 : loading BERT model bert-large-uncased
2019-03-12 20:50:55,590 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:50:55,646 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:50:55,647 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4p7cv9ko
2019-03-12 20:51:03,038 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:51:08,244 : Computing embedding for train
2019-03-12 20:51:19,649 : Computed train embeddings
2019-03-12 20:51:19,649 : Computing embedding for dev
2019-03-12 20:51:21,206 : Computed dev embeddings
2019-03-12 20:51:21,206 : Computing embedding for test
2019-03-12 20:51:33,444 : Computed test embeddings
2019-03-12 20:51:54,246 : Dev : Pearson 0.7634025153663506
2019-03-12 20:51:54,246 : Test : Pearson 0.7619016921236156 Spearman 0.6893070618415091 MSE 0.4269440286055696                        for SICK Relatedness

2019-03-12 20:51:54,247 : 

***** Transfer task : STSBenchmark*****


2019-03-12 20:51:54,296 : loading BERT model bert-large-uncased
2019-03-12 20:51:54,296 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:51:54,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:51:54,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp16rvd3nd
2019-03-12 20:52:01,753 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:52:06,931 : Computing embedding for train
2019-03-12 20:52:25,673 : Computed train embeddings
2019-03-12 20:52:25,673 : Computing embedding for dev
2019-03-12 20:52:31,371 : Computed dev embeddings
2019-03-12 20:52:31,371 : Computing embedding for test
2019-03-12 20:52:36,024 : Computed test embeddings
2019-03-12 20:52:55,198 : Dev : Pearson 0.6803184129709453
2019-03-12 20:52:55,198 : Test : Pearson 0.6454740131980484 Spearman 0.6373780322141608 MSE 1.686444214791793                        for SICK Relatedness

2019-03-12 20:52:55,199 : ***** Transfer task : SNLI Entailment*****


2019-03-12 20:53:00,250 : loading BERT model bert-large-uncased
2019-03-12 20:53:00,250 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 20:53:00,385 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 20:53:00,386 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3vr6p_ek
2019-03-12 20:53:07,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 20:53:13,520 : PROGRESS (encoding): 0.00%
2019-03-12 20:56:04,606 : PROGRESS (encoding): 14.56%
2019-03-12 20:59:19,252 : PROGRESS (encoding): 29.12%
2019-03-12 21:02:31,008 : PROGRESS (encoding): 43.69%
2019-03-12 21:05:55,970 : PROGRESS (encoding): 58.25%
2019-03-12 21:09:43,989 : PROGRESS (encoding): 72.81%
2019-03-12 21:13:31,024 : PROGRESS (encoding): 87.37%
2019-03-12 21:17:34,973 : PROGRESS (encoding): 0.00%
2019-03-12 21:18:05,803 : PROGRESS (encoding): 0.00%
2019-03-12 21:18:35,510 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:19:00,296 : [('reg:1e-09', 61.87)]
2019-03-12 21:19:00,296 : Validation : best param found is reg = 1e-09 with score             61.87
2019-03-12 21:19:00,296 : Evaluating...
2019-03-12 21:19:26,262 : Dev acc : 61.87 Test acc : 62.16 for SNLI

2019-03-12 21:19:26,262 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 21:19:26,467 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 21:19:27,540 : loading BERT model bert-large-uncased
2019-03-12 21:19:27,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:19:27,568 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:19:27,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2s7y2omx
2019-03-12 21:19:35,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:19:40,256 : Computing embeddings for train/dev/test
2019-03-12 21:23:16,286 : Computed embeddings
2019-03-12 21:23:16,286 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:23:43,345 : [('reg:1e-05', 94.17), ('reg:0.0001', 88.48), ('reg:0.001', 86.89), ('reg:0.01', 83.75)]
2019-03-12 21:23:43,345 : Validation : best param found is reg = 1e-05 with score             94.17
2019-03-12 21:23:43,345 : Evaluating...
2019-03-12 21:23:54,084 : 
Dev acc : 94.2 Test acc : 94.5 for LENGTH classification

2019-03-12 21:23:54,085 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 21:23:54,462 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 21:23:54,508 : loading BERT model bert-large-uncased
2019-03-12 21:23:54,508 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:23:54,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:23:54,537 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkd_01yg8
2019-03-12 21:24:02,007 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:24:07,245 : Computing embeddings for train/dev/test
2019-03-12 21:27:26,365 : Computed embeddings
2019-03-12 21:27:26,366 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:27:53,524 : [('reg:1e-05', 25.06), ('reg:0.0001', 2.29), ('reg:0.001', 0.54), ('reg:0.01', 0.27)]
2019-03-12 21:27:53,524 : Validation : best param found is reg = 1e-05 with score             25.06
2019-03-12 21:27:53,525 : Evaluating...
2019-03-12 21:27:59,764 : 
Dev acc : 25.1 Test acc : 25.4 for WORDCONTENT classification

2019-03-12 21:27:59,766 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 21:28:00,150 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 21:28:00,217 : loading BERT model bert-large-uncased
2019-03-12 21:28:00,218 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:28:00,243 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:28:00,243 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1co1gxuk
2019-03-12 21:28:07,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:28:13,028 : Computing embeddings for train/dev/test
2019-03-12 21:31:20,323 : Computed embeddings
2019-03-12 21:31:20,323 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:31:43,189 : [('reg:1e-05', 30.92), ('reg:0.0001', 29.89), ('reg:0.001', 27.31), ('reg:0.01', 23.79)]
2019-03-12 21:31:43,189 : Validation : best param found is reg = 1e-05 with score             30.92
2019-03-12 21:31:43,189 : Evaluating...
2019-03-12 21:31:49,626 : 
Dev acc : 30.9 Test acc : 31.5 for DEPTH classification

2019-03-12 21:31:49,627 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 21:31:50,015 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 21:31:50,079 : loading BERT model bert-large-uncased
2019-03-12 21:31:50,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:31:50,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:31:50,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpotbmy1xg
2019-03-12 21:31:57,650 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:32:03,008 : Computing embeddings for train/dev/test
2019-03-12 21:34:56,300 : Computed embeddings
2019-03-12 21:34:56,300 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:35:30,563 : [('reg:1e-05', 55.98), ('reg:0.0001', 50.08), ('reg:0.001', 36.16), ('reg:0.01', 17.77)]
2019-03-12 21:35:30,564 : Validation : best param found is reg = 1e-05 with score             55.98
2019-03-12 21:35:30,564 : Evaluating...
2019-03-12 21:35:39,961 : 
Dev acc : 56.0 Test acc : 56.1 for TOPCONSTITUENTS classification

2019-03-12 21:35:39,962 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 21:35:40,309 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 21:35:40,376 : loading BERT model bert-large-uncased
2019-03-12 21:35:40,376 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:35:40,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:35:40,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3t6hvi5g
2019-03-12 21:35:48,019 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:35:53,211 : Computing embeddings for train/dev/test
2019-03-12 21:39:00,590 : Computed embeddings
2019-03-12 21:39:00,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:39:37,562 : [('reg:1e-05', 66.09), ('reg:0.0001', 65.55), ('reg:0.001', 62.87), ('reg:0.01', 50.19)]
2019-03-12 21:39:37,562 : Validation : best param found is reg = 1e-05 with score             66.09
2019-03-12 21:39:37,563 : Evaluating...
2019-03-12 21:39:48,436 : 
Dev acc : 66.1 Test acc : 65.7 for BIGRAMSHIFT classification

2019-03-12 21:39:48,437 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 21:39:48,989 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 21:39:49,055 : loading BERT model bert-large-uncased
2019-03-12 21:39:49,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:39:49,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:39:49,084 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl_3xg64y
2019-03-12 21:39:56,556 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:40:01,725 : Computing embeddings for train/dev/test
2019-03-12 21:43:04,750 : Computed embeddings
2019-03-12 21:43:04,750 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:43:33,009 : [('reg:1e-05', 85.25), ('reg:0.0001', 85.33), ('reg:0.001', 85.03), ('reg:0.01', 80.55)]
2019-03-12 21:43:33,009 : Validation : best param found is reg = 0.0001 with score             85.33
2019-03-12 21:43:33,009 : Evaluating...
2019-03-12 21:43:38,350 : 
Dev acc : 85.3 Test acc : 83.9 for TENSE classification

2019-03-12 21:43:38,351 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 21:43:38,774 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 21:43:38,836 : loading BERT model bert-large-uncased
2019-03-12 21:43:38,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:43:38,861 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:43:38,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0cawfshx
2019-03-12 21:43:46,321 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:43:51,572 : Computing embeddings for train/dev/test
2019-03-12 21:47:05,441 : Computed embeddings
2019-03-12 21:47:05,441 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:47:35,395 : [('reg:1e-05', 78.92), ('reg:0.0001', 77.76), ('reg:0.001', 73.8), ('reg:0.01', 71.52)]
2019-03-12 21:47:35,396 : Validation : best param found is reg = 1e-05 with score             78.92
2019-03-12 21:47:35,396 : Evaluating...
2019-03-12 21:47:44,078 : 
Dev acc : 78.9 Test acc : 78.7 for SUBJNUMBER classification

2019-03-12 21:47:44,079 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 21:47:44,482 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 21:47:44,549 : loading BERT model bert-large-uncased
2019-03-12 21:47:44,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:47:44,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:47:44,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp60sbvd6u
2019-03-12 21:47:52,158 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:47:57,371 : Computing embeddings for train/dev/test
2019-03-12 21:51:07,904 : Computed embeddings
2019-03-12 21:51:07,904 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:51:32,753 : [('reg:1e-05', 76.92), ('reg:0.0001', 76.85), ('reg:0.001', 74.99), ('reg:0.01', 69.69)]
2019-03-12 21:51:32,753 : Validation : best param found is reg = 1e-05 with score             76.92
2019-03-12 21:51:32,753 : Evaluating...
2019-03-12 21:51:40,154 : 
Dev acc : 76.9 Test acc : 77.9 for OBJNUMBER classification

2019-03-12 21:51:40,155 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 21:51:40,730 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 21:51:40,798 : loading BERT model bert-large-uncased
2019-03-12 21:51:40,798 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:51:40,823 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:51:40,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphb4sffyg
2019-03-12 21:51:48,265 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:51:53,476 : Computing embeddings for train/dev/test
2019-03-12 21:55:34,121 : Computed embeddings
2019-03-12 21:55:34,121 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 21:55:55,951 : [('reg:1e-05', 54.28), ('reg:0.0001', 54.06), ('reg:0.001', 53.53), ('reg:0.01', 52.12)]
2019-03-12 21:55:55,951 : Validation : best param found is reg = 1e-05 with score             54.28
2019-03-12 21:55:55,951 : Evaluating...
2019-03-12 21:56:01,911 : 
Dev acc : 54.3 Test acc : 55.3 for ODDMANOUT classification

2019-03-12 21:56:01,912 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 21:56:02,299 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 21:56:02,375 : loading BERT model bert-large-uncased
2019-03-12 21:56:02,375 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 21:56:02,497 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 21:56:02,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnq98cmam
2019-03-12 21:56:09,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 21:56:15,178 : Computing embeddings for train/dev/test
2019-03-12 21:59:51,934 : Computed embeddings
2019-03-12 21:59:51,934 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:00:17,477 : [('reg:1e-05', 57.44), ('reg:0.0001', 57.07), ('reg:0.001', 54.62), ('reg:0.01', 50.09)]
2019-03-12 22:00:17,477 : Validation : best param found is reg = 1e-05 with score             57.44
2019-03-12 22:00:17,477 : Evaluating...
2019-03-12 22:00:24,086 : 
Dev acc : 57.4 Test acc : 57.6 for COORDINATIONINVERSION classification

2019-03-12 22:00:24,088 : total results: {'STS12': {'MSRpar': {'pearson': (0.24219099329600188, 1.7932761149064665e-11), 'spearman': SpearmanrResult(correlation=0.2472055179085842, pvalue=6.623813794638781e-12), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3740925027110512, 2.5185268440760672e-26), 'spearman': SpearmanrResult(correlation=0.39257063317692503, pvalue=4.818060978756408e-29), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.41291399239708193, 2.518753144453078e-20), 'spearman': SpearmanrResult(correlation=0.48643631673498466, pvalue=1.2244679283757764e-28), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5028604215239822, 2.5833031681262984e-49), 'spearman': SpearmanrResult(correlation=0.5122021249854161, pvalue=2.135970850209919e-51), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5505962679848636, 5.2644091846005005e-33), 'spearman': SpearmanrResult(correlation=0.4749673755015848, pvalue=7.624928434273722e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4165308355825962, 'wmean': 0.40172888403619617}, 'spearman': {'mean': 0.4226763936614989, 'wmean': 0.4108011451929486}}}, 'STS13': {'FNWN': {'pearson': (0.21191345697290798, 0.0034189480504763997), 'spearman': SpearmanrResult(correlation=0.21794417360318669, pvalue=0.0025896894171542006), 'nsamples': 189}, 'headlines': {'pearson': (0.39694887106243787, 1.0304438791358759e-29), 'spearman': SpearmanrResult(correlation=0.3898463249293377, pvalue=1.2434458418509858e-28), 'nsamples': 750}, 'OnWN': {'pearson': (0.12428547820445868, 0.0031918547894130003), 'spearman': SpearmanrResult(correlation=0.17363754669903378, pvalue=3.550182662444629e-05), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2443826020799348, 'wmean': 0.27165829995827284}, 'spearman': {'mean': 0.26047601507718604, 'wmean': 0.28732457080410895}}}, 'STS14': {'deft-forum': {'pearson': (0.10236070021074475, 0.029926300295917627), 'spearman': SpearmanrResult(correlation=0.13645170563052816, pvalue=0.0037304020253932373), 'nsamples': 450}, 'deft-news': {'pearson': (0.27095091643936836, 1.9092086863782775e-06), 'spearman': SpearmanrResult(correlation=0.290201357887202, pvalue=3.121379221369995e-07), 'nsamples': 300}, 'headlines': {'pearson': (0.3756608934569761, 1.5036070510345573e-26), 'spearman': SpearmanrResult(correlation=0.34895647912898253, pvalue=6.71999776017294e-23), 'nsamples': 750}, 'images': {'pearson': (0.4611402855003572, 9.238193240028125e-41), 'spearman': SpearmanrResult(correlation=0.4534514584080405, pvalue=2.6124732081519934e-39), 'nsamples': 750}, 'OnWN': {'pearson': (0.29266116666918485, 2.808451758646713e-16), 'spearman': SpearmanrResult(correlation=0.348050508174, pvalue=8.814968967672294e-23), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5094089760149446, 9.100417296766209e-51), 'spearman': SpearmanrResult(correlation=0.4827828676467851, pvalue=4.7411089969511126e-45), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.33536382304859597, 'wmean': 0.3617336216687314}, 'spearman': {'mean': 0.34331572947925637, 'wmean': 0.3662385759782012}}}, 'STS15': {'answers-forums': {'pearson': (0.3090505928530087, 9.649687148770516e-10), 'spearman': SpearmanrResult(correlation=0.3019911752422192, pvalue=2.3955078107700627e-09), 'nsamples': 375}, 'answers-students': {'pearson': (0.5150786709020904, 4.733710434567339e-52), 'spearman': SpearmanrResult(correlation=0.5178083696487777, pvalue=1.1178952382147335e-52), 'nsamples': 750}, 'belief': {'pearson': (0.26288206578254714, 2.4067802813450005e-07), 'spearman': SpearmanrResult(correlation=0.24898279635403195, pvalue=1.0472931193318627e-06), 'nsamples': 375}, 'headlines': {'pearson': (0.5055449128699133, 6.611577899562092e-50), 'spearman': SpearmanrResult(correlation=0.5157074706231407, pvalue=3.398748553219199e-52), 'nsamples': 750}, 'images': {'pearson': (0.36538149066553477, 4.199329574056408e-25), 'spearman': SpearmanrResult(correlation=0.4008850467483224, pvalue=2.5245578298425306e-30), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.39158754661461886, 'wmean': 0.4179928509388291}, 'spearman': {'mean': 0.3970749717232984, 'wmean': 0.4274719682045916}}}, 'STS16': {'answer-answer': {'pearson': (0.4086198206192323, 1.2149814647767634e-11), 'spearman': SpearmanrResult(correlation=0.4092468246177356, pvalue=1.1226289825982294e-11), 'nsamples': 254}, 'headlines': {'pearson': (0.524340600982121, 5.466998757746972e-19), 'spearman': SpearmanrResult(correlation=0.5316123526426995, pvalue=1.448409539376548e-19), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5436362052235386, 4.349076438408815e-19), 'spearman': SpearmanrResult(correlation=0.5573288592195903, pvalue=3.605002109186869e-20), 'nsamples': 230}, 'postediting': {'pearson': (0.6289059569317096, 2.852975821363423e-28), 'spearman': SpearmanrResult(correlation=0.6434415021145622, pvalue=6.496518724770663e-30), 'nsamples': 244}, 'question-question': {'pearson': (0.2791130066729988, 4.267114442247988e-05), 'spearman': SpearmanrResult(correlation=0.24940309915024478, pvalue=0.00027087441318694207), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.47692311808592003, 'wmean': 0.48159716961993326}, 'spearman': {'mean': 0.47820652754896653, 'wmean': 0.48366844951079013}}}, 'MR': {'devacc': 72.11, 'acc': 71.21, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.6, 'acc': 72.4, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.53, 'acc': 85.05, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 92.82, 'acc': 91.93, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.75, 'acc': 79.24, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.78, 'acc': 38.82, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.02, 'acc': 81.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.41, 'acc': 67.25, 'f1': 72.69, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.0, 'acc': 74.28, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7634025153663506, 'pearson': 0.7619016921236156, 'spearman': 0.6893070618415091, 'mse': 0.4269440286055696, 'yhat': array([2.43781116, 3.84294612, 1.6272995 , ..., 3.36338293, 4.82073415,        4.68764003]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6803184129709453, 'pearson': 0.6454740131980484, 'spearman': 0.6373780322141608, 'mse': 1.686444214791793, 'yhat': array([2.91598362, 1.96605827, 2.17553173, ..., 4.00947015, 3.32911415,        3.29337776]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.87, 'acc': 62.16, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 94.17, 'acc': 94.45, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 25.06, 'acc': 25.4, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.92, 'acc': 31.48, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 55.98, 'acc': 56.1, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 66.09, 'acc': 65.66, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.33, 'acc': 83.87, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.92, 'acc': 78.67, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 76.92, 'acc': 77.92, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 54.28, 'acc': 55.31, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.44, 'acc': 57.64, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 22:00:24,088 : STS12 p=0.4017, STS12 s=0.4108, STS13 p=0.2717, STS13 s=0.2873, STS14 p=0.3617, STS14 s=0.3662, STS15 p=0.4180, STS15 s=0.4275, STS 16 p=0.4816, STS16 s=0.4837, STS B p=0.6455, STS B s=0.6374, STS B m=1.6864, SICK-R p=0.7619, SICK-R s=0.6893, SICK-P m=0.4269
2019-03-12 22:00:24,088 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 22:00:24,088 : 0.4017,0.4108,0.2717,0.2873,0.3617,0.3662,0.4180,0.4275,0.4816,0.4837,0.6455,0.6374,1.6864,0.7619,0.6893,0.4269
2019-03-12 22:00:24,088 : MR=71.21, CR=72.40, SUBJ=91.93, MPQA=85.05, SST-B=79.24, SST-F=38.82, TREC=81.60, SICK-E=74.28, SNLI=62.16, MRPC=67.25, MRPC f=72.69
2019-03-12 22:00:24,088 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 22:00:24,088 : 71.21,72.40,91.93,85.05,79.24,38.82,81.60,74.28,62.16,67.25,72.69
2019-03-12 22:00:24,088 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 22:00:24,088 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 22:00:24,088 : na,na,na,na,na,na,na,na,na,na
2019-03-12 22:00:24,088 : SentLen=94.45, WC=25.40, TreeDepth=31.48, TopConst=56.10, BShift=65.66, Tense=83.87, SubjNum=78.67, ObjNum=77.92, SOMO=55.31, CoordInv=57.64, average=62.65
2019-03-12 22:00:24,088 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 22:00:24,088 : 94.45,25.40,31.48,56.10,65.66,83.87,78.67,77.92,55.31,57.64,62.65
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,088 : layer 6
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,088 : ********************************************************************************
2019-03-12 22:00:24,184 : ***** Transfer task : STS12 *****


2019-03-12 22:00:24,198 : loading BERT model bert-large-uncased
2019-03-12 22:00:24,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:00:24,217 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:00:24,218 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphflvl97s
2019-03-12 22:00:31,704 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:00:40,913 : MSRpar : pearson = 0.3272, spearman = 0.3414
2019-03-12 22:00:42,559 : MSRvid : pearson = 0.4731, spearman = 0.4827
2019-03-12 22:00:43,975 : SMTeuroparl : pearson = 0.4591, spearman = 0.5227
2019-03-12 22:00:46,686 : surprise.OnWN : pearson = 0.5739, spearman = 0.5701
2019-03-12 22:00:48,121 : surprise.SMTnews : pearson = 0.6236, spearman = 0.5353
2019-03-12 22:00:48,121 : ALL (weighted average) : Pearson = 0.4795,             Spearman = 0.4823
2019-03-12 22:00:48,121 : ALL (average) : Pearson = 0.4914,             Spearman = 0.4904

2019-03-12 22:00:48,122 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 22:00:48,140 : loading BERT model bert-large-uncased
2019-03-12 22:00:48,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:00:48,163 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:00:48,163 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppjsa4_jb
2019-03-12 22:00:55,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:01:02,213 : FNWN : pearson = 0.3089, spearman = 0.3164
2019-03-12 22:01:04,112 : headlines : pearson = 0.5139, spearman = 0.5013
2019-03-12 22:01:05,587 : OnWN : pearson = 0.3039, spearman = 0.3532
2019-03-12 22:01:05,587 : ALL (weighted average) : Pearson = 0.4096,             Spearman = 0.4226
2019-03-12 22:01:05,587 : ALL (average) : Pearson = 0.3756,             Spearman = 0.3903

2019-03-12 22:01:05,587 : ***** Transfer task : STS14 *****


2019-03-12 22:01:05,604 : loading BERT model bert-large-uncased
2019-03-12 22:01:05,605 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:01:05,622 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:01:05,623 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjtbt6zv9
2019-03-12 22:01:13,099 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:01:19,734 : deft-forum : pearson = 0.2390, spearman = 0.2592
2019-03-12 22:01:21,378 : deft-news : pearson = 0.5035, spearman = 0.4737
2019-03-12 22:01:23,552 : headlines : pearson = 0.4597, spearman = 0.4208
2019-03-12 22:01:25,631 : images : pearson = 0.5642, spearman = 0.5554
2019-03-12 22:01:27,767 : OnWN : pearson = 0.4276, spearman = 0.4796
2019-03-12 22:01:30,634 : tweet-news : pearson = 0.6113, spearman = 0.5821
2019-03-12 22:01:30,634 : ALL (weighted average) : Pearson = 0.4815,             Spearman = 0.4766
2019-03-12 22:01:30,635 : ALL (average) : Pearson = 0.4676,             Spearman = 0.4618

2019-03-12 22:01:30,635 : ***** Transfer task : STS15 *****


2019-03-12 22:01:30,684 : loading BERT model bert-large-uncased
2019-03-12 22:01:30,684 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:01:30,702 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:01:30,702 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptp3p94ph
2019-03-12 22:01:38,120 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:01:45,217 : answers-forums : pearson = 0.4328, spearman = 0.4278
2019-03-12 22:01:47,307 : answers-students : pearson = 0.6006, spearman = 0.6122
2019-03-12 22:01:49,364 : belief : pearson = 0.4250, spearman = 0.4301
2019-03-12 22:01:51,620 : headlines : pearson = 0.5783, spearman = 0.5805
2019-03-12 22:01:53,760 : images : pearson = 0.4973, spearman = 0.5257
2019-03-12 22:01:53,760 : ALL (weighted average) : Pearson = 0.5263,             Spearman = 0.5368
2019-03-12 22:01:53,760 : ALL (average) : Pearson = 0.5068,             Spearman = 0.5153

2019-03-12 22:01:53,760 : ***** Transfer task : STS16 *****


2019-03-12 22:01:53,831 : loading BERT model bert-large-uncased
2019-03-12 22:01:53,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:01:53,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:01:53,849 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjyh4bl1z
2019-03-12 22:02:01,357 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:02:07,536 : answer-answer : pearson = 0.4869, spearman = 0.5099
2019-03-12 22:02:08,199 : headlines : pearson = 0.6068, spearman = 0.6134
2019-03-12 22:02:09,082 : plagiarism : pearson = 0.6781, spearman = 0.7053
2019-03-12 22:02:10,581 : postediting : pearson = 0.7218, spearman = 0.7387
2019-03-12 22:02:11,188 : question-question : pearson = 0.3977, spearman = 0.3758
2019-03-12 22:02:11,188 : ALL (weighted average) : Pearson = 0.5818,             Spearman = 0.5930
2019-03-12 22:02:11,188 : ALL (average) : Pearson = 0.5782,             Spearman = 0.5886

2019-03-12 22:02:11,188 : ***** Transfer task : MR *****


2019-03-12 22:02:11,208 : loading BERT model bert-large-uncased
2019-03-12 22:02:11,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:02:11,227 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:02:11,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf_sh0y4x
2019-03-12 22:02:18,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:02:23,958 : Generating sentence embeddings
2019-03-12 22:02:55,605 : Generated sentence embeddings
2019-03-12 22:02:55,606 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 22:03:05,846 : Best param found at split 1: l2reg = 0.0001                 with score 73.83
2019-03-12 22:03:16,459 : Best param found at split 2: l2reg = 1e-05                 with score 73.29
2019-03-12 22:03:27,469 : Best param found at split 3: l2reg = 1e-05                 with score 73.76
2019-03-12 22:03:38,229 : Best param found at split 4: l2reg = 1e-05                 with score 73.81
2019-03-12 22:03:49,002 : Best param found at split 5: l2reg = 0.001                 with score 73.48
2019-03-12 22:03:49,525 : Dev acc : 73.63 Test acc : 73.28

2019-03-12 22:03:49,526 : ***** Transfer task : CR *****


2019-03-12 22:03:49,534 : loading BERT model bert-large-uncased
2019-03-12 22:03:49,534 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:03:49,554 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:03:49,554 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzjaa8dp_
2019-03-12 22:03:57,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:04:02,214 : Generating sentence embeddings
2019-03-12 22:04:10,556 : Generated sentence embeddings
2019-03-12 22:04:10,556 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 22:04:13,102 : Best param found at split 1: l2reg = 1e-05                 with score 77.61
2019-03-12 22:04:16,327 : Best param found at split 2: l2reg = 1e-05                 with score 77.48
2019-03-12 22:04:20,098 : Best param found at split 3: l2reg = 1e-05                 with score 78.31
2019-03-12 22:04:23,932 : Best param found at split 4: l2reg = 1e-05                 with score 77.66
2019-03-12 22:04:27,997 : Best param found at split 5: l2reg = 1e-05                 with score 78.19
2019-03-12 22:04:28,219 : Dev acc : 77.85 Test acc : 75.97

2019-03-12 22:04:28,219 : ***** Transfer task : MPQA *****


2019-03-12 22:04:28,226 : loading BERT model bert-large-uncased
2019-03-12 22:04:28,226 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:04:28,277 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:04:28,277 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph1kdm0ry
2019-03-12 22:04:35,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:04:40,973 : Generating sentence embeddings
2019-03-12 22:04:48,552 : Generated sentence embeddings
2019-03-12 22:04:48,553 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 22:04:58,383 : Best param found at split 1: l2reg = 1e-05                 with score 84.29
2019-03-12 22:05:09,627 : Best param found at split 2: l2reg = 0.001                 with score 84.9
2019-03-12 22:05:21,058 : Best param found at split 3: l2reg = 0.0001                 with score 85.85
2019-03-12 22:05:31,857 : Best param found at split 4: l2reg = 0.0001                 with score 86.13
2019-03-12 22:05:41,951 : Best param found at split 5: l2reg = 0.0001                 with score 85.06
2019-03-12 22:05:42,551 : Dev acc : 85.25 Test acc : 85.61

2019-03-12 22:05:42,552 : ***** Transfer task : SUBJ *****


2019-03-12 22:05:42,567 : loading BERT model bert-large-uncased
2019-03-12 22:05:42,567 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:05:42,587 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:05:42,587 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn24t4xrs
2019-03-12 22:05:50,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:05:55,405 : Generating sentence embeddings
2019-03-12 22:06:26,382 : Generated sentence embeddings
2019-03-12 22:06:26,383 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 22:06:36,343 : Best param found at split 1: l2reg = 0.0001                 with score 93.19
2019-03-12 22:06:47,048 : Best param found at split 2: l2reg = 1e-05                 with score 93.35
2019-03-12 22:06:57,053 : Best param found at split 3: l2reg = 1e-05                 with score 93.0
2019-03-12 22:07:06,243 : Best param found at split 4: l2reg = 0.0001                 with score 93.46
2019-03-12 22:07:17,158 : Best param found at split 5: l2reg = 1e-05                 with score 93.14
2019-03-12 22:07:17,891 : Dev acc : 93.23 Test acc : 93.23

2019-03-12 22:07:17,892 : ***** Transfer task : SST Binary classification *****


2019-03-12 22:07:17,984 : loading BERT model bert-large-uncased
2019-03-12 22:07:17,984 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:07:18,057 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:07:18,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8wjfltp7
2019-03-12 22:07:25,482 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:07:30,719 : Computing embedding for train
2019-03-12 22:09:11,432 : Computed train embeddings
2019-03-12 22:09:11,433 : Computing embedding for dev
2019-03-12 22:09:13,633 : Computed dev embeddings
2019-03-12 22:09:13,633 : Computing embedding for test
2019-03-12 22:09:18,261 : Computed test embeddings
2019-03-12 22:09:18,261 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:09:36,264 : [('reg:1e-05', 78.1), ('reg:0.0001', 78.44), ('reg:0.001', 78.1), ('reg:0.01', 76.49)]
2019-03-12 22:09:36,264 : Validation : best param found is reg = 0.0001 with score             78.44
2019-03-12 22:09:36,264 : Evaluating...
2019-03-12 22:09:40,538 : 
Dev acc : 78.44 Test acc : 80.12 for             SST Binary classification

2019-03-12 22:09:40,539 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 22:09:40,588 : loading BERT model bert-large-uncased
2019-03-12 22:09:40,588 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:09:40,610 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:09:40,610 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplhkzb4ph
2019-03-12 22:09:48,009 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:09:53,277 : Computing embedding for train
2019-03-12 22:10:15,294 : Computed train embeddings
2019-03-12 22:10:15,295 : Computing embedding for dev
2019-03-12 22:10:18,177 : Computed dev embeddings
2019-03-12 22:10:18,177 : Computing embedding for test
2019-03-12 22:10:23,859 : Computed test embeddings
2019-03-12 22:10:23,860 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:10:26,595 : [('reg:1e-05', 37.87), ('reg:0.0001', 37.97), ('reg:0.001', 38.24), ('reg:0.01', 37.24)]
2019-03-12 22:10:26,596 : Validation : best param found is reg = 0.001 with score             38.24
2019-03-12 22:10:26,596 : Evaluating...
2019-03-12 22:10:27,323 : 
Dev acc : 38.24 Test acc : 42.31 for             SST Fine-Grained classification

2019-03-12 22:10:27,323 : ***** Transfer task : TREC *****


2019-03-12 22:10:27,337 : loading BERT model bert-large-uncased
2019-03-12 22:10:27,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:10:27,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:10:27,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzyo4oswp
2019-03-12 22:10:34,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:10:47,636 : Computed train embeddings
2019-03-12 22:10:48,227 : Computed test embeddings
2019-03-12 22:10:48,227 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 22:10:55,154 : [('reg:1e-05', 70.1), ('reg:0.0001', 69.48), ('reg:0.001', 65.83), ('reg:0.01', 51.32)]
2019-03-12 22:10:55,154 : Cross-validation : best param found is reg = 1e-05             with score 70.1
2019-03-12 22:10:55,154 : Evaluating...
2019-03-12 22:10:55,659 : 
Dev acc : 70.1 Test acc : 85.6             for TREC

2019-03-12 22:10:55,660 : ***** Transfer task : MRPC *****


2019-03-12 22:10:55,682 : loading BERT model bert-large-uncased
2019-03-12 22:10:55,682 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:10:55,703 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:10:55,703 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb88rxobc
2019-03-12 22:11:03,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:11:08,410 : Computing embedding for train
2019-03-12 22:11:30,811 : Computed train embeddings
2019-03-12 22:11:30,811 : Computing embedding for test
2019-03-12 22:11:40,634 : Computed test embeddings
2019-03-12 22:11:40,656 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 22:11:46,021 : [('reg:1e-05', 72.11), ('reg:0.0001', 71.96), ('reg:0.001', 70.39), ('reg:0.01', 68.57)]
2019-03-12 22:11:46,021 : Cross-validation : best param found is reg = 1e-05             with score 72.11
2019-03-12 22:11:46,022 : Evaluating...
2019-03-12 22:11:46,276 : Dev acc : 72.11 Test acc 69.74; Test F1 76.25 for MRPC.

2019-03-12 22:11:46,277 : ***** Transfer task : SICK-Entailment*****


2019-03-12 22:11:46,337 : loading BERT model bert-large-uncased
2019-03-12 22:11:46,337 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:11:46,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:11:46,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpokdibco8
2019-03-12 22:11:53,774 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:11:58,989 : Computing embedding for train
2019-03-12 22:12:10,339 : Computed train embeddings
2019-03-12 22:12:10,339 : Computing embedding for dev
2019-03-12 22:12:11,893 : Computed dev embeddings
2019-03-12 22:12:11,893 : Computing embedding for test
2019-03-12 22:12:24,107 : Computed test embeddings
2019-03-12 22:12:24,143 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:12:25,279 : [('reg:1e-05', 75.0), ('reg:0.0001', 75.2), ('reg:0.001', 71.6), ('reg:0.01', 66.8)]
2019-03-12 22:12:25,279 : Validation : best param found is reg = 0.0001 with score             75.2
2019-03-12 22:12:25,279 : Evaluating...
2019-03-12 22:12:25,532 : 
Dev acc : 75.2 Test acc : 74.02 for                        SICK entailment

2019-03-12 22:12:25,532 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 22:12:25,559 : loading BERT model bert-large-uncased
2019-03-12 22:12:25,559 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:12:25,615 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:12:25,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp66mgsrqk
2019-03-12 22:12:33,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:12:38,318 : Computing embedding for train
2019-03-12 22:12:49,721 : Computed train embeddings
2019-03-12 22:12:49,722 : Computing embedding for dev
2019-03-12 22:12:51,283 : Computed dev embeddings
2019-03-12 22:12:51,283 : Computing embedding for test
2019-03-12 22:13:03,538 : Computed test embeddings
2019-03-12 22:13:24,179 : Dev : Pearson 0.7378986212396934
2019-03-12 22:13:24,180 : Test : Pearson 0.7550968566697335 Spearman 0.6864505288077006 MSE 0.4377599200450264                        for SICK Relatedness

2019-03-12 22:13:24,180 : 

***** Transfer task : STSBenchmark*****


2019-03-12 22:13:24,221 : loading BERT model bert-large-uncased
2019-03-12 22:13:24,221 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:13:24,249 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:13:24,250 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe913d2wl
2019-03-12 22:13:31,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:13:36,885 : Computing embedding for train
2019-03-12 22:13:55,594 : Computed train embeddings
2019-03-12 22:13:55,594 : Computing embedding for dev
2019-03-12 22:14:01,286 : Computed dev embeddings
2019-03-12 22:14:01,286 : Computing embedding for test
2019-03-12 22:14:05,943 : Computed test embeddings
2019-03-12 22:14:23,920 : Dev : Pearson 0.6952361550551612
2019-03-12 22:14:23,920 : Test : Pearson 0.6613818595823496 Spearman 0.6509284571533352 MSE 1.6198846253216497                        for SICK Relatedness

2019-03-12 22:14:23,920 : ***** Transfer task : SNLI Entailment*****


2019-03-12 22:14:28,954 : loading BERT model bert-large-uncased
2019-03-12 22:14:28,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:14:29,039 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:14:29,039 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbf2nto_w
2019-03-12 22:14:36,401 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:14:42,076 : PROGRESS (encoding): 0.00%
2019-03-12 22:17:32,643 : PROGRESS (encoding): 14.56%
2019-03-12 22:20:45,728 : PROGRESS (encoding): 29.12%
2019-03-12 22:23:57,661 : PROGRESS (encoding): 43.69%
2019-03-12 22:27:21,256 : PROGRESS (encoding): 58.25%
2019-03-12 22:31:08,834 : PROGRESS (encoding): 72.81%
2019-03-12 22:34:54,187 : PROGRESS (encoding): 87.37%
2019-03-12 22:38:57,991 : PROGRESS (encoding): 0.00%
2019-03-12 22:39:28,593 : PROGRESS (encoding): 0.00%
2019-03-12 22:39:58,037 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:40:33,431 : [('reg:1e-09', 65.15)]
2019-03-12 22:40:33,431 : Validation : best param found is reg = 1e-09 with score             65.15
2019-03-12 22:40:33,431 : Evaluating...
2019-03-12 22:41:11,013 : Dev acc : 65.15 Test acc : 65.91 for SNLI

2019-03-12 22:41:11,013 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-12 22:41:11,220 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-12 22:41:12,282 : loading BERT model bert-large-uncased
2019-03-12 22:41:12,282 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:41:12,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:41:12,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplcehkfry
2019-03-12 22:41:19,710 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:41:24,903 : Computing embeddings for train/dev/test
2019-03-12 22:44:59,124 : Computed embeddings
2019-03-12 22:44:59,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:45:21,229 : [('reg:1e-05', 93.72), ('reg:0.0001', 88.7), ('reg:0.001', 86.54), ('reg:0.01', 83.76)]
2019-03-12 22:45:21,229 : Validation : best param found is reg = 1e-05 with score             93.72
2019-03-12 22:45:21,229 : Evaluating...
2019-03-12 22:45:31,930 : 
Dev acc : 93.7 Test acc : 93.5 for LENGTH classification

2019-03-12 22:45:31,931 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-12 22:45:32,182 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-12 22:45:32,228 : loading BERT model bert-large-uncased
2019-03-12 22:45:32,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:45:32,257 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:45:32,257 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpca9sdwvk
2019-03-12 22:45:39,711 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:45:44,850 : Computing embeddings for train/dev/test
2019-03-12 22:49:02,919 : Computed embeddings
2019-03-12 22:49:02,919 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:49:31,439 : [('reg:1e-05', 33.38), ('reg:0.0001', 2.96), ('reg:0.001', 0.42), ('reg:0.01', 0.28)]
2019-03-12 22:49:31,439 : Validation : best param found is reg = 1e-05 with score             33.38
2019-03-12 22:49:31,439 : Evaluating...
2019-03-12 22:49:39,464 : 
Dev acc : 33.4 Test acc : 34.5 for WORDCONTENT classification

2019-03-12 22:49:39,465 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-12 22:49:40,001 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-12 22:49:40,066 : loading BERT model bert-large-uncased
2019-03-12 22:49:40,067 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:49:40,091 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:49:40,091 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps89ddfl8
2019-03-12 22:49:47,559 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:49:52,786 : Computing embeddings for train/dev/test
2019-03-12 22:52:58,946 : Computed embeddings
2019-03-12 22:52:58,947 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:53:21,713 : [('reg:1e-05', 31.48), ('reg:0.0001', 30.57), ('reg:0.001', 26.59), ('reg:0.01', 21.71)]
2019-03-12 22:53:21,713 : Validation : best param found is reg = 1e-05 with score             31.48
2019-03-12 22:53:21,713 : Evaluating...
2019-03-12 22:53:28,097 : 
Dev acc : 31.5 Test acc : 31.8 for DEPTH classification

2019-03-12 22:53:28,098 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-12 22:53:28,467 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-12 22:53:28,529 : loading BERT model bert-large-uncased
2019-03-12 22:53:28,529 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:53:28,637 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:53:28,637 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdj8n8fr5
2019-03-12 22:53:36,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:53:41,316 : Computing embeddings for train/dev/test
2019-03-12 22:56:33,379 : Computed embeddings
2019-03-12 22:56:33,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 22:57:03,698 : [('reg:1e-05', 57.92), ('reg:0.0001', 50.95), ('reg:0.001', 35.66), ('reg:0.01', 18.7)]
2019-03-12 22:57:03,698 : Validation : best param found is reg = 1e-05 with score             57.92
2019-03-12 22:57:03,698 : Evaluating...
2019-03-12 22:57:12,578 : 
Dev acc : 57.9 Test acc : 57.8 for TOPCONSTITUENTS classification

2019-03-12 22:57:12,579 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-12 22:57:12,955 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-12 22:57:13,022 : loading BERT model bert-large-uncased
2019-03-12 22:57:13,022 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 22:57:13,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 22:57:13,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp992e8de7
2019-03-12 22:57:20,497 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 22:57:25,750 : Computing embeddings for train/dev/test
2019-03-12 23:00:32,869 : Computed embeddings
2019-03-12 23:00:32,869 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:01:05,971 : [('reg:1e-05', 70.8), ('reg:0.0001', 70.37), ('reg:0.001', 67.24), ('reg:0.01', 57.2)]
2019-03-12 23:01:05,971 : Validation : best param found is reg = 1e-05 with score             70.8
2019-03-12 23:01:05,971 : Evaluating...
2019-03-12 23:01:12,942 : 
Dev acc : 70.8 Test acc : 70.3 for BIGRAMSHIFT classification

2019-03-12 23:01:12,943 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-12 23:01:13,330 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-12 23:01:13,395 : loading BERT model bert-large-uncased
2019-03-12 23:01:13,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:01:13,425 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:01:13,425 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqof4vvq2
2019-03-12 23:01:20,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:01:26,142 : Computing embeddings for train/dev/test
2019-03-12 23:04:28,390 : Computed embeddings
2019-03-12 23:04:28,390 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:05:02,043 : [('reg:1e-05', 86.68), ('reg:0.0001', 86.48), ('reg:0.001', 86.72), ('reg:0.01', 83.83)]
2019-03-12 23:05:02,043 : Validation : best param found is reg = 0.001 with score             86.72
2019-03-12 23:05:02,043 : Evaluating...
2019-03-12 23:05:08,047 : 
Dev acc : 86.7 Test acc : 84.7 for TENSE classification

2019-03-12 23:05:08,048 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-12 23:05:08,457 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-12 23:05:08,520 : loading BERT model bert-large-uncased
2019-03-12 23:05:08,521 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:05:08,636 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:05:08,636 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5wdkrvep
2019-03-12 23:05:16,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:05:21,276 : Computing embeddings for train/dev/test
2019-03-12 23:08:33,996 : Computed embeddings
2019-03-12 23:08:33,997 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:09:05,920 : [('reg:1e-05', 80.37), ('reg:0.0001', 78.1), ('reg:0.001', 76.66), ('reg:0.01', 72.2)]
2019-03-12 23:09:05,920 : Validation : best param found is reg = 1e-05 with score             80.37
2019-03-12 23:09:05,920 : Evaluating...
2019-03-12 23:09:16,466 : 
Dev acc : 80.4 Test acc : 78.6 for SUBJNUMBER classification

2019-03-12 23:09:16,467 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-12 23:09:16,864 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-12 23:09:16,931 : loading BERT model bert-large-uncased
2019-03-12 23:09:16,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:09:17,045 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:09:17,045 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjtwt2sjk
2019-03-12 23:09:24,478 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:09:30,102 : Computing embeddings for train/dev/test
2019-03-12 23:12:39,723 : Computed embeddings
2019-03-12 23:12:39,723 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:13:07,210 : [('reg:1e-05', 77.59), ('reg:0.0001', 77.57), ('reg:0.001', 75.84), ('reg:0.01', 71.37)]
2019-03-12 23:13:07,210 : Validation : best param found is reg = 1e-05 with score             77.59
2019-03-12 23:13:07,211 : Evaluating...
2019-03-12 23:13:13,199 : 
Dev acc : 77.6 Test acc : 78.7 for OBJNUMBER classification

2019-03-12 23:13:13,200 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-12 23:13:13,758 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-12 23:13:13,826 : loading BERT model bert-large-uncased
2019-03-12 23:13:13,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:13:13,853 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:13:13,854 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd4ra07es
2019-03-12 23:13:21,319 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:13:26,649 : Computing embeddings for train/dev/test
2019-03-12 23:17:05,926 : Computed embeddings
2019-03-12 23:17:05,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:17:33,340 : [('reg:1e-05', 56.5), ('reg:0.0001', 56.32), ('reg:0.001', 56.26), ('reg:0.01', 54.75)]
2019-03-12 23:17:33,340 : Validation : best param found is reg = 1e-05 with score             56.5
2019-03-12 23:17:33,340 : Evaluating...
2019-03-12 23:17:37,979 : 
Dev acc : 56.5 Test acc : 57.1 for ODDMANOUT classification

2019-03-12 23:17:37,981 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-12 23:17:38,365 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-12 23:17:38,448 : loading BERT model bert-large-uncased
2019-03-12 23:17:38,448 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:17:38,479 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:17:38,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvtxwuozg
2019-03-12 23:17:45,981 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:17:51,252 : Computing embeddings for train/dev/test
2019-03-12 23:21:27,069 : Computed embeddings
2019-03-12 23:21:27,069 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:21:55,633 : [('reg:1e-05', 57.54), ('reg:0.0001', 57.36), ('reg:0.001', 56.18), ('reg:0.01', 52.0)]
2019-03-12 23:21:55,633 : Validation : best param found is reg = 1e-05 with score             57.54
2019-03-12 23:21:55,634 : Evaluating...
2019-03-12 23:22:02,740 : 
Dev acc : 57.5 Test acc : 57.8 for COORDINATIONINVERSION classification

2019-03-12 23:22:02,742 : total results: {'STS12': {'MSRpar': {'pearson': (0.32721203980083385, 3.559921910362672e-20), 'spearman': SpearmanrResult(correlation=0.34138911683752643, pvalue=6.309679082941258e-22), 'nsamples': 750}, 'MSRvid': {'pearson': (0.47306894742184197, 4.357950435337411e-43), 'spearman': SpearmanrResult(correlation=0.4826678946216955, pvalue=5.005983068862275e-45), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.459065814922897, 2.6363768592736307e-25), 'spearman': SpearmanrResult(correlation=0.5226597716799761, pvalue=1.5417223785384953e-33), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5738860352600768, 6.538702786611702e-67), 'spearman': SpearmanrResult(correlation=0.5701115273716407, pvalue=7.257162594238214e-66), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6236294002235607, 2.2233948686531098e-44), 'spearman': SpearmanrResult(correlation=0.5352512480959967, pvalue=5.895021475169522e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.49137244752584214, 'wmean': 0.4794609416347731}, 'spearman': {'mean': 0.49041591172136717, 'wmean': 0.48233349012694937}}}, 'STS13': {'FNWN': {'pearson': (0.3089094687290143, 1.5261431134441236e-05), 'spearman': SpearmanrResult(correlation=0.31640112467180037, pvalue=9.189066263207982e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.5139466955035218, 8.579554010102104e-52), 'spearman': SpearmanrResult(correlation=0.5012844652641778, pvalue=5.717327971071183e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.3039132126140302, 1.8811121021427657e-13), 'spearman': SpearmanrResult(correlation=0.35320661755223987, pvalue=6.3093592972108846e-18), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.37558979228218875, 'wmean': 0.40955948232926404}, 'spearman': {'mean': 0.39029740249607264, 'wmean': 0.4226080493052734}}}, 'STS14': {'deft-forum': {'pearson': (0.23904915156713083, 2.874763894902056e-07), 'spearman': SpearmanrResult(correlation=0.2592172826118904, pvalue=2.4165527066036853e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.5034839200965116, 1.0967351674897468e-20), 'spearman': SpearmanrResult(correlation=0.4736763451391866, pvalue=3.4890910568843816e-18), 'nsamples': 300}, 'headlines': {'pearson': (0.45968690797711803, 1.7490130978050654e-40), 'spearman': SpearmanrResult(correlation=0.42081505744845005, pvalue=1.5163610397846345e-33), 'nsamples': 750}, 'images': {'pearson': (0.5642087202048477, 2.941468501620795e-64), 'spearman': SpearmanrResult(correlation=0.555353100207339, pvalue=6.618234506496644e-62), 'nsamples': 750}, 'OnWN': {'pearson': (0.4275806230292487, 1.0891167663184787e-34), 'spearman': SpearmanrResult(correlation=0.4795548657727246, pvalue=2.164423217194717e-44), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6113326293154823, 4.64589861025926e-78), 'spearman': SpearmanrResult(correlation=0.5821212538837625, pvalue=3.070336413867113e-69), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.46755699203172324, 'wmean': 0.48152638790111596}, 'spearman': {'mean': 0.4617896508438921, 'wmean': 0.476569036987017}}}, 'STS15': {'answers-forums': {'pearson': (0.4328209334616487, 1.4900743146377618e-18), 'spearman': SpearmanrResult(correlation=0.42781651635265067, pvalue=4.038794731935463e-18), 'nsamples': 375}, 'answers-students': {'pearson': (0.6005522374197412, 1.0671737135104544e-74), 'spearman': SpearmanrResult(correlation=0.6122234165224263, pvalue=2.4183467105016347e-78), 'nsamples': 750}, 'belief': {'pearson': (0.42497466938715744, 7.062849680546836e-18), 'spearman': SpearmanrResult(correlation=0.4300824118702456, pvalue=2.5767295209193056e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.5783339829461321, 3.6827234347418738e-68), 'spearman': SpearmanrResult(correlation=0.5804979864604631, pvalue=8.940918691163077e-69), 'nsamples': 750}, 'images': {'pearson': (0.4973239230303793, 4.1337934374488873e-48), 'spearman': SpearmanrResult(correlation=0.5256897631366393, pvalue=1.6082603233968427e-54), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5068011492490118, 'wmean': 0.5262769862051639}, 'spearman': {'mean': 0.515262018868485, 'wmean': 0.5368401575577443}}}, 'STS16': {'answer-answer': {'pearson': (0.4869073186480994, 1.586601678415385e-16), 'spearman': SpearmanrResult(correlation=0.5099293939562913, pvalue=3.2336162645583005e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.60681720058563, 1.9428194262293084e-26), 'spearman': SpearmanrResult(correlation=0.6133896522434569, pvalue=3.9686589739624285e-27), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6780570846620798, 2.5339472074562346e-32), 'spearman': SpearmanrResult(correlation=0.7052846165581463, pvalue=6.441812264975988e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.7218126145574326, 1.4745496996114068e-40), 'spearman': SpearmanrResult(correlation=0.738688871242381, pvalue=2.4132698863583477e-43), 'nsamples': 244}, 'question-question': {'pearson': (0.3976502269353647, 2.4921810388873656e-09), 'spearman': SpearmanrResult(correlation=0.37584446173853914, pvalue=2.0479789865448364e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5782488890777213, 'wmean': 0.5817506296258198}, 'spearman': {'mean': 0.588627399147763, 'wmean': 0.5929705972752007}}}, 'MR': {'devacc': 73.63, 'acc': 73.28, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.85, 'acc': 75.97, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.25, 'acc': 85.61, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.23, 'acc': 93.23, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.44, 'acc': 80.12, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.24, 'acc': 42.31, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.1, 'acc': 85.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.11, 'acc': 69.74, 'f1': 76.25, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 75.2, 'acc': 74.02, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7378986212396934, 'pearson': 0.7550968566697335, 'spearman': 0.6864505288077006, 'mse': 0.4377599200450264, 'yhat': array([2.19959245, 4.05547888, 2.73511759, ..., 3.3025335 , 4.63159167,        4.59873462]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6952361550551612, 'pearson': 0.6613818595823496, 'spearman': 0.6509284571533352, 'mse': 1.6198846253216497, 'yhat': array([2.67584509, 1.32185116, 2.32268413, ..., 4.09390965, 3.63956993,        3.58301695]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.15, 'acc': 65.91, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 93.72, 'acc': 93.52, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 33.38, 'acc': 34.46, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.48, 'acc': 31.84, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.92, 'acc': 57.78, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 70.8, 'acc': 70.28, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.72, 'acc': 84.72, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.37, 'acc': 78.64, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.59, 'acc': 78.72, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.5, 'acc': 57.08, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.54, 'acc': 57.78, 'ndev': 10002, 'ntest': 10002}}
2019-03-12 23:22:02,742 : STS12 p=0.4795, STS12 s=0.4823, STS13 p=0.4096, STS13 s=0.4226, STS14 p=0.4815, STS14 s=0.4766, STS15 p=0.5263, STS15 s=0.5368, STS 16 p=0.5818, STS16 s=0.5930, STS B p=0.6614, STS B s=0.6509, STS B m=1.6199, SICK-R p=0.7551, SICK-R s=0.6865, SICK-P m=0.4378
2019-03-12 23:22:02,742 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-12 23:22:02,742 : 0.4795,0.4823,0.4096,0.4226,0.4815,0.4766,0.5263,0.5368,0.5818,0.5930,0.6614,0.6509,1.6199,0.7551,0.6865,0.4378
2019-03-12 23:22:02,742 : MR=73.28, CR=75.97, SUBJ=93.23, MPQA=85.61, SST-B=80.12, SST-F=42.31, TREC=85.60, SICK-E=74.02, SNLI=65.91, MRPC=69.74, MRPC f=76.25
2019-03-12 23:22:02,742 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-12 23:22:02,742 : 73.28,75.97,93.23,85.61,80.12,42.31,85.60,74.02,65.91,69.74,76.25
2019-03-12 23:22:02,742 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-12 23:22:02,742 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-12 23:22:02,742 : na,na,na,na,na,na,na,na,na,na
2019-03-12 23:22:02,743 : SentLen=93.52, WC=34.46, TreeDepth=31.84, TopConst=57.78, BShift=70.28, Tense=84.72, SubjNum=78.64, ObjNum=78.72, SOMO=57.08, CoordInv=57.78, average=64.48
2019-03-12 23:22:02,743 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-12 23:22:02,743 : 93.52,34.46,31.84,57.78,70.28,84.72,78.64,78.72,57.08,57.78,64.48
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,743 : layer 7
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,743 : ********************************************************************************
2019-03-12 23:22:02,835 : ***** Transfer task : STS12 *****


2019-03-12 23:22:02,847 : loading BERT model bert-large-uncased
2019-03-12 23:22:02,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:22:02,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:22:02,865 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpckok9pjz
2019-03-12 23:22:10,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:22:19,608 : MSRpar : pearson = 0.3230, spearman = 0.3524
2019-03-12 23:22:21,257 : MSRvid : pearson = 0.4588, spearman = 0.4732
2019-03-12 23:22:22,674 : SMTeuroparl : pearson = 0.4787, spearman = 0.5508
2019-03-12 23:22:25,376 : surprise.OnWN : pearson = 0.5783, spearman = 0.5803
2019-03-12 23:22:26,807 : surprise.SMTnews : pearson = 0.6275, spearman = 0.5359
2019-03-12 23:22:26,807 : ALL (weighted average) : Pearson = 0.4795,             Spearman = 0.4894
2019-03-12 23:22:26,807 : ALL (average) : Pearson = 0.4933,             Spearman = 0.4985

2019-03-12 23:22:26,807 : ***** Transfer task : STS13 (-SMT) *****


2019-03-12 23:22:26,816 : loading BERT model bert-large-uncased
2019-03-12 23:22:26,816 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:22:26,833 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:22:26,833 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbrbna0qd
2019-03-12 23:22:34,215 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:22:41,191 : FNWN : pearson = 0.3051, spearman = 0.2943
2019-03-12 23:22:43,087 : headlines : pearson = 0.5548, spearman = 0.5396
2019-03-12 23:22:44,556 : OnWN : pearson = 0.2985, spearman = 0.3483
2019-03-12 23:22:44,557 : ALL (weighted average) : Pearson = 0.4275,             Spearman = 0.4372
2019-03-12 23:22:44,557 : ALL (average) : Pearson = 0.3862,             Spearman = 0.3941

2019-03-12 23:22:44,557 : ***** Transfer task : STS14 *****


2019-03-12 23:22:44,572 : loading BERT model bert-large-uncased
2019-03-12 23:22:44,572 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:22:44,589 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:22:44,589 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpepxnvnub
2019-03-12 23:22:51,984 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:22:58,492 : deft-forum : pearson = 0.2527, spearman = 0.2742
2019-03-12 23:23:00,137 : deft-news : pearson = 0.5419, spearman = 0.5379
2019-03-12 23:23:02,310 : headlines : pearson = 0.4891, spearman = 0.4442
2019-03-12 23:23:04,390 : images : pearson = 0.5523, spearman = 0.5452
2019-03-12 23:23:06,525 : OnWN : pearson = 0.4226, spearman = 0.4784
2019-03-12 23:23:09,395 : tweet-news : pearson = 0.6074, spearman = 0.5786
2019-03-12 23:23:09,396 : ALL (weighted average) : Pearson = 0.4880,             Spearman = 0.4852
2019-03-12 23:23:09,396 : ALL (average) : Pearson = 0.4777,             Spearman = 0.4764

2019-03-12 23:23:09,396 : ***** Transfer task : STS15 *****


2019-03-12 23:23:09,428 : loading BERT model bert-large-uncased
2019-03-12 23:23:09,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:23:09,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:23:09,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphwxu4hx0
2019-03-12 23:23:16,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:23:23,884 : answers-forums : pearson = 0.4418, spearman = 0.4271
2019-03-12 23:23:25,977 : answers-students : pearson = 0.5952, spearman = 0.6086
2019-03-12 23:23:28,035 : belief : pearson = 0.4520, spearman = 0.4678
2019-03-12 23:23:30,290 : headlines : pearson = 0.6109, spearman = 0.6094
2019-03-12 23:23:32,429 : images : pearson = 0.5339, spearman = 0.5652
2019-03-12 23:23:32,429 : ALL (weighted average) : Pearson = 0.5467,             Spearman = 0.5577
2019-03-12 23:23:32,429 : ALL (average) : Pearson = 0.5268,             Spearman = 0.5356

2019-03-12 23:23:32,429 : ***** Transfer task : STS16 *****


2019-03-12 23:23:32,499 : loading BERT model bert-large-uncased
2019-03-12 23:23:32,499 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:23:32,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:23:32,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqn5h0lo2
2019-03-12 23:23:39,979 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:23:46,091 : answer-answer : pearson = 0.4725, spearman = 0.4918
2019-03-12 23:23:46,755 : headlines : pearson = 0.6421, spearman = 0.6447
2019-03-12 23:23:47,640 : plagiarism : pearson = 0.6704, spearman = 0.6937
2019-03-12 23:23:49,143 : postediting : pearson = 0.7128, spearman = 0.7412
2019-03-12 23:23:49,751 : question-question : pearson = 0.3555, spearman = 0.3312
2019-03-12 23:23:49,752 : ALL (weighted average) : Pearson = 0.5753,             Spearman = 0.5861
2019-03-12 23:23:49,752 : ALL (average) : Pearson = 0.5707,             Spearman = 0.5805

2019-03-12 23:23:49,752 : ***** Transfer task : MR *****


2019-03-12 23:23:49,767 : loading BERT model bert-large-uncased
2019-03-12 23:23:49,767 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:23:49,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:23:49,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr1yonv41
2019-03-12 23:23:57,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:24:02,403 : Generating sentence embeddings
2019-03-12 23:24:34,012 : Generated sentence embeddings
2019-03-12 23:24:34,012 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:24:43,853 : Best param found at split 1: l2reg = 0.0001                 with score 73.44
2019-03-12 23:24:54,143 : Best param found at split 2: l2reg = 0.0001                 with score 72.46
2019-03-12 23:25:03,951 : Best param found at split 3: l2reg = 1e-05                 with score 73.96
2019-03-12 23:25:15,611 : Best param found at split 4: l2reg = 0.001                 with score 74.14
2019-03-12 23:25:26,233 : Best param found at split 5: l2reg = 0.0001                 with score 72.83
2019-03-12 23:25:26,844 : Dev acc : 73.37 Test acc : 73.17

2019-03-12 23:25:26,845 : ***** Transfer task : CR *****


2019-03-12 23:25:26,853 : loading BERT model bert-large-uncased
2019-03-12 23:25:26,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:25:26,873 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:25:26,873 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb3gg7als
2019-03-12 23:25:34,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:25:39,469 : Generating sentence embeddings
2019-03-12 23:25:47,786 : Generated sentence embeddings
2019-03-12 23:25:47,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:25:51,570 : Best param found at split 1: l2reg = 1e-05                 with score 77.18
2019-03-12 23:25:55,516 : Best param found at split 2: l2reg = 0.0001                 with score 77.77
2019-03-12 23:25:59,203 : Best param found at split 3: l2reg = 1e-05                 with score 77.32
2019-03-12 23:26:03,063 : Best param found at split 4: l2reg = 0.0001                 with score 77.39
2019-03-12 23:26:06,513 : Best param found at split 5: l2reg = 1e-05                 with score 77.36
2019-03-12 23:26:06,729 : Dev acc : 77.4 Test acc : 75.6

2019-03-12 23:26:06,730 : ***** Transfer task : MPQA *****


2019-03-12 23:26:06,736 : loading BERT model bert-large-uncased
2019-03-12 23:26:06,736 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:26:06,785 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:26:06,786 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfiazi5ne
2019-03-12 23:26:14,217 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:26:19,506 : Generating sentence embeddings
2019-03-12 23:26:27,085 : Generated sentence embeddings
2019-03-12 23:26:27,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:26:36,037 : Best param found at split 1: l2reg = 0.0001                 with score 83.86
2019-03-12 23:26:46,373 : Best param found at split 2: l2reg = 0.0001                 with score 83.81
2019-03-12 23:26:57,013 : Best param found at split 3: l2reg = 1e-05                 with score 85.57
2019-03-12 23:27:08,703 : Best param found at split 4: l2reg = 0.0001                 with score 86.03
2019-03-12 23:27:19,706 : Best param found at split 5: l2reg = 1e-05                 with score 84.92
2019-03-12 23:27:20,498 : Dev acc : 84.84 Test acc : 84.37

2019-03-12 23:27:20,499 : ***** Transfer task : SUBJ *****


2019-03-12 23:27:20,516 : loading BERT model bert-large-uncased
2019-03-12 23:27:20,516 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:27:20,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:27:20,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphw38r6wi
2019-03-12 23:27:27,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:27:33,169 : Generating sentence embeddings
2019-03-12 23:28:04,137 : Generated sentence embeddings
2019-03-12 23:28:04,137 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-12 23:28:15,619 : Best param found at split 1: l2reg = 1e-05                 with score 93.19
2019-03-12 23:28:26,611 : Best param found at split 2: l2reg = 1e-05                 with score 93.58
2019-03-12 23:28:37,145 : Best param found at split 3: l2reg = 1e-05                 with score 92.96
2019-03-12 23:28:47,840 : Best param found at split 4: l2reg = 1e-05                 with score 93.5
2019-03-12 23:28:58,626 : Best param found at split 5: l2reg = 0.0001                 with score 93.14
2019-03-12 23:28:59,278 : Dev acc : 93.27 Test acc : 93.18

2019-03-12 23:28:59,279 : ***** Transfer task : SST Binary classification *****


2019-03-12 23:28:59,371 : loading BERT model bert-large-uncased
2019-03-12 23:28:59,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:28:59,446 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:28:59,446 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpokvllzb8
2019-03-12 23:29:06,856 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:29:12,037 : Computing embedding for train
2019-03-12 23:30:52,617 : Computed train embeddings
2019-03-12 23:30:52,617 : Computing embedding for dev
2019-03-12 23:30:54,818 : Computed dev embeddings
2019-03-12 23:30:54,818 : Computing embedding for test
2019-03-12 23:30:59,444 : Computed test embeddings
2019-03-12 23:30:59,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:31:18,678 : [('reg:1e-05', 78.1), ('reg:0.0001', 78.1), ('reg:0.001', 77.98), ('reg:0.01', 76.49)]
2019-03-12 23:31:18,679 : Validation : best param found is reg = 1e-05 with score             78.1
2019-03-12 23:31:18,679 : Evaluating...
2019-03-12 23:31:23,746 : 
Dev acc : 78.1 Test acc : 79.19 for             SST Binary classification

2019-03-12 23:31:23,746 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-12 23:31:23,800 : loading BERT model bert-large-uncased
2019-03-12 23:31:23,800 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:31:23,821 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:31:23,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt_y85tkd
2019-03-12 23:31:31,229 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:31:36,503 : Computing embedding for train
2019-03-12 23:31:58,540 : Computed train embeddings
2019-03-12 23:31:58,541 : Computing embedding for dev
2019-03-12 23:32:01,424 : Computed dev embeddings
2019-03-12 23:32:01,424 : Computing embedding for test
2019-03-12 23:32:07,104 : Computed test embeddings
2019-03-12 23:32:07,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:32:09,787 : [('reg:1e-05', 38.69), ('reg:0.0001', 39.15), ('reg:0.001', 38.24), ('reg:0.01', 36.69)]
2019-03-12 23:32:09,787 : Validation : best param found is reg = 0.0001 with score             39.15
2019-03-12 23:32:09,787 : Evaluating...
2019-03-12 23:32:10,526 : 
Dev acc : 39.15 Test acc : 41.13 for             SST Fine-Grained classification

2019-03-12 23:32:10,527 : ***** Transfer task : TREC *****


2019-03-12 23:32:10,541 : loading BERT model bert-large-uncased
2019-03-12 23:32:10,541 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:32:10,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:32:10,560 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxjcta2_v
2019-03-12 23:32:17,958 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:32:30,757 : Computed train embeddings
2019-03-12 23:32:31,350 : Computed test embeddings
2019-03-12 23:32:31,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:32:38,691 : [('reg:1e-05', 71.79), ('reg:0.0001', 71.5), ('reg:0.001', 64.57), ('reg:0.01', 52.46)]
2019-03-12 23:32:38,691 : Cross-validation : best param found is reg = 1e-05             with score 71.79
2019-03-12 23:32:38,691 : Evaluating...
2019-03-12 23:32:39,141 : 
Dev acc : 71.79 Test acc : 84.6             for TREC

2019-03-12 23:32:39,142 : ***** Transfer task : MRPC *****


2019-03-12 23:32:39,163 : loading BERT model bert-large-uncased
2019-03-12 23:32:39,163 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:32:39,186 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:32:39,186 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw_m9rtcp
2019-03-12 23:32:46,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:32:51,709 : Computing embedding for train
2019-03-12 23:33:14,107 : Computed train embeddings
2019-03-12 23:33:14,107 : Computing embedding for test
2019-03-12 23:33:23,935 : Computed test embeddings
2019-03-12 23:33:23,955 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-12 23:33:28,476 : [('reg:1e-05', 71.03), ('reg:0.0001', 71.54), ('reg:0.001', 70.66), ('reg:0.01', 69.38)]
2019-03-12 23:33:28,476 : Cross-validation : best param found is reg = 0.0001             with score 71.54
2019-03-12 23:33:28,476 : Evaluating...
2019-03-12 23:33:28,742 : Dev acc : 71.54 Test acc 66.38; Test F1 71.2 for MRPC.

2019-03-12 23:33:28,742 : ***** Transfer task : SICK-Entailment*****


2019-03-12 23:33:28,805 : loading BERT model bert-large-uncased
2019-03-12 23:33:28,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:33:28,824 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:33:28,824 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphf5abafj
2019-03-12 23:33:36,295 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:33:41,443 : Computing embedding for train
2019-03-12 23:33:52,827 : Computed train embeddings
2019-03-12 23:33:52,827 : Computing embedding for dev
2019-03-12 23:33:54,382 : Computed dev embeddings
2019-03-12 23:33:54,383 : Computing embedding for test
2019-03-12 23:34:06,605 : Computed test embeddings
2019-03-12 23:34:06,642 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-12 23:34:08,149 : [('reg:1e-05', 74.8), ('reg:0.0001', 74.2), ('reg:0.001', 70.4), ('reg:0.01', 64.4)]
2019-03-12 23:34:08,149 : Validation : best param found is reg = 1e-05 with score             74.8
2019-03-12 23:34:08,149 : Evaluating...
2019-03-12 23:34:08,451 : 
Dev acc : 74.8 Test acc : 72.48 for                        SICK entailment

2019-03-12 23:34:08,452 : ***** Transfer task : SICK-Relatedness*****


2019-03-12 23:34:08,478 : loading BERT model bert-large-uncased
2019-03-12 23:34:08,478 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:34:08,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:34:08,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk6jf_7sw
2019-03-12 23:34:15,967 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:34:21,248 : Computing embedding for train
2019-03-12 23:34:32,638 : Computed train embeddings
2019-03-12 23:34:32,638 : Computing embedding for dev
2019-03-12 23:34:34,193 : Computed dev embeddings
2019-03-12 23:34:34,193 : Computing embedding for test
2019-03-12 23:34:46,411 : Computed test embeddings
2019-03-12 23:35:07,352 : Dev : Pearson 0.7668187717388535
2019-03-12 23:35:07,353 : Test : Pearson 0.7655073677075773 Spearman 0.6893253517375539 MSE 0.42144926874462446                        for SICK Relatedness

2019-03-12 23:35:07,353 : 

***** Transfer task : STSBenchmark*****


2019-03-12 23:35:07,392 : loading BERT model bert-large-uncased
2019-03-12 23:35:07,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:35:07,421 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:35:07,421 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplfuvftwm
2019-03-12 23:35:14,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:35:20,005 : Computing embedding for train
2019-03-12 23:35:38,713 : Computed train embeddings
2019-03-12 23:35:38,713 : Computing embedding for dev
2019-03-12 23:35:44,398 : Computed dev embeddings
2019-03-12 23:35:44,398 : Computing embedding for test
2019-03-12 23:35:49,038 : Computed test embeddings
2019-03-12 23:36:06,363 : Dev : Pearson 0.6996682438719831
2019-03-12 23:36:06,363 : Test : Pearson 0.6492805896720899 Spearman 0.6378013678850581 MSE 1.6321127637792752                        for SICK Relatedness

2019-03-12 23:36:06,363 : ***** Transfer task : SNLI Entailment*****


2019-03-12 23:36:11,389 : loading BERT model bert-large-uncased
2019-03-12 23:36:11,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-12 23:36:11,509 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-12 23:36:11,509 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx2h5ocit
2019-03-12 23:36:18,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-12 23:36:24,641 : PROGRESS (encoding): 0.00%
2019-03-12 23:39:14,433 : PROGRESS (encoding): 14.56%
2019-03-12 23:42:26,870 : PROGRESS (encoding): 29.12%
2019-03-12 23:45:37,764 : PROGRESS (encoding): 43.69%
2019-03-12 23:49:02,000 : PROGRESS (encoding): 58.25%
2019-03-12 23:52:48,723 : PROGRESS (encoding): 72.81%
2019-03-12 23:56:33,543 : PROGRESS (encoding): 87.37%
2019-03-13 00:00:37,542 : PROGRESS (encoding): 0.00%
2019-03-13 00:01:08,146 : PROGRESS (encoding): 0.00%
2019-03-13 00:01:37,663 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:02:14,830 : [('reg:1e-09', 64.6)]
2019-03-13 00:02:14,830 : Validation : best param found is reg = 1e-09 with score             64.6
2019-03-13 00:02:14,831 : Evaluating...
2019-03-13 00:02:51,279 : Dev acc : 64.6 Test acc : 64.94 for SNLI

2019-03-13 00:02:51,279 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 00:02:51,484 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 00:02:52,576 : loading BERT model bert-large-uncased
2019-03-13 00:02:52,576 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:02:52,603 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:02:52,604 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsda6hlt4
2019-03-13 00:03:00,097 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:03:05,244 : Computing embeddings for train/dev/test
2019-03-13 00:06:39,721 : Computed embeddings
2019-03-13 00:06:39,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:07:04,452 : [('reg:1e-05', 91.33), ('reg:0.0001', 91.26), ('reg:0.001', 86.49), ('reg:0.01', 83.75)]
2019-03-13 00:07:04,453 : Validation : best param found is reg = 1e-05 with score             91.33
2019-03-13 00:07:04,453 : Evaluating...
2019-03-13 00:07:11,938 : 
Dev acc : 91.3 Test acc : 91.5 for LENGTH classification

2019-03-13 00:07:11,938 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 00:07:12,315 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 00:07:12,360 : loading BERT model bert-large-uncased
2019-03-13 00:07:12,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:07:12,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:07:12,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnouozzto
2019-03-13 00:07:19,856 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:07:25,136 : Computing embeddings for train/dev/test
2019-03-13 00:10:43,021 : Computed embeddings
2019-03-13 00:10:43,022 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:11:12,641 : [('reg:1e-05', 29.61), ('reg:0.0001', 2.6), ('reg:0.001', 0.56), ('reg:0.01', 0.3)]
2019-03-13 00:11:12,641 : Validation : best param found is reg = 1e-05 with score             29.61
2019-03-13 00:11:12,641 : Evaluating...
2019-03-13 00:11:21,163 : 
Dev acc : 29.6 Test acc : 30.6 for WORDCONTENT classification

2019-03-13 00:11:21,165 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 00:11:21,543 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 00:11:21,610 : loading BERT model bert-large-uncased
2019-03-13 00:11:21,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:11:21,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:11:21,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm3_k02l6
2019-03-13 00:11:29,074 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:11:34,334 : Computing embeddings for train/dev/test
2019-03-13 00:14:40,652 : Computed embeddings
2019-03-13 00:14:40,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:15:02,853 : [('reg:1e-05', 30.8), ('reg:0.0001', 29.93), ('reg:0.001', 26.98), ('reg:0.01', 24.39)]
2019-03-13 00:15:02,853 : Validation : best param found is reg = 1e-05 with score             30.8
2019-03-13 00:15:02,853 : Evaluating...
2019-03-13 00:15:09,051 : 
Dev acc : 30.8 Test acc : 31.7 for DEPTH classification

2019-03-13 00:15:09,052 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 00:15:09,436 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 00:15:09,499 : loading BERT model bert-large-uncased
2019-03-13 00:15:09,499 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:15:09,608 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:15:09,608 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8eb_r5ar
2019-03-13 00:15:17,051 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:15:22,221 : Computing embeddings for train/dev/test
2019-03-13 00:18:14,346 : Computed embeddings
2019-03-13 00:18:14,346 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:18:44,927 : [('reg:1e-05', 56.31), ('reg:0.0001', 52.71), ('reg:0.001', 38.13), ('reg:0.01', 19.18)]
2019-03-13 00:18:44,928 : Validation : best param found is reg = 1e-05 with score             56.31
2019-03-13 00:18:44,928 : Evaluating...
2019-03-13 00:18:52,212 : 
Dev acc : 56.3 Test acc : 55.8 for TOPCONSTITUENTS classification

2019-03-13 00:18:52,213 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 00:18:52,557 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 00:18:52,623 : loading BERT model bert-large-uncased
2019-03-13 00:18:52,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:18:52,741 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:18:52,741 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcmt50b92
2019-03-13 00:19:00,170 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:19:05,361 : Computing embeddings for train/dev/test
2019-03-13 00:22:11,974 : Computed embeddings
2019-03-13 00:22:11,974 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:22:43,153 : [('reg:1e-05', 71.1), ('reg:0.0001', 71.59), ('reg:0.001', 70.72), ('reg:0.01', 65.44)]
2019-03-13 00:22:43,153 : Validation : best param found is reg = 0.0001 with score             71.59
2019-03-13 00:22:43,153 : Evaluating...
2019-03-13 00:22:50,933 : 
Dev acc : 71.6 Test acc : 71.0 for BIGRAMSHIFT classification

2019-03-13 00:22:50,934 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 00:22:51,490 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 00:22:51,555 : loading BERT model bert-large-uncased
2019-03-13 00:22:51,555 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:22:51,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:22:51,584 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa7hx1qtk
2019-03-13 00:22:59,048 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:23:04,313 : Computing embeddings for train/dev/test
2019-03-13 00:26:06,321 : Computed embeddings
2019-03-13 00:26:06,322 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:26:30,978 : [('reg:1e-05', 87.77), ('reg:0.0001', 88.1), ('reg:0.001', 87.65), ('reg:0.01', 85.76)]
2019-03-13 00:26:30,978 : Validation : best param found is reg = 0.0001 with score             88.1
2019-03-13 00:26:30,978 : Evaluating...
2019-03-13 00:26:38,538 : 
Dev acc : 88.1 Test acc : 86.0 for TENSE classification

2019-03-13 00:26:38,539 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 00:26:38,966 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 00:26:39,031 : loading BERT model bert-large-uncased
2019-03-13 00:26:39,032 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:26:39,057 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:26:39,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzx8ik9js
2019-03-13 00:26:46,575 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:26:51,946 : Computing embeddings for train/dev/test
2019-03-13 00:30:05,345 : Computed embeddings
2019-03-13 00:30:05,345 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:30:35,849 : [('reg:1e-05', 79.23), ('reg:0.0001', 78.56), ('reg:0.001', 76.69), ('reg:0.01', 74.04)]
2019-03-13 00:30:35,849 : Validation : best param found is reg = 1e-05 with score             79.23
2019-03-13 00:30:35,849 : Evaluating...
2019-03-13 00:30:43,286 : 
Dev acc : 79.2 Test acc : 78.7 for SUBJNUMBER classification

2019-03-13 00:30:43,287 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 00:30:43,691 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 00:30:43,758 : loading BERT model bert-large-uncased
2019-03-13 00:30:43,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:30:43,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:30:43,871 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18g93j1m
2019-03-13 00:30:51,313 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:30:56,579 : Computing embeddings for train/dev/test
2019-03-13 00:34:05,864 : Computed embeddings
2019-03-13 00:34:05,864 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:34:32,699 : [('reg:1e-05', 79.17), ('reg:0.0001', 79.2), ('reg:0.001', 77.43), ('reg:0.01', 72.44)]
2019-03-13 00:34:32,700 : Validation : best param found is reg = 0.0001 with score             79.2
2019-03-13 00:34:32,700 : Evaluating...
2019-03-13 00:34:40,197 : 
Dev acc : 79.2 Test acc : 79.4 for OBJNUMBER classification

2019-03-13 00:34:40,198 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 00:34:40,789 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 00:34:40,858 : loading BERT model bert-large-uncased
2019-03-13 00:34:40,858 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:34:40,885 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:34:40,885 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqx6bur0s
2019-03-13 00:34:48,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:34:53,581 : Computing embeddings for train/dev/test
2019-03-13 00:38:32,167 : Computed embeddings
2019-03-13 00:38:32,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:38:56,233 : [('reg:1e-05', 56.54), ('reg:0.0001', 56.83), ('reg:0.001', 57.18), ('reg:0.01', 55.77)]
2019-03-13 00:38:56,234 : Validation : best param found is reg = 0.001 with score             57.18
2019-03-13 00:38:56,234 : Evaluating...
2019-03-13 00:39:03,711 : 
Dev acc : 57.2 Test acc : 58.2 for ODDMANOUT classification

2019-03-13 00:39:03,712 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 00:39:04,103 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 00:39:04,179 : loading BERT model bert-large-uncased
2019-03-13 00:39:04,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:39:04,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:39:04,302 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6fkrxwui
2019-03-13 00:39:11,715 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:39:16,925 : Computing embeddings for train/dev/test
2019-03-13 00:42:52,654 : Computed embeddings
2019-03-13 00:42:52,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:43:22,862 : [('reg:1e-05', 57.61), ('reg:0.0001', 57.45), ('reg:0.001', 55.12), ('reg:0.01', 51.57)]
2019-03-13 00:43:22,863 : Validation : best param found is reg = 1e-05 with score             57.61
2019-03-13 00:43:22,863 : Evaluating...
2019-03-13 00:43:30,286 : 
Dev acc : 57.6 Test acc : 58.1 for COORDINATIONINVERSION classification

2019-03-13 00:43:30,288 : total results: {'STS12': {'MSRpar': {'pearson': (0.32303076095608896, 1.1238397289053062e-19), 'spearman': SpearmanrResult(correlation=0.35244171141739505, pvalue=2.346399284486663e-23), 'nsamples': 750}, 'MSRvid': {'pearson': (0.45877682728754976, 2.6043313229651384e-40), 'spearman': SpearmanrResult(correlation=0.47316925401796317, pvalue=4.1622552304099706e-43), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.478695712947476, 1.1514051528086857e-27), 'spearman': SpearmanrResult(correlation=0.5507809122078705, pvalue=9.038477032387899e-38), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.578289715584638, 3.79050791984144e-68), 'spearman': SpearmanrResult(correlation=0.5803405212678894, pvalue=9.914547462472156e-69), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6275252729724314, 4.507687356206853e-45), 'spearman': SpearmanrResult(correlation=0.5358508451961593, pvalue=4.924666060401002e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4932636579496368, 'wmean': 0.4794648951190795}, 'spearman': {'mean': 0.49851664882145547, 'wmean': 0.4894068664620707}}}, 'STS13': {'FNWN': {'pearson': (0.3050913471765782, 1.9660963155294823e-05), 'spearman': SpearmanrResult(correlation=0.29430798034590083, pvalue=3.945685540795466e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.554825573840125, 9.091580483515912e-62), 'spearman': SpearmanrResult(correlation=0.539619591124732, pvalue=6.755794713636897e-58), 'nsamples': 750}, 'OnWN': {'pearson': (0.29854975255664357, 5.167080297608694e-13), 'spearman': SpearmanrResult(correlation=0.34831073797160217, pvalue=1.911071926077506e-17), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.3861555578577823, 'wmean': 0.42751190412049606}, 'spearman': {'mean': 0.394079436480745, 'wmean': 0.43716081708732873}}}, 'STS14': {'deft-forum': {'pearson': (0.25268896745305264, 5.5146790858006765e-08), 'spearman': SpearmanrResult(correlation=0.2742053815062157, pvalue=3.332268838724105e-09), 'nsamples': 450}, 'deft-news': {'pearson': (0.5419161582090939, 2.6851868053152318e-24), 'spearman': SpearmanrResult(correlation=0.5378621161536362, pvalue=6.7915088265422e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.48905443331989545, 2.367730204061601e-46), 'spearman': SpearmanrResult(correlation=0.4441720167928966, pvalue=1.318738015758362e-37), 'nsamples': 750}, 'images': {'pearson': (0.5522880904095643, 4.154441989835022e-61), 'spearman': SpearmanrResult(correlation=0.5451591886516574, pvalue=2.7693305316539445e-59), 'nsamples': 750}, 'OnWN': {'pearson': (0.42259605641934617, 7.624972605879107e-34), 'spearman': SpearmanrResult(correlation=0.47843335780858687, pvalue=3.6544070142538186e-44), 'nsamples': 750}, 'tweet-news': {'pearson': (0.60744648267726, 7.8268933911851e-77), 'spearman': SpearmanrResult(correlation=0.5785568219812517, pvalue=3.184735295158653e-68), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.477665031414702, 'wmean': 0.48795298131630704}, 'spearman': {'mean': 0.4763981471490408, 'wmean': 0.4851978921199153}}}, 'STS15': {'answers-forums': {'pearson': (0.4417676715315958, 2.403882774436599e-19), 'spearman': SpearmanrResult(correlation=0.4271465324938719, pvalue=4.609811223829011e-18), 'nsamples': 375}, 'answers-students': {'pearson': (0.5952487954572225, 4.318131138928266e-73), 'spearman': SpearmanrResult(correlation=0.60864437154038, pvalue=3.291021969159483e-77), 'nsamples': 750}, 'belief': {'pearson': (0.45203714125491967, 2.7679092958679794e-20), 'spearman': SpearmanrResult(correlation=0.4677753335042297, pvalue=8.718584606441773e-22), 'nsamples': 375}, 'headlines': {'pearson': (0.6108540090200678, 6.592484723724264e-78), 'spearman': SpearmanrResult(correlation=0.6093868261391419, pvalue=1.920069796013522e-77), 'nsamples': 750}, 'images': {'pearson': (0.5338529195973063, 1.7647457958968899e-56), 'spearman': SpearmanrResult(correlation=0.5651650939568549, pvalue=1.6228055222991733e-64), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5267521073722223, 'wmean': 0.5467145326169636}, 'spearman': {'mean': 0.5356236315268956, 'wmean': 0.5576643061588569}}}, 'STS16': {'answer-answer': {'pearson': (0.4724674648487873, 1.577035407146125e-15), 'spearman': SpearmanrResult(correlation=0.49178244704811835, pvalue=7.128203825645138e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.6421095578573319, 2.4287978887329892e-30), 'spearman': SpearmanrResult(correlation=0.6447320660362026, pvalue=1.1878835017997326e-30), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6703887444120614, 2.2249450680425723e-31), 'spearman': SpearmanrResult(correlation=0.6936563043132916, pvalue=2.4730981233639573e-34), 'nsamples': 230}, 'postediting': {'pearson': (0.7128299037653807, 3.7081640892142597e-39), 'spearman': SpearmanrResult(correlation=0.7411864514203322, pvalue=8.951891759755922e-44), 'nsamples': 244}, 'question-question': {'pearson': (0.35553547731866686, 1.2728619546931728e-07), 'spearman': SpearmanrResult(correlation=0.33122050049534474, pvalue=9.649097419439923e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5706662296404457, 'wmean': 0.5753109936519359}, 'spearman': {'mean': 0.5805155538626579, 'wmean': 0.5860594896588379}}}, 'MR': {'devacc': 73.37, 'acc': 73.17, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.4, 'acc': 75.6, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.84, 'acc': 84.37, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.27, 'acc': 93.18, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.1, 'acc': 79.19, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.15, 'acc': 41.13, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 71.79, 'acc': 84.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.54, 'acc': 66.38, 'f1': 71.2, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 74.8, 'acc': 72.48, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7668187717388535, 'pearson': 0.7655073677075773, 'spearman': 0.6893253517375539, 'mse': 0.42144926874462446, 'yhat': array([1.72592874, 3.96679444, 2.24838901, ..., 3.19468494, 4.70858575,        4.6181782 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6996682438719831, 'pearson': 0.6492805896720899, 'spearman': 0.6378013678850581, 'mse': 1.6321127637792752, 'yhat': array([2.36477944, 1.20631822, 2.44660943, ..., 4.04114353, 3.57431316,        3.3724187 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.6, 'acc': 64.94, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.33, 'acc': 91.48, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 29.61, 'acc': 30.55, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.8, 'acc': 31.66, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 56.31, 'acc': 55.77, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 71.59, 'acc': 70.99, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.1, 'acc': 85.97, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.23, 'acc': 78.73, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.2, 'acc': 79.43, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.18, 'acc': 58.19, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.61, 'acc': 58.13, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 00:43:30,288 : STS12 p=0.4795, STS12 s=0.4894, STS13 p=0.4275, STS13 s=0.4372, STS14 p=0.4880, STS14 s=0.4852, STS15 p=0.5467, STS15 s=0.5577, STS 16 p=0.5753, STS16 s=0.5861, STS B p=0.6493, STS B s=0.6378, STS B m=1.6321, SICK-R p=0.7655, SICK-R s=0.6893, SICK-P m=0.4214
2019-03-13 00:43:30,288 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 00:43:30,288 : 0.4795,0.4894,0.4275,0.4372,0.4880,0.4852,0.5467,0.5577,0.5753,0.5861,0.6493,0.6378,1.6321,0.7655,0.6893,0.4214
2019-03-13 00:43:30,288 : MR=73.17, CR=75.60, SUBJ=93.18, MPQA=84.37, SST-B=79.19, SST-F=41.13, TREC=84.60, SICK-E=72.48, SNLI=64.94, MRPC=66.38, MRPC f=71.20
2019-03-13 00:43:30,288 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 00:43:30,288 : 73.17,75.60,93.18,84.37,79.19,41.13,84.60,72.48,64.94,66.38,71.20
2019-03-13 00:43:30,288 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 00:43:30,288 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 00:43:30,288 : na,na,na,na,na,na,na,na,na,na
2019-03-13 00:43:30,288 : SentLen=91.48, WC=30.55, TreeDepth=31.66, TopConst=55.77, BShift=70.99, Tense=85.97, SubjNum=78.73, ObjNum=79.43, SOMO=58.19, CoordInv=58.13, average=64.09
2019-03-13 00:43:30,288 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 00:43:30,288 : 91.48,30.55,31.66,55.77,70.99,85.97,78.73,79.43,58.19,58.13,64.09
2019-03-13 00:43:30,288 : ********************************************************************************
2019-03-13 00:43:30,288 : ********************************************************************************
2019-03-13 00:43:30,288 : ********************************************************************************
2019-03-13 00:43:30,288 : layer 8
2019-03-13 00:43:30,288 : ********************************************************************************
2019-03-13 00:43:30,288 : ********************************************************************************
2019-03-13 00:43:30,289 : ********************************************************************************
2019-03-13 00:43:30,376 : ***** Transfer task : STS12 *****


2019-03-13 00:43:30,388 : loading BERT model bert-large-uncased
2019-03-13 00:43:30,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:43:30,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:43:30,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8j__08pn
2019-03-13 00:43:37,881 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:43:47,142 : MSRpar : pearson = 0.2788, spearman = 0.3163
2019-03-13 00:43:48,786 : MSRvid : pearson = 0.3872, spearman = 0.4121
2019-03-13 00:43:50,205 : SMTeuroparl : pearson = 0.4528, spearman = 0.5464
2019-03-13 00:43:52,915 : surprise.OnWN : pearson = 0.5426, spearman = 0.5559
2019-03-13 00:43:54,348 : surprise.SMTnews : pearson = 0.6175, spearman = 0.5344
2019-03-13 00:43:54,348 : ALL (weighted average) : Pearson = 0.4378,             Spearman = 0.4592
2019-03-13 00:43:54,348 : ALL (average) : Pearson = 0.4558,             Spearman = 0.4730

2019-03-13 00:43:54,348 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 00:43:54,356 : loading BERT model bert-large-uncased
2019-03-13 00:43:54,356 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:43:54,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:43:54,374 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjheodmti
2019-03-13 00:44:01,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:44:08,566 : FNWN : pearson = 0.2369, spearman = 0.2410
2019-03-13 00:44:10,463 : headlines : pearson = 0.5322, spearman = 0.5244
2019-03-13 00:44:11,937 : OnWN : pearson = 0.2392, spearman = 0.2863
2019-03-13 00:44:11,937 : ALL (weighted average) : Pearson = 0.3854,             Spearman = 0.3997
2019-03-13 00:44:11,937 : ALL (average) : Pearson = 0.3361,             Spearman = 0.3506

2019-03-13 00:44:11,938 : ***** Transfer task : STS14 *****


2019-03-13 00:44:11,954 : loading BERT model bert-large-uncased
2019-03-13 00:44:11,954 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:44:11,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:44:11,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5w2i9zou
2019-03-13 00:44:19,393 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:44:26,114 : deft-forum : pearson = 0.2331, spearman = 0.2565
2019-03-13 00:44:27,756 : deft-news : pearson = 0.5237, spearman = 0.5459
2019-03-13 00:44:29,931 : headlines : pearson = 0.4568, spearman = 0.4305
2019-03-13 00:44:32,016 : images : pearson = 0.5144, spearman = 0.5172
2019-03-13 00:44:34,154 : OnWN : pearson = 0.3329, spearman = 0.3891
2019-03-13 00:44:37,028 : tweet-news : pearson = 0.4995, spearman = 0.4969
2019-03-13 00:44:37,029 : ALL (weighted average) : Pearson = 0.4306,             Spearman = 0.4412
2019-03-13 00:44:37,029 : ALL (average) : Pearson = 0.4267,             Spearman = 0.4394

2019-03-13 00:44:37,029 : ***** Transfer task : STS15 *****


2019-03-13 00:44:37,062 : loading BERT model bert-large-uncased
2019-03-13 00:44:37,062 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:44:37,080 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:44:37,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9lfsprwl
2019-03-13 00:44:44,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:44:51,768 : answers-forums : pearson = 0.3635, spearman = 0.3686
2019-03-13 00:44:53,861 : answers-students : pearson = 0.5412, spearman = 0.5620
2019-03-13 00:44:55,916 : belief : pearson = 0.4067, spearman = 0.4282
2019-03-13 00:44:58,178 : headlines : pearson = 0.6108, spearman = 0.6109
2019-03-13 00:45:00,320 : images : pearson = 0.4793, spearman = 0.5175
2019-03-13 00:45:00,320 : ALL (weighted average) : Pearson = 0.5041,             Spearman = 0.5222
2019-03-13 00:45:00,320 : ALL (average) : Pearson = 0.4803,             Spearman = 0.4974

2019-03-13 00:45:00,321 : ***** Transfer task : STS16 *****


2019-03-13 00:45:00,389 : loading BERT model bert-large-uncased
2019-03-13 00:45:00,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:45:00,407 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:45:00,407 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe_m2e5d_
2019-03-13 00:45:07,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:45:14,066 : answer-answer : pearson = 0.4058, spearman = 0.4348
2019-03-13 00:45:14,726 : headlines : pearson = 0.6444, spearman = 0.6465
2019-03-13 00:45:15,609 : plagiarism : pearson = 0.6039, spearman = 0.6443
2019-03-13 00:45:17,109 : postediting : pearson = 0.6625, spearman = 0.7304
2019-03-13 00:45:17,717 : question-question : pearson = 0.3059, spearman = 0.2908
2019-03-13 00:45:17,717 : ALL (weighted average) : Pearson = 0.5295,             Spearman = 0.5553
2019-03-13 00:45:17,718 : ALL (average) : Pearson = 0.5245,             Spearman = 0.5494

2019-03-13 00:45:17,718 : ***** Transfer task : MR *****


2019-03-13 00:45:17,737 : loading BERT model bert-large-uncased
2019-03-13 00:45:17,737 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:45:17,755 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:45:17,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ykz4hmi
2019-03-13 00:45:25,179 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:45:30,422 : Generating sentence embeddings
2019-03-13 00:46:02,036 : Generated sentence embeddings
2019-03-13 00:46:02,037 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:46:11,870 : Best param found at split 1: l2reg = 0.0001                 with score 73.09
2019-03-13 00:46:20,488 : Best param found at split 2: l2reg = 1e-05                 with score 72.78
2019-03-13 00:46:31,353 : Best param found at split 3: l2reg = 0.0001                 with score 73.26
2019-03-13 00:46:42,239 : Best param found at split 4: l2reg = 1e-05                 with score 73.41
2019-03-13 00:46:53,176 : Best param found at split 5: l2reg = 1e-05                 with score 72.13
2019-03-13 00:46:53,691 : Dev acc : 72.93 Test acc : 72.82

2019-03-13 00:46:53,692 : ***** Transfer task : CR *****


2019-03-13 00:46:53,700 : loading BERT model bert-large-uncased
2019-03-13 00:46:53,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:46:53,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:46:53,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_dk0cptr
2019-03-13 00:47:01,158 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:47:06,374 : Generating sentence embeddings
2019-03-13 00:47:14,699 : Generated sentence embeddings
2019-03-13 00:47:14,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:47:17,972 : Best param found at split 1: l2reg = 1e-05                 with score 76.55
2019-03-13 00:47:21,207 : Best param found at split 2: l2reg = 0.001                 with score 77.21
2019-03-13 00:47:24,145 : Best param found at split 3: l2reg = 0.0001                 with score 76.19
2019-03-13 00:47:27,883 : Best param found at split 4: l2reg = 1e-05                 with score 76.17
2019-03-13 00:47:30,877 : Best param found at split 5: l2reg = 0.0001                 with score 75.9
2019-03-13 00:47:31,097 : Dev acc : 76.4 Test acc : 75.65

2019-03-13 00:47:31,098 : ***** Transfer task : MPQA *****


2019-03-13 00:47:31,104 : loading BERT model bert-large-uncased
2019-03-13 00:47:31,104 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:47:31,155 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:47:31,155 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpryv8hm64
2019-03-13 00:47:38,566 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:47:43,756 : Generating sentence embeddings
2019-03-13 00:47:51,343 : Generated sentence embeddings
2019-03-13 00:47:51,344 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:48:00,145 : Best param found at split 1: l2reg = 1e-05                 with score 81.41
2019-03-13 00:48:09,883 : Best param found at split 2: l2reg = 1e-05                 with score 84.76
2019-03-13 00:48:20,831 : Best param found at split 3: l2reg = 1e-05                 with score 81.83
2019-03-13 00:48:32,163 : Best param found at split 4: l2reg = 0.0001                 with score 85.62
2019-03-13 00:48:39,946 : Best param found at split 5: l2reg = 1e-05                 with score 83.93
2019-03-13 00:48:40,440 : Dev acc : 83.51 Test acc : 85.14

2019-03-13 00:48:40,441 : ***** Transfer task : SUBJ *****


2019-03-13 00:48:40,456 : loading BERT model bert-large-uncased
2019-03-13 00:48:40,456 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:48:40,476 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:48:40,477 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0jlcsz04
2019-03-13 00:48:47,930 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:48:53,046 : Generating sentence embeddings
2019-03-13 00:49:23,987 : Generated sentence embeddings
2019-03-13 00:49:23,988 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 00:49:33,373 : Best param found at split 1: l2reg = 1e-05                 with score 93.09
2019-03-13 00:49:43,097 : Best param found at split 2: l2reg = 0.0001                 with score 93.66
2019-03-13 00:49:52,943 : Best param found at split 3: l2reg = 0.001                 with score 92.48
2019-03-13 00:50:02,590 : Best param found at split 4: l2reg = 1e-05                 with score 93.24
2019-03-13 00:50:13,202 : Best param found at split 5: l2reg = 1e-05                 with score 93.35
2019-03-13 00:50:14,014 : Dev acc : 93.16 Test acc : 93.07

2019-03-13 00:50:14,015 : ***** Transfer task : SST Binary classification *****


2019-03-13 00:50:14,107 : loading BERT model bert-large-uncased
2019-03-13 00:50:14,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:50:14,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:50:14,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj1li1xgt
2019-03-13 00:50:21,561 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:50:26,842 : Computing embedding for train
2019-03-13 00:52:07,538 : Computed train embeddings
2019-03-13 00:52:07,538 : Computing embedding for dev
2019-03-13 00:52:09,738 : Computed dev embeddings
2019-03-13 00:52:09,738 : Computing embedding for test
2019-03-13 00:52:14,367 : Computed test embeddings
2019-03-13 00:52:14,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:52:29,194 : [('reg:1e-05', 77.18), ('reg:0.0001', 77.29), ('reg:0.001', 76.49), ('reg:0.01', 73.85)]
2019-03-13 00:52:29,194 : Validation : best param found is reg = 0.0001 with score             77.29
2019-03-13 00:52:29,194 : Evaluating...
2019-03-13 00:52:32,812 : 
Dev acc : 77.29 Test acc : 79.68 for             SST Binary classification

2019-03-13 00:52:32,813 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 00:52:32,863 : loading BERT model bert-large-uncased
2019-03-13 00:52:32,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:52:32,885 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:52:32,886 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptxfty4_e
2019-03-13 00:52:40,354 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:52:45,564 : Computing embedding for train
2019-03-13 00:53:07,634 : Computed train embeddings
2019-03-13 00:53:07,634 : Computing embedding for dev
2019-03-13 00:53:10,522 : Computed dev embeddings
2019-03-13 00:53:10,522 : Computing embedding for test
2019-03-13 00:53:16,217 : Computed test embeddings
2019-03-13 00:53:16,217 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:53:18,535 : [('reg:1e-05', 36.6), ('reg:0.0001', 37.06), ('reg:0.001', 37.6), ('reg:0.01', 35.06)]
2019-03-13 00:53:18,535 : Validation : best param found is reg = 0.001 with score             37.6
2019-03-13 00:53:18,535 : Evaluating...
2019-03-13 00:53:19,163 : 
Dev acc : 37.6 Test acc : 41.72 for             SST Fine-Grained classification

2019-03-13 00:53:19,164 : ***** Transfer task : TREC *****


2019-03-13 00:53:19,176 : loading BERT model bert-large-uncased
2019-03-13 00:53:19,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:53:19,195 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:53:19,195 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdbfm_9eu
2019-03-13 00:53:26,642 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:53:39,527 : Computed train embeddings
2019-03-13 00:53:40,119 : Computed test embeddings
2019-03-13 00:53:40,120 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:53:47,294 : [('reg:1e-05', 68.98), ('reg:0.0001', 68.54), ('reg:0.001', 64.62), ('reg:0.01', 54.26)]
2019-03-13 00:53:47,294 : Cross-validation : best param found is reg = 1e-05             with score 68.98
2019-03-13 00:53:47,294 : Evaluating...
2019-03-13 00:53:47,797 : 
Dev acc : 68.98 Test acc : 77.2             for TREC

2019-03-13 00:53:47,798 : ***** Transfer task : MRPC *****


2019-03-13 00:53:47,820 : loading BERT model bert-large-uncased
2019-03-13 00:53:47,821 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:53:47,841 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:53:47,841 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8sd9ve1g
2019-03-13 00:53:55,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:54:00,469 : Computing embedding for train
2019-03-13 00:54:22,857 : Computed train embeddings
2019-03-13 00:54:22,858 : Computing embedding for test
2019-03-13 00:54:32,678 : Computed test embeddings
2019-03-13 00:54:32,699 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 00:54:36,649 : [('reg:1e-05', 70.36), ('reg:0.0001', 70.41), ('reg:0.001', 70.53), ('reg:0.01', 69.11)]
2019-03-13 00:54:36,649 : Cross-validation : best param found is reg = 0.001             with score 70.53
2019-03-13 00:54:36,649 : Evaluating...
2019-03-13 00:54:36,998 : Dev acc : 70.53 Test acc 72.17; Test F1 81.0 for MRPC.

2019-03-13 00:54:36,998 : ***** Transfer task : SICK-Entailment*****


2019-03-13 00:54:37,058 : loading BERT model bert-large-uncased
2019-03-13 00:54:37,058 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:54:37,078 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:54:37,078 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwwfldhje
2019-03-13 00:54:44,505 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:54:49,787 : Computing embedding for train
2019-03-13 00:55:01,150 : Computed train embeddings
2019-03-13 00:55:01,150 : Computing embedding for dev
2019-03-13 00:55:02,704 : Computed dev embeddings
2019-03-13 00:55:02,704 : Computing embedding for test
2019-03-13 00:55:14,905 : Computed test embeddings
2019-03-13 00:55:14,942 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 00:55:16,411 : [('reg:1e-05', 76.4), ('reg:0.0001', 76.4), ('reg:0.001', 62.2), ('reg:0.01', 64.4)]
2019-03-13 00:55:16,411 : Validation : best param found is reg = 1e-05 with score             76.4
2019-03-13 00:55:16,411 : Evaluating...
2019-03-13 00:55:16,881 : 
Dev acc : 76.4 Test acc : 74.41 for                        SICK entailment

2019-03-13 00:55:16,882 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 00:55:16,909 : loading BERT model bert-large-uncased
2019-03-13 00:55:16,909 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:55:16,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:55:16,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpshtxdq5p
2019-03-13 00:55:24,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:55:29,621 : Computing embedding for train
2019-03-13 00:55:41,020 : Computed train embeddings
2019-03-13 00:55:41,020 : Computing embedding for dev
2019-03-13 00:55:42,579 : Computed dev embeddings
2019-03-13 00:55:42,579 : Computing embedding for test
2019-03-13 00:55:54,816 : Computed test embeddings
2019-03-13 00:56:12,401 : Dev : Pearson 0.7597828286483611
2019-03-13 00:56:12,407 : Test : Pearson 0.7499068903780305 Spearman 0.6836757468820703 MSE 0.4464634885233647                        for SICK Relatedness

2019-03-13 00:56:12,408 : 

***** Transfer task : STSBenchmark*****


2019-03-13 00:56:12,471 : loading BERT model bert-large-uncased
2019-03-13 00:56:12,471 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:56:12,490 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:56:12,490 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsbfpz_jo
2019-03-13 00:56:19,918 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:56:25,202 : Computing embedding for train
2019-03-13 00:56:43,894 : Computed train embeddings
2019-03-13 00:56:43,894 : Computing embedding for dev
2019-03-13 00:56:49,590 : Computed dev embeddings
2019-03-13 00:56:49,591 : Computing embedding for test
2019-03-13 00:56:54,240 : Computed test embeddings
2019-03-13 00:57:13,418 : Dev : Pearson 0.6872996152137189
2019-03-13 00:57:13,419 : Test : Pearson 0.6400860217664857 Spearman 0.6324338962767275 MSE 1.708857725588874                        for SICK Relatedness

2019-03-13 00:57:13,419 : ***** Transfer task : SNLI Entailment*****


2019-03-13 00:57:18,437 : loading BERT model bert-large-uncased
2019-03-13 00:57:18,437 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 00:57:18,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 00:57:18,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7_fl8zre
2019-03-13 00:57:25,913 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 00:57:31,547 : PROGRESS (encoding): 0.00%
2019-03-13 01:00:21,253 : PROGRESS (encoding): 14.56%
2019-03-13 01:03:33,372 : PROGRESS (encoding): 29.12%
2019-03-13 01:06:44,599 : PROGRESS (encoding): 43.69%
2019-03-13 01:10:08,385 : PROGRESS (encoding): 58.25%
2019-03-13 01:13:55,055 : PROGRESS (encoding): 72.81%
2019-03-13 01:17:39,989 : PROGRESS (encoding): 87.37%
2019-03-13 01:21:43,878 : PROGRESS (encoding): 0.00%
2019-03-13 01:22:14,555 : PROGRESS (encoding): 0.00%
2019-03-13 01:22:44,110 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:23:18,659 : [('reg:1e-09', 63.15)]
2019-03-13 01:23:18,659 : Validation : best param found is reg = 1e-09 with score             63.15
2019-03-13 01:23:18,659 : Evaluating...
2019-03-13 01:23:54,789 : Dev acc : 63.15 Test acc : 63.45 for SNLI

2019-03-13 01:23:54,789 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 01:23:54,996 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 01:23:56,050 : loading BERT model bert-large-uncased
2019-03-13 01:23:56,051 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:23:56,077 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:23:56,077 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2zf947xw
2019-03-13 01:24:03,559 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:24:08,938 : Computing embeddings for train/dev/test
2019-03-13 01:27:42,405 : Computed embeddings
2019-03-13 01:27:42,405 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:28:09,081 : [('reg:1e-05', 91.93), ('reg:0.0001', 90.84), ('reg:0.001', 86.53), ('reg:0.01', 82.92)]
2019-03-13 01:28:09,082 : Validation : best param found is reg = 1e-05 with score             91.93
2019-03-13 01:28:09,082 : Evaluating...
2019-03-13 01:28:16,360 : 
Dev acc : 91.9 Test acc : 92.8 for LENGTH classification

2019-03-13 01:28:16,361 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 01:28:16,613 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 01:28:16,659 : loading BERT model bert-large-uncased
2019-03-13 01:28:16,659 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:28:16,689 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:28:16,689 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoi9wn1cx
2019-03-13 01:28:24,136 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:28:29,217 : Computing embeddings for train/dev/test
2019-03-13 01:31:46,518 : Computed embeddings
2019-03-13 01:31:46,518 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:32:15,081 : [('reg:1e-05', 19.57), ('reg:0.0001', 2.29), ('reg:0.001', 0.58), ('reg:0.01', 0.32)]
2019-03-13 01:32:15,082 : Validation : best param found is reg = 1e-05 with score             19.57
2019-03-13 01:32:15,082 : Evaluating...
2019-03-13 01:32:21,169 : 
Dev acc : 19.6 Test acc : 20.5 for WORDCONTENT classification

2019-03-13 01:32:21,170 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 01:32:21,702 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 01:32:21,768 : loading BERT model bert-large-uncased
2019-03-13 01:32:21,768 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:32:21,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:32:21,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmrpgvm9f
2019-03-13 01:32:29,185 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:32:34,465 : Computing embeddings for train/dev/test
2019-03-13 01:35:40,471 : Computed embeddings
2019-03-13 01:35:40,471 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:36:02,227 : [('reg:1e-05', 30.25), ('reg:0.0001', 29.58), ('reg:0.001', 27.66), ('reg:0.01', 23.66)]
2019-03-13 01:36:02,227 : Validation : best param found is reg = 1e-05 with score             30.25
2019-03-13 01:36:02,227 : Evaluating...
2019-03-13 01:36:07,593 : 
Dev acc : 30.2 Test acc : 30.5 for DEPTH classification

2019-03-13 01:36:07,594 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 01:36:07,964 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 01:36:08,027 : loading BERT model bert-large-uncased
2019-03-13 01:36:08,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:36:08,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:36:08,137 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwa3eyx9f
2019-03-13 01:36:15,566 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:36:20,924 : Computing embeddings for train/dev/test
2019-03-13 01:39:12,956 : Computed embeddings
2019-03-13 01:39:12,957 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:39:46,754 : [('reg:1e-05', 56.37), ('reg:0.0001', 51.34), ('reg:0.001', 38.38), ('reg:0.01', 19.05)]
2019-03-13 01:39:46,754 : Validation : best param found is reg = 1e-05 with score             56.37
2019-03-13 01:39:46,754 : Evaluating...
2019-03-13 01:39:56,586 : 
Dev acc : 56.4 Test acc : 56.5 for TOPCONSTITUENTS classification

2019-03-13 01:39:56,587 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 01:39:56,963 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 01:39:57,029 : loading BERT model bert-large-uncased
2019-03-13 01:39:57,029 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:39:57,058 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:39:57,058 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm7z54xue
2019-03-13 01:40:04,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:40:09,841 : Computing embeddings for train/dev/test
2019-03-13 01:43:15,626 : Computed embeddings
2019-03-13 01:43:15,626 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:43:53,387 : [('reg:1e-05', 71.91), ('reg:0.0001', 71.07), ('reg:0.001', 70.41), ('reg:0.01', 64.65)]
2019-03-13 01:43:53,388 : Validation : best param found is reg = 1e-05 with score             71.91
2019-03-13 01:43:53,388 : Evaluating...
2019-03-13 01:44:03,457 : 
Dev acc : 71.9 Test acc : 71.5 for BIGRAMSHIFT classification

2019-03-13 01:44:03,458 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 01:44:03,846 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 01:44:03,911 : loading BERT model bert-large-uncased
2019-03-13 01:44:03,911 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:44:03,942 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:44:03,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3uk_n9pl
2019-03-13 01:44:11,398 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:44:16,624 : Computing embeddings for train/dev/test
2019-03-13 01:47:17,815 : Computed embeddings
2019-03-13 01:47:17,816 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:47:44,214 : [('reg:1e-05', 88.26), ('reg:0.0001', 88.42), ('reg:0.001', 88.2), ('reg:0.01', 86.32)]
2019-03-13 01:47:44,214 : Validation : best param found is reg = 0.0001 with score             88.42
2019-03-13 01:47:44,214 : Evaluating...
2019-03-13 01:47:51,749 : 
Dev acc : 88.4 Test acc : 86.3 for TENSE classification

2019-03-13 01:47:51,750 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 01:47:52,150 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 01:47:52,212 : loading BERT model bert-large-uncased
2019-03-13 01:47:52,213 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:47:52,326 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:47:52,326 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps60kav4e
2019-03-13 01:47:59,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:48:05,093 : Computing embeddings for train/dev/test
2019-03-13 01:51:17,775 : Computed embeddings
2019-03-13 01:51:17,775 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:51:49,385 : [('reg:1e-05', 78.89), ('reg:0.0001', 81.12), ('reg:0.001', 79.51), ('reg:0.01', 73.92)]
2019-03-13 01:51:49,385 : Validation : best param found is reg = 0.0001 with score             81.12
2019-03-13 01:51:49,386 : Evaluating...
2019-03-13 01:51:56,830 : 
Dev acc : 81.1 Test acc : 80.3 for SUBJNUMBER classification

2019-03-13 01:51:56,831 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 01:51:57,233 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 01:51:57,300 : loading BERT model bert-large-uncased
2019-03-13 01:51:57,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:51:57,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:51:57,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpluu5xjvp
2019-03-13 01:52:04,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:52:10,237 : Computing embeddings for train/dev/test
2019-03-13 01:55:19,605 : Computed embeddings
2019-03-13 01:55:19,606 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 01:55:47,770 : [('reg:1e-05', 78.71), ('reg:0.0001', 78.16), ('reg:0.001', 76.3), ('reg:0.01', 66.83)]
2019-03-13 01:55:47,770 : Validation : best param found is reg = 1e-05 with score             78.71
2019-03-13 01:55:47,771 : Evaluating...
2019-03-13 01:55:55,267 : 
Dev acc : 78.7 Test acc : 79.7 for OBJNUMBER classification

2019-03-13 01:55:55,269 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 01:55:55,861 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 01:55:55,933 : loading BERT model bert-large-uncased
2019-03-13 01:55:55,933 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 01:55:55,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 01:55:55,962 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzx9ipcvt
2019-03-13 01:56:03,481 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 01:56:08,761 : Computing embeddings for train/dev/test
2019-03-13 01:59:47,048 : Computed embeddings
2019-03-13 01:59:47,048 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:00:14,951 : [('reg:1e-05', 58.79), ('reg:0.0001', 58.7), ('reg:0.001', 58.25), ('reg:0.01', 56.38)]
2019-03-13 02:00:14,952 : Validation : best param found is reg = 1e-05 with score             58.79
2019-03-13 02:00:14,952 : Evaluating...
2019-03-13 02:00:22,300 : 
Dev acc : 58.8 Test acc : 59.3 for ODDMANOUT classification

2019-03-13 02:00:22,301 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 02:00:22,679 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 02:00:22,760 : loading BERT model bert-large-uncased
2019-03-13 02:00:22,760 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:00:22,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:00:22,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjzn9irya
2019-03-13 02:00:30,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:00:35,504 : Computing embeddings for train/dev/test
2019-03-13 02:04:10,846 : Computed embeddings
2019-03-13 02:04:10,846 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:04:33,954 : [('reg:1e-05', 53.19), ('reg:0.0001', 53.01), ('reg:0.001', 51.28), ('reg:0.01', 50.0)]
2019-03-13 02:04:33,954 : Validation : best param found is reg = 1e-05 with score             53.19
2019-03-13 02:04:33,954 : Evaluating...
2019-03-13 02:04:38,819 : 
Dev acc : 53.2 Test acc : 53.3 for COORDINATIONINVERSION classification

2019-03-13 02:04:38,821 : total results: {'STS12': {'MSRpar': {'pearson': (0.27882778164561006, 7.36518481018571e-15), 'spearman': SpearmanrResult(correlation=0.3163017895920087, pvalue=6.885397726353186e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3872344597450641, 3.0604050428272863e-28), 'spearman': SpearmanrResult(correlation=0.4120628471800341, pvalue=4.1904272212392275e-32), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.45281435063721653, 1.3836340390236928e-24), 'spearman': SpearmanrResult(correlation=0.5464487368517744, pvalue=4.3082043487983265e-37), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5425555589619123, 1.2520800509794236e-58), 'spearman': SpearmanrResult(correlation=0.5558901886557408, pvalue=4.78731135623609e-62), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6175427070399385, 2.572993012589024e-43), 'spearman': SpearmanrResult(correlation=0.534411425674783, pvalue=7.5792352510510525e-31), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4557949716059483, 'wmean': 0.43780716773354494}, 'spearman': {'mean': 0.47302299759086813, 'wmean': 0.45921533080117133}}}, 'STS13': {'FNWN': {'pearson': (0.2368678972806469, 0.0010321262816042848), 'spearman': SpearmanrResult(correlation=0.24103515330472164, pvalue=0.0008344678071096382), 'nsamples': 189}, 'headlines': {'pearson': (0.5321870374576831, 4.476675849112969e-56), 'spearman': SpearmanrResult(correlation=0.524449174385259, pvalue=3.158829905173216e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.23921123622499904, 9.655186981251896e-09), 'spearman': SpearmanrResult(correlation=0.2862853004789396, pvalue=4.8181240741302176e-12), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.336088723654443, 'wmean': 0.3854038761343527}, 'spearman': {'mean': 0.3505898760563067, 'wmean': 0.3996657188881478}}}, 'STS14': {'deft-forum': {'pearson': (0.2330779478311828, 5.744922133254903e-07), 'spearman': SpearmanrResult(correlation=0.256514242493656, pvalue=3.410034942580098e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.5236557817446513, 1.5923081516988214e-22), 'spearman': SpearmanrResult(correlation=0.5458660637234309, pvalue=1.0741179876815522e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.4568468493026517, 6.034546843739078e-40), 'spearman': SpearmanrResult(correlation=0.4305420353736591, pvalue=3.3745689507890327e-35), 'nsamples': 750}, 'images': {'pearson': (0.5144340113345256, 6.643569638445293e-52), 'spearman': SpearmanrResult(correlation=0.5172474889114006, pvalue=1.5054175161522095e-52), 'nsamples': 750}, 'OnWN': {'pearson': (0.33286130698746375, 7.319125508834905e-21), 'spearman': SpearmanrResult(correlation=0.38912825481869046, pvalue=1.5941008445330116e-28), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4994788591055486, 1.4134457363260084e-48), 'spearman': SpearmanrResult(correlation=0.4969436127324044, pvalue=4.9918047545802203e-48), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.42672579271767064, 'wmean': 0.430586021625352}, 'spearman': {'mean': 0.4393736163422069, 'wmean': 0.4412232725643441}}}, 'STS15': {'answers-forums': {'pearson': (0.3635318866228351, 3.6929717397945354e-13), 'spearman': SpearmanrResult(correlation=0.3686104282725344, pvalue=1.636085109554311e-13), 'nsamples': 375}, 'answers-students': {'pearson': (0.5412421190264166, 2.6669453574511948e-58), 'spearman': SpearmanrResult(correlation=0.5619544609207983, pvalue=1.1859621126719625e-63), 'nsamples': 750}, 'belief': {'pearson': (0.40672246133325374, 2.2610353557755516e-16), 'spearman': SpearmanrResult(correlation=0.42816628304374876, pvalue=3.768936331673406e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.6108375834272775, 6.672064606135091e-78), 'spearman': SpearmanrResult(correlation=0.6108609714339138, pvalue=6.559038425540701e-78), 'nsamples': 750}, 'images': {'pearson': (0.47926781998182133, 2.4753912806357297e-44), 'spearman': SpearmanrResult(correlation=0.5174780778402148, pvalue=1.3321298961635883e-52), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4803203740783209, 'wmean': 0.50411867410339}, 'spearman': {'mean': 0.497414044302242, 'wmean': 0.5221704664632671}}}, 'STS16': {'answer-answer': {'pearson': (0.4057892511740917, 1.732518845676941e-11), 'spearman': SpearmanrResult(correlation=0.43481794488977454, pvalue=3.8688566661246756e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.6444233020408279, 1.2927140207246224e-30), 'spearman': SpearmanrResult(correlation=0.6465189883761017, pvalue=7.267411202868492e-31), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6038990885731577, 3.008505340110144e-24), 'spearman': SpearmanrResult(correlation=0.6443315397456543, pvalue=2.2528217701003944e-28), 'nsamples': 230}, 'postediting': {'pearson': (0.6625409284456161, 3.2683811103423857e-32), 'spearman': SpearmanrResult(correlation=0.7304045293640589, pvalue=5.978136452108248e-42), 'nsamples': 244}, 'question-question': {'pearson': (0.3058557966391674, 6.681722797373091e-06), 'spearman': SpearmanrResult(correlation=0.2907536145933579, pvalue=1.9477542206160267e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5245016733745722, 'wmean': 0.5295215096260776}, 'spearman': {'mean': 0.5493653233937895, 'wmean': 0.5553199417065722}}}, 'MR': {'devacc': 72.93, 'acc': 72.82, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.4, 'acc': 75.65, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.51, 'acc': 85.14, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.16, 'acc': 93.07, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.29, 'acc': 79.68, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.6, 'acc': 41.72, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 68.98, 'acc': 77.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.53, 'acc': 72.17, 'f1': 81.0, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.4, 'acc': 74.41, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7597828286483611, 'pearson': 0.7499068903780305, 'spearman': 0.6836757468820703, 'mse': 0.4464634885233647, 'yhat': array([1.6200445 , 4.50117408, 2.04031979, ..., 2.73136416, 4.61732432,        4.2634139 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6872996152137189, 'pearson': 0.6400860217664857, 'spearman': 0.6324338962767275, 'mse': 1.708857725588874, 'yhat': array([2.57401424, 1.24853859, 2.59862547, ..., 3.94832786, 3.63570942,        3.37810101]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.15, 'acc': 63.45, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.93, 'acc': 92.76, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 19.57, 'acc': 20.53, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.25, 'acc': 30.5, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 56.37, 'acc': 56.52, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 71.91, 'acc': 71.48, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.42, 'acc': 86.31, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.12, 'acc': 80.35, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.71, 'acc': 79.72, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 58.79, 'acc': 59.32, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.19, 'acc': 53.33, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 02:04:38,821 : STS12 p=0.4378, STS12 s=0.4592, STS13 p=0.3854, STS13 s=0.3997, STS14 p=0.4306, STS14 s=0.4412, STS15 p=0.5041, STS15 s=0.5222, STS 16 p=0.5295, STS16 s=0.5553, STS B p=0.6401, STS B s=0.6324, STS B m=1.7089, SICK-R p=0.7499, SICK-R s=0.6837, SICK-P m=0.4465
2019-03-13 02:04:38,821 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 02:04:38,821 : 0.4378,0.4592,0.3854,0.3997,0.4306,0.4412,0.5041,0.5222,0.5295,0.5553,0.6401,0.6324,1.7089,0.7499,0.6837,0.4465
2019-03-13 02:04:38,821 : MR=72.82, CR=75.65, SUBJ=93.07, MPQA=85.14, SST-B=79.68, SST-F=41.72, TREC=77.20, SICK-E=74.41, SNLI=63.45, MRPC=72.17, MRPC f=81.00
2019-03-13 02:04:38,821 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 02:04:38,821 : 72.82,75.65,93.07,85.14,79.68,41.72,77.20,74.41,63.45,72.17,81.00
2019-03-13 02:04:38,821 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 02:04:38,821 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 02:04:38,821 : na,na,na,na,na,na,na,na,na,na
2019-03-13 02:04:38,821 : SentLen=92.76, WC=20.53, TreeDepth=30.50, TopConst=56.52, BShift=71.48, Tense=86.31, SubjNum=80.35, ObjNum=79.72, SOMO=59.32, CoordInv=53.33, average=63.08
2019-03-13 02:04:38,821 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 02:04:38,821 : 92.76,20.53,30.50,56.52,71.48,86.31,80.35,79.72,59.32,53.33,63.08
2019-03-13 02:04:38,821 : ********************************************************************************
2019-03-13 02:04:38,821 : ********************************************************************************
2019-03-13 02:04:38,821 : ********************************************************************************
2019-03-13 02:04:38,821 : layer 9
2019-03-13 02:04:38,822 : ********************************************************************************
2019-03-13 02:04:38,822 : ********************************************************************************
2019-03-13 02:04:38,822 : ********************************************************************************
2019-03-13 02:04:38,915 : ***** Transfer task : STS12 *****


2019-03-13 02:04:38,928 : loading BERT model bert-large-uncased
2019-03-13 02:04:38,928 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:04:38,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:04:38,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr02hymf_
2019-03-13 02:04:46,438 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:04:55,806 : MSRpar : pearson = 0.2474, spearman = 0.2955
2019-03-13 02:04:57,452 : MSRvid : pearson = 0.3343, spearman = 0.3730
2019-03-13 02:04:58,866 : SMTeuroparl : pearson = 0.4459, spearman = 0.5393
2019-03-13 02:05:01,580 : surprise.OnWN : pearson = 0.4987, spearman = 0.5235
2019-03-13 02:05:03,016 : surprise.SMTnews : pearson = 0.5908, spearman = 0.5045
2019-03-13 02:05:03,016 : ALL (weighted average) : Pearson = 0.4024,             Spearman = 0.4321
2019-03-13 02:05:03,016 : ALL (average) : Pearson = 0.4234,             Spearman = 0.4472

2019-03-13 02:05:03,016 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 02:05:03,025 : loading BERT model bert-large-uncased
2019-03-13 02:05:03,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:03,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:03,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4mksj6p2
2019-03-13 02:05:10,423 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:05:16,840 : FNWN : pearson = 0.1940, spearman = 0.2104
2019-03-13 02:05:18,738 : headlines : pearson = 0.4750, spearman = 0.4954
2019-03-13 02:05:20,210 : OnWN : pearson = 0.2055, spearman = 0.2502
2019-03-13 02:05:20,211 : ALL (weighted average) : Pearson = 0.3388,             Spearman = 0.3678
2019-03-13 02:05:20,211 : ALL (average) : Pearson = 0.2915,             Spearman = 0.3187

2019-03-13 02:05:20,211 : ***** Transfer task : STS14 *****


2019-03-13 02:05:20,226 : loading BERT model bert-large-uncased
2019-03-13 02:05:20,226 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:20,243 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:20,243 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi602t9rf
2019-03-13 02:05:27,669 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:05:34,294 : deft-forum : pearson = 0.2071, spearman = 0.2533
2019-03-13 02:05:35,937 : deft-news : pearson = 0.5023, spearman = 0.5504
2019-03-13 02:05:38,115 : headlines : pearson = 0.4006, spearman = 0.4103
2019-03-13 02:05:40,197 : images : pearson = 0.4542, spearman = 0.4805
2019-03-13 02:05:42,336 : OnWN : pearson = 0.2930, spearman = 0.3333
2019-03-13 02:05:45,203 : tweet-news : pearson = 0.3928, spearman = 0.4170
2019-03-13 02:05:45,204 : ALL (weighted average) : Pearson = 0.3731,             Spearman = 0.4026
2019-03-13 02:05:45,204 : ALL (average) : Pearson = 0.3750,             Spearman = 0.4075

2019-03-13 02:05:45,204 : ***** Transfer task : STS15 *****


2019-03-13 02:05:45,237 : loading BERT model bert-large-uncased
2019-03-13 02:05:45,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:05:45,255 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:05:45,255 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzktdhidx
2019-03-13 02:05:52,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:05:59,873 : answers-forums : pearson = 0.2966, spearman = 0.3299
2019-03-13 02:06:01,961 : answers-students : pearson = 0.4980, spearman = 0.5475
2019-03-13 02:06:04,015 : belief : pearson = 0.3532, spearman = 0.4110
2019-03-13 02:06:06,273 : headlines : pearson = 0.5753, spearman = 0.5827
2019-03-13 02:06:08,413 : images : pearson = 0.4283, spearman = 0.4838
2019-03-13 02:06:08,413 : ALL (weighted average) : Pearson = 0.4566,             Spearman = 0.4961
2019-03-13 02:06:08,413 : ALL (average) : Pearson = 0.4303,             Spearman = 0.4710

2019-03-13 02:06:08,413 : ***** Transfer task : STS16 *****


2019-03-13 02:06:08,484 : loading BERT model bert-large-uncased
2019-03-13 02:06:08,484 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:06:08,502 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:06:08,502 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfa98rzps
2019-03-13 02:06:15,889 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:06:22,160 : answer-answer : pearson = 0.3651, spearman = 0.4197
2019-03-13 02:06:22,821 : headlines : pearson = 0.6245, spearman = 0.6359
2019-03-13 02:06:23,705 : plagiarism : pearson = 0.5004, spearman = 0.6158
2019-03-13 02:06:25,202 : postediting : pearson = 0.5935, spearman = 0.7218
2019-03-13 02:06:25,810 : question-question : pearson = 0.2653, spearman = 0.2490
2019-03-13 02:06:25,810 : ALL (weighted average) : Pearson = 0.4752,             Spearman = 0.5352
2019-03-13 02:06:25,811 : ALL (average) : Pearson = 0.4697,             Spearman = 0.5284

2019-03-13 02:06:25,811 : ***** Transfer task : MR *****


2019-03-13 02:06:25,826 : loading BERT model bert-large-uncased
2019-03-13 02:06:25,826 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:06:25,846 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:06:25,847 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprdsgfhuy
2019-03-13 02:06:33,238 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:06:38,532 : Generating sentence embeddings
2019-03-13 02:07:10,087 : Generated sentence embeddings
2019-03-13 02:07:10,087 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:07:18,760 : Best param found at split 1: l2reg = 0.0001                 with score 72.69
2019-03-13 02:07:27,155 : Best param found at split 2: l2reg = 0.001                 with score 72.78
2019-03-13 02:07:36,736 : Best param found at split 3: l2reg = 1e-05                 with score 72.86
2019-03-13 02:07:47,774 : Best param found at split 4: l2reg = 0.0001                 with score 72.98
2019-03-13 02:07:57,990 : Best param found at split 5: l2reg = 1e-05                 with score 72.44
2019-03-13 02:07:58,600 : Dev acc : 72.75 Test acc : 71.66

2019-03-13 02:07:58,601 : ***** Transfer task : CR *****


2019-03-13 02:07:58,609 : loading BERT model bert-large-uncased
2019-03-13 02:07:58,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:07:58,629 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:07:58,629 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf54aqnxm
2019-03-13 02:08:06,040 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:08:11,294 : Generating sentence embeddings
2019-03-13 02:08:19,633 : Generated sentence embeddings
2019-03-13 02:08:19,633 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:08:23,468 : Best param found at split 1: l2reg = 1e-05                 with score 75.82
2019-03-13 02:08:27,396 : Best param found at split 2: l2reg = 1e-05                 with score 76.35
2019-03-13 02:08:31,239 : Best param found at split 3: l2reg = 0.01                 with score 75.96
2019-03-13 02:08:35,084 : Best param found at split 4: l2reg = 1e-05                 with score 75.87
2019-03-13 02:08:38,697 : Best param found at split 5: l2reg = 0.001                 with score 75.27
2019-03-13 02:08:38,885 : Dev acc : 75.85 Test acc : 74.99

2019-03-13 02:08:38,885 : ***** Transfer task : MPQA *****


2019-03-13 02:08:38,891 : loading BERT model bert-large-uncased
2019-03-13 02:08:38,891 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:08:38,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:08:38,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdw046t0t
2019-03-13 02:08:46,386 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:08:51,678 : Generating sentence embeddings
2019-03-13 02:08:59,260 : Generated sentence embeddings
2019-03-13 02:08:59,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:09:06,463 : Best param found at split 1: l2reg = 0.01                 with score 80.74
2019-03-13 02:09:14,332 : Best param found at split 2: l2reg = 1e-05                 with score 84.09
2019-03-13 02:09:24,352 : Best param found at split 3: l2reg = 0.001                 with score 83.34
2019-03-13 02:09:36,345 : Best param found at split 4: l2reg = 1e-05                 with score 84.44
2019-03-13 02:09:48,956 : Best param found at split 5: l2reg = 0.0001                 with score 85.39
2019-03-13 02:09:49,468 : Dev acc : 83.6 Test acc : 82.99

2019-03-13 02:09:49,469 : ***** Transfer task : SUBJ *****


2019-03-13 02:09:49,485 : loading BERT model bert-large-uncased
2019-03-13 02:09:49,486 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:09:49,504 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:09:49,504 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo8oeyrix
2019-03-13 02:09:56,958 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:10:02,151 : Generating sentence embeddings
2019-03-13 02:10:33,072 : Generated sentence embeddings
2019-03-13 02:10:33,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 02:10:42,367 : Best param found at split 1: l2reg = 1e-05                 with score 93.3
2019-03-13 02:10:51,367 : Best param found at split 2: l2reg = 1e-05                 with score 93.14
2019-03-13 02:11:01,127 : Best param found at split 3: l2reg = 0.0001                 with score 92.71
2019-03-13 02:11:11,625 : Best param found at split 4: l2reg = 1e-05                 with score 93.41
2019-03-13 02:11:22,887 : Best param found at split 5: l2reg = 1e-05                 with score 93.29
2019-03-13 02:11:23,543 : Dev acc : 93.17 Test acc : 92.68

2019-03-13 02:11:23,544 : ***** Transfer task : SST Binary classification *****


2019-03-13 02:11:23,635 : loading BERT model bert-large-uncased
2019-03-13 02:11:23,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:11:23,709 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:11:23,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxnvzcgbt
2019-03-13 02:11:31,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:11:36,334 : Computing embedding for train
2019-03-13 02:13:16,958 : Computed train embeddings
2019-03-13 02:13:16,958 : Computing embedding for dev
2019-03-13 02:13:19,162 : Computed dev embeddings
2019-03-13 02:13:19,162 : Computing embedding for test
2019-03-13 02:13:23,791 : Computed test embeddings
2019-03-13 02:13:23,791 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:13:45,865 : [('reg:1e-05', 78.44), ('reg:0.0001', 78.21), ('reg:0.001', 77.75), ('reg:0.01', 74.77)]
2019-03-13 02:13:45,865 : Validation : best param found is reg = 1e-05 with score             78.44
2019-03-13 02:13:45,865 : Evaluating...
2019-03-13 02:13:51,442 : 
Dev acc : 78.44 Test acc : 78.42 for             SST Binary classification

2019-03-13 02:13:51,442 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 02:13:51,496 : loading BERT model bert-large-uncased
2019-03-13 02:13:51,496 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:13:51,516 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:13:51,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpepiloel5
2019-03-13 02:13:58,928 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:14:04,196 : Computing embedding for train
2019-03-13 02:14:26,196 : Computed train embeddings
2019-03-13 02:14:26,196 : Computing embedding for dev
2019-03-13 02:14:29,073 : Computed dev embeddings
2019-03-13 02:14:29,073 : Computing embedding for test
2019-03-13 02:14:34,749 : Computed test embeddings
2019-03-13 02:14:34,750 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:14:37,072 : [('reg:1e-05', 38.51), ('reg:0.0001', 39.06), ('reg:0.001', 39.96), ('reg:0.01', 34.42)]
2019-03-13 02:14:37,072 : Validation : best param found is reg = 0.001 with score             39.96
2019-03-13 02:14:37,072 : Evaluating...
2019-03-13 02:14:37,729 : 
Dev acc : 39.96 Test acc : 41.45 for             SST Fine-Grained classification

2019-03-13 02:14:37,730 : ***** Transfer task : TREC *****


2019-03-13 02:14:37,743 : loading BERT model bert-large-uncased
2019-03-13 02:14:37,743 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:14:37,762 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:14:37,762 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfgb5lsf4
2019-03-13 02:14:45,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:14:58,178 : Computed train embeddings
2019-03-13 02:14:58,770 : Computed test embeddings
2019-03-13 02:14:58,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:15:06,542 : [('reg:1e-05', 70.54), ('reg:0.0001', 70.32), ('reg:0.001', 66.01), ('reg:0.01', 47.71)]
2019-03-13 02:15:06,542 : Cross-validation : best param found is reg = 1e-05             with score 70.54
2019-03-13 02:15:06,542 : Evaluating...
2019-03-13 02:15:06,936 : 
Dev acc : 70.54 Test acc : 83.0             for TREC

2019-03-13 02:15:06,937 : ***** Transfer task : MRPC *****


2019-03-13 02:15:06,958 : loading BERT model bert-large-uncased
2019-03-13 02:15:06,958 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:15:06,980 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:15:06,981 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp8yty1we
2019-03-13 02:15:14,371 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:15:19,462 : Computing embedding for train
2019-03-13 02:15:41,835 : Computed train embeddings
2019-03-13 02:15:41,835 : Computing embedding for test
2019-03-13 02:15:51,648 : Computed test embeddings
2019-03-13 02:15:51,669 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 02:15:56,844 : [('reg:1e-05', 70.54), ('reg:0.0001', 70.17), ('reg:0.001', 69.38), ('reg:0.01', 68.62)]
2019-03-13 02:15:56,845 : Cross-validation : best param found is reg = 1e-05             with score 70.54
2019-03-13 02:15:56,845 : Evaluating...
2019-03-13 02:15:57,111 : Dev acc : 70.54 Test acc 70.49; Test F1 81.21 for MRPC.

2019-03-13 02:15:57,111 : ***** Transfer task : SICK-Entailment*****


2019-03-13 02:15:57,173 : loading BERT model bert-large-uncased
2019-03-13 02:15:57,173 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:15:57,192 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:15:57,192 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpri3cwcee
2019-03-13 02:16:04,652 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:16:09,924 : Computing embedding for train
2019-03-13 02:16:21,296 : Computed train embeddings
2019-03-13 02:16:21,297 : Computing embedding for dev
2019-03-13 02:16:22,848 : Computed dev embeddings
2019-03-13 02:16:22,848 : Computing embedding for test
2019-03-13 02:16:35,054 : Computed test embeddings
2019-03-13 02:16:35,092 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:16:36,706 : [('reg:1e-05', 64.2), ('reg:0.0001', 72.4), ('reg:0.001', 67.2), ('reg:0.01', 63.2)]
2019-03-13 02:16:36,706 : Validation : best param found is reg = 0.0001 with score             72.4
2019-03-13 02:16:36,706 : Evaluating...
2019-03-13 02:16:37,201 : 
Dev acc : 72.4 Test acc : 70.73 for                        SICK entailment

2019-03-13 02:16:37,202 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 02:16:37,228 : loading BERT model bert-large-uncased
2019-03-13 02:16:37,228 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:16:37,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:16:37,287 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxvyyv0ng
2019-03-13 02:16:44,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:16:50,030 : Computing embedding for train
2019-03-13 02:17:01,418 : Computed train embeddings
2019-03-13 02:17:01,418 : Computing embedding for dev
2019-03-13 02:17:02,975 : Computed dev embeddings
2019-03-13 02:17:02,975 : Computing embedding for test
2019-03-13 02:17:15,226 : Computed test embeddings
2019-03-13 02:17:35,461 : Dev : Pearson 0.7283020369454082
2019-03-13 02:17:35,462 : Test : Pearson 0.7444554582451489 Spearman 0.678345008425783 MSE 0.4567399484727557                        for SICK Relatedness

2019-03-13 02:17:35,462 : 

***** Transfer task : STSBenchmark*****


2019-03-13 02:17:35,531 : loading BERT model bert-large-uncased
2019-03-13 02:17:35,531 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:17:35,552 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:17:35,552 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu0g83pz6
2019-03-13 02:17:42,931 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:17:48,140 : Computing embedding for train
2019-03-13 02:18:06,861 : Computed train embeddings
2019-03-13 02:18:06,862 : Computing embedding for dev
2019-03-13 02:18:12,550 : Computed dev embeddings
2019-03-13 02:18:12,550 : Computing embedding for test
2019-03-13 02:18:17,202 : Computed test embeddings
2019-03-13 02:18:36,228 : Dev : Pearson 0.6827440222859731
2019-03-13 02:18:36,228 : Test : Pearson 0.630060609214181 Spearman 0.6236108758130098 MSE 1.73061198143915                        for SICK Relatedness

2019-03-13 02:18:36,229 : ***** Transfer task : SNLI Entailment*****


2019-03-13 02:18:41,330 : loading BERT model bert-large-uncased
2019-03-13 02:18:41,331 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:18:41,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:18:41,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8pb1bk9i
2019-03-13 02:18:48,878 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:18:54,506 : PROGRESS (encoding): 0.00%
2019-03-13 02:21:43,017 : PROGRESS (encoding): 14.56%
2019-03-13 02:24:56,144 : PROGRESS (encoding): 29.12%
2019-03-13 02:28:07,486 : PROGRESS (encoding): 43.69%
2019-03-13 02:31:31,024 : PROGRESS (encoding): 58.25%
2019-03-13 02:35:17,681 : PROGRESS (encoding): 72.81%
2019-03-13 02:39:02,384 : PROGRESS (encoding): 87.37%
2019-03-13 02:43:06,126 : PROGRESS (encoding): 0.00%
2019-03-13 02:43:36,755 : PROGRESS (encoding): 0.00%
2019-03-13 02:44:06,213 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:44:42,866 : [('reg:1e-09', 57.6)]
2019-03-13 02:44:42,867 : Validation : best param found is reg = 1e-09 with score             57.6
2019-03-13 02:44:42,867 : Evaluating...
2019-03-13 02:45:18,983 : Dev acc : 57.6 Test acc : 57.58 for SNLI

2019-03-13 02:45:18,983 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 02:45:19,185 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 02:45:20,245 : loading BERT model bert-large-uncased
2019-03-13 02:45:20,245 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:45:20,282 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:45:20,283 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_g4e_cct
2019-03-13 02:45:27,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:45:32,993 : Computing embeddings for train/dev/test
2019-03-13 02:49:06,367 : Computed embeddings
2019-03-13 02:49:06,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:49:31,168 : [('reg:1e-05', 91.31), ('reg:0.0001', 89.33), ('reg:0.001', 84.83), ('reg:0.01', 74.26)]
2019-03-13 02:49:31,168 : Validation : best param found is reg = 1e-05 with score             91.31
2019-03-13 02:49:31,168 : Evaluating...
2019-03-13 02:49:37,591 : 
Dev acc : 91.3 Test acc : 91.9 for LENGTH classification

2019-03-13 02:49:37,592 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 02:49:37,968 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 02:49:38,012 : loading BERT model bert-large-uncased
2019-03-13 02:49:38,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:49:38,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:49:38,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp17ttlfo9
2019-03-13 02:49:45,502 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:49:50,776 : Computing embeddings for train/dev/test
2019-03-13 02:53:07,508 : Computed embeddings
2019-03-13 02:53:07,508 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:53:38,255 : [('reg:1e-05', 10.26), ('reg:0.0001', 1.72), ('reg:0.001', 0.46), ('reg:0.01', 0.25)]
2019-03-13 02:53:38,255 : Validation : best param found is reg = 1e-05 with score             10.26
2019-03-13 02:53:38,255 : Evaluating...
2019-03-13 02:53:45,737 : 
Dev acc : 10.3 Test acc : 10.1 for WORDCONTENT classification

2019-03-13 02:53:45,738 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 02:53:46,113 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 02:53:46,179 : loading BERT model bert-large-uncased
2019-03-13 02:53:46,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:53:46,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:53:46,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpje8xdtac
2019-03-13 02:53:53,626 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:53:58,887 : Computing embeddings for train/dev/test
2019-03-13 02:57:04,427 : Computed embeddings
2019-03-13 02:57:04,427 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 02:57:28,122 : [('reg:1e-05', 30.79), ('reg:0.0001', 29.99), ('reg:0.001', 27.03), ('reg:0.01', 23.75)]
2019-03-13 02:57:28,122 : Validation : best param found is reg = 1e-05 with score             30.79
2019-03-13 02:57:28,123 : Evaluating...
2019-03-13 02:57:34,379 : 
Dev acc : 30.8 Test acc : 31.1 for DEPTH classification

2019-03-13 02:57:34,380 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 02:57:34,766 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 02:57:34,831 : loading BERT model bert-large-uncased
2019-03-13 02:57:34,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 02:57:34,940 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 02:57:34,940 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5fg9bnl9
2019-03-13 02:57:42,367 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 02:57:47,672 : Computing embeddings for train/dev/test
2019-03-13 03:00:38,622 : Computed embeddings
2019-03-13 03:00:38,622 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:01:11,021 : [('reg:1e-05', 58.37), ('reg:0.0001', 54.48), ('reg:0.001', 39.8), ('reg:0.01', 18.49)]
2019-03-13 03:01:11,021 : Validation : best param found is reg = 1e-05 with score             58.37
2019-03-13 03:01:11,021 : Evaluating...
2019-03-13 03:01:19,878 : 
Dev acc : 58.4 Test acc : 58.2 for TOPCONSTITUENTS classification

2019-03-13 03:01:19,879 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 03:01:20,222 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 03:01:20,289 : loading BERT model bert-large-uncased
2019-03-13 03:01:20,289 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:01:20,408 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:01:20,408 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb602ujm6
2019-03-13 03:01:27,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:01:33,263 : Computing embeddings for train/dev/test
2019-03-13 03:04:39,045 : Computed embeddings
2019-03-13 03:04:39,045 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:05:12,577 : [('reg:1e-05', 71.14), ('reg:0.0001', 71.16), ('reg:0.001', 68.6), ('reg:0.01', 55.38)]
2019-03-13 03:05:12,577 : Validation : best param found is reg = 0.0001 with score             71.16
2019-03-13 03:05:12,577 : Evaluating...
2019-03-13 03:05:20,841 : 
Dev acc : 71.2 Test acc : 70.9 for BIGRAMSHIFT classification

2019-03-13 03:05:20,842 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 03:05:21,403 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 03:05:21,468 : loading BERT model bert-large-uncased
2019-03-13 03:05:21,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:05:21,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:05:21,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7ygknno5
2019-03-13 03:05:28,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:05:34,289 : Computing embeddings for train/dev/test
2019-03-13 03:08:35,318 : Computed embeddings
2019-03-13 03:08:35,318 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:09:04,126 : [('reg:1e-05', 88.16), ('reg:0.0001', 88.33), ('reg:0.001', 87.04), ('reg:0.01', 86.61)]
2019-03-13 03:09:04,127 : Validation : best param found is reg = 0.0001 with score             88.33
2019-03-13 03:09:04,127 : Evaluating...
2019-03-13 03:09:10,406 : 
Dev acc : 88.3 Test acc : 86.6 for TENSE classification

2019-03-13 03:09:10,407 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 03:09:10,828 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 03:09:10,891 : loading BERT model bert-large-uncased
2019-03-13 03:09:10,892 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:09:10,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:09:10,916 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpscdjofpy
2019-03-13 03:09:18,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:09:23,731 : Computing embeddings for train/dev/test
2019-03-13 03:12:36,406 : Computed embeddings
2019-03-13 03:12:36,406 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:13:05,228 : [('reg:1e-05', 75.75), ('reg:0.0001', 75.97), ('reg:0.001', 79.87), ('reg:0.01', 74.67)]
2019-03-13 03:13:05,229 : Validation : best param found is reg = 0.001 with score             79.87
2019-03-13 03:13:05,229 : Evaluating...
2019-03-13 03:13:10,454 : 
Dev acc : 79.9 Test acc : 78.8 for SUBJNUMBER classification

2019-03-13 03:13:10,455 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 03:13:10,861 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 03:13:10,928 : loading BERT model bert-large-uncased
2019-03-13 03:13:10,928 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:13:11,043 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:13:11,043 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1dtrj4o4
2019-03-13 03:13:18,524 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:13:23,807 : Computing embeddings for train/dev/test
2019-03-13 03:16:32,970 : Computed embeddings
2019-03-13 03:16:32,971 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:16:57,736 : [('reg:1e-05', 77.7), ('reg:0.0001', 78.28), ('reg:0.001', 75.72), ('reg:0.01', 67.35)]
2019-03-13 03:16:57,736 : Validation : best param found is reg = 0.0001 with score             78.28
2019-03-13 03:16:57,737 : Evaluating...
2019-03-13 03:17:03,851 : 
Dev acc : 78.3 Test acc : 78.5 for OBJNUMBER classification

2019-03-13 03:17:03,852 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 03:17:04,445 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 03:17:04,515 : loading BERT model bert-large-uncased
2019-03-13 03:17:04,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:17:04,542 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:17:04,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_e6szcxp
2019-03-13 03:17:12,020 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:17:17,266 : Computing embeddings for train/dev/test
2019-03-13 03:20:55,283 : Computed embeddings
2019-03-13 03:20:55,283 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:21:18,260 : [('reg:1e-05', 58.95), ('reg:0.0001', 58.94), ('reg:0.001', 59.14), ('reg:0.01', 57.55)]
2019-03-13 03:21:18,260 : Validation : best param found is reg = 0.001 with score             59.14
2019-03-13 03:21:18,260 : Evaluating...
2019-03-13 03:21:23,557 : 
Dev acc : 59.1 Test acc : 58.7 for ODDMANOUT classification

2019-03-13 03:21:23,558 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 03:21:23,958 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 03:21:24,036 : loading BERT model bert-large-uncased
2019-03-13 03:21:24,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:21:24,163 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:21:24,164 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxjw6rvnz
2019-03-13 03:21:31,608 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:21:36,938 : Computing embeddings for train/dev/test
2019-03-13 03:25:12,714 : Computed embeddings
2019-03-13 03:25:12,714 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:25:38,251 : [('reg:1e-05', 51.57), ('reg:0.0001', 50.67), ('reg:0.001', 50.22), ('reg:0.01', 50.21)]
2019-03-13 03:25:38,251 : Validation : best param found is reg = 1e-05 with score             51.57
2019-03-13 03:25:38,251 : Evaluating...
2019-03-13 03:25:45,793 : 
Dev acc : 51.6 Test acc : 51.6 for COORDINATIONINVERSION classification

2019-03-13 03:25:45,795 : total results: {'STS12': {'MSRpar': {'pearson': (0.24736956611913613, 6.409066637045364e-12), 'spearman': SpearmanrResult(correlation=0.2955162349830339, pvalue=1.3989031836505844e-16), 'nsamples': 750}, 'MSRvid': {'pearson': (0.33431559737801325, 4.844826952412405e-21), 'spearman': SpearmanrResult(correlation=0.37296087600439787, pvalue=3.647669572974303e-26), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4459117114935093, 8.297619334855178e-24), 'spearman': SpearmanrResult(correlation=0.5392817642911933, pvalue=5.430402177661962e-36), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.498719763903955, 2.064599678488539e-48), 'spearman': SpearmanrResult(correlation=0.5235471192386947, pvalue=5.1515159586009657e-54), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5907823593287961, 6.629973111770226e-39), 'spearman': SpearmanrResult(correlation=0.5045043477631869, pvalue=3.7521803537605704e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4234197996446819, 'wmean': 0.4024129126443175}, 'spearman': {'mean': 0.4471620684561014, 'wmean': 0.4320610480169769}}}, 'STS13': {'FNWN': {'pearson': (0.19400211315032512, 0.007476128913624453), 'spearman': SpearmanrResult(correlation=0.21039715265050116, pvalue=0.003662071467519667), 'nsamples': 189}, 'headlines': {'pearson': (0.4749656505044886, 1.8232893501912028e-43), 'spearman': SpearmanrResult(correlation=0.4954083140553386, pvalue=1.0662866131580482e-47), 'nsamples': 750}, 'OnWN': {'pearson': (0.20548470143901, 9.172148428155771e-07), 'spearman': SpearmanrResult(correlation=0.25021695842801434, pvalue=1.865247944555094e-09), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.29148415503127456, 'wmean': 0.33877836984737497}, 'spearman': {'mean': 0.31867414171128466, 'wmean': 0.3677953407137098}}}, 'STS14': {'deft-forum': {'pearson': (0.20706278873645328, 9.496515106679382e-06), 'spearman': SpearmanrResult(correlation=0.253279876159531, pvalue=5.122626684571341e-08), 'nsamples': 450}, 'deft-news': {'pearson': (0.5023456901940818, 1.381100698597231e-20), 'spearman': SpearmanrResult(correlation=0.5503998756862996, pvalue=3.6963352082276996e-25), 'nsamples': 300}, 'headlines': {'pearson': (0.400620815716043, 2.7761807365884585e-30), 'spearman': SpearmanrResult(correlation=0.41028263440168594, pvalue=8.133427794330353e-32), 'nsamples': 750}, 'images': {'pearson': (0.454154491779426, 1.9313826851772715e-39), 'spearman': SpearmanrResult(correlation=0.48053159235698684, pvalue=1.3694457712923096e-44), 'nsamples': 750}, 'OnWN': {'pearson': (0.2929714136526653, 2.6046012382078116e-16), 'spearman': SpearmanrResult(correlation=0.3332648277565851, pvalue=6.52887139900935e-21), 'nsamples': 750}, 'tweet-news': {'pearson': (0.39282696369518844, 4.4048541150493097e-29), 'spearman': SpearmanrResult(correlation=0.41702737682520047, pvalue=6.454241286780647e-33), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.37499702729564294, 'wmean': 0.3731499268325656}, 'spearman': {'mean': 0.4074643638643815, 'wmean': 0.40264686146213935}}}, 'STS15': {'answers-forums': {'pearson': (0.2966227161894036, 4.706331764607647e-09), 'spearman': SpearmanrResult(correlation=0.32986975793381357, pvalue=5.721725482543827e-11), 'nsamples': 375}, 'answers-students': {'pearson': (0.49798779622988454, 2.972491176023799e-48), 'spearman': SpearmanrResult(correlation=0.5474627735757267, pvalue=7.20804930105772e-60), 'nsamples': 750}, 'belief': {'pearson': (0.353167182230714, 1.861219482052005e-12), 'spearman': SpearmanrResult(correlation=0.41097283519912126, pvalue=1.0278368410629036e-16), 'nsamples': 375}, 'headlines': {'pearson': (0.5753395393812905, 2.5665004918960478e-67), 'spearman': SpearmanrResult(correlation=0.5826965247035051, pvalue=2.0991896717157776e-69), 'nsamples': 750}, 'images': {'pearson': (0.4283388757718471, 8.077280245409934e-35), 'spearman': SpearmanrResult(correlation=0.4838352371735563, pvalue=2.879789309886553e-45), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.43029122196062797, 'wmean': 0.4566402901482702}, 'spearman': {'mean': 0.47096742571714456, 'wmean': 0.4961039580048139}}}, 'STS16': {'answer-answer': {'pearson': (0.36505114244920944, 2.00360140563027e-09), 'spearman': SpearmanrResult(correlation=0.4197012500684901, pvalue=2.931778294374466e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.6244714066686444, 2.502101005564786e-28), 'spearman': SpearmanrResult(correlation=0.6359082253884417, pvalue=1.2823185225166636e-29), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5003975836709408, 5.603005554315877e-16), 'spearman': SpearmanrResult(correlation=0.6157930533802173, pvalue=2.1204657981076495e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.5934508613071041, 1.3141415613438368e-24), 'spearman': SpearmanrResult(correlation=0.7218015092609035, pvalue=1.4805559005862523e-40), 'nsamples': 244}, 'question-question': {'pearson': (0.26525099657015944, 0.00010383622615224178), 'spearman': SpearmanrResult(correlation=0.24901952359943766, pvalue=0.0002770262648046854), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.46972439813321165, 'wmean': 0.4751663432790934}, 'spearman': {'mean': 0.5284447123394981, 'wmean': 0.5351958824692339}}}, 'MR': {'devacc': 72.75, 'acc': 71.66, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.85, 'acc': 74.99, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.6, 'acc': 82.99, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.17, 'acc': 92.68, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.44, 'acc': 78.42, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.96, 'acc': 41.45, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.54, 'acc': 83.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.54, 'acc': 70.49, 'f1': 81.21, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.4, 'acc': 70.73, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7283020369454082, 'pearson': 0.7444554582451489, 'spearman': 0.678345008425783, 'mse': 0.4567399484727557, 'yhat': array([2.30049056, 3.98056824, 1.08879293, ..., 2.61484718, 4.45215067,        4.06476504]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6827440222859731, 'pearson': 0.630060609214181, 'spearman': 0.6236108758130098, 'mse': 1.73061198143915, 'yhat': array([2.24977947, 1.35697947, 2.46467468, ..., 3.85390658, 3.55792455,        3.42780688]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.6, 'acc': 57.58, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 91.31, 'acc': 91.92, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 10.26, 'acc': 10.05, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.79, 'acc': 31.14, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 58.37, 'acc': 58.2, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 71.16, 'acc': 70.9, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.33, 'acc': 86.63, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.87, 'acc': 78.85, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.28, 'acc': 78.5, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.14, 'acc': 58.69, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 51.57, 'acc': 51.59, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 03:25:45,795 : STS12 p=0.4024, STS12 s=0.4321, STS13 p=0.3388, STS13 s=0.3678, STS14 p=0.3731, STS14 s=0.4026, STS15 p=0.4566, STS15 s=0.4961, STS 16 p=0.4752, STS16 s=0.5352, STS B p=0.6301, STS B s=0.6236, STS B m=1.7306, SICK-R p=0.7445, SICK-R s=0.6783, SICK-P m=0.4567
2019-03-13 03:25:45,795 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 03:25:45,795 : 0.4024,0.4321,0.3388,0.3678,0.3731,0.4026,0.4566,0.4961,0.4752,0.5352,0.6301,0.6236,1.7306,0.7445,0.6783,0.4567
2019-03-13 03:25:45,795 : MR=71.66, CR=74.99, SUBJ=92.68, MPQA=82.99, SST-B=78.42, SST-F=41.45, TREC=83.00, SICK-E=70.73, SNLI=57.58, MRPC=70.49, MRPC f=81.21
2019-03-13 03:25:45,795 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 03:25:45,795 : 71.66,74.99,92.68,82.99,78.42,41.45,83.00,70.73,57.58,70.49,81.21
2019-03-13 03:25:45,795 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 03:25:45,795 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 03:25:45,795 : na,na,na,na,na,na,na,na,na,na
2019-03-13 03:25:45,795 : SentLen=91.92, WC=10.05, TreeDepth=31.14, TopConst=58.20, BShift=70.90, Tense=86.63, SubjNum=78.85, ObjNum=78.50, SOMO=58.69, CoordInv=51.59, average=61.65
2019-03-13 03:25:45,795 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 03:25:45,795 : 91.92,10.05,31.14,58.20,70.90,86.63,78.85,78.50,58.69,51.59,61.65
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,795 : layer 10
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,795 : ********************************************************************************
2019-03-13 03:25:45,883 : ***** Transfer task : STS12 *****


2019-03-13 03:25:45,895 : loading BERT model bert-large-uncased
2019-03-13 03:25:45,895 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:25:45,913 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:25:45,913 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzj4m_puf
2019-03-13 03:25:53,367 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:02,735 : MSRpar : pearson = 0.2191, spearman = 0.2908
2019-03-13 03:26:04,383 : MSRvid : pearson = 0.3257, spearman = 0.3598
2019-03-13 03:26:05,800 : SMTeuroparl : pearson = 0.4457, spearman = 0.5287
2019-03-13 03:26:08,508 : surprise.OnWN : pearson = 0.4022, spearman = 0.4275
2019-03-13 03:26:09,941 : surprise.SMTnews : pearson = 0.5223, spearman = 0.5040
2019-03-13 03:26:09,941 : ALL (weighted average) : Pearson = 0.3614,             Spearman = 0.4029
2019-03-13 03:26:09,941 : ALL (average) : Pearson = 0.3830,             Spearman = 0.4222

2019-03-13 03:26:09,941 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 03:26:09,949 : loading BERT model bert-large-uncased
2019-03-13 03:26:09,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:26:09,967 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:26:09,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzjuzai5p
2019-03-13 03:26:17,454 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:23,948 : FNWN : pearson = 0.1811, spearman = 0.2311
2019-03-13 03:26:25,847 : headlines : pearson = 0.3645, spearman = 0.4478
2019-03-13 03:26:27,321 : OnWN : pearson = 0.1375, spearman = 0.1922
2019-03-13 03:26:27,321 : ALL (weighted average) : Pearson = 0.2565,             Spearman = 0.3249
2019-03-13 03:26:27,321 : ALL (average) : Pearson = 0.2277,             Spearman = 0.2904

2019-03-13 03:26:27,321 : ***** Transfer task : STS14 *****


2019-03-13 03:26:27,338 : loading BERT model bert-large-uncased
2019-03-13 03:26:27,338 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:26:27,355 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:26:27,355 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4afrilhm
2019-03-13 03:26:34,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:26:41,327 : deft-forum : pearson = 0.1574, spearman = 0.2104
2019-03-13 03:26:42,970 : deft-news : pearson = 0.4779, spearman = 0.5389
2019-03-13 03:26:45,147 : headlines : pearson = 0.2720, spearman = 0.3560
2019-03-13 03:26:47,233 : images : pearson = 0.3824, spearman = 0.4482
2019-03-13 03:26:49,371 : OnWN : pearson = 0.1618, spearman = 0.2158
2019-03-13 03:26:52,243 : tweet-news : pearson = 0.2836, spearman = 0.3397
2019-03-13 03:26:52,243 : ALL (weighted average) : Pearson = 0.2771,             Spearman = 0.3403
2019-03-13 03:26:52,243 : ALL (average) : Pearson = 0.2892,             Spearman = 0.3515

2019-03-13 03:26:52,243 : ***** Transfer task : STS15 *****


2019-03-13 03:26:52,277 : loading BERT model bert-large-uncased
2019-03-13 03:26:52,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:26:52,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:26:52,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf_slq4yv
2019-03-13 03:26:59,712 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:27:06,886 : answers-forums : pearson = 0.2352, spearman = 0.3008
2019-03-13 03:27:08,973 : answers-students : pearson = 0.4274, spearman = 0.5059
2019-03-13 03:27:11,028 : belief : pearson = 0.3064, spearman = 0.3800
2019-03-13 03:27:13,286 : headlines : pearson = 0.5061, spearman = 0.5468
2019-03-13 03:27:15,426 : images : pearson = 0.3276, spearman = 0.4371
2019-03-13 03:27:15,426 : ALL (weighted average) : Pearson = 0.3830,             Spearman = 0.4575
2019-03-13 03:27:15,426 : ALL (average) : Pearson = 0.3605,             Spearman = 0.4341

2019-03-13 03:27:15,426 : ***** Transfer task : STS16 *****


2019-03-13 03:27:15,494 : loading BERT model bert-large-uncased
2019-03-13 03:27:15,494 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:27:15,512 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:27:15,512 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_vedaow5
2019-03-13 03:27:22,998 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:27:29,121 : answer-answer : pearson = 0.3202, spearman = 0.3863
2019-03-13 03:27:29,783 : headlines : pearson = 0.5267, spearman = 0.6036
2019-03-13 03:27:30,667 : plagiarism : pearson = 0.4021, spearman = 0.5875
2019-03-13 03:27:32,165 : postediting : pearson = 0.5475, spearman = 0.7280
2019-03-13 03:27:32,772 : question-question : pearson = 0.2028, spearman = 0.2131
2019-03-13 03:27:32,773 : ALL (weighted average) : Pearson = 0.4055,             Spearman = 0.5107
2019-03-13 03:27:32,773 : ALL (average) : Pearson = 0.3999,             Spearman = 0.5037

2019-03-13 03:27:32,773 : ***** Transfer task : MR *****


2019-03-13 03:27:32,791 : loading BERT model bert-large-uncased
2019-03-13 03:27:32,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:27:32,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:27:32,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyig1ja7a
2019-03-13 03:27:40,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:27:45,601 : Generating sentence embeddings
2019-03-13 03:28:17,201 : Generated sentence embeddings
2019-03-13 03:28:17,202 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:28:27,210 : Best param found at split 1: l2reg = 1e-05                 with score 73.3
2019-03-13 03:28:36,103 : Best param found at split 2: l2reg = 0.001                 with score 72.52
2019-03-13 03:28:44,371 : Best param found at split 3: l2reg = 1e-05                 with score 72.87
2019-03-13 03:28:55,144 : Best param found at split 4: l2reg = 0.0001                 with score 72.66
2019-03-13 03:29:06,168 : Best param found at split 5: l2reg = 1e-05                 with score 72.54
2019-03-13 03:29:06,948 : Dev acc : 72.78 Test acc : 71.11

2019-03-13 03:29:06,949 : ***** Transfer task : CR *****


2019-03-13 03:29:06,957 : loading BERT model bert-large-uncased
2019-03-13 03:29:06,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:29:06,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:29:06,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzkkekdfa
2019-03-13 03:29:14,442 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:29:19,798 : Generating sentence embeddings
2019-03-13 03:29:28,125 : Generated sentence embeddings
2019-03-13 03:29:28,126 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:29:31,758 : Best param found at split 1: l2reg = 0.0001                 with score 75.49
2019-03-13 03:29:34,743 : Best param found at split 2: l2reg = 0.0001                 with score 76.05
2019-03-13 03:29:37,870 : Best param found at split 3: l2reg = 0.0001                 with score 76.03
2019-03-13 03:29:42,067 : Best param found at split 4: l2reg = 0.0001                 with score 76.27
2019-03-13 03:29:46,186 : Best param found at split 5: l2reg = 0.001                 with score 75.77
2019-03-13 03:29:46,504 : Dev acc : 75.92 Test acc : 75.02

2019-03-13 03:29:46,504 : ***** Transfer task : MPQA *****


2019-03-13 03:29:46,511 : loading BERT model bert-large-uncased
2019-03-13 03:29:46,511 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:29:46,567 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:29:46,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiykxnmtg
2019-03-13 03:29:54,082 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:29:59,295 : Generating sentence embeddings
2019-03-13 03:30:06,884 : Generated sentence embeddings
2019-03-13 03:30:06,884 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:30:15,511 : Best param found at split 1: l2reg = 1e-05                 with score 77.94
2019-03-13 03:30:25,346 : Best param found at split 2: l2reg = 0.001                 with score 81.2
2019-03-13 03:30:36,093 : Best param found at split 3: l2reg = 0.0001                 with score 83.44
2019-03-13 03:30:47,775 : Best param found at split 4: l2reg = 0.0001                 with score 84.27
2019-03-13 03:30:59,509 : Best param found at split 5: l2reg = 1e-05                 with score 83.94
2019-03-13 03:31:00,140 : Dev acc : 82.16 Test acc : 83.7

2019-03-13 03:31:00,140 : ***** Transfer task : SUBJ *****


2019-03-13 03:31:00,155 : loading BERT model bert-large-uncased
2019-03-13 03:31:00,156 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:31:00,176 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:31:00,176 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprcur5iru
2019-03-13 03:31:07,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:31:12,979 : Generating sentence embeddings
2019-03-13 03:31:43,933 : Generated sentence embeddings
2019-03-13 03:31:43,933 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 03:31:54,157 : Best param found at split 1: l2reg = 0.001                 with score 93.45
2019-03-13 03:32:05,221 : Best param found at split 2: l2reg = 1e-05                 with score 93.85
2019-03-13 03:32:16,349 : Best param found at split 3: l2reg = 1e-05                 with score 93.59
2019-03-13 03:32:26,499 : Best param found at split 4: l2reg = 0.0001                 with score 93.8
2019-03-13 03:32:35,541 : Best param found at split 5: l2reg = 1e-05                 with score 93.78
2019-03-13 03:32:36,014 : Dev acc : 93.69 Test acc : 93.4

2019-03-13 03:32:36,015 : ***** Transfer task : SST Binary classification *****


2019-03-13 03:32:36,107 : loading BERT model bert-large-uncased
2019-03-13 03:32:36,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:32:36,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:32:36,179 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe335ch0e
2019-03-13 03:32:43,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:32:48,782 : Computing embedding for train
2019-03-13 03:34:29,483 : Computed train embeddings
2019-03-13 03:34:29,483 : Computing embedding for dev
2019-03-13 03:34:31,683 : Computed dev embeddings
2019-03-13 03:34:31,683 : Computing embedding for test
2019-03-13 03:34:36,313 : Computed test embeddings
2019-03-13 03:34:36,313 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:34:54,929 : [('reg:1e-05', 77.75), ('reg:0.0001', 79.13), ('reg:0.001', 76.61), ('reg:0.01', 72.36)]
2019-03-13 03:34:54,929 : Validation : best param found is reg = 0.0001 with score             79.13
2019-03-13 03:34:54,929 : Evaluating...
2019-03-13 03:35:00,615 : 
Dev acc : 79.13 Test acc : 81.11 for             SST Binary classification

2019-03-13 03:35:00,615 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 03:35:00,667 : loading BERT model bert-large-uncased
2019-03-13 03:35:00,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:35:00,689 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:35:00,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsdxvorql
2019-03-13 03:35:08,177 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:35:13,447 : Computing embedding for train
2019-03-13 03:35:35,463 : Computed train embeddings
2019-03-13 03:35:35,463 : Computing embedding for dev
2019-03-13 03:35:38,343 : Computed dev embeddings
2019-03-13 03:35:38,343 : Computing embedding for test
2019-03-13 03:35:44,019 : Computed test embeddings
2019-03-13 03:35:44,019 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:35:46,398 : [('reg:1e-05', 38.51), ('reg:0.0001', 38.51), ('reg:0.001', 38.87), ('reg:0.01', 33.51)]
2019-03-13 03:35:46,398 : Validation : best param found is reg = 0.001 with score             38.87
2019-03-13 03:35:46,398 : Evaluating...
2019-03-13 03:35:47,033 : 
Dev acc : 38.87 Test acc : 42.4 for             SST Fine-Grained classification

2019-03-13 03:35:47,034 : ***** Transfer task : TREC *****


2019-03-13 03:35:47,047 : loading BERT model bert-large-uncased
2019-03-13 03:35:47,047 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:35:47,066 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:35:47,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp31cigum7
2019-03-13 03:35:54,538 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:36:07,390 : Computed train embeddings
2019-03-13 03:36:07,982 : Computed test embeddings
2019-03-13 03:36:07,983 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:36:15,305 : [('reg:1e-05', 69.22), ('reg:0.0001', 68.56), ('reg:0.001', 63.5), ('reg:0.01', 43.82)]
2019-03-13 03:36:15,305 : Cross-validation : best param found is reg = 1e-05             with score 69.22
2019-03-13 03:36:15,305 : Evaluating...
2019-03-13 03:36:15,740 : 
Dev acc : 69.22 Test acc : 83.8             for TREC

2019-03-13 03:36:15,741 : ***** Transfer task : MRPC *****


2019-03-13 03:36:15,763 : loading BERT model bert-large-uncased
2019-03-13 03:36:15,763 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:36:15,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:36:15,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpojtnm0fz
2019-03-13 03:36:23,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:36:28,471 : Computing embedding for train
2019-03-13 03:36:50,845 : Computed train embeddings
2019-03-13 03:36:50,845 : Computing embedding for test
2019-03-13 03:37:00,670 : Computed test embeddings
2019-03-13 03:37:00,691 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 03:37:05,500 : [('reg:1e-05', 69.95), ('reg:0.0001', 69.31), ('reg:0.001', 68.84), ('reg:0.01', 67.76)]
2019-03-13 03:37:05,500 : Cross-validation : best param found is reg = 1e-05             with score 69.95
2019-03-13 03:37:05,500 : Evaluating...
2019-03-13 03:37:05,808 : Dev acc : 69.95 Test acc 64.75; Test F1 69.17 for MRPC.

2019-03-13 03:37:05,809 : ***** Transfer task : SICK-Entailment*****


2019-03-13 03:37:05,896 : loading BERT model bert-large-uncased
2019-03-13 03:37:05,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:37:05,918 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:37:05,919 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzibzzdmx
2019-03-13 03:37:13,308 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:37:18,479 : Computing embedding for train
2019-03-13 03:37:29,830 : Computed train embeddings
2019-03-13 03:37:29,830 : Computing embedding for dev
2019-03-13 03:37:31,391 : Computed dev embeddings
2019-03-13 03:37:31,391 : Computing embedding for test
2019-03-13 03:37:43,585 : Computed test embeddings
2019-03-13 03:37:43,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 03:37:45,087 : [('reg:1e-05', 63.4), ('reg:0.0001', 72.2), ('reg:0.001', 71.0), ('reg:0.01', 54.6)]
2019-03-13 03:37:45,087 : Validation : best param found is reg = 0.0001 with score             72.2
2019-03-13 03:37:45,087 : Evaluating...
2019-03-13 03:37:45,534 : 
Dev acc : 72.2 Test acc : 70.04 for                        SICK entailment

2019-03-13 03:37:45,534 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 03:37:45,561 : loading BERT model bert-large-uncased
2019-03-13 03:37:45,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:37:45,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:37:45,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpva_zzpjr
2019-03-13 03:37:53,043 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:37:58,283 : Computing embedding for train
2019-03-13 03:38:09,686 : Computed train embeddings
2019-03-13 03:38:09,687 : Computing embedding for dev
2019-03-13 03:38:11,244 : Computed dev embeddings
2019-03-13 03:38:11,244 : Computing embedding for test
2019-03-13 03:38:23,483 : Computed test embeddings
2019-03-13 03:38:47,085 : Dev : Pearson 0.7351324397056671
2019-03-13 03:38:47,085 : Test : Pearson 0.74269725026534 Spearman 0.6814633414396689 MSE 0.4570139357666656                        for SICK Relatedness

2019-03-13 03:38:47,086 : 

***** Transfer task : STSBenchmark*****


2019-03-13 03:38:47,157 : loading BERT model bert-large-uncased
2019-03-13 03:38:47,157 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:38:47,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:38:47,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzzx8mbm_
2019-03-13 03:38:54,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:39:00,076 : Computing embedding for train
2019-03-13 03:39:18,705 : Computed train embeddings
2019-03-13 03:39:18,705 : Computing embedding for dev
2019-03-13 03:39:24,383 : Computed dev embeddings
2019-03-13 03:39:24,383 : Computing embedding for test
2019-03-13 03:39:29,021 : Computed test embeddings
2019-03-13 03:39:48,246 : Dev : Pearson 0.6761202885904911
2019-03-13 03:39:48,246 : Test : Pearson 0.6056237448199885 Spearman 0.5944316643777023 MSE 1.8344076409690253                        for SICK Relatedness

2019-03-13 03:39:48,246 : ***** Transfer task : SNLI Entailment*****


2019-03-13 03:39:53,356 : loading BERT model bert-large-uncased
2019-03-13 03:39:53,356 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 03:39:53,425 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 03:39:53,425 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3xo3la3f
2019-03-13 03:40:00,839 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 03:40:06,477 : PROGRESS (encoding): 0.00%
2019-03-13 03:42:54,724 : PROGRESS (encoding): 14.56%
2019-03-13 03:46:07,065 : PROGRESS (encoding): 29.12%
2019-03-13 03:49:17,803 : PROGRESS (encoding): 43.69%
2019-03-13 03:52:41,510 : PROGRESS (encoding): 58.25%
2019-03-13 03:56:27,621 : PROGRESS (encoding): 72.81%
2019-03-13 04:00:12,430 : PROGRESS (encoding): 87.37%
2019-03-13 04:04:15,538 : PROGRESS (encoding): 0.00%
2019-03-13 04:04:46,181 : PROGRESS (encoding): 0.00%
2019-03-13 04:05:15,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:06:08,707 : [('reg:1e-09', 62.22)]
2019-03-13 04:06:08,707 : Validation : best param found is reg = 1e-09 with score             62.22
2019-03-13 04:06:08,708 : Evaluating...
2019-03-13 04:07:02,097 : Dev acc : 62.22 Test acc : 61.79 for SNLI

2019-03-13 04:07:02,098 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 04:07:02,306 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 04:07:03,371 : loading BERT model bert-large-uncased
2019-03-13 04:07:03,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:07:03,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:07:03,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi_9u8tvh
2019-03-13 04:07:10,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:07:16,127 : Computing embeddings for train/dev/test
2019-03-13 04:10:48,395 : Computed embeddings
2019-03-13 04:10:48,395 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:11:14,227 : [('reg:1e-05', 90.46), ('reg:0.0001', 86.53), ('reg:0.001', 85.02), ('reg:0.01', 70.79)]
2019-03-13 04:11:14,227 : Validation : best param found is reg = 1e-05 with score             90.46
2019-03-13 04:11:14,227 : Evaluating...
2019-03-13 04:11:20,151 : 
Dev acc : 90.5 Test acc : 90.4 for LENGTH classification

2019-03-13 04:11:20,152 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 04:11:20,406 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 04:11:20,453 : loading BERT model bert-large-uncased
2019-03-13 04:11:20,453 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:11:20,484 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:11:20,484 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpiwwywdde
2019-03-13 04:11:27,922 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:11:33,007 : Computing embeddings for train/dev/test
2019-03-13 04:14:49,526 : Computed embeddings
2019-03-13 04:14:49,526 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:15:19,504 : [('reg:1e-05', 6.75), ('reg:0.0001', 1.27), ('reg:0.001', 0.42), ('reg:0.01', 0.34)]
2019-03-13 04:15:19,504 : Validation : best param found is reg = 1e-05 with score             6.75
2019-03-13 04:15:19,505 : Evaluating...
2019-03-13 04:15:27,219 : 
Dev acc : 6.8 Test acc : 6.5 for WORDCONTENT classification

2019-03-13 04:15:27,220 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 04:15:27,753 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 04:15:27,819 : loading BERT model bert-large-uncased
2019-03-13 04:15:27,819 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:15:27,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:15:27,843 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsehjqw0v
2019-03-13 04:15:35,302 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:15:40,479 : Computing embeddings for train/dev/test
2019-03-13 04:18:45,041 : Computed embeddings
2019-03-13 04:18:45,041 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:19:07,063 : [('reg:1e-05', 30.9), ('reg:0.0001', 30.04), ('reg:0.001', 27.15), ('reg:0.01', 23.18)]
2019-03-13 04:19:07,064 : Validation : best param found is reg = 1e-05 with score             30.9
2019-03-13 04:19:07,064 : Evaluating...
2019-03-13 04:19:13,462 : 
Dev acc : 30.9 Test acc : 30.9 for DEPTH classification

2019-03-13 04:19:13,463 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 04:19:13,833 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 04:19:13,896 : loading BERT model bert-large-uncased
2019-03-13 04:19:13,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:19:14,005 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:19:14,005 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt3h434yw
2019-03-13 04:19:21,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:19:26,869 : Computing embeddings for train/dev/test
2019-03-13 04:22:17,444 : Computed embeddings
2019-03-13 04:22:17,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:22:52,752 : [('reg:1e-05', 59.44), ('reg:0.0001', 55.84), ('reg:0.001', 36.25), ('reg:0.01', 16.01)]
2019-03-13 04:22:52,752 : Validation : best param found is reg = 1e-05 with score             59.44
2019-03-13 04:22:52,752 : Evaluating...
2019-03-13 04:23:01,585 : 
Dev acc : 59.4 Test acc : 59.0 for TOPCONSTITUENTS classification

2019-03-13 04:23:01,587 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 04:23:01,965 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 04:23:02,032 : loading BERT model bert-large-uncased
2019-03-13 04:23:02,033 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:23:02,062 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:23:02,062 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv75fqko9
2019-03-13 04:23:09,544 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:23:14,857 : Computing embeddings for train/dev/test
2019-03-13 04:26:19,598 : Computed embeddings
2019-03-13 04:26:19,598 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:26:55,745 : [('reg:1e-05', 71.18), ('reg:0.0001', 71.22), ('reg:0.001', 67.85), ('reg:0.01', 65.18)]
2019-03-13 04:26:55,745 : Validation : best param found is reg = 0.0001 with score             71.22
2019-03-13 04:26:55,745 : Evaluating...
2019-03-13 04:27:04,558 : 
Dev acc : 71.2 Test acc : 70.3 for BIGRAMSHIFT classification

2019-03-13 04:27:04,559 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 04:27:04,947 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 04:27:05,013 : loading BERT model bert-large-uncased
2019-03-13 04:27:05,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:27:05,042 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:27:05,042 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp23iygohw
2019-03-13 04:27:12,507 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:27:17,780 : Computing embeddings for train/dev/test
2019-03-13 04:30:18,700 : Computed embeddings
2019-03-13 04:30:18,700 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:30:50,906 : [('reg:1e-05', 88.63), ('reg:0.0001', 88.46), ('reg:0.001', 88.31), ('reg:0.01', 86.36)]
2019-03-13 04:30:50,906 : Validation : best param found is reg = 1e-05 with score             88.63
2019-03-13 04:30:50,907 : Evaluating...
2019-03-13 04:30:58,076 : 
Dev acc : 88.6 Test acc : 86.6 for TENSE classification

2019-03-13 04:30:58,077 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 04:30:58,494 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 04:30:58,559 : loading BERT model bert-large-uncased
2019-03-13 04:30:58,559 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:30:58,681 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:30:58,681 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3qwinnkj
2019-03-13 04:31:06,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:31:11,463 : Computing embeddings for train/dev/test
2019-03-13 04:34:23,308 : Computed embeddings
2019-03-13 04:34:23,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:34:54,264 : [('reg:1e-05', 82.99), ('reg:0.0001', 82.1), ('reg:0.001', 76.32), ('reg:0.01', 74.04)]
2019-03-13 04:34:54,265 : Validation : best param found is reg = 1e-05 with score             82.99
2019-03-13 04:34:54,265 : Evaluating...
2019-03-13 04:35:03,477 : 
Dev acc : 83.0 Test acc : 82.0 for SUBJNUMBER classification

2019-03-13 04:35:03,478 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 04:35:03,886 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 04:35:03,953 : loading BERT model bert-large-uncased
2019-03-13 04:35:03,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:35:04,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:35:04,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpubrrhu0u
2019-03-13 04:35:11,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:35:16,797 : Computing embeddings for train/dev/test
2019-03-13 04:38:25,342 : Computed embeddings
2019-03-13 04:38:25,342 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:38:53,105 : [('reg:1e-05', 79.59), ('reg:0.0001', 78.95), ('reg:0.001', 75.99), ('reg:0.01', 67.2)]
2019-03-13 04:38:53,105 : Validation : best param found is reg = 1e-05 with score             79.59
2019-03-13 04:38:53,105 : Evaluating...
2019-03-13 04:39:01,203 : 
Dev acc : 79.6 Test acc : 80.2 for OBJNUMBER classification

2019-03-13 04:39:01,204 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 04:39:01,776 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 04:39:01,846 : loading BERT model bert-large-uncased
2019-03-13 04:39:01,847 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:39:01,874 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:39:01,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4petkz7n
2019-03-13 04:39:09,285 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:39:14,577 : Computing embeddings for train/dev/test
2019-03-13 04:42:52,354 : Computed embeddings
2019-03-13 04:42:52,355 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:43:21,702 : [('reg:1e-05', 60.11), ('reg:0.0001', 59.94), ('reg:0.001', 59.2), ('reg:0.01', 57.6)]
2019-03-13 04:43:21,702 : Validation : best param found is reg = 1e-05 with score             60.11
2019-03-13 04:43:21,702 : Evaluating...
2019-03-13 04:43:29,008 : 
Dev acc : 60.1 Test acc : 60.6 for ODDMANOUT classification

2019-03-13 04:43:29,009 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 04:43:29,385 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 04:43:29,466 : loading BERT model bert-large-uncased
2019-03-13 04:43:29,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:43:29,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:43:29,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr8jh9p03
2019-03-13 04:43:36,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:43:42,145 : Computing embeddings for train/dev/test
2019-03-13 04:47:17,552 : Computed embeddings
2019-03-13 04:47:17,553 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:47:35,319 : [('reg:1e-05', 56.36), ('reg:0.0001', 54.68), ('reg:0.001', 51.16), ('reg:0.01', 50.68)]
2019-03-13 04:47:35,319 : Validation : best param found is reg = 1e-05 with score             56.36
2019-03-13 04:47:35,319 : Evaluating...
2019-03-13 04:47:41,295 : 
Dev acc : 56.4 Test acc : 56.2 for COORDINATIONINVERSION classification

2019-03-13 04:47:41,297 : total results: {'STS12': {'MSRpar': {'pearson': (0.2190609285706529, 1.3359317647472424e-09), 'spearman': SpearmanrResult(correlation=0.29077487761903825, pvalue=4.431743643505891e-16), 'nsamples': 750}, 'MSRvid': {'pearson': (0.325704626761493, 5.399210932576199e-20), 'spearman': SpearmanrResult(correlation=0.3598004327133883, pvalue=2.4357800762274446e-24), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.44568095647668604, 8.803478331120554e-24), 'spearman': SpearmanrResult(correlation=0.528743561058119, pvalue=2.0223301093397724e-34), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.40216365590232245, 1.5921770358570003e-30), 'spearman': SpearmanrResult(correlation=0.4275130814270789, pvalue=1.1184620520551843e-34), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5222681324530764, 2.6551131761060074e-29), 'spearman': SpearmanrResult(correlation=0.5040044148296954, pvalue=4.29527519615025e-27), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3829756600328461, 'wmean': 0.361373697650395}, 'spearman': {'mean': 0.4221672735294639, 'wmean': 0.4029463802645927}}}, 'STS13': {'FNWN': {'pearson': (0.1811088914198581, 0.012632061171189245), 'spearman': SpearmanrResult(correlation=0.23114321921929284, pvalue=0.001373985775739723), 'nsamples': 189}, 'headlines': {'pearson': (0.3644941817834396, 5.566326213773956e-25), 'spearman': SpearmanrResult(correlation=0.44777497985523984, pvalue=2.918219719123862e-38), 'nsamples': 750}, 'OnWN': {'pearson': (0.13754731246208693, 0.0010902488474415736), 'spearman': SpearmanrResult(correlation=0.19222246247099634, pvalue=4.527952778342816e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2277167952217949, 'wmean': 0.2565095060714424}, 'spearman': {'mean': 0.29038022051517637, 'wmean': 0.32490273651340346}}}, 'STS14': {'deft-forum': {'pearson': (0.15741195614210932, 0.0008056756238826406), 'spearman': SpearmanrResult(correlation=0.21035789481354886, pvalue=6.782335733452131e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.4779408712356507, 1.5829584325419229e-18), 'spearman': SpearmanrResult(correlation=0.5388993063228541, pvalue=5.36268789216653e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.27199838477985944, 3.4576326622034496e-14), 'spearman': SpearmanrResult(correlation=0.3560307432560915, pvalue=7.831078347890471e-24), 'nsamples': 750}, 'images': {'pearson': (0.38242039202050016, 1.576004077592218e-27), 'spearman': SpearmanrResult(correlation=0.44824126283160665, pvalue=2.397562137131097e-38), 'nsamples': 750}, 'OnWN': {'pearson': (0.16184436248622738, 8.416915456249296e-06), 'spearman': SpearmanrResult(correlation=0.21582784452755902, pvalue=2.353041378512566e-09), 'nsamples': 750}, 'tweet-news': {'pearson': (0.2836124800094201, 2.4287328376933153e-15), 'spearman': SpearmanrResult(correlation=0.33969563187297086, pvalue=1.0328550803747817e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2892047411122945, 'wmean': 0.2770998282951066}, 'spearman': {'mean': 0.3515087806041051, 'wmean': 0.34031398838109983}}}, 'STS15': {'answers-forums': {'pearson': (0.23515114256422898, 4.160746683244391e-06), 'spearman': SpearmanrResult(correlation=0.30077633048909486, pvalue=2.7944302279828024e-09), 'nsamples': 375}, 'answers-students': {'pearson': (0.4274210117560395, 1.1597306488024884e-34), 'spearman': SpearmanrResult(correlation=0.5058935317564733, pvalue=5.534205250733522e-50), 'nsamples': 750}, 'belief': {'pearson': (0.30643676673893955, 1.3550422498292317e-09), 'spearman': SpearmanrResult(correlation=0.3800059545274453, pvalue=2.497262584944605e-14), 'nsamples': 375}, 'headlines': {'pearson': (0.5060649894262078, 5.070251208299342e-50), 'spearman': SpearmanrResult(correlation=0.546807896462377, pvalue=1.0579408607555427e-59), 'nsamples': 750}, 'images': {'pearson': (0.3276294684262603, 3.1708041396811603e-20), 'spearman': SpearmanrResult(correlation=0.43708766932377224, pvalue=2.4290841882802345e-36), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3605406757823353, 'wmean': 0.382977356065023}, 'spearman': {'mean': 0.43411427651183254, 'wmean': 0.45754506001272316}}}, 'STS16': {'answer-answer': {'pearson': (0.3202082093722206, 1.8273656014196923e-07), 'spearman': SpearmanrResult(correlation=0.3863476634101688, pvalue=1.8142151273875058e-10), 'nsamples': 254}, 'headlines': {'pearson': (0.5267443472663734, 3.5366764643372746e-19), 'spearman': SpearmanrResult(correlation=0.6036127478639394, pvalue=4.1588227586348846e-26), 'nsamples': 249}, 'plagiarism': {'pearson': (0.4020550954377747, 2.396100048479737e-10), 'spearman': SpearmanrResult(correlation=0.5875150780814007, pvalue=9.734696033089196e-23), 'nsamples': 230}, 'postediting': {'pearson': (0.5474925078280406, 1.7495065801112175e-20), 'spearman': SpearmanrResult(correlation=0.7279526105783314, pvalue=1.5109806260317802e-41), 'nsamples': 244}, 'question-question': {'pearson': (0.2027937217353158, 0.003231296080070906), 'spearman': SpearmanrResult(correlation=0.21307186726725633, pvalue=0.0019514835257339283), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.399858776327945, 'wmean': 0.4055117701123795}, 'spearman': {'mean': 0.5036999934402193, 'wmean': 0.510719060643335}}}, 'MR': {'devacc': 72.78, 'acc': 71.11, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.92, 'acc': 75.02, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.16, 'acc': 83.7, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.69, 'acc': 93.4, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.13, 'acc': 81.11, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.87, 'acc': 42.4, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 69.22, 'acc': 83.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.95, 'acc': 64.75, 'f1': 69.17, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 70.04, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7351324397056671, 'pearson': 0.74269725026534, 'spearman': 0.6814633414396689, 'mse': 0.4570139357666656, 'yhat': array([2.28194753, 4.14482321, 1.67520142, ..., 2.81307642, 4.5338268 ,        4.13052366]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6761202885904911, 'pearson': 0.6056237448199885, 'spearman': 0.5944316643777023, 'mse': 1.8344076409690253, 'yhat': array([2.01084363, 1.52071657, 2.48907574, ..., 3.78377044, 3.4637244 ,        3.27442376]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.22, 'acc': 61.79, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 90.46, 'acc': 90.44, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 6.75, 'acc': 6.53, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.9, 'acc': 30.85, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 59.44, 'acc': 59.01, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 71.22, 'acc': 70.32, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.63, 'acc': 86.63, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.99, 'acc': 81.98, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.59, 'acc': 80.18, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.11, 'acc': 60.61, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.36, 'acc': 56.17, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 04:47:41,297 : STS12 p=0.3614, STS12 s=0.4029, STS13 p=0.2565, STS13 s=0.3249, STS14 p=0.2771, STS14 s=0.3403, STS15 p=0.3830, STS15 s=0.4575, STS 16 p=0.4055, STS16 s=0.5107, STS B p=0.6056, STS B s=0.5944, STS B m=1.8344, SICK-R p=0.7427, SICK-R s=0.6815, SICK-P m=0.4570
2019-03-13 04:47:41,297 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 04:47:41,297 : 0.3614,0.4029,0.2565,0.3249,0.2771,0.3403,0.3830,0.4575,0.4055,0.5107,0.6056,0.5944,1.8344,0.7427,0.6815,0.4570
2019-03-13 04:47:41,297 : MR=71.11, CR=75.02, SUBJ=93.40, MPQA=83.70, SST-B=81.11, SST-F=42.40, TREC=83.80, SICK-E=70.04, SNLI=61.79, MRPC=64.75, MRPC f=69.17
2019-03-13 04:47:41,297 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 04:47:41,297 : 71.11,75.02,93.40,83.70,81.11,42.40,83.80,70.04,61.79,64.75,69.17
2019-03-13 04:47:41,297 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 04:47:41,297 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 04:47:41,297 : na,na,na,na,na,na,na,na,na,na
2019-03-13 04:47:41,297 : SentLen=90.44, WC=6.53, TreeDepth=30.85, TopConst=59.01, BShift=70.32, Tense=86.63, SubjNum=81.98, ObjNum=80.18, SOMO=60.61, CoordInv=56.17, average=62.27
2019-03-13 04:47:41,297 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 04:47:41,297 : 90.44,6.53,30.85,59.01,70.32,86.63,81.98,80.18,60.61,56.17,62.27
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,297 : layer 11
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,297 : ********************************************************************************
2019-03-13 04:47:41,390 : ***** Transfer task : STS12 *****


2019-03-13 04:47:41,402 : loading BERT model bert-large-uncased
2019-03-13 04:47:41,402 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:47:41,419 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:47:41,419 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvw9hfmxm
2019-03-13 04:47:48,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:47:58,222 : MSRpar : pearson = 0.2052, spearman = 0.2853
2019-03-13 04:47:59,874 : MSRvid : pearson = 0.3277, spearman = 0.3732
2019-03-13 04:48:01,296 : SMTeuroparl : pearson = 0.4405, spearman = 0.5366
2019-03-13 04:48:04,012 : surprise.OnWN : pearson = 0.3089, spearman = 0.3651
2019-03-13 04:48:05,449 : surprise.SMTnews : pearson = 0.4503, spearman = 0.4745
2019-03-13 04:48:05,449 : ALL (weighted average) : Pearson = 0.3260,             Spearman = 0.3872
2019-03-13 04:48:05,449 : ALL (average) : Pearson = 0.3465,             Spearman = 0.4069

2019-03-13 04:48:05,449 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 04:48:05,457 : loading BERT model bert-large-uncased
2019-03-13 04:48:05,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:48:05,475 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:48:05,475 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplgz5nahq
2019-03-13 04:48:12,882 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:48:19,427 : FNWN : pearson = 0.1620, spearman = 0.2399
2019-03-13 04:48:21,325 : headlines : pearson = 0.3192, spearman = 0.4399
2019-03-13 04:48:22,800 : OnWN : pearson = 0.1241, spearman = 0.1826
2019-03-13 04:48:22,800 : ALL (weighted average) : Pearson = 0.2264,             Spearman = 0.3185
2019-03-13 04:48:22,800 : ALL (average) : Pearson = 0.2018,             Spearman = 0.2875

2019-03-13 04:48:22,800 : ***** Transfer task : STS14 *****


2019-03-13 04:48:22,815 : loading BERT model bert-large-uncased
2019-03-13 04:48:22,816 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:48:22,833 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:48:22,833 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd24ica89
2019-03-13 04:48:30,241 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:48:36,727 : deft-forum : pearson = 0.1573, spearman = 0.2233
2019-03-13 04:48:38,372 : deft-news : pearson = 0.4552, spearman = 0.5390
2019-03-13 04:48:40,550 : headlines : pearson = 0.2139, spearman = 0.3414
2019-03-13 04:48:42,637 : images : pearson = 0.4352, spearman = 0.5054
2019-03-13 04:48:44,775 : OnWN : pearson = 0.1213, spearman = 0.2125
2019-03-13 04:48:47,647 : tweet-news : pearson = 0.2485, spearman = 0.2947
2019-03-13 04:48:47,647 : ALL (weighted average) : Pearson = 0.2591,             Spearman = 0.3407
2019-03-13 04:48:47,647 : ALL (average) : Pearson = 0.2719,             Spearman = 0.3527

2019-03-13 04:48:47,647 : ***** Transfer task : STS15 *****


2019-03-13 04:48:47,679 : loading BERT model bert-large-uncased
2019-03-13 04:48:47,679 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:48:47,697 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:48:47,697 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptfj92bu5
2019-03-13 04:48:55,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:49:02,172 : answers-forums : pearson = 0.2103, spearman = 0.2750
2019-03-13 04:49:04,264 : answers-students : pearson = 0.4123, spearman = 0.5110
2019-03-13 04:49:06,323 : belief : pearson = 0.3458, spearman = 0.4317
2019-03-13 04:49:08,582 : headlines : pearson = 0.4493, spearman = 0.5199
2019-03-13 04:49:10,722 : images : pearson = 0.2462, spearman = 0.4425
2019-03-13 04:49:10,723 : ALL (weighted average) : Pearson = 0.3465,             Spearman = 0.4567
2019-03-13 04:49:10,723 : ALL (average) : Pearson = 0.3328,             Spearman = 0.4360

2019-03-13 04:49:10,723 : ***** Transfer task : STS16 *****


2019-03-13 04:49:10,793 : loading BERT model bert-large-uncased
2019-03-13 04:49:10,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:49:10,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:49:10,811 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprxbtrfv6
2019-03-13 04:49:18,243 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:49:24,403 : answer-answer : pearson = 0.3100, spearman = 0.4126
2019-03-13 04:49:25,065 : headlines : pearson = 0.4166, spearman = 0.5640
2019-03-13 04:49:25,948 : plagiarism : pearson = 0.3904, spearman = 0.6187
2019-03-13 04:49:27,448 : postediting : pearson = 0.5228, spearman = 0.7375
2019-03-13 04:49:28,056 : question-question : pearson = 0.1873, spearman = 0.2337
2019-03-13 04:49:28,057 : ALL (weighted average) : Pearson = 0.3701,             Spearman = 0.5197
2019-03-13 04:49:28,057 : ALL (average) : Pearson = 0.3654,             Spearman = 0.5133

2019-03-13 04:49:28,057 : ***** Transfer task : MR *****


2019-03-13 04:49:28,072 : loading BERT model bert-large-uncased
2019-03-13 04:49:28,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:49:28,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:49:28,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq1jvmnu5
2019-03-13 04:49:35,559 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:49:40,745 : Generating sentence embeddings
2019-03-13 04:50:12,313 : Generated sentence embeddings
2019-03-13 04:50:12,314 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:50:23,796 : Best param found at split 1: l2reg = 0.0001                 with score 75.02
2019-03-13 04:50:34,401 : Best param found at split 2: l2reg = 0.0001                 with score 73.95
2019-03-13 04:50:45,608 : Best param found at split 3: l2reg = 1e-05                 with score 74.72
2019-03-13 04:50:56,531 : Best param found at split 4: l2reg = 1e-05                 with score 73.63
2019-03-13 04:51:06,644 : Best param found at split 5: l2reg = 0.0001                 with score 73.39
2019-03-13 04:51:07,245 : Dev acc : 74.14 Test acc : 73.76

2019-03-13 04:51:07,246 : ***** Transfer task : CR *****


2019-03-13 04:51:07,254 : loading BERT model bert-large-uncased
2019-03-13 04:51:07,254 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:51:07,274 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:51:07,274 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppasl8ceo
2019-03-13 04:51:14,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:51:19,868 : Generating sentence embeddings
2019-03-13 04:51:28,166 : Generated sentence embeddings
2019-03-13 04:51:28,167 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:51:31,961 : Best param found at split 1: l2reg = 1e-05                 with score 76.71
2019-03-13 04:51:35,874 : Best param found at split 2: l2reg = 0.0001                 with score 75.55
2019-03-13 04:51:39,762 : Best param found at split 3: l2reg = 1e-05                 with score 76.79
2019-03-13 04:51:43,894 : Best param found at split 4: l2reg = 1e-05                 with score 76.07
2019-03-13 04:51:47,894 : Best param found at split 5: l2reg = 0.0001                 with score 77.09
2019-03-13 04:51:48,147 : Dev acc : 76.44 Test acc : 76.4

2019-03-13 04:51:48,147 : ***** Transfer task : MPQA *****


2019-03-13 04:51:48,153 : loading BERT model bert-large-uncased
2019-03-13 04:51:48,154 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:51:48,202 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:51:48,203 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsqlh49vi
2019-03-13 04:51:55,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:52:00,766 : Generating sentence embeddings
2019-03-13 04:52:08,334 : Generated sentence embeddings
2019-03-13 04:52:08,335 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:52:16,891 : Best param found at split 1: l2reg = 1e-05                 with score 79.93
2019-03-13 04:52:26,188 : Best param found at split 2: l2reg = 1e-05                 with score 79.9
2019-03-13 04:52:35,009 : Best param found at split 3: l2reg = 0.001                 with score 79.85
2019-03-13 04:52:42,928 : Best param found at split 4: l2reg = 0.0001                 with score 81.4
2019-03-13 04:52:54,468 : Best param found at split 5: l2reg = 1e-05                 with score 81.99
2019-03-13 04:52:55,323 : Dev acc : 80.61 Test acc : 84.21

2019-03-13 04:52:55,324 : ***** Transfer task : SUBJ *****


2019-03-13 04:52:55,340 : loading BERT model bert-large-uncased
2019-03-13 04:52:55,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:52:55,359 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:52:55,359 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4prnqco4
2019-03-13 04:53:02,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:53:08,019 : Generating sentence embeddings
2019-03-13 04:53:38,999 : Generated sentence embeddings
2019-03-13 04:53:39,000 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 04:53:49,099 : Best param found at split 1: l2reg = 1e-05                 with score 93.21
2019-03-13 04:54:00,542 : Best param found at split 2: l2reg = 1e-05                 with score 93.68
2019-03-13 04:54:12,125 : Best param found at split 3: l2reg = 1e-05                 with score 93.39
2019-03-13 04:54:23,105 : Best param found at split 4: l2reg = 1e-05                 with score 93.8
2019-03-13 04:54:34,766 : Best param found at split 5: l2reg = 1e-05                 with score 93.53
2019-03-13 04:54:35,577 : Dev acc : 93.52 Test acc : 93.1

2019-03-13 04:54:35,578 : ***** Transfer task : SST Binary classification *****


2019-03-13 04:54:35,669 : loading BERT model bert-large-uncased
2019-03-13 04:54:35,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:54:35,744 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:54:35,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0v8thy7p
2019-03-13 04:54:43,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:54:48,516 : Computing embedding for train
2019-03-13 04:56:29,262 : Computed train embeddings
2019-03-13 04:56:29,262 : Computing embedding for dev
2019-03-13 04:56:31,467 : Computed dev embeddings
2019-03-13 04:56:31,467 : Computing embedding for test
2019-03-13 04:56:36,101 : Computed test embeddings
2019-03-13 04:56:36,101 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:56:54,719 : [('reg:1e-05', 77.75), ('reg:0.0001', 78.67), ('reg:0.001', 77.29), ('reg:0.01', 73.17)]
2019-03-13 04:56:54,719 : Validation : best param found is reg = 0.0001 with score             78.67
2019-03-13 04:56:54,719 : Evaluating...
2019-03-13 04:56:59,697 : 
Dev acc : 78.67 Test acc : 79.02 for             SST Binary classification

2019-03-13 04:56:59,697 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 04:56:59,752 : loading BERT model bert-large-uncased
2019-03-13 04:56:59,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:56:59,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:56:59,789 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp709ey8q2
2019-03-13 04:57:07,204 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:57:12,389 : Computing embedding for train
2019-03-13 04:57:34,415 : Computed train embeddings
2019-03-13 04:57:34,415 : Computing embedding for dev
2019-03-13 04:57:37,298 : Computed dev embeddings
2019-03-13 04:57:37,298 : Computing embedding for test
2019-03-13 04:57:42,984 : Computed test embeddings
2019-03-13 04:57:42,984 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:57:45,645 : [('reg:1e-05', 39.87), ('reg:0.0001', 38.96), ('reg:0.001', 38.15), ('reg:0.01', 33.51)]
2019-03-13 04:57:45,645 : Validation : best param found is reg = 1e-05 with score             39.87
2019-03-13 04:57:45,645 : Evaluating...
2019-03-13 04:57:46,358 : 
Dev acc : 39.87 Test acc : 42.9 for             SST Fine-Grained classification

2019-03-13 04:57:46,358 : ***** Transfer task : TREC *****


2019-03-13 04:57:46,373 : loading BERT model bert-large-uncased
2019-03-13 04:57:46,373 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:57:46,393 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:57:46,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5qhmfj1k
2019-03-13 04:57:53,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:58:06,504 : Computed train embeddings
2019-03-13 04:58:07,097 : Computed test embeddings
2019-03-13 04:58:07,097 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:58:12,833 : [('reg:1e-05', 69.54), ('reg:0.0001', 68.56), ('reg:0.001', 55.67), ('reg:0.01', 38.77)]
2019-03-13 04:58:12,834 : Cross-validation : best param found is reg = 1e-05             with score 69.54
2019-03-13 04:58:12,834 : Evaluating...
2019-03-13 04:58:13,244 : 
Dev acc : 69.54 Test acc : 82.2             for TREC

2019-03-13 04:58:13,244 : ***** Transfer task : MRPC *****


2019-03-13 04:58:13,265 : loading BERT model bert-large-uncased
2019-03-13 04:58:13,265 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:58:13,289 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:58:13,289 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdfaxd7vl
2019-03-13 04:58:20,713 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:58:25,788 : Computing embedding for train
2019-03-13 04:58:48,163 : Computed train embeddings
2019-03-13 04:58:48,163 : Computing embedding for test
2019-03-13 04:58:57,973 : Computed test embeddings
2019-03-13 04:58:57,993 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 04:59:02,405 : [('reg:1e-05', 67.79), ('reg:0.0001', 69.11), ('reg:0.001', 68.87), ('reg:0.01', 68.23)]
2019-03-13 04:59:02,405 : Cross-validation : best param found is reg = 0.0001             with score 69.11
2019-03-13 04:59:02,405 : Evaluating...
2019-03-13 04:59:02,712 : Dev acc : 69.11 Test acc 66.49; Test F1 79.87 for MRPC.

2019-03-13 04:59:02,713 : ***** Transfer task : SICK-Entailment*****


2019-03-13 04:59:02,775 : loading BERT model bert-large-uncased
2019-03-13 04:59:02,775 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:59:02,794 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:59:02,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphztjayt_
2019-03-13 04:59:10,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:59:15,576 : Computing embedding for train
2019-03-13 04:59:26,904 : Computed train embeddings
2019-03-13 04:59:26,904 : Computing embedding for dev
2019-03-13 04:59:28,455 : Computed dev embeddings
2019-03-13 04:59:28,455 : Computing embedding for test
2019-03-13 04:59:40,631 : Computed test embeddings
2019-03-13 04:59:40,668 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 04:59:42,386 : [('reg:1e-05', 71.2), ('reg:0.0001', 71.6), ('reg:0.001', 65.8), ('reg:0.01', 58.0)]
2019-03-13 04:59:42,387 : Validation : best param found is reg = 0.0001 with score             71.6
2019-03-13 04:59:42,387 : Evaluating...
2019-03-13 04:59:42,886 : 
Dev acc : 71.6 Test acc : 70.69 for                        SICK entailment

2019-03-13 04:59:42,886 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 04:59:42,913 : loading BERT model bert-large-uncased
2019-03-13 04:59:42,913 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 04:59:42,970 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 04:59:42,970 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp83bu72l0
2019-03-13 04:59:50,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 04:59:55,720 : Computing embedding for train
2019-03-13 05:00:07,098 : Computed train embeddings
2019-03-13 05:00:07,098 : Computing embedding for dev
2019-03-13 05:00:08,670 : Computed dev embeddings
2019-03-13 05:00:08,670 : Computing embedding for test
2019-03-13 05:00:20,880 : Computed test embeddings
2019-03-13 05:00:45,136 : Dev : Pearson 0.7222266040857914
2019-03-13 05:00:45,136 : Test : Pearson 0.725759870690953 Spearman 0.6620217405879729 MSE 0.4825431569147064                        for SICK Relatedness

2019-03-13 05:00:45,137 : 

***** Transfer task : STSBenchmark*****


2019-03-13 05:00:45,174 : loading BERT model bert-large-uncased
2019-03-13 05:00:45,175 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:00:45,203 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:00:45,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps2tqoqwq
2019-03-13 05:00:52,670 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:00:57,888 : Computing embedding for train
2019-03-13 05:01:16,534 : Computed train embeddings
2019-03-13 05:01:16,534 : Computing embedding for dev
2019-03-13 05:01:22,203 : Computed dev embeddings
2019-03-13 05:01:22,203 : Computing embedding for test
2019-03-13 05:01:26,841 : Computed test embeddings
2019-03-13 05:01:47,148 : Dev : Pearson 0.6668943186738171
2019-03-13 05:01:47,148 : Test : Pearson 0.5863837623145278 Spearman 0.572597327405141 MSE 1.7810623062741078                        for SICK Relatedness

2019-03-13 05:01:47,148 : ***** Transfer task : SNLI Entailment*****


2019-03-13 05:01:52,269 : loading BERT model bert-large-uncased
2019-03-13 05:01:52,269 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:01:52,406 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:01:52,406 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp99wk2luo
2019-03-13 05:01:59,842 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:02:05,485 : PROGRESS (encoding): 0.00%
2019-03-13 05:04:53,412 : PROGRESS (encoding): 14.56%
2019-03-13 05:08:05,803 : PROGRESS (encoding): 29.12%
2019-03-13 05:11:16,838 : PROGRESS (encoding): 43.69%
2019-03-13 05:14:40,061 : PROGRESS (encoding): 58.25%
2019-03-13 05:18:25,785 : PROGRESS (encoding): 72.81%
2019-03-13 05:22:10,443 : PROGRESS (encoding): 87.37%
2019-03-13 05:26:13,989 : PROGRESS (encoding): 0.00%
2019-03-13 05:26:44,622 : PROGRESS (encoding): 0.00%
2019-03-13 05:27:14,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:27:43,581 : [('reg:1e-09', 60.59)]
2019-03-13 05:27:43,581 : Validation : best param found is reg = 1e-09 with score             60.59
2019-03-13 05:27:43,581 : Evaluating...
2019-03-13 05:28:14,201 : Dev acc : 60.59 Test acc : 60.2 for SNLI

2019-03-13 05:28:14,201 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 05:28:14,406 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 05:28:15,481 : loading BERT model bert-large-uncased
2019-03-13 05:28:15,481 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:28:15,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:28:15,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpke1ujtth
2019-03-13 05:28:22,959 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:28:28,124 : Computing embeddings for train/dev/test
2019-03-13 05:32:00,808 : Computed embeddings
2019-03-13 05:32:00,808 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:32:27,968 : [('reg:1e-05', 90.39), ('reg:0.0001', 86.95), ('reg:0.001', 82.58), ('reg:0.01', 58.44)]
2019-03-13 05:32:27,968 : Validation : best param found is reg = 1e-05 with score             90.39
2019-03-13 05:32:27,968 : Evaluating...
2019-03-13 05:32:35,510 : 
Dev acc : 90.4 Test acc : 91.3 for LENGTH classification

2019-03-13 05:32:35,511 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 05:32:35,889 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 05:32:35,935 : loading BERT model bert-large-uncased
2019-03-13 05:32:35,935 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:32:35,965 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:32:35,966 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps5x23omz
2019-03-13 05:32:43,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:32:48,750 : Computing embeddings for train/dev/test
2019-03-13 05:36:05,352 : Computed embeddings
2019-03-13 05:36:05,353 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:36:38,013 : [('reg:1e-05', 5.68), ('reg:0.0001', 0.76), ('reg:0.001', 0.22), ('reg:0.01', 0.19)]
2019-03-13 05:36:38,013 : Validation : best param found is reg = 1e-05 with score             5.68
2019-03-13 05:36:38,013 : Evaluating...
2019-03-13 05:36:45,256 : 
Dev acc : 5.7 Test acc : 5.9 for WORDCONTENT classification

2019-03-13 05:36:45,257 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 05:36:45,638 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 05:36:45,705 : loading BERT model bert-large-uncased
2019-03-13 05:36:45,705 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:36:45,730 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:36:45,730 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzbr68ndn
2019-03-13 05:36:53,148 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:36:58,418 : Computing embeddings for train/dev/test
2019-03-13 05:40:03,213 : Computed embeddings
2019-03-13 05:40:03,213 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:40:22,348 : [('reg:1e-05', 31.36), ('reg:0.0001', 29.87), ('reg:0.001', 23.2), ('reg:0.01', 18.07)]
2019-03-13 05:40:22,348 : Validation : best param found is reg = 1e-05 with score             31.36
2019-03-13 05:40:22,348 : Evaluating...
2019-03-13 05:40:28,201 : 
Dev acc : 31.4 Test acc : 31.7 for DEPTH classification

2019-03-13 05:40:28,202 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 05:40:28,585 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 05:40:28,648 : loading BERT model bert-large-uncased
2019-03-13 05:40:28,648 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:40:28,755 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:40:28,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw6yuexzy
2019-03-13 05:40:36,202 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:40:41,521 : Computing embeddings for train/dev/test
2019-03-13 05:43:32,305 : Computed embeddings
2019-03-13 05:43:32,305 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:44:02,285 : [('reg:1e-05', 63.55), ('reg:0.0001', 52.33), ('reg:0.001', 25.32), ('reg:0.01', 13.03)]
2019-03-13 05:44:02,285 : Validation : best param found is reg = 1e-05 with score             63.55
2019-03-13 05:44:02,285 : Evaluating...
2019-03-13 05:44:08,403 : 
Dev acc : 63.5 Test acc : 63.1 for TOPCONSTITUENTS classification

2019-03-13 05:44:08,404 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 05:44:08,751 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 05:44:08,817 : loading BERT model bert-large-uncased
2019-03-13 05:44:08,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:44:08,939 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:44:08,939 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuo1xn7j9
2019-03-13 05:44:16,391 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:44:21,580 : Computing embeddings for train/dev/test
2019-03-13 05:47:26,211 : Computed embeddings
2019-03-13 05:47:26,211 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:47:59,959 : [('reg:1e-05', 77.46), ('reg:0.0001', 76.28), ('reg:0.001', 72.82), ('reg:0.01', 64.63)]
2019-03-13 05:47:59,959 : Validation : best param found is reg = 1e-05 with score             77.46
2019-03-13 05:47:59,959 : Evaluating...
2019-03-13 05:48:09,720 : 
Dev acc : 77.5 Test acc : 77.0 for BIGRAMSHIFT classification

2019-03-13 05:48:09,721 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 05:48:10,294 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 05:48:10,361 : loading BERT model bert-large-uncased
2019-03-13 05:48:10,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:48:10,391 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:48:10,391 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsmwifmhp
2019-03-13 05:48:17,841 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:48:23,096 : Computing embeddings for train/dev/test
2019-03-13 05:51:23,982 : Computed embeddings
2019-03-13 05:51:23,982 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:51:52,817 : [('reg:1e-05', 88.61), ('reg:0.0001', 88.59), ('reg:0.001', 87.48), ('reg:0.01', 81.32)]
2019-03-13 05:51:52,817 : Validation : best param found is reg = 1e-05 with score             88.61
2019-03-13 05:51:52,818 : Evaluating...
2019-03-13 05:52:00,184 : 
Dev acc : 88.6 Test acc : 86.5 for TENSE classification

2019-03-13 05:52:00,186 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 05:52:00,602 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 05:52:00,666 : loading BERT model bert-large-uncased
2019-03-13 05:52:00,666 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:52:00,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:52:00,691 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpozthtbog
2019-03-13 05:52:08,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:52:13,370 : Computing embeddings for train/dev/test
2019-03-13 05:55:25,287 : Computed embeddings
2019-03-13 05:55:25,287 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:55:53,670 : [('reg:1e-05', 82.92), ('reg:0.0001', 82.38), ('reg:0.001', 78.7), ('reg:0.01', 72.69)]
2019-03-13 05:55:53,670 : Validation : best param found is reg = 1e-05 with score             82.92
2019-03-13 05:55:53,670 : Evaluating...
2019-03-13 05:56:02,186 : 
Dev acc : 82.9 Test acc : 81.4 for SUBJNUMBER classification

2019-03-13 05:56:02,187 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 05:56:02,600 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 05:56:02,667 : loading BERT model bert-large-uncased
2019-03-13 05:56:02,667 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 05:56:02,786 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 05:56:02,787 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8faxrx9c
2019-03-13 05:56:10,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 05:56:15,579 : Computing embeddings for train/dev/test
2019-03-13 05:59:23,908 : Computed embeddings
2019-03-13 05:59:23,908 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 05:59:55,888 : [('reg:1e-05', 80.26), ('reg:0.0001', 78.55), ('reg:0.001', 73.25), ('reg:0.01', 60.5)]
2019-03-13 05:59:55,888 : Validation : best param found is reg = 1e-05 with score             80.26
2019-03-13 05:59:55,888 : Evaluating...
2019-03-13 06:00:07,927 : 
Dev acc : 80.3 Test acc : 80.9 for OBJNUMBER classification

2019-03-13 06:00:07,928 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 06:00:08,519 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 06:00:08,588 : loading BERT model bert-large-uncased
2019-03-13 06:00:08,588 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:00:08,615 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:00:08,615 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxgzwi78g
2019-03-13 06:00:16,087 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:00:21,300 : Computing embeddings for train/dev/test
2019-03-13 06:03:58,681 : Computed embeddings
2019-03-13 06:03:58,681 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:04:19,637 : [('reg:1e-05', 61.14), ('reg:0.0001', 59.87), ('reg:0.001', 56.59), ('reg:0.01', 50.19)]
2019-03-13 06:04:19,638 : Validation : best param found is reg = 1e-05 with score             61.14
2019-03-13 06:04:19,638 : Evaluating...
2019-03-13 06:04:26,597 : 
Dev acc : 61.1 Test acc : 61.0 for ODDMANOUT classification

2019-03-13 06:04:26,599 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 06:04:26,987 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 06:04:27,064 : loading BERT model bert-large-uncased
2019-03-13 06:04:27,064 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:04:27,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:04:27,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp31l64d7_
2019-03-13 06:04:34,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:04:39,918 : Computing embeddings for train/dev/test
2019-03-13 06:08:15,688 : Computed embeddings
2019-03-13 06:08:15,688 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:08:45,608 : [('reg:1e-05', 60.83), ('reg:0.0001', 59.87), ('reg:0.001', 57.04), ('reg:0.01', 52.58)]
2019-03-13 06:08:45,609 : Validation : best param found is reg = 1e-05 with score             60.83
2019-03-13 06:08:45,609 : Evaluating...
2019-03-13 06:08:55,592 : 
Dev acc : 60.8 Test acc : 60.7 for COORDINATIONINVERSION classification

2019-03-13 06:08:55,595 : total results: {'STS12': {'MSRpar': {'pearson': (0.20518868149960218, 1.424714494875699e-08), 'spearman': SpearmanrResult(correlation=0.2852618451922406, pvalue=1.648624881468519e-15), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3277264775772152, 3.086564859208051e-20), 'spearman': SpearmanrResult(correlation=0.3731712091732865, pvalue=3.4053617438359837e-26), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.440514835581353, 3.272885757044067e-23), 'spearman': SpearmanrResult(correlation=0.5365921479081419, pvalue=1.3837804927334406e-35), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3088832923936703, 4.818244680977954e-18), 'spearman': SpearmanrResult(correlation=0.3651124069892649, pvalue=4.574417851540274e-25), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.45028771465920303, 2.5712205955647693e-21), 'spearman': SpearmanrResult(correlation=0.4744608931680737, pvalue=8.633825824470404e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.34652020034220876, 'wmean': 0.32600062621741593}, 'spearman': {'mean': 0.4069197004862016, 'wmean': 0.38715083277992035}}}, 'STS13': {'FNWN': {'pearson': (0.16196178437680273, 0.02597692528954906), 'spearman': SpearmanrResult(correlation=0.23993931624040774, pvalue=0.0008827572598081654), 'nsamples': 189}, 'headlines': {'pearson': (0.319209276106911, 3.163925109523364e-19), 'spearman': SpearmanrResult(correlation=0.4399128037013469, pvalue=7.66380413597375e-37), 'nsamples': 750}, 'OnWN': {'pearson': (0.12408892880741586, 0.0032407234329335658), 'spearman': SpearmanrResult(correlation=0.18263261918259568, pvalue=1.34442595075136e-05), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.20175332976370985, 'wmean': 0.2264210822589062}, 'spearman': {'mean': 0.2874949130414501, 'wmean': 0.31849335527125555}}}, 'STS14': {'deft-forum': {'pearson': (0.15732042402928115, 0.0008114348984903932), 'spearman': SpearmanrResult(correlation=0.2233181417651128, pvalue=1.712648930130404e-06), 'nsamples': 450}, 'deft-news': {'pearson': (0.4552228346851741, 9.418497978753315e-17), 'spearman': SpearmanrResult(correlation=0.539008093122694, pvalue=5.231212392071808e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.21393019876308278, 3.2669650755052705e-09), 'spearman': SpearmanrResult(correlation=0.34137210832383424, pvalue=6.341083469791495e-22), 'nsamples': 750}, 'images': {'pearson': (0.43522771684723954, 5.16071894307152e-36), 'spearman': SpearmanrResult(correlation=0.5053862786075646, pvalue=7.168479679750844e-50), 'nsamples': 750}, 'OnWN': {'pearson': (0.12125090127791922, 0.0008768001880637697), 'spearman': SpearmanrResult(correlation=0.21248177634034726, pvalue=4.188299813168791e-09), 'nsamples': 750}, 'tweet-news': {'pearson': (0.24853393283123792, 5.0687998270607075e-12), 'spearman': SpearmanrResult(correlation=0.2947311174304614, pvalue=1.6957497409309573e-16), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2719143347389891, 'wmean': 0.25908482760222357}, 'spearman': {'mean': 0.35271625259833567, 'wmean': 0.34071308060207056}}}, 'STS15': {'answers-forums': {'pearson': (0.21031699676607454, 4.039985111044173e-05), 'spearman': SpearmanrResult(correlation=0.2750355809628524, pvalue=6.19758829483142e-08), 'nsamples': 375}, 'answers-students': {'pearson': (0.41229596786484174, 3.840729919833259e-32), 'spearman': SpearmanrResult(correlation=0.511034960861323, pvalue=3.920472963231045e-51), 'nsamples': 750}, 'belief': {'pearson': (0.34577005917938647, 5.697019624549982e-12), 'spearman': SpearmanrResult(correlation=0.43166000342236854, pvalue=1.880645967864486e-18), 'nsamples': 375}, 'headlines': {'pearson': (0.44934039307298146, 1.5068210049719615e-38), 'spearman': SpearmanrResult(correlation=0.5199324747807954, pvalue=3.60333689571949e-53), 'nsamples': 750}, 'images': {'pearson': (0.24615791607583998, 8.170866239882643e-12), 'spearman': SpearmanrResult(correlation=0.44248571472100773, pvalue=2.654916169262915e-37), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3327762665918249, 'wmean': 0.3464594512465984}, 'spearman': {'mean': 0.4360297469496694, 'wmean': 0.45670023563893414}}}, 'STS16': {'answer-answer': {'pearson': (0.30995266366840635, 4.6519389344840647e-07), 'spearman': SpearmanrResult(correlation=0.41261686160350725, pvalue=7.319321657185714e-12), 'nsamples': 254}, 'headlines': {'pearson': (0.41655644903489264, 7.174207142254638e-12), 'spearman': SpearmanrResult(correlation=0.5640245142644023, pvalue=2.587241222472487e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.39042106162631557, 8.572859224679889e-10), 'spearman': SpearmanrResult(correlation=0.6186636849412452, pvalue=1.099091108156448e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.5227852035907598, 1.6229594705447399e-18), 'spearman': SpearmanrResult(correlation=0.7374974727140192, pvalue=3.85763616481063e-43), 'nsamples': 244}, 'question-question': {'pearson': (0.1873451465983482, 0.006602278570259617), 'spearman': SpearmanrResult(correlation=0.23372276809886453, pvalue=0.0006594097230385726), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.36541210490374454, 'wmean': 0.37011981607986194}, 'spearman': {'mean': 0.5133050603244078, 'wmean': 0.519676961475967}}}, 'MR': {'devacc': 74.14, 'acc': 73.76, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 76.44, 'acc': 76.4, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.61, 'acc': 84.21, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.52, 'acc': 93.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.67, 'acc': 79.02, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.87, 'acc': 42.9, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 69.54, 'acc': 82.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.11, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.6, 'acc': 70.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7222266040857914, 'pearson': 0.725759870690953, 'spearman': 0.6620217405879729, 'mse': 0.4825431569147064, 'yhat': array([2.23615315, 3.88938116, 1.2728433 , ..., 3.34249013, 4.11312716,        4.56404449]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6668943186738171, 'pearson': 0.5863837623145278, 'spearman': 0.572597327405141, 'mse': 1.7810623062741078, 'yhat': array([1.99883504, 1.65370192, 2.36062551, ..., 3.95304462, 3.56725299,        3.30369105]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.59, 'acc': 60.2, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 90.39, 'acc': 91.31, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 5.68, 'acc': 5.87, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.36, 'acc': 31.7, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.55, 'acc': 63.08, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 77.46, 'acc': 77.03, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.61, 'acc': 86.5, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.92, 'acc': 81.41, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.26, 'acc': 80.91, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.14, 'acc': 60.99, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 60.83, 'acc': 60.71, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 06:08:55,595 : STS12 p=0.3260, STS12 s=0.3872, STS13 p=0.2264, STS13 s=0.3185, STS14 p=0.2591, STS14 s=0.3407, STS15 p=0.3465, STS15 s=0.4567, STS 16 p=0.3701, STS16 s=0.5197, STS B p=0.5864, STS B s=0.5726, STS B m=1.7811, SICK-R p=0.7258, SICK-R s=0.6620, SICK-P m=0.4825
2019-03-13 06:08:55,595 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 06:08:55,595 : 0.3260,0.3872,0.2264,0.3185,0.2591,0.3407,0.3465,0.4567,0.3701,0.5197,0.5864,0.5726,1.7811,0.7258,0.6620,0.4825
2019-03-13 06:08:55,595 : MR=73.76, CR=76.40, SUBJ=93.10, MPQA=84.21, SST-B=79.02, SST-F=42.90, TREC=82.20, SICK-E=70.69, SNLI=60.20, MRPC=66.49, MRPC f=79.87
2019-03-13 06:08:55,595 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 06:08:55,595 : 73.76,76.40,93.10,84.21,79.02,42.90,82.20,70.69,60.20,66.49,79.87
2019-03-13 06:08:55,595 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 06:08:55,595 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 06:08:55,596 : na,na,na,na,na,na,na,na,na,na
2019-03-13 06:08:55,596 : SentLen=91.31, WC=5.87, TreeDepth=31.70, TopConst=63.08, BShift=77.03, Tense=86.50, SubjNum=81.41, ObjNum=80.91, SOMO=60.99, CoordInv=60.71, average=63.95
2019-03-13 06:08:55,596 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 06:08:55,596 : 91.31,5.87,31.70,63.08,77.03,86.50,81.41,80.91,60.99,60.71,63.95
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,596 : layer 12
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,596 : ********************************************************************************
2019-03-13 06:08:55,706 : ***** Transfer task : STS12 *****


2019-03-13 06:08:55,722 : loading BERT model bert-large-uncased
2019-03-13 06:08:55,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:08:55,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:08:55,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbhx42yss
2019-03-13 06:09:03,381 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:12,640 : MSRpar : pearson = 0.2623, spearman = 0.3120
2019-03-13 06:09:14,286 : MSRvid : pearson = 0.3621, spearman = 0.3943
2019-03-13 06:09:15,702 : SMTeuroparl : pearson = 0.4449, spearman = 0.5580
2019-03-13 06:09:18,402 : surprise.OnWN : pearson = 0.3332, spearman = 0.3968
2019-03-13 06:09:19,834 : surprise.SMTnews : pearson = 0.5803, spearman = 0.5480
2019-03-13 06:09:19,834 : ALL (weighted average) : Pearson = 0.3713,             Spearman = 0.4189
2019-03-13 06:09:19,834 : ALL (average) : Pearson = 0.3966,             Spearman = 0.4418

2019-03-13 06:09:19,834 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 06:09:19,842 : loading BERT model bert-large-uncased
2019-03-13 06:09:19,842 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:09:19,860 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:09:19,860 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvu9n338z
2019-03-13 06:09:27,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:33,914 : FNWN : pearson = 0.1767, spearman = 0.2034
2019-03-13 06:09:35,808 : headlines : pearson = 0.4239, spearman = 0.4893
2019-03-13 06:09:37,276 : OnWN : pearson = 0.2825, spearman = 0.3418
2019-03-13 06:09:37,276 : ALL (weighted average) : Pearson = 0.3398,             Spearman = 0.3981
2019-03-13 06:09:37,276 : ALL (average) : Pearson = 0.2943,             Spearman = 0.3448

2019-03-13 06:09:37,276 : ***** Transfer task : STS14 *****


2019-03-13 06:09:37,293 : loading BERT model bert-large-uncased
2019-03-13 06:09:37,293 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:09:37,311 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:09:37,311 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg1bjt7dj
2019-03-13 06:09:44,835 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:09:51,496 : deft-forum : pearson = 0.3027, spearman = 0.3085
2019-03-13 06:09:53,135 : deft-news : pearson = 0.5766, spearman = 0.5926
2019-03-13 06:09:55,307 : headlines : pearson = 0.2931, spearman = 0.3745
2019-03-13 06:09:57,388 : images : pearson = 0.4607, spearman = 0.4750
2019-03-13 06:09:59,519 : OnWN : pearson = 0.2991, spearman = 0.3835
2019-03-13 06:10:02,387 : tweet-news : pearson = 0.3828, spearman = 0.4011
2019-03-13 06:10:02,387 : ALL (weighted average) : Pearson = 0.3696,             Spearman = 0.4112
2019-03-13 06:10:02,387 : ALL (average) : Pearson = 0.3858,             Spearman = 0.4225

2019-03-13 06:10:02,387 : ***** Transfer task : STS15 *****


2019-03-13 06:10:02,421 : loading BERT model bert-large-uncased
2019-03-13 06:10:02,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:10:02,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:10:02,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5xqx13hl
2019-03-13 06:10:09,877 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:10:16,997 : answers-forums : pearson = 0.4025, spearman = 0.4474
2019-03-13 06:10:19,082 : answers-students : pearson = 0.4765, spearman = 0.5603
2019-03-13 06:10:21,134 : belief : pearson = 0.5106, spearman = 0.5721
2019-03-13 06:10:23,389 : headlines : pearson = 0.5033, spearman = 0.5404
2019-03-13 06:10:25,527 : images : pearson = 0.3947, spearman = 0.5129
2019-03-13 06:10:25,527 : ALL (weighted average) : Pearson = 0.4578,             Spearman = 0.5308
2019-03-13 06:10:25,527 : ALL (average) : Pearson = 0.4575,             Spearman = 0.5266

2019-03-13 06:10:25,528 : ***** Transfer task : STS16 *****


2019-03-13 06:10:25,596 : loading BERT model bert-large-uncased
2019-03-13 06:10:25,596 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:10:25,614 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:10:25,614 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj_8h7s3v
2019-03-13 06:10:33,088 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:10:39,160 : answer-answer : pearson = 0.3836, spearman = 0.4419
2019-03-13 06:10:39,821 : headlines : pearson = 0.5288, spearman = 0.6177
2019-03-13 06:10:40,705 : plagiarism : pearson = 0.5525, spearman = 0.7218
2019-03-13 06:10:42,203 : postediting : pearson = 0.6985, spearman = 0.8026
2019-03-13 06:10:42,810 : question-question : pearson = 0.2560, spearman = 0.2822
2019-03-13 06:10:42,810 : ALL (weighted average) : Pearson = 0.4891,             Spearman = 0.5791
2019-03-13 06:10:42,810 : ALL (average) : Pearson = 0.4839,             Spearman = 0.5732

2019-03-13 06:10:42,810 : ***** Transfer task : MR *****


2019-03-13 06:10:42,830 : loading BERT model bert-large-uncased
2019-03-13 06:10:42,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:10:42,850 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:10:42,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdr5hthjr
2019-03-13 06:10:50,339 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:10:55,662 : Generating sentence embeddings
2019-03-13 06:11:27,227 : Generated sentence embeddings
2019-03-13 06:11:27,227 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:11:36,993 : Best param found at split 1: l2reg = 0.0001                 with score 73.21
2019-03-13 06:11:48,076 : Best param found at split 2: l2reg = 1e-05                 with score 72.31
2019-03-13 06:12:00,017 : Best param found at split 3: l2reg = 1e-05                 with score 73.9
2019-03-13 06:12:10,783 : Best param found at split 4: l2reg = 1e-05                 with score 73.02
2019-03-13 06:12:21,799 : Best param found at split 5: l2reg = 1e-05                 with score 73.42
2019-03-13 06:12:22,322 : Dev acc : 73.17 Test acc : 73.02

2019-03-13 06:12:22,323 : ***** Transfer task : CR *****


2019-03-13 06:12:22,331 : loading BERT model bert-large-uncased
2019-03-13 06:12:22,331 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:12:22,350 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:12:22,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdlqz6isi
2019-03-13 06:12:29,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:12:34,980 : Generating sentence embeddings
2019-03-13 06:12:43,285 : Generated sentence embeddings
2019-03-13 06:12:43,285 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:12:47,182 : Best param found at split 1: l2reg = 0.0001                 with score 75.46
2019-03-13 06:12:51,400 : Best param found at split 2: l2reg = 1e-05                 with score 76.08
2019-03-13 06:12:55,641 : Best param found at split 3: l2reg = 1e-05                 with score 76.16
2019-03-13 06:12:59,428 : Best param found at split 4: l2reg = 1e-05                 with score 75.57
2019-03-13 06:13:03,231 : Best param found at split 5: l2reg = 0.0001                 with score 75.44
2019-03-13 06:13:03,420 : Dev acc : 75.74 Test acc : 75.55

2019-03-13 06:13:03,421 : ***** Transfer task : MPQA *****


2019-03-13 06:13:03,428 : loading BERT model bert-large-uncased
2019-03-13 06:13:03,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:13:03,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:13:03,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6o58deek
2019-03-13 06:13:10,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:13:16,204 : Generating sentence embeddings
2019-03-13 06:13:23,760 : Generated sentence embeddings
2019-03-13 06:13:23,761 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:13:32,368 : Best param found at split 1: l2reg = 1e-05                 with score 78.85
2019-03-13 06:13:40,172 : Best param found at split 2: l2reg = 0.0001                 with score 80.28
2019-03-13 06:13:51,200 : Best param found at split 3: l2reg = 1e-05                 with score 80.41
2019-03-13 06:14:02,316 : Best param found at split 4: l2reg = 0.0001                 with score 82.09
2019-03-13 06:14:14,308 : Best param found at split 5: l2reg = 1e-05                 with score 80.89
2019-03-13 06:14:14,828 : Dev acc : 80.5 Test acc : 83.08

2019-03-13 06:14:14,829 : ***** Transfer task : SUBJ *****


2019-03-13 06:14:14,844 : loading BERT model bert-large-uncased
2019-03-13 06:14:14,844 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:14:14,865 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:14:14,865 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvhofymmb
2019-03-13 06:14:22,305 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:14:27,507 : Generating sentence embeddings
2019-03-13 06:14:58,498 : Generated sentence embeddings
2019-03-13 06:14:58,499 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 06:15:08,450 : Best param found at split 1: l2reg = 1e-05                 with score 93.06
2019-03-13 06:15:20,381 : Best param found at split 2: l2reg = 1e-05                 with score 93.31
2019-03-13 06:15:31,760 : Best param found at split 3: l2reg = 1e-05                 with score 93.02
2019-03-13 06:15:43,569 : Best param found at split 4: l2reg = 1e-05                 with score 93.68
2019-03-13 06:15:56,396 : Best param found at split 5: l2reg = 1e-05                 with score 93.24
2019-03-13 06:15:56,968 : Dev acc : 93.26 Test acc : 92.12

2019-03-13 06:15:56,970 : ***** Transfer task : SST Binary classification *****


2019-03-13 06:15:57,062 : loading BERT model bert-large-uncased
2019-03-13 06:15:57,062 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:15:57,136 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:15:57,136 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0bbyj577
2019-03-13 06:16:04,584 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:16:09,851 : Computing embedding for train
2019-03-13 06:17:50,655 : Computed train embeddings
2019-03-13 06:17:50,655 : Computing embedding for dev
2019-03-13 06:17:52,860 : Computed dev embeddings
2019-03-13 06:17:52,860 : Computing embedding for test
2019-03-13 06:17:57,495 : Computed test embeddings
2019-03-13 06:17:57,495 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:18:17,724 : [('reg:1e-05', 78.67), ('reg:0.0001', 78.44), ('reg:0.001', 76.61), ('reg:0.01', 73.05)]
2019-03-13 06:18:17,724 : Validation : best param found is reg = 1e-05 with score             78.67
2019-03-13 06:18:17,724 : Evaluating...
2019-03-13 06:18:22,735 : 
Dev acc : 78.67 Test acc : 78.53 for             SST Binary classification

2019-03-13 06:18:22,735 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 06:18:22,787 : loading BERT model bert-large-uncased
2019-03-13 06:18:22,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:18:22,810 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:18:22,810 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0o2xr06r
2019-03-13 06:18:30,315 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:18:35,576 : Computing embedding for train
2019-03-13 06:18:57,584 : Computed train embeddings
2019-03-13 06:18:57,584 : Computing embedding for dev
2019-03-13 06:19:00,457 : Computed dev embeddings
2019-03-13 06:19:00,457 : Computing embedding for test
2019-03-13 06:19:06,124 : Computed test embeddings
2019-03-13 06:19:06,125 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:19:08,628 : [('reg:1e-05', 39.06), ('reg:0.0001', 38.42), ('reg:0.001', 36.69), ('reg:0.01', 28.07)]
2019-03-13 06:19:08,628 : Validation : best param found is reg = 1e-05 with score             39.06
2019-03-13 06:19:08,629 : Evaluating...
2019-03-13 06:19:09,354 : 
Dev acc : 39.06 Test acc : 40.36 for             SST Fine-Grained classification

2019-03-13 06:19:09,354 : ***** Transfer task : TREC *****


2019-03-13 06:19:09,368 : loading BERT model bert-large-uncased
2019-03-13 06:19:09,368 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:19:09,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:19:09,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvsjlddqe
2019-03-13 06:19:16,848 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:19:29,741 : Computed train embeddings
2019-03-13 06:19:30,333 : Computed test embeddings
2019-03-13 06:19:30,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:19:37,606 : [('reg:1e-05', 67.59), ('reg:0.0001', 66.25), ('reg:0.001', 51.84), ('reg:0.01', 35.75)]
2019-03-13 06:19:37,606 : Cross-validation : best param found is reg = 1e-05             with score 67.59
2019-03-13 06:19:37,606 : Evaluating...
2019-03-13 06:19:38,162 : 
Dev acc : 67.59 Test acc : 83.6             for TREC

2019-03-13 06:19:38,163 : ***** Transfer task : MRPC *****


2019-03-13 06:19:38,185 : loading BERT model bert-large-uncased
2019-03-13 06:19:38,185 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:19:38,206 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:19:38,206 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp96257rxm
2019-03-13 06:19:45,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:19:50,928 : Computing embedding for train
2019-03-13 06:20:13,247 : Computed train embeddings
2019-03-13 06:20:13,247 : Computing embedding for test
2019-03-13 06:20:23,061 : Computed test embeddings
2019-03-13 06:20:23,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 06:20:26,438 : [('reg:1e-05', 68.57), ('reg:0.0001', 68.45), ('reg:0.001', 68.57), ('reg:0.01', 67.64)]
2019-03-13 06:20:26,438 : Cross-validation : best param found is reg = 1e-05             with score 68.57
2019-03-13 06:20:26,438 : Evaluating...
2019-03-13 06:20:26,666 : Dev acc : 68.57 Test acc 64.52; Test F1 69.34 for MRPC.

2019-03-13 06:20:26,666 : ***** Transfer task : SICK-Entailment*****


2019-03-13 06:20:26,727 : loading BERT model bert-large-uncased
2019-03-13 06:20:26,727 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:20:26,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:20:26,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp242itd6x
2019-03-13 06:20:34,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:20:39,490 : Computing embedding for train
2019-03-13 06:20:50,836 : Computed train embeddings
2019-03-13 06:20:50,836 : Computing embedding for dev
2019-03-13 06:20:52,384 : Computed dev embeddings
2019-03-13 06:20:52,384 : Computing embedding for test
2019-03-13 06:21:04,551 : Computed test embeddings
2019-03-13 06:21:04,588 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:21:05,760 : [('reg:1e-05', 68.4), ('reg:0.0001', 67.4), ('reg:0.001', 64.4), ('reg:0.01', 56.4)]
2019-03-13 06:21:05,760 : Validation : best param found is reg = 1e-05 with score             68.4
2019-03-13 06:21:05,760 : Evaluating...
2019-03-13 06:21:06,060 : 
Dev acc : 68.4 Test acc : 67.73 for                        SICK entailment

2019-03-13 06:21:06,061 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 06:21:06,087 : loading BERT model bert-large-uncased
2019-03-13 06:21:06,088 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:21:06,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:21:06,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkr_agh7s
2019-03-13 06:21:13,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:21:18,882 : Computing embedding for train
2019-03-13 06:21:30,237 : Computed train embeddings
2019-03-13 06:21:30,237 : Computing embedding for dev
2019-03-13 06:21:31,791 : Computed dev embeddings
2019-03-13 06:21:31,791 : Computing embedding for test
2019-03-13 06:21:43,999 : Computed test embeddings
2019-03-13 06:22:28,195 : Dev : Pearson 0.6999904200708945
2019-03-13 06:22:28,195 : Test : Pearson 0.7178456836060281 Spearman 0.669584205088523 MSE 0.49536074387175943                        for SICK Relatedness

2019-03-13 06:22:28,196 : 

***** Transfer task : STSBenchmark*****


2019-03-13 06:22:28,236 : loading BERT model bert-large-uncased
2019-03-13 06:22:28,236 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:22:28,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:22:28,265 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpam_lqtq1
2019-03-13 06:22:35,733 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:22:41,245 : Computing embedding for train
2019-03-13 06:22:59,799 : Computed train embeddings
2019-03-13 06:22:59,799 : Computing embedding for dev
2019-03-13 06:23:05,453 : Computed dev embeddings
2019-03-13 06:23:05,453 : Computing embedding for test
2019-03-13 06:23:10,068 : Computed test embeddings
2019-03-13 06:23:36,967 : Dev : Pearson 0.6615916206540305
2019-03-13 06:23:36,967 : Test : Pearson 0.593396414339349 Spearman 0.5794375183095145 MSE 1.782029835616381                        for SICK Relatedness

2019-03-13 06:23:36,968 : ***** Transfer task : SNLI Entailment*****


2019-03-13 06:23:42,028 : loading BERT model bert-large-uncased
2019-03-13 06:23:42,028 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:23:42,121 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:23:42,121 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsxh0hftp
2019-03-13 06:23:49,554 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:23:55,213 : PROGRESS (encoding): 0.00%
2019-03-13 06:26:42,504 : PROGRESS (encoding): 14.56%
2019-03-13 06:29:55,091 : PROGRESS (encoding): 29.12%
2019-03-13 06:33:05,762 : PROGRESS (encoding): 43.69%
2019-03-13 06:36:29,311 : PROGRESS (encoding): 58.25%
2019-03-13 06:40:15,161 : PROGRESS (encoding): 72.81%
2019-03-13 06:44:00,065 : PROGRESS (encoding): 87.37%
2019-03-13 06:48:03,614 : PROGRESS (encoding): 0.00%
2019-03-13 06:48:34,209 : PROGRESS (encoding): 0.00%
2019-03-13 06:49:03,703 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:49:46,216 : [('reg:1e-09', 63.39)]
2019-03-13 06:49:46,216 : Validation : best param found is reg = 1e-09 with score             63.39
2019-03-13 06:49:46,216 : Evaluating...
2019-03-13 06:50:26,353 : Dev acc : 63.39 Test acc : 63.39 for SNLI

2019-03-13 06:50:26,353 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 06:50:26,564 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 06:50:27,635 : loading BERT model bert-large-uncased
2019-03-13 06:50:27,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:50:27,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:50:27,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5w_3z4z3
2019-03-13 06:50:35,499 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:50:40,773 : Computing embeddings for train/dev/test
2019-03-13 06:54:13,144 : Computed embeddings
2019-03-13 06:54:13,144 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:54:45,498 : [('reg:1e-05', 89.02), ('reg:0.0001', 83.3), ('reg:0.001', 64.42), ('reg:0.01', 35.3)]
2019-03-13 06:54:45,498 : Validation : best param found is reg = 1e-05 with score             89.02
2019-03-13 06:54:45,498 : Evaluating...
2019-03-13 06:54:55,859 : 
Dev acc : 89.0 Test acc : 89.5 for LENGTH classification

2019-03-13 06:54:55,860 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 06:54:56,115 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 06:54:56,161 : loading BERT model bert-large-uncased
2019-03-13 06:54:56,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:54:56,190 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:54:56,190 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgw0k00q6
2019-03-13 06:55:03,643 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:55:08,702 : Computing embeddings for train/dev/test
2019-03-13 06:58:24,261 : Computed embeddings
2019-03-13 06:58:24,261 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 06:58:52,804 : [('reg:1e-05', 4.1), ('reg:0.0001', 0.54), ('reg:0.001', 0.17), ('reg:0.01', 0.17)]
2019-03-13 06:58:52,804 : Validation : best param found is reg = 1e-05 with score             4.1
2019-03-13 06:58:52,804 : Evaluating...
2019-03-13 06:58:59,429 : 
Dev acc : 4.1 Test acc : 3.7 for WORDCONTENT classification

2019-03-13 06:58:59,430 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 06:58:59,960 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 06:59:00,025 : loading BERT model bert-large-uncased
2019-03-13 06:59:00,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 06:59:00,049 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 06:59:00,049 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptewef0fu
2019-03-13 06:59:07,524 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 06:59:12,729 : Computing embeddings for train/dev/test
2019-03-13 07:02:16,892 : Computed embeddings
2019-03-13 07:02:16,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:02:43,461 : [('reg:1e-05', 31.04), ('reg:0.0001', 26.98), ('reg:0.001', 20.72), ('reg:0.01', 18.89)]
2019-03-13 07:02:43,461 : Validation : best param found is reg = 1e-05 with score             31.04
2019-03-13 07:02:43,461 : Evaluating...
2019-03-13 07:02:49,891 : 
Dev acc : 31.0 Test acc : 31.0 for DEPTH classification

2019-03-13 07:02:49,892 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 07:02:50,261 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 07:02:50,323 : loading BERT model bert-large-uncased
2019-03-13 07:02:50,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:02:50,434 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:02:50,434 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4bkaklds
2019-03-13 07:02:57,895 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:03:03,120 : Computing embeddings for train/dev/test
2019-03-13 07:05:53,073 : Computed embeddings
2019-03-13 07:05:53,073 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:06:24,421 : [('reg:1e-05', 62.92), ('reg:0.0001', 51.78), ('reg:0.001', 26.58), ('reg:0.01', 9.82)]
2019-03-13 07:06:24,421 : Validation : best param found is reg = 1e-05 with score             62.92
2019-03-13 07:06:24,421 : Evaluating...
2019-03-13 07:06:32,356 : 
Dev acc : 62.9 Test acc : 62.7 for TOPCONSTITUENTS classification

2019-03-13 07:06:32,357 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 07:06:32,742 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 07:06:32,809 : loading BERT model bert-large-uncased
2019-03-13 07:06:32,809 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:06:32,838 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:06:32,839 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnwv62va0
2019-03-13 07:06:40,277 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:06:45,528 : Computing embeddings for train/dev/test
2019-03-13 07:09:50,162 : Computed embeddings
2019-03-13 07:09:50,162 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:10:24,522 : [('reg:1e-05', 83.54), ('reg:0.0001', 81.92), ('reg:0.001', 76.96), ('reg:0.01', 68.89)]
2019-03-13 07:10:24,523 : Validation : best param found is reg = 1e-05 with score             83.54
2019-03-13 07:10:24,523 : Evaluating...
2019-03-13 07:10:34,080 : 
Dev acc : 83.5 Test acc : 82.3 for BIGRAMSHIFT classification

2019-03-13 07:10:34,081 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 07:10:34,471 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 07:10:34,536 : loading BERT model bert-large-uncased
2019-03-13 07:10:34,536 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:10:34,565 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:10:34,565 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf2fcddbj
2019-03-13 07:10:42,028 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:10:47,332 : Computing embeddings for train/dev/test
2019-03-13 07:13:47,876 : Computed embeddings
2019-03-13 07:13:47,876 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:14:18,486 : [('reg:1e-05', 88.68), ('reg:0.0001', 88.39), ('reg:0.001', 86.47), ('reg:0.01', 79.32)]
2019-03-13 07:14:18,486 : Validation : best param found is reg = 1e-05 with score             88.68
2019-03-13 07:14:18,486 : Evaluating...
2019-03-13 07:14:25,897 : 
Dev acc : 88.7 Test acc : 86.3 for TENSE classification

2019-03-13 07:14:25,898 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 07:14:26,299 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 07:14:26,361 : loading BERT model bert-large-uncased
2019-03-13 07:14:26,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:14:26,477 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:14:26,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpekgc3u_z
2019-03-13 07:14:33,874 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:14:39,045 : Computing embeddings for train/dev/test
2019-03-13 07:17:51,183 : Computed embeddings
2019-03-13 07:17:51,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:18:24,406 : [('reg:1e-05', 82.97), ('reg:0.0001', 81.28), ('reg:0.001', 75.87), ('reg:0.01', 68.49)]
2019-03-13 07:18:24,406 : Validation : best param found is reg = 1e-05 with score             82.97
2019-03-13 07:18:24,406 : Evaluating...
2019-03-13 07:18:31,863 : 
Dev acc : 83.0 Test acc : 81.5 for SUBJNUMBER classification

2019-03-13 07:18:31,864 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 07:18:32,266 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 07:18:32,333 : loading BERT model bert-large-uncased
2019-03-13 07:18:32,333 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:18:32,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:18:32,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgfdlxxae
2019-03-13 07:18:39,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:18:45,384 : Computing embeddings for train/dev/test
2019-03-13 07:21:52,954 : Computed embeddings
2019-03-13 07:21:52,954 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:22:21,423 : [('reg:1e-05', 80.37), ('reg:0.0001', 78.18), ('reg:0.001', 72.35), ('reg:0.01', 62.5)]
2019-03-13 07:22:21,423 : Validation : best param found is reg = 1e-05 with score             80.37
2019-03-13 07:22:21,424 : Evaluating...
2019-03-13 07:22:30,237 : 
Dev acc : 80.4 Test acc : 80.6 for OBJNUMBER classification

2019-03-13 07:22:30,238 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 07:22:30,805 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 07:22:30,875 : loading BERT model bert-large-uncased
2019-03-13 07:22:30,875 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:22:30,903 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:22:30,903 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsiv2lxh9
2019-03-13 07:22:38,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:22:43,534 : Computing embeddings for train/dev/test
2019-03-13 07:26:20,861 : Computed embeddings
2019-03-13 07:26:20,861 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:26:50,717 : [('reg:1e-05', 61.81), ('reg:0.0001', 61.55), ('reg:0.001', 59.36), ('reg:0.01', 52.31)]
2019-03-13 07:26:50,717 : Validation : best param found is reg = 1e-05 with score             61.81
2019-03-13 07:26:50,717 : Evaluating...
2019-03-13 07:26:58,079 : 
Dev acc : 61.8 Test acc : 61.9 for ODDMANOUT classification

2019-03-13 07:26:58,080 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 07:26:58,463 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 07:26:58,545 : loading BERT model bert-large-uncased
2019-03-13 07:26:58,545 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:26:58,576 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:26:58,576 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2efm4gwx
2019-03-13 07:27:06,086 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:27:11,336 : Computing embeddings for train/dev/test
2019-03-13 07:30:46,986 : Computed embeddings
2019-03-13 07:30:46,987 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:31:18,629 : [('reg:1e-05', 59.1), ('reg:0.0001', 54.98), ('reg:0.001', 50.19), ('reg:0.01', 50.0)]
2019-03-13 07:31:18,629 : Validation : best param found is reg = 1e-05 with score             59.1
2019-03-13 07:31:18,630 : Evaluating...
2019-03-13 07:31:27,905 : 
Dev acc : 59.1 Test acc : 59.6 for COORDINATIONINVERSION classification

2019-03-13 07:31:27,907 : total results: {'STS12': {'MSRpar': {'pearson': (0.2623207290228175, 2.8737432567216123e-13), 'spearman': SpearmanrResult(correlation=0.31196925180684665, pvalue=2.1592375090695776e-18), 'nsamples': 750}, 'MSRvid': {'pearson': (0.3620566184735331, 1.2017645940640772e-24), 'spearman': SpearmanrResult(correlation=0.39427754422383593, pvalue=2.648002381962707e-29), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.44488038058259005, 1.0806075917438884e-23), 'spearman': SpearmanrResult(correlation=0.5580158478025729, pvalue=6.326052379823462e-39), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3332133568786793, 6.624782260965279e-21), 'spearman': SpearmanrResult(correlation=0.3968139673332445, pvalue=1.0809714925642145e-29), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5803130937911275, 2.7396798676446925e-37), 'spearman': SpearmanrResult(correlation=0.5479787572150026, pvalue=1.1978146452379525e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3965568357497495, 'wmean': 0.3712799380281021}, 'spearman': {'mean': 0.44181107367630046, 'wmean': 0.4189409172436011}}}, 'STS13': {'FNWN': {'pearson': (0.17667968379302038, 0.015015960779429747), 'spearman': SpearmanrResult(correlation=0.20338210396761291, pvalue=0.005002227750711693), 'nsamples': 189}, 'headlines': {'pearson': (0.42385390558197344, 4.680525037224218e-34), 'spearman': SpearmanrResult(correlation=0.4893359574489117, pvalue=2.0666717267415544e-46), 'nsamples': 750}, 'OnWN': {'pearson': (0.2825065452549609, 9.381511033575013e-12), 'spearman': SpearmanrResult(correlation=0.34179154120083804, pvalue=8.112762651390081e-17), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2943467115433182, 'wmean': 0.3398460408742627}, 'spearman': {'mean': 0.3448365342057875, 'wmean': 0.3981241602334885}}}, 'STS14': {'deft-forum': {'pearson': (0.30270489312932025, 5.454854739419824e-11), 'spearman': SpearmanrResult(correlation=0.30846411575778426, pvalue=2.2452227717092208e-11), 'nsamples': 450}, 'deft-news': {'pearson': (0.576616097265531, 5.562956318072913e-28), 'spearman': SpearmanrResult(correlation=0.5926290706556019, pvalue=7.809914284854852e-30), 'nsamples': 300}, 'headlines': {'pearson': (0.29305866181237805, 2.5499450603222563e-16), 'spearman': SpearmanrResult(correlation=0.3744662805281231, pvalue=2.227775546012326e-26), 'nsamples': 750}, 'images': {'pearson': (0.46066571187817074, 1.1382833732783973e-40), 'spearman': SpearmanrResult(correlation=0.4749662093318414, pvalue=1.8228198481265193e-43), 'nsamples': 750}, 'OnWN': {'pearson': (0.29911820315768717, 5.741764198909887e-17), 'spearman': SpearmanrResult(correlation=0.383486389530504, pvalue=1.0989183530805192e-27), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3828245647895573, 1.3748444748863747e-27), 'spearman': SpearmanrResult(correlation=0.4011389822081658, pvalue=2.304069760467009e-30), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.38583135533877405, 'wmean': 0.36958730328431955}, 'spearman': {'mean': 0.42252517466867007, 'wmean': 0.41123759186310915}}}, 'STS15': {'answers-forums': {'pearson': (0.40248549590003957, 4.906591331463525e-16), 'spearman': SpearmanrResult(correlation=0.44735827639998405, pvalue=7.47832984472233e-20), 'nsamples': 375}, 'answers-students': {'pearson': (0.47645521770997845, 9.162014532162454e-44), 'spearman': SpearmanrResult(correlation=0.5602972771737732, pvalue=3.282783758915633e-63), 'nsamples': 750}, 'belief': {'pearson': (0.5106212441066315, 2.7277351324799125e-26), 'spearman': SpearmanrResult(correlation=0.572053886573357, pvalue=5.639601438192607e-34), 'nsamples': 375}, 'headlines': {'pearson': (0.5033333281009421, 2.0337233945886948e-49), 'spearman': SpearmanrResult(correlation=0.5404484065353454, pvalue=4.204881136017196e-58), 'nsamples': 750}, 'images': {'pearson': (0.39467619150321315, 2.3013724443154478e-29), 'spearman': SpearmanrResult(correlation=0.5129098618162854, pvalue=1.4762840185010568e-51), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.457514295464161, 'wmean': 0.45775452682936735}, 'spearman': {'mean': 0.526613541699749, 'wmean': 0.5308404067530187}}}, 'STS16': {'answer-answer': {'pearson': (0.38364011894532607, 2.486163395320126e-10), 'spearman': SpearmanrResult(correlation=0.4419433258215291, pvalue=1.437472708545225e-13), 'nsamples': 254}, 'headlines': {'pearson': (0.5287732412086332, 2.4421142562055333e-19), 'spearman': SpearmanrResult(correlation=0.6176502836739697, pvalue=1.3892615337142617e-27), 'nsamples': 249}, 'plagiarism': {'pearson': (0.552459375703055, 8.855373970945232e-20), 'spearman': SpearmanrResult(correlation=0.7217557846766411, pvalue=2.6774646655351114e-38), 'nsamples': 230}, 'postediting': {'pearson': (0.6984889846502642, 4.975668566599197e-37), 'spearman': SpearmanrResult(correlation=0.8026398173936468, pvalue=3.1294917224053986e-56), 'nsamples': 244}, 'question-question': {'pearson': (0.25596222620009507, 0.0001834946229589838), 'spearman': SpearmanrResult(correlation=0.2821779075881633, pvalue=3.482681682586915e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.48386478934147464, 'wmean': 0.4891249588661464}, 'spearman': {'mean': 0.5732334238307899, 'wmean': 0.5791497925793341}}}, 'MR': {'devacc': 73.17, 'acc': 73.02, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.74, 'acc': 75.55, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.5, 'acc': 83.08, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.26, 'acc': 92.12, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.67, 'acc': 78.53, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.06, 'acc': 40.36, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.59, 'acc': 83.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.57, 'acc': 64.52, 'f1': 69.34, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 68.4, 'acc': 67.73, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6999904200708945, 'pearson': 0.7178456836060281, 'spearman': 0.669584205088523, 'mse': 0.49536074387175943, 'yhat': array([2.62010594, 3.7826832 , 1.54959284, ..., 3.0853609 , 4.2176099 ,        4.18636916]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6615916206540305, 'pearson': 0.593396414339349, 'spearman': 0.5794375183095145, 'mse': 1.782029835616381, 'yhat': array([1.92895858, 2.18555086, 1.75429622, ..., 3.95107519, 3.65962314,        3.19566842]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.39, 'acc': 63.39, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 89.02, 'acc': 89.46, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 4.1, 'acc': 3.7, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.04, 'acc': 31.01, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.92, 'acc': 62.67, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 83.54, 'acc': 82.29, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.68, 'acc': 86.32, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.97, 'acc': 81.54, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.37, 'acc': 80.58, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 61.81, 'acc': 61.93, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 59.1, 'acc': 59.63, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 07:31:27,907 : STS12 p=0.3713, STS12 s=0.4189, STS13 p=0.3398, STS13 s=0.3981, STS14 p=0.3696, STS14 s=0.4112, STS15 p=0.4578, STS15 s=0.5308, STS 16 p=0.4891, STS16 s=0.5791, STS B p=0.5934, STS B s=0.5794, STS B m=1.7820, SICK-R p=0.7178, SICK-R s=0.6696, SICK-P m=0.4954
2019-03-13 07:31:27,908 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 07:31:27,908 : 0.3713,0.4189,0.3398,0.3981,0.3696,0.4112,0.4578,0.5308,0.4891,0.5791,0.5934,0.5794,1.7820,0.7178,0.6696,0.4954
2019-03-13 07:31:27,908 : MR=73.02, CR=75.55, SUBJ=92.12, MPQA=83.08, SST-B=78.53, SST-F=40.36, TREC=83.60, SICK-E=67.73, SNLI=63.39, MRPC=64.52, MRPC f=69.34
2019-03-13 07:31:27,908 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 07:31:27,908 : 73.02,75.55,92.12,83.08,78.53,40.36,83.60,67.73,63.39,64.52,69.34
2019-03-13 07:31:27,908 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 07:31:27,908 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 07:31:27,908 : na,na,na,na,na,na,na,na,na,na
2019-03-13 07:31:27,908 : SentLen=89.46, WC=3.70, TreeDepth=31.01, TopConst=62.67, BShift=82.29, Tense=86.32, SubjNum=81.54, ObjNum=80.58, SOMO=61.93, CoordInv=59.63, average=63.91
2019-03-13 07:31:27,908 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 07:31:27,908 : 89.46,3.70,31.01,62.67,82.29,86.32,81.54,80.58,61.93,59.63,63.91
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:27,908 : layer 13
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:27,908 : ********************************************************************************
2019-03-13 07:31:28,001 : ***** Transfer task : STS12 *****


2019-03-13 07:31:28,014 : loading BERT model bert-large-uncased
2019-03-13 07:31:28,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:31:28,031 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:31:28,031 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0vmtc5vg
2019-03-13 07:31:35,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:31:45,067 : MSRpar : pearson = 0.3330, spearman = 0.3889
2019-03-13 07:31:46,710 : MSRvid : pearson = 0.4108, spearman = 0.4319
2019-03-13 07:31:48,124 : SMTeuroparl : pearson = 0.4432, spearman = 0.5668
2019-03-13 07:31:50,820 : surprise.OnWN : pearson = 0.5210, spearman = 0.5428
2019-03-13 07:31:52,248 : surprise.SMTnews : pearson = 0.5710, spearman = 0.5458
2019-03-13 07:31:52,248 : ALL (weighted average) : Pearson = 0.4440,             Spearman = 0.4828
2019-03-13 07:31:52,248 : ALL (average) : Pearson = 0.4558,             Spearman = 0.4952

2019-03-13 07:31:52,248 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 07:31:52,257 : loading BERT model bert-large-uncased
2019-03-13 07:31:52,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:31:52,274 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:31:52,274 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpte5lhcic
2019-03-13 07:31:59,697 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:32:06,223 : FNWN : pearson = 0.3283, spearman = 0.3237
2019-03-13 07:32:08,117 : headlines : pearson = 0.5207, spearman = 0.5168
2019-03-13 07:32:09,584 : OnWN : pearson = 0.3680, spearman = 0.4096
2019-03-13 07:32:09,584 : ALL (weighted average) : Pearson = 0.4393,             Spearman = 0.4524
2019-03-13 07:32:09,584 : ALL (average) : Pearson = 0.4057,             Spearman = 0.4167

2019-03-13 07:32:09,584 : ***** Transfer task : STS14 *****


2019-03-13 07:32:09,599 : loading BERT model bert-large-uncased
2019-03-13 07:32:09,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:32:09,616 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:32:09,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptnsxxfd5
2019-03-13 07:32:17,075 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:32:23,642 : deft-forum : pearson = 0.2883, spearman = 0.2927
2019-03-13 07:32:25,282 : deft-news : pearson = 0.6067, spearman = 0.5932
2019-03-13 07:32:27,456 : headlines : pearson = 0.4365, spearman = 0.4156
2019-03-13 07:32:29,534 : images : pearson = 0.4900, spearman = 0.5008
2019-03-13 07:32:31,664 : OnWN : pearson = 0.4601, spearman = 0.5136
2019-03-13 07:32:34,527 : tweet-news : pearson = 0.5431, spearman = 0.5172
2019-03-13 07:32:34,527 : ALL (weighted average) : Pearson = 0.4691,             Spearman = 0.4720
2019-03-13 07:32:34,527 : ALL (average) : Pearson = 0.4708,             Spearman = 0.4722

2019-03-13 07:32:34,528 : ***** Transfer task : STS15 *****


2019-03-13 07:32:34,559 : loading BERT model bert-large-uncased
2019-03-13 07:32:34,560 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:32:34,594 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:32:34,594 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0233ckzd
2019-03-13 07:32:42,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:32:49,119 : answers-forums : pearson = 0.4648, spearman = 0.4682
2019-03-13 07:32:51,204 : answers-students : pearson = 0.5320, spearman = 0.5734
2019-03-13 07:32:53,258 : belief : pearson = 0.5626, spearman = 0.6075
2019-03-13 07:32:55,510 : headlines : pearson = 0.5548, spearman = 0.5602
2019-03-13 07:32:57,646 : images : pearson = 0.5566, spearman = 0.6109
2019-03-13 07:32:57,646 : ALL (weighted average) : Pearson = 0.5393,             Spearman = 0.5706
2019-03-13 07:32:57,646 : ALL (average) : Pearson = 0.5342,             Spearman = 0.5640

2019-03-13 07:32:57,646 : ***** Transfer task : STS16 *****


2019-03-13 07:32:57,718 : loading BERT model bert-large-uncased
2019-03-13 07:32:57,718 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:32:57,736 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:32:57,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1na__hfb
2019-03-13 07:33:05,177 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:33:11,292 : answer-answer : pearson = 0.5002, spearman = 0.5319
2019-03-13 07:33:11,952 : headlines : pearson = 0.5818, spearman = 0.6012
2019-03-13 07:33:12,832 : plagiarism : pearson = 0.7282, spearman = 0.7632
2019-03-13 07:33:14,330 : postediting : pearson = 0.7864, spearman = 0.8297
2019-03-13 07:33:14,936 : question-question : pearson = 0.2522, spearman = 0.2698
2019-03-13 07:33:14,937 : ALL (weighted average) : Pearson = 0.5767,             Spearman = 0.6064
2019-03-13 07:33:14,937 : ALL (average) : Pearson = 0.5697,             Spearman = 0.5992

2019-03-13 07:33:14,937 : ***** Transfer task : MR *****


2019-03-13 07:33:14,952 : loading BERT model bert-large-uncased
2019-03-13 07:33:14,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:33:14,972 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:33:14,972 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1jsfl1ak
2019-03-13 07:33:22,426 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:33:27,624 : Generating sentence embeddings
2019-03-13 07:33:59,192 : Generated sentence embeddings
2019-03-13 07:33:59,192 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:34:09,000 : Best param found at split 1: l2reg = 1e-05                 with score 73.99
2019-03-13 07:34:20,304 : Best param found at split 2: l2reg = 1e-05                 with score 74.14
2019-03-13 07:34:32,081 : Best param found at split 3: l2reg = 1e-05                 with score 74.62
2019-03-13 07:34:42,524 : Best param found at split 4: l2reg = 0.0001                 with score 73.52
2019-03-13 07:34:52,240 : Best param found at split 5: l2reg = 1e-05                 with score 74.49
2019-03-13 07:34:52,680 : Dev acc : 74.15 Test acc : 74.2

2019-03-13 07:34:52,681 : ***** Transfer task : CR *****


2019-03-13 07:34:52,689 : loading BERT model bert-large-uncased
2019-03-13 07:34:52,689 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:34:52,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:34:52,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpckuq0qiw
2019-03-13 07:35:00,120 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:35:05,376 : Generating sentence embeddings
2019-03-13 07:35:13,684 : Generated sentence embeddings
2019-03-13 07:35:13,685 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:35:17,691 : Best param found at split 1: l2reg = 0.0001                 with score 77.44
2019-03-13 07:35:21,692 : Best param found at split 2: l2reg = 1e-05                 with score 77.91
2019-03-13 07:35:25,961 : Best param found at split 3: l2reg = 1e-05                 with score 77.98
2019-03-13 07:35:29,265 : Best param found at split 4: l2reg = 1e-05                 with score 77.19
2019-03-13 07:35:32,922 : Best param found at split 5: l2reg = 1e-05                 with score 77.59
2019-03-13 07:35:33,110 : Dev acc : 77.62 Test acc : 76.16

2019-03-13 07:35:33,110 : ***** Transfer task : MPQA *****


2019-03-13 07:35:33,116 : loading BERT model bert-large-uncased
2019-03-13 07:35:33,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:35:33,166 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:35:33,166 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt949x6r9
2019-03-13 07:35:40,594 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:35:45,762 : Generating sentence embeddings
2019-03-13 07:35:53,333 : Generated sentence embeddings
2019-03-13 07:35:53,333 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:36:02,141 : Best param found at split 1: l2reg = 1e-05                 with score 78.06
2019-03-13 07:36:11,836 : Best param found at split 2: l2reg = 0.0001                 with score 77.99
2019-03-13 07:36:22,711 : Best param found at split 3: l2reg = 0.0001                 with score 79.23
2019-03-13 07:36:33,661 : Best param found at split 4: l2reg = 1e-05                 with score 79.35
2019-03-13 07:36:44,710 : Best param found at split 5: l2reg = 0.0001                 with score 79.88
2019-03-13 07:36:45,393 : Dev acc : 78.9 Test acc : 81.56

2019-03-13 07:36:45,394 : ***** Transfer task : SUBJ *****


2019-03-13 07:36:45,411 : loading BERT model bert-large-uncased
2019-03-13 07:36:45,411 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:36:45,430 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:36:45,430 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkhtzw0hw
2019-03-13 07:36:52,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:36:58,117 : Generating sentence embeddings
2019-03-13 07:37:29,118 : Generated sentence embeddings
2019-03-13 07:37:29,118 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 07:37:40,251 : Best param found at split 1: l2reg = 1e-05                 with score 93.17
2019-03-13 07:37:51,206 : Best param found at split 2: l2reg = 1e-05                 with score 93.1
2019-03-13 07:38:01,764 : Best param found at split 3: l2reg = 1e-05                 with score 92.71
2019-03-13 07:38:12,544 : Best param found at split 4: l2reg = 1e-05                 with score 93.35
2019-03-13 07:38:22,963 : Best param found at split 5: l2reg = 1e-05                 with score 92.95
2019-03-13 07:38:23,538 : Dev acc : 93.06 Test acc : 92.29

2019-03-13 07:38:23,539 : ***** Transfer task : SST Binary classification *****


2019-03-13 07:38:23,630 : loading BERT model bert-large-uncased
2019-03-13 07:38:23,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:38:23,711 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:38:23,711 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpepjztc6v
2019-03-13 07:38:31,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:38:36,493 : Computing embedding for train
2019-03-13 07:40:17,264 : Computed train embeddings
2019-03-13 07:40:17,264 : Computing embedding for dev
2019-03-13 07:40:19,466 : Computed dev embeddings
2019-03-13 07:40:19,466 : Computing embedding for test
2019-03-13 07:40:24,098 : Computed test embeddings
2019-03-13 07:40:24,098 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:40:44,219 : [('reg:1e-05', 78.44), ('reg:0.0001', 78.67), ('reg:0.001', 75.92), ('reg:0.01', 70.99)]
2019-03-13 07:40:44,219 : Validation : best param found is reg = 0.0001 with score             78.67
2019-03-13 07:40:44,219 : Evaluating...
2019-03-13 07:40:49,004 : 
Dev acc : 78.67 Test acc : 78.25 for             SST Binary classification

2019-03-13 07:40:49,004 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 07:40:49,059 : loading BERT model bert-large-uncased
2019-03-13 07:40:49,059 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:40:49,079 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:40:49,079 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1o8ydxm_
2019-03-13 07:40:56,474 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:41:01,657 : Computing embedding for train
2019-03-13 07:41:23,628 : Computed train embeddings
2019-03-13 07:41:23,628 : Computing embedding for dev
2019-03-13 07:41:26,505 : Computed dev embeddings
2019-03-13 07:41:26,505 : Computing embedding for test
2019-03-13 07:41:32,173 : Computed test embeddings
2019-03-13 07:41:32,174 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:41:34,642 : [('reg:1e-05', 40.05), ('reg:0.0001', 39.15), ('reg:0.001', 38.87), ('reg:0.01', 33.51)]
2019-03-13 07:41:34,642 : Validation : best param found is reg = 1e-05 with score             40.05
2019-03-13 07:41:34,642 : Evaluating...
2019-03-13 07:41:35,355 : 
Dev acc : 40.05 Test acc : 40.59 for             SST Fine-Grained classification

2019-03-13 07:41:35,356 : ***** Transfer task : TREC *****


2019-03-13 07:41:35,369 : loading BERT model bert-large-uncased
2019-03-13 07:41:35,369 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:41:35,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:41:35,388 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_g5x1ha3
2019-03-13 07:41:42,841 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:41:55,751 : Computed train embeddings
2019-03-13 07:41:56,343 : Computed test embeddings
2019-03-13 07:41:56,343 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:42:03,779 : [('reg:1e-05', 70.3), ('reg:0.0001', 65.89), ('reg:0.001', 51.69), ('reg:0.01', 35.06)]
2019-03-13 07:42:03,779 : Cross-validation : best param found is reg = 1e-05             with score 70.3
2019-03-13 07:42:03,779 : Evaluating...
2019-03-13 07:42:04,230 : 
Dev acc : 70.3 Test acc : 82.6             for TREC

2019-03-13 07:42:04,231 : ***** Transfer task : MRPC *****


2019-03-13 07:42:04,252 : loading BERT model bert-large-uncased
2019-03-13 07:42:04,252 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:42:04,274 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:42:04,274 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpil373856
2019-03-13 07:42:11,673 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:42:16,855 : Computing embedding for train
2019-03-13 07:42:39,216 : Computed train embeddings
2019-03-13 07:42:39,216 : Computing embedding for test
2019-03-13 07:42:49,027 : Computed test embeddings
2019-03-13 07:42:49,047 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 07:42:54,090 : [('reg:1e-05', 69.55), ('reg:0.0001', 69.6), ('reg:0.001', 69.87), ('reg:0.01', 67.62)]
2019-03-13 07:42:54,090 : Cross-validation : best param found is reg = 0.001             with score 69.87
2019-03-13 07:42:54,090 : Evaluating...
2019-03-13 07:42:54,375 : Dev acc : 69.87 Test acc 67.54; Test F1 80.11 for MRPC.

2019-03-13 07:42:54,376 : ***** Transfer task : SICK-Entailment*****


2019-03-13 07:42:54,438 : loading BERT model bert-large-uncased
2019-03-13 07:42:54,438 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:42:54,457 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:42:54,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8dp4jmo3
2019-03-13 07:43:01,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:43:07,183 : Computing embedding for train
2019-03-13 07:43:18,530 : Computed train embeddings
2019-03-13 07:43:18,530 : Computing embedding for dev
2019-03-13 07:43:20,077 : Computed dev embeddings
2019-03-13 07:43:20,078 : Computing embedding for test
2019-03-13 07:43:32,245 : Computed test embeddings
2019-03-13 07:43:32,282 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 07:43:33,749 : [('reg:1e-05', 64.0), ('reg:0.0001', 63.4), ('reg:0.001', 63.6), ('reg:0.01', 56.4)]
2019-03-13 07:43:33,749 : Validation : best param found is reg = 1e-05 with score             64.0
2019-03-13 07:43:33,749 : Evaluating...
2019-03-13 07:43:34,099 : 
Dev acc : 64.0 Test acc : 63.26 for                        SICK entailment

2019-03-13 07:43:34,100 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 07:43:34,127 : loading BERT model bert-large-uncased
2019-03-13 07:43:34,127 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:43:34,184 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:43:34,184 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfksu0ncl
2019-03-13 07:43:41,679 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:43:47,012 : Computing embedding for train
2019-03-13 07:43:58,369 : Computed train embeddings
2019-03-13 07:43:58,369 : Computing embedding for dev
2019-03-13 07:43:59,920 : Computed dev embeddings
2019-03-13 07:43:59,920 : Computing embedding for test
2019-03-13 07:44:12,113 : Computed test embeddings
2019-03-13 07:44:52,189 : Dev : Pearson 0.7036093456250924
2019-03-13 07:44:52,189 : Test : Pearson 0.7322408421205844 Spearman 0.6820167398653958 MSE 0.4743907121666623                        for SICK Relatedness

2019-03-13 07:44:52,190 : 

***** Transfer task : STSBenchmark*****


2019-03-13 07:44:52,229 : loading BERT model bert-large-uncased
2019-03-13 07:44:52,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:44:52,259 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:44:52,259 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp91rt7nm5
2019-03-13 07:44:59,700 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:45:04,975 : Computing embedding for train
2019-03-13 07:45:23,559 : Computed train embeddings
2019-03-13 07:45:23,559 : Computing embedding for dev
2019-03-13 07:45:29,204 : Computed dev embeddings
2019-03-13 07:45:29,204 : Computing embedding for test
2019-03-13 07:45:33,821 : Computed test embeddings
2019-03-13 07:46:05,716 : Dev : Pearson 0.6934827920608553
2019-03-13 07:46:05,716 : Test : Pearson 0.6051400695398885 Spearman 0.5945608859276116 MSE 1.7760194105058575                        for SICK Relatedness

2019-03-13 07:46:05,716 : ***** Transfer task : SNLI Entailment*****


2019-03-13 07:46:10,813 : loading BERT model bert-large-uncased
2019-03-13 07:46:10,813 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 07:46:10,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 07:46:10,934 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpur0l3x7i
2019-03-13 07:46:18,354 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 07:46:24,008 : PROGRESS (encoding): 0.00%
2019-03-13 07:49:11,355 : PROGRESS (encoding): 14.56%
2019-03-13 07:52:23,400 : PROGRESS (encoding): 29.12%
2019-03-13 07:55:34,557 : PROGRESS (encoding): 43.69%
2019-03-13 07:58:57,130 : PROGRESS (encoding): 58.25%
2019-03-13 08:02:43,198 : PROGRESS (encoding): 72.81%
2019-03-13 08:06:27,968 : PROGRESS (encoding): 87.37%
2019-03-13 08:10:31,006 : PROGRESS (encoding): 0.00%
2019-03-13 08:11:01,573 : PROGRESS (encoding): 0.00%
2019-03-13 08:11:30,955 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:12:12,385 : [('reg:1e-09', 64.15)]
2019-03-13 08:12:12,386 : Validation : best param found is reg = 1e-09 with score             64.15
2019-03-13 08:12:12,386 : Evaluating...
2019-03-13 08:12:54,114 : Dev acc : 64.15 Test acc : 65.31 for SNLI

2019-03-13 08:12:54,114 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 08:12:54,316 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 08:12:55,392 : loading BERT model bert-large-uncased
2019-03-13 08:12:55,392 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:12:55,418 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:12:55,418 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9x1r62f8
2019-03-13 08:13:02,866 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:13:08,093 : Computing embeddings for train/dev/test
2019-03-13 08:16:40,020 : Computed embeddings
2019-03-13 08:16:40,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:17:09,123 : [('reg:1e-05', 86.08), ('reg:0.0001', 77.04), ('reg:0.001', 54.19), ('reg:0.01', 33.1)]
2019-03-13 08:17:09,123 : Validation : best param found is reg = 1e-05 with score             86.08
2019-03-13 08:17:09,123 : Evaluating...
2019-03-13 08:17:18,994 : 
Dev acc : 86.1 Test acc : 87.3 for LENGTH classification

2019-03-13 08:17:18,995 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 08:17:19,388 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 08:17:19,436 : loading BERT model bert-large-uncased
2019-03-13 08:17:19,436 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:17:19,469 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:17:19,469 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9wcsjtlz
2019-03-13 08:17:26,906 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:17:32,199 : Computing embeddings for train/dev/test
2019-03-13 08:20:47,875 : Computed embeddings
2019-03-13 08:20:47,875 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:21:14,450 : [('reg:1e-05', 4.12), ('reg:0.0001', 0.37), ('reg:0.001', 0.23), ('reg:0.01', 0.18)]
2019-03-13 08:21:14,450 : Validation : best param found is reg = 1e-05 with score             4.12
2019-03-13 08:21:14,450 : Evaluating...
2019-03-13 08:21:22,096 : 
Dev acc : 4.1 Test acc : 3.9 for WORDCONTENT classification

2019-03-13 08:21:22,098 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 08:21:22,478 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 08:21:22,543 : loading BERT model bert-large-uncased
2019-03-13 08:21:22,544 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:21:22,569 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:21:22,569 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyp9afqff
2019-03-13 08:21:30,071 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:21:35,416 : Computing embeddings for train/dev/test
2019-03-13 08:24:39,454 : Computed embeddings
2019-03-13 08:24:39,454 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:25:01,845 : [('reg:1e-05', 30.91), ('reg:0.0001', 27.23), ('reg:0.001', 19.69), ('reg:0.01', 18.22)]
2019-03-13 08:25:01,845 : Validation : best param found is reg = 1e-05 with score             30.91
2019-03-13 08:25:01,845 : Evaluating...
2019-03-13 08:25:08,286 : 
Dev acc : 30.9 Test acc : 31.2 for DEPTH classification

2019-03-13 08:25:08,287 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 08:25:08,677 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 08:25:08,742 : loading BERT model bert-large-uncased
2019-03-13 08:25:08,742 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:25:08,852 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:25:08,852 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp20zryar2
2019-03-13 08:25:16,366 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:25:21,829 : Computing embeddings for train/dev/test
2019-03-13 08:28:11,954 : Computed embeddings
2019-03-13 08:28:11,955 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:28:44,006 : [('reg:1e-05', 66.21), ('reg:0.0001', 52.72), ('reg:0.001', 25.38), ('reg:0.01', 9.12)]
2019-03-13 08:28:44,006 : Validation : best param found is reg = 1e-05 with score             66.21
2019-03-13 08:28:44,006 : Evaluating...
2019-03-13 08:28:52,946 : 
Dev acc : 66.2 Test acc : 66.5 for TOPCONSTITUENTS classification

2019-03-13 08:28:52,947 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 08:28:53,306 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 08:28:53,374 : loading BERT model bert-large-uncased
2019-03-13 08:28:53,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:28:53,503 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:28:53,503 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt0s0fm0z
2019-03-13 08:29:00,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:29:06,186 : Computing embeddings for train/dev/test
2019-03-13 08:32:10,761 : Computed embeddings
2019-03-13 08:32:10,761 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:32:47,099 : [('reg:1e-05', 88.53), ('reg:0.0001', 87.07), ('reg:0.001', 83.39), ('reg:0.01', 76.78)]
2019-03-13 08:32:47,099 : Validation : best param found is reg = 1e-05 with score             88.53
2019-03-13 08:32:47,099 : Evaluating...
2019-03-13 08:32:57,350 : 
Dev acc : 88.5 Test acc : 88.1 for BIGRAMSHIFT classification

2019-03-13 08:32:57,351 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 08:32:57,904 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 08:32:57,969 : loading BERT model bert-large-uncased
2019-03-13 08:32:57,969 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:32:57,998 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:32:57,998 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0wby12vf
2019-03-13 08:33:05,417 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:33:10,647 : Computing embeddings for train/dev/test
2019-03-13 08:36:11,421 : Computed embeddings
2019-03-13 08:36:11,421 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:36:40,178 : [('reg:1e-05', 89.06), ('reg:0.0001', 88.29), ('reg:0.001', 85.21), ('reg:0.01', 76.32)]
2019-03-13 08:36:40,178 : Validation : best param found is reg = 1e-05 with score             89.06
2019-03-13 08:36:40,178 : Evaluating...
2019-03-13 08:36:49,265 : 
Dev acc : 89.1 Test acc : 85.7 for TENSE classification

2019-03-13 08:36:49,266 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 08:36:49,686 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 08:36:49,749 : loading BERT model bert-large-uncased
2019-03-13 08:36:49,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:36:49,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:36:49,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbj72d7q5
2019-03-13 08:36:57,220 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:37:02,481 : Computing embeddings for train/dev/test
2019-03-13 08:40:14,142 : Computed embeddings
2019-03-13 08:40:14,142 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:40:50,459 : [('reg:1e-05', 84.6), ('reg:0.0001', 81.82), ('reg:0.001', 75.64), ('reg:0.01', 69.05)]
2019-03-13 08:40:50,459 : Validation : best param found is reg = 1e-05 with score             84.6
2019-03-13 08:40:50,459 : Evaluating...
2019-03-13 08:41:00,883 : 
Dev acc : 84.6 Test acc : 83.6 for SUBJNUMBER classification

2019-03-13 08:41:00,884 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 08:41:01,316 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 08:41:01,386 : loading BERT model bert-large-uncased
2019-03-13 08:41:01,387 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:41:01,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:41:01,513 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7aivds4g
2019-03-13 08:41:08,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:41:14,269 : Computing embeddings for train/dev/test
2019-03-13 08:44:21,496 : Computed embeddings
2019-03-13 08:44:21,496 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:44:51,969 : [('reg:1e-05', 81.7), ('reg:0.0001', 79.26), ('reg:0.001', 73.72), ('reg:0.01', 59.98)]
2019-03-13 08:44:51,969 : Validation : best param found is reg = 1e-05 with score             81.7
2019-03-13 08:44:51,970 : Evaluating...
2019-03-13 08:45:01,778 : 
Dev acc : 81.7 Test acc : 81.3 for OBJNUMBER classification

2019-03-13 08:45:01,779 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 08:45:02,364 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 08:45:02,432 : loading BERT model bert-large-uncased
2019-03-13 08:45:02,432 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:45:02,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:45:02,459 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpn9tw2yl1
2019-03-13 08:45:09,857 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:45:15,069 : Computing embeddings for train/dev/test
2019-03-13 08:48:52,370 : Computed embeddings
2019-03-13 08:48:52,370 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:49:24,412 : [('reg:1e-05', 64.05), ('reg:0.0001', 63.68), ('reg:0.001', 61.74), ('reg:0.01', 55.52)]
2019-03-13 08:49:24,412 : Validation : best param found is reg = 1e-05 with score             64.05
2019-03-13 08:49:24,412 : Evaluating...
2019-03-13 08:49:32,978 : 
Dev acc : 64.0 Test acc : 64.6 for ODDMANOUT classification

2019-03-13 08:49:32,979 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 08:49:33,378 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 08:49:33,456 : loading BERT model bert-large-uncased
2019-03-13 08:49:33,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:49:33,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:49:33,584 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu1jbtv5p
2019-03-13 08:49:41,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:49:46,283 : Computing embeddings for train/dev/test
2019-03-13 08:53:21,951 : Computed embeddings
2019-03-13 08:53:21,951 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 08:53:48,761 : [('reg:1e-05', 61.99), ('reg:0.0001', 59.41), ('reg:0.001', 50.54), ('reg:0.01', 50.0)]
2019-03-13 08:53:48,761 : Validation : best param found is reg = 1e-05 with score             61.99
2019-03-13 08:53:48,761 : Evaluating...
2019-03-13 08:53:55,058 : 
Dev acc : 62.0 Test acc : 61.3 for COORDINATIONINVERSION classification

2019-03-13 08:53:55,060 : total results: {'STS12': {'MSRpar': {'pearson': (0.3330454612280139, 6.947408500065338e-21), 'spearman': SpearmanrResult(correlation=0.3889396821079955, pvalue=1.701391887626206e-28), 'nsamples': 750}, 'MSRvid': {'pearson': (0.410842535033031, 6.605047947527026e-32), 'spearman': SpearmanrResult(correlation=0.43185295495948434, pvalue=2.001466904273818e-35), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.44317372977059816, 1.669706029097856e-23), 'spearman': SpearmanrResult(correlation=0.5667825493880466, pvalue=2.307558782939666e-40), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5210276768458223, 2.0036471551091133e-53), 'spearman': SpearmanrResult(correlation=0.5427932787788031, pvalue=1.0915482576089338e-58), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5710452832582286, 6.61469065866222e-36), 'spearman': SpearmanrResult(correlation=0.5458174057394983, pvalue=2.3488126200184072e-32), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4558269372271388, 'wmean': 0.4439995382287285}, 'spearman': {'mean': 0.49523717419476565, 'wmean': 0.4828261814491266}}}, 'STS13': {'FNWN': {'pearson': (0.32832069015514415, 3.983900377075523e-06), 'spearman': SpearmanrResult(correlation=0.3236995777050195, pvalue=5.531666364665144e-06), 'nsamples': 189}, 'headlines': {'pearson': (0.5206683684835937, 2.429655344039257e-53), 'spearman': SpearmanrResult(correlation=0.5168148265948134, pvalue=1.8932154150960638e-52), 'nsamples': 750}, 'OnWN': {'pearson': (0.3679893606318131, 1.9723401513930818e-19), 'spearman': SpearmanrResult(correlation=0.409608365304942, pvalue=4.1554567268057444e-24), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40565947309018363, 'wmean': 0.43933061207764307}, 'spearman': {'mean': 0.41670758986825834, 'wmean': 0.4523870887122875}}}, 'STS14': {'deft-forum': {'pearson': (0.2883181921775925, 4.606180250813459e-10), 'spearman': SpearmanrResult(correlation=0.2926702683076043, pvalue=2.446199891334056e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.6066894289666788, 1.5083976405241357e-31), 'spearman': SpearmanrResult(correlation=0.5932421481820903, pvalue=6.601632324501445e-30), 'nsamples': 300}, 'headlines': {'pearson': (0.43651265871949674, 3.0678602092644274e-36), 'spearman': SpearmanrResult(correlation=0.41559537257838486, pvalue=1.1106636451421533e-32), 'nsamples': 750}, 'images': {'pearson': (0.49003210017946897, 1.4756790031874986e-46), 'spearman': SpearmanrResult(correlation=0.5008098950750275, pvalue=7.256653482333388e-49), 'nsamples': 750}, 'OnWN': {'pearson': (0.46007289269496243, 1.47673806510396e-40), 'spearman': SpearmanrResult(correlation=0.5135561778069168, pvalue=1.0527787787375616e-51), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5431429591461749, 8.918708988107377e-59), 'spearman': SpearmanrResult(correlation=0.5171741693136653, pvalue=1.565077199294421e-52), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.47079470531406237, 'wmean': 0.469085459526666}, 'spearman': {'mean': 0.4721746718772815, 'wmean': 0.47200692700627866}}}, 'STS15': {'answers-forums': {'pearson': (0.464804564864303, 1.697822007999395e-21), 'spearman': SpearmanrResult(correlation=0.4682477878306776, pvalue=7.837059406864541e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.5320058983146477, 4.951975151907112e-56), 'spearman': SpearmanrResult(correlation=0.5733534104741584, pvalue=9.200559334600925e-67), 'nsamples': 750}, 'belief': {'pearson': (0.5626160333766337, 1.0901577247772885e-32), 'spearman': SpearmanrResult(correlation=0.6074512253933543, pvalue=3.43705410918107e-39), 'nsamples': 375}, 'headlines': {'pearson': (0.5547894301471401, 9.29134208714567e-62), 'spearman': SpearmanrResult(correlation=0.5601621923282946, pvalue=3.565955423815844e-63), 'nsamples': 750}, 'images': {'pearson': (0.5566356924294137, 3.0509634282189875e-62), 'spearman': SpearmanrResult(correlation=0.6109491323522818, pvalue=6.149852611377604e-78), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5341703238264277, 'wmean': 0.5392853300029173}, 'spearman': {'mean': 0.5640327496757533, 'wmean': 0.5705785604416876}}}, 'STS16': {'answer-answer': {'pearson': (0.5001780219764604, 1.743524919786975e-17), 'spearman': SpearmanrResult(correlation=0.5318651780745395, pvalue=5.960779389337486e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.5817658951041788, 5.9761632228469736e-24), 'spearman': SpearmanrResult(correlation=0.6011655354469764, pvalue=7.394336131671828e-26), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7281715851729231, 2.8387078999494882e-39), 'spearman': SpearmanrResult(correlation=0.7632401885625408, pvalue=3.8824950026553765e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7863895443664768, 1.53565663094927e-52), 'spearman': SpearmanrResult(correlation=0.8297177512068372, pvalue=3.234613239868998e-63), 'nsamples': 244}, 'question-question': {'pearson': (0.25216618533389046, 0.00023017663137446176), 'spearman': SpearmanrResult(correlation=0.2698182886318562, pvalue=7.787414254931237e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5697342463907858, 'wmean': 0.5766999760648712}, 'spearman': {'mean': 0.59916138838455, 'wmean': 0.6063847980987697}}}, 'MR': {'devacc': 74.15, 'acc': 74.2, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.62, 'acc': 76.16, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 78.9, 'acc': 81.56, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.06, 'acc': 92.29, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 78.67, 'acc': 78.25, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.05, 'acc': 40.59, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 70.3, 'acc': 82.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.87, 'acc': 67.54, 'f1': 80.11, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 64.0, 'acc': 63.26, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7036093456250924, 'pearson': 0.7322408421205844, 'spearman': 0.6820167398653958, 'mse': 0.4743907121666623, 'yhat': array([2.70323446, 3.89596578, 1.42184137, ..., 3.11789336, 4.28459973,        4.6384285 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6934827920608553, 'pearson': 0.6051400695398885, 'spearman': 0.5945608859276116, 'mse': 1.7760194105058575, 'yhat': array([1.80573898, 1.67273906, 2.57286318, ..., 3.98492469, 3.58537485,        3.34566134]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 64.15, 'acc': 65.31, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 86.08, 'acc': 87.31, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 4.12, 'acc': 3.89, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.91, 'acc': 31.21, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 66.21, 'acc': 66.53, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.53, 'acc': 88.06, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.06, 'acc': 85.69, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.6, 'acc': 83.58, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 81.7, 'acc': 81.32, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.05, 'acc': 64.56, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 61.99, 'acc': 61.31, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 08:53:55,061 : STS12 p=0.4440, STS12 s=0.4828, STS13 p=0.4393, STS13 s=0.4524, STS14 p=0.4691, STS14 s=0.4720, STS15 p=0.5393, STS15 s=0.5706, STS 16 p=0.5767, STS16 s=0.6064, STS B p=0.6051, STS B s=0.5946, STS B m=1.7760, SICK-R p=0.7322, SICK-R s=0.6820, SICK-P m=0.4744
2019-03-13 08:53:55,061 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 08:53:55,061 : 0.4440,0.4828,0.4393,0.4524,0.4691,0.4720,0.5393,0.5706,0.5767,0.6064,0.6051,0.5946,1.7760,0.7322,0.6820,0.4744
2019-03-13 08:53:55,061 : MR=74.20, CR=76.16, SUBJ=92.29, MPQA=81.56, SST-B=78.25, SST-F=40.59, TREC=82.60, SICK-E=63.26, SNLI=65.31, MRPC=67.54, MRPC f=80.11
2019-03-13 08:53:55,061 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 08:53:55,061 : 74.20,76.16,92.29,81.56,78.25,40.59,82.60,63.26,65.31,67.54,80.11
2019-03-13 08:53:55,061 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 08:53:55,061 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 08:53:55,061 : na,na,na,na,na,na,na,na,na,na
2019-03-13 08:53:55,061 : SentLen=87.31, WC=3.89, TreeDepth=31.21, TopConst=66.53, BShift=88.06, Tense=85.69, SubjNum=83.58, ObjNum=81.32, SOMO=64.56, CoordInv=61.31, average=65.35
2019-03-13 08:53:55,061 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 08:53:55,061 : 87.31,3.89,31.21,66.53,88.06,85.69,83.58,81.32,64.56,61.31,65.35
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,061 : layer 14
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,061 : ********************************************************************************
2019-03-13 08:53:55,149 : ***** Transfer task : STS12 *****


2019-03-13 08:53:55,161 : loading BERT model bert-large-uncased
2019-03-13 08:53:55,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:53:55,179 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:53:55,179 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfo9pajt9
2019-03-13 08:54:02,604 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:54:11,899 : MSRpar : pearson = 0.3398, spearman = 0.3884
2019-03-13 08:54:13,546 : MSRvid : pearson = 0.4475, spearman = 0.4669
2019-03-13 08:54:14,964 : SMTeuroparl : pearson = 0.4700, spearman = 0.5964
2019-03-13 08:54:17,673 : surprise.OnWN : pearson = 0.5819, spearman = 0.6095
2019-03-13 08:54:19,106 : surprise.SMTnews : pearson = 0.6227, spearman = 0.5625
2019-03-13 08:54:19,106 : ALL (weighted average) : Pearson = 0.4797,             Spearman = 0.5138
2019-03-13 08:54:19,106 : ALL (average) : Pearson = 0.4924,             Spearman = 0.5247

2019-03-13 08:54:19,106 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 08:54:19,114 : loading BERT model bert-large-uncased
2019-03-13 08:54:19,114 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:54:19,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:54:19,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5lblws3t
2019-03-13 08:54:26,549 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:54:33,154 : FNWN : pearson = 0.2588, spearman = 0.2811
2019-03-13 08:54:35,056 : headlines : pearson = 0.5453, spearman = 0.5331
2019-03-13 08:54:36,529 : OnWN : pearson = 0.4046, spearman = 0.4353
2019-03-13 08:54:36,529 : ALL (weighted average) : Pearson = 0.4566,             Spearman = 0.4648
2019-03-13 08:54:36,529 : ALL (average) : Pearson = 0.4029,             Spearman = 0.4165

2019-03-13 08:54:36,529 : ***** Transfer task : STS14 *****


2019-03-13 08:54:36,546 : loading BERT model bert-large-uncased
2019-03-13 08:54:36,546 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:54:36,564 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:54:36,564 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj5y_nbjd
2019-03-13 08:54:44,020 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:54:50,601 : deft-forum : pearson = 0.2908, spearman = 0.2883
2019-03-13 08:54:52,246 : deft-news : pearson = 0.6288, spearman = 0.6012
2019-03-13 08:54:54,423 : headlines : pearson = 0.4755, spearman = 0.4381
2019-03-13 08:54:56,508 : images : pearson = 0.4913, spearman = 0.4937
2019-03-13 08:54:58,644 : OnWN : pearson = 0.5438, spearman = 0.5783
2019-03-13 08:55:01,517 : tweet-news : pearson = 0.6031, spearman = 0.5586
2019-03-13 08:55:01,517 : ALL (weighted average) : Pearson = 0.5079,             Spearman = 0.4964
2019-03-13 08:55:01,517 : ALL (average) : Pearson = 0.5056,             Spearman = 0.4930

2019-03-13 08:55:01,517 : ***** Transfer task : STS15 *****


2019-03-13 08:55:01,551 : loading BERT model bert-large-uncased
2019-03-13 08:55:01,551 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:55:01,568 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:55:01,568 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphrnv40x7
2019-03-13 08:55:08,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:55:16,110 : answers-forums : pearson = 0.5284, spearman = 0.5180
2019-03-13 08:55:18,202 : answers-students : pearson = 0.5837, spearman = 0.6047
2019-03-13 08:55:20,260 : belief : pearson = 0.6151, spearman = 0.6458
2019-03-13 08:55:22,522 : headlines : pearson = 0.5724, spearman = 0.5710
2019-03-13 08:55:24,663 : images : pearson = 0.6034, spearman = 0.6294
2019-03-13 08:55:24,663 : ALL (weighted average) : Pearson = 0.5828,             Spearman = 0.5968
2019-03-13 08:55:24,663 : ALL (average) : Pearson = 0.5806,             Spearman = 0.5938

2019-03-13 08:55:24,663 : ***** Transfer task : STS16 *****


2019-03-13 08:55:24,732 : loading BERT model bert-large-uncased
2019-03-13 08:55:24,733 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:55:24,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:55:24,750 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1n9wlx18
2019-03-13 08:55:32,172 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:55:38,117 : answer-answer : pearson = 0.5478, spearman = 0.5819
2019-03-13 08:55:38,780 : headlines : pearson = 0.6252, spearman = 0.6236
2019-03-13 08:55:39,665 : plagiarism : pearson = 0.7722, spearman = 0.7912
2019-03-13 08:55:41,170 : postediting : pearson = 0.8031, spearman = 0.8396
2019-03-13 08:55:41,780 : question-question : pearson = 0.3293, spearman = 0.3594
2019-03-13 08:55:41,780 : ALL (weighted average) : Pearson = 0.6216,             Spearman = 0.6450
2019-03-13 08:55:41,780 : ALL (average) : Pearson = 0.6155,             Spearman = 0.6391

2019-03-13 08:55:41,780 : ***** Transfer task : MR *****


2019-03-13 08:55:41,799 : loading BERT model bert-large-uncased
2019-03-13 08:55:41,799 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:55:41,817 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:55:41,818 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph7_iu978
2019-03-13 08:55:49,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:55:54,595 : Generating sentence embeddings
2019-03-13 08:56:26,253 : Generated sentence embeddings
2019-03-13 08:56:26,254 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:56:35,434 : Best param found at split 1: l2reg = 1e-05                 with score 74.43
2019-03-13 08:56:46,719 : Best param found at split 2: l2reg = 1e-05                 with score 74.64
2019-03-13 08:56:58,969 : Best param found at split 3: l2reg = 1e-05                 with score 75.02
2019-03-13 08:57:09,398 : Best param found at split 4: l2reg = 1e-05                 with score 73.56
2019-03-13 08:57:21,540 : Best param found at split 5: l2reg = 1e-05                 with score 74.19
2019-03-13 08:57:22,289 : Dev acc : 74.37 Test acc : 74.23

2019-03-13 08:57:22,290 : ***** Transfer task : CR *****


2019-03-13 08:57:22,298 : loading BERT model bert-large-uncased
2019-03-13 08:57:22,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:57:22,319 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:57:22,319 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmjonopiu
2019-03-13 08:57:30,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:57:35,157 : Generating sentence embeddings
2019-03-13 08:57:43,506 : Generated sentence embeddings
2019-03-13 08:57:43,507 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:57:48,307 : Best param found at split 1: l2reg = 1e-05                 with score 78.67
2019-03-13 08:57:52,560 : Best param found at split 2: l2reg = 1e-05                 with score 78.04
2019-03-13 08:57:57,139 : Best param found at split 3: l2reg = 1e-05                 with score 78.81
2019-03-13 08:58:01,309 : Best param found at split 4: l2reg = 1e-05                 with score 78.62
2019-03-13 08:58:05,873 : Best param found at split 5: l2reg = 0.0001                 with score 77.56
2019-03-13 08:58:06,206 : Dev acc : 78.34 Test acc : 75.18

2019-03-13 08:58:06,207 : ***** Transfer task : MPQA *****


2019-03-13 08:58:06,214 : loading BERT model bert-large-uncased
2019-03-13 08:58:06,215 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:58:06,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:58:06,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmph20ck3ag
2019-03-13 08:58:13,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:58:18,884 : Generating sentence embeddings
2019-03-13 08:58:26,476 : Generated sentence embeddings
2019-03-13 08:58:26,476 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 08:58:36,093 : Best param found at split 1: l2reg = 1e-05                 with score 81.73
2019-03-13 08:58:45,894 : Best param found at split 2: l2reg = 0.0001                 with score 82.54
2019-03-13 08:58:57,263 : Best param found at split 3: l2reg = 1e-05                 with score 79.84
2019-03-13 08:59:08,427 : Best param found at split 4: l2reg = 1e-05                 with score 80.9
2019-03-13 08:59:19,270 : Best param found at split 5: l2reg = 1e-05                 with score 81.27
2019-03-13 08:59:19,786 : Dev acc : 81.26 Test acc : 83.07

2019-03-13 08:59:19,787 : ***** Transfer task : SUBJ *****


2019-03-13 08:59:19,802 : loading BERT model bert-large-uncased
2019-03-13 08:59:19,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 08:59:19,822 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 08:59:19,823 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqhgm04qk
2019-03-13 08:59:27,256 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 08:59:32,606 : Generating sentence embeddings
2019-03-13 09:00:03,603 : Generated sentence embeddings
2019-03-13 09:00:03,603 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 09:00:14,680 : Best param found at split 1: l2reg = 1e-05                 with score 93.94
2019-03-13 09:00:25,849 : Best param found at split 2: l2reg = 1e-05                 with score 93.89
2019-03-13 09:00:37,278 : Best param found at split 3: l2reg = 1e-05                 with score 93.7
2019-03-13 09:00:48,305 : Best param found at split 4: l2reg = 1e-05                 with score 94.11
2019-03-13 09:00:59,603 : Best param found at split 5: l2reg = 1e-05                 with score 93.61
2019-03-13 09:01:00,423 : Dev acc : 93.85 Test acc : 93.71

2019-03-13 09:01:00,424 : ***** Transfer task : SST Binary classification *****


2019-03-13 09:01:00,517 : loading BERT model bert-large-uncased
2019-03-13 09:01:00,517 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:01:00,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:01:00,592 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppnino2td
2019-03-13 09:01:08,027 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:01:13,324 : Computing embedding for train
2019-03-13 09:02:54,064 : Computed train embeddings
2019-03-13 09:02:54,064 : Computing embedding for dev
2019-03-13 09:02:56,267 : Computed dev embeddings
2019-03-13 09:02:56,267 : Computing embedding for test
2019-03-13 09:03:00,894 : Computed test embeddings
2019-03-13 09:03:00,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:03:20,368 : [('reg:1e-05', 81.19), ('reg:0.0001', 80.05), ('reg:0.001', 77.06), ('reg:0.01', 71.79)]
2019-03-13 09:03:20,368 : Validation : best param found is reg = 1e-05 with score             81.19
2019-03-13 09:03:20,368 : Evaluating...
2019-03-13 09:03:25,377 : 
Dev acc : 81.19 Test acc : 80.67 for             SST Binary classification

2019-03-13 09:03:25,378 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 09:03:25,428 : loading BERT model bert-large-uncased
2019-03-13 09:03:25,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:03:25,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:03:25,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpok6uhsaa
2019-03-13 09:03:32,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:03:38,178 : Computing embedding for train
2019-03-13 09:04:00,135 : Computed train embeddings
2019-03-13 09:04:00,136 : Computing embedding for dev
2019-03-13 09:04:03,013 : Computed dev embeddings
2019-03-13 09:04:03,013 : Computing embedding for test
2019-03-13 09:04:08,690 : Computed test embeddings
2019-03-13 09:04:08,690 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:04:11,144 : [('reg:1e-05', 41.51), ('reg:0.0001', 41.6), ('reg:0.001', 40.42), ('reg:0.01', 32.06)]
2019-03-13 09:04:11,145 : Validation : best param found is reg = 0.0001 with score             41.6
2019-03-13 09:04:11,145 : Evaluating...
2019-03-13 09:04:11,781 : 
Dev acc : 41.6 Test acc : 43.35 for             SST Fine-Grained classification

2019-03-13 09:04:11,782 : ***** Transfer task : TREC *****


2019-03-13 09:04:11,794 : loading BERT model bert-large-uncased
2019-03-13 09:04:11,794 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:04:11,812 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:04:11,813 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbdzbbcn1
2019-03-13 09:04:19,242 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:04:32,029 : Computed train embeddings
2019-03-13 09:04:32,621 : Computed test embeddings
2019-03-13 09:04:32,621 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 09:04:39,389 : [('reg:1e-05', 73.17), ('reg:0.0001', 71.11), ('reg:0.001', 53.47), ('reg:0.01', 38.37)]
2019-03-13 09:04:39,390 : Cross-validation : best param found is reg = 1e-05             with score 73.17
2019-03-13 09:04:39,390 : Evaluating...
2019-03-13 09:04:40,055 : 
Dev acc : 73.17 Test acc : 87.8             for TREC

2019-03-13 09:04:40,056 : ***** Transfer task : MRPC *****


2019-03-13 09:04:40,078 : loading BERT model bert-large-uncased
2019-03-13 09:04:40,079 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:04:40,099 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:04:40,099 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi6lqq9m6
2019-03-13 09:04:47,550 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:04:52,682 : Computing embedding for train
2019-03-13 09:05:15,029 : Computed train embeddings
2019-03-13 09:05:15,029 : Computing embedding for test
2019-03-13 09:05:24,836 : Computed test embeddings
2019-03-13 09:05:24,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 09:05:29,454 : [('reg:1e-05', 69.9), ('reg:0.0001', 69.06), ('reg:0.001', 69.09), ('reg:0.01', 67.71)]
2019-03-13 09:05:29,454 : Cross-validation : best param found is reg = 1e-05             with score 69.9
2019-03-13 09:05:29,454 : Evaluating...
2019-03-13 09:05:29,760 : Dev acc : 69.9 Test acc 64.29; Test F1 68.18 for MRPC.

2019-03-13 09:05:29,761 : ***** Transfer task : SICK-Entailment*****


2019-03-13 09:05:29,821 : loading BERT model bert-large-uncased
2019-03-13 09:05:29,821 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:05:29,840 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:05:29,840 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkpgvwax4
2019-03-13 09:05:37,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:05:42,593 : Computing embedding for train
2019-03-13 09:05:53,925 : Computed train embeddings
2019-03-13 09:05:53,925 : Computing embedding for dev
2019-03-13 09:05:55,476 : Computed dev embeddings
2019-03-13 09:05:55,476 : Computing embedding for test
2019-03-13 09:06:07,667 : Computed test embeddings
2019-03-13 09:06:07,703 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:06:08,929 : [('reg:1e-05', 64.4), ('reg:0.0001', 73.0), ('reg:0.001', 66.2), ('reg:0.01', 56.4)]
2019-03-13 09:06:08,929 : Validation : best param found is reg = 0.0001 with score             73.0
2019-03-13 09:06:08,929 : Evaluating...
2019-03-13 09:06:09,230 : 
Dev acc : 73.0 Test acc : 71.46 for                        SICK entailment

2019-03-13 09:06:09,231 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 09:06:09,257 : loading BERT model bert-large-uncased
2019-03-13 09:06:09,258 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:06:09,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:06:09,315 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplnzo7t1i
2019-03-13 09:06:16,753 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:06:22,015 : Computing embedding for train
2019-03-13 09:06:33,394 : Computed train embeddings
2019-03-13 09:06:33,394 : Computing embedding for dev
2019-03-13 09:06:34,950 : Computed dev embeddings
2019-03-13 09:06:34,950 : Computing embedding for test
2019-03-13 09:06:47,172 : Computed test embeddings
2019-03-13 09:07:30,522 : Dev : Pearson 0.7219179802005992
2019-03-13 09:07:30,522 : Test : Pearson 0.7485555942820931 Spearman 0.6963963298287459 MSE 0.45287112039335564                        for SICK Relatedness

2019-03-13 09:07:30,523 : 

***** Transfer task : STSBenchmark*****


2019-03-13 09:07:30,563 : loading BERT model bert-large-uncased
2019-03-13 09:07:30,563 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:07:30,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:07:30,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1j8la5p8
2019-03-13 09:07:38,026 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:07:43,334 : Computing embedding for train
2019-03-13 09:08:01,885 : Computed train embeddings
2019-03-13 09:08:01,885 : Computing embedding for dev
2019-03-13 09:08:07,532 : Computed dev embeddings
2019-03-13 09:08:07,532 : Computing embedding for test
2019-03-13 09:08:12,147 : Computed test embeddings
2019-03-13 09:08:42,109 : Dev : Pearson 0.6843491878847726
2019-03-13 09:08:42,109 : Test : Pearson 0.6310764098081472 Spearman 0.6290626159710294 MSE 1.697673494350461                        for SICK Relatedness

2019-03-13 09:08:42,109 : ***** Transfer task : SNLI Entailment*****


2019-03-13 09:08:47,169 : loading BERT model bert-large-uncased
2019-03-13 09:08:47,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:08:47,239 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:08:47,239 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptx72iat0
2019-03-13 09:08:54,622 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:09:00,203 : PROGRESS (encoding): 0.00%
2019-03-13 09:11:47,248 : PROGRESS (encoding): 14.56%
2019-03-13 09:14:59,876 : PROGRESS (encoding): 29.12%
2019-03-13 09:18:10,798 : PROGRESS (encoding): 43.69%
2019-03-13 09:21:33,752 : PROGRESS (encoding): 58.25%
2019-03-13 09:25:19,846 : PROGRESS (encoding): 72.81%
2019-03-13 09:29:04,610 : PROGRESS (encoding): 87.37%
2019-03-13 09:33:07,711 : PROGRESS (encoding): 0.00%
2019-03-13 09:33:38,344 : PROGRESS (encoding): 0.00%
2019-03-13 09:34:07,854 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:34:45,206 : [('reg:1e-09', 63.81)]
2019-03-13 09:34:45,206 : Validation : best param found is reg = 1e-09 with score             63.81
2019-03-13 09:34:45,206 : Evaluating...
2019-03-13 09:35:21,886 : Dev acc : 63.81 Test acc : 63.98 for SNLI

2019-03-13 09:35:21,887 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 09:35:22,096 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 09:35:23,158 : loading BERT model bert-large-uncased
2019-03-13 09:35:23,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:35:23,184 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:35:23,185 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9zrs6hyq
2019-03-13 09:35:30,623 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:35:35,915 : Computing embeddings for train/dev/test
2019-03-13 09:39:08,046 : Computed embeddings
2019-03-13 09:39:08,046 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:39:31,037 : [('reg:1e-05', 82.64), ('reg:0.0001', 77.52), ('reg:0.001', 59.61), ('reg:0.01', 34.29)]
2019-03-13 09:39:31,037 : Validation : best param found is reg = 1e-05 with score             82.64
2019-03-13 09:39:31,037 : Evaluating...
2019-03-13 09:39:38,547 : 
Dev acc : 82.6 Test acc : 83.0 for LENGTH classification

2019-03-13 09:39:38,548 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 09:39:38,802 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 09:39:38,848 : loading BERT model bert-large-uncased
2019-03-13 09:39:38,848 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:39:38,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:39:38,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsqis43ht
2019-03-13 09:39:46,323 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:39:51,549 : Computing embeddings for train/dev/test
2019-03-13 09:43:07,012 : Computed embeddings
2019-03-13 09:43:07,012 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:43:40,410 : [('reg:1e-05', 4.54), ('reg:0.0001', 0.5), ('reg:0.001', 0.19), ('reg:0.01', 0.17)]
2019-03-13 09:43:40,410 : Validation : best param found is reg = 1e-05 with score             4.54
2019-03-13 09:43:40,410 : Evaluating...
2019-03-13 09:43:49,154 : 
Dev acc : 4.5 Test acc : 4.5 for WORDCONTENT classification

2019-03-13 09:43:49,155 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 09:43:49,689 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 09:43:49,755 : loading BERT model bert-large-uncased
2019-03-13 09:43:49,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:43:49,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:43:49,780 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu1z150mj
2019-03-13 09:43:57,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:44:02,492 : Computing embeddings for train/dev/test
2019-03-13 09:47:05,832 : Computed embeddings
2019-03-13 09:47:05,832 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:47:29,435 : [('reg:1e-05', 32.47), ('reg:0.0001', 27.88), ('reg:0.001', 21.06), ('reg:0.01', 19.35)]
2019-03-13 09:47:29,435 : Validation : best param found is reg = 1e-05 with score             32.47
2019-03-13 09:47:29,435 : Evaluating...
2019-03-13 09:47:37,048 : 
Dev acc : 32.5 Test acc : 31.8 for DEPTH classification

2019-03-13 09:47:37,049 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 09:47:37,447 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 09:47:37,512 : loading BERT model bert-large-uncased
2019-03-13 09:47:37,512 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:47:37,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:47:37,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpew5o5nc5
2019-03-13 09:47:45,091 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:47:50,414 : Computing embeddings for train/dev/test
2019-03-13 09:50:40,451 : Computed embeddings
2019-03-13 09:50:40,451 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:51:07,635 : [('reg:1e-05', 70.63), ('reg:0.0001', 57.38), ('reg:0.001', 28.06), ('reg:0.01', 9.25)]
2019-03-13 09:51:07,636 : Validation : best param found is reg = 1e-05 with score             70.63
2019-03-13 09:51:07,636 : Evaluating...
2019-03-13 09:51:16,432 : 
Dev acc : 70.6 Test acc : 71.1 for TOPCONSTITUENTS classification

2019-03-13 09:51:16,433 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 09:51:16,809 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 09:51:16,875 : loading BERT model bert-large-uncased
2019-03-13 09:51:16,875 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:51:16,904 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:51:16,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmputwzdihi
2019-03-13 09:51:24,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:51:29,574 : Computing embeddings for train/dev/test
2019-03-13 09:54:34,787 : Computed embeddings
2019-03-13 09:54:34,787 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:55:06,274 : [('reg:1e-05', 90.21), ('reg:0.0001', 89.12), ('reg:0.001', 86.42), ('reg:0.01', 82.62)]
2019-03-13 09:55:06,274 : Validation : best param found is reg = 1e-05 with score             90.21
2019-03-13 09:55:06,275 : Evaluating...
2019-03-13 09:55:13,694 : 
Dev acc : 90.2 Test acc : 89.8 for BIGRAMSHIFT classification

2019-03-13 09:55:13,695 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 09:55:14,082 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 09:55:14,147 : loading BERT model bert-large-uncased
2019-03-13 09:55:14,147 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:55:14,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:55:14,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj41y18wc
2019-03-13 09:55:21,616 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:55:26,872 : Computing embeddings for train/dev/test
2019-03-13 09:58:28,348 : Computed embeddings
2019-03-13 09:58:28,348 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 09:58:51,987 : [('reg:1e-05', 89.58), ('reg:0.0001', 89.22), ('reg:0.001', 87.45), ('reg:0.01', 80.67)]
2019-03-13 09:58:51,987 : Validation : best param found is reg = 1e-05 with score             89.58
2019-03-13 09:58:51,987 : Evaluating...
2019-03-13 09:58:57,572 : 
Dev acc : 89.6 Test acc : 87.5 for TENSE classification

2019-03-13 09:58:57,573 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 09:58:57,975 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 09:58:58,038 : loading BERT model bert-large-uncased
2019-03-13 09:58:58,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 09:58:58,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 09:58:58,151 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjfzmqa7h
2019-03-13 09:59:05,579 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 09:59:10,818 : Computing embeddings for train/dev/test
2019-03-13 10:02:22,336 : Computed embeddings
2019-03-13 10:02:22,336 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:02:53,203 : [('reg:1e-05', 84.46), ('reg:0.0001', 82.11), ('reg:0.001', 77.66), ('reg:0.01', 71.58)]
2019-03-13 10:02:53,203 : Validation : best param found is reg = 1e-05 with score             84.46
2019-03-13 10:02:53,203 : Evaluating...
2019-03-13 10:03:03,345 : 
Dev acc : 84.5 Test acc : 84.6 for SUBJNUMBER classification

2019-03-13 10:03:03,346 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 10:03:03,752 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 10:03:03,818 : loading BERT model bert-large-uncased
2019-03-13 10:03:03,818 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:03:03,934 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:03:03,935 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpznzf1e52
2019-03-13 10:03:11,422 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:03:16,628 : Computing embeddings for train/dev/test
2019-03-13 10:06:23,505 : Computed embeddings
2019-03-13 10:06:23,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:06:55,589 : [('reg:1e-05', 82.27), ('reg:0.0001', 80.45), ('reg:0.001', 76.18), ('reg:0.01', 67.1)]
2019-03-13 10:06:55,589 : Validation : best param found is reg = 1e-05 with score             82.27
2019-03-13 10:06:55,589 : Evaluating...
2019-03-13 10:07:05,199 : 
Dev acc : 82.3 Test acc : 83.1 for OBJNUMBER classification

2019-03-13 10:07:05,200 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 10:07:05,757 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 10:07:05,825 : loading BERT model bert-large-uncased
2019-03-13 10:07:05,826 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:07:05,853 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:07:05,853 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnp35doof
2019-03-13 10:07:13,292 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:07:18,528 : Computing embeddings for train/dev/test
2019-03-13 10:10:56,189 : Computed embeddings
2019-03-13 10:10:56,189 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:11:26,641 : [('reg:1e-05', 65.1), ('reg:0.0001', 64.79), ('reg:0.001', 63.06), ('reg:0.01', 59.86)]
2019-03-13 10:11:26,642 : Validation : best param found is reg = 1e-05 with score             65.1
2019-03-13 10:11:26,642 : Evaluating...
2019-03-13 10:11:35,297 : 
Dev acc : 65.1 Test acc : 65.3 for ODDMANOUT classification

2019-03-13 10:11:35,298 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 10:11:35,673 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 10:11:35,753 : loading BERT model bert-large-uncased
2019-03-13 10:11:35,753 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:11:35,781 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:11:35,781 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkcmfeger
2019-03-13 10:11:43,193 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:11:48,376 : Computing embeddings for train/dev/test
2019-03-13 10:15:24,170 : Computed embeddings
2019-03-13 10:15:24,170 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:15:54,878 : [('reg:1e-05', 63.3), ('reg:0.0001', 59.99), ('reg:0.001', 53.28), ('reg:0.01', 50.0)]
2019-03-13 10:15:54,879 : Validation : best param found is reg = 1e-05 with score             63.3
2019-03-13 10:15:54,879 : Evaluating...
2019-03-13 10:16:02,300 : 
Dev acc : 63.3 Test acc : 62.7 for COORDINATIONINVERSION classification

2019-03-13 10:16:02,302 : total results: {'STS12': {'MSRpar': {'pearson': (0.33981656735700655, 9.972375854130955e-22), 'spearman': SpearmanrResult(correlation=0.38839715067820546, pvalue=2.051584108992151e-28), 'nsamples': 750}, 'MSRvid': {'pearson': (0.44745597092102185, 3.337590049762499e-38), 'spearman': SpearmanrResult(correlation=0.4669326928531336, pvalue=7.03774210296332e-42), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.47000740167360955, 1.3320643051315347e-26), 'spearman': SpearmanrResult(correlation=0.5963627434016594, pvalue=1.510437018107898e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5818906387659796, 3.575137571155894e-69), 'spearman': SpearmanrResult(correlation=0.6094639640781407, pvalue=1.8153835731636604e-77), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6226934794762449, 3.251314988920183e-44), 'spearman': SpearmanrResult(correlation=0.5625060723424514, pvalue=1.1386269342985018e-34), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4923728116387725, 'wmean': 0.47974918869440625}, 'spearman': {'mean': 0.5247325246707182, 'wmean': 0.5137599027648357}}}, 'STS13': {'FNWN': {'pearson': (0.2587670561386952, 0.00032402899146935255), 'spearman': SpearmanrResult(correlation=0.2811482603400866, pvalue=8.897269932591403e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.5453143341520861, 2.530157334561716e-59), 'spearman': SpearmanrResult(correlation=0.533093686155995, pvalue=2.699052402908145e-56), 'nsamples': 750}, 'OnWN': {'pearson': (0.4046116207660935, 1.6436359958668816e-23), 'spearman': SpearmanrResult(correlation=0.4353214388281634, pvalue=2.4084422982075346e-27), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.40289767035229157, 'wmean': 0.4565865623160376}, 'spearman': {'mean': 0.416521128441415, 'wmean': 0.4647817420025815}}}, 'STS14': {'deft-forum': {'pearson': (0.29080111933560826, 3.21446746782654e-10), 'spearman': SpearmanrResult(correlation=0.2882832482379339, pvalue=4.629444071165144e-10), 'nsamples': 450}, 'deft-news': {'pearson': (0.628799675982612, 2.012315307827811e-34), 'spearman': SpearmanrResult(correlation=0.6011923470230405, pvalue=7.223941681612276e-31), 'nsamples': 300}, 'headlines': {'pearson': (0.4755396110556347, 1.399172694139219e-43), 'spearman': SpearmanrResult(correlation=0.43806200689514857, pvalue=1.633768402150101e-36), 'nsamples': 750}, 'images': {'pearson': (0.49130255716996374, 7.964570699718187e-47), 'spearman': SpearmanrResult(correlation=0.4937150587454716, pvalue=2.451687568865921e-47), 'nsamples': 750}, 'OnWN': {'pearson': (0.5437997564159256, 6.098466161641431e-59), 'spearman': SpearmanrResult(correlation=0.5783128311719558, pvalue=3.733839124541282e-68), 'nsamples': 750}, 'tweet-news': {'pearson': (0.6031042636618679, 1.7544153441745675e-75), 'spearman': SpearmanrResult(correlation=0.5585855284164598, pvalue=9.340328649840399e-63), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.505557830603602, 'wmean': 0.5079493460595603}, 'spearman': {'mean': 0.49302517008166835, 'wmean': 0.49642446259620243}}}, 'STS15': {'answers-forums': {'pearson': (0.5283848653828935, 2.3627613792228753e-28), 'spearman': SpearmanrResult(correlation=0.5179698953154119, pvalue=3.956371513077363e-27), 'nsamples': 375}, 'answers-students': {'pearson': (0.5836677569501139, 1.1028236537501039e-69), 'spearman': SpearmanrResult(correlation=0.6046920922922147, pvalue=5.658342032121156e-76), 'nsamples': 750}, 'belief': {'pearson': (0.6150852038714091, 2.1082630324988037e-40), 'spearman': SpearmanrResult(correlation=0.6458267730861922, pvalue=1.2258771733391135e-45), 'nsamples': 375}, 'headlines': {'pearson': (0.5724497163813235, 1.640022559780859e-66), 'spearman': SpearmanrResult(correlation=0.5710082406358381, pvalue=4.108330200742265e-66), 'nsamples': 750}, 'images': {'pearson': (0.603423095499525, 1.398536172775111e-75), 'spearman': SpearmanrResult(correlation=0.629440308151529, pvalue=5.252328228646658e-84), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.580602127617053, 'wmean': 0.5828189008645285}, 'spearman': {'mean': 0.5937874618962372, 'wmean': 0.596759743820096}}}, 'STS16': {'answer-answer': {'pearson': (0.5478481728965897, 2.6905581401924255e-21), 'spearman': SpearmanrResult(correlation=0.5819068940964442, pvalue=2.041135005171221e-24), 'nsamples': 254}, 'headlines': {'pearson': (0.6252162580888062, 2.0696392108587838e-28), 'spearman': SpearmanrResult(correlation=0.6236099697558115, pvalue=3.114136205356449e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7722339285630495, 8.291392680312042e-47), 'spearman': SpearmanrResult(correlation=0.7911773142373358, pvalue=1.3663274800470056e-50), 'nsamples': 230}, 'postediting': {'pearson': (0.8030594530026159, 2.4867322459543245e-56), 'spearman': SpearmanrResult(correlation=0.8395542574134666, pvalue=4.568036030662618e-66), 'nsamples': 244}, 'question-question': {'pearson': (0.32933822703028987, 1.1207501759069748e-06), 'spearman': SpearmanrResult(correlation=0.3593757176529149, pvalue=9.09776048664073e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.6155392079162703, 'wmean': 0.6216056355238759}, 'spearman': {'mean': 0.6391248306311946, 'wmean': 0.6450377568656208}}}, 'MR': {'devacc': 74.37, 'acc': 74.23, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 78.34, 'acc': 75.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.26, 'acc': 83.07, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.85, 'acc': 93.71, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.19, 'acc': 80.67, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.6, 'acc': 43.35, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 73.17, 'acc': 87.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.9, 'acc': 64.29, 'f1': 68.18, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.0, 'acc': 71.46, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7219179802005992, 'pearson': 0.7485555942820931, 'spearman': 0.6963963298287459, 'mse': 0.45287112039335564, 'yhat': array([2.98594369, 4.11659627, 2.32997331, ..., 3.51700075, 4.3535688 ,        4.49405651]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6843491878847726, 'pearson': 0.6310764098081472, 'spearman': 0.6290626159710294, 'mse': 1.697673494350461, 'yhat': array([1.69181347, 1.51942426, 2.58952115, ..., 3.93034941, 3.50399384,        3.43851065]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 63.81, 'acc': 63.98, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 82.64, 'acc': 83.05, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 4.54, 'acc': 4.55, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 32.47, 'acc': 31.82, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.63, 'acc': 71.13, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.21, 'acc': 89.76, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.58, 'acc': 87.46, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.46, 'acc': 84.6, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 82.27, 'acc': 83.09, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.1, 'acc': 65.32, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 63.3, 'acc': 62.68, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 10:16:02,302 : STS12 p=0.4797, STS12 s=0.5138, STS13 p=0.4566, STS13 s=0.4648, STS14 p=0.5079, STS14 s=0.4964, STS15 p=0.5828, STS15 s=0.5968, STS 16 p=0.6216, STS16 s=0.6450, STS B p=0.6311, STS B s=0.6291, STS B m=1.6977, SICK-R p=0.7486, SICK-R s=0.6964, SICK-P m=0.4529
2019-03-13 10:16:02,302 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 10:16:02,302 : 0.4797,0.5138,0.4566,0.4648,0.5079,0.4964,0.5828,0.5968,0.6216,0.6450,0.6311,0.6291,1.6977,0.7486,0.6964,0.4529
2019-03-13 10:16:02,302 : MR=74.23, CR=75.18, SUBJ=93.71, MPQA=83.07, SST-B=80.67, SST-F=43.35, TREC=87.80, SICK-E=71.46, SNLI=63.98, MRPC=64.29, MRPC f=68.18
2019-03-13 10:16:02,302 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 10:16:02,302 : 74.23,75.18,93.71,83.07,80.67,43.35,87.80,71.46,63.98,64.29,68.18
2019-03-13 10:16:02,302 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 10:16:02,302 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 10:16:02,302 : na,na,na,na,na,na,na,na,na,na
2019-03-13 10:16:02,302 : SentLen=83.05, WC=4.55, TreeDepth=31.82, TopConst=71.13, BShift=89.76, Tense=87.46, SubjNum=84.60, ObjNum=83.09, SOMO=65.32, CoordInv=62.68, average=66.35
2019-03-13 10:16:02,302 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 10:16:02,302 : 83.05,4.55,31.82,71.13,89.76,87.46,84.60,83.09,65.32,62.68,66.35
2019-03-13 10:16:02,302 : ********************************************************************************
2019-03-13 10:16:02,302 : ********************************************************************************
2019-03-13 10:16:02,302 : ********************************************************************************
2019-03-13 10:16:02,302 : layer 15
2019-03-13 10:16:02,302 : ********************************************************************************
2019-03-13 10:16:02,302 : ********************************************************************************
2019-03-13 10:16:02,303 : ********************************************************************************
2019-03-13 10:16:02,393 : ***** Transfer task : STS12 *****


2019-03-13 10:16:02,406 : loading BERT model bert-large-uncased
2019-03-13 10:16:02,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:16:02,423 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:16:02,423 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphl2u2js5
2019-03-13 10:16:09,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:16:19,056 : MSRpar : pearson = 0.3279, spearman = 0.3930
2019-03-13 10:16:20,703 : MSRvid : pearson = 0.4177, spearman = 0.4414
2019-03-13 10:16:22,119 : SMTeuroparl : pearson = 0.4848, spearman = 0.5997
2019-03-13 10:16:24,831 : surprise.OnWN : pearson = 0.4871, spearman = 0.5502
2019-03-13 10:16:26,266 : surprise.SMTnews : pearson = 0.7072, spearman = 0.5999
2019-03-13 10:16:26,266 : ALL (weighted average) : Pearson = 0.4599,             Spearman = 0.4997
2019-03-13 10:16:26,266 : ALL (average) : Pearson = 0.4849,             Spearman = 0.5168

2019-03-13 10:16:26,266 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 10:16:26,275 : loading BERT model bert-large-uncased
2019-03-13 10:16:26,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:16:26,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:16:26,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr2ble1ol
2019-03-13 10:16:33,708 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:16:40,176 : FNWN : pearson = 0.2073, spearman = 0.2517
2019-03-13 10:16:42,078 : headlines : pearson = 0.5537, spearman = 0.5434
2019-03-13 10:16:43,553 : OnWN : pearson = 0.5275, spearman = 0.5516
2019-03-13 10:16:43,553 : ALL (weighted average) : Pearson = 0.5003,             Spearman = 0.5097
2019-03-13 10:16:43,553 : ALL (average) : Pearson = 0.4295,             Spearman = 0.4489

2019-03-13 10:16:43,553 : ***** Transfer task : STS14 *****


2019-03-13 10:16:43,568 : loading BERT model bert-large-uncased
2019-03-13 10:16:43,569 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:16:43,586 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:16:43,586 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvwfxpnb_
2019-03-13 10:16:51,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:16:57,564 : deft-forum : pearson = 0.1762, spearman = 0.1779
2019-03-13 10:16:59,210 : deft-news : pearson = 0.6458, spearman = 0.6384
2019-03-13 10:17:01,389 : headlines : pearson = 0.5014, spearman = 0.4606
2019-03-13 10:17:03,480 : images : pearson = 0.4646, spearman = 0.4565
2019-03-13 10:17:05,624 : OnWN : pearson = 0.6281, spearman = 0.6587
2019-03-13 10:17:08,503 : tweet-news : pearson = 0.5445, spearman = 0.5136
2019-03-13 10:17:08,504 : ALL (weighted average) : Pearson = 0.5005,             Spearman = 0.4903
2019-03-13 10:17:08,504 : ALL (average) : Pearson = 0.4934,             Spearman = 0.4843

2019-03-13 10:17:08,504 : ***** Transfer task : STS15 *****


2019-03-13 10:17:08,552 : loading BERT model bert-large-uncased
2019-03-13 10:17:08,553 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:17:08,577 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:17:08,577 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpln_y66cu
2019-03-13 10:17:16,000 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:17:22,979 : answers-forums : pearson = 0.4973, spearman = 0.4966
2019-03-13 10:17:25,074 : answers-students : pearson = 0.5967, spearman = 0.6461
2019-03-13 10:17:27,134 : belief : pearson = 0.6136, spearman = 0.6680
2019-03-13 10:17:29,398 : headlines : pearson = 0.5948, spearman = 0.5973
2019-03-13 10:17:31,544 : images : pearson = 0.5102, spearman = 0.5320
2019-03-13 10:17:31,544 : ALL (weighted average) : Pearson = 0.5643,             Spearman = 0.5894
2019-03-13 10:17:31,544 : ALL (average) : Pearson = 0.5625,             Spearman = 0.5880

2019-03-13 10:17:31,544 : ***** Transfer task : STS16 *****


2019-03-13 10:17:31,618 : loading BERT model bert-large-uncased
2019-03-13 10:17:31,618 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:17:31,636 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:17:31,636 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmltx9svi
2019-03-13 10:17:39,066 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:17:45,075 : answer-answer : pearson = 0.5387, spearman = 0.5495
2019-03-13 10:17:45,738 : headlines : pearson = 0.6112, spearman = 0.6243
2019-03-13 10:17:46,626 : plagiarism : pearson = 0.7722, spearman = 0.7902
2019-03-13 10:17:48,133 : postediting : pearson = 0.7539, spearman = 0.8209
2019-03-13 10:17:48,742 : question-question : pearson = 0.3455, spearman = 0.3766
2019-03-13 10:17:48,742 : ALL (weighted average) : Pearson = 0.6094,             Spearman = 0.6372
2019-03-13 10:17:48,742 : ALL (average) : Pearson = 0.6043,             Spearman = 0.6323

2019-03-13 10:17:48,742 : ***** Transfer task : MR *****


2019-03-13 10:17:48,758 : loading BERT model bert-large-uncased
2019-03-13 10:17:48,758 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:17:48,779 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:17:48,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpru6abuuk
2019-03-13 10:17:56,249 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:18:01,534 : Generating sentence embeddings
2019-03-13 10:18:33,169 : Generated sentence embeddings
2019-03-13 10:18:33,170 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:18:42,482 : Best param found at split 1: l2reg = 0.0001                 with score 77.22
2019-03-13 10:18:53,072 : Best param found at split 2: l2reg = 1e-05                 with score 77.35
2019-03-13 10:19:03,802 : Best param found at split 3: l2reg = 1e-05                 with score 77.58
2019-03-13 10:19:13,785 : Best param found at split 4: l2reg = 1e-05                 with score 76.94
2019-03-13 10:19:24,212 : Best param found at split 5: l2reg = 1e-05                 with score 77.32
2019-03-13 10:19:24,828 : Dev acc : 77.28 Test acc : 76.79

2019-03-13 10:19:24,829 : ***** Transfer task : CR *****


2019-03-13 10:19:24,836 : loading BERT model bert-large-uncased
2019-03-13 10:19:24,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:19:24,857 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:19:24,857 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwjk01rra
2019-03-13 10:19:32,304 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:19:37,517 : Generating sentence embeddings
2019-03-13 10:19:45,834 : Generated sentence embeddings
2019-03-13 10:19:45,834 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:19:49,374 : Best param found at split 1: l2reg = 1e-05                 with score 81.09
2019-03-13 10:19:52,603 : Best param found at split 2: l2reg = 1e-05                 with score 81.22
2019-03-13 10:19:55,904 : Best param found at split 3: l2reg = 1e-05                 with score 82.22
2019-03-13 10:19:59,673 : Best param found at split 4: l2reg = 0.0001                 with score 81.5
2019-03-13 10:20:03,698 : Best param found at split 5: l2reg = 1e-05                 with score 80.64
2019-03-13 10:20:03,881 : Dev acc : 81.33 Test acc : 80.55

2019-03-13 10:20:03,882 : ***** Transfer task : MPQA *****


2019-03-13 10:20:03,888 : loading BERT model bert-large-uncased
2019-03-13 10:20:03,888 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:20:03,937 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:20:03,938 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbt0nlrgx
2019-03-13 10:20:11,321 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:20:16,534 : Generating sentence embeddings
2019-03-13 10:20:24,106 : Generated sentence embeddings
2019-03-13 10:20:24,106 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:20:32,291 : Best param found at split 1: l2reg = 1e-05                 with score 77.46
2019-03-13 10:20:41,907 : Best param found at split 2: l2reg = 0.001                 with score 80.56
2019-03-13 10:20:51,282 : Best param found at split 3: l2reg = 1e-05                 with score 82.37
2019-03-13 10:21:02,815 : Best param found at split 4: l2reg = 0.0001                 with score 83.59
2019-03-13 10:21:13,320 : Best param found at split 5: l2reg = 1e-05                 with score 82.71
2019-03-13 10:21:13,836 : Dev acc : 81.34 Test acc : 83.69

2019-03-13 10:21:13,837 : ***** Transfer task : SUBJ *****


2019-03-13 10:21:13,853 : loading BERT model bert-large-uncased
2019-03-13 10:21:13,853 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:21:13,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:21:13,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpblre0z8q
2019-03-13 10:21:21,314 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:21:26,545 : Generating sentence embeddings
2019-03-13 10:21:57,577 : Generated sentence embeddings
2019-03-13 10:21:57,577 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 10:22:07,435 : Best param found at split 1: l2reg = 1e-05                 with score 94.35
2019-03-13 10:22:16,171 : Best param found at split 2: l2reg = 1e-05                 with score 94.45
2019-03-13 10:22:25,892 : Best param found at split 3: l2reg = 1e-05                 with score 94.01
2019-03-13 10:22:34,611 : Best param found at split 4: l2reg = 1e-05                 with score 94.64
2019-03-13 10:22:43,871 : Best param found at split 5: l2reg = 1e-05                 with score 94.35
2019-03-13 10:22:44,608 : Dev acc : 94.36 Test acc : 93.84

2019-03-13 10:22:44,609 : ***** Transfer task : SST Binary classification *****


2019-03-13 10:22:44,700 : loading BERT model bert-large-uncased
2019-03-13 10:22:44,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:22:44,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:22:44,774 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8xo0vywx
2019-03-13 10:22:52,192 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:22:57,508 : Computing embedding for train
2019-03-13 10:24:38,354 : Computed train embeddings
2019-03-13 10:24:38,354 : Computing embedding for dev
2019-03-13 10:24:40,562 : Computed dev embeddings
2019-03-13 10:24:40,562 : Computing embedding for test
2019-03-13 10:24:45,202 : Computed test embeddings
2019-03-13 10:24:45,202 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:25:02,533 : [('reg:1e-05', 82.8), ('reg:0.0001', 80.96), ('reg:0.001', 79.7), ('reg:0.01', 73.62)]
2019-03-13 10:25:02,533 : Validation : best param found is reg = 1e-05 with score             82.8
2019-03-13 10:25:02,533 : Evaluating...
2019-03-13 10:25:07,599 : 
Dev acc : 82.8 Test acc : 82.1 for             SST Binary classification

2019-03-13 10:25:07,599 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 10:25:07,653 : loading BERT model bert-large-uncased
2019-03-13 10:25:07,654 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:25:07,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:25:07,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm6kechwi
2019-03-13 10:25:15,164 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:25:20,509 : Computing embedding for train
2019-03-13 10:25:42,547 : Computed train embeddings
2019-03-13 10:25:42,547 : Computing embedding for dev
2019-03-13 10:25:45,434 : Computed dev embeddings
2019-03-13 10:25:45,434 : Computing embedding for test
2019-03-13 10:25:51,115 : Computed test embeddings
2019-03-13 10:25:51,116 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:25:53,806 : [('reg:1e-05', 43.32), ('reg:0.0001', 42.78), ('reg:0.001', 41.33), ('reg:0.01', 36.97)]
2019-03-13 10:25:53,807 : Validation : best param found is reg = 1e-05 with score             43.32
2019-03-13 10:25:53,807 : Evaluating...
2019-03-13 10:25:54,540 : 
Dev acc : 43.32 Test acc : 43.48 for             SST Fine-Grained classification

2019-03-13 10:25:54,540 : ***** Transfer task : TREC *****


2019-03-13 10:25:54,554 : loading BERT model bert-large-uncased
2019-03-13 10:25:54,554 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:25:54,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:25:54,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7jzlmq7q
2019-03-13 10:26:02,057 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:26:14,919 : Computed train embeddings
2019-03-13 10:26:15,512 : Computed test embeddings
2019-03-13 10:26:15,512 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:26:22,843 : [('reg:1e-05', 77.66), ('reg:0.0001', 76.19), ('reg:0.001', 61.39), ('reg:0.01', 43.28)]
2019-03-13 10:26:22,843 : Cross-validation : best param found is reg = 1e-05             with score 77.66
2019-03-13 10:26:22,843 : Evaluating...
2019-03-13 10:26:23,493 : 
Dev acc : 77.66 Test acc : 89.4             for TREC

2019-03-13 10:26:23,494 : ***** Transfer task : MRPC *****


2019-03-13 10:26:23,514 : loading BERT model bert-large-uncased
2019-03-13 10:26:23,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:26:23,537 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:26:23,538 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbldefu_8
2019-03-13 10:26:31,035 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:26:36,216 : Computing embedding for train
2019-03-13 10:26:58,574 : Computed train embeddings
2019-03-13 10:26:58,574 : Computing embedding for test
2019-03-13 10:27:08,394 : Computed test embeddings
2019-03-13 10:27:08,415 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 10:27:12,851 : [('reg:1e-05', 69.46), ('reg:0.0001', 69.26), ('reg:0.001', 69.45), ('reg:0.01', 68.65)]
2019-03-13 10:27:12,851 : Cross-validation : best param found is reg = 1e-05             with score 69.46
2019-03-13 10:27:12,851 : Evaluating...
2019-03-13 10:27:13,157 : Dev acc : 69.46 Test acc 71.42; Test F1 80.61 for MRPC.

2019-03-13 10:27:13,157 : ***** Transfer task : SICK-Entailment*****


2019-03-13 10:27:13,218 : loading BERT model bert-large-uncased
2019-03-13 10:27:13,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:27:13,237 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:27:13,237 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv26re3dz
2019-03-13 10:27:20,677 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:27:26,040 : Computing embedding for train
2019-03-13 10:27:37,389 : Computed train embeddings
2019-03-13 10:27:37,389 : Computing embedding for dev
2019-03-13 10:27:38,939 : Computed dev embeddings
2019-03-13 10:27:38,939 : Computing embedding for test
2019-03-13 10:27:51,132 : Computed test embeddings
2019-03-13 10:27:51,169 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:27:52,344 : [('reg:1e-05', 73.2), ('reg:0.0001', 72.4), ('reg:0.001', 70.0), ('reg:0.01', 56.4)]
2019-03-13 10:27:52,345 : Validation : best param found is reg = 1e-05 with score             73.2
2019-03-13 10:27:52,345 : Evaluating...
2019-03-13 10:27:52,646 : 
Dev acc : 73.2 Test acc : 71.91 for                        SICK entailment

2019-03-13 10:27:52,647 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 10:27:52,673 : loading BERT model bert-large-uncased
2019-03-13 10:27:52,674 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:27:52,731 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:27:52,731 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplh6____8
2019-03-13 10:28:00,175 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:28:05,459 : Computing embedding for train
2019-03-13 10:28:16,823 : Computed train embeddings
2019-03-13 10:28:16,823 : Computing embedding for dev
2019-03-13 10:28:18,378 : Computed dev embeddings
2019-03-13 10:28:18,378 : Computing embedding for test
2019-03-13 10:28:30,575 : Computed test embeddings
2019-03-13 10:29:15,644 : Dev : Pearson 0.7344729396820031
2019-03-13 10:29:15,644 : Test : Pearson 0.7699345713223195 Spearman 0.7184080674845417 MSE 0.41708482025977933                        for SICK Relatedness

2019-03-13 10:29:15,645 : 

***** Transfer task : STSBenchmark*****


2019-03-13 10:29:15,713 : loading BERT model bert-large-uncased
2019-03-13 10:29:15,713 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:29:15,735 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:29:15,735 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpy1pfvb0i
2019-03-13 10:29:23,180 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:29:28,368 : Computing embedding for train
2019-03-13 10:29:46,946 : Computed train embeddings
2019-03-13 10:29:46,946 : Computing embedding for dev
2019-03-13 10:29:52,595 : Computed dev embeddings
2019-03-13 10:29:52,595 : Computing embedding for test
2019-03-13 10:29:57,204 : Computed test embeddings
2019-03-13 10:30:22,359 : Dev : Pearson 0.6988567283468886
2019-03-13 10:30:22,359 : Test : Pearson 0.6465228724828627 Spearman 0.635487763594477 MSE 1.6271677972078324                        for SICK Relatedness

2019-03-13 10:30:22,359 : ***** Transfer task : SNLI Entailment*****


2019-03-13 10:30:27,523 : loading BERT model bert-large-uncased
2019-03-13 10:30:27,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:30:27,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:30:27,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0golxmvs
2019-03-13 10:30:35,057 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:30:40,792 : PROGRESS (encoding): 0.00%
2019-03-13 10:33:28,295 : PROGRESS (encoding): 14.56%
2019-03-13 10:36:40,211 : PROGRESS (encoding): 29.12%
2019-03-13 10:39:50,852 : PROGRESS (encoding): 43.69%
2019-03-13 10:43:13,822 : PROGRESS (encoding): 58.25%
2019-03-13 10:46:59,680 : PROGRESS (encoding): 72.81%
2019-03-13 10:50:44,325 : PROGRESS (encoding): 87.37%
2019-03-13 10:54:49,935 : PROGRESS (encoding): 0.00%
2019-03-13 10:55:20,797 : PROGRESS (encoding): 0.00%
2019-03-13 10:55:50,293 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 10:56:26,696 : [('reg:1e-09', 67.93)]
2019-03-13 10:56:26,696 : Validation : best param found is reg = 1e-09 with score             67.93
2019-03-13 10:56:26,696 : Evaluating...
2019-03-13 10:57:02,517 : Dev acc : 67.93 Test acc : 67.21 for SNLI

2019-03-13 10:57:02,517 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 10:57:02,721 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 10:57:03,774 : loading BERT model bert-large-uncased
2019-03-13 10:57:03,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 10:57:03,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 10:57:03,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr1fpayul
2019-03-13 10:57:11,248 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 10:57:16,496 : Computing embeddings for train/dev/test
2019-03-13 11:00:48,630 : Computed embeddings
2019-03-13 11:00:48,630 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:01:12,844 : [('reg:1e-05', 78.11), ('reg:0.0001', 71.65), ('reg:0.001', 54.53), ('reg:0.01', 33.94)]
2019-03-13 11:01:12,844 : Validation : best param found is reg = 1e-05 with score             78.11
2019-03-13 11:01:12,844 : Evaluating...
2019-03-13 11:01:20,234 : 
Dev acc : 78.1 Test acc : 79.2 for LENGTH classification

2019-03-13 11:01:20,235 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 11:01:20,616 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 11:01:20,662 : loading BERT model bert-large-uncased
2019-03-13 11:01:20,663 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:01:20,693 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:01:20,693 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptzpyw4vo
2019-03-13 11:01:28,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:01:33,391 : Computing embeddings for train/dev/test
2019-03-13 11:04:49,161 : Computed embeddings
2019-03-13 11:04:49,161 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:05:22,113 : [('reg:1e-05', 13.75), ('reg:0.0001', 1.3), ('reg:0.001', 0.32), ('reg:0.01', 0.25)]
2019-03-13 11:05:22,113 : Validation : best param found is reg = 1e-05 with score             13.75
2019-03-13 11:05:22,113 : Evaluating...
2019-03-13 11:05:29,770 : 
Dev acc : 13.8 Test acc : 14.4 for WORDCONTENT classification

2019-03-13 11:05:29,771 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 11:05:30,153 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 11:05:30,219 : loading BERT model bert-large-uncased
2019-03-13 11:05:30,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:05:30,244 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:05:30,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcr12ssuv
2019-03-13 11:05:37,701 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:05:42,950 : Computing embeddings for train/dev/test
2019-03-13 11:08:47,072 : Computed embeddings
2019-03-13 11:08:47,072 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:09:14,978 : [('reg:1e-05', 31.23), ('reg:0.0001', 27.69), ('reg:0.001', 20.93), ('reg:0.01', 18.07)]
2019-03-13 11:09:14,979 : Validation : best param found is reg = 1e-05 with score             31.23
2019-03-13 11:09:14,979 : Evaluating...
2019-03-13 11:09:22,614 : 
Dev acc : 31.2 Test acc : 30.9 for DEPTH classification

2019-03-13 11:09:22,615 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 11:09:22,997 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 11:09:23,061 : loading BERT model bert-large-uncased
2019-03-13 11:09:23,061 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:09:23,170 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:09:23,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9pndeow9
2019-03-13 11:09:30,678 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:09:35,941 : Computing embeddings for train/dev/test
2019-03-13 11:12:26,315 : Computed embeddings
2019-03-13 11:12:26,315 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:12:58,422 : [('reg:1e-05', 72.65), ('reg:0.0001', 62.93), ('reg:0.001', 42.37), ('reg:0.01', 18.7)]
2019-03-13 11:12:58,422 : Validation : best param found is reg = 1e-05 with score             72.65
2019-03-13 11:12:58,422 : Evaluating...
2019-03-13 11:13:07,435 : 
Dev acc : 72.7 Test acc : 72.4 for TOPCONSTITUENTS classification

2019-03-13 11:13:07,436 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 11:13:07,779 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 11:13:07,845 : loading BERT model bert-large-uncased
2019-03-13 11:13:07,845 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:13:07,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:13:07,962 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2i5czgdz
2019-03-13 11:13:15,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:13:20,619 : Computing embeddings for train/dev/test
2019-03-13 11:16:25,809 : Computed embeddings
2019-03-13 11:16:25,809 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:16:55,408 : [('reg:1e-05', 90.98), ('reg:0.0001', 89.57), ('reg:0.001', 87.45), ('reg:0.01', 84.03)]
2019-03-13 11:16:55,408 : Validation : best param found is reg = 1e-05 with score             90.98
2019-03-13 11:16:55,408 : Evaluating...
2019-03-13 11:17:03,396 : 
Dev acc : 91.0 Test acc : 90.6 for BIGRAMSHIFT classification

2019-03-13 11:17:03,397 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 11:17:03,956 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 11:17:04,021 : loading BERT model bert-large-uncased
2019-03-13 11:17:04,021 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:17:04,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:17:04,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1zw7sfw0
2019-03-13 11:17:11,511 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:17:16,813 : Computing embeddings for train/dev/test
2019-03-13 11:20:19,318 : Computed embeddings
2019-03-13 11:20:19,318 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:20:49,630 : [('reg:1e-05', 89.1), ('reg:0.0001', 88.57), ('reg:0.001', 85.16), ('reg:0.01', 79.13)]
2019-03-13 11:20:49,630 : Validation : best param found is reg = 1e-05 with score             89.1
2019-03-13 11:20:49,630 : Evaluating...
2019-03-13 11:20:59,239 : 
Dev acc : 89.1 Test acc : 87.7 for TENSE classification

2019-03-13 11:20:59,240 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 11:20:59,659 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 11:20:59,721 : loading BERT model bert-large-uncased
2019-03-13 11:20:59,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:20:59,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:20:59,746 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp763xote4
2019-03-13 11:21:07,246 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:21:12,537 : Computing embeddings for train/dev/test
2019-03-13 11:24:23,880 : Computed embeddings
2019-03-13 11:24:23,880 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:24:51,907 : [('reg:1e-05', 85.83), ('reg:0.0001', 83.35), ('reg:0.001', 79.94), ('reg:0.01', 73.14)]
2019-03-13 11:24:51,907 : Validation : best param found is reg = 1e-05 with score             85.83
2019-03-13 11:24:51,907 : Evaluating...
2019-03-13 11:25:00,594 : 
Dev acc : 85.8 Test acc : 85.2 for SUBJNUMBER classification

2019-03-13 11:25:00,595 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 11:25:01,006 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 11:25:01,072 : loading BERT model bert-large-uncased
2019-03-13 11:25:01,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:25:01,188 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:25:01,188 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd_is3mdd
2019-03-13 11:25:08,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:25:13,944 : Computing embeddings for train/dev/test
2019-03-13 11:28:21,926 : Computed embeddings
2019-03-13 11:28:21,926 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:28:44,770 : [('reg:1e-05', 80.27), ('reg:0.0001', 79.14), ('reg:0.001', 75.26), ('reg:0.01', 68.42)]
2019-03-13 11:28:44,770 : Validation : best param found is reg = 1e-05 with score             80.27
2019-03-13 11:28:44,771 : Evaluating...
2019-03-13 11:28:51,284 : 
Dev acc : 80.3 Test acc : 81.5 for OBJNUMBER classification

2019-03-13 11:28:51,285 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 11:28:51,872 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 11:28:51,941 : loading BERT model bert-large-uncased
2019-03-13 11:28:51,941 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:28:51,966 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:28:51,967 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq0gvg7ki
2019-03-13 11:28:59,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:29:04,781 : Computing embeddings for train/dev/test
2019-03-13 11:32:43,425 : Computed embeddings
2019-03-13 11:32:43,425 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:33:09,154 : [('reg:1e-05', 65.71), ('reg:0.0001', 65.16), ('reg:0.001', 63.42), ('reg:0.01', 55.26)]
2019-03-13 11:33:09,154 : Validation : best param found is reg = 1e-05 with score             65.71
2019-03-13 11:33:09,154 : Evaluating...
2019-03-13 11:33:15,207 : 
Dev acc : 65.7 Test acc : 65.4 for ODDMANOUT classification

2019-03-13 11:33:15,208 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 11:33:15,600 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 11:33:15,676 : loading BERT model bert-large-uncased
2019-03-13 11:33:15,677 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:33:15,801 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:33:15,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp08e5y5x3
2019-03-13 11:33:23,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:33:28,563 : Computing embeddings for train/dev/test
2019-03-13 11:37:06,837 : Computed embeddings
2019-03-13 11:37:06,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:37:35,539 : [('reg:1e-05', 66.6), ('reg:0.0001', 65.14), ('reg:0.001', 59.07), ('reg:0.01', 50.17)]
2019-03-13 11:37:35,539 : Validation : best param found is reg = 1e-05 with score             66.6
2019-03-13 11:37:35,539 : Evaluating...
2019-03-13 11:37:40,844 : 
Dev acc : 66.6 Test acc : 66.3 for COORDINATIONINVERSION classification

2019-03-13 11:37:40,846 : total results: {'STS12': {'MSRpar': {'pearson': (0.3279068174727822, 2.935790494605786e-20), 'spearman': SpearmanrResult(correlation=0.3929704407240752, pvalue=4.1890937727786623e-29), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4177277007879061, 4.94472657933281e-33), 'spearman': SpearmanrResult(correlation=0.44139246163161916, pvalue=4.170215778916795e-37), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4847800233986213, 1.987450090987637e-28), 'spearman': SpearmanrResult(correlation=0.5997011834707756, pvalue=3.6294061847998e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4870856423723331, 6.10702306193337e-46), 'spearman': SpearmanrResult(correlation=0.550196433637404, pvalue=1.4394726707691227e-60), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.7071766836884779, 9.55346586482709e-62), 'spearman': SpearmanrResult(correlation=0.5998577101272162, pvalue=2.355191600137253e-40), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4849353735440241, 'wmean': 0.4598512380973088}, 'spearman': {'mean': 0.516823645918218, 'wmean': 0.49968647733226157}}}, 'STS13': {'FNWN': {'pearson': (0.20728382398741962, 0.0042107420081908395), 'spearman': SpearmanrResult(correlation=0.25166477282856653, pvalue=0.00047716020543280344), 'nsamples': 189}, 'headlines': {'pearson': (0.5537212315454053, 1.7640916180726962e-61), 'spearman': SpearmanrResult(correlation=0.5433603278857628, pvalue=7.86517320796416e-59), 'nsamples': 750}, 'OnWN': {'pearson': (0.5275474192473029, 1.6346940600077099e-41), 'spearman': SpearmanrResult(correlation=0.5516293495429089, pvalue=5.526792839730088e-46), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.42951749159337593, 'wmean': 0.5002811123936088}, 'spearman': {'mean': 0.4488848167524127, 'wmean': 0.5096993020483287}}}, 'STS14': {'deft-forum': {'pearson': (0.17615223170019187, 0.00017281031391505245), 'spearman': SpearmanrResult(correlation=0.1778605625076427, pvalue=0.00014896947449711235), 'nsamples': 450}, 'deft-news': {'pearson': (0.6458409432536792, 8.407904877112854e-37), 'spearman': SpearmanrResult(correlation=0.6383690305035141, pvalue=9.688137281564197e-36), 'nsamples': 300}, 'headlines': {'pearson': (0.5013741618657831, 5.465180471595078e-49), 'spearman': SpearmanrResult(correlation=0.46055223620394203, pvalue=1.1964866707531965e-40), 'nsamples': 750}, 'images': {'pearson': (0.4646211031885364, 1.9780217429922615e-41), 'spearman': SpearmanrResult(correlation=0.45651518196914004, pvalue=6.968291894250098e-40), 'nsamples': 750}, 'OnWN': {'pearson': (0.6280735021716176, 1.524022515346406e-83), 'spearman': SpearmanrResult(correlation=0.6586678704024731, pvalue=1.7527281059817543e-94), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5444565804901156, 4.166458196460092e-59), 'spearman': SpearmanrResult(correlation=0.5135519632790122, pvalue=1.0551048469790059e-51), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.49341975377832065, 'wmean': 0.5005106128075278}, 'spearman': {'mean': 0.4842528074776207, 'wmean': 0.49027024031211175}}}, 'STS15': {'answers-forums': {'pearson': (0.4972552316668563, 8.1236639114642775e-25), 'spearman': SpearmanrResult(correlation=0.49660335080145945, pvalue=9.54976428862783e-25), 'nsamples': 375}, 'answers-students': {'pearson': (0.5967263600678709, 1.5508559982155702e-73), 'spearman': SpearmanrResult(correlation=0.6460529860652331, pvalue=8.066092919683612e-90), 'nsamples': 750}, 'belief': {'pearson': (0.6135913773549976, 3.6622470358397816e-40), 'spearman': SpearmanrResult(correlation=0.6679848277549953, pvalue=8.401989058300682e-50), 'nsamples': 375}, 'headlines': {'pearson': (0.5948349313099988, 5.747108082714544e-73), 'spearman': SpearmanrResult(correlation=0.5973093701943006, pvalue=1.033850505608998e-73), 'nsamples': 750}, 'images': {'pearson': (0.5102116347603894, 6.008566927981111e-51), 'spearman': SpearmanrResult(correlation=0.5320314660256881, pvalue=4.881963145657646e-56), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5625239070320226, 'wmean': 0.5642990576622965}, 'spearman': {'mean': 0.5879964001683353, 'wmean': 0.5894219778908623}}}, 'STS16': {'answer-answer': {'pearson': (0.5387460699615831, 1.602601816080341e-20), 'spearman': SpearmanrResult(correlation=0.5495169709663893, pvalue=1.928332991114803e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.6111780611951757, 6.80068388340172e-27), 'spearman': SpearmanrResult(correlation=0.624294230964695, pvalue=2.617428615883964e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7721723106774462, 8.517895783247765e-47), 'spearman': SpearmanrResult(correlation=0.7901971967574026, pvalue=2.1918891809883206e-50), 'nsamples': 230}, 'postediting': {'pearson': (0.7538990300940008, 4.788467983098475e-46), 'spearman': SpearmanrResult(correlation=0.8208947013963482, pvalue=8.22288594936177e-61), 'nsamples': 244}, 'question-question': {'pearson': (0.34552571590627423, 2.992125524481832e-07), 'spearman': SpearmanrResult(correlation=0.37656232352350594, pvalue=1.915425488135268e-08), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.604304237566896, 'wmean': 0.6094356732133228}, 'spearman': {'mean': 0.6322930847216683, 'wmean': 0.637244318842324}}}, 'MR': {'devacc': 77.28, 'acc': 76.79, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.33, 'acc': 80.55, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.34, 'acc': 83.69, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.36, 'acc': 93.84, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.8, 'acc': 82.1, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.32, 'acc': 43.48, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 77.66, 'acc': 89.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.46, 'acc': 71.42, 'f1': 80.61, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.2, 'acc': 71.91, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7344729396820031, 'pearson': 0.7699345713223195, 'spearman': 0.7184080674845417, 'mse': 0.41708482025977933, 'yhat': array([3.46971462, 3.99407236, 1.49508598, ..., 3.25421153, 4.257654  ,        4.55316577]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6988567283468886, 'pearson': 0.6465228724828627, 'spearman': 0.635487763594477, 'mse': 1.6271677972078324, 'yhat': array([1.34689956, 1.73416941, 2.20485093, ..., 4.03851035, 3.5771273 ,        3.3968758 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 67.93, 'acc': 67.21, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 78.11, 'acc': 79.17, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 13.75, 'acc': 14.39, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.23, 'acc': 30.95, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.65, 'acc': 72.39, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 90.98, 'acc': 90.57, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.1, 'acc': 87.73, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.83, 'acc': 85.18, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.27, 'acc': 81.46, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 65.71, 'acc': 65.41, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.6, 'acc': 66.34, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 11:37:40,846 : STS12 p=0.4599, STS12 s=0.4997, STS13 p=0.5003, STS13 s=0.5097, STS14 p=0.5005, STS14 s=0.4903, STS15 p=0.5643, STS15 s=0.5894, STS 16 p=0.6094, STS16 s=0.6372, STS B p=0.6465, STS B s=0.6355, STS B m=1.6272, SICK-R p=0.7699, SICK-R s=0.7184, SICK-P m=0.4171
2019-03-13 11:37:40,846 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 11:37:40,846 : 0.4599,0.4997,0.5003,0.5097,0.5005,0.4903,0.5643,0.5894,0.6094,0.6372,0.6465,0.6355,1.6272,0.7699,0.7184,0.4171
2019-03-13 11:37:40,846 : MR=76.79, CR=80.55, SUBJ=93.84, MPQA=83.69, SST-B=82.10, SST-F=43.48, TREC=89.40, SICK-E=71.91, SNLI=67.21, MRPC=71.42, MRPC f=80.61
2019-03-13 11:37:40,846 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 11:37:40,846 : 76.79,80.55,93.84,83.69,82.10,43.48,89.40,71.91,67.21,71.42,80.61
2019-03-13 11:37:40,846 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 11:37:40,846 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 11:37:40,846 : na,na,na,na,na,na,na,na,na,na
2019-03-13 11:37:40,846 : SentLen=79.17, WC=14.39, TreeDepth=30.95, TopConst=72.39, BShift=90.57, Tense=87.73, SubjNum=85.18, ObjNum=81.46, SOMO=65.41, CoordInv=66.34, average=67.36
2019-03-13 11:37:40,846 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 11:37:40,847 : 79.17,14.39,30.95,72.39,90.57,87.73,85.18,81.46,65.41,66.34,67.36
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,847 : layer 16
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,847 : ********************************************************************************
2019-03-13 11:37:40,934 : ***** Transfer task : STS12 *****


2019-03-13 11:37:40,946 : loading BERT model bert-large-uncased
2019-03-13 11:37:40,947 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:37:40,964 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:37:40,964 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplt2ha6ez
2019-03-13 11:37:48,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:37:57,713 : MSRpar : pearson = 0.3426, spearman = 0.3939
2019-03-13 11:37:59,363 : MSRvid : pearson = 0.4156, spearman = 0.4444
2019-03-13 11:38:00,782 : SMTeuroparl : pearson = 0.4861, spearman = 0.6008
2019-03-13 11:38:03,498 : surprise.OnWN : pearson = 0.4759, spearman = 0.5464
2019-03-13 11:38:04,935 : surprise.SMTnews : pearson = 0.6728, spearman = 0.5896
2019-03-13 11:38:04,935 : ALL (weighted average) : Pearson = 0.4560,             Spearman = 0.4985
2019-03-13 11:38:04,935 : ALL (average) : Pearson = 0.4786,             Spearman = 0.5150

2019-03-13 11:38:04,935 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 11:38:04,943 : loading BERT model bert-large-uncased
2019-03-13 11:38:04,943 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:38:04,960 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:38:04,961 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_ijbcvr1
2019-03-13 11:38:12,375 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:38:18,913 : FNWN : pearson = 0.1598, spearman = 0.1614
2019-03-13 11:38:20,813 : headlines : pearson = 0.5391, spearman = 0.5229
2019-03-13 11:38:22,288 : OnWN : pearson = 0.5119, spearman = 0.5264
2019-03-13 11:38:22,289 : ALL (weighted average) : Pearson = 0.4811,             Spearman = 0.4787
2019-03-13 11:38:22,289 : ALL (average) : Pearson = 0.4036,             Spearman = 0.4036

2019-03-13 11:38:22,289 : ***** Transfer task : STS14 *****


2019-03-13 11:38:22,305 : loading BERT model bert-large-uncased
2019-03-13 11:38:22,306 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:38:22,323 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:38:22,323 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj1ib941k
2019-03-13 11:38:29,768 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:38:36,378 : deft-forum : pearson = 0.1681, spearman = 0.1763
2019-03-13 11:38:38,024 : deft-news : pearson = 0.6456, spearman = 0.6130
2019-03-13 11:38:40,202 : headlines : pearson = 0.4956, spearman = 0.4578
2019-03-13 11:38:42,287 : images : pearson = 0.3977, spearman = 0.4010
2019-03-13 11:38:44,426 : OnWN : pearson = 0.5977, spearman = 0.6222
2019-03-13 11:38:47,307 : tweet-news : pearson = 0.5718, spearman = 0.5428
2019-03-13 11:38:47,307 : ALL (weighted average) : Pearson = 0.4844,             Spearman = 0.4749
2019-03-13 11:38:47,307 : ALL (average) : Pearson = 0.4794,             Spearman = 0.4688

2019-03-13 11:38:47,307 : ***** Transfer task : STS15 *****


2019-03-13 11:38:47,340 : loading BERT model bert-large-uncased
2019-03-13 11:38:47,340 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:38:47,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:38:47,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo_mla9a_
2019-03-13 11:38:54,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:39:01,865 : answers-forums : pearson = 0.4960, spearman = 0.4833
2019-03-13 11:39:03,962 : answers-students : pearson = 0.5611, spearman = 0.5723
2019-03-13 11:39:06,024 : belief : pearson = 0.6013, spearman = 0.6454
2019-03-13 11:39:08,290 : headlines : pearson = 0.5936, spearman = 0.5845
2019-03-13 11:39:10,440 : images : pearson = 0.4807, spearman = 0.4894
2019-03-13 11:39:10,440 : ALL (weighted average) : Pearson = 0.5460,             Spearman = 0.5526
2019-03-13 11:39:10,440 : ALL (average) : Pearson = 0.5465,             Spearman = 0.5550

2019-03-13 11:39:10,440 : ***** Transfer task : STS16 *****


2019-03-13 11:39:10,509 : loading BERT model bert-large-uncased
2019-03-13 11:39:10,509 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:39:10,527 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:39:10,527 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_da3an0e
2019-03-13 11:39:17,955 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:39:24,099 : answer-answer : pearson = 0.5184, spearman = 0.5294
2019-03-13 11:39:24,763 : headlines : pearson = 0.6158, spearman = 0.6204
2019-03-13 11:39:25,652 : plagiarism : pearson = 0.7433, spearman = 0.7670
2019-03-13 11:39:27,157 : postediting : pearson = 0.7827, spearman = 0.8152
2019-03-13 11:39:27,767 : question-question : pearson = 0.2257, spearman = 0.2694
2019-03-13 11:39:27,767 : ALL (weighted average) : Pearson = 0.5853,             Spearman = 0.6076
2019-03-13 11:39:27,767 : ALL (average) : Pearson = 0.5772,             Spearman = 0.6003

2019-03-13 11:39:27,767 : ***** Transfer task : MR *****


2019-03-13 11:39:27,786 : loading BERT model bert-large-uncased
2019-03-13 11:39:27,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:39:27,805 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:39:27,805 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpcnh76ge4
2019-03-13 11:39:35,245 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:39:40,534 : Generating sentence embeddings
2019-03-13 11:40:12,279 : Generated sentence embeddings
2019-03-13 11:40:12,280 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:40:21,908 : Best param found at split 1: l2reg = 1e-05                 with score 77.88
2019-03-13 11:40:30,338 : Best param found at split 2: l2reg = 1e-05                 with score 78.31
2019-03-13 11:40:42,077 : Best param found at split 3: l2reg = 1e-05                 with score 78.92
2019-03-13 11:40:50,792 : Best param found at split 4: l2reg = 1e-05                 with score 78.3
2019-03-13 11:40:59,182 : Best param found at split 5: l2reg = 1e-05                 with score 78.14
2019-03-13 11:40:59,964 : Dev acc : 78.31 Test acc : 78.43

2019-03-13 11:40:59,965 : ***** Transfer task : CR *****


2019-03-13 11:40:59,973 : loading BERT model bert-large-uncased
2019-03-13 11:40:59,973 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:40:59,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:40:59,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoaaqss20
2019-03-13 11:41:07,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:41:12,637 : Generating sentence embeddings
2019-03-13 11:41:21,003 : Generated sentence embeddings
2019-03-13 11:41:21,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:41:24,892 : Best param found at split 1: l2reg = 1e-05                 with score 81.29
2019-03-13 11:41:28,522 : Best param found at split 2: l2reg = 1e-05                 with score 81.58
2019-03-13 11:41:32,067 : Best param found at split 3: l2reg = 1e-05                 with score 82.15
2019-03-13 11:41:36,467 : Best param found at split 4: l2reg = 1e-05                 with score 81.76
2019-03-13 11:41:40,519 : Best param found at split 5: l2reg = 1e-05                 with score 81.26
2019-03-13 11:41:40,738 : Dev acc : 81.61 Test acc : 78.86

2019-03-13 11:41:40,738 : ***** Transfer task : MPQA *****


2019-03-13 11:41:40,745 : loading BERT model bert-large-uncased
2019-03-13 11:41:40,745 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:41:40,795 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:41:40,795 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp320hfngk
2019-03-13 11:41:48,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:41:53,491 : Generating sentence embeddings
2019-03-13 11:42:01,100 : Generated sentence embeddings
2019-03-13 11:42:01,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:42:09,878 : Best param found at split 1: l2reg = 1e-05                 with score 80.02
2019-03-13 11:42:19,547 : Best param found at split 2: l2reg = 0.001                 with score 80.5
2019-03-13 11:42:30,808 : Best param found at split 3: l2reg = 0.0001                 with score 81.69
2019-03-13 11:42:43,015 : Best param found at split 4: l2reg = 0.0001                 with score 84.16
2019-03-13 11:42:53,644 : Best param found at split 5: l2reg = 0.001                 with score 83.21
2019-03-13 11:42:54,163 : Dev acc : 81.92 Test acc : 83.1

2019-03-13 11:42:54,164 : ***** Transfer task : SUBJ *****


2019-03-13 11:42:54,179 : loading BERT model bert-large-uncased
2019-03-13 11:42:54,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:42:54,200 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:42:54,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv4u66ffi
2019-03-13 11:43:01,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:43:06,838 : Generating sentence embeddings
2019-03-13 11:43:37,928 : Generated sentence embeddings
2019-03-13 11:43:37,929 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 11:43:48,919 : Best param found at split 1: l2reg = 1e-05                 with score 94.75
2019-03-13 11:44:00,401 : Best param found at split 2: l2reg = 1e-05                 with score 94.8
2019-03-13 11:44:11,796 : Best param found at split 3: l2reg = 1e-05                 with score 94.38
2019-03-13 11:44:23,551 : Best param found at split 4: l2reg = 1e-05                 with score 94.72
2019-03-13 11:44:35,394 : Best param found at split 5: l2reg = 0.0001                 with score 94.32
2019-03-13 11:44:36,151 : Dev acc : 94.59 Test acc : 94.11

2019-03-13 11:44:36,152 : ***** Transfer task : SST Binary classification *****


2019-03-13 11:44:36,243 : loading BERT model bert-large-uncased
2019-03-13 11:44:36,243 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:44:36,316 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:44:36,317 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4f5_yft4
2019-03-13 11:44:43,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:44:49,038 : Computing embedding for train
2019-03-13 11:46:30,105 : Computed train embeddings
2019-03-13 11:46:30,105 : Computing embedding for dev
2019-03-13 11:46:32,315 : Computed dev embeddings
2019-03-13 11:46:32,316 : Computing embedding for test
2019-03-13 11:46:37,006 : Computed test embeddings
2019-03-13 11:46:37,006 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:46:54,018 : [('reg:1e-05', 83.14), ('reg:0.0001', 82.34), ('reg:0.001', 80.28), ('reg:0.01', 76.38)]
2019-03-13 11:46:54,018 : Validation : best param found is reg = 1e-05 with score             83.14
2019-03-13 11:46:54,018 : Evaluating...
2019-03-13 11:46:58,282 : 
Dev acc : 83.14 Test acc : 82.48 for             SST Binary classification

2019-03-13 11:46:58,283 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 11:46:58,333 : loading BERT model bert-large-uncased
2019-03-13 11:46:58,334 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:46:58,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:46:58,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplpswi4t7
2019-03-13 11:47:05,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:47:11,061 : Computing embedding for train
2019-03-13 11:47:33,129 : Computed train embeddings
2019-03-13 11:47:33,129 : Computing embedding for dev
2019-03-13 11:47:36,013 : Computed dev embeddings
2019-03-13 11:47:36,013 : Computing embedding for test
2019-03-13 11:47:41,704 : Computed test embeddings
2019-03-13 11:47:41,704 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:47:43,987 : [('reg:1e-05', 43.6), ('reg:0.0001', 43.42), ('reg:0.001', 41.51), ('reg:0.01', 37.87)]
2019-03-13 11:47:43,988 : Validation : best param found is reg = 1e-05 with score             43.6
2019-03-13 11:47:43,988 : Evaluating...
2019-03-13 11:47:44,568 : 
Dev acc : 43.6 Test acc : 44.93 for             SST Fine-Grained classification

2019-03-13 11:47:44,569 : ***** Transfer task : TREC *****


2019-03-13 11:47:44,581 : loading BERT model bert-large-uncased
2019-03-13 11:47:44,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:47:44,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:47:44,601 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm1ym4zpv
2019-03-13 11:47:52,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:48:04,934 : Computed train embeddings
2019-03-13 11:48:05,527 : Computed test embeddings
2019-03-13 11:48:05,527 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:48:13,298 : [('reg:1e-05', 79.8), ('reg:0.0001', 78.61), ('reg:0.001', 64.75), ('reg:0.01', 43.21)]
2019-03-13 11:48:13,298 : Cross-validation : best param found is reg = 1e-05             with score 79.8
2019-03-13 11:48:13,298 : Evaluating...
2019-03-13 11:48:13,911 : 
Dev acc : 79.8 Test acc : 91.6             for TREC

2019-03-13 11:48:13,911 : ***** Transfer task : MRPC *****


2019-03-13 11:48:13,933 : loading BERT model bert-large-uncased
2019-03-13 11:48:13,934 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:48:13,954 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:48:13,954 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1b219hf5
2019-03-13 11:48:21,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:48:26,885 : Computing embedding for train
2019-03-13 11:48:49,308 : Computed train embeddings
2019-03-13 11:48:49,309 : Computing embedding for test
2019-03-13 11:48:59,140 : Computed test embeddings
2019-03-13 11:48:59,162 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 11:49:03,646 : [('reg:1e-05', 69.97), ('reg:0.0001', 69.46), ('reg:0.001', 70.22), ('reg:0.01', 68.28)]
2019-03-13 11:49:03,646 : Cross-validation : best param found is reg = 0.001             with score 70.22
2019-03-13 11:49:03,646 : Evaluating...
2019-03-13 11:49:03,871 : Dev acc : 70.22 Test acc 66.67; Test F1 79.94 for MRPC.

2019-03-13 11:49:03,871 : ***** Transfer task : SICK-Entailment*****


2019-03-13 11:49:03,931 : loading BERT model bert-large-uncased
2019-03-13 11:49:03,931 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:49:03,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:49:03,950 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdxpadcfr
2019-03-13 11:49:11,397 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:49:16,696 : Computing embedding for train
2019-03-13 11:49:28,085 : Computed train embeddings
2019-03-13 11:49:28,086 : Computing embedding for dev
2019-03-13 11:49:29,641 : Computed dev embeddings
2019-03-13 11:49:29,641 : Computing embedding for test
2019-03-13 11:49:41,865 : Computed test embeddings
2019-03-13 11:49:41,901 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 11:49:43,073 : [('reg:1e-05', 72.2), ('reg:0.0001', 69.6), ('reg:0.001', 70.0), ('reg:0.01', 56.4)]
2019-03-13 11:49:43,073 : Validation : best param found is reg = 1e-05 with score             72.2
2019-03-13 11:49:43,073 : Evaluating...
2019-03-13 11:49:43,423 : 
Dev acc : 72.2 Test acc : 69.37 for                        SICK entailment

2019-03-13 11:49:43,424 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 11:49:43,451 : loading BERT model bert-large-uncased
2019-03-13 11:49:43,451 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:49:43,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:49:43,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ymyc2q_
2019-03-13 11:49:50,939 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:49:56,255 : Computing embedding for train
2019-03-13 11:50:07,656 : Computed train embeddings
2019-03-13 11:50:07,656 : Computing embedding for dev
2019-03-13 11:50:09,214 : Computed dev embeddings
2019-03-13 11:50:09,214 : Computing embedding for test
2019-03-13 11:50:21,469 : Computed test embeddings
2019-03-13 11:51:08,852 : Dev : Pearson 0.7565538692097076
2019-03-13 11:51:08,852 : Test : Pearson 0.7736922549206854 Spearman 0.7206778686599586 MSE 0.4085247303536526                        for SICK Relatedness

2019-03-13 11:51:08,853 : 

***** Transfer task : STSBenchmark*****


2019-03-13 11:51:08,924 : loading BERT model bert-large-uncased
2019-03-13 11:51:08,925 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:51:08,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:51:08,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmzdcopqa
2019-03-13 11:51:16,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:51:21,779 : Computing embedding for train
2019-03-13 11:51:40,372 : Computed train embeddings
2019-03-13 11:51:40,372 : Computing embedding for dev
2019-03-13 11:51:46,039 : Computed dev embeddings
2019-03-13 11:51:46,039 : Computing embedding for test
2019-03-13 11:51:50,668 : Computed test embeddings
2019-03-13 11:52:13,346 : Dev : Pearson 0.6887746800949036
2019-03-13 11:52:13,346 : Test : Pearson 0.6442066672531649 Spearman 0.6309629477037366 MSE 1.6813679460700235                        for SICK Relatedness

2019-03-13 11:52:13,346 : ***** Transfer task : SNLI Entailment*****


2019-03-13 11:52:18,395 : loading BERT model bert-large-uncased
2019-03-13 11:52:18,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 11:52:18,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 11:52:18,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfbiig_ck
2019-03-13 11:52:25,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 11:52:31,591 : PROGRESS (encoding): 0.00%
2019-03-13 11:55:20,518 : PROGRESS (encoding): 14.56%
2019-03-13 11:58:33,825 : PROGRESS (encoding): 29.12%
2019-03-13 12:01:44,491 : PROGRESS (encoding): 43.69%
2019-03-13 12:05:07,810 : PROGRESS (encoding): 58.25%
2019-03-13 12:08:54,507 : PROGRESS (encoding): 72.81%
2019-03-13 12:12:39,735 : PROGRESS (encoding): 87.37%
2019-03-13 12:16:43,776 : PROGRESS (encoding): 0.00%
2019-03-13 12:17:14,434 : PROGRESS (encoding): 0.00%
2019-03-13 12:17:44,040 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:18:32,069 : [('reg:1e-09', 65.5)]
2019-03-13 12:18:32,069 : Validation : best param found is reg = 1e-09 with score             65.5
2019-03-13 12:18:32,069 : Evaluating...
2019-03-13 12:19:18,677 : Dev acc : 65.5 Test acc : 64.83 for SNLI

2019-03-13 12:19:18,677 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 12:19:18,885 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 12:19:19,981 : loading BERT model bert-large-uncased
2019-03-13 12:19:19,981 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:19:20,010 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:19:20,017 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5ddgdec6
2019-03-13 12:19:27,471 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:19:32,697 : Computing embeddings for train/dev/test
2019-03-13 12:23:06,014 : Computed embeddings
2019-03-13 12:23:06,014 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:23:31,239 : [('reg:1e-05', 73.79), ('reg:0.0001', 64.14), ('reg:0.001', 50.05), ('reg:0.01', 33.42)]
2019-03-13 12:23:31,239 : Validation : best param found is reg = 1e-05 with score             73.79
2019-03-13 12:23:31,239 : Evaluating...
2019-03-13 12:23:38,984 : 
Dev acc : 73.8 Test acc : 74.6 for LENGTH classification

2019-03-13 12:23:38,984 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 12:23:39,238 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 12:23:39,283 : loading BERT model bert-large-uncased
2019-03-13 12:23:39,283 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:23:39,314 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:23:39,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_boknsq6
2019-03-13 12:23:46,764 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:23:51,846 : Computing embeddings for train/dev/test
2019-03-13 12:27:09,275 : Computed embeddings
2019-03-13 12:27:09,275 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:27:41,842 : [('reg:1e-05', 9.75), ('reg:0.0001', 0.99), ('reg:0.001', 0.35), ('reg:0.01', 0.24)]
2019-03-13 12:27:41,842 : Validation : best param found is reg = 1e-05 with score             9.75
2019-03-13 12:27:41,842 : Evaluating...
2019-03-13 12:27:48,190 : 
Dev acc : 9.8 Test acc : 10.1 for WORDCONTENT classification

2019-03-13 12:27:48,191 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 12:27:48,727 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 12:27:48,792 : loading BERT model bert-large-uncased
2019-03-13 12:27:48,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:27:48,816 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:27:48,816 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx5hi0xzo
2019-03-13 12:27:56,294 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:28:01,524 : Computing embeddings for train/dev/test
2019-03-13 12:31:07,903 : Computed embeddings
2019-03-13 12:31:07,903 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:31:31,548 : [('reg:1e-05', 30.98), ('reg:0.0001', 28.3), ('reg:0.001', 21.9), ('reg:0.01', 18.07)]
2019-03-13 12:31:31,549 : Validation : best param found is reg = 1e-05 with score             30.98
2019-03-13 12:31:31,549 : Evaluating...
2019-03-13 12:31:37,250 : 
Dev acc : 31.0 Test acc : 31.4 for DEPTH classification

2019-03-13 12:31:37,251 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 12:31:37,627 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 12:31:37,691 : loading BERT model bert-large-uncased
2019-03-13 12:31:37,692 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:31:37,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:31:37,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7y2lmgcn
2019-03-13 12:31:45,263 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:31:50,471 : Computing embeddings for train/dev/test
2019-03-13 12:34:43,199 : Computed embeddings
2019-03-13 12:34:43,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:35:10,356 : [('reg:1e-05', 73.15), ('reg:0.0001', 64.66), ('reg:0.001', 46.8), ('reg:0.01', 23.04)]
2019-03-13 12:35:10,356 : Validation : best param found is reg = 1e-05 with score             73.15
2019-03-13 12:35:10,356 : Evaluating...
2019-03-13 12:35:17,957 : 
Dev acc : 73.2 Test acc : 73.2 for TOPCONSTITUENTS classification

2019-03-13 12:35:17,958 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 12:35:18,342 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 12:35:18,409 : loading BERT model bert-large-uncased
2019-03-13 12:35:18,409 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:35:18,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:35:18,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpysukl9cf
2019-03-13 12:35:25,902 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:35:31,132 : Computing embeddings for train/dev/test
2019-03-13 12:38:39,360 : Computed embeddings
2019-03-13 12:38:39,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:39:11,335 : [('reg:1e-05', 91.24), ('reg:0.0001', 90.66), ('reg:0.001', 88.1), ('reg:0.01', 85.17)]
2019-03-13 12:39:11,335 : Validation : best param found is reg = 1e-05 with score             91.24
2019-03-13 12:39:11,335 : Evaluating...
2019-03-13 12:39:19,841 : 
Dev acc : 91.2 Test acc : 90.7 for BIGRAMSHIFT classification

2019-03-13 12:39:19,842 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 12:39:20,248 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 12:39:20,313 : loading BERT model bert-large-uncased
2019-03-13 12:39:20,314 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:39:20,343 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:39:20,344 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe86djydh
2019-03-13 12:39:27,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:39:33,070 : Computing embeddings for train/dev/test
2019-03-13 12:42:35,343 : Computed embeddings
2019-03-13 12:42:35,343 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:43:08,447 : [('reg:1e-05', 89.0), ('reg:0.0001', 88.79), ('reg:0.001', 86.29), ('reg:0.01', 79.32)]
2019-03-13 12:43:08,447 : Validation : best param found is reg = 1e-05 with score             89.0
2019-03-13 12:43:08,447 : Evaluating...
2019-03-13 12:43:16,982 : 
Dev acc : 89.0 Test acc : 88.0 for TENSE classification

2019-03-13 12:43:16,983 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 12:43:17,404 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 12:43:17,468 : loading BERT model bert-large-uncased
2019-03-13 12:43:17,469 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:43:17,590 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:43:17,590 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_gx4dx2d
2019-03-13 12:43:25,105 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:43:30,486 : Computing embeddings for train/dev/test
2019-03-13 12:46:41,908 : Computed embeddings
2019-03-13 12:46:41,908 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:47:10,971 : [('reg:1e-05', 86.86), ('reg:0.0001', 84.9), ('reg:0.001', 80.93), ('reg:0.01', 74.59)]
2019-03-13 12:47:10,971 : Validation : best param found is reg = 1e-05 with score             86.86
2019-03-13 12:47:10,971 : Evaluating...
2019-03-13 12:47:19,624 : 
Dev acc : 86.9 Test acc : 86.0 for SUBJNUMBER classification

2019-03-13 12:47:19,625 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 12:47:20,026 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 12:47:20,093 : loading BERT model bert-large-uncased
2019-03-13 12:47:20,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:47:20,209 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:47:20,209 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzv2ubc9c
2019-03-13 12:47:27,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:47:32,974 : Computing embeddings for train/dev/test
2019-03-13 12:50:41,856 : Computed embeddings
2019-03-13 12:50:41,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:51:06,443 : [('reg:1e-05', 80.19), ('reg:0.0001', 78.88), ('reg:0.001', 74.41), ('reg:0.01', 64.22)]
2019-03-13 12:51:06,444 : Validation : best param found is reg = 1e-05 with score             80.19
2019-03-13 12:51:06,444 : Evaluating...
2019-03-13 12:51:12,926 : 
Dev acc : 80.2 Test acc : 81.3 for OBJNUMBER classification

2019-03-13 12:51:12,927 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 12:51:13,488 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 12:51:13,557 : loading BERT model bert-large-uncased
2019-03-13 12:51:13,557 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:51:13,584 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:51:13,584 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi_uwz2l0
2019-03-13 12:51:21,037 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:51:26,352 : Computing embeddings for train/dev/test
2019-03-13 12:55:06,687 : Computed embeddings
2019-03-13 12:55:06,687 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 12:55:35,413 : [('reg:1e-05', 66.6), ('reg:0.0001', 66.08), ('reg:0.001', 62.5), ('reg:0.01', 55.23)]
2019-03-13 12:55:35,413 : Validation : best param found is reg = 1e-05 with score             66.6
2019-03-13 12:55:35,413 : Evaluating...
2019-03-13 12:55:42,845 : 
Dev acc : 66.6 Test acc : 68.0 for ODDMANOUT classification

2019-03-13 12:55:42,846 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 12:55:43,230 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 12:55:43,312 : loading BERT model bert-large-uncased
2019-03-13 12:55:43,312 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 12:55:43,342 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 12:55:43,342 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpu58a9peg
2019-03-13 12:55:50,788 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 12:55:56,099 : Computing embeddings for train/dev/test
2019-03-13 12:59:36,123 : Computed embeddings
2019-03-13 12:59:36,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:00:06,175 : [('reg:1e-05', 68.32), ('reg:0.0001', 66.53), ('reg:0.001', 60.82), ('reg:0.01', 52.19)]
2019-03-13 13:00:06,176 : Validation : best param found is reg = 1e-05 with score             68.32
2019-03-13 13:00:06,176 : Evaluating...
2019-03-13 13:00:13,670 : 
Dev acc : 68.3 Test acc : 67.5 for COORDINATIONINVERSION classification

2019-03-13 13:00:13,672 : total results: {'STS12': {'MSRpar': {'pearson': (0.3426310593568044, 4.387358789665564e-22), 'spearman': SpearmanrResult(correlation=0.393885067006159, pvalue=3.039663299471882e-29), 'nsamples': 750}, 'MSRvid': {'pearson': (0.41562453639840646, 1.0984819990734588e-32), 'spearman': SpearmanrResult(correlation=0.44436417605982886, pvalue=1.2173646097967988e-37), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4861210663972297, 1.342989410389734e-28), 'spearman': SpearmanrResult(correlation=0.6007542112754556, pvalue=2.3066921521838934e-46), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.47587261515221685, 1.1996619286177863e-43), 'spearman': SpearmanrResult(correlation=0.5463950500320053, pvalue=1.3468758174702274e-59), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6728079437499096, 6.538337195709629e-54), 'spearman': SpearmanrResult(correlation=0.5895562045020643, pvalue=1.032401673491644e-38), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.4786114442109134, 'wmean': 0.4559768652551844}, 'spearman': {'mean': 0.5149909417751026, 'wmean': 0.4985400027011752}}}, 'STS13': {'FNWN': {'pearson': (0.1597529947823057, 0.0281067996044078), 'spearman': SpearmanrResult(correlation=0.16142214512009456, pvalue=0.026484078920448444), 'nsamples': 189}, 'headlines': {'pearson': (0.5390638400609652, 9.277573550584663e-58), 'spearman': SpearmanrResult(correlation=0.5229334777000713, pvalue=7.179030542057572e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.511868002914741, 8.650179866777323e-39), 'spearman': SpearmanrResult(correlation=0.5264240467067886, pvalue=2.590185346762765e-41), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.403561612586004, 'wmean': 0.4810994304631663}, 'spearman': {'mean': 0.40359322317565144, 'wmean': 0.47868852260350647}}}, 'STS14': {'deft-forum': {'pearson': (0.16814350534365155, 0.00034034807689650335), 'spearman': SpearmanrResult(correlation=0.17630172008789952, pvalue=0.0001705892703537447), 'nsamples': 450}, 'deft-news': {'pearson': (0.6456350067375206, 9.002090112535063e-37), 'spearman': SpearmanrResult(correlation=0.6129984881107731, pvalue=2.4059254543380466e-32), 'nsamples': 300}, 'headlines': {'pearson': (0.4955580987786442, 9.903531082920503e-48), 'spearman': SpearmanrResult(correlation=0.45781020776618314, pvalue=3.969818821228517e-40), 'nsamples': 750}, 'images': {'pearson': (0.39768232543523463, 7.940070129764123e-30), 'spearman': SpearmanrResult(correlation=0.4009593808974657, pvalue=2.4579360190913297e-30), 'nsamples': 750}, 'OnWN': {'pearson': (0.5977167044954501, 7.783895557095625e-74), 'spearman': SpearmanrResult(correlation=0.6221962409263349, pvalue=1.4004243465132927e-81), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5718248025577036, 2.443364174380726e-66), 'spearman': SpearmanrResult(correlation=0.5427663257066381, pvalue=1.1086683644632674e-58), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.47942674055803414, 'wmean': 0.4843844074336464}, 'spearman': {'mean': 0.46883872724921566, 'wmean': 0.47494251651873415}}}, 'STS15': {'answers-forums': {'pearson': (0.496000963424063, 1.1085788816275362e-24), 'spearman': SpearmanrResult(correlation=0.4832745593878562, pvalue=2.4191289234651922e-23), 'nsamples': 375}, 'answers-students': {'pearson': (0.561073792418184, 2.0387423004371373e-63), 'spearman': SpearmanrResult(correlation=0.5722594997967129, pvalue=1.8517847656425173e-66), 'nsamples': 750}, 'belief': {'pearson': (0.6013360809961484, 3.046723775231956e-38), 'spearman': SpearmanrResult(correlation=0.6454064398015774, pvalue=1.4591729426417755e-45), 'nsamples': 375}, 'headlines': {'pearson': (0.5935528539046805, 1.3896729548987363e-72), 'spearman': SpearmanrResult(correlation=0.5844761240743993, pvalue=6.443438547672865e-70), 'nsamples': 750}, 'images': {'pearson': (0.48073841979807924, 1.2426989030020445e-44), 'spearman': SpearmanrResult(correlation=0.4893961593828722, pvalue=2.0074033451260673e-46), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5465404221082311, 'wmean': 0.5460083970827624}, 'spearman': {'mean': 0.5549625564886835, 'wmean': 0.5526180707121753}}}, 'STS16': {'answer-answer': {'pearson': (0.5184106919352212, 7.143769402462112e-19), 'spearman': SpearmanrResult(correlation=0.5293961526099006, pvalue=9.480928465495697e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.6158054411975273, 2.192918265133675e-27), 'spearman': SpearmanrResult(correlation=0.620402337198586, pvalue=6.992194730028381e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7433441479286524, 1.0801914359033389e-41), 'spearman': SpearmanrResult(correlation=0.7670026327683691, pvalue=7.933117987806804e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.7827300206774757, 9.401020862233033e-52), 'spearman': SpearmanrResult(correlation=0.8151679432999936, pvalue=2.5493516835548803e-59), 'nsamples': 244}, 'question-question': {'pearson': (0.22567842691609832, 0.0010179516995015873), 'spearman': SpearmanrResult(correlation=0.2694107911604967, pvalue=7.99158626863637e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5771937457309949, 'wmean': 0.5852731373558929}, 'spearman': {'mean': 0.6002759714074692, 'wmean': 0.6075592274703456}}}, 'MR': {'devacc': 78.31, 'acc': 78.43, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.61, 'acc': 78.86, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.92, 'acc': 83.1, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.59, 'acc': 94.11, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.14, 'acc': 82.48, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.6, 'acc': 44.93, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 79.8, 'acc': 91.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.22, 'acc': 66.67, 'f1': 79.94, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 72.2, 'acc': 69.37, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7565538692097076, 'pearson': 0.7736922549206854, 'spearman': 0.7206778686599586, 'mse': 0.4085247303536526, 'yhat': array([3.34710168, 4.22862097, 1.33570944, ..., 3.02814572, 4.29222563,        4.67007621]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6887746800949036, 'pearson': 0.6442066672531649, 'spearman': 0.6309629477037366, 'mse': 1.6813679460700235, 'yhat': array([1.54192106, 2.27346716, 2.24605849, ..., 3.92252159, 3.57599524,        3.41668112]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 65.5, 'acc': 64.83, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 73.79, 'acc': 74.58, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 9.75, 'acc': 10.09, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.98, 'acc': 31.37, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 73.15, 'acc': 73.25, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.24, 'acc': 90.71, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.0, 'acc': 88.01, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 86.86, 'acc': 86.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.19, 'acc': 81.32, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.6, 'acc': 67.96, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.32, 'acc': 67.52, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 13:00:13,673 : STS12 p=0.4560, STS12 s=0.4985, STS13 p=0.4811, STS13 s=0.4787, STS14 p=0.4844, STS14 s=0.4749, STS15 p=0.5460, STS15 s=0.5526, STS 16 p=0.5853, STS16 s=0.6076, STS B p=0.6442, STS B s=0.6310, STS B m=1.6814, SICK-R p=0.7737, SICK-R s=0.7207, SICK-P m=0.4085
2019-03-13 13:00:13,673 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 13:00:13,673 : 0.4560,0.4985,0.4811,0.4787,0.4844,0.4749,0.5460,0.5526,0.5853,0.6076,0.6442,0.6310,1.6814,0.7737,0.7207,0.4085
2019-03-13 13:00:13,673 : MR=78.43, CR=78.86, SUBJ=94.11, MPQA=83.10, SST-B=82.48, SST-F=44.93, TREC=91.60, SICK-E=69.37, SNLI=64.83, MRPC=66.67, MRPC f=79.94
2019-03-13 13:00:13,673 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 13:00:13,673 : 78.43,78.86,94.11,83.10,82.48,44.93,91.60,69.37,64.83,66.67,79.94
2019-03-13 13:00:13,673 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 13:00:13,673 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 13:00:13,673 : na,na,na,na,na,na,na,na,na,na
2019-03-13 13:00:13,673 : SentLen=74.58, WC=10.09, TreeDepth=31.37, TopConst=73.25, BShift=90.71, Tense=88.01, SubjNum=86.00, ObjNum=81.32, SOMO=67.96, CoordInv=67.52, average=67.08
2019-03-13 13:00:13,673 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 13:00:13,673 : 74.58,10.09,31.37,73.25,90.71,88.01,86.00,81.32,67.96,67.52,67.08
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,673 : layer 17
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,673 : ********************************************************************************
2019-03-13 13:00:13,767 : ***** Transfer task : STS12 *****


2019-03-13 13:00:13,779 : loading BERT model bert-large-uncased
2019-03-13 13:00:13,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:00:13,797 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:00:13,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzsmu1hp3
2019-03-13 13:00:21,226 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:00:30,520 : MSRpar : pearson = 0.2573, spearman = 0.3238
2019-03-13 13:00:32,174 : MSRvid : pearson = 0.2038, spearman = 0.2478
2019-03-13 13:00:33,598 : SMTeuroparl : pearson = 0.3755, spearman = 0.4931
2019-03-13 13:00:36,318 : surprise.OnWN : pearson = 0.2072, spearman = 0.3191
2019-03-13 13:00:37,756 : surprise.SMTnews : pearson = 0.6062, spearman = 0.5513
2019-03-13 13:00:37,756 : ALL (weighted average) : Pearson = 0.2946,             Spearman = 0.3586
2019-03-13 13:00:37,756 : ALL (average) : Pearson = 0.3300,             Spearman = 0.3870

2019-03-13 13:00:37,757 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 13:00:37,765 : loading BERT model bert-large-uncased
2019-03-13 13:00:37,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:00:37,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:00:37,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplbf2v23r
2019-03-13 13:00:45,190 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:00:51,785 : FNWN : pearson = -0.0765, spearman = -0.0445
2019-03-13 13:00:53,692 : headlines : pearson = 0.1676, spearman = 0.2641
2019-03-13 13:00:55,171 : OnWN : pearson = 0.1887, spearman = 0.2177
2019-03-13 13:00:55,172 : ALL (weighted average) : Pearson = 0.1447,             Spearman = 0.2079
2019-03-13 13:00:55,172 : ALL (average) : Pearson = 0.0933,             Spearman = 0.1458

2019-03-13 13:00:55,172 : ***** Transfer task : STS14 *****


2019-03-13 13:00:55,187 : loading BERT model bert-large-uncased
2019-03-13 13:00:55,187 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:00:55,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:00:55,205 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp02lljl53
2019-03-13 13:01:02,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:01:09,217 : deft-forum : pearson = 0.0450, spearman = 0.0954
2019-03-13 13:01:10,869 : deft-news : pearson = 0.4991, spearman = 0.5442
2019-03-13 13:01:13,057 : headlines : pearson = 0.2205, spearman = 0.2786
2019-03-13 13:01:15,151 : images : pearson = 0.1017, spearman = 0.1559
2019-03-13 13:01:17,300 : OnWN : pearson = 0.2054, spearman = 0.2989
2019-03-13 13:01:20,187 : tweet-news : pearson = 0.1949, spearman = 0.3208
2019-03-13 13:01:20,187 : ALL (weighted average) : Pearson = 0.1898,             Spearman = 0.2658
2019-03-13 13:01:20,187 : ALL (average) : Pearson = 0.2111,             Spearman = 0.2823

2019-03-13 13:01:20,187 : ***** Transfer task : STS15 *****


2019-03-13 13:01:20,219 : loading BERT model bert-large-uncased
2019-03-13 13:01:20,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:01:20,237 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:01:20,237 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmponk4sbf4
2019-03-13 13:01:27,646 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:01:34,622 : answers-forums : pearson = 0.3778, spearman = 0.4025
2019-03-13 13:01:36,724 : answers-students : pearson = 0.3088, spearman = 0.4081
2019-03-13 13:01:38,792 : belief : pearson = 0.1890, spearman = 0.4178
2019-03-13 13:01:41,064 : headlines : pearson = 0.2700, spearman = 0.3903
2019-03-13 13:01:43,220 : images : pearson = 0.0905, spearman = 0.2119
2019-03-13 13:01:43,220 : ALL (weighted average) : Pearson = 0.2382,             Spearman = 0.3551
2019-03-13 13:01:43,220 : ALL (average) : Pearson = 0.2472,             Spearman = 0.3661

2019-03-13 13:01:43,220 : ***** Transfer task : STS16 *****


2019-03-13 13:01:43,290 : loading BERT model bert-large-uncased
2019-03-13 13:01:43,290 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:01:43,308 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:01:43,308 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdlnwoje7
2019-03-13 13:01:50,755 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:01:56,831 : answer-answer : pearson = 0.3527, spearman = 0.4079
2019-03-13 13:01:57,497 : headlines : pearson = 0.2456, spearman = 0.4026
2019-03-13 13:01:58,387 : plagiarism : pearson = 0.5622, spearman = 0.6826
2019-03-13 13:01:59,900 : postediting : pearson = 0.5389, spearman = 0.7283
2019-03-13 13:02:00,512 : question-question : pearson = -0.0510, spearman = 0.0286
2019-03-13 13:02:00,513 : ALL (weighted average) : Pearson = 0.3380,             Spearman = 0.4591
2019-03-13 13:02:00,513 : ALL (average) : Pearson = 0.3297,             Spearman = 0.4500

2019-03-13 13:02:00,513 : ***** Transfer task : MR *****


2019-03-13 13:02:00,528 : loading BERT model bert-large-uncased
2019-03-13 13:02:00,528 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:02:00,548 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:02:00,548 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyflodl98
2019-03-13 13:02:08,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:02:13,285 : Generating sentence embeddings
2019-03-13 13:02:45,103 : Generated sentence embeddings
2019-03-13 13:02:45,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:02:56,104 : Best param found at split 1: l2reg = 1e-05                 with score 80.43
2019-03-13 13:03:05,501 : Best param found at split 2: l2reg = 1e-05                 with score 80.12
2019-03-13 13:03:14,256 : Best param found at split 3: l2reg = 0.0001                 with score 81.0
2019-03-13 13:03:24,028 : Best param found at split 4: l2reg = 0.0001                 with score 79.8
2019-03-13 13:03:32,143 : Best param found at split 5: l2reg = 0.001                 with score 79.67
2019-03-13 13:03:32,584 : Dev acc : 80.2 Test acc : 80.44

2019-03-13 13:03:32,585 : ***** Transfer task : CR *****


2019-03-13 13:03:32,592 : loading BERT model bert-large-uncased
2019-03-13 13:03:32,592 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:03:32,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:03:32,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphx5g2tbr
2019-03-13 13:03:40,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:03:45,231 : Generating sentence embeddings
2019-03-13 13:03:53,596 : Generated sentence embeddings
2019-03-13 13:03:53,596 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:03:57,400 : Best param found at split 1: l2reg = 1e-05                 with score 85.33
2019-03-13 13:04:01,398 : Best param found at split 2: l2reg = 1e-05                 with score 85.82
2019-03-13 13:04:05,543 : Best param found at split 3: l2reg = 0.0001                 with score 86.13
2019-03-13 13:04:09,759 : Best param found at split 4: l2reg = 0.0001                 with score 85.57
2019-03-13 13:04:13,724 : Best param found at split 5: l2reg = 1e-05                 with score 85.47
2019-03-13 13:04:13,941 : Dev acc : 85.66 Test acc : 83.21

2019-03-13 13:04:13,941 : ***** Transfer task : MPQA *****


2019-03-13 13:04:13,947 : loading BERT model bert-large-uncased
2019-03-13 13:04:13,948 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:04:13,998 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:04:13,998 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzcsn62mi
2019-03-13 13:04:21,429 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:04:26,569 : Generating sentence embeddings
2019-03-13 13:04:34,182 : Generated sentence embeddings
2019-03-13 13:04:34,183 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:04:44,048 : Best param found at split 1: l2reg = 1e-05                 with score 81.91
2019-03-13 13:04:53,240 : Best param found at split 2: l2reg = 1e-05                 with score 80.86
2019-03-13 13:05:05,029 : Best param found at split 3: l2reg = 1e-05                 with score 82.91
2019-03-13 13:05:15,755 : Best param found at split 4: l2reg = 1e-05                 with score 81.93
2019-03-13 13:05:26,625 : Best param found at split 5: l2reg = 0.001                 with score 82.42
2019-03-13 13:05:27,141 : Dev acc : 82.01 Test acc : 83.23

2019-03-13 13:05:27,142 : ***** Transfer task : SUBJ *****


2019-03-13 13:05:27,158 : loading BERT model bert-large-uncased
2019-03-13 13:05:27,158 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:05:27,177 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:05:27,177 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzg6lvieo
2019-03-13 13:05:34,599 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:05:39,824 : Generating sentence embeddings
2019-03-13 13:06:10,992 : Generated sentence embeddings
2019-03-13 13:06:10,993 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 13:06:22,029 : Best param found at split 1: l2reg = 1e-05                 with score 95.12
2019-03-13 13:06:32,658 : Best param found at split 2: l2reg = 1e-05                 with score 95.38
2019-03-13 13:06:41,924 : Best param found at split 3: l2reg = 1e-05                 with score 94.81
2019-03-13 13:06:52,942 : Best param found at split 4: l2reg = 1e-05                 with score 95.06
2019-03-13 13:07:03,380 : Best param found at split 5: l2reg = 1e-05                 with score 95.03
2019-03-13 13:07:03,853 : Dev acc : 95.08 Test acc : 94.18

2019-03-13 13:07:03,854 : ***** Transfer task : SST Binary classification *****


2019-03-13 13:07:03,946 : loading BERT model bert-large-uncased
2019-03-13 13:07:03,946 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:07:04,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:07:04,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpaw2yjn5x
2019-03-13 13:07:11,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:07:16,659 : Computing embedding for train
2019-03-13 13:08:58,226 : Computed train embeddings
2019-03-13 13:08:58,226 : Computing embedding for dev
2019-03-13 13:09:00,480 : Computed dev embeddings
2019-03-13 13:09:00,480 : Computing embedding for test
2019-03-13 13:09:05,241 : Computed test embeddings
2019-03-13 13:09:05,241 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:09:23,156 : [('reg:1e-05', 84.06), ('reg:0.0001', 84.17), ('reg:0.001', 82.91), ('reg:0.01', 79.36)]
2019-03-13 13:09:23,156 : Validation : best param found is reg = 0.0001 with score             84.17
2019-03-13 13:09:23,156 : Evaluating...
2019-03-13 13:09:28,138 : 
Dev acc : 84.17 Test acc : 85.56 for             SST Binary classification

2019-03-13 13:09:28,138 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 13:09:28,193 : loading BERT model bert-large-uncased
2019-03-13 13:09:28,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:09:28,214 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:09:28,214 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmheuh6_q
2019-03-13 13:09:35,654 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:09:40,969 : Computing embedding for train
2019-03-13 13:10:03,098 : Computed train embeddings
2019-03-13 13:10:03,098 : Computing embedding for dev
2019-03-13 13:10:05,996 : Computed dev embeddings
2019-03-13 13:10:05,996 : Computing embedding for test
2019-03-13 13:10:11,710 : Computed test embeddings
2019-03-13 13:10:11,710 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:10:14,225 : [('reg:1e-05', 45.5), ('reg:0.0001', 45.05), ('reg:0.001', 43.96), ('reg:0.01', 42.14)]
2019-03-13 13:10:14,225 : Validation : best param found is reg = 1e-05 with score             45.5
2019-03-13 13:10:14,225 : Evaluating...
2019-03-13 13:10:14,874 : 
Dev acc : 45.5 Test acc : 45.25 for             SST Fine-Grained classification

2019-03-13 13:10:14,874 : ***** Transfer task : TREC *****


2019-03-13 13:10:14,888 : loading BERT model bert-large-uncased
2019-03-13 13:10:14,889 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:10:14,907 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:10:14,908 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6nj9vfkj
2019-03-13 13:10:22,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:10:35,288 : Computed train embeddings
2019-03-13 13:10:35,883 : Computed test embeddings
2019-03-13 13:10:35,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 13:10:43,376 : [('reg:1e-05', 76.16), ('reg:0.0001', 75.53), ('reg:0.001', 66.63), ('reg:0.01', 40.55)]
2019-03-13 13:10:43,376 : Cross-validation : best param found is reg = 1e-05             with score 76.16
2019-03-13 13:10:43,376 : Evaluating...
2019-03-13 13:10:43,992 : 
Dev acc : 76.16 Test acc : 91.4             for TREC

2019-03-13 13:10:43,993 : ***** Transfer task : MRPC *****


2019-03-13 13:10:44,014 : loading BERT model bert-large-uncased
2019-03-13 13:10:44,014 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:10:44,036 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:10:44,036 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt3_oj3tm
2019-03-13 13:10:51,462 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:10:56,686 : Computing embedding for train
2019-03-13 13:11:19,153 : Computed train embeddings
2019-03-13 13:11:19,153 : Computing embedding for test
2019-03-13 13:11:29,012 : Computed test embeddings
2019-03-13 13:11:29,033 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 13:11:33,700 : [('reg:1e-05', 69.41), ('reg:0.0001', 69.26), ('reg:0.001', 68.92), ('reg:0.01', 68.15)]
2019-03-13 13:11:33,700 : Cross-validation : best param found is reg = 1e-05             with score 69.41
2019-03-13 13:11:33,700 : Evaluating...
2019-03-13 13:11:34,009 : Dev acc : 69.41 Test acc 71.54; Test F1 81.04 for MRPC.

2019-03-13 13:11:34,009 : ***** Transfer task : SICK-Entailment*****


2019-03-13 13:11:34,071 : loading BERT model bert-large-uncased
2019-03-13 13:11:34,071 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:11:34,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:11:34,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw9r1p7op
2019-03-13 13:11:41,530 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:11:46,825 : Computing embedding for train
2019-03-13 13:11:58,216 : Computed train embeddings
2019-03-13 13:11:58,216 : Computing embedding for dev
2019-03-13 13:11:59,777 : Computed dev embeddings
2019-03-13 13:11:59,777 : Computing embedding for test
2019-03-13 13:12:12,017 : Computed test embeddings
2019-03-13 13:12:12,055 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:12:13,483 : [('reg:1e-05', 73.0), ('reg:0.0001', 71.6), ('reg:0.001', 68.8), ('reg:0.01', 63.0)]
2019-03-13 13:12:13,484 : Validation : best param found is reg = 1e-05 with score             73.0
2019-03-13 13:12:13,484 : Evaluating...
2019-03-13 13:12:13,834 : 
Dev acc : 73.0 Test acc : 69.9 for                        SICK entailment

2019-03-13 13:12:13,835 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 13:12:13,862 : loading BERT model bert-large-uncased
2019-03-13 13:12:13,862 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:12:13,920 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:12:13,920 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptwdtw_56
2019-03-13 13:12:21,371 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:12:26,643 : Computing embedding for train
2019-03-13 13:12:38,040 : Computed train embeddings
2019-03-13 13:12:38,040 : Computing embedding for dev
2019-03-13 13:12:39,600 : Computed dev embeddings
2019-03-13 13:12:39,600 : Computing embedding for test
2019-03-13 13:12:51,853 : Computed test embeddings
2019-03-13 13:13:15,101 : Dev : Pearson 0.6683565792175079
2019-03-13 13:13:15,101 : Test : Pearson 0.707422270495342 Spearman 0.6621661713085291 MSE 0.5105851078954158                        for SICK Relatedness

2019-03-13 13:13:15,102 : 

***** Transfer task : STSBenchmark*****


2019-03-13 13:13:15,141 : loading BERT model bert-large-uncased
2019-03-13 13:13:15,141 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:13:15,170 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:13:15,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpevh9xwlx
2019-03-13 13:13:22,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:13:27,807 : Computing embedding for train
2019-03-13 13:13:46,525 : Computed train embeddings
2019-03-13 13:13:46,525 : Computing embedding for dev
2019-03-13 13:13:52,221 : Computed dev embeddings
2019-03-13 13:13:52,221 : Computing embedding for test
2019-03-13 13:13:56,869 : Computed test embeddings
2019-03-13 13:14:19,691 : Dev : Pearson 0.6252391844237434
2019-03-13 13:14:19,691 : Test : Pearson 0.5620206248483772 Spearman 0.5574585959584424 MSE 1.8780760316624443                        for SICK Relatedness

2019-03-13 13:14:19,691 : ***** Transfer task : SNLI Entailment*****


2019-03-13 13:14:24,739 : loading BERT model bert-large-uncased
2019-03-13 13:14:24,740 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:14:24,861 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:14:24,861 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpidikjvx8
2019-03-13 13:14:32,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:14:38,053 : PROGRESS (encoding): 0.00%
2019-03-13 13:17:28,322 : PROGRESS (encoding): 14.56%
2019-03-13 13:20:40,481 : PROGRESS (encoding): 29.12%
2019-03-13 13:23:51,904 : PROGRESS (encoding): 43.69%
2019-03-13 13:27:15,475 : PROGRESS (encoding): 58.25%
2019-03-13 13:31:02,731 : PROGRESS (encoding): 72.81%
2019-03-13 13:34:48,796 : PROGRESS (encoding): 87.37%
2019-03-13 13:38:53,628 : PROGRESS (encoding): 0.00%
2019-03-13 13:39:24,282 : PROGRESS (encoding): 0.00%
2019-03-13 13:39:53,738 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:40:25,346 : [('reg:1e-09', 61.77)]
2019-03-13 13:40:25,346 : Validation : best param found is reg = 1e-09 with score             61.77
2019-03-13 13:40:25,347 : Evaluating...
2019-03-13 13:40:57,238 : Dev acc : 61.77 Test acc : 61.74 for SNLI

2019-03-13 13:40:57,239 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 13:40:57,444 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 13:40:58,535 : loading BERT model bert-large-uncased
2019-03-13 13:40:58,535 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:40:58,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:40:58,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdha88853
2019-03-13 13:41:06,078 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:41:11,280 : Computing embeddings for train/dev/test
2019-03-13 13:44:45,020 : Computed embeddings
2019-03-13 13:44:45,020 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:45:10,211 : [('reg:1e-05', 74.51), ('reg:0.0001', 68.61), ('reg:0.001', 57.26), ('reg:0.01', 41.74)]
2019-03-13 13:45:10,211 : Validation : best param found is reg = 1e-05 with score             74.51
2019-03-13 13:45:10,211 : Evaluating...
2019-03-13 13:45:18,935 : 
Dev acc : 74.5 Test acc : 76.5 for LENGTH classification

2019-03-13 13:45:18,936 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 13:45:19,313 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 13:45:19,357 : loading BERT model bert-large-uncased
2019-03-13 13:45:19,357 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:45:19,386 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:45:19,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp60axq279
2019-03-13 13:45:26,854 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:45:32,107 : Computing embeddings for train/dev/test
2019-03-13 13:48:50,474 : Computed embeddings
2019-03-13 13:48:50,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:49:18,785 : [('reg:1e-05', 10.07), ('reg:0.0001', 1.06), ('reg:0.001', 0.27), ('reg:0.01', 0.18)]
2019-03-13 13:49:18,786 : Validation : best param found is reg = 1e-05 with score             10.07
2019-03-13 13:49:18,786 : Evaluating...
2019-03-13 13:49:26,438 : 
Dev acc : 10.1 Test acc : 10.5 for WORDCONTENT classification

2019-03-13 13:49:26,440 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 13:49:26,816 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 13:49:26,882 : loading BERT model bert-large-uncased
2019-03-13 13:49:26,882 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:49:26,906 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:49:26,907 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpow83i7y5
2019-03-13 13:49:34,385 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:49:39,654 : Computing embeddings for train/dev/test
2019-03-13 13:52:46,070 : Computed embeddings
2019-03-13 13:52:46,070 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:53:12,728 : [('reg:1e-05', 31.99), ('reg:0.0001', 30.21), ('reg:0.001', 24.76), ('reg:0.01', 18.12)]
2019-03-13 13:53:12,728 : Validation : best param found is reg = 1e-05 with score             31.99
2019-03-13 13:53:12,728 : Evaluating...
2019-03-13 13:53:20,062 : 
Dev acc : 32.0 Test acc : 32.1 for DEPTH classification

2019-03-13 13:53:20,063 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 13:53:20,443 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 13:53:20,507 : loading BERT model bert-large-uncased
2019-03-13 13:53:20,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:53:20,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:53:20,613 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqq3jjitk
2019-03-13 13:53:28,029 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:53:33,286 : Computing embeddings for train/dev/test
2019-03-13 13:56:26,052 : Computed embeddings
2019-03-13 13:56:26,052 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 13:56:55,404 : [('reg:1e-05', 72.92), ('reg:0.0001', 67.19), ('reg:0.001', 53.91), ('reg:0.01', 24.38)]
2019-03-13 13:56:55,405 : Validation : best param found is reg = 1e-05 with score             72.92
2019-03-13 13:56:55,405 : Evaluating...
2019-03-13 13:57:02,752 : 
Dev acc : 72.9 Test acc : 72.2 for TOPCONSTITUENTS classification

2019-03-13 13:57:02,753 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 13:57:03,098 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 13:57:03,164 : loading BERT model bert-large-uncased
2019-03-13 13:57:03,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 13:57:03,284 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 13:57:03,284 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeirk7or1
2019-03-13 13:57:10,722 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 13:57:16,053 : Computing embeddings for train/dev/test
2019-03-13 14:00:24,340 : Computed embeddings
2019-03-13 14:00:24,340 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:00:53,626 : [('reg:1e-05', 91.83), ('reg:0.0001', 91.52), ('reg:0.001', 89.42), ('reg:0.01', 86.03)]
2019-03-13 14:00:53,627 : Validation : best param found is reg = 1e-05 with score             91.83
2019-03-13 14:00:53,627 : Evaluating...
2019-03-13 14:01:02,087 : 
Dev acc : 91.8 Test acc : 91.6 for BIGRAMSHIFT classification

2019-03-13 14:01:02,088 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 14:01:02,644 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 14:01:02,709 : loading BERT model bert-large-uncased
2019-03-13 14:01:02,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:01:02,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:01:02,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpeaqvoday
2019-03-13 14:01:10,206 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:01:15,447 : Computing embeddings for train/dev/test
2019-03-13 14:04:17,837 : Computed embeddings
2019-03-13 14:04:17,837 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:04:40,514 : [('reg:1e-05', 90.49), ('reg:0.0001', 90.52), ('reg:0.001', 90.41), ('reg:0.01', 89.82)]
2019-03-13 14:04:40,514 : Validation : best param found is reg = 0.0001 with score             90.52
2019-03-13 14:04:40,515 : Evaluating...
2019-03-13 14:04:46,766 : 
Dev acc : 90.5 Test acc : 89.1 for TENSE classification

2019-03-13 14:04:46,768 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 14:04:47,193 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 14:04:47,257 : loading BERT model bert-large-uncased
2019-03-13 14:04:47,257 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:04:47,282 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:04:47,282 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6mwtp1_r
2019-03-13 14:04:54,759 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:05:00,135 : Computing embeddings for train/dev/test
2019-03-13 14:08:12,758 : Computed embeddings
2019-03-13 14:08:12,758 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:08:39,454 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.36), ('reg:0.001', 81.22), ('reg:0.01', 72.68)]
2019-03-13 14:08:39,455 : Validation : best param found is reg = 1e-05 with score             84.75
2019-03-13 14:08:39,455 : Evaluating...
2019-03-13 14:08:45,787 : 
Dev acc : 84.8 Test acc : 84.4 for SUBJNUMBER classification

2019-03-13 14:08:45,788 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 14:08:46,207 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 14:08:46,275 : loading BERT model bert-large-uncased
2019-03-13 14:08:46,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:08:46,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:08:46,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp94bq0c_o
2019-03-13 14:08:53,850 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:08:59,188 : Computing embeddings for train/dev/test
2019-03-13 14:12:09,271 : Computed embeddings
2019-03-13 14:12:09,272 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:12:35,156 : [('reg:1e-05', 80.73), ('reg:0.0001', 79.27), ('reg:0.001', 76.07), ('reg:0.01', 64.03)]
2019-03-13 14:12:35,156 : Validation : best param found is reg = 1e-05 with score             80.73
2019-03-13 14:12:35,156 : Evaluating...
2019-03-13 14:12:43,718 : 
Dev acc : 80.7 Test acc : 81.8 for OBJNUMBER classification

2019-03-13 14:12:43,719 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 14:12:44,294 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 14:12:44,361 : loading BERT model bert-large-uncased
2019-03-13 14:12:44,361 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:12:44,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:12:44,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq338es76
2019-03-13 14:12:51,814 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:12:57,084 : Computing embeddings for train/dev/test
2019-03-13 14:16:38,390 : Computed embeddings
2019-03-13 14:16:38,391 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:17:11,690 : [('reg:1e-05', 69.37), ('reg:0.0001', 68.62), ('reg:0.001', 67.49), ('reg:0.01', 64.94)]
2019-03-13 14:17:11,690 : Validation : best param found is reg = 1e-05 with score             69.37
2019-03-13 14:17:11,690 : Evaluating...
2019-03-13 14:17:20,284 : 
Dev acc : 69.4 Test acc : 69.1 for ODDMANOUT classification

2019-03-13 14:17:20,285 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 14:17:20,688 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 14:17:20,766 : loading BERT model bert-large-uncased
2019-03-13 14:17:20,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:17:20,898 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:17:20,898 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm23_azsl
2019-03-13 14:17:28,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:17:33,622 : Computing embeddings for train/dev/test
2019-03-13 14:21:14,343 : Computed embeddings
2019-03-13 14:21:14,343 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:21:45,762 : [('reg:1e-05', 73.27), ('reg:0.0001', 72.61), ('reg:0.001', 71.06), ('reg:0.01', 65.84)]
2019-03-13 14:21:45,763 : Validation : best param found is reg = 1e-05 with score             73.27
2019-03-13 14:21:45,763 : Evaluating...
2019-03-13 14:21:56,140 : 
Dev acc : 73.3 Test acc : 72.9 for COORDINATIONINVERSION classification

2019-03-13 14:21:56,141 : total results: {'STS12': {'MSRpar': {'pearson': (0.257321746939413, 8.298147696437899e-13), 'spearman': SpearmanrResult(correlation=0.32380050102543795, pvalue=9.107056081313904e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (0.20384672495797515, 1.7761008727443305e-08), 'spearman': SpearmanrResult(correlation=0.24784410243922883, pvalue=5.82549485246653e-12), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.37548329578646655, 8.136285010640307e-17), 'spearman': SpearmanrResult(correlation=0.4931300647863416, pvalue=1.6828857394489756e-29), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.20720865032576088, 1.0195132247996998e-08), 'spearman': SpearmanrResult(correlation=0.319124301431787, pvalue=3.2370438988473426e-19), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6061935561327957, 2.1494362174018226e-41), 'spearman': SpearmanrResult(correlation=0.5512903614108935, pvalue=4.228081986391855e-33), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3300107948284823, 'wmean': 0.2945627102092456}, 'spearman': {'mean': 0.38703786621873776, 'wmean': 0.3585547723977534}}}, 'STS13': {'FNWN': {'pearson': (-0.07645509929647604, 0.29572621034327307), 'spearman': SpearmanrResult(correlation=-0.04446069338009879, pvalue=0.5435330223102908), 'nsamples': 189}, 'headlines': {'pearson': (0.16763465900453445, 3.913788948751674e-06), 'spearman': SpearmanrResult(correlation=0.26408379518034764, pvalue=1.9664193020506587e-13), 'nsamples': 750}, 'OnWN': {'pearson': (0.18865473579750155, 6.831797568374715e-06), 'spearman': SpearmanrResult(correlation=0.21773804968335092, pvalue=1.9054248660822765e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.09327809850185331, 'wmean': 0.1447408581791768}, 'spearman': {'mean': 0.14578705049453325, 'wmean': 0.20787388080585462}}}, 'STS14': {'deft-forum': {'pearson': (0.04504341350007065, 0.3404171099052343), 'spearman': SpearmanrResult(correlation=0.09544603924634049, pvalue=0.04300031152401756), 'nsamples': 450}, 'deft-news': {'pearson': (0.4990651233124388, 2.671242013083266e-20), 'spearman': SpearmanrResult(correlation=0.5442408356943261, pvalue=1.568258900166342e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.22047489044618013, 1.0400605528058556e-09), 'spearman': SpearmanrResult(correlation=0.2786344588702967, pvalue=7.699357120032959e-15), 'nsamples': 750}, 'images': {'pearson': (0.10165763917811445, 0.005326623630664084), 'spearman': SpearmanrResult(correlation=0.15592179355054708, pvalue=1.792923016914166e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.20537481334250934, 1.3816478398659707e-08), 'spearman': SpearmanrResult(correlation=0.2989179331390696, pvalue=6.035202196207708e-17), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19494904736963523, 7.378069899266412e-08), 'spearman': SpearmanrResult(correlation=0.32079405558130564, pvalue=2.063435055829188e-19), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.21109415452482475, 'wmean': 0.18982169755229142}, 'spearman': {'mean': 0.28232585268031424, 'wmean': 0.26584643979335076}}}, 'STS15': {'answers-forums': {'pearson': (0.37775558690676986, 3.6410170403305265e-14), 'spearman': SpearmanrResult(correlation=0.4025496845833284, pvalue=4.849736490948306e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.3087640811573223, 4.96903953720926e-18), 'spearman': SpearmanrResult(correlation=0.4080817866644129, pvalue=1.8363270283199142e-31), 'nsamples': 750}, 'belief': {'pearson': (0.18902882161546747, 0.00023181944392337378), 'spearman': SpearmanrResult(correlation=0.41778047121872086, pvalue=2.8398155434892733e-17), 'nsamples': 375}, 'headlines': {'pearson': (0.26999506548386737, 5.397840385922716e-14), 'spearman': SpearmanrResult(correlation=0.3902530001139573, pvalue=1.0799563461941867e-28), 'nsamples': 750}, 'images': {'pearson': (0.09047035469753632, 0.013190482430863388), 'spearman': SpearmanrResult(correlation=0.2118687398003319, pvalue=4.650212280392963e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.24720278197219264, 'wmean': 0.23815542639996118}, 'spearman': {'mean': 0.3661067364761502, 'wmean': 0.35509215111993164}}}, 'STS16': {'answer-answer': {'pearson': (0.35265910126782135, 7.492821740812222e-09), 'spearman': SpearmanrResult(correlation=0.4078983940312973, pvalue=1.3304085763683703e-11), 'nsamples': 254}, 'headlines': {'pearson': (0.24560969228199706, 8.9914328837741e-05), 'spearman': SpearmanrResult(correlation=0.4025601353419162, pvalue=4.063151288620776e-11), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5622417547892637, 1.4341448022410396e-20), 'spearman': SpearmanrResult(correlation=0.6826056252603853, pvalue=6.768407338875031e-33), 'nsamples': 230}, 'postediting': {'pearson': (0.5388520966081785, 8.899354136036173e-20), 'spearman': SpearmanrResult(correlation=0.7282821475287508, pvalue=1.334728642361575e-41), 'nsamples': 244}, 'question-question': {'pearson': (-0.05101847455707608, 0.4631803713273217), 'spearman': SpearmanrResult(correlation=0.02860482872868606, pvalue=0.68097044332935), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.3296688340780369, 'wmean': 0.3379973685427835}, 'spearman': {'mean': 0.4499902261782071, 'wmean': 0.4591249686300893}}}, 'MR': {'devacc': 80.2, 'acc': 80.44, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.66, 'acc': 83.21, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.01, 'acc': 83.23, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.08, 'acc': 94.18, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.17, 'acc': 85.56, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.5, 'acc': 45.25, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 76.16, 'acc': 91.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.41, 'acc': 71.54, 'f1': 81.04, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.0, 'acc': 69.9, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6683565792175079, 'pearson': 0.707422270495342, 'spearman': 0.6621661713085291, 'mse': 0.5105851078954158, 'yhat': array([2.97284913, 3.96999937, 2.7909373 , ..., 3.39066268, 4.38598666,        4.65415476]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6252391844237434, 'pearson': 0.5620206248483772, 'spearman': 0.5574585959584424, 'mse': 1.8780760316624443, 'yhat': array([1.69202695, 2.24989081, 2.68681253, ..., 3.80450107, 3.53324468,        3.75397645]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.77, 'acc': 61.74, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 74.51, 'acc': 76.5, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 10.07, 'acc': 10.46, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 31.99, 'acc': 32.11, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 72.92, 'acc': 72.23, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.83, 'acc': 91.64, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.52, 'acc': 89.07, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 84.75, 'acc': 84.42, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 80.73, 'acc': 81.84, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 69.37, 'acc': 69.09, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 73.27, 'acc': 72.92, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 14:21:56,142 : STS12 p=0.2946, STS12 s=0.3586, STS13 p=0.1447, STS13 s=0.2079, STS14 p=0.1898, STS14 s=0.2658, STS15 p=0.2382, STS15 s=0.3551, STS 16 p=0.3380, STS16 s=0.4591, STS B p=0.5620, STS B s=0.5575, STS B m=1.8781, SICK-R p=0.7074, SICK-R s=0.6622, SICK-P m=0.5106
2019-03-13 14:21:56,142 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 14:21:56,142 : 0.2946,0.3586,0.1447,0.2079,0.1898,0.2658,0.2382,0.3551,0.3380,0.4591,0.5620,0.5575,1.8781,0.7074,0.6622,0.5106
2019-03-13 14:21:56,142 : MR=80.44, CR=83.21, SUBJ=94.18, MPQA=83.23, SST-B=85.56, SST-F=45.25, TREC=91.40, SICK-E=69.90, SNLI=61.74, MRPC=71.54, MRPC f=81.04
2019-03-13 14:21:56,142 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 14:21:56,142 : 80.44,83.21,94.18,83.23,85.56,45.25,91.40,69.90,61.74,71.54,81.04
2019-03-13 14:21:56,142 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 14:21:56,142 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 14:21:56,142 : na,na,na,na,na,na,na,na,na,na
2019-03-13 14:21:56,142 : SentLen=76.50, WC=10.46, TreeDepth=32.11, TopConst=72.23, BShift=91.64, Tense=89.07, SubjNum=84.42, ObjNum=81.84, SOMO=69.09, CoordInv=72.92, average=68.03
2019-03-13 14:21:56,142 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 14:21:56,142 : 76.50,10.46,32.11,72.23,91.64,89.07,84.42,81.84,69.09,72.92,68.03
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,142 : layer 18
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,142 : ********************************************************************************
2019-03-13 14:21:56,231 : ***** Transfer task : STS12 *****


2019-03-13 14:21:56,244 : loading BERT model bert-large-uncased
2019-03-13 14:21:56,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:21:56,261 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:21:56,261 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpciqkcglv
2019-03-13 14:22:03,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:22:13,126 : MSRpar : pearson = 0.1884, spearman = 0.3045
2019-03-13 14:22:14,780 : MSRvid : pearson = -0.0194, spearman = 0.0353
2019-03-13 14:22:16,202 : SMTeuroparl : pearson = 0.1336, spearman = 0.4193
2019-03-13 14:22:18,922 : surprise.OnWN : pearson = 0.1580, spearman = 0.2054
2019-03-13 14:22:20,364 : surprise.SMTnews : pearson = 0.4765, spearman = 0.5260
2019-03-13 14:22:20,364 : ALL (weighted average) : Pearson = 0.1598,             Spearman = 0.2610
2019-03-13 14:22:20,364 : ALL (average) : Pearson = 0.1874,             Spearman = 0.2981

2019-03-13 14:22:20,364 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 14:22:20,372 : loading BERT model bert-large-uncased
2019-03-13 14:22:20,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:22:20,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:22:20,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgy3jo2i4
2019-03-13 14:22:27,856 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:22:34,354 : FNWN : pearson = -0.0572, spearman = -0.0744
2019-03-13 14:22:36,262 : headlines : pearson = -0.0097, spearman = -0.0041
2019-03-13 14:22:37,744 : OnWN : pearson = 0.2149, spearman = 0.2147
2019-03-13 14:22:37,744 : ALL (weighted average) : Pearson = 0.0683,             Spearman = 0.0688
2019-03-13 14:22:37,744 : ALL (average) : Pearson = 0.0494,             Spearman = 0.0454

2019-03-13 14:22:37,744 : ***** Transfer task : STS14 *****


2019-03-13 14:22:37,761 : loading BERT model bert-large-uncased
2019-03-13 14:22:37,761 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:22:37,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:22:37,779 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpee3y01ef
2019-03-13 14:22:45,194 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:22:51,741 : deft-forum : pearson = 0.0268, spearman = 0.0528
2019-03-13 14:22:53,392 : deft-news : pearson = 0.4177, spearman = 0.5325
2019-03-13 14:22:55,579 : headlines : pearson = 0.1229, spearman = 0.1370
2019-03-13 14:22:57,673 : images : pearson = -0.0602, spearman = 0.0528
2019-03-13 14:22:59,823 : OnWN : pearson = 0.2460, spearman = 0.2544
2019-03-13 14:23:02,714 : tweet-news : pearson = 0.2005, spearman = 0.2227
2019-03-13 14:23:02,715 : ALL (weighted average) : Pearson = 0.1385,             Spearman = 0.1823
2019-03-13 14:23:02,715 : ALL (average) : Pearson = 0.1590,             Spearman = 0.2087

2019-03-13 14:23:02,715 : ***** Transfer task : STS15 *****


2019-03-13 14:23:02,766 : loading BERT model bert-large-uncased
2019-03-13 14:23:02,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:23:02,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:23:02,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphglym0r1
2019-03-13 14:23:10,212 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:23:17,311 : answers-forums : pearson = 0.2611, spearman = 0.3838
2019-03-13 14:23:19,414 : answers-students : pearson = 0.1499, spearman = 0.2177
2019-03-13 14:23:21,487 : belief : pearson = 0.0864, spearman = 0.3246
2019-03-13 14:23:23,764 : headlines : pearson = 0.1907, spearman = 0.2179
2019-03-13 14:23:25,922 : images : pearson = -0.0349, spearman = 0.1402
2019-03-13 14:23:25,922 : ALL (weighted average) : Pearson = 0.1199,             Spearman = 0.2325
2019-03-13 14:23:25,922 : ALL (average) : Pearson = 0.1306,             Spearman = 0.2568

2019-03-13 14:23:25,922 : ***** Transfer task : STS16 *****


2019-03-13 14:23:25,990 : loading BERT model bert-large-uncased
2019-03-13 14:23:25,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:23:26,008 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:23:26,008 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkwxhq12z
2019-03-13 14:23:33,443 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:23:39,572 : answer-answer : pearson = 0.2209, spearman = 0.3580
2019-03-13 14:23:40,238 : headlines : pearson = 0.1205, spearman = 0.1805
2019-03-13 14:23:41,129 : plagiarism : pearson = 0.1670, spearman = 0.5759
2019-03-13 14:23:42,643 : postediting : pearson = 0.2740, spearman = 0.6761
2019-03-13 14:23:43,258 : question-question : pearson = -0.0384, spearman = 0.0527
2019-03-13 14:23:43,258 : ALL (weighted average) : Pearson = 0.1546,             Spearman = 0.3746
2019-03-13 14:23:43,258 : ALL (average) : Pearson = 0.1488,             Spearman = 0.3686

2019-03-13 14:23:43,258 : ***** Transfer task : MR *****


2019-03-13 14:23:43,277 : loading BERT model bert-large-uncased
2019-03-13 14:23:43,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:23:43,295 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:23:43,295 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf4sy_yay
2019-03-13 14:23:50,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:23:56,029 : Generating sentence embeddings
2019-03-13 14:24:27,865 : Generated sentence embeddings
2019-03-13 14:24:27,865 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:24:38,251 : Best param found at split 1: l2reg = 1e-05                 with score 82.27
2019-03-13 14:24:49,504 : Best param found at split 2: l2reg = 0.001                 with score 82.18
2019-03-13 14:25:01,588 : Best param found at split 3: l2reg = 0.001                 with score 82.17
2019-03-13 14:25:13,009 : Best param found at split 4: l2reg = 1e-05                 with score 82.06
2019-03-13 14:25:23,786 : Best param found at split 5: l2reg = 0.0001                 with score 81.71
2019-03-13 14:25:24,592 : Dev acc : 82.08 Test acc : 81.78

2019-03-13 14:25:24,594 : ***** Transfer task : CR *****


2019-03-13 14:25:24,602 : loading BERT model bert-large-uncased
2019-03-13 14:25:24,602 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:25:24,621 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:25:24,621 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx575dkgc
2019-03-13 14:25:32,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:25:37,296 : Generating sentence embeddings
2019-03-13 14:25:45,670 : Generated sentence embeddings
2019-03-13 14:25:45,671 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:25:49,534 : Best param found at split 1: l2reg = 0.0001                 with score 88.11
2019-03-13 14:25:53,391 : Best param found at split 2: l2reg = 1e-05                 with score 87.55
2019-03-13 14:25:57,161 : Best param found at split 3: l2reg = 1e-05                 with score 87.98
2019-03-13 14:26:01,200 : Best param found at split 4: l2reg = 1e-05                 with score 87.98
2019-03-13 14:26:05,239 : Best param found at split 5: l2reg = 1e-05                 with score 87.39
2019-03-13 14:26:05,427 : Dev acc : 87.8 Test acc : 87.26

2019-03-13 14:26:05,427 : ***** Transfer task : MPQA *****


2019-03-13 14:26:05,434 : loading BERT model bert-large-uncased
2019-03-13 14:26:05,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:26:05,485 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:26:05,485 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw0wiytmb
2019-03-13 14:26:12,918 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:26:18,201 : Generating sentence embeddings
2019-03-13 14:26:25,824 : Generated sentence embeddings
2019-03-13 14:26:25,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:26:35,144 : Best param found at split 1: l2reg = 1e-05                 with score 78.45
2019-03-13 14:26:46,184 : Best param found at split 2: l2reg = 1e-05                 with score 81.19
2019-03-13 14:26:56,234 : Best param found at split 3: l2reg = 1e-05                 with score 82.52
2019-03-13 14:27:07,212 : Best param found at split 4: l2reg = 1e-05                 with score 81.37
2019-03-13 14:27:17,706 : Best param found at split 5: l2reg = 1e-05                 with score 82.34
2019-03-13 14:27:18,228 : Dev acc : 81.17 Test acc : 84.39

2019-03-13 14:27:18,229 : ***** Transfer task : SUBJ *****


2019-03-13 14:27:18,244 : loading BERT model bert-large-uncased
2019-03-13 14:27:18,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:27:18,264 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:27:18,264 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx647lugx
2019-03-13 14:27:25,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:27:31,069 : Generating sentence embeddings
2019-03-13 14:28:02,295 : Generated sentence embeddings
2019-03-13 14:28:02,295 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 14:28:11,725 : Best param found at split 1: l2reg = 1e-05                 with score 95.79
2019-03-13 14:28:22,372 : Best param found at split 2: l2reg = 1e-05                 with score 95.79
2019-03-13 14:28:33,100 : Best param found at split 3: l2reg = 1e-05                 with score 95.54
2019-03-13 14:28:43,580 : Best param found at split 4: l2reg = 1e-05                 with score 95.57
2019-03-13 14:28:54,627 : Best param found at split 5: l2reg = 1e-05                 with score 95.69
2019-03-13 14:28:55,121 : Dev acc : 95.68 Test acc : 95.17

2019-03-13 14:28:55,122 : ***** Transfer task : SST Binary classification *****


2019-03-13 14:28:55,213 : loading BERT model bert-large-uncased
2019-03-13 14:28:55,214 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:28:55,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:28:55,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyjfwfc0m
2019-03-13 14:29:02,734 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:29:07,991 : Computing embedding for train
2019-03-13 14:30:49,878 : Computed train embeddings
2019-03-13 14:30:49,878 : Computing embedding for dev
2019-03-13 14:30:52,139 : Computed dev embeddings
2019-03-13 14:30:52,139 : Computing embedding for test
2019-03-13 14:30:56,909 : Computed test embeddings
2019-03-13 14:30:56,909 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:31:13,782 : [('reg:1e-05', 86.93), ('reg:0.0001', 87.04), ('reg:0.001', 87.04), ('reg:0.01', 84.52)]
2019-03-13 14:31:13,782 : Validation : best param found is reg = 0.0001 with score             87.04
2019-03-13 14:31:13,782 : Evaluating...
2019-03-13 14:31:17,871 : 
Dev acc : 87.04 Test acc : 87.64 for             SST Binary classification

2019-03-13 14:31:17,871 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 14:31:17,920 : loading BERT model bert-large-uncased
2019-03-13 14:31:17,921 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:31:17,942 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:31:17,942 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx985ie6r
2019-03-13 14:31:25,364 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:31:30,620 : Computing embedding for train
2019-03-13 14:31:52,778 : Computed train embeddings
2019-03-13 14:31:52,778 : Computing embedding for dev
2019-03-13 14:31:55,674 : Computed dev embeddings
2019-03-13 14:31:55,674 : Computing embedding for test
2019-03-13 14:32:01,400 : Computed test embeddings
2019-03-13 14:32:01,400 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:32:04,164 : [('reg:1e-05', 43.87), ('reg:0.0001', 43.69), ('reg:0.001', 46.5), ('reg:0.01', 43.96)]
2019-03-13 14:32:04,164 : Validation : best param found is reg = 0.001 with score             46.5
2019-03-13 14:32:04,164 : Evaluating...
2019-03-13 14:32:04,986 : 
Dev acc : 46.5 Test acc : 48.19 for             SST Fine-Grained classification

2019-03-13 14:32:04,986 : ***** Transfer task : TREC *****


2019-03-13 14:32:04,999 : loading BERT model bert-large-uncased
2019-03-13 14:32:04,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:32:05,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:32:05,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps24whmi5
2019-03-13 14:32:12,450 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:32:25,414 : Computed train embeddings
2019-03-13 14:32:26,009 : Computed test embeddings
2019-03-13 14:32:26,009 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:32:33,303 : [('reg:1e-05', 64.12), ('reg:0.0001', 63.42), ('reg:0.001', 56.05), ('reg:0.01', 41.0)]
2019-03-13 14:32:33,303 : Cross-validation : best param found is reg = 1e-05             with score 64.12
2019-03-13 14:32:33,303 : Evaluating...
2019-03-13 14:32:33,745 : 
Dev acc : 64.12 Test acc : 84.8             for TREC

2019-03-13 14:32:33,746 : ***** Transfer task : MRPC *****


2019-03-13 14:32:33,768 : loading BERT model bert-large-uncased
2019-03-13 14:32:33,768 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:32:33,788 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:32:33,788 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyfa6_519
2019-03-13 14:32:41,197 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:32:46,361 : Computing embedding for train
2019-03-13 14:33:08,867 : Computed train embeddings
2019-03-13 14:33:08,867 : Computing embedding for test
2019-03-13 14:33:18,742 : Computed test embeddings
2019-03-13 14:33:18,763 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 14:33:23,240 : [('reg:1e-05', 70.31), ('reg:0.0001', 70.9), ('reg:0.001', 69.53), ('reg:0.01', 69.48)]
2019-03-13 14:33:23,240 : Cross-validation : best param found is reg = 0.0001             with score 70.9
2019-03-13 14:33:23,240 : Evaluating...
2019-03-13 14:33:23,507 : Dev acc : 70.9 Test acc 71.65; Test F1 80.15 for MRPC.

2019-03-13 14:33:23,507 : ***** Transfer task : SICK-Entailment*****


2019-03-13 14:33:23,568 : loading BERT model bert-large-uncased
2019-03-13 14:33:23,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:33:23,587 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:33:23,588 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp1ci_5vme
2019-03-13 14:33:31,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:33:36,336 : Computing embedding for train
2019-03-13 14:33:47,758 : Computed train embeddings
2019-03-13 14:33:47,758 : Computing embedding for dev
2019-03-13 14:33:49,317 : Computed dev embeddings
2019-03-13 14:33:49,317 : Computing embedding for test
2019-03-13 14:34:01,581 : Computed test embeddings
2019-03-13 14:34:01,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 14:34:03,048 : [('reg:1e-05', 70.6), ('reg:0.0001', 71.0), ('reg:0.001', 71.0), ('reg:0.01', 67.2)]
2019-03-13 14:34:03,048 : Validation : best param found is reg = 0.0001 with score             71.0
2019-03-13 14:34:03,048 : Evaluating...
2019-03-13 14:34:03,445 : 
Dev acc : 71.0 Test acc : 66.45 for                        SICK entailment

2019-03-13 14:34:03,446 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 14:34:03,472 : loading BERT model bert-large-uncased
2019-03-13 14:34:03,473 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:34:03,529 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:34:03,529 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9j953tnw
2019-03-13 14:34:10,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:34:16,333 : Computing embedding for train
2019-03-13 14:34:27,779 : Computed train embeddings
2019-03-13 14:34:27,779 : Computing embedding for dev
2019-03-13 14:34:29,343 : Computed dev embeddings
2019-03-13 14:34:29,343 : Computing embedding for test
2019-03-13 14:34:41,643 : Computed test embeddings
2019-03-13 14:35:02,249 : Dev : Pearson 0.5793180507680529
2019-03-13 14:35:02,249 : Test : Pearson 0.6074244130697228 Spearman 0.5822728991464544 MSE 0.6434869301470193                        for SICK Relatedness

2019-03-13 14:35:02,250 : 

***** Transfer task : STSBenchmark*****


2019-03-13 14:35:02,316 : loading BERT model bert-large-uncased
2019-03-13 14:35:02,316 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:35:02,335 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:35:02,335 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7omg5108
2019-03-13 14:35:09,737 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:35:15,025 : Computing embedding for train
2019-03-13 14:35:33,782 : Computed train embeddings
2019-03-13 14:35:33,782 : Computing embedding for dev
2019-03-13 14:35:39,488 : Computed dev embeddings
2019-03-13 14:35:39,488 : Computing embedding for test
2019-03-13 14:35:44,145 : Computed test embeddings
2019-03-13 14:36:02,684 : Dev : Pearson 0.5175963032316544
2019-03-13 14:36:02,685 : Test : Pearson 0.4757372105034993 Spearman 0.4723829447464167 MSE 1.9490685077711707                        for SICK Relatedness

2019-03-13 14:36:02,685 : ***** Transfer task : SNLI Entailment*****


2019-03-13 14:36:07,787 : loading BERT model bert-large-uncased
2019-03-13 14:36:07,787 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 14:36:07,872 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 14:36:07,872 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzdjc65uv
2019-03-13 14:36:15,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 14:36:21,043 : PROGRESS (encoding): 0.00%
2019-03-13 14:39:11,829 : PROGRESS (encoding): 14.56%
2019-03-13 14:42:23,848 : PROGRESS (encoding): 29.12%
2019-03-13 14:45:35,428 : PROGRESS (encoding): 43.69%
2019-03-13 14:48:59,321 : PROGRESS (encoding): 58.25%
2019-03-13 14:52:46,704 : PROGRESS (encoding): 72.81%
2019-03-13 14:56:32,583 : PROGRESS (encoding): 87.37%
2019-03-13 15:00:36,709 : PROGRESS (encoding): 0.00%
2019-03-13 15:01:07,330 : PROGRESS (encoding): 0.00%
2019-03-13 15:01:36,843 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:02:13,143 : [('reg:1e-09', 58.81)]
2019-03-13 15:02:13,143 : Validation : best param found is reg = 1e-09 with score             58.81
2019-03-13 15:02:13,143 : Evaluating...
2019-03-13 15:02:50,247 : Dev acc : 58.81 Test acc : 58.38 for SNLI

2019-03-13 15:02:50,247 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 15:02:50,456 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 15:02:51,515 : loading BERT model bert-large-uncased
2019-03-13 15:02:51,515 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:02:51,542 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:02:51,542 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvrvumd77
2019-03-13 15:02:58,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:03:04,147 : Computing embeddings for train/dev/test
2019-03-13 15:06:38,435 : Computed embeddings
2019-03-13 15:06:38,436 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:07:09,597 : [('reg:1e-05', 73.54), ('reg:0.0001', 70.02), ('reg:0.001', 60.66), ('reg:0.01', 51.92)]
2019-03-13 15:07:09,597 : Validation : best param found is reg = 1e-05 with score             73.54
2019-03-13 15:07:09,597 : Evaluating...
2019-03-13 15:07:19,310 : 
Dev acc : 73.5 Test acc : 72.7 for LENGTH classification

2019-03-13 15:07:19,311 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 15:07:19,562 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 15:07:19,608 : loading BERT model bert-large-uncased
2019-03-13 15:07:19,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:07:19,639 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:07:19,639 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt27cak0z
2019-03-13 15:07:27,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:07:32,403 : Computing embeddings for train/dev/test
2019-03-13 15:10:50,123 : Computed embeddings
2019-03-13 15:10:50,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:11:18,783 : [('reg:1e-05', 16.65), ('reg:0.0001', 2.61), ('reg:0.001', 0.68), ('reg:0.01', 0.42)]
2019-03-13 15:11:18,783 : Validation : best param found is reg = 1e-05 with score             16.65
2019-03-13 15:11:18,783 : Evaluating...
2019-03-13 15:11:27,653 : 
Dev acc : 16.6 Test acc : 17.0 for WORDCONTENT classification

2019-03-13 15:11:27,654 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 15:11:28,187 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 15:11:28,253 : loading BERT model bert-large-uncased
2019-03-13 15:11:28,253 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:11:28,277 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:11:28,277 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj8iskioq
2019-03-13 15:11:35,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:11:40,893 : Computing embeddings for train/dev/test
2019-03-13 15:14:47,923 : Computed embeddings
2019-03-13 15:14:47,923 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:15:10,946 : [('reg:1e-05', 29.48), ('reg:0.0001', 27.83), ('reg:0.001', 25.76), ('reg:0.01', 21.03)]
2019-03-13 15:15:10,946 : Validation : best param found is reg = 1e-05 with score             29.48
2019-03-13 15:15:10,946 : Evaluating...
2019-03-13 15:15:17,782 : 
Dev acc : 29.5 Test acc : 29.4 for DEPTH classification

2019-03-13 15:15:17,783 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 15:15:18,160 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 15:15:18,224 : loading BERT model bert-large-uncased
2019-03-13 15:15:18,224 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:15:18,337 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:15:18,337 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo0qfk0jf
2019-03-13 15:15:25,823 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:15:30,995 : Computing embeddings for train/dev/test
2019-03-13 15:18:24,376 : Computed embeddings
2019-03-13 15:18:24,376 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:18:55,014 : [('reg:1e-05', 70.84), ('reg:0.0001', 66.17), ('reg:0.001', 54.19), ('reg:0.01', 38.69)]
2019-03-13 15:18:55,015 : Validation : best param found is reg = 1e-05 with score             70.84
2019-03-13 15:18:55,015 : Evaluating...
2019-03-13 15:19:02,596 : 
Dev acc : 70.8 Test acc : 70.1 for TOPCONSTITUENTS classification

2019-03-13 15:19:02,597 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 15:19:02,973 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 15:19:03,039 : loading BERT model bert-large-uncased
2019-03-13 15:19:03,039 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:19:03,069 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:19:03,069 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3hog87sr
2019-03-13 15:19:10,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:19:15,764 : Computing embeddings for train/dev/test
2019-03-13 15:22:23,879 : Computed embeddings
2019-03-13 15:22:23,879 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:22:58,296 : [('reg:1e-05', 90.85), ('reg:0.0001', 91.06), ('reg:0.001', 89.44), ('reg:0.01', 86.32)]
2019-03-13 15:22:58,296 : Validation : best param found is reg = 0.0001 with score             91.06
2019-03-13 15:22:58,296 : Evaluating...
2019-03-13 15:23:07,107 : 
Dev acc : 91.1 Test acc : 90.7 for BIGRAMSHIFT classification

2019-03-13 15:23:07,108 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 15:23:07,503 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 15:23:07,568 : loading BERT model bert-large-uncased
2019-03-13 15:23:07,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:23:07,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:23:07,599 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbx_5frrc
2019-03-13 15:23:15,084 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:23:20,371 : Computing embeddings for train/dev/test
2019-03-13 15:26:21,490 : Computed embeddings
2019-03-13 15:26:21,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:26:48,513 : [('reg:1e-05', 89.91), ('reg:0.0001', 89.96), ('reg:0.001', 89.66), ('reg:0.01', 89.69)]
2019-03-13 15:26:48,513 : Validation : best param found is reg = 0.0001 with score             89.96
2019-03-13 15:26:48,513 : Evaluating...
2019-03-13 15:26:55,007 : 
Dev acc : 90.0 Test acc : 88.5 for TENSE classification

2019-03-13 15:26:55,008 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 15:26:55,413 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 15:26:55,476 : loading BERT model bert-large-uncased
2019-03-13 15:26:55,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:26:55,592 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:26:55,592 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfekdh5y8
2019-03-13 15:27:03,035 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:27:08,243 : Computing embeddings for train/dev/test
2019-03-13 15:30:21,511 : Computed embeddings
2019-03-13 15:30:21,511 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:30:48,287 : [('reg:1e-05', 85.27), ('reg:0.0001', 83.6), ('reg:0.001', 80.25), ('reg:0.01', 75.83)]
2019-03-13 15:30:48,288 : Validation : best param found is reg = 1e-05 with score             85.27
2019-03-13 15:30:48,288 : Evaluating...
2019-03-13 15:30:56,749 : 
Dev acc : 85.3 Test acc : 84.9 for SUBJNUMBER classification

2019-03-13 15:30:56,750 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 15:30:57,152 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 15:30:57,219 : loading BERT model bert-large-uncased
2019-03-13 15:30:57,219 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:30:57,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:30:57,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4r3k08wt
2019-03-13 15:31:04,826 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:31:10,170 : Computing embeddings for train/dev/test
2019-03-13 15:34:20,044 : Computed embeddings
2019-03-13 15:34:20,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:34:45,722 : [('reg:1e-05', 78.44), ('reg:0.0001', 78.66), ('reg:0.001', 75.92), ('reg:0.01', 70.0)]
2019-03-13 15:34:45,722 : Validation : best param found is reg = 0.0001 with score             78.66
2019-03-13 15:34:45,722 : Evaluating...
2019-03-13 15:34:53,015 : 
Dev acc : 78.7 Test acc : 78.6 for OBJNUMBER classification

2019-03-13 15:34:53,016 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 15:34:53,587 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 15:34:53,657 : loading BERT model bert-large-uncased
2019-03-13 15:34:53,657 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:34:53,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:34:53,687 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpmojoq55_
2019-03-13 15:35:01,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:35:06,443 : Computing embeddings for train/dev/test
2019-03-13 15:38:48,708 : Computed embeddings
2019-03-13 15:38:48,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:39:16,597 : [('reg:1e-05', 67.74), ('reg:0.0001', 67.62), ('reg:0.001', 66.61), ('reg:0.01', 65.59)]
2019-03-13 15:39:16,597 : Validation : best param found is reg = 1e-05 with score             67.74
2019-03-13 15:39:16,597 : Evaluating...
2019-03-13 15:39:24,073 : 
Dev acc : 67.7 Test acc : 67.6 for ODDMANOUT classification

2019-03-13 15:39:24,074 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 15:39:24,449 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 15:39:24,529 : loading BERT model bert-large-uncased
2019-03-13 15:39:24,530 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:39:24,559 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:39:24,559 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvl5mdw9s
2019-03-13 15:39:32,035 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:39:37,263 : Computing embeddings for train/dev/test
2019-03-13 15:43:18,213 : Computed embeddings
2019-03-13 15:43:18,213 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:43:41,780 : [('reg:1e-05', 71.99), ('reg:0.0001', 70.24), ('reg:0.001', 71.91), ('reg:0.01', 58.13)]
2019-03-13 15:43:41,781 : Validation : best param found is reg = 1e-05 with score             71.99
2019-03-13 15:43:41,781 : Evaluating...
2019-03-13 15:43:47,239 : 
Dev acc : 72.0 Test acc : 71.0 for COORDINATIONINVERSION classification

2019-03-13 15:43:47,241 : total results: {'STS12': {'MSRpar': {'pearson': (0.18842092522699588, 2.012906993980562e-07), 'spearman': SpearmanrResult(correlation=0.30454781365465894, pvalue=1.464534840618679e-17), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.019351504846077845, 0.5967163919565037), 'spearman': SpearmanrResult(correlation=0.035336351797011174, pvalue=0.3338372825547007), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.13357857538530046, 0.004145398627955593), 'spearman': SpearmanrResult(correlation=0.4192641236445495, pvalue=5.760995232217924e-21), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.15798425979623196, 1.3820980652853664e-05), 'spearman': SpearmanrResult(correlation=0.20536932988846843, pvalue=1.3828983468674874e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.47649969446266305, 5.228938008241529e-24), 'spearman': SpearmanrResult(correlation=0.5260135947282053, pvalue=9.0021257908491e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.1874263900050227, 'wmean': 0.1598218160634871}, 'spearman': {'mean': 0.2981062427425787, 'wmean': 0.2610237382736506}}}, 'STS13': {'FNWN': {'pearson': (-0.05718806044840954, 0.4344319692315328), 'spearman': SpearmanrResult(correlation=-0.07444297364461597, pvalue=0.30865625021894083), 'nsamples': 189}, 'headlines': {'pearson': (-0.009680506281184633, 0.79125938129555), 'spearman': SpearmanrResult(correlation=-0.004147870134457418, pvalue=0.9097093800161548), 'nsamples': 750}, 'OnWN': {'pearson': (0.21492604022714581, 2.7555258671535486e-07), 'spearman': SpearmanrResult(correlation=0.21466734088224623, pvalue=2.849940384566648e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.049352491165850554, 'wmean': 0.06833639028786062}, 'spearman': {'mean': 0.04535883236772428, 'wmean': 0.06883183574350978}}}, 'STS14': {'deft-forum': {'pearson': (0.02679499494938996, 0.5707627285603911), 'spearman': SpearmanrResult(correlation=0.05284223812732279, pvalue=0.2633035411848253), 'nsamples': 450}, 'deft-news': {'pearson': (0.41773348668826393, 4.2434065675257995e-14), 'spearman': SpearmanrResult(correlation=0.5325439569469214, pvalue=2.2512967869170485e-23), 'nsamples': 300}, 'headlines': {'pearson': (0.12286231054385012, 0.0007465331439280284), 'spearman': SpearmanrResult(correlation=0.13695056391865024, pvalue=0.00016854047039006377), 'nsamples': 750}, 'images': {'pearson': (-0.06017002333068586, 0.09964696783810909), 'spearman': SpearmanrResult(correlation=0.05280034770202887, pvalue=0.14857224078845757), 'nsamples': 750}, 'OnWN': {'pearson': (0.24604850434432407, 8.351501150371872e-12), 'spearman': SpearmanrResult(correlation=0.25437434082039945, pvalue=1.5343474517427302e-12), 'nsamples': 750}, 'tweet-news': {'pearson': (0.20046391763332871, 3.075663118995305e-08), 'spearman': SpearmanrResult(correlation=0.2226938456753207, pvalue=6.99756851905504e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.15895553180474517, 'wmean': 0.13847502016715135}, 'spearman': {'mean': 0.2087008821984406, 'wmean': 0.1823084047543123}}}, 'STS15': {'answers-forums': {'pearson': (0.26105619742165653, 2.9339694349426e-07), 'spearman': SpearmanrResult(correlation=0.38375543963384917, pvalue=1.3237131066850087e-14), 'nsamples': 375}, 'answers-students': {'pearson': (0.14991452813119516, 3.7552069145615505e-05), 'spearman': SpearmanrResult(correlation=0.21767569929703515, pvalue=1.704462420543993e-09), 'nsamples': 750}, 'belief': {'pearson': (0.0864126004496981, 0.09473884741456585), 'spearman': SpearmanrResult(correlation=0.324552458242603, pvalue=1.2020169066000722e-10), 'nsamples': 375}, 'headlines': {'pearson': (0.1906901373854904, 1.4256560626622019e-07), 'spearman': SpearmanrResult(correlation=0.2179396076813909, pvalue=1.6273635607308155e-09), 'nsamples': 750}, 'images': {'pearson': (-0.03487030516454076, 0.34025585330748054), 'spearman': SpearmanrResult(correlation=0.14015764948404205, pvalue=0.00011763786849119224), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.13064063164469988, 'wmean': 0.11986718982195553}, 'spearman': {'mean': 0.256816170867784, 'wmean': 0.23248172635017353}}}, 'STS16': {'answer-answer': {'pearson': (0.22088731559859162, 0.00038977495391181717), 'spearman': SpearmanrResult(correlation=0.35796669329428366, pvalue=4.288454456515928e-09), 'nsamples': 254}, 'headlines': {'pearson': (0.120526516328096, 0.05753347842649989), 'spearman': SpearmanrResult(correlation=0.18048585778133205, pvalue=0.0042741037784987355), 'nsamples': 249}, 'plagiarism': {'pearson': (0.16698417484435119, 0.011197420018786139), 'spearman': SpearmanrResult(correlation=0.5759438778780742, pvalue=1.0099431623831298e-21), 'nsamples': 230}, 'postediting': {'pearson': (0.2740062150462718, 1.4150463854491852e-05), 'spearman': SpearmanrResult(correlation=0.6761295566743631, pvalue=5.937615351811455e-34), 'nsamples': 244}, 'question-question': {'pearson': (-0.03835133656342091, 0.581416700524432), 'spearman': SpearmanrResult(correlation=0.052699130299714056, pvalue=0.4485586936472066), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.14881057705077794, 'wmean': 0.15460786515301372}, 'spearman': {'mean': 0.3686450231855534, 'wmean': 0.3746385671647906}}}, 'MR': {'devacc': 82.08, 'acc': 81.78, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.8, 'acc': 87.26, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.17, 'acc': 84.39, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.68, 'acc': 95.17, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 87.04, 'acc': 87.64, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 46.5, 'acc': 48.19, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 64.12, 'acc': 84.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.9, 'acc': 71.65, 'f1': 80.15, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.0, 'acc': 66.45, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.5793180507680529, 'pearson': 0.6074244130697228, 'spearman': 0.5822728991464544, 'mse': 0.6434869301470193, 'yhat': array([3.19187734, 3.72683372, 3.36142504, ..., 3.21192131, 4.19945317,        4.71945881]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5175963032316544, 'pearson': 0.4757372105034993, 'spearman': 0.4723829447464167, 'mse': 1.9490685077711707, 'yhat': array([3.15662085, 2.33458297, 2.63951158, ..., 3.86390565, 3.59189444,        3.77421914]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.81, 'acc': 58.38, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 73.54, 'acc': 72.74, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 16.65, 'acc': 16.96, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 29.48, 'acc': 29.39, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.84, 'acc': 70.06, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 91.06, 'acc': 90.7, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.96, 'acc': 88.55, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.27, 'acc': 84.91, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.66, 'acc': 78.6, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.74, 'acc': 67.56, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 71.99, 'acc': 70.97, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 15:43:47,241 : STS12 p=0.1598, STS12 s=0.2610, STS13 p=0.0683, STS13 s=0.0688, STS14 p=0.1385, STS14 s=0.1823, STS15 p=0.1199, STS15 s=0.2325, STS 16 p=0.1546, STS16 s=0.3746, STS B p=0.4757, STS B s=0.4724, STS B m=1.9491, SICK-R p=0.6074, SICK-R s=0.5823, SICK-P m=0.6435
2019-03-13 15:43:47,241 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 15:43:47,241 : 0.1598,0.2610,0.0683,0.0688,0.1385,0.1823,0.1199,0.2325,0.1546,0.3746,0.4757,0.4724,1.9491,0.6074,0.5823,0.6435
2019-03-13 15:43:47,241 : MR=81.78, CR=87.26, SUBJ=95.17, MPQA=84.39, SST-B=87.64, SST-F=48.19, TREC=84.80, SICK-E=66.45, SNLI=58.38, MRPC=71.65, MRPC f=80.15
2019-03-13 15:43:47,241 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 15:43:47,241 : 81.78,87.26,95.17,84.39,87.64,48.19,84.80,66.45,58.38,71.65,80.15
2019-03-13 15:43:47,241 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 15:43:47,241 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 15:43:47,241 : na,na,na,na,na,na,na,na,na,na
2019-03-13 15:43:47,241 : SentLen=72.74, WC=16.96, TreeDepth=29.39, TopConst=70.06, BShift=90.70, Tense=88.55, SubjNum=84.91, ObjNum=78.60, SOMO=67.56, CoordInv=70.97, average=67.04
2019-03-13 15:43:47,241 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 15:43:47,241 : 72.74,16.96,29.39,70.06,90.70,88.55,84.91,78.60,67.56,70.97,67.04
2019-03-13 15:43:47,241 : ********************************************************************************
2019-03-13 15:43:47,241 : ********************************************************************************
2019-03-13 15:43:47,241 : ********************************************************************************
2019-03-13 15:43:47,241 : layer 19
2019-03-13 15:43:47,241 : ********************************************************************************
2019-03-13 15:43:47,242 : ********************************************************************************
2019-03-13 15:43:47,242 : ********************************************************************************
2019-03-13 15:43:47,333 : ***** Transfer task : STS12 *****


2019-03-13 15:43:47,345 : loading BERT model bert-large-uncased
2019-03-13 15:43:47,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:43:47,362 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:43:47,362 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2ch0rtvg
2019-03-13 15:43:54,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:44:04,136 : MSRpar : pearson = 0.2077, spearman = 0.3208
2019-03-13 15:44:05,791 : MSRvid : pearson = -0.0087, spearman = 0.1551
2019-03-13 15:44:07,217 : SMTeuroparl : pearson = 0.0639, spearman = 0.4374
2019-03-13 15:44:09,944 : surprise.OnWN : pearson = 0.1180, spearman = 0.1346
2019-03-13 15:44:11,390 : surprise.SMTnews : pearson = 0.4010, spearman = 0.4799
2019-03-13 15:44:11,390 : ALL (weighted average) : Pearson = 0.1374,             Spearman = 0.2735
2019-03-13 15:44:11,390 : ALL (average) : Pearson = 0.1564,             Spearman = 0.3056

2019-03-13 15:44:11,390 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 15:44:11,398 : loading BERT model bert-large-uncased
2019-03-13 15:44:11,399 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:44:11,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:44:11,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3dxp2it2
2019-03-13 15:44:18,842 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:44:25,369 : FNWN : pearson = -0.0408, spearman = -0.0786
2019-03-13 15:44:27,282 : headlines : pearson = -0.0335, spearman = -0.0302
2019-03-13 15:44:28,763 : OnWN : pearson = 0.1802, spearman = 0.1994
2019-03-13 15:44:28,764 : ALL (weighted average) : Pearson = 0.0455,             Spearman = 0.0495
2019-03-13 15:44:28,764 : ALL (average) : Pearson = 0.0353,             Spearman = 0.0302

2019-03-13 15:44:28,764 : ***** Transfer task : STS14 *****


2019-03-13 15:44:28,779 : loading BERT model bert-large-uncased
2019-03-13 15:44:28,779 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:44:28,796 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:44:28,796 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4_f6z4br
2019-03-13 15:44:36,255 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:44:42,736 : deft-forum : pearson = 0.0323, spearman = 0.0567
2019-03-13 15:44:44,390 : deft-news : pearson = 0.3908, spearman = 0.5008
2019-03-13 15:44:46,581 : headlines : pearson = 0.0820, spearman = 0.1027
2019-03-13 15:44:48,680 : images : pearson = -0.0224, spearman = 0.0678
2019-03-13 15:44:50,832 : OnWN : pearson = 0.1866, spearman = 0.2267
2019-03-13 15:44:53,729 : tweet-news : pearson = 0.1966, spearman = 0.2296
2019-03-13 15:44:53,729 : ALL (weighted average) : Pearson = 0.1237,             Spearman = 0.1722
2019-03-13 15:44:53,729 : ALL (average) : Pearson = 0.1443,             Spearman = 0.1974

2019-03-13 15:44:53,730 : ***** Transfer task : STS15 *****


2019-03-13 15:44:53,777 : loading BERT model bert-large-uncased
2019-03-13 15:44:53,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:44:53,794 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:44:53,794 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi900o771
2019-03-13 15:45:01,230 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:45:08,235 : answers-forums : pearson = 0.2555, spearman = 0.4082
2019-03-13 15:45:10,340 : answers-students : pearson = 0.1116, spearman = 0.1536
2019-03-13 15:45:12,413 : belief : pearson = 0.0932, spearman = 0.3478
2019-03-13 15:45:14,693 : headlines : pearson = 0.1600, spearman = 0.1636
2019-03-13 15:45:16,850 : images : pearson = -0.0079, spearman = 0.1998
2019-03-13 15:45:16,850 : ALL (weighted average) : Pearson = 0.1095,             Spearman = 0.2238
2019-03-13 15:45:16,850 : ALL (average) : Pearson = 0.1225,             Spearman = 0.2546

2019-03-13 15:45:16,850 : ***** Transfer task : STS16 *****


2019-03-13 15:45:16,919 : loading BERT model bert-large-uncased
2019-03-13 15:45:16,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:45:16,937 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:45:16,937 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp18enobqz
2019-03-13 15:45:24,367 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:45:30,467 : answer-answer : pearson = 0.2404, spearman = 0.4448
2019-03-13 15:45:31,134 : headlines : pearson = 0.1207, spearman = 0.1379
2019-03-13 15:45:32,026 : plagiarism : pearson = 0.1615, spearman = 0.6148
2019-03-13 15:45:33,537 : postediting : pearson = 0.2875, spearman = 0.6443
2019-03-13 15:45:34,147 : question-question : pearson = 0.0207, spearman = 0.1396
2019-03-13 15:45:34,147 : ALL (weighted average) : Pearson = 0.1710,             Spearman = 0.4006
2019-03-13 15:45:34,147 : ALL (average) : Pearson = 0.1662,             Spearman = 0.3963

2019-03-13 15:45:34,147 : ***** Transfer task : MR *****


2019-03-13 15:45:34,162 : loading BERT model bert-large-uncased
2019-03-13 15:45:34,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:45:34,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:45:34,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzx5ystp9
2019-03-13 15:45:41,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:45:46,739 : Generating sentence embeddings
2019-03-13 15:46:18,589 : Generated sentence embeddings
2019-03-13 15:46:18,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:46:27,186 : Best param found at split 1: l2reg = 0.0001                 with score 81.46
2019-03-13 15:46:37,197 : Best param found at split 2: l2reg = 1e-05                 with score 81.52
2019-03-13 15:46:47,349 : Best param found at split 3: l2reg = 0.0001                 with score 80.7
2019-03-13 15:46:57,880 : Best param found at split 4: l2reg = 0.0001                 with score 80.86
2019-03-13 15:47:07,728 : Best param found at split 5: l2reg = 1e-05                 with score 81.3
2019-03-13 15:47:08,254 : Dev acc : 81.17 Test acc : 80.5

2019-03-13 15:47:08,255 : ***** Transfer task : CR *****


2019-03-13 15:47:08,263 : loading BERT model bert-large-uncased
2019-03-13 15:47:08,263 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:47:08,283 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:47:08,283 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp__l02mif
2019-03-13 15:47:15,735 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:47:21,009 : Generating sentence embeddings
2019-03-13 15:47:29,398 : Generated sentence embeddings
2019-03-13 15:47:29,398 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:47:32,554 : Best param found at split 1: l2reg = 1e-05                 with score 87.58
2019-03-13 15:47:35,417 : Best param found at split 2: l2reg = 0.0001                 with score 86.75
2019-03-13 15:47:38,364 : Best param found at split 3: l2reg = 0.0001                 with score 87.28
2019-03-13 15:47:41,247 : Best param found at split 4: l2reg = 0.001                 with score 87.22
2019-03-13 15:47:44,023 : Best param found at split 5: l2reg = 0.001                 with score 87.75
2019-03-13 15:47:44,159 : Dev acc : 87.32 Test acc : 85.38

2019-03-13 15:47:44,160 : ***** Transfer task : MPQA *****


2019-03-13 15:47:44,165 : loading BERT model bert-large-uncased
2019-03-13 15:47:44,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:47:44,214 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:47:44,214 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5hu25mt1
2019-03-13 15:47:51,649 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:47:56,868 : Generating sentence embeddings
2019-03-13 15:48:04,505 : Generated sentence embeddings
2019-03-13 15:48:04,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:48:13,851 : Best param found at split 1: l2reg = 1e-05                 with score 79.73
2019-03-13 15:48:23,315 : Best param found at split 2: l2reg = 1e-05                 with score 80.26
2019-03-13 15:48:32,999 : Best param found at split 3: l2reg = 0.0001                 with score 79.21
2019-03-13 15:48:43,817 : Best param found at split 4: l2reg = 0.001                 with score 77.7
2019-03-13 15:48:54,789 : Best param found at split 5: l2reg = 1e-05                 with score 78.0
2019-03-13 15:48:55,401 : Dev acc : 78.98 Test acc : 81.26

2019-03-13 15:48:55,402 : ***** Transfer task : SUBJ *****


2019-03-13 15:48:55,417 : loading BERT model bert-large-uncased
2019-03-13 15:48:55,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:48:55,436 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:48:55,436 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp10tug968
2019-03-13 15:49:02,885 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:49:08,117 : Generating sentence embeddings
2019-03-13 15:49:39,301 : Generated sentence embeddings
2019-03-13 15:49:39,302 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 15:49:47,740 : Best param found at split 1: l2reg = 0.001                 with score 95.54
2019-03-13 15:49:57,538 : Best param found at split 2: l2reg = 0.0001                 with score 95.75
2019-03-13 15:50:08,148 : Best param found at split 3: l2reg = 0.001                 with score 95.36
2019-03-13 15:50:18,778 : Best param found at split 4: l2reg = 0.001                 with score 95.89
2019-03-13 15:50:28,987 : Best param found at split 5: l2reg = 0.001                 with score 95.56
2019-03-13 15:50:29,400 : Dev acc : 95.62 Test acc : 94.92

2019-03-13 15:50:29,401 : ***** Transfer task : SST Binary classification *****


2019-03-13 15:50:29,492 : loading BERT model bert-large-uncased
2019-03-13 15:50:29,492 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:50:29,566 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:50:29,566 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnjscotxw
2019-03-13 15:50:37,006 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:50:42,240 : Computing embedding for train
2019-03-13 15:52:24,157 : Computed train embeddings
2019-03-13 15:52:24,157 : Computing embedding for dev
2019-03-13 15:52:26,441 : Computed dev embeddings
2019-03-13 15:52:26,441 : Computing embedding for test
2019-03-13 15:52:31,229 : Computed test embeddings
2019-03-13 15:52:31,229 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:52:50,531 : [('reg:1e-05', 86.12), ('reg:0.0001', 85.89), ('reg:0.001', 85.09), ('reg:0.01', 84.63)]
2019-03-13 15:52:50,532 : Validation : best param found is reg = 1e-05 with score             86.12
2019-03-13 15:52:50,532 : Evaluating...
2019-03-13 15:52:55,465 : 
Dev acc : 86.12 Test acc : 87.53 for             SST Binary classification

2019-03-13 15:52:55,465 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 15:52:55,520 : loading BERT model bert-large-uncased
2019-03-13 15:52:55,520 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:52:55,540 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:52:55,540 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpls2bwdld
2019-03-13 15:53:02,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:53:08,189 : Computing embedding for train
2019-03-13 15:53:30,340 : Computed train embeddings
2019-03-13 15:53:30,340 : Computing embedding for dev
2019-03-13 15:53:33,240 : Computed dev embeddings
2019-03-13 15:53:33,240 : Computing embedding for test
2019-03-13 15:53:38,956 : Computed test embeddings
2019-03-13 15:53:38,956 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:53:41,795 : [('reg:1e-05', 42.69), ('reg:0.0001', 41.96), ('reg:0.001', 45.05), ('reg:0.01', 43.23)]
2019-03-13 15:53:41,795 : Validation : best param found is reg = 0.001 with score             45.05
2019-03-13 15:53:41,795 : Evaluating...
2019-03-13 15:53:42,643 : 
Dev acc : 45.05 Test acc : 45.93 for             SST Fine-Grained classification

2019-03-13 15:53:42,644 : ***** Transfer task : TREC *****


2019-03-13 15:53:42,658 : loading BERT model bert-large-uncased
2019-03-13 15:53:42,658 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:53:42,677 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:53:42,677 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp3elpq51
2019-03-13 15:53:50,143 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:54:02,891 : Computed train embeddings
2019-03-13 15:54:03,486 : Computed test embeddings
2019-03-13 15:54:03,486 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:54:10,652 : [('reg:1e-05', 57.8), ('reg:0.0001', 57.15), ('reg:0.001', 50.6), ('reg:0.01', 40.42)]
2019-03-13 15:54:10,652 : Cross-validation : best param found is reg = 1e-05             with score 57.8
2019-03-13 15:54:10,652 : Evaluating...
2019-03-13 15:54:11,048 : 
Dev acc : 57.8 Test acc : 70.6             for TREC

2019-03-13 15:54:11,049 : ***** Transfer task : MRPC *****


2019-03-13 15:54:11,069 : loading BERT model bert-large-uncased
2019-03-13 15:54:11,069 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:54:11,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:54:11,092 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxfsdnx54
2019-03-13 15:54:18,540 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:54:23,640 : Computing embedding for train
2019-03-13 15:54:46,119 : Computed train embeddings
2019-03-13 15:54:46,119 : Computing embedding for test
2019-03-13 15:54:55,977 : Computed test embeddings
2019-03-13 15:54:55,997 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 15:54:59,859 : [('reg:1e-05', 69.85), ('reg:0.0001', 70.36), ('reg:0.001', 69.97), ('reg:0.01', 69.48)]
2019-03-13 15:54:59,859 : Cross-validation : best param found is reg = 0.0001             with score 70.36
2019-03-13 15:54:59,859 : Evaluating...
2019-03-13 15:55:00,085 : Dev acc : 70.36 Test acc 69.22; Test F1 80.91 for MRPC.

2019-03-13 15:55:00,085 : ***** Transfer task : SICK-Entailment*****


2019-03-13 15:55:00,148 : loading BERT model bert-large-uncased
2019-03-13 15:55:00,148 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:55:00,167 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:55:00,167 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfuf2hme2
2019-03-13 15:55:07,598 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:55:12,843 : Computing embedding for train
2019-03-13 15:55:24,252 : Computed train embeddings
2019-03-13 15:55:24,252 : Computing embedding for dev
2019-03-13 15:55:25,811 : Computed dev embeddings
2019-03-13 15:55:25,811 : Computing embedding for test
2019-03-13 15:55:38,068 : Computed test embeddings
2019-03-13 15:55:38,104 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 15:55:39,668 : [('reg:1e-05', 62.6), ('reg:0.0001', 65.2), ('reg:0.001', 64.6), ('reg:0.01', 59.6)]
2019-03-13 15:55:39,669 : Validation : best param found is reg = 0.0001 with score             65.2
2019-03-13 15:55:39,669 : Evaluating...
2019-03-13 15:55:40,117 : 
Dev acc : 65.2 Test acc : 61.56 for                        SICK entailment

2019-03-13 15:55:40,118 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 15:55:40,144 : loading BERT model bert-large-uncased
2019-03-13 15:55:40,144 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:55:40,200 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:55:40,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpc_rugjlt
2019-03-13 15:55:47,663 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:55:52,940 : Computing embedding for train
2019-03-13 15:56:04,368 : Computed train embeddings
2019-03-13 15:56:04,368 : Computing embedding for dev
2019-03-13 15:56:05,929 : Computed dev embeddings
2019-03-13 15:56:05,929 : Computing embedding for test
2019-03-13 15:56:18,192 : Computed test embeddings
2019-03-13 15:56:33,289 : Dev : Pearson 0.43521726613308115
2019-03-13 15:56:33,289 : Test : Pearson 0.4014739444012462 Spearman 0.4343130695810307 MSE 0.8558553546415697                        for SICK Relatedness

2019-03-13 15:56:33,290 : 

***** Transfer task : STSBenchmark*****


2019-03-13 15:56:33,359 : loading BERT model bert-large-uncased
2019-03-13 15:56:33,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:56:33,379 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:56:33,379 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp02eurgqi
2019-03-13 15:56:40,805 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:56:46,004 : Computing embedding for train
2019-03-13 15:57:04,789 : Computed train embeddings
2019-03-13 15:57:04,789 : Computing embedding for dev
2019-03-13 15:57:10,498 : Computed dev embeddings
2019-03-13 15:57:10,498 : Computing embedding for test
2019-03-13 15:57:15,160 : Computed test embeddings
2019-03-13 15:57:32,174 : Dev : Pearson 0.455849689889842
2019-03-13 15:57:32,175 : Test : Pearson 0.43691736383611235 Spearman 0.4354850598993249 MSE 2.002310831975285                        for SICK Relatedness

2019-03-13 15:57:32,175 : ***** Transfer task : SNLI Entailment*****


2019-03-13 15:57:37,248 : loading BERT model bert-large-uncased
2019-03-13 15:57:37,248 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 15:57:37,368 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 15:57:37,369 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz6ylncab
2019-03-13 15:57:44,869 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 15:57:50,607 : PROGRESS (encoding): 0.00%
2019-03-13 16:00:41,277 : PROGRESS (encoding): 14.56%
2019-03-13 16:03:52,955 : PROGRESS (encoding): 29.12%
2019-03-13 16:07:03,920 : PROGRESS (encoding): 43.69%
2019-03-13 16:10:28,196 : PROGRESS (encoding): 58.25%
2019-03-13 16:14:14,972 : PROGRESS (encoding): 72.81%
2019-03-13 16:18:00,498 : PROGRESS (encoding): 87.37%
2019-03-13 16:22:04,183 : PROGRESS (encoding): 0.00%
2019-03-13 16:22:34,782 : PROGRESS (encoding): 0.00%
2019-03-13 16:23:04,349 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:23:41,157 : [('reg:1e-09', 57.77)]
2019-03-13 16:23:41,157 : Validation : best param found is reg = 1e-09 with score             57.77
2019-03-13 16:23:41,157 : Evaluating...
2019-03-13 16:24:15,517 : Dev acc : 57.77 Test acc : 57.6 for SNLI

2019-03-13 16:24:15,517 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 16:24:15,722 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 16:24:16,807 : loading BERT model bert-large-uncased
2019-03-13 16:24:16,807 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:24:16,839 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:24:16,839 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3k5hqa2u
2019-03-13 16:24:24,381 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:24:29,564 : Computing embeddings for train/dev/test
2019-03-13 16:28:04,044 : Computed embeddings
2019-03-13 16:28:04,044 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:28:33,399 : [('reg:1e-05', 70.23), ('reg:0.0001', 63.84), ('reg:0.001', 58.1), ('reg:0.01', 51.85)]
2019-03-13 16:28:33,399 : Validation : best param found is reg = 1e-05 with score             70.23
2019-03-13 16:28:33,399 : Evaluating...
2019-03-13 16:28:43,052 : 
Dev acc : 70.2 Test acc : 71.7 for LENGTH classification

2019-03-13 16:28:43,052 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 16:28:43,431 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 16:28:43,476 : loading BERT model bert-large-uncased
2019-03-13 16:28:43,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:28:43,513 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:28:43,513 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsiw27pdh
2019-03-13 16:28:50,962 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:28:56,227 : Computing embeddings for train/dev/test
2019-03-13 16:32:14,883 : Computed embeddings
2019-03-13 16:32:14,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:32:49,014 : [('reg:1e-05', 21.5), ('reg:0.0001', 5.57), ('reg:0.001', 1.32), ('reg:0.01', 0.55)]
2019-03-13 16:32:49,014 : Validation : best param found is reg = 1e-05 with score             21.5
2019-03-13 16:32:49,014 : Evaluating...
2019-03-13 16:33:01,212 : 
Dev acc : 21.5 Test acc : 21.5 for WORDCONTENT classification

2019-03-13 16:33:01,213 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 16:33:01,590 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 16:33:01,656 : loading BERT model bert-large-uncased
2019-03-13 16:33:01,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:33:01,682 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:33:01,682 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp3eosxbt7
2019-03-13 16:33:09,129 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:33:14,443 : Computing embeddings for train/dev/test
2019-03-13 16:36:22,490 : Computed embeddings
2019-03-13 16:36:22,490 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:36:49,887 : [('reg:1e-05', 28.15), ('reg:0.0001', 26.68), ('reg:0.001', 24.63), ('reg:0.01', 21.77)]
2019-03-13 16:36:49,887 : Validation : best param found is reg = 1e-05 with score             28.15
2019-03-13 16:36:49,887 : Evaluating...
2019-03-13 16:36:57,579 : 
Dev acc : 28.1 Test acc : 28.0 for DEPTH classification

2019-03-13 16:36:57,580 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 16:36:57,963 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 16:36:58,026 : loading BERT model bert-large-uncased
2019-03-13 16:36:58,026 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:36:58,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:36:58,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpoo_41qv4
2019-03-13 16:37:05,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:37:10,922 : Computing embeddings for train/dev/test
2019-03-13 16:40:05,100 : Computed embeddings
2019-03-13 16:40:05,100 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:40:36,559 : [('reg:1e-05', 63.62), ('reg:0.0001', 59.1), ('reg:0.001', 50.1), ('reg:0.01', 33.71)]
2019-03-13 16:40:36,560 : Validation : best param found is reg = 1e-05 with score             63.62
2019-03-13 16:40:36,560 : Evaluating...
2019-03-13 16:40:42,678 : 
Dev acc : 63.6 Test acc : 63.1 for TOPCONSTITUENTS classification

2019-03-13 16:40:42,679 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 16:40:43,030 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 16:40:43,098 : loading BERT model bert-large-uncased
2019-03-13 16:40:43,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:40:43,224 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:40:43,224 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpya4vbpw0
2019-03-13 16:40:50,610 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:40:55,868 : Computing embeddings for train/dev/test
2019-03-13 16:44:04,021 : Computed embeddings
2019-03-13 16:44:04,021 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:44:29,959 : [('reg:1e-05', 89.95), ('reg:0.0001', 89.89), ('reg:0.001', 88.84), ('reg:0.01', 87.05)]
2019-03-13 16:44:29,959 : Validation : best param found is reg = 1e-05 with score             89.95
2019-03-13 16:44:29,960 : Evaluating...
2019-03-13 16:44:39,117 : 
Dev acc : 90.0 Test acc : 89.7 for BIGRAMSHIFT classification

2019-03-13 16:44:39,118 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 16:44:39,682 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 16:44:39,748 : loading BERT model bert-large-uncased
2019-03-13 16:44:39,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:44:39,778 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:44:39,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfo4l1l43
2019-03-13 16:44:47,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:44:52,452 : Computing embeddings for train/dev/test
2019-03-13 16:47:54,752 : Computed embeddings
2019-03-13 16:47:54,752 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:48:20,948 : [('reg:1e-05', 89.29), ('reg:0.0001', 89.37), ('reg:0.001', 89.44), ('reg:0.01', 89.45)]
2019-03-13 16:48:20,949 : Validation : best param found is reg = 0.01 with score             89.45
2019-03-13 16:48:20,949 : Evaluating...
2019-03-13 16:48:28,427 : 
Dev acc : 89.5 Test acc : 87.7 for TENSE classification

2019-03-13 16:48:28,428 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 16:48:28,853 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 16:48:28,916 : loading BERT model bert-large-uncased
2019-03-13 16:48:28,916 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:48:28,941 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:48:28,941 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpyo2qmb_a
2019-03-13 16:48:36,406 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:48:41,601 : Computing embeddings for train/dev/test
2019-03-13 16:51:55,781 : Computed embeddings
2019-03-13 16:51:55,781 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:52:18,148 : [('reg:1e-05', 80.46), ('reg:0.0001', 80.41), ('reg:0.001', 78.21), ('reg:0.01', 70.28)]
2019-03-13 16:52:18,148 : Validation : best param found is reg = 1e-05 with score             80.46
2019-03-13 16:52:18,148 : Evaluating...
2019-03-13 16:52:24,548 : 
Dev acc : 80.5 Test acc : 80.9 for SUBJNUMBER classification

2019-03-13 16:52:24,549 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 16:52:24,956 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 16:52:25,022 : loading BERT model bert-large-uncased
2019-03-13 16:52:25,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:52:25,138 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:52:25,139 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnjzz87m9
2019-03-13 16:52:32,593 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:52:37,887 : Computing embeddings for train/dev/test
2019-03-13 16:55:49,065 : Computed embeddings
2019-03-13 16:55:49,065 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 16:56:14,737 : [('reg:1e-05', 75.9), ('reg:0.0001', 75.3), ('reg:0.001', 73.28), ('reg:0.01', 64.77)]
2019-03-13 16:56:14,738 : Validation : best param found is reg = 1e-05 with score             75.9
2019-03-13 16:56:14,738 : Evaluating...
2019-03-13 16:56:21,069 : 
Dev acc : 75.9 Test acc : 77.2 for OBJNUMBER classification

2019-03-13 16:56:21,070 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 16:56:21,661 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 16:56:21,729 : loading BERT model bert-large-uncased
2019-03-13 16:56:21,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 16:56:21,756 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 16:56:21,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2j65ptea
2019-03-13 16:56:29,254 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 16:56:34,518 : Computing embeddings for train/dev/test
2019-03-13 17:00:17,577 : Computed embeddings
2019-03-13 17:00:17,577 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:00:45,909 : [('reg:1e-05', 66.79), ('reg:0.0001', 67.22), ('reg:0.001', 66.52), ('reg:0.01', 65.06)]
2019-03-13 17:00:45,910 : Validation : best param found is reg = 0.0001 with score             67.22
2019-03-13 17:00:45,910 : Evaluating...
2019-03-13 17:00:55,273 : 
Dev acc : 67.2 Test acc : 67.6 for ODDMANOUT classification

2019-03-13 17:00:55,274 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 17:00:55,667 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 17:00:55,744 : loading BERT model bert-large-uncased
2019-03-13 17:00:55,744 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:00:55,867 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:00:55,867 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpky980fro
2019-03-13 17:01:03,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:01:08,547 : Computing embeddings for train/dev/test
2019-03-13 17:04:51,298 : Computed embeddings
2019-03-13 17:04:51,298 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:05:12,978 : [('reg:1e-05', 70.43), ('reg:0.0001', 70.39), ('reg:0.001', 69.48), ('reg:0.01', 64.81)]
2019-03-13 17:05:12,978 : Validation : best param found is reg = 1e-05 with score             70.43
2019-03-13 17:05:12,978 : Evaluating...
2019-03-13 17:05:17,472 : 
Dev acc : 70.4 Test acc : 70.7 for COORDINATIONINVERSION classification

2019-03-13 17:05:17,474 : total results: {'STS12': {'MSRpar': {'pearson': (0.2076945869810804, 9.401785667751082e-09), 'spearman': SpearmanrResult(correlation=0.32075102450378573, pvalue=2.0875934814299698e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.0086643007656053, 0.812741878151172), 'spearman': SpearmanrResult(correlation=0.1551360292770381, pvalue=1.9780869383861394e-05), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.0638693012755453, 0.1719333109478372), 'spearman': SpearmanrResult(correlation=0.43735581692673425, pvalue=7.225505534670129e-23), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.11802272129316363, 0.0012032108551106849), 'spearman': SpearmanrResult(correlation=0.13462108804104897, pvalue=0.00021778092374254496), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.40101087464195523, 7.584151407405593e-17), 'spearman': SpearmanrResult(correlation=0.47989702616515684, pvalue=2.2503181518845413e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.15638663668522784, 'wmean': 0.13742249160202527}, 'spearman': {'mean': 0.3055521969827528, 'wmean': 0.27352198834481123}}}, 'STS13': {'FNWN': {'pearson': (-0.040765074804379055, 0.5775678422000954), 'spearman': SpearmanrResult(correlation=-0.0786401186933825, pvalue=0.2820983739137557), 'nsamples': 189}, 'headlines': {'pearson': (-0.03350288185502871, 0.3595404728709448), 'spearman': SpearmanrResult(correlation=-0.030247786311005116, pvalue=0.40813655249044833), 'nsamples': 750}, 'OnWN': {'pearson': (0.1802182114335866, 1.752961709578259e-05), 'spearman': SpearmanrResult(correlation=0.1993751340919401, pvalue=1.93956413950215e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.03531675159139294, 'wmean': 0.04551377072329527}, 'spearman': {'mean': 0.030162409695850833, 'wmean': 0.04953375203951685}}}, 'STS14': {'deft-forum': {'pearson': (0.032259473298418724, 0.4948589351336026), 'spearman': SpearmanrResult(correlation=0.05668683889264732, pvalue=0.23008882796605976), 'nsamples': 450}, 'deft-news': {'pearson': (0.3908024999924534, 2.189428714526347e-12), 'spearman': SpearmanrResult(correlation=0.5007645391912854, pvalue=1.8997241346713104e-20), 'nsamples': 300}, 'headlines': {'pearson': (0.082029930670003, 0.02467058006736839), 'spearman': SpearmanrResult(correlation=0.10270692076119431, pvalue=0.0048701072142143245), 'nsamples': 750}, 'images': {'pearson': (-0.02235803655323968, 0.54096446410355), 'spearman': SpearmanrResult(correlation=0.0678070776613563, pvalue=0.06345178098531015), 'nsamples': 750}, 'OnWN': {'pearson': (0.18661224545420196, 2.6419988394643475e-07), 'spearman': SpearmanrResult(correlation=0.2267192541169725, pvalue=3.373389579109065e-10), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19660662305006016, 5.686764765731334e-08), 'spearman': SpearmanrResult(correlation=0.2296172324761933, pvalue=1.9779050150644003e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.14432545598531626, 'wmean': 0.1237134893194116}, 'spearman': {'mean': 0.19738364384994153, 'wmean': 0.1722336808055638}}}, 'STS15': {'answers-forums': {'pearson': (0.25552156158803585, 5.299758883725008e-07), 'spearman': SpearmanrResult(correlation=0.4082130441374368, pvalue=1.7170814904818404e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.11160785435871222, 0.0022061608930379396), 'spearman': SpearmanrResult(correlation=0.15357709369564937, pvalue=2.4005785360657974e-05), 'nsamples': 750}, 'belief': {'pearson': (0.0932383708096278, 0.07131664161850662), 'spearman': SpearmanrResult(correlation=0.34777835338718965, pvalue=4.216952110973615e-12), 'nsamples': 375}, 'headlines': {'pearson': (0.1600467540924681, 1.0618958085618193e-05), 'spearman': SpearmanrResult(correlation=0.16358910067998536, pvalue=6.701100932971009e-06), 'nsamples': 750}, 'images': {'pearson': (-0.007858396676774513, 0.8298800480327317), 'spearman': SpearmanrResult(correlation=0.19984841717936302, pvalue=3.395375063952265e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1225112288344139, 'wmean': 0.10954404449330941}, 'spearman': {'mean': 0.2546012018159248, 'wmean': 0.2237525775793277}}}, 'STS16': {'answer-answer': {'pearson': (0.240426047678968, 0.00010893561673093984), 'spearman': SpearmanrResult(correlation=0.44476726638050434, pvalue=9.646947468752227e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.12066923215737597, 0.05723638621879218), 'spearman': SpearmanrResult(correlation=0.13791823767286998, pvalue=0.02957242089137736), 'nsamples': 249}, 'plagiarism': {'pearson': (0.16154189380744471, 0.014179381456859501), 'spearman': SpearmanrResult(correlation=0.6147670405639896, pvalue=2.677496347798215e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.2875424398912487, 4.988460388743828e-06), 'spearman': SpearmanrResult(correlation=0.6442567136251781, pvalue=5.222976602608367e-30), 'nsamples': 244}, 'question-question': {'pearson': (0.02072530167662857, 0.7658121102115928), 'spearman': SpearmanrResult(correlation=0.139554604722139, pvalue=0.0438742557208152), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.1661809830423332, 'wmean': 0.1709624231679906}, 'spearman': {'mean': 0.3962527725929362, 'wmean': 0.4005687155837949}}}, 'MR': {'devacc': 81.17, 'acc': 80.5, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.32, 'acc': 85.38, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 78.98, 'acc': 81.26, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.62, 'acc': 94.92, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.12, 'acc': 87.53, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 45.05, 'acc': 45.93, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 57.8, 'acc': 70.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.36, 'acc': 69.22, 'f1': 80.91, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 65.2, 'acc': 61.56, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.43521726613308115, 'pearson': 0.4014739444012462, 'spearman': 0.4343130695810307, 'mse': 0.8558553546415697, 'yhat': array([3.52507191, 3.68678103, 3.6681533 , ..., 3.03175802, 4.57701488,        4.52670166]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.455849689889842, 'pearson': 0.43691736383611235, 'spearman': 0.4354850598993249, 'mse': 2.002310831975285, 'yhat': array([2.99455816, 2.88655119, 2.14268683, ..., 3.79768536, 3.35870173,        3.63268669]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.77, 'acc': 57.6, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 70.23, 'acc': 71.69, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 21.5, 'acc': 21.51, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.15, 'acc': 28.03, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 63.62, 'acc': 63.09, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 89.95, 'acc': 89.73, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.45, 'acc': 87.66, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 80.46, 'acc': 80.88, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 75.9, 'acc': 77.16, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 67.22, 'acc': 67.58, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 70.43, 'acc': 70.66, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 17:05:17,474 : STS12 p=0.1374, STS12 s=0.2735, STS13 p=0.0455, STS13 s=0.0495, STS14 p=0.1237, STS14 s=0.1722, STS15 p=0.1095, STS15 s=0.2238, STS 16 p=0.1710, STS16 s=0.4006, STS B p=0.4369, STS B s=0.4355, STS B m=2.0023, SICK-R p=0.4015, SICK-R s=0.4343, SICK-P m=0.8559
2019-03-13 17:05:17,474 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 17:05:17,474 : 0.1374,0.2735,0.0455,0.0495,0.1237,0.1722,0.1095,0.2238,0.1710,0.4006,0.4369,0.4355,2.0023,0.4015,0.4343,0.8559
2019-03-13 17:05:17,475 : MR=80.50, CR=85.38, SUBJ=94.92, MPQA=81.26, SST-B=87.53, SST-F=45.93, TREC=70.60, SICK-E=61.56, SNLI=57.60, MRPC=69.22, MRPC f=80.91
2019-03-13 17:05:17,475 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 17:05:17,475 : 80.50,85.38,94.92,81.26,87.53,45.93,70.60,61.56,57.60,69.22,80.91
2019-03-13 17:05:17,475 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 17:05:17,475 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 17:05:17,475 : na,na,na,na,na,na,na,na,na,na
2019-03-13 17:05:17,475 : SentLen=71.69, WC=21.51, TreeDepth=28.03, TopConst=63.09, BShift=89.73, Tense=87.66, SubjNum=80.88, ObjNum=77.16, SOMO=67.58, CoordInv=70.66, average=65.80
2019-03-13 17:05:17,475 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 17:05:17,475 : 71.69,21.51,28.03,63.09,89.73,87.66,80.88,77.16,67.58,70.66,65.80
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,475 : layer 20
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,475 : ********************************************************************************
2019-03-13 17:05:17,562 : ***** Transfer task : STS12 *****


2019-03-13 17:05:17,574 : loading BERT model bert-large-uncased
2019-03-13 17:05:17,574 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:05:17,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:05:17,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppd_g_7l8
2019-03-13 17:05:25,005 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:05:34,282 : MSRpar : pearson = 0.2240, spearman = 0.3232
2019-03-13 17:05:35,945 : MSRvid : pearson = -0.0136, spearman = 0.1348
2019-03-13 17:05:37,374 : SMTeuroparl : pearson = 0.1156, spearman = 0.4337
2019-03-13 17:05:40,108 : surprise.OnWN : pearson = 0.1025, spearman = 0.1682
2019-03-13 17:05:41,557 : surprise.SMTnews : pearson = 0.4115, spearman = 0.4715
2019-03-13 17:05:41,557 : ALL (weighted average) : Pearson = 0.1454,             Spearman = 0.2757
2019-03-13 17:05:41,557 : ALL (average) : Pearson = 0.1680,             Spearman = 0.3063

2019-03-13 17:05:41,557 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 17:05:41,565 : loading BERT model bert-large-uncased
2019-03-13 17:05:41,565 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:05:41,583 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:05:41,583 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppcqrqtuz
2019-03-13 17:05:49,024 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:05:55,508 : FNWN : pearson = -0.0362, spearman = -0.0911
2019-03-13 17:05:57,426 : headlines : pearson = -0.0341, spearman = 0.0526
2019-03-13 17:05:58,912 : OnWN : pearson = 0.1675, spearman = 0.1868
2019-03-13 17:05:58,913 : ALL (weighted average) : Pearson = 0.0410,             Spearman = 0.0847
2019-03-13 17:05:58,913 : ALL (average) : Pearson = 0.0324,             Spearman = 0.0494

2019-03-13 17:05:58,913 : ***** Transfer task : STS14 *****


2019-03-13 17:05:58,929 : loading BERT model bert-large-uncased
2019-03-13 17:05:58,929 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:05:58,947 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:05:58,947 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9f2aq0fv
2019-03-13 17:06:06,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:06:13,141 : deft-forum : pearson = 0.0214, spearman = 0.0377
2019-03-13 17:06:14,800 : deft-news : pearson = 0.4714, spearman = 0.5236
2019-03-13 17:06:17,000 : headlines : pearson = 0.0759, spearman = 0.1450
2019-03-13 17:06:19,103 : images : pearson = 0.0078, spearman = 0.0788
2019-03-13 17:06:21,262 : OnWN : pearson = 0.1719, spearman = 0.2333
2019-03-13 17:06:24,162 : tweet-news : pearson = 0.2099, spearman = 0.2547
2019-03-13 17:06:24,162 : ALL (weighted average) : Pearson = 0.1334,             Spearman = 0.1888
2019-03-13 17:06:24,163 : ALL (average) : Pearson = 0.1597,             Spearman = 0.2122

2019-03-13 17:06:24,163 : ***** Transfer task : STS15 *****


2019-03-13 17:06:24,212 : loading BERT model bert-large-uncased
2019-03-13 17:06:24,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:06:24,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:06:24,230 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm9y9pggd
2019-03-13 17:06:31,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:06:38,879 : answers-forums : pearson = 0.3011, spearman = 0.3649
2019-03-13 17:06:40,990 : answers-students : pearson = 0.1230, spearman = 0.1501
2019-03-13 17:06:43,070 : belief : pearson = 0.1586, spearman = 0.3300
2019-03-13 17:06:45,350 : headlines : pearson = 0.1568, spearman = 0.2214
2019-03-13 17:06:47,510 : images : pearson = 0.0284, spearman = 0.2027
2019-03-13 17:06:47,510 : ALL (weighted average) : Pearson = 0.1345,             Spearman = 0.2304
2019-03-13 17:06:47,510 : ALL (average) : Pearson = 0.1536,             Spearman = 0.2538

2019-03-13 17:06:47,510 : ***** Transfer task : STS16 *****


2019-03-13 17:06:47,579 : loading BERT model bert-large-uncased
2019-03-13 17:06:47,579 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:06:47,597 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:06:47,597 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdgw6zjd1
2019-03-13 17:06:55,036 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:07:01,192 : answer-answer : pearson = 0.2942, spearman = 0.4568
2019-03-13 17:07:01,860 : headlines : pearson = 0.1348, spearman = 0.2158
2019-03-13 17:07:02,752 : plagiarism : pearson = 0.2191, spearman = 0.6131
2019-03-13 17:07:04,266 : postediting : pearson = 0.3915, spearman = 0.6243
2019-03-13 17:07:04,878 : question-question : pearson = 0.0438, spearman = 0.1683
2019-03-13 17:07:04,878 : ALL (weighted average) : Pearson = 0.2221,             Spearman = 0.4201
2019-03-13 17:07:04,878 : ALL (average) : Pearson = 0.2167,             Spearman = 0.4156

2019-03-13 17:07:04,879 : ***** Transfer task : MR *****


2019-03-13 17:07:04,898 : loading BERT model bert-large-uncased
2019-03-13 17:07:04,898 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:07:04,916 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:07:04,917 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp34h7_dru
2019-03-13 17:07:12,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:07:17,548 : Generating sentence embeddings
2019-03-13 17:07:49,523 : Generated sentence embeddings
2019-03-13 17:07:49,523 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:07:57,409 : Best param found at split 1: l2reg = 1e-05                 with score 82.08
2019-03-13 17:08:08,984 : Best param found at split 2: l2reg = 0.001                 with score 82.22
2019-03-13 17:08:19,286 : Best param found at split 3: l2reg = 1e-05                 with score 81.79
2019-03-13 17:08:29,774 : Best param found at split 4: l2reg = 0.001                 with score 81.22
2019-03-13 17:08:41,111 : Best param found at split 5: l2reg = 0.001                 with score 82.37
2019-03-13 17:08:41,637 : Dev acc : 81.94 Test acc : 80.32

2019-03-13 17:08:41,638 : ***** Transfer task : CR *****


2019-03-13 17:08:41,646 : loading BERT model bert-large-uncased
2019-03-13 17:08:41,646 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:08:41,665 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:08:41,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpfy935xvg
2019-03-13 17:08:49,122 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:08:54,343 : Generating sentence embeddings
2019-03-13 17:09:02,734 : Generated sentence embeddings
2019-03-13 17:09:02,734 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:09:06,387 : Best param found at split 1: l2reg = 0.001                 with score 88.24
2019-03-13 17:09:10,030 : Best param found at split 2: l2reg = 0.001                 with score 86.62
2019-03-13 17:09:13,426 : Best param found at split 3: l2reg = 0.001                 with score 87.15
2019-03-13 17:09:17,187 : Best param found at split 4: l2reg = 0.01                 with score 87.42
2019-03-13 17:09:20,896 : Best param found at split 5: l2reg = 0.001                 with score 87.19
2019-03-13 17:09:21,085 : Dev acc : 87.32 Test acc : 86.17

2019-03-13 17:09:21,086 : ***** Transfer task : MPQA *****


2019-03-13 17:09:21,093 : loading BERT model bert-large-uncased
2019-03-13 17:09:21,093 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:09:21,144 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:09:21,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpj7oszbx5
2019-03-13 17:09:28,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:09:33,759 : Generating sentence embeddings
2019-03-13 17:09:41,397 : Generated sentence embeddings
2019-03-13 17:09:41,397 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:09:51,215 : Best param found at split 1: l2reg = 1e-05                 with score 78.68
2019-03-13 17:10:01,588 : Best param found at split 2: l2reg = 0.0001                 with score 79.44
2019-03-13 17:10:11,324 : Best param found at split 3: l2reg = 1e-05                 with score 80.42
2019-03-13 17:10:21,415 : Best param found at split 4: l2reg = 1e-05                 with score 78.87
2019-03-13 17:10:32,663 : Best param found at split 5: l2reg = 0.0001                 with score 79.24
2019-03-13 17:10:33,255 : Dev acc : 79.33 Test acc : 81.07

2019-03-13 17:10:33,256 : ***** Transfer task : SUBJ *****


2019-03-13 17:10:33,271 : loading BERT model bert-large-uncased
2019-03-13 17:10:33,271 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:10:33,292 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:10:33,292 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp80suc5lm
2019-03-13 17:10:40,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:10:45,913 : Generating sentence embeddings
2019-03-13 17:11:17,143 : Generated sentence embeddings
2019-03-13 17:11:17,144 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 17:11:26,811 : Best param found at split 1: l2reg = 1e-05                 with score 95.14
2019-03-13 17:11:36,594 : Best param found at split 2: l2reg = 0.001                 with score 95.35
2019-03-13 17:11:47,511 : Best param found at split 3: l2reg = 0.0001                 with score 95.0
2019-03-13 17:11:57,934 : Best param found at split 4: l2reg = 1e-05                 with score 95.69
2019-03-13 17:12:07,853 : Best param found at split 5: l2reg = 0.001                 with score 95.32
2019-03-13 17:12:08,379 : Dev acc : 95.3 Test acc : 94.87

2019-03-13 17:12:08,380 : ***** Transfer task : SST Binary classification *****


2019-03-13 17:12:08,472 : loading BERT model bert-large-uncased
2019-03-13 17:12:08,472 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:12:08,545 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:12:08,545 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt2kmuvu3
2019-03-13 17:12:15,963 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:12:21,185 : Computing embedding for train
2019-03-13 17:14:03,715 : Computed train embeddings
2019-03-13 17:14:03,715 : Computing embedding for dev
2019-03-13 17:14:06,004 : Computed dev embeddings
2019-03-13 17:14:06,004 : Computing embedding for test
2019-03-13 17:14:10,807 : Computed test embeddings
2019-03-13 17:14:10,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:14:27,135 : [('reg:1e-05', 86.01), ('reg:0.0001', 86.12), ('reg:0.001', 86.01), ('reg:0.01', 84.4)]
2019-03-13 17:14:27,135 : Validation : best param found is reg = 0.0001 with score             86.12
2019-03-13 17:14:27,135 : Evaluating...
2019-03-13 17:14:31,251 : 
Dev acc : 86.12 Test acc : 85.83 for             SST Binary classification

2019-03-13 17:14:31,251 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 17:14:31,302 : loading BERT model bert-large-uncased
2019-03-13 17:14:31,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:14:31,323 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:14:31,324 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp98z_un8z
2019-03-13 17:14:38,719 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:14:43,954 : Computing embedding for train
2019-03-13 17:15:06,127 : Computed train embeddings
2019-03-13 17:15:06,127 : Computing embedding for dev
2019-03-13 17:15:09,030 : Computed dev embeddings
2019-03-13 17:15:09,030 : Computing embedding for test
2019-03-13 17:15:14,754 : Computed test embeddings
2019-03-13 17:15:14,754 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:15:17,328 : [('reg:1e-05', 40.15), ('reg:0.0001', 44.6), ('reg:0.001', 42.42), ('reg:0.01', 40.51)]
2019-03-13 17:15:17,328 : Validation : best param found is reg = 0.0001 with score             44.6
2019-03-13 17:15:17,328 : Evaluating...
2019-03-13 17:15:18,148 : 
Dev acc : 44.6 Test acc : 42.04 for             SST Fine-Grained classification

2019-03-13 17:15:18,148 : ***** Transfer task : TREC *****


2019-03-13 17:15:18,162 : loading BERT model bert-large-uncased
2019-03-13 17:15:18,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:15:18,180 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:15:18,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp14aswj6e
2019-03-13 17:15:25,624 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:15:38,537 : Computed train embeddings
2019-03-13 17:15:39,134 : Computed test embeddings
2019-03-13 17:15:39,134 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 17:15:45,547 : [('reg:1e-05', 60.42), ('reg:0.0001', 59.98), ('reg:0.001', 50.03), ('reg:0.01', 42.55)]
2019-03-13 17:15:45,547 : Cross-validation : best param found is reg = 1e-05             with score 60.42
2019-03-13 17:15:45,547 : Evaluating...
2019-03-13 17:15:46,078 : 
Dev acc : 60.42 Test acc : 69.4             for TREC

2019-03-13 17:15:46,079 : ***** Transfer task : MRPC *****


2019-03-13 17:15:46,101 : loading BERT model bert-large-uncased
2019-03-13 17:15:46,101 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:15:46,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:15:46,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ldv27jq
2019-03-13 17:15:53,565 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:15:58,769 : Computing embedding for train
2019-03-13 17:16:21,295 : Computed train embeddings
2019-03-13 17:16:21,295 : Computing embedding for test
2019-03-13 17:16:31,189 : Computed test embeddings
2019-03-13 17:16:31,209 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 17:16:34,748 : [('reg:1e-05', 70.76), ('reg:0.0001', 70.22), ('reg:0.001', 71.05), ('reg:0.01', 69.99)]
2019-03-13 17:16:34,748 : Cross-validation : best param found is reg = 0.001             with score 71.05
2019-03-13 17:16:34,748 : Evaluating...
2019-03-13 17:16:34,945 : Dev acc : 71.05 Test acc 70.55; Test F1 81.27 for MRPC.

2019-03-13 17:16:34,946 : ***** Transfer task : SICK-Entailment*****


2019-03-13 17:16:35,007 : loading BERT model bert-large-uncased
2019-03-13 17:16:35,007 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:16:35,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:16:35,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp96t8c3a3
2019-03-13 17:16:42,437 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:16:47,653 : Computing embedding for train
2019-03-13 17:16:59,080 : Computed train embeddings
2019-03-13 17:16:59,081 : Computing embedding for dev
2019-03-13 17:17:00,645 : Computed dev embeddings
2019-03-13 17:17:00,646 : Computing embedding for test
2019-03-13 17:17:12,942 : Computed test embeddings
2019-03-13 17:17:12,979 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:17:14,744 : [('reg:1e-05', 63.4), ('reg:0.0001', 60.8), ('reg:0.001', 61.8), ('reg:0.01', 60.4)]
2019-03-13 17:17:14,744 : Validation : best param found is reg = 1e-05 with score             63.4
2019-03-13 17:17:14,744 : Evaluating...
2019-03-13 17:17:15,289 : 
Dev acc : 63.4 Test acc : 60.89 for                        SICK entailment

2019-03-13 17:17:15,290 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 17:17:15,317 : loading BERT model bert-large-uncased
2019-03-13 17:17:15,317 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:17:15,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:17:15,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpprxfji74
2019-03-13 17:17:22,817 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:17:28,056 : Computing embedding for train
2019-03-13 17:17:39,509 : Computed train embeddings
2019-03-13 17:17:39,509 : Computing embedding for dev
2019-03-13 17:17:41,076 : Computed dev embeddings
2019-03-13 17:17:41,076 : Computing embedding for test
2019-03-13 17:17:53,391 : Computed test embeddings
2019-03-13 17:18:07,707 : Dev : Pearson 0.41232251117319657
2019-03-13 17:18:07,707 : Test : Pearson 0.361240662490443 Spearman 0.38776785973171307 MSE 0.8915441079595435                        for SICK Relatedness

2019-03-13 17:18:07,708 : 

***** Transfer task : STSBenchmark*****


2019-03-13 17:18:07,748 : loading BERT model bert-large-uncased
2019-03-13 17:18:07,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:18:07,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:18:07,778 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprz6khmpn
2019-03-13 17:18:15,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:18:20,441 : Computing embedding for train
2019-03-13 17:18:39,262 : Computed train embeddings
2019-03-13 17:18:39,262 : Computing embedding for dev
2019-03-13 17:18:44,995 : Computed dev embeddings
2019-03-13 17:18:44,995 : Computing embedding for test
2019-03-13 17:18:49,680 : Computed test embeddings
2019-03-13 17:19:08,524 : Dev : Pearson 0.39684061829833156
2019-03-13 17:19:08,524 : Test : Pearson 0.40600905805578513 Spearman 0.39422406076593314 MSE 1.995218357829048                        for SICK Relatedness

2019-03-13 17:19:08,524 : ***** Transfer task : SNLI Entailment*****


2019-03-13 17:19:13,562 : loading BERT model bert-large-uncased
2019-03-13 17:19:13,562 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:19:13,642 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:19:13,642 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp629spotf
2019-03-13 17:19:21,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:19:26,718 : PROGRESS (encoding): 0.00%
2019-03-13 17:22:18,545 : PROGRESS (encoding): 14.56%
2019-03-13 17:25:30,678 : PROGRESS (encoding): 29.12%
2019-03-13 17:28:42,774 : PROGRESS (encoding): 43.69%
2019-03-13 17:32:07,809 : PROGRESS (encoding): 58.25%
2019-03-13 17:35:56,247 : PROGRESS (encoding): 72.81%
2019-03-13 17:39:42,598 : PROGRESS (encoding): 87.37%
2019-03-13 17:43:46,624 : PROGRESS (encoding): 0.00%
2019-03-13 17:44:17,297 : PROGRESS (encoding): 0.00%
2019-03-13 17:44:46,885 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:45:17,041 : [('reg:1e-09', 57.86)]
2019-03-13 17:45:17,041 : Validation : best param found is reg = 1e-09 with score             57.86
2019-03-13 17:45:17,041 : Evaluating...
2019-03-13 17:45:47,196 : Dev acc : 57.86 Test acc : 57.77 for SNLI

2019-03-13 17:45:47,196 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 17:45:47,407 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 17:45:48,507 : loading BERT model bert-large-uncased
2019-03-13 17:45:48,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:45:48,535 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:45:48,535 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_kmjtman
2019-03-13 17:45:55,994 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:46:01,344 : Computing embeddings for train/dev/test
2019-03-13 17:49:37,276 : Computed embeddings
2019-03-13 17:49:37,276 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:50:06,137 : [('reg:1e-05', 62.84), ('reg:0.0001', 61.25), ('reg:0.001', 54.91), ('reg:0.01', 46.22)]
2019-03-13 17:50:06,137 : Validation : best param found is reg = 1e-05 with score             62.84
2019-03-13 17:50:06,137 : Evaluating...
2019-03-13 17:50:12,799 : 
Dev acc : 62.8 Test acc : 64.0 for LENGTH classification

2019-03-13 17:50:12,800 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 17:50:13,053 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 17:50:13,098 : loading BERT model bert-large-uncased
2019-03-13 17:50:13,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:50:13,129 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:50:13,129 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0ocamypw
2019-03-13 17:50:20,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:50:25,759 : Computing embeddings for train/dev/test
2019-03-13 17:53:45,195 : Computed embeddings
2019-03-13 17:53:45,195 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:54:16,833 : [('reg:1e-05', 28.77), ('reg:0.0001', 8.88), ('reg:0.001', 1.32), ('reg:0.01', 0.59)]
2019-03-13 17:54:16,833 : Validation : best param found is reg = 1e-05 with score             28.77
2019-03-13 17:54:16,833 : Evaluating...
2019-03-13 17:54:27,668 : 
Dev acc : 28.8 Test acc : 28.3 for WORDCONTENT classification

2019-03-13 17:54:27,669 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 17:54:28,210 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 17:54:28,277 : loading BERT model bert-large-uncased
2019-03-13 17:54:28,277 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:54:28,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:54:28,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz8kfexv9
2019-03-13 17:54:35,709 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:54:40,905 : Computing embeddings for train/dev/test
2019-03-13 17:57:49,615 : Computed embeddings
2019-03-13 17:57:49,615 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 17:58:13,106 : [('reg:1e-05', 27.01), ('reg:0.0001', 26.44), ('reg:0.001', 23.61), ('reg:0.01', 22.19)]
2019-03-13 17:58:13,106 : Validation : best param found is reg = 1e-05 with score             27.01
2019-03-13 17:58:13,107 : Evaluating...
2019-03-13 17:58:19,244 : 
Dev acc : 27.0 Test acc : 26.6 for DEPTH classification

2019-03-13 17:58:19,245 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 17:58:19,631 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 17:58:19,697 : loading BERT model bert-large-uncased
2019-03-13 17:58:19,697 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 17:58:19,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 17:58:19,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnjb_2lzr
2019-03-13 17:58:27,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 17:58:32,618 : Computing embeddings for train/dev/test
2019-03-13 18:01:27,056 : Computed embeddings
2019-03-13 18:01:27,056 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:01:55,050 : [('reg:1e-05', 60.78), ('reg:0.0001', 58.52), ('reg:0.001', 52.27), ('reg:0.01', 35.21)]
2019-03-13 18:01:55,050 : Validation : best param found is reg = 1e-05 with score             60.78
2019-03-13 18:01:55,051 : Evaluating...
2019-03-13 18:02:02,767 : 
Dev acc : 60.8 Test acc : 59.9 for TOPCONSTITUENTS classification

2019-03-13 18:02:02,768 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 18:02:03,164 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 18:02:03,233 : loading BERT model bert-large-uncased
2019-03-13 18:02:03,233 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:02:03,265 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:02:03,265 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpv0zni9fn
2019-03-13 18:02:10,668 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:02:15,891 : Computing embeddings for train/dev/test
2019-03-13 18:05:23,555 : Computed embeddings
2019-03-13 18:05:23,555 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:05:51,775 : [('reg:1e-05', 88.91), ('reg:0.0001', 88.77), ('reg:0.001', 88.48), ('reg:0.01', 87.18)]
2019-03-13 18:05:51,775 : Validation : best param found is reg = 1e-05 with score             88.91
2019-03-13 18:05:51,775 : Evaluating...
2019-03-13 18:05:59,133 : 
Dev acc : 88.9 Test acc : 88.6 for BIGRAMSHIFT classification

2019-03-13 18:05:59,134 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 18:05:59,527 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 18:05:59,593 : loading BERT model bert-large-uncased
2019-03-13 18:05:59,593 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:05:59,623 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:05:59,624 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_6iih9gv
2019-03-13 18:06:07,060 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:06:12,262 : Computing embeddings for train/dev/test
2019-03-13 18:09:14,326 : Computed embeddings
2019-03-13 18:09:14,327 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:09:40,352 : [('reg:1e-05', 88.55), ('reg:0.0001', 88.4), ('reg:0.001', 88.28), ('reg:0.01', 89.69)]
2019-03-13 18:09:40,352 : Validation : best param found is reg = 0.01 with score             89.69
2019-03-13 18:09:40,352 : Evaluating...
2019-03-13 18:09:46,877 : 
Dev acc : 89.7 Test acc : 87.8 for TENSE classification

2019-03-13 18:09:46,878 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 18:09:47,288 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 18:09:47,352 : loading BERT model bert-large-uncased
2019-03-13 18:09:47,352 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:09:47,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:09:47,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4onrambw
2019-03-13 18:09:54,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:10:00,292 : Computing embeddings for train/dev/test
2019-03-13 18:13:14,345 : Computed embeddings
2019-03-13 18:13:14,345 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:13:43,790 : [('reg:1e-05', 79.55), ('reg:0.0001', 79.27), ('reg:0.001', 76.94), ('reg:0.01', 74.34)]
2019-03-13 18:13:43,791 : Validation : best param found is reg = 1e-05 with score             79.55
2019-03-13 18:13:43,791 : Evaluating...
2019-03-13 18:13:51,407 : 
Dev acc : 79.5 Test acc : 79.8 for SUBJNUMBER classification

2019-03-13 18:13:51,408 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 18:13:51,803 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 18:13:51,870 : loading BERT model bert-large-uncased
2019-03-13 18:13:51,870 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:13:51,982 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:13:51,982 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkxnqqxn_
2019-03-13 18:13:59,438 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:14:04,837 : Computing embeddings for train/dev/test
2019-03-13 18:17:16,130 : Computed embeddings
2019-03-13 18:17:16,130 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:17:48,273 : [('reg:1e-05', 71.41), ('reg:0.0001', 72.57), ('reg:0.001', 70.7), ('reg:0.01', 68.89)]
2019-03-13 18:17:48,273 : Validation : best param found is reg = 0.0001 with score             72.57
2019-03-13 18:17:48,273 : Evaluating...
2019-03-13 18:17:56,814 : 
Dev acc : 72.6 Test acc : 73.6 for OBJNUMBER classification

2019-03-13 18:17:56,815 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 18:17:57,371 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 18:17:57,439 : loading BERT model bert-large-uncased
2019-03-13 18:17:57,439 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:17:57,467 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:17:57,467 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9wz4tbo3
2019-03-13 18:18:04,912 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:18:10,170 : Computing embeddings for train/dev/test
2019-03-13 18:21:53,544 : Computed embeddings
2019-03-13 18:21:53,544 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:22:26,773 : [('reg:1e-05', 66.35), ('reg:0.0001', 66.26), ('reg:0.001', 66.22), ('reg:0.01', 65.0)]
2019-03-13 18:22:26,773 : Validation : best param found is reg = 1e-05 with score             66.35
2019-03-13 18:22:26,773 : Evaluating...
2019-03-13 18:22:35,222 : 
Dev acc : 66.3 Test acc : 66.4 for ODDMANOUT classification

2019-03-13 18:22:35,223 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 18:22:35,601 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 18:22:35,682 : loading BERT model bert-large-uncased
2019-03-13 18:22:35,683 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:22:35,712 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:22:35,712 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvxxhp4ua
2019-03-13 18:22:43,131 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:22:48,356 : Computing embeddings for train/dev/test
2019-03-13 18:26:30,752 : Computed embeddings
2019-03-13 18:26:30,752 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:26:57,409 : [('reg:1e-05', 71.96), ('reg:0.0001', 72.01), ('reg:0.001', 71.78), ('reg:0.01', 66.18)]
2019-03-13 18:26:57,410 : Validation : best param found is reg = 0.0001 with score             72.01
2019-03-13 18:26:57,410 : Evaluating...
2019-03-13 18:27:04,918 : 
Dev acc : 72.0 Test acc : 71.8 for COORDINATIONINVERSION classification

2019-03-13 18:27:04,920 : total results: {'STS12': {'MSRpar': {'pearson': (0.22398978342444648, 5.540982365730094e-10), 'spearman': SpearmanrResult(correlation=0.32315408439270316, pvalue=1.086651451162536e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.013617646508787733, 0.7096472856107043), 'spearman': SpearmanrResult(correlation=0.13480978716273082, pvalue=0.00021333819719610426), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.11557472263854603, 0.013224636163683995), 'spearman': SpearmanrResult(correlation=0.4337019974291832, pvalue=1.787335298171416e-22), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.10254725777594886, 0.004937212091515437), 'spearman': SpearmanrResult(correlation=0.1682181020297666, pvalue=3.617888889437777e-06), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4115278927482908, 9.696922573000467e-18), 'spearman': SpearmanrResult(correlation=0.47148156381163264, pvalue=1.785866889041377e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.1680044020156889, 'wmean': 0.14541118819702906}, 'spearman': {'mean': 0.30627310696520327, 'wmean': 0.2756843117663246}}}, 'STS13': {'FNWN': {'pearson': (-0.03616812432201198, 0.6212407793481611), 'spearman': SpearmanrResult(correlation=-0.09106673281781376, pvalue=0.2126739642481841), 'nsamples': 189}, 'headlines': {'pearson': (-0.03411242102767314, 0.35086104824454123), 'spearman': SpearmanrResult(correlation=0.05264527728630421, pvalue=0.14976997404925171), 'nsamples': 750}, 'OnWN': {'pearson': (0.16753521472016, 6.676997713351937e-05), 'spearman': SpearmanrResult(correlation=0.1867638709829528, pvalue=8.46958570286247e-06), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.03241822312349163, 'wmean': 0.041044776126929756}, 'spearman': {'mean': 0.04944747181714775, 'wmean': 0.08469791805573192}}}, 'STS14': {'deft-forum': {'pearson': (0.021436348460922048, 0.6501736461042202), 'spearman': SpearmanrResult(correlation=0.037687466268511965, pvalue=0.42514232765271354), 'nsamples': 450}, 'deft-news': {'pearson': (0.471420590403105, 5.276489986691614e-18), 'spearman': SpearmanrResult(correlation=0.5235704813992055, pvalue=1.622034727499215e-22), 'nsamples': 300}, 'headlines': {'pearson': (0.07590501516992836, 0.03768393346735101), 'spearman': SpearmanrResult(correlation=0.14495035712231483, pvalue=6.774580043554967e-05), 'nsamples': 750}, 'images': {'pearson': (0.007777703373554849, 0.8316006545587097), 'spearman': SpearmanrResult(correlation=0.07884217201811035, pvalue=0.03085467259913323), 'nsamples': 750}, 'OnWN': {'pearson': (0.17189794345501835, 2.189944145281117e-06), 'spearman': SpearmanrResult(correlation=0.2332984526106693, pvalue=9.934567081720902e-11), 'nsamples': 750}, 'tweet-news': {'pearson': (0.20986135221112703, 6.535720815752873e-09), 'spearman': SpearmanrResult(correlation=0.2546646541719463, pvalue=1.4447107123825222e-12), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1597164921789426, 'wmean': 0.13337441188948476}, 'spearman': {'mean': 0.21216893059845968, 'wmean': 0.188759261648766}}}, 'STS15': {'answers-forums': {'pearson': (0.3010830261964198, 2.6880305675648474e-09), 'spearman': SpearmanrResult(correlation=0.36491354083114214, pvalue=2.9634818072941744e-13), 'nsamples': 375}, 'answers-students': {'pearson': (0.12302167647880295, 0.000734675763941273), 'spearman': SpearmanrResult(correlation=0.15006861435686492, pvalue=3.6859461009588096e-05), 'nsamples': 750}, 'belief': {'pearson': (0.15864182395446563, 0.002060534296834166), 'spearman': SpearmanrResult(correlation=0.329990136503527, pvalue=5.625433066315801e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.1567731148966075, 1.6109525372484162e-05), 'spearman': SpearmanrResult(correlation=0.22141678349247162, pvalue=8.794750278334692e-10), 'nsamples': 750}, 'images': {'pearson': (0.028414201148459595, 0.4371505614318584), 'spearman': SpearmanrResult(correlation=0.20266963831189597, pvalue=2.152345681871909e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1535867685349511, 'wmean': 0.13451785439982816}, 'spearman': {'mean': 0.25381174269918033, 'wmean': 0.23040171870714177}}}, 'STS16': {'answer-answer': {'pearson': (0.2942410376612858, 1.8207327514666937e-06), 'spearman': SpearmanrResult(correlation=0.45676153414647774, pvalue=1.700627987613988e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.1348290892008371, 0.033454865358806804), 'spearman': SpearmanrResult(correlation=0.21575713206031394, pvalue=0.0006082205934507928), 'nsamples': 249}, 'plagiarism': {'pearson': (0.21906369350085725, 0.0008231666948102859), 'spearman': SpearmanrResult(correlation=0.6130546812837813, pvalue=3.9442625484934663e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.3914846016493467, 2.3188964507771688e-10), 'spearman': SpearmanrResult(correlation=0.6243003693586167, pvalue=9.074165286993843e-28), 'nsamples': 244}, 'question-question': {'pearson': (0.04382963042176174, 0.5286027539030789), 'spearman': SpearmanrResult(correlation=0.16831307394661732, pvalue=0.014847175063132417), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.21668961048681773, 'wmean': 0.22207162887247978}, 'spearman': {'mean': 0.4156373581591614, 'wmean': 0.42010982700660937}}}, 'MR': {'devacc': 81.94, 'acc': 80.32, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.32, 'acc': 86.17, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 79.33, 'acc': 81.07, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.3, 'acc': 94.87, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.12, 'acc': 85.83, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 44.6, 'acc': 42.04, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 60.42, 'acc': 69.4, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.05, 'acc': 70.55, 'f1': 81.27, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 63.4, 'acc': 60.89, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.41232251117319657, 'pearson': 0.361240662490443, 'spearman': 0.38776785973171307, 'mse': 0.8915441079595435, 'yhat': array([3.66149952, 3.63144397, 3.59960293, ..., 3.15132822, 4.57630957,        4.65873668]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.39684061829833156, 'pearson': 0.40600905805578513, 'spearman': 0.39422406076593314, 'mse': 1.995218357829048, 'yhat': array([2.9362008 , 2.91286889, 2.20746464, ..., 3.90869656, 3.16954754,        3.97103053]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.86, 'acc': 57.77, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 62.84, 'acc': 63.98, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.77, 'acc': 28.31, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.01, 'acc': 26.61, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 60.78, 'acc': 59.91, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.91, 'acc': 88.61, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.69, 'acc': 87.85, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.55, 'acc': 79.81, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 72.57, 'acc': 73.64, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 66.35, 'acc': 66.43, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 72.01, 'acc': 71.82, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 18:27:04,920 : STS12 p=0.1454, STS12 s=0.2757, STS13 p=0.0410, STS13 s=0.0847, STS14 p=0.1334, STS14 s=0.1888, STS15 p=0.1345, STS15 s=0.2304, STS 16 p=0.2221, STS16 s=0.4201, STS B p=0.4060, STS B s=0.3942, STS B m=1.9952, SICK-R p=0.3612, SICK-R s=0.3878, SICK-P m=0.8915
2019-03-13 18:27:04,920 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 18:27:04,920 : 0.1454,0.2757,0.0410,0.0847,0.1334,0.1888,0.1345,0.2304,0.2221,0.4201,0.4060,0.3942,1.9952,0.3612,0.3878,0.8915
2019-03-13 18:27:04,920 : MR=80.32, CR=86.17, SUBJ=94.87, MPQA=81.07, SST-B=85.83, SST-F=42.04, TREC=69.40, SICK-E=60.89, SNLI=57.77, MRPC=70.55, MRPC f=81.27
2019-03-13 18:27:04,920 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 18:27:04,920 : 80.32,86.17,94.87,81.07,85.83,42.04,69.40,60.89,57.77,70.55,81.27
2019-03-13 18:27:04,920 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 18:27:04,920 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 18:27:04,920 : na,na,na,na,na,na,na,na,na,na
2019-03-13 18:27:04,920 : SentLen=63.98, WC=28.31, TreeDepth=26.61, TopConst=59.91, BShift=88.61, Tense=87.85, SubjNum=79.81, ObjNum=73.64, SOMO=66.43, CoordInv=71.82, average=64.70
2019-03-13 18:27:04,920 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 18:27:04,920 : 63.98,28.31,26.61,59.91,88.61,87.85,79.81,73.64,66.43,71.82,64.70
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:04,920 : layer 21
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:04,920 : ********************************************************************************
2019-03-13 18:27:05,011 : ***** Transfer task : STS12 *****


2019-03-13 18:27:05,023 : loading BERT model bert-large-uncased
2019-03-13 18:27:05,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:27:05,040 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:27:05,040 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvtqz3tby
2019-03-13 18:27:12,463 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:27:21,809 : MSRpar : pearson = 0.2331, spearman = 0.3250
2019-03-13 18:27:23,463 : MSRvid : pearson = -0.0076, spearman = 0.1149
2019-03-13 18:27:24,889 : SMTeuroparl : pearson = 0.1411, spearman = 0.4492
2019-03-13 18:27:27,614 : surprise.OnWN : pearson = 0.1097, spearman = 0.1896
2019-03-13 18:27:29,057 : surprise.SMTnews : pearson = 0.4157, spearman = 0.4752
2019-03-13 18:27:29,057 : ALL (weighted average) : Pearson = 0.1551,             Spearman = 0.2793
2019-03-13 18:27:29,057 : ALL (average) : Pearson = 0.1784,             Spearman = 0.3108

2019-03-13 18:27:29,057 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 18:27:29,066 : loading BERT model bert-large-uncased
2019-03-13 18:27:29,066 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:27:29,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:27:29,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpozxwn354
2019-03-13 18:27:36,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:27:43,054 : FNWN : pearson = -0.0195, spearman = 0.0072
2019-03-13 18:27:44,963 : headlines : pearson = -0.0252, spearman = 0.0928
2019-03-13 18:27:46,446 : OnWN : pearson = 0.1622, spearman = 0.1400
2019-03-13 18:27:46,446 : ALL (weighted average) : Pearson = 0.0456,             Spearman = 0.0997
2019-03-13 18:27:46,446 : ALL (average) : Pearson = 0.0392,             Spearman = 0.0800

2019-03-13 18:27:46,446 : ***** Transfer task : STS14 *****


2019-03-13 18:27:46,461 : loading BERT model bert-large-uncased
2019-03-13 18:27:46,462 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:27:46,479 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:27:46,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbkar4wzn
2019-03-13 18:27:53,879 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:28:00,521 : deft-forum : pearson = 0.0326, spearman = 0.0500
2019-03-13 18:28:02,175 : deft-news : pearson = 0.5022, spearman = 0.5388
2019-03-13 18:28:04,365 : headlines : pearson = 0.0799, spearman = 0.1657
2019-03-13 18:28:06,460 : images : pearson = 0.0154, spearman = 0.0884
2019-03-13 18:28:08,613 : OnWN : pearson = 0.1743, spearman = 0.2249
2019-03-13 18:28:11,509 : tweet-news : pearson = 0.2076, spearman = 0.2369
2019-03-13 18:28:11,509 : ALL (weighted average) : Pearson = 0.1395,             Spearman = 0.1923
2019-03-13 18:28:11,509 : ALL (average) : Pearson = 0.1687,             Spearman = 0.2175

2019-03-13 18:28:11,510 : ***** Transfer task : STS15 *****


2019-03-13 18:28:11,557 : loading BERT model bert-large-uncased
2019-03-13 18:28:11,557 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:28:11,575 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:28:11,575 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm41yuct4
2019-03-13 18:28:18,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:28:25,992 : answers-forums : pearson = 0.3602, spearman = 0.4063
2019-03-13 18:28:28,101 : answers-students : pearson = 0.1334, spearman = 0.1456
2019-03-13 18:28:30,178 : belief : pearson = 0.1861, spearman = 0.3408
2019-03-13 18:28:32,459 : headlines : pearson = 0.1638, spearman = 0.2445
2019-03-13 18:28:34,621 : images : pearson = 0.0426, spearman = 0.2109
2019-03-13 18:28:34,621 : ALL (weighted average) : Pearson = 0.1532,             Spearman = 0.2436
2019-03-13 18:28:34,621 : ALL (average) : Pearson = 0.1772,             Spearman = 0.2696

2019-03-13 18:28:34,621 : ***** Transfer task : STS16 *****


2019-03-13 18:28:34,691 : loading BERT model bert-large-uncased
2019-03-13 18:28:34,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:28:34,709 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:28:34,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdfiuy8dl
2019-03-13 18:28:42,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:28:48,198 : answer-answer : pearson = 0.3515, spearman = 0.5009
2019-03-13 18:28:48,865 : headlines : pearson = 0.1436, spearman = 0.2510
2019-03-13 18:28:49,759 : plagiarism : pearson = 0.2637, spearman = 0.6201
2019-03-13 18:28:51,275 : postediting : pearson = 0.4334, spearman = 0.6203
2019-03-13 18:28:51,887 : question-question : pearson = 0.0571, spearman = 0.2099
2019-03-13 18:28:51,887 : ALL (weighted average) : Pearson = 0.2558,             Spearman = 0.4448
2019-03-13 18:28:51,887 : ALL (average) : Pearson = 0.2499,             Spearman = 0.4405

2019-03-13 18:28:51,887 : ***** Transfer task : MR *****


2019-03-13 18:28:51,902 : loading BERT model bert-large-uncased
2019-03-13 18:28:51,902 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:28:51,922 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:28:51,922 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptq0mdote
2019-03-13 18:28:59,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:29:04,502 : Generating sentence embeddings
2019-03-13 18:29:36,444 : Generated sentence embeddings
2019-03-13 18:29:36,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:29:44,948 : Best param found at split 1: l2reg = 0.001                 with score 81.34
2019-03-13 18:29:56,594 : Best param found at split 2: l2reg = 0.0001                 with score 81.42
2019-03-13 18:30:07,606 : Best param found at split 3: l2reg = 0.001                 with score 80.88
2019-03-13 18:30:18,219 : Best param found at split 4: l2reg = 0.0001                 with score 81.43
2019-03-13 18:30:29,829 : Best param found at split 5: l2reg = 0.0001                 with score 81.3
2019-03-13 18:30:30,627 : Dev acc : 81.27 Test acc : 80.89

2019-03-13 18:30:30,628 : ***** Transfer task : CR *****


2019-03-13 18:30:30,636 : loading BERT model bert-large-uncased
2019-03-13 18:30:30,637 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:30:30,657 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:30:30,657 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpg3x2esox
2019-03-13 18:30:38,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:30:43,259 : Generating sentence embeddings
2019-03-13 18:30:51,661 : Generated sentence embeddings
2019-03-13 18:30:51,661 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:30:55,060 : Best param found at split 1: l2reg = 0.001                 with score 86.68
2019-03-13 18:30:58,713 : Best param found at split 2: l2reg = 0.001                 with score 86.09
2019-03-13 18:31:02,189 : Best param found at split 3: l2reg = 0.001                 with score 87.22
2019-03-13 18:31:06,177 : Best param found at split 4: l2reg = 0.001                 with score 87.82
2019-03-13 18:31:10,251 : Best param found at split 5: l2reg = 0.001                 with score 87.32
2019-03-13 18:31:10,439 : Dev acc : 87.03 Test acc : 86.14

2019-03-13 18:31:10,440 : ***** Transfer task : MPQA *****


2019-03-13 18:31:10,446 : loading BERT model bert-large-uncased
2019-03-13 18:31:10,446 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:31:10,496 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:31:10,497 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpp1p7veo3
2019-03-13 18:31:17,876 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:31:23,137 : Generating sentence embeddings
2019-03-13 18:31:30,763 : Generated sentence embeddings
2019-03-13 18:31:30,764 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:31:40,169 : Best param found at split 1: l2reg = 1e-05                 with score 80.92
2019-03-13 18:31:50,991 : Best param found at split 2: l2reg = 1e-05                 with score 81.21
2019-03-13 18:32:02,146 : Best param found at split 3: l2reg = 1e-05                 with score 81.47
2019-03-13 18:32:11,328 : Best param found at split 4: l2reg = 0.0001                 with score 81.25
2019-03-13 18:32:21,109 : Best param found at split 5: l2reg = 0.0001                 with score 79.01
2019-03-13 18:32:21,948 : Dev acc : 80.77 Test acc : 81.94

2019-03-13 18:32:21,949 : ***** Transfer task : SUBJ *****


2019-03-13 18:32:21,967 : loading BERT model bert-large-uncased
2019-03-13 18:32:21,967 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:32:21,986 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:32:21,986 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf_ao2612
2019-03-13 18:32:29,448 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:32:34,603 : Generating sentence embeddings
2019-03-13 18:33:05,874 : Generated sentence embeddings
2019-03-13 18:33:05,874 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 18:33:14,392 : Best param found at split 1: l2reg = 0.0001                 with score 95.22
2019-03-13 18:33:24,407 : Best param found at split 2: l2reg = 0.0001                 with score 95.3
2019-03-13 18:33:34,312 : Best param found at split 3: l2reg = 1e-05                 with score 95.11
2019-03-13 18:33:44,037 : Best param found at split 4: l2reg = 1e-05                 with score 95.15
2019-03-13 18:33:54,409 : Best param found at split 5: l2reg = 0.001                 with score 94.88
2019-03-13 18:33:54,900 : Dev acc : 95.13 Test acc : 94.51

2019-03-13 18:33:54,901 : ***** Transfer task : SST Binary classification *****


2019-03-13 18:33:54,993 : loading BERT model bert-large-uncased
2019-03-13 18:33:54,993 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:33:55,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:33:55,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_ggovkl8
2019-03-13 18:34:02,503 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:34:07,715 : Computing embedding for train
2019-03-13 18:35:50,199 : Computed train embeddings
2019-03-13 18:35:50,199 : Computing embedding for dev
2019-03-13 18:35:52,497 : Computed dev embeddings
2019-03-13 18:35:52,497 : Computing embedding for test
2019-03-13 18:35:57,316 : Computed test embeddings
2019-03-13 18:35:57,316 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:36:13,478 : [('reg:1e-05', 85.67), ('reg:0.0001', 85.89), ('reg:0.001', 85.09), ('reg:0.01', 84.4)]
2019-03-13 18:36:13,478 : Validation : best param found is reg = 0.0001 with score             85.89
2019-03-13 18:36:13,478 : Evaluating...
2019-03-13 18:36:17,701 : 
Dev acc : 85.89 Test acc : 86.0 for             SST Binary classification

2019-03-13 18:36:17,702 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 18:36:17,757 : loading BERT model bert-large-uncased
2019-03-13 18:36:17,757 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:36:17,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:36:17,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphn3hdh39
2019-03-13 18:36:25,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:36:30,471 : Computing embedding for train
2019-03-13 18:36:52,662 : Computed train embeddings
2019-03-13 18:36:52,662 : Computing embedding for dev
2019-03-13 18:36:55,579 : Computed dev embeddings
2019-03-13 18:36:55,579 : Computing embedding for test
2019-03-13 18:37:01,304 : Computed test embeddings
2019-03-13 18:37:01,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:37:03,453 : [('reg:1e-05', 42.87), ('reg:0.0001', 42.23), ('reg:0.001', 40.42), ('reg:0.01', 38.69)]
2019-03-13 18:37:03,453 : Validation : best param found is reg = 1e-05 with score             42.87
2019-03-13 18:37:03,453 : Evaluating...
2019-03-13 18:37:04,269 : 
Dev acc : 42.87 Test acc : 44.66 for             SST Fine-Grained classification

2019-03-13 18:37:04,270 : ***** Transfer task : TREC *****


2019-03-13 18:37:04,284 : loading BERT model bert-large-uncased
2019-03-13 18:37:04,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:37:04,302 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:37:04,303 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqa5ajel7
2019-03-13 18:37:11,744 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:37:24,606 : Computed train embeddings
2019-03-13 18:37:25,202 : Computed test embeddings
2019-03-13 18:37:25,202 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:37:33,422 : [('reg:1e-05', 67.41), ('reg:0.0001', 65.83), ('reg:0.001', 60.65), ('reg:0.01', 46.31)]
2019-03-13 18:37:33,422 : Cross-validation : best param found is reg = 1e-05             with score 67.41
2019-03-13 18:37:33,423 : Evaluating...
2019-03-13 18:37:33,971 : 
Dev acc : 67.41 Test acc : 78.0             for TREC

2019-03-13 18:37:33,971 : ***** Transfer task : MRPC *****


2019-03-13 18:37:33,992 : loading BERT model bert-large-uncased
2019-03-13 18:37:33,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:37:34,015 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:37:34,015 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0eqqvhrk
2019-03-13 18:37:41,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:37:46,624 : Computing embedding for train
2019-03-13 18:38:09,145 : Computed train embeddings
2019-03-13 18:38:09,145 : Computing embedding for test
2019-03-13 18:38:19,021 : Computed test embeddings
2019-03-13 18:38:19,042 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 18:38:23,805 : [('reg:1e-05', 70.02), ('reg:0.0001', 69.43), ('reg:0.001', 69.36), ('reg:0.01', 69.9)]
2019-03-13 18:38:23,805 : Cross-validation : best param found is reg = 1e-05             with score 70.02
2019-03-13 18:38:23,805 : Evaluating...
2019-03-13 18:38:24,063 : Dev acc : 70.02 Test acc 69.8; Test F1 79.86 for MRPC.

2019-03-13 18:38:24,063 : ***** Transfer task : SICK-Entailment*****


2019-03-13 18:38:24,133 : loading BERT model bert-large-uncased
2019-03-13 18:38:24,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:38:24,151 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:38:24,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqbo42sxn
2019-03-13 18:38:31,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:38:36,743 : Computing embedding for train
2019-03-13 18:38:48,166 : Computed train embeddings
2019-03-13 18:38:48,166 : Computing embedding for dev
2019-03-13 18:38:49,727 : Computed dev embeddings
2019-03-13 18:38:49,727 : Computing embedding for test
2019-03-13 18:39:02,001 : Computed test embeddings
2019-03-13 18:39:02,038 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 18:39:03,241 : [('reg:1e-05', 60.8), ('reg:0.0001', 64.0), ('reg:0.001', 61.4), ('reg:0.01', 63.8)]
2019-03-13 18:39:03,241 : Validation : best param found is reg = 0.0001 with score             64.0
2019-03-13 18:39:03,241 : Evaluating...
2019-03-13 18:39:03,539 : 
Dev acc : 64.0 Test acc : 60.99 for                        SICK entailment

2019-03-13 18:39:03,540 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 18:39:03,566 : loading BERT model bert-large-uncased
2019-03-13 18:39:03,566 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:39:03,622 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:39:03,622 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdyro734e
2019-03-13 18:39:11,061 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:39:16,356 : Computing embedding for train
2019-03-13 18:39:27,805 : Computed train embeddings
2019-03-13 18:39:27,805 : Computing embedding for dev
2019-03-13 18:39:29,371 : Computed dev embeddings
2019-03-13 18:39:29,371 : Computing embedding for test
2019-03-13 18:39:41,723 : Computed test embeddings
2019-03-13 18:39:56,773 : Dev : Pearson 0.40700222797331626
2019-03-13 18:39:56,773 : Test : Pearson 0.35909652065512543 Spearman 0.3879064751332581 MSE 0.8910342909590496                        for SICK Relatedness

2019-03-13 18:39:56,774 : 

***** Transfer task : STSBenchmark*****


2019-03-13 18:39:56,843 : loading BERT model bert-large-uncased
2019-03-13 18:39:56,843 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:39:56,863 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:39:56,863 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdt05wett
2019-03-13 18:40:04,302 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:40:09,528 : Computing embedding for train
2019-03-13 18:40:28,392 : Computed train embeddings
2019-03-13 18:40:28,392 : Computing embedding for dev
2019-03-13 18:40:34,118 : Computed dev embeddings
2019-03-13 18:40:34,118 : Computing embedding for test
2019-03-13 18:40:38,793 : Computed test embeddings
2019-03-13 18:40:56,315 : Dev : Pearson 0.3995164009574984
2019-03-13 18:40:56,315 : Test : Pearson 0.395566714671997 Spearman 0.3853057671554483 MSE 2.0168145392728745                        for SICK Relatedness

2019-03-13 18:40:56,315 : ***** Transfer task : SNLI Entailment*****


2019-03-13 18:41:01,370 : loading BERT model bert-large-uncased
2019-03-13 18:41:01,370 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 18:41:01,491 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 18:41:01,491 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpd445s8jj
2019-03-13 18:41:08,946 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 18:41:14,759 : PROGRESS (encoding): 0.00%
2019-03-13 18:44:05,907 : PROGRESS (encoding): 14.56%
2019-03-13 18:47:19,562 : PROGRESS (encoding): 29.12%
2019-03-13 18:50:31,013 : PROGRESS (encoding): 43.69%
2019-03-13 18:53:56,087 : PROGRESS (encoding): 58.25%
2019-03-13 18:57:44,009 : PROGRESS (encoding): 72.81%
2019-03-13 19:01:29,971 : PROGRESS (encoding): 87.37%
2019-03-13 19:05:34,126 : PROGRESS (encoding): 0.00%
2019-03-13 19:06:04,902 : PROGRESS (encoding): 0.00%
2019-03-13 19:06:34,731 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:07:01,363 : [('reg:1e-09', 58.55)]
2019-03-13 19:07:01,363 : Validation : best param found is reg = 1e-09 with score             58.55
2019-03-13 19:07:01,363 : Evaluating...
2019-03-13 19:07:27,383 : Dev acc : 58.55 Test acc : 58.16 for SNLI

2019-03-13 19:07:27,384 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 19:07:27,619 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 19:07:28,711 : loading BERT model bert-large-uncased
2019-03-13 19:07:28,711 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:07:28,738 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:07:28,738 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgskz8p8c
2019-03-13 19:07:36,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:07:41,361 : Computing embeddings for train/dev/test
2019-03-13 19:11:18,026 : Computed embeddings
2019-03-13 19:11:18,027 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:11:39,965 : [('reg:1e-05', 61.86), ('reg:0.0001', 57.42), ('reg:0.001', 54.16), ('reg:0.01', 45.33)]
2019-03-13 19:11:39,965 : Validation : best param found is reg = 1e-05 with score             61.86
2019-03-13 19:11:39,965 : Evaluating...
2019-03-13 19:11:45,495 : 
Dev acc : 61.9 Test acc : 60.6 for LENGTH classification

2019-03-13 19:11:45,496 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 19:11:45,871 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 19:11:45,915 : loading BERT model bert-large-uncased
2019-03-13 19:11:45,915 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:11:45,945 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:11:45,945 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp34hp5ncf
2019-03-13 19:11:53,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:11:58,554 : Computing embeddings for train/dev/test
2019-03-13 19:15:18,838 : Computed embeddings
2019-03-13 19:15:18,838 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:15:42,719 : [('reg:1e-05', 30.26), ('reg:0.0001', 13.51), ('reg:0.001', 2.15), ('reg:0.01', 0.71)]
2019-03-13 19:15:42,719 : Validation : best param found is reg = 1e-05 with score             30.26
2019-03-13 19:15:42,719 : Evaluating...
2019-03-13 19:15:53,654 : 
Dev acc : 30.3 Test acc : 29.8 for WORDCONTENT classification

2019-03-13 19:15:53,655 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 19:15:54,032 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 19:15:54,098 : loading BERT model bert-large-uncased
2019-03-13 19:15:54,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:15:54,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:15:54,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdnqqspvt
2019-03-13 19:16:01,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:16:06,707 : Computing embeddings for train/dev/test
2019-03-13 19:19:16,940 : Computed embeddings
2019-03-13 19:19:16,940 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:19:41,648 : [('reg:1e-05', 25.0), ('reg:0.0001', 24.02), ('reg:0.001', 23.82), ('reg:0.01', 22.0)]
2019-03-13 19:19:41,648 : Validation : best param found is reg = 1e-05 with score             25.0
2019-03-13 19:19:41,648 : Evaluating...
2019-03-13 19:19:48,122 : 
Dev acc : 25.0 Test acc : 25.2 for DEPTH classification

2019-03-13 19:19:48,123 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 19:19:48,504 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 19:19:48,567 : loading BERT model bert-large-uncased
2019-03-13 19:19:48,568 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:19:48,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:19:48,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpubyp82k8
2019-03-13 19:19:56,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:20:01,427 : Computing embeddings for train/dev/test
2019-03-13 19:22:56,411 : Computed embeddings
2019-03-13 19:22:56,411 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:23:29,396 : [('reg:1e-05', 57.9), ('reg:0.0001', 55.01), ('reg:0.001', 47.16), ('reg:0.01', 36.5)]
2019-03-13 19:23:29,397 : Validation : best param found is reg = 1e-05 with score             57.9
2019-03-13 19:23:29,397 : Evaluating...
2019-03-13 19:23:36,578 : 
Dev acc : 57.9 Test acc : 57.7 for TOPCONSTITUENTS classification

2019-03-13 19:23:36,579 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 19:23:36,924 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 19:23:36,990 : loading BERT model bert-large-uncased
2019-03-13 19:23:36,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:23:37,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:23:37,111 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpl_ov88xb
2019-03-13 19:23:44,575 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:23:49,848 : Computing embeddings for train/dev/test
2019-03-13 19:26:58,484 : Computed embeddings
2019-03-13 19:26:58,485 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:27:23,034 : [('reg:1e-05', 87.23), ('reg:0.0001', 87.22), ('reg:0.001', 87.88), ('reg:0.01', 86.14)]
2019-03-13 19:27:23,034 : Validation : best param found is reg = 0.001 with score             87.88
2019-03-13 19:27:23,034 : Evaluating...
2019-03-13 19:27:27,558 : 
Dev acc : 87.9 Test acc : 87.3 for BIGRAMSHIFT classification

2019-03-13 19:27:27,559 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 19:27:28,131 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 19:27:28,198 : loading BERT model bert-large-uncased
2019-03-13 19:27:28,198 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:27:28,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:27:28,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprmn8zdx6
2019-03-13 19:27:35,724 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:27:40,989 : Computing embeddings for train/dev/test
2019-03-13 19:30:43,729 : Computed embeddings
2019-03-13 19:30:43,730 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:31:12,871 : [('reg:1e-05', 87.7), ('reg:0.0001', 88.1), ('reg:0.001', 89.19), ('reg:0.01', 88.88)]
2019-03-13 19:31:12,871 : Validation : best param found is reg = 0.001 with score             89.19
2019-03-13 19:31:12,871 : Evaluating...
2019-03-13 19:31:21,525 : 
Dev acc : 89.2 Test acc : 87.1 for TENSE classification

2019-03-13 19:31:21,526 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 19:31:21,964 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 19:31:22,030 : loading BERT model bert-large-uncased
2019-03-13 19:31:22,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:31:22,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:31:22,056 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppshpe15y
2019-03-13 19:31:29,457 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:31:34,770 : Computing embeddings for train/dev/test
2019-03-13 19:34:49,718 : Computed embeddings
2019-03-13 19:34:49,718 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:35:17,792 : [('reg:1e-05', 77.47), ('reg:0.0001', 77.51), ('reg:0.001', 75.39), ('reg:0.01', 74.65)]
2019-03-13 19:35:17,792 : Validation : best param found is reg = 0.0001 with score             77.51
2019-03-13 19:35:17,792 : Evaluating...
2019-03-13 19:35:24,195 : 
Dev acc : 77.5 Test acc : 76.7 for SUBJNUMBER classification

2019-03-13 19:35:24,196 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 19:35:24,602 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 19:35:24,669 : loading BERT model bert-large-uncased
2019-03-13 19:35:24,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:35:24,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:35:24,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpakwo8zkt
2019-03-13 19:35:32,211 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:35:37,399 : Computing embeddings for train/dev/test
2019-03-13 19:38:48,153 : Computed embeddings
2019-03-13 19:38:48,153 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:39:22,565 : [('reg:1e-05', 71.5), ('reg:0.0001', 71.67), ('reg:0.001', 71.79), ('reg:0.01', 70.47)]
2019-03-13 19:39:22,565 : Validation : best param found is reg = 0.001 with score             71.79
2019-03-13 19:39:22,565 : Evaluating...
2019-03-13 19:39:31,080 : 
Dev acc : 71.8 Test acc : 72.6 for OBJNUMBER classification

2019-03-13 19:39:31,081 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 19:39:31,663 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 19:39:31,731 : loading BERT model bert-large-uncased
2019-03-13 19:39:31,732 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:39:31,757 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:39:31,758 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnkwc0xe2
2019-03-13 19:39:39,166 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:39:44,420 : Computing embeddings for train/dev/test
2019-03-13 19:43:27,669 : Computed embeddings
2019-03-13 19:43:27,669 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:43:51,149 : [('reg:1e-05', 64.79), ('reg:0.0001', 64.77), ('reg:0.001', 64.56), ('reg:0.01', 64.39)]
2019-03-13 19:43:51,149 : Validation : best param found is reg = 1e-05 with score             64.79
2019-03-13 19:43:51,149 : Evaluating...
2019-03-13 19:43:56,158 : 
Dev acc : 64.8 Test acc : 65.1 for ODDMANOUT classification

2019-03-13 19:43:56,159 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 19:43:56,546 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 19:43:56,623 : loading BERT model bert-large-uncased
2019-03-13 19:43:56,623 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:43:56,745 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:43:56,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplrotdst7
2019-03-13 19:44:04,149 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:44:09,381 : Computing embeddings for train/dev/test
2019-03-13 19:47:52,106 : Computed embeddings
2019-03-13 19:47:52,106 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:48:21,864 : [('reg:1e-05', 69.66), ('reg:0.0001', 69.69), ('reg:0.001', 69.7), ('reg:0.01', 68.34)]
2019-03-13 19:48:21,864 : Validation : best param found is reg = 0.001 with score             69.7
2019-03-13 19:48:21,864 : Evaluating...
2019-03-13 19:48:29,405 : 
Dev acc : 69.7 Test acc : 69.6 for COORDINATIONINVERSION classification

2019-03-13 19:48:29,407 : total results: {'STS12': {'MSRpar': {'pearson': (0.23310927521564864, 1.0295356618701546e-10), 'spearman': SpearmanrResult(correlation=0.3250160571003041, pvalue=6.525615787405945e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.007551622376531604, 0.8364255986453674), 'spearman': SpearmanrResult(correlation=0.11489924354914399, pvalue=0.0016224461242935451), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.14107266669712748, 0.002451187474201573), 'spearman': SpearmanrResult(correlation=0.4492117710752018, pvalue=3.541999862612122e-24), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.10974256362001251, 0.0026167356831227873), 'spearman': SpearmanrResult(correlation=0.1895824055409865, pvalue=1.6879965377256995e-07), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.41570399601251595, 4.198466394549184e-18), 'spearman': SpearmanrResult(correlation=0.4751920082935335, pvalue=7.215585283341446e-24), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.1784153758337546, 'wmean': 0.15511371002809607}, 'spearman': {'mean': 0.31078029711183397, 'wmean': 0.2792513172057476}}}, 'STS13': {'FNWN': {'pearson': (-0.01946221489302431, 0.79038445866819), 'spearman': SpearmanrResult(correlation=0.007209360588966003, pvalue=0.9215701362170715), 'nsamples': 189}, 'headlines': {'pearson': (-0.02520534740841443, 0.49067537932668426), 'spearman': SpearmanrResult(correlation=0.09281178045716187, pvalue=0.010990315529593094), 'nsamples': 750}, 'OnWN': {'pearson': (0.16215255129258468, 0.00011448398218357372), 'spearman': SpearmanrResult(correlation=0.13995312335675644, pvalue=0.000887903797027152), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.039161662997048645, 'wmean': 0.04559014140269839}, 'spearman': {'mean': 0.07999142146762811, 'wmean': 0.09965673779821756}}}, 'STS14': {'deft-forum': {'pearson': (0.03261515736636202, 0.4901116445802596), 'spearman': SpearmanrResult(correlation=0.050003722726532474, pvalue=0.28984982943290666), 'nsamples': 450}, 'deft-news': {'pearson': (0.502163063459333, 1.4330295821271146e-20), 'spearman': SpearmanrResult(correlation=0.5387864618130273, pvalue=5.5025076744316315e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.07994362041607747, 0.028582363600792113), 'spearman': SpearmanrResult(correlation=0.16570141831207597, pvalue=5.068713909577384e-06), 'nsamples': 750}, 'images': {'pearson': (0.015431387812721914, 0.6730788867401516), 'spearman': SpearmanrResult(correlation=0.08841523524376134, pvalue=0.015432926191420093), 'nsamples': 750}, 'OnWN': {'pearson': (0.17430650979719203, 1.5675258806912175e-06), 'spearman': SpearmanrResult(correlation=0.22492435126811505, pvalue=4.678474967001339e-10), 'nsamples': 750}, 'tweet-news': {'pearson': (0.20755815984752912, 9.618259395064108e-09), 'spearman': SpearmanrResult(correlation=0.23691820571195413, pvalue=4.9901802461058954e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.16866964978320262, 'wmean': 0.13953479953541417}, 'spearman': {'mean': 0.2174582325125777, 'wmean': 0.19229520577940737}}}, 'STS15': {'answers-forums': {'pearson': (0.36024659123284797, 6.20568315535139e-13), 'spearman': SpearmanrResult(correlation=0.4062720983892518, pvalue=2.456407333687532e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.13335696933275087, 0.00024985383504249925), 'spearman': SpearmanrResult(correlation=0.14559139345094768, pvalue=6.28422588555222e-05), 'nsamples': 750}, 'belief': {'pearson': (0.18609109162784104, 0.0002908608091880787), 'spearman': SpearmanrResult(correlation=0.34081917279326385, pvalue=1.1851141346413435e-11), 'nsamples': 375}, 'headlines': {'pearson': (0.1637989632503273, 6.518801325925439e-06), 'spearman': SpearmanrResult(correlation=0.24445775686161575, pvalue=1.1463451978645317e-11), 'nsamples': 750}, 'images': {'pearson': (0.04256074402444847, 0.24436023260928164), 'spearman': SpearmanrResult(correlation=0.21092379225912367, pvalue=5.460589311402675e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.17721087189364312, 'wmean': 0.1532213795094678}, 'spearman': {'mean': 0.2696128427508405, 'wmean': 0.24362964454073624}}}, 'STS16': {'answer-answer': {'pearson': (0.35149402809431984, 8.45759079091069e-09), 'spearman': SpearmanrResult(correlation=0.5009371311094641, pvalue=1.532146694776602e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.14359053708052577, 0.02344063260498931), 'spearman': SpearmanrResult(correlation=0.2510241539602854, pvalue=6.189037093786081e-05), 'nsamples': 249}, 'plagiarism': {'pearson': (0.2636846986870844, 5.141929018493179e-05), 'spearman': SpearmanrResult(correlation=0.6200729496150971, pvalue=7.940483204513998e-26), 'nsamples': 230}, 'postediting': {'pearson': (0.433434175373708, 1.3498370426115345e-12), 'spearman': SpearmanrResult(correlation=0.6202930141116725, pvalue=2.4450364468689974e-27), 'nsamples': 244}, 'question-question': {'pearson': (0.057099599817158196, 0.4115329996480971), 'spearman': SpearmanrResult(correlation=0.20994770130631982, pvalue=0.00228040232972052), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.24986060781055924, 'wmean': 0.2557949095446951}, 'spearman': {'mean': 0.44045499002056776, 'wmean': 0.4448485573909411}}}, 'MR': {'devacc': 81.27, 'acc': 80.89, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 87.03, 'acc': 86.14, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.77, 'acc': 81.94, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.13, 'acc': 94.51, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 85.89, 'acc': 86.0, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.87, 'acc': 44.66, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.41, 'acc': 78.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 70.02, 'acc': 69.8, 'f1': 79.86, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 64.0, 'acc': 60.99, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.40700222797331626, 'pearson': 0.35909652065512543, 'spearman': 0.3879064751332581, 'mse': 0.8910342909590496, 'yhat': array([3.53525428, 3.58053161, 3.54506487, ..., 3.34039219, 4.22238017,        4.0532719 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.3995164009574984, 'pearson': 0.395566714671997, 'spearman': 0.3853057671554483, 'mse': 2.0168145392728745, 'yhat': array([2.94479138, 2.90427406, 2.78151346, ..., 3.96022141, 3.7858452 ,        3.90662235]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.55, 'acc': 58.16, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 61.86, 'acc': 60.6, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 30.26, 'acc': 29.76, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.0, 'acc': 25.21, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 57.9, 'acc': 57.68, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.88, 'acc': 87.29, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.19, 'acc': 87.11, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.51, 'acc': 76.68, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 71.79, 'acc': 72.58, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.79, 'acc': 65.09, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.7, 'acc': 69.58, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 19:48:29,408 : STS12 p=0.1551, STS12 s=0.2793, STS13 p=0.0456, STS13 s=0.0997, STS14 p=0.1395, STS14 s=0.1923, STS15 p=0.1532, STS15 s=0.2436, STS 16 p=0.2558, STS16 s=0.4448, STS B p=0.3956, STS B s=0.3853, STS B m=2.0168, SICK-R p=0.3591, SICK-R s=0.3879, SICK-P m=0.8910
2019-03-13 19:48:29,408 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 19:48:29,408 : 0.1551,0.2793,0.0456,0.0997,0.1395,0.1923,0.1532,0.2436,0.2558,0.4448,0.3956,0.3853,2.0168,0.3591,0.3879,0.8910
2019-03-13 19:48:29,408 : MR=80.89, CR=86.14, SUBJ=94.51, MPQA=81.94, SST-B=86.00, SST-F=44.66, TREC=78.00, SICK-E=60.99, SNLI=58.16, MRPC=69.80, MRPC f=79.86
2019-03-13 19:48:29,408 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 19:48:29,408 : 80.89,86.14,94.51,81.94,86.00,44.66,78.00,60.99,58.16,69.80,79.86
2019-03-13 19:48:29,408 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 19:48:29,408 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 19:48:29,408 : na,na,na,na,na,na,na,na,na,na
2019-03-13 19:48:29,408 : SentLen=60.60, WC=29.76, TreeDepth=25.21, TopConst=57.68, BShift=87.29, Tense=87.11, SubjNum=76.68, ObjNum=72.58, SOMO=65.09, CoordInv=69.58, average=63.16
2019-03-13 19:48:29,408 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 19:48:29,408 : 60.60,29.76,25.21,57.68,87.29,87.11,76.68,72.58,65.09,69.58,63.16
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,408 : layer 22
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,408 : ********************************************************************************
2019-03-13 19:48:29,495 : ***** Transfer task : STS12 *****


2019-03-13 19:48:29,508 : loading BERT model bert-large-uncased
2019-03-13 19:48:29,508 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:48:29,525 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:48:29,525 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0h9o82ie
2019-03-13 19:48:36,922 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:48:46,218 : MSRpar : pearson = 0.2291, spearman = 0.3231
2019-03-13 19:48:47,873 : MSRvid : pearson = -0.0044, spearman = 0.1378
2019-03-13 19:48:49,299 : SMTeuroparl : pearson = 0.1088, spearman = 0.4138
2019-03-13 19:48:52,021 : surprise.OnWN : pearson = 0.1174, spearman = 0.2098
2019-03-13 19:48:53,462 : surprise.SMTnews : pearson = 0.4141, spearman = 0.4926
2019-03-13 19:48:53,463 : ALL (weighted average) : Pearson = 0.1518,             Spearman = 0.2862
2019-03-13 19:48:53,463 : ALL (average) : Pearson = 0.1730,             Spearman = 0.3154

2019-03-13 19:48:53,463 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 19:48:53,471 : loading BERT model bert-large-uncased
2019-03-13 19:48:53,471 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:48:53,488 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:48:53,488 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjg9tyok7
2019-03-13 19:49:00,957 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:49:07,356 : FNWN : pearson = -0.0252, spearman = -0.0222
2019-03-13 19:49:09,265 : headlines : pearson = -0.0095, spearman = 0.1628
2019-03-13 19:49:10,749 : OnWN : pearson = 0.1581, spearman = 0.1385
2019-03-13 19:49:10,749 : ALL (weighted average) : Pearson = 0.0512,             Spearman = 0.1304
2019-03-13 19:49:10,749 : ALL (average) : Pearson = 0.0411,             Spearman = 0.0930

2019-03-13 19:49:10,749 : ***** Transfer task : STS14 *****


2019-03-13 19:49:10,766 : loading BERT model bert-large-uncased
2019-03-13 19:49:10,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:49:10,783 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:49:10,783 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdv3x6b44
2019-03-13 19:49:18,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:49:24,801 : deft-forum : pearson = 0.0289, spearman = 0.0560
2019-03-13 19:49:26,458 : deft-news : pearson = 0.4890, spearman = 0.5493
2019-03-13 19:49:28,651 : headlines : pearson = 0.0900, spearman = 0.1957
2019-03-13 19:49:30,753 : images : pearson = 0.0129, spearman = 0.1068
2019-03-13 19:49:32,906 : OnWN : pearson = 0.1687, spearman = 0.2250
2019-03-13 19:49:35,805 : tweet-news : pearson = 0.1962, spearman = 0.2222
2019-03-13 19:49:35,805 : ALL (weighted average) : Pearson = 0.1361,             Spearman = 0.2006
2019-03-13 19:49:35,805 : ALL (average) : Pearson = 0.1643,             Spearman = 0.2258

2019-03-13 19:49:35,805 : ***** Transfer task : STS15 *****


2019-03-13 19:49:35,856 : loading BERT model bert-large-uncased
2019-03-13 19:49:35,856 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:49:35,873 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:49:35,874 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz6tlb7cn
2019-03-13 19:49:43,253 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:49:50,389 : answers-forums : pearson = 0.3099, spearman = 0.3631
2019-03-13 19:49:52,495 : answers-students : pearson = 0.1324, spearman = 0.1442
2019-03-13 19:49:54,566 : belief : pearson = 0.1243, spearman = 0.3037
2019-03-13 19:49:56,843 : headlines : pearson = 0.1810, spearman = 0.2961
2019-03-13 19:49:59,003 : images : pearson = 0.0320, spearman = 0.2131
2019-03-13 19:49:59,004 : ALL (weighted average) : Pearson = 0.1406,             Spearman = 0.2467
2019-03-13 19:49:59,004 : ALL (average) : Pearson = 0.1559,             Spearman = 0.2640

2019-03-13 19:49:59,004 : ***** Transfer task : STS16 *****


2019-03-13 19:49:59,072 : loading BERT model bert-large-uncased
2019-03-13 19:49:59,072 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:49:59,090 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:49:59,090 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpa1taf_6s
2019-03-13 19:50:06,545 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:50:12,821 : answer-answer : pearson = 0.3187, spearman = 0.5006
2019-03-13 19:50:13,489 : headlines : pearson = 0.1575, spearman = 0.3190
2019-03-13 19:50:14,380 : plagiarism : pearson = 0.2397, spearman = 0.6223
2019-03-13 19:50:15,892 : postediting : pearson = 0.3760, spearman = 0.6258
2019-03-13 19:50:16,503 : question-question : pearson = 0.0596, spearman = 0.3126
2019-03-13 19:50:16,503 : ALL (weighted average) : Pearson = 0.2356,             Spearman = 0.4787
2019-03-13 19:50:16,503 : ALL (average) : Pearson = 0.2303,             Spearman = 0.4761

2019-03-13 19:50:16,503 : ***** Transfer task : MR *****


2019-03-13 19:50:16,523 : loading BERT model bert-large-uncased
2019-03-13 19:50:16,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:50:16,542 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:50:16,542 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpvlkzf3ct
2019-03-13 19:50:23,958 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:50:29,168 : Generating sentence embeddings
2019-03-13 19:51:01,084 : Generated sentence embeddings
2019-03-13 19:51:01,085 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:51:09,898 : Best param found at split 1: l2reg = 0.001                 with score 80.51
2019-03-13 19:51:20,039 : Best param found at split 2: l2reg = 0.01                 with score 81.04
2019-03-13 19:51:30,352 : Best param found at split 3: l2reg = 0.001                 with score 81.14
2019-03-13 19:51:40,666 : Best param found at split 4: l2reg = 0.001                 with score 80.54
2019-03-13 19:51:50,808 : Best param found at split 5: l2reg = 0.001                 with score 80.83
2019-03-13 19:51:51,424 : Dev acc : 80.81 Test acc : 80.32

2019-03-13 19:51:51,425 : ***** Transfer task : CR *****


2019-03-13 19:51:51,433 : loading BERT model bert-large-uncased
2019-03-13 19:51:51,434 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:51:51,453 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:51:51,453 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuaqza4f8
2019-03-13 19:51:58,863 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:52:04,187 : Generating sentence embeddings
2019-03-13 19:52:12,577 : Generated sentence embeddings
2019-03-13 19:52:12,577 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:52:16,224 : Best param found at split 1: l2reg = 1e-05                 with score 86.82
2019-03-13 19:52:19,744 : Best param found at split 2: l2reg = 1e-05                 with score 86.02
2019-03-13 19:52:24,106 : Best param found at split 3: l2reg = 0.0001                 with score 86.85
2019-03-13 19:52:27,904 : Best param found at split 4: l2reg = 0.001                 with score 86.53
2019-03-13 19:52:31,351 : Best param found at split 5: l2reg = 0.001                 with score 86.39
2019-03-13 19:52:31,605 : Dev acc : 86.52 Test acc : 85.41

2019-03-13 19:52:31,606 : ***** Transfer task : MPQA *****


2019-03-13 19:52:31,612 : loading BERT model bert-large-uncased
2019-03-13 19:52:31,612 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:52:31,664 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:52:31,665 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwiww2t9i
2019-03-13 19:52:39,117 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:52:44,361 : Generating sentence embeddings
2019-03-13 19:52:52,007 : Generated sentence embeddings
2019-03-13 19:52:52,008 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:53:02,503 : Best param found at split 1: l2reg = 1e-05                 with score 79.5
2019-03-13 19:53:14,228 : Best param found at split 2: l2reg = 1e-05                 with score 80.3
2019-03-13 19:53:25,742 : Best param found at split 3: l2reg = 1e-05                 with score 81.51
2019-03-13 19:53:36,386 : Best param found at split 4: l2reg = 1e-05                 with score 80.55
2019-03-13 19:53:47,014 : Best param found at split 5: l2reg = 1e-05                 with score 79.96
2019-03-13 19:53:47,876 : Dev acc : 80.36 Test acc : 81.74

2019-03-13 19:53:47,877 : ***** Transfer task : SUBJ *****


2019-03-13 19:53:47,892 : loading BERT model bert-large-uncased
2019-03-13 19:53:47,892 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:53:47,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:53:47,912 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4yvsht92
2019-03-13 19:53:55,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:54:00,562 : Generating sentence embeddings
2019-03-13 19:54:31,779 : Generated sentence embeddings
2019-03-13 19:54:31,780 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 19:54:39,217 : Best param found at split 1: l2reg = 1e-05                 with score 95.19
2019-03-13 19:54:48,069 : Best param found at split 2: l2reg = 0.001                 with score 95.32
2019-03-13 19:54:58,175 : Best param found at split 3: l2reg = 0.0001                 with score 94.96
2019-03-13 19:55:09,192 : Best param found at split 4: l2reg = 0.001                 with score 95.55
2019-03-13 19:55:20,146 : Best param found at split 5: l2reg = 0.001                 with score 95.35
2019-03-13 19:55:20,802 : Dev acc : 95.27 Test acc : 94.89

2019-03-13 19:55:20,803 : ***** Transfer task : SST Binary classification *****


2019-03-13 19:55:20,896 : loading BERT model bert-large-uncased
2019-03-13 19:55:20,897 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:55:20,970 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:55:20,970 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprtlyupq2
2019-03-13 19:55:28,346 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:55:33,587 : Computing embedding for train
2019-03-13 19:57:16,061 : Computed train embeddings
2019-03-13 19:57:16,061 : Computing embedding for dev
2019-03-13 19:57:18,362 : Computed dev embeddings
2019-03-13 19:57:18,362 : Computing embedding for test
2019-03-13 19:57:23,185 : Computed test embeddings
2019-03-13 19:57:23,185 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:57:37,791 : [('reg:1e-05', 85.89), ('reg:0.0001', 86.24), ('reg:0.001', 86.24), ('reg:0.01', 85.09)]
2019-03-13 19:57:37,791 : Validation : best param found is reg = 0.0001 with score             86.24
2019-03-13 19:57:37,791 : Evaluating...
2019-03-13 19:57:42,054 : 
Dev acc : 86.24 Test acc : 86.05 for             SST Binary classification

2019-03-13 19:57:42,055 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 19:57:42,105 : loading BERT model bert-large-uncased
2019-03-13 19:57:42,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:57:42,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:57:42,127 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqriqdae6
2019-03-13 19:57:49,528 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:57:54,802 : Computing embedding for train
2019-03-13 19:58:16,996 : Computed train embeddings
2019-03-13 19:58:16,996 : Computing embedding for dev
2019-03-13 19:58:19,899 : Computed dev embeddings
2019-03-13 19:58:19,899 : Computing embedding for test
2019-03-13 19:58:25,627 : Computed test embeddings
2019-03-13 19:58:25,627 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 19:58:28,530 : [('reg:1e-05', 41.96), ('reg:0.0001', 41.6), ('reg:0.001', 40.05), ('reg:0.01', 40.96)]
2019-03-13 19:58:28,530 : Validation : best param found is reg = 1e-05 with score             41.96
2019-03-13 19:58:28,530 : Evaluating...
2019-03-13 19:58:29,256 : 
Dev acc : 41.96 Test acc : 42.94 for             SST Fine-Grained classification

2019-03-13 19:58:29,256 : ***** Transfer task : TREC *****


2019-03-13 19:58:29,270 : loading BERT model bert-large-uncased
2019-03-13 19:58:29,270 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:58:29,288 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:58:29,288 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp76810h5k
2019-03-13 19:58:36,683 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:58:49,475 : Computed train embeddings
2019-03-13 19:58:50,071 : Computed test embeddings
2019-03-13 19:58:50,071 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:58:57,964 : [('reg:1e-05', 66.93), ('reg:0.0001', 64.87), ('reg:0.001', 63.17), ('reg:0.01', 47.17)]
2019-03-13 19:58:57,964 : Cross-validation : best param found is reg = 1e-05             with score 66.93
2019-03-13 19:58:57,964 : Evaluating...
2019-03-13 19:58:58,464 : 
Dev acc : 66.93 Test acc : 77.0             for TREC

2019-03-13 19:58:58,465 : ***** Transfer task : MRPC *****


2019-03-13 19:58:58,487 : loading BERT model bert-large-uncased
2019-03-13 19:58:58,487 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:58:58,507 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:58:58,508 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpysmmwrn0
2019-03-13 19:59:05,948 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 19:59:11,496 : Computing embedding for train
2019-03-13 19:59:34,029 : Computed train embeddings
2019-03-13 19:59:34,029 : Computing embedding for test
2019-03-13 19:59:43,906 : Computed test embeddings
2019-03-13 19:59:43,927 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 19:59:48,831 : [('reg:1e-05', 69.53), ('reg:0.0001', 69.33), ('reg:0.001', 69.87), ('reg:0.01', 69.53)]
2019-03-13 19:59:48,831 : Cross-validation : best param found is reg = 0.001             with score 69.87
2019-03-13 19:59:48,831 : Evaluating...
2019-03-13 19:59:49,096 : Dev acc : 69.87 Test acc 64.75; Test F1 72.26 for MRPC.

2019-03-13 19:59:49,096 : ***** Transfer task : SICK-Entailment*****


2019-03-13 19:59:49,167 : loading BERT model bert-large-uncased
2019-03-13 19:59:49,168 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 19:59:49,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 19:59:49,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpt0puqxdd
2019-03-13 19:59:56,609 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:00:01,833 : Computing embedding for train
2019-03-13 20:00:13,271 : Computed train embeddings
2019-03-13 20:00:13,271 : Computing embedding for dev
2019-03-13 20:00:14,834 : Computed dev embeddings
2019-03-13 20:00:14,834 : Computing embedding for test
2019-03-13 20:00:27,131 : Computed test embeddings
2019-03-13 20:00:27,168 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:00:28,499 : [('reg:1e-05', 64.4), ('reg:0.0001', 63.6), ('reg:0.001', 62.6), ('reg:0.01', 57.4)]
2019-03-13 20:00:28,500 : Validation : best param found is reg = 1e-05 with score             64.4
2019-03-13 20:00:28,500 : Evaluating...
2019-03-13 20:00:28,902 : 
Dev acc : 64.4 Test acc : 61.46 for                        SICK entailment

2019-03-13 20:00:28,902 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 20:00:28,930 : loading BERT model bert-large-uncased
2019-03-13 20:00:28,930 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:00:28,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:00:28,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkrguw_2z
2019-03-13 20:00:36,436 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:00:41,731 : Computing embedding for train
2019-03-13 20:00:53,171 : Computed train embeddings
2019-03-13 20:00:53,171 : Computing embedding for dev
2019-03-13 20:00:54,738 : Computed dev embeddings
2019-03-13 20:00:54,738 : Computing embedding for test
2019-03-13 20:01:07,033 : Computed test embeddings
2019-03-13 20:01:43,100 : Dev : Pearson 0.4978832040490991
2019-03-13 20:01:43,101 : Test : Pearson 0.490711009813074 Spearman 0.4751290966573236 MSE 0.8200533253659319                        for SICK Relatedness

2019-03-13 20:01:43,101 : 

***** Transfer task : STSBenchmark*****


2019-03-13 20:01:43,141 : loading BERT model bert-large-uncased
2019-03-13 20:01:43,142 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:01:43,169 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:01:43,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9szab5p8
2019-03-13 20:01:50,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:01:55,867 : Computing embedding for train
2019-03-13 20:02:14,579 : Computed train embeddings
2019-03-13 20:02:14,579 : Computing embedding for dev
2019-03-13 20:02:20,275 : Computed dev embeddings
2019-03-13 20:02:20,275 : Computing embedding for test
2019-03-13 20:02:24,933 : Computed test embeddings
2019-03-13 20:02:44,099 : Dev : Pearson 0.413308242841023
2019-03-13 20:02:44,099 : Test : Pearson 0.4078922139555519 Spearman 0.39488302401065056 MSE 2.0491012701248206                        for SICK Relatedness

2019-03-13 20:02:44,099 : ***** Transfer task : SNLI Entailment*****


2019-03-13 20:02:49,086 : loading BERT model bert-large-uncased
2019-03-13 20:02:49,086 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:02:49,155 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:02:49,155 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbg6w_oyk
2019-03-13 20:02:56,541 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:03:02,212 : PROGRESS (encoding): 0.00%
2019-03-13 20:05:52,867 : PROGRESS (encoding): 14.56%
2019-03-13 20:09:06,835 : PROGRESS (encoding): 29.12%
2019-03-13 20:12:19,079 : PROGRESS (encoding): 43.69%
2019-03-13 20:15:44,072 : PROGRESS (encoding): 58.25%
2019-03-13 20:19:31,914 : PROGRESS (encoding): 72.81%
2019-03-13 20:23:17,844 : PROGRESS (encoding): 87.37%
2019-03-13 20:27:22,637 : PROGRESS (encoding): 0.00%
2019-03-13 20:27:53,431 : PROGRESS (encoding): 0.00%
2019-03-13 20:28:23,124 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:28:55,542 : [('reg:1e-09', 58.96)]
2019-03-13 20:28:55,542 : Validation : best param found is reg = 1e-09 with score             58.96
2019-03-13 20:28:55,542 : Evaluating...
2019-03-13 20:29:27,853 : Dev acc : 58.96 Test acc : 58.7 for SNLI

2019-03-13 20:29:27,853 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 20:29:28,065 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 20:29:29,153 : loading BERT model bert-large-uncased
2019-03-13 20:29:29,153 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:29:29,182 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:29:29,182 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp8ccr3vyx
2019-03-13 20:29:36,612 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:29:41,907 : Computing embeddings for train/dev/test
2019-03-13 20:33:18,178 : Computed embeddings
2019-03-13 20:33:18,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:33:49,633 : [('reg:1e-05', 57.54), ('reg:0.0001', 56.46), ('reg:0.001', 51.52), ('reg:0.01', 47.55)]
2019-03-13 20:33:49,633 : Validation : best param found is reg = 1e-05 with score             57.54
2019-03-13 20:33:49,633 : Evaluating...
2019-03-13 20:33:55,731 : 
Dev acc : 57.5 Test acc : 59.5 for LENGTH classification

2019-03-13 20:33:55,732 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 20:33:55,984 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 20:33:56,030 : loading BERT model bert-large-uncased
2019-03-13 20:33:56,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:33:56,060 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:33:56,060 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpsaxh2c77
2019-03-13 20:34:03,493 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:34:08,639 : Computing embeddings for train/dev/test
2019-03-13 20:37:29,260 : Computed embeddings
2019-03-13 20:37:29,260 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:37:59,001 : [('reg:1e-05', 28.1), ('reg:0.0001', 13.6), ('reg:0.001', 2.19), ('reg:0.01', 0.58)]
2019-03-13 20:37:59,001 : Validation : best param found is reg = 1e-05 with score             28.1
2019-03-13 20:37:59,002 : Evaluating...
2019-03-13 20:38:09,979 : 
Dev acc : 28.1 Test acc : 28.3 for WORDCONTENT classification

2019-03-13 20:38:09,980 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 20:38:10,509 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 20:38:10,574 : loading BERT model bert-large-uncased
2019-03-13 20:38:10,574 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:38:10,598 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:38:10,598 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmps_ree9h3
2019-03-13 20:38:18,010 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:38:23,232 : Computing embeddings for train/dev/test
2019-03-13 20:41:31,258 : Computed embeddings
2019-03-13 20:41:31,259 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:41:59,425 : [('reg:1e-05', 24.39), ('reg:0.0001', 24.33), ('reg:0.001', 24.09), ('reg:0.01', 21.96)]
2019-03-13 20:41:59,425 : Validation : best param found is reg = 1e-05 with score             24.39
2019-03-13 20:41:59,425 : Evaluating...
2019-03-13 20:42:05,894 : 
Dev acc : 24.4 Test acc : 23.8 for DEPTH classification

2019-03-13 20:42:05,895 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 20:42:06,266 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 20:42:06,328 : loading BERT model bert-large-uncased
2019-03-13 20:42:06,329 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:42:06,438 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:42:06,438 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpygbavcu3
2019-03-13 20:42:13,847 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:42:19,007 : Computing embeddings for train/dev/test
2019-03-13 20:45:14,351 : Computed embeddings
2019-03-13 20:45:14,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:45:46,471 : [('reg:1e-05', 54.53), ('reg:0.0001', 51.25), ('reg:0.001', 45.62), ('reg:0.01', 33.4)]
2019-03-13 20:45:46,471 : Validation : best param found is reg = 1e-05 with score             54.53
2019-03-13 20:45:46,471 : Evaluating...
2019-03-13 20:45:55,157 : 
Dev acc : 54.5 Test acc : 53.9 for TOPCONSTITUENTS classification

2019-03-13 20:45:55,158 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 20:45:55,534 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 20:45:55,601 : loading BERT model bert-large-uncased
2019-03-13 20:45:55,601 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:45:55,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:45:55,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp7vss_4sr
2019-03-13 20:46:03,087 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:46:08,421 : Computing embeddings for train/dev/test
2019-03-13 20:49:16,038 : Computed embeddings
2019-03-13 20:49:16,038 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:49:46,300 : [('reg:1e-05', 86.46), ('reg:0.0001', 86.88), ('reg:0.001', 87.18), ('reg:0.01', 84.67)]
2019-03-13 20:49:46,300 : Validation : best param found is reg = 0.001 with score             87.18
2019-03-13 20:49:46,300 : Evaluating...
2019-03-13 20:49:54,086 : 
Dev acc : 87.2 Test acc : 87.1 for BIGRAMSHIFT classification

2019-03-13 20:49:54,087 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 20:49:54,482 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 20:49:54,547 : loading BERT model bert-large-uncased
2019-03-13 20:49:54,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:49:54,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:49:54,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpdf3zhulq
2019-03-13 20:50:02,014 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:50:07,281 : Computing embeddings for train/dev/test
2019-03-13 20:53:09,629 : Computed embeddings
2019-03-13 20:53:09,629 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:53:37,338 : [('reg:1e-05', 88.84), ('reg:0.0001', 88.83), ('reg:0.001', 89.03), ('reg:0.01', 89.02)]
2019-03-13 20:53:37,338 : Validation : best param found is reg = 0.001 with score             89.03
2019-03-13 20:53:37,338 : Evaluating...
2019-03-13 20:53:46,272 : 
Dev acc : 89.0 Test acc : 87.2 for TENSE classification

2019-03-13 20:53:46,273 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 20:53:46,686 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 20:53:46,749 : loading BERT model bert-large-uncased
2019-03-13 20:53:46,749 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:53:46,864 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:53:46,864 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpnh4k8_xd
2019-03-13 20:53:54,364 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:53:59,579 : Computing embeddings for train/dev/test
2019-03-13 20:57:13,780 : Computed embeddings
2019-03-13 20:57:13,780 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 20:57:35,314 : [('reg:1e-05', 75.0), ('reg:0.0001', 74.82), ('reg:0.001', 75.1), ('reg:0.01', 72.54)]
2019-03-13 20:57:35,314 : Validation : best param found is reg = 0.001 with score             75.1
2019-03-13 20:57:35,314 : Evaluating...
2019-03-13 20:57:40,676 : 
Dev acc : 75.1 Test acc : 74.0 for SUBJNUMBER classification

2019-03-13 20:57:40,677 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 20:57:41,094 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 20:57:41,164 : loading BERT model bert-large-uncased
2019-03-13 20:57:41,164 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 20:57:41,289 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 20:57:41,289 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpluh0li6m
2019-03-13 20:57:48,741 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 20:57:53,971 : Computing embeddings for train/dev/test
2019-03-13 21:01:05,351 : Computed embeddings
2019-03-13 21:01:05,352 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:01:32,978 : [('reg:1e-05', 69.32), ('reg:0.0001', 69.34), ('reg:0.001', 69.03), ('reg:0.01', 65.55)]
2019-03-13 21:01:32,978 : Validation : best param found is reg = 0.0001 with score             69.34
2019-03-13 21:01:32,978 : Evaluating...
2019-03-13 21:01:40,467 : 
Dev acc : 69.3 Test acc : 70.4 for OBJNUMBER classification

2019-03-13 21:01:40,468 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 21:01:41,035 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 21:01:41,104 : loading BERT model bert-large-uncased
2019-03-13 21:01:41,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:01:41,133 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:01:41,133 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp243p2wn_
2019-03-13 21:01:48,611 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:01:53,844 : Computing embeddings for train/dev/test
2019-03-13 21:05:37,695 : Computed embeddings
2019-03-13 21:05:37,695 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:06:02,249 : [('reg:1e-05', 63.9), ('reg:0.0001', 63.82), ('reg:0.001', 63.62), ('reg:0.01', 63.11)]
2019-03-13 21:06:02,249 : Validation : best param found is reg = 1e-05 with score             63.9
2019-03-13 21:06:02,249 : Evaluating...
2019-03-13 21:06:09,532 : 
Dev acc : 63.9 Test acc : 64.8 for ODDMANOUT classification

2019-03-13 21:06:09,533 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 21:06:09,908 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 21:06:09,988 : loading BERT model bert-large-uncased
2019-03-13 21:06:09,989 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:06:10,017 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:06:10,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_stv0wcx
2019-03-13 21:06:17,449 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:06:22,661 : Computing embeddings for train/dev/test
2019-03-13 21:10:05,305 : Computed embeddings
2019-03-13 21:10:05,305 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:10:34,058 : [('reg:1e-05', 68.46), ('reg:0.0001', 68.45), ('reg:0.001', 68.1), ('reg:0.01', 61.56)]
2019-03-13 21:10:34,058 : Validation : best param found is reg = 1e-05 with score             68.46
2019-03-13 21:10:34,058 : Evaluating...
2019-03-13 21:10:41,505 : 
Dev acc : 68.5 Test acc : 67.7 for COORDINATIONINVERSION classification

2019-03-13 21:10:41,507 : total results: {'STS12': {'MSRpar': {'pearson': (0.22907237837958927, 2.187949594962718e-10), 'spearman': SpearmanrResult(correlation=0.323129608970981, pvalue=1.093934188788297e-19), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.004426855242908203, 0.903665052100634), 'spearman': SpearmanrResult(correlation=0.1377761474533146, pvalue=0.00015375440203749), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.10877814334384547, 0.019750183763193985), 'spearman': SpearmanrResult(correlation=0.41378826348118775, pvalue=2.059610242946967e-20), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.1174122871648287, 0.0012763157698688786), 'spearman': SpearmanrResult(correlation=0.20976239535180458, pvalue=6.645729710249282e-09), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4141357050222313, 5.7572074849574385e-18), 'spearman': SpearmanrResult(correlation=0.4926071800195832, pvalue=8.820052545778275e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.17299433173351733, 'wmean': 0.15177370393334225}, 'spearman': {'mean': 0.31541271905537427, 'wmean': 0.28619053783711523}}}, 'STS13': {'FNWN': {'pearson': (-0.025232469434023946, 0.7303629729482874), 'spearman': SpearmanrResult(correlation=-0.022163527356859527, pvalue=0.7621078593588522), 'nsamples': 189}, 'headlines': {'pearson': (-0.009452703104336503, 0.7960620062400956), 'spearman': SpearmanrResult(correlation=0.16276109329318883, pvalue=7.4689728864521675e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.15810823984764832, 0.00016978642135038777), 'spearman': SpearmanrResult(correlation=0.1385486733596381, pvalue=0.001001348565137596), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.04114102243642929, 'wmean': 0.05122683900216521}, 'spearman': {'mean': 0.09304874643198913, 'wmean': 0.13040514603613476}}}, 'STS14': {'deft-forum': {'pearson': (0.028882036507377293, 0.5411307700827287), 'spearman': SpearmanrResult(correlation=0.056017218690532056, pvalue=0.23564956928731245), 'nsamples': 450}, 'deft-news': {'pearson': (0.4890105883418247, 1.931087338215088e-19), 'spearman': SpearmanrResult(correlation=0.549286565551873, pvalue=4.810458924003044e-25), 'nsamples': 300}, 'headlines': {'pearson': (0.08995715989261718, 0.013721706804909018), 'spearman': SpearmanrResult(correlation=0.19569641210538882, pvalue=6.562652599043283e-08), 'nsamples': 750}, 'images': {'pearson': (0.012874100132759962, 0.7248395603511982), 'spearman': SpearmanrResult(correlation=0.10682730931665617, pvalue=0.003399534484926883), 'nsamples': 750}, 'OnWN': {'pearson': (0.1686645730437921, 3.406041204671078e-06), 'spearman': SpearmanrResult(correlation=0.22499579708737455, pvalue=4.618206964410442e-10), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19621658667685152, 6.047290585924072e-08), 'spearman': SpearmanrResult(correlation=0.222231857231517, pvalue=7.602051886823624e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.16426750743253712, 'wmean': 0.1361291753974354}, 'spearman': {'mean': 0.22584252666389026, 'wmean': 0.20061526663520102}}}, 'STS15': {'answers-forums': {'pearson': (0.30993518357463334, 8.595802217637382e-10), 'spearman': SpearmanrResult(correlation=0.36311665187792447, pvalue=3.9446580867496467e-13), 'nsamples': 375}, 'answers-students': {'pearson': (0.13244842476721377, 0.0002755802176247033), 'spearman': SpearmanrResult(correlation=0.1442420523869997, pvalue=7.35832717240097e-05), 'nsamples': 750}, 'belief': {'pearson': (0.12428705836180629, 0.01603501682883061), 'spearman': SpearmanrResult(correlation=0.3036772032437123, pvalue=1.932157515888765e-09), 'nsamples': 375}, 'headlines': {'pearson': (0.18095291813170644, 6.082722125243291e-07), 'spearman': SpearmanrResult(correlation=0.2960925451127703, pvalue=1.2141605615357278e-16), 'nsamples': 750}, 'images': {'pearson': (0.03198865935911107, 0.38167827315417213), 'spearman': SpearmanrResult(correlation=0.21311919096098209, pvalue=3.7553536803456905e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1559224488388942, 'wmean': 0.14062528080656275}, 'spearman': {'mean': 0.2640495287164778, 'wmean': 0.2467126790053926}}}, 'STS16': {'answer-answer': {'pearson': (0.31868498962966085, 2.1040871992717132e-07), 'spearman': SpearmanrResult(correlation=0.5005576527112411, pvalue=1.6344666364305488e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.15748932928655673, 0.01283979837940493), 'spearman': SpearmanrResult(correlation=0.3189954000008265, pvalue=2.699048486329878e-07), 'nsamples': 249}, 'plagiarism': {'pearson': (0.23969487155992109, 0.00024352319906076016), 'spearman': SpearmanrResult(correlation=0.6222752700936797, pvalue=4.7618019785517165e-26), 'nsamples': 230}, 'postediting': {'pearson': (0.37595788374377886, 1.307174054378832e-09), 'spearman': SpearmanrResult(correlation=0.6258395978301683, pvalue=6.177269052248887e-28), 'nsamples': 244}, 'question-question': {'pearson': (0.059585778929767685, 0.39143542089304595), 'spearman': SpearmanrResult(correlation=0.312614073907766, pvalue=4.059156000308223e-06), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.23028257062993704, 'wmean': 0.23564738806650237}, 'spearman': {'mean': 0.47605639890873624, 'wmean': 0.4786981566843942}}}, 'MR': {'devacc': 80.81, 'acc': 80.32, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 86.52, 'acc': 85.41, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.36, 'acc': 81.74, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.27, 'acc': 94.89, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 86.24, 'acc': 86.05, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.96, 'acc': 42.94, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 66.93, 'acc': 77.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.87, 'acc': 64.75, 'f1': 72.26, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 64.4, 'acc': 61.46, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.4978832040490991, 'pearson': 0.490711009813074, 'spearman': 0.4751290966573236, 'mse': 0.8200533253659319, 'yhat': array([3.66304392, 4.14819963, 1.88058356, ..., 3.30531945, 4.42146208,        3.92269274]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.413308242841023, 'pearson': 0.4078922139555519, 'spearman': 0.39488302401065056, 'mse': 2.0491012701248206, 'yhat': array([2.98054348, 2.97405533, 2.45297641, ..., 4.38823552, 3.95404777,        3.85952877]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.96, 'acc': 58.7, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 57.54, 'acc': 59.47, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 28.1, 'acc': 28.33, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 24.39, 'acc': 23.76, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 54.53, 'acc': 53.9, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.18, 'acc': 87.14, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.03, 'acc': 87.24, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.1, 'acc': 74.05, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 69.34, 'acc': 70.43, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.9, 'acc': 64.84, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 68.46, 'acc': 67.66, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 21:10:41,507 : STS12 p=0.1518, STS12 s=0.2862, STS13 p=0.0512, STS13 s=0.1304, STS14 p=0.1361, STS14 s=0.2006, STS15 p=0.1406, STS15 s=0.2467, STS 16 p=0.2356, STS16 s=0.4787, STS B p=0.4079, STS B s=0.3949, STS B m=2.0491, SICK-R p=0.4907, SICK-R s=0.4751, SICK-P m=0.8201
2019-03-13 21:10:41,507 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 21:10:41,507 : 0.1518,0.2862,0.0512,0.1304,0.1361,0.2006,0.1406,0.2467,0.2356,0.4787,0.4079,0.3949,2.0491,0.4907,0.4751,0.8201
2019-03-13 21:10:41,507 : MR=80.32, CR=85.41, SUBJ=94.89, MPQA=81.74, SST-B=86.05, SST-F=42.94, TREC=77.00, SICK-E=61.46, SNLI=58.70, MRPC=64.75, MRPC f=72.26
2019-03-13 21:10:41,507 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 21:10:41,507 : 80.32,85.41,94.89,81.74,86.05,42.94,77.00,61.46,58.70,64.75,72.26
2019-03-13 21:10:41,507 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 21:10:41,507 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 21:10:41,507 : na,na,na,na,na,na,na,na,na,na
2019-03-13 21:10:41,507 : SentLen=59.47, WC=28.33, TreeDepth=23.76, TopConst=53.90, BShift=87.14, Tense=87.24, SubjNum=74.05, ObjNum=70.43, SOMO=64.84, CoordInv=67.66, average=61.68
2019-03-13 21:10:41,507 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 21:10:41,507 : 59.47,28.33,23.76,53.90,87.14,87.24,74.05,70.43,64.84,67.66,61.68
2019-03-13 21:10:41,507 : ********************************************************************************
2019-03-13 21:10:41,507 : ********************************************************************************
2019-03-13 21:10:41,507 : ********************************************************************************
2019-03-13 21:10:41,507 : layer 23
2019-03-13 21:10:41,507 : ********************************************************************************
2019-03-13 21:10:41,508 : ********************************************************************************
2019-03-13 21:10:41,508 : ********************************************************************************
2019-03-13 21:10:41,598 : ***** Transfer task : STS12 *****


2019-03-13 21:10:41,610 : loading BERT model bert-large-uncased
2019-03-13 21:10:41,610 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:10:41,627 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:10:41,628 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq63tlkax
2019-03-13 21:10:49,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:10:58,384 : MSRpar : pearson = 0.2343, spearman = 0.3245
2019-03-13 21:11:00,040 : MSRvid : pearson = -0.0194, spearman = 0.1331
2019-03-13 21:11:01,465 : SMTeuroparl : pearson = 0.0827, spearman = 0.4057
2019-03-13 21:11:04,189 : surprise.OnWN : pearson = 0.1191, spearman = 0.2045
2019-03-13 21:11:05,631 : surprise.SMTnews : pearson = 0.3968, spearman = 0.4878
2019-03-13 21:11:05,631 : ALL (weighted average) : Pearson = 0.1437,             Spearman = 0.2823
2019-03-13 21:11:05,631 : ALL (average) : Pearson = 0.1627,             Spearman = 0.3111

2019-03-13 21:11:05,632 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 21:11:05,640 : loading BERT model bert-large-uncased
2019-03-13 21:11:05,640 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:11:05,659 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:11:05,659 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9zr8k611
2019-03-13 21:11:13,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:11:19,578 : FNWN : pearson = -0.0228, spearman = 0.0159
2019-03-13 21:11:21,492 : headlines : pearson = -0.0023, spearman = 0.1680
2019-03-13 21:11:22,974 : OnWN : pearson = 0.1513, spearman = 0.1251
2019-03-13 21:11:22,974 : ALL (weighted average) : Pearson = 0.0525,             Spearman = 0.1328
2019-03-13 21:11:22,974 : ALL (average) : Pearson = 0.0420,             Spearman = 0.1030

2019-03-13 21:11:22,975 : ***** Transfer task : STS14 *****


2019-03-13 21:11:22,989 : loading BERT model bert-large-uncased
2019-03-13 21:11:22,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:11:23,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:11:23,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_4zbcbuh
2019-03-13 21:11:30,413 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:11:36,962 : deft-forum : pearson = 0.0197, spearman = 0.0560
2019-03-13 21:11:38,619 : deft-news : pearson = 0.4623, spearman = 0.5508
2019-03-13 21:11:40,816 : headlines : pearson = 0.0970, spearman = 0.1919
2019-03-13 21:11:42,913 : images : pearson = 0.0051, spearman = 0.1103
2019-03-13 21:11:45,068 : OnWN : pearson = 0.1602, spearman = 0.1984
2019-03-13 21:11:47,967 : tweet-news : pearson = 0.1899, spearman = 0.2353
2019-03-13 21:11:47,967 : ALL (weighted average) : Pearson = 0.1298,             Spearman = 0.1980
2019-03-13 21:11:47,967 : ALL (average) : Pearson = 0.1557,             Spearman = 0.2238

2019-03-13 21:11:47,967 : ***** Transfer task : STS15 *****


2019-03-13 21:11:47,999 : loading BERT model bert-large-uncased
2019-03-13 21:11:47,999 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:11:48,017 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:11:48,017 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpr56_sj5y
2019-03-13 21:11:55,437 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:12:02,415 : answers-forums : pearson = 0.2730, spearman = 0.3664
2019-03-13 21:12:04,525 : answers-students : pearson = 0.1331, spearman = 0.1794
2019-03-13 21:12:06,606 : belief : pearson = 0.0672, spearman = 0.2650
2019-03-13 21:12:08,885 : headlines : pearson = 0.1893, spearman = 0.3022
2019-03-13 21:12:11,046 : images : pearson = 0.0118, spearman = 0.2011
2019-03-13 21:12:11,046 : ALL (weighted average) : Pearson = 0.1261,             Spearman = 0.2496
2019-03-13 21:12:11,046 : ALL (average) : Pearson = 0.1349,             Spearman = 0.2628

2019-03-13 21:12:11,046 : ***** Transfer task : STS16 *****


2019-03-13 21:12:11,116 : loading BERT model bert-large-uncased
2019-03-13 21:12:11,116 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:12:11,134 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:12:11,134 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpi3h_nwgs
2019-03-13 21:12:18,571 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:12:24,787 : answer-answer : pearson = 0.2875, spearman = 0.5102
2019-03-13 21:12:25,454 : headlines : pearson = 0.1593, spearman = 0.3328
2019-03-13 21:12:26,347 : plagiarism : pearson = 0.2282, spearman = 0.6208
2019-03-13 21:12:27,861 : postediting : pearson = 0.3073, spearman = 0.6156
2019-03-13 21:12:28,475 : question-question : pearson = 0.0539, spearman = 0.2726
2019-03-13 21:12:28,475 : ALL (weighted average) : Pearson = 0.2120,             Spearman = 0.4742
2019-03-13 21:12:28,475 : ALL (average) : Pearson = 0.2072,             Spearman = 0.4704

2019-03-13 21:12:28,475 : ***** Transfer task : MR *****


2019-03-13 21:12:28,490 : loading BERT model bert-large-uncased
2019-03-13 21:12:28,490 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:12:28,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:12:28,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6xh4ige1
2019-03-13 21:12:35,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:12:41,294 : Generating sentence embeddings
2019-03-13 21:13:13,192 : Generated sentence embeddings
2019-03-13 21:13:13,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:13:22,740 : Best param found at split 1: l2reg = 0.0001                 with score 80.55
2019-03-13 21:13:33,453 : Best param found at split 2: l2reg = 0.001                 with score 80.59
2019-03-13 21:13:43,819 : Best param found at split 3: l2reg = 0.0001                 with score 79.6
2019-03-13 21:13:54,287 : Best param found at split 4: l2reg = 0.001                 with score 80.21
2019-03-13 21:14:05,186 : Best param found at split 5: l2reg = 1e-05                 with score 80.6
2019-03-13 21:14:05,714 : Dev acc : 80.31 Test acc : 79.95

2019-03-13 21:14:05,715 : ***** Transfer task : CR *****


2019-03-13 21:14:05,722 : loading BERT model bert-large-uncased
2019-03-13 21:14:05,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:14:05,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:14:05,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpgec21ro_
2019-03-13 21:14:13,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:14:18,522 : Generating sentence embeddings
2019-03-13 21:14:26,913 : Generated sentence embeddings
2019-03-13 21:14:26,913 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:14:30,357 : Best param found at split 1: l2reg = 0.01                 with score 85.79
2019-03-13 21:14:34,178 : Best param found at split 2: l2reg = 0.0001                 with score 85.0
2019-03-13 21:14:38,384 : Best param found at split 3: l2reg = 0.001                 with score 85.6
2019-03-13 21:14:42,236 : Best param found at split 4: l2reg = 0.001                 with score 86.0
2019-03-13 21:14:45,905 : Best param found at split 5: l2reg = 0.01                 with score 85.63
2019-03-13 21:14:46,092 : Dev acc : 85.6 Test acc : 85.27

2019-03-13 21:14:46,092 : ***** Transfer task : MPQA *****


2019-03-13 21:14:46,098 : loading BERT model bert-large-uncased
2019-03-13 21:14:46,098 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:14:46,148 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:14:46,148 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_w2ac81v
2019-03-13 21:14:53,594 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:14:58,871 : Generating sentence embeddings
2019-03-13 21:15:06,534 : Generated sentence embeddings
2019-03-13 21:15:06,535 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:15:15,969 : Best param found at split 1: l2reg = 0.0001                 with score 77.97
2019-03-13 21:15:26,375 : Best param found at split 2: l2reg = 1e-05                 with score 78.61
2019-03-13 21:15:38,049 : Best param found at split 3: l2reg = 1e-05                 with score 78.95
2019-03-13 21:15:48,329 : Best param found at split 4: l2reg = 1e-05                 with score 79.07
2019-03-13 21:15:58,731 : Best param found at split 5: l2reg = 1e-05                 with score 76.16
2019-03-13 21:15:59,633 : Dev acc : 78.15 Test acc : 79.54

2019-03-13 21:15:59,634 : ***** Transfer task : SUBJ *****


2019-03-13 21:15:59,650 : loading BERT model bert-large-uncased
2019-03-13 21:15:59,651 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:15:59,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:15:59,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqgtrct88
2019-03-13 21:16:07,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:16:12,468 : Generating sentence embeddings
2019-03-13 21:16:43,709 : Generated sentence embeddings
2019-03-13 21:16:43,709 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 21:16:51,977 : Best param found at split 1: l2reg = 0.0001                 with score 94.7
2019-03-13 21:17:01,766 : Best param found at split 2: l2reg = 0.001                 with score 94.98
2019-03-13 21:17:11,432 : Best param found at split 3: l2reg = 1e-05                 with score 94.75
2019-03-13 21:17:21,318 : Best param found at split 4: l2reg = 0.0001                 with score 94.91
2019-03-13 21:17:32,045 : Best param found at split 5: l2reg = 0.001                 with score 94.52
2019-03-13 21:17:32,397 : Dev acc : 94.77 Test acc : 94.31

2019-03-13 21:17:32,398 : ***** Transfer task : SST Binary classification *****


2019-03-13 21:17:32,488 : loading BERT model bert-large-uncased
2019-03-13 21:17:32,489 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:17:32,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:17:32,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq_yf1ui8
2019-03-13 21:17:39,994 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:17:45,271 : Computing embedding for train
2019-03-13 21:19:27,478 : Computed train embeddings
2019-03-13 21:19:27,478 : Computing embedding for dev
2019-03-13 21:19:29,775 : Computed dev embeddings
2019-03-13 21:19:29,775 : Computing embedding for test
2019-03-13 21:19:34,590 : Computed test embeddings
2019-03-13 21:19:34,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:19:51,037 : [('reg:1e-05', 84.75), ('reg:0.0001', 84.29), ('reg:0.001', 84.17), ('reg:0.01', 81.54)]
2019-03-13 21:19:51,037 : Validation : best param found is reg = 1e-05 with score             84.75
2019-03-13 21:19:51,037 : Evaluating...
2019-03-13 21:19:55,328 : 
Dev acc : 84.75 Test acc : 85.28 for             SST Binary classification

2019-03-13 21:19:55,329 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 21:19:55,383 : loading BERT model bert-large-uncased
2019-03-13 21:19:55,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:19:55,404 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:19:55,404 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmphzedr9m7
2019-03-13 21:20:02,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:20:08,078 : Computing embedding for train
2019-03-13 21:20:30,256 : Computed train embeddings
2019-03-13 21:20:30,256 : Computing embedding for dev
2019-03-13 21:20:33,161 : Computed dev embeddings
2019-03-13 21:20:33,161 : Computing embedding for test
2019-03-13 21:20:38,883 : Computed test embeddings
2019-03-13 21:20:38,883 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:20:41,476 : [('reg:1e-05', 40.78), ('reg:0.0001', 38.06), ('reg:0.001', 42.33), ('reg:0.01', 40.96)]
2019-03-13 21:20:41,476 : Validation : best param found is reg = 0.001 with score             42.33
2019-03-13 21:20:41,476 : Evaluating...
2019-03-13 21:20:42,317 : 
Dev acc : 42.33 Test acc : 41.99 for             SST Fine-Grained classification

2019-03-13 21:20:42,318 : ***** Transfer task : TREC *****


2019-03-13 21:20:42,332 : loading BERT model bert-large-uncased
2019-03-13 21:20:42,332 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:20:42,351 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:20:42,351 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpba8dwe5s
2019-03-13 21:20:49,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:21:02,609 : Computed train embeddings
2019-03-13 21:21:03,202 : Computed test embeddings
2019-03-13 21:21:03,203 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 21:21:11,278 : [('reg:1e-05', 60.71), ('reg:0.0001', 59.65), ('reg:0.001', 51.65), ('reg:0.01', 44.53)]
2019-03-13 21:21:11,278 : Cross-validation : best param found is reg = 1e-05             with score 60.71
2019-03-13 21:21:11,278 : Evaluating...
2019-03-13 21:21:11,725 : 
Dev acc : 60.71 Test acc : 68.6             for TREC

2019-03-13 21:21:11,725 : ***** Transfer task : MRPC *****


2019-03-13 21:21:11,746 : loading BERT model bert-large-uncased
2019-03-13 21:21:11,747 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:21:11,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:21:11,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppcn0wvr0
2019-03-13 21:21:19,186 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:21:24,357 : Computing embedding for train
2019-03-13 21:21:46,867 : Computed train embeddings
2019-03-13 21:21:46,867 : Computing embedding for test
2019-03-13 21:21:56,754 : Computed test embeddings
2019-03-13 21:21:56,776 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 21:22:01,645 : [('reg:1e-05', 69.33), ('reg:0.0001', 69.43), ('reg:0.001', 68.97), ('reg:0.01', 69.51)]
2019-03-13 21:22:01,645 : Cross-validation : best param found is reg = 0.01             with score 69.51
2019-03-13 21:22:01,646 : Evaluating...
2019-03-13 21:22:02,077 : Dev acc : 69.51 Test acc 64.93; Test F1 73.34 for MRPC.

2019-03-13 21:22:02,078 : ***** Transfer task : SICK-Entailment*****


2019-03-13 21:22:02,140 : loading BERT model bert-large-uncased
2019-03-13 21:22:02,140 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:22:02,159 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:22:02,159 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzkvf_g9l
2019-03-13 21:22:09,590 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:22:14,844 : Computing embedding for train
2019-03-13 21:22:26,262 : Computed train embeddings
2019-03-13 21:22:26,262 : Computing embedding for dev
2019-03-13 21:22:27,825 : Computed dev embeddings
2019-03-13 21:22:27,825 : Computing embedding for test
2019-03-13 21:22:40,110 : Computed test embeddings
2019-03-13 21:22:40,146 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:22:41,631 : [('reg:1e-05', 62.6), ('reg:0.0001', 61.8), ('reg:0.001', 64.0), ('reg:0.01', 60.8)]
2019-03-13 21:22:41,631 : Validation : best param found is reg = 0.001 with score             64.0
2019-03-13 21:22:41,632 : Evaluating...
2019-03-13 21:22:41,938 : 
Dev acc : 64.0 Test acc : 60.85 for                        SICK entailment

2019-03-13 21:22:41,938 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 21:22:41,965 : loading BERT model bert-large-uncased
2019-03-13 21:22:41,965 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:22:42,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:22:42,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp5i74op2x
2019-03-13 21:22:49,520 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:22:54,692 : Computing embedding for train
2019-03-13 21:23:06,147 : Computed train embeddings
2019-03-13 21:23:06,147 : Computing embedding for dev
2019-03-13 21:23:07,716 : Computed dev embeddings
2019-03-13 21:23:07,716 : Computing embedding for test
2019-03-13 21:23:20,023 : Computed test embeddings
2019-03-13 21:23:43,959 : Dev : Pearson 0.3698505235956299
2019-03-13 21:23:43,959 : Test : Pearson 0.33592021834802843 Spearman 0.3696139224924025 MSE 0.9444868301836438                        for SICK Relatedness

2019-03-13 21:23:43,960 : 

***** Transfer task : STSBenchmark*****


2019-03-13 21:23:44,000 : loading BERT model bert-large-uncased
2019-03-13 21:23:44,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:23:44,029 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:23:44,029 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkrhr8bkb
2019-03-13 21:23:51,485 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:23:56,636 : Computing embedding for train
2019-03-13 21:24:15,415 : Computed train embeddings
2019-03-13 21:24:15,415 : Computing embedding for dev
2019-03-13 21:24:21,122 : Computed dev embeddings
2019-03-13 21:24:21,123 : Computing embedding for test
2019-03-13 21:24:25,797 : Computed test embeddings
2019-03-13 21:24:43,523 : Dev : Pearson 0.3705771174086627
2019-03-13 21:24:43,523 : Test : Pearson 0.39072035286473583 Spearman 0.37889591692985397 MSE 2.073606313196394                        for SICK Relatedness

2019-03-13 21:24:43,524 : ***** Transfer task : SNLI Entailment*****


2019-03-13 21:24:48,635 : loading BERT model bert-large-uncased
2019-03-13 21:24:48,635 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:24:48,775 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:24:48,775 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo7h3p817
2019-03-13 21:24:56,221 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:25:01,955 : PROGRESS (encoding): 0.00%
2019-03-13 21:27:51,510 : PROGRESS (encoding): 14.56%
2019-03-13 21:31:05,094 : PROGRESS (encoding): 29.12%
2019-03-13 21:34:17,498 : PROGRESS (encoding): 43.69%
2019-03-13 21:37:42,105 : PROGRESS (encoding): 58.25%
2019-03-13 21:41:28,937 : PROGRESS (encoding): 72.81%
2019-03-13 21:45:14,393 : PROGRESS (encoding): 87.37%
2019-03-13 21:49:19,081 : PROGRESS (encoding): 0.00%
2019-03-13 21:49:49,750 : PROGRESS (encoding): 0.00%
2019-03-13 21:50:19,252 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:50:44,676 : [('reg:1e-09', 54.07)]
2019-03-13 21:50:44,676 : Validation : best param found is reg = 1e-09 with score             54.07
2019-03-13 21:50:44,676 : Evaluating...
2019-03-13 21:51:08,400 : Dev acc : 54.07 Test acc : 54.01 for SNLI

2019-03-13 21:51:08,400 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 21:51:08,602 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 21:51:09,647 : loading BERT model bert-large-uncased
2019-03-13 21:51:09,647 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:51:09,673 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:51:09,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpwg358wox
2019-03-13 21:51:17,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:51:22,298 : Computing embeddings for train/dev/test
2019-03-13 21:54:59,135 : Computed embeddings
2019-03-13 21:54:59,135 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:55:24,898 : [('reg:1e-05', 54.53), ('reg:0.0001', 54.23), ('reg:0.001', 46.79), ('reg:0.01', 45.05)]
2019-03-13 21:55:24,898 : Validation : best param found is reg = 1e-05 with score             54.53
2019-03-13 21:55:24,898 : Evaluating...
2019-03-13 21:55:30,312 : 
Dev acc : 54.5 Test acc : 53.6 for LENGTH classification

2019-03-13 21:55:30,313 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 21:55:30,700 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 21:55:30,745 : loading BERT model bert-large-uncased
2019-03-13 21:55:30,746 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:55:30,775 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:55:30,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp2fhb1w8k
2019-03-13 21:55:38,219 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:55:43,461 : Computing embeddings for train/dev/test
2019-03-13 21:59:03,981 : Computed embeddings
2019-03-13 21:59:03,981 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 21:59:33,171 : [('reg:1e-05', 26.87), ('reg:0.0001', 12.32), ('reg:0.001', 3.03), ('reg:0.01', 0.68)]
2019-03-13 21:59:33,172 : Validation : best param found is reg = 1e-05 with score             26.87
2019-03-13 21:59:33,172 : Evaluating...
2019-03-13 21:59:43,966 : 
Dev acc : 26.9 Test acc : 27.0 for WORDCONTENT classification

2019-03-13 21:59:43,967 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 21:59:44,348 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 21:59:44,416 : loading BERT model bert-large-uncased
2019-03-13 21:59:44,416 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 21:59:44,441 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 21:59:44,441 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpjbpl5olv
2019-03-13 21:59:51,844 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 21:59:57,163 : Computing embeddings for train/dev/test
2019-03-13 22:03:05,800 : Computed embeddings
2019-03-13 22:03:05,800 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:03:29,886 : [('reg:1e-05', 23.87), ('reg:0.0001', 23.76), ('reg:0.001', 23.34), ('reg:0.01', 21.65)]
2019-03-13 22:03:29,886 : Validation : best param found is reg = 1e-05 with score             23.87
2019-03-13 22:03:29,886 : Evaluating...
2019-03-13 22:03:36,370 : 
Dev acc : 23.9 Test acc : 23.9 for DEPTH classification

2019-03-13 22:03:36,371 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 22:03:36,760 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 22:03:36,825 : loading BERT model bert-large-uncased
2019-03-13 22:03:36,825 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:03:36,933 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:03:36,933 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4lh9sn64
2019-03-13 22:03:44,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:03:49,719 : Computing embeddings for train/dev/test
2019-03-13 22:06:44,444 : Computed embeddings
2019-03-13 22:06:44,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:07:13,764 : [('reg:1e-05', 47.8), ('reg:0.0001', 47.09), ('reg:0.001', 42.14), ('reg:0.01', 28.86)]
2019-03-13 22:07:13,764 : Validation : best param found is reg = 1e-05 with score             47.8
2019-03-13 22:07:13,764 : Evaluating...
2019-03-13 22:07:21,414 : 
Dev acc : 47.8 Test acc : 47.2 for TOPCONSTITUENTS classification

2019-03-13 22:07:21,415 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 22:07:21,768 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 22:07:21,836 : loading BERT model bert-large-uncased
2019-03-13 22:07:21,836 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:07:21,960 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:07:21,960 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpe_9db5dm
2019-03-13 22:07:29,394 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:07:34,651 : Computing embeddings for train/dev/test
2019-03-13 22:10:42,133 : Computed embeddings
2019-03-13 22:10:42,133 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:11:15,978 : [('reg:1e-05', 85.75), ('reg:0.0001', 85.78), ('reg:0.001', 84.95), ('reg:0.01', 84.27)]
2019-03-13 22:11:15,978 : Validation : best param found is reg = 0.0001 with score             85.78
2019-03-13 22:11:15,978 : Evaluating...
2019-03-13 22:11:25,564 : 
Dev acc : 85.8 Test acc : 85.9 for BIGRAMSHIFT classification

2019-03-13 22:11:25,565 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 22:11:26,139 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 22:11:26,205 : loading BERT model bert-large-uncased
2019-03-13 22:11:26,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:11:26,234 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:11:26,235 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0gfguvm6
2019-03-13 22:11:33,730 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:11:38,987 : Computing embeddings for train/dev/test
2019-03-13 22:14:40,842 : Computed embeddings
2019-03-13 22:14:40,842 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:15:06,046 : [('reg:1e-05', 88.52), ('reg:0.0001', 88.55), ('reg:0.001', 88.4), ('reg:0.01', 88.42)]
2019-03-13 22:15:06,046 : Validation : best param found is reg = 0.0001 with score             88.55
2019-03-13 22:15:06,046 : Evaluating...
2019-03-13 22:15:11,991 : 
Dev acc : 88.5 Test acc : 87.5 for TENSE classification

2019-03-13 22:15:11,992 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 22:15:12,429 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 22:15:12,493 : loading BERT model bert-large-uncased
2019-03-13 22:15:12,493 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:15:12,520 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:15:12,520 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpz941hqob
2019-03-13 22:15:20,004 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:15:25,189 : Computing embeddings for train/dev/test
2019-03-13 22:18:39,546 : Computed embeddings
2019-03-13 22:18:39,546 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:19:04,787 : [('reg:1e-05', 73.67), ('reg:0.0001', 73.69), ('reg:0.001', 74.04), ('reg:0.01', 70.9)]
2019-03-13 22:19:04,787 : Validation : best param found is reg = 0.001 with score             74.04
2019-03-13 22:19:04,787 : Evaluating...
2019-03-13 22:19:11,339 : 
Dev acc : 74.0 Test acc : 73.1 for SUBJNUMBER classification

2019-03-13 22:19:11,340 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 22:19:11,761 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 22:19:11,829 : loading BERT model bert-large-uncased
2019-03-13 22:19:11,830 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:19:11,954 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:19:11,955 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpqnpo4fbm
2019-03-13 22:19:19,431 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:19:24,702 : Computing embeddings for train/dev/test
2019-03-13 22:22:35,692 : Computed embeddings
2019-03-13 22:22:35,693 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:23:07,555 : [('reg:1e-05', 65.6), ('reg:0.0001', 65.63), ('reg:0.001', 65.23), ('reg:0.01', 64.14)]
2019-03-13 22:23:07,555 : Validation : best param found is reg = 0.0001 with score             65.63
2019-03-13 22:23:07,555 : Evaluating...
2019-03-13 22:23:15,044 : 
Dev acc : 65.6 Test acc : 66.5 for OBJNUMBER classification

2019-03-13 22:23:15,045 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 22:23:15,624 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 22:23:15,693 : loading BERT model bert-large-uncased
2019-03-13 22:23:15,693 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:23:15,719 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:23:15,719 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpo29orr30
2019-03-13 22:23:23,164 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:23:28,353 : Computing embeddings for train/dev/test
2019-03-13 22:27:12,428 : Computed embeddings
2019-03-13 22:27:12,429 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:27:45,520 : [('reg:1e-05', 63.54), ('reg:0.0001', 63.54), ('reg:0.001', 63.53), ('reg:0.01', 62.68)]
2019-03-13 22:27:45,520 : Validation : best param found is reg = 1e-05 with score             63.54
2019-03-13 22:27:45,520 : Evaluating...
2019-03-13 22:27:54,856 : 
Dev acc : 63.5 Test acc : 64.3 for ODDMANOUT classification

2019-03-13 22:27:54,857 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 22:27:55,249 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 22:27:55,325 : loading BERT model bert-large-uncased
2019-03-13 22:27:55,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:27:55,451 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:27:55,451 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpul6mber9
2019-03-13 22:28:02,920 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:28:08,162 : Computing embeddings for train/dev/test
2019-03-13 22:31:50,673 : Computed embeddings
2019-03-13 22:31:50,673 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:32:19,605 : [('reg:1e-05', 61.92), ('reg:0.0001', 61.84), ('reg:0.001', 66.82), ('reg:0.01', 62.59)]
2019-03-13 22:32:19,606 : Validation : best param found is reg = 0.001 with score             66.82
2019-03-13 22:32:19,606 : Evaluating...
2019-03-13 22:32:27,568 : 
Dev acc : 66.8 Test acc : 66.5 for COORDINATIONINVERSION classification

2019-03-13 22:32:27,570 : total results: {'STS12': {'MSRpar': {'pearson': (0.23434815846425094, 8.14591584211148e-11), 'spearman': SpearmanrResult(correlation=0.32454556941489454, pvalue=7.425543466314967e-20), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.019430980896128647, 0.5952095828167796), 'spearman': SpearmanrResult(correlation=0.13313182864103054, pvalue=0.00025601066498332815), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.082691693723606, 0.07675849784802251), 'spearman': SpearmanrResult(correlation=0.4056629650686569, pvalue=1.3069203599707245e-19), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.1190631613965984, 0.0010874449918864724), 'spearman': SpearmanrResult(correlation=0.20451325786788407, pvalue=1.592193628171299e-08), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.39677014198759314, 1.7030684773688315e-16), 'spearman': SpearmanrResult(correlation=0.4878201611820579, pvalue=3.035409961652513e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.16268843493518398, 'wmean': 0.14374260884675846}, 'spearman': {'mean': 0.3111347564349048, 'wmean': 0.2823302886811491}}}, 'STS13': {'FNWN': {'pearson': (-0.022848330150329506, 0.7549876741967013), 'spearman': SpearmanrResult(correlation=0.015907455921402372, pvalue=0.8280101559936457), 'nsamples': 189}, 'headlines': {'pearson': (-0.002340316138836279, 0.9489818661637665), 'spearman': SpearmanrResult(correlation=0.16800463140011182, pvalue=3.7235799167896526e-06), 'nsamples': 750}, 'OnWN': {'pearson': (0.15127798228187764, 0.0003233861173795531), 'spearman': SpearmanrResult(correlation=0.12509751640942196, pvalue=0.002996975661972314), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.04202977866423729, 'wmean': 0.05252891770506258}, 'spearman': {'mean': 0.10300320124364538, 'wmean': 0.13279312628327644}}}, 'STS14': {'deft-forum': {'pearson': (0.01971005107589517, 0.6766850486667184), 'spearman': SpearmanrResult(correlation=0.05595569556747431, pvalue=0.2361651906848145), 'nsamples': 450}, 'deft-news': {'pearson': (0.46225811726050825, 2.7447122063705955e-17), 'spearman': SpearmanrResult(correlation=0.5508058488340793, pvalue=3.3569345706106887e-25), 'nsamples': 300}, 'headlines': {'pearson': (0.09695105217551493, 0.007885190318663023), 'spearman': SpearmanrResult(correlation=0.19194837614008553, pvalue=1.1753469205969428e-07), 'nsamples': 750}, 'images': {'pearson': (0.005119722989055529, 0.8886786020124965), 'spearman': SpearmanrResult(correlation=0.11032001962956309, pvalue=0.002482732175014318), 'nsamples': 750}, 'OnWN': {'pearson': (0.1602045182202487, 1.0405615506103467e-05), 'spearman': SpearmanrResult(correlation=0.19836696040465857, pvalue=4.302366649822267e-08), 'nsamples': 750}, 'tweet-news': {'pearson': (0.18991134477784538, 1.605585727101379e-07), 'spearman': SpearmanrResult(correlation=0.23526675303514608, pvalue=6.841606389313042e-11), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.15569246774984466, 'wmean': 0.129783183142481}, 'spearman': {'mean': 0.22377727560183447, 'wmean': 0.19795957321671392}}}, 'STS15': {'answers-forums': {'pearson': (0.27297617613050706, 7.836393756514412e-08), 'spearman': SpearmanrResult(correlation=0.366365400643603, pvalue=2.348960274551818e-13), 'nsamples': 375}, 'answers-students': {'pearson': (0.13314757888350143, 0.00025557537248299634), 'spearman': SpearmanrResult(correlation=0.17937426969064482, pvalue=7.640677198901761e-07), 'nsamples': 750}, 'belief': {'pearson': (0.0672129921717025, 0.19404767435374828), 'spearman': SpearmanrResult(correlation=0.26500462014425874, pvalue=1.9082005702617817e-07), 'nsamples': 375}, 'headlines': {'pearson': (0.1892609779612743, 1.7724597909065344e-07), 'spearman': SpearmanrResult(correlation=0.3021910188891623, pvalue=2.6594228855610138e-17), 'nsamples': 750}, 'images': {'pearson': (0.011784004652129776, 0.7473078746763091), 'spearman': SpearmanrResult(correlation=0.20109306937263155, pvalue=2.7790550443280686e-08), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.13487634595982304, 'wmean': 0.1260717864120026}, 'spearman': {'mean': 0.2628056757480601, 'wmean': 0.24958584208659237}}}, 'STS16': {'answer-answer': {'pearson': (0.28748854640720045, 3.1942169942442245e-06), 'spearman': SpearmanrResult(correlation=0.5102178124058713, pvalue=3.073882837815971e-18), 'nsamples': 254}, 'headlines': {'pearson': (0.15934199364469412, 0.011808033505862053), 'spearman': SpearmanrResult(correlation=0.332807679823289, pvalue=7.476833292451714e-08), 'nsamples': 249}, 'plagiarism': {'pearson': (0.22822450493979302, 0.00048575598220020326), 'spearman': SpearmanrResult(correlation=0.6208279307254757, pvalue=6.666761544683965e-26), 'nsamples': 230}, 'postediting': {'pearson': (0.3072873217342011, 9.867459393136064e-07), 'spearman': SpearmanrResult(correlation=0.6155709873143115, pvalue=7.720433696001331e-27), 'nsamples': 244}, 'question-question': {'pearson': (0.053873487019957846, 0.4384981597546618), 'spearman': SpearmanrResult(correlation=0.2726359515119749, pvalue=6.504068024078331e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.20724317074916931, 'wmean': 0.21199624673813355}, 'spearman': {'mean': 0.4704120723561845, 'wmean': 0.4742285796497845}}}, 'MR': {'devacc': 80.31, 'acc': 79.95, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 85.6, 'acc': 85.27, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 78.15, 'acc': 79.54, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.77, 'acc': 94.31, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.75, 'acc': 85.28, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.33, 'acc': 41.99, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 60.71, 'acc': 68.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.51, 'acc': 64.93, 'f1': 73.34, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 64.0, 'acc': 60.85, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.3698505235956299, 'pearson': 0.33592021834802843, 'spearman': 0.3696139224924025, 'mse': 0.9444868301836438, 'yhat': array([3.5946584 , 3.55449248, 3.4216726 , ..., 3.0264149 , 4.58731475,        3.38175649]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.3705771174086627, 'pearson': 0.39072035286473583, 'spearman': 0.37889591692985397, 'mse': 2.073606313196394, 'yhat': array([2.96510977, 2.97955589, 2.77672514, ..., 4.13189021, 3.49195055,        3.92646633]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 54.07, 'acc': 54.01, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 54.53, 'acc': 53.6, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 26.87, 'acc': 27.01, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 23.87, 'acc': 23.89, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 47.8, 'acc': 47.19, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.78, 'acc': 85.87, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.55, 'acc': 87.49, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.04, 'acc': 73.07, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 65.63, 'acc': 66.48, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.54, 'acc': 64.29, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.82, 'acc': 66.53, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 22:32:27,570 : STS12 p=0.1437, STS12 s=0.2823, STS13 p=0.0525, STS13 s=0.1328, STS14 p=0.1298, STS14 s=0.1980, STS15 p=0.1261, STS15 s=0.2496, STS 16 p=0.2120, STS16 s=0.4742, STS B p=0.3907, STS B s=0.3789, STS B m=2.0736, SICK-R p=0.3359, SICK-R s=0.3696, SICK-P m=0.9445
2019-03-13 22:32:27,570 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 22:32:27,570 : 0.1437,0.2823,0.0525,0.1328,0.1298,0.1980,0.1261,0.2496,0.2120,0.4742,0.3907,0.3789,2.0736,0.3359,0.3696,0.9445
2019-03-13 22:32:27,570 : MR=79.95, CR=85.27, SUBJ=94.31, MPQA=79.54, SST-B=85.28, SST-F=41.99, TREC=68.60, SICK-E=60.85, SNLI=54.01, MRPC=64.93, MRPC f=73.34
2019-03-13 22:32:27,570 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 22:32:27,570 : 79.95,85.27,94.31,79.54,85.28,41.99,68.60,60.85,54.01,64.93,73.34
2019-03-13 22:32:27,570 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 22:32:27,570 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 22:32:27,570 : na,na,na,na,na,na,na,na,na,na
2019-03-13 22:32:27,571 : SentLen=53.60, WC=27.01, TreeDepth=23.89, TopConst=47.19, BShift=85.87, Tense=87.49, SubjNum=73.07, ObjNum=66.48, SOMO=64.29, CoordInv=66.53, average=59.54
2019-03-13 22:32:27,571 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 22:32:27,571 : 53.60,27.01,23.89,47.19,85.87,87.49,73.07,66.48,64.29,66.53,59.54
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,571 : layer 24
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,571 : ********************************************************************************
2019-03-13 22:32:27,658 : ***** Transfer task : STS12 *****


2019-03-13 22:32:27,670 : loading BERT model bert-large-uncased
2019-03-13 22:32:27,670 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:32:27,687 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:32:27,688 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpf41i2zhv
2019-03-13 22:32:35,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:32:44,379 : MSRpar : pearson = 0.2315, spearman = 0.3011
2019-03-13 22:32:46,035 : MSRvid : pearson = -0.0006, spearman = 0.1094
2019-03-13 22:32:47,460 : SMTeuroparl : pearson = 0.2139, spearman = 0.4451
2019-03-13 22:32:50,187 : surprise.OnWN : pearson = 0.2035, spearman = 0.3138
2019-03-13 22:32:51,630 : surprise.SMTnews : pearson = 0.4675, spearman = 0.5260
2019-03-13 22:32:51,630 : ALL (weighted average) : Pearson = 0.1964,             Spearman = 0.3080
2019-03-13 22:32:51,630 : ALL (average) : Pearson = 0.2231,             Spearman = 0.3391

2019-03-13 22:32:51,630 : ***** Transfer task : STS13 (-SMT) *****


2019-03-13 22:32:51,638 : loading BERT model bert-large-uncased
2019-03-13 22:32:51,638 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:32:51,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:32:51,656 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp69047vil
2019-03-13 22:32:59,078 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:33:05,625 : FNWN : pearson = 0.0357, spearman = 0.1423
2019-03-13 22:33:07,535 : headlines : pearson = 0.1534, spearman = 0.3065
2019-03-13 22:33:09,017 : OnWN : pearson = 0.2250, spearman = 0.2818
2019-03-13 22:33:09,017 : ALL (weighted average) : Pearson = 0.1654,             Spearman = 0.2766
2019-03-13 22:33:09,017 : ALL (average) : Pearson = 0.1381,             Spearman = 0.2436

2019-03-13 22:33:09,018 : ***** Transfer task : STS14 *****


2019-03-13 22:33:09,034 : loading BERT model bert-large-uncased
2019-03-13 22:33:09,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:33:09,052 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:33:09,052 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpkv6pvmug
2019-03-13 22:33:16,467 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:33:23,097 : deft-forum : pearson = 0.0379, spearman = 0.0600
2019-03-13 22:33:24,752 : deft-news : pearson = 0.5696, spearman = 0.5614
2019-03-13 22:33:26,947 : headlines : pearson = 0.2116, spearman = 0.2907
2019-03-13 22:33:29,049 : images : pearson = 0.0502, spearman = 0.1454
2019-03-13 22:33:31,203 : OnWN : pearson = 0.2630, spearman = 0.3663
2019-03-13 22:33:34,100 : tweet-news : pearson = 0.2798, spearman = 0.3274
2019-03-13 22:33:34,100 : ALL (weighted average) : Pearson = 0.2110,             Spearman = 0.2781
2019-03-13 22:33:34,101 : ALL (average) : Pearson = 0.2354,             Spearman = 0.2919

2019-03-13 22:33:34,101 : ***** Transfer task : STS15 *****


2019-03-13 22:33:34,134 : loading BERT model bert-large-uncased
2019-03-13 22:33:34,134 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:33:34,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:33:34,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmplzm9z9rc
2019-03-13 22:33:41,568 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:33:48,748 : answers-forums : pearson = 0.3667, spearman = 0.3905
2019-03-13 22:33:50,856 : answers-students : pearson = 0.2279, spearman = 0.2801
2019-03-13 22:33:52,932 : belief : pearson = 0.1859, spearman = 0.3088
2019-03-13 22:33:55,211 : headlines : pearson = 0.3434, spearman = 0.4320
2019-03-13 22:33:57,374 : images : pearson = 0.0746, spearman = 0.2160
2019-03-13 22:33:57,374 : ALL (weighted average) : Pearson = 0.2306,             Spearman = 0.3194
2019-03-13 22:33:57,374 : ALL (average) : Pearson = 0.2397,             Spearman = 0.3255

2019-03-13 22:33:57,375 : ***** Transfer task : STS16 *****


2019-03-13 22:33:57,443 : loading BERT model bert-large-uncased
2019-03-13 22:33:57,443 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:33:57,461 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:33:57,461 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpm9igewo7
2019-03-13 22:34:04,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:34:10,929 : answer-answer : pearson = 0.3707, spearman = 0.4901
2019-03-13 22:34:11,596 : headlines : pearson = 0.3089, spearman = 0.4568
2019-03-13 22:34:12,488 : plagiarism : pearson = 0.4127, spearman = 0.6184
2019-03-13 22:34:14,005 : postediting : pearson = 0.4952, spearman = 0.6528
2019-03-13 22:34:14,617 : question-question : pearson = 0.1348, spearman = 0.2951
2019-03-13 22:34:14,617 : ALL (weighted average) : Pearson = 0.3499,             Spearman = 0.5071
2019-03-13 22:34:14,617 : ALL (average) : Pearson = 0.3445,             Spearman = 0.5026

2019-03-13 22:34:14,617 : ***** Transfer task : MR *****


2019-03-13 22:34:14,636 : loading BERT model bert-large-uncased
2019-03-13 22:34:14,636 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:34:14,655 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:34:14,655 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp707q0ibx
2019-03-13 22:34:22,083 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:34:27,381 : Generating sentence embeddings
2019-03-13 22:34:59,338 : Generated sentence embeddings
2019-03-13 22:34:59,339 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:35:09,767 : Best param found at split 1: l2reg = 0.0001                 with score 81.18
2019-03-13 22:35:19,106 : Best param found at split 2: l2reg = 1e-05                 with score 80.01
2019-03-13 22:35:27,377 : Best param found at split 3: l2reg = 0.001                 with score 81.37
2019-03-13 22:35:38,782 : Best param found at split 4: l2reg = 1e-05                 with score 81.76
2019-03-13 22:35:50,534 : Best param found at split 5: l2reg = 1e-05                 with score 81.14
2019-03-13 22:35:51,150 : Dev acc : 81.09 Test acc : 79.14

2019-03-13 22:35:51,152 : ***** Transfer task : CR *****


2019-03-13 22:35:51,160 : loading BERT model bert-large-uncased
2019-03-13 22:35:51,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:35:51,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:35:51,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmptlcpd3iz
2019-03-13 22:35:58,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:36:03,812 : Generating sentence embeddings
2019-03-13 22:36:12,219 : Generated sentence embeddings
2019-03-13 22:36:12,220 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:36:16,146 : Best param found at split 1: l2reg = 1e-05                 with score 84.96
2019-03-13 22:36:19,880 : Best param found at split 2: l2reg = 0.0001                 with score 84.2
2019-03-13 22:36:23,849 : Best param found at split 3: l2reg = 1e-05                 with score 85.43
2019-03-13 22:36:27,780 : Best param found at split 4: l2reg = 0.01                 with score 85.3
2019-03-13 22:36:32,197 : Best param found at split 5: l2reg = 1e-05                 with score 85.01
2019-03-13 22:36:32,383 : Dev acc : 84.98 Test acc : 83.76

2019-03-13 22:36:32,384 : ***** Transfer task : MPQA *****


2019-03-13 22:36:32,390 : loading BERT model bert-large-uncased
2019-03-13 22:36:32,390 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:36:32,440 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:36:32,440 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpledzp243
2019-03-13 22:36:39,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:36:45,143 : Generating sentence embeddings
2019-03-13 22:36:52,782 : Generated sentence embeddings
2019-03-13 22:36:52,782 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:37:02,803 : Best param found at split 1: l2reg = 0.0001                 with score 83.32
2019-03-13 22:37:12,947 : Best param found at split 2: l2reg = 0.001                 with score 81.85
2019-03-13 22:37:23,829 : Best param found at split 3: l2reg = 0.0001                 with score 83.35
2019-03-13 22:37:34,435 : Best param found at split 4: l2reg = 0.01                 with score 81.45
2019-03-13 22:37:44,815 : Best param found at split 5: l2reg = 0.01                 with score 81.66
2019-03-13 22:37:45,361 : Dev acc : 82.33 Test acc : 82.24

2019-03-13 22:37:45,362 : ***** Transfer task : SUBJ *****


2019-03-13 22:37:45,376 : loading BERT model bert-large-uncased
2019-03-13 22:37:45,377 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:37:45,397 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:37:45,397 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpzgsucbde
2019-03-13 22:37:52,800 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:37:58,029 : Generating sentence embeddings
2019-03-13 22:38:29,266 : Generated sentence embeddings
2019-03-13 22:38:29,266 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-03-13 22:38:39,298 : Best param found at split 1: l2reg = 0.001                 with score 95.44
2019-03-13 22:38:49,858 : Best param found at split 2: l2reg = 0.001                 with score 95.65
2019-03-13 22:39:00,377 : Best param found at split 3: l2reg = 0.0001                 with score 95.11
2019-03-13 22:39:10,374 : Best param found at split 4: l2reg = 0.001                 with score 95.51
2019-03-13 22:39:20,064 : Best param found at split 5: l2reg = 0.0001                 with score 94.99
2019-03-13 22:39:20,555 : Dev acc : 95.34 Test acc : 94.67

2019-03-13 22:39:20,557 : ***** Transfer task : SST Binary classification *****


2019-03-13 22:39:20,647 : loading BERT model bert-large-uncased
2019-03-13 22:39:20,648 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:39:20,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:39:20,722 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4isxlcx3
2019-03-13 22:39:28,137 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:39:33,344 : Computing embedding for train
2019-03-13 22:41:15,615 : Computed train embeddings
2019-03-13 22:41:15,615 : Computing embedding for dev
2019-03-13 22:41:17,912 : Computed dev embeddings
2019-03-13 22:41:17,912 : Computing embedding for test
2019-03-13 22:41:22,740 : Computed test embeddings
2019-03-13 22:41:22,740 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:41:43,468 : [('reg:1e-05', 84.63), ('reg:0.0001', 82.91), ('reg:0.001', 82.8), ('reg:0.01', 81.31)]
2019-03-13 22:41:43,468 : Validation : best param found is reg = 1e-05 with score             84.63
2019-03-13 22:41:43,469 : Evaluating...
2019-03-13 22:41:49,940 : 
Dev acc : 84.63 Test acc : 83.53 for             SST Binary classification

2019-03-13 22:41:49,940 : ***** Transfer task : SST Fine-Grained classification *****


2019-03-13 22:41:49,990 : loading BERT model bert-large-uncased
2019-03-13 22:41:49,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:41:50,012 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:41:50,012 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpq1uzs5t0
2019-03-13 22:41:57,461 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:42:02,776 : Computing embedding for train
2019-03-13 22:42:24,932 : Computed train embeddings
2019-03-13 22:42:24,932 : Computing embedding for dev
2019-03-13 22:42:27,834 : Computed dev embeddings
2019-03-13 22:42:27,834 : Computing embedding for test
2019-03-13 22:42:33,555 : Computed test embeddings
2019-03-13 22:42:33,556 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:42:36,053 : [('reg:1e-05', 42.33), ('reg:0.0001', 42.96), ('reg:0.001', 43.23), ('reg:0.01', 42.78)]
2019-03-13 22:42:36,053 : Validation : best param found is reg = 0.001 with score             43.23
2019-03-13 22:42:36,053 : Evaluating...
2019-03-13 22:42:36,690 : 
Dev acc : 43.23 Test acc : 42.85 for             SST Fine-Grained classification

2019-03-13 22:42:36,691 : ***** Transfer task : TREC *****


2019-03-13 22:42:36,703 : loading BERT model bert-large-uncased
2019-03-13 22:42:36,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:42:36,722 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:42:36,723 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp_3uyggvn
2019-03-13 22:42:44,120 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:42:57,043 : Computed train embeddings
2019-03-13 22:42:57,639 : Computed test embeddings
2019-03-13 22:42:57,639 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:43:04,475 : [('reg:1e-05', 71.22), ('reg:0.0001', 71.39), ('reg:0.001', 69.86), ('reg:0.01', 59.23)]
2019-03-13 22:43:04,475 : Cross-validation : best param found is reg = 0.0001             with score 71.39
2019-03-13 22:43:04,475 : Evaluating...
2019-03-13 22:43:04,868 : 
Dev acc : 71.39 Test acc : 82.2             for TREC

2019-03-13 22:43:04,869 : ***** Transfer task : MRPC *****


2019-03-13 22:43:04,891 : loading BERT model bert-large-uncased
2019-03-13 22:43:04,892 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:43:04,912 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:43:04,912 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpx_nxlw8g
2019-03-13 22:43:12,347 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:43:17,497 : Computing embedding for train
2019-03-13 22:43:40,049 : Computed train embeddings
2019-03-13 22:43:40,049 : Computing embedding for test
2019-03-13 22:43:49,944 : Computed test embeddings
2019-03-13 22:43:49,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-03-13 22:43:54,543 : [('reg:1e-05', 69.5), ('reg:0.0001', 69.21), ('reg:0.001', 69.5), ('reg:0.01', 69.36)]
2019-03-13 22:43:54,544 : Cross-validation : best param found is reg = 1e-05             with score 69.5
2019-03-13 22:43:54,544 : Evaluating...
2019-03-13 22:43:54,852 : Dev acc : 69.5 Test acc 67.3; Test F1 78.19 for MRPC.

2019-03-13 22:43:54,852 : ***** Transfer task : SICK-Entailment*****


2019-03-13 22:43:54,913 : loading BERT model bert-large-uncased
2019-03-13 22:43:54,913 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:43:54,933 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:43:54,933 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpku5yfd5t
2019-03-13 22:44:02,356 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:44:07,626 : Computing embedding for train
2019-03-13 22:44:19,069 : Computed train embeddings
2019-03-13 22:44:19,069 : Computing embedding for dev
2019-03-13 22:44:20,629 : Computed dev embeddings
2019-03-13 22:44:20,629 : Computing embedding for test
2019-03-13 22:44:32,913 : Computed test embeddings
2019-03-13 22:44:32,950 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 22:44:34,369 : [('reg:1e-05', 70.4), ('reg:0.0001', 63.6), ('reg:0.001', 69.2), ('reg:0.01', 64.6)]
2019-03-13 22:44:34,370 : Validation : best param found is reg = 1e-05 with score             70.4
2019-03-13 22:44:34,370 : Evaluating...
2019-03-13 22:44:34,721 : 
Dev acc : 70.4 Test acc : 65.7 for                        SICK entailment

2019-03-13 22:44:34,721 : ***** Transfer task : SICK-Relatedness*****


2019-03-13 22:44:34,748 : loading BERT model bert-large-uncased
2019-03-13 22:44:34,748 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:44:34,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:44:34,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpk9rugosr
2019-03-13 22:44:42,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:44:47,342 : Computing embedding for train
2019-03-13 22:44:58,788 : Computed train embeddings
2019-03-13 22:44:58,789 : Computing embedding for dev
2019-03-13 22:45:00,355 : Computed dev embeddings
2019-03-13 22:45:00,355 : Computing embedding for test
2019-03-13 22:45:12,668 : Computed test embeddings
2019-03-13 22:45:25,569 : Dev : Pearson 0.6214560271989338
2019-03-13 22:45:25,578 : Test : Pearson 0.6350883533349884 Spearman 0.5688710032923001 MSE 0.6153755022886268                        for SICK Relatedness

2019-03-13 22:45:25,579 : 

***** Transfer task : STSBenchmark*****


2019-03-13 22:45:25,628 : loading BERT model bert-large-uncased
2019-03-13 22:45:25,629 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:45:25,647 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:45:25,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp0qo50ces
2019-03-13 22:45:33,008 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:45:38,250 : Computing embedding for train
2019-03-13 22:45:57,090 : Computed train embeddings
2019-03-13 22:45:57,090 : Computing embedding for dev
2019-03-13 22:46:02,824 : Computed dev embeddings
2019-03-13 22:46:02,824 : Computing embedding for test
2019-03-13 22:46:07,508 : Computed test embeddings
2019-03-13 22:46:26,063 : Dev : Pearson 0.4389963559035228
2019-03-13 22:46:26,063 : Test : Pearson 0.4176433557403998 Spearman 0.4227623941550932 MSE 1.9749487576884428                        for SICK Relatedness

2019-03-13 22:46:26,063 : ***** Transfer task : SNLI Entailment*****


2019-03-13 22:46:31,201 : loading BERT model bert-large-uncased
2019-03-13 22:46:31,201 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 22:46:31,274 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 22:46:31,274 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppe3gbkfm
2019-03-13 22:46:38,731 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 22:46:44,389 : PROGRESS (encoding): 0.00%
2019-03-13 22:49:35,160 : PROGRESS (encoding): 14.56%
2019-03-13 22:52:48,520 : PROGRESS (encoding): 29.12%
2019-03-13 22:56:01,148 : PROGRESS (encoding): 43.69%
2019-03-13 22:59:25,262 : PROGRESS (encoding): 58.25%
2019-03-13 23:03:11,234 : PROGRESS (encoding): 72.81%
2019-03-13 23:06:56,048 : PROGRESS (encoding): 87.37%
2019-03-13 23:10:59,365 : PROGRESS (encoding): 0.00%
2019-03-13 23:11:29,952 : PROGRESS (encoding): 0.00%
2019-03-13 23:11:59,359 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:12:26,647 : [('reg:1e-09', 52.29)]
2019-03-13 23:12:26,647 : Validation : best param found is reg = 1e-09 with score             52.29
2019-03-13 23:12:26,647 : Evaluating...
2019-03-13 23:12:51,490 : Dev acc : 52.29 Test acc : 52.9 for SNLI

2019-03-13 23:12:51,490 : ***** (Probing) Transfer task : LENGTH classification *****
2019-03-13 23:12:51,698 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-03-13 23:12:52,760 : loading BERT model bert-large-uncased
2019-03-13 23:12:52,761 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:12:52,787 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:12:52,787 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp6oywrrca
2019-03-13 23:13:00,214 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:13:05,497 : Computing embeddings for train/dev/test
2019-03-13 23:16:38,869 : Computed embeddings
2019-03-13 23:16:38,870 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:17:08,315 : [('reg:1e-05', 55.96), ('reg:0.0001', 55.4), ('reg:0.001', 51.03), ('reg:0.01', 42.4)]
2019-03-13 23:17:08,315 : Validation : best param found is reg = 1e-05 with score             55.96
2019-03-13 23:17:08,315 : Evaluating...
2019-03-13 23:17:17,063 : 
Dev acc : 56.0 Test acc : 56.1 for LENGTH classification

2019-03-13 23:17:17,064 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-03-13 23:17:17,316 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-03-13 23:17:17,363 : loading BERT model bert-large-uncased
2019-03-13 23:17:17,363 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:17:17,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:17:17,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpxv_dzd3r
2019-03-13 23:17:24,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:17:29,971 : Computing embeddings for train/dev/test
2019-03-13 23:20:46,633 : Computed embeddings
2019-03-13 23:20:46,633 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:21:21,233 : [('reg:1e-05', 23.56), ('reg:0.0001', 9.08), ('reg:0.001', 1.83), ('reg:0.01', 0.52)]
2019-03-13 23:21:21,233 : Validation : best param found is reg = 1e-05 with score             23.56
2019-03-13 23:21:21,233 : Evaluating...
2019-03-13 23:21:29,578 : 
Dev acc : 23.6 Test acc : 23.7 for WORDCONTENT classification

2019-03-13 23:21:29,579 : ***** (Probing) Transfer task : DEPTH classification *****
2019-03-13 23:21:30,112 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-03-13 23:21:30,177 : loading BERT model bert-large-uncased
2019-03-13 23:21:30,177 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:21:30,201 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:21:30,201 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmppm6zb56k
2019-03-13 23:21:37,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:21:42,998 : Computing embeddings for train/dev/test
2019-03-13 23:24:48,505 : Computed embeddings
2019-03-13 23:24:48,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:25:09,533 : [('reg:1e-05', 24.31), ('reg:0.0001', 24.29), ('reg:0.001', 23.78), ('reg:0.01', 21.42)]
2019-03-13 23:25:09,533 : Validation : best param found is reg = 1e-05 with score             24.31
2019-03-13 23:25:09,533 : Evaluating...
2019-03-13 23:25:14,889 : 
Dev acc : 24.3 Test acc : 24.8 for DEPTH classification

2019-03-13 23:25:14,890 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-03-13 23:25:15,263 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-03-13 23:25:15,327 : loading BERT model bert-large-uncased
2019-03-13 23:25:15,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:25:15,439 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:25:15,439 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpuxgar3hz
2019-03-13 23:25:22,867 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:25:28,100 : Computing embeddings for train/dev/test
2019-03-13 23:28:19,134 : Computed embeddings
2019-03-13 23:28:19,134 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:28:50,648 : [('reg:1e-05', 50.61), ('reg:0.0001', 49.59), ('reg:0.001', 42.6), ('reg:0.01', 25.44)]
2019-03-13 23:28:50,648 : Validation : best param found is reg = 1e-05 with score             50.61
2019-03-13 23:28:50,648 : Evaluating...
2019-03-13 23:28:59,337 : 
Dev acc : 50.6 Test acc : 50.8 for TOPCONSTITUENTS classification

2019-03-13 23:28:59,338 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-03-13 23:28:59,711 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-03-13 23:28:59,777 : loading BERT model bert-large-uncased
2019-03-13 23:28:59,777 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:28:59,806 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:28:59,806 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmprrq06grt
2019-03-13 23:29:07,069 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:29:12,347 : Computing embeddings for train/dev/test
2019-03-13 23:32:14,721 : Computed embeddings
2019-03-13 23:32:14,721 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:33:06,790 : [('reg:1e-05', 87.26), ('reg:0.0001', 87.16), ('reg:0.001', 87.09), ('reg:0.01', 84.71)]
2019-03-13 23:33:06,791 : Validation : best param found is reg = 1e-05 with score             87.26
2019-03-13 23:33:06,791 : Evaluating...
2019-03-13 23:33:19,750 : 
Dev acc : 87.3 Test acc : 86.8 for BIGRAMSHIFT classification

2019-03-13 23:33:19,751 : ***** (Probing) Transfer task : TENSE classification *****
2019-03-13 23:33:20,137 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-03-13 23:33:20,201 : loading BERT model bert-large-uncased
2019-03-13 23:33:20,202 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:33:20,230 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:33:20,231 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpbxkzam1u
2019-03-13 23:33:27,351 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:33:32,474 : Computing embeddings for train/dev/test
2019-03-13 23:36:29,354 : Computed embeddings
2019-03-13 23:36:29,354 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:37:06,221 : [('reg:1e-05', 88.69), ('reg:0.0001', 88.76), ('reg:0.001', 88.79), ('reg:0.01', 88.67)]
2019-03-13 23:37:06,222 : Validation : best param found is reg = 0.001 with score             88.79
2019-03-13 23:37:06,222 : Evaluating...
2019-03-13 23:37:17,307 : 
Dev acc : 88.8 Test acc : 87.3 for TENSE classification

2019-03-13 23:37:17,308 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-03-13 23:37:17,707 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-03-13 23:37:17,767 : loading BERT model bert-large-uncased
2019-03-13 23:37:17,767 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:37:17,878 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:37:17,878 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp9bf4d9tt
2019-03-13 23:37:25,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:37:30,151 : Computing embeddings for train/dev/test
2019-03-13 23:40:37,472 : Computed embeddings
2019-03-13 23:40:37,473 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:41:19,533 : [('reg:1e-05', 73.15), ('reg:0.0001', 73.07), ('reg:0.001', 73.0), ('reg:0.01', 71.55)]
2019-03-13 23:41:19,533 : Validation : best param found is reg = 1e-05 with score             73.15
2019-03-13 23:41:19,533 : Evaluating...
2019-03-13 23:41:27,307 : 
Dev acc : 73.2 Test acc : 71.8 for SUBJNUMBER classification

2019-03-13 23:41:27,308 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-03-13 23:41:27,699 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-03-13 23:41:27,765 : loading BERT model bert-large-uncased
2019-03-13 23:41:27,766 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:41:27,876 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:41:27,876 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmp4smyrmpk
2019-03-13 23:41:35,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:41:40,240 : Computing embeddings for train/dev/test
2019-03-13 23:44:44,162 : Computed embeddings
2019-03-13 23:44:44,162 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:45:32,373 : [('reg:1e-05', 67.13), ('reg:0.0001', 66.96), ('reg:0.001', 66.27), ('reg:0.01', 61.65)]
2019-03-13 23:45:32,373 : Validation : best param found is reg = 1e-05 with score             67.13
2019-03-13 23:45:32,373 : Evaluating...
2019-03-13 23:45:43,323 : 
Dev acc : 67.1 Test acc : 68.5 for OBJNUMBER classification

2019-03-13 23:45:43,325 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-03-13 23:45:43,881 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-03-13 23:45:43,949 : loading BERT model bert-large-uncased
2019-03-13 23:45:43,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:45:43,976 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:45:43,976 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpw81hrhvy
2019-03-13 23:45:51,215 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:45:56,361 : Computing embeddings for train/dev/test
2019-03-13 23:49:29,324 : Computed embeddings
2019-03-13 23:49:29,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:50:27,887 : [('reg:1e-05', 63.71), ('reg:0.0001', 63.76), ('reg:0.001', 63.6), ('reg:0.01', 62.67)]
2019-03-13 23:50:27,887 : Validation : best param found is reg = 0.0001 with score             63.76
2019-03-13 23:50:27,887 : Evaluating...
2019-03-13 23:50:41,337 : 
Dev acc : 63.8 Test acc : 64.0 for ODDMANOUT classification

2019-03-13 23:50:41,338 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-03-13 23:50:41,705 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-03-13 23:50:41,785 : loading BERT model bert-large-uncased
2019-03-13 23:50:41,785 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased-vocab.txt
2019-03-13 23:50:41,814 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz
2019-03-13 23:50:41,814 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-large-uncased.tar.gz to temp dir /tmp/tmpb_uitos_
2019-03-13 23:50:49,035 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-03-13 23:50:54,242 : Computing embeddings for train/dev/test
2019-03-13 23:54:24,827 : Computed embeddings
2019-03-13 23:54:24,827 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-03-13 23:55:13,730 : [('reg:1e-05', 62.93), ('reg:0.0001', 66.29), ('reg:0.001', 62.31), ('reg:0.01', 63.59)]
2019-03-13 23:55:13,730 : Validation : best param found is reg = 0.0001 with score             66.29
2019-03-13 23:55:13,730 : Evaluating...
2019-03-13 23:55:26,641 : 
Dev acc : 66.3 Test acc : 66.4 for COORDINATIONINVERSION classification

2019-03-13 23:55:26,643 : total results: {'STS12': {'MSRpar': {'pearson': (0.2314523107355443, 1.4052765887018652e-10), 'spearman': SpearmanrResult(correlation=0.301084878477451, pvalue=3.512151827404391e-17), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.000614693941259611, 0.9865913751967422), 'spearman': SpearmanrResult(correlation=0.1094494398420883, pvalue=0.0026872531124990232), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.21387948759541087, 3.7763446282102614e-06), 'spearman': SpearmanrResult(correlation=0.4450806616804578, pvalue=1.0266477353031842e-23), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.2035154397886862, 1.8750073035190724e-08), 'spearman': SpearmanrResult(correlation=0.31378134057497103, pvalue=1.3417953313895316e-18), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.46748659835887574, 4.680296427034659e-23), 'spearman': SpearmanrResult(correlation=0.5259956257677655, pvalue=9.049250968972997e-30), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.22314382850745149, 'wmean': 0.19641654761541608}, 'spearman': {'mean': 0.3390783892685468, 'wmean': 0.3080440870539097}}}, 'STS13': {'FNWN': {'pearson': (0.035716373073604916, 0.6256084434647748), 'spearman': SpearmanrResult(correlation=0.14233676171218185, pvalue=0.05072666287240029), 'nsamples': 189}, 'headlines': {'pearson': (0.15343739507296922, 2.4423610720839883e-05), 'spearman': SpearmanrResult(correlation=0.3064833282100324, pvalue=8.936166286690982e-18), 'nsamples': 750}, 'OnWN': {'pearson': (0.2250490864217591, 7.134040123003708e-08), 'spearman': SpearmanrResult(correlation=0.2818422664380167, pvalue=1.053648488107858e-11), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.13806761818944444, 'wmean': 0.16538731886549674}, 'spearman': {'mean': 0.24355411878674363, 'wmean': 0.2765851037285693}}}, 'STS14': {'deft-forum': {'pearson': (0.03793366992407405, 0.4221218560441884), 'spearman': SpearmanrResult(correlation=0.06003245006517801, pvalue=0.203698328003488), 'nsamples': 450}, 'deft-news': {'pearson': (0.5696116231994525, 3.343359996649691e-27), 'spearman': SpearmanrResult(correlation=0.5614248065536895, pvalue=2.5780481872248884e-26), 'nsamples': 300}, 'headlines': {'pearson': (0.21158810170194006, 4.877828825104389e-09), 'spearman': SpearmanrResult(correlation=0.29066831547264915, pvalue=4.546970527539122e-16), 'nsamples': 750}, 'images': {'pearson': (0.0501612542079352, 0.16997156454844722), 'spearman': SpearmanrResult(correlation=0.14536054464305637, pvalue=6.456814658142696e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.2630055053684044, 2.4808265017354435e-13), 'spearman': SpearmanrResult(correlation=0.36628447537717573, pvalue=3.149452941537391e-25), 'nsamples': 750}, 'tweet-news': {'pearson': (0.2798469409321576, 5.825596819680768e-15), 'spearman': SpearmanrResult(correlation=0.3274429600883225, pvalue=3.339183669759262e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2353578492223273, 'wmean': 0.21104133068893255}, 'spearman': {'mean': 0.2918689253666786, 'wmean': 0.2780691376483573}}}, 'STS15': {'answers-forums': {'pearson': (0.36671414964050725, 2.2210394184025442e-13), 'spearman': SpearmanrResult(correlation=0.3904766072957927, pvalue=4.1563931507021e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.22794702349140003, 2.692884660222442e-10), 'spearman': SpearmanrResult(correlation=0.2801129049534196, pvalue=5.4789005818653924e-15), 'nsamples': 750}, 'belief': {'pearson': (0.18591183191194474, 0.0002948831639797435), 'spearman': SpearmanrResult(correlation=0.3088218669985666, pvalue=9.941984741490403e-10), 'nsamples': 375}, 'headlines': {'pearson': (0.34340690440674254, 3.493498540269412e-22), 'spearman': SpearmanrResult(correlation=0.43196783178188597, pvalue=1.9117019445228103e-35), 'nsamples': 750}, 'images': {'pearson': (0.07464466877093706, 0.040986111041123076), 'spearman': SpearmanrResult(correlation=0.21599437384373288, pvalue=2.2859156962865176e-09), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.23972491564430629, 'wmean': 0.23057789686132643}, 'spearman': {'mean': 0.32547471697467956, 'wmean': 0.31943108693155453}}}, 'STS16': {'answer-answer': {'pearson': (0.37070354429966607, 1.0771858893891544e-09), 'spearman': SpearmanrResult(correlation=0.4900519363506385, pvalue=9.483332825803805e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.3088898209246454, 6.633815790472863e-07), 'spearman': SpearmanrResult(correlation=0.4568323039692012, pvalue=3.050959227214024e-14), 'nsamples': 249}, 'plagiarism': {'pearson': (0.4126662912071717, 7.170187247455321e-11), 'spearman': SpearmanrResult(correlation=0.6183696080256567, pvalue=1.1759958782105926e-25), 'nsamples': 230}, 'postediting': {'pearson': (0.4952441674134076, 1.6690051753476549e-16), 'spearman': SpearmanrResult(correlation=0.65276239425804, pvalue=5.149457552999339e-31), 'nsamples': 244}, 'question-question': {'pearson': (0.13479259992272732, 0.05166956414034989), 'spearman': SpearmanrResult(correlation=0.29511084541395693, pvalue=1.439274953089487e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.3444592847535236, 'wmean': 0.34991293665490963}, 'spearman': {'mean': 0.5026254176034987, 'wmean': 0.5070840103353904}}}, 'MR': {'devacc': 81.09, 'acc': 79.14, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 84.98, 'acc': 83.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.33, 'acc': 82.24, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 95.34, 'acc': 94.67, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.63, 'acc': 83.53, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.23, 'acc': 42.85, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 71.39, 'acc': 82.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.5, 'acc': 67.3, 'f1': 78.19, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 70.4, 'acc': 65.7, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6214560271989338, 'pearson': 0.6350883533349884, 'spearman': 0.5688710032923001, 'mse': 0.6153755022886268, 'yhat': array([3.6398573 , 4.12356864, 1.91248262, ..., 3.22534575, 4.25042431,        4.48328465]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.4389963559035228, 'pearson': 0.4176433557403998, 'spearman': 0.4227623941550932, 'mse': 1.9749487576884428, 'yhat': array([3.32874242, 1.42118887, 2.49397695, ..., 3.80269592, 3.1019336 ,        3.71389079]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 52.29, 'acc': 52.9, 'ndev': 9842, 'ntest': 9824}, 'Length': {'devacc': 55.96, 'acc': 56.09, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 23.56, 'acc': 23.71, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 24.31, 'acc': 24.82, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 50.61, 'acc': 50.8, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.26, 'acc': 86.75, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 88.79, 'acc': 87.29, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.15, 'acc': 71.81, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 67.13, 'acc': 68.52, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.76, 'acc': 64.03, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 66.29, 'acc': 66.36, 'ndev': 10002, 'ntest': 10002}}
2019-03-13 23:55:26,643 : STS12 p=0.1964, STS12 s=0.3080, STS13 p=0.1654, STS13 s=0.2766, STS14 p=0.2110, STS14 s=0.2781, STS15 p=0.2306, STS15 s=0.3194, STS 16 p=0.3499, STS16 s=0.5071, STS B p=0.4176, STS B s=0.4228, STS B m=1.9749, SICK-R p=0.6351, SICK-R s=0.5689, SICK-P m=0.6154
2019-03-13 23:55:26,643 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-03-13 23:55:26,643 : 0.1964,0.3080,0.1654,0.2766,0.2110,0.2781,0.2306,0.3194,0.3499,0.5071,0.4176,0.4228,1.9749,0.6351,0.5689,0.6154
2019-03-13 23:55:26,643 : MR=79.14, CR=83.76, SUBJ=94.67, MPQA=82.24, SST-B=83.53, SST-F=42.85, TREC=82.20, SICK-E=65.70, SNLI=52.90, MRPC=67.30, MRPC f=78.19
2019-03-13 23:55:26,643 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-03-13 23:55:26,643 : 79.14,83.76,94.67,82.24,83.53,42.85,82.20,65.70,52.90,67.30,78.19
2019-03-13 23:55:26,643 : COCO r1i2t=na, COCO r5i2t=na, COCO r10i2t=na, COCO medr_i2t=na, COCO r1t2i=na, COCO r5t2i=na, COCO r10t2i=na, COCO medr_t2i=na
2019-03-13 23:55:26,644 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-03-13 23:55:26,644 : na,na,na,na,na,na,na,na,na,na
2019-03-13 23:55:26,644 : SentLen=56.09, WC=23.71, TreeDepth=24.82, TopConst=50.80, BShift=86.75, Tense=87.29, SubjNum=71.81, ObjNum=68.52, SOMO=64.03, CoordInv=66.36, average=60.02
2019-03-13 23:55:26,644 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-03-13 23:55:26,644 : 56.09,23.71,24.82,50.80,86.75,87.29,71.81,68.52,64.03,66.36,60.02
