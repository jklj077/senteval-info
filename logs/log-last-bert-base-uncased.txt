2019-02-13 16:54:49,895 : ********************************************************************************
2019-02-13 16:54:49,895 : ********************************************************************************
2019-02-13 16:54:49,895 : ********************************************************************************
2019-02-13 16:54:49,895 : layer 0
2019-02-13 16:54:49,896 : ********************************************************************************
2019-02-13 16:54:49,896 : ********************************************************************************
2019-02-13 16:54:49,896 : ********************************************************************************
2019-02-13 16:54:49,896 : ***** Transfer task : STS12 *****


2019-02-13 16:54:49,931 : loading BERT model bert-base-uncased
2019-02-13 16:54:49,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:54:49,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:54:49,950 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk5u7s7r8
2019-02-13 16:54:52,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:54:58,141 : MSRpar : pearson = 0.0652, spearman = 0.0776
2019-02-13 16:54:58,788 : MSRvid : pearson = 0.0080, spearman = 0.0099
2019-02-13 16:54:59,342 : SMTeuroparl : pearson = 0.2837, spearman = 0.3244
2019-02-13 16:55:00,326 : surprise.OnWN : pearson = 0.2765, spearman = 0.3659
2019-02-13 16:55:00,877 : surprise.SMTnews : pearson = 0.3352, spearman = 0.3065
2019-02-13 16:55:00,877 : ALL (weighted average) : Pearson = 0.1693,             Spearman = 0.1966
2019-02-13 16:55:00,877 : ALL (average) : Pearson = 0.1937,             Spearman = 0.2168

2019-02-13 16:55:00,877 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 16:55:00,886 : loading BERT model bert-base-uncased
2019-02-13 16:55:00,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:00,905 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:00,905 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjhce6kfp
2019-02-13 16:55:03,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:05,426 : FNWN : pearson = 0.0776, spearman = 0.1127
2019-02-13 16:55:06,189 : headlines : pearson = -0.0248, spearman = -0.0307
2019-02-13 16:55:06,757 : OnWN : pearson = -0.2764, spearman = -0.2428
2019-02-13 16:55:06,757 : ALL (weighted average) : Pearson = -0.1060,             Spearman = -0.0920
2019-02-13 16:55:06,757 : ALL (average) : Pearson = -0.0745,             Spearman = -0.0536

2019-02-13 16:55:06,757 : ***** Transfer task : STS14 *****


2019-02-13 16:55:06,776 : loading BERT model bert-base-uncased
2019-02-13 16:55:06,776 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:06,828 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:06,829 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprv4170n8
2019-02-13 16:55:09,279 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:11,439 : deft-forum : pearson = -0.1768, spearman = -0.1675
2019-02-13 16:55:12,091 : deft-news : pearson = -0.1012, spearman = -0.0866
2019-02-13 16:55:13,009 : headlines : pearson = 0.0041, spearman = -0.0063
2019-02-13 16:55:13,899 : images : pearson = 0.0633, spearman = 0.0772
2019-02-13 16:55:15,807 : OnWN : pearson = -0.1909, spearman = -0.1769
2019-02-13 16:55:17,379 : tweet-news : pearson = 0.0134, spearman = 0.0522
2019-02-13 16:55:17,379 : ALL (weighted average) : Pearson = -0.0513,             Spearman = -0.0378
2019-02-13 16:55:17,379 : ALL (average) : Pearson = -0.0647,             Spearman = -0.0513

2019-02-13 16:55:17,379 : ***** Transfer task : STS15 *****


2019-02-13 16:55:17,412 : loading BERT model bert-base-uncased
2019-02-13 16:55:17,412 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:17,431 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:17,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxlnrfnjq
2019-02-13 16:55:19,870 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:22,050 : answers-forums : pearson = -0.0802, spearman = -0.0910
2019-02-13 16:55:22,861 : answers-students : pearson = 0.1966, spearman = 0.2159
2019-02-13 16:55:23,607 : belief : pearson = -0.0877, spearman = -0.1251
2019-02-13 16:55:25,145 : headlines : pearson = 0.0679, spearman = 0.0681
2019-02-13 16:55:26,506 : images : pearson = 0.0890, spearman = 0.0956
2019-02-13 16:55:26,507 : ALL (weighted average) : Pearson = 0.0674,             Spearman = 0.0679
2019-02-13 16:55:26,507 : ALL (average) : Pearson = 0.0371,             Spearman = 0.0327

2019-02-13 16:55:26,507 : ***** Transfer task : STS16 *****


2019-02-13 16:55:26,576 : loading BERT model bert-base-uncased
2019-02-13 16:55:26,577 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:26,594 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:26,594 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9yljq_i5
2019-02-13 16:55:29,052 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:30,925 : answer-answer : pearson = 0.1554, spearman = 0.1604
2019-02-13 16:55:31,416 : headlines : pearson = 0.1425, spearman = 0.1453
2019-02-13 16:55:31,932 : plagiarism : pearson = 0.0703, spearman = 0.0969
2019-02-13 16:55:32,863 : postediting : pearson = 0.2851, spearman = 0.2548
2019-02-13 16:55:33,242 : question-question : pearson = -0.0288, spearman = -0.0308
2019-02-13 16:55:33,242 : ALL (weighted average) : Pearson = 0.1304,             Spearman = 0.1307
2019-02-13 16:55:33,242 : ALL (average) : Pearson = 0.1249,             Spearman = 0.1253

2019-02-13 16:55:33,242 : ***** Transfer task : MR *****


2019-02-13 16:55:33,261 : loading BERT model bert-base-uncased
2019-02-13 16:55:33,261 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:55:33,280 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:55:33,280 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprpgge7fx
2019-02-13 16:55:35,749 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:55:37,216 : Generating sentence embeddings
2019-02-13 16:55:51,179 : Generated sentence embeddings
2019-02-13 16:55:51,180 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:56:00,609 : Best param found at split 1: l2reg = 0.01                 with score 50.09
2019-02-13 16:56:17,937 : Best param found at split 2: l2reg = 1e-05                 with score 50.01
2019-02-13 16:56:28,645 : Best param found at split 3: l2reg = 1e-05                 with score 50.13
2019-02-13 16:56:39,136 : Best param found at split 4: l2reg = 1e-05                 with score 50.05
2019-02-13 16:56:48,961 : Best param found at split 5: l2reg = 0.01                 with score 50.07
2019-02-13 16:56:49,488 : Dev acc : 50.07 Test acc : 50.18

2019-02-13 16:56:49,489 : ***** Transfer task : CR *****


2019-02-13 16:56:49,497 : loading BERT model bert-base-uncased
2019-02-13 16:56:49,497 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:56:49,517 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:56:49,517 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphrz1yuqp
2019-02-13 16:56:51,964 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:56:53,427 : Generating sentence embeddings
2019-02-13 16:56:57,500 : Generated sentence embeddings
2019-02-13 16:56:57,500 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:57:00,474 : Best param found at split 1: l2reg = 0.01                 with score 63.8
2019-02-13 16:57:04,520 : Best param found at split 2: l2reg = 0.0001                 with score 63.8
2019-02-13 16:57:08,288 : Best param found at split 3: l2reg = 0.0001                 with score 63.94
2019-02-13 16:57:11,432 : Best param found at split 4: l2reg = 1e-05                 with score 63.79
2019-02-13 16:57:14,542 : Best param found at split 5: l2reg = 1e-05                 with score 63.75
2019-02-13 16:57:14,765 : Dev acc : 63.82 Test acc : 63.71

2019-02-13 16:57:14,765 : ***** Transfer task : MPQA *****


2019-02-13 16:57:14,771 : loading BERT model bert-base-uncased
2019-02-13 16:57:14,771 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:57:14,789 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:57:14,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3z2y98h2
2019-02-13 16:57:17,222 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:57:18,727 : Generating sentence embeddings
2019-02-13 16:57:23,823 : Generated sentence embeddings
2019-02-13 16:57:23,824 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:57:33,953 : Best param found at split 1: l2reg = 1e-05                 with score 68.78
2019-02-13 16:57:45,911 : Best param found at split 2: l2reg = 1e-05                 with score 68.78
2019-02-13 16:57:55,796 : Best param found at split 3: l2reg = 0.0001                 with score 68.77
2019-02-13 16:58:07,581 : Best param found at split 4: l2reg = 1e-05                 with score 68.77
2019-02-13 16:58:19,406 : Best param found at split 5: l2reg = 1e-05                 with score 68.77
2019-02-13 16:58:19,927 : Dev acc : 68.77 Test acc : 68.77

2019-02-13 16:58:19,928 : ***** Transfer task : SUBJ *****


2019-02-13 16:58:19,945 : loading BERT model bert-base-uncased
2019-02-13 16:58:19,945 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:58:19,965 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:58:19,965 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv2x6lzaz
2019-02-13 16:58:22,398 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:58:23,832 : Generating sentence embeddings
2019-02-13 16:58:38,131 : Generated sentence embeddings
2019-02-13 16:58:38,131 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 16:58:46,195 : Best param found at split 1: l2reg = 1e-05                 with score 50.61
2019-02-13 16:58:57,015 : Best param found at split 2: l2reg = 0.01                 with score 51.42
2019-02-13 16:59:08,232 : Best param found at split 3: l2reg = 1e-05                 with score 51.2
2019-02-13 16:59:18,347 : Best param found at split 4: l2reg = 1e-05                 with score 51.54
2019-02-13 16:59:31,737 : Best param found at split 5: l2reg = 0.0001                 with score 51.49
2019-02-13 16:59:32,648 : Dev acc : 51.25 Test acc : 50.41

2019-02-13 16:59:32,649 : ***** Transfer task : SST Binary classification *****


2019-02-13 16:59:32,782 : loading BERT model bert-base-uncased
2019-02-13 16:59:32,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 16:59:32,804 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 16:59:32,804 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb0awpzxq
2019-02-13 16:59:35,273 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 16:59:36,709 : Computing embedding for train
2019-02-13 17:00:29,908 : Computed train embeddings
2019-02-13 17:00:29,908 : Computing embedding for dev
2019-02-13 17:00:30,972 : Computed dev embeddings
2019-02-13 17:00:30,972 : Computing embedding for test
2019-02-13 17:00:33,244 : Computed test embeddings
2019-02-13 17:00:33,244 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:00:53,302 : [('reg:1e-05', 50.92), ('reg:0.0001', 50.92), ('reg:0.001', 50.92), ('reg:0.01', 50.92)]
2019-02-13 17:00:53,302 : Validation : best param found is reg = 1e-05 with score             50.92
2019-02-13 17:00:53,302 : Evaluating...
2019-02-13 17:00:57,997 : 
Dev acc : 50.92 Test acc : 49.92 for             SST Binary classification

2019-02-13 17:00:57,997 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 17:00:58,046 : loading BERT model bert-base-uncased
2019-02-13 17:00:58,046 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:00:58,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:00:58,068 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcdq8ftpe
2019-02-13 17:01:00,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:01:01,939 : Computing embedding for train
2019-02-13 17:01:11,536 : Computed train embeddings
2019-02-13 17:01:11,536 : Computing embedding for dev
2019-02-13 17:01:12,941 : Computed dev embeddings
2019-02-13 17:01:12,941 : Computing embedding for test
2019-02-13 17:01:15,783 : Computed test embeddings
2019-02-13 17:01:15,783 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:01:19,641 : [('reg:1e-05', 26.25), ('reg:0.0001', 26.25), ('reg:0.001', 25.34), ('reg:0.01', 26.25)]
2019-02-13 17:01:19,641 : Validation : best param found is reg = 1e-05 with score             26.25
2019-02-13 17:01:19,641 : Evaluating...
2019-02-13 17:01:20,675 : 
Dev acc : 26.25 Test acc : 28.64 for             SST Fine-Grained classification

2019-02-13 17:01:20,676 : ***** Transfer task : TREC *****


2019-02-13 17:01:20,689 : loading BERT model bert-base-uncased
2019-02-13 17:01:20,689 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:01:20,708 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:01:20,709 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcukqdyey
2019-02-13 17:01:23,142 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:01:29,634 : Computed train embeddings
2019-02-13 17:01:30,062 : Computed test embeddings
2019-02-13 17:01:30,062 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 17:01:44,340 : [('reg:1e-05', 25.13), ('reg:0.0001', 24.1), ('reg:0.001', 23.31), ('reg:0.01', 24.43)]
2019-02-13 17:01:44,340 : Cross-validation : best param found is reg = 1e-05             with score 25.13
2019-02-13 17:01:44,340 : Evaluating...
2019-02-13 17:01:45,083 : 
Dev acc : 25.13 Test acc : 19.6             for TREC

2019-02-13 17:01:45,084 : ***** Transfer task : MRPC *****


2019-02-13 17:01:45,106 : loading BERT model bert-base-uncased
2019-02-13 17:01:45,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:01:45,126 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:01:45,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbbz7bl2_
2019-02-13 17:01:47,561 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:01:49,059 : Computing embedding for train
2019-02-13 17:02:03,341 : Computed train embeddings
2019-02-13 17:02:03,341 : Computing embedding for test
2019-02-13 17:02:10,795 : Computed test embeddings
2019-02-13 17:02:10,811 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 17:02:17,934 : [('reg:1e-05', 67.52), ('reg:0.0001', 67.69), ('reg:0.001', 67.59), ('reg:0.01', 67.66)]
2019-02-13 17:02:17,935 : Cross-validation : best param found is reg = 0.0001             with score 67.69
2019-02-13 17:02:17,935 : Evaluating...
2019-02-13 17:02:18,300 : Dev acc : 67.69 Test acc 64.75; Test F1 75.58 for MRPC.

2019-02-13 17:02:18,300 : ***** Transfer task : SICK-Entailment*****


2019-02-13 17:02:18,325 : loading BERT model bert-base-uncased
2019-02-13 17:02:18,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:02:18,384 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:02:18,384 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe69vt_cc
2019-02-13 17:02:20,834 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:02:22,299 : Computing embedding for train
2019-02-13 17:02:28,736 : Computed train embeddings
2019-02-13 17:02:28,736 : Computing embedding for dev
2019-02-13 17:02:29,562 : Computed dev embeddings
2019-02-13 17:02:29,562 : Computing embedding for test
2019-02-13 17:02:36,504 : Computed test embeddings
2019-02-13 17:02:36,532 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:02:38,020 : [('reg:1e-05', 56.4), ('reg:0.0001', 56.4), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-13 17:02:38,020 : Validation : best param found is reg = 1e-05 with score             56.4
2019-02-13 17:02:38,020 : Evaluating...
2019-02-13 17:02:38,457 : 
Dev acc : 56.4 Test acc : 56.69 for                        SICK entailment

2019-02-13 17:02:38,458 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 17:02:38,485 : loading BERT model bert-base-uncased
2019-02-13 17:02:38,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:02:38,504 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:02:38,504 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj1jmbxi1
2019-02-13 17:02:40,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:02:42,456 : Computing embedding for train
2019-02-13 17:02:48,843 : Computed train embeddings
2019-02-13 17:02:48,843 : Computing embedding for dev
2019-02-13 17:02:49,664 : Computed dev embeddings
2019-02-13 17:02:49,664 : Computing embedding for test
2019-02-13 17:02:56,052 : Computed test embeddings
2019-02-13 17:03:23,382 : Dev : Pearson 0.2746697275749451
2019-02-13 17:03:23,382 : Test : Pearson 0.2434321998825252 Spearman 0.24753727352965577 MSE 0.9597591680469696                        for SICK Relatedness

2019-02-13 17:03:23,382 : 

***** Transfer task : STSBenchmark*****


2019-02-13 17:03:23,421 : loading BERT model bert-base-uncased
2019-02-13 17:03:23,421 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:03:23,450 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:03:23,450 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp070opvz
2019-02-13 17:03:25,904 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:03:27,374 : Computing embedding for train
2019-02-13 17:03:37,153 : Computed train embeddings
2019-02-13 17:03:37,153 : Computing embedding for dev
2019-02-13 17:03:39,302 : Computed dev embeddings
2019-02-13 17:03:39,302 : Computing embedding for test
2019-02-13 17:03:41,128 : Computed test embeddings
2019-02-13 17:04:24,123 : Dev : Pearson 0.05712477508860854
2019-02-13 17:04:24,123 : Test : Pearson 0.15582446126841487 Spearman 0.15097357916941093 MSE 2.506092591538535                        for SICK Relatedness

2019-02-13 17:04:24,124 : ***** Transfer task : SNLI Entailment*****


2019-02-13 17:04:28,902 : loading BERT model bert-base-uncased
2019-02-13 17:04:28,902 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:04:29,026 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:04:29,026 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmpuh6sx5
2019-02-13 17:04:31,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:04:33,279 : PROGRESS (encoding): 0.00%
2019-02-13 17:06:19,530 : PROGRESS (encoding): 14.56%
2019-02-13 17:08:26,078 : PROGRESS (encoding): 29.12%
2019-02-13 17:10:42,963 : PROGRESS (encoding): 43.69%
2019-02-13 17:13:06,338 : PROGRESS (encoding): 58.25%
2019-02-13 17:15:41,428 : PROGRESS (encoding): 72.81%
2019-02-13 17:18:20,816 : PROGRESS (encoding): 87.37%
2019-02-13 17:20:57,207 : PROGRESS (encoding): 0.00%
2019-02-13 17:21:18,272 : PROGRESS (encoding): 0.00%
2019-02-13 17:21:40,193 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 17:22:42,211 : [('reg:1e-09', 36.43)]
2019-02-13 17:22:42,211 : Validation : best param found is reg = 1e-09 with score             36.43
2019-02-13 17:22:42,211 : Evaluating...
2019-02-13 17:23:48,189 : Dev acc : 36.43 Test acc : 36.18 for SNLI

2019-02-13 17:23:48,189 : ***** Transfer task: Image Caption Retrieval *****


2019-02-13 17:23:56,851 : loading BERT model bert-base-uncased
2019-02-13 17:23:56,851 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 17:23:56,896 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 17:23:56,896 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfddd4fha
2019-02-13 17:23:59,331 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 17:24:00,824 : Computing embedding for train
2019-02-13 17:34:45,365 : Computed train embeddings
2019-02-13 17:34:45,366 : Computing embedding for dev
2019-02-13 17:35:18,103 : Computed dev embeddings
2019-02-13 17:35:18,103 : Computing embedding for test
2019-02-13 17:35:51,909 : Computed test embeddings
2019-02-13 17:35:51,928 : prepare data
2019-02-13 17:35:51,989 : start epoch
2019-02-13 17:36:36,786 : samples : 64000
2019-02-13 17:36:47,642 : Image to text: 0.0, 0.06, 0.14, 3089.0
2019-02-13 17:36:58,084 : Text to Image: 0.036, 0.164, 0.272, 2234.0
2019-02-13 17:37:42,899 : samples : 128000
2019-02-13 17:37:52,017 : Image to text: 0.02, 0.08, 0.14, 2853.0
2019-02-13 17:38:02,187 : Text to Image: 0.028, 0.148, 0.32, 2206.0
2019-02-13 17:38:46,424 : samples : 192000
2019-02-13 17:38:52,913 : Image to text: 0.0, 0.16, 0.24, 2903.0
2019-02-13 17:39:00,323 : Text to Image: 0.032, 0.128, 0.284, 2219.0
2019-02-13 17:39:41,485 : samples : 256000
2019-02-13 17:39:47,970 : Image to text: 0.02, 0.12, 0.16, 2913.0
2019-02-13 17:39:55,366 : Text to Image: 0.028, 0.148, 0.296, 2196.0
2019-02-13 17:40:37,088 : samples : 320000
2019-02-13 17:40:43,550 : Image to text: 0.0, 0.08, 0.18, 3071.0
2019-02-13 17:40:50,950 : Text to Image: 0.028, 0.16, 0.304, 2179.0
2019-02-13 17:41:31,560 : samples : 384000
2019-02-13 17:41:38,047 : Image to text: 0.02, 0.12, 0.18, 2896.0
2019-02-13 17:41:45,481 : Text to Image: 0.028, 0.14, 0.316, 2185.0
2019-02-13 17:42:27,390 : samples : 448000
2019-02-13 17:42:33,807 : Image to text: 0.04, 0.1, 0.16, 2947.0
2019-02-13 17:42:41,201 : Text to Image: 0.036, 0.156, 0.32, 2167.0
2019-02-13 17:43:21,698 : samples : 512000
2019-02-13 17:43:27,920 : Image to text: 0.02, 0.14, 0.3, 2909.0
2019-02-13 17:43:35,292 : Text to Image: 0.048, 0.188, 0.348, 2160.0
2019-02-13 17:44:11,385 : Epoch 1 finished
2019-02-13 17:44:11,725 : Image to text: 0.1, 0.4, 0.8, 617.0
2019-02-13 17:44:12,066 : Text to Image: 0.1, 0.64, 1.5, 448.0
2019-02-13 17:44:12,406 : Image to text: 0.0, 0.6, 1.0, 578.0
2019-02-13 17:44:12,746 : Text to Image: 0.08, 0.76, 1.56, 449.0
2019-02-13 17:44:13,085 : Image to text: 0.1, 0.6, 1.2, 572.0
2019-02-13 17:44:13,426 : Text to Image: 0.28, 0.76, 1.44, 423.0
2019-02-13 17:44:13,765 : Image to text: 0.2, 0.6, 1.0, 597.0
2019-02-13 17:44:14,106 : Text to Image: 0.16, 0.76, 1.46, 431.0
2019-02-13 17:44:14,445 : Image to text: 0.1, 0.4, 1.1, 547.0
2019-02-13 17:44:14,785 : Text to Image: 0.16, 0.82, 1.76, 428.0
2019-02-13 17:44:14,786 : Dev mean Text to Image: 0.15600000000000003, 0.748, 1.544, 435.79999999999995
2019-02-13 17:44:14,786 : Dev mean Image to text: 0.1, 0.52, 1.02, 582.1999999999999
2019-02-13 17:44:14,786 : start epoch
2019-02-13 17:44:57,813 : samples : 64000
2019-02-13 17:45:04,328 : Image to text: 0.0, 0.1, 0.28, 2897.0
2019-02-13 17:45:11,740 : Text to Image: 0.004, 0.16, 0.336, 2161.0
2019-02-13 17:45:54,638 : samples : 128000
2019-02-13 17:46:01,170 : Image to text: 0.02, 0.12, 0.24, 2863.0
2019-02-13 17:46:08,568 : Text to Image: 0.036, 0.176, 0.364, 2187.0
2019-02-13 17:46:49,899 : samples : 192000
2019-02-13 17:46:56,376 : Image to text: 0.04, 0.16, 0.24, 3087.0
2019-02-13 17:47:03,814 : Text to Image: 0.032, 0.152, 0.308, 2171.0
2019-02-13 17:47:45,463 : samples : 256000
2019-02-13 17:47:52,025 : Image to text: 0.02, 0.12, 0.18, 2994.0
2019-02-13 17:47:59,685 : Text to Image: 0.028, 0.128, 0.284, 2182.0
2019-02-13 17:48:39,836 : samples : 320000
2019-02-13 17:48:46,334 : Image to text: 0.02, 0.12, 0.2, 2861.0
2019-02-13 17:48:53,907 : Text to Image: 0.04, 0.116, 0.304, 2157.0
2019-02-13 17:49:36,015 : samples : 384000
2019-02-13 17:49:42,512 : Image to text: 0.02, 0.06, 0.16, 2811.0
2019-02-13 17:49:49,930 : Text to Image: 0.016, 0.156, 0.304, 2160.0
2019-02-13 17:50:31,926 : samples : 448000
2019-02-13 17:50:38,398 : Image to text: 0.02, 0.1, 0.2, 2926.0
2019-02-13 17:50:45,868 : Text to Image: 0.02, 0.144, 0.304, 2167.0
2019-02-13 17:51:28,095 : samples : 512000
2019-02-13 17:51:34,632 : Image to text: 0.02, 0.12, 0.2, 2857.0
2019-02-13 17:51:42,087 : Text to Image: 0.028, 0.16, 0.352, 2167.0
2019-02-13 17:52:17,837 : Epoch 2 finished
2019-02-13 17:52:18,143 : Image to text: 0.1, 0.5, 1.0, 634.0
2019-02-13 17:52:18,467 : Text to Image: 0.18, 0.74, 1.42, 449.0
2019-02-13 17:52:18,769 : Image to text: 0.0, 0.9, 1.3, 584.0
2019-02-13 17:52:19,090 : Text to Image: 0.16, 0.64, 1.5, 438.0
2019-02-13 17:52:19,389 : Image to text: 0.2, 0.8, 1.2, 561.0
2019-02-13 17:52:19,709 : Text to Image: 0.1, 0.9, 1.74, 427.0
2019-02-13 17:52:19,995 : Image to text: 0.0, 0.5, 1.1, 594.0
2019-02-13 17:52:20,317 : Text to Image: 0.14, 0.7, 1.6, 434.0
2019-02-13 17:52:20,603 : Image to text: 0.1, 0.7, 1.5, 578.0
2019-02-13 17:52:20,923 : Text to Image: 0.16, 0.88, 1.5, 430.0
2019-02-13 17:52:20,923 : Dev mean Text to Image: 0.14800000000000002, 0.772, 1.552, 435.59999999999997
2019-02-13 17:52:20,923 : Dev mean Image to text: 0.08, 0.68, 1.22, 590.2
2019-02-13 17:52:20,923 : start epoch
2019-02-13 17:53:01,782 : samples : 64000
2019-02-13 17:53:08,155 : Image to text: 0.02, 0.08, 0.18, 2879.0
2019-02-13 17:53:15,624 : Text to Image: 0.02, 0.152, 0.292, 2174.0
2019-02-13 17:53:57,245 : samples : 128000
2019-02-13 17:54:03,505 : Image to text: 0.02, 0.1, 0.14, 2874.0
2019-02-13 17:54:10,910 : Text to Image: 0.024, 0.156, 0.32, 2161.0
2019-02-13 17:54:52,815 : samples : 192000
2019-02-13 17:54:59,283 : Image to text: 0.0, 0.06, 0.16, 2909.0
2019-02-13 17:55:06,596 : Text to Image: 0.032, 0.164, 0.32, 2146.0
2019-02-13 17:55:50,002 : samples : 256000
2019-02-13 17:55:56,547 : Image to text: 0.04, 0.08, 0.22, 2862.0
2019-02-13 17:56:03,947 : Text to Image: 0.044, 0.144, 0.312, 2151.0
2019-02-13 17:56:44,454 : samples : 320000
2019-02-13 17:56:50,974 : Image to text: 0.0, 0.08, 0.22, 2866.0
2019-02-13 17:56:58,356 : Text to Image: 0.028, 0.136, 0.268, 2142.0
2019-02-13 17:57:39,582 : samples : 384000
2019-02-13 17:57:45,943 : Image to text: 0.0, 0.08, 0.26, 2890.0
2019-02-13 17:57:53,415 : Text to Image: 0.032, 0.18, 0.276, 2170.0
2019-02-13 17:58:36,070 : samples : 448000
2019-02-13 17:58:42,331 : Image to text: 0.0, 0.1, 0.16, 2913.0
2019-02-13 17:58:49,707 : Text to Image: 0.032, 0.148, 0.312, 2157.0
2019-02-13 17:59:32,011 : samples : 512000
2019-02-13 17:59:38,319 : Image to text: 0.06, 0.12, 0.28, 2887.0
2019-02-13 17:59:45,601 : Text to Image: 0.028, 0.168, 0.356, 2166.0
2019-02-13 18:00:22,570 : Epoch 3 finished
2019-02-13 18:00:22,878 : Image to text: 0.1, 0.7, 1.3, 587.0
2019-02-13 18:00:23,199 : Text to Image: 0.14, 0.76, 1.46, 448.0
2019-02-13 18:00:23,499 : Image to text: 0.0, 0.5, 1.1, 568.0
2019-02-13 18:00:23,820 : Text to Image: 0.14, 0.78, 1.54, 450.0
2019-02-13 18:00:24,109 : Image to text: 0.1, 0.6, 1.6, 568.0
2019-02-13 18:00:24,431 : Text to Image: 0.24, 0.74, 1.54, 424.0
2019-02-13 18:00:24,730 : Image to text: 0.3, 0.5, 1.4, 566.0
2019-02-13 18:00:25,050 : Text to Image: 0.12, 0.8, 1.4, 429.0
2019-02-13 18:00:25,349 : Image to text: 0.2, 0.8, 1.2, 587.0
2019-02-13 18:00:25,669 : Text to Image: 0.08, 0.76, 1.58, 431.0
2019-02-13 18:00:25,669 : Dev mean Text to Image: 0.14400000000000002, 0.768, 1.504, 436.4
2019-02-13 18:00:25,669 : Dev mean Image to text: 0.14, 0.62, 1.32, 575.2
2019-02-13 18:00:25,669 : start epoch
2019-02-13 18:01:07,806 : samples : 64000
2019-02-13 18:01:14,344 : Image to text: 0.0, 0.02, 0.16, 2936.0
2019-02-13 18:01:21,766 : Text to Image: 0.036, 0.18, 0.336, 2166.0
2019-02-13 18:02:04,112 : samples : 128000
2019-02-13 18:02:10,643 : Image to text: 0.02, 0.06, 0.22, 2793.0
2019-02-13 18:02:18,057 : Text to Image: 0.02, 0.168, 0.344, 2155.0
2019-02-13 18:03:00,257 : samples : 192000
2019-02-13 18:03:06,790 : Image to text: 0.0, 0.04, 0.16, 2851.0
2019-02-13 18:03:14,157 : Text to Image: 0.028, 0.124, 0.336, 2168.0
2019-02-13 18:03:56,142 : samples : 256000
2019-02-13 18:04:02,614 : Image to text: 0.04, 0.14, 0.24, 2886.0
2019-02-13 18:04:10,126 : Text to Image: 0.048, 0.172, 0.332, 2157.0
2019-02-13 18:04:51,188 : samples : 320000
2019-02-13 18:04:57,675 : Image to text: 0.04, 0.08, 0.22, 2884.0
2019-02-13 18:05:05,109 : Text to Image: 0.012, 0.164, 0.292, 2165.0
2019-02-13 18:05:46,784 : samples : 384000
2019-02-13 18:05:53,244 : Image to text: 0.04, 0.16, 0.34, 2963.0
2019-02-13 18:06:00,659 : Text to Image: 0.024, 0.18, 0.344, 2155.0
2019-02-13 18:06:42,013 : samples : 448000
2019-02-13 18:06:48,478 : Image to text: 0.0, 0.12, 0.24, 2818.0
2019-02-13 18:06:55,891 : Text to Image: 0.028, 0.156, 0.324, 2150.0
2019-02-13 18:07:37,207 : samples : 512000
2019-02-13 18:07:43,698 : Image to text: 0.04, 0.26, 0.36, 2904.0
2019-02-13 18:07:51,129 : Text to Image: 0.036, 0.14, 0.32, 2150.0
2019-02-13 18:08:26,088 : Epoch 4 finished
2019-02-13 18:08:26,395 : Image to text: 0.0, 0.5, 1.3, 605.0
2019-02-13 18:08:26,716 : Text to Image: 0.26, 0.76, 1.56, 446.0
2019-02-13 18:08:27,003 : Image to text: 0.0, 0.5, 1.2, 548.0
2019-02-13 18:08:27,324 : Text to Image: 0.18, 0.88, 1.76, 448.0
2019-02-13 18:08:27,623 : Image to text: 0.2, 0.5, 1.0, 548.0
2019-02-13 18:08:27,943 : Text to Image: 0.14, 0.86, 1.6, 421.0
2019-02-13 18:08:28,241 : Image to text: 0.1, 0.3, 1.1, 612.0
2019-02-13 18:08:28,561 : Text to Image: 0.12, 0.88, 1.62, 424.0
2019-02-13 18:08:28,846 : Image to text: 0.3, 1.0, 1.3, 580.0
2019-02-13 18:08:29,164 : Text to Image: 0.12, 0.92, 1.58, 421.0
2019-02-13 18:08:29,164 : Dev mean Text to Image: 0.16399999999999998, 0.8599999999999999, 1.624, 432.0
2019-02-13 18:08:29,164 : Dev mean Image to text: 0.12, 0.56, 1.18, 578.6
2019-02-13 18:08:29,164 : start epoch
2019-02-13 18:09:13,937 : samples : 64000
2019-02-13 18:09:20,263 : Image to text: 0.0, 0.12, 0.36, 2824.0
2019-02-13 18:09:27,530 : Text to Image: 0.028, 0.16, 0.312, 2157.0
2019-02-13 18:10:08,796 : samples : 128000
2019-02-13 18:10:15,172 : Image to text: 0.1, 0.18, 0.28, 2806.0
2019-02-13 18:10:22,608 : Text to Image: 0.032, 0.132, 0.284, 2153.0
2019-02-13 18:11:05,483 : samples : 192000
2019-02-13 18:11:11,889 : Image to text: 0.0, 0.12, 0.2, 2782.0
2019-02-13 18:11:19,541 : Text to Image: 0.028, 0.132, 0.296, 2159.0
2019-02-13 18:12:02,381 : samples : 256000
2019-02-13 18:12:08,843 : Image to text: 0.02, 0.14, 0.24, 2809.0
2019-02-13 18:12:16,450 : Text to Image: 0.02, 0.168, 0.332, 2152.0
2019-02-13 18:12:57,343 : samples : 320000
2019-02-13 18:13:03,789 : Image to text: 0.04, 0.18, 0.38, 2840.0
2019-02-13 18:13:11,165 : Text to Image: 0.048, 0.188, 0.344, 2163.0
2019-02-13 18:13:53,367 : samples : 384000
2019-02-13 18:13:59,707 : Image to text: 0.02, 0.08, 0.22, 2857.0
2019-02-13 18:14:06,998 : Text to Image: 0.04, 0.176, 0.344, 2150.0
2019-02-13 18:14:48,177 : samples : 448000
2019-02-13 18:14:54,332 : Image to text: 0.02, 0.04, 0.12, 2878.0
2019-02-13 18:15:01,583 : Text to Image: 0.036, 0.136, 0.308, 2165.0
2019-02-13 18:15:43,609 : samples : 512000
2019-02-13 18:15:50,115 : Image to text: 0.0, 0.1, 0.24, 2860.0
2019-02-13 18:15:57,482 : Text to Image: 0.048, 0.176, 0.332, 2162.0
2019-02-13 18:16:31,898 : Epoch 5 finished
2019-02-13 18:16:32,198 : Image to text: 0.1, 0.4, 1.1, 567.0
2019-02-13 18:16:32,519 : Text to Image: 0.16, 0.78, 1.44, 447.0
2019-02-13 18:16:32,818 : Image to text: 0.0, 0.5, 1.2, 544.0
2019-02-13 18:16:33,140 : Text to Image: 0.22, 0.8, 1.42, 441.0
2019-02-13 18:16:33,440 : Image to text: 0.2, 0.5, 1.4, 574.0
2019-02-13 18:16:33,762 : Text to Image: 0.14, 0.68, 1.64, 424.0
2019-02-13 18:16:34,048 : Image to text: 0.3, 0.5, 1.1, 557.0
2019-02-13 18:16:34,366 : Text to Image: 0.24, 0.86, 1.5, 428.0
2019-02-13 18:16:34,662 : Image to text: 0.1, 0.6, 1.0, 559.0
2019-02-13 18:16:34,981 : Text to Image: 0.18, 1.02, 1.72, 427.0
2019-02-13 18:16:34,981 : Dev mean Text to Image: 0.18800000000000003, 0.8280000000000001, 1.544, 433.4
2019-02-13 18:16:34,981 : Dev mean Image to text: 0.13999999999999999, 0.5, 1.16, 560.1999999999999
2019-02-13 18:16:34,982 : start epoch
2019-02-13 18:17:15,272 : samples : 64000
2019-02-13 18:17:21,790 : Image to text: 0.02, 0.22, 0.36, 2895.0
2019-02-13 18:17:29,161 : Text to Image: 0.032, 0.188, 0.368, 2159.0
2019-02-13 18:18:09,378 : samples : 128000
2019-02-13 18:18:15,891 : Image to text: 0.0, 0.08, 0.22, 2817.0
2019-02-13 18:18:23,312 : Text to Image: 0.044, 0.14, 0.28, 2155.0
2019-02-13 18:19:05,380 : samples : 192000
2019-02-13 18:19:11,976 : Image to text: 0.04, 0.12, 0.22, 2827.0
2019-02-13 18:19:19,492 : Text to Image: 0.052, 0.188, 0.372, 2155.0
2019-02-13 18:20:01,536 : samples : 256000
2019-02-13 18:20:08,079 : Image to text: 0.02, 0.14, 0.3, 2962.0
2019-02-13 18:20:15,510 : Text to Image: 0.04, 0.136, 0.32, 2155.0
2019-02-13 18:20:55,595 : samples : 320000
2019-02-13 18:21:01,933 : Image to text: 0.06, 0.22, 0.32, 2850.0
2019-02-13 18:21:09,388 : Text to Image: 0.032, 0.164, 0.312, 2149.0
2019-02-13 18:21:49,273 : samples : 384000
2019-02-13 18:21:55,823 : Image to text: 0.02, 0.16, 0.22, 2924.0
2019-02-13 18:22:03,214 : Text to Image: 0.028, 0.184, 0.336, 2160.0
2019-02-13 18:22:43,378 : samples : 448000
2019-02-13 18:22:49,897 : Image to text: 0.0, 0.2, 0.32, 2852.0
2019-02-13 18:22:57,336 : Text to Image: 0.032, 0.168, 0.328, 2164.0
2019-02-13 18:23:37,520 : samples : 512000
2019-02-13 18:23:44,186 : Image to text: 0.04, 0.12, 0.26, 2920.0
2019-02-13 18:23:51,580 : Text to Image: 0.036, 0.176, 0.332, 2150.0
2019-02-13 18:24:25,824 : Epoch 6 finished
2019-02-13 18:24:26,113 : Image to text: 0.0, 0.6, 1.4, 581.0
2019-02-13 18:24:26,407 : Text to Image: 0.24, 0.76, 1.46, 446.0
2019-02-13 18:24:26,698 : Image to text: 0.0, 0.8, 1.5, 567.0
2019-02-13 18:24:26,990 : Text to Image: 0.16, 0.68, 1.66, 441.0
2019-02-13 18:24:27,282 : Image to text: 0.1, 0.5, 0.8, 564.0
2019-02-13 18:24:27,577 : Text to Image: 0.16, 0.64, 1.44, 423.0
2019-02-13 18:24:27,868 : Image to text: 0.1, 0.4, 1.2, 574.0
2019-02-13 18:24:28,159 : Text to Image: 0.12, 0.96, 1.72, 427.0
2019-02-13 18:24:28,449 : Image to text: 0.3, 1.1, 2.1, 554.0
2019-02-13 18:24:28,740 : Text to Image: 0.18, 0.8, 1.54, 422.0
2019-02-13 18:24:28,740 : Dev mean Text to Image: 0.17200000000000001, 0.7680000000000001, 1.5639999999999998, 431.79999999999995
2019-02-13 18:24:28,740 : Dev mean Image to text: 0.1, 0.68, 1.4, 568.0
2019-02-13 18:24:28,740 : start epoch
2019-02-13 18:25:08,860 : samples : 64000
2019-02-13 18:25:15,312 : Image to text: 0.04, 0.14, 0.32, 2913.0
2019-02-13 18:25:22,969 : Text to Image: 0.012, 0.144, 0.292, 2148.0
2019-02-13 18:26:03,468 : samples : 128000
2019-02-13 18:26:09,979 : Image to text: 0.0, 0.12, 0.34, 2827.0
2019-02-13 18:26:17,455 : Text to Image: 0.048, 0.164, 0.284, 2149.0
2019-02-13 18:26:57,882 : samples : 192000
2019-02-13 18:27:04,467 : Image to text: 0.04, 0.18, 0.28, 2877.0
2019-02-13 18:27:12,087 : Text to Image: 0.024, 0.176, 0.348, 2160.0
2019-02-13 18:27:52,342 : samples : 256000
2019-02-13 18:27:58,907 : Image to text: 0.04, 0.16, 0.3, 2830.0
2019-02-13 18:28:06,416 : Text to Image: 0.028, 0.156, 0.356, 2148.0
2019-02-13 18:28:51,003 : samples : 320000
2019-02-13 18:28:57,524 : Image to text: 0.02, 0.18, 0.22, 2839.0
2019-02-13 18:29:04,925 : Text to Image: 0.028, 0.14, 0.328, 2156.0
2019-02-13 18:29:45,727 : samples : 384000
2019-02-13 18:29:52,212 : Image to text: 0.02, 0.12, 0.36, 2790.0
2019-02-13 18:29:59,698 : Text to Image: 0.028, 0.144, 0.28, 2145.0
2019-02-13 18:30:41,012 : samples : 448000
2019-02-13 18:30:47,535 : Image to text: 0.06, 0.16, 0.26, 2903.0
2019-02-13 18:30:55,228 : Text to Image: 0.064, 0.168, 0.348, 2159.0
2019-02-13 18:31:36,107 : samples : 512000
2019-02-13 18:31:42,596 : Image to text: 0.0, 0.16, 0.3, 2876.0
2019-02-13 18:31:50,225 : Text to Image: 0.048, 0.172, 0.312, 2153.0
2019-02-13 18:32:25,086 : Epoch 7 finished
2019-02-13 18:32:25,378 : Image to text: 0.1, 0.3, 0.8, 640.0
2019-02-13 18:32:25,697 : Text to Image: 0.1, 0.86, 1.66, 441.0
2019-02-13 18:32:25,996 : Image to text: 0.1, 0.8, 1.6, 566.0
2019-02-13 18:32:26,317 : Text to Image: 0.18, 0.94, 1.7, 443.0
2019-02-13 18:32:26,615 : Image to text: 0.2, 0.9, 1.2, 572.0
2019-02-13 18:32:26,926 : Text to Image: 0.14, 0.76, 1.52, 427.0
2019-02-13 18:32:27,219 : Image to text: 0.3, 0.9, 1.5, 616.0
2019-02-13 18:32:27,539 : Text to Image: 0.16, 0.68, 1.4, 428.0
2019-02-13 18:32:27,836 : Image to text: 0.4, 0.6, 1.2, 588.0
2019-02-13 18:32:28,155 : Text to Image: 0.18, 0.72, 1.6, 422.0
2019-02-13 18:32:28,155 : Dev mean Text to Image: 0.152, 0.792, 1.576, 432.20000000000005
2019-02-13 18:32:28,155 : Dev mean Image to text: 0.22000000000000003, 0.7000000000000001, 1.26, 596.4
2019-02-13 18:32:28,155 : start epoch
2019-02-13 18:33:08,921 : samples : 64000
2019-02-13 18:33:15,374 : Image to text: 0.04, 0.18, 0.28, 2807.0
2019-02-13 18:33:22,834 : Text to Image: 0.044, 0.152, 0.324, 2172.0
2019-02-13 18:34:04,213 : samples : 128000
2019-02-13 18:34:10,689 : Image to text: 0.04, 0.12, 0.2, 2824.0
2019-02-13 18:34:18,102 : Text to Image: 0.036, 0.176, 0.372, 2163.0
2019-02-13 18:34:58,692 : samples : 192000
2019-02-13 18:35:05,190 : Image to text: 0.0, 0.16, 0.26, 2863.0
2019-02-13 18:35:12,704 : Text to Image: 0.04, 0.156, 0.344, 2163.0
2019-02-13 18:35:54,080 : samples : 256000
2019-02-13 18:36:00,486 : Image to text: 0.04, 0.16, 0.28, 2927.0
2019-02-13 18:36:07,995 : Text to Image: 0.036, 0.176, 0.312, 2157.0
2019-02-13 18:36:48,393 : samples : 320000
2019-02-13 18:36:54,661 : Image to text: 0.0, 0.14, 0.26, 2927.0
2019-02-13 18:37:02,087 : Text to Image: 0.048, 0.176, 0.348, 2155.0
2019-02-13 18:37:43,418 : samples : 384000
2019-02-13 18:37:49,711 : Image to text: 0.04, 0.1, 0.3, 2864.0
2019-02-13 18:37:57,042 : Text to Image: 0.032, 0.192, 0.316, 2149.0
2019-02-13 18:38:38,968 : samples : 448000
2019-02-13 18:38:45,458 : Image to text: 0.04, 0.14, 0.3, 2934.0
2019-02-13 18:38:52,855 : Text to Image: 0.04, 0.188, 0.336, 2149.0
2019-02-13 18:39:37,615 : samples : 512000
2019-02-13 18:39:44,132 : Image to text: 0.06, 0.14, 0.22, 2880.0
2019-02-13 18:39:51,532 : Text to Image: 0.032, 0.172, 0.352, 2152.0
2019-02-13 18:40:27,611 : Epoch 8 finished
2019-02-13 18:40:27,952 : Image to text: 0.0, 0.1, 0.5, 599.0
2019-02-13 18:40:28,294 : Text to Image: 0.1, 0.84, 1.48, 445.0
2019-02-13 18:40:28,633 : Image to text: 0.1, 0.7, 1.3, 569.0
2019-02-13 18:40:28,975 : Text to Image: 0.12, 0.8, 1.64, 435.0
2019-02-13 18:40:29,316 : Image to text: 0.1, 0.8, 1.2, 574.0
2019-02-13 18:40:29,629 : Text to Image: 0.16, 0.92, 1.74, 421.0
2019-02-13 18:40:29,918 : Image to text: 0.2, 0.7, 1.1, 596.0
2019-02-13 18:40:30,208 : Text to Image: 0.12, 0.84, 1.52, 431.0
2019-02-13 18:40:30,495 : Image to text: 0.3, 0.7, 1.3, 548.0
2019-02-13 18:40:30,786 : Text to Image: 0.16, 1.04, 1.82, 423.0
2019-02-13 18:40:30,786 : Dev mean Text to Image: 0.132, 0.8879999999999999, 1.6399999999999997, 431.0
2019-02-13 18:40:30,786 : Dev mean Image to text: 0.14, 0.6, 1.08, 577.1999999999999
2019-02-13 18:40:30,786 : start epoch
2019-02-13 18:41:15,512 : samples : 64000
2019-02-13 18:41:22,023 : Image to text: 0.02, 0.12, 0.26, 2890.0
2019-02-13 18:41:29,690 : Text to Image: 0.036, 0.204, 0.356, 2156.0
2019-02-13 18:42:16,417 : samples : 128000
2019-02-13 18:42:22,904 : Image to text: 0.04, 0.18, 0.38, 2947.0
2019-02-13 18:42:30,481 : Text to Image: 0.032, 0.152, 0.348, 2157.0
2019-02-13 18:43:12,068 : samples : 192000
2019-02-13 18:43:18,552 : Image to text: 0.0, 0.1, 0.24, 2904.0
2019-02-13 18:43:26,072 : Text to Image: 0.036, 0.172, 0.304, 2164.0
2019-02-13 18:44:07,044 : samples : 256000
2019-02-13 18:44:13,594 : Image to text: 0.02, 0.14, 0.22, 2941.0
2019-02-13 18:44:20,977 : Text to Image: 0.04, 0.168, 0.336, 2155.0
2019-02-13 18:45:01,582 : samples : 320000
2019-02-13 18:45:08,089 : Image to text: 0.06, 0.2, 0.38, 2876.0
2019-02-13 18:45:15,454 : Text to Image: 0.028, 0.196, 0.336, 2158.0
2019-02-13 18:45:56,101 : samples : 384000
2019-02-13 18:46:02,555 : Image to text: 0.02, 0.1, 0.3, 2942.0
2019-02-13 18:46:09,953 : Text to Image: 0.036, 0.164, 0.328, 2159.0
2019-02-13 18:46:50,374 : samples : 448000
2019-02-13 18:46:56,943 : Image to text: 0.06, 0.22, 0.4, 2905.0
2019-02-13 18:47:04,318 : Text to Image: 0.024, 0.156, 0.332, 2150.0
2019-02-13 18:47:44,369 : samples : 512000
2019-02-13 18:47:50,878 : Image to text: 0.0, 0.14, 0.24, 2822.0
2019-02-13 18:47:58,354 : Text to Image: 0.048, 0.216, 0.348, 2149.0
2019-02-13 18:48:32,814 : Epoch 9 finished
2019-02-13 18:48:33,118 : Image to text: 0.2, 0.4, 1.2, 576.0
2019-02-13 18:48:33,439 : Text to Image: 0.18, 0.72, 1.48, 445.0
2019-02-13 18:48:33,735 : Image to text: 0.0, 0.6, 1.0, 536.0
2019-02-13 18:48:34,046 : Text to Image: 0.18, 0.8, 1.62, 439.0
2019-02-13 18:48:34,340 : Image to text: 0.2, 1.1, 1.6, 549.0
2019-02-13 18:48:34,660 : Text to Image: 0.18, 0.98, 1.78, 422.0
2019-02-13 18:48:34,958 : Image to text: 0.4, 0.9, 1.4, 578.0
2019-02-13 18:48:35,277 : Text to Image: 0.12, 0.6, 1.32, 430.0
2019-02-13 18:48:35,573 : Image to text: 0.4, 0.8, 1.2, 539.0
2019-02-13 18:48:35,893 : Text to Image: 0.18, 0.88, 1.54, 419.0
2019-02-13 18:48:35,893 : Dev mean Text to Image: 0.16799999999999998, 0.796, 1.548, 431.00000000000006
2019-02-13 18:48:35,893 : Dev mean Image to text: 0.24, 0.7600000000000001, 1.28, 555.5999999999999
2019-02-13 18:48:35,893 : start epoch
2019-02-13 18:49:17,497 : samples : 64000
2019-02-13 18:49:24,005 : Image to text: 0.04, 0.1, 0.3, 2916.0
2019-02-13 18:49:31,460 : Text to Image: 0.036, 0.164, 0.336, 2154.0
2019-02-13 18:50:13,095 : samples : 128000
2019-02-13 18:50:19,620 : Image to text: 0.06, 0.16, 0.32, 2829.0
2019-02-13 18:50:27,055 : Text to Image: 0.032, 0.148, 0.316, 2159.0
2019-02-13 18:51:09,514 : samples : 192000
2019-02-13 18:51:15,885 : Image to text: 0.0, 0.14, 0.24, 2866.0
2019-02-13 18:51:23,351 : Text to Image: 0.04, 0.156, 0.308, 2158.0
2019-02-13 18:52:06,280 : samples : 256000
2019-02-13 18:52:12,511 : Image to text: 0.04, 0.1, 0.2, 2803.0
2019-02-13 18:52:19,913 : Text to Image: 0.036, 0.172, 0.352, 2143.0
2019-02-13 18:53:02,891 : samples : 320000
2019-02-13 18:53:09,137 : Image to text: 0.0, 0.12, 0.22, 2834.0
2019-02-13 18:53:16,445 : Text to Image: 0.036, 0.172, 0.376, 2153.0
2019-02-13 18:53:59,247 : samples : 384000
2019-02-13 18:54:05,561 : Image to text: 0.02, 0.06, 0.18, 2907.0
2019-02-13 18:54:12,937 : Text to Image: 0.04, 0.168, 0.316, 2148.0
2019-02-13 18:54:55,828 : samples : 448000
2019-02-13 18:55:02,220 : Image to text: 0.02, 0.22, 0.38, 2893.0
2019-02-13 18:55:09,786 : Text to Image: 0.032, 0.132, 0.304, 2155.0
2019-02-13 18:55:52,326 : samples : 512000
2019-02-13 18:55:58,626 : Image to text: 0.02, 0.16, 0.28, 2890.0
2019-02-13 18:56:06,051 : Text to Image: 0.036, 0.14, 0.304, 2155.0
2019-02-13 18:56:40,942 : Epoch 10 finished
2019-02-13 18:56:41,283 : Image to text: 0.0, 0.6, 0.9, 594.0
2019-02-13 18:56:41,623 : Text to Image: 0.14, 0.76, 1.52, 442.0
2019-02-13 18:56:41,964 : Image to text: 0.2, 0.8, 1.5, 530.0
2019-02-13 18:56:42,305 : Text to Image: 0.14, 0.8, 1.6, 445.0
2019-02-13 18:56:42,645 : Image to text: 0.4, 1.0, 1.5, 571.0
2019-02-13 18:56:42,986 : Text to Image: 0.2, 0.8, 1.38, 424.0
2019-02-13 18:56:43,326 : Image to text: 0.1, 0.8, 1.5, 588.0
2019-02-13 18:56:43,666 : Text to Image: 0.18, 0.66, 1.38, 424.0
2019-02-13 18:56:44,007 : Image to text: 0.4, 0.9, 1.2, 551.0
2019-02-13 18:56:44,348 : Text to Image: 0.14, 1.0, 1.88, 421.0
2019-02-13 18:56:44,348 : Dev mean Text to Image: 0.16, 0.804, 1.552, 431.2
2019-02-13 18:56:44,348 : Dev mean Image to text: 0.21999999999999997, 0.8200000000000001, 1.32, 566.8000000000001
2019-02-13 18:56:44,348 : start epoch
2019-02-13 18:57:26,381 : samples : 64000
2019-02-13 18:57:32,949 : Image to text: 0.02, 0.14, 0.26, 2953.0
2019-02-13 18:57:40,434 : Text to Image: 0.028, 0.16, 0.34, 2155.0
2019-02-13 18:58:23,931 : samples : 128000
2019-02-13 18:58:30,423 : Image to text: 0.04, 0.16, 0.22, 2975.0
2019-02-13 18:58:37,884 : Text to Image: 0.024, 0.136, 0.296, 2153.0
2019-02-13 18:59:18,152 : samples : 192000
2019-02-13 18:59:24,637 : Image to text: 0.02, 0.1, 0.2, 2897.0
2019-02-13 18:59:32,100 : Text to Image: 0.036, 0.148, 0.344, 2153.0
2019-02-13 19:00:13,539 : samples : 256000
2019-02-13 19:00:19,993 : Image to text: 0.0, 0.12, 0.18, 2983.0
2019-02-13 19:00:27,409 : Text to Image: 0.028, 0.148, 0.316, 2158.0
2019-02-13 19:01:08,177 : samples : 320000
2019-02-13 19:01:14,625 : Image to text: 0.0, 0.1, 0.26, 2853.0
2019-02-13 19:01:22,058 : Text to Image: 0.032, 0.164, 0.336, 2142.0
2019-02-13 19:02:02,968 : samples : 384000
2019-02-13 19:02:09,441 : Image to text: 0.02, 0.08, 0.3, 2793.0
2019-02-13 19:02:16,879 : Text to Image: 0.044, 0.204, 0.388, 2144.0
2019-02-13 19:02:58,218 : samples : 448000
2019-02-13 19:03:04,690 : Image to text: 0.04, 0.14, 0.3, 2865.0
2019-02-13 19:03:12,166 : Text to Image: 0.036, 0.168, 0.336, 2142.0
2019-02-13 19:03:52,965 : samples : 512000
2019-02-13 19:03:59,419 : Image to text: 0.04, 0.16, 0.3, 2867.0
2019-02-13 19:04:06,819 : Text to Image: 0.032, 0.164, 0.324, 2165.0
2019-02-13 19:04:42,327 : Epoch 11 finished
2019-02-13 19:04:42,615 : Image to text: 0.0, 0.3, 1.1, 582.0
2019-02-13 19:04:42,935 : Text to Image: 0.1, 0.74, 1.38, 440.0
2019-02-13 19:04:43,233 : Image to text: 0.1, 0.6, 0.8, 545.0
2019-02-13 19:04:43,550 : Text to Image: 0.16, 0.84, 1.54, 437.0
2019-02-13 19:04:43,849 : Image to text: 0.0, 0.7, 1.3, 574.0
2019-02-13 19:04:44,170 : Text to Image: 0.16, 0.84, 1.52, 425.0
2019-02-13 19:04:44,469 : Image to text: 0.2, 0.5, 0.8, 586.0
2019-02-13 19:04:44,788 : Text to Image: 0.16, 0.74, 1.52, 424.0
2019-02-13 19:04:45,073 : Image to text: 0.2, 0.6, 1.1, 612.0
2019-02-13 19:04:45,393 : Text to Image: 0.26, 0.98, 1.86, 423.0
2019-02-13 19:04:45,393 : Dev mean Text to Image: 0.168, 0.8279999999999998, 1.564, 429.79999999999995
2019-02-13 19:04:45,393 : Dev mean Image to text: 0.1, 0.5399999999999999, 1.02, 579.8
2019-02-13 19:04:45,394 : start epoch
2019-02-13 19:05:27,378 : samples : 64000
2019-02-13 19:05:33,693 : Image to text: 0.0, 0.1, 0.22, 2870.0
2019-02-13 19:05:41,157 : Text to Image: 0.02, 0.164, 0.34, 2152.0
2019-02-13 19:06:22,276 : samples : 128000
2019-02-13 19:06:28,598 : Image to text: 0.04, 0.1, 0.24, 2991.0
2019-02-13 19:06:35,952 : Text to Image: 0.04, 0.18, 0.328, 2154.0
2019-02-13 19:07:17,423 : samples : 192000
2019-02-13 19:07:23,856 : Image to text: 0.04, 0.18, 0.28, 2959.0
2019-02-13 19:07:31,167 : Text to Image: 0.028, 0.156, 0.312, 2152.0
2019-02-13 19:08:10,932 : samples : 256000
2019-02-13 19:08:17,437 : Image to text: 0.04, 0.14, 0.34, 2855.0
2019-02-13 19:08:24,916 : Text to Image: 0.024, 0.168, 0.296, 2161.0
2019-02-13 19:09:04,970 : samples : 320000
2019-02-13 19:09:11,542 : Image to text: 0.06, 0.14, 0.2, 2880.0
2019-02-13 19:09:19,216 : Text to Image: 0.044, 0.184, 0.348, 2160.0
2019-02-13 19:09:59,423 : samples : 384000
2019-02-13 19:10:05,959 : Image to text: 0.02, 0.16, 0.32, 2849.0
2019-02-13 19:10:13,579 : Text to Image: 0.024, 0.184, 0.4, 2147.0
2019-02-13 19:10:55,725 : samples : 448000
2019-02-13 19:11:02,200 : Image to text: 0.02, 0.12, 0.2, 2812.0
2019-02-13 19:11:09,670 : Text to Image: 0.032, 0.188, 0.36, 2160.0
2019-02-13 19:11:50,907 : samples : 512000
2019-02-13 19:11:57,393 : Image to text: 0.0, 0.08, 0.2, 2876.0
2019-02-13 19:12:04,822 : Text to Image: 0.032, 0.156, 0.312, 2157.0
2019-02-13 19:12:39,593 : Epoch 12 finished
2019-02-13 19:12:39,884 : Image to text: 0.0, 0.3, 1.1, 566.0
2019-02-13 19:12:40,206 : Text to Image: 0.22, 0.82, 1.44, 441.0
2019-02-13 19:12:40,505 : Image to text: 0.2, 0.8, 1.0, 547.0
2019-02-13 19:12:40,826 : Text to Image: 0.2, 0.78, 1.6, 438.0
2019-02-13 19:12:41,113 : Image to text: 0.1, 0.9, 1.4, 553.0
2019-02-13 19:12:41,435 : Text to Image: 0.1, 0.84, 1.7, 425.0
2019-02-13 19:12:41,734 : Image to text: 0.3, 0.7, 1.3, 595.0
2019-02-13 19:12:42,053 : Text to Image: 0.16, 0.7, 1.42, 431.0
2019-02-13 19:12:42,351 : Image to text: 0.2, 1.2, 1.9, 526.0
2019-02-13 19:12:42,672 : Text to Image: 0.2, 0.84, 1.48, 423.0
2019-02-13 19:12:42,672 : Dev mean Text to Image: 0.17600000000000002, 0.7959999999999998, 1.528, 431.6
2019-02-13 19:12:42,672 : Dev mean Image to text: 0.16, 0.78, 1.3399999999999999, 557.4000000000001
2019-02-13 19:12:45,738 : 
Test scores | Image to text:             0.16, 0.8800000000000001, 1.52, 558.2
2019-02-13 19:12:45,738 : Test scores | Text to image:             0.152, 0.732, 1.512, 433.8

2019-02-13 19:12:45,833 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 19:12:46,047 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 19:12:46,714 : loading BERT model bert-base-uncased
2019-02-13 19:12:46,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:12:46,746 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:12:46,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprkm9nw5l
2019-02-13 19:12:49,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:12:50,648 : Computing embeddings for train/dev/test
2019-02-13 19:14:26,677 : Computed embeddings
2019-02-13 19:14:26,677 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:14:51,646 : [('reg:1e-05', 86.66), ('reg:0.0001', 86.66), ('reg:0.001', 86.72), ('reg:0.01', 86.72)]
2019-02-13 19:14:51,646 : Validation : best param found is reg = 0.001 with score             86.72
2019-02-13 19:14:51,646 : Evaluating...
2019-02-13 19:14:57,920 : 
Dev acc : 86.7 Test acc : 87.1 for LENGTH classification

2019-02-13 19:14:57,921 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 19:14:58,300 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 19:14:58,345 : loading BERT model bert-base-uncased
2019-02-13 19:14:58,345 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:14:58,374 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:14:58,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1ywep2te
2019-02-13 19:15:00,828 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:15:02,271 : Computing embeddings for train/dev/test
2019-02-13 19:16:32,437 : Computed embeddings
2019-02-13 19:16:32,437 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:17:02,831 : [('reg:1e-05', 0.15), ('reg:0.0001', 0.16), ('reg:0.001', 0.14), ('reg:0.01', 0.14)]
2019-02-13 19:17:02,831 : Validation : best param found is reg = 0.0001 with score             0.16
2019-02-13 19:17:02,831 : Evaluating...
2019-02-13 19:17:11,896 : 
Dev acc : 0.2 Test acc : 0.1 for WORDCONTENT classification

2019-02-13 19:17:11,897 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 19:17:12,214 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 19:17:12,284 : loading BERT model bert-base-uncased
2019-02-13 19:17:12,284 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:17:12,309 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:17:12,309 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv1mqc1a8
2019-02-13 19:17:14,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:17:16,182 : Computing embeddings for train/dev/test
2019-02-13 19:18:40,759 : Computed embeddings
2019-02-13 19:18:40,759 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:19:09,512 : [('reg:1e-05', 18.07), ('reg:0.0001', 18.07), ('reg:0.001', 18.07), ('reg:0.01', 18.07)]
2019-02-13 19:19:09,512 : Validation : best param found is reg = 1e-05 with score             18.07
2019-02-13 19:19:09,512 : Evaluating...
2019-02-13 19:19:16,811 : 
Dev acc : 18.1 Test acc : 17.9 for DEPTH classification

2019-02-13 19:19:16,812 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 19:19:17,194 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 19:19:17,255 : loading BERT model bert-base-uncased
2019-02-13 19:19:17,256 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:19:17,367 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:19:17,368 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvvf5cj56
2019-02-13 19:19:19,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:19:21,228 : Computing embeddings for train/dev/test
2019-02-13 19:20:40,198 : Computed embeddings
2019-02-13 19:20:40,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:21:09,007 : [('reg:1e-05', 7.96), ('reg:0.0001', 8.16), ('reg:0.001', 8.03), ('reg:0.01', 7.85)]
2019-02-13 19:21:09,007 : Validation : best param found is reg = 0.0001 with score             8.16
2019-02-13 19:21:09,007 : Evaluating...
2019-02-13 19:21:16,547 : 
Dev acc : 8.2 Test acc : 8.1 for TOPCONSTITUENTS classification

2019-02-13 19:21:16,549 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 19:21:17,128 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 19:21:17,199 : loading BERT model bert-base-uncased
2019-02-13 19:21:17,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:21:17,236 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:21:17,236 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2764ren_
2019-02-13 19:21:19,684 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:21:21,125 : Computing embeddings for train/dev/test
2019-02-13 19:22:46,312 : Computed embeddings
2019-02-13 19:22:46,312 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:23:10,980 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-13 19:23:10,980 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-13 19:23:10,980 : Evaluating...
2019-02-13 19:23:17,147 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-13 19:23:17,148 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 19:23:17,525 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 19:23:17,591 : loading BERT model bert-base-uncased
2019-02-13 19:23:17,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:23:17,707 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:23:17,707 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4s4dyi3d
2019-02-13 19:23:20,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:23:21,557 : Computing embeddings for train/dev/test
2019-02-13 19:24:45,292 : Computed embeddings
2019-02-13 19:24:45,292 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:25:18,157 : [('reg:1e-05', 56.26), ('reg:0.0001', 56.26), ('reg:0.001', 56.26), ('reg:0.01', 56.26)]
2019-02-13 19:25:18,157 : Validation : best param found is reg = 1e-05 with score             56.26
2019-02-13 19:25:18,157 : Evaluating...
2019-02-13 19:25:26,704 : 
Dev acc : 56.3 Test acc : 56.2 for TENSE classification

2019-02-13 19:25:26,705 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 19:25:27,279 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 19:25:27,342 : loading BERT model bert-base-uncased
2019-02-13 19:25:27,342 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:25:27,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:25:27,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi9syvgj1
2019-02-13 19:25:29,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:25:31,219 : Computing embeddings for train/dev/test
2019-02-13 19:26:59,192 : Computed embeddings
2019-02-13 19:26:59,192 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:27:23,982 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-13 19:27:23,982 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-13 19:27:23,982 : Evaluating...
2019-02-13 19:27:30,188 : 
Dev acc : 50.0 Test acc : 50.0 for SUBJNUMBER classification

2019-02-13 19:27:30,189 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 19:27:30,825 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 19:27:30,896 : loading BERT model bert-base-uncased
2019-02-13 19:27:30,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:27:30,930 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:27:30,930 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplixcyy6x
2019-02-13 19:27:33,373 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:27:34,951 : Computing embeddings for train/dev/test
2019-02-13 19:29:02,616 : Computed embeddings
2019-02-13 19:29:02,616 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:29:34,523 : [('reg:1e-05', 50.44), ('reg:0.0001', 50.44), ('reg:0.001', 50.44), ('reg:0.01', 50.0)]
2019-02-13 19:29:34,523 : Validation : best param found is reg = 1e-05 with score             50.44
2019-02-13 19:29:34,523 : Evaluating...
2019-02-13 19:29:43,141 : 
Dev acc : 50.4 Test acc : 50.0 for OBJNUMBER classification

2019-02-13 19:29:43,142 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 19:29:43,582 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 19:29:43,655 : loading BERT model bert-base-uncased
2019-02-13 19:29:43,655 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:29:43,686 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:29:43,686 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpk1sgge81
2019-02-13 19:29:46,152 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:29:47,570 : Computing embeddings for train/dev/test
2019-02-13 19:31:27,538 : Computed embeddings
2019-02-13 19:31:27,538 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:31:54,972 : [('reg:1e-05', 50.55), ('reg:0.0001', 50.55), ('reg:0.001', 50.55), ('reg:0.01', 50.43)]
2019-02-13 19:31:54,972 : Validation : best param found is reg = 1e-05 with score             50.55
2019-02-13 19:31:54,972 : Evaluating...
2019-02-13 19:32:00,212 : 
Dev acc : 50.5 Test acc : 50.4 for ODDMANOUT classification

2019-02-13 19:32:00,213 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 19:32:00,625 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 19:32:00,701 : loading BERT model bert-base-uncased
2019-02-13 19:32:00,701 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:32:00,826 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:32:00,826 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkwa4nyln
2019-02-13 19:32:03,271 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:32:04,786 : Computing embeddings for train/dev/test
2019-02-13 19:34:35,916 : Computed embeddings
2019-02-13 19:34:35,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:35:02,350 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.06)]
2019-02-13 19:35:02,350 : Validation : best param found is reg = 0.01 with score             50.06
2019-02-13 19:35:02,350 : Evaluating...
2019-02-13 19:35:10,546 : 
Dev acc : 50.1 Test acc : 50.0 for COORDINATIONINVERSION classification

2019-02-13 19:35:10,548 : total results: {'STS12': {'MSRpar': {'pearson': (0.06519014353878502, 0.07438694735432651), 'spearman': SpearmanrResult(correlation=0.07755738603077245, pvalue=0.03369914683550406), 'nsamples': 750}, 'MSRvid': {'pearson': (0.007989156358833132, 0.8270936178395233), 'spearman': SpearmanrResult(correlation=0.009860507784090259, pvalue=0.7874701030268882), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.28371369226003196, 6.038540462922363e-10), 'spearman': SpearmanrResult(correlation=0.3244281459147074, pvalue=1.03839271327529e-12), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.276484667839894, 1.2581364603949433e-14), 'spearman': SpearmanrResult(correlation=0.36586477225740427, pvalue=3.6004632881659743e-25), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.33519792701426454, 6.223059586637765e-12), 'spearman': SpearmanrResult(correlation=0.30651154405042214, pvalue=3.987151399011364e-10), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.19371511740236175, 'wmean': 0.16931033894117772}, 'spearman': {'mean': 0.2168444712074793, 'wmean': 0.19664498861170188}}}, 'STS13': {'FNWN': {'pearson': (0.07763571021471889, 0.2883094416781043), 'spearman': SpearmanrResult(correlation=0.1126810709069134, pvalue=0.12264828412370572), 'nsamples': 189}, 'headlines': {'pearson': (-0.024757742645968892, 0.4984112984552992), 'spearman': SpearmanrResult(correlation=-0.030737187303621065, pvalue=0.40059048273926146), 'nsamples': 750}, 'OnWN': {'pearson': (-0.2764289463744574, 2.682924292094767e-11), 'spearman': SpearmanrResult(correlation=-0.2428271026919361, pvalue=5.6751203167386365e-09), 'nsamples': 561}, 'all': {'pearson': {'mean': -0.0745169929352358, 'wmean': -0.10598119777997694}, 'spearman': {'mean': -0.053627739696214594, 'wmean': -0.09198811512432355}}}, 'STS14': {'deft-forum': {'pearson': (-0.1768080177957525, 0.0001632640801861746), 'spearman': SpearmanrResult(correlation=-0.16748491291957468, pvalue=0.0003593794283401932), 'nsamples': 450}, 'deft-news': {'pearson': (-0.1011597456406334, 0.08023683113650985), 'spearman': SpearmanrResult(correlation=-0.08662205121716776, pvalue=0.13442203968269048), 'nsamples': 300}, 'headlines': {'pearson': (0.004146505020180567, 0.909738968899282), 'spearman': SpearmanrResult(correlation=-0.006310799197966122, pvalue=0.863011599637009), 'nsamples': 750}, 'images': {'pearson': (0.0632902005360681, 0.08325232206083065), 'spearman': SpearmanrResult(correlation=0.07722023599656953, pvalue=0.03448164577003555), 'nsamples': 750}, 'OnWN': {'pearson': (-0.19087561440695677, 1.385765469433715e-07), 'spearman': SpearmanrResult(correlation=-0.17688185881689747, pvalue=1.0907602515089496e-06), 'nsamples': 750}, 'tweet-news': {'pearson': (0.013352091232149532, 0.7150603135914506), 'spearman': SpearmanrResult(correlation=0.05218418834520922, pvalue=0.15337476523270424), 'nsamples': 750}, 'all': {'pearson': {'mean': -0.06467576350915742, 'wmean': -0.051327105310452696}, 'spearman': {'mean': -0.05131586630163789, 'wmean': -0.03778560038233935}}}, 'STS15': {'answers-forums': {'pearson': (-0.08019232342310517, 0.12108611219861086), 'spearman': SpearmanrResult(correlation=-0.09096678245857164, pvalue=0.07852104993787086), 'nsamples': 375}, 'answers-students': {'pearson': (0.19655305391355762, 5.735020100184965e-08), 'spearman': SpearmanrResult(correlation=0.21587997709435403, pvalue=2.3318240477972085e-09), 'nsamples': 750}, 'belief': {'pearson': (-0.08766956697545145, 0.09001630621318289), 'spearman': SpearmanrResult(correlation=-0.1250921360130106, pvalue=0.015357387818915276), 'nsamples': 375}, 'headlines': {'pearson': (0.06792055371102083, 0.0630093209402559), 'spearman': SpearmanrResult(correlation=0.0680610944005921, pvalue=0.062464863310455726), 'nsamples': 750}, 'images': {'pearson': (0.08897904716187185, 0.014786614194200388), 'spearman': SpearmanrResult(correlation=0.09558043179631819, pvalue=0.008813350234727648), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.03711815287757873, 'wmean': 0.067380427396793}, 'spearman': {'mean': 0.032692516963936415, 'wmean': 0.06787301101386829}}}, 'STS16': {'answer-answer': {'pearson': (0.15537205986127062, 0.01317207979229257), 'spearman': SpearmanrResult(correlation=0.16043866424430367, pvalue=0.010438019113019125), 'nsamples': 254}, 'headlines': {'pearson': (0.1425252002541505, 0.02450054919402335), 'spearman': SpearmanrResult(correlation=0.14528103465916165, pvalue=0.0218399230845252), 'nsamples': 249}, 'plagiarism': {'pearson': (0.07030645394071902, 0.28834682490984725), 'spearman': SpearmanrResult(correlation=0.0969058951821808, pvalue=0.14289535788727886), 'nsamples': 230}, 'postediting': {'pearson': (0.2850594649297514, 6.0646451511302325e-06), 'spearman': SpearmanrResult(correlation=0.25479274844483646, pvalue=5.668495328008193e-05), 'nsamples': 244}, 'question-question': {'pearson': (-0.02878454584172033, 0.6790766969176292), 'spearman': SpearmanrResult(correlation=-0.0307793838251481, pvalue=0.658193581318516), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.12489572662883426, 'wmean': 0.1304066625938882}, 'spearman': {'mean': 0.12532779174106687, 'wmean': 0.13065033190655154}}}, 'MR': {'devacc': 50.07, 'acc': 50.18, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 63.82, 'acc': 63.71, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 68.77, 'acc': 68.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 51.25, 'acc': 50.41, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 50.92, 'acc': 49.92, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 26.25, 'acc': 28.64, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 25.13, 'acc': 19.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.69, 'acc': 64.75, 'f1': 75.58, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 56.4, 'acc': 56.69, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.2746697275749451, 'pearson': 0.2434321998825252, 'spearman': 0.24753727352965577, 'mse': 0.9597591680469696, 'yhat': array([3.01633853, 3.11956881, 3.42923389, ..., 3.30130555, 3.57238161,        3.79356797]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.05712477508860854, 'pearson': 0.15582446126841487, 'spearman': 0.15097357916941093, 'mse': 2.506092591538535, 'yhat': array([2.19172716, 3.19927706, 2.37531832, ..., 2.96712931, 3.43266701,        3.34305847]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 36.43, 'acc': 36.18, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 4.876, 'acc': [(0.16, 0.8800000000000001, 1.52, 558.2), (0.152, 0.732, 1.512, 433.8)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 86.72, 'acc': 87.14, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.16, 'acc': 0.06, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 18.07, 'acc': 17.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 8.16, 'acc': 8.08, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 56.26, 'acc': 56.19, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 50.44, 'acc': 50.04, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.55, 'acc': 50.44, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 50.06, 'acc': 50.02, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 19:35:10,548 : STS12 p=0.1693, STS12 s=0.1966, STS13 p=-0.1060, STS13 s=-0.0920, STS14 p=-0.0513, STS14 s=-0.0378, STS15 p=0.0674, STS15 s=0.0679, STS 16 p=0.1304, STS16 s=0.1307, STS B p=0.1558, STS B s=0.1510, STS B m=2.5061, SICK-R p=0.2434, SICK-R s=0.2475, SICK-P m=0.9598
2019-02-13 19:35:10,549 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-13 19:35:10,549 : 0.1693,0.1966,-0.1060,-0.0920,-0.0513,-0.0378,0.0674,0.0679,0.1304,0.1307,0.1558,0.1510,2.5061,0.2434,0.2475,0.9598
2019-02-13 19:35:10,549 : MR=50.18, CR=63.71, SUBJ=50.41, MPQA=68.77, SST-B=49.92, SST-F=28.64, TREC=19.60, SICK-E=56.69, SNLI=36.18, MRPC=64.75, MRPC f=75.58
2019-02-13 19:35:10,549 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-13 19:35:10,549 : 50.18,63.71,50.41,68.77,49.92,28.64,19.60,56.69,36.18,64.75,75.58
2019-02-13 19:35:10,549 : COCO r1i2t=0.16, COCO r5i2t=0.88, COCO r10i2t=1.52, COCO medr_i2t=558.20, COCO r1t2i=0.15, COCO r5t2i=0.73, COCO r10t2i=1.51, COCO medr_t2i=433.80
2019-02-13 19:35:10,549 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-13 19:35:10,549 : 0.16,0.88,1.52,558.20,0.15,0.73,1.51,433.80
2019-02-13 19:35:10,549 : SentLen=87.14, WC=0.06, TreeDepth=17.88, TopConst=8.08, BShift=50.00, Tense=56.19, SubjNum=50.00, ObjNum=50.04, SOMO=50.44, CoordInv=50.02, average=41.98
2019-02-13 19:35:10,549 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-13 19:35:10,549 : 87.14,0.06,17.88,8.08,50.00,56.19,50.00,50.04,50.44,50.02,41.98
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,549 : layer 1
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,549 : ********************************************************************************
2019-02-13 19:35:10,639 : ***** Transfer task : STS12 *****


2019-02-13 19:35:10,652 : loading BERT model bert-base-uncased
2019-02-13 19:35:10,652 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:10,670 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:10,670 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi7wb3iwd
2019-02-13 19:35:13,157 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:16,381 : MSRpar : pearson = 0.0687, spearman = 0.0974
2019-02-13 19:35:17,051 : MSRvid : pearson = 0.0516, spearman = 0.0885
2019-02-13 19:35:17,623 : SMTeuroparl : pearson = 0.3139, spearman = 0.4121
2019-02-13 19:35:18,645 : surprise.OnWN : pearson = 0.3698, spearman = 0.4177
2019-02-13 19:35:19,219 : surprise.SMTnews : pearson = 0.4037, spearman = 0.3970
2019-02-13 19:35:19,219 : ALL (weighted average) : Pearson = 0.2164,             Spearman = 0.2575
2019-02-13 19:35:19,219 : ALL (average) : Pearson = 0.2415,             Spearman = 0.2825

2019-02-13 19:35:19,219 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 19:35:19,229 : loading BERT model bert-base-uncased
2019-02-13 19:35:19,229 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:19,247 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:19,248 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpagq7mum0
2019-02-13 19:35:21,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:23,842 : FNWN : pearson = 0.1704, spearman = 0.1944
2019-02-13 19:35:25,796 : headlines : pearson = 0.0664, spearman = 0.0828
2019-02-13 19:35:26,816 : OnWN : pearson = -0.1455, spearman = -0.0969
2019-02-13 19:35:26,816 : ALL (weighted average) : Pearson = 0.0003,             Spearman = 0.0296
2019-02-13 19:35:26,816 : ALL (average) : Pearson = 0.0305,             Spearman = 0.0601

2019-02-13 19:35:26,816 : ***** Transfer task : STS14 *****


2019-02-13 19:35:26,831 : loading BERT model bert-base-uncased
2019-02-13 19:35:26,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:26,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:26,849 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeez7y25f
2019-02-13 19:35:29,286 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:31,742 : deft-forum : pearson = -0.1402, spearman = -0.1193
2019-02-13 19:35:32,626 : deft-news : pearson = -0.0097, spearman = 0.0299
2019-02-13 19:35:34,065 : headlines : pearson = 0.1168, spearman = 0.1187
2019-02-13 19:35:35,480 : images : pearson = 0.1012, spearman = 0.1495
2019-02-13 19:35:36,949 : OnWN : pearson = -0.0174, spearman = 0.0449
2019-02-13 19:35:38,649 : tweet-news : pearson = 0.1662, spearman = 0.1804
2019-02-13 19:35:38,649 : ALL (weighted average) : Pearson = 0.0557,             Spearman = 0.0868
2019-02-13 19:35:38,649 : ALL (average) : Pearson = 0.0361,             Spearman = 0.0673

2019-02-13 19:35:38,649 : ***** Transfer task : STS15 *****


2019-02-13 19:35:38,710 : loading BERT model bert-base-uncased
2019-02-13 19:35:38,710 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:38,728 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:38,728 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7rlkxwuc
2019-02-13 19:35:41,169 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:43,765 : answers-forums : pearson = -0.0366, spearman = -0.0508
2019-02-13 19:35:45,253 : answers-students : pearson = 0.2440, spearman = 0.2350
2019-02-13 19:35:46,359 : belief : pearson = -0.0217, spearman = -0.0359
2019-02-13 19:35:47,941 : headlines : pearson = 0.1962, spearman = 0.2164
2019-02-13 19:35:49,490 : images : pearson = 0.1096, spearman = 0.1514
2019-02-13 19:35:49,490 : ALL (weighted average) : Pearson = 0.1302,             Spearman = 0.1398
2019-02-13 19:35:49,490 : ALL (average) : Pearson = 0.0983,             Spearman = 0.1032

2019-02-13 19:35:49,490 : ***** Transfer task : STS16 *****


2019-02-13 19:35:49,561 : loading BERT model bert-base-uncased
2019-02-13 19:35:49,561 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:49,579 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:49,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjt4ari_q
2019-02-13 19:35:52,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:35:54,123 : answer-answer : pearson = 0.2150, spearman = 0.2482
2019-02-13 19:35:54,665 : headlines : pearson = 0.2714, spearman = 0.2832
2019-02-13 19:35:55,254 : plagiarism : pearson = 0.1070, spearman = 0.1361
2019-02-13 19:35:56,056 : postediting : pearson = 0.3810, spearman = 0.4203
2019-02-13 19:35:56,573 : question-question : pearson = -0.0285, spearman = -0.0246
2019-02-13 19:35:56,574 : ALL (weighted average) : Pearson = 0.1971,             Spearman = 0.2212
2019-02-13 19:35:56,574 : ALL (average) : Pearson = 0.1892,             Spearman = 0.2127

2019-02-13 19:35:56,574 : ***** Transfer task : MR *****


2019-02-13 19:35:56,591 : loading BERT model bert-base-uncased
2019-02-13 19:35:56,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:35:56,612 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:35:56,612 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoe6lutcr
2019-02-13 19:35:59,053 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:36:00,634 : Generating sentence embeddings
2019-02-13 19:36:18,575 : Generated sentence embeddings
2019-02-13 19:36:18,576 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 19:36:45,523 : Best param found at split 1: l2reg = 0.001                 with score 65.21
2019-02-13 19:37:12,680 : Best param found at split 2: l2reg = 0.0001                 with score 61.71
2019-02-13 19:37:28,040 : Best param found at split 3: l2reg = 0.001                 with score 59.38
2019-02-13 19:37:41,697 : Best param found at split 4: l2reg = 0.0001                 with score 61.09
2019-02-13 19:37:55,004 : Best param found at split 5: l2reg = 1e-05                 with score 60.22
2019-02-13 19:37:55,708 : Dev acc : 61.52 Test acc : 55.9

2019-02-13 19:37:55,709 : ***** Transfer task : CR *****


2019-02-13 19:37:55,717 : loading BERT model bert-base-uncased
2019-02-13 19:37:55,717 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:37:55,736 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:37:55,736 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps62dqj4b
2019-02-13 19:37:58,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:37:59,580 : Generating sentence embeddings
2019-02-13 19:38:03,018 : Generated sentence embeddings
2019-02-13 19:38:03,019 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 19:38:08,688 : Best param found at split 1: l2reg = 1e-05                 with score 66.41
2019-02-13 19:38:14,697 : Best param found at split 2: l2reg = 1e-05                 with score 66.18
2019-02-13 19:38:22,349 : Best param found at split 3: l2reg = 1e-05                 with score 67.52
2019-02-13 19:38:30,084 : Best param found at split 4: l2reg = 1e-05                 with score 67.76
2019-02-13 19:38:37,246 : Best param found at split 5: l2reg = 0.0001                 with score 66.37
2019-02-13 19:38:37,620 : Dev acc : 66.85 Test acc : 64.98

2019-02-13 19:38:37,620 : ***** Transfer task : MPQA *****


2019-02-13 19:38:37,626 : loading BERT model bert-base-uncased
2019-02-13 19:38:37,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:38:37,645 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:38:37,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7un2y8ga
2019-02-13 19:38:40,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:38:41,588 : Generating sentence embeddings
2019-02-13 19:38:51,003 : Generated sentence embeddings
2019-02-13 19:38:51,003 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 19:39:16,754 : Best param found at split 1: l2reg = 0.0001                 with score 81.31
2019-02-13 19:39:41,164 : Best param found at split 2: l2reg = 1e-05                 with score 82.08
2019-02-13 19:39:53,992 : Best param found at split 3: l2reg = 1e-05                 with score 81.03
2019-02-13 19:40:09,444 : Best param found at split 4: l2reg = 1e-05                 with score 82.79
2019-02-13 19:40:24,780 : Best param found at split 5: l2reg = 1e-05                 with score 83.27
2019-02-13 19:40:26,146 : Dev acc : 82.1 Test acc : 81.73

2019-02-13 19:40:26,147 : ***** Transfer task : SUBJ *****


2019-02-13 19:40:26,161 : loading BERT model bert-base-uncased
2019-02-13 19:40:26,161 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:40:26,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:40:26,183 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpazddcb3d
2019-02-13 19:40:28,651 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:40:30,140 : Generating sentence embeddings
2019-02-13 19:40:44,440 : Generated sentence embeddings
2019-02-13 19:40:44,440 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 19:41:08,265 : Best param found at split 1: l2reg = 0.0001                 with score 86.55
2019-02-13 19:41:33,401 : Best param found at split 2: l2reg = 1e-05                 with score 87.49
2019-02-13 19:42:01,564 : Best param found at split 3: l2reg = 0.0001                 with score 86.46
2019-02-13 19:42:17,170 : Best param found at split 4: l2reg = 0.001                 with score 86.48
2019-02-13 19:42:31,277 : Best param found at split 5: l2reg = 0.0001                 with score 86.52
2019-02-13 19:42:32,018 : Dev acc : 86.7 Test acc : 86.57

2019-02-13 19:42:32,019 : ***** Transfer task : SST Binary classification *****


2019-02-13 19:42:32,149 : loading BERT model bert-base-uncased
2019-02-13 19:42:32,149 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:42:32,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:42:32,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd9knl0th
2019-02-13 19:42:34,621 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:42:36,080 : Computing embedding for train
2019-02-13 19:43:27,775 : Computed train embeddings
2019-02-13 19:43:27,775 : Computing embedding for dev
2019-02-13 19:43:28,871 : Computed dev embeddings
2019-02-13 19:43:28,871 : Computing embedding for test
2019-02-13 19:43:31,276 : Computed test embeddings
2019-02-13 19:43:31,276 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:44:09,195 : [('reg:1e-05', 71.79), ('reg:0.0001', 70.87), ('reg:0.001', 68.35), ('reg:0.01', 64.45)]
2019-02-13 19:44:09,195 : Validation : best param found is reg = 1e-05 with score             71.79
2019-02-13 19:44:09,195 : Evaluating...
2019-02-13 19:44:18,281 : 
Dev acc : 71.79 Test acc : 73.42 for             SST Binary classification

2019-02-13 19:44:18,282 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 19:44:18,335 : loading BERT model bert-base-uncased
2019-02-13 19:44:18,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:44:18,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:44:18,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp8x_mrwn
2019-02-13 19:44:20,803 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:44:22,301 : Computing embedding for train
2019-02-13 19:44:36,690 : Computed train embeddings
2019-02-13 19:44:36,690 : Computing embedding for dev
2019-02-13 19:44:38,625 : Computed dev embeddings
2019-02-13 19:44:38,625 : Computing embedding for test
2019-02-13 19:44:42,601 : Computed test embeddings
2019-02-13 19:44:42,601 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:44:48,058 : [('reg:1e-05', 32.15), ('reg:0.0001', 33.7), ('reg:0.001', 32.97), ('reg:0.01', 30.25)]
2019-02-13 19:44:48,058 : Validation : best param found is reg = 0.0001 with score             33.7
2019-02-13 19:44:48,058 : Evaluating...
2019-02-13 19:44:49,235 : 
Dev acc : 33.7 Test acc : 32.08 for             SST Fine-Grained classification

2019-02-13 19:44:49,236 : ***** Transfer task : TREC *****


2019-02-13 19:44:49,249 : loading BERT model bert-base-uncased
2019-02-13 19:44:49,249 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:44:49,268 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:44:49,268 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpme1uab57
2019-02-13 19:44:51,698 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:44:58,901 : Computed train embeddings
2019-02-13 19:44:59,402 : Computed test embeddings
2019-02-13 19:44:59,403 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 19:45:10,276 : [('reg:1e-05', 55.58), ('reg:0.0001', 55.08), ('reg:0.001', 47.71), ('reg:0.01', 43.78)]
2019-02-13 19:45:10,277 : Cross-validation : best param found is reg = 1e-05             with score 55.58
2019-02-13 19:45:10,277 : Evaluating...
2019-02-13 19:45:10,816 : 
Dev acc : 55.58 Test acc : 67.2             for TREC

2019-02-13 19:45:10,817 : ***** Transfer task : MRPC *****


2019-02-13 19:45:10,839 : loading BERT model bert-base-uncased
2019-02-13 19:45:10,839 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:45:10,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:45:10,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsd0yuyyl
2019-02-13 19:45:13,340 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:45:14,798 : Computing embedding for train
2019-02-13 19:45:25,616 : Computed train embeddings
2019-02-13 19:45:25,616 : Computing embedding for test
2019-02-13 19:45:30,296 : Computed test embeddings
2019-02-13 19:45:30,313 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 19:45:35,615 : [('reg:1e-05', 68.89), ('reg:0.0001', 67.66), ('reg:0.001', 68.33), ('reg:0.01', 68.25)]
2019-02-13 19:45:35,615 : Cross-validation : best param found is reg = 1e-05             with score 68.89
2019-02-13 19:45:35,615 : Evaluating...
2019-02-13 19:45:36,078 : Dev acc : 68.89 Test acc 68.93; Test F1 80.66 for MRPC.

2019-02-13 19:45:36,078 : ***** Transfer task : SICK-Entailment*****


2019-02-13 19:45:36,145 : loading BERT model bert-base-uncased
2019-02-13 19:45:36,146 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:45:36,165 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:45:36,165 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp52w5in0c
2019-02-13 19:45:38,613 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:45:40,100 : Computing embedding for train
2019-02-13 19:45:46,519 : Computed train embeddings
2019-02-13 19:45:46,519 : Computing embedding for dev
2019-02-13 19:45:47,344 : Computed dev embeddings
2019-02-13 19:45:47,344 : Computing embedding for test
2019-02-13 19:45:53,914 : Computed test embeddings
2019-02-13 19:45:53,942 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 19:45:55,727 : [('reg:1e-05', 68.4), ('reg:0.0001', 68.8), ('reg:0.001', 59.4), ('reg:0.01', 57.2)]
2019-02-13 19:45:55,727 : Validation : best param found is reg = 0.0001 with score             68.8
2019-02-13 19:45:55,727 : Evaluating...
2019-02-13 19:45:56,194 : 
Dev acc : 68.8 Test acc : 66.63 for                        SICK entailment

2019-02-13 19:45:56,194 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 19:45:56,223 : loading BERT model bert-base-uncased
2019-02-13 19:45:56,223 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:45:56,243 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:45:56,244 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6g4h31tv
2019-02-13 19:45:58,746 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:46:00,243 : Computing embedding for train
2019-02-13 19:46:07,418 : Computed train embeddings
2019-02-13 19:46:07,418 : Computing embedding for dev
2019-02-13 19:46:08,325 : Computed dev embeddings
2019-02-13 19:46:08,325 : Computing embedding for test
2019-02-13 19:46:16,888 : Computed test embeddings
2019-02-13 19:47:18,563 : Dev : Pearson 0.6285206911444029
2019-02-13 19:47:18,563 : Test : Pearson 0.6325756375157819 Spearman 0.5773794338596717 MSE 0.6155502234112711                        for SICK Relatedness

2019-02-13 19:47:18,564 : 

***** Transfer task : STSBenchmark*****


2019-02-13 19:47:18,615 : loading BERT model bert-base-uncased
2019-02-13 19:47:18,615 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:47:18,640 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:47:18,640 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi0tr5a3i
2019-02-13 19:47:21,073 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:47:22,602 : Computing embedding for train
2019-02-13 19:47:35,755 : Computed train embeddings
2019-02-13 19:47:35,755 : Computing embedding for dev
2019-02-13 19:47:39,489 : Computed dev embeddings
2019-02-13 19:47:39,490 : Computing embedding for test
2019-02-13 19:47:43,273 : Computed test embeddings
2019-02-13 19:48:20,749 : Dev : Pearson 0.39438672521957946
2019-02-13 19:48:20,749 : Test : Pearson 0.4013450438392224 Spearman 0.38778956174024726 MSE 2.0274987578665833                        for SICK Relatedness

2019-02-13 19:48:20,750 : ***** Transfer task : SNLI Entailment*****


2019-02-13 19:48:25,507 : loading BERT model bert-base-uncased
2019-02-13 19:48:25,507 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 19:48:25,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 19:48:25,632 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7r_mkbh9
2019-02-13 19:48:28,071 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 19:48:29,848 : PROGRESS (encoding): 0.00%
2019-02-13 19:50:49,244 : PROGRESS (encoding): 14.56%
2019-02-13 19:53:11,696 : PROGRESS (encoding): 29.12%
2019-02-13 19:55:11,742 : PROGRESS (encoding): 43.69%
2019-02-13 19:57:40,131 : PROGRESS (encoding): 58.25%
2019-02-13 20:00:26,785 : PROGRESS (encoding): 72.81%
2019-02-13 20:02:38,615 : PROGRESS (encoding): 87.37%
2019-02-13 20:04:46,122 : PROGRESS (encoding): 0.00%
2019-02-13 20:05:02,117 : PROGRESS (encoding): 0.00%
2019-02-13 20:05:16,367 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 20:06:07,320 : [('reg:1e-09', 58.69)]
2019-02-13 20:06:07,321 : Validation : best param found is reg = 1e-09 with score             58.69
2019-02-13 20:06:07,321 : Evaluating...
2019-02-13 20:07:03,109 : Dev acc : 58.69 Test acc : 59.4 for SNLI

2019-02-13 20:07:03,109 : ***** Transfer task: Image Caption Retrieval *****


2019-02-13 20:07:11,830 : loading BERT model bert-base-uncased
2019-02-13 20:07:11,831 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 20:07:11,877 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 20:07:11,877 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5u101q6o
2019-02-13 20:07:14,311 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 20:07:15,772 : Computing embedding for train
2019-02-13 20:16:55,264 : Computed train embeddings
2019-02-13 20:16:55,265 : Computing embedding for dev
2019-02-13 20:17:21,255 : Computed dev embeddings
2019-02-13 20:17:21,255 : Computing embedding for test
2019-02-13 20:17:48,697 : Computed test embeddings
2019-02-13 20:17:48,716 : prepare data
2019-02-13 20:17:48,778 : start epoch
2019-02-13 20:18:32,927 : samples : 64000
2019-02-13 20:18:45,511 : Image to text: 2.8, 10.14, 16.48, 59.0
2019-02-13 20:18:55,441 : Text to Image: 2.032, 7.58, 12.676, 77.0
2019-02-13 20:19:39,244 : samples : 128000
2019-02-13 20:19:51,775 : Image to text: 4.08, 13.64, 22.08, 43.0
2019-02-13 20:20:01,797 : Text to Image: 2.796, 10.292, 16.868, 57.0
2019-02-13 20:20:45,065 : samples : 192000
2019-02-13 20:20:57,657 : Image to text: 4.16, 14.46, 22.74, 38.0
2019-02-13 20:21:07,680 : Text to Image: 3.36, 11.792, 19.284, 48.0
2019-02-13 20:21:51,060 : samples : 256000
2019-02-13 20:22:03,714 : Image to text: 4.9, 15.52, 23.94, 38.0
2019-02-13 20:22:13,745 : Text to Image: 3.308, 11.524, 19.184, 48.0
2019-02-13 20:22:57,447 : samples : 320000
2019-02-13 20:23:10,053 : Image to text: 4.94, 15.42, 24.52, 38.0
2019-02-13 20:23:20,057 : Text to Image: 3.272, 11.968, 19.644, 48.0
2019-02-13 20:24:02,525 : samples : 384000
2019-02-13 20:24:15,181 : Image to text: 4.68, 17.2, 26.18, 34.0
2019-02-13 20:24:25,195 : Text to Image: 4.288, 14.448, 22.928, 39.0
2019-02-13 20:25:07,898 : samples : 448000
2019-02-13 20:25:20,550 : Image to text: 5.78, 18.12, 28.0, 31.0
2019-02-13 20:25:30,532 : Text to Image: 4.308, 14.96, 23.824, 37.0
2019-02-13 20:26:13,378 : samples : 512000
2019-02-13 20:26:26,036 : Image to text: 5.28, 17.04, 27.1, 32.0
2019-02-13 20:26:36,085 : Text to Image: 3.796, 13.44, 21.592, 42.0
2019-02-13 20:27:13,428 : Epoch 1 finished
2019-02-13 20:27:14,335 : Image to text: 18.4, 46.5, 59.6, 6.0
2019-02-13 20:27:15,152 : Text to Image: 14.1, 39.82, 56.26, 8.0
2019-02-13 20:27:16,084 : Image to text: 15.7, 44.6, 62.2, 7.0
2019-02-13 20:27:16,821 : Text to Image: 12.98, 40.08, 56.7, 8.0
2019-02-13 20:27:17,887 : Image to text: 15.9, 43.5, 59.7, 7.0
2019-02-13 20:27:18,664 : Text to Image: 12.4, 39.66, 56.38, 8.0
2019-02-13 20:27:19,599 : Image to text: 18.7, 46.8, 62.1, 6.0
2019-02-13 20:27:20,381 : Text to Image: 12.94, 38.9, 55.8, 8.0
2019-02-13 20:27:21,290 : Image to text: 17.6, 47.3, 63.0, 6.0
2019-02-13 20:27:22,064 : Text to Image: 13.28, 41.26, 57.2, 8.0
2019-02-13 20:27:22,065 : Dev mean Text to Image: 13.14, 39.944, 56.46799999999999, 8.0
2019-02-13 20:27:22,065 : Dev mean Image to text: 17.26, 45.74, 61.32, 6.3999999999999995
2019-02-13 20:27:22,065 : start epoch
2019-02-13 20:28:07,008 : samples : 64000
2019-02-13 20:28:18,189 : Image to text: 5.38, 18.02, 28.68, 30.0
2019-02-13 20:28:25,344 : Text to Image: 4.432, 14.932, 23.916, 36.0
2019-02-13 20:29:06,205 : samples : 128000
2019-02-13 20:29:16,110 : Image to text: 5.82, 17.98, 28.14, 29.0
2019-02-13 20:29:22,840 : Text to Image: 4.48, 15.332, 24.44, 35.0
2019-02-13 20:30:06,958 : samples : 192000
2019-02-13 20:30:19,844 : Image to text: 5.06, 17.02, 27.4, 30.0
2019-02-13 20:30:30,335 : Text to Image: 4.476, 15.212, 23.848, 38.0
2019-02-13 20:31:15,485 : samples : 256000
2019-02-13 20:31:28,473 : Image to text: 6.06, 19.22, 29.36, 29.0
2019-02-13 20:31:38,979 : Text to Image: 4.988, 16.844, 26.096, 33.0
2019-02-13 20:32:24,595 : samples : 320000
2019-02-13 20:32:37,629 : Image to text: 6.0, 18.9, 29.84, 27.0
2019-02-13 20:32:48,142 : Text to Image: 4.868, 16.48, 25.7, 33.0
2019-02-13 20:33:33,745 : samples : 384000
2019-02-13 20:33:46,759 : Image to text: 5.64, 19.04, 28.46, 29.0
2019-02-13 20:33:57,281 : Text to Image: 4.432, 15.508, 24.712, 35.0
2019-02-13 20:34:42,664 : samples : 448000
2019-02-13 20:34:55,686 : Image to text: 6.1, 19.9, 30.58, 27.0
2019-02-13 20:35:06,212 : Text to Image: 4.852, 16.388, 26.024, 33.0
2019-02-13 20:35:51,346 : samples : 512000
2019-02-13 20:36:04,245 : Image to text: 6.3, 20.28, 30.5, 26.0
2019-02-13 20:36:14,703 : Text to Image: 5.124, 16.792, 25.976, 33.0
2019-02-13 20:36:53,185 : Epoch 2 finished
2019-02-13 20:36:54,272 : Image to text: 18.5, 49.4, 65.2, 6.0
2019-02-13 20:36:55,091 : Text to Image: 16.14, 43.24, 59.98, 7.0
2019-02-13 20:36:56,154 : Image to text: 17.6, 48.3, 63.7, 6.0
2019-02-13 20:36:57,051 : Text to Image: 14.84, 43.58, 61.12, 7.0
2019-02-13 20:36:58,192 : Image to text: 18.3, 48.1, 64.2, 6.0
2019-02-13 20:36:59,098 : Text to Image: 15.36, 43.42, 60.88, 7.0
2019-02-13 20:37:00,173 : Image to text: 20.1, 45.3, 63.9, 6.0
2019-02-13 20:37:01,086 : Text to Image: 14.78, 42.58, 60.46, 7.0
2019-02-13 20:37:02,238 : Image to text: 18.2, 48.8, 63.8, 6.0
2019-02-13 20:37:03,097 : Text to Image: 15.76, 44.62, 60.44, 7.0
2019-02-13 20:37:03,097 : Dev mean Text to Image: 15.376000000000001, 43.488, 60.576, 7.0
2019-02-13 20:37:03,097 : Dev mean Image to text: 18.540000000000003, 47.98, 64.16000000000001, 6.0
2019-02-13 20:37:03,097 : start epoch
2019-02-13 20:37:48,458 : samples : 64000
2019-02-13 20:37:59,671 : Image to text: 5.96, 19.76, 30.56, 26.0
2019-02-13 20:38:07,094 : Text to Image: 4.476, 15.812, 25.152, 34.0
2019-02-13 20:38:47,581 : samples : 128000
2019-02-13 20:38:57,909 : Image to text: 5.94, 19.38, 30.32, 27.0
2019-02-13 20:39:05,350 : Text to Image: 4.608, 16.204, 25.704, 33.0
2019-02-13 20:39:47,571 : samples : 192000
2019-02-13 20:39:57,944 : Image to text: 6.24, 20.14, 30.82, 27.0
2019-02-13 20:40:05,409 : Text to Image: 5.332, 17.916, 27.984, 30.0
2019-02-13 20:40:46,064 : samples : 256000
2019-02-13 20:40:56,395 : Image to text: 6.5, 20.82, 30.96, 25.0
2019-02-13 20:41:03,831 : Text to Image: 5.716, 18.152, 28.236, 29.0
2019-02-13 20:41:44,431 : samples : 320000
2019-02-13 20:41:54,783 : Image to text: 6.68, 21.2, 31.26, 25.0
2019-02-13 20:42:02,218 : Text to Image: 5.48, 17.912, 27.884, 31.0
2019-02-13 20:42:42,414 : samples : 384000
2019-02-13 20:42:52,752 : Image to text: 6.28, 20.44, 30.14, 27.0
2019-02-13 20:43:00,191 : Text to Image: 5.328, 17.012, 26.124, 32.0
2019-02-13 20:43:40,442 : samples : 448000
2019-02-13 20:43:50,686 : Image to text: 6.86, 22.08, 32.72, 24.0
2019-02-13 20:43:58,066 : Text to Image: 5.656, 18.968, 28.92, 29.0
2019-02-13 20:44:38,559 : samples : 512000
2019-02-13 20:44:48,845 : Image to text: 6.42, 20.42, 30.86, 26.0
2019-02-13 20:44:56,212 : Text to Image: 4.876, 16.128, 26.044, 34.0
2019-02-13 20:45:31,659 : Epoch 3 finished
2019-02-13 20:45:32,139 : Image to text: 21.0, 49.2, 66.0, 6.0
2019-02-13 20:45:32,480 : Text to Image: 16.1, 44.94, 62.22, 7.0
2019-02-13 20:45:32,966 : Image to text: 20.3, 50.7, 64.9, 5.0
2019-02-13 20:45:33,307 : Text to Image: 16.2, 44.08, 62.38, 7.0
2019-02-13 20:45:33,796 : Image to text: 18.0, 50.7, 66.1, 5.0
2019-02-13 20:45:34,137 : Text to Image: 14.96, 43.78, 62.02, 7.0
2019-02-13 20:45:34,622 : Image to text: 21.9, 49.0, 65.6, 6.0
2019-02-13 20:45:34,967 : Text to Image: 15.48, 44.04, 62.14, 7.0
2019-02-13 20:45:35,364 : Image to text: 19.9, 49.2, 64.7, 6.0
2019-02-13 20:45:35,665 : Text to Image: 15.42, 45.04, 61.52, 7.0
2019-02-13 20:45:35,665 : Dev mean Text to Image: 15.632, 44.37599999999999, 62.056, 7.0
2019-02-13 20:45:35,665 : Dev mean Image to text: 20.220000000000002, 49.760000000000005, 65.46, 5.6000000000000005
2019-02-13 20:45:35,666 : start epoch
2019-02-13 20:46:15,877 : samples : 64000
2019-02-13 20:46:26,161 : Image to text: 7.46, 22.3, 32.96, 23.0
2019-02-13 20:46:33,591 : Text to Image: 6.128, 19.452, 29.784, 27.0
2019-02-13 20:47:13,982 : samples : 128000
2019-02-13 20:47:24,302 : Image to text: 7.22, 21.8, 32.76, 24.0
2019-02-13 20:47:31,747 : Text to Image: 5.44, 18.16, 27.58, 30.0
2019-02-13 20:48:12,071 : samples : 192000
2019-02-13 20:48:22,404 : Image to text: 6.96, 21.4, 33.02, 24.0
2019-02-13 20:48:29,876 : Text to Image: 5.82, 18.316, 28.292, 29.0
2019-02-13 20:49:11,376 : samples : 256000
2019-02-13 20:49:21,683 : Image to text: 7.76, 22.24, 33.56, 23.0
2019-02-13 20:49:29,078 : Text to Image: 5.88, 19.044, 28.952, 28.0
2019-02-13 20:50:10,763 : samples : 320000
2019-02-13 20:50:21,059 : Image to text: 7.78, 22.98, 33.1, 23.0
2019-02-13 20:50:28,460 : Text to Image: 6.244, 19.808, 30.432, 27.0
2019-02-13 20:51:10,534 : samples : 384000
2019-02-13 20:51:20,765 : Image to text: 7.36, 21.74, 33.06, 23.0
2019-02-13 20:51:28,169 : Text to Image: 5.704, 18.944, 29.124, 28.0
2019-02-13 20:52:10,426 : samples : 448000
2019-02-13 20:52:20,634 : Image to text: 7.6, 22.52, 33.82, 23.0
2019-02-13 20:52:28,043 : Text to Image: 6.424, 19.716, 30.184, 27.0
2019-02-13 20:53:10,152 : samples : 512000
2019-02-13 20:53:20,379 : Image to text: 7.28, 23.12, 34.22, 22.0
2019-02-13 20:53:27,778 : Text to Image: 6.76, 20.768, 31.068, 25.0
2019-02-13 20:54:02,027 : Epoch 4 finished
2019-02-13 20:54:02,502 : Image to text: 20.1, 49.9, 67.4, 6.0
2019-02-13 20:54:02,841 : Text to Image: 16.3, 43.46, 61.12, 7.0
2019-02-13 20:54:03,319 : Image to text: 20.5, 50.1, 67.7, 5.0
2019-02-13 20:54:03,658 : Text to Image: 15.24, 43.52, 61.06, 7.0
2019-02-13 20:54:04,138 : Image to text: 21.0, 51.7, 67.1, 5.0
2019-02-13 20:54:04,478 : Text to Image: 16.62, 44.48, 61.98, 7.0
2019-02-13 20:54:04,953 : Image to text: 21.2, 50.7, 68.3, 5.0
2019-02-13 20:54:05,293 : Text to Image: 15.9, 43.34, 60.4, 7.0
2019-02-13 20:54:05,756 : Image to text: 21.9, 51.6, 68.3, 5.0
2019-02-13 20:54:06,096 : Text to Image: 16.3, 45.16, 61.54, 7.0
2019-02-13 20:54:06,096 : Dev mean Text to Image: 16.072, 43.992000000000004, 61.22, 7.0
2019-02-13 20:54:06,096 : Dev mean Image to text: 20.94, 50.800000000000004, 67.75999999999999, 5.2
2019-02-13 20:54:06,097 : start epoch
2019-02-13 20:54:45,972 : samples : 64000
2019-02-13 20:54:56,205 : Image to text: 8.36, 22.22, 34.02, 22.0
2019-02-13 20:55:03,617 : Text to Image: 6.452, 20.228, 30.68, 26.0
2019-02-13 20:55:44,071 : samples : 128000
2019-02-13 20:55:54,310 : Image to text: 7.42, 22.32, 33.62, 23.0
2019-02-13 20:56:01,729 : Text to Image: 6.14, 19.712, 30.068, 27.0
2019-02-13 20:56:41,907 : samples : 192000
2019-02-13 20:56:52,155 : Image to text: 7.68, 23.78, 35.0, 21.0
2019-02-13 20:56:59,553 : Text to Image: 6.088, 19.784, 30.388, 27.0
2019-02-13 20:57:39,928 : samples : 256000
2019-02-13 20:57:50,205 : Image to text: 6.88, 22.56, 34.02, 23.0
2019-02-13 20:57:57,590 : Text to Image: 6.116, 20.132, 30.608, 26.0
2019-02-13 20:58:39,908 : samples : 320000
2019-02-13 20:58:50,125 : Image to text: 7.38, 22.2, 33.38, 22.0
2019-02-13 20:58:57,536 : Text to Image: 5.896, 18.968, 29.356, 27.0
2019-02-13 20:59:41,911 : samples : 384000
2019-02-13 20:59:52,277 : Image to text: 7.34, 22.62, 34.44, 20.0
2019-02-13 20:59:59,743 : Text to Image: 6.424, 20.604, 31.188, 26.0
2019-02-13 21:00:41,045 : samples : 448000
2019-02-13 21:00:51,306 : Image to text: 7.6, 22.78, 34.34, 21.0
2019-02-13 21:00:58,732 : Text to Image: 6.84, 21.676, 32.176, 24.0
2019-02-13 21:01:39,946 : samples : 512000
2019-02-13 21:01:50,212 : Image to text: 7.74, 23.3, 35.1, 22.0
2019-02-13 21:01:57,687 : Text to Image: 6.336, 20.388, 31.12, 26.0
2019-02-13 21:02:35,617 : Epoch 5 finished
2019-02-13 21:02:36,093 : Image to text: 22.8, 50.4, 66.8, 5.0
2019-02-13 21:02:36,432 : Text to Image: 17.4, 47.14, 63.9, 6.0
2019-02-13 21:02:36,910 : Image to text: 19.4, 53.0, 68.0, 5.0
2019-02-13 21:02:37,250 : Text to Image: 16.58, 45.6, 63.26, 6.0
2019-02-13 21:02:37,729 : Image to text: 19.9, 50.2, 66.0, 5.0
2019-02-13 21:02:38,069 : Text to Image: 16.8, 46.54, 63.02, 6.0
2019-02-13 21:02:38,544 : Image to text: 20.7, 53.0, 68.7, 5.0
2019-02-13 21:02:38,883 : Text to Image: 17.32, 46.56, 64.2, 6.0
2019-02-13 21:02:39,354 : Image to text: 21.8, 53.2, 68.4, 5.0
2019-02-13 21:02:39,694 : Text to Image: 17.58, 46.98, 63.16, 6.0
2019-02-13 21:02:39,694 : Dev mean Text to Image: 17.136, 46.56400000000001, 63.508, 6.0
2019-02-13 21:02:39,694 : Dev mean Image to text: 20.92, 51.96, 67.58, 5.0
2019-02-13 21:02:39,695 : start epoch
2019-02-13 21:03:21,006 : samples : 64000
2019-02-13 21:03:31,235 : Image to text: 7.42, 23.08, 34.32, 22.0
2019-02-13 21:03:38,614 : Text to Image: 6.536, 20.224, 30.596, 26.0
2019-02-13 21:04:18,974 : samples : 128000
2019-02-13 21:04:29,208 : Image to text: 6.88, 21.92, 33.08, 23.0
2019-02-13 21:04:36,616 : Text to Image: 5.688, 18.512, 28.5, 29.0
2019-02-13 21:05:18,309 : samples : 192000
2019-02-13 21:05:28,558 : Image to text: 7.72, 23.34, 34.32, 22.0
2019-02-13 21:05:35,960 : Text to Image: 6.296, 20.336, 30.672, 26.0
2019-02-13 21:06:17,729 : samples : 256000
2019-02-13 21:06:27,954 : Image to text: 7.94, 24.12, 35.2, 22.0
2019-02-13 21:06:35,385 : Text to Image: 6.64, 20.544, 31.084, 26.0
2019-02-13 21:07:16,667 : samples : 320000
2019-02-13 21:07:26,823 : Image to text: 7.96, 24.24, 36.32, 20.0
2019-02-13 21:07:34,215 : Text to Image: 6.944, 21.528, 32.668, 24.0
2019-02-13 21:08:16,424 : samples : 384000
2019-02-13 21:08:26,551 : Image to text: 8.7, 23.98, 35.64, 21.0
2019-02-13 21:08:33,930 : Text to Image: 7.036, 21.3, 31.724, 24.0
2019-02-13 21:09:16,294 : samples : 448000
2019-02-13 21:09:26,421 : Image to text: 8.58, 23.72, 35.22, 22.0
2019-02-13 21:09:33,813 : Text to Image: 6.52, 20.676, 31.34, 25.0
2019-02-13 21:10:16,389 : samples : 512000
2019-02-13 21:10:26,487 : Image to text: 7.76, 24.14, 35.78, 20.0
2019-02-13 21:10:33,868 : Text to Image: 6.348, 20.112, 30.6, 26.0
2019-02-13 21:11:10,639 : Epoch 6 finished
2019-02-13 21:11:11,065 : Image to text: 21.8, 52.5, 67.4, 5.0
2019-02-13 21:11:11,416 : Text to Image: 18.2, 49.02, 65.54, 6.0
2019-02-13 21:11:11,891 : Image to text: 22.2, 53.1, 68.9, 5.0
2019-02-13 21:11:12,231 : Text to Image: 17.02, 48.16, 65.52, 6.0
2019-02-13 21:11:12,705 : Image to text: 21.4, 51.6, 69.4, 5.0
2019-02-13 21:11:13,045 : Text to Image: 18.56, 48.58, 65.78, 6.0
2019-02-13 21:11:13,517 : Image to text: 23.9, 55.2, 71.2, 5.0
2019-02-13 21:11:13,858 : Text to Image: 18.56, 48.24, 66.06, 6.0
2019-02-13 21:11:14,328 : Image to text: 24.6, 55.6, 71.3, 4.0
2019-02-13 21:11:14,668 : Text to Image: 19.28, 49.26, 66.02, 6.0
2019-02-13 21:11:14,668 : Dev mean Text to Image: 18.324, 48.652, 65.78399999999999, 6.0
2019-02-13 21:11:14,668 : Dev mean Image to text: 22.78, 53.60000000000001, 69.64, 4.8
2019-02-13 21:11:14,669 : start epoch
2019-02-13 21:12:00,562 : samples : 64000
2019-02-13 21:12:10,826 : Image to text: 7.94, 24.18, 36.04, 21.0
2019-02-13 21:12:18,165 : Text to Image: 6.776, 21.356, 32.22, 24.0
2019-02-13 21:13:06,392 : samples : 128000
2019-02-13 21:13:16,477 : Image to text: 7.98, 24.54, 36.98, 20.0
2019-02-13 21:13:23,820 : Text to Image: 6.788, 20.988, 31.684, 25.0
2019-02-13 21:14:06,561 : samples : 192000
2019-02-13 21:14:16,637 : Image to text: 8.08, 24.26, 35.86, 20.0
2019-02-13 21:14:23,990 : Text to Image: 6.784, 21.628, 32.736, 24.0
2019-02-13 21:15:06,620 : samples : 256000
2019-02-13 21:15:16,711 : Image to text: 8.0, 24.1, 35.68, 20.0
2019-02-13 21:15:24,044 : Text to Image: 6.964, 21.672, 32.772, 23.0
2019-02-13 21:16:06,859 : samples : 320000
2019-02-13 21:16:16,948 : Image to text: 8.4, 25.16, 37.42, 19.0
2019-02-13 21:16:24,295 : Text to Image: 7.352, 22.276, 33.376, 23.0
2019-02-13 21:17:07,213 : samples : 384000
2019-02-13 21:17:17,280 : Image to text: 8.16, 25.1, 36.52, 20.0
2019-02-13 21:17:24,650 : Text to Image: 7.084, 21.544, 32.424, 24.0
2019-02-13 21:18:07,794 : samples : 448000
2019-02-13 21:18:17,850 : Image to text: 8.32, 24.44, 35.88, 20.0
2019-02-13 21:18:25,230 : Text to Image: 7.088, 22.296, 33.396, 23.0
2019-02-13 21:19:06,991 : samples : 512000
2019-02-13 21:19:17,076 : Image to text: 8.82, 24.92, 36.76, 20.0
2019-02-13 21:19:24,418 : Text to Image: 7.0, 21.64, 32.512, 24.0
2019-02-13 21:20:00,293 : Epoch 7 finished
2019-02-13 21:20:00,734 : Image to text: 23.3, 52.5, 68.0, 5.0
2019-02-13 21:20:01,065 : Text to Image: 19.0, 49.7, 67.48, 6.0
2019-02-13 21:20:01,495 : Image to text: 22.3, 54.2, 71.7, 5.0
2019-02-13 21:20:01,825 : Text to Image: 19.02, 49.52, 66.88, 6.0
2019-02-13 21:20:02,268 : Image to text: 21.9, 55.2, 69.5, 5.0
2019-02-13 21:20:02,598 : Text to Image: 18.9, 50.9, 67.88, 5.0
2019-02-13 21:20:03,027 : Image to text: 24.6, 54.9, 70.0, 5.0
2019-02-13 21:20:03,357 : Text to Image: 19.36, 50.12, 67.66, 5.0
2019-02-13 21:20:03,846 : Image to text: 24.3, 52.6, 70.1, 5.0
2019-02-13 21:20:04,186 : Text to Image: 19.0, 49.98, 66.92, 6.0
2019-02-13 21:20:04,186 : Dev mean Text to Image: 19.055999999999997, 50.044, 67.364, 5.6000000000000005
2019-02-13 21:20:04,186 : Dev mean Image to text: 23.28, 53.879999999999995, 69.86, 5.0
2019-02-13 21:20:04,186 : start epoch
2019-02-13 21:20:45,818 : samples : 64000
2019-02-13 21:20:56,104 : Image to text: 8.28, 24.02, 35.56, 21.0
2019-02-13 21:21:03,422 : Text to Image: 6.312, 20.18, 31.108, 25.0
2019-02-13 21:21:44,580 : samples : 128000
2019-02-13 21:21:54,627 : Image to text: 8.78, 24.84, 36.84, 19.0
2019-02-13 21:22:01,908 : Text to Image: 7.524, 22.744, 33.676, 22.0
2019-02-13 21:22:42,150 : samples : 192000
2019-02-13 21:22:52,197 : Image to text: 8.7, 24.94, 37.54, 19.0
2019-02-13 21:22:59,485 : Text to Image: 7.02, 22.068, 32.768, 24.0
2019-02-13 21:23:40,953 : samples : 256000
2019-02-13 21:23:50,981 : Image to text: 7.84, 23.74, 35.12, 20.0
2019-02-13 21:23:58,263 : Text to Image: 6.532, 20.9, 31.592, 25.0
2019-02-13 21:24:38,470 : samples : 320000
2019-02-13 21:24:48,510 : Image to text: 8.34, 25.48, 37.24, 19.0
2019-02-13 21:24:55,781 : Text to Image: 7.24, 21.776, 32.572, 24.0
2019-02-13 21:25:37,869 : samples : 384000
2019-02-13 21:25:47,921 : Image to text: 9.3, 25.4, 36.8, 19.0
2019-02-13 21:25:55,192 : Text to Image: 6.568, 20.968, 31.488, 25.0
2019-02-13 21:26:37,590 : samples : 448000
2019-02-13 21:26:47,630 : Image to text: 8.62, 26.0, 37.98, 19.0
2019-02-13 21:26:54,923 : Text to Image: 7.284, 22.468, 33.356, 23.0
2019-02-13 21:27:37,030 : samples : 512000
2019-02-13 21:27:47,060 : Image to text: 8.44, 25.16, 37.22, 19.0
2019-02-13 21:27:54,326 : Text to Image: 6.948, 22.016, 32.728, 24.0
2019-02-13 21:28:30,205 : Epoch 8 finished
2019-02-13 21:28:30,632 : Image to text: 23.7, 54.0, 70.3, 5.0
2019-02-13 21:28:30,956 : Text to Image: 19.2, 50.56, 67.9, 5.0
2019-02-13 21:28:31,384 : Image to text: 24.0, 55.4, 72.1, 4.0
2019-02-13 21:28:31,713 : Text to Image: 19.3, 50.32, 67.84, 5.0
2019-02-13 21:28:32,158 : Image to text: 23.7, 55.0, 70.6, 4.0
2019-02-13 21:28:32,490 : Text to Image: 19.4, 50.32, 67.4, 5.0
2019-02-13 21:28:32,925 : Image to text: 22.2, 56.1, 74.2, 4.0
2019-02-13 21:28:33,255 : Text to Image: 19.72, 50.68, 68.96, 5.0
2019-02-13 21:28:33,699 : Image to text: 23.5, 55.3, 71.5, 4.0
2019-02-13 21:28:34,028 : Text to Image: 20.28, 51.14, 67.5, 5.0
2019-02-13 21:28:34,029 : Dev mean Text to Image: 19.580000000000002, 50.604000000000006, 67.92, 5.0
2019-02-13 21:28:34,029 : Dev mean Image to text: 23.419999999999998, 55.16, 71.74, 4.2
2019-02-13 21:28:34,029 : start epoch
2019-02-13 21:29:15,758 : samples : 64000
2019-02-13 21:29:26,022 : Image to text: 8.54, 24.42, 36.94, 19.0
2019-02-13 21:29:33,390 : Text to Image: 7.284, 22.816, 33.588, 22.0
2019-02-13 21:30:16,072 : samples : 128000
2019-02-13 21:30:26,105 : Image to text: 8.08, 24.64, 36.44, 20.0
2019-02-13 21:30:33,384 : Text to Image: 6.892, 21.268, 31.932, 25.0
2019-02-13 21:31:16,413 : samples : 192000
2019-02-13 21:31:26,469 : Image to text: 8.62, 25.0, 37.0, 20.0
2019-02-13 21:31:33,734 : Text to Image: 6.832, 21.364, 31.92, 24.0
2019-02-13 21:32:16,815 : samples : 256000
2019-02-13 21:32:26,851 : Image to text: 9.2, 25.4, 36.88, 19.0
2019-02-13 21:32:34,144 : Text to Image: 7.068, 22.06, 32.692, 23.0
2019-02-13 21:33:18,880 : samples : 320000
2019-02-13 21:33:28,923 : Image to text: 9.02, 25.44, 37.18, 19.0
2019-02-13 21:33:36,187 : Text to Image: 7.472, 22.256, 33.488, 23.0
2019-02-13 21:34:18,421 : samples : 384000
2019-02-13 21:34:28,466 : Image to text: 8.3, 24.0, 36.28, 20.0
2019-02-13 21:34:35,733 : Text to Image: 7.132, 22.084, 32.848, 23.0
2019-02-13 21:35:17,971 : samples : 448000
2019-02-13 21:35:28,044 : Image to text: 8.66, 24.42, 35.64, 20.0
2019-02-13 21:35:35,297 : Text to Image: 6.924, 21.556, 32.452, 24.0
2019-02-13 21:36:17,416 : samples : 512000
2019-02-13 21:36:27,458 : Image to text: 9.18, 26.2, 38.88, 18.0
2019-02-13 21:36:34,728 : Text to Image: 7.404, 22.892, 33.788, 23.0
2019-02-13 21:37:09,333 : Epoch 9 finished
2019-02-13 21:37:09,765 : Image to text: 24.6, 53.0, 70.2, 5.0
2019-02-13 21:37:10,092 : Text to Image: 20.08, 51.84, 68.24, 5.0
2019-02-13 21:37:10,519 : Image to text: 22.5, 55.2, 70.4, 4.0
2019-02-13 21:37:10,846 : Text to Image: 20.04, 51.16, 69.18, 5.0
2019-02-13 21:37:11,273 : Image to text: 23.1, 55.5, 71.4, 4.0
2019-02-13 21:37:11,602 : Text to Image: 20.12, 51.86, 68.46, 5.0
2019-02-13 21:37:12,044 : Image to text: 25.4, 55.2, 73.0, 4.0
2019-02-13 21:37:12,374 : Text to Image: 21.04, 51.78, 69.64, 5.0
2019-02-13 21:37:12,802 : Image to text: 26.1, 56.3, 72.6, 4.0
2019-02-13 21:37:13,131 : Text to Image: 20.86, 53.08, 69.08, 5.0
2019-02-13 21:37:13,131 : Dev mean Text to Image: 20.428, 51.944, 68.92, 5.0
2019-02-13 21:37:13,131 : Dev mean Image to text: 24.339999999999996, 55.04, 71.52000000000001, 4.2
2019-02-13 21:37:13,131 : start epoch
2019-02-13 21:37:55,998 : samples : 64000
2019-02-13 21:38:06,269 : Image to text: 8.44, 24.88, 36.88, 20.0
2019-02-13 21:38:13,674 : Text to Image: 7.496, 22.736, 33.932, 22.0
2019-02-13 21:38:53,649 : samples : 128000
2019-02-13 21:39:03,712 : Image to text: 8.8, 25.66, 38.14, 18.0
2019-02-13 21:39:10,970 : Text to Image: 7.468, 22.9, 34.152, 22.0
2019-02-13 21:39:52,930 : samples : 192000
2019-02-13 21:40:02,970 : Image to text: 8.24, 24.64, 36.8, 19.0
2019-02-13 21:40:10,226 : Text to Image: 7.108, 21.556, 32.44, 24.0
2019-02-13 21:40:52,881 : samples : 256000
2019-02-13 21:41:02,918 : Image to text: 9.2, 26.3, 37.1, 19.0
2019-02-13 21:41:10,171 : Text to Image: 6.916, 21.728, 32.664, 24.0
2019-02-13 21:41:52,751 : samples : 320000
2019-02-13 21:42:02,796 : Image to text: 8.76, 26.14, 37.86, 18.0
2019-02-13 21:42:10,056 : Text to Image: 7.404, 22.908, 34.2, 22.0
2019-02-13 21:42:52,464 : samples : 384000
2019-02-13 21:43:02,502 : Image to text: 9.1, 26.18, 37.84, 18.0
2019-02-13 21:43:09,759 : Text to Image: 7.464, 23.488, 34.532, 21.0
2019-02-13 21:43:51,525 : samples : 448000
2019-02-13 21:44:01,559 : Image to text: 8.02, 25.14, 36.54, 20.0
2019-02-13 21:44:08,818 : Text to Image: 6.928, 21.556, 32.52, 24.0
2019-02-13 21:44:49,061 : samples : 512000
2019-02-13 21:44:59,107 : Image to text: 9.08, 25.28, 36.44, 19.0
2019-02-13 21:45:06,369 : Text to Image: 7.02, 22.288, 33.456, 22.0
2019-02-13 21:45:40,991 : Epoch 10 finished
2019-02-13 21:45:41,421 : Image to text: 24.0, 52.4, 68.1, 5.0
2019-02-13 21:45:41,748 : Text to Image: 19.1, 49.34, 66.02, 6.0
2019-02-13 21:45:42,171 : Image to text: 21.6, 53.6, 68.8, 5.0
2019-02-13 21:45:42,499 : Text to Image: 18.32, 50.2, 67.94, 5.0
2019-02-13 21:45:42,939 : Image to text: 23.7, 55.8, 71.5, 4.0
2019-02-13 21:45:43,267 : Text to Image: 19.48, 50.56, 67.8, 5.0
2019-02-13 21:45:43,694 : Image to text: 22.6, 55.7, 69.6, 4.0
2019-02-13 21:45:44,022 : Text to Image: 19.02, 49.82, 67.66, 6.0
2019-02-13 21:45:44,460 : Image to text: 25.9, 56.5, 70.7, 4.0
2019-02-13 21:45:44,788 : Text to Image: 20.52, 51.8, 67.68, 5.0
2019-02-13 21:45:44,788 : Dev mean Text to Image: 19.288, 50.344, 67.42, 5.4
2019-02-13 21:45:44,788 : Dev mean Image to text: 23.560000000000002, 54.8, 69.74, 4.3999999999999995
2019-02-13 21:45:44,788 : start epoch
2019-02-13 21:46:25,160 : samples : 64000
2019-02-13 21:46:35,569 : Image to text: 9.2, 26.48, 38.7, 18.0
2019-02-13 21:46:43,005 : Text to Image: 7.768, 24.012, 35.596, 21.0
2019-02-13 21:47:24,766 : samples : 128000
2019-02-13 21:47:34,817 : Image to text: 9.08, 25.36, 37.16, 19.0
2019-02-13 21:47:42,074 : Text to Image: 7.268, 22.884, 34.168, 22.0
2019-02-13 21:48:23,498 : samples : 192000
2019-02-13 21:48:33,563 : Image to text: 8.88, 26.46, 38.56, 18.0
2019-02-13 21:48:40,840 : Text to Image: 7.356, 23.06, 34.284, 22.0
2019-02-13 21:49:23,795 : samples : 256000
2019-02-13 21:49:33,848 : Image to text: 8.84, 26.42, 38.16, 18.0
2019-02-13 21:49:41,138 : Text to Image: 7.36, 22.656, 33.62, 23.0
2019-02-13 21:50:23,335 : samples : 320000
2019-02-13 21:50:33,418 : Image to text: 9.42, 26.14, 37.7, 18.0
2019-02-13 21:50:40,761 : Text to Image: 7.656, 23.108, 34.256, 22.0
2019-02-13 21:51:23,717 : samples : 384000
2019-02-13 21:51:33,807 : Image to text: 9.3, 26.36, 38.76, 18.0
2019-02-13 21:51:41,123 : Text to Image: 7.644, 23.624, 34.704, 21.0
2019-02-13 21:52:23,636 : samples : 448000
2019-02-13 21:52:33,705 : Image to text: 8.88, 25.2, 37.34, 19.0
2019-02-13 21:52:40,996 : Text to Image: 7.464, 22.428, 33.72, 22.0
2019-02-13 21:53:23,761 : samples : 512000
2019-02-13 21:53:33,825 : Image to text: 8.94, 26.86, 39.44, 17.0
2019-02-13 21:53:41,131 : Text to Image: 7.688, 23.692, 34.796, 21.0
2019-02-13 21:54:17,389 : Epoch 11 finished
2019-02-13 21:54:17,820 : Image to text: 24.6, 53.9, 69.0, 5.0
2019-02-13 21:54:18,148 : Text to Image: 20.66, 51.82, 68.74, 5.0
2019-02-13 21:54:18,576 : Image to text: 22.4, 54.1, 70.1, 5.0
2019-02-13 21:54:18,906 : Text to Image: 19.84, 52.08, 68.88, 5.0
2019-02-13 21:54:19,350 : Image to text: 22.3, 56.6, 72.6, 4.0
2019-02-13 21:54:19,682 : Text to Image: 19.72, 51.58, 68.88, 5.0
2019-02-13 21:54:20,113 : Image to text: 23.9, 57.3, 71.4, 4.0
2019-02-13 21:54:20,449 : Text to Image: 19.08, 51.62, 69.3, 5.0
2019-02-13 21:54:20,887 : Image to text: 25.9, 56.5, 71.1, 4.0
2019-02-13 21:54:21,218 : Text to Image: 19.88, 51.72, 67.94, 5.0
2019-02-13 21:54:21,218 : Dev mean Text to Image: 19.836, 51.764, 68.74799999999999, 5.0
2019-02-13 21:54:21,218 : Dev mean Image to text: 23.82, 55.68000000000001, 70.84, 4.3999999999999995
2019-02-13 21:54:21,218 : start epoch
2019-02-13 21:55:04,710 : samples : 64000
2019-02-13 21:55:14,962 : Image to text: 9.46, 25.94, 38.28, 19.0
2019-02-13 21:55:22,352 : Text to Image: 7.78, 23.484, 34.736, 21.0
2019-02-13 21:56:05,304 : samples : 128000
2019-02-13 21:56:15,375 : Image to text: 9.12, 26.16, 38.68, 18.0
2019-02-13 21:56:22,695 : Text to Image: 7.512, 22.464, 33.68, 22.0
2019-02-13 21:57:05,546 : samples : 192000
2019-02-13 21:57:15,632 : Image to text: 9.4, 26.66, 39.0, 18.0
2019-02-13 21:57:22,929 : Text to Image: 7.58, 23.6, 34.964, 21.0
2019-02-13 21:58:05,658 : samples : 256000
2019-02-13 21:58:15,754 : Image to text: 9.44, 26.36, 37.7, 19.0
2019-02-13 21:58:23,082 : Text to Image: 7.328, 22.808, 33.988, 22.0
2019-02-13 21:59:04,871 : samples : 320000
2019-02-13 21:59:14,961 : Image to text: 8.32, 26.32, 37.84, 18.0
2019-02-13 21:59:22,280 : Text to Image: 7.252, 22.6, 33.524, 23.0
2019-02-13 22:00:04,955 : samples : 384000
2019-02-13 22:00:15,047 : Image to text: 9.02, 26.84, 38.02, 19.0
2019-02-13 22:00:22,370 : Text to Image: 7.576, 23.104, 34.612, 22.0
2019-02-13 22:01:04,600 : samples : 448000
2019-02-13 22:01:14,681 : Image to text: 9.36, 26.98, 39.24, 18.0
2019-02-13 22:01:21,977 : Text to Image: 7.476, 23.144, 34.576, 22.0
2019-02-13 22:02:01,918 : samples : 512000
2019-02-13 22:02:12,004 : Image to text: 9.3, 26.82, 38.46, 18.0
2019-02-13 22:02:19,359 : Text to Image: 7.708, 23.88, 35.256, 21.0
2019-02-13 22:02:56,034 : Epoch 12 finished
2019-02-13 22:02:56,461 : Image to text: 25.5, 54.2, 68.4, 4.0
2019-02-13 22:02:56,786 : Text to Image: 20.34, 51.38, 68.76, 5.0
2019-02-13 22:02:57,207 : Image to text: 23.0, 56.7, 70.6, 4.0
2019-02-13 22:02:57,532 : Text to Image: 19.92, 51.94, 68.36, 5.0
2019-02-13 22:02:57,953 : Image to text: 23.1, 56.8, 72.1, 4.0
2019-02-13 22:02:58,279 : Text to Image: 20.48, 51.98, 69.1, 5.0
2019-02-13 22:02:58,710 : Image to text: 24.8, 54.5, 71.3, 4.0
2019-02-13 22:02:59,035 : Text to Image: 20.52, 51.36, 70.0, 5.0
2019-02-13 22:02:59,454 : Image to text: 26.1, 56.4, 72.0, 4.0
2019-02-13 22:02:59,781 : Text to Image: 19.64, 51.54, 68.96, 5.0
2019-02-13 22:02:59,781 : Dev mean Text to Image: 20.18, 51.64, 69.036, 5.0
2019-02-13 22:02:59,781 : Dev mean Image to text: 24.5, 55.72, 70.88, 4.0
2019-02-13 22:02:59,781 : start epoch
2019-02-13 22:03:41,107 : samples : 64000
2019-02-13 22:03:51,394 : Image to text: 9.08, 26.4, 38.64, 18.0
2019-02-13 22:03:58,811 : Text to Image: 7.42, 22.628, 33.616, 22.0
2019-02-13 22:04:39,191 : samples : 128000
2019-02-13 22:04:49,287 : Image to text: 9.0, 26.76, 39.14, 18.0
2019-02-13 22:04:56,581 : Text to Image: 7.756, 23.76, 35.216, 21.0
2019-02-13 22:05:36,499 : samples : 192000
2019-02-13 22:05:46,579 : Image to text: 9.18, 27.08, 39.8, 17.0
2019-02-13 22:05:53,910 : Text to Image: 7.776, 24.112, 35.652, 21.0
2019-02-13 22:06:34,060 : samples : 256000
2019-02-13 22:06:44,139 : Image to text: 9.48, 27.3, 39.36, 18.0
2019-02-13 22:06:51,466 : Text to Image: 7.5, 23.508, 34.888, 22.0
2019-02-13 22:07:34,111 : samples : 320000
2019-02-13 22:07:44,210 : Image to text: 10.06, 26.48, 38.0, 19.0
2019-02-13 22:07:51,507 : Text to Image: 7.752, 23.536, 34.944, 21.0
2019-02-13 22:08:34,911 : samples : 384000
2019-02-13 22:08:45,005 : Image to text: 8.48, 26.94, 38.36, 18.0
2019-02-13 22:08:52,326 : Text to Image: 7.528, 23.072, 34.252, 22.0
2019-02-13 22:09:35,506 : samples : 448000
2019-02-13 22:09:45,587 : Image to text: 9.16, 26.38, 38.48, 18.0
2019-02-13 22:09:52,882 : Text to Image: 7.28, 22.504, 33.776, 22.0
2019-02-13 22:10:36,891 : samples : 512000
2019-02-13 22:10:46,970 : Image to text: 9.28, 27.06, 38.84, 17.0
2019-02-13 22:10:54,284 : Text to Image: 7.888, 24.12, 35.208, 21.0
2019-02-13 22:11:28,477 : Epoch 13 finished
2019-02-13 22:11:28,918 : Image to text: 26.8, 57.1, 69.8, 4.0
2019-02-13 22:11:29,249 : Text to Image: 21.32, 52.2, 68.58, 5.0
2019-02-13 22:11:29,682 : Image to text: 23.7, 57.0, 72.6, 4.0
2019-02-13 22:11:30,014 : Text to Image: 20.48, 52.14, 69.66, 5.0
2019-02-13 22:11:30,464 : Image to text: 22.4, 59.0, 74.6, 4.0
2019-02-13 22:11:30,799 : Text to Image: 20.04, 52.44, 68.94, 5.0
2019-02-13 22:11:31,239 : Image to text: 25.0, 56.5, 72.8, 4.0
2019-02-13 22:11:31,585 : Text to Image: 19.88, 51.6, 69.4, 5.0
2019-02-13 22:11:32,018 : Image to text: 26.6, 57.8, 73.5, 4.0
2019-02-13 22:11:32,348 : Text to Image: 20.94, 53.08, 69.6, 5.0
2019-02-13 22:11:32,348 : Dev mean Text to Image: 20.531999999999996, 52.292, 69.23599999999999, 5.0
2019-02-13 22:11:32,348 : Dev mean Image to text: 24.900000000000002, 57.480000000000004, 72.66, 4.0
2019-02-13 22:11:32,349 : start epoch
2019-02-13 22:12:12,818 : samples : 64000
2019-02-13 22:12:23,127 : Image to text: 10.02, 28.02, 39.76, 17.0
2019-02-13 22:12:30,497 : Text to Image: 8.096, 24.12, 36.068, 20.0
2019-02-13 22:13:10,947 : samples : 128000
2019-02-13 22:13:21,006 : Image to text: 9.22, 26.94, 38.62, 18.0
2019-02-13 22:13:28,297 : Text to Image: 7.42, 23.212, 34.444, 21.0
2019-02-13 22:14:09,552 : samples : 192000
2019-02-13 22:14:19,629 : Image to text: 9.42, 27.74, 39.66, 18.0
2019-02-13 22:14:26,913 : Text to Image: 7.876, 23.888, 35.336, 21.0
2019-02-13 22:15:10,045 : samples : 256000
2019-02-13 22:15:20,098 : Image to text: 9.46, 28.12, 39.9, 17.0
2019-02-13 22:15:27,394 : Text to Image: 8.304, 24.912, 36.184, 20.0
2019-02-13 22:16:10,836 : samples : 320000
2019-02-13 22:16:20,925 : Image to text: 9.36, 26.6, 38.34, 18.0
2019-02-13 22:16:28,224 : Text to Image: 7.592, 23.208, 34.344, 22.0
2019-02-13 22:17:11,378 : samples : 384000
2019-02-13 22:17:21,453 : Image to text: 9.1, 26.66, 39.12, 18.0
2019-02-13 22:17:28,753 : Text to Image: 7.944, 24.176, 35.404, 21.0
2019-02-13 22:18:11,799 : samples : 448000
2019-02-13 22:18:21,871 : Image to text: 9.08, 26.6, 38.22, 18.0
2019-02-13 22:18:29,172 : Text to Image: 7.74, 23.7, 34.644, 21.0
2019-02-13 22:19:09,639 : samples : 512000
2019-02-13 22:19:19,714 : Image to text: 9.32, 26.86, 39.26, 18.0
2019-02-13 22:19:27,008 : Text to Image: 8.036, 24.136, 35.556, 21.0
2019-02-13 22:20:01,999 : Epoch 14 finished
2019-02-13 22:20:02,423 : Image to text: 25.2, 54.6, 69.4, 4.0
2019-02-13 22:20:02,743 : Text to Image: 20.28, 51.72, 69.24, 5.0
2019-02-13 22:20:03,173 : Image to text: 23.2, 56.0, 72.8, 4.0
2019-02-13 22:20:03,499 : Text to Image: 19.92, 52.3, 69.34, 5.0
2019-02-13 22:20:03,919 : Image to text: 24.2, 58.3, 72.6, 4.0
2019-02-13 22:20:04,247 : Text to Image: 19.88, 52.6, 69.04, 5.0
2019-02-13 22:20:04,675 : Image to text: 24.0, 57.0, 72.3, 4.0
2019-02-13 22:20:05,004 : Text to Image: 19.96, 52.36, 69.86, 5.0
2019-02-13 22:20:05,447 : Image to text: 25.5, 55.1, 71.6, 4.0
2019-02-13 22:20:05,778 : Text to Image: 20.96, 52.64, 69.5, 5.0
2019-02-13 22:20:05,778 : Dev mean Text to Image: 20.200000000000003, 52.324, 69.396, 5.0
2019-02-13 22:20:05,778 : Dev mean Image to text: 24.42, 56.2, 71.74, 4.0
2019-02-13 22:20:09,710 : 
Test scores | Image to text:             24.16, 55.49999999999999, 71.36, 4.4
2019-02-13 22:20:09,710 : Test scores | Text to image:             20.196, 51.764, 68.97200000000001, 5.0

2019-02-13 22:20:09,803 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-13 22:20:10,012 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-13 22:20:10,630 : loading BERT model bert-base-uncased
2019-02-13 22:20:10,630 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:20:10,660 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:20:10,661 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnvdux33p
2019-02-13 22:20:13,101 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:20:14,544 : Computing embeddings for train/dev/test
2019-02-13 22:21:49,225 : Computed embeddings
2019-02-13 22:21:49,225 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:22:12,940 : [('reg:1e-05', 88.73), ('reg:0.0001', 88.3), ('reg:0.001', 87.02), ('reg:0.01', 86.75)]
2019-02-13 22:22:12,940 : Validation : best param found is reg = 1e-05 with score             88.73
2019-02-13 22:22:12,941 : Evaluating...
2019-02-13 22:22:19,082 : 
Dev acc : 88.7 Test acc : 89.2 for LENGTH classification

2019-02-13 22:22:19,083 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-13 22:22:19,428 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-13 22:22:19,475 : loading BERT model bert-base-uncased
2019-02-13 22:22:19,475 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:22:19,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:22:19,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9wqgnbj4
2019-02-13 22:22:22,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:22:23,487 : Computing embeddings for train/dev/test
2019-02-13 22:23:53,053 : Computed embeddings
2019-02-13 22:23:53,053 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:24:30,666 : [('reg:1e-05', 1.66), ('reg:0.0001', 0.44), ('reg:0.001', 0.19), ('reg:0.01', 0.2)]
2019-02-13 22:24:30,666 : Validation : best param found is reg = 1e-05 with score             1.66
2019-02-13 22:24:30,666 : Evaluating...
2019-02-13 22:24:44,942 : 
Dev acc : 1.7 Test acc : 1.7 for WORDCONTENT classification

2019-02-13 22:24:44,944 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-13 22:24:45,317 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-13 22:24:45,387 : loading BERT model bert-base-uncased
2019-02-13 22:24:45,387 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:24:45,491 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:24:45,491 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0fzvlwry
2019-02-13 22:24:47,938 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:24:49,401 : Computing embeddings for train/dev/test
2019-02-13 22:26:15,197 : Computed embeddings
2019-02-13 22:26:15,197 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:26:42,186 : [('reg:1e-05', 27.1), ('reg:0.0001', 26.27), ('reg:0.001', 25.59), ('reg:0.01', 22.36)]
2019-02-13 22:26:42,187 : Validation : best param found is reg = 1e-05 with score             27.1
2019-02-13 22:26:42,187 : Evaluating...
2019-02-13 22:26:51,125 : 
Dev acc : 27.1 Test acc : 26.4 for DEPTH classification

2019-02-13 22:26:51,126 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-13 22:26:51,722 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-13 22:26:51,789 : loading BERT model bert-base-uncased
2019-02-13 22:26:51,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:26:51,820 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:26:51,820 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7zakzxlr
2019-02-13 22:26:54,266 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:26:55,763 : Computing embeddings for train/dev/test
2019-02-13 22:28:15,217 : Computed embeddings
2019-02-13 22:28:15,217 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:28:53,546 : [('reg:1e-05', 47.33), ('reg:0.0001', 45.88), ('reg:0.001', 33.84), ('reg:0.01', 18.29)]
2019-02-13 22:28:53,546 : Validation : best param found is reg = 1e-05 with score             47.33
2019-02-13 22:28:53,546 : Evaluating...
2019-02-13 22:29:02,559 : 
Dev acc : 47.3 Test acc : 48.3 for TOPCONSTITUENTS classification

2019-02-13 22:29:02,560 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-13 22:29:02,963 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-13 22:29:03,035 : loading BERT model bert-base-uncased
2019-02-13 22:29:03,035 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:29:03,073 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:29:03,073 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppa9ocnwk
2019-02-13 22:29:05,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:29:07,023 : Computing embeddings for train/dev/test
2019-02-13 22:30:32,936 : Computed embeddings
2019-02-13 22:30:32,936 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:30:58,034 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-13 22:30:58,035 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-13 22:30:58,035 : Evaluating...
2019-02-13 22:31:03,769 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-13 22:31:03,770 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-13 22:31:04,190 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-13 22:31:04,259 : loading BERT model bert-base-uncased
2019-02-13 22:31:04,260 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:31:04,394 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:31:04,394 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj16wjf72
2019-02-13 22:31:06,846 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:31:08,322 : Computing embeddings for train/dev/test
2019-02-13 22:32:32,655 : Computed embeddings
2019-02-13 22:32:32,655 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:32:59,763 : [('reg:1e-05', 80.32), ('reg:0.0001', 80.23), ('reg:0.001', 79.31), ('reg:0.01', 71.6)]
2019-02-13 22:32:59,763 : Validation : best param found is reg = 1e-05 with score             80.32
2019-02-13 22:32:59,763 : Evaluating...
2019-02-13 22:33:07,328 : 
Dev acc : 80.3 Test acc : 79.6 for TENSE classification

2019-02-13 22:33:07,329 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-13 22:33:07,751 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-13 22:33:07,819 : loading BERT model bert-base-uncased
2019-02-13 22:33:07,819 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:33:07,950 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:33:07,950 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr8yartd3
2019-02-13 22:33:10,395 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:33:11,862 : Computing embeddings for train/dev/test
2019-02-13 22:34:41,844 : Computed embeddings
2019-02-13 22:34:41,844 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:35:12,034 : [('reg:1e-05', 73.61), ('reg:0.0001', 73.56), ('reg:0.001', 71.92), ('reg:0.01', 63.51)]
2019-02-13 22:35:12,034 : Validation : best param found is reg = 1e-05 with score             73.61
2019-02-13 22:35:12,034 : Evaluating...
2019-02-13 22:35:20,733 : 
Dev acc : 73.6 Test acc : 71.9 for SUBJNUMBER classification

2019-02-13 22:35:20,734 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-13 22:35:21,367 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-13 22:35:21,438 : loading BERT model bert-base-uncased
2019-02-13 22:35:21,438 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:35:21,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:35:21,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmporfug3s6
2019-02-13 22:35:23,926 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:35:25,408 : Computing embeddings for train/dev/test
2019-02-13 22:36:53,198 : Computed embeddings
2019-02-13 22:36:53,198 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:37:29,057 : [('reg:1e-05', 69.15), ('reg:0.0001', 68.75), ('reg:0.001', 62.91), ('reg:0.01', 64.02)]
2019-02-13 22:37:29,057 : Validation : best param found is reg = 1e-05 with score             69.15
2019-02-13 22:37:29,057 : Evaluating...
2019-02-13 22:37:38,981 : 
Dev acc : 69.2 Test acc : 69.9 for OBJNUMBER classification

2019-02-13 22:37:38,982 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-13 22:37:39,556 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-13 22:37:39,626 : loading BERT model bert-base-uncased
2019-02-13 22:37:39,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:37:39,654 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:37:39,654 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp044fcq16
2019-02-13 22:37:42,160 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:37:43,605 : Computing embeddings for train/dev/test
2019-02-13 22:39:23,112 : Computed embeddings
2019-02-13 22:39:23,113 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:39:49,978 : [('reg:1e-05', 50.19), ('reg:0.0001', 50.19), ('reg:0.001', 50.19), ('reg:0.01', 50.19)]
2019-02-13 22:39:49,979 : Validation : best param found is reg = 1e-05 with score             50.19
2019-02-13 22:39:49,979 : Evaluating...
2019-02-13 22:39:55,489 : 
Dev acc : 50.2 Test acc : 49.9 for ODDMANOUT classification

2019-02-13 22:39:55,490 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-13 22:39:55,948 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-13 22:39:56,030 : loading BERT model bert-base-uncased
2019-02-13 22:39:56,030 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:39:56,065 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:39:56,066 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1j6f79e9
2019-02-13 22:39:58,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:39:59,996 : Computing embeddings for train/dev/test
2019-02-13 22:41:39,793 : Computed embeddings
2019-02-13 22:41:39,793 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:42:14,784 : [('reg:1e-05', 53.69), ('reg:0.0001', 53.5), ('reg:0.001', 52.34), ('reg:0.01', 50.36)]
2019-02-13 22:42:14,784 : Validation : best param found is reg = 1e-05 with score             53.69
2019-02-13 22:42:14,784 : Evaluating...
2019-02-13 22:42:23,097 : 
Dev acc : 53.7 Test acc : 53.6 for COORDINATIONINVERSION classification

2019-02-13 22:42:23,100 : total results: {'STS12': {'MSRpar': {'pearson': (0.06868694829523837, 0.06008723917707235), 'spearman': SpearmanrResult(correlation=0.09735065306344412, pvalue=0.007631561396187266), 'nsamples': 750}, 'MSRvid': {'pearson': (0.0515708123563372, 0.1582717111271189), 'spearman': SpearmanrResult(correlation=0.08853020678109076, pvalue=0.015299152778427862), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3139330403083308, 5.893526974366358e-12), 'spearman': SpearmanrResult(correlation=0.4120950884141659, pvalue=3.039615860408271e-20), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.36976825426426624, 1.0290805640546665e-25), 'spearman': SpearmanrResult(correlation=0.41774606530103797, pvalue=4.910260594623189e-33), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4037285534581422, 4.488483683791101e-17), 'spearman': SpearmanrResult(correlation=0.3969900792016263, pvalue=1.633559610501222e-16), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.24153752173646295, 'wmean': 0.2164422360097181}, 'spearman': {'mean': 0.282542418552273, 'wmean': 0.25748741346291204}}}, 'STS13': {'FNWN': {'pearson': (0.17043444303183117, 0.019041182241642095), 'spearman': SpearmanrResult(correlation=0.19435605806271905, pvalue=0.007365922153894599), 'nsamples': 189}, 'headlines': {'pearson': (0.06641416778048133, 0.06909462984814585), 'spearman': SpearmanrResult(correlation=0.08275228303032812, pvalue=0.02342856717939756), 'nsamples': 750}, 'OnWN': {'pearson': (-0.14546161701456373, 0.0005481819642489772), 'spearman': SpearmanrResult(correlation=-0.09685995333809476, pvalue=0.021764630065625037), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.03046233126591626, 'wmean': 0.0002791789488045614}, 'spearman': {'mean': 0.060082795918317465, 'wmean': 0.02963938228261922}}}, 'STS14': {'deft-forum': {'pearson': (-0.14020452836544406, 0.00287636877816833), 'spearman': SpearmanrResult(correlation=-0.11934012598639665, pvalue=0.011289572756483714), 'nsamples': 450}, 'deft-news': {'pearson': (-0.009737647402089617, 0.8666140569941962), 'spearman': SpearmanrResult(correlation=0.029851045303682454, pvalue=0.6065588887849926), 'nsamples': 300}, 'headlines': {'pearson': (0.11677318887904334, 0.0013572208919238854), 'spearman': SpearmanrResult(correlation=0.11866608343329156, pvalue=0.0011303560968565864), 'nsamples': 750}, 'images': {'pearson': (0.10116957298696035, 0.0055518291710597075), 'spearman': SpearmanrResult(correlation=0.14953130228470402, pvalue=3.932850324081206e-05), 'nsamples': 750}, 'OnWN': {'pearson': (-0.017437051974232235, 0.6335237147195845), 'spearman': SpearmanrResult(correlation=0.04488895688370547, pvalue=0.21948329157472693), 'nsamples': 750}, 'tweet-news': {'pearson': (0.16622271999203517, 4.728694059950482e-06), 'spearman': SpearmanrResult(correlation=0.1803776219322851, pvalue=6.611324879967754e-07), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.03613104235271216, 'wmean': 0.05574213078074087}, 'spearman': {'mean': 0.06732914730854533, 'wmean': 0.08676006141272422}}}, 'STS15': {'answers-forums': {'pearson': (-0.03662355885691694, 0.4795156842052348), 'spearman': SpearmanrResult(correlation=-0.05080779364604418, pvalue=0.3264754430481087), 'nsamples': 375}, 'answers-students': {'pearson': (0.24401991209240864, 1.2502744881348901e-11), 'spearman': SpearmanrResult(correlation=0.23495375469236318, pvalue=7.261317506653758e-11), 'nsamples': 750}, 'belief': {'pearson': (-0.02169596379680108, 0.6753716524834816), 'spearman': SpearmanrResult(correlation=-0.03592052809745738, pvalue=0.4879960469276158), 'nsamples': 375}, 'headlines': {'pearson': (0.19618290985415235, 6.079435723909279e-08), 'spearman': SpearmanrResult(correlation=0.21640470228386877, pvalue=2.128364327125358e-09), 'nsamples': 750}, 'images': {'pearson': (0.1096428303878674, 0.002640536960568458), 'spearman': SpearmanrResult(correlation=0.15137214555498293, pvalue=3.1465612821453676e-05), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.09830522593614208, 'wmean': 0.13017147275189234}, 'spearman': {'mean': 0.10320045615754266, 'wmean': 0.13984161041486604}}}, 'STS16': {'answer-answer': {'pearson': (0.21502515597521268, 0.0005595847639808302), 'spearman': SpearmanrResult(correlation=0.24822458824888274, pvalue=6.352832091570554e-05), 'nsamples': 254}, 'headlines': {'pearson': (0.27137524716467143, 1.4090409990315678e-05), 'spearman': SpearmanrResult(correlation=0.2831848140481574, pvalue=5.642525976276218e-06), 'nsamples': 249}, 'plagiarism': {'pearson': (0.10699848948915734, 0.10555037621259747), 'spearman': SpearmanrResult(correlation=0.1361130244465599, pvalue=0.03914890502316515), 'nsamples': 230}, 'postediting': {'pearson': (0.3809620738471511, 7.560690600670114e-10), 'spearman': SpearmanrResult(correlation=0.4203240602732528, pvalue=7.283897557821178e-12), 'nsamples': 244}, 'question-question': {'pearson': (-0.028515090334287126, 0.6819167990715685), 'spearman': SpearmanrResult(correlation=-0.024552190401559402, pvalue=0.7241848896525008), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.1891691752283811, 'wmean': 0.19712780007002717}, 'spearman': {'mean': 0.21265885932305872, 'wmean': 0.221159968506462}}}, 'MR': {'devacc': 61.52, 'acc': 55.9, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 66.85, 'acc': 64.98, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 82.1, 'acc': 81.73, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 86.7, 'acc': 86.57, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 71.79, 'acc': 73.42, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 33.7, 'acc': 32.08, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 55.58, 'acc': 67.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.89, 'acc': 68.93, 'f1': 80.66, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 68.8, 'acc': 66.63, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6285206911444029, 'pearson': 0.6325756375157819, 'spearman': 0.5773794338596717, 'mse': 0.6155502234112711, 'yhat': array([2.94329933, 3.66125645, 1.37064445, ..., 2.92580577, 3.90421435,        4.05271417]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.39438672521957946, 'pearson': 0.4013450438392224, 'spearman': 0.38778956174024726, 'mse': 2.0274987578665833, 'yhat': array([2.8438763 , 1.50664366, 2.2298432 , ..., 4.00391013, 3.41901146,        3.2815869 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.69, 'acc': 59.4, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 297.09999999999997, 'acc': [(24.16, 55.49999999999999, 71.36, 4.4), (20.196, 51.764, 68.97200000000001, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 88.73, 'acc': 89.22, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 1.66, 'acc': 1.72, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.1, 'acc': 26.42, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 47.33, 'acc': 48.3, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 80.32, 'acc': 79.56, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 73.61, 'acc': 71.89, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 69.15, 'acc': 69.91, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 50.19, 'acc': 49.87, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 53.69, 'acc': 53.59, 'ndev': 10002, 'ntest': 10002}}
2019-02-13 22:42:23,100 : STS12 p=0.2164, STS12 s=0.2575, STS13 p=0.0003, STS13 s=0.0296, STS14 p=0.0557, STS14 s=0.0868, STS15 p=0.1302, STS15 s=0.1398, STS 16 p=0.1971, STS16 s=0.2212, STS B p=0.4013, STS B s=0.3878, STS B m=2.0275, SICK-R p=0.6326, SICK-R s=0.5774, SICK-P m=0.6156
2019-02-13 22:42:23,100 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-13 22:42:23,100 : 0.2164,0.2575,0.0003,0.0296,0.0557,0.0868,0.1302,0.1398,0.1971,0.2212,0.4013,0.3878,2.0275,0.6326,0.5774,0.6156
2019-02-13 22:42:23,100 : MR=55.90, CR=64.98, SUBJ=86.57, MPQA=81.73, SST-B=73.42, SST-F=32.08, TREC=67.20, SICK-E=66.63, SNLI=59.40, MRPC=68.93, MRPC f=80.66
2019-02-13 22:42:23,100 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-13 22:42:23,100 : 55.90,64.98,86.57,81.73,73.42,32.08,67.20,66.63,59.40,68.93,80.66
2019-02-13 22:42:23,100 : COCO r1i2t=24.16, COCO r5i2t=55.50, COCO r10i2t=71.36, COCO medr_i2t=4.40, COCO r1t2i=20.20, COCO r5t2i=51.76, COCO r10t2i=68.97, COCO medr_t2i=5.00
2019-02-13 22:42:23,100 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-13 22:42:23,100 : 24.16,55.50,71.36,4.40,20.20,51.76,68.97,5.00
2019-02-13 22:42:23,100 : SentLen=89.22, WC=1.72, TreeDepth=26.42, TopConst=48.30, BShift=50.00, Tense=79.56, SubjNum=71.89, ObjNum=69.91, SOMO=49.87, CoordInv=53.59, average=54.05
2019-02-13 22:42:23,100 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-13 22:42:23,100 : 89.22,1.72,26.42,48.30,50.00,79.56,71.89,69.91,49.87,53.59,54.05
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,100 : layer 2
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,100 : ********************************************************************************
2019-02-13 22:42:23,195 : ***** Transfer task : STS12 *****


2019-02-13 22:42:23,208 : loading BERT model bert-base-uncased
2019-02-13 22:42:23,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:42:23,226 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:42:23,226 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsxmlgql8
2019-02-13 22:42:25,676 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:42:28,949 : MSRpar : pearson = 0.1115, spearman = 0.1385
2019-02-13 22:42:29,737 : MSRvid : pearson = 0.1304, spearman = 0.1467
2019-02-13 22:42:30,423 : SMTeuroparl : pearson = 0.3526, spearman = 0.4482
2019-02-13 22:42:31,604 : surprise.OnWN : pearson = 0.3726, spearman = 0.3949
2019-02-13 22:42:32,271 : surprise.SMTnews : pearson = 0.5044, spearman = 0.4430
2019-02-13 22:42:32,272 : ALL (weighted average) : Pearson = 0.2651,             Spearman = 0.2872
2019-02-13 22:42:32,272 : ALL (average) : Pearson = 0.2943,             Spearman = 0.3143

2019-02-13 22:42:32,272 : ***** Transfer task : STS13 (-SMT) *****


2019-02-13 22:42:32,283 : loading BERT model bert-base-uncased
2019-02-13 22:42:32,283 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:42:32,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:42:32,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ugz_iur
2019-02-13 22:42:34,745 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:42:36,850 : FNWN : pearson = 0.1718, spearman = 0.1710
2019-02-13 22:42:37,835 : headlines : pearson = 0.1152, spearman = 0.1413
2019-02-13 22:42:38,575 : OnWN : pearson = -0.0753, spearman = -0.0237
2019-02-13 22:42:38,576 : ALL (weighted average) : Pearson = 0.0511,             Spearman = 0.0834
2019-02-13 22:42:38,576 : ALL (average) : Pearson = 0.0706,             Spearman = 0.0962

2019-02-13 22:42:38,576 : ***** Transfer task : STS14 *****


2019-02-13 22:42:38,591 : loading BERT model bert-base-uncased
2019-02-13 22:42:38,591 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:42:38,608 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:42:38,609 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmxf1_er5
2019-02-13 22:42:41,047 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:42:43,229 : deft-forum : pearson = -0.0976, spearman = -0.0687
2019-02-13 22:42:44,022 : deft-news : pearson = 0.1188, spearman = 0.1287
2019-02-13 22:42:45,062 : headlines : pearson = 0.1616, spearman = 0.1636
2019-02-13 22:42:45,999 : images : pearson = 0.1435, spearman = 0.1823
2019-02-13 22:42:46,952 : OnWN : pearson = 0.0579, spearman = 0.1178
2019-02-13 22:42:48,202 : tweet-news : pearson = 0.2078, spearman = 0.2226
2019-02-13 22:42:48,202 : ALL (weighted average) : Pearson = 0.1120,             Spearman = 0.1393
2019-02-13 22:42:48,202 : ALL (average) : Pearson = 0.0987,             Spearman = 0.1244

2019-02-13 22:42:48,202 : ***** Transfer task : STS15 *****


2019-02-13 22:42:48,235 : loading BERT model bert-base-uncased
2019-02-13 22:42:48,235 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:42:48,253 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:42:48,254 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_xu6chjx
2019-02-13 22:42:50,692 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:42:53,002 : answers-forums : pearson = 0.0196, spearman = -0.0005
2019-02-13 22:42:53,947 : answers-students : pearson = 0.2939, spearman = 0.2847
2019-02-13 22:42:54,805 : belief : pearson = 0.0611, spearman = 0.0489
2019-02-13 22:42:55,844 : headlines : pearson = 0.2532, spearman = 0.2803
2019-02-13 22:42:56,796 : images : pearson = 0.1223, spearman = 0.1674
2019-02-13 22:42:56,797 : ALL (weighted average) : Pearson = 0.1774,             Spearman = 0.1892
2019-02-13 22:42:56,797 : ALL (average) : Pearson = 0.1500,             Spearman = 0.1562

2019-02-13 22:42:56,797 : ***** Transfer task : STS16 *****


2019-02-13 22:42:56,871 : loading BERT model bert-base-uncased
2019-02-13 22:42:56,871 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:42:56,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:42:56,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6687veai
2019-02-13 22:42:59,344 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:43:01,223 : answer-answer : pearson = 0.2212, spearman = 0.2369
2019-02-13 22:43:01,523 : headlines : pearson = 0.3153, spearman = 0.3332
2019-02-13 22:43:01,919 : plagiarism : pearson = 0.1316, spearman = 0.1420
2019-02-13 22:43:02,554 : postediting : pearson = 0.4427, spearman = 0.4986
2019-02-13 22:43:02,830 : question-question : pearson = -0.0300, spearman = -0.0192
2019-02-13 22:43:02,830 : ALL (weighted average) : Pearson = 0.2249,             Spearman = 0.2474
2019-02-13 22:43:02,830 : ALL (average) : Pearson = 0.2162,             Spearman = 0.2383

2019-02-13 22:43:02,830 : ***** Transfer task : MR *****


2019-02-13 22:43:02,850 : loading BERT model bert-base-uncased
2019-02-13 22:43:02,850 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:43:02,871 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:43:02,871 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpokx19f1t
2019-02-13 22:43:05,320 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:43:06,818 : Generating sentence embeddings
2019-02-13 22:43:20,444 : Generated sentence embeddings
2019-02-13 22:43:20,444 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 22:43:31,359 : Best param found at split 1: l2reg = 1e-05                 with score 62.43
2019-02-13 22:43:44,641 : Best param found at split 2: l2reg = 0.0001                 with score 62.66
2019-02-13 22:43:53,863 : Best param found at split 3: l2reg = 1e-05                 with score 64.0
2019-02-13 22:44:05,675 : Best param found at split 4: l2reg = 1e-05                 with score 62.88
2019-02-13 22:44:18,739 : Best param found at split 5: l2reg = 1e-05                 with score 62.66
2019-02-13 22:44:19,468 : Dev acc : 62.93 Test acc : 61.05

2019-02-13 22:44:19,469 : ***** Transfer task : CR *****


2019-02-13 22:44:19,477 : loading BERT model bert-base-uncased
2019-02-13 22:44:19,477 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:44:19,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:44:19,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp07tfxq30
2019-02-13 22:44:21,952 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:44:23,423 : Generating sentence embeddings
2019-02-13 22:44:27,198 : Generated sentence embeddings
2019-02-13 22:44:27,199 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 22:44:30,911 : Best param found at split 1: l2reg = 1e-05                 with score 66.68
2019-02-13 22:44:34,738 : Best param found at split 2: l2reg = 0.0001                 with score 64.72
2019-02-13 22:44:38,293 : Best param found at split 3: l2reg = 0.0001                 with score 66.95
2019-02-13 22:44:42,245 : Best param found at split 4: l2reg = 0.0001                 with score 68.39
2019-02-13 22:44:45,733 : Best param found at split 5: l2reg = 0.0001                 with score 66.6
2019-02-13 22:44:45,969 : Dev acc : 66.67 Test acc : 66.34

2019-02-13 22:44:45,970 : ***** Transfer task : MPQA *****


2019-02-13 22:44:45,976 : loading BERT model bert-base-uncased
2019-02-13 22:44:45,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:44:45,995 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:44:45,995 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvfdgol9r
2019-02-13 22:44:48,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:44:49,947 : Generating sentence embeddings
2019-02-13 22:44:53,675 : Generated sentence embeddings
2019-02-13 22:44:53,676 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 22:45:04,895 : Best param found at split 1: l2reg = 0.001                 with score 80.85
2019-02-13 22:45:17,096 : Best param found at split 2: l2reg = 1e-05                 with score 77.94
2019-02-13 22:45:29,816 : Best param found at split 3: l2reg = 0.001                 with score 80.82
2019-02-13 22:45:42,616 : Best param found at split 4: l2reg = 1e-05                 with score 81.66
2019-02-13 22:45:56,305 : Best param found at split 5: l2reg = 0.001                 with score 82.41
2019-02-13 22:45:57,023 : Dev acc : 80.74 Test acc : 81.13

2019-02-13 22:45:57,023 : ***** Transfer task : SUBJ *****


2019-02-13 22:45:57,040 : loading BERT model bert-base-uncased
2019-02-13 22:45:57,040 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:45:57,061 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:45:57,061 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqqo126hm
2019-02-13 22:45:59,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:46:00,952 : Generating sentence embeddings
2019-02-13 22:46:14,368 : Generated sentence embeddings
2019-02-13 22:46:14,369 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-13 22:46:26,278 : Best param found at split 1: l2reg = 0.0001                 with score 87.89
2019-02-13 22:46:38,573 : Best param found at split 2: l2reg = 0.0001                 with score 89.5
2019-02-13 22:46:49,040 : Best param found at split 3: l2reg = 1e-05                 with score 88.5
2019-02-13 22:47:02,236 : Best param found at split 4: l2reg = 1e-05                 with score 89.28
2019-02-13 22:47:14,862 : Best param found at split 5: l2reg = 1e-05                 with score 88.25
2019-02-13 22:47:15,689 : Dev acc : 88.68 Test acc : 88.8

2019-02-13 22:47:15,691 : ***** Transfer task : SST Binary classification *****


2019-02-13 22:47:15,834 : loading BERT model bert-base-uncased
2019-02-13 22:47:15,834 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:47:15,859 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:47:15,859 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvwxdfajx
2019-02-13 22:47:18,328 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:47:19,821 : Computing embedding for train
2019-02-13 22:48:05,809 : Computed train embeddings
2019-02-13 22:48:05,809 : Computing embedding for dev
2019-02-13 22:48:06,785 : Computed dev embeddings
2019-02-13 22:48:06,785 : Computing embedding for test
2019-02-13 22:48:08,882 : Computed test embeddings
2019-02-13 22:48:08,882 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:48:26,851 : [('reg:1e-05', 74.54), ('reg:0.0001', 74.43), ('reg:0.001', 72.71), ('reg:0.01', 70.87)]
2019-02-13 22:48:26,851 : Validation : best param found is reg = 1e-05 with score             74.54
2019-02-13 22:48:26,851 : Evaluating...
2019-02-13 22:48:31,706 : 
Dev acc : 74.54 Test acc : 75.29 for             SST Binary classification

2019-02-13 22:48:31,706 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-13 22:48:31,755 : loading BERT model bert-base-uncased
2019-02-13 22:48:31,755 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:48:31,777 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:48:31,777 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpryzydsns
2019-02-13 22:48:34,224 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:48:35,684 : Computing embedding for train
2019-02-13 22:48:45,244 : Computed train embeddings
2019-02-13 22:48:45,244 : Computing embedding for dev
2019-02-13 22:48:46,460 : Computed dev embeddings
2019-02-13 22:48:46,460 : Computing embedding for test
2019-02-13 22:48:48,897 : Computed test embeddings
2019-02-13 22:48:48,897 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:48:51,101 : [('reg:1e-05', 34.7), ('reg:0.0001', 31.97), ('reg:0.001', 34.24), ('reg:0.01', 31.88)]
2019-02-13 22:48:51,101 : Validation : best param found is reg = 1e-05 with score             34.7
2019-02-13 22:48:51,101 : Evaluating...
2019-02-13 22:48:51,675 : 
Dev acc : 34.7 Test acc : 35.34 for             SST Fine-Grained classification

2019-02-13 22:48:51,675 : ***** Transfer task : TREC *****


2019-02-13 22:48:51,688 : loading BERT model bert-base-uncased
2019-02-13 22:48:51,688 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:48:51,710 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:48:51,710 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmd2jmsug
2019-02-13 22:48:54,152 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:48:59,043 : Computed train embeddings
2019-02-13 22:48:59,307 : Computed test embeddings
2019-02-13 22:48:59,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 22:49:07,722 : [('reg:1e-05', 51.01), ('reg:0.0001', 51.38), ('reg:0.001', 46.33), ('reg:0.01', 42.65)]
2019-02-13 22:49:07,723 : Cross-validation : best param found is reg = 0.0001             with score 51.38
2019-02-13 22:49:07,723 : Evaluating...
2019-02-13 22:49:08,247 : 
Dev acc : 51.38 Test acc : 69.6             for TREC

2019-02-13 22:49:08,248 : ***** Transfer task : MRPC *****


2019-02-13 22:49:08,303 : loading BERT model bert-base-uncased
2019-02-13 22:49:08,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:49:08,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:49:08,325 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7fqzpmq6
2019-02-13 22:49:10,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:49:12,226 : Computing embedding for train
2019-02-13 22:49:22,709 : Computed train embeddings
2019-02-13 22:49:22,709 : Computing embedding for test
2019-02-13 22:49:27,362 : Computed test embeddings
2019-02-13 22:49:27,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-13 22:49:31,921 : [('reg:1e-05', 68.35), ('reg:0.0001', 69.31), ('reg:0.001', 68.33), ('reg:0.01', 67.89)]
2019-02-13 22:49:31,921 : Cross-validation : best param found is reg = 0.0001             with score 69.31
2019-02-13 22:49:31,921 : Evaluating...
2019-02-13 22:49:32,268 : Dev acc : 69.31 Test acc 66.84; Test F1 74.46 for MRPC.

2019-02-13 22:49:32,268 : ***** Transfer task : SICK-Entailment*****


2019-02-13 22:49:32,292 : loading BERT model bert-base-uncased
2019-02-13 22:49:32,292 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:49:32,349 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:49:32,349 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpozaw18ka
2019-02-13 22:49:34,780 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:49:36,264 : Computing embedding for train
2019-02-13 22:49:41,429 : Computed train embeddings
2019-02-13 22:49:41,429 : Computing embedding for dev
2019-02-13 22:49:42,114 : Computed dev embeddings
2019-02-13 22:49:42,115 : Computing embedding for test
2019-02-13 22:49:47,673 : Computed test embeddings
2019-02-13 22:49:47,702 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 22:49:49,478 : [('reg:1e-05', 62.0), ('reg:0.0001', 69.4), ('reg:0.001', 62.6), ('reg:0.01', 61.6)]
2019-02-13 22:49:49,478 : Validation : best param found is reg = 0.0001 with score             69.4
2019-02-13 22:49:49,478 : Evaluating...
2019-02-13 22:49:50,006 : 
Dev acc : 69.4 Test acc : 68.48 for                        SICK entailment

2019-02-13 22:49:50,006 : ***** Transfer task : SICK-Relatedness*****


2019-02-13 22:49:50,034 : loading BERT model bert-base-uncased
2019-02-13 22:49:50,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:49:50,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:49:50,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsj4rpv7b
2019-02-13 22:49:52,500 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:49:53,953 : Computing embedding for train
2019-02-13 22:49:59,116 : Computed train embeddings
2019-02-13 22:49:59,117 : Computing embedding for dev
2019-02-13 22:49:59,834 : Computed dev embeddings
2019-02-13 22:49:59,834 : Computing embedding for test
2019-02-13 22:50:05,399 : Computed test embeddings
2019-02-13 22:50:25,443 : Dev : Pearson 0.6325980508115783
2019-02-13 22:50:25,443 : Test : Pearson 0.6648471955075063 Spearman 0.6058460661491777 MSE 0.5702086083522679                        for SICK Relatedness

2019-02-13 22:50:25,444 : 

***** Transfer task : STSBenchmark*****


2019-02-13 22:50:25,522 : loading BERT model bert-base-uncased
2019-02-13 22:50:25,522 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:50:25,543 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:50:25,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps090ad_i
2019-02-13 22:50:27,976 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:50:29,414 : Computing embedding for train
2019-02-13 22:50:37,688 : Computed train embeddings
2019-02-13 22:50:37,688 : Computing embedding for dev
2019-02-13 22:50:40,168 : Computed dev embeddings
2019-02-13 22:50:40,168 : Computing embedding for test
2019-02-13 22:50:42,175 : Computed test embeddings
2019-02-13 22:51:06,540 : Dev : Pearson 0.46426771350960594
2019-02-13 22:51:06,540 : Test : Pearson 0.46085922696707404 Spearman 0.44907630475641513 MSE 2.051803230704794                        for SICK Relatedness

2019-02-13 22:51:06,540 : ***** Transfer task : SNLI Entailment*****


2019-02-13 22:51:11,547 : loading BERT model bert-base-uncased
2019-02-13 22:51:11,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 22:51:11,679 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 22:51:11,679 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0uy50hhd
2019-02-13 22:51:14,118 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 22:51:15,743 : PROGRESS (encoding): 0.00%
2019-02-13 22:52:35,767 : PROGRESS (encoding): 14.56%
2019-02-13 22:54:05,060 : PROGRESS (encoding): 29.12%
2019-02-13 22:55:35,452 : PROGRESS (encoding): 43.69%
2019-02-13 22:57:13,588 : PROGRESS (encoding): 58.25%
2019-02-13 22:59:00,421 : PROGRESS (encoding): 72.81%
2019-02-13 23:00:45,963 : PROGRESS (encoding): 87.37%
2019-02-13 23:02:37,863 : PROGRESS (encoding): 0.00%
2019-02-13 23:02:51,527 : PROGRESS (encoding): 0.00%
2019-02-13 23:03:04,938 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-13 23:03:43,119 : [('reg:1e-09', 59.32)]
2019-02-13 23:03:43,119 : Validation : best param found is reg = 1e-09 with score             59.32
2019-02-13 23:03:43,119 : Evaluating...
2019-02-13 23:04:23,678 : Dev acc : 59.32 Test acc : 59.16 for SNLI

2019-02-13 23:04:23,679 : ***** Transfer task: Image Caption Retrieval *****


2019-02-13 23:04:32,613 : loading BERT model bert-base-uncased
2019-02-13 23:04:32,613 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-13 23:04:32,664 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-13 23:04:32,664 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr3b_ukui
2019-02-13 23:04:35,112 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-13 23:04:36,563 : Computing embedding for train
2019-02-13 23:12:20,227 : Computed train embeddings
2019-02-13 23:12:20,228 : Computing embedding for dev
2019-02-13 23:12:39,593 : Computed dev embeddings
2019-02-13 23:12:39,593 : Computing embedding for test
2019-02-13 23:13:01,109 : Computed test embeddings
2019-02-13 23:13:01,125 : prepare data
2019-02-13 23:13:01,186 : start epoch
2019-02-13 23:13:43,335 : samples : 64000
2019-02-13 23:13:53,470 : Image to text: 3.1, 11.74, 19.68, 50.0
2019-02-13 23:14:00,823 : Text to Image: 2.188, 8.56, 14.72, 66.0
2019-02-13 23:14:43,017 : samples : 128000
2019-02-13 23:14:53,124 : Image to text: 4.26, 15.76, 24.82, 35.0
2019-02-13 23:15:00,475 : Text to Image: 3.384, 12.084, 19.712, 48.0
2019-02-13 23:15:42,469 : samples : 192000
2019-02-13 23:15:52,559 : Image to text: 5.3, 16.84, 27.08, 32.0
2019-02-13 23:15:59,912 : Text to Image: 4.18, 14.3, 22.432, 40.0
2019-02-13 23:16:43,545 : samples : 256000
2019-02-13 23:16:53,802 : Image to text: 5.22, 16.78, 26.36, 35.0
2019-02-13 23:17:01,200 : Text to Image: 3.484, 12.72, 20.76, 45.0
2019-02-13 23:17:41,829 : samples : 320000
2019-02-13 23:17:52,186 : Image to text: 5.54, 17.96, 27.24, 32.0
2019-02-13 23:17:59,574 : Text to Image: 3.668, 13.756, 21.908, 42.0
2019-02-13 23:18:40,073 : samples : 384000
2019-02-13 23:18:50,302 : Image to text: 6.28, 19.94, 30.28, 28.0
2019-02-13 23:18:57,679 : Text to Image: 4.908, 15.924, 25.452, 34.0
2019-02-13 23:19:37,782 : samples : 448000
2019-02-13 23:19:48,044 : Image to text: 6.34, 19.94, 30.06, 27.0
2019-02-13 23:19:55,457 : Text to Image: 5.116, 16.8, 26.12, 33.0
2019-02-13 23:20:37,235 : samples : 512000
2019-02-13 23:20:47,529 : Image to text: 6.82, 20.68, 30.92, 27.0
2019-02-13 23:20:54,952 : Text to Image: 5.036, 17.152, 26.548, 32.0
2019-02-13 23:21:30,957 : Epoch 1 finished
2019-02-13 23:21:31,356 : Image to text: 17.7, 44.9, 62.5, 7.0
2019-02-13 23:21:31,657 : Text to Image: 15.18, 40.7, 57.9, 8.0
2019-02-13 23:21:32,054 : Image to text: 17.9, 48.8, 64.6, 6.0
2019-02-13 23:21:32,354 : Text to Image: 14.36, 41.98, 59.3, 7.0
2019-02-13 23:21:32,751 : Image to text: 18.6, 46.0, 63.2, 6.0
2019-02-13 23:21:33,055 : Text to Image: 14.28, 41.26, 57.56, 8.0
2019-02-13 23:21:33,471 : Image to text: 21.3, 49.6, 65.1, 6.0
2019-02-13 23:21:33,782 : Text to Image: 14.22, 40.74, 58.48, 8.0
2019-02-13 23:21:34,203 : Image to text: 19.4, 49.1, 65.6, 6.0
2019-02-13 23:21:34,515 : Text to Image: 14.8, 42.28, 58.98, 8.0
2019-02-13 23:21:34,515 : Dev mean Text to Image: 14.568000000000001, 41.391999999999996, 58.443999999999996, 7.799999999999999
2019-02-13 23:21:34,515 : Dev mean Image to text: 18.98, 47.68, 64.2, 6.2
2019-02-13 23:21:34,516 : start epoch
2019-02-13 23:22:17,854 : samples : 64000
2019-02-13 23:22:28,060 : Image to text: 6.86, 21.66, 32.6, 24.0
2019-02-13 23:22:35,469 : Text to Image: 5.424, 17.52, 27.456, 30.0
2019-02-13 23:23:18,118 : samples : 128000
2019-02-13 23:23:28,325 : Image to text: 7.28, 20.62, 31.42, 25.0
2019-02-13 23:23:35,731 : Text to Image: 5.256, 17.964, 27.612, 30.0
2019-02-13 23:24:19,248 : samples : 192000
2019-02-13 23:24:29,359 : Image to text: 6.06, 19.48, 30.54, 26.0
2019-02-13 23:24:36,751 : Text to Image: 4.876, 16.444, 25.748, 33.0
2019-02-13 23:25:20,050 : samples : 256000
2019-02-13 23:25:30,335 : Image to text: 6.96, 21.64, 32.26, 25.0
2019-02-13 23:25:37,758 : Text to Image: 6.012, 18.496, 28.532, 29.0
2019-02-13 23:26:19,649 : samples : 320000
2019-02-13 23:26:29,908 : Image to text: 7.32, 22.22, 32.62, 24.0
2019-02-13 23:26:37,294 : Text to Image: 5.384, 17.184, 27.16, 31.0
2019-02-13 23:27:18,215 : samples : 384000
2019-02-13 23:27:28,589 : Image to text: 6.5, 21.44, 32.72, 23.0
2019-02-13 23:27:36,019 : Text to Image: 5.228, 17.056, 27.0, 32.0
2019-02-13 23:28:16,588 : samples : 448000
2019-02-13 23:28:27,048 : Image to text: 7.26, 21.52, 33.22, 24.0
2019-02-13 23:28:34,516 : Text to Image: 5.284, 17.668, 27.684, 30.0
2019-02-13 23:29:15,067 : samples : 512000
2019-02-13 23:29:25,374 : Image to text: 7.12, 22.5, 33.24, 24.0
2019-02-13 23:29:32,781 : Text to Image: 5.472, 18.088, 27.62, 31.0
2019-02-13 23:30:07,308 : Epoch 2 finished
2019-02-13 23:30:07,732 : Image to text: 18.8, 48.0, 64.1, 6.0
2019-02-13 23:30:08,041 : Text to Image: 16.68, 45.82, 62.02, 7.0
2019-02-13 23:30:08,463 : Image to text: 20.5, 49.7, 66.6, 6.0
2019-02-13 23:30:08,773 : Text to Image: 15.48, 45.34, 63.3, 6.0
2019-02-13 23:30:09,196 : Image to text: 20.9, 49.5, 64.0, 6.0
2019-02-13 23:30:09,509 : Text to Image: 16.14, 44.8, 61.8, 7.0
2019-02-13 23:30:09,937 : Image to text: 21.6, 51.4, 67.1, 5.0
2019-02-13 23:30:10,251 : Text to Image: 15.98, 45.1, 62.9, 7.0
2019-02-13 23:30:10,676 : Image to text: 23.2, 51.0, 66.6, 5.0
2019-02-13 23:30:10,986 : Text to Image: 16.9, 45.28, 62.06, 7.0
2019-02-13 23:30:10,986 : Dev mean Text to Image: 16.236, 45.268, 62.416, 6.799999999999999
2019-02-13 23:30:10,986 : Dev mean Image to text: 21.0, 49.92, 65.67999999999999, 5.6
2019-02-13 23:30:10,986 : start epoch
2019-02-13 23:30:51,400 : samples : 64000
2019-02-13 23:31:01,504 : Image to text: 6.64, 22.0, 33.14, 23.0
2019-02-13 23:31:08,842 : Text to Image: 4.996, 17.336, 26.796, 32.0
2019-02-13 23:31:49,079 : samples : 128000
2019-02-13 23:31:59,199 : Image to text: 7.4, 23.04, 34.44, 22.0
2019-02-13 23:32:06,541 : Text to Image: 5.9, 19.652, 29.96, 27.0
2019-02-13 23:32:50,714 : samples : 192000
2019-02-13 23:33:00,810 : Image to text: 7.5, 23.7, 34.54, 22.0
2019-02-13 23:33:08,114 : Text to Image: 5.844, 19.504, 29.956, 27.0
2019-02-13 23:33:53,306 : samples : 256000
2019-02-13 23:34:03,592 : Image to text: 7.48, 23.3, 34.7, 22.0
2019-02-13 23:34:11,014 : Text to Image: 6.104, 19.54, 29.628, 27.0
2019-02-13 23:34:53,962 : samples : 320000
2019-02-13 23:35:04,261 : Image to text: 8.04, 22.42, 33.7, 22.0
2019-02-13 23:35:11,697 : Text to Image: 6.076, 19.28, 29.124, 28.0
2019-02-13 23:35:52,175 : samples : 384000
2019-02-13 23:36:02,463 : Image to text: 7.38, 23.88, 35.1, 22.0
2019-02-13 23:36:09,882 : Text to Image: 6.236, 19.78, 30.3, 26.0
2019-02-13 23:36:50,444 : samples : 448000
2019-02-13 23:37:00,694 : Image to text: 8.36, 24.44, 35.22, 21.0
2019-02-13 23:37:08,085 : Text to Image: 6.772, 21.068, 31.456, 25.0
2019-02-13 23:37:48,629 : samples : 512000
2019-02-13 23:37:58,998 : Image to text: 7.4, 24.0, 34.78, 23.0
2019-02-13 23:38:06,403 : Text to Image: 5.204, 18.376, 28.096, 29.0
2019-02-13 23:38:40,802 : Epoch 3 finished
2019-02-13 23:38:41,210 : Image to text: 20.8, 53.4, 67.0, 5.0
2019-02-13 23:38:41,519 : Text to Image: 16.58, 46.36, 63.24, 6.0
2019-02-13 23:38:41,938 : Image to text: 20.4, 52.1, 67.2, 5.0
2019-02-13 23:38:42,249 : Text to Image: 16.78, 46.8, 64.42, 6.0
2019-02-13 23:38:42,673 : Image to text: 19.9, 52.7, 68.5, 5.0
2019-02-13 23:38:42,984 : Text to Image: 16.18, 46.26, 63.92, 6.0
2019-02-13 23:38:43,410 : Image to text: 23.5, 56.3, 68.8, 4.0
2019-02-13 23:38:43,722 : Text to Image: 16.92, 45.96, 63.84, 6.0
2019-02-13 23:38:44,147 : Image to text: 21.0, 51.9, 68.8, 5.0
2019-02-13 23:38:44,457 : Text to Image: 17.1, 47.08, 64.38, 6.0
2019-02-13 23:38:44,457 : Dev mean Text to Image: 16.712000000000003, 46.49199999999999, 63.96, 6.0
2019-02-13 23:38:44,457 : Dev mean Image to text: 21.119999999999997, 53.28, 68.06, 4.8
2019-02-13 23:38:44,458 : start epoch
2019-02-13 23:39:24,714 : samples : 64000
2019-02-13 23:39:34,816 : Image to text: 8.16, 24.52, 36.02, 21.0
2019-02-13 23:39:42,122 : Text to Image: 6.88, 20.78, 31.38, 25.0
2019-02-13 23:40:23,198 : samples : 128000
2019-02-13 23:40:33,273 : Image to text: 8.08, 24.48, 35.82, 21.0
2019-02-13 23:40:40,598 : Text to Image: 6.528, 20.152, 30.936, 26.0
2019-02-13 23:41:20,943 : samples : 192000
2019-02-13 23:41:31,040 : Image to text: 7.76, 23.22, 35.3, 21.0
2019-02-13 23:41:38,372 : Text to Image: 6.212, 19.776, 30.12, 27.0
2019-02-13 23:42:18,704 : samples : 256000
2019-02-13 23:42:29,016 : Image to text: 8.28, 25.0, 36.34, 20.0
2019-02-13 23:42:39,049 : Text to Image: 6.9, 21.196, 31.776, 24.0
2019-02-13 23:43:23,086 : samples : 320000
2019-02-13 23:43:35,732 : Image to text: 8.9, 24.44, 36.42, 20.0
2019-02-13 23:43:45,797 : Text to Image: 6.964, 21.828, 32.384, 24.0
2019-02-13 23:44:27,748 : samples : 384000
2019-02-13 23:44:37,769 : Image to text: 7.94, 24.32, 36.58, 20.0
2019-02-13 23:44:44,797 : Text to Image: 6.68, 21.536, 32.508, 24.0
2019-02-13 23:45:28,099 : samples : 448000
2019-02-13 23:45:40,774 : Image to text: 8.5, 24.92, 37.28, 19.0
2019-02-13 23:45:50,854 : Text to Image: 7.204, 21.396, 32.136, 24.0
2019-02-13 23:46:33,153 : samples : 512000
2019-02-13 23:46:43,237 : Image to text: 8.58, 25.56, 36.12, 19.0
2019-02-13 23:46:50,356 : Text to Image: 7.192, 22.664, 33.928, 22.0
2019-02-13 23:47:25,784 : Epoch 4 finished
2019-02-13 23:47:26,715 : Image to text: 21.7, 53.2, 70.4, 5.0
2019-02-13 23:47:27,497 : Text to Image: 17.56, 46.82, 64.84, 6.0
2019-02-13 23:47:28,454 : Image to text: 23.3, 53.6, 71.0, 5.0
2019-02-13 23:47:29,204 : Text to Image: 16.92, 46.66, 64.66, 6.0
2019-02-13 23:47:30,152 : Image to text: 23.7, 53.8, 68.5, 5.0
2019-02-13 23:47:30,896 : Text to Image: 17.86, 47.3, 64.6, 6.0
2019-02-13 23:47:31,820 : Image to text: 26.4, 57.4, 73.3, 4.0
2019-02-13 23:47:32,620 : Text to Image: 17.06, 46.08, 64.28, 6.0
2019-02-13 23:47:33,585 : Image to text: 25.9, 56.5, 71.7, 4.0
2019-02-13 23:47:34,348 : Text to Image: 17.9, 47.58, 64.38, 6.0
2019-02-13 23:47:34,348 : Dev mean Text to Image: 17.459999999999997, 46.888, 64.55199999999999, 6.0
2019-02-13 23:47:34,348 : Dev mean Image to text: 24.2, 54.89999999999999, 70.98, 4.6
2019-02-13 23:47:34,348 : start epoch
2019-02-13 23:48:19,267 : samples : 64000
2019-02-13 23:48:30,841 : Image to text: 8.3, 24.2, 36.56, 19.0
2019-02-13 23:48:38,045 : Text to Image: 7.064, 22.04, 32.892, 23.0
2019-02-13 23:49:19,280 : samples : 128000
2019-02-13 23:49:31,841 : Image to text: 7.98, 24.88, 36.58, 19.0
2019-02-13 23:49:41,827 : Text to Image: 6.684, 21.308, 32.068, 24.0
2019-02-13 23:50:26,183 : samples : 192000
2019-02-13 23:50:36,667 : Image to text: 8.64, 25.5, 37.38, 19.0
2019-02-13 23:50:43,843 : Text to Image: 6.752, 21.064, 32.092, 24.0
2019-02-13 23:51:25,242 : samples : 256000
2019-02-13 23:51:37,845 : Image to text: 8.84, 25.18, 36.76, 19.0
2019-02-13 23:51:47,882 : Text to Image: 6.812, 21.844, 33.26, 23.0
2019-02-13 23:52:32,166 : samples : 320000
2019-02-13 23:52:44,815 : Image to text: 8.66, 24.24, 36.14, 21.0
2019-02-13 23:52:52,217 : Text to Image: 6.416, 21.096, 32.096, 24.0
2019-02-13 23:53:32,848 : samples : 384000
2019-02-13 23:53:44,588 : Image to text: 8.32, 26.0, 37.78, 18.0
2019-02-13 23:53:54,619 : Text to Image: 7.084, 21.488, 32.344, 24.0
2019-02-13 23:54:38,270 : samples : 448000
2019-02-13 23:54:50,914 : Image to text: 8.76, 25.72, 37.52, 18.0
2019-02-13 23:55:00,504 : Text to Image: 7.352, 23.108, 34.368, 22.0
2019-02-13 23:55:40,933 : samples : 512000
2019-02-13 23:55:51,706 : Image to text: 9.1, 26.26, 38.52, 18.0
2019-02-13 23:56:01,703 : Text to Image: 7.48, 22.808, 33.92, 22.0
2019-02-13 23:56:38,504 : Epoch 5 finished
2019-02-13 23:56:39,444 : Image to text: 22.7, 56.3, 70.9, 4.0
2019-02-13 23:56:40,236 : Text to Image: 18.16, 48.6, 66.28, 6.0
2019-02-13 23:56:41,171 : Image to text: 23.0, 56.1, 70.4, 5.0
2019-02-13 23:56:41,946 : Text to Image: 17.48, 46.84, 65.7, 6.0
2019-02-13 23:56:42,945 : Image to text: 22.6, 53.9, 69.6, 5.0
2019-02-13 23:56:43,721 : Text to Image: 18.02, 48.06, 65.42, 6.0
2019-02-13 23:56:44,661 : Image to text: 25.9, 58.5, 73.7, 4.0
2019-02-13 23:56:45,458 : Text to Image: 18.3, 48.82, 65.84, 6.0
2019-02-13 23:56:46,441 : Image to text: 24.1, 57.2, 72.6, 4.0
2019-02-13 23:56:47,201 : Text to Image: 18.84, 48.58, 65.76, 6.0
2019-02-13 23:56:47,202 : Dev mean Text to Image: 18.16, 48.18, 65.80000000000001, 6.0
2019-02-13 23:56:47,202 : Dev mean Image to text: 23.66, 56.39999999999999, 71.44000000000001, 4.3999999999999995
2019-02-13 23:56:47,202 : start epoch
2019-02-13 23:57:29,956 : samples : 64000
2019-02-13 23:57:40,013 : Image to text: 8.84, 25.74, 37.04, 19.0
2019-02-13 23:57:47,157 : Text to Image: 7.348, 22.296, 33.168, 23.0
2019-02-13 23:58:29,782 : samples : 128000
2019-02-13 23:58:42,420 : Image to text: 8.12, 24.02, 35.96, 20.0
2019-02-13 23:58:52,472 : Text to Image: 6.596, 20.26, 30.916, 25.0
2019-02-13 23:59:35,059 : samples : 192000
2019-02-13 23:59:45,143 : Image to text: 9.12, 25.8, 37.06, 19.0
2019-02-13 23:59:52,010 : Text to Image: 7.332, 22.28, 33.504, 23.0
2019-02-14 00:00:35,280 : samples : 256000
2019-02-14 00:00:47,954 : Image to text: 8.94, 26.44, 37.74, 18.0
2019-02-14 00:00:58,069 : Text to Image: 7.304, 22.264, 33.124, 23.0
2019-02-14 00:01:41,273 : samples : 320000
2019-02-14 00:01:51,343 : Image to text: 9.18, 26.12, 38.64, 18.0
2019-02-14 00:01:58,541 : Text to Image: 7.56, 22.808, 33.828, 22.0
2019-02-14 00:02:39,809 : samples : 384000
2019-02-14 00:02:52,428 : Image to text: 8.94, 25.62, 37.6, 19.0
2019-02-14 00:03:02,528 : Text to Image: 7.376, 22.644, 33.86, 22.0
2019-02-14 00:03:46,815 : samples : 448000
2019-02-14 00:03:57,909 : Image to text: 8.54, 25.98, 38.94, 18.0
2019-02-14 00:04:05,101 : Text to Image: 7.132, 22.256, 33.228, 23.0
2019-02-14 00:04:45,950 : samples : 512000
2019-02-14 00:04:56,689 : Image to text: 8.3, 26.26, 38.62, 17.0
2019-02-14 00:05:05,267 : Text to Image: 7.136, 22.676, 33.904, 22.0
2019-02-14 00:05:41,402 : Epoch 6 finished
2019-02-14 00:05:41,860 : Image to text: 22.9, 55.8, 72.5, 4.0
2019-02-14 00:05:42,221 : Text to Image: 20.28, 52.3, 69.6, 5.0
2019-02-14 00:05:42,669 : Image to text: 23.0, 57.4, 71.4, 4.0
2019-02-14 00:05:43,029 : Text to Image: 20.08, 52.1, 69.08, 5.0
2019-02-14 00:05:43,478 : Image to text: 24.1, 56.7, 72.6, 4.0
2019-02-14 00:05:43,839 : Text to Image: 20.86, 52.72, 69.38, 5.0
2019-02-14 00:05:44,287 : Image to text: 26.7, 59.8, 75.9, 4.0
2019-02-14 00:05:44,648 : Text to Image: 20.36, 52.56, 69.88, 5.0
2019-02-14 00:05:45,095 : Image to text: 26.9, 58.5, 73.0, 4.0
2019-02-14 00:05:45,459 : Text to Image: 20.94, 52.54, 69.08, 5.0
2019-02-14 00:05:45,459 : Dev mean Text to Image: 20.503999999999998, 52.444, 69.404, 5.0
2019-02-14 00:05:45,459 : Dev mean Image to text: 24.72, 57.64, 73.08, 4.0
2019-02-14 00:05:45,459 : start epoch
2019-02-14 00:06:25,932 : samples : 64000
2019-02-14 00:06:36,711 : Image to text: 9.04, 26.46, 39.2, 17.0
2019-02-14 00:06:43,943 : Text to Image: 7.768, 23.604, 35.24, 21.0
2019-02-14 00:07:24,637 : samples : 128000
2019-02-14 00:07:34,684 : Image to text: 9.02, 26.46, 39.16, 17.0
2019-02-14 00:07:41,919 : Text to Image: 7.536, 22.952, 34.356, 22.0
2019-02-14 00:08:23,498 : samples : 192000
2019-02-14 00:08:33,512 : Image to text: 9.54, 26.68, 39.3, 18.0
2019-02-14 00:08:40,676 : Text to Image: 7.636, 23.772, 35.332, 21.0
2019-02-14 00:09:21,091 : samples : 256000
2019-02-14 00:09:33,716 : Image to text: 8.5, 25.92, 38.5, 19.0
2019-02-14 00:09:43,867 : Text to Image: 7.564, 22.908, 34.236, 22.0
2019-02-14 00:10:26,678 : samples : 320000
2019-02-14 00:10:36,737 : Image to text: 9.3, 26.76, 38.78, 17.0
2019-02-14 00:10:44,682 : Text to Image: 7.924, 24.164, 35.824, 20.0
2019-02-14 00:11:26,995 : samples : 384000
2019-02-14 00:11:38,084 : Image to text: 9.54, 26.68, 39.42, 17.0
2019-02-14 00:11:46,783 : Text to Image: 7.496, 23.54, 34.924, 21.0
2019-02-14 00:12:28,075 : samples : 448000
2019-02-14 00:12:40,823 : Image to text: 9.3, 27.08, 39.88, 17.0
2019-02-14 00:12:48,256 : Text to Image: 8.36, 24.62, 36.384, 20.0
2019-02-14 00:13:30,203 : samples : 512000
2019-02-14 00:13:42,768 : Image to text: 9.76, 27.44, 39.18, 17.0
2019-02-14 00:13:52,767 : Text to Image: 7.816, 23.328, 34.888, 21.0
2019-02-14 00:14:29,567 : Epoch 7 finished
2019-02-14 00:14:30,462 : Image to text: 22.7, 55.1, 72.5, 5.0
2019-02-14 00:14:31,204 : Text to Image: 19.88, 52.16, 68.78, 5.0
2019-02-14 00:14:32,091 : Image to text: 24.7, 56.5, 71.3, 4.0
2019-02-14 00:14:32,837 : Text to Image: 20.32, 52.3, 69.9, 5.0
2019-02-14 00:14:33,755 : Image to text: 23.2, 57.7, 72.4, 4.0
2019-02-14 00:14:34,511 : Text to Image: 20.12, 52.86, 69.6, 5.0
2019-02-14 00:14:35,438 : Image to text: 27.1, 59.2, 73.5, 4.0
2019-02-14 00:14:36,207 : Text to Image: 20.9, 53.22, 70.26, 5.0
2019-02-14 00:14:37,085 : Image to text: 25.9, 58.9, 73.1, 4.0
2019-02-14 00:14:37,844 : Text to Image: 20.38, 52.38, 68.92, 5.0
2019-02-14 00:14:37,844 : Dev mean Text to Image: 20.32, 52.583999999999996, 69.492, 5.0
2019-02-14 00:14:37,844 : Dev mean Image to text: 24.72, 57.480000000000004, 72.56, 4.2
2019-02-14 00:14:37,844 : start epoch
2019-02-14 00:15:21,593 : samples : 64000
2019-02-14 00:15:34,256 : Image to text: 9.18, 26.34, 39.12, 18.0
2019-02-14 00:15:44,241 : Text to Image: 7.396, 21.924, 33.308, 23.0
2019-02-14 00:16:27,923 : samples : 128000
2019-02-14 00:16:40,543 : Image to text: 9.84, 27.3, 39.44, 17.0
2019-02-14 00:16:50,577 : Text to Image: 7.712, 23.492, 34.824, 21.0
2019-02-14 00:17:34,257 : samples : 192000
2019-02-14 00:17:46,924 : Image to text: 9.82, 27.64, 39.18, 17.0
2019-02-14 00:17:56,998 : Text to Image: 7.516, 22.772, 34.112, 22.0
2019-02-14 00:18:40,772 : samples : 256000
2019-02-14 00:18:53,401 : Image to text: 8.96, 26.68, 38.88, 18.0
2019-02-14 00:19:03,414 : Text to Image: 7.344, 23.216, 34.5, 22.0
2019-02-14 00:19:47,368 : samples : 320000
2019-02-14 00:19:59,981 : Image to text: 10.38, 27.92, 39.96, 17.0
2019-02-14 00:20:10,016 : Text to Image: 8.048, 23.68, 35.28, 21.0
2019-02-14 00:20:52,435 : samples : 384000
2019-02-14 00:21:05,079 : Image to text: 9.54, 28.14, 40.36, 17.0
2019-02-14 00:21:15,057 : Text to Image: 7.544, 22.988, 34.404, 22.0
2019-02-14 00:21:57,148 : samples : 448000
2019-02-14 00:22:09,726 : Image to text: 10.5, 28.44, 41.62, 15.0
2019-02-14 00:22:19,718 : Text to Image: 8.816, 25.304, 36.576, 19.0
2019-02-14 00:23:02,083 : samples : 512000
2019-02-14 00:23:14,752 : Image to text: 9.6, 28.0, 40.18, 16.0
2019-02-14 00:23:24,790 : Text to Image: 8.004, 23.724, 35.288, 21.0
2019-02-14 00:24:01,876 : Epoch 8 finished
2019-02-14 00:24:02,833 : Image to text: 25.6, 57.6, 72.8, 4.0
2019-02-14 00:24:03,576 : Text to Image: 20.28, 52.14, 69.7, 5.0
2019-02-14 00:24:04,515 : Image to text: 24.9, 58.3, 74.3, 4.0
2019-02-14 00:24:05,269 : Text to Image: 20.78, 52.04, 70.22, 5.0
2019-02-14 00:24:06,232 : Image to text: 24.2, 57.1, 71.4, 4.0
2019-02-14 00:24:06,985 : Text to Image: 20.58, 52.92, 70.12, 5.0
2019-02-14 00:24:08,053 : Image to text: 26.6, 61.7, 76.4, 4.0
2019-02-14 00:24:08,791 : Text to Image: 21.02, 53.48, 70.64, 5.0
2019-02-14 00:24:09,737 : Image to text: 26.6, 59.1, 73.7, 4.0
2019-02-14 00:24:10,484 : Text to Image: 20.2, 53.12, 69.72, 5.0
2019-02-14 00:24:10,484 : Dev mean Text to Image: 20.572, 52.739999999999995, 70.08, 5.0
2019-02-14 00:24:10,484 : Dev mean Image to text: 25.58, 58.76, 73.72, 4.0
2019-02-14 00:24:10,484 : start epoch
2019-02-14 00:24:54,668 : samples : 64000
2019-02-14 00:25:06,297 : Image to text: 9.98, 27.92, 40.08, 17.0
2019-02-14 00:25:13,483 : Text to Image: 8.052, 24.516, 35.72, 20.0
2019-02-14 00:25:54,224 : samples : 128000
2019-02-14 00:26:04,060 : Image to text: 9.4, 26.78, 39.34, 17.0
2019-02-14 00:26:10,784 : Text to Image: 7.736, 23.068, 34.408, 22.0
2019-02-14 00:26:55,034 : samples : 192000
2019-02-14 00:27:07,898 : Image to text: 9.5, 27.28, 39.24, 17.0
2019-02-14 00:27:18,299 : Text to Image: 7.86, 23.624, 34.628, 21.0
2019-02-14 00:28:03,621 : samples : 256000
2019-02-14 00:28:16,627 : Image to text: 10.3, 28.06, 40.38, 17.0
2019-02-14 00:28:27,110 : Text to Image: 7.688, 23.552, 35.136, 21.0
2019-02-14 00:29:12,213 : samples : 320000
2019-02-14 00:29:25,294 : Image to text: 10.3, 28.44, 41.14, 16.0
2019-02-14 00:29:35,784 : Text to Image: 8.572, 24.864, 36.484, 20.0
2019-02-14 00:30:20,964 : samples : 384000
2019-02-14 00:30:33,979 : Image to text: 9.46, 27.7, 39.52, 17.0
2019-02-14 00:30:44,465 : Text to Image: 7.984, 24.132, 35.716, 20.0
2019-02-14 00:31:29,817 : samples : 448000
2019-02-14 00:31:42,856 : Image to text: 9.36, 27.26, 39.74, 17.0
2019-02-14 00:31:53,372 : Text to Image: 7.76, 23.628, 34.832, 21.0
2019-02-14 00:32:38,123 : samples : 512000
2019-02-14 00:32:51,049 : Image to text: 9.68, 28.38, 41.76, 15.0
2019-02-14 00:33:01,502 : Text to Image: 8.452, 24.628, 35.944, 20.0
2019-02-14 00:33:39,815 : Epoch 9 finished
2019-02-14 00:33:40,929 : Image to text: 25.0, 56.6, 72.1, 4.0
2019-02-14 00:33:41,820 : Text to Image: 20.88, 53.82, 70.8, 5.0
2019-02-14 00:33:42,907 : Image to text: 25.1, 56.4, 72.6, 4.0
2019-02-14 00:33:43,819 : Text to Image: 21.06, 53.22, 70.82, 5.0
2019-02-14 00:33:44,967 : Image to text: 24.0, 58.0, 73.4, 4.0
2019-02-14 00:33:45,797 : Text to Image: 21.18, 54.8, 70.4, 5.0
2019-02-14 00:33:46,809 : Image to text: 26.6, 59.1, 76.9, 4.0
2019-02-14 00:33:47,783 : Text to Image: 21.58, 53.92, 70.92, 5.0
2019-02-14 00:33:48,926 : Image to text: 27.5, 60.5, 73.7, 4.0
2019-02-14 00:33:49,838 : Text to Image: 21.76, 53.2, 70.16, 5.0
2019-02-14 00:33:49,838 : Dev mean Text to Image: 21.291999999999998, 53.792, 70.61999999999999, 5.0
2019-02-14 00:33:49,838 : Dev mean Image to text: 25.64, 58.120000000000005, 73.74, 4.0
2019-02-14 00:33:49,838 : start epoch
2019-02-14 00:34:34,431 : samples : 64000
2019-02-14 00:34:44,766 : Image to text: 9.06, 26.86, 39.28, 17.0
2019-02-14 00:34:52,191 : Text to Image: 7.732, 23.156, 34.68, 21.0
2019-02-14 00:35:33,846 : samples : 128000
2019-02-14 00:35:44,270 : Image to text: 10.42, 28.18, 41.16, 16.0
2019-02-14 00:35:51,700 : Text to Image: 8.348, 24.888, 36.612, 19.0
2019-02-14 00:36:33,609 : samples : 192000
2019-02-14 00:36:43,938 : Image to text: 9.82, 26.92, 40.36, 17.0
2019-02-14 00:36:51,353 : Text to Image: 7.556, 23.512, 34.668, 21.0
2019-02-14 00:37:32,693 : samples : 256000
2019-02-14 00:37:43,010 : Image to text: 10.28, 27.78, 40.68, 16.0
2019-02-14 00:37:50,421 : Text to Image: 7.868, 23.996, 35.844, 21.0
2019-02-14 00:38:32,492 : samples : 320000
2019-02-14 00:38:42,844 : Image to text: 9.44, 26.78, 39.86, 16.0
2019-02-14 00:38:50,266 : Text to Image: 7.992, 23.748, 35.228, 21.0
2019-02-14 00:39:31,034 : samples : 384000
2019-02-14 00:39:41,365 : Image to text: 9.9, 28.08, 40.52, 16.0
2019-02-14 00:39:48,762 : Text to Image: 8.36, 25.096, 36.756, 19.0
2019-02-14 00:40:30,135 : samples : 448000
2019-02-14 00:40:40,418 : Image to text: 8.84, 25.56, 38.66, 18.0
2019-02-14 00:40:47,825 : Text to Image: 7.348, 22.232, 33.0, 23.0
2019-02-14 00:41:28,719 : samples : 512000
2019-02-14 00:41:38,995 : Image to text: 10.32, 28.02, 40.42, 17.0
2019-02-14 00:41:46,386 : Text to Image: 8.232, 24.832, 36.476, 19.0
2019-02-14 00:42:25,698 : Epoch 10 finished
2019-02-14 00:42:26,130 : Image to text: 25.1, 57.7, 74.8, 4.0
2019-02-14 00:42:26,443 : Text to Image: 21.46, 52.64, 69.64, 5.0
2019-02-14 00:42:26,872 : Image to text: 25.0, 56.2, 73.5, 4.0
2019-02-14 00:42:27,187 : Text to Image: 21.18, 53.76, 71.24, 5.0
2019-02-14 00:42:27,614 : Image to text: 24.7, 59.7, 74.5, 4.0
2019-02-14 00:42:27,928 : Text to Image: 21.08, 54.1, 70.8, 5.0
2019-02-14 00:42:28,352 : Image to text: 27.1, 60.2, 76.1, 4.0
2019-02-14 00:42:28,664 : Text to Image: 21.08, 53.92, 71.36, 5.0
2019-02-14 00:42:29,089 : Image to text: 27.9, 60.0, 74.6, 4.0
2019-02-14 00:42:29,401 : Text to Image: 22.36, 54.02, 70.32, 5.0
2019-02-14 00:42:29,401 : Dev mean Text to Image: 21.431999999999995, 53.688, 70.672, 5.0
2019-02-14 00:42:29,401 : Dev mean Image to text: 25.96, 58.76, 74.69999999999999, 4.0
2019-02-14 00:42:29,401 : start epoch
2019-02-14 00:43:12,313 : samples : 64000
2019-02-14 00:43:22,575 : Image to text: 10.78, 29.18, 42.12, 15.0
2019-02-14 00:43:29,981 : Text to Image: 8.692, 25.9, 37.608, 19.0
2019-02-14 00:44:11,525 : samples : 128000
2019-02-14 00:44:21,862 : Image to text: 9.88, 27.52, 40.0, 17.0
2019-02-14 00:44:29,309 : Text to Image: 8.056, 24.092, 35.908, 20.0
2019-02-14 00:45:10,778 : samples : 192000
2019-02-14 00:45:21,126 : Image to text: 9.64, 27.22, 39.84, 17.0
2019-02-14 00:45:28,575 : Text to Image: 7.796, 23.812, 35.404, 21.0
2019-02-14 00:46:09,738 : samples : 256000
2019-02-14 00:46:20,052 : Image to text: 10.06, 27.98, 41.32, 15.0
2019-02-14 00:46:27,463 : Text to Image: 8.152, 24.504, 35.8, 20.0
2019-02-14 00:47:08,943 : samples : 320000
2019-02-14 00:47:19,210 : Image to text: 9.86, 28.08, 40.04, 16.0
2019-02-14 00:47:26,641 : Text to Image: 7.656, 23.888, 35.624, 21.0
2019-02-14 00:48:08,597 : samples : 384000
2019-02-14 00:48:18,957 : Image to text: 10.4, 28.46, 41.44, 16.0
2019-02-14 00:48:26,392 : Text to Image: 8.32, 25.08, 36.716, 20.0
2019-02-14 00:49:08,505 : samples : 448000
2019-02-14 00:49:18,795 : Image to text: 9.92, 28.56, 40.82, 16.0
2019-02-14 00:49:26,209 : Text to Image: 8.244, 24.94, 36.756, 19.0
2019-02-14 00:50:07,578 : samples : 512000
2019-02-14 00:50:17,873 : Image to text: 9.94, 28.64, 41.94, 15.0
2019-02-14 00:50:25,287 : Text to Image: 8.648, 25.408, 37.388, 19.0
2019-02-14 00:50:59,752 : Epoch 11 finished
2019-02-14 00:51:00,150 : Image to text: 26.8, 57.1, 73.5, 4.0
2019-02-14 00:51:00,450 : Text to Image: 22.22, 55.06, 71.82, 4.0
2019-02-14 00:51:00,848 : Image to text: 25.9, 57.0, 73.6, 4.0
2019-02-14 00:51:01,149 : Text to Image: 21.14, 54.12, 71.2, 5.0
2019-02-14 00:51:01,566 : Image to text: 25.4, 59.3, 73.7, 4.0
2019-02-14 00:51:01,878 : Text to Image: 21.82, 54.66, 71.02, 5.0
2019-02-14 00:51:02,299 : Image to text: 25.6, 59.9, 75.4, 4.0
2019-02-14 00:51:02,612 : Text to Image: 20.94, 54.3, 71.92, 5.0
2019-02-14 00:51:03,037 : Image to text: 28.4, 58.9, 72.8, 4.0
2019-02-14 00:51:03,349 : Text to Image: 22.0, 54.08, 71.18, 5.0
2019-02-14 00:51:03,350 : Dev mean Text to Image: 21.624000000000002, 54.444, 71.428, 4.8
2019-02-14 00:51:03,350 : Dev mean Image to text: 26.419999999999998, 58.44, 73.8, 4.0
2019-02-14 00:51:03,350 : start epoch
2019-02-14 00:51:43,738 : samples : 64000
2019-02-14 00:51:53,961 : Image to text: 10.62, 29.32, 42.26, 15.0
2019-02-14 00:52:01,328 : Text to Image: 8.74, 25.532, 37.508, 19.0
2019-02-14 00:52:41,586 : samples : 128000
2019-02-14 00:52:51,842 : Image to text: 10.7, 29.84, 41.54, 16.0
2019-02-14 00:52:59,263 : Text to Image: 8.288, 24.804, 36.364, 20.0
2019-02-14 00:53:39,292 : samples : 192000
2019-02-14 00:53:49,630 : Image to text: 10.24, 28.04, 41.52, 16.0
2019-02-14 00:53:57,059 : Text to Image: 8.576, 25.056, 37.18, 19.0
2019-02-14 00:54:38,972 : samples : 256000
2019-02-14 00:54:49,312 : Image to text: 10.34, 28.64, 41.92, 15.0
2019-02-14 00:54:56,715 : Text to Image: 8.484, 25.256, 37.348, 19.0
2019-02-14 00:55:38,181 : samples : 320000
2019-02-14 00:55:48,513 : Image to text: 10.38, 28.58, 41.16, 16.0
2019-02-14 00:55:55,929 : Text to Image: 8.336, 24.212, 35.716, 21.0
2019-02-14 00:56:37,880 : samples : 384000
2019-02-14 00:56:48,197 : Image to text: 10.16, 29.2, 41.86, 15.0
2019-02-14 00:56:55,631 : Text to Image: 8.756, 25.312, 36.952, 19.0
2019-02-14 00:57:37,870 : samples : 448000
2019-02-14 00:57:48,203 : Image to text: 10.26, 29.56, 42.0, 15.0
2019-02-14 00:57:55,618 : Text to Image: 8.732, 25.728, 37.54, 19.0
2019-02-14 00:58:36,159 : samples : 512000
2019-02-14 00:58:46,452 : Image to text: 10.24, 28.22, 41.04, 16.0
2019-02-14 00:58:53,830 : Text to Image: 8.136, 24.948, 36.62, 19.0
2019-02-14 00:59:28,080 : Epoch 12 finished
2019-02-14 00:59:28,573 : Image to text: 25.0, 55.0, 71.0, 4.0
2019-02-14 00:59:28,914 : Text to Image: 19.5, 51.36, 68.84, 5.0
2019-02-14 00:59:29,412 : Image to text: 24.1, 56.0, 71.8, 4.0
2019-02-14 00:59:29,714 : Text to Image: 18.98, 51.58, 68.76, 5.0
2019-02-14 00:59:30,115 : Image to text: 26.7, 58.6, 73.3, 4.0
2019-02-14 00:59:30,418 : Text to Image: 20.2, 51.78, 69.42, 5.0
2019-02-14 00:59:30,818 : Image to text: 27.1, 57.7, 74.0, 4.0
2019-02-14 00:59:31,121 : Text to Image: 19.2, 51.3, 69.02, 5.0
2019-02-14 00:59:31,532 : Image to text: 26.3, 58.1, 72.9, 4.0
2019-02-14 00:59:31,841 : Text to Image: 19.9, 51.88, 69.22, 5.0
2019-02-14 00:59:31,841 : Dev mean Text to Image: 19.556, 51.58, 69.05199999999999, 5.0
2019-02-14 00:59:31,841 : Dev mean Image to text: 25.839999999999996, 57.08, 72.6, 4.0
2019-02-14 00:59:31,841 : start epoch
2019-02-14 01:00:12,141 : samples : 64000
2019-02-14 01:00:22,366 : Image to text: 9.56, 28.28, 40.88, 16.0
2019-02-14 01:00:29,772 : Text to Image: 7.748, 23.88, 35.764, 20.0
2019-02-14 01:01:10,903 : samples : 128000
2019-02-14 01:01:21,263 : Image to text: 9.98, 28.08, 41.62, 15.0
2019-02-14 01:01:28,699 : Text to Image: 8.38, 24.78, 36.688, 20.0
2019-02-14 01:02:09,908 : samples : 192000
2019-02-14 01:02:20,279 : Image to text: 10.02, 28.84, 41.8, 16.0
2019-02-14 01:02:27,744 : Text to Image: 8.44, 25.312, 37.356, 19.0
2019-02-14 01:03:09,273 : samples : 256000
2019-02-14 01:03:19,565 : Image to text: 10.66, 29.96, 42.4, 15.0
2019-02-14 01:03:26,935 : Text to Image: 8.84, 26.068, 38.16, 18.0
2019-02-14 01:04:07,210 : samples : 320000
2019-02-14 01:04:17,497 : Image to text: 10.62, 28.92, 41.54, 16.0
2019-02-14 01:04:24,875 : Text to Image: 8.284, 24.908, 36.788, 19.0
2019-02-14 01:05:05,109 : samples : 384000
2019-02-14 01:05:15,380 : Image to text: 9.76, 28.38, 41.82, 15.0
2019-02-14 01:05:22,778 : Text to Image: 8.264, 24.76, 36.464, 19.0
2019-02-14 01:06:04,019 : samples : 448000
2019-02-14 01:06:14,294 : Image to text: 10.52, 29.08, 40.88, 15.0
2019-02-14 01:06:21,695 : Text to Image: 8.176, 24.836, 36.784, 19.0
2019-02-14 01:07:03,480 : samples : 512000
2019-02-14 01:07:13,715 : Image to text: 10.34, 28.94, 41.78, 15.0
2019-02-14 01:07:21,107 : Text to Image: 8.412, 25.156, 37.312, 19.0
2019-02-14 01:07:55,527 : Epoch 13 finished
2019-02-14 01:07:56,002 : Image to text: 25.1, 57.4, 72.5, 4.0
2019-02-14 01:07:56,343 : Text to Image: 22.32, 54.28, 71.18, 5.0
2019-02-14 01:07:56,815 : Image to text: 24.3, 58.7, 74.0, 4.0
2019-02-14 01:07:57,155 : Text to Image: 21.64, 54.32, 71.88, 5.0
2019-02-14 01:07:57,626 : Image to text: 26.6, 59.8, 74.7, 4.0
2019-02-14 01:07:57,966 : Text to Image: 21.38, 54.9, 71.7, 5.0
2019-02-14 01:07:58,441 : Image to text: 27.0, 61.6, 75.9, 3.0
2019-02-14 01:07:58,795 : Text to Image: 21.48, 53.84, 72.32, 5.0
2019-02-14 01:07:59,192 : Image to text: 28.0, 60.6, 75.0, 4.0
2019-02-14 01:07:59,493 : Text to Image: 21.94, 54.56, 71.36, 5.0
2019-02-14 01:07:59,493 : Dev mean Text to Image: 21.752000000000002, 54.38, 71.688, 5.0
2019-02-14 01:07:59,493 : Dev mean Image to text: 26.200000000000003, 59.620000000000005, 74.42, 3.8000000000000007
2019-02-14 01:07:59,493 : start epoch
2019-02-14 01:08:40,222 : samples : 64000
2019-02-14 01:08:50,431 : Image to text: 10.6, 28.98, 41.82, 15.0
2019-02-14 01:08:57,841 : Text to Image: 9.144, 26.08, 38.136, 18.0
2019-02-14 01:09:39,774 : samples : 128000
2019-02-14 01:09:50,031 : Image to text: 10.84, 28.88, 41.74, 15.0
2019-02-14 01:09:57,431 : Text to Image: 8.436, 25.42, 37.22, 19.0
2019-02-14 01:10:39,360 : samples : 192000
2019-02-14 01:10:49,617 : Image to text: 11.18, 29.86, 43.52, 14.0
2019-02-14 01:10:56,998 : Text to Image: 8.896, 25.784, 37.812, 19.0
2019-02-14 01:11:38,929 : samples : 256000
2019-02-14 01:11:49,289 : Image to text: 10.72, 30.36, 43.38, 14.0
2019-02-14 01:11:56,703 : Text to Image: 8.92, 26.244, 38.164, 18.0
2019-02-14 01:12:36,989 : samples : 320000
2019-02-14 01:12:47,402 : Image to text: 10.1, 28.88, 41.36, 15.0
2019-02-14 01:12:54,850 : Text to Image: 8.376, 25.036, 36.944, 19.0
2019-02-14 01:13:40,032 : samples : 384000
2019-02-14 01:13:50,284 : Image to text: 10.86, 28.98, 41.92, 15.0
2019-02-14 01:13:57,737 : Text to Image: 8.248, 25.148, 37.132, 19.0
2019-02-14 01:14:38,395 : samples : 448000
2019-02-14 01:14:48,656 : Image to text: 10.0, 29.3, 41.72, 15.0
2019-02-14 01:14:56,094 : Text to Image: 8.256, 25.164, 37.108, 19.0
2019-02-14 01:15:36,363 : samples : 512000
2019-02-14 01:15:46,624 : Image to text: 9.66, 29.0, 41.3, 15.0
2019-02-14 01:15:54,035 : Text to Image: 8.496, 25.44, 37.176, 19.0
2019-02-14 01:16:28,786 : Epoch 14 finished
2019-02-14 01:16:29,263 : Image to text: 25.7, 59.5, 74.6, 4.0
2019-02-14 01:16:29,603 : Text to Image: 21.94, 55.56, 72.1, 4.0
2019-02-14 01:16:30,084 : Image to text: 25.4, 60.3, 76.1, 4.0
2019-02-14 01:16:30,424 : Text to Image: 21.7, 54.62, 72.52, 5.0
2019-02-14 01:16:30,903 : Image to text: 26.3, 61.0, 74.0, 4.0
2019-02-14 01:16:31,243 : Text to Image: 21.82, 54.92, 71.24, 4.0
2019-02-14 01:16:31,717 : Image to text: 27.7, 64.2, 77.7, 3.0
2019-02-14 01:16:32,058 : Text to Image: 21.96, 56.14, 73.04, 4.0
2019-02-14 01:16:32,534 : Image to text: 30.3, 60.9, 74.3, 3.0
2019-02-14 01:16:32,874 : Text to Image: 22.4, 55.2, 72.08, 5.0
2019-02-14 01:16:32,874 : Dev mean Text to Image: 21.964, 55.288000000000004, 72.196, 4.4
2019-02-14 01:16:32,874 : Dev mean Image to text: 27.08, 61.18, 75.34, 3.6000000000000005
2019-02-14 01:16:32,875 : start epoch
2019-02-14 01:17:13,414 : samples : 64000
2019-02-14 01:17:23,610 : Image to text: 10.24, 30.04, 43.06, 15.0
2019-02-14 01:17:30,993 : Text to Image: 8.708, 26.028, 38.088, 18.0
2019-02-14 01:18:11,495 : samples : 128000
2019-02-14 01:18:21,730 : Image to text: 10.56, 30.5, 43.22, 14.0
2019-02-14 01:18:29,107 : Text to Image: 8.552, 25.804, 37.952, 18.0
2019-02-14 01:19:09,562 : samples : 192000
2019-02-14 01:19:19,826 : Image to text: 11.14, 29.6, 42.36, 15.0
2019-02-14 01:19:27,254 : Text to Image: 8.876, 26.068, 38.156, 18.0
2019-02-14 01:20:09,615 : samples : 256000
2019-02-14 01:20:19,860 : Image to text: 10.74, 28.92, 41.92, 15.0
2019-02-14 01:20:27,256 : Text to Image: 8.776, 25.66, 37.748, 19.0
2019-02-14 01:21:08,159 : samples : 320000
2019-02-14 01:21:18,468 : Image to text: 10.08, 28.08, 41.08, 15.0
2019-02-14 01:21:25,951 : Text to Image: 8.104, 24.708, 36.264, 20.0
2019-02-14 01:22:08,084 : samples : 384000
2019-02-14 01:22:18,370 : Image to text: 10.06, 28.96, 41.8, 15.0
2019-02-14 01:22:25,808 : Text to Image: 8.752, 25.996, 37.992, 18.0
2019-02-14 01:23:06,445 : samples : 448000
2019-02-14 01:23:16,717 : Image to text: 10.74, 29.38, 42.36, 15.0
2019-02-14 01:23:24,163 : Text to Image: 8.696, 26.016, 38.22, 18.0
2019-02-14 01:24:06,488 : samples : 512000
2019-02-14 01:24:16,726 : Image to text: 11.14, 29.54, 42.74, 15.0
2019-02-14 01:24:24,184 : Text to Image: 8.408, 25.252, 37.18, 19.0
2019-02-14 01:24:58,476 : Epoch 15 finished
2019-02-14 01:24:58,939 : Image to text: 27.5, 58.0, 74.5, 4.0
2019-02-14 01:24:59,280 : Text to Image: 22.44, 55.5, 72.42, 4.0
2019-02-14 01:24:59,751 : Image to text: 27.3, 58.4, 74.1, 4.0
2019-02-14 01:25:00,093 : Text to Image: 21.18, 54.12, 71.06, 5.0
2019-02-14 01:25:00,545 : Image to text: 27.3, 61.0, 75.0, 4.0
2019-02-14 01:25:00,887 : Text to Image: 21.22, 54.98, 71.4, 5.0
2019-02-14 01:25:01,358 : Image to text: 28.2, 62.9, 78.7, 3.0
2019-02-14 01:25:01,699 : Text to Image: 22.0, 54.24, 71.92, 5.0
2019-02-14 01:25:02,165 : Image to text: 27.9, 60.3, 74.8, 4.0
2019-02-14 01:25:02,507 : Text to Image: 22.46, 55.3, 71.34, 4.0
2019-02-14 01:25:02,507 : Dev mean Text to Image: 21.860000000000003, 54.828, 71.628, 4.6
2019-02-14 01:25:02,507 : Dev mean Image to text: 27.64, 60.120000000000005, 75.42, 3.8000000000000007
2019-02-14 01:25:02,507 : start epoch
2019-02-14 01:25:42,806 : samples : 64000
2019-02-14 01:25:53,087 : Image to text: 9.6, 28.18, 41.32, 16.0
2019-02-14 01:26:00,535 : Text to Image: 8.1, 24.18, 35.832, 20.0
2019-02-14 01:26:45,534 : samples : 128000
2019-02-14 01:26:55,782 : Image to text: 10.28, 30.0, 42.22, 15.0
2019-02-14 01:27:03,201 : Text to Image: 8.792, 25.772, 38.088, 18.0
2019-02-14 01:27:45,767 : samples : 192000
2019-02-14 01:27:55,988 : Image to text: 9.92, 28.6, 41.58, 16.0
2019-02-14 01:28:03,400 : Text to Image: 8.44, 25.208, 37.076, 19.0
2019-02-14 01:28:43,832 : samples : 256000
2019-02-14 01:28:54,033 : Image to text: 10.4, 29.94, 43.34, 14.0
2019-02-14 01:29:01,460 : Text to Image: 9.184, 26.164, 38.42, 18.0
2019-02-14 01:29:42,805 : samples : 320000
2019-02-14 01:29:53,024 : Image to text: 10.24, 29.28, 41.44, 16.0
2019-02-14 01:30:00,472 : Text to Image: 8.048, 24.672, 36.856, 19.0
2019-02-14 01:30:43,234 : samples : 384000
2019-02-14 01:30:53,412 : Image to text: 10.34, 29.1, 42.0, 15.0
2019-02-14 01:31:00,900 : Text to Image: 8.6, 25.48, 37.56, 19.0
2019-02-14 01:31:43,537 : samples : 448000
2019-02-14 01:31:53,701 : Image to text: 10.78, 29.62, 43.06, 15.0
2019-02-14 01:32:01,130 : Text to Image: 8.82, 25.792, 37.464, 19.0
2019-02-14 01:32:44,215 : samples : 512000
2019-02-14 01:32:54,372 : Image to text: 10.82, 30.38, 42.28, 15.0
2019-02-14 01:33:01,789 : Text to Image: 8.704, 26.084, 38.1, 18.0
2019-02-14 01:33:38,804 : Epoch 16 finished
2019-02-14 01:33:39,272 : Image to text: 28.1, 58.4, 74.7, 4.0
2019-02-14 01:33:39,614 : Text to Image: 22.46, 55.68, 72.4, 4.0
2019-02-14 01:33:40,085 : Image to text: 24.0, 59.3, 74.6, 4.0
2019-02-14 01:33:40,426 : Text to Image: 22.18, 56.52, 73.18, 4.0
2019-02-14 01:33:40,895 : Image to text: 28.7, 61.1, 76.2, 4.0
2019-02-14 01:33:41,237 : Text to Image: 22.52, 56.74, 72.32, 4.0
2019-02-14 01:33:41,709 : Image to text: 28.4, 65.0, 79.0, 3.0
2019-02-14 01:33:42,050 : Text to Image: 22.9, 56.14, 74.04, 4.0
2019-02-14 01:33:42,515 : Image to text: 28.7, 61.5, 75.2, 3.0
2019-02-14 01:33:42,857 : Text to Image: 23.64, 55.52, 72.4, 4.0
2019-02-14 01:33:42,857 : Dev mean Text to Image: 22.740000000000002, 56.12, 72.868, 4.0
2019-02-14 01:33:42,857 : Dev mean Image to text: 27.58, 61.06, 75.94000000000001, 3.6000000000000005
2019-02-14 01:33:42,857 : start epoch
2019-02-14 01:34:25,543 : samples : 64000
2019-02-14 01:34:35,932 : Image to text: 10.36, 27.92, 40.8, 16.0
2019-02-14 01:34:43,304 : Text to Image: 8.068, 24.928, 36.668, 20.0
2019-02-14 01:35:25,879 : samples : 128000
2019-02-14 01:35:36,019 : Image to text: 11.04, 30.18, 43.28, 14.0
2019-02-14 01:35:43,445 : Text to Image: 9.04, 26.5, 38.584, 18.0
2019-02-14 01:36:27,548 : samples : 192000
2019-02-14 01:36:37,659 : Image to text: 11.4, 29.94, 42.92, 15.0
2019-02-14 01:36:45,047 : Text to Image: 9.092, 26.516, 38.632, 18.0
2019-02-14 01:37:27,539 : samples : 256000
2019-02-14 01:37:37,650 : Image to text: 10.78, 31.38, 43.7, 14.0
2019-02-14 01:37:45,078 : Text to Image: 9.256, 26.336, 38.528, 18.0
2019-02-14 01:38:26,419 : samples : 320000
2019-02-14 01:38:36,530 : Image to text: 11.02, 29.86, 42.3, 14.0
2019-02-14 01:38:43,921 : Text to Image: 8.88, 25.944, 37.848, 18.0
2019-02-14 01:39:24,760 : samples : 384000
2019-02-14 01:39:34,891 : Image to text: 11.28, 29.54, 42.4, 15.0
2019-02-14 01:39:42,285 : Text to Image: 8.428, 25.456, 37.524, 19.0
2019-02-14 01:40:22,618 : samples : 448000
2019-02-14 01:40:32,700 : Image to text: 10.7, 29.68, 42.56, 15.0
2019-02-14 01:40:40,058 : Text to Image: 8.552, 25.608, 37.332, 18.0
2019-02-14 01:41:21,599 : samples : 512000
2019-02-14 01:41:31,685 : Image to text: 10.7, 28.92, 41.88, 15.0
2019-02-14 01:41:39,064 : Text to Image: 9.052, 26.352, 38.38, 18.0
2019-02-14 01:42:14,746 : Epoch 17 finished
2019-02-14 01:42:15,184 : Image to text: 26.0, 59.5, 74.2, 4.0
2019-02-14 01:42:15,514 : Text to Image: 22.36, 55.8, 72.94, 4.0
2019-02-14 01:42:15,943 : Image to text: 27.3, 59.2, 73.2, 4.0
2019-02-14 01:42:16,273 : Text to Image: 22.08, 55.34, 72.4, 5.0
2019-02-14 01:42:16,765 : Image to text: 27.2, 60.9, 75.9, 3.0
2019-02-14 01:42:17,105 : Text to Image: 22.58, 56.06, 72.82, 4.0
2019-02-14 01:42:17,573 : Image to text: 29.1, 61.9, 76.3, 3.0
2019-02-14 01:42:17,913 : Text to Image: 23.02, 55.28, 73.04, 4.0
2019-02-14 01:42:18,387 : Image to text: 27.9, 61.2, 74.6, 3.0
2019-02-14 01:42:18,726 : Text to Image: 23.0, 56.0, 72.8, 4.0
2019-02-14 01:42:18,726 : Dev mean Text to Image: 22.607999999999997, 55.696, 72.8, 4.2
2019-02-14 01:42:18,726 : Dev mean Image to text: 27.5, 60.54, 74.84, 3.4000000000000004
2019-02-14 01:42:22,784 : 
Test scores | Image to text:             27.080000000000002, 59.5, 74.96, 3.8
2019-02-14 01:42:22,784 : Test scores | Text to image:             22.336, 55.135999999999996, 72.136, 4.6

2019-02-14 01:42:22,900 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 01:42:23,110 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 01:42:23,729 : loading BERT model bert-base-uncased
2019-02-14 01:42:23,729 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:42:23,760 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:42:23,761 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgvx1hgqr
2019-02-14 01:42:26,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:42:27,650 : Computing embeddings for train/dev/test
2019-02-14 01:44:01,445 : Computed embeddings
2019-02-14 01:44:01,445 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:44:25,489 : [('reg:1e-05', 88.08), ('reg:0.0001', 87.93), ('reg:0.001', 87.15), ('reg:0.01', 86.74)]
2019-02-14 01:44:25,489 : Validation : best param found is reg = 1e-05 with score             88.08
2019-02-14 01:44:25,489 : Evaluating...
2019-02-14 01:44:30,293 : 
Dev acc : 88.1 Test acc : 88.7 for LENGTH classification

2019-02-14 01:44:30,293 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 01:44:30,650 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 01:44:30,700 : loading BERT model bert-base-uncased
2019-02-14 01:44:30,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:44:30,809 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:44:30,809 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj3rhp9c8
2019-02-14 01:44:33,252 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:44:34,725 : Computing embeddings for train/dev/test
2019-02-14 01:46:03,710 : Computed embeddings
2019-02-14 01:46:03,710 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:46:43,928 : [('reg:1e-05', 1.75), ('reg:0.0001', 0.63), ('reg:0.001', 0.16), ('reg:0.01', 0.13)]
2019-02-14 01:46:43,929 : Validation : best param found is reg = 1e-05 with score             1.75
2019-02-14 01:46:43,929 : Evaluating...
2019-02-14 01:46:58,162 : 
Dev acc : 1.8 Test acc : 1.7 for WORDCONTENT classification

2019-02-14 01:46:58,164 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 01:46:58,722 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 01:46:58,792 : loading BERT model bert-base-uncased
2019-02-14 01:46:58,792 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:46:58,820 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:46:58,821 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd0n5ojbl
2019-02-14 01:47:01,273 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:47:02,723 : Computing embeddings for train/dev/test
2019-02-14 01:48:27,517 : Computed embeddings
2019-02-14 01:48:27,517 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:48:57,785 : [('reg:1e-05', 30.8), ('reg:0.0001', 25.22), ('reg:0.001', 27.18), ('reg:0.01', 23.97)]
2019-02-14 01:48:57,785 : Validation : best param found is reg = 1e-05 with score             30.8
2019-02-14 01:48:57,785 : Evaluating...
2019-02-14 01:49:05,818 : 
Dev acc : 30.8 Test acc : 31.1 for DEPTH classification

2019-02-14 01:49:05,819 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 01:49:06,423 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 01:49:06,491 : loading BERT model bert-base-uncased
2019-02-14 01:49:06,491 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:49:06,521 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:49:06,521 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptmy809ww
2019-02-14 01:49:08,963 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:49:10,438 : Computing embeddings for train/dev/test
2019-02-14 01:50:29,640 : Computed embeddings
2019-02-14 01:50:29,640 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:51:05,230 : [('reg:1e-05', 49.36), ('reg:0.0001', 46.17), ('reg:0.001', 32.91), ('reg:0.01', 17.43)]
2019-02-14 01:51:05,231 : Validation : best param found is reg = 1e-05 with score             49.36
2019-02-14 01:51:05,231 : Evaluating...
2019-02-14 01:51:15,611 : 
Dev acc : 49.4 Test acc : 49.4 for TOPCONSTITUENTS classification

2019-02-14 01:51:15,612 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 01:51:16,019 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 01:51:16,089 : loading BERT model bert-base-uncased
2019-02-14 01:51:16,089 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:51:16,124 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:51:16,124 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvu6p1d8o
2019-02-14 01:51:18,575 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:51:20,034 : Computing embeddings for train/dev/test
2019-02-14 01:52:44,620 : Computed embeddings
2019-02-14 01:52:44,620 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:53:08,032 : [('reg:1e-05', 50.0), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-14 01:53:08,032 : Validation : best param found is reg = 1e-05 with score             50.0
2019-02-14 01:53:08,032 : Evaluating...
2019-02-14 01:53:14,032 : 
Dev acc : 50.0 Test acc : 50.0 for BIGRAMSHIFT classification

2019-02-14 01:53:14,033 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 01:53:14,418 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 01:53:14,484 : loading BERT model bert-base-uncased
2019-02-14 01:53:14,484 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:53:14,600 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:53:14,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2fiyblso
2019-02-14 01:53:17,041 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:53:18,463 : Computing embeddings for train/dev/test
2019-02-14 01:54:41,731 : Computed embeddings
2019-02-14 01:54:41,731 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:55:15,672 : [('reg:1e-05', 84.33), ('reg:0.0001', 83.84), ('reg:0.001', 83.19), ('reg:0.01', 75.26)]
2019-02-14 01:55:15,672 : Validation : best param found is reg = 1e-05 with score             84.33
2019-02-14 01:55:15,672 : Evaluating...
2019-02-14 01:55:25,299 : 
Dev acc : 84.3 Test acc : 83.0 for TENSE classification

2019-02-14 01:55:25,300 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 01:55:25,885 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 01:55:25,949 : loading BERT model bert-base-uncased
2019-02-14 01:55:25,949 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:55:25,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:55:25,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpixigd421
2019-02-14 01:55:28,423 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:55:29,847 : Computing embeddings for train/dev/test
2019-02-14 01:56:57,403 : Computed embeddings
2019-02-14 01:56:57,403 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:57:38,187 : [('reg:1e-05', 74.64), ('reg:0.0001', 74.41), ('reg:0.001', 69.05), ('reg:0.01', 54.02)]
2019-02-14 01:57:38,187 : Validation : best param found is reg = 1e-05 with score             74.64
2019-02-14 01:57:38,187 : Evaluating...
2019-02-14 01:57:48,738 : 
Dev acc : 74.6 Test acc : 73.1 for SUBJNUMBER classification

2019-02-14 01:57:48,739 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 01:57:49,175 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 01:57:49,246 : loading BERT model bert-base-uncased
2019-02-14 01:57:49,247 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:57:49,378 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:57:49,378 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqw3zc8kz
2019-02-14 01:57:51,858 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 01:57:53,313 : Computing embeddings for train/dev/test
2019-02-14 01:59:19,138 : Computed embeddings
2019-02-14 01:59:19,138 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 01:59:50,180 : [('reg:1e-05', 69.89), ('reg:0.0001', 68.15), ('reg:0.001', 56.77), ('reg:0.01', 64.12)]
2019-02-14 01:59:50,180 : Validation : best param found is reg = 1e-05 with score             69.89
2019-02-14 01:59:50,180 : Evaluating...
2019-02-14 01:59:57,302 : 
Dev acc : 69.9 Test acc : 71.3 for OBJNUMBER classification

2019-02-14 01:59:57,304 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 01:59:57,920 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 01:59:57,993 : loading BERT model bert-base-uncased
2019-02-14 01:59:57,994 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 01:59:58,027 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 01:59:58,027 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpglyeytzr
2019-02-14 02:00:00,480 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:00:01,943 : Computing embeddings for train/dev/test
2019-02-14 02:01:40,712 : Computed embeddings
2019-02-14 02:01:40,712 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:02:12,875 : [('reg:1e-05', 51.84), ('reg:0.0001', 51.59), ('reg:0.001', 51.02), ('reg:0.01', 50.55)]
2019-02-14 02:02:12,875 : Validation : best param found is reg = 1e-05 with score             51.84
2019-02-14 02:02:12,875 : Evaluating...
2019-02-14 02:02:21,494 : 
Dev acc : 51.8 Test acc : 51.9 for ODDMANOUT classification

2019-02-14 02:02:21,495 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 02:02:22,132 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 02:02:22,211 : loading BERT model bert-base-uncased
2019-02-14 02:02:22,211 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:02:22,245 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:02:22,245 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpy5_d44wi
2019-02-14 02:02:24,688 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:02:26,108 : Computing embeddings for train/dev/test
2019-02-14 02:04:04,652 : Computed embeddings
2019-02-14 02:04:04,652 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:04:33,679 : [('reg:1e-05', 56.86), ('reg:0.0001', 56.7), ('reg:0.001', 55.44), ('reg:0.01', 50.92)]
2019-02-14 02:04:33,679 : Validation : best param found is reg = 1e-05 with score             56.86
2019-02-14 02:04:33,679 : Evaluating...
2019-02-14 02:04:40,004 : 
Dev acc : 56.9 Test acc : 56.7 for COORDINATIONINVERSION classification

2019-02-14 02:04:40,006 : total results: {'STS12': {'MSRpar': {'pearson': (0.11150196319575643, 0.0022277906226877736), 'spearman': SpearmanrResult(correlation=0.13849549248077525, pvalue=0.00014187323064618262), 'nsamples': 750}, 'MSRvid': {'pearson': (0.1303537077374942, 0.0003446327970319405), 'spearman': SpearmanrResult(correlation=0.14669156333694566, pvalue=5.519921099448956e-05), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.3526110304605168, 6.955127645301385e-15), 'spearman': SpearmanrResult(correlation=0.448208786175153, pvalue=4.59240086937054e-24), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.3726036949572619, 4.0988243381921433e-26), 'spearman': SpearmanrResult(correlation=0.3949277567817086, pvalue=2.1061699605997542e-29), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5043516808588757, 3.910404613625233e-27), 'spearman': SpearmanrResult(correlation=0.44297442020055394, pvalue=1.3200679711935579e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.29428441544198103, 'wmean': 0.2650995199684534}, 'spearman': {'mean': 0.31425960379502726, 'wmean': 0.2871817039781172}}}, 'STS13': {'FNWN': {'pearson': (0.17176282775714688, 0.018114262540188792), 'spearman': SpearmanrResult(correlation=0.17096572774849347, pvalue=0.018665639394484804), 'nsamples': 189}, 'headlines': {'pearson': (0.11522529584442581, 0.0015731219596549157), 'spearman': SpearmanrResult(correlation=0.1413403803592641, pvalue=0.00010282685931580446), 'nsamples': 750}, 'OnWN': {'pearson': (-0.07532543626990736, 0.07464005949601844), 'spearman': SpearmanrResult(correlation=-0.02367053358226946, pvalue=0.5758376805506601), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.0705542291105551, 'wmean': 0.05108305105466807}, 'spearman': {'mean': 0.09621185817516271, 'wmean': 0.08335909231617346}}}, 'STS14': {'deft-forum': {'pearson': (-0.09759643926537727, 0.03849754303848418), 'spearman': SpearmanrResult(correlation=-0.06866195018349795, pvalue=0.14588936182703335), 'nsamples': 450}, 'deft-news': {'pearson': (0.11878921643929986, 0.039764592256402546), 'spearman': SpearmanrResult(correlation=0.1287422313326558, pvalue=0.025756790440117912), 'nsamples': 300}, 'headlines': {'pearson': (0.1616081439621139, 8.679168685258503e-06), 'spearman': SpearmanrResult(correlation=0.16355825369363644, pvalue=6.72830337908402e-06), 'nsamples': 750}, 'images': {'pearson': (0.14349290346516003, 8.02722478850614e-05), 'spearman': SpearmanrResult(correlation=0.18226567748759928, pvalue=5.02440030675557e-07), 'nsamples': 750}, 'OnWN': {'pearson': (0.05792955698015941, 0.11293192975705653), 'spearman': SpearmanrResult(correlation=0.11780560017207385, pvalue=0.0012287587793780017), 'nsamples': 750}, 'tweet-news': {'pearson': (0.2077829678875792, 9.264079361033445e-09), 'spearman': SpearmanrResult(correlation=0.22258314283783365, pvalue=7.138004994673022e-10), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.09866772491148917, 'wmean': 0.11195427906230122}, 'spearman': {'mean': 0.12438215922338351, 'wmean': 0.13930247932282133}}}, 'STS15': {'answers-forums': {'pearson': (0.019578491293825728, 0.7055001643346958), 'spearman': SpearmanrResult(correlation=-0.00047596915249188774, pvalue=0.9926704701296668), 'nsamples': 375}, 'answers-students': {'pearson': (0.29386521160589246, 2.0952517541475719e-16), 'spearman': SpearmanrResult(correlation=0.2846687481959845, pvalue=1.8956343723004387e-15), 'nsamples': 750}, 'belief': {'pearson': (0.061065750568676594, 0.23812490441745343), 'spearman': SpearmanrResult(correlation=0.04894784921335373, pvalue=0.3445193597217321), 'nsamples': 375}, 'headlines': {'pearson': (0.25320355087112123, 1.954433472904783e-12), 'spearman': SpearmanrResult(correlation=0.2803360061130528, pvalue=5.203776382163088e-15), 'nsamples': 750}, 'images': {'pearson': (0.1223418259442861, 0.0007865058706202969), 'spearman': SpearmanrResult(correlation=0.16740440263860773, pvalue=4.036822598825289e-06), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.15001096605676043, 'wmean': 0.17743317733813774}, 'spearman': {'mean': 0.15617620740170138, 'wmean': 0.18916127424451898}}}, 'STS16': {'answer-answer': {'pearson': (0.22115483425931046, 0.0003833083526658623), 'spearman': SpearmanrResult(correlation=0.23693724302298808, pvalue=0.0001378725440905739), 'nsamples': 254}, 'headlines': {'pearson': (0.31530035112801746, 3.7643584620268786e-07), 'spearman': SpearmanrResult(correlation=0.33322100478351724, pvalue=7.187963613753647e-08), 'nsamples': 249}, 'plagiarism': {'pearson': (0.1316272321290531, 0.046148364235548536), 'spearman': SpearmanrResult(correlation=0.1420366453152499, pvalue=0.03129629704072796), 'nsamples': 230}, 'postediting': {'pearson': (0.44268274378571365, 3.931360141991781e-13), 'spearman': SpearmanrResult(correlation=0.49858865742522285, pvalue=9.720423060280128e-17), 'nsamples': 244}, 'question-question': {'pearson': (-0.03001175346189555, 0.6661993847618783), 'spearman': SpearmanrResult(correlation=-0.019154009136549445, pvalue=0.7831081647814933), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.21615068156803985, 'wmean': 0.22487311275936034}, 'spearman': {'mean': 0.23832590828208572, 'wmean': 0.24744937845165071}}}, 'MR': {'devacc': 62.93, 'acc': 61.05, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 66.67, 'acc': 66.34, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.74, 'acc': 81.13, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 88.68, 'acc': 88.8, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 74.54, 'acc': 75.29, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 34.7, 'acc': 35.34, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 51.38, 'acc': 69.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.31, 'acc': 66.84, 'f1': 74.46, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.4, 'acc': 68.48, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6325980508115783, 'pearson': 0.6648471955075063, 'spearman': 0.6058460661491777, 'mse': 0.5702086083522679, 'yhat': array([2.56269325, 3.86201562, 2.22453873, ..., 2.93733231, 4.17136139,        4.33068526]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.46426771350960594, 'pearson': 0.46085922696707404, 'spearman': 0.44907630475641513, 'mse': 2.051803230704794, 'yhat': array([2.51506517, 1.94014322, 2.45855711, ..., 3.81190957, 3.57487122,        3.29198756]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 59.32, 'acc': 59.16, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 316.308, 'acc': [(27.080000000000002, 59.5, 74.96, 3.8), (22.336, 55.135999999999996, 72.136, 4.6)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 88.08, 'acc': 88.65, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 1.75, 'acc': 1.69, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 30.8, 'acc': 31.07, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 49.36, 'acc': 49.37, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 50.0, 'acc': 50.0, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 84.33, 'acc': 83.05, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 74.64, 'acc': 73.12, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 69.89, 'acc': 71.3, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 51.84, 'acc': 51.85, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.86, 'acc': 56.71, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 02:04:40,006 : STS12 p=0.2651, STS12 s=0.2872, STS13 p=0.0511, STS13 s=0.0834, STS14 p=0.1120, STS14 s=0.1393, STS15 p=0.1774, STS15 s=0.1892, STS 16 p=0.2249, STS16 s=0.2474, STS B p=0.4609, STS B s=0.4491, STS B m=2.0518, SICK-R p=0.6648, SICK-R s=0.6058, SICK-P m=0.5702
2019-02-14 02:04:40,006 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 02:04:40,006 : 0.2651,0.2872,0.0511,0.0834,0.1120,0.1393,0.1774,0.1892,0.2249,0.2474,0.4609,0.4491,2.0518,0.6648,0.6058,0.5702
2019-02-14 02:04:40,006 : MR=61.05, CR=66.34, SUBJ=88.80, MPQA=81.13, SST-B=75.29, SST-F=35.34, TREC=69.60, SICK-E=68.48, SNLI=59.16, MRPC=66.84, MRPC f=74.46
2019-02-14 02:04:40,007 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 02:04:40,007 : 61.05,66.34,88.80,81.13,75.29,35.34,69.60,68.48,59.16,66.84,74.46
2019-02-14 02:04:40,007 : COCO r1i2t=27.08, COCO r5i2t=59.50, COCO r10i2t=74.96, COCO medr_i2t=3.80, COCO r1t2i=22.34, COCO r5t2i=55.14, COCO r10t2i=72.14, COCO medr_t2i=4.60
2019-02-14 02:04:40,007 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 02:04:40,007 : 27.08,59.50,74.96,3.80,22.34,55.14,72.14,4.60
2019-02-14 02:04:40,007 : SentLen=88.65, WC=1.69, TreeDepth=31.07, TopConst=49.37, BShift=50.00, Tense=83.05, SubjNum=73.12, ObjNum=71.30, SOMO=51.85, CoordInv=56.71, average=55.68
2019-02-14 02:04:40,007 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 02:04:40,007 : 88.65,1.69,31.07,49.37,50.00,83.05,73.12,71.30,51.85,56.71,55.68
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,007 : layer 3
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,007 : ********************************************************************************
2019-02-14 02:04:40,092 : ***** Transfer task : STS12 *****


2019-02-14 02:04:40,104 : loading BERT model bert-base-uncased
2019-02-14 02:04:40,105 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:04:40,122 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:04:40,122 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjnv_dve6
2019-02-14 02:04:42,564 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:04:45,740 : MSRpar : pearson = 0.1826, spearman = 0.2071
2019-02-14 02:04:46,497 : MSRvid : pearson = 0.2949, spearman = 0.2993
2019-02-14 02:04:47,141 : SMTeuroparl : pearson = 0.4516, spearman = 0.5590
2019-02-14 02:04:48,299 : surprise.OnWN : pearson = 0.3467, spearman = 0.3384
2019-02-14 02:04:48,943 : surprise.SMTnews : pearson = 0.6151, spearman = 0.4877
2019-02-14 02:04:48,943 : ALL (weighted average) : Pearson = 0.3446,             Spearman = 0.3491
2019-02-14 02:04:48,943 : ALL (average) : Pearson = 0.3782,             Spearman = 0.3783

2019-02-14 02:04:48,943 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 02:04:48,953 : loading BERT model bert-base-uncased
2019-02-14 02:04:48,953 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:04:48,971 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:04:48,971 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgwvfq4u0
2019-02-14 02:04:51,402 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:04:53,430 : FNWN : pearson = 0.1930, spearman = 0.1881
2019-02-14 02:04:54,314 : headlines : pearson = 0.1940, spearman = 0.2387
2019-02-14 02:04:54,996 : OnWN : pearson = 0.1886, spearman = 0.2066
2019-02-14 02:04:54,996 : ALL (weighted average) : Pearson = 0.1919,             Spearman = 0.2204
2019-02-14 02:04:54,996 : ALL (average) : Pearson = 0.1919,             Spearman = 0.2112

2019-02-14 02:04:54,996 : ***** Transfer task : STS14 *****


2019-02-14 02:04:55,012 : loading BERT model bert-base-uncased
2019-02-14 02:04:55,013 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:04:55,032 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:04:55,032 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt4qqemq7
2019-02-14 02:04:57,475 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:04:59,577 : deft-forum : pearson = 0.1018, spearman = 0.1513
2019-02-14 02:05:00,259 : deft-news : pearson = 0.3486, spearman = 0.3778
2019-02-14 02:05:01,267 : headlines : pearson = 0.2547, spearman = 0.2588
2019-02-14 02:05:02,246 : images : pearson = 0.3088, spearman = 0.3315
2019-02-14 02:05:03,206 : OnWN : pearson = 0.2449, spearman = 0.2699
2019-02-14 02:05:04,441 : tweet-news : pearson = 0.3060, spearman = 0.3296
2019-02-14 02:05:04,442 : ALL (weighted average) : Pearson = 0.2630,             Spearman = 0.2863
2019-02-14 02:05:04,442 : ALL (average) : Pearson = 0.2608,             Spearman = 0.2865

2019-02-14 02:05:04,442 : ***** Transfer task : STS15 *****


2019-02-14 02:05:04,476 : loading BERT model bert-base-uncased
2019-02-14 02:05:04,476 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:05:04,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:05:04,496 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz55y3kfc
2019-02-14 02:05:06,935 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:05:09,248 : answers-forums : pearson = 0.3043, spearman = 0.3039
2019-02-14 02:05:10,169 : answers-students : pearson = 0.3848, spearman = 0.3842
2019-02-14 02:05:11,041 : belief : pearson = 0.3545, spearman = 0.4050
2019-02-14 02:05:12,073 : headlines : pearson = 0.3294, spearman = 0.3709
2019-02-14 02:05:13,046 : images : pearson = 0.1900, spearman = 0.3371
2019-02-14 02:05:13,047 : ALL (weighted average) : Pearson = 0.3084,             Spearman = 0.3616
2019-02-14 02:05:13,047 : ALL (average) : Pearson = 0.3126,             Spearman = 0.3602

2019-02-14 02:05:13,047 : ***** Transfer task : STS16 *****


2019-02-14 02:05:13,123 : loading BERT model bert-base-uncased
2019-02-14 02:05:13,123 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:05:13,142 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:05:13,142 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr8yu_yl5
2019-02-14 02:05:15,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:05:17,443 : answer-answer : pearson = 0.2995, spearman = 0.3600
2019-02-14 02:05:17,751 : headlines : pearson = 0.3621, spearman = 0.4215
2019-02-14 02:05:18,146 : plagiarism : pearson = 0.3511, spearman = 0.4213
2019-02-14 02:05:18,787 : postediting : pearson = 0.5665, spearman = 0.7032
2019-02-14 02:05:19,074 : question-question : pearson = 0.2301, spearman = 0.2517
2019-02-14 02:05:19,074 : ALL (weighted average) : Pearson = 0.3653,             Spearman = 0.4363
2019-02-14 02:05:19,074 : ALL (average) : Pearson = 0.3618,             Spearman = 0.4315

2019-02-14 02:05:19,074 : ***** Transfer task : MR *****


2019-02-14 02:05:19,094 : loading BERT model bert-base-uncased
2019-02-14 02:05:19,094 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:05:19,114 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:05:19,115 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpapo5rshp
2019-02-14 02:05:21,575 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:05:23,077 : Generating sentence embeddings
2019-02-14 02:05:37,029 : Generated sentence embeddings
2019-02-14 02:05:37,030 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 02:05:47,532 : Best param found at split 1: l2reg = 1e-05                 with score 68.42
2019-02-14 02:05:59,926 : Best param found at split 2: l2reg = 1e-05                 with score 67.16
2019-02-14 02:06:12,094 : Best param found at split 3: l2reg = 1e-05                 with score 68.21
2019-02-14 02:06:23,890 : Best param found at split 4: l2reg = 1e-05                 with score 67.5
2019-02-14 02:06:34,968 : Best param found at split 5: l2reg = 1e-05                 with score 68.53
2019-02-14 02:06:35,606 : Dev acc : 67.96 Test acc : 68.81

2019-02-14 02:06:35,607 : ***** Transfer task : CR *****


2019-02-14 02:06:35,614 : loading BERT model bert-base-uncased
2019-02-14 02:06:35,614 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:06:35,634 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:06:35,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmx0vzbjc
2019-02-14 02:06:38,077 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:06:39,505 : Generating sentence embeddings
2019-02-14 02:06:43,471 : Generated sentence embeddings
2019-02-14 02:06:43,472 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 02:06:47,080 : Best param found at split 1: l2reg = 1e-05                 with score 69.66
2019-02-14 02:06:50,866 : Best param found at split 2: l2reg = 1e-05                 with score 65.58
2019-02-14 02:06:54,923 : Best param found at split 3: l2reg = 1e-05                 with score 69.27
2019-02-14 02:06:59,065 : Best param found at split 4: l2reg = 1e-05                 with score 66.14
2019-02-14 02:07:02,524 : Best param found at split 5: l2reg = 1e-05                 with score 67.69
2019-02-14 02:07:02,785 : Dev acc : 67.67 Test acc : 68.13

2019-02-14 02:07:02,785 : ***** Transfer task : MPQA *****


2019-02-14 02:07:02,791 : loading BERT model bert-base-uncased
2019-02-14 02:07:02,791 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:07:02,811 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:07:02,812 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6suq6fy0
2019-02-14 02:07:05,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:07:06,755 : Generating sentence embeddings
2019-02-14 02:07:10,487 : Generated sentence embeddings
2019-02-14 02:07:10,488 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 02:07:21,104 : Best param found at split 1: l2reg = 1e-05                 with score 79.88
2019-02-14 02:07:32,204 : Best param found at split 2: l2reg = 1e-05                 with score 78.21
2019-02-14 02:07:45,156 : Best param found at split 3: l2reg = 1e-05                 with score 79.89
2019-02-14 02:07:57,119 : Best param found at split 4: l2reg = 1e-05                 with score 81.24
2019-02-14 02:08:11,808 : Best param found at split 5: l2reg = 0.0001                 with score 80.33
2019-02-14 02:08:12,424 : Dev acc : 79.91 Test acc : 81.62

2019-02-14 02:08:12,425 : ***** Transfer task : SUBJ *****


2019-02-14 02:08:12,443 : loading BERT model bert-base-uncased
2019-02-14 02:08:12,444 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:08:12,463 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:08:12,464 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7u3dmmy7
2019-02-14 02:08:14,914 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:08:16,396 : Generating sentence embeddings
2019-02-14 02:08:29,679 : Generated sentence embeddings
2019-02-14 02:08:29,679 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 02:08:40,578 : Best param found at split 1: l2reg = 1e-05                 with score 89.55
2019-02-14 02:08:53,043 : Best param found at split 2: l2reg = 1e-05                 with score 90.12
2019-02-14 02:09:05,803 : Best param found at split 3: l2reg = 0.0001                 with score 89.9
2019-02-14 02:09:19,420 : Best param found at split 4: l2reg = 1e-05                 with score 90.2
2019-02-14 02:09:33,401 : Best param found at split 5: l2reg = 1e-05                 with score 89.57
2019-02-14 02:09:34,203 : Dev acc : 89.87 Test acc : 89.7

2019-02-14 02:09:34,204 : ***** Transfer task : SST Binary classification *****


2019-02-14 02:09:34,335 : loading BERT model bert-base-uncased
2019-02-14 02:09:34,335 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:09:34,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:09:34,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqfa7pnc1
2019-02-14 02:09:36,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:09:38,216 : Computing embedding for train
2019-02-14 02:10:22,660 : Computed train embeddings
2019-02-14 02:10:22,660 : Computing embedding for dev
2019-02-14 02:10:23,601 : Computed dev embeddings
2019-02-14 02:10:23,601 : Computing embedding for test
2019-02-14 02:10:25,660 : Computed test embeddings
2019-02-14 02:10:25,660 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:10:44,940 : [('reg:1e-05', 74.31), ('reg:0.0001', 73.51), ('reg:0.001', 71.56), ('reg:0.01', 65.48)]
2019-02-14 02:10:44,941 : Validation : best param found is reg = 1e-05 with score             74.31
2019-02-14 02:10:44,941 : Evaluating...
2019-02-14 02:10:49,330 : 
Dev acc : 74.31 Test acc : 75.67 for             SST Binary classification

2019-02-14 02:10:49,330 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 02:10:49,384 : loading BERT model bert-base-uncased
2019-02-14 02:10:49,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:10:49,410 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:10:49,410 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmwetnrx4
2019-02-14 02:10:51,862 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:10:53,344 : Computing embedding for train
2019-02-14 02:11:02,836 : Computed train embeddings
2019-02-14 02:11:02,836 : Computing embedding for dev
2019-02-14 02:11:04,064 : Computed dev embeddings
2019-02-14 02:11:04,064 : Computing embedding for test
2019-02-14 02:11:06,505 : Computed test embeddings
2019-02-14 02:11:06,505 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:11:09,145 : [('reg:1e-05', 36.6), ('reg:0.0001', 36.06), ('reg:0.001', 34.24), ('reg:0.01', 29.43)]
2019-02-14 02:11:09,145 : Validation : best param found is reg = 1e-05 with score             36.6
2019-02-14 02:11:09,145 : Evaluating...
2019-02-14 02:11:09,790 : 
Dev acc : 36.6 Test acc : 38.78 for             SST Fine-Grained classification

2019-02-14 02:11:09,790 : ***** Transfer task : TREC *****


2019-02-14 02:11:09,805 : loading BERT model bert-base-uncased
2019-02-14 02:11:09,805 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:11:09,827 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:11:09,827 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6gfmdcu2
2019-02-14 02:11:12,267 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:11:17,539 : Computed train embeddings
2019-02-14 02:11:17,834 : Computed test embeddings
2019-02-14 02:11:17,834 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 02:11:24,689 : [('reg:1e-05', 43.57), ('reg:0.0001', 41.66), ('reg:0.001', 36.83), ('reg:0.01', 25.62)]
2019-02-14 02:11:24,689 : Cross-validation : best param found is reg = 1e-05             with score 43.57
2019-02-14 02:11:24,689 : Evaluating...
2019-02-14 02:11:25,044 : 
Dev acc : 43.57 Test acc : 62.6             for TREC

2019-02-14 02:11:25,044 : ***** Transfer task : MRPC *****


2019-02-14 02:11:25,066 : loading BERT model bert-base-uncased
2019-02-14 02:11:25,066 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:11:25,087 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:11:25,087 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa5utawmd
2019-02-14 02:11:27,524 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:11:28,972 : Computing embedding for train
2019-02-14 02:11:38,654 : Computed train embeddings
2019-02-14 02:11:38,654 : Computing embedding for test
2019-02-14 02:11:42,822 : Computed test embeddings
2019-02-14 02:11:42,839 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 02:11:47,500 : [('reg:1e-05', 67.66), ('reg:0.0001', 67.57), ('reg:0.001', 68.23), ('reg:0.01', 67.69)]
2019-02-14 02:11:47,500 : Cross-validation : best param found is reg = 0.001             with score 68.23
2019-02-14 02:11:47,500 : Evaluating...
2019-02-14 02:11:47,807 : Dev acc : 68.23 Test acc 66.43; Test F1 79.83 for MRPC.

2019-02-14 02:11:47,807 : ***** Transfer task : SICK-Entailment*****


2019-02-14 02:11:47,832 : loading BERT model bert-base-uncased
2019-02-14 02:11:47,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:11:47,894 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:11:47,895 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt6isb2zs
2019-02-14 02:11:50,335 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:11:51,792 : Computing embedding for train
2019-02-14 02:11:56,955 : Computed train embeddings
2019-02-14 02:11:56,955 : Computing embedding for dev
2019-02-14 02:11:57,640 : Computed dev embeddings
2019-02-14 02:11:57,641 : Computing embedding for test
2019-02-14 02:12:03,159 : Computed test embeddings
2019-02-14 02:12:03,187 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:12:04,678 : [('reg:1e-05', 66.2), ('reg:0.0001', 61.8), ('reg:0.001', 56.4), ('reg:0.01', 56.8)]
2019-02-14 02:12:04,678 : Validation : best param found is reg = 1e-05 with score             66.2
2019-02-14 02:12:04,679 : Evaluating...
2019-02-14 02:12:05,147 : 
Dev acc : 66.2 Test acc : 65.94 for                        SICK entailment

2019-02-14 02:12:05,148 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 02:12:05,176 : loading BERT model bert-base-uncased
2019-02-14 02:12:05,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:12:05,197 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:12:05,197 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd3mva6db
2019-02-14 02:12:07,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:12:09,119 : Computing embedding for train
2019-02-14 02:12:14,591 : Computed train embeddings
2019-02-14 02:12:14,591 : Computing embedding for dev
2019-02-14 02:12:15,348 : Computed dev embeddings
2019-02-14 02:12:15,348 : Computing embedding for test
2019-02-14 02:12:21,562 : Computed test embeddings
2019-02-14 02:13:22,783 : Dev : Pearson 0.6934138272695507
2019-02-14 02:13:22,783 : Test : Pearson 0.6927277226087686 Spearman 0.6272474148524694 MSE 0.5352644240487313                        for SICK Relatedness

2019-02-14 02:13:22,784 : 

***** Transfer task : STSBenchmark*****


2019-02-14 02:13:22,823 : loading BERT model bert-base-uncased
2019-02-14 02:13:22,824 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:13:22,853 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:13:22,853 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqsgxn2yn
2019-02-14 02:13:25,289 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:13:26,736 : Computing embedding for train
2019-02-14 02:13:34,982 : Computed train embeddings
2019-02-14 02:13:34,982 : Computing embedding for dev
2019-02-14 02:13:37,408 : Computed dev embeddings
2019-02-14 02:13:37,408 : Computing embedding for test
2019-02-14 02:13:39,407 : Computed test embeddings
2019-02-14 02:14:10,291 : Dev : Pearson 0.6041423060828881
2019-02-14 02:14:10,291 : Test : Pearson 0.5380553763211487 Spearman 0.5255586416665712 MSE 1.9826149596249012                        for SICK Relatedness

2019-02-14 02:14:10,291 : ***** Transfer task : SNLI Entailment*****


2019-02-14 02:14:15,047 : loading BERT model bert-base-uncased
2019-02-14 02:14:15,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:14:15,173 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:14:15,173 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp42h051oc
2019-02-14 02:14:17,609 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:14:19,231 : PROGRESS (encoding): 0.00%
2019-02-14 02:15:37,695 : PROGRESS (encoding): 14.56%
2019-02-14 02:17:06,065 : PROGRESS (encoding): 29.12%
2019-02-14 02:18:35,445 : PROGRESS (encoding): 43.69%
2019-02-14 02:20:12,905 : PROGRESS (encoding): 58.25%
2019-02-14 02:21:59,995 : PROGRESS (encoding): 72.81%
2019-02-14 02:23:45,287 : PROGRESS (encoding): 87.37%
2019-02-14 02:25:36,723 : PROGRESS (encoding): 0.00%
2019-02-14 02:25:50,276 : PROGRESS (encoding): 0.00%
2019-02-14 02:26:03,992 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 02:26:41,992 : [('reg:1e-09', 57.79)]
2019-02-14 02:26:41,992 : Validation : best param found is reg = 1e-09 with score             57.79
2019-02-14 02:26:41,993 : Evaluating...
2019-02-14 02:27:20,650 : Dev acc : 57.79 Test acc : 57.7 for SNLI

2019-02-14 02:27:20,650 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 02:27:29,761 : loading BERT model bert-base-uncased
2019-02-14 02:27:29,761 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 02:27:29,814 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 02:27:29,814 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8dz7ak03
2019-02-14 02:27:32,260 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 02:27:33,742 : Computing embedding for train
2019-02-14 02:35:15,838 : Computed train embeddings
2019-02-14 02:35:15,838 : Computing embedding for dev
2019-02-14 02:35:35,473 : Computed dev embeddings
2019-02-14 02:35:35,474 : Computing embedding for test
2019-02-14 02:35:56,928 : Computed test embeddings
2019-02-14 02:35:56,944 : prepare data
2019-02-14 02:35:57,009 : start epoch
2019-02-14 02:36:39,976 : samples : 64000
2019-02-14 02:36:50,075 : Image to text: 3.4, 11.98, 19.4, 52.0
2019-02-14 02:36:57,383 : Text to Image: 2.176, 8.284, 14.336, 68.0
2019-02-14 02:37:40,307 : samples : 128000
2019-02-14 02:37:50,399 : Image to text: 4.92, 15.32, 24.36, 39.0
2019-02-14 02:37:57,742 : Text to Image: 2.904, 10.404, 17.576, 53.0
2019-02-14 02:38:40,260 : samples : 192000
2019-02-14 02:38:50,556 : Image to text: 4.72, 17.14, 26.66, 33.0
2019-02-14 02:38:57,965 : Text to Image: 3.612, 12.732, 20.672, 45.0
2019-02-14 02:39:39,286 : samples : 256000
2019-02-14 02:39:49,564 : Image to text: 5.38, 17.2, 26.76, 33.0
2019-02-14 02:39:56,960 : Text to Image: 3.924, 13.816, 21.884, 41.0
2019-02-14 02:40:38,096 : samples : 320000
2019-02-14 02:40:48,338 : Image to text: 4.84, 15.62, 24.56, 38.0
2019-02-14 02:40:55,719 : Text to Image: 2.652, 10.668, 18.036, 51.0
2019-02-14 02:41:40,057 : samples : 384000
2019-02-14 02:41:50,346 : Image to text: 6.04, 19.0, 28.76, 29.0
2019-02-14 02:41:57,773 : Text to Image: 4.32, 14.796, 23.464, 37.0
2019-02-14 02:42:38,458 : samples : 448000
2019-02-14 02:42:48,715 : Image to text: 6.94, 20.96, 30.7, 26.0
2019-02-14 02:42:56,139 : Text to Image: 5.464, 17.968, 27.728, 30.0
2019-02-14 02:43:38,704 : samples : 512000
2019-02-14 02:43:48,979 : Image to text: 6.6, 20.2, 30.84, 27.0
2019-02-14 02:43:56,395 : Text to Image: 5.048, 17.184, 26.744, 32.0
2019-02-14 02:44:32,066 : Epoch 1 finished
2019-02-14 02:44:32,494 : Image to text: 17.6, 44.8, 63.1, 7.0
2019-02-14 02:44:32,809 : Text to Image: 12.9, 38.2, 55.48, 8.0
2019-02-14 02:44:33,237 : Image to text: 17.8, 48.0, 64.4, 6.0
2019-02-14 02:44:33,551 : Text to Image: 12.66, 38.52, 56.68, 8.0
2019-02-14 02:44:33,978 : Image to text: 18.0, 47.0, 61.9, 6.0
2019-02-14 02:44:34,291 : Text to Image: 13.3, 39.72, 55.9, 8.0
2019-02-14 02:44:34,716 : Image to text: 18.6, 49.8, 64.8, 6.0
2019-02-14 02:44:35,027 : Text to Image: 13.38, 38.44, 55.72, 9.0
2019-02-14 02:44:35,452 : Image to text: 17.3, 45.2, 63.5, 7.0
2019-02-14 02:44:35,760 : Text to Image: 12.62, 39.46, 56.7, 8.0
2019-02-14 02:44:35,760 : Dev mean Text to Image: 12.972000000000001, 38.868, 56.096000000000004, 8.200000000000001
2019-02-14 02:44:35,760 : Dev mean Image to text: 17.86, 46.96, 63.53999999999999, 6.4
2019-02-14 02:44:35,761 : start epoch
2019-02-14 02:45:18,453 : samples : 64000
2019-02-14 02:45:28,565 : Image to text: 7.38, 21.06, 31.98, 26.0
2019-02-14 02:45:35,886 : Text to Image: 5.152, 16.604, 25.8, 33.0
2019-02-14 02:46:18,371 : samples : 128000
2019-02-14 02:46:28,473 : Image to text: 7.12, 21.18, 31.92, 25.0
2019-02-14 02:46:35,798 : Text to Image: 5.792, 18.484, 28.296, 29.0
2019-02-14 02:47:17,827 : samples : 192000
2019-02-14 02:47:28,141 : Image to text: 6.46, 20.84, 30.9, 26.0
2019-02-14 02:47:35,555 : Text to Image: 5.248, 17.88, 27.4, 31.0
2019-02-14 02:48:17,878 : samples : 256000
2019-02-14 02:48:28,092 : Image to text: 7.36, 21.28, 32.3, 24.0
2019-02-14 02:48:35,480 : Text to Image: 5.572, 17.908, 27.84, 30.0
2019-02-14 02:49:17,228 : samples : 320000
2019-02-14 02:49:27,464 : Image to text: 8.0, 23.84, 34.0, 23.0
2019-02-14 02:49:34,826 : Text to Image: 5.884, 18.592, 28.272, 29.0
2019-02-14 02:50:16,801 : samples : 384000
2019-02-14 02:50:27,143 : Image to text: 6.42, 20.66, 31.06, 26.0
2019-02-14 02:50:34,532 : Text to Image: 5.08, 17.156, 26.52, 32.0
2019-02-14 02:51:16,401 : samples : 448000
2019-02-14 02:51:26,757 : Image to text: 7.44, 21.48, 31.56, 25.0
2019-02-14 02:51:34,172 : Text to Image: 4.844, 16.592, 26.324, 33.0
2019-02-14 02:52:16,166 : samples : 512000
2019-02-14 02:52:26,515 : Image to text: 7.62, 21.42, 32.32, 24.0
2019-02-14 02:52:33,923 : Text to Image: 5.128, 17.72, 27.308, 31.0
2019-02-14 02:53:08,873 : Epoch 2 finished
2019-02-14 02:53:09,303 : Image to text: 18.2, 47.3, 64.0, 6.0
2019-02-14 02:53:09,616 : Text to Image: 15.54, 42.34, 59.74, 7.0
2019-02-14 02:53:10,046 : Image to text: 17.1, 47.0, 62.0, 6.0
2019-02-14 02:53:10,359 : Text to Image: 13.74, 42.2, 59.56, 7.0
2019-02-14 02:53:10,789 : Image to text: 20.4, 47.8, 63.4, 6.0
2019-02-14 02:53:11,102 : Text to Image: 14.46, 42.64, 60.3, 7.0
2019-02-14 02:53:11,532 : Image to text: 22.4, 48.7, 66.3, 6.0
2019-02-14 02:53:11,849 : Text to Image: 15.2, 41.28, 58.7, 8.0
2019-02-14 02:53:12,277 : Image to text: 18.1, 48.0, 64.3, 6.0
2019-02-14 02:53:12,592 : Text to Image: 14.96, 43.6, 60.96, 7.0
2019-02-14 02:53:12,592 : Dev mean Text to Image: 14.780000000000001, 42.412, 59.852000000000004, 7.199999999999999
2019-02-14 02:53:12,592 : Dev mean Image to text: 19.240000000000002, 47.76, 64.0, 6.0
2019-02-14 02:53:12,593 : start epoch
2019-02-14 02:53:52,508 : samples : 64000
2019-02-14 02:54:02,548 : Image to text: 7.66, 22.3, 32.72, 24.0
2019-02-14 02:54:09,808 : Text to Image: 5.176, 17.024, 26.968, 31.0
2019-02-14 02:54:50,153 : samples : 128000
2019-02-14 02:55:00,222 : Image to text: 8.2, 23.82, 33.64, 23.0
2019-02-14 02:55:07,476 : Text to Image: 5.86, 19.076, 29.252, 28.0
2019-02-14 02:55:51,625 : samples : 192000
2019-02-14 02:56:01,862 : Image to text: 7.82, 23.82, 34.68, 22.0
2019-02-14 02:56:09,231 : Text to Image: 5.972, 19.856, 29.82, 27.0
2019-02-14 02:56:50,940 : samples : 256000
2019-02-14 02:57:01,159 : Image to text: 7.78, 23.88, 34.14, 23.0
2019-02-14 02:57:08,559 : Text to Image: 5.552, 18.504, 28.58, 29.0
2019-02-14 02:57:49,049 : samples : 320000
2019-02-14 02:57:59,281 : Image to text: 8.06, 23.58, 33.84, 23.0
2019-02-14 02:58:06,692 : Text to Image: 6.552, 19.828, 29.716, 28.0
2019-02-14 02:58:47,092 : samples : 384000
2019-02-14 02:58:57,336 : Image to text: 8.0, 24.08, 34.48, 22.0
2019-02-14 02:59:04,748 : Text to Image: 6.228, 19.06, 29.072, 28.0
2019-02-14 02:59:49,183 : samples : 448000
2019-02-14 02:59:59,487 : Image to text: 8.2, 24.12, 35.3, 22.0
2019-02-14 03:00:06,882 : Text to Image: 6.548, 20.012, 30.168, 27.0
2019-02-14 03:00:47,384 : samples : 512000
2019-02-14 03:00:57,650 : Image to text: 7.86, 23.18, 33.54, 23.0
2019-02-14 03:01:05,024 : Text to Image: 4.92, 16.812, 26.408, 32.0
2019-02-14 03:01:43,911 : Epoch 3 finished
2019-02-14 03:01:44,341 : Image to text: 20.7, 50.2, 66.6, 5.0
2019-02-14 03:01:44,656 : Text to Image: 16.56, 44.96, 62.18, 7.0
2019-02-14 03:01:45,085 : Image to text: 19.5, 49.8, 66.3, 6.0
2019-02-14 03:01:45,400 : Text to Image: 15.4, 43.88, 61.74, 7.0
2019-02-14 03:01:45,828 : Image to text: 21.7, 50.8, 65.8, 5.0
2019-02-14 03:01:46,143 : Text to Image: 15.36, 44.46, 62.58, 7.0
2019-02-14 03:01:46,568 : Image to text: 23.4, 53.0, 68.2, 5.0
2019-02-14 03:01:46,881 : Text to Image: 14.98, 42.9, 61.6, 7.0
2019-02-14 03:01:47,302 : Image to text: 23.1, 50.9, 66.4, 5.0
2019-02-14 03:01:47,637 : Text to Image: 15.8, 45.16, 62.16, 7.0
2019-02-14 03:01:47,637 : Dev mean Text to Image: 15.62, 44.272000000000006, 62.05199999999999, 7.0
2019-02-14 03:01:47,637 : Dev mean Image to text: 21.68, 50.94, 66.66, 5.2
2019-02-14 03:01:47,638 : start epoch
2019-02-14 03:02:28,009 : samples : 64000
2019-02-14 03:02:38,054 : Image to text: 8.08, 24.66, 35.92, 21.0
2019-02-14 03:02:45,316 : Text to Image: 6.672, 21.084, 32.004, 25.0
2019-02-14 03:03:25,461 : samples : 128000
2019-02-14 03:03:35,506 : Image to text: 8.7, 25.0, 35.7, 21.0
2019-02-14 03:03:42,766 : Text to Image: 6.248, 20.044, 30.748, 26.0
2019-02-14 03:04:22,759 : samples : 192000
2019-02-14 03:04:32,972 : Image to text: 8.32, 24.4, 35.46, 21.0
2019-02-14 03:04:40,348 : Text to Image: 6.148, 19.94, 30.172, 27.0
2019-02-14 03:05:20,701 : samples : 256000
2019-02-14 03:05:31,052 : Image to text: 8.7, 23.5, 34.52, 22.0
2019-02-14 03:05:38,463 : Text to Image: 6.38, 20.212, 30.628, 26.0
2019-02-14 03:06:18,917 : samples : 320000
2019-02-14 03:06:29,221 : Image to text: 8.34, 25.02, 36.6, 20.0
2019-02-14 03:06:36,616 : Text to Image: 7.008, 21.164, 31.872, 25.0
2019-02-14 03:07:16,982 : samples : 384000
2019-02-14 03:07:27,223 : Image to text: 7.86, 24.1, 36.26, 21.0
2019-02-14 03:07:34,621 : Text to Image: 6.74, 21.216, 31.668, 25.0
2019-02-14 03:08:15,213 : samples : 448000
2019-02-14 03:08:25,425 : Image to text: 8.48, 24.98, 35.64, 20.0
2019-02-14 03:08:32,779 : Text to Image: 6.68, 20.908, 31.684, 25.0
2019-02-14 03:09:13,329 : samples : 512000
2019-02-14 03:09:23,599 : Image to text: 8.0, 24.46, 36.28, 21.0
2019-02-14 03:09:30,968 : Text to Image: 6.724, 20.92, 31.608, 24.0
2019-02-14 03:10:05,361 : Epoch 4 finished
2019-02-14 03:10:05,791 : Image to text: 21.5, 54.8, 70.4, 5.0
2019-02-14 03:10:06,107 : Text to Image: 18.06, 46.72, 65.14, 6.0
2019-02-14 03:10:06,535 : Image to text: 22.8, 54.6, 69.4, 5.0
2019-02-14 03:10:06,849 : Text to Image: 16.86, 46.44, 64.62, 6.0
2019-02-14 03:10:07,275 : Image to text: 23.0, 54.7, 69.9, 5.0
2019-02-14 03:10:07,593 : Text to Image: 17.7, 47.68, 64.58, 6.0
2019-02-14 03:10:08,019 : Image to text: 24.1, 55.6, 71.8, 4.0
2019-02-14 03:10:08,342 : Text to Image: 17.08, 46.06, 64.5, 6.0
2019-02-14 03:10:08,803 : Image to text: 24.6, 54.4, 70.0, 5.0
2019-02-14 03:10:09,141 : Text to Image: 17.36, 47.32, 64.18, 6.0
2019-02-14 03:10:09,141 : Dev mean Text to Image: 17.412000000000003, 46.843999999999994, 64.604, 6.0
2019-02-14 03:10:09,141 : Dev mean Image to text: 23.200000000000003, 54.81999999999999, 70.3, 4.8
2019-02-14 03:10:09,141 : start epoch
2019-02-14 03:10:49,447 : samples : 64000
2019-02-14 03:10:59,492 : Image to text: 9.32, 26.02, 37.66, 18.0
2019-02-14 03:11:06,788 : Text to Image: 7.28, 22.432, 33.128, 23.0
2019-02-14 03:11:46,773 : samples : 128000
2019-02-14 03:11:56,813 : Image to text: 9.04, 24.66, 36.78, 20.0
2019-02-14 03:12:04,106 : Text to Image: 6.684, 20.864, 31.712, 25.0
2019-02-14 03:12:44,246 : samples : 192000
2019-02-14 03:12:54,647 : Image to text: 8.96, 24.96, 36.3, 20.0
2019-02-14 03:13:02,090 : Text to Image: 6.276, 19.836, 30.104, 27.0
2019-02-14 03:13:42,469 : samples : 256000
2019-02-14 03:13:52,927 : Image to text: 8.9, 24.92, 36.88, 20.0
2019-02-14 03:14:00,369 : Text to Image: 6.728, 21.712, 32.708, 24.0
2019-02-14 03:14:41,579 : samples : 320000
2019-02-14 03:14:52,036 : Image to text: 9.22, 25.46, 37.2, 19.0
2019-02-14 03:14:59,494 : Text to Image: 6.94, 21.932, 33.188, 23.0
2019-02-14 03:15:40,145 : samples : 384000
2019-02-14 03:15:50,540 : Image to text: 9.42, 26.3, 37.72, 19.0
2019-02-14 03:15:57,979 : Text to Image: 7.128, 21.98, 33.148, 24.0
2019-02-14 03:16:39,237 : samples : 448000
2019-02-14 03:16:49,535 : Image to text: 9.48, 26.0, 37.02, 19.0
2019-02-14 03:16:56,932 : Text to Image: 6.928, 21.724, 32.612, 23.0
2019-02-14 03:17:38,564 : samples : 512000
2019-02-14 03:17:48,851 : Image to text: 9.82, 26.44, 38.28, 18.0
2019-02-14 03:17:56,261 : Text to Image: 7.668, 22.724, 33.816, 22.0
2019-02-14 03:18:30,818 : Epoch 5 finished
2019-02-14 03:18:31,252 : Image to text: 23.8, 54.2, 71.4, 5.0
2019-02-14 03:18:31,573 : Text to Image: 18.22, 49.96, 67.58, 6.0
2019-02-14 03:18:32,002 : Image to text: 24.0, 55.3, 71.4, 4.0
2019-02-14 03:18:32,317 : Text to Image: 18.46, 48.28, 66.38, 6.0
2019-02-14 03:18:32,779 : Image to text: 24.1, 54.0, 68.0, 5.0
2019-02-14 03:18:33,118 : Text to Image: 18.46, 48.9, 66.32, 6.0
2019-02-14 03:18:33,566 : Image to text: 27.5, 57.8, 72.9, 4.0
2019-02-14 03:18:33,919 : Text to Image: 18.92, 48.62, 65.62, 6.0
2019-02-14 03:18:34,357 : Image to text: 27.0, 57.3, 71.8, 4.0
2019-02-14 03:18:34,690 : Text to Image: 18.92, 49.76, 66.12, 6.0
2019-02-14 03:18:34,690 : Dev mean Text to Image: 18.596, 49.104, 66.404, 6.0
2019-02-14 03:18:34,690 : Dev mean Image to text: 25.28, 55.720000000000006, 71.10000000000001, 4.3999999999999995
2019-02-14 03:18:34,691 : start epoch
2019-02-14 03:19:16,590 : samples : 64000
2019-02-14 03:19:26,642 : Image to text: 9.24, 26.32, 37.7, 19.0
2019-02-14 03:19:33,933 : Text to Image: 7.02, 21.808, 32.324, 24.0
2019-02-14 03:20:15,267 : samples : 128000
2019-02-14 03:20:25,316 : Image to text: 8.58, 23.84, 35.12, 21.0
2019-02-14 03:20:32,612 : Text to Image: 6.256, 20.012, 30.604, 25.0
2019-02-14 03:21:15,552 : samples : 192000
2019-02-14 03:21:25,842 : Image to text: 8.66, 25.3, 35.94, 20.0
2019-02-14 03:21:33,280 : Text to Image: 6.64, 20.812, 31.656, 25.0
2019-02-14 03:22:15,356 : samples : 256000
2019-02-14 03:22:25,797 : Image to text: 9.6, 27.14, 39.14, 18.0
2019-02-14 03:22:33,263 : Text to Image: 7.344, 22.72, 33.44, 23.0
2019-02-14 03:23:15,538 : samples : 320000
2019-02-14 03:23:25,913 : Image to text: 9.5, 26.24, 38.72, 18.0
2019-02-14 03:23:33,347 : Text to Image: 7.216, 21.972, 32.892, 23.0
2019-02-14 03:24:15,585 : samples : 384000
2019-02-14 03:24:26,034 : Image to text: 9.16, 25.82, 38.14, 19.0
2019-02-14 03:24:33,512 : Text to Image: 7.176, 22.028, 33.512, 22.0
2019-02-14 03:25:15,879 : samples : 448000
2019-02-14 03:25:26,269 : Image to text: 9.14, 25.6, 38.44, 19.0
2019-02-14 03:25:33,700 : Text to Image: 6.82, 21.856, 32.728, 23.0
2019-02-14 03:26:15,835 : samples : 512000
2019-02-14 03:26:26,202 : Image to text: 9.56, 26.52, 38.08, 18.0
2019-02-14 03:26:33,647 : Text to Image: 7.152, 22.328, 33.452, 23.0
2019-02-14 03:27:09,590 : Epoch 6 finished
2019-02-14 03:27:10,047 : Image to text: 21.6, 54.4, 73.1, 5.0
2019-02-14 03:27:10,381 : Text to Image: 21.7, 53.32, 70.22, 5.0
2019-02-14 03:27:10,829 : Image to text: 24.3, 56.7, 71.5, 4.0
2019-02-14 03:27:11,160 : Text to Image: 20.48, 52.08, 69.3, 5.0
2019-02-14 03:27:11,591 : Image to text: 24.3, 56.7, 70.4, 4.0
2019-02-14 03:27:11,922 : Text to Image: 20.6, 53.38, 69.94, 5.0
2019-02-14 03:27:12,364 : Image to text: 24.2, 60.4, 73.5, 4.0
2019-02-14 03:27:12,694 : Text to Image: 20.54, 51.92, 69.54, 5.0
2019-02-14 03:27:13,124 : Image to text: 26.9, 57.5, 72.6, 4.0
2019-02-14 03:27:13,455 : Text to Image: 21.44, 53.46, 69.62, 5.0
2019-02-14 03:27:13,455 : Dev mean Text to Image: 20.952, 52.832, 69.724, 5.0
2019-02-14 03:27:13,455 : Dev mean Image to text: 24.259999999999998, 57.14, 72.22, 4.2
2019-02-14 03:27:13,455 : start epoch
2019-02-14 03:27:56,271 : samples : 64000
2019-02-14 03:28:06,389 : Image to text: 9.08, 26.74, 38.92, 18.0
2019-02-14 03:28:13,770 : Text to Image: 7.52, 23.02, 34.084, 22.0
2019-02-14 03:28:59,007 : samples : 128000
2019-02-14 03:29:09,134 : Image to text: 9.28, 27.34, 39.22, 17.0
2019-02-14 03:29:16,513 : Text to Image: 7.252, 22.364, 33.496, 23.0
2019-02-14 03:29:58,958 : samples : 192000
2019-02-14 03:30:11,137 : Image to text: 9.78, 27.1, 39.46, 18.0
2019-02-14 03:30:21,164 : Text to Image: 8.012, 24.3, 35.5, 20.0
2019-02-14 03:31:06,218 : samples : 256000
2019-02-14 03:31:18,847 : Image to text: 9.76, 26.74, 38.16, 18.0
2019-02-14 03:31:28,856 : Text to Image: 7.548, 22.988, 34.26, 22.0
2019-02-14 03:32:10,078 : samples : 320000
2019-02-14 03:32:20,048 : Image to text: 9.74, 27.08, 38.36, 18.0
2019-02-14 03:32:29,012 : Text to Image: 7.936, 23.328, 34.808, 21.0
2019-02-14 03:33:12,671 : samples : 384000
2019-02-14 03:33:25,283 : Image to text: 9.02, 26.7, 38.3, 18.0
2019-02-14 03:33:35,347 : Text to Image: 7.592, 23.256, 34.376, 22.0
2019-02-14 03:34:17,124 : samples : 448000
2019-02-14 03:34:27,181 : Image to text: 9.38, 26.24, 38.22, 18.0
2019-02-14 03:34:34,289 : Text to Image: 7.708, 23.328, 34.756, 21.0
2019-02-14 03:35:16,224 : samples : 512000
2019-02-14 03:35:28,873 : Image to text: 10.06, 27.82, 39.64, 18.0
2019-02-14 03:35:38,982 : Text to Image: 7.472, 23.016, 34.444, 21.0
2019-02-14 03:36:15,633 : Epoch 7 finished
2019-02-14 03:36:16,088 : Image to text: 22.4, 53.6, 71.1, 5.0
2019-02-14 03:36:16,451 : Text to Image: 18.68, 50.94, 67.7, 5.0
2019-02-14 03:36:16,901 : Image to text: 22.8, 55.5, 70.7, 4.0
2019-02-14 03:36:17,265 : Text to Image: 18.54, 50.7, 67.94, 5.0
2019-02-14 03:36:17,714 : Image to text: 23.4, 55.5, 72.2, 4.0
2019-02-14 03:36:18,078 : Text to Image: 18.76, 50.68, 68.04, 5.0
2019-02-14 03:36:18,528 : Image to text: 26.6, 55.7, 72.3, 4.0
2019-02-14 03:36:18,880 : Text to Image: 19.24, 49.7, 69.44, 6.0
2019-02-14 03:36:19,329 : Image to text: 24.2, 55.4, 70.5, 4.0
2019-02-14 03:36:19,692 : Text to Image: 19.4, 50.78, 68.2, 5.0
2019-02-14 03:36:19,692 : Dev mean Text to Image: 18.924, 50.559999999999995, 68.26400000000001, 5.2
2019-02-14 03:36:19,692 : Dev mean Image to text: 23.88, 55.14, 71.36, 4.2
2019-02-14 03:36:19,692 : start epoch
2019-02-14 03:37:02,040 : samples : 64000
2019-02-14 03:37:14,608 : Image to text: 9.42, 25.88, 37.36, 19.0
2019-02-14 03:37:24,614 : Text to Image: 6.908, 21.664, 32.46, 24.0
2019-02-14 03:38:08,591 : samples : 128000
2019-02-14 03:38:18,666 : Image to text: 9.76, 27.56, 39.68, 17.0
2019-02-14 03:38:25,880 : Text to Image: 7.804, 23.46, 34.852, 21.0
2019-02-14 03:39:07,657 : samples : 192000
2019-02-14 03:39:20,243 : Image to text: 9.64, 27.66, 40.02, 17.0
2019-02-14 03:39:30,271 : Text to Image: 7.68, 23.38, 34.448, 22.0
2019-02-14 03:40:14,812 : samples : 256000
2019-02-14 03:40:24,877 : Image to text: 9.44, 27.26, 38.4, 18.0
2019-02-14 03:40:32,106 : Text to Image: 7.392, 22.812, 34.372, 22.0
2019-02-14 03:41:12,796 : samples : 320000
2019-02-14 03:41:25,369 : Image to text: 10.08, 27.68, 39.9, 17.0
2019-02-14 03:41:35,397 : Text to Image: 7.676, 23.108, 34.268, 22.0
2019-02-14 03:42:19,287 : samples : 384000
2019-02-14 03:42:31,903 : Image to text: 10.12, 28.78, 40.26, 17.0
2019-02-14 03:42:39,945 : Text to Image: 7.48, 22.844, 34.372, 22.0
2019-02-14 03:43:21,286 : samples : 448000
2019-02-14 03:43:33,837 : Image to text: 10.34, 28.9, 40.42, 16.0
2019-02-14 03:43:43,861 : Text to Image: 8.136, 24.336, 35.584, 20.0
2019-02-14 03:44:28,589 : samples : 512000
2019-02-14 03:44:40,422 : Image to text: 10.18, 27.78, 39.46, 17.0
2019-02-14 03:44:47,645 : Text to Image: 7.976, 23.288, 34.92, 21.0
2019-02-14 03:45:22,736 : Epoch 8 finished
2019-02-14 03:45:23,641 : Image to text: 24.5, 56.8, 73.0, 4.0
2019-02-14 03:45:24,390 : Text to Image: 20.26, 52.92, 69.62, 5.0
2019-02-14 03:45:25,328 : Image to text: 24.4, 56.5, 72.7, 4.0
2019-02-14 03:45:26,092 : Text to Image: 18.88, 51.2, 68.98, 5.0
2019-02-14 03:45:27,012 : Image to text: 24.7, 56.7, 71.9, 4.0
2019-02-14 03:45:27,767 : Text to Image: 20.56, 52.78, 70.02, 5.0
2019-02-14 03:45:28,703 : Image to text: 27.9, 59.2, 74.8, 4.0
2019-02-14 03:45:29,459 : Text to Image: 20.06, 51.64, 69.06, 5.0
2019-02-14 03:45:30,406 : Image to text: 25.6, 57.0, 72.9, 4.0
2019-02-14 03:45:31,179 : Text to Image: 20.18, 51.76, 68.8, 5.0
2019-02-14 03:45:31,179 : Dev mean Text to Image: 19.988, 52.06, 69.296, 5.0
2019-02-14 03:45:31,179 : Dev mean Image to text: 25.42, 57.24, 73.06, 4.0
2019-02-14 03:45:31,179 : start epoch
2019-02-14 03:46:14,411 : samples : 64000
2019-02-14 03:46:27,043 : Image to text: 10.62, 27.8, 40.16, 16.0
2019-02-14 03:46:36,994 : Text to Image: 8.412, 24.456, 35.596, 20.0
2019-02-14 03:47:18,041 : samples : 128000
2019-02-14 03:47:29,423 : Image to text: 9.74, 26.26, 38.56, 18.0
2019-02-14 03:47:39,496 : Text to Image: 7.816, 23.084, 34.728, 21.0
2019-02-14 03:48:23,926 : samples : 192000
2019-02-14 03:48:36,592 : Image to text: 10.04, 27.66, 38.7, 18.0
2019-02-14 03:48:46,672 : Text to Image: 7.764, 23.224, 34.932, 21.0
2019-02-14 03:49:28,217 : samples : 256000
2019-02-14 03:49:38,236 : Image to text: 9.74, 27.64, 39.4, 17.0
2019-02-14 03:49:45,413 : Text to Image: 7.672, 23.2, 34.392, 21.0
2019-02-14 03:50:27,912 : samples : 320000
2019-02-14 03:50:40,573 : Image to text: 10.16, 28.84, 40.98, 16.0
2019-02-14 03:50:50,643 : Text to Image: 7.932, 23.932, 35.632, 21.0
2019-02-14 03:51:34,325 : samples : 384000
2019-02-14 03:51:44,407 : Image to text: 9.84, 27.48, 39.12, 17.0
2019-02-14 03:51:51,541 : Text to Image: 7.54, 23.332, 34.488, 21.0
2019-02-14 03:52:32,976 : samples : 448000
2019-02-14 03:52:44,276 : Image to text: 9.72, 27.16, 39.38, 17.0
2019-02-14 03:52:51,677 : Text to Image: 8.092, 23.936, 35.06, 21.0
2019-02-14 03:53:33,528 : samples : 512000
2019-02-14 03:53:43,588 : Image to text: 9.92, 28.8, 40.66, 16.0
2019-02-14 03:53:50,775 : Text to Image: 8.288, 24.572, 36.168, 20.0
2019-02-14 03:54:25,664 : Epoch 9 finished
2019-02-14 03:54:26,126 : Image to text: 25.0, 57.6, 74.5, 4.0
2019-02-14 03:54:26,488 : Text to Image: 21.22, 54.94, 70.62, 5.0
2019-02-14 03:54:26,937 : Image to text: 26.9, 59.5, 72.8, 4.0
2019-02-14 03:54:27,299 : Text to Image: 19.92, 53.0, 70.4, 5.0
2019-02-14 03:54:27,746 : Image to text: 26.1, 58.5, 73.3, 4.0
2019-02-14 03:54:28,109 : Text to Image: 21.36, 54.38, 70.28, 5.0
2019-02-14 03:54:28,557 : Image to text: 27.0, 59.9, 75.7, 4.0
2019-02-14 03:54:28,920 : Text to Image: 20.76, 53.1, 70.26, 5.0
2019-02-14 03:54:29,367 : Image to text: 26.7, 59.3, 73.6, 4.0
2019-02-14 03:54:29,729 : Text to Image: 22.18, 54.2, 70.34, 5.0
2019-02-14 03:54:29,729 : Dev mean Text to Image: 21.088, 53.92400000000001, 70.38000000000001, 5.0
2019-02-14 03:54:29,729 : Dev mean Image to text: 26.34, 58.96000000000001, 73.98, 4.0
2019-02-14 03:54:29,729 : start epoch
2019-02-14 03:55:10,358 : samples : 64000
2019-02-14 03:55:20,422 : Image to text: 9.48, 27.4, 39.92, 17.0
2019-02-14 03:55:27,658 : Text to Image: 8.092, 23.536, 34.996, 21.0
2019-02-14 03:56:09,736 : samples : 128000
2019-02-14 03:56:19,781 : Image to text: 10.6, 28.82, 40.88, 16.0
2019-02-14 03:56:27,019 : Text to Image: 8.488, 25.02, 36.516, 20.0
2019-02-14 03:57:08,467 : samples : 192000
2019-02-14 03:57:21,153 : Image to text: 9.8, 26.96, 38.82, 18.0
2019-02-14 03:57:31,353 : Text to Image: 7.036, 21.96, 33.028, 24.0
2019-02-14 03:58:13,473 : samples : 256000
2019-02-14 03:58:24,240 : Image to text: 10.4, 29.12, 40.64, 16.0
2019-02-14 03:58:34,251 : Text to Image: 8.108, 24.028, 35.5, 21.0
2019-02-14 03:59:16,019 : samples : 320000
2019-02-14 03:59:27,614 : Image to text: 10.1, 28.2, 40.84, 17.0
2019-02-14 03:59:35,744 : Text to Image: 7.84, 23.512, 35.128, 21.0
2019-02-14 04:00:17,112 : samples : 384000
2019-02-14 04:00:29,270 : Image to text: 9.74, 27.44, 38.98, 17.0
2019-02-14 04:00:36,518 : Text to Image: 7.736, 23.956, 35.476, 20.0
2019-02-14 04:01:18,592 : samples : 448000
2019-02-14 04:01:31,118 : Image to text: 7.86, 24.66, 36.22, 20.0
2019-02-14 04:01:41,060 : Text to Image: 6.596, 21.24, 32.0, 24.0
2019-02-14 04:02:24,612 : samples : 512000
2019-02-14 04:02:37,186 : Image to text: 10.84, 27.94, 40.56, 16.0
2019-02-14 04:02:47,136 : Text to Image: 8.308, 24.924, 36.524, 20.0
2019-02-14 04:03:24,351 : Epoch 10 finished
2019-02-14 04:03:25,264 : Image to text: 26.2, 56.9, 73.5, 4.0
2019-02-14 04:03:26,002 : Text to Image: 21.6, 53.78, 70.74, 5.0
2019-02-14 04:03:26,900 : Image to text: 26.0, 59.0, 72.3, 4.0
2019-02-14 04:03:27,636 : Text to Image: 19.66, 52.34, 70.16, 5.0
2019-02-14 04:03:28,547 : Image to text: 24.5, 58.2, 73.4, 4.0
2019-02-14 04:03:29,296 : Text to Image: 21.18, 53.46, 70.2, 5.0
2019-02-14 04:03:30,203 : Image to text: 26.8, 59.3, 74.7, 4.0
2019-02-14 04:03:30,935 : Text to Image: 21.16, 53.48, 70.2, 5.0
2019-02-14 04:03:31,835 : Image to text: 26.4, 59.5, 74.4, 4.0
2019-02-14 04:03:32,575 : Text to Image: 21.9, 54.14, 70.78, 5.0
2019-02-14 04:03:32,575 : Dev mean Text to Image: 21.099999999999998, 53.44, 70.416, 5.0
2019-02-14 04:03:32,575 : Dev mean Image to text: 25.980000000000004, 58.58, 73.66, 4.0
2019-02-14 04:03:32,575 : start epoch
2019-02-14 04:04:16,261 : samples : 64000
2019-02-14 04:04:28,856 : Image to text: 10.6, 29.56, 41.58, 16.0
2019-02-14 04:04:38,873 : Text to Image: 8.58, 25.58, 37.448, 19.0
2019-02-14 04:05:22,346 : samples : 128000
2019-02-14 04:05:34,965 : Image to text: 10.08, 28.22, 40.6, 16.0
2019-02-14 04:05:45,092 : Text to Image: 8.072, 24.428, 36.18, 20.0
2019-02-14 04:06:28,515 : samples : 192000
2019-02-14 04:06:41,131 : Image to text: 10.04, 27.36, 39.7, 17.0
2019-02-14 04:06:51,172 : Text to Image: 7.636, 23.6, 35.024, 22.0
2019-02-14 04:07:35,131 : samples : 256000
2019-02-14 04:07:47,741 : Image to text: 9.92, 27.54, 40.12, 17.0
2019-02-14 04:07:57,729 : Text to Image: 7.624, 23.072, 34.54, 22.0
2019-02-14 04:08:40,472 : samples : 320000
2019-02-14 04:08:53,080 : Image to text: 10.98, 27.58, 40.58, 16.0
2019-02-14 04:09:03,050 : Text to Image: 7.528, 23.46, 35.152, 21.0
2019-02-14 04:09:46,144 : samples : 384000
2019-02-14 04:09:58,758 : Image to text: 10.56, 28.44, 41.04, 15.0
2019-02-14 04:10:08,772 : Text to Image: 8.368, 25.176, 36.572, 19.0
2019-02-14 04:10:51,887 : samples : 448000
2019-02-14 04:11:04,528 : Image to text: 10.32, 27.84, 40.0, 17.0
2019-02-14 04:11:14,547 : Text to Image: 7.848, 24.288, 35.876, 20.0
2019-02-14 04:11:58,373 : samples : 512000
2019-02-14 04:12:11,068 : Image to text: 10.54, 28.8, 41.04, 16.0
2019-02-14 04:12:20,071 : Text to Image: 8.404, 24.08, 35.768, 20.0
2019-02-14 04:12:56,589 : Epoch 11 finished
2019-02-14 04:12:57,041 : Image to text: 25.1, 57.3, 73.3, 4.0
2019-02-14 04:12:57,400 : Text to Image: 22.4, 54.14, 71.28, 5.0
2019-02-14 04:12:57,853 : Image to text: 26.7, 60.0, 73.7, 4.0
2019-02-14 04:12:58,215 : Text to Image: 20.38, 53.46, 70.84, 5.0
2019-02-14 04:12:58,668 : Image to text: 25.3, 57.7, 72.5, 4.0
2019-02-14 04:12:59,026 : Text to Image: 20.6, 53.08, 70.74, 5.0
2019-02-14 04:12:59,479 : Image to text: 28.1, 62.0, 77.2, 4.0
2019-02-14 04:12:59,832 : Text to Image: 20.94, 53.08, 71.14, 5.0
2019-02-14 04:13:00,315 : Image to text: 28.4, 59.0, 74.7, 4.0
2019-02-14 04:13:00,675 : Text to Image: 21.16, 53.3, 71.28, 5.0
2019-02-14 04:13:00,675 : Dev mean Text to Image: 21.095999999999997, 53.41199999999999, 71.05600000000001, 5.0
2019-02-14 04:13:00,675 : Dev mean Image to text: 26.72, 59.2, 74.28, 4.0
2019-02-14 04:13:00,675 : start epoch
2019-02-14 04:13:41,694 : samples : 64000
2019-02-14 04:13:51,830 : Image to text: 10.5, 28.88, 41.54, 16.0
2019-02-14 04:13:58,624 : Text to Image: 8.988, 25.708, 37.32, 19.0
2019-02-14 04:14:42,429 : samples : 128000
2019-02-14 04:14:55,287 : Image to text: 10.54, 28.88, 41.92, 15.0
2019-02-14 04:15:05,701 : Text to Image: 8.284, 24.94, 36.524, 20.0
2019-02-14 04:15:50,496 : samples : 192000
2019-02-14 04:16:03,414 : Image to text: 10.08, 27.54, 39.92, 16.0
2019-02-14 04:16:13,835 : Text to Image: 8.46, 25.0, 36.488, 19.0
2019-02-14 04:16:58,822 : samples : 256000
2019-02-14 04:17:11,782 : Image to text: 11.14, 29.38, 41.6, 15.0
2019-02-14 04:17:22,288 : Text to Image: 8.108, 24.336, 35.96, 20.0
2019-02-14 04:18:07,358 : samples : 320000
2019-02-14 04:18:20,380 : Image to text: 10.36, 28.86, 40.72, 16.0
2019-02-14 04:18:30,901 : Text to Image: 7.832, 23.408, 34.48, 22.0
2019-02-14 04:19:16,085 : samples : 384000
2019-02-14 04:19:28,989 : Image to text: 10.34, 28.16, 40.9, 16.0
2019-02-14 04:19:39,535 : Text to Image: 8.508, 24.896, 36.608, 19.0
2019-02-14 04:20:24,625 : samples : 448000
2019-02-14 04:20:37,443 : Image to text: 11.22, 29.2, 41.78, 15.0
2019-02-14 04:20:47,892 : Text to Image: 8.912, 25.316, 37.256, 19.0
2019-02-14 04:21:32,914 : samples : 512000
2019-02-14 04:21:45,839 : Image to text: 10.02, 28.68, 41.02, 16.0
2019-02-14 04:21:54,832 : Text to Image: 8.12, 24.352, 36.476, 20.0
2019-02-14 04:22:32,370 : Epoch 12 finished
2019-02-14 04:22:32,794 : Image to text: 24.6, 56.0, 72.3, 4.0
2019-02-14 04:22:33,121 : Text to Image: 19.12, 51.0, 68.12, 5.0
2019-02-14 04:22:33,556 : Image to text: 23.3, 55.5, 70.3, 5.0
2019-02-14 04:22:33,883 : Text to Image: 18.12, 49.32, 67.92, 6.0
2019-02-14 04:22:34,306 : Image to text: 25.6, 57.1, 72.0, 4.0
2019-02-14 04:22:34,636 : Text to Image: 19.74, 51.16, 68.0, 5.0
2019-02-14 04:22:35,078 : Image to text: 26.0, 58.5, 73.9, 4.0
2019-02-14 04:22:35,410 : Text to Image: 18.42, 49.48, 67.06, 6.0
2019-02-14 04:22:35,843 : Image to text: 25.5, 55.8, 72.7, 4.0
2019-02-14 04:22:36,175 : Text to Image: 18.48, 50.14, 67.54, 5.0
2019-02-14 04:22:36,175 : Dev mean Text to Image: 18.776000000000003, 50.22, 67.728, 5.4
2019-02-14 04:22:36,175 : Dev mean Image to text: 25.0, 56.58, 72.24, 4.2
2019-02-14 04:22:39,994 : 
Test scores | Image to text:             26.320000000000004, 58.17999999999999, 73.61999999999999, 3.8
2019-02-14 04:22:39,994 : Test scores | Text to image:             21.416000000000004, 52.99600000000001, 70.352, 4.8

2019-02-14 04:22:40,094 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 04:22:40,308 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 04:22:40,976 : loading BERT model bert-base-uncased
2019-02-14 04:22:40,977 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:22:41,010 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:22:41,010 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeqi_vabj
2019-02-14 04:22:43,455 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:22:44,907 : Computing embeddings for train/dev/test
2019-02-14 04:24:21,607 : Computed embeddings
2019-02-14 04:24:21,607 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:24:46,106 : [('reg:1e-05', 87.67), ('reg:0.0001', 85.92), ('reg:0.001', 85.86), ('reg:0.01', 59.84)]
2019-02-14 04:24:46,106 : Validation : best param found is reg = 1e-05 with score             87.67
2019-02-14 04:24:46,106 : Evaluating...
2019-02-14 04:24:52,223 : 
Dev acc : 87.7 Test acc : 88.4 for LENGTH classification

2019-02-14 04:24:52,223 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 04:24:52,612 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 04:24:52,658 : loading BERT model bert-base-uncased
2019-02-14 04:24:52,659 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:24:52,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:24:52,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8rkaonhs
2019-02-14 04:24:55,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:24:56,583 : Computing embeddings for train/dev/test
2019-02-14 04:26:26,308 : Computed embeddings
2019-02-14 04:26:26,308 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:27:08,594 : [('reg:1e-05', 2.11), ('reg:0.0001', 0.43), ('reg:0.001', 0.11), ('reg:0.01', 0.14)]
2019-02-14 04:27:08,594 : Validation : best param found is reg = 1e-05 with score             2.11
2019-02-14 04:27:08,594 : Evaluating...
2019-02-14 04:27:20,609 : 
Dev acc : 2.1 Test acc : 2.0 for WORDCONTENT classification

2019-02-14 04:27:20,611 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 04:27:20,946 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 04:27:21,022 : loading BERT model bert-base-uncased
2019-02-14 04:27:21,023 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:27:21,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:27:21,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ufhctkv
2019-02-14 04:27:23,498 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:27:24,944 : Computing embeddings for train/dev/test
2019-02-14 04:28:49,274 : Computed embeddings
2019-02-14 04:28:49,274 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:29:20,166 : [('reg:1e-05', 27.9), ('reg:0.0001', 23.93), ('reg:0.001', 18.21), ('reg:0.01', 18.07)]
2019-02-14 04:29:20,166 : Validation : best param found is reg = 1e-05 with score             27.9
2019-02-14 04:29:20,166 : Evaluating...
2019-02-14 04:29:26,888 : 
Dev acc : 27.9 Test acc : 27.3 for DEPTH classification

2019-02-14 04:29:26,889 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 04:29:27,304 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 04:29:27,371 : loading BERT model bert-base-uncased
2019-02-14 04:29:27,372 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:29:27,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:29:27,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeqhg4kna
2019-02-14 04:29:29,944 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:29:31,397 : Computing embeddings for train/dev/test
2019-02-14 04:30:50,847 : Computed embeddings
2019-02-14 04:30:50,848 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:31:23,692 : [('reg:1e-05', 46.12), ('reg:0.0001', 33.42), ('reg:0.001', 18.85), ('reg:0.01', 9.02)]
2019-02-14 04:31:23,692 : Validation : best param found is reg = 1e-05 with score             46.12
2019-02-14 04:31:23,692 : Evaluating...
2019-02-14 04:31:34,892 : 
Dev acc : 46.1 Test acc : 46.0 for TOPCONSTITUENTS classification

2019-02-14 04:31:34,893 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 04:31:35,428 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 04:31:35,494 : loading BERT model bert-base-uncased
2019-02-14 04:31:35,495 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:31:35,526 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:31:35,526 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8qpeqxqs
2019-02-14 04:31:37,969 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:31:39,422 : Computing embeddings for train/dev/test
2019-02-14 04:33:03,618 : Computed embeddings
2019-02-14 04:33:03,618 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:33:36,590 : [('reg:1e-05', 59.7), ('reg:0.0001', 59.04), ('reg:0.001', 55.77), ('reg:0.01', 52.06)]
2019-02-14 04:33:36,590 : Validation : best param found is reg = 1e-05 with score             59.7
2019-02-14 04:33:36,590 : Evaluating...
2019-02-14 04:33:44,944 : 
Dev acc : 59.7 Test acc : 60.0 for BIGRAMSHIFT classification

2019-02-14 04:33:44,945 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 04:33:45,356 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 04:33:45,428 : loading BERT model bert-base-uncased
2019-02-14 04:33:45,428 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:33:45,563 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:33:45,563 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsvvb09ef
2019-02-14 04:33:48,018 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:33:49,467 : Computing embeddings for train/dev/test
2019-02-14 04:35:11,916 : Computed embeddings
2019-02-14 04:35:11,916 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:35:50,908 : [('reg:1e-05', 85.95), ('reg:0.0001', 84.52), ('reg:0.001', 79.62), ('reg:0.01', 67.64)]
2019-02-14 04:35:50,908 : Validation : best param found is reg = 1e-05 with score             85.95
2019-02-14 04:35:50,909 : Evaluating...
2019-02-14 04:36:01,722 : 
Dev acc : 86.0 Test acc : 84.7 for TENSE classification

2019-02-14 04:36:01,723 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 04:36:02,345 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 04:36:02,413 : loading BERT model bert-base-uncased
2019-02-14 04:36:02,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:36:02,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:36:02,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_4sljp9r
2019-02-14 04:36:04,897 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:36:06,329 : Computing embeddings for train/dev/test
2019-02-14 04:37:34,101 : Computed embeddings
2019-02-14 04:37:34,101 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:38:08,135 : [('reg:1e-05', 75.59), ('reg:0.0001', 73.5), ('reg:0.001', 67.95), ('reg:0.01', 61.89)]
2019-02-14 04:38:08,135 : Validation : best param found is reg = 1e-05 with score             75.59
2019-02-14 04:38:08,135 : Evaluating...
2019-02-14 04:38:18,046 : 
Dev acc : 75.6 Test acc : 74.9 for SUBJNUMBER classification

2019-02-14 04:38:18,047 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 04:38:18,693 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 04:38:18,765 : loading BERT model bert-base-uncased
2019-02-14 04:38:18,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:38:18,799 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:38:18,799 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw0x7wvc0
2019-02-14 04:38:21,264 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:38:22,741 : Computing embeddings for train/dev/test
2019-02-14 04:39:49,943 : Computed embeddings
2019-02-14 04:39:49,943 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:40:16,414 : [('reg:1e-05', 69.03), ('reg:0.0001', 66.88), ('reg:0.001', 58.11), ('reg:0.01', 50.01)]
2019-02-14 04:40:16,414 : Validation : best param found is reg = 1e-05 with score             69.03
2019-02-14 04:40:16,414 : Evaluating...
2019-02-14 04:40:23,832 : 
Dev acc : 69.0 Test acc : 70.1 for OBJNUMBER classification

2019-02-14 04:40:23,833 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 04:40:24,276 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 04:40:24,351 : loading BERT model bert-base-uncased
2019-02-14 04:40:24,351 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:40:24,383 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:40:24,383 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuey4xsmt
2019-02-14 04:40:26,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:40:28,287 : Computing embeddings for train/dev/test
2019-02-14 04:42:07,360 : Computed embeddings
2019-02-14 04:42:07,360 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:42:37,997 : [('reg:1e-05', 55.24), ('reg:0.0001', 54.38), ('reg:0.001', 53.49), ('reg:0.01', 50.19)]
2019-02-14 04:42:37,998 : Validation : best param found is reg = 1e-05 with score             55.24
2019-02-14 04:42:37,998 : Evaluating...
2019-02-14 04:42:44,840 : 
Dev acc : 55.2 Test acc : 56.0 for ODDMANOUT classification

2019-02-14 04:42:44,841 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 04:42:45,251 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 04:42:45,327 : loading BERT model bert-base-uncased
2019-02-14 04:42:45,327 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:42:45,455 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:42:45,455 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3qm34grd
2019-02-14 04:42:47,888 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:42:49,303 : Computing embeddings for train/dev/test
2019-02-14 04:44:26,693 : Computed embeddings
2019-02-14 04:44:26,693 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:44:56,217 : [('reg:1e-05', 57.3), ('reg:0.0001', 57.33), ('reg:0.001', 55.46), ('reg:0.01', 50.25)]
2019-02-14 04:44:56,217 : Validation : best param found is reg = 0.0001 with score             57.33
2019-02-14 04:44:56,217 : Evaluating...
2019-02-14 04:45:03,670 : 
Dev acc : 57.3 Test acc : 56.8 for COORDINATIONINVERSION classification

2019-02-14 04:45:03,672 : total results: {'STS12': {'MSRpar': {'pearson': (0.18258224363849862, 4.79706955657332e-07), 'spearman': SpearmanrResult(correlation=0.20713688350854775, pvalue=1.0317670480440374e-08), 'nsamples': 750}, 'MSRvid': {'pearson': (0.29493923195232363, 1.6115121113994902e-16), 'spearman': SpearmanrResult(correlation=0.2993430982414356, pvalue=5.4289651023371335e-17), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.45161727236457483, 1.8932495929941107e-24), 'spearman': SpearmanrResult(correlation=0.559036597658479, pvalue=4.324101006710235e-39), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.34672886151058546, 1.3075429286853528e-22), 'spearman': SpearmanrResult(correlation=0.3384138472799185, pvalue=1.4967940039632032e-21), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6150803365373121, 6.824453972220229e-43), 'spearman': SpearmanrResult(correlation=0.4877014040715104, pvalue=3.1291465362046395e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3781895892006589, 'wmean': 0.34456149778628803}, 'spearman': {'mean': 0.3783263661519783, 'wmean': 0.34905438556055374}}}, 'STS13': {'FNWN': {'pearson': (0.19303481018851615, 0.0077848334696506314), 'spearman': SpearmanrResult(correlation=0.1881365145294057, pvalue=0.009528583187220309), 'nsamples': 189}, 'headlines': {'pearson': (0.1940245983196068, 8.522828628050367e-08), 'spearman': SpearmanrResult(correlation=0.23874199531044557, pvalue=3.512161268847544e-11), 'nsamples': 750}, 'OnWN': {'pearson': (0.18855430791072647, 6.910591416398881e-06), 'spearman': SpearmanrResult(correlation=0.20663026190087713, pvalue=7.950238570765087e-07), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.19187123880628312, 'wmean': 0.19185399640216816}, 'spearman': {'mean': 0.2111695905802428, 'wmean': 0.22035591643685598}}}, 'STS14': {'deft-forum': {'pearson': (0.10180943444030614, 0.03082605980740905), 'spearman': SpearmanrResult(correlation=0.15131967390615036, pvalue=0.0012836714992520753), 'nsamples': 450}, 'deft-news': {'pearson': (0.348555065679338, 5.376551711201867e-10), 'spearman': SpearmanrResult(correlation=0.37778173267545107, pvalue=1.3010065643910538e-11), 'nsamples': 300}, 'headlines': {'pearson': (0.2546911847387671, 1.4367796515210039e-12), 'spearman': SpearmanrResult(correlation=0.25881200892715706, pvalue=6.063413339731868e-13), 'nsamples': 750}, 'images': {'pearson': (0.30878068457049096, 4.947761852865853e-18), 'spearman': SpearmanrResult(correlation=0.33152449702951986, pvalue=1.067384493207486e-20), 'nsamples': 750}, 'OnWN': {'pearson': (0.244935073870872, 1.0426631345271177e-11), 'spearman': SpearmanrResult(correlation=0.2698717590195484, pvalue=5.5472026685095957e-14), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3059807507705099, 1.0162841892516761e-17), 'spearman': SpearmanrResult(correlation=0.3295659657772263, pvalue=1.8489920511224776e-20), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.2607920323450474, 'wmean': 0.2629790761773118}, 'spearman': {'mean': 0.28647927288917546, 'wmean': 0.2863357456334645}}}, 'STS15': {'answers-forums': {'pearson': (0.3042770439226696, 1.7893081408804964e-09), 'spearman': SpearmanrResult(correlation=0.30386092735590337, pvalue=1.887268151655058e-09), 'nsamples': 375}, 'answers-students': {'pearson': (0.38476354539030927, 7.121846965609512e-28), 'spearman': SpearmanrResult(correlation=0.3841731811856881, pvalue=8.705049339282671e-28), 'nsamples': 750}, 'belief': {'pearson': (0.3544519993618435, 1.527941044031472e-12), 'spearman': SpearmanrResult(correlation=0.4049639839178593, pvalue=3.1227455498794134e-16), 'nsamples': 375}, 'headlines': {'pearson': (0.3293506250043086, 1.9636557635114354e-20), 'spearman': SpearmanrResult(correlation=0.3708674098863812, pvalue=7.210122133255127e-26), 'nsamples': 750}, 'images': {'pearson': (0.18997880731986594, 1.589170541820313e-07), 'spearman': SpearmanrResult(correlation=0.3370730534359489, pvalue=2.2024023418912765e-21), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.31256440419979936, 'wmean': 0.30836437483918505}, 'spearman': {'mean': 0.3601877111563562, 'wmean': 0.36163152503622487}}}, 'STS16': {'answer-answer': {'pearson': (0.2994910080344135, 1.1643597999007261e-06), 'spearman': SpearmanrResult(correlation=0.3599958013123607, pvalue=3.4551528795103045e-09), 'nsamples': 254}, 'headlines': {'pearson': (0.3620505576663733, 3.97618616495931e-09), 'spearman': SpearmanrResult(correlation=0.421493394628834, pvalue=3.816769776059078e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.3511346267423327, 4.4644007105714435e-08), 'spearman': SpearmanrResult(correlation=0.42127622159762307, pvalue=2.6100941476058378e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.5664669800484227, 4.14834034487168e-22), 'spearman': SpearmanrResult(correlation=0.7032134915127007, pvalue=1.0232883670989474e-37), 'nsamples': 244}, 'question-question': {'pearson': (0.23009230811004563, 0.0008036369613702031), 'spearman': SpearmanrResult(correlation=0.2516597842618491, pvalue=0.0002371806937571548), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.36184709612031757, 'wmean': 0.365336850402377}, 'spearman': {'mean': 0.4315277386626736, 'wmean': 0.4363113883669461}}}, 'MR': {'devacc': 67.96, 'acc': 68.81, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 67.67, 'acc': 68.13, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 79.91, 'acc': 81.62, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 89.87, 'acc': 89.7, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 74.31, 'acc': 75.67, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.6, 'acc': 38.78, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 43.57, 'acc': 62.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.23, 'acc': 66.43, 'f1': 79.83, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 66.2, 'acc': 65.94, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6934138272695507, 'pearson': 0.6927277226087686, 'spearman': 0.6272474148524694, 'mse': 0.5352644240487313, 'yhat': array([2.86071182, 3.52342626, 2.76511126, ..., 2.90512739, 4.10462923,        4.4931746 ]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6041423060828881, 'pearson': 0.5380553763211487, 'spearman': 0.5255586416665712, 'mse': 1.9826149596249012, 'yhat': array([2.07135638, 2.84204565, 2.77527885, ..., 3.8453881 , 3.51348694,        3.2195209 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.79, 'acc': 57.7, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 305.764, 'acc': [(26.320000000000004, 58.17999999999999, 73.61999999999999, 3.8), (21.416000000000004, 52.99600000000001, 70.352, 4.8)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 87.67, 'acc': 88.41, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 2.11, 'acc': 2.02, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.9, 'acc': 27.27, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 46.12, 'acc': 46.04, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 59.7, 'acc': 60.02, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 85.95, 'acc': 84.73, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 75.59, 'acc': 74.92, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 69.03, 'acc': 70.1, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 55.24, 'acc': 55.99, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.33, 'acc': 56.81, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 04:45:03,672 : STS12 p=0.3446, STS12 s=0.3491, STS13 p=0.1919, STS13 s=0.2204, STS14 p=0.2630, STS14 s=0.2863, STS15 p=0.3084, STS15 s=0.3616, STS 16 p=0.3653, STS16 s=0.4363, STS B p=0.5381, STS B s=0.5256, STS B m=1.9826, SICK-R p=0.6927, SICK-R s=0.6272, SICK-P m=0.5353
2019-02-14 04:45:03,672 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 04:45:03,673 : 0.3446,0.3491,0.1919,0.2204,0.2630,0.2863,0.3084,0.3616,0.3653,0.4363,0.5381,0.5256,1.9826,0.6927,0.6272,0.5353
2019-02-14 04:45:03,673 : MR=68.81, CR=68.13, SUBJ=89.70, MPQA=81.62, SST-B=75.67, SST-F=38.78, TREC=62.60, SICK-E=65.94, SNLI=57.70, MRPC=66.43, MRPC f=79.83
2019-02-14 04:45:03,673 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 04:45:03,673 : 68.81,68.13,89.70,81.62,75.67,38.78,62.60,65.94,57.70,66.43,79.83
2019-02-14 04:45:03,673 : COCO r1i2t=26.32, COCO r5i2t=58.18, COCO r10i2t=73.62, COCO medr_i2t=3.80, COCO r1t2i=21.42, COCO r5t2i=53.00, COCO r10t2i=70.35, COCO medr_t2i=4.80
2019-02-14 04:45:03,673 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 04:45:03,673 : 26.32,58.18,73.62,3.80,21.42,53.00,70.35,4.80
2019-02-14 04:45:03,673 : SentLen=88.41, WC=2.02, TreeDepth=27.27, TopConst=46.04, BShift=60.02, Tense=84.73, SubjNum=74.92, ObjNum=70.10, SOMO=55.99, CoordInv=56.81, average=56.63
2019-02-14 04:45:03,673 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 04:45:03,673 : 88.41,2.02,27.27,46.04,60.02,84.73,74.92,70.10,55.99,56.81,56.63
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,673 : layer 4
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,673 : ********************************************************************************
2019-02-14 04:45:03,766 : ***** Transfer task : STS12 *****


2019-02-14 04:45:03,780 : loading BERT model bert-base-uncased
2019-02-14 04:45:03,780 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:03,797 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:03,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfmhetj0b
2019-02-14 04:45:06,236 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:09,467 : MSRpar : pearson = 0.4247, spearman = 0.4286
2019-02-14 04:45:10,228 : MSRvid : pearson = 0.4137, spearman = 0.4678
2019-02-14 04:45:10,853 : SMTeuroparl : pearson = 0.4736, spearman = 0.5954
2019-02-14 04:45:12,040 : surprise.OnWN : pearson = 0.2283, spearman = 0.2420
2019-02-14 04:45:12,716 : surprise.SMTnews : pearson = 0.5458, spearman = 0.4661
2019-02-14 04:45:12,716 : ALL (weighted average) : Pearson = 0.3974,             Spearman = 0.4225
2019-02-14 04:45:12,716 : ALL (average) : Pearson = 0.4172,             Spearman = 0.4400

2019-02-14 04:45:12,716 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 04:45:12,726 : loading BERT model bert-base-uncased
2019-02-14 04:45:12,726 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:12,745 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:12,745 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprimxjfdu
2019-02-14 04:45:15,185 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:17,246 : FNWN : pearson = 0.2818, spearman = 0.2883
2019-02-14 04:45:18,130 : headlines : pearson = 0.2014, spearman = 0.3368
2019-02-14 04:45:18,809 : OnWN : pearson = 0.2568, spearman = 0.2942
2019-02-14 04:45:18,809 : ALL (weighted average) : Pearson = 0.2322,             Spearman = 0.3147
2019-02-14 04:45:18,809 : ALL (average) : Pearson = 0.2467,             Spearman = 0.3064

2019-02-14 04:45:18,809 : ***** Transfer task : STS14 *****


2019-02-14 04:45:18,824 : loading BERT model bert-base-uncased
2019-02-14 04:45:18,824 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:18,843 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:18,844 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw0x1bpok
2019-02-14 04:45:21,283 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:23,400 : deft-forum : pearson = 0.1982, spearman = 0.2454
2019-02-14 04:45:24,122 : deft-news : pearson = 0.4501, spearman = 0.5476
2019-02-14 04:45:25,106 : headlines : pearson = 0.2092, spearman = 0.2612
2019-02-14 04:45:26,084 : images : pearson = 0.5302, spearman = 0.5265
2019-02-14 04:45:27,045 : OnWN : pearson = 0.2751, spearman = 0.3017
2019-02-14 04:45:28,280 : tweet-news : pearson = 0.2959, spearman = 0.3511
2019-02-14 04:45:28,280 : ALL (weighted average) : Pearson = 0.3219,             Spearman = 0.3613
2019-02-14 04:45:28,280 : ALL (average) : Pearson = 0.3265,             Spearman = 0.3722

2019-02-14 04:45:28,280 : ***** Transfer task : STS15 *****


2019-02-14 04:45:28,346 : loading BERT model bert-base-uncased
2019-02-14 04:45:28,346 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:28,365 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:28,365 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps1wlwwia
2019-02-14 04:45:30,810 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:33,123 : answers-forums : pearson = 0.3921, spearman = 0.4094
2019-02-14 04:45:34,039 : answers-students : pearson = 0.3477, spearman = 0.3933
2019-02-14 04:45:34,909 : belief : pearson = 0.4150, spearman = 0.4575
2019-02-14 04:45:35,937 : headlines : pearson = 0.3194, spearman = 0.4108
2019-02-14 04:45:36,915 : images : pearson = 0.1949, spearman = 0.4838
2019-02-14 04:45:36,915 : ALL (weighted average) : Pearson = 0.3164,             Spearman = 0.4303
2019-02-14 04:45:36,915 : ALL (average) : Pearson = 0.3338,             Spearman = 0.4310

2019-02-14 04:45:36,915 : ***** Transfer task : STS16 *****


2019-02-14 04:45:36,990 : loading BERT model bert-base-uncased
2019-02-14 04:45:36,990 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:37,009 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:37,009 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptibpv_o9
2019-02-14 04:45:39,519 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:41,380 : answer-answer : pearson = 0.3656, spearman = 0.4976
2019-02-14 04:45:41,726 : headlines : pearson = 0.3408, spearman = 0.4755
2019-02-14 04:45:42,161 : plagiarism : pearson = 0.5014, spearman = 0.6544
2019-02-14 04:45:42,856 : postediting : pearson = 0.5983, spearman = 0.7807
2019-02-14 04:45:43,171 : question-question : pearson = 0.3619, spearman = 0.4077
2019-02-14 04:45:43,171 : ALL (weighted average) : Pearson = 0.4339,             Spearman = 0.5658
2019-02-14 04:45:43,171 : ALL (average) : Pearson = 0.4336,             Spearman = 0.5632

2019-02-14 04:45:43,172 : ***** Transfer task : MR *****


2019-02-14 04:45:43,189 : loading BERT model bert-base-uncased
2019-02-14 04:45:43,189 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:45:43,209 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:45:43,209 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvdu7b74h
2019-02-14 04:45:45,642 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:45:47,085 : Generating sentence embeddings
2019-02-14 04:46:00,664 : Generated sentence embeddings
2019-02-14 04:46:00,664 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 04:46:11,426 : Best param found at split 1: l2reg = 1e-05                 with score 69.35
2019-02-14 04:46:24,643 : Best param found at split 2: l2reg = 1e-05                 with score 70.15
2019-02-14 04:46:37,855 : Best param found at split 3: l2reg = 0.0001                 with score 70.48
2019-02-14 04:46:49,421 : Best param found at split 4: l2reg = 0.0001                 with score 70.19
2019-02-14 04:47:01,227 : Best param found at split 5: l2reg = 1e-05                 with score 70.4
2019-02-14 04:47:02,056 : Dev acc : 70.11 Test acc : 70.91

2019-02-14 04:47:02,057 : ***** Transfer task : CR *****


2019-02-14 04:47:02,065 : loading BERT model bert-base-uncased
2019-02-14 04:47:02,065 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:47:02,087 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:47:02,087 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8587dzbw
2019-02-14 04:47:04,556 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:47:06,029 : Generating sentence embeddings
2019-02-14 04:47:09,684 : Generated sentence embeddings
2019-02-14 04:47:09,684 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 04:47:14,049 : Best param found at split 1: l2reg = 1e-05                 with score 73.1
2019-02-14 04:47:18,748 : Best param found at split 2: l2reg = 0.0001                 with score 73.2
2019-02-14 04:47:23,638 : Best param found at split 3: l2reg = 1e-05                 with score 71.22
2019-02-14 04:47:28,352 : Best param found at split 4: l2reg = 1e-05                 with score 72.76
2019-02-14 04:47:32,128 : Best param found at split 5: l2reg = 1e-05                 with score 70.18
2019-02-14 04:47:32,295 : Dev acc : 72.09 Test acc : 71.44

2019-02-14 04:47:32,295 : ***** Transfer task : MPQA *****


2019-02-14 04:47:32,301 : loading BERT model bert-base-uncased
2019-02-14 04:47:32,301 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:47:32,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:47:32,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4kn2aas1
2019-02-14 04:47:34,767 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:47:36,242 : Generating sentence embeddings
2019-02-14 04:47:40,087 : Generated sentence embeddings
2019-02-14 04:47:40,087 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 04:47:49,200 : Best param found at split 1: l2reg = 1e-05                 with score 79.53
2019-02-14 04:47:58,361 : Best param found at split 2: l2reg = 1e-05                 with score 80.3
2019-02-14 04:48:09,125 : Best param found at split 3: l2reg = 0.0001                 with score 79.81
2019-02-14 04:48:20,807 : Best param found at split 4: l2reg = 1e-05                 with score 80.37
2019-02-14 04:48:33,859 : Best param found at split 5: l2reg = 1e-05                 with score 81.25
2019-02-14 04:48:34,651 : Dev acc : 80.25 Test acc : 82.01

2019-02-14 04:48:34,652 : ***** Transfer task : SUBJ *****


2019-02-14 04:48:34,667 : loading BERT model bert-base-uncased
2019-02-14 04:48:34,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:48:34,692 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:48:34,692 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoynwbycn
2019-02-14 04:48:37,139 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:48:38,572 : Generating sentence embeddings
2019-02-14 04:48:52,866 : Generated sentence embeddings
2019-02-14 04:48:52,867 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 04:49:05,595 : Best param found at split 1: l2reg = 1e-05                 with score 91.06
2019-02-14 04:49:18,941 : Best param found at split 2: l2reg = 1e-05                 with score 91.02
2019-02-14 04:49:31,697 : Best param found at split 3: l2reg = 1e-05                 with score 90.41
2019-02-14 04:49:45,426 : Best param found at split 4: l2reg = 1e-05                 with score 91.22
2019-02-14 04:49:59,371 : Best param found at split 5: l2reg = 0.0001                 with score 90.78
2019-02-14 04:50:00,156 : Dev acc : 90.9 Test acc : 90.28

2019-02-14 04:50:00,157 : ***** Transfer task : SST Binary classification *****


2019-02-14 04:50:00,298 : loading BERT model bert-base-uncased
2019-02-14 04:50:00,299 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:50:00,325 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:50:00,325 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_8ad3ec9
2019-02-14 04:50:02,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:50:04,240 : Computing embedding for train
2019-02-14 04:50:50,139 : Computed train embeddings
2019-02-14 04:50:50,139 : Computing embedding for dev
2019-02-14 04:50:51,077 : Computed dev embeddings
2019-02-14 04:50:51,077 : Computing embedding for test
2019-02-14 04:50:53,082 : Computed test embeddings
2019-02-14 04:50:53,082 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:51:14,147 : [('reg:1e-05', 76.49), ('reg:0.0001', 76.15), ('reg:0.001', 72.71), ('reg:0.01', 70.3)]
2019-02-14 04:51:14,147 : Validation : best param found is reg = 1e-05 with score             76.49
2019-02-14 04:51:14,147 : Evaluating...
2019-02-14 04:51:20,048 : 
Dev acc : 76.49 Test acc : 77.38 for             SST Binary classification

2019-02-14 04:51:20,049 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 04:51:20,103 : loading BERT model bert-base-uncased
2019-02-14 04:51:20,103 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:51:20,127 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:51:20,128 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphy3br431
2019-02-14 04:51:22,582 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:51:24,032 : Computing embedding for train
2019-02-14 04:51:34,449 : Computed train embeddings
2019-02-14 04:51:34,450 : Computing embedding for dev
2019-02-14 04:51:35,806 : Computed dev embeddings
2019-02-14 04:51:35,806 : Computing embedding for test
2019-02-14 04:51:38,215 : Computed test embeddings
2019-02-14 04:51:38,216 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:51:41,024 : [('reg:1e-05', 36.88), ('reg:0.0001', 37.87), ('reg:0.001', 33.79), ('reg:0.01', 26.25)]
2019-02-14 04:51:41,024 : Validation : best param found is reg = 0.0001 with score             37.87
2019-02-14 04:51:41,024 : Evaluating...
2019-02-14 04:51:41,767 : 
Dev acc : 37.87 Test acc : 38.55 for             SST Fine-Grained classification

2019-02-14 04:51:41,767 : ***** Transfer task : TREC *****


2019-02-14 04:51:41,781 : loading BERT model bert-base-uncased
2019-02-14 04:51:41,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:51:41,800 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:51:41,801 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxi010th1
2019-02-14 04:51:44,238 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:51:49,121 : Computed train embeddings
2019-02-14 04:51:49,388 : Computed test embeddings
2019-02-14 04:51:49,388 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 04:51:57,578 : [('reg:1e-05', 49.93), ('reg:0.0001', 46.5), ('reg:0.001', 37.6), ('reg:0.01', 25.9)]
2019-02-14 04:51:57,578 : Cross-validation : best param found is reg = 1e-05             with score 49.93
2019-02-14 04:51:57,578 : Evaluating...
2019-02-14 04:51:58,047 : 
Dev acc : 49.93 Test acc : 70.2             for TREC

2019-02-14 04:51:58,048 : ***** Transfer task : MRPC *****


2019-02-14 04:51:58,070 : loading BERT model bert-base-uncased
2019-02-14 04:51:58,070 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:51:58,092 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:51:58,093 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6z1sguf2
2019-02-14 04:52:00,537 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:52:02,081 : Computing embedding for train
2019-02-14 04:52:11,696 : Computed train embeddings
2019-02-14 04:52:11,697 : Computing embedding for test
2019-02-14 04:52:15,841 : Computed test embeddings
2019-02-14 04:52:15,857 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 04:52:21,119 : [('reg:1e-05', 71.05), ('reg:0.0001', 71.1), ('reg:0.001', 68.23), ('reg:0.01', 68.15)]
2019-02-14 04:52:21,119 : Cross-validation : best param found is reg = 0.0001             with score 71.1
2019-02-14 04:52:21,119 : Evaluating...
2019-02-14 04:52:21,479 : Dev acc : 71.1 Test acc 70.9; Test F1 77.41 for MRPC.

2019-02-14 04:52:21,479 : ***** Transfer task : SICK-Entailment*****


2019-02-14 04:52:21,549 : loading BERT model bert-base-uncased
2019-02-14 04:52:21,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:52:21,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:52:21,572 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpethblcmy
2019-02-14 04:52:24,051 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:52:25,513 : Computing embedding for train
2019-02-14 04:52:31,219 : Computed train embeddings
2019-02-14 04:52:31,220 : Computing embedding for dev
2019-02-14 04:52:31,974 : Computed dev embeddings
2019-02-14 04:52:31,974 : Computing embedding for test
2019-02-14 04:52:38,080 : Computed test embeddings
2019-02-14 04:52:38,108 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 04:52:39,740 : [('reg:1e-05', 67.6), ('reg:0.0001', 63.8), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-14 04:52:39,741 : Validation : best param found is reg = 1e-05 with score             67.6
2019-02-14 04:52:39,741 : Evaluating...
2019-02-14 04:52:40,272 : 
Dev acc : 67.6 Test acc : 66.21 for                        SICK entailment

2019-02-14 04:52:40,273 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 04:52:40,300 : loading BERT model bert-base-uncased
2019-02-14 04:52:40,300 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:52:40,356 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:52:40,356 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2u1j_qa_
2019-02-14 04:52:42,800 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:52:44,214 : Computing embedding for train
2019-02-14 04:52:49,307 : Computed train embeddings
2019-02-14 04:52:49,307 : Computing embedding for dev
2019-02-14 04:52:49,989 : Computed dev embeddings
2019-02-14 04:52:49,989 : Computing embedding for test
2019-02-14 04:52:55,536 : Computed test embeddings
2019-02-14 04:54:01,762 : Dev : Pearson 0.7340140961446187
2019-02-14 04:54:01,762 : Test : Pearson 0.7420884179577911 Spearman 0.6807010038962058 MSE 0.46158558783285336                        for SICK Relatedness

2019-02-14 04:54:01,763 : 

***** Transfer task : STSBenchmark*****


2019-02-14 04:54:01,832 : loading BERT model bert-base-uncased
2019-02-14 04:54:01,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:54:01,854 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:54:01,854 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwizdfgdj
2019-02-14 04:54:04,297 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:54:05,770 : Computing embedding for train
2019-02-14 04:54:14,071 : Computed train embeddings
2019-02-14 04:54:14,071 : Computing embedding for dev
2019-02-14 04:54:16,543 : Computed dev embeddings
2019-02-14 04:54:16,543 : Computing embedding for test
2019-02-14 04:54:18,546 : Computed test embeddings
2019-02-14 04:55:13,270 : Dev : Pearson 0.6779534773581283
2019-02-14 04:55:13,270 : Test : Pearson 0.6105704575956474 Spearman 0.6060365842232175 MSE 1.6944107535483328                        for SICK Relatedness

2019-02-14 04:55:13,270 : ***** Transfer task : SNLI Entailment*****


2019-02-14 04:55:18,206 : loading BERT model bert-base-uncased
2019-02-14 04:55:18,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 04:55:18,338 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 04:55:18,338 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppeh1p7di
2019-02-14 04:55:20,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 04:55:22,478 : PROGRESS (encoding): 0.00%
2019-02-14 04:56:41,790 : PROGRESS (encoding): 14.56%
2019-02-14 04:58:08,980 : PROGRESS (encoding): 29.12%
2019-02-14 04:59:38,324 : PROGRESS (encoding): 43.69%
2019-02-14 05:01:14,755 : PROGRESS (encoding): 58.25%
2019-02-14 05:03:01,501 : PROGRESS (encoding): 72.81%
2019-02-14 05:04:46,337 : PROGRESS (encoding): 87.37%
2019-02-14 05:06:37,438 : PROGRESS (encoding): 0.00%
2019-02-14 05:06:51,043 : PROGRESS (encoding): 0.00%
2019-02-14 05:07:04,210 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 05:07:57,631 : [('reg:1e-09', 60.07)]
2019-02-14 05:07:57,631 : Validation : best param found is reg = 1e-09 with score             60.07
2019-02-14 05:07:57,631 : Evaluating...
2019-02-14 05:08:52,069 : Dev acc : 60.07 Test acc : 59.52 for SNLI

2019-02-14 05:08:52,069 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 05:09:00,744 : loading BERT model bert-base-uncased
2019-02-14 05:09:00,745 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 05:09:00,792 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 05:09:00,792 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl1tkyb64
2019-02-14 05:09:03,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 05:09:04,687 : Computing embedding for train
2019-02-14 05:16:43,774 : Computed train embeddings
2019-02-14 05:16:43,774 : Computing embedding for dev
2019-02-14 05:17:03,681 : Computed dev embeddings
2019-02-14 05:17:03,681 : Computing embedding for test
2019-02-14 05:17:23,820 : Computed test embeddings
2019-02-14 05:17:23,836 : prepare data
2019-02-14 05:17:23,901 : start epoch
2019-02-14 05:18:04,897 : samples : 64000
2019-02-14 05:18:15,201 : Image to text: 4.18, 14.16, 21.98, 42.0
2019-02-14 05:18:22,621 : Text to Image: 3.084, 11.392, 18.688, 50.0
2019-02-14 05:19:04,072 : samples : 128000
2019-02-14 05:19:14,320 : Image to text: 5.88, 18.0, 27.02, 32.0
2019-02-14 05:19:21,733 : Text to Image: 3.752, 13.224, 21.456, 42.0
2019-02-14 05:20:03,320 : samples : 192000
2019-02-14 05:20:13,606 : Image to text: 6.12, 19.48, 29.62, 30.0
2019-02-14 05:20:21,018 : Text to Image: 4.556, 15.448, 24.368, 36.0
2019-02-14 05:21:02,866 : samples : 256000
2019-02-14 05:21:13,176 : Image to text: 6.04, 19.52, 29.36, 29.0
2019-02-14 05:21:20,612 : Text to Image: 4.324, 15.472, 24.324, 36.0
2019-02-14 05:22:02,420 : samples : 320000
2019-02-14 05:22:12,739 : Image to text: 5.82, 17.58, 27.32, 32.0
2019-02-14 05:22:20,193 : Text to Image: 3.256, 12.268, 20.2, 44.0
2019-02-14 05:23:01,952 : samples : 384000
2019-02-14 05:23:12,278 : Image to text: 6.82, 21.54, 31.94, 25.0
2019-02-14 05:23:19,684 : Text to Image: 5.4, 17.404, 27.172, 31.0
2019-02-14 05:24:02,057 : samples : 448000
2019-02-14 05:24:12,368 : Image to text: 7.36, 21.62, 32.74, 25.0
2019-02-14 05:24:19,820 : Text to Image: 5.3, 17.584, 27.34, 30.0
2019-02-14 05:25:01,925 : samples : 512000
2019-02-14 05:25:12,211 : Image to text: 6.58, 20.42, 31.32, 25.0
2019-02-14 05:25:19,627 : Text to Image: 4.82, 16.972, 26.344, 32.0
2019-02-14 05:25:55,232 : Epoch 1 finished
2019-02-14 05:25:55,677 : Image to text: 20.6, 48.8, 64.8, 6.0
2019-02-14 05:25:56,008 : Text to Image: 15.88, 42.68, 58.72, 8.0
2019-02-14 05:25:56,438 : Image to text: 21.7, 48.5, 65.3, 6.0
2019-02-14 05:25:56,768 : Text to Image: 15.54, 41.6, 60.08, 7.0
2019-02-14 05:25:57,209 : Image to text: 19.4, 47.8, 66.1, 6.0
2019-02-14 05:25:57,538 : Text to Image: 15.0, 42.76, 60.02, 7.0
2019-02-14 05:25:57,966 : Image to text: 20.8, 52.8, 67.0, 5.0
2019-02-14 05:25:58,296 : Text to Image: 15.34, 42.7, 60.0, 7.0
2019-02-14 05:25:58,723 : Image to text: 20.9, 50.6, 66.3, 5.0
2019-02-14 05:25:59,054 : Text to Image: 15.16, 43.36, 60.24, 7.0
2019-02-14 05:25:59,054 : Dev mean Text to Image: 15.383999999999999, 42.620000000000005, 59.812, 7.200000000000001
2019-02-14 05:25:59,054 : Dev mean Image to text: 20.68, 49.7, 65.89999999999999, 5.6
2019-02-14 05:25:59,054 : start epoch
2019-02-14 05:26:42,798 : samples : 64000
2019-02-14 05:26:53,098 : Image to text: 7.42, 23.4, 33.72, 23.0
2019-02-14 05:27:00,483 : Text to Image: 5.776, 18.696, 28.868, 28.0
2019-02-14 05:27:41,973 : samples : 128000
2019-02-14 05:27:52,332 : Image to text: 7.54, 23.28, 35.06, 22.0
2019-02-14 05:27:59,791 : Text to Image: 6.364, 19.672, 30.296, 26.0
2019-02-14 05:28:40,863 : samples : 192000
2019-02-14 05:28:51,240 : Image to text: 7.34, 22.66, 33.2, 23.0
2019-02-14 05:28:58,617 : Text to Image: 5.448, 18.336, 27.996, 30.0
2019-02-14 05:29:40,478 : samples : 256000
2019-02-14 05:29:50,703 : Image to text: 7.98, 24.26, 34.82, 22.0
2019-02-14 05:29:58,104 : Text to Image: 6.636, 20.736, 31.232, 25.0
2019-02-14 05:30:39,603 : samples : 320000
2019-02-14 05:30:50,011 : Image to text: 8.02, 25.08, 36.34, 20.0
2019-02-14 05:30:57,430 : Text to Image: 6.42, 20.076, 30.44, 27.0
2019-02-14 05:31:38,632 : samples : 384000
2019-02-14 05:31:48,938 : Image to text: 7.8, 23.34, 34.76, 22.0
2019-02-14 05:31:56,317 : Text to Image: 5.872, 19.096, 29.788, 27.0
2019-02-14 05:32:38,104 : samples : 448000
2019-02-14 05:32:48,424 : Image to text: 7.14, 22.32, 33.34, 24.0
2019-02-14 05:32:55,826 : Text to Image: 5.552, 17.888, 28.008, 29.0
2019-02-14 05:33:37,057 : samples : 512000
2019-02-14 05:33:47,406 : Image to text: 8.52, 25.12, 36.44, 20.0
2019-02-14 05:33:54,865 : Text to Image: 7.1, 21.416, 32.1, 24.0
2019-02-14 05:34:30,662 : Epoch 2 finished
2019-02-14 05:34:31,101 : Image to text: 19.2, 52.0, 68.4, 5.0
2019-02-14 05:34:31,432 : Text to Image: 17.54, 46.44, 63.08, 6.0
2019-02-14 05:34:31,862 : Image to text: 23.2, 51.3, 70.2, 5.0
2019-02-14 05:34:32,191 : Text to Image: 16.36, 46.02, 63.42, 6.0
2019-02-14 05:34:32,620 : Image to text: 23.4, 50.9, 67.0, 5.0
2019-02-14 05:34:32,949 : Text to Image: 17.18, 46.82, 63.9, 6.0
2019-02-14 05:34:33,391 : Image to text: 22.3, 53.5, 68.3, 5.0
2019-02-14 05:34:33,719 : Text to Image: 16.08, 45.52, 63.72, 7.0
2019-02-14 05:34:34,145 : Image to text: 22.8, 54.8, 70.5, 5.0
2019-02-14 05:34:34,473 : Text to Image: 17.16, 45.86, 63.7, 6.0
2019-02-14 05:34:34,473 : Dev mean Text to Image: 16.863999999999997, 46.132000000000005, 63.564, 6.2
2019-02-14 05:34:34,474 : Dev mean Image to text: 22.18, 52.5, 68.88, 5.0
2019-02-14 05:34:34,474 : start epoch
2019-02-14 05:35:16,989 : samples : 64000
2019-02-14 05:35:27,408 : Image to text: 7.84, 23.4, 34.76, 21.0
2019-02-14 05:35:34,835 : Text to Image: 6.092, 19.284, 29.508, 27.0
2019-02-14 05:36:17,566 : samples : 128000
2019-02-14 05:36:27,959 : Image to text: 8.0, 24.56, 36.1, 20.0
2019-02-14 05:36:35,365 : Text to Image: 6.28, 19.872, 30.9, 26.0
2019-02-14 05:37:17,763 : samples : 192000
2019-02-14 05:37:28,167 : Image to text: 7.68, 24.32, 36.36, 20.0
2019-02-14 05:37:35,637 : Text to Image: 6.604, 20.572, 30.996, 25.0
2019-02-14 05:38:17,862 : samples : 256000
2019-02-14 05:38:28,245 : Image to text: 8.54, 25.52, 36.5, 20.0
2019-02-14 05:38:35,722 : Text to Image: 6.588, 20.76, 31.304, 26.0
2019-02-14 05:39:17,907 : samples : 320000
2019-02-14 05:39:28,287 : Image to text: 8.58, 25.48, 36.66, 20.0
2019-02-14 05:39:35,767 : Text to Image: 6.664, 20.628, 30.828, 26.0
2019-02-14 05:40:17,448 : samples : 384000
2019-02-14 05:40:27,770 : Image to text: 8.88, 26.44, 37.08, 19.0
2019-02-14 05:40:35,239 : Text to Image: 6.924, 21.288, 31.992, 25.0
2019-02-14 05:41:16,627 : samples : 448000
2019-02-14 05:41:26,864 : Image to text: 9.2, 26.04, 37.36, 19.0
2019-02-14 05:41:34,239 : Text to Image: 6.804, 21.504, 32.076, 23.0
2019-02-14 05:42:15,874 : samples : 512000
2019-02-14 05:42:26,132 : Image to text: 8.92, 27.18, 38.46, 18.0
2019-02-14 05:42:33,506 : Text to Image: 7.332, 22.332, 33.788, 23.0
2019-02-14 05:43:08,442 : Epoch 3 finished
2019-02-14 05:43:08,873 : Image to text: 21.8, 53.5, 69.6, 5.0
2019-02-14 05:43:09,197 : Text to Image: 19.34, 48.96, 66.96, 6.0
2019-02-14 05:43:09,616 : Image to text: 22.7, 53.2, 70.1, 5.0
2019-02-14 05:43:09,942 : Text to Image: 18.5, 49.12, 66.48, 6.0
2019-02-14 05:43:10,382 : Image to text: 23.9, 54.3, 70.0, 5.0
2019-02-14 05:43:10,711 : Text to Image: 18.42, 49.82, 67.22, 6.0
2019-02-14 05:43:11,141 : Image to text: 25.0, 56.5, 71.9, 4.0
2019-02-14 05:43:11,472 : Text to Image: 17.92, 49.52, 67.66, 6.0
2019-02-14 05:43:11,919 : Image to text: 24.4, 55.7, 73.3, 5.0
2019-02-14 05:43:12,251 : Text to Image: 18.76, 49.74, 67.14, 6.0
2019-02-14 05:43:12,251 : Dev mean Text to Image: 18.587999999999997, 49.431999999999995, 67.092, 6.0
2019-02-14 05:43:12,251 : Dev mean Image to text: 23.56, 54.64, 70.98, 4.8
2019-02-14 05:43:12,251 : start epoch
2019-02-14 05:43:52,843 : samples : 64000
2019-02-14 05:44:03,085 : Image to text: 9.28, 27.08, 37.66, 18.0
2019-02-14 05:44:10,458 : Text to Image: 7.412, 22.96, 34.14, 22.0
2019-02-14 05:44:51,203 : samples : 128000
2019-02-14 05:45:01,458 : Image to text: 9.5, 27.46, 38.7, 18.0
2019-02-14 05:45:08,883 : Text to Image: 7.244, 22.836, 34.056, 22.0
2019-02-14 05:45:52,501 : samples : 192000
2019-02-14 05:46:02,871 : Image to text: 8.96, 26.64, 38.62, 18.0
2019-02-14 05:46:10,321 : Text to Image: 7.224, 22.3, 33.284, 23.0
2019-02-14 05:46:51,559 : samples : 256000
2019-02-14 05:47:01,839 : Image to text: 8.98, 26.42, 37.46, 19.0
2019-02-14 05:47:09,232 : Text to Image: 7.188, 21.608, 32.5, 24.0
2019-02-14 05:47:51,712 : samples : 320000
2019-02-14 05:48:01,998 : Image to text: 8.74, 26.34, 39.52, 18.0
2019-02-14 05:48:09,406 : Text to Image: 7.632, 22.524, 33.9, 22.0
2019-02-14 05:48:51,863 : samples : 384000
2019-02-14 05:49:02,109 : Image to text: 8.5, 25.78, 37.46, 18.0
2019-02-14 05:49:09,504 : Text to Image: 7.136, 21.748, 32.764, 23.0
2019-02-14 05:49:50,585 : samples : 448000
2019-02-14 05:50:00,876 : Image to text: 9.7, 27.12, 39.66, 17.0
2019-02-14 05:50:08,285 : Text to Image: 7.392, 22.488, 33.872, 22.0
2019-02-14 05:50:48,435 : samples : 512000
2019-02-14 05:50:58,677 : Image to text: 9.04, 27.42, 39.12, 17.0
2019-02-14 05:51:06,039 : Text to Image: 7.576, 23.364, 34.6, 21.0
2019-02-14 05:51:40,349 : Epoch 4 finished
2019-02-14 05:51:40,781 : Image to text: 24.1, 55.9, 71.7, 4.0
2019-02-14 05:51:41,108 : Text to Image: 19.86, 50.14, 67.4, 5.0
2019-02-14 05:51:41,534 : Image to text: 23.0, 56.0, 72.7, 4.0
2019-02-14 05:51:41,862 : Text to Image: 19.3, 48.78, 66.9, 6.0
2019-02-14 05:51:42,303 : Image to text: 25.2, 57.0, 71.6, 4.0
2019-02-14 05:51:42,632 : Text to Image: 18.88, 49.96, 67.42, 6.0
2019-02-14 05:51:43,060 : Image to text: 27.3, 57.3, 73.4, 4.0
2019-02-14 05:51:43,388 : Text to Image: 18.88, 49.96, 67.12, 6.0
2019-02-14 05:51:43,827 : Image to text: 27.2, 58.9, 73.6, 4.0
2019-02-14 05:51:44,154 : Text to Image: 18.78, 50.32, 66.34, 5.0
2019-02-14 05:51:44,154 : Dev mean Text to Image: 19.14, 49.832, 67.036, 5.6000000000000005
2019-02-14 05:51:44,154 : Dev mean Image to text: 25.36, 57.02, 72.6, 4.0
2019-02-14 05:51:44,155 : start epoch
2019-02-14 05:52:24,606 : samples : 64000
2019-02-14 05:52:34,836 : Image to text: 9.64, 27.56, 40.38, 17.0
2019-02-14 05:52:42,219 : Text to Image: 7.892, 23.572, 34.628, 21.0
2019-02-14 05:53:22,402 : samples : 128000
2019-02-14 05:53:32,770 : Image to text: 9.8, 26.56, 38.48, 17.0
2019-02-14 05:53:40,169 : Text to Image: 8.012, 23.376, 34.696, 21.0
2019-02-14 05:54:20,442 : samples : 192000
2019-02-14 05:54:30,672 : Image to text: 9.58, 26.36, 38.92, 18.0
2019-02-14 05:54:38,034 : Text to Image: 6.896, 21.68, 32.348, 24.0
2019-02-14 05:55:18,530 : samples : 256000
2019-02-14 05:55:28,781 : Image to text: 9.56, 27.34, 39.74, 18.0
2019-02-14 05:55:36,146 : Text to Image: 7.94, 24.176, 35.608, 20.0
2019-02-14 05:56:17,838 : samples : 320000
2019-02-14 05:56:28,225 : Image to text: 9.68, 27.02, 39.12, 17.0
2019-02-14 05:56:35,695 : Text to Image: 7.208, 22.648, 33.676, 22.0
2019-02-14 05:57:17,094 : samples : 384000
2019-02-14 05:57:27,469 : Image to text: 9.64, 27.12, 40.44, 16.0
2019-02-14 05:57:34,886 : Text to Image: 7.62, 22.6, 33.696, 22.0
2019-02-14 05:58:15,432 : samples : 448000
2019-02-14 05:58:25,827 : Image to text: 9.42, 26.7, 38.44, 18.0
2019-02-14 05:58:33,283 : Text to Image: 7.636, 23.204, 34.628, 21.0
2019-02-14 05:59:13,952 : samples : 512000
2019-02-14 05:59:24,199 : Image to text: 10.3, 28.5, 40.18, 16.0
2019-02-14 05:59:31,621 : Text to Image: 8.04, 23.812, 35.248, 21.0
2019-02-14 06:00:06,677 : Epoch 5 finished
2019-02-14 06:00:07,107 : Image to text: 25.8, 55.9, 71.9, 4.0
2019-02-14 06:00:07,437 : Text to Image: 21.06, 53.1, 70.46, 5.0
2019-02-14 06:00:07,877 : Image to text: 24.2, 56.4, 72.0, 4.0
2019-02-14 06:00:08,206 : Text to Image: 21.18, 52.6, 69.56, 5.0
2019-02-14 06:00:08,633 : Image to text: 25.8, 57.9, 73.1, 4.0
2019-02-14 06:00:08,963 : Text to Image: 20.72, 52.96, 70.58, 5.0
2019-02-14 06:00:09,406 : Image to text: 25.9, 58.8, 73.8, 4.0
2019-02-14 06:00:09,736 : Text to Image: 20.84, 53.62, 71.26, 5.0
2019-02-14 06:00:10,165 : Image to text: 26.0, 59.2, 74.2, 4.0
2019-02-14 06:00:10,495 : Text to Image: 21.38, 53.48, 70.38, 5.0
2019-02-14 06:00:10,495 : Dev mean Text to Image: 21.036, 53.152, 70.448, 5.0
2019-02-14 06:00:10,495 : Dev mean Image to text: 25.54, 57.64, 73.0, 4.0
2019-02-14 06:00:10,495 : start epoch
2019-02-14 06:00:52,826 : samples : 64000
2019-02-14 06:01:03,150 : Image to text: 9.56, 28.5, 40.68, 17.0
2019-02-14 06:01:10,593 : Text to Image: 7.772, 23.572, 34.86, 21.0
2019-02-14 06:01:51,519 : samples : 128000
2019-02-14 06:02:01,832 : Image to text: 9.04, 25.8, 37.96, 19.0
2019-02-14 06:02:09,267 : Text to Image: 7.236, 22.516, 33.548, 22.0
2019-02-14 06:02:50,843 : samples : 192000
2019-02-14 06:03:01,106 : Image to text: 9.8, 27.74, 39.04, 17.0
2019-02-14 06:03:08,493 : Text to Image: 8.072, 23.384, 34.916, 21.0
2019-02-14 06:03:48,805 : samples : 256000
2019-02-14 06:03:59,066 : Image to text: 10.36, 27.92, 40.12, 16.0
2019-02-14 06:04:06,474 : Text to Image: 8.312, 23.892, 35.692, 20.0
2019-02-14 06:04:48,088 : samples : 320000
2019-02-14 06:04:58,373 : Image to text: 9.02, 26.56, 39.02, 17.0
2019-02-14 06:05:05,790 : Text to Image: 7.672, 23.188, 34.34, 22.0
2019-02-14 06:05:46,328 : samples : 384000
2019-02-14 06:05:56,628 : Image to text: 10.38, 27.2, 39.72, 16.0
2019-02-14 06:06:04,041 : Text to Image: 7.928, 23.828, 35.024, 21.0
2019-02-14 06:06:44,474 : samples : 448000
2019-02-14 06:06:54,765 : Image to text: 10.22, 28.2, 40.44, 17.0
2019-02-14 06:07:02,187 : Text to Image: 8.172, 24.0, 35.624, 20.0
2019-02-14 06:07:42,479 : samples : 512000
2019-02-14 06:07:52,700 : Image to text: 10.08, 27.94, 40.4, 17.0
2019-02-14 06:08:00,082 : Text to Image: 7.288, 22.336, 33.752, 22.0
2019-02-14 06:08:34,445 : Epoch 6 finished
2019-02-14 06:08:34,871 : Image to text: 24.0, 55.6, 72.9, 4.0
2019-02-14 06:08:35,200 : Text to Image: 20.4, 52.58, 69.44, 5.0
2019-02-14 06:08:35,640 : Image to text: 25.2, 60.1, 71.6, 4.0
2019-02-14 06:08:35,970 : Text to Image: 19.86, 51.86, 69.62, 5.0
2019-02-14 06:08:36,404 : Image to text: 26.0, 57.9, 72.0, 4.0
2019-02-14 06:08:36,737 : Text to Image: 20.78, 51.98, 69.58, 5.0
2019-02-14 06:08:37,188 : Image to text: 25.9, 60.1, 75.9, 4.0
2019-02-14 06:08:37,521 : Text to Image: 20.12, 52.38, 69.7, 5.0
2019-02-14 06:08:37,955 : Image to text: 26.8, 58.6, 75.8, 4.0
2019-02-14 06:08:38,298 : Text to Image: 21.24, 52.28, 69.26, 5.0
2019-02-14 06:08:38,298 : Dev mean Text to Image: 20.479999999999997, 52.215999999999994, 69.52000000000001, 5.0
2019-02-14 06:08:38,298 : Dev mean Image to text: 25.58, 58.459999999999994, 73.64, 4.0
2019-02-14 06:08:38,298 : start epoch
2019-02-14 06:09:19,227 : samples : 64000
2019-02-14 06:09:29,455 : Image to text: 9.5, 27.62, 40.44, 16.0
2019-02-14 06:09:36,861 : Text to Image: 7.996, 24.188, 35.792, 20.0
2019-02-14 06:10:17,156 : samples : 128000
2019-02-14 06:10:27,528 : Image to text: 9.44, 27.08, 39.14, 18.0
2019-02-14 06:10:34,956 : Text to Image: 7.208, 22.06, 33.072, 23.0
2019-02-14 06:11:15,435 : samples : 192000
2019-02-14 06:11:25,763 : Image to text: 9.82, 27.48, 40.38, 17.0
2019-02-14 06:11:33,194 : Text to Image: 8.184, 23.88, 35.292, 21.0
2019-02-14 06:12:15,279 : samples : 256000
2019-02-14 06:12:25,649 : Image to text: 10.04, 28.02, 40.12, 17.0
2019-02-14 06:12:33,101 : Text to Image: 8.144, 23.732, 35.044, 21.0
2019-02-14 06:13:14,786 : samples : 320000
2019-02-14 06:13:25,045 : Image to text: 10.0, 28.34, 39.56, 17.0
2019-02-14 06:13:32,453 : Text to Image: 7.804, 23.636, 35.012, 21.0
2019-02-14 06:14:12,989 : samples : 384000
2019-02-14 06:14:23,369 : Image to text: 9.4, 27.48, 39.38, 17.0
2019-02-14 06:14:30,784 : Text to Image: 7.516, 23.184, 34.8, 21.0
2019-02-14 06:15:12,047 : samples : 448000
2019-02-14 06:15:22,354 : Image to text: 9.7, 26.98, 39.34, 17.0
2019-02-14 06:15:29,766 : Text to Image: 8.152, 24.056, 35.636, 20.0
2019-02-14 06:16:10,770 : samples : 512000
2019-02-14 06:16:21,040 : Image to text: 10.32, 28.3, 41.14, 16.0
2019-02-14 06:16:28,493 : Text to Image: 7.88, 24.216, 35.776, 20.0
2019-02-14 06:17:03,005 : Epoch 7 finished
2019-02-14 06:17:03,424 : Image to text: 24.5, 56.0, 72.7, 4.0
2019-02-14 06:17:03,749 : Text to Image: 21.58, 54.5, 72.48, 5.0
2019-02-14 06:17:04,168 : Image to text: 26.9, 58.3, 73.3, 4.0
2019-02-14 06:17:04,492 : Text to Image: 22.66, 54.62, 71.4, 5.0
2019-02-14 06:17:04,923 : Image to text: 25.4, 56.9, 74.9, 4.0
2019-02-14 06:17:05,249 : Text to Image: 21.52, 54.96, 72.14, 5.0
2019-02-14 06:17:05,675 : Image to text: 26.2, 58.9, 75.3, 4.0
2019-02-14 06:17:06,003 : Text to Image: 20.76, 54.8, 72.36, 5.0
2019-02-14 06:17:06,443 : Image to text: 26.8, 60.7, 74.2, 4.0
2019-02-14 06:17:06,772 : Text to Image: 22.36, 53.86, 70.46, 5.0
2019-02-14 06:17:06,772 : Dev mean Text to Image: 21.775999999999996, 54.548, 71.768, 5.0
2019-02-14 06:17:06,773 : Dev mean Image to text: 25.96, 58.16, 74.08000000000001, 4.0
2019-02-14 06:17:06,773 : start epoch
2019-02-14 06:17:47,469 : samples : 64000
2019-02-14 06:17:57,852 : Image to text: 9.48, 27.94, 39.56, 17.0
2019-02-14 06:18:05,301 : Text to Image: 7.38, 22.616, 33.568, 22.0
2019-02-14 06:18:45,723 : samples : 128000
2019-02-14 06:18:56,023 : Image to text: 10.02, 28.66, 41.46, 16.0
2019-02-14 06:19:03,472 : Text to Image: 8.388, 24.672, 36.356, 19.0
2019-02-14 06:19:45,432 : samples : 192000
2019-02-14 06:19:55,746 : Image to text: 10.12, 29.96, 42.4, 15.0
2019-02-14 06:20:03,189 : Text to Image: 8.464, 24.612, 36.04, 20.0
2019-02-14 06:20:43,724 : samples : 256000
2019-02-14 06:20:54,024 : Image to text: 9.54, 26.66, 38.46, 17.0
2019-02-14 06:21:01,439 : Text to Image: 7.736, 23.436, 35.076, 21.0
2019-02-14 06:21:42,139 : samples : 320000
2019-02-14 06:21:52,428 : Image to text: 10.92, 29.72, 41.62, 15.0
2019-02-14 06:21:59,847 : Text to Image: 9.084, 25.748, 37.992, 19.0
2019-02-14 06:22:40,489 : samples : 384000
2019-02-14 06:22:50,864 : Image to text: 10.5, 28.62, 41.14, 16.0
2019-02-14 06:22:58,289 : Text to Image: 8.272, 23.564, 35.508, 20.0
2019-02-14 06:23:38,713 : samples : 448000
2019-02-14 06:23:49,023 : Image to text: 11.52, 29.12, 41.68, 15.0
2019-02-14 06:23:56,471 : Text to Image: 8.528, 24.82, 36.296, 19.0
2019-02-14 06:24:37,393 : samples : 512000
2019-02-14 06:24:47,791 : Image to text: 9.64, 27.74, 39.88, 17.0
2019-02-14 06:24:55,282 : Text to Image: 8.004, 24.004, 35.94, 20.0
2019-02-14 06:25:29,975 : Epoch 8 finished
2019-02-14 06:25:30,405 : Image to text: 25.6, 60.1, 75.4, 4.0
2019-02-14 06:25:30,735 : Text to Image: 21.98, 55.16, 72.24, 5.0
2019-02-14 06:25:31,176 : Image to text: 26.4, 61.3, 75.7, 4.0
2019-02-14 06:25:31,506 : Text to Image: 21.74, 53.8, 71.1, 5.0
2019-02-14 06:25:31,936 : Image to text: 27.8, 59.2, 74.2, 4.0
2019-02-14 06:25:32,267 : Text to Image: 21.9, 54.84, 71.88, 5.0
2019-02-14 06:25:32,717 : Image to text: 27.9, 60.7, 75.8, 4.0
2019-02-14 06:25:33,051 : Text to Image: 22.34, 55.22, 71.94, 5.0
2019-02-14 06:25:33,487 : Image to text: 30.4, 60.8, 76.1, 3.0
2019-02-14 06:25:33,820 : Text to Image: 22.36, 54.76, 71.0, 5.0
2019-02-14 06:25:33,820 : Dev mean Text to Image: 22.064, 54.756, 71.63199999999999, 5.0
2019-02-14 06:25:33,820 : Dev mean Image to text: 27.619999999999997, 60.42, 75.44, 3.8000000000000003
2019-02-14 06:25:33,820 : start epoch
2019-02-14 06:26:16,083 : samples : 64000
2019-02-14 06:26:26,375 : Image to text: 10.32, 29.02, 41.72, 15.0
2019-02-14 06:26:33,782 : Text to Image: 8.448, 25.088, 37.12, 19.0
2019-02-14 06:27:15,986 : samples : 128000
2019-02-14 06:27:26,237 : Image to text: 10.48, 28.68, 41.36, 16.0
2019-02-14 06:27:33,610 : Text to Image: 7.932, 23.724, 35.204, 21.0
2019-02-14 06:28:15,913 : samples : 192000
2019-02-14 06:28:26,177 : Image to text: 10.18, 28.54, 40.7, 16.0
2019-02-14 06:28:33,558 : Text to Image: 8.032, 23.768, 35.032, 21.0
2019-02-14 06:29:15,932 : samples : 256000
2019-02-14 06:29:26,247 : Image to text: 10.72, 29.68, 41.68, 16.0
2019-02-14 06:29:33,670 : Text to Image: 8.76, 25.028, 37.064, 19.0
2019-02-14 06:30:15,270 : samples : 320000
2019-02-14 06:30:25,519 : Image to text: 11.26, 29.88, 42.94, 14.0
2019-02-14 06:30:32,938 : Text to Image: 8.94, 25.624, 37.548, 18.0
2019-02-14 06:31:14,038 : samples : 384000
2019-02-14 06:31:24,348 : Image to text: 9.74, 28.3, 41.14, 16.0
2019-02-14 06:31:31,764 : Text to Image: 8.04, 24.004, 35.628, 20.0
2019-02-14 06:32:12,538 : samples : 448000
2019-02-14 06:32:22,791 : Image to text: 10.3, 29.14, 42.14, 15.0
2019-02-14 06:32:30,184 : Text to Image: 8.82, 25.104, 37.084, 19.0
2019-02-14 06:33:11,495 : samples : 512000
2019-02-14 06:33:21,767 : Image to text: 10.7, 29.24, 41.54, 16.0
2019-02-14 06:33:29,176 : Text to Image: 8.484, 24.976, 36.712, 19.0
2019-02-14 06:34:03,855 : Epoch 9 finished
2019-02-14 06:34:04,283 : Image to text: 27.7, 60.1, 76.3, 4.0
2019-02-14 06:34:04,612 : Text to Image: 23.2, 55.0, 72.02, 5.0
2019-02-14 06:34:05,051 : Image to text: 25.9, 60.1, 73.6, 4.0
2019-02-14 06:34:05,380 : Text to Image: 22.02, 54.5, 71.94, 5.0
2019-02-14 06:34:05,813 : Image to text: 28.7, 58.9, 75.3, 4.0
2019-02-14 06:34:06,145 : Text to Image: 22.78, 55.12, 72.34, 5.0
2019-02-14 06:34:06,577 : Image to text: 28.1, 59.6, 76.0, 4.0
2019-02-14 06:34:06,907 : Text to Image: 21.8, 54.52, 71.72, 5.0
2019-02-14 06:34:07,348 : Image to text: 27.6, 61.0, 74.5, 4.0
2019-02-14 06:34:07,678 : Text to Image: 22.78, 54.5, 70.82, 5.0
2019-02-14 06:34:07,678 : Dev mean Text to Image: 22.516000000000002, 54.728, 71.768, 5.0
2019-02-14 06:34:07,678 : Dev mean Image to text: 27.6, 59.94, 75.14, 4.0
2019-02-14 06:34:07,678 : start epoch
2019-02-14 06:34:48,851 : samples : 64000
2019-02-14 06:34:59,143 : Image to text: 10.68, 29.12, 41.16, 16.0
2019-02-14 06:35:06,556 : Text to Image: 8.932, 25.5, 37.436, 19.0
2019-02-14 06:35:48,435 : samples : 128000
2019-02-14 06:35:58,741 : Image to text: 10.66, 28.68, 42.8, 15.0
2019-02-14 06:36:06,152 : Text to Image: 8.604, 25.092, 36.544, 19.0
2019-02-14 06:36:46,869 : samples : 192000
2019-02-14 06:36:57,352 : Image to text: 10.5, 29.3, 42.54, 15.0
2019-02-14 06:37:04,936 : Text to Image: 8.1, 24.368, 35.628, 20.0
2019-02-14 06:37:47,136 : samples : 256000
2019-02-14 06:37:57,599 : Image to text: 10.96, 29.54, 42.74, 15.0
2019-02-14 06:38:05,123 : Text to Image: 8.72, 24.988, 36.672, 19.0
2019-02-14 06:38:46,386 : samples : 320000
2019-02-14 06:38:56,772 : Image to text: 10.48, 29.24, 42.32, 15.0
2019-02-14 06:39:04,205 : Text to Image: 8.668, 25.464, 37.364, 19.0
2019-02-14 06:39:46,392 : samples : 384000
2019-02-14 06:39:56,745 : Image to text: 11.04, 29.74, 41.94, 15.0
2019-02-14 06:40:04,227 : Text to Image: 8.916, 25.964, 37.828, 18.0
2019-02-14 06:40:45,468 : samples : 448000
2019-02-14 06:40:55,851 : Image to text: 8.34, 26.48, 38.12, 18.0
2019-02-14 06:41:03,333 : Text to Image: 6.98, 21.712, 32.608, 23.0
2019-02-14 06:41:45,662 : samples : 512000
2019-02-14 06:41:56,061 : Image to text: 10.8, 29.02, 41.4, 16.0
2019-02-14 06:42:03,519 : Text to Image: 8.524, 25.572, 37.564, 19.0
2019-02-14 06:42:39,923 : Epoch 10 finished
2019-02-14 06:42:40,358 : Image to text: 25.3, 61.0, 75.7, 4.0
2019-02-14 06:42:40,689 : Text to Image: 22.78, 55.28, 72.22, 5.0
2019-02-14 06:42:41,118 : Image to text: 26.8, 60.3, 76.3, 4.0
2019-02-14 06:42:41,448 : Text to Image: 21.5, 55.0, 71.6, 4.0
2019-02-14 06:42:41,880 : Image to text: 27.1, 59.2, 76.0, 4.0
2019-02-14 06:42:42,213 : Text to Image: 22.36, 55.16, 71.98, 5.0
2019-02-14 06:42:42,661 : Image to text: 29.0, 61.8, 76.3, 3.0
2019-02-14 06:42:42,993 : Text to Image: 22.6, 55.2, 72.82, 5.0
2019-02-14 06:42:43,424 : Image to text: 29.5, 62.7, 77.9, 3.0
2019-02-14 06:42:43,756 : Text to Image: 22.8, 55.24, 72.0, 5.0
2019-02-14 06:42:43,756 : Dev mean Text to Image: 22.408, 55.176, 72.124, 4.8
2019-02-14 06:42:43,756 : Dev mean Image to text: 27.54, 60.99999999999999, 76.44, 3.6000000000000005
2019-02-14 06:42:43,756 : start epoch
2019-02-14 06:43:26,425 : samples : 64000
2019-02-14 06:43:36,766 : Image to text: 10.34, 29.3, 42.28, 15.0
2019-02-14 06:43:44,250 : Text to Image: 8.644, 25.516, 37.196, 19.0
2019-02-14 06:44:25,381 : samples : 128000
2019-02-14 06:44:35,688 : Image to text: 11.16, 29.02, 41.56, 16.0
2019-02-14 06:44:43,136 : Text to Image: 8.56, 25.164, 36.856, 19.0
2019-02-14 06:45:25,023 : samples : 192000
2019-02-14 06:45:35,389 : Image to text: 10.16, 28.64, 41.02, 15.0
2019-02-14 06:45:42,824 : Text to Image: 8.356, 24.408, 35.992, 20.0
2019-02-14 06:46:23,095 : samples : 256000
2019-02-14 06:46:33,438 : Image to text: 10.2, 28.38, 41.5, 16.0
2019-02-14 06:46:40,898 : Text to Image: 8.4, 24.636, 36.448, 20.0
2019-02-14 06:47:21,534 : samples : 320000
2019-02-14 06:47:31,886 : Image to text: 10.82, 29.72, 42.04, 15.0
2019-02-14 06:47:39,335 : Text to Image: 8.64, 25.072, 37.156, 19.0
2019-02-14 06:48:19,951 : samples : 384000
2019-02-14 06:48:30,237 : Image to text: 10.3, 28.98, 42.76, 15.0
2019-02-14 06:48:37,670 : Text to Image: 8.724, 25.18, 37.068, 19.0
2019-02-14 06:49:18,437 : samples : 448000
2019-02-14 06:49:28,903 : Image to text: 10.74, 30.12, 42.58, 15.0
2019-02-14 06:49:36,396 : Text to Image: 8.512, 25.356, 37.416, 19.0
2019-02-14 06:50:18,352 : samples : 512000
2019-02-14 06:50:28,756 : Image to text: 11.16, 29.34, 42.88, 15.0
2019-02-14 06:50:36,228 : Text to Image: 8.744, 25.48, 37.396, 19.0
2019-02-14 06:51:11,364 : Epoch 11 finished
2019-02-14 06:51:11,789 : Image to text: 27.8, 59.3, 75.8, 4.0
2019-02-14 06:51:12,114 : Text to Image: 23.32, 56.12, 72.68, 4.0
2019-02-14 06:51:12,534 : Image to text: 26.8, 59.8, 76.4, 4.0
2019-02-14 06:51:12,859 : Text to Image: 23.12, 54.46, 72.16, 5.0
2019-02-14 06:51:13,293 : Image to text: 28.6, 57.6, 74.0, 4.0
2019-02-14 06:51:13,622 : Text to Image: 22.14, 54.92, 71.84, 5.0
2019-02-14 06:51:14,052 : Image to text: 29.0, 60.0, 76.7, 4.0
2019-02-14 06:51:14,383 : Text to Image: 21.88, 55.44, 73.02, 4.0
2019-02-14 06:51:14,828 : Image to text: 28.9, 61.5, 78.3, 3.0
2019-02-14 06:51:15,161 : Text to Image: 23.14, 55.06, 71.64, 5.0
2019-02-14 06:51:15,162 : Dev mean Text to Image: 22.72, 55.2, 72.268, 4.6
2019-02-14 06:51:15,162 : Dev mean Image to text: 28.22, 59.64, 76.24, 3.8000000000000003
2019-02-14 06:51:15,162 : start epoch
2019-02-14 06:51:56,292 : samples : 64000
2019-02-14 06:52:06,715 : Image to text: 10.58, 30.0, 43.2, 14.0
2019-02-14 06:52:14,154 : Text to Image: 9.076, 26.096, 38.116, 18.0
2019-02-14 06:52:57,497 : samples : 128000
2019-02-14 06:53:07,806 : Image to text: 10.96, 29.96, 42.84, 14.0
2019-02-14 06:53:15,213 : Text to Image: 9.02, 25.804, 37.596, 19.0
2019-02-14 06:53:55,903 : samples : 192000
2019-02-14 06:54:06,212 : Image to text: 10.26, 29.58, 41.96, 15.0
2019-02-14 06:54:13,633 : Text to Image: 8.476, 25.296, 37.088, 19.0
2019-02-14 06:54:55,132 : samples : 256000
2019-02-14 06:55:05,430 : Image to text: 11.1, 29.5, 42.76, 15.0
2019-02-14 06:55:12,836 : Text to Image: 8.868, 26.024, 37.976, 18.0
2019-02-14 06:55:53,727 : samples : 320000
2019-02-14 06:56:04,117 : Image to text: 10.44, 29.76, 43.24, 15.0
2019-02-14 06:56:11,570 : Text to Image: 8.6, 24.948, 36.98, 19.0
2019-02-14 06:56:51,921 : samples : 384000
2019-02-14 06:57:02,257 : Image to text: 11.06, 30.18, 42.78, 14.0
2019-02-14 06:57:09,717 : Text to Image: 9.172, 26.124, 37.956, 18.0
2019-02-14 06:57:51,159 : samples : 448000
2019-02-14 06:58:01,536 : Image to text: 10.98, 30.22, 43.72, 14.0
2019-02-14 06:58:08,978 : Text to Image: 9.416, 26.248, 38.436, 18.0
2019-02-14 06:58:51,238 : samples : 512000
2019-02-14 06:59:01,568 : Image to text: 10.52, 28.94, 41.82, 15.0
2019-02-14 06:59:09,019 : Text to Image: 9.0, 25.812, 38.144, 18.0
2019-02-14 06:59:44,797 : Epoch 12 finished
2019-02-14 06:59:45,232 : Image to text: 25.6, 57.5, 73.0, 4.0
2019-02-14 06:59:45,562 : Text to Image: 20.22, 52.9, 69.98, 5.0
2019-02-14 06:59:45,990 : Image to text: 26.8, 58.0, 72.0, 4.0
2019-02-14 06:59:46,319 : Text to Image: 20.2, 52.0, 69.8, 5.0
2019-02-14 06:59:46,747 : Image to text: 25.9, 57.6, 72.6, 4.0
2019-02-14 06:59:47,078 : Text to Image: 19.92, 53.04, 70.16, 5.0
2019-02-14 06:59:47,523 : Image to text: 26.0, 58.3, 74.8, 4.0
2019-02-14 06:59:47,853 : Text to Image: 19.82, 50.86, 70.0, 5.0
2019-02-14 06:59:48,282 : Image to text: 27.9, 59.8, 75.0, 4.0
2019-02-14 06:59:48,612 : Text to Image: 20.98, 51.86, 69.3, 5.0
2019-02-14 06:59:48,612 : Dev mean Text to Image: 20.228, 52.132000000000005, 69.848, 5.0
2019-02-14 06:59:48,612 : Dev mean Image to text: 26.439999999999998, 58.24, 73.47999999999999, 4.0
2019-02-14 06:59:52,409 : 
Test scores | Image to text:             25.84, 59.08, 74.36, 4.0
2019-02-14 06:59:52,409 : Test scores | Text to image:             21.716, 54.040000000000006, 71.34, 4.8

2019-02-14 06:59:52,504 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 06:59:52,872 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 06:59:53,546 : loading BERT model bert-base-uncased
2019-02-14 06:59:53,547 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 06:59:53,580 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 06:59:53,580 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbeqn2vmj
2019-02-14 06:59:56,023 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 06:59:57,471 : Computing embeddings for train/dev/test
2019-02-14 07:01:47,107 : Computed embeddings
2019-02-14 07:01:47,107 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:02:48,401 : [('reg:1e-05', 85.81), ('reg:0.0001', 76.19), ('reg:0.001', 55.07), ('reg:0.01', 31.73)]
2019-02-14 07:02:48,402 : Validation : best param found is reg = 1e-05 with score             85.81
2019-02-14 07:02:48,402 : Evaluating...
2019-02-14 07:02:58,004 : 
Dev acc : 85.8 Test acc : 87.3 for LENGTH classification

2019-02-14 07:02:58,004 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 07:02:58,341 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 07:02:58,386 : loading BERT model bert-base-uncased
2019-02-14 07:02:58,386 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:02:58,415 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:02:58,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo9unkgka
2019-02-14 07:03:00,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:03:02,307 : Computing embeddings for train/dev/test
2019-02-14 07:04:47,477 : Computed embeddings
2019-02-14 07:04:47,477 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:05:47,528 : [('reg:1e-05', 0.71), ('reg:0.0001', 0.21), ('reg:0.001', 0.15), ('reg:0.01', 0.1)]
2019-02-14 07:05:47,529 : Validation : best param found is reg = 1e-05 with score             0.71
2019-02-14 07:05:47,529 : Evaluating...
2019-02-14 07:06:05,124 : 
Dev acc : 0.7 Test acc : 0.7 for WORDCONTENT classification

2019-02-14 07:06:05,126 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 07:06:05,474 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 07:06:05,539 : loading BERT model bert-base-uncased
2019-02-14 07:06:05,539 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:06:05,635 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:06:05,635 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw31lpt0k
2019-02-14 07:06:08,072 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:06:09,501 : Computing embeddings for train/dev/test
2019-02-14 07:07:54,926 : Computed embeddings
2019-02-14 07:07:54,927 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:08:53,487 : [('reg:1e-05', 26.98), ('reg:0.0001', 21.96), ('reg:0.001', 20.53), ('reg:0.01', 18.07)]
2019-02-14 07:08:53,487 : Validation : best param found is reg = 1e-05 with score             26.98
2019-02-14 07:08:53,487 : Evaluating...
2019-02-14 07:09:10,727 : 
Dev acc : 27.0 Test acc : 26.9 for DEPTH classification

2019-02-14 07:09:10,728 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 07:09:11,100 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 07:09:11,162 : loading BERT model bert-base-uncased
2019-02-14 07:09:11,162 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:09:11,273 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:09:11,274 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpumzztnt8
2019-02-14 07:09:13,704 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:09:15,202 : Computing embeddings for train/dev/test
2019-02-14 07:10:50,174 : Computed embeddings
2019-02-14 07:10:50,175 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:12:05,498 : [('reg:1e-05', 44.47), ('reg:0.0001', 25.63), ('reg:0.001', 15.39), ('reg:0.01', 9.1)]
2019-02-14 07:12:05,498 : Validation : best param found is reg = 1e-05 with score             44.47
2019-02-14 07:12:05,499 : Evaluating...
2019-02-14 07:12:21,347 : 
Dev acc : 44.5 Test acc : 44.6 for TOPCONSTITUENTS classification

2019-02-14 07:12:21,348 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 07:12:21,878 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 07:12:21,944 : loading BERT model bert-base-uncased
2019-02-14 07:12:21,944 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:12:21,977 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:12:21,977 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxoq1mcqk
2019-02-14 07:12:24,416 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:12:25,851 : Computing embeddings for train/dev/test
2019-02-14 07:14:07,862 : Computed embeddings
2019-02-14 07:14:07,862 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:15:19,450 : [('reg:1e-05', 72.52), ('reg:0.0001', 66.76), ('reg:0.001', 55.89), ('reg:0.01', 50.0)]
2019-02-14 07:15:19,450 : Validation : best param found is reg = 1e-05 with score             72.52
2019-02-14 07:15:19,450 : Evaluating...
2019-02-14 07:15:35,442 : 
Dev acc : 72.5 Test acc : 72.4 for BIGRAMSHIFT classification

2019-02-14 07:15:35,443 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 07:15:36,011 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 07:15:36,078 : loading BERT model bert-base-uncased
2019-02-14 07:15:36,078 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:15:36,110 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:15:36,110 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppk67gb_o
2019-02-14 07:15:38,567 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:15:40,008 : Computing embeddings for train/dev/test
2019-02-14 07:17:25,934 : Computed embeddings
2019-02-14 07:17:25,934 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:18:20,608 : [('reg:1e-05', 86.72), ('reg:0.0001', 85.06), ('reg:0.001', 77.6), ('reg:0.01', 69.68)]
2019-02-14 07:18:20,608 : Validation : best param found is reg = 1e-05 with score             86.72
2019-02-14 07:18:20,608 : Evaluating...
2019-02-14 07:18:30,112 : 
Dev acc : 86.7 Test acc : 84.6 for TENSE classification

2019-02-14 07:18:30,113 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 07:18:30,539 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 07:18:30,603 : loading BERT model bert-base-uncased
2019-02-14 07:18:30,603 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:18:30,630 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:18:30,630 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdkkarg5y
2019-02-14 07:18:33,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:18:34,558 : Computing embeddings for train/dev/test
2019-02-14 07:20:28,223 : Computed embeddings
2019-02-14 07:20:28,224 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:21:24,572 : [('reg:1e-05', 76.01), ('reg:0.0001', 73.04), ('reg:0.001', 66.65), ('reg:0.01', 50.0)]
2019-02-14 07:21:24,572 : Validation : best param found is reg = 1e-05 with score             76.01
2019-02-14 07:21:24,572 : Evaluating...
2019-02-14 07:21:34,680 : 
Dev acc : 76.0 Test acc : 75.1 for SUBJNUMBER classification

2019-02-14 07:21:34,681 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 07:21:35,098 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 07:21:35,166 : loading BERT model bert-base-uncased
2019-02-14 07:21:35,166 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:21:35,285 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:21:35,285 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpitv1mjc8
2019-02-14 07:21:37,728 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:21:39,169 : Computing embeddings for train/dev/test
2019-02-14 07:23:36,129 : Computed embeddings
2019-02-14 07:23:36,130 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:24:24,107 : [('reg:1e-05', 74.14), ('reg:0.0001', 70.51), ('reg:0.001', 65.78), ('reg:0.01', 64.1)]
2019-02-14 07:24:24,107 : Validation : best param found is reg = 1e-05 with score             74.14
2019-02-14 07:24:24,107 : Evaluating...
2019-02-14 07:24:35,407 : 
Dev acc : 74.1 Test acc : 74.9 for OBJNUMBER classification

2019-02-14 07:24:35,409 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 07:24:35,794 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 07:24:35,863 : loading BERT model bert-base-uncased
2019-02-14 07:24:35,863 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:24:35,988 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:24:35,988 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqas51a32
2019-02-14 07:24:38,433 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:24:39,863 : Computing embeddings for train/dev/test
2019-02-14 07:26:57,673 : Computed embeddings
2019-02-14 07:26:57,674 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:27:45,814 : [('reg:1e-05', 56.67), ('reg:0.0001', 55.63), ('reg:0.001', 53.54), ('reg:0.01', 50.19)]
2019-02-14 07:27:45,814 : Validation : best param found is reg = 1e-05 with score             56.67
2019-02-14 07:27:45,814 : Evaluating...
2019-02-14 07:27:54,634 : 
Dev acc : 56.7 Test acc : 57.3 for ODDMANOUT classification

2019-02-14 07:27:54,635 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 07:27:55,246 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 07:27:55,325 : loading BERT model bert-base-uncased
2019-02-14 07:27:55,325 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:27:55,358 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:27:55,358 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptfwqzorp
2019-02-14 07:27:57,797 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:27:59,253 : Computing embeddings for train/dev/test
2019-02-14 07:30:19,291 : Computed embeddings
2019-02-14 07:30:19,291 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:30:55,672 : [('reg:1e-05', 57.04), ('reg:0.0001', 50.0), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-14 07:30:55,672 : Validation : best param found is reg = 1e-05 with score             57.04
2019-02-14 07:30:55,672 : Evaluating...
2019-02-14 07:31:09,256 : 
Dev acc : 57.0 Test acc : 56.2 for COORDINATIONINVERSION classification

2019-02-14 07:31:09,258 : total results: {'STS12': {'MSRpar': {'pearson': (0.4246750743584045, 3.399744323797231e-34), 'spearman': SpearmanrResult(correlation=0.4285806819522429, pvalue=7.341751160172879e-35), 'nsamples': 750}, 'MSRvid': {'pearson': (0.41370442223557624, 2.265329231141244e-32), 'spearman': SpearmanrResult(correlation=0.46781675808668016, pvalue=4.730217694443713e-42), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4735809968620228, 4.907492425192223e-27), 'spearman': SpearmanrResult(correlation=0.5954006466933889, pvalue=2.2710386813644786e-45), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.22826132013679418, 2.5414356876302227e-10), 'spearman': SpearmanrResult(correlation=0.24203439235003843, pvalue=1.849269575953463e-11), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.5458030757506033, 2.359284576355346e-32), 'spearman': SpearmanrResult(correlation=0.4660558544713526, pvalue=6.588623321244695e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.41720497786868027, 'wmean': 0.39740338395503233}, 'spearman': {'mean': 0.43997766671074057, 'wmean': 0.42248039158882117}}}, 'STS13': {'FNWN': {'pearson': (0.28178277213054104, 8.563046846551647e-05), 'spearman': SpearmanrResult(correlation=0.2882563148883079, pvalue=5.7634354494779537e-05), 'nsamples': 189}, 'headlines': {'pearson': (0.2013947759873814, 2.646835442613422e-08), 'spearman': SpearmanrResult(correlation=0.3368271677204082, pvalue=2.3635675054428848e-21), 'nsamples': 750}, 'OnWN': {'pearson': (0.2568115273980048, 6.702657283163543e-10), 'spearman': SpearmanrResult(correlation=0.2941527077847897, pvalue=1.1648115849429828e-12), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.24666302517197577, 'wmean': 0.23224952852899264}, 'spearman': {'mean': 0.30641206346450195, 'wmean': 0.31474699224764224}}}, 'STS14': {'deft-forum': {'pearson': (0.19822074102570592, 2.282474386963135e-05), 'spearman': SpearmanrResult(correlation=0.24535873508435993, pvalue=1.355677955068693e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.4500697487096231, 2.2826657101653576e-16), 'spearman': SpearmanrResult(correlation=0.5475584494843545, pvalue=7.226719106881992e-25), 'nsamples': 300}, 'headlines': {'pearson': (0.20920333343248498, 7.301811389557493e-09), 'spearman': SpearmanrResult(correlation=0.2611792965049886, pvalue=3.6683339502543294e-13), 'nsamples': 750}, 'images': {'pearson': (0.5302077611580785, 1.3438868432084545e-55), 'spearman': SpearmanrResult(correlation=0.5264951362089555, pvalue=1.0360573409564195e-54), 'nsamples': 750}, 'OnWN': {'pearson': (0.2751220839057249, 1.7137039515666383e-14), 'spearman': SpearmanrResult(correlation=0.30172786003468677, pvalue=2.988309767880309e-17), 'nsamples': 750}, 'tweet-news': {'pearson': (0.29593047170295433, 1.263539600526414e-16), 'spearman': SpearmanrResult(correlation=0.35109726908603983, pvalue=3.526605164531446e-23), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3264590233224286, 'wmean': 0.3218847988597031}, 'spearman': {'mean': 0.37223612440056414, 'wmean': 0.3613476365358057}}}, 'STS15': {'answers-forums': {'pearson': (0.39211435482682827, 3.1216058344045495e-15), 'spearman': SpearmanrResult(correlation=0.40944457204740137, pvalue=1.366449923094941e-16), 'nsamples': 375}, 'answers-students': {'pearson': (0.3477081522720446, 9.764622981628873e-23), 'spearman': SpearmanrResult(correlation=0.3932741407316995, pvalue=3.7663096322896355e-29), 'nsamples': 750}, 'belief': {'pearson': (0.41502876599586697, 4.7932127648982436e-17), 'spearman': SpearmanrResult(correlation=0.4575037018700258, pvalue=8.49829031148208e-21), 'nsamples': 375}, 'headlines': {'pearson': (0.3194476322547036, 2.967403981802861e-19), 'spearman': SpearmanrResult(correlation=0.4107969559986625, pvalue=6.718019432787236e-32), 'nsamples': 750}, 'images': {'pearson': (0.19491688444447525, 7.415274384038237e-08), 'spearman': SpearmanrResult(correlation=0.4838296008361882, pvalue=2.887502499853801e-45), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3338431579587837, 'wmean': 0.3164110573456428}, 'spearman': {'mean': 0.43096979429679544, 'wmean': 0.43034370863131594}}}, 'STS16': {'answer-answer': {'pearson': (0.36562192426597034, 1.8829189264450535e-09), 'spearman': SpearmanrResult(correlation=0.4976486354739611, pvalue=2.6757478445445115e-17), 'nsamples': 254}, 'headlines': {'pearson': (0.34077172631826724, 3.4637114294322546e-08), 'spearman': SpearmanrResult(correlation=0.47550342732468803, pvalue=1.8817187311432002e-15), 'nsamples': 249}, 'plagiarism': {'pearson': (0.5014137108588148, 4.789482385288067e-16), 'spearman': SpearmanrResult(correlation=0.6543731333279511, pvalue=1.6968933735729948e-29), 'nsamples': 230}, 'postediting': {'pearson': (0.5982604063912708, 4.44846263848154e-25), 'spearman': SpearmanrResult(correlation=0.7806930817086951, pvalue=2.53851719141376e-51), 'nsamples': 244}, 'question-question': {'pearson': (0.36190705017347585, 7.273479220088621e-08), 'spearman': SpearmanrResult(correlation=0.40766302466179943, pvalue=8.989824676943345e-10), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.43359496360155986, 'wmean': 0.433945526779139}, 'spearman': {'mean': 0.563176260499419, 'wmean': 0.5657669574796795}}}, 'MR': {'devacc': 70.11, 'acc': 70.91, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 72.09, 'acc': 71.44, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 80.25, 'acc': 82.01, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.9, 'acc': 90.28, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.49, 'acc': 77.38, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 37.87, 'acc': 38.55, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 49.93, 'acc': 70.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.1, 'acc': 70.9, 'f1': 77.41, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 67.6, 'acc': 66.21, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7340140961446187, 'pearson': 0.7420884179577911, 'spearman': 0.6807010038962058, 'mse': 0.46158558783285336, 'yhat': array([3.06656804, 3.5557588 , 4.07716266, ..., 2.80545122, 4.18764962,        4.37067704]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6779534773581283, 'pearson': 0.6105704575956474, 'spearman': 0.6060365842232175, 'mse': 1.6944107535483328, 'yhat': array([2.49793577, 2.1936787 , 3.01676632, ..., 3.71320925, 3.64013872,        3.02300216]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 60.07, 'acc': 59.52, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 314.688, 'acc': [(25.84, 59.08, 74.36, 4.0), (21.716, 54.040000000000006, 71.34, 4.8)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 85.81, 'acc': 87.26, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.71, 'acc': 0.65, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.98, 'acc': 26.88, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 44.47, 'acc': 44.64, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 72.52, 'acc': 72.41, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 86.72, 'acc': 84.61, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 76.01, 'acc': 75.06, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.14, 'acc': 74.86, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 56.67, 'acc': 57.33, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 57.04, 'acc': 56.17, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 07:31:09,259 : STS12 p=0.3974, STS12 s=0.4225, STS13 p=0.2322, STS13 s=0.3147, STS14 p=0.3219, STS14 s=0.3613, STS15 p=0.3164, STS15 s=0.4303, STS 16 p=0.4339, STS16 s=0.5658, STS B p=0.6106, STS B s=0.6060, STS B m=1.6944, SICK-R p=0.7421, SICK-R s=0.6807, SICK-P m=0.4616
2019-02-14 07:31:09,259 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 07:31:09,259 : 0.3974,0.4225,0.2322,0.3147,0.3219,0.3613,0.3164,0.4303,0.4339,0.5658,0.6106,0.6060,1.6944,0.7421,0.6807,0.4616
2019-02-14 07:31:09,259 : MR=70.91, CR=71.44, SUBJ=90.28, MPQA=82.01, SST-B=77.38, SST-F=38.55, TREC=70.20, SICK-E=66.21, SNLI=59.52, MRPC=70.90, MRPC f=77.41
2019-02-14 07:31:09,259 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 07:31:09,259 : 70.91,71.44,90.28,82.01,77.38,38.55,70.20,66.21,59.52,70.90,77.41
2019-02-14 07:31:09,259 : COCO r1i2t=25.84, COCO r5i2t=59.08, COCO r10i2t=74.36, COCO medr_i2t=4.00, COCO r1t2i=21.72, COCO r5t2i=54.04, COCO r10t2i=71.34, COCO medr_t2i=4.80
2019-02-14 07:31:09,259 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 07:31:09,259 : 25.84,59.08,74.36,4.00,21.72,54.04,71.34,4.80
2019-02-14 07:31:09,259 : SentLen=87.26, WC=0.65, TreeDepth=26.88, TopConst=44.64, BShift=72.41, Tense=84.61, SubjNum=75.06, ObjNum=74.86, SOMO=57.33, CoordInv=56.17, average=57.99
2019-02-14 07:31:09,259 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 07:31:09,259 : 87.26,0.65,26.88,44.64,72.41,84.61,75.06,74.86,57.33,56.17,57.99
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,259 : layer 5
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,259 : ********************************************************************************
2019-02-14 07:31:09,345 : ***** Transfer task : STS12 *****


2019-02-14 07:31:09,357 : loading BERT model bert-base-uncased
2019-02-14 07:31:09,358 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:09,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:09,376 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpswrvg0tu
2019-02-14 07:31:11,816 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:15,240 : MSRpar : pearson = 0.3764, spearman = 0.4159
2019-02-14 07:31:16,206 : MSRvid : pearson = 0.4266, spearman = 0.4726
2019-02-14 07:31:16,986 : SMTeuroparl : pearson = 0.3963, spearman = 0.5414
2019-02-14 07:31:18,356 : surprise.OnWN : pearson = 0.2989, spearman = 0.2763
2019-02-14 07:31:19,113 : surprise.SMTnews : pearson = 0.4789, spearman = 0.4670
2019-02-14 07:31:19,113 : ALL (weighted average) : Pearson = 0.3859,             Spearman = 0.4210
2019-02-14 07:31:19,113 : ALL (average) : Pearson = 0.3954,             Spearman = 0.4346

2019-02-14 07:31:19,113 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 07:31:19,121 : loading BERT model bert-base-uncased
2019-02-14 07:31:19,121 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:19,139 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:19,139 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4w3e2qmm
2019-02-14 07:31:21,578 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:23,689 : FNWN : pearson = 0.2279, spearman = 0.2346
2019-02-14 07:31:24,776 : headlines : pearson = 0.3622, spearman = 0.4204
2019-02-14 07:31:25,509 : OnWN : pearson = 0.1139, spearman = 0.1408
2019-02-14 07:31:25,509 : ALL (weighted average) : Pearson = 0.2524,             Spearman = 0.2924
2019-02-14 07:31:25,509 : ALL (average) : Pearson = 0.2347,             Spearman = 0.2653

2019-02-14 07:31:25,509 : ***** Transfer task : STS14 *****


2019-02-14 07:31:25,526 : loading BERT model bert-base-uncased
2019-02-14 07:31:25,526 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:25,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:25,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphu1gzjgs
2019-02-14 07:31:28,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:30,964 : deft-forum : pearson = 0.1926, spearman = 0.2437
2019-02-14 07:31:31,926 : deft-news : pearson = 0.4226, spearman = 0.5678
2019-02-14 07:31:33,356 : headlines : pearson = 0.3416, spearman = 0.3597
2019-02-14 07:31:34,934 : images : pearson = 0.5493, spearman = 0.5553
2019-02-14 07:31:36,508 : OnWN : pearson = 0.1890, spearman = 0.2201
2019-02-14 07:31:37,612 : tweet-news : pearson = 0.3797, spearman = 0.4143
2019-02-14 07:31:37,612 : ALL (weighted average) : Pearson = 0.3488,             Spearman = 0.3846
2019-02-14 07:31:37,612 : ALL (average) : Pearson = 0.3458,             Spearman = 0.3935

2019-02-14 07:31:37,612 : ***** Transfer task : STS15 *****


2019-02-14 07:31:37,645 : loading BERT model bert-base-uncased
2019-02-14 07:31:37,645 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:37,663 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:37,663 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe5hfamhx
2019-02-14 07:31:40,109 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:42,733 : answers-forums : pearson = 0.3484, spearman = 0.3941
2019-02-14 07:31:43,597 : answers-students : pearson = 0.2561, spearman = 0.2959
2019-02-14 07:31:44,353 : belief : pearson = 0.3290, spearman = 0.3690
2019-02-14 07:31:45,242 : headlines : pearson = 0.4347, spearman = 0.4836
2019-02-14 07:31:46,089 : images : pearson = 0.2403, spearman = 0.4935
2019-02-14 07:31:46,090 : ALL (weighted average) : Pearson = 0.3175,             Spearman = 0.4136
2019-02-14 07:31:46,090 : ALL (average) : Pearson = 0.3217,             Spearman = 0.4072

2019-02-14 07:31:46,090 : ***** Transfer task : STS16 *****


2019-02-14 07:31:46,165 : loading BERT model bert-base-uncased
2019-02-14 07:31:46,165 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:46,187 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:46,187 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprp8_bsrn
2019-02-14 07:31:48,678 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:50,778 : answer-answer : pearson = 0.3383, spearman = 0.5171
2019-02-14 07:31:51,312 : headlines : pearson = 0.4908, spearman = 0.5570
2019-02-14 07:31:51,959 : plagiarism : pearson = 0.6763, spearman = 0.7071
2019-02-14 07:31:52,732 : postediting : pearson = 0.6508, spearman = 0.7788
2019-02-14 07:31:52,983 : question-question : pearson = 0.3460, spearman = 0.3531
2019-02-14 07:31:52,983 : ALL (weighted average) : Pearson = 0.5015,             Spearman = 0.5873
2019-02-14 07:31:52,983 : ALL (average) : Pearson = 0.5004,             Spearman = 0.5826

2019-02-14 07:31:52,984 : ***** Transfer task : MR *****


2019-02-14 07:31:53,000 : loading BERT model bert-base-uncased
2019-02-14 07:31:53,000 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:31:53,022 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:31:53,022 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqumn4_eo
2019-02-14 07:31:55,480 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:31:57,064 : Generating sentence embeddings
2019-02-14 07:32:12,380 : Generated sentence embeddings
2019-02-14 07:32:12,380 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:32:30,807 : Best param found at split 1: l2reg = 1e-05                 with score 68.85
2019-02-14 07:32:45,931 : Best param found at split 2: l2reg = 1e-05                 with score 70.07
2019-02-14 07:33:00,020 : Best param found at split 3: l2reg = 1e-05                 with score 70.67
2019-02-14 07:33:13,723 : Best param found at split 4: l2reg = 1e-05                 with score 70.01
2019-02-14 07:33:27,518 : Best param found at split 5: l2reg = 1e-05                 with score 70.52
2019-02-14 07:33:28,294 : Dev acc : 70.02 Test acc : 69.86

2019-02-14 07:33:28,295 : ***** Transfer task : CR *****


2019-02-14 07:33:28,303 : loading BERT model bert-base-uncased
2019-02-14 07:33:28,303 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:33:28,323 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:33:28,323 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvree2obm
2019-02-14 07:33:30,775 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:33:32,217 : Generating sentence embeddings
2019-02-14 07:33:36,351 : Generated sentence embeddings
2019-02-14 07:33:36,351 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:33:40,073 : Best param found at split 1: l2reg = 1e-05                 with score 71.32
2019-02-14 07:33:46,259 : Best param found at split 2: l2reg = 1e-05                 with score 69.59
2019-02-14 07:33:50,725 : Best param found at split 3: l2reg = 1e-05                 with score 71.42
2019-02-14 07:33:55,347 : Best param found at split 4: l2reg = 0.0001                 with score 71.3
2019-02-14 07:33:59,814 : Best param found at split 5: l2reg = 1e-05                 with score 70.87
2019-02-14 07:34:00,153 : Dev acc : 70.9 Test acc : 71.18

2019-02-14 07:34:00,154 : ***** Transfer task : MPQA *****


2019-02-14 07:34:00,160 : loading BERT model bert-base-uncased
2019-02-14 07:34:00,160 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:34:00,181 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:34:00,181 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpz31mouzu
2019-02-14 07:34:02,660 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:34:04,148 : Generating sentence embeddings
2019-02-14 07:34:09,241 : Generated sentence embeddings
2019-02-14 07:34:09,242 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:34:21,729 : Best param found at split 1: l2reg = 1e-05                 with score 75.51
2019-02-14 07:34:33,385 : Best param found at split 2: l2reg = 1e-05                 with score 76.0
2019-02-14 07:34:47,708 : Best param found at split 3: l2reg = 1e-05                 with score 76.35
2019-02-14 07:35:00,038 : Best param found at split 4: l2reg = 1e-05                 with score 75.79
2019-02-14 07:35:14,005 : Best param found at split 5: l2reg = 1e-05                 with score 78.55
2019-02-14 07:35:14,864 : Dev acc : 76.44 Test acc : 79.56

2019-02-14 07:35:14,864 : ***** Transfer task : SUBJ *****


2019-02-14 07:35:14,879 : loading BERT model bert-base-uncased
2019-02-14 07:35:14,879 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:35:14,900 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:35:14,900 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2zrbja9p
2019-02-14 07:35:17,341 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:35:18,791 : Generating sentence embeddings
2019-02-14 07:35:33,158 : Generated sentence embeddings
2019-02-14 07:35:33,159 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 07:35:47,967 : Best param found at split 1: l2reg = 1e-05                 with score 91.1
2019-02-14 07:36:06,624 : Best param found at split 2: l2reg = 1e-05                 with score 91.42
2019-02-14 07:36:19,623 : Best param found at split 3: l2reg = 1e-05                 with score 91.05
2019-02-14 07:36:35,054 : Best param found at split 4: l2reg = 1e-05                 with score 91.39
2019-02-14 07:36:47,973 : Best param found at split 5: l2reg = 1e-05                 with score 90.98
2019-02-14 07:36:48,909 : Dev acc : 91.19 Test acc : 90.4

2019-02-14 07:36:48,910 : ***** Transfer task : SST Binary classification *****


2019-02-14 07:36:49,038 : loading BERT model bert-base-uncased
2019-02-14 07:36:49,038 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:36:49,063 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:36:49,063 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7gqyw69f
2019-02-14 07:36:51,512 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:36:52,987 : Computing embedding for train
2019-02-14 07:37:48,536 : Computed train embeddings
2019-02-14 07:37:48,536 : Computing embedding for dev
2019-02-14 07:37:49,790 : Computed dev embeddings
2019-02-14 07:37:49,790 : Computing embedding for test
2019-02-14 07:37:52,514 : Computed test embeddings
2019-02-14 07:37:52,514 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:38:24,095 : [('reg:1e-05', 77.75), ('reg:0.0001', 77.06), ('reg:0.001', 73.28), ('reg:0.01', 59.4)]
2019-02-14 07:38:24,095 : Validation : best param found is reg = 1e-05 with score             77.75
2019-02-14 07:38:24,095 : Evaluating...
2019-02-14 07:38:29,619 : 
Dev acc : 77.75 Test acc : 75.56 for             SST Binary classification

2019-02-14 07:38:29,619 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 07:38:29,676 : loading BERT model bert-base-uncased
2019-02-14 07:38:29,676 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:38:29,700 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:38:29,700 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf1hmcbnj
2019-02-14 07:38:32,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:38:33,629 : Computing embedding for train
2019-02-14 07:38:44,180 : Computed train embeddings
2019-02-14 07:38:44,180 : Computing embedding for dev
2019-02-14 07:38:45,553 : Computed dev embeddings
2019-02-14 07:38:45,553 : Computing embedding for test
2019-02-14 07:38:48,293 : Computed test embeddings
2019-02-14 07:38:48,294 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:38:51,427 : [('reg:1e-05', 38.78), ('reg:0.0001', 38.24), ('reg:0.001', 29.52), ('reg:0.01', 26.25)]
2019-02-14 07:38:51,427 : Validation : best param found is reg = 1e-05 with score             38.78
2019-02-14 07:38:51,427 : Evaluating...
2019-02-14 07:38:52,396 : 
Dev acc : 38.78 Test acc : 38.73 for             SST Fine-Grained classification

2019-02-14 07:38:52,396 : ***** Transfer task : TREC *****


2019-02-14 07:38:52,410 : loading BERT model bert-base-uncased
2019-02-14 07:38:52,410 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:38:52,429 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:38:52,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa2u0v9hp
2019-02-14 07:38:54,872 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:39:00,040 : Computed train embeddings
2019-02-14 07:39:00,581 : Computed test embeddings
2019-02-14 07:39:00,581 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 07:39:16,033 : [('reg:1e-05', 48.76), ('reg:0.0001', 43.43), ('reg:0.001', 37.8), ('reg:0.01', 25.99)]
2019-02-14 07:39:16,034 : Cross-validation : best param found is reg = 1e-05             with score 48.76
2019-02-14 07:39:16,034 : Evaluating...
2019-02-14 07:39:16,673 : 
Dev acc : 48.76 Test acc : 74.6             for TREC

2019-02-14 07:39:16,674 : ***** Transfer task : MRPC *****


2019-02-14 07:39:16,696 : loading BERT model bert-base-uncased
2019-02-14 07:39:16,696 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:39:16,717 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:39:16,717 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqttas48g
2019-02-14 07:39:19,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:39:20,708 : Computing embedding for train
2019-02-14 07:39:31,963 : Computed train embeddings
2019-02-14 07:39:31,963 : Computing embedding for test
2019-02-14 07:39:36,551 : Computed test embeddings
2019-02-14 07:39:36,568 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 07:39:42,379 : [('reg:1e-05', 69.99), ('reg:0.0001', 69.87), ('reg:0.001', 69.16), ('reg:0.01', 67.64)]
2019-02-14 07:39:42,379 : Cross-validation : best param found is reg = 1e-05             with score 69.99
2019-02-14 07:39:42,379 : Evaluating...
2019-02-14 07:39:42,696 : Dev acc : 69.99 Test acc 70.9; Test F1 78.6 for MRPC.

2019-02-14 07:39:42,696 : ***** Transfer task : SICK-Entailment*****


2019-02-14 07:39:42,762 : loading BERT model bert-base-uncased
2019-02-14 07:39:42,762 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:39:42,782 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:39:42,782 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwlp084fl
2019-02-14 07:39:45,234 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:39:46,761 : Computing embedding for train
2019-02-14 07:39:58,883 : Computed train embeddings
2019-02-14 07:39:58,884 : Computing embedding for dev
2019-02-14 07:40:00,424 : Computed dev embeddings
2019-02-14 07:40:00,424 : Computing embedding for test
2019-02-14 07:40:07,916 : Computed test embeddings
2019-02-14 07:40:07,944 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 07:40:09,888 : [('reg:1e-05', 73.4), ('reg:0.0001', 64.8), ('reg:0.001', 56.4), ('reg:0.01', 60.2)]
2019-02-14 07:40:09,888 : Validation : best param found is reg = 1e-05 with score             73.4
2019-02-14 07:40:09,888 : Evaluating...
2019-02-14 07:40:10,441 : 
Dev acc : 73.4 Test acc : 73.13 for                        SICK entailment

2019-02-14 07:40:10,441 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 07:40:10,468 : loading BERT model bert-base-uncased
2019-02-14 07:40:10,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:40:10,525 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:40:10,525 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpql5duk2h
2019-02-14 07:40:12,987 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:40:14,431 : Computing embedding for train
2019-02-14 07:40:22,186 : Computed train embeddings
2019-02-14 07:40:22,186 : Computing embedding for dev
2019-02-14 07:40:23,277 : Computed dev embeddings
2019-02-14 07:40:23,277 : Computing embedding for test
2019-02-14 07:40:31,995 : Computed test embeddings
2019-02-14 07:42:06,281 : Dev : Pearson 0.713676119497383
2019-02-14 07:42:06,281 : Test : Pearson 0.7255760575039417 Spearman 0.6703817875684596 MSE 0.4859321647357864                        for SICK Relatedness

2019-02-14 07:42:06,282 : 

***** Transfer task : STSBenchmark*****


2019-02-14 07:42:06,321 : loading BERT model bert-base-uncased
2019-02-14 07:42:06,321 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:42:06,350 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:42:06,350 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmsahftoi
2019-02-14 07:42:08,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:42:10,306 : Computing embedding for train
2019-02-14 07:42:21,680 : Computed train embeddings
2019-02-14 07:42:21,680 : Computing embedding for dev
2019-02-14 07:42:24,575 : Computed dev embeddings
2019-02-14 07:42:24,575 : Computing embedding for test
2019-02-14 07:42:27,084 : Computed test embeddings
2019-02-14 07:44:23,360 : Dev : Pearson 0.6758631551611493
2019-02-14 07:44:23,360 : Test : Pearson 0.5986588437013479 Spearman 0.5931515273544508 MSE 1.7307283205729138                        for SICK Relatedness

2019-02-14 07:44:23,361 : ***** Transfer task : SNLI Entailment*****


2019-02-14 07:44:28,175 : loading BERT model bert-base-uncased
2019-02-14 07:44:28,176 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 07:44:28,318 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 07:44:28,318 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppzg2owyq
2019-02-14 07:44:30,752 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 07:44:32,487 : PROGRESS (encoding): 0.00%
2019-02-14 07:46:42,034 : PROGRESS (encoding): 14.56%
2019-02-14 07:49:02,814 : PROGRESS (encoding): 29.12%
2019-02-14 07:51:25,486 : PROGRESS (encoding): 43.69%
2019-02-14 07:53:55,665 : PROGRESS (encoding): 58.25%
2019-02-14 07:56:41,151 : PROGRESS (encoding): 72.81%
2019-02-14 07:59:17,211 : PROGRESS (encoding): 87.37%
2019-02-14 08:01:12,065 : PROGRESS (encoding): 0.00%
2019-02-14 08:01:30,854 : PROGRESS (encoding): 0.00%
2019-02-14 08:01:53,973 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 08:03:36,112 : [('reg:1e-09', 62.99)]
2019-02-14 08:03:36,112 : Validation : best param found is reg = 1e-09 with score             62.99
2019-02-14 08:03:36,112 : Evaluating...
2019-02-14 08:05:20,842 : Dev acc : 62.99 Test acc : 62.54 for SNLI

2019-02-14 08:05:20,842 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 08:05:29,525 : loading BERT model bert-base-uncased
2019-02-14 08:05:29,526 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 08:05:29,572 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 08:05:29,573 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpefqbwq00
2019-02-14 08:05:32,064 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 08:05:33,617 : Computing embedding for train
2019-02-14 08:15:41,151 : Computed train embeddings
2019-02-14 08:15:41,151 : Computing embedding for dev
2019-02-14 08:16:00,665 : Computed dev embeddings
2019-02-14 08:16:00,665 : Computing embedding for test
2019-02-14 08:16:20,683 : Computed test embeddings
2019-02-14 08:16:20,699 : prepare data
2019-02-14 08:16:20,766 : start epoch
2019-02-14 08:17:01,326 : samples : 64000
2019-02-14 08:17:11,540 : Image to text: 3.82, 13.9, 22.88, 42.0
2019-02-14 08:17:18,892 : Text to Image: 2.956, 10.632, 17.684, 53.0
2019-02-14 08:18:00,442 : samples : 128000
2019-02-14 08:18:10,731 : Image to text: 5.64, 17.2, 27.3, 32.0
2019-02-14 08:18:18,173 : Text to Image: 4.044, 13.952, 22.092, 41.0
2019-02-14 08:18:58,766 : samples : 192000
2019-02-14 08:19:09,010 : Image to text: 5.94, 18.44, 27.96, 30.0
2019-02-14 08:19:16,386 : Text to Image: 4.224, 14.536, 22.7, 39.0
2019-02-14 08:19:57,371 : samples : 256000
2019-02-14 08:20:07,620 : Image to text: 6.06, 18.72, 28.66, 29.0
2019-02-14 08:20:14,992 : Text to Image: 4.648, 15.88, 24.844, 35.0
2019-02-14 08:20:55,628 : samples : 320000
2019-02-14 08:21:05,859 : Image to text: 5.5, 17.88, 27.82, 31.0
2019-02-14 08:21:13,222 : Text to Image: 3.208, 12.492, 20.516, 44.0
2019-02-14 08:21:56,769 : samples : 384000
2019-02-14 08:22:06,857 : Image to text: 6.1, 20.86, 31.38, 26.0
2019-02-14 08:22:14,220 : Text to Image: 4.912, 16.484, 25.872, 33.0
2019-02-14 08:22:57,353 : samples : 448000
2019-02-14 08:23:07,468 : Image to text: 6.5, 20.64, 32.04, 25.0
2019-02-14 08:23:14,826 : Text to Image: 5.884, 18.068, 27.86, 30.0
2019-02-14 08:24:00,501 : samples : 512000
2019-02-14 08:24:10,619 : Image to text: 6.62, 21.12, 31.34, 26.0
2019-02-14 08:24:17,981 : Text to Image: 5.44, 17.48, 27.244, 31.0
2019-02-14 08:24:52,479 : Epoch 1 finished
2019-02-14 08:24:52,898 : Image to text: 19.9, 49.9, 66.1, 6.0
2019-02-14 08:24:53,221 : Text to Image: 16.16, 43.38, 61.24, 7.0
2019-02-14 08:24:53,640 : Image to text: 19.3, 49.1, 66.9, 6.0
2019-02-14 08:24:53,962 : Text to Image: 15.54, 42.96, 61.36, 7.0
2019-02-14 08:24:54,400 : Image to text: 21.1, 51.1, 66.7, 5.0
2019-02-14 08:24:54,730 : Text to Image: 16.28, 44.3, 61.1, 7.0
2019-02-14 08:24:55,161 : Image to text: 23.3, 51.0, 66.8, 5.0
2019-02-14 08:24:55,493 : Text to Image: 15.46, 43.5, 61.12, 7.0
2019-02-14 08:24:55,942 : Image to text: 23.2, 51.5, 67.7, 5.0
2019-02-14 08:24:56,272 : Text to Image: 16.82, 45.4, 62.4, 7.0
2019-02-14 08:24:56,272 : Dev mean Text to Image: 16.052, 43.908, 61.444, 7.0
2019-02-14 08:24:56,272 : Dev mean Image to text: 21.36, 50.519999999999996, 66.84, 5.4
2019-02-14 08:24:56,272 : start epoch
2019-02-14 08:25:37,196 : samples : 64000
2019-02-14 08:25:47,520 : Image to text: 7.68, 23.52, 34.0, 22.0
2019-02-14 08:25:54,951 : Text to Image: 5.936, 19.056, 29.364, 27.0
2019-02-14 08:26:36,165 : samples : 128000
2019-02-14 08:26:46,450 : Image to text: 7.6, 23.18, 33.16, 23.0
2019-02-14 08:26:53,858 : Text to Image: 6.432, 19.604, 30.06, 27.0
2019-02-14 08:27:34,597 : samples : 192000
2019-02-14 08:27:44,864 : Image to text: 7.62, 23.26, 34.32, 21.0
2019-02-14 08:27:52,284 : Text to Image: 6.2, 19.26, 29.648, 27.0
2019-02-14 08:28:33,300 : samples : 256000
2019-02-14 08:28:43,572 : Image to text: 7.46, 23.16, 34.56, 22.0
2019-02-14 08:28:50,974 : Text to Image: 6.612, 20.428, 31.0, 25.0
2019-02-14 08:29:31,595 : samples : 320000
2019-02-14 08:29:41,870 : Image to text: 8.12, 23.78, 35.08, 21.0
2019-02-14 08:29:49,282 : Text to Image: 6.324, 19.444, 29.536, 27.0
2019-02-14 08:30:29,686 : samples : 384000
2019-02-14 08:30:39,771 : Image to text: 7.16, 22.72, 33.6, 22.0
2019-02-14 08:30:47,112 : Text to Image: 5.46, 18.46, 28.584, 28.0
2019-02-14 08:31:28,462 : samples : 448000
2019-02-14 08:31:38,580 : Image to text: 7.2, 21.88, 32.16, 25.0
2019-02-14 08:31:45,953 : Text to Image: 5.608, 18.52, 28.66, 28.0
2019-02-14 08:32:27,436 : samples : 512000
2019-02-14 08:32:37,556 : Image to text: 8.64, 24.38, 36.1, 20.0
2019-02-14 08:32:44,885 : Text to Image: 6.564, 20.732, 31.396, 25.0
2019-02-14 08:33:19,672 : Epoch 2 finished
2019-02-14 08:33:20,099 : Image to text: 22.3, 52.9, 69.4, 5.0
2019-02-14 08:33:20,427 : Text to Image: 17.34, 46.28, 63.22, 6.0
2019-02-14 08:33:20,851 : Image to text: 20.3, 51.1, 66.9, 5.0
2019-02-14 08:33:21,178 : Text to Image: 17.1, 45.12, 63.62, 7.0
2019-02-14 08:33:21,603 : Image to text: 21.4, 51.8, 66.9, 5.0
2019-02-14 08:33:21,933 : Text to Image: 17.5, 46.98, 63.46, 6.0
2019-02-14 08:33:22,377 : Image to text: 23.0, 53.9, 68.8, 5.0
2019-02-14 08:33:22,709 : Text to Image: 16.4, 46.2, 63.82, 6.0
2019-02-14 08:33:23,141 : Image to text: 23.7, 54.0, 70.8, 5.0
2019-02-14 08:33:23,472 : Text to Image: 17.5, 47.16, 63.66, 6.0
2019-02-14 08:33:23,472 : Dev mean Text to Image: 17.168, 46.348, 63.556, 6.2
2019-02-14 08:33:23,472 : Dev mean Image to text: 22.14, 52.739999999999995, 68.56, 5.0
2019-02-14 08:33:23,473 : start epoch
2019-02-14 08:34:05,626 : samples : 64000
2019-02-14 08:34:15,983 : Image to text: 7.46, 23.68, 34.82, 22.0
2019-02-14 08:34:23,394 : Text to Image: 6.028, 18.988, 28.848, 28.0
2019-02-14 08:35:05,491 : samples : 128000
2019-02-14 08:35:15,857 : Image to text: 8.18, 23.88, 35.34, 21.0
2019-02-14 08:35:23,241 : Text to Image: 6.364, 20.568, 30.932, 25.0
2019-02-14 08:36:04,354 : samples : 192000
2019-02-14 08:36:14,637 : Image to text: 8.32, 23.84, 35.74, 21.0
2019-02-14 08:36:22,060 : Text to Image: 6.72, 21.096, 32.048, 25.0
2019-02-14 08:37:02,550 : samples : 256000
2019-02-14 08:37:12,831 : Image to text: 7.82, 23.78, 34.74, 20.0
2019-02-14 08:37:20,240 : Text to Image: 5.772, 19.032, 29.192, 28.0
2019-02-14 08:38:00,671 : samples : 320000
2019-02-14 08:38:10,931 : Image to text: 8.08, 24.94, 36.1, 20.0
2019-02-14 08:38:18,330 : Text to Image: 6.904, 21.088, 31.872, 25.0
2019-02-14 08:39:01,212 : samples : 384000
2019-02-14 08:39:11,312 : Image to text: 8.62, 25.34, 36.98, 20.0
2019-02-14 08:39:18,719 : Text to Image: 7.096, 21.848, 32.632, 23.0
2019-02-14 08:39:59,934 : samples : 448000
2019-02-14 08:40:10,069 : Image to text: 8.24, 25.76, 37.5, 19.0
2019-02-14 08:40:17,458 : Text to Image: 7.008, 21.472, 32.028, 23.0
2019-02-14 08:40:59,749 : samples : 512000
2019-02-14 08:41:09,902 : Image to text: 9.5, 25.44, 37.44, 19.0
2019-02-14 08:41:17,285 : Text to Image: 7.256, 22.212, 32.944, 23.0
2019-02-14 08:41:52,038 : Epoch 3 finished
2019-02-14 08:41:52,463 : Image to text: 21.8, 51.8, 69.2, 5.0
2019-02-14 08:41:52,788 : Text to Image: 19.08, 49.02, 66.52, 6.0
2019-02-14 08:41:53,206 : Image to text: 20.8, 51.8, 69.1, 5.0
2019-02-14 08:41:53,530 : Text to Image: 17.84, 47.84, 65.74, 6.0
2019-02-14 08:41:53,956 : Image to text: 22.4, 53.1, 67.4, 5.0
2019-02-14 08:41:54,286 : Text to Image: 17.52, 48.38, 66.78, 6.0
2019-02-14 08:41:54,728 : Image to text: 22.8, 54.9, 71.6, 5.0
2019-02-14 08:41:55,058 : Text to Image: 17.82, 48.5, 66.84, 6.0
2019-02-14 08:41:55,486 : Image to text: 26.1, 54.9, 71.2, 4.0
2019-02-14 08:41:55,815 : Text to Image: 18.32, 49.04, 66.36, 6.0
2019-02-14 08:41:55,815 : Dev mean Text to Image: 18.116, 48.556000000000004, 66.44800000000001, 6.0
2019-02-14 08:41:55,815 : Dev mean Image to text: 22.78, 53.3, 69.7, 4.8
2019-02-14 08:41:55,815 : start epoch
2019-02-14 08:42:37,273 : samples : 64000
2019-02-14 08:42:47,585 : Image to text: 8.92, 26.58, 37.92, 19.0
2019-02-14 08:42:54,968 : Text to Image: 7.364, 22.252, 33.164, 23.0
2019-02-14 08:43:35,437 : samples : 128000
2019-02-14 08:43:45,750 : Image to text: 9.22, 26.84, 38.36, 18.0
2019-02-14 08:43:53,147 : Text to Image: 7.664, 22.912, 34.112, 22.0
2019-02-14 08:44:34,845 : samples : 192000
2019-02-14 08:44:45,174 : Image to text: 9.08, 25.14, 35.9, 20.0
2019-02-14 08:44:52,572 : Text to Image: 6.9, 21.164, 32.168, 24.0
2019-02-14 08:45:33,715 : samples : 256000
2019-02-14 08:45:43,992 : Image to text: 9.2, 26.56, 37.64, 18.0
2019-02-14 08:45:51,406 : Text to Image: 7.112, 21.652, 32.512, 24.0
2019-02-14 08:46:33,187 : samples : 320000
2019-02-14 08:46:43,459 : Image to text: 9.2, 26.7, 38.74, 18.0
2019-02-14 08:46:50,860 : Text to Image: 7.78, 23.32, 34.292, 22.0
2019-02-14 08:47:32,589 : samples : 384000
2019-02-14 08:47:42,707 : Image to text: 8.6, 25.02, 35.82, 20.0
2019-02-14 08:47:50,145 : Text to Image: 6.4, 20.884, 31.744, 24.0
2019-02-14 08:48:30,086 : samples : 448000
2019-02-14 08:48:40,201 : Image to text: 9.38, 25.9, 37.54, 19.0
2019-02-14 08:48:47,621 : Text to Image: 7.336, 22.832, 33.768, 22.0
2019-02-14 08:49:28,867 : samples : 512000
2019-02-14 08:49:38,987 : Image to text: 9.08, 26.34, 38.76, 18.0
2019-02-14 08:49:46,407 : Text to Image: 7.756, 22.832, 34.236, 22.0
2019-02-14 08:50:22,480 : Epoch 4 finished
2019-02-14 08:50:22,920 : Image to text: 22.9, 57.5, 72.0, 4.0
2019-02-14 08:50:23,252 : Text to Image: 19.24, 50.68, 68.06, 5.0
2019-02-14 08:50:23,685 : Image to text: 23.3, 53.3, 71.4, 5.0
2019-02-14 08:50:24,023 : Text to Image: 18.72, 48.62, 65.66, 6.0
2019-02-14 08:50:24,461 : Image to text: 24.2, 56.4, 72.4, 4.0
2019-02-14 08:50:24,792 : Text to Image: 19.3, 50.74, 67.38, 5.0
2019-02-14 08:50:25,221 : Image to text: 26.3, 57.4, 72.9, 4.0
2019-02-14 08:50:25,564 : Text to Image: 18.56, 48.98, 66.22, 6.0
2019-02-14 08:50:25,995 : Image to text: 27.3, 57.6, 72.9, 4.0
2019-02-14 08:50:26,327 : Text to Image: 19.34, 49.88, 66.28, 6.0
2019-02-14 08:50:26,327 : Dev mean Text to Image: 19.032, 49.78, 66.72, 5.6000000000000005
2019-02-14 08:50:26,327 : Dev mean Image to text: 24.8, 56.44, 72.32, 4.2
2019-02-14 08:50:26,328 : start epoch
2019-02-14 08:51:08,120 : samples : 64000
2019-02-14 08:51:18,438 : Image to text: 9.58, 26.48, 37.98, 19.0
2019-02-14 08:51:25,874 : Text to Image: 7.76, 23.34, 34.384, 21.0
2019-02-14 08:52:06,236 : samples : 128000
2019-02-14 08:52:16,467 : Image to text: 9.46, 27.06, 37.82, 18.0
2019-02-14 08:52:23,857 : Text to Image: 7.184, 22.68, 34.044, 22.0
2019-02-14 08:53:04,363 : samples : 192000
2019-02-14 08:53:14,596 : Image to text: 9.08, 26.36, 37.8, 19.0
2019-02-14 08:53:21,978 : Text to Image: 6.992, 21.852, 32.96, 23.0
2019-02-14 08:54:02,746 : samples : 256000
2019-02-14 08:54:13,034 : Image to text: 9.16, 26.3, 37.76, 18.0
2019-02-14 08:54:20,402 : Text to Image: 7.984, 23.724, 35.176, 20.0
2019-02-14 08:55:00,804 : samples : 320000
2019-02-14 08:55:11,050 : Image to text: 9.5, 25.92, 37.78, 19.0
2019-02-14 08:55:18,435 : Text to Image: 6.704, 21.248, 31.968, 24.0
2019-02-14 08:55:58,875 : samples : 384000
2019-02-14 08:56:09,040 : Image to text: 9.62, 26.6, 38.72, 18.0
2019-02-14 08:56:16,494 : Text to Image: 7.9, 23.076, 34.072, 22.0
2019-02-14 08:56:56,879 : samples : 448000
2019-02-14 08:57:07,048 : Image to text: 8.66, 25.06, 37.54, 18.0
2019-02-14 08:57:14,422 : Text to Image: 7.184, 22.128, 33.732, 22.0
2019-02-14 08:57:54,999 : samples : 512000
2019-02-14 08:58:05,204 : Image to text: 9.82, 27.28, 40.0, 17.0
2019-02-14 08:58:12,646 : Text to Image: 7.856, 23.84, 35.368, 20.0
2019-02-14 08:58:47,157 : Epoch 5 finished
2019-02-14 08:58:47,587 : Image to text: 23.1, 55.9, 71.4, 4.0
2019-02-14 08:58:47,918 : Text to Image: 20.54, 51.82, 69.44, 5.0
2019-02-14 08:58:48,359 : Image to text: 20.5, 54.3, 71.2, 5.0
2019-02-14 08:58:48,689 : Text to Image: 19.66, 50.64, 68.38, 5.0
2019-02-14 08:58:49,117 : Image to text: 25.2, 54.7, 70.9, 4.0
2019-02-14 08:58:49,447 : Text to Image: 20.34, 52.08, 68.7, 5.0
2019-02-14 08:58:49,887 : Image to text: 26.1, 56.7, 73.0, 4.0
2019-02-14 08:58:50,218 : Text to Image: 19.92, 51.58, 69.0, 5.0
2019-02-14 08:58:50,648 : Image to text: 25.6, 58.4, 74.1, 4.0
2019-02-14 08:58:50,979 : Text to Image: 20.74, 52.14, 69.8, 5.0
2019-02-14 08:58:50,979 : Dev mean Text to Image: 20.24, 51.652, 69.064, 5.0
2019-02-14 08:58:50,979 : Dev mean Image to text: 24.099999999999998, 56.00000000000001, 72.12, 4.2
2019-02-14 08:58:50,979 : start epoch
2019-02-14 08:59:34,894 : samples : 64000
2019-02-14 08:59:45,218 : Image to text: 10.56, 27.84, 39.88, 17.0
2019-02-14 08:59:52,628 : Text to Image: 8.0, 23.792, 35.412, 20.0
2019-02-14 09:00:33,253 : samples : 128000
2019-02-14 09:00:43,504 : Image to text: 8.48, 25.16, 37.32, 18.0
2019-02-14 09:00:50,892 : Text to Image: 7.048, 22.088, 33.16, 23.0
2019-02-14 09:01:31,601 : samples : 192000
2019-02-14 09:01:41,882 : Image to text: 9.16, 26.1, 37.64, 18.0
2019-02-14 09:01:49,301 : Text to Image: 7.22, 22.024, 33.604, 22.0
2019-02-14 09:02:30,049 : samples : 256000
2019-02-14 09:02:40,470 : Image to text: 10.04, 27.48, 38.62, 18.0
2019-02-14 09:02:47,886 : Text to Image: 7.612, 23.248, 34.372, 21.0
2019-02-14 09:03:29,873 : samples : 320000
2019-02-14 09:03:40,237 : Image to text: 9.46, 26.94, 38.86, 18.0
2019-02-14 09:03:47,645 : Text to Image: 7.816, 23.408, 34.78, 21.0
2019-02-14 09:04:30,486 : samples : 384000
2019-02-14 09:04:40,673 : Image to text: 9.28, 26.26, 37.94, 18.0
2019-02-14 09:04:48,138 : Text to Image: 7.508, 22.768, 33.632, 22.0
2019-02-14 09:05:30,992 : samples : 448000
2019-02-14 09:05:41,141 : Image to text: 9.98, 28.74, 40.58, 16.0
2019-02-14 09:05:48,640 : Text to Image: 8.356, 24.48, 36.304, 20.0
2019-02-14 09:06:31,118 : samples : 512000
2019-02-14 09:06:41,264 : Image to text: 9.92, 27.94, 40.06, 17.0
2019-02-14 09:06:48,698 : Text to Image: 7.932, 23.232, 34.876, 21.0
2019-02-14 09:07:24,790 : Epoch 6 finished
2019-02-14 09:07:25,228 : Image to text: 24.8, 59.0, 74.4, 4.0
2019-02-14 09:07:25,558 : Text to Image: 21.52, 53.36, 70.88, 5.0
2019-02-14 09:07:25,988 : Image to text: 25.0, 56.7, 73.8, 4.0
2019-02-14 09:07:26,318 : Text to Image: 21.14, 52.36, 70.32, 5.0
2019-02-14 09:07:26,760 : Image to text: 26.4, 57.7, 72.0, 4.0
2019-02-14 09:07:27,091 : Text to Image: 21.6, 53.5, 70.76, 5.0
2019-02-14 09:07:27,524 : Image to text: 26.7, 59.8, 73.9, 4.0
2019-02-14 09:07:27,855 : Text to Image: 20.94, 53.32, 70.32, 5.0
2019-02-14 09:07:28,296 : Image to text: 28.5, 58.9, 74.3, 4.0
2019-02-14 09:07:28,625 : Text to Image: 22.2, 53.46, 70.64, 5.0
2019-02-14 09:07:28,626 : Dev mean Text to Image: 21.479999999999997, 53.199999999999996, 70.584, 5.0
2019-02-14 09:07:28,626 : Dev mean Image to text: 26.279999999999998, 58.42, 73.68, 4.0
2019-02-14 09:07:28,626 : start epoch
2019-02-14 09:08:09,435 : samples : 64000
2019-02-14 09:08:19,715 : Image to text: 9.66, 28.36, 40.04, 17.0
2019-02-14 09:08:27,132 : Text to Image: 8.404, 25.284, 36.696, 19.0
2019-02-14 09:09:07,749 : samples : 128000
2019-02-14 09:09:18,037 : Image to text: 10.4, 27.74, 39.94, 17.0
2019-02-14 09:09:25,455 : Text to Image: 7.668, 22.54, 33.988, 22.0
2019-02-14 09:10:07,657 : samples : 192000
2019-02-14 09:10:18,066 : Image to text: 9.82, 26.84, 39.32, 17.0
2019-02-14 09:10:25,493 : Text to Image: 7.936, 23.44, 34.952, 21.0
2019-02-14 09:11:07,347 : samples : 256000
2019-02-14 09:11:17,731 : Image to text: 9.68, 28.24, 40.1, 17.0
2019-02-14 09:11:25,190 : Text to Image: 8.008, 24.052, 35.672, 21.0
2019-02-14 09:12:07,243 : samples : 320000
2019-02-14 09:12:17,628 : Image to text: 10.24, 28.28, 39.94, 17.0
2019-02-14 09:12:25,076 : Text to Image: 8.284, 24.084, 35.892, 20.0
2019-02-14 09:13:08,916 : samples : 384000
2019-02-14 09:13:19,105 : Image to text: 9.6, 27.04, 39.16, 17.0
2019-02-14 09:13:26,646 : Text to Image: 7.86, 23.556, 34.82, 21.0
2019-02-14 09:14:08,055 : samples : 448000
2019-02-14 09:14:18,271 : Image to text: 8.94, 27.06, 39.4, 17.0
2019-02-14 09:14:25,787 : Text to Image: 8.236, 24.116, 35.504, 20.0
2019-02-14 09:15:06,156 : samples : 512000
2019-02-14 09:15:16,386 : Image to text: 10.48, 28.72, 41.16, 16.0
2019-02-14 09:15:23,914 : Text to Image: 8.524, 24.532, 35.896, 20.0
2019-02-14 09:15:58,510 : Epoch 7 finished
2019-02-14 09:15:58,959 : Image to text: 25.1, 57.7, 75.2, 4.0
2019-02-14 09:15:59,292 : Text to Image: 21.88, 54.94, 72.12, 5.0
2019-02-14 09:15:59,725 : Image to text: 22.5, 54.8, 73.0, 4.0
2019-02-14 09:16:00,057 : Text to Image: 22.5, 54.5, 72.5, 5.0
2019-02-14 09:16:00,489 : Image to text: 25.6, 57.7, 74.0, 4.0
2019-02-14 09:16:00,820 : Text to Image: 22.18, 55.64, 71.86, 4.0
2019-02-14 09:16:01,263 : Image to text: 26.7, 57.4, 73.4, 4.0
2019-02-14 09:16:01,593 : Text to Image: 21.72, 53.92, 72.06, 5.0
2019-02-14 09:16:02,021 : Image to text: 27.0, 58.2, 73.4, 4.0
2019-02-14 09:16:02,351 : Text to Image: 21.88, 53.56, 70.52, 5.0
2019-02-14 09:16:02,351 : Dev mean Text to Image: 22.031999999999996, 54.512, 71.812, 4.8
2019-02-14 09:16:02,351 : Dev mean Image to text: 25.380000000000003, 57.16, 73.8, 4.0
2019-02-14 09:16:02,352 : start epoch
2019-02-14 09:16:44,292 : samples : 64000
2019-02-14 09:16:54,692 : Image to text: 10.24, 27.14, 38.88, 18.0
2019-02-14 09:17:02,140 : Text to Image: 7.464, 22.872, 33.856, 22.0
2019-02-14 09:17:43,764 : samples : 128000
2019-02-14 09:17:54,063 : Image to text: 10.24, 28.62, 40.4, 16.0
2019-02-14 09:18:01,467 : Text to Image: 8.492, 24.928, 36.84, 19.0
2019-02-14 09:18:43,135 : samples : 192000
2019-02-14 09:18:53,427 : Image to text: 10.34, 28.2, 40.44, 17.0
2019-02-14 09:19:00,852 : Text to Image: 8.096, 24.748, 35.928, 20.0
2019-02-14 09:19:41,791 : samples : 256000
2019-02-14 09:19:52,032 : Image to text: 10.12, 27.82, 39.9, 17.0
2019-02-14 09:19:59,412 : Text to Image: 7.98, 24.424, 36.348, 20.0
2019-02-14 09:20:40,374 : samples : 320000
2019-02-14 09:20:50,673 : Image to text: 11.3, 29.28, 41.4, 16.0
2019-02-14 09:20:58,095 : Text to Image: 9.0, 25.744, 37.288, 19.0
2019-02-14 09:21:42,810 : samples : 384000
2019-02-14 09:21:52,955 : Image to text: 9.46, 27.42, 39.78, 17.0
2019-02-14 09:22:00,481 : Text to Image: 8.372, 24.324, 35.692, 20.0
2019-02-14 09:22:41,460 : samples : 448000
2019-02-14 09:22:51,650 : Image to text: 9.9, 27.92, 40.5, 16.0
2019-02-14 09:22:59,145 : Text to Image: 8.196, 24.08, 35.824, 20.0
2019-02-14 09:23:39,723 : samples : 512000
2019-02-14 09:23:49,892 : Image to text: 9.12, 27.16, 39.3, 17.0
2019-02-14 09:23:57,379 : Text to Image: 8.0, 23.876, 35.084, 21.0
2019-02-14 09:24:32,211 : Epoch 8 finished
2019-02-14 09:24:32,664 : Image to text: 25.7, 58.9, 75.5, 4.0
2019-02-14 09:24:32,999 : Text to Image: 21.44, 54.78, 71.62, 5.0
2019-02-14 09:24:33,443 : Image to text: 23.9, 58.5, 73.9, 4.0
2019-02-14 09:24:33,776 : Text to Image: 19.98, 52.1, 69.94, 5.0
2019-02-14 09:24:34,220 : Image to text: 27.1, 59.3, 74.3, 4.0
2019-02-14 09:24:34,552 : Text to Image: 21.78, 54.2, 70.66, 5.0
2019-02-14 09:24:34,983 : Image to text: 26.8, 57.4, 75.1, 4.0
2019-02-14 09:24:35,315 : Text to Image: 19.82, 53.22, 71.04, 5.0
2019-02-14 09:24:35,743 : Image to text: 28.0, 59.1, 74.1, 4.0
2019-02-14 09:24:36,073 : Text to Image: 20.78, 53.7, 69.86, 5.0
2019-02-14 09:24:36,073 : Dev mean Text to Image: 20.759999999999998, 53.599999999999994, 70.624, 5.0
2019-02-14 09:24:36,073 : Dev mean Image to text: 26.299999999999997, 58.63999999999999, 74.58, 4.0
2019-02-14 09:24:36,073 : start epoch
2019-02-14 09:25:17,333 : samples : 64000
2019-02-14 09:25:27,725 : Image to text: 9.98, 28.08, 40.38, 16.0
2019-02-14 09:25:35,192 : Text to Image: 8.356, 24.824, 36.28, 20.0
2019-02-14 09:26:16,523 : samples : 128000
2019-02-14 09:26:26,858 : Image to text: 9.78, 28.36, 40.64, 16.0
2019-02-14 09:26:34,274 : Text to Image: 7.828, 23.708, 35.352, 21.0
2019-02-14 09:27:14,863 : samples : 192000
2019-02-14 09:27:25,260 : Image to text: 10.38, 28.82, 40.96, 16.0
2019-02-14 09:27:32,724 : Text to Image: 8.096, 24.188, 35.544, 20.0
2019-02-14 09:28:15,658 : samples : 256000
2019-02-14 09:28:25,980 : Image to text: 10.72, 29.32, 41.44, 16.0
2019-02-14 09:28:33,432 : Text to Image: 7.972, 24.056, 35.752, 20.0
2019-02-14 09:29:17,414 : samples : 320000
2019-02-14 09:29:27,806 : Image to text: 10.34, 28.72, 40.58, 16.0
2019-02-14 09:29:35,269 : Text to Image: 8.296, 24.568, 36.288, 19.0
2019-02-14 09:30:16,074 : samples : 384000
2019-02-14 09:30:26,323 : Image to text: 9.7, 28.22, 40.48, 16.0
2019-02-14 09:30:33,786 : Text to Image: 8.216, 24.048, 35.604, 20.0
2019-02-14 09:31:21,734 : samples : 448000
2019-02-14 09:31:31,922 : Image to text: 10.38, 28.5, 40.94, 16.0
2019-02-14 09:31:39,453 : Text to Image: 8.508, 24.68, 36.5, 19.0
2019-02-14 09:32:21,242 : samples : 512000
2019-02-14 09:32:31,387 : Image to text: 10.74, 28.8, 41.34, 15.0
2019-02-14 09:32:38,900 : Text to Image: 8.828, 25.716, 37.408, 18.0
2019-02-14 09:33:14,380 : Epoch 9 finished
2019-02-14 09:33:14,803 : Image to text: 24.7, 59.3, 75.5, 4.0
2019-02-14 09:33:15,131 : Text to Image: 22.88, 55.68, 72.8, 4.0
2019-02-14 09:33:15,566 : Image to text: 25.7, 58.5, 74.1, 4.0
2019-02-14 09:33:15,892 : Text to Image: 22.3, 53.46, 71.72, 5.0
2019-02-14 09:33:16,316 : Image to text: 28.1, 60.5, 74.2, 4.0
2019-02-14 09:33:16,644 : Text to Image: 22.0, 55.9, 72.44, 4.0
2019-02-14 09:33:17,084 : Image to text: 27.4, 59.6, 75.8, 4.0
2019-02-14 09:33:17,414 : Text to Image: 21.82, 55.26, 72.36, 5.0
2019-02-14 09:33:17,845 : Image to text: 28.4, 62.4, 75.4, 4.0
2019-02-14 09:33:18,176 : Text to Image: 23.32, 55.58, 70.92, 4.0
2019-02-14 09:33:18,176 : Dev mean Text to Image: 22.464, 55.175999999999995, 72.048, 4.4
2019-02-14 09:33:18,176 : Dev mean Image to text: 26.86, 60.06, 75.0, 4.0
2019-02-14 09:33:18,177 : start epoch
2019-02-14 09:33:59,649 : samples : 64000
2019-02-14 09:34:09,871 : Image to text: 9.64, 27.88, 39.98, 17.0
2019-02-14 09:34:17,268 : Text to Image: 7.884, 23.656, 35.244, 21.0
2019-02-14 09:34:57,909 : samples : 128000
2019-02-14 09:35:08,157 : Image to text: 10.38, 29.24, 41.68, 15.0
2019-02-14 09:35:15,545 : Text to Image: 8.976, 25.768, 37.724, 19.0
2019-02-14 09:35:56,095 : samples : 192000
2019-02-14 09:36:06,400 : Image to text: 10.96, 29.16, 41.1, 15.0
2019-02-14 09:36:13,849 : Text to Image: 7.892, 24.992, 36.732, 20.0
2019-02-14 09:36:54,909 : samples : 256000
2019-02-14 09:37:05,267 : Image to text: 10.96, 28.94, 41.98, 15.0
2019-02-14 09:37:12,691 : Text to Image: 8.52, 24.876, 37.208, 19.0
2019-02-14 09:37:53,666 : samples : 320000
2019-02-14 09:38:03,990 : Image to text: 10.68, 28.08, 40.98, 16.0
2019-02-14 09:38:11,429 : Text to Image: 8.232, 24.668, 36.452, 20.0
2019-02-14 09:38:54,260 : samples : 384000
2019-02-14 09:39:04,498 : Image to text: 10.54, 29.04, 42.48, 15.0
2019-02-14 09:39:11,983 : Text to Image: 8.168, 25.156, 37.072, 19.0
2019-02-14 09:39:52,939 : samples : 448000
2019-02-14 09:40:03,106 : Image to text: 9.3, 28.0, 40.58, 16.0
2019-02-14 09:40:10,573 : Text to Image: 7.92, 23.72, 35.068, 21.0
2019-02-14 09:40:52,302 : samples : 512000
2019-02-14 09:41:02,465 : Image to text: 10.32, 28.72, 41.04, 15.0
2019-02-14 09:41:09,932 : Text to Image: 8.5, 25.432, 37.4, 19.0
2019-02-14 09:41:44,538 : Epoch 10 finished
2019-02-14 09:41:44,967 : Image to text: 25.0, 59.6, 76.0, 4.0
2019-02-14 09:41:45,294 : Text to Image: 22.5, 55.22, 72.02, 5.0
2019-02-14 09:41:45,717 : Image to text: 24.2, 58.3, 75.1, 4.0
2019-02-14 09:41:46,043 : Text to Image: 21.88, 54.92, 71.56, 5.0
2019-02-14 09:41:46,469 : Image to text: 29.1, 60.1, 75.1, 4.0
2019-02-14 09:41:46,800 : Text to Image: 21.98, 55.5, 71.58, 5.0
2019-02-14 09:41:47,244 : Image to text: 26.6, 59.9, 74.9, 4.0
2019-02-14 09:41:47,577 : Text to Image: 21.38, 54.6, 72.78, 5.0
2019-02-14 09:41:48,011 : Image to text: 30.4, 61.4, 76.3, 3.0
2019-02-14 09:41:48,343 : Text to Image: 22.8, 54.74, 71.66, 5.0
2019-02-14 09:41:48,343 : Dev mean Text to Image: 22.107999999999997, 54.996, 71.91999999999999, 5.0
2019-02-14 09:41:48,343 : Dev mean Image to text: 27.060000000000002, 59.86, 75.48, 3.8000000000000003
2019-02-14 09:41:48,343 : start epoch
2019-02-14 09:42:29,693 : samples : 64000
2019-02-14 09:42:40,040 : Image to text: 10.42, 28.02, 40.2, 16.0
2019-02-14 09:42:47,489 : Text to Image: 8.16, 24.864, 36.604, 20.0
2019-02-14 09:43:29,085 : samples : 128000
2019-02-14 09:43:39,428 : Image to text: 10.48, 29.22, 41.26, 16.0
2019-02-14 09:43:46,876 : Text to Image: 8.556, 24.972, 36.86, 19.0
2019-02-14 09:44:28,047 : samples : 192000
2019-02-14 09:44:38,394 : Image to text: 10.82, 28.34, 40.74, 16.0
2019-02-14 09:44:45,812 : Text to Image: 8.012, 24.288, 36.228, 20.0
2019-02-14 09:45:26,743 : samples : 256000
2019-02-14 09:45:37,049 : Image to text: 10.4, 28.8, 41.54, 16.0
2019-02-14 09:45:44,468 : Text to Image: 8.56, 25.284, 36.94, 19.0
2019-02-14 09:46:25,390 : samples : 320000
2019-02-14 09:46:35,670 : Image to text: 10.0, 28.7, 40.96, 15.0
2019-02-14 09:46:43,081 : Text to Image: 7.952, 24.26, 36.028, 20.0
2019-02-14 09:47:25,084 : samples : 384000
2019-02-14 09:47:35,238 : Image to text: 10.84, 28.4, 41.24, 15.0
2019-02-14 09:47:42,760 : Text to Image: 8.304, 24.796, 36.596, 20.0
2019-02-14 09:48:29,555 : samples : 448000
2019-02-14 09:48:39,729 : Image to text: 10.98, 28.98, 41.44, 15.0
2019-02-14 09:48:47,237 : Text to Image: 8.944, 26.224, 38.228, 18.0
2019-02-14 09:49:31,058 : samples : 512000
2019-02-14 09:49:41,223 : Image to text: 10.58, 29.54, 42.26, 15.0
2019-02-14 09:49:48,720 : Text to Image: 8.596, 24.908, 36.716, 19.0
2019-02-14 09:50:23,554 : Epoch 11 finished
2019-02-14 09:50:23,986 : Image to text: 26.6, 59.9, 76.3, 4.0
2019-02-14 09:50:24,314 : Text to Image: 22.82, 56.18, 72.76, 4.0
2019-02-14 09:50:24,743 : Image to text: 24.9, 57.5, 75.1, 4.0
2019-02-14 09:50:25,074 : Text to Image: 22.92, 54.5, 72.98, 5.0
2019-02-14 09:50:25,518 : Image to text: 28.0, 59.8, 73.8, 4.0
2019-02-14 09:50:25,850 : Text to Image: 22.66, 55.6, 72.56, 4.0
2019-02-14 09:50:26,280 : Image to text: 26.9, 59.5, 75.1, 4.0
2019-02-14 09:50:26,611 : Text to Image: 21.64, 55.96, 73.42, 4.0
2019-02-14 09:50:27,039 : Image to text: 27.2, 60.9, 76.4, 4.0
2019-02-14 09:50:27,368 : Text to Image: 22.58, 55.46, 72.5, 4.0
2019-02-14 09:50:27,368 : Dev mean Text to Image: 22.524, 55.54, 72.84400000000001, 4.2
2019-02-14 09:50:27,368 : Dev mean Image to text: 26.72, 59.519999999999996, 75.34, 4.0
2019-02-14 09:50:27,369 : start epoch
2019-02-14 09:51:08,488 : samples : 64000
2019-02-14 09:51:18,777 : Image to text: 10.82, 29.96, 42.3, 15.0
2019-02-14 09:51:26,180 : Text to Image: 9.08, 25.828, 37.928, 18.0
2019-02-14 09:52:06,753 : samples : 128000
2019-02-14 09:52:17,044 : Image to text: 10.8, 29.98, 42.7, 15.0
2019-02-14 09:52:24,452 : Text to Image: 8.796, 25.272, 36.924, 19.0
2019-02-14 09:53:05,020 : samples : 192000
2019-02-14 09:53:15,300 : Image to text: 10.52, 28.66, 40.98, 16.0
2019-02-14 09:53:22,712 : Text to Image: 8.74, 25.104, 37.596, 18.0
2019-02-14 09:54:03,551 : samples : 256000
2019-02-14 09:54:13,851 : Image to text: 10.88, 30.22, 42.42, 15.0
2019-02-14 09:54:21,257 : Text to Image: 8.46, 25.732, 37.804, 18.0
2019-02-14 09:55:02,776 : samples : 320000
2019-02-14 09:55:13,057 : Image to text: 10.24, 29.36, 42.14, 15.0
2019-02-14 09:55:20,454 : Text to Image: 8.704, 25.376, 37.568, 19.0
2019-02-14 09:56:01,646 : samples : 384000
2019-02-14 09:56:11,863 : Image to text: 11.28, 29.3, 42.8, 15.0
2019-02-14 09:56:19,344 : Text to Image: 8.896, 26.248, 38.56, 18.0
2019-02-14 09:57:00,561 : samples : 448000
2019-02-14 09:57:10,737 : Image to text: 10.96, 29.46, 41.8, 15.0
2019-02-14 09:57:18,233 : Text to Image: 9.176, 26.356, 38.004, 18.0
2019-02-14 09:58:00,618 : samples : 512000
2019-02-14 09:58:10,847 : Image to text: 10.52, 29.5, 42.1, 15.0
2019-02-14 09:58:18,342 : Text to Image: 9.036, 26.336, 38.264, 18.0
2019-02-14 09:58:53,927 : Epoch 12 finished
2019-02-14 09:58:54,351 : Image to text: 24.7, 58.5, 73.8, 4.0
2019-02-14 09:58:54,678 : Text to Image: 22.02, 54.38, 71.74, 5.0
2019-02-14 09:58:55,113 : Image to text: 24.4, 58.9, 74.5, 4.0
2019-02-14 09:58:55,440 : Text to Image: 21.36, 54.04, 71.28, 5.0
2019-02-14 09:58:55,862 : Image to text: 27.2, 58.6, 74.9, 4.0
2019-02-14 09:58:56,189 : Text to Image: 22.6, 55.36, 71.66, 4.0
2019-02-14 09:58:56,611 : Image to text: 27.2, 58.3, 74.6, 4.0
2019-02-14 09:58:56,938 : Text to Image: 21.72, 53.98, 71.46, 5.0
2019-02-14 09:58:57,372 : Image to text: 28.5, 60.6, 77.2, 4.0
2019-02-14 09:58:57,699 : Text to Image: 22.14, 54.6, 71.58, 5.0
2019-02-14 09:58:57,700 : Dev mean Text to Image: 21.968, 54.472, 71.544, 4.8
2019-02-14 09:58:57,700 : Dev mean Image to text: 26.4, 58.980000000000004, 75.0, 4.0
2019-02-14 09:58:57,700 : start epoch
2019-02-14 09:59:38,559 : samples : 64000
2019-02-14 09:59:48,855 : Image to text: 10.94, 28.8, 41.64, 15.0
2019-02-14 09:59:56,221 : Text to Image: 8.744, 25.688, 37.616, 18.0
2019-02-14 10:00:38,614 : samples : 128000
2019-02-14 10:00:48,840 : Image to text: 10.84, 29.54, 42.16, 15.0
2019-02-14 10:00:56,216 : Text to Image: 8.492, 25.264, 37.184, 19.0
2019-02-14 10:01:36,736 : samples : 192000
2019-02-14 10:01:46,968 : Image to text: 10.38, 28.4, 42.4, 14.0
2019-02-14 10:01:54,363 : Text to Image: 9.024, 26.172, 38.156, 18.0
2019-02-14 10:02:34,945 : samples : 256000
2019-02-14 10:02:45,259 : Image to text: 11.02, 30.0, 42.4, 15.0
2019-02-14 10:02:52,655 : Text to Image: 8.98, 25.824, 37.552, 18.0
2019-02-14 10:03:33,129 : samples : 320000
2019-02-14 10:03:43,349 : Image to text: 11.04, 29.1, 41.76, 15.0
2019-02-14 10:03:50,736 : Text to Image: 8.636, 25.688, 37.692, 19.0
2019-02-14 10:04:31,145 : samples : 384000
2019-02-14 10:04:41,349 : Image to text: 10.58, 29.36, 41.78, 15.0
2019-02-14 10:04:48,828 : Text to Image: 8.84, 25.548, 37.564, 18.0
2019-02-14 10:05:29,807 : samples : 448000
2019-02-14 10:05:40,009 : Image to text: 10.64, 29.3, 42.22, 15.0
2019-02-14 10:05:47,478 : Text to Image: 8.348, 25.156, 37.252, 19.0
2019-02-14 10:06:29,532 : samples : 512000
2019-02-14 10:06:39,691 : Image to text: 10.86, 29.34, 41.78, 15.0
2019-02-14 10:06:47,201 : Text to Image: 8.668, 25.224, 37.576, 19.0
2019-02-14 10:07:23,295 : Epoch 13 finished
2019-02-14 10:07:23,724 : Image to text: 26.7, 59.5, 75.3, 4.0
2019-02-14 10:07:24,053 : Text to Image: 23.12, 56.68, 73.32, 4.0
2019-02-14 10:07:24,491 : Image to text: 27.8, 57.8, 75.4, 4.0
2019-02-14 10:07:24,820 : Text to Image: 23.0, 54.66, 73.18, 5.0
2019-02-14 10:07:25,246 : Image to text: 28.3, 59.5, 75.3, 4.0
2019-02-14 10:07:25,575 : Text to Image: 21.7, 55.94, 72.58, 4.0
2019-02-14 10:07:26,004 : Image to text: 28.9, 59.7, 75.6, 4.0
2019-02-14 10:07:26,336 : Text to Image: 21.92, 55.92, 73.4, 4.0
2019-02-14 10:07:26,770 : Image to text: 30.3, 59.8, 75.6, 4.0
2019-02-14 10:07:27,104 : Text to Image: 23.6, 55.86, 72.78, 4.0
2019-02-14 10:07:27,104 : Dev mean Text to Image: 22.668, 55.812, 73.05199999999999, 4.2
2019-02-14 10:07:27,104 : Dev mean Image to text: 28.400000000000006, 59.26, 75.44, 4.0
2019-02-14 10:07:27,104 : start epoch
2019-02-14 10:08:07,935 : samples : 64000
2019-02-14 10:08:18,287 : Image to text: 10.34, 29.28, 41.56, 15.0
2019-02-14 10:08:25,716 : Text to Image: 8.86, 25.868, 37.596, 18.0
2019-02-14 10:09:06,467 : samples : 128000
2019-02-14 10:09:16,799 : Image to text: 10.94, 29.6, 41.28, 15.0
2019-02-14 10:09:24,236 : Text to Image: 8.656, 26.32, 38.348, 18.0
2019-02-14 10:10:05,009 : samples : 192000
2019-02-14 10:10:15,294 : Image to text: 11.3, 31.12, 42.96, 15.0
2019-02-14 10:10:22,704 : Text to Image: 9.008, 26.524, 38.364, 18.0
2019-02-14 10:11:04,870 : samples : 256000
2019-02-14 10:11:15,200 : Image to text: 10.66, 30.82, 43.0, 14.0
2019-02-14 10:11:22,604 : Text to Image: 9.216, 26.256, 38.244, 18.0
2019-02-14 10:12:04,240 : samples : 320000
2019-02-14 10:12:14,525 : Image to text: 10.64, 29.44, 42.32, 15.0
2019-02-14 10:12:21,925 : Text to Image: 8.588, 25.252, 37.12, 19.0
2019-02-14 10:13:03,838 : samples : 384000
2019-02-14 10:13:14,061 : Image to text: 10.56, 29.56, 41.78, 15.0
2019-02-14 10:13:21,590 : Text to Image: 8.924, 25.768, 38.092, 18.0
2019-02-14 10:14:04,389 : samples : 448000
2019-02-14 10:14:14,604 : Image to text: 10.82, 29.32, 41.6, 15.0
2019-02-14 10:14:22,137 : Text to Image: 9.152, 26.344, 38.556, 18.0
2019-02-14 10:15:04,257 : samples : 512000
2019-02-14 10:15:14,431 : Image to text: 11.02, 30.06, 42.56, 15.0
2019-02-14 10:15:21,959 : Text to Image: 9.092, 26.116, 37.916, 18.0
2019-02-14 10:15:56,562 : Epoch 14 finished
2019-02-14 10:15:56,997 : Image to text: 27.4, 62.1, 77.6, 3.0
2019-02-14 10:15:57,327 : Text to Image: 23.52, 57.24, 74.06, 4.0
2019-02-14 10:15:57,755 : Image to text: 25.3, 59.0, 75.5, 4.0
2019-02-14 10:15:58,084 : Text to Image: 23.7, 55.9, 73.48, 5.0
2019-02-14 10:15:58,526 : Image to text: 28.7, 61.3, 75.2, 4.0
2019-02-14 10:15:58,857 : Text to Image: 23.92, 57.86, 73.8, 4.0
2019-02-14 10:15:59,291 : Image to text: 30.1, 61.8, 77.5, 3.0
2019-02-14 10:15:59,638 : Text to Image: 22.82, 57.16, 74.0, 4.0
2019-02-14 10:16:00,079 : Image to text: 27.7, 63.3, 77.0, 3.0
2019-02-14 10:16:00,414 : Text to Image: 24.34, 56.8, 73.24, 4.0
2019-02-14 10:16:00,414 : Dev mean Text to Image: 23.660000000000004, 56.992000000000004, 73.716, 4.2
2019-02-14 10:16:00,414 : Dev mean Image to text: 27.84, 61.5, 76.56, 3.4000000000000004
2019-02-14 10:16:00,414 : start epoch
2019-02-14 10:16:41,409 : samples : 64000
2019-02-14 10:16:51,750 : Image to text: 10.92, 29.94, 42.3, 15.0
2019-02-14 10:16:59,203 : Text to Image: 9.312, 26.76, 38.432, 18.0
2019-02-14 10:17:40,018 : samples : 128000
2019-02-14 10:17:50,381 : Image to text: 10.66, 30.62, 42.88, 15.0
2019-02-14 10:17:57,806 : Text to Image: 9.456, 26.696, 38.952, 17.0
2019-02-14 10:18:39,291 : samples : 192000
2019-02-14 10:18:49,672 : Image to text: 11.88, 30.74, 43.52, 14.0
2019-02-14 10:18:57,092 : Text to Image: 9.396, 26.592, 38.732, 18.0
2019-02-14 10:19:38,397 : samples : 256000
2019-02-14 10:19:48,698 : Image to text: 11.0, 29.22, 41.7, 16.0
2019-02-14 10:19:56,113 : Text to Image: 8.484, 25.368, 37.252, 19.0
2019-02-14 10:20:37,814 : samples : 320000
2019-02-14 10:20:48,120 : Image to text: 10.08, 28.32, 41.88, 15.0
2019-02-14 10:20:55,543 : Text to Image: 8.86, 26.052, 38.18, 18.0
2019-02-14 10:21:38,287 : samples : 384000
2019-02-14 10:21:48,485 : Image to text: 11.12, 29.8, 42.28, 15.0
2019-02-14 10:21:56,103 : Text to Image: 8.748, 26.304, 38.084, 18.0
2019-02-14 10:22:37,993 : samples : 448000
2019-02-14 10:22:48,287 : Image to text: 10.88, 29.42, 42.26, 15.0
2019-02-14 10:22:55,871 : Text to Image: 9.16, 26.22, 38.136, 18.0
2019-02-14 10:23:37,374 : samples : 512000
2019-02-14 10:23:47,605 : Image to text: 10.64, 29.86, 43.36, 14.0
2019-02-14 10:23:55,215 : Text to Image: 8.948, 25.512, 37.236, 19.0
2019-02-14 10:24:30,996 : Epoch 15 finished
2019-02-14 10:24:31,437 : Image to text: 28.5, 61.2, 75.0, 4.0
2019-02-14 10:24:31,768 : Text to Image: 23.82, 57.88, 74.3, 4.0
2019-02-14 10:24:32,199 : Image to text: 26.5, 60.0, 76.6, 4.0
2019-02-14 10:24:32,529 : Text to Image: 23.46, 55.74, 73.24, 4.0
2019-02-14 10:24:32,970 : Image to text: 28.0, 59.8, 74.2, 4.0
2019-02-14 10:24:33,300 : Text to Image: 23.48, 56.96, 73.34, 4.0
2019-02-14 10:24:33,727 : Image to text: 25.6, 61.7, 76.2, 3.0
2019-02-14 10:24:34,055 : Text to Image: 23.2, 56.8, 74.0, 4.0
2019-02-14 10:24:34,481 : Image to text: 29.3, 61.0, 77.1, 4.0
2019-02-14 10:24:34,811 : Text to Image: 23.96, 57.56, 73.88, 4.0
2019-02-14 10:24:34,811 : Dev mean Text to Image: 23.583999999999996, 56.988, 73.752, 4.0
2019-02-14 10:24:34,811 : Dev mean Image to text: 27.580000000000002, 60.74000000000001, 75.82, 3.8000000000000007
2019-02-14 10:24:38,636 : 
Test scores | Image to text:             27.92, 61.58, 76.16000000000001, 3.1999999999999997
2019-02-14 10:24:38,636 : Test scores | Text to image:             23.136, 56.58, 73.34, 4.0

2019-02-14 10:24:38,728 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 10:24:39,096 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 10:24:39,765 : loading BERT model bert-base-uncased
2019-02-14 10:24:39,765 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:24:39,797 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:24:39,797 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxyq6l4xw
2019-02-14 10:24:42,251 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:24:43,714 : Computing embeddings for train/dev/test
2019-02-14 10:26:19,442 : Computed embeddings
2019-02-14 10:26:19,442 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:26:46,619 : [('reg:1e-05', 74.52), ('reg:0.0001', 62.66), ('reg:0.001', 44.94), ('reg:0.01', 26.64)]
2019-02-14 10:26:46,619 : Validation : best param found is reg = 1e-05 with score             74.52
2019-02-14 10:26:46,620 : Evaluating...
2019-02-14 10:26:54,020 : 
Dev acc : 74.5 Test acc : 74.4 for LENGTH classification

2019-02-14 10:26:54,021 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 10:26:54,369 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 10:26:54,418 : loading BERT model bert-base-uncased
2019-02-14 10:26:54,418 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:26:54,452 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:26:54,453 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3gox0ahr
2019-02-14 10:26:56,925 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:26:58,382 : Computing embeddings for train/dev/test
2019-02-14 10:28:27,549 : Computed embeddings
2019-02-14 10:28:27,549 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:28:57,565 : [('reg:1e-05', 0.54), ('reg:0.0001', 0.17), ('reg:0.001', 0.11), ('reg:0.01', 0.13)]
2019-02-14 10:28:57,565 : Validation : best param found is reg = 1e-05 with score             0.54
2019-02-14 10:28:57,565 : Evaluating...
2019-02-14 10:29:04,275 : 
Dev acc : 0.5 Test acc : 0.5 for WORDCONTENT classification

2019-02-14 10:29:04,277 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 10:29:04,629 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 10:29:04,694 : loading BERT model bert-base-uncased
2019-02-14 10:29:04,694 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:29:04,790 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:29:04,790 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwfzucuua
2019-02-14 10:29:07,236 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:29:08,675 : Computing embeddings for train/dev/test
2019-02-14 10:30:31,123 : Computed embeddings
2019-02-14 10:30:31,123 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:30:55,871 : [('reg:1e-05', 25.87), ('reg:0.0001', 21.75), ('reg:0.001', 18.19), ('reg:0.01', 18.07)]
2019-02-14 10:30:55,871 : Validation : best param found is reg = 1e-05 with score             25.87
2019-02-14 10:30:55,872 : Evaluating...
2019-02-14 10:31:02,807 : 
Dev acc : 25.9 Test acc : 26.2 for DEPTH classification

2019-02-14 10:31:02,808 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 10:31:03,181 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 10:31:03,244 : loading BERT model bert-base-uncased
2019-02-14 10:31:03,244 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:31:03,357 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:31:03,357 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplaq3wl3_
2019-02-14 10:31:05,799 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:31:07,225 : Computing embeddings for train/dev/test
2019-02-14 10:32:25,330 : Computed embeddings
2019-02-14 10:32:25,330 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:33:06,136 : [('reg:1e-05', 52.62), ('reg:0.0001', 29.26), ('reg:0.001', 16.4), ('reg:0.01', 8.69)]
2019-02-14 10:33:06,136 : Validation : best param found is reg = 1e-05 with score             52.62
2019-02-14 10:33:06,137 : Evaluating...
2019-02-14 10:33:15,470 : 
Dev acc : 52.6 Test acc : 52.7 for TOPCONSTITUENTS classification

2019-02-14 10:33:15,471 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 10:33:16,046 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 10:33:16,119 : loading BERT model bert-base-uncased
2019-02-14 10:33:16,119 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:33:16,156 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:33:16,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvkna5uxj
2019-02-14 10:33:18,639 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:33:20,123 : Computing embeddings for train/dev/test
2019-02-14 10:34:44,243 : Computed embeddings
2019-02-14 10:34:44,244 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:35:16,146 : [('reg:1e-05', 78.58), ('reg:0.0001', 77.67), ('reg:0.001', 72.52), ('reg:0.01', 52.63)]
2019-02-14 10:35:16,146 : Validation : best param found is reg = 1e-05 with score             78.58
2019-02-14 10:35:16,146 : Evaluating...
2019-02-14 10:35:24,714 : 
Dev acc : 78.6 Test acc : 77.7 for BIGRAMSHIFT classification

2019-02-14 10:35:24,715 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 10:35:25,324 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 10:35:25,395 : loading BERT model bert-base-uncased
2019-02-14 10:35:25,395 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:35:25,426 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:35:25,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr482lxw0
2019-02-14 10:35:27,880 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:35:29,355 : Computing embeddings for train/dev/test
2019-02-14 10:36:51,821 : Computed embeddings
2019-02-14 10:36:51,821 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:37:26,101 : [('reg:1e-05', 87.43), ('reg:0.0001', 85.37), ('reg:0.001', 75.88), ('reg:0.01', 65.74)]
2019-02-14 10:37:26,102 : Validation : best param found is reg = 1e-05 with score             87.43
2019-02-14 10:37:26,102 : Evaluating...
2019-02-14 10:37:37,307 : 
Dev acc : 87.4 Test acc : 85.8 for TENSE classification

2019-02-14 10:37:37,308 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 10:37:37,770 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 10:37:37,837 : loading BERT model bert-base-uncased
2019-02-14 10:37:37,838 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:37:37,868 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:37:37,869 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpt7pqzw8a
2019-02-14 10:37:40,322 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:37:41,793 : Computing embeddings for train/dev/test
2019-02-14 10:39:09,496 : Computed embeddings
2019-02-14 10:39:09,496 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:40:09,178 : [('reg:1e-05', 77.25), ('reg:0.0001', 73.44), ('reg:0.001', 63.75), ('reg:0.01', 50.0)]
2019-02-14 10:40:09,178 : Validation : best param found is reg = 1e-05 with score             77.25
2019-02-14 10:40:09,179 : Evaluating...
2019-02-14 10:40:30,404 : 
Dev acc : 77.2 Test acc : 76.3 for SUBJNUMBER classification

2019-02-14 10:40:30,405 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 10:40:30,827 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 10:40:30,896 : loading BERT model bert-base-uncased
2019-02-14 10:40:30,896 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:40:31,018 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:40:31,018 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfjkjm30t
2019-02-14 10:40:33,465 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:40:35,005 : Computing embeddings for train/dev/test
2019-02-14 10:42:30,322 : Computed embeddings
2019-02-14 10:42:30,322 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:43:33,870 : [('reg:1e-05', 74.88), ('reg:0.0001', 71.04), ('reg:0.001', 65.25), ('reg:0.01', 50.01)]
2019-02-14 10:43:33,870 : Validation : best param found is reg = 1e-05 with score             74.88
2019-02-14 10:43:33,870 : Evaluating...
2019-02-14 10:43:56,651 : 
Dev acc : 74.9 Test acc : 75.9 for OBJNUMBER classification

2019-02-14 10:43:56,652 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 10:43:57,236 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 10:43:57,304 : loading BERT model bert-base-uncased
2019-02-14 10:43:57,305 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:43:57,334 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:43:57,334 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqsd7e7zm
2019-02-14 10:43:59,781 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:44:01,285 : Computing embeddings for train/dev/test
2019-02-14 10:46:04,921 : Computed embeddings
2019-02-14 10:46:04,921 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:47:19,794 : [('reg:1e-05', 57.95), ('reg:0.0001', 56.82), ('reg:0.001', 55.61), ('reg:0.01', 53.57)]
2019-02-14 10:47:19,795 : Validation : best param found is reg = 1e-05 with score             57.95
2019-02-14 10:47:19,795 : Evaluating...
2019-02-14 10:47:36,895 : 
Dev acc : 58.0 Test acc : 59.3 for ODDMANOUT classification

2019-02-14 10:47:36,896 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 10:47:37,294 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 10:47:37,371 : loading BERT model bert-base-uncased
2019-02-14 10:47:37,371 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:47:37,499 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:47:37,499 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvju8oaj7
2019-02-14 10:47:39,940 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:47:41,386 : Computing embeddings for train/dev/test
2019-02-14 10:49:33,166 : Computed embeddings
2019-02-14 10:49:33,166 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:50:28,938 : [('reg:1e-05', 55.28), ('reg:0.0001', 52.25), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-14 10:50:28,938 : Validation : best param found is reg = 1e-05 with score             55.28
2019-02-14 10:50:28,938 : Evaluating...
2019-02-14 10:50:43,538 : 
Dev acc : 55.3 Test acc : 55.5 for COORDINATIONINVERSION classification

2019-02-14 10:50:43,540 : total results: {'STS12': {'MSRpar': {'pearson': (0.3764137168715032, 1.172661975425707e-26), 'spearman': SpearmanrResult(correlation=0.41590900493584193, pvalue=9.863920771340027e-33), 'nsamples': 750}, 'MSRvid': {'pearson': (0.4266047570926971, 1.5982813871053431e-34), 'spearman': SpearmanrResult(correlation=0.4725509639081748, pvalue=5.523537451037856e-43), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.39630762627338906, 1.0311988011111104e-18), 'spearman': SpearmanrResult(correlation=0.5413777854682624, pvalue=2.6043939017482102e-36), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.2988867102122673, 6.082261291421577e-17), 'spearman': SpearmanrResult(correlation=0.27625503656878153, pvalue=1.325555591261281e-14), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4789436556561154, 2.8537289991642627e-24), 'spearman': SpearmanrResult(correlation=0.46696987262483813, pvalue=5.296558549160579e-23), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.3954312932211944, 'wmean': 0.3859178272839853}, 'spearman': {'mean': 0.4346125327011798, 'wmean': 0.42096191659164783}}}, 'STS13': {'FNWN': {'pearson': (0.22789906137580462, 0.0016109434230917503), 'spearman': SpearmanrResult(correlation=0.23460100123121289, pvalue=0.0011568826694464945), 'nsamples': 189}, 'headlines': {'pearson': (0.36224150126735266, 1.1338830433076607e-24), 'spearman': SpearmanrResult(correlation=0.42038701387902194, pvalue=1.7876936349792412e-33), 'nsamples': 750}, 'OnWN': {'pearson': (0.11387232525427114, 0.00693630068406696), 'spearman': SpearmanrResult(correlation=0.14081645113648233, pvalue=0.0008241928254784578), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.23467096263247614, 'wmean': 0.25242428201212513}, 'spearman': {'mean': 0.2652681554155724, 'wmean': 0.29241858581968816}}}, 'STS14': {'deft-forum': {'pearson': (0.19261288528484552, 3.9029740827177885e-05), 'spearman': SpearmanrResult(correlation=0.24373735310782738, pvalue=1.6478009750751078e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.4226305290351124, 1.9926487352544095e-14), 'spearman': SpearmanrResult(correlation=0.5678436006517888, pvalue=5.222245435665815e-27), 'nsamples': 300}, 'headlines': {'pearson': (0.34162007831945373, 5.898134507705492e-22), 'spearman': SpearmanrResult(correlation=0.35973813338770794, pvalue=2.4835640348419836e-24), 'nsamples': 750}, 'images': {'pearson': (0.5493242125077499, 2.410623767715791e-60), 'spearman': SpearmanrResult(correlation=0.5553304968185541, pvalue=6.708969477515299e-62), 'nsamples': 750}, 'OnWN': {'pearson': (0.1890055073780382, 1.8424854215074558e-07), 'spearman': SpearmanrResult(correlation=0.22005333833586863, pvalue=1.1208580772881261e-09), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3796623036006655, 3.981411141425153e-27), 'spearman': SpearmanrResult(correlation=0.4142785414747451, pvalue=1.8253949329072667e-32), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3458092526876442, 'wmean': 0.3488464089181719}, 'spearman': {'mean': 0.39349691062941533, 'wmean': 0.3845560724284575}}}, 'STS15': {'answers-forums': {'pearson': (0.3484303738211297, 3.822793302654411e-12), 'spearman': SpearmanrResult(correlation=0.3940712693565587, pvalue=2.2125802216897544e-15), 'nsamples': 375}, 'answers-students': {'pearson': (0.2560921049679273, 1.0733972761138574e-12), 'spearman': SpearmanrResult(correlation=0.2959283056426417, pvalue=1.264212739095925e-16), 'nsamples': 750}, 'belief': {'pearson': (0.3289597410297165, 6.503469450651487e-11), 'spearman': SpearmanrResult(correlation=0.3689992207665658, pvalue=1.5363145186357327e-13), 'nsamples': 375}, 'headlines': {'pearson': (0.43467933455369384, 6.438924808204332e-36), 'spearman': SpearmanrResult(correlation=0.483646952300032, pvalue=3.148869491272271e-45), 'nsamples': 750}, 'images': {'pearson': (0.24034251567108583, 2.5743482770572953e-11), 'spearman': SpearmanrResult(correlation=0.4934573018222355, pvalue=2.7818332109657753e-47), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.32170081400871064, 'wmean': 0.3174522531545325}, 'spearman': {'mean': 0.4072206099776068, 'wmean': 0.41364195120661784}}}, 'STS16': {'answer-answer': {'pearson': (0.3382731712235954, 3.231079528125321e-08), 'spearman': SpearmanrResult(correlation=0.5170521577126315, pvalue=9.124340286661174e-19), 'nsamples': 254}, 'headlines': {'pearson': (0.49078762885086, 1.689634685253154e-16), 'spearman': SpearmanrResult(correlation=0.5570062203990763, pvalue=1.0800943139064742e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.6762657293263575, 4.234111392890284e-32), 'spearman': SpearmanrResult(correlation=0.7071231694658421, pvalue=3.55974343832853e-36), 'nsamples': 230}, 'postediting': {'pearson': (0.6507692010474436, 8.921393704943734e-31), 'spearman': SpearmanrResult(correlation=0.7788173317468106, pvalue=6.277358927442403e-51), 'nsamples': 244}, 'question-question': {'pearson': (0.34600370317531787, 2.874389159713934e-07), 'spearman': SpearmanrResult(correlation=0.35311324806683536, pvalue=1.5695964338294825e-07), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5004198867247149, 'wmean': 0.5014933236415998}, 'spearman': {'mean': 0.5826224254782392, 'wmean': 0.5872649441043108}}}, 'MR': {'devacc': 70.02, 'acc': 69.86, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 70.9, 'acc': 71.18, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 76.44, 'acc': 79.56, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.19, 'acc': 90.4, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.75, 'acc': 75.56, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 38.78, 'acc': 38.73, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 48.76, 'acc': 74.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.99, 'acc': 70.9, 'f1': 78.6, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 73.4, 'acc': 73.13, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.713676119497383, 'pearson': 0.7255760575039417, 'spearman': 0.6703817875684596, 'mse': 0.4859321647357864, 'yhat': array([2.60496824, 3.5013248 , 3.44914068, ..., 3.07638691, 4.2891269 ,        4.44994978]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6758631551611493, 'pearson': 0.5986588437013479, 'spearman': 0.5931515273544508, 'mse': 1.7307283205729138, 'yhat': array([2.05022721, 2.27076249, 3.47687207, ..., 3.88927199, 3.85797842,        3.03417339]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 62.99, 'acc': 62.54, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 320.26800000000003, 'acc': [(27.92, 61.58, 76.16000000000001, 3.1999999999999997), (23.136, 56.58, 73.34, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 74.52, 'acc': 74.42, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.54, 'acc': 0.5, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.87, 'acc': 26.2, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 52.62, 'acc': 52.72, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 78.58, 'acc': 77.73, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.43, 'acc': 85.84, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 77.25, 'acc': 76.26, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.88, 'acc': 75.92, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 57.95, 'acc': 59.29, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 55.28, 'acc': 55.5, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 10:50:43,540 : STS12 p=0.3859, STS12 s=0.4210, STS13 p=0.2524, STS13 s=0.2924, STS14 p=0.3488, STS14 s=0.3846, STS15 p=0.3175, STS15 s=0.4136, STS 16 p=0.5015, STS16 s=0.5873, STS B p=0.5987, STS B s=0.5932, STS B m=1.7307, SICK-R p=0.7256, SICK-R s=0.6704, SICK-P m=0.4859
2019-02-14 10:50:43,540 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 10:50:43,540 : 0.3859,0.4210,0.2524,0.2924,0.3488,0.3846,0.3175,0.4136,0.5015,0.5873,0.5987,0.5932,1.7307,0.7256,0.6704,0.4859
2019-02-14 10:50:43,540 : MR=69.86, CR=71.18, SUBJ=90.40, MPQA=79.56, SST-B=75.56, SST-F=38.73, TREC=74.60, SICK-E=73.13, SNLI=62.54, MRPC=70.90, MRPC f=78.60
2019-02-14 10:50:43,540 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 10:50:43,540 : 69.86,71.18,90.40,79.56,75.56,38.73,74.60,73.13,62.54,70.90,78.60
2019-02-14 10:50:43,540 : COCO r1i2t=27.92, COCO r5i2t=61.58, COCO r10i2t=76.16, COCO medr_i2t=3.20, COCO r1t2i=23.14, COCO r5t2i=56.58, COCO r10t2i=73.34, COCO medr_t2i=4.00
2019-02-14 10:50:43,540 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 10:50:43,540 : 27.92,61.58,76.16,3.20,23.14,56.58,73.34,4.00
2019-02-14 10:50:43,540 : SentLen=74.42, WC=0.50, TreeDepth=26.20, TopConst=52.72, BShift=77.73, Tense=85.84, SubjNum=76.26, ObjNum=75.92, SOMO=59.29, CoordInv=55.50, average=58.44
2019-02-14 10:50:43,540 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 10:50:43,540 : 74.42,0.50,26.20,52.72,77.73,85.84,76.26,75.92,59.29,55.50,58.44
2019-02-14 10:50:43,540 : ********************************************************************************
2019-02-14 10:50:43,540 : ********************************************************************************
2019-02-14 10:50:43,540 : ********************************************************************************
2019-02-14 10:50:43,540 : layer 6
2019-02-14 10:50:43,541 : ********************************************************************************
2019-02-14 10:50:43,541 : ********************************************************************************
2019-02-14 10:50:43,541 : ********************************************************************************
2019-02-14 10:50:43,632 : ***** Transfer task : STS12 *****


2019-02-14 10:50:43,669 : loading BERT model bert-base-uncased
2019-02-14 10:50:43,669 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:50:43,685 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:50:43,686 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzobjbn2f
2019-02-14 10:50:46,123 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:50:49,551 : MSRpar : pearson = 0.3989, spearman = 0.4146
2019-02-14 10:50:50,524 : MSRvid : pearson = 0.3327, spearman = 0.3481
2019-02-14 10:50:51,302 : SMTeuroparl : pearson = 0.4152, spearman = 0.5468
2019-02-14 10:50:52,669 : surprise.OnWN : pearson = 0.4768, spearman = 0.4879
2019-02-14 10:50:53,429 : surprise.SMTnews : pearson = 0.4779, spearman = 0.4359
2019-02-14 10:50:53,429 : ALL (weighted average) : Pearson = 0.4143,             Spearman = 0.4385
2019-02-14 10:50:53,429 : ALL (average) : Pearson = 0.4203,             Spearman = 0.4467

2019-02-14 10:50:53,429 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 10:50:53,440 : loading BERT model bert-base-uncased
2019-02-14 10:50:53,441 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:50:53,458 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:50:53,458 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpi_e69jen
2019-02-14 10:50:55,895 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:50:57,985 : FNWN : pearson = 0.1883, spearman = 0.2013
2019-02-14 10:50:59,085 : headlines : pearson = 0.4503, spearman = 0.4613
2019-02-14 10:50:59,898 : OnWN : pearson = 0.0994, spearman = 0.1613
2019-02-14 10:50:59,898 : ALL (weighted average) : Pearson = 0.2861,             Spearman = 0.3163
2019-02-14 10:50:59,898 : ALL (average) : Pearson = 0.2460,             Spearman = 0.2746

2019-02-14 10:50:59,898 : ***** Transfer task : STS14 *****


2019-02-14 10:50:59,914 : loading BERT model bert-base-uncased
2019-02-14 10:50:59,914 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:50:59,931 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:50:59,931 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvm8scfev
2019-02-14 10:51:02,383 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:51:04,615 : deft-forum : pearson = 0.2330, spearman = 0.2376
2019-02-14 10:51:05,405 : deft-news : pearson = 0.5025, spearman = 0.5376
2019-02-14 10:51:06,592 : headlines : pearson = 0.4023, spearman = 0.3921
2019-02-14 10:51:07,744 : images : pearson = 0.4613, spearman = 0.4749
2019-02-14 10:51:08,916 : OnWN : pearson = 0.2561, spearman = 0.3192
2019-02-14 10:51:10,356 : tweet-news : pearson = 0.4577, spearman = 0.4710
2019-02-14 10:51:10,356 : ALL (weighted average) : Pearson = 0.3836,             Spearman = 0.4030
2019-02-14 10:51:10,356 : ALL (average) : Pearson = 0.3855,             Spearman = 0.4054

2019-02-14 10:51:10,356 : ***** Transfer task : STS15 *****


2019-02-14 10:51:10,417 : loading BERT model bert-base-uncased
2019-02-14 10:51:10,417 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:51:10,435 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:51:10,435 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpbwak9utx
2019-02-14 10:51:12,909 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:51:15,332 : answers-forums : pearson = 0.3258, spearman = 0.3327
2019-02-14 10:51:16,487 : answers-students : pearson = 0.4653, spearman = 0.4987
2019-02-14 10:51:17,243 : belief : pearson = 0.3767, spearman = 0.4032
2019-02-14 10:51:18,133 : headlines : pearson = 0.5016, spearman = 0.5131
2019-02-14 10:51:18,978 : images : pearson = 0.4445, spearman = 0.4943
2019-02-14 10:51:18,978 : ALL (weighted average) : Pearson = 0.4407,             Spearman = 0.4685
2019-02-14 10:51:18,979 : ALL (average) : Pearson = 0.4228,             Spearman = 0.4484

2019-02-14 10:51:18,979 : ***** Transfer task : STS16 *****


2019-02-14 10:51:19,018 : loading BERT model bert-base-uncased
2019-02-14 10:51:19,018 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:51:19,036 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:51:19,037 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyrqt6n7d
2019-02-14 10:51:21,487 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:51:23,458 : answer-answer : pearson = 0.4156, spearman = 0.5316
2019-02-14 10:51:23,851 : headlines : pearson = 0.5677, spearman = 0.5871
2019-02-14 10:51:24,322 : plagiarism : pearson = 0.7467, spearman = 0.7626
2019-02-14 10:51:24,976 : postediting : pearson = 0.7399, spearman = 0.7640
2019-02-14 10:51:25,350 : question-question : pearson = 0.1694, spearman = 0.1956
2019-02-14 10:51:25,350 : ALL (weighted average) : Pearson = 0.5351,             Spearman = 0.5766
2019-02-14 10:51:25,350 : ALL (average) : Pearson = 0.5279,             Spearman = 0.5682

2019-02-14 10:51:25,350 : ***** Transfer task : MR *****


2019-02-14 10:51:25,401 : loading BERT model bert-base-uncased
2019-02-14 10:51:25,401 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:51:25,420 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:51:25,421 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprgzl5peu
2019-02-14 10:51:27,905 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:51:29,450 : Generating sentence embeddings
2019-02-14 10:51:44,449 : Generated sentence embeddings
2019-02-14 10:51:44,450 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 10:52:06,782 : Best param found at split 1: l2reg = 1e-05                 with score 66.5
2019-02-14 10:52:29,267 : Best param found at split 2: l2reg = 1e-05                 with score 69.05
2019-02-14 10:52:54,248 : Best param found at split 3: l2reg = 1e-05                 with score 70.04
2019-02-14 10:53:11,867 : Best param found at split 4: l2reg = 1e-05                 with score 67.55
2019-02-14 10:53:25,122 : Best param found at split 5: l2reg = 1e-05                 with score 70.29
2019-02-14 10:53:25,793 : Dev acc : 68.69 Test acc : 68.97

2019-02-14 10:53:25,794 : ***** Transfer task : CR *****


2019-02-14 10:53:25,802 : loading BERT model bert-base-uncased
2019-02-14 10:53:25,802 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:53:25,822 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:53:25,823 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe1ytlp98
2019-02-14 10:53:28,270 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:53:29,732 : Generating sentence embeddings
2019-02-14 10:53:33,917 : Generated sentence embeddings
2019-02-14 10:53:33,918 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 10:53:37,955 : Best param found at split 1: l2reg = 1e-05                 with score 67.31
2019-02-14 10:53:42,424 : Best param found at split 2: l2reg = 1e-05                 with score 68.4
2019-02-14 10:53:46,602 : Best param found at split 3: l2reg = 1e-05                 with score 65.73
2019-02-14 10:53:51,200 : Best param found at split 4: l2reg = 1e-05                 with score 68.79
2019-02-14 10:53:55,438 : Best param found at split 5: l2reg = 1e-05                 with score 68.39
2019-02-14 10:53:55,688 : Dev acc : 67.72 Test acc : 68.35

2019-02-14 10:53:55,688 : ***** Transfer task : MPQA *****


2019-02-14 10:53:55,724 : loading BERT model bert-base-uncased
2019-02-14 10:53:55,724 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:53:55,743 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:53:55,743 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv1v2ijlf
2019-02-14 10:53:58,217 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:53:59,681 : Generating sentence embeddings
2019-02-14 10:54:03,806 : Generated sentence embeddings
2019-02-14 10:54:03,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 10:54:18,705 : Best param found at split 1: l2reg = 1e-05                 with score 73.14
2019-02-14 10:54:35,382 : Best param found at split 2: l2reg = 1e-05                 with score 72.21
2019-02-14 10:54:57,399 : Best param found at split 3: l2reg = 1e-05                 with score 74.96
2019-02-14 10:55:22,387 : Best param found at split 4: l2reg = 1e-05                 with score 75.0
2019-02-14 10:55:44,762 : Best param found at split 5: l2reg = 1e-05                 with score 76.46
2019-02-14 10:55:45,653 : Dev acc : 74.35 Test acc : 79.17

2019-02-14 10:55:45,654 : ***** Transfer task : SUBJ *****


2019-02-14 10:55:45,668 : loading BERT model bert-base-uncased
2019-02-14 10:55:45,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:55:45,690 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:55:45,690 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpivx8bi7f
2019-02-14 10:55:48,167 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:55:49,659 : Generating sentence embeddings
2019-02-14 10:56:04,245 : Generated sentence embeddings
2019-02-14 10:56:04,245 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 10:56:18,422 : Best param found at split 1: l2reg = 1e-05                 with score 89.89
2019-02-14 10:56:29,580 : Best param found at split 2: l2reg = 1e-05                 with score 90.54
2019-02-14 10:56:47,001 : Best param found at split 3: l2reg = 0.0001                 with score 89.55
2019-02-14 10:57:08,564 : Best param found at split 4: l2reg = 1e-05                 with score 90.31
2019-02-14 10:57:37,631 : Best param found at split 5: l2reg = 1e-05                 with score 90.02
2019-02-14 10:57:39,596 : Dev acc : 90.06 Test acc : 88.85

2019-02-14 10:57:39,597 : ***** Transfer task : SST Binary classification *****


2019-02-14 10:57:39,691 : loading BERT model bert-base-uncased
2019-02-14 10:57:39,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:57:39,769 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:57:39,769 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6arhap5w
2019-02-14 10:57:42,259 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:57:43,770 : Computing embedding for train
2019-02-14 10:58:49,877 : Computed train embeddings
2019-02-14 10:58:49,877 : Computing embedding for dev
2019-02-14 10:58:50,943 : Computed dev embeddings
2019-02-14 10:58:50,944 : Computing embedding for test
2019-02-14 10:58:53,161 : Computed test embeddings
2019-02-14 10:58:53,161 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:59:16,000 : [('reg:1e-05', 76.72), ('reg:0.0001', 75.23), ('reg:0.001', 68.69), ('reg:0.01', 52.64)]
2019-02-14 10:59:16,000 : Validation : best param found is reg = 1e-05 with score             76.72
2019-02-14 10:59:16,000 : Evaluating...
2019-02-14 10:59:27,465 : 
Dev acc : 76.72 Test acc : 76.0 for             SST Binary classification

2019-02-14 10:59:27,466 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 10:59:27,516 : loading BERT model bert-base-uncased
2019-02-14 10:59:27,516 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:59:27,539 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:59:27,539 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp68yjowec
2019-02-14 10:59:30,002 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 10:59:31,484 : Computing embedding for train
2019-02-14 10:59:42,851 : Computed train embeddings
2019-02-14 10:59:42,851 : Computing embedding for dev
2019-02-14 10:59:44,364 : Computed dev embeddings
2019-02-14 10:59:44,365 : Computing embedding for test
2019-02-14 10:59:47,421 : Computed test embeddings
2019-02-14 10:59:47,421 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 10:59:52,936 : [('reg:1e-05', 36.97), ('reg:0.0001', 36.69), ('reg:0.001', 28.34), ('reg:0.01', 26.25)]
2019-02-14 10:59:52,936 : Validation : best param found is reg = 1e-05 with score             36.97
2019-02-14 10:59:52,936 : Evaluating...
2019-02-14 10:59:54,780 : 
Dev acc : 36.97 Test acc : 37.06 for             SST Fine-Grained classification

2019-02-14 10:59:54,780 : ***** Transfer task : TREC *****


2019-02-14 10:59:54,793 : loading BERT model bert-base-uncased
2019-02-14 10:59:54,793 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 10:59:54,813 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 10:59:54,813 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpustj5q85
2019-02-14 10:59:57,252 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:00:04,735 : Computed train embeddings
2019-02-14 11:00:05,290 : Computed test embeddings
2019-02-14 11:00:05,290 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 11:00:20,747 : [('reg:1e-05', 46.87), ('reg:0.0001', 42.39), ('reg:0.001', 35.82), ('reg:0.01', 25.57)]
2019-02-14 11:00:20,747 : Cross-validation : best param found is reg = 1e-05             with score 46.87
2019-02-14 11:00:20,747 : Evaluating...
2019-02-14 11:00:21,971 : 
Dev acc : 46.87 Test acc : 54.6             for TREC

2019-02-14 11:00:21,972 : ***** Transfer task : MRPC *****


2019-02-14 11:00:21,992 : loading BERT model bert-base-uncased
2019-02-14 11:00:21,992 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:00:22,015 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:00:22,015 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd04mjd_3
2019-02-14 11:00:24,456 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:00:25,940 : Computing embedding for train
2019-02-14 11:00:40,042 : Computed train embeddings
2019-02-14 11:00:40,042 : Computing embedding for test
2019-02-14 11:00:45,150 : Computed test embeddings
2019-02-14 11:00:45,166 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 11:00:55,260 : [('reg:1e-05', 69.26), ('reg:0.0001', 69.26), ('reg:0.001', 68.72), ('reg:0.01', 67.62)]
2019-02-14 11:00:55,260 : Cross-validation : best param found is reg = 1e-05             with score 69.26
2019-02-14 11:00:55,260 : Evaluating...
2019-02-14 11:00:55,974 : Dev acc : 69.26 Test acc 72.7; Test F1 80.59 for MRPC.

2019-02-14 11:00:55,975 : ***** Transfer task : SICK-Entailment*****


2019-02-14 11:00:56,036 : loading BERT model bert-base-uncased
2019-02-14 11:00:56,036 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:00:56,056 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:00:56,057 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4qlnkkcs
2019-02-14 11:00:58,496 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:00:59,929 : Computing embedding for train
2019-02-14 11:01:06,389 : Computed train embeddings
2019-02-14 11:01:06,390 : Computing embedding for dev
2019-02-14 11:01:07,214 : Computed dev embeddings
2019-02-14 11:01:07,215 : Computing embedding for test
2019-02-14 11:01:14,153 : Computed test embeddings
2019-02-14 11:01:14,181 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:01:15,809 : [('reg:1e-05', 62.0), ('reg:0.0001', 61.2), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-14 11:01:15,809 : Validation : best param found is reg = 1e-05 with score             62.0
2019-02-14 11:01:15,809 : Evaluating...
2019-02-14 11:01:16,210 : 
Dev acc : 62.0 Test acc : 62.19 for                        SICK entailment

2019-02-14 11:01:16,211 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 11:01:16,237 : loading BERT model bert-base-uncased
2019-02-14 11:01:16,237 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:01:16,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:01:16,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqdegdvo5
2019-02-14 11:01:18,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:01:20,175 : Computing embedding for train
2019-02-14 11:01:26,613 : Computed train embeddings
2019-02-14 11:01:26,613 : Computing embedding for dev
2019-02-14 11:01:27,434 : Computed dev embeddings
2019-02-14 11:01:27,434 : Computing embedding for test
2019-02-14 11:01:34,320 : Computed test embeddings
2019-02-14 11:03:52,021 : Dev : Pearson 0.6986867134634558
2019-02-14 11:03:52,021 : Test : Pearson 0.6976025716509199 Spearman 0.6670387387020428 MSE 0.5243659271841756                        for SICK Relatedness

2019-02-14 11:03:52,022 : 

***** Transfer task : STSBenchmark*****


2019-02-14 11:03:52,091 : loading BERT model bert-base-uncased
2019-02-14 11:03:52,091 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:03:52,112 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:03:52,112 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkigmrsco
2019-02-14 11:03:54,596 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:03:56,069 : Computing embedding for train
2019-02-14 11:04:05,915 : Computed train embeddings
2019-02-14 11:04:05,915 : Computing embedding for dev
2019-02-14 11:04:08,770 : Computed dev embeddings
2019-02-14 11:04:08,770 : Computing embedding for test
2019-02-14 11:04:11,142 : Computed test embeddings
2019-02-14 11:06:05,889 : Dev : Pearson 0.6116760848689891
2019-02-14 11:06:05,889 : Test : Pearson 0.54161073709479 Spearman 0.5394175618542567 MSE 1.9022788282560792                        for SICK Relatedness

2019-02-14 11:06:05,889 : ***** Transfer task : SNLI Entailment*****


2019-02-14 11:06:10,631 : loading BERT model bert-base-uncased
2019-02-14 11:06:10,632 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:06:10,756 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:06:10,756 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmper2z67s6
2019-02-14 11:06:13,231 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:06:14,975 : PROGRESS (encoding): 0.00%
2019-02-14 11:07:57,817 : PROGRESS (encoding): 14.56%
2019-02-14 11:09:51,494 : PROGRESS (encoding): 29.12%
2019-02-14 11:11:37,883 : PROGRESS (encoding): 43.69%
2019-02-14 11:13:35,795 : PROGRESS (encoding): 58.25%
2019-02-14 11:15:41,652 : PROGRESS (encoding): 72.81%
2019-02-14 11:18:10,964 : PROGRESS (encoding): 87.37%
2019-02-14 11:20:29,153 : PROGRESS (encoding): 0.00%
2019-02-14 11:20:44,809 : PROGRESS (encoding): 0.00%
2019-02-14 11:21:03,921 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 11:22:06,312 : [('reg:1e-09', 57.58)]
2019-02-14 11:22:06,312 : Validation : best param found is reg = 1e-09 with score             57.58
2019-02-14 11:22:06,312 : Evaluating...
2019-02-14 11:23:44,105 : Dev acc : 57.58 Test acc : 58.71 for SNLI

2019-02-14 11:23:44,105 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 11:23:52,960 : loading BERT model bert-base-uncased
2019-02-14 11:23:52,960 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 11:23:53,007 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 11:23:53,007 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp14o8gug1
2019-02-14 11:23:55,449 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 11:23:56,980 : Computing embedding for train
2019-02-14 11:34:28,877 : Computed train embeddings
2019-02-14 11:34:28,877 : Computing embedding for dev
2019-02-14 11:34:56,617 : Computed dev embeddings
2019-02-14 11:34:56,617 : Computing embedding for test
2019-02-14 11:35:27,127 : Computed test embeddings
2019-02-14 11:35:27,145 : prepare data
2019-02-14 11:35:27,211 : start epoch
2019-02-14 11:36:09,878 : samples : 64000
2019-02-14 11:36:22,500 : Image to text: 2.82, 10.52, 18.06, 59.0
2019-02-14 11:36:32,485 : Text to Image: 2.008, 7.868, 13.672, 75.0
2019-02-14 11:37:15,336 : samples : 128000
2019-02-14 11:37:27,997 : Image to text: 3.76, 13.2, 21.86, 46.0
2019-02-14 11:37:38,071 : Text to Image: 2.076, 7.9, 13.88, 74.0
2019-02-14 11:38:22,271 : samples : 192000
2019-02-14 11:38:34,959 : Image to text: 4.34, 15.92, 24.46, 36.0
2019-02-14 11:38:42,797 : Text to Image: 3.256, 12.388, 20.096, 46.0
2019-02-14 11:39:23,522 : samples : 256000
2019-02-14 11:39:33,534 : Image to text: 5.24, 17.72, 27.84, 32.0
2019-02-14 11:39:40,820 : Text to Image: 4.64, 15.484, 24.1, 37.0
2019-02-14 11:40:23,491 : samples : 320000
2019-02-14 11:40:36,427 : Image to text: 5.3, 17.26, 26.82, 33.0
2019-02-14 11:40:46,814 : Text to Image: 3.152, 11.94, 19.868, 45.0
2019-02-14 11:41:31,122 : samples : 384000
2019-02-14 11:41:44,004 : Image to text: 6.12, 19.38, 29.36, 28.0
2019-02-14 11:41:54,402 : Text to Image: 4.74, 15.716, 24.64, 35.0
2019-02-14 11:42:39,865 : samples : 448000
2019-02-14 11:42:52,824 : Image to text: 5.86, 19.34, 29.5, 28.0
2019-02-14 11:43:03,339 : Text to Image: 5.156, 17.084, 26.576, 32.0
2019-02-14 11:43:48,493 : samples : 512000
2019-02-14 11:44:01,479 : Image to text: 5.98, 19.02, 28.84, 29.0
2019-02-14 11:44:12,079 : Text to Image: 4.128, 14.312, 22.792, 40.0
2019-02-14 11:44:50,559 : Epoch 1 finished
2019-02-14 11:44:51,592 : Image to text: 16.9, 46.5, 62.5, 6.0
2019-02-14 11:44:52,494 : Text to Image: 14.2, 38.74, 56.68, 8.0
2019-02-14 11:44:53,600 : Image to text: 18.9, 45.6, 62.1, 6.0
2019-02-14 11:44:54,483 : Text to Image: 13.12, 39.88, 56.64, 8.0
2019-02-14 11:44:55,550 : Image to text: 19.6, 48.4, 63.9, 6.0
2019-02-14 11:44:56,380 : Text to Image: 13.5, 40.06, 56.28, 8.0
2019-02-14 11:44:57,523 : Image to text: 19.8, 48.2, 63.4, 6.0
2019-02-14 11:44:58,479 : Text to Image: 14.06, 39.74, 56.5, 8.0
2019-02-14 11:44:59,579 : Image to text: 16.6, 47.6, 63.0, 6.0
2019-02-14 11:45:00,524 : Text to Image: 14.52, 40.36, 56.3, 8.0
2019-02-14 11:45:00,524 : Dev mean Text to Image: 13.879999999999999, 39.756, 56.48, 8.0
2019-02-14 11:45:00,524 : Dev mean Image to text: 18.36, 47.260000000000005, 62.980000000000004, 6.0
2019-02-14 11:45:00,525 : start epoch
2019-02-14 11:45:45,668 : samples : 64000
2019-02-14 11:45:58,555 : Image to text: 6.66, 21.1, 32.4, 25.0
2019-02-14 11:46:08,991 : Text to Image: 4.996, 16.7, 26.26, 32.0
2019-02-14 11:46:53,348 : samples : 128000
2019-02-14 11:47:06,308 : Image to text: 6.2, 20.06, 30.58, 26.0
2019-02-14 11:47:16,768 : Text to Image: 4.832, 16.5, 26.168, 32.0
2019-02-14 11:48:01,467 : samples : 192000
2019-02-14 11:48:14,401 : Image to text: 7.08, 21.86, 32.5, 24.0
2019-02-14 11:48:23,110 : Text to Image: 5.272, 17.044, 27.0, 32.0
2019-02-14 11:49:03,178 : samples : 256000
2019-02-14 11:49:13,245 : Image to text: 6.66, 21.16, 32.68, 25.0
2019-02-14 11:49:20,588 : Text to Image: 5.396, 17.476, 27.004, 31.0
2019-02-14 11:50:01,183 : samples : 320000
2019-02-14 11:50:11,295 : Image to text: 6.34, 19.9, 30.54, 26.0
2019-02-14 11:50:18,646 : Text to Image: 4.92, 16.828, 26.34, 32.0
2019-02-14 11:50:59,765 : samples : 384000
2019-02-14 11:51:09,804 : Image to text: 7.18, 22.38, 32.58, 23.0
2019-02-14 11:51:17,126 : Text to Image: 5.724, 18.82, 28.972, 28.0
2019-02-14 11:51:59,595 : samples : 448000
2019-02-14 11:52:09,651 : Image to text: 7.18, 21.96, 32.84, 23.0
2019-02-14 11:52:17,021 : Text to Image: 5.048, 17.564, 27.376, 30.0
2019-02-14 11:52:57,087 : samples : 512000
2019-02-14 11:53:07,115 : Image to text: 6.88, 21.54, 32.12, 26.0
2019-02-14 11:53:14,449 : Text to Image: 5.0, 16.996, 26.416, 31.0
2019-02-14 11:53:50,801 : Epoch 2 finished
2019-02-14 11:53:51,243 : Image to text: 17.0, 47.2, 64.3, 6.0
2019-02-14 11:53:51,573 : Text to Image: 13.42, 39.84, 56.4, 8.0
2019-02-14 11:53:52,002 : Image to text: 16.8, 46.2, 61.8, 7.0
2019-02-14 11:53:52,343 : Text to Image: 12.54, 37.2, 54.78, 9.0
2019-02-14 11:53:52,771 : Image to text: 19.5, 48.8, 65.1, 6.0
2019-02-14 11:53:53,102 : Text to Image: 13.36, 39.56, 56.24, 8.0
2019-02-14 11:53:53,589 : Image to text: 19.0, 48.3, 64.1, 6.0
2019-02-14 11:53:53,929 : Text to Image: 13.48, 37.76, 54.62, 9.0
2019-02-14 11:53:54,406 : Image to text: 19.4, 49.3, 63.0, 6.0
2019-02-14 11:53:54,746 : Text to Image: 12.9, 40.0, 56.88, 8.0
2019-02-14 11:53:54,746 : Dev mean Text to Image: 13.14, 38.872, 55.78399999999999, 8.4
2019-02-14 11:53:54,746 : Dev mean Image to text: 18.34, 47.959999999999994, 63.66, 6.2
2019-02-14 11:53:54,746 : start epoch
2019-02-14 11:54:36,393 : samples : 64000
2019-02-14 11:54:46,807 : Image to text: 7.24, 21.84, 32.38, 24.0
2019-02-14 11:54:54,275 : Text to Image: 4.584, 15.54, 24.532, 35.0
2019-02-14 11:55:36,233 : samples : 128000
2019-02-14 11:55:46,498 : Image to text: 7.86, 23.58, 35.48, 21.0
2019-02-14 11:55:53,982 : Text to Image: 6.392, 20.624, 31.056, 25.0
2019-02-14 11:56:35,434 : samples : 192000
2019-02-14 11:56:45,823 : Image to text: 7.84, 23.76, 34.8, 22.0
2019-02-14 11:56:53,149 : Text to Image: 5.484, 18.436, 28.964, 28.0
2019-02-14 11:57:33,977 : samples : 256000
2019-02-14 11:57:44,065 : Image to text: 7.36, 23.0, 33.76, 23.0
2019-02-14 11:57:51,434 : Text to Image: 5.668, 18.752, 28.98, 28.0
2019-02-14 11:58:32,684 : samples : 320000
2019-02-14 11:58:42,764 : Image to text: 7.44, 23.58, 35.48, 22.0
2019-02-14 11:58:50,157 : Text to Image: 6.552, 20.604, 31.092, 25.0
2019-02-14 11:59:33,445 : samples : 384000
2019-02-14 11:59:43,519 : Image to text: 7.68, 24.04, 35.4, 21.0
2019-02-14 11:59:50,864 : Text to Image: 6.056, 19.58, 29.724, 27.0
2019-02-14 12:00:32,053 : samples : 448000
2019-02-14 12:00:42,104 : Image to text: 8.32, 25.36, 36.6, 20.0
2019-02-14 12:00:49,430 : Text to Image: 6.296, 20.156, 30.616, 26.0
2019-02-14 12:01:31,092 : samples : 512000
2019-02-14 12:01:41,155 : Image to text: 7.28, 23.62, 34.94, 22.0
2019-02-14 12:01:48,498 : Text to Image: 5.516, 18.552, 28.748, 28.0
2019-02-14 12:02:25,016 : Epoch 3 finished
2019-02-14 12:02:25,460 : Image to text: 19.4, 51.5, 68.6, 5.0
2019-02-14 12:02:25,792 : Text to Image: 14.82, 42.1, 60.2, 7.0
2019-02-14 12:02:26,226 : Image to text: 21.2, 49.2, 65.0, 6.0
2019-02-14 12:02:26,558 : Text to Image: 14.28, 42.2, 60.12, 7.0
2019-02-14 12:02:27,001 : Image to text: 22.9, 53.4, 67.8, 5.0
2019-02-14 12:02:27,332 : Text to Image: 14.98, 43.38, 61.26, 7.0
2019-02-14 12:02:27,762 : Image to text: 23.2, 54.6, 70.4, 5.0
2019-02-14 12:02:28,135 : Text to Image: 14.12, 42.6, 61.16, 7.0
2019-02-14 12:02:28,600 : Image to text: 23.0, 53.8, 66.8, 5.0
2019-02-14 12:02:28,940 : Text to Image: 15.48, 42.1, 59.34, 8.0
2019-02-14 12:02:28,940 : Dev mean Text to Image: 14.736, 42.476, 60.416000000000004, 7.199999999999999
2019-02-14 12:02:28,940 : Dev mean Image to text: 21.939999999999998, 52.5, 67.72, 5.2
2019-02-14 12:02:28,940 : start epoch
2019-02-14 12:03:11,261 : samples : 64000
2019-02-14 12:03:21,559 : Image to text: 7.2, 23.28, 34.44, 21.0
2019-02-14 12:03:29,031 : Text to Image: 5.26, 17.632, 27.248, 31.0
2019-02-14 12:04:10,697 : samples : 128000
2019-02-14 12:04:20,961 : Image to text: 7.72, 23.86, 35.06, 22.0
2019-02-14 12:04:28,406 : Text to Image: 5.988, 19.128, 29.368, 28.0
2019-02-14 12:05:10,275 : samples : 192000
2019-02-14 12:05:20,603 : Image to text: 7.3, 23.68, 35.76, 21.0
2019-02-14 12:05:27,901 : Text to Image: 6.132, 19.9, 30.608, 26.0
2019-02-14 12:06:09,040 : samples : 256000
2019-02-14 12:06:18,843 : Image to text: 8.82, 25.18, 36.42, 20.0
2019-02-14 12:06:25,564 : Text to Image: 6.824, 21.268, 32.264, 24.0
2019-02-14 12:07:05,321 : samples : 320000
2019-02-14 12:07:15,109 : Image to text: 8.32, 24.42, 35.74, 20.0
2019-02-14 12:07:21,830 : Text to Image: 6.528, 20.336, 31.076, 26.0
2019-02-14 12:08:01,565 : samples : 384000
2019-02-14 12:08:11,365 : Image to text: 7.42, 24.48, 36.08, 21.0
2019-02-14 12:08:18,089 : Text to Image: 6.212, 20.372, 31.136, 25.0
2019-02-14 12:08:58,063 : samples : 448000
2019-02-14 12:09:07,845 : Image to text: 8.36, 24.84, 36.24, 21.0
2019-02-14 12:09:14,565 : Text to Image: 6.556, 20.264, 31.064, 26.0
2019-02-14 12:09:54,628 : samples : 512000
2019-02-14 12:10:04,437 : Image to text: 8.72, 24.14, 36.34, 20.0
2019-02-14 12:10:11,161 : Text to Image: 6.424, 19.864, 30.568, 26.0
2019-02-14 12:10:45,219 : Epoch 4 finished
2019-02-14 12:10:45,596 : Image to text: 21.2, 53.2, 71.8, 5.0
2019-02-14 12:10:45,874 : Text to Image: 17.9, 47.92, 65.6, 6.0
2019-02-14 12:10:46,250 : Image to text: 21.6, 52.4, 67.9, 5.0
2019-02-14 12:10:46,528 : Text to Image: 16.56, 46.4, 64.3, 6.0
2019-02-14 12:10:46,904 : Image to text: 23.9, 55.1, 70.2, 4.0
2019-02-14 12:10:47,183 : Text to Image: 17.22, 47.3, 64.78, 6.0
2019-02-14 12:10:47,558 : Image to text: 24.4, 56.1, 73.1, 4.0
2019-02-14 12:10:47,837 : Text to Image: 16.9, 47.28, 63.84, 6.0
2019-02-14 12:10:48,212 : Image to text: 23.9, 54.6, 69.4, 5.0
2019-02-14 12:10:48,490 : Text to Image: 17.42, 48.44, 64.22, 6.0
2019-02-14 12:10:48,490 : Dev mean Text to Image: 17.2, 47.468, 64.54799999999999, 6.0
2019-02-14 12:10:48,490 : Dev mean Image to text: 23.0, 54.28, 70.48, 4.6
2019-02-14 12:10:48,491 : start epoch
2019-02-14 12:11:28,491 : samples : 64000
2019-02-14 12:11:38,291 : Image to text: 8.5, 24.1, 35.36, 20.0
2019-02-14 12:11:45,045 : Text to Image: 6.072, 19.7, 30.384, 26.0
2019-02-14 12:12:25,628 : samples : 128000
2019-02-14 12:12:35,460 : Image to text: 8.78, 24.94, 36.72, 20.0
2019-02-14 12:12:42,195 : Text to Image: 6.704, 20.82, 31.524, 25.0
2019-02-14 12:13:22,398 : samples : 192000
2019-02-14 12:13:32,203 : Image to text: 8.16, 24.84, 35.96, 21.0
2019-02-14 12:13:38,920 : Text to Image: 6.324, 20.196, 30.824, 26.0
2019-02-14 12:14:19,049 : samples : 256000
2019-02-14 12:14:28,830 : Image to text: 8.22, 25.48, 36.34, 20.0
2019-02-14 12:14:35,556 : Text to Image: 6.436, 20.564, 31.168, 25.0
2019-02-14 12:15:15,553 : samples : 320000
2019-02-14 12:15:25,349 : Image to text: 8.12, 25.58, 37.6, 19.0
2019-02-14 12:15:32,079 : Text to Image: 6.6, 21.204, 32.388, 23.0
2019-02-14 12:16:12,079 : samples : 384000
2019-02-14 12:16:21,874 : Image to text: 8.8, 25.6, 38.54, 19.0
2019-02-14 12:16:28,597 : Text to Image: 7.152, 22.424, 33.54, 23.0
2019-02-14 12:17:08,736 : samples : 448000
2019-02-14 12:17:18,580 : Image to text: 7.98, 25.1, 36.66, 20.0
2019-02-14 12:17:25,329 : Text to Image: 6.644, 20.632, 31.552, 24.0
2019-02-14 12:18:05,425 : samples : 512000
2019-02-14 12:18:15,257 : Image to text: 8.78, 26.12, 37.94, 19.0
2019-02-14 12:18:21,995 : Text to Image: 7.156, 22.428, 33.832, 22.0
2019-02-14 12:18:56,037 : Epoch 5 finished
2019-02-14 12:18:56,414 : Image to text: 22.6, 55.0, 72.1, 5.0
2019-02-14 12:18:56,694 : Text to Image: 18.8, 49.98, 67.8, 6.0
2019-02-14 12:18:57,070 : Image to text: 24.4, 53.7, 69.7, 5.0
2019-02-14 12:18:57,349 : Text to Image: 19.32, 50.36, 67.76, 5.0
2019-02-14 12:18:57,727 : Image to text: 24.7, 55.4, 70.8, 5.0
2019-02-14 12:18:58,006 : Text to Image: 18.1, 49.76, 66.92, 6.0
2019-02-14 12:18:58,383 : Image to text: 25.5, 56.2, 73.1, 4.0
2019-02-14 12:18:58,662 : Text to Image: 19.22, 49.44, 68.02, 6.0
2019-02-14 12:18:59,039 : Image to text: 25.5, 57.0, 72.2, 4.0
2019-02-14 12:18:59,318 : Text to Image: 18.58, 50.46, 66.7, 5.0
2019-02-14 12:18:59,318 : Dev mean Text to Image: 18.804, 49.99999999999999, 67.44, 5.6000000000000005
2019-02-14 12:18:59,318 : Dev mean Image to text: 24.54, 55.46, 71.58, 4.6
2019-02-14 12:18:59,319 : start epoch
2019-02-14 12:19:39,510 : samples : 64000
2019-02-14 12:19:49,259 : Image to text: 9.42, 27.34, 39.74, 17.0
2019-02-14 12:19:55,958 : Text to Image: 7.264, 22.936, 33.948, 22.0
2019-02-14 12:20:36,155 : samples : 128000
2019-02-14 12:20:45,927 : Image to text: 8.62, 25.86, 39.02, 18.0
2019-02-14 12:20:52,636 : Text to Image: 7.068, 21.56, 32.604, 23.0
2019-02-14 12:21:33,317 : samples : 192000
2019-02-14 12:21:43,107 : Image to text: 8.9, 25.56, 37.64, 19.0
2019-02-14 12:21:49,836 : Text to Image: 7.304, 22.164, 33.296, 23.0
2019-02-14 12:22:30,435 : samples : 256000
2019-02-14 12:22:40,214 : Image to text: 9.44, 26.82, 38.16, 19.0
2019-02-14 12:22:46,915 : Text to Image: 7.144, 22.32, 33.472, 23.0
2019-02-14 12:23:27,010 : samples : 320000
2019-02-14 12:23:36,776 : Image to text: 8.7, 26.16, 37.24, 19.0
2019-02-14 12:23:43,474 : Text to Image: 6.976, 21.576, 32.52, 24.0
2019-02-14 12:24:23,618 : samples : 384000
2019-02-14 12:24:33,396 : Image to text: 8.54, 26.6, 38.92, 18.0
2019-02-14 12:24:40,098 : Text to Image: 7.304, 22.704, 34.1, 22.0
2019-02-14 12:25:20,268 : samples : 448000
2019-02-14 12:25:30,018 : Image to text: 8.5, 25.52, 37.6, 19.0
2019-02-14 12:25:36,721 : Text to Image: 7.152, 22.376, 33.724, 22.0
2019-02-14 12:26:16,925 : samples : 512000
2019-02-14 12:26:26,701 : Image to text: 8.98, 27.14, 38.46, 18.0
2019-02-14 12:26:33,410 : Text to Image: 7.216, 22.292, 33.468, 22.0
2019-02-14 12:27:07,717 : Epoch 6 finished
2019-02-14 12:27:08,092 : Image to text: 22.2, 52.7, 70.2, 5.0
2019-02-14 12:27:08,370 : Text to Image: 17.46, 48.32, 65.66, 6.0
2019-02-14 12:27:08,745 : Image to text: 22.0, 51.9, 69.6, 5.0
2019-02-14 12:27:09,023 : Text to Image: 16.28, 47.6, 64.64, 6.0
2019-02-14 12:27:09,397 : Image to text: 23.5, 52.4, 68.3, 5.0
2019-02-14 12:27:09,675 : Text to Image: 17.14, 46.96, 63.9, 6.0
2019-02-14 12:27:10,051 : Image to text: 23.7, 55.0, 70.7, 5.0
2019-02-14 12:27:10,329 : Text to Image: 17.56, 46.48, 65.0, 6.0
2019-02-14 12:27:10,703 : Image to text: 21.8, 52.6, 68.0, 5.0
2019-02-14 12:27:10,981 : Text to Image: 17.14, 47.2, 63.08, 6.0
2019-02-14 12:27:10,981 : Dev mean Text to Image: 17.116, 47.312, 64.456, 6.0
2019-02-14 12:27:10,981 : Dev mean Image to text: 22.64, 52.92, 69.36, 5.0
2019-02-14 12:27:10,981 : start epoch
2019-02-14 12:27:51,180 : samples : 64000
2019-02-14 12:28:00,956 : Image to text: 8.88, 26.12, 38.7, 18.0
2019-02-14 12:28:07,660 : Text to Image: 7.436, 22.368, 33.316, 23.0
2019-02-14 12:28:47,909 : samples : 128000
2019-02-14 12:28:57,689 : Image to text: 8.7, 26.0, 37.9, 18.0
2019-02-14 12:29:04,409 : Text to Image: 6.772, 21.54, 33.004, 23.0
2019-02-14 12:29:44,550 : samples : 192000
2019-02-14 12:29:54,363 : Image to text: 8.7, 26.44, 38.42, 18.0
2019-02-14 12:30:01,054 : Text to Image: 7.36, 23.256, 34.568, 21.0
2019-02-14 12:30:41,071 : samples : 256000
2019-02-14 12:30:50,854 : Image to text: 9.02, 25.34, 37.68, 19.0
2019-02-14 12:30:57,556 : Text to Image: 6.916, 21.508, 32.932, 24.0
2019-02-14 12:31:37,531 : samples : 320000
2019-02-14 12:31:47,346 : Image to text: 8.94, 25.58, 37.08, 19.0
2019-02-14 12:31:54,092 : Text to Image: 7.136, 21.844, 32.844, 23.0
2019-02-14 12:32:34,244 : samples : 384000
2019-02-14 12:32:44,030 : Image to text: 9.12, 27.04, 39.58, 17.0
2019-02-14 12:32:50,745 : Text to Image: 7.72, 24.048, 35.62, 21.0
2019-02-14 12:33:30,988 : samples : 448000
2019-02-14 12:33:40,744 : Image to text: 9.32, 27.32, 39.36, 17.0
2019-02-14 12:33:47,455 : Text to Image: 7.7, 23.648, 35.28, 21.0
2019-02-14 12:34:27,813 : samples : 512000
2019-02-14 12:34:37,582 : Image to text: 9.38, 26.22, 38.72, 18.0
2019-02-14 12:34:44,292 : Text to Image: 7.104, 22.568, 33.74, 22.0
2019-02-14 12:35:18,788 : Epoch 7 finished
2019-02-14 12:35:19,164 : Image to text: 22.4, 56.2, 72.0, 5.0
2019-02-14 12:35:19,441 : Text to Image: 19.44, 52.02, 69.8, 5.0
2019-02-14 12:35:19,817 : Image to text: 23.2, 56.3, 71.4, 5.0
2019-02-14 12:35:20,095 : Text to Image: 20.38, 51.64, 69.62, 5.0
2019-02-14 12:35:20,471 : Image to text: 25.7, 57.7, 70.7, 4.0
2019-02-14 12:35:20,749 : Text to Image: 19.62, 52.26, 69.2, 5.0
2019-02-14 12:35:21,125 : Image to text: 27.0, 59.1, 73.8, 4.0
2019-02-14 12:35:21,403 : Text to Image: 19.58, 51.42, 69.86, 5.0
2019-02-14 12:35:21,778 : Image to text: 24.8, 56.4, 71.7, 4.0
2019-02-14 12:35:22,055 : Text to Image: 20.14, 51.7, 67.9, 5.0
2019-02-14 12:35:22,055 : Dev mean Text to Image: 19.832, 51.80799999999999, 69.27600000000001, 5.0
2019-02-14 12:35:22,056 : Dev mean Image to text: 24.619999999999997, 57.14, 71.92, 4.3999999999999995
2019-02-14 12:35:22,056 : start epoch
2019-02-14 12:36:02,452 : samples : 64000
2019-02-14 12:36:12,251 : Image to text: 8.8, 27.0, 38.86, 18.0
2019-02-14 12:36:18,959 : Text to Image: 7.376, 22.5, 33.876, 22.0
2019-02-14 12:36:58,990 : samples : 128000
2019-02-14 12:37:08,774 : Image to text: 9.28, 27.58, 39.34, 17.0
2019-02-14 12:37:15,485 : Text to Image: 7.992, 24.288, 35.848, 20.0
2019-02-14 12:37:55,345 : samples : 192000
2019-02-14 12:38:05,133 : Image to text: 9.66, 28.3, 40.18, 17.0
2019-02-14 12:38:11,842 : Text to Image: 7.692, 22.916, 34.068, 22.0
2019-02-14 12:38:51,793 : samples : 256000
2019-02-14 12:39:01,557 : Image to text: 8.8, 26.66, 39.14, 18.0
2019-02-14 12:39:08,257 : Text to Image: 7.556, 23.216, 34.86, 21.0
2019-02-14 12:39:48,207 : samples : 320000
2019-02-14 12:39:57,983 : Image to text: 9.54, 27.54, 40.34, 16.0
2019-02-14 12:40:04,692 : Text to Image: 7.736, 23.148, 34.4, 22.0
2019-02-14 12:40:44,832 : samples : 384000
2019-02-14 12:40:54,619 : Image to text: 9.84, 28.72, 40.4, 17.0
2019-02-14 12:41:01,328 : Text to Image: 8.12, 24.064, 35.592, 21.0
2019-02-14 12:41:41,835 : samples : 448000
2019-02-14 12:41:51,687 : Image to text: 9.42, 27.66, 40.08, 17.0
2019-02-14 12:41:58,446 : Text to Image: 7.736, 23.604, 35.248, 21.0
2019-02-14 12:42:38,807 : samples : 512000
2019-02-14 12:42:48,580 : Image to text: 9.72, 28.3, 40.46, 17.0
2019-02-14 12:42:55,287 : Text to Image: 7.82, 23.9, 35.68, 20.0
2019-02-14 12:43:29,594 : Epoch 8 finished
2019-02-14 12:43:29,968 : Image to text: 23.8, 56.8, 71.6, 4.0
2019-02-14 12:43:30,247 : Text to Image: 18.78, 49.78, 67.5, 6.0
2019-02-14 12:43:30,623 : Image to text: 24.1, 56.6, 71.5, 4.0
2019-02-14 12:43:30,900 : Text to Image: 17.26, 48.48, 66.22, 6.0
2019-02-14 12:43:31,274 : Image to text: 25.1, 56.2, 73.8, 4.0
2019-02-14 12:43:31,552 : Text to Image: 17.96, 49.0, 66.68, 6.0
2019-02-14 12:43:31,926 : Image to text: 26.0, 59.7, 75.5, 4.0
2019-02-14 12:43:32,204 : Text to Image: 18.28, 48.94, 66.22, 6.0
2019-02-14 12:43:32,579 : Image to text: 24.0, 55.8, 72.8, 4.0
2019-02-14 12:43:32,856 : Text to Image: 18.5, 49.24, 65.46, 6.0
2019-02-14 12:43:32,856 : Dev mean Text to Image: 18.156000000000002, 49.088, 66.416, 6.0
2019-02-14 12:43:32,856 : Dev mean Image to text: 24.6, 57.019999999999996, 73.03999999999999, 4.0
2019-02-14 12:43:32,856 : start epoch
2019-02-14 12:44:12,966 : samples : 64000
2019-02-14 12:44:22,769 : Image to text: 9.72, 27.6, 40.72, 16.0
2019-02-14 12:44:29,475 : Text to Image: 7.948, 23.772, 35.12, 21.0
2019-02-14 12:45:09,565 : samples : 128000
2019-02-14 12:45:19,357 : Image to text: 9.18, 27.9, 40.16, 16.0
2019-02-14 12:45:26,058 : Text to Image: 7.776, 23.844, 34.788, 21.0
2019-02-14 12:46:05,845 : samples : 192000
2019-02-14 12:46:15,628 : Image to text: 9.58, 27.38, 39.24, 18.0
2019-02-14 12:46:22,330 : Text to Image: 7.128, 22.06, 33.492, 22.0
2019-02-14 12:47:02,242 : samples : 256000
2019-02-14 12:47:12,013 : Image to text: 9.2, 27.5, 39.88, 17.0
2019-02-14 12:47:18,723 : Text to Image: 7.692, 23.208, 34.764, 21.0
2019-02-14 12:47:59,187 : samples : 320000
2019-02-14 12:48:08,959 : Image to text: 9.52, 27.84, 39.7, 17.0
2019-02-14 12:48:15,673 : Text to Image: 7.212, 22.312, 33.832, 22.0
2019-02-14 12:48:56,330 : samples : 384000
2019-02-14 12:49:06,108 : Image to text: 9.18, 27.42, 38.88, 18.0
2019-02-14 12:49:12,813 : Text to Image: 7.396, 22.76, 34.12, 22.0
2019-02-14 12:49:53,403 : samples : 448000
2019-02-14 12:50:03,182 : Image to text: 9.48, 27.48, 39.3, 18.0
2019-02-14 12:50:09,905 : Text to Image: 7.64, 23.328, 34.752, 21.0
2019-02-14 12:50:50,322 : samples : 512000
2019-02-14 12:51:00,105 : Image to text: 10.2, 28.8, 40.56, 16.0
2019-02-14 12:51:06,831 : Text to Image: 7.768, 23.428, 34.792, 21.0
2019-02-14 12:51:40,945 : Epoch 9 finished
2019-02-14 12:51:41,321 : Image to text: 25.4, 58.4, 74.4, 4.0
2019-02-14 12:51:41,598 : Text to Image: 22.3, 54.5, 71.66, 5.0
2019-02-14 12:51:41,974 : Image to text: 26.8, 59.1, 73.1, 4.0
2019-02-14 12:51:42,251 : Text to Image: 21.8, 53.72, 71.18, 5.0
2019-02-14 12:51:42,627 : Image to text: 28.6, 57.9, 73.4, 4.0
2019-02-14 12:51:42,905 : Text to Image: 21.5, 55.18, 71.84, 5.0
2019-02-14 12:51:43,281 : Image to text: 27.1, 62.4, 75.6, 3.0
2019-02-14 12:51:43,560 : Text to Image: 21.8, 54.84, 70.68, 5.0
2019-02-14 12:51:43,935 : Image to text: 27.0, 58.7, 72.8, 4.0
2019-02-14 12:51:44,212 : Text to Image: 23.42, 54.14, 70.2, 5.0
2019-02-14 12:51:44,213 : Dev mean Text to Image: 22.164, 54.476, 71.11200000000001, 5.0
2019-02-14 12:51:44,213 : Dev mean Image to text: 26.980000000000004, 59.300000000000004, 73.86, 3.8000000000000007
2019-02-14 12:51:44,213 : start epoch
2019-02-14 12:52:24,026 : samples : 64000
2019-02-14 12:52:33,790 : Image to text: 9.68, 27.84, 39.52, 17.0
2019-02-14 12:52:40,470 : Text to Image: 7.756, 23.196, 34.856, 21.0
2019-02-14 12:53:20,349 : samples : 128000
2019-02-14 12:53:30,121 : Image to text: 10.24, 28.36, 40.28, 16.0
2019-02-14 12:53:36,822 : Text to Image: 8.192, 24.832, 36.384, 20.0
2019-02-14 12:54:16,586 : samples : 192000
2019-02-14 12:54:26,352 : Image to text: 10.02, 27.84, 40.92, 16.0
2019-02-14 12:54:33,065 : Text to Image: 7.296, 22.252, 33.54, 22.0
2019-02-14 12:55:12,987 : samples : 256000
2019-02-14 12:55:22,755 : Image to text: 10.02, 28.28, 40.96, 16.0
2019-02-14 12:55:29,455 : Text to Image: 8.236, 24.332, 36.128, 20.0
2019-02-14 12:56:09,310 : samples : 320000
2019-02-14 12:56:19,104 : Image to text: 9.22, 26.36, 38.7, 17.0
2019-02-14 12:56:25,808 : Text to Image: 6.976, 22.364, 33.528, 23.0
2019-02-14 12:57:05,822 : samples : 384000
2019-02-14 12:57:15,581 : Image to text: 9.92, 28.26, 40.66, 16.0
2019-02-14 12:57:22,286 : Text to Image: 7.868, 23.888, 35.748, 20.0
2019-02-14 12:58:02,113 : samples : 448000
2019-02-14 12:58:11,904 : Image to text: 9.58, 27.48, 40.06, 16.0
2019-02-14 12:58:18,608 : Text to Image: 7.708, 23.264, 34.668, 22.0
2019-02-14 12:58:58,308 : samples : 512000
2019-02-14 12:59:08,093 : Image to text: 10.02, 28.24, 40.78, 16.0
2019-02-14 12:59:14,797 : Text to Image: 8.14, 24.424, 36.292, 20.0
2019-02-14 12:59:48,706 : Epoch 10 finished
2019-02-14 12:59:49,080 : Image to text: 23.1, 60.2, 74.4, 4.0
2019-02-14 12:59:49,358 : Text to Image: 22.16, 55.08, 72.88, 5.0
2019-02-14 12:59:49,733 : Image to text: 25.6, 58.6, 73.3, 4.0
2019-02-14 12:59:50,010 : Text to Image: 21.08, 53.4, 70.8, 5.0
2019-02-14 12:59:50,385 : Image to text: 27.2, 58.2, 72.6, 4.0
2019-02-14 12:59:50,663 : Text to Image: 21.62, 54.58, 71.3, 5.0
2019-02-14 12:59:51,038 : Image to text: 26.4, 61.3, 76.0, 3.0
2019-02-14 12:59:51,316 : Text to Image: 21.6, 54.3, 71.3, 5.0
2019-02-14 12:59:51,690 : Image to text: 27.9, 57.6, 73.8, 4.0
2019-02-14 12:59:51,968 : Text to Image: 21.78, 54.88, 71.08, 5.0
2019-02-14 12:59:51,968 : Dev mean Text to Image: 21.648000000000003, 54.44799999999999, 71.472, 5.0
2019-02-14 12:59:51,968 : Dev mean Image to text: 26.04, 59.18000000000001, 74.02000000000001, 3.8000000000000007
2019-02-14 12:59:55,229 : 
Test scores | Image to text:             26.08, 59.040000000000006, 74.14, 4.0
2019-02-14 12:59:55,229 : Test scores | Text to image:             21.496000000000002, 54.152, 70.648, 4.8

2019-02-14 12:59:55,353 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 12:59:55,709 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 12:59:56,338 : loading BERT model bert-base-uncased
2019-02-14 12:59:56,339 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 12:59:56,370 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 12:59:56,370 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw4650j1z
2019-02-14 12:59:58,802 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:00:00,231 : Computing embeddings for train/dev/test
2019-02-14 13:01:21,120 : Computed embeddings
2019-02-14 13:01:21,120 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:01:39,888 : [('reg:1e-05', 65.64), ('reg:0.0001', 55.54), ('reg:0.001', 39.14), ('reg:0.01', 16.7)]
2019-02-14 13:01:39,889 : Validation : best param found is reg = 1e-05 with score             65.64
2019-02-14 13:01:39,889 : Evaluating...
2019-02-14 13:01:45,546 : 
Dev acc : 65.6 Test acc : 67.3 for LENGTH classification

2019-02-14 13:01:45,546 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 13:01:45,886 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 13:01:45,932 : loading BERT model bert-base-uncased
2019-02-14 13:01:45,932 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:01:45,963 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:01:45,963 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpa38isd71
2019-02-14 13:01:48,390 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:01:49,816 : Computing embeddings for train/dev/test
2019-02-14 13:03:05,594 : Computed embeddings
2019-02-14 13:03:05,595 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:03:33,183 : [('reg:1e-05', 0.4), ('reg:0.0001', 0.14), ('reg:0.001', 0.11), ('reg:0.01', 0.1)]
2019-02-14 13:03:33,183 : Validation : best param found is reg = 1e-05 with score             0.4
2019-02-14 13:03:33,183 : Evaluating...
2019-02-14 13:03:41,439 : 
Dev acc : 0.4 Test acc : 0.4 for WORDCONTENT classification

2019-02-14 13:03:41,441 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 13:03:41,798 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 13:03:41,863 : loading BERT model bert-base-uncased
2019-02-14 13:03:41,864 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:03:41,889 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:03:41,889 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpqifb423p
2019-02-14 13:03:44,362 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:03:45,783 : Computing embeddings for train/dev/test
2019-02-14 13:04:56,870 : Computed embeddings
2019-02-14 13:04:56,871 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:05:23,353 : [('reg:1e-05', 26.38), ('reg:0.0001', 21.84), ('reg:0.001', 18.26), ('reg:0.01', 18.07)]
2019-02-14 13:05:23,353 : Validation : best param found is reg = 1e-05 with score             26.38
2019-02-14 13:05:23,353 : Evaluating...
2019-02-14 13:05:29,922 : 
Dev acc : 26.4 Test acc : 25.5 for DEPTH classification

2019-02-14 13:05:29,923 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 13:05:30,308 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 13:05:30,370 : loading BERT model bert-base-uncased
2019-02-14 13:05:30,370 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:05:30,480 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:05:30,480 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmozv6v86
2019-02-14 13:05:32,933 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:05:34,343 : Computing embeddings for train/dev/test
2019-02-14 13:06:40,899 : Computed embeddings
2019-02-14 13:06:40,899 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:07:09,140 : [('reg:1e-05', 62.92), ('reg:0.0001', 35.41), ('reg:0.001', 10.48), ('reg:0.01', 8.5)]
2019-02-14 13:07:09,140 : Validation : best param found is reg = 1e-05 with score             62.92
2019-02-14 13:07:09,140 : Evaluating...
2019-02-14 13:07:18,612 : 
Dev acc : 62.9 Test acc : 62.9 for TOPCONSTITUENTS classification

2019-02-14 13:07:18,613 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 13:07:18,960 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 13:07:19,027 : loading BERT model bert-base-uncased
2019-02-14 13:07:19,027 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:07:19,150 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:07:19,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2fgg12g1
2019-02-14 13:07:21,602 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:07:23,023 : Computing embeddings for train/dev/test
2019-02-14 13:08:34,931 : Computed embeddings
2019-02-14 13:08:34,931 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:09:06,665 : [('reg:1e-05', 82.76), ('reg:0.0001', 81.23), ('reg:0.001', 75.78), ('reg:0.01', 51.38)]
2019-02-14 13:09:06,665 : Validation : best param found is reg = 1e-05 with score             82.76
2019-02-14 13:09:06,665 : Evaluating...
2019-02-14 13:09:16,484 : 
Dev acc : 82.8 Test acc : 81.6 for BIGRAMSHIFT classification

2019-02-14 13:09:16,485 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 13:09:17,063 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 13:09:17,131 : loading BERT model bert-base-uncased
2019-02-14 13:09:17,131 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:09:17,162 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:09:17,163 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpap1_lj9l
2019-02-14 13:09:19,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:09:21,032 : Computing embeddings for train/dev/test
2019-02-14 13:10:31,383 : Computed embeddings
2019-02-14 13:10:31,383 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:11:01,676 : [('reg:1e-05', 87.87), ('reg:0.0001', 86.05), ('reg:0.001', 76.93), ('reg:0.01', 67.73)]
2019-02-14 13:11:01,676 : Validation : best param found is reg = 1e-05 with score             87.87
2019-02-14 13:11:01,676 : Evaluating...
2019-02-14 13:11:12,622 : 
Dev acc : 87.9 Test acc : 86.8 for TENSE classification

2019-02-14 13:11:12,623 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 13:11:13,043 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 13:11:13,106 : loading BERT model bert-base-uncased
2019-02-14 13:11:13,106 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:11:13,135 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:11:13,135 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpuro1ij04
2019-02-14 13:11:15,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:11:17,061 : Computing embeddings for train/dev/test
2019-02-14 13:12:31,894 : Computed embeddings
2019-02-14 13:12:31,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:13:05,547 : [('reg:1e-05', 81.45), ('reg:0.0001', 79.03), ('reg:0.001', 70.97), ('reg:0.01', 50.0)]
2019-02-14 13:13:05,547 : Validation : best param found is reg = 1e-05 with score             81.45
2019-02-14 13:13:05,547 : Evaluating...
2019-02-14 13:13:15,498 : 
Dev acc : 81.5 Test acc : 80.2 for SUBJNUMBER classification

2019-02-14 13:13:15,499 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 13:13:15,922 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 13:13:15,989 : loading BERT model bert-base-uncased
2019-02-14 13:13:15,989 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:13:16,016 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:13:16,016 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1pc_ok0y
2019-02-14 13:13:18,460 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:13:19,885 : Computing embeddings for train/dev/test
2019-02-14 13:14:33,279 : Computed embeddings
2019-02-14 13:14:33,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:15:03,006 : [('reg:1e-05', 77.35), ('reg:0.0001', 72.09), ('reg:0.001', 63.82), ('reg:0.01', 61.63)]
2019-02-14 13:15:03,006 : Validation : best param found is reg = 1e-05 with score             77.35
2019-02-14 13:15:03,006 : Evaluating...
2019-02-14 13:15:12,566 : 
Dev acc : 77.3 Test acc : 79.0 for OBJNUMBER classification

2019-02-14 13:15:12,567 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 13:15:12,960 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 13:15:13,028 : loading BERT model bert-base-uncased
2019-02-14 13:15:13,028 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:15:13,152 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:15:13,152 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpl4mwhdwn
2019-02-14 13:15:15,592 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:15:17,011 : Computing embeddings for train/dev/test
2019-02-14 13:16:41,154 : Computed embeddings
2019-02-14 13:16:41,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:17:14,992 : [('reg:1e-05', 59.04), ('reg:0.0001', 57.25), ('reg:0.001', 54.95), ('reg:0.01', 50.19)]
2019-02-14 13:17:14,993 : Validation : best param found is reg = 1e-05 with score             59.04
2019-02-14 13:17:14,993 : Evaluating...
2019-02-14 13:17:23,738 : 
Dev acc : 59.0 Test acc : 60.0 for ODDMANOUT classification

2019-02-14 13:17:23,739 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 13:17:24,138 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 13:17:24,214 : loading BERT model bert-base-uncased
2019-02-14 13:17:24,214 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:17:24,340 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:17:24,340 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmh553o8j
2019-02-14 13:17:26,777 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:17:28,199 : Computing embeddings for train/dev/test
2019-02-14 13:18:51,598 : Computed embeddings
2019-02-14 13:18:51,598 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:19:19,999 : [('reg:1e-05', 56.05), ('reg:0.0001', 50.29), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-14 13:19:19,999 : Validation : best param found is reg = 1e-05 with score             56.05
2019-02-14 13:19:19,999 : Evaluating...
2019-02-14 13:19:30,903 : 
Dev acc : 56.0 Test acc : 56.4 for COORDINATIONINVERSION classification

2019-02-14 13:19:30,906 : total results: {'STS12': {'MSRpar': {'pearson': (0.39890830953354295, 5.1283001241398034e-30), 'spearman': SpearmanrResult(correlation=0.414614866972636, pvalue=1.608192978421931e-32), 'nsamples': 750}, 'MSRvid': {'pearson': (0.33266468759746587, 7.73768117787614e-21), 'spearman': SpearmanrResult(correlation=0.34810324106775864, pvalue=8.677041218635792e-23), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.4151876144641015, 1.4905878042519672e-20), 'spearman': SpearmanrResult(correlation=0.5468367816001559, pvalue=3.749301406629519e-37), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.4767669979085707, 7.929563411438962e-44), 'spearman': SpearmanrResult(correlation=0.4879415182423953, pvalue=4.048293704510233e-46), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4779108751441314, 3.68816754878048e-24), 'spearman': SpearmanrResult(correlation=0.4358765344239953, pvalue=6.221573672889338e-20), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.42028769692956247, 'wmean': 0.4142575773813435}, 'spearman': {'mean': 0.4466745884613882, 'wmean': 0.4385159394149737}}}, 'STS13': {'FNWN': {'pearson': (0.18833740305422522, 0.00945078368807984), 'spearman': SpearmanrResult(correlation=0.20130829931228056, pvalue=0.005475090224838535), 'nsamples': 189}, 'headlines': {'pearson': (0.4503457662883263, 9.838143720312074e-39), 'spearman': SpearmanrResult(correlation=0.4612830985491905, pvalue=8.675127717005991e-41), 'nsamples': 750}, 'OnWN': {'pearson': (0.09943458415592683, 0.0184858825428958), 'spearman': SpearmanrResult(correlation=0.1612509971231926, pvalue=0.00012509889904440744), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.24603925116615946, 'wmean': 0.28609193040331216}, 'spearman': {'mean': 0.2746141316615545, 'wmean': 0.3163142679120166}}}, 'STS14': {'deft-forum': {'pearson': (0.23300025976041036, 5.796203345083411e-07), 'spearman': SpearmanrResult(correlation=0.23762879989301172, pvalue=3.3950725840498245e-07), 'nsamples': 450}, 'deft-news': {'pearson': (0.5025160102324262, 1.3343417368069778e-20), 'spearman': SpearmanrResult(correlation=0.5375951626952655, pvalue=7.216258802250657e-24), 'nsamples': 300}, 'headlines': {'pearson': (0.40230746691248376, 1.5115412759692527e-30), 'spearman': SpearmanrResult(correlation=0.3921358079517284, pvalue=5.608554899846246e-29), 'nsamples': 750}, 'images': {'pearson': (0.46130699768922584, 8.584286636857383e-41), 'spearman': SpearmanrResult(correlation=0.4748619251091736, pvalue=1.9125470247016364e-43), 'nsamples': 750}, 'OnWN': {'pearson': (0.256129302080353, 1.0650936396565341e-12), 'spearman': SpearmanrResult(correlation=0.3191507060148055, pvalue=3.2141467131556186e-19), 'nsamples': 750}, 'tweet-news': {'pearson': (0.4576777989881973, 4.2053563425350736e-40), 'spearman': SpearmanrResult(correlation=0.47102260135708096, pvalue=1.1089484446300289e-42), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.38548963927718277, 'wmean': 0.3836456251238953}, 'spearman': {'mean': 0.4053991671701776, 'wmean': 0.4029572770893403}}}, 'STS15': {'answers-forums': {'pearson': (0.3258218234610144, 1.008133793118413e-10), 'spearman': SpearmanrResult(correlation=0.332725912424606, pvalue=3.8173880228239106e-11), 'nsamples': 375}, 'answers-students': {'pearson': (0.4652810877917812, 1.4738200721570099e-41), 'spearman': SpearmanrResult(correlation=0.4987295625364571, pvalue=2.0545387728722715e-48), 'nsamples': 750}, 'belief': {'pearson': (0.37665220890149914, 4.375763054286995e-14), 'spearman': SpearmanrResult(correlation=0.4032241895428678, pvalue=4.290049444935592e-16), 'nsamples': 375}, 'headlines': {'pearson': (0.5015658368233854, 4.9627867675238154e-49), 'spearman': SpearmanrResult(correlation=0.5131253399626333, pvalue=1.3190210208223188e-51), 'nsamples': 750}, 'images': {'pearson': (0.44452147105863327, 1.1401681721411957e-37), 'spearman': SpearmanrResult(correlation=0.4942986292652096, pvalue=1.8410841203476195e-47), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4227684856072627, 'wmean': 0.4406513529637642}, 'spearman': {'mean': 0.4484207267463548, 'wmean': 0.46853214568700924}}}, 'STS16': {'answer-answer': {'pearson': (0.4155560352105585, 5.020563163513436e-12), 'spearman': SpearmanrResult(correlation=0.5316381383620205, pvalue=6.221646110170063e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.567727481689487, 1.2006074189255204e-22), 'spearman': SpearmanrResult(correlation=0.5870858057037144, pvalue=1.8451508512266372e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7467296387005437, 2.952529863133019e-42), 'spearman': SpearmanrResult(correlation=0.7625774536874789, pvalue=5.119921267782848e-45), 'nsamples': 230}, 'postediting': {'pearson': (0.7399085234868538, 1.4890298404203175e-43), 'spearman': SpearmanrResult(correlation=0.7639572997575838, pvalue=6.055773190431862e-48), 'nsamples': 244}, 'question-question': {'pearson': (0.1694179749791548, 0.014194162556943703), 'spearman': SpearmanrResult(correlation=0.19556234095251984, pvalue=0.004543874642797153), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5278679308133196, 'wmean': 0.5350836671894813}, 'spearman': {'mean': 0.5681642076926634, 'wmean': 0.5766369118990096}}}, 'MR': {'devacc': 68.69, 'acc': 68.97, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 67.72, 'acc': 68.35, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 74.35, 'acc': 79.17, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.06, 'acc': 88.85, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 76.72, 'acc': 76.0, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 36.97, 'acc': 37.06, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 46.87, 'acc': 54.6, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.26, 'acc': 72.7, 'f1': 80.59, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 62.0, 'acc': 62.19, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6986867134634558, 'pearson': 0.6976025716509199, 'spearman': 0.6670387387020428, 'mse': 0.5243659271841756, 'yhat': array([2.30853472, 3.86379249, 3.51795982, ..., 2.87849801, 4.16439561,        4.55578638]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6116760848689891, 'pearson': 0.54161073709479, 'spearman': 0.5394175618542567, 'mse': 1.9022788282560792, 'yhat': array([2.25550614, 2.17359069, 3.48527271, ..., 3.85459097, 3.60177528,        3.20486947]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.58, 'acc': 58.71, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 307.892, 'acc': [(26.08, 59.040000000000006, 74.14, 4.0), (21.496000000000002, 54.152, 70.648, 4.8)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 65.64, 'acc': 67.34, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.4, 'acc': 0.38, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 26.38, 'acc': 25.47, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.92, 'acc': 62.91, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 82.76, 'acc': 81.59, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 87.87, 'acc': 86.81, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 81.45, 'acc': 80.18, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.35, 'acc': 78.99, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 59.04, 'acc': 59.97, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 56.05, 'acc': 56.35, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 13:19:30,906 : STS12 p=0.4143, STS12 s=0.4385, STS13 p=0.2861, STS13 s=0.3163, STS14 p=0.3836, STS14 s=0.4030, STS15 p=0.4407, STS15 s=0.4685, STS 16 p=0.5351, STS16 s=0.5766, STS B p=0.5416, STS B s=0.5394, STS B m=1.9023, SICK-R p=0.6976, SICK-R s=0.6670, SICK-P m=0.5244
2019-02-14 13:19:30,906 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 13:19:30,907 : 0.4143,0.4385,0.2861,0.3163,0.3836,0.4030,0.4407,0.4685,0.5351,0.5766,0.5416,0.5394,1.9023,0.6976,0.6670,0.5244
2019-02-14 13:19:30,907 : MR=68.97, CR=68.35, SUBJ=88.85, MPQA=79.17, SST-B=76.00, SST-F=37.06, TREC=54.60, SICK-E=62.19, SNLI=58.71, MRPC=72.70, MRPC f=80.59
2019-02-14 13:19:30,907 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 13:19:30,907 : 68.97,68.35,88.85,79.17,76.00,37.06,54.60,62.19,58.71,72.70,80.59
2019-02-14 13:19:30,907 : COCO r1i2t=26.08, COCO r5i2t=59.04, COCO r10i2t=74.14, COCO medr_i2t=4.00, COCO r1t2i=21.50, COCO r5t2i=54.15, COCO r10t2i=70.65, COCO medr_t2i=4.80
2019-02-14 13:19:30,907 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 13:19:30,907 : 26.08,59.04,74.14,4.00,21.50,54.15,70.65,4.80
2019-02-14 13:19:30,907 : SentLen=67.34, WC=0.38, TreeDepth=25.47, TopConst=62.91, BShift=81.59, Tense=86.81, SubjNum=80.18, ObjNum=78.99, SOMO=59.97, CoordInv=56.35, average=60.00
2019-02-14 13:19:30,907 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 13:19:30,907 : 67.34,0.38,25.47,62.91,81.59,86.81,80.18,78.99,59.97,56.35,60.00
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:30,907 : layer 7
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:30,907 : ********************************************************************************
2019-02-14 13:19:31,000 : ***** Transfer task : STS12 *****


2019-02-14 13:19:31,037 : loading BERT model bert-base-uncased
2019-02-14 13:19:31,037 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:19:31,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:19:31,054 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpdqqpwtg6
2019-02-14 13:19:33,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:19:36,456 : MSRpar : pearson = 0.3159, spearman = 0.3650
2019-02-14 13:19:37,120 : MSRvid : pearson = 0.2727, spearman = 0.3202
2019-02-14 13:19:37,686 : SMTeuroparl : pearson = 0.4696, spearman = 0.5578
2019-02-14 13:19:38,695 : surprise.OnWN : pearson = 0.5121, spearman = 0.5323
2019-02-14 13:19:39,260 : surprise.SMTnews : pearson = 0.5728, spearman = 0.4982
2019-02-14 13:19:39,261 : ALL (weighted average) : Pearson = 0.4085,             Spearman = 0.4401
2019-02-14 13:19:39,261 : ALL (average) : Pearson = 0.4286,             Spearman = 0.4547

2019-02-14 13:19:39,261 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 13:19:39,269 : loading BERT model bert-base-uncased
2019-02-14 13:19:39,270 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:19:39,287 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:19:39,287 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb_5w1hab
2019-02-14 13:19:41,742 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:19:43,690 : FNWN : pearson = 0.2036, spearman = 0.2146
2019-02-14 13:19:44,460 : headlines : pearson = 0.4583, spearman = 0.4536
2019-02-14 13:19:45,038 : OnWN : pearson = 0.1458, spearman = 0.1830
2019-02-14 13:19:45,039 : ALL (weighted average) : Pearson = 0.3093,             Spearman = 0.3223
2019-02-14 13:19:45,039 : ALL (average) : Pearson = 0.2692,             Spearman = 0.2837

2019-02-14 13:19:45,039 : ***** Transfer task : STS14 *****


2019-02-14 13:19:45,055 : loading BERT model bert-base-uncased
2019-02-14 13:19:45,055 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:19:45,104 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:19:45,104 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2cfhybv4
2019-02-14 13:19:47,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:19:49,570 : deft-forum : pearson = 0.0815, spearman = 0.0977
2019-02-14 13:19:50,191 : deft-news : pearson = 0.5395, spearman = 0.5523
2019-02-14 13:19:51,051 : headlines : pearson = 0.4196, spearman = 0.3809
2019-02-14 13:19:51,872 : images : pearson = 0.3821, spearman = 0.3904
2019-02-14 13:19:52,707 : OnWN : pearson = 0.3361, spearman = 0.3786
2019-02-14 13:19:53,800 : tweet-news : pearson = 0.4536, spearman = 0.4510
2019-02-14 13:19:53,800 : ALL (weighted average) : Pearson = 0.3712,             Spearman = 0.3761
2019-02-14 13:19:53,800 : ALL (average) : Pearson = 0.3687,             Spearman = 0.3751

2019-02-14 13:19:53,801 : ***** Transfer task : STS15 *****


2019-02-14 13:19:53,832 : loading BERT model bert-base-uncased
2019-02-14 13:19:53,832 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:19:53,849 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:19:53,850 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmm3fu_y3
2019-02-14 13:19:56,278 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:19:58,482 : answers-forums : pearson = 0.4639, spearman = 0.4749
2019-02-14 13:19:59,303 : answers-students : pearson = 0.5228, spearman = 0.5365
2019-02-14 13:20:00,049 : belief : pearson = 0.5053, spearman = 0.5397
2019-02-14 13:20:00,927 : headlines : pearson = 0.5008, spearman = 0.5087
2019-02-14 13:20:01,771 : images : pearson = 0.4790, spearman = 0.4884
2019-02-14 13:20:01,771 : ALL (weighted average) : Pearson = 0.4968,             Spearman = 0.5102
2019-02-14 13:20:01,771 : ALL (average) : Pearson = 0.4943,             Spearman = 0.5096

2019-02-14 13:20:01,771 : ***** Transfer task : STS16 *****


2019-02-14 13:20:01,844 : loading BERT model bert-base-uncased
2019-02-14 13:20:01,844 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:20:01,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:20:01,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfkscewno
2019-02-14 13:20:04,284 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:20:06,035 : answer-answer : pearson = 0.5021, spearman = 0.5339
2019-02-14 13:20:06,305 : headlines : pearson = 0.5769, spearman = 0.5888
2019-02-14 13:20:06,648 : plagiarism : pearson = 0.7256, spearman = 0.7555
2019-02-14 13:20:07,192 : postediting : pearson = 0.7688, spearman = 0.7918
2019-02-14 13:20:07,441 : question-question : pearson = 0.0993, spearman = 0.1091
2019-02-14 13:20:07,441 : ALL (weighted average) : Pearson = 0.5450,             Spearman = 0.5666
2019-02-14 13:20:07,441 : ALL (average) : Pearson = 0.5345,             Spearman = 0.5558

2019-02-14 13:20:07,441 : ***** Transfer task : MR *****


2019-02-14 13:20:07,457 : loading BERT model bert-base-uncased
2019-02-14 13:20:07,458 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:20:07,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:20:07,479 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ws285xn
2019-02-14 13:20:09,896 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:20:11,355 : Generating sentence embeddings
2019-02-14 13:20:23,068 : Generated sentence embeddings
2019-02-14 13:20:23,068 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 13:20:33,375 : Best param found at split 1: l2reg = 1e-05                 with score 72.85
2019-02-14 13:20:44,982 : Best param found at split 2: l2reg = 1e-05                 with score 72.43
2019-02-14 13:20:56,864 : Best param found at split 3: l2reg = 1e-05                 with score 73.11
2019-02-14 13:21:07,708 : Best param found at split 4: l2reg = 1e-05                 with score 72.57
2019-02-14 13:21:17,333 : Best param found at split 5: l2reg = 0.0001                 with score 73.05
2019-02-14 13:21:17,722 : Dev acc : 72.8 Test acc : 72.73

2019-02-14 13:21:17,723 : ***** Transfer task : CR *****


2019-02-14 13:21:17,730 : loading BERT model bert-base-uncased
2019-02-14 13:21:17,730 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:21:17,750 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:21:17,751 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzjnfqqqz
2019-02-14 13:21:20,195 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:21:21,635 : Generating sentence embeddings
2019-02-14 13:21:24,842 : Generated sentence embeddings
2019-02-14 13:21:24,842 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 13:21:27,866 : Best param found at split 1: l2reg = 1e-05                 with score 75.52
2019-02-14 13:21:31,100 : Best param found at split 2: l2reg = 1e-05                 with score 74.79
2019-02-14 13:21:35,045 : Best param found at split 3: l2reg = 1e-05                 with score 75.79
2019-02-14 13:21:38,964 : Best param found at split 4: l2reg = 1e-05                 with score 75.6
2019-02-14 13:21:42,810 : Best param found at split 5: l2reg = 1e-05                 with score 74.45
2019-02-14 13:21:42,999 : Dev acc : 75.23 Test acc : 75.12

2019-02-14 13:21:43,000 : ***** Transfer task : MPQA *****


2019-02-14 13:21:43,005 : loading BERT model bert-base-uncased
2019-02-14 13:21:43,005 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:21:43,024 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:21:43,024 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphr_svo8g
2019-02-14 13:21:45,468 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:21:46,933 : Generating sentence embeddings
2019-02-14 13:21:50,157 : Generated sentence embeddings
2019-02-14 13:21:50,157 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 13:21:59,591 : Best param found at split 1: l2reg = 1e-05                 with score 77.88
2019-02-14 13:22:09,394 : Best param found at split 2: l2reg = 1e-05                 with score 79.97
2019-02-14 13:22:21,206 : Best param found at split 3: l2reg = 1e-05                 with score 79.29
2019-02-14 13:22:32,617 : Best param found at split 4: l2reg = 0.0001                 with score 79.51
2019-02-14 13:22:44,253 : Best param found at split 5: l2reg = 1e-05                 with score 77.61
2019-02-14 13:22:44,951 : Dev acc : 78.85 Test acc : 82.27

2019-02-14 13:22:44,952 : ***** Transfer task : SUBJ *****


2019-02-14 13:22:44,969 : loading BERT model bert-base-uncased
2019-02-14 13:22:44,970 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:22:44,989 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:22:44,989 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_1ec69kh
2019-02-14 13:22:47,420 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:22:48,842 : Generating sentence embeddings
2019-02-14 13:23:00,342 : Generated sentence embeddings
2019-02-14 13:23:00,343 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 13:23:12,109 : Best param found at split 1: l2reg = 1e-05                 with score 91.46
2019-02-14 13:23:24,577 : Best param found at split 2: l2reg = 1e-05                 with score 91.76
2019-02-14 13:23:35,516 : Best param found at split 3: l2reg = 1e-05                 with score 91.22
2019-02-14 13:23:47,342 : Best param found at split 4: l2reg = 1e-05                 with score 91.79
2019-02-14 13:23:58,480 : Best param found at split 5: l2reg = 1e-05                 with score 91.75
2019-02-14 13:23:59,100 : Dev acc : 91.6 Test acc : 91.47

2019-02-14 13:23:59,101 : ***** Transfer task : SST Binary classification *****


2019-02-14 13:23:59,233 : loading BERT model bert-base-uncased
2019-02-14 13:23:59,234 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:23:59,257 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:23:59,257 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnh5dxsci
2019-02-14 13:24:01,686 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:24:03,108 : Computing embedding for train
2019-02-14 13:24:41,505 : Computed train embeddings
2019-02-14 13:24:41,506 : Computing embedding for dev
2019-02-14 13:24:42,328 : Computed dev embeddings
2019-02-14 13:24:42,328 : Computing embedding for test
2019-02-14 13:24:44,109 : Computed test embeddings
2019-02-14 13:24:44,109 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:25:01,286 : [('reg:1e-05', 80.5), ('reg:0.0001', 78.9), ('reg:0.001', 74.54), ('reg:0.01', 65.25)]
2019-02-14 13:25:01,286 : Validation : best param found is reg = 1e-05 with score             80.5
2019-02-14 13:25:01,286 : Evaluating...
2019-02-14 13:25:04,656 : 
Dev acc : 80.5 Test acc : 80.29 for             SST Binary classification

2019-02-14 13:25:04,656 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 13:25:04,706 : loading BERT model bert-base-uncased
2019-02-14 13:25:04,706 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:25:04,728 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:25:04,728 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpifvilgma
2019-02-14 13:25:07,162 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:25:08,574 : Computing embedding for train
2019-02-14 13:25:16,799 : Computed train embeddings
2019-02-14 13:25:16,799 : Computing embedding for dev
2019-02-14 13:25:17,866 : Computed dev embeddings
2019-02-14 13:25:17,866 : Computing embedding for test
2019-02-14 13:25:19,987 : Computed test embeddings
2019-02-14 13:25:19,987 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:25:22,324 : [('reg:1e-05', 39.96), ('reg:0.0001', 38.42), ('reg:0.001', 33.61), ('reg:0.01', 26.25)]
2019-02-14 13:25:22,324 : Validation : best param found is reg = 1e-05 with score             39.96
2019-02-14 13:25:22,324 : Evaluating...
2019-02-14 13:25:22,986 : 
Dev acc : 39.96 Test acc : 42.22 for             SST Fine-Grained classification

2019-02-14 13:25:22,987 : ***** Transfer task : TREC *****


2019-02-14 13:25:23,001 : loading BERT model bert-base-uncased
2019-02-14 13:25:23,001 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:25:23,021 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:25:23,021 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpskewidfx
2019-02-14 13:25:25,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:25:29,917 : Computed train embeddings
2019-02-14 13:25:30,146 : Computed test embeddings
2019-02-14 13:25:30,146 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 13:25:37,350 : [('reg:1e-05', 60.2), ('reg:0.0001', 53.87), ('reg:0.001', 44.02), ('reg:0.01', 31.73)]
2019-02-14 13:25:37,350 : Cross-validation : best param found is reg = 1e-05             with score 60.2
2019-02-14 13:25:37,350 : Evaluating...
2019-02-14 13:25:38,025 : 
Dev acc : 60.2 Test acc : 77.2             for TREC

2019-02-14 13:25:38,025 : ***** Transfer task : MRPC *****


2019-02-14 13:25:38,048 : loading BERT model bert-base-uncased
2019-02-14 13:25:38,048 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:25:38,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:25:38,069 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprwcm7_7p
2019-02-14 13:25:40,504 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:25:41,924 : Computing embedding for train
2019-02-14 13:25:50,321 : Computed train embeddings
2019-02-14 13:25:50,322 : Computing embedding for test
2019-02-14 13:25:53,950 : Computed test embeddings
2019-02-14 13:25:53,966 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 13:25:58,368 : [('reg:1e-05', 68.74), ('reg:0.0001', 68.69), ('reg:0.001', 67.89), ('reg:0.01', 68.25)]
2019-02-14 13:25:58,368 : Cross-validation : best param found is reg = 1e-05             with score 68.74
2019-02-14 13:25:58,368 : Evaluating...
2019-02-14 13:25:58,636 : Dev acc : 68.74 Test acc 67.94; Test F1 75.08 for MRPC.

2019-02-14 13:25:58,636 : ***** Transfer task : SICK-Entailment*****


2019-02-14 13:25:58,660 : loading BERT model bert-base-uncased
2019-02-14 13:25:58,660 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:25:58,717 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:25:58,717 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpon6_hbi6
2019-02-14 13:26:01,146 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:26:02,556 : Computing embedding for train
2019-02-14 13:26:07,032 : Computed train embeddings
2019-02-14 13:26:07,032 : Computing embedding for dev
2019-02-14 13:26:07,618 : Computed dev embeddings
2019-02-14 13:26:07,618 : Computing embedding for test
2019-02-14 13:26:12,403 : Computed test embeddings
2019-02-14 13:26:12,431 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:26:13,712 : [('reg:1e-05', 68.6), ('reg:0.0001', 69.0), ('reg:0.001', 56.4), ('reg:0.01', 56.4)]
2019-02-14 13:26:13,713 : Validation : best param found is reg = 0.0001 with score             69.0
2019-02-14 13:26:13,713 : Evaluating...
2019-02-14 13:26:14,123 : 
Dev acc : 69.0 Test acc : 68.36 for                        SICK entailment

2019-02-14 13:26:14,123 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 13:26:14,150 : loading BERT model bert-base-uncased
2019-02-14 13:26:14,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:26:14,170 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:26:14,170 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6qhh4xsd
2019-02-14 13:26:16,603 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:26:18,038 : Computing embedding for train
2019-02-14 13:26:22,477 : Computed train embeddings
2019-02-14 13:26:22,477 : Computing embedding for dev
2019-02-14 13:26:23,060 : Computed dev embeddings
2019-02-14 13:26:23,061 : Computing embedding for test
2019-02-14 13:26:27,882 : Computed test embeddings
2019-02-14 13:27:30,670 : Dev : Pearson 0.6906131772241485
2019-02-14 13:27:30,670 : Test : Pearson 0.7045048191840649 Spearman 0.6729525904289914 MSE 0.5137214283401571                        for SICK Relatedness

2019-02-14 13:27:30,671 : 

***** Transfer task : STSBenchmark*****


2019-02-14 13:27:30,721 : loading BERT model bert-base-uncased
2019-02-14 13:27:30,722 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:27:30,747 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:27:30,747 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmaivn_lx
2019-02-14 13:27:33,184 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:27:34,639 : Computing embedding for train
2019-02-14 13:27:41,812 : Computed train embeddings
2019-02-14 13:27:41,812 : Computing embedding for dev
2019-02-14 13:27:43,933 : Computed dev embeddings
2019-02-14 13:27:43,934 : Computing embedding for test
2019-02-14 13:27:45,648 : Computed test embeddings
2019-02-14 13:28:22,546 : Dev : Pearson 0.641185482784188
2019-02-14 13:28:22,546 : Test : Pearson 0.5653916197916385 Spearman 0.5575592013832982 MSE 1.8883422406589319                        for SICK Relatedness

2019-02-14 13:28:22,546 : ***** Transfer task : SNLI Entailment*****


2019-02-14 13:28:27,424 : loading BERT model bert-base-uncased
2019-02-14 13:28:27,425 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:28:27,550 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:28:27,551 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp993h79p1
2019-02-14 13:28:29,993 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:28:31,578 : PROGRESS (encoding): 0.00%
2019-02-14 13:29:37,775 : PROGRESS (encoding): 14.56%
2019-02-14 13:30:51,606 : PROGRESS (encoding): 29.12%
2019-02-14 13:32:06,284 : PROGRESS (encoding): 43.69%
2019-02-14 13:33:28,001 : PROGRESS (encoding): 58.25%
2019-02-14 13:34:58,875 : PROGRESS (encoding): 72.81%
2019-02-14 13:36:28,996 : PROGRESS (encoding): 87.37%
2019-02-14 13:38:04,129 : PROGRESS (encoding): 0.00%
2019-02-14 13:38:15,869 : PROGRESS (encoding): 0.00%
2019-02-14 13:38:27,205 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 13:38:59,613 : [('reg:1e-09', 61.23)]
2019-02-14 13:38:59,613 : Validation : best param found is reg = 1e-09 with score             61.23
2019-02-14 13:38:59,613 : Evaluating...
2019-02-14 13:39:29,123 : Dev acc : 61.23 Test acc : 61.44 for SNLI

2019-02-14 13:39:29,123 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 13:39:37,907 : loading BERT model bert-base-uncased
2019-02-14 13:39:37,907 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 13:39:37,954 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 13:39:37,954 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8akgmb8y
2019-02-14 13:39:40,382 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 13:39:41,820 : Computing embedding for train
2019-02-14 13:46:09,969 : Computed train embeddings
2019-02-14 13:46:09,969 : Computing embedding for dev
2019-02-14 13:46:26,665 : Computed dev embeddings
2019-02-14 13:46:26,665 : Computing embedding for test
2019-02-14 13:46:43,951 : Computed test embeddings
2019-02-14 13:46:43,966 : prepare data
2019-02-14 13:46:44,029 : start epoch
2019-02-14 13:47:24,015 : samples : 64000
2019-02-14 13:47:33,757 : Image to text: 2.26, 9.3, 15.52, 63.0
2019-02-14 13:47:40,444 : Text to Image: 1.924, 7.932, 13.652, 76.0
2019-02-14 13:48:20,292 : samples : 128000
2019-02-14 13:48:30,069 : Image to text: 3.66, 13.48, 22.08, 42.0
2019-02-14 13:48:36,756 : Text to Image: 2.528, 9.896, 16.52, 60.0
2019-02-14 13:49:16,370 : samples : 192000
2019-02-14 13:49:26,138 : Image to text: 4.16, 15.5, 24.84, 37.0
2019-02-14 13:49:32,830 : Text to Image: 3.16, 12.632, 20.28, 46.0
2019-02-14 13:50:12,524 : samples : 256000
2019-02-14 13:50:22,295 : Image to text: 4.78, 16.54, 26.58, 32.0
2019-02-14 13:50:28,989 : Text to Image: 3.656, 13.48, 21.82, 41.0
2019-02-14 13:51:08,982 : samples : 320000
2019-02-14 13:51:18,735 : Image to text: 4.94, 15.94, 25.64, 34.0
2019-02-14 13:51:25,431 : Text to Image: 3.748, 13.52, 21.96, 40.0
2019-02-14 13:52:05,458 : samples : 384000
2019-02-14 13:52:15,210 : Image to text: 5.06, 17.74, 28.64, 28.0
2019-02-14 13:52:21,893 : Text to Image: 4.196, 15.24, 24.192, 36.0
2019-02-14 13:53:02,039 : samples : 448000
2019-02-14 13:53:11,800 : Image to text: 4.68, 16.92, 27.1, 31.0
2019-02-14 13:53:18,492 : Text to Image: 3.86, 13.76, 22.184, 40.0
2019-02-14 13:53:58,547 : samples : 512000
2019-02-14 13:54:08,320 : Image to text: 5.62, 18.92, 29.12, 28.0
2019-02-14 13:54:15,008 : Text to Image: 4.832, 16.66, 25.96, 33.0
2019-02-14 13:54:49,079 : Epoch 1 finished
2019-02-14 13:54:49,452 : Image to text: 17.3, 45.7, 61.9, 6.0
2019-02-14 13:54:49,729 : Text to Image: 13.62, 39.86, 55.96, 8.0
2019-02-14 13:54:50,102 : Image to text: 17.5, 44.8, 62.6, 7.0
2019-02-14 13:54:50,378 : Text to Image: 13.22, 38.66, 56.3, 9.0
2019-02-14 13:54:50,752 : Image to text: 18.2, 46.3, 63.6, 6.0
2019-02-14 13:54:51,029 : Text to Image: 12.88, 38.42, 55.32, 9.0
2019-02-14 13:54:51,402 : Image to text: 15.5, 47.0, 63.7, 6.0
2019-02-14 13:54:51,679 : Text to Image: 13.78, 40.22, 57.84, 8.0
2019-02-14 13:54:52,052 : Image to text: 16.8, 47.8, 63.8, 6.0
2019-02-14 13:54:52,329 : Text to Image: 13.82, 39.5, 56.34, 8.0
2019-02-14 13:54:52,329 : Dev mean Text to Image: 13.464000000000002, 39.332, 56.352000000000004, 8.4
2019-02-14 13:54:52,329 : Dev mean Image to text: 17.06, 46.31999999999999, 63.12, 6.2
2019-02-14 13:54:52,329 : start epoch
2019-02-14 13:55:32,236 : samples : 64000
2019-02-14 13:55:41,980 : Image to text: 6.88, 21.02, 32.76, 23.0
2019-02-14 13:55:48,672 : Text to Image: 4.916, 17.304, 27.14, 31.0
2019-02-14 13:56:28,590 : samples : 128000
2019-02-14 13:56:38,337 : Image to text: 5.4, 19.26, 29.82, 26.0
2019-02-14 13:56:45,017 : Text to Image: 4.216, 15.052, 24.076, 36.0
2019-02-14 13:57:24,966 : samples : 192000
2019-02-14 13:57:34,749 : Image to text: 5.6, 18.7, 30.16, 27.0
2019-02-14 13:57:41,448 : Text to Image: 4.888, 16.776, 26.26, 33.0
2019-02-14 13:58:21,377 : samples : 256000
2019-02-14 13:58:31,148 : Image to text: 5.64, 19.64, 31.3, 25.0
2019-02-14 13:58:37,843 : Text to Image: 5.284, 17.828, 27.604, 31.0
2019-02-14 13:59:17,838 : samples : 320000
2019-02-14 13:59:27,617 : Image to text: 6.3, 20.9, 32.06, 24.0
2019-02-14 13:59:34,311 : Text to Image: 5.448, 18.376, 28.104, 29.0
2019-02-14 14:00:14,191 : samples : 384000
2019-02-14 14:00:23,935 : Image to text: 6.96, 21.1, 32.44, 24.0
2019-02-14 14:00:30,597 : Text to Image: 5.304, 18.656, 28.768, 29.0
2019-02-14 14:01:10,268 : samples : 448000
2019-02-14 14:01:19,972 : Image to text: 6.48, 20.88, 31.46, 25.0
2019-02-14 14:01:26,682 : Text to Image: 5.008, 16.684, 26.392, 31.0
2019-02-14 14:02:06,466 : samples : 512000
2019-02-14 14:02:16,208 : Image to text: 6.42, 21.5, 31.88, 25.0
2019-02-14 14:02:22,879 : Text to Image: 5.228, 17.688, 28.132, 31.0
2019-02-14 14:02:56,745 : Epoch 2 finished
2019-02-14 14:02:57,120 : Image to text: 18.7, 49.2, 64.3, 6.0
2019-02-14 14:02:57,397 : Text to Image: 15.48, 43.32, 59.98, 7.0
2019-02-14 14:02:57,772 : Image to text: 18.7, 47.5, 64.5, 6.0
2019-02-14 14:02:58,048 : Text to Image: 14.24, 40.94, 58.44, 8.0
2019-02-14 14:02:58,423 : Image to text: 20.7, 50.4, 67.2, 5.0
2019-02-14 14:02:58,700 : Text to Image: 15.1, 41.74, 58.5, 8.0
2019-02-14 14:02:59,075 : Image to text: 18.8, 49.3, 66.4, 6.0
2019-02-14 14:02:59,352 : Text to Image: 14.64, 42.0, 59.22, 7.0
2019-02-14 14:02:59,726 : Image to text: 19.9, 50.1, 64.0, 5.0
2019-02-14 14:03:00,002 : Text to Image: 14.92, 43.34, 59.72, 7.0
2019-02-14 14:03:00,002 : Dev mean Text to Image: 14.876, 42.267999999999994, 59.172, 7.4
2019-02-14 14:03:00,002 : Dev mean Image to text: 19.36, 49.3, 65.28, 5.6
2019-02-14 14:03:00,003 : start epoch
2019-02-14 14:03:39,641 : samples : 64000
2019-02-14 14:03:49,420 : Image to text: 6.54, 22.64, 33.56, 22.0
2019-02-14 14:03:56,101 : Text to Image: 5.5, 18.516, 28.408, 29.0
2019-02-14 14:04:35,849 : samples : 128000
2019-02-14 14:04:45,623 : Image to text: 6.52, 22.56, 33.82, 22.0
2019-02-14 14:04:52,317 : Text to Image: 5.704, 19.124, 29.948, 27.0
2019-02-14 14:05:32,275 : samples : 192000
2019-02-14 14:05:42,034 : Image to text: 6.4, 22.94, 34.18, 22.0
2019-02-14 14:05:48,735 : Text to Image: 5.988, 19.716, 30.044, 27.0
2019-02-14 14:06:28,763 : samples : 256000
2019-02-14 14:06:38,532 : Image to text: 7.1, 22.02, 34.46, 22.0
2019-02-14 14:06:45,235 : Text to Image: 6.14, 20.192, 30.792, 26.0
2019-02-14 14:07:25,048 : samples : 320000
2019-02-14 14:07:34,817 : Image to text: 6.98, 23.34, 34.04, 22.0
2019-02-14 14:07:41,514 : Text to Image: 5.408, 18.0, 27.96, 30.0
2019-02-14 14:08:21,178 : samples : 384000
2019-02-14 14:08:30,913 : Image to text: 7.0, 22.66, 34.44, 21.0
2019-02-14 14:08:37,605 : Text to Image: 5.548, 19.004, 29.016, 28.0
2019-02-14 14:09:17,211 : samples : 448000
2019-02-14 14:09:26,971 : Image to text: 7.46, 24.32, 35.84, 20.0
2019-02-14 14:09:33,674 : Text to Image: 6.432, 20.136, 30.776, 26.0
2019-02-14 14:10:13,375 : samples : 512000
2019-02-14 14:10:23,158 : Image to text: 7.98, 23.0, 34.5, 22.0
2019-02-14 14:10:29,853 : Text to Image: 5.392, 18.264, 28.288, 29.0
2019-02-14 14:11:03,659 : Epoch 3 finished
2019-02-14 14:11:04,033 : Image to text: 20.9, 54.2, 67.9, 5.0
2019-02-14 14:11:04,311 : Text to Image: 15.8, 45.04, 62.66, 7.0
2019-02-14 14:11:04,685 : Image to text: 20.0, 50.9, 66.1, 5.0
2019-02-14 14:11:04,962 : Text to Image: 14.12, 42.46, 60.44, 7.0
2019-02-14 14:11:05,336 : Image to text: 21.4, 52.8, 67.6, 5.0
2019-02-14 14:11:05,613 : Text to Image: 14.16, 43.58, 61.02, 7.0
2019-02-14 14:11:05,987 : Image to text: 22.1, 53.3, 68.9, 5.0
2019-02-14 14:11:06,264 : Text to Image: 15.16, 44.22, 61.52, 7.0
2019-02-14 14:11:06,637 : Image to text: 20.0, 51.2, 67.3, 5.0
2019-02-14 14:11:06,914 : Text to Image: 15.12, 43.22, 61.36, 7.0
2019-02-14 14:11:06,915 : Dev mean Text to Image: 14.872, 43.704, 61.4, 7.0
2019-02-14 14:11:06,915 : Dev mean Image to text: 20.88, 52.48, 67.56, 5.0
2019-02-14 14:11:06,915 : start epoch
2019-02-14 14:11:46,673 : samples : 64000
2019-02-14 14:11:56,442 : Image to text: 7.04, 22.7, 34.16, 22.0
2019-02-14 14:12:03,136 : Text to Image: 5.416, 18.432, 28.332, 29.0
2019-02-14 14:12:42,936 : samples : 128000
2019-02-14 14:12:52,692 : Image to text: 8.6, 25.26, 37.0, 19.0
2019-02-14 14:12:59,386 : Text to Image: 6.864, 22.1, 32.704, 24.0
2019-02-14 14:13:39,273 : samples : 192000
2019-02-14 14:13:49,058 : Image to text: 7.08, 23.74, 35.42, 21.0
2019-02-14 14:13:55,797 : Text to Image: 6.104, 19.7, 30.192, 26.0
2019-02-14 14:14:35,494 : samples : 256000
2019-02-14 14:14:45,266 : Image to text: 7.92, 24.68, 36.84, 20.0
2019-02-14 14:14:51,965 : Text to Image: 6.928, 21.032, 31.872, 24.0
2019-02-14 14:15:31,539 : samples : 320000
2019-02-14 14:15:41,298 : Image to text: 8.12, 23.94, 36.54, 20.0
2019-02-14 14:15:47,997 : Text to Image: 6.788, 21.404, 32.164, 24.0
2019-02-14 14:16:27,649 : samples : 384000
2019-02-14 14:16:37,413 : Image to text: 7.78, 24.76, 36.58, 20.0
2019-02-14 14:16:44,115 : Text to Image: 6.588, 21.116, 32.144, 24.0
2019-02-14 14:17:23,810 : samples : 448000
2019-02-14 14:17:33,558 : Image to text: 7.66, 24.08, 35.5, 21.0
2019-02-14 14:17:40,254 : Text to Image: 6.328, 20.148, 30.956, 26.0
2019-02-14 14:18:19,962 : samples : 512000
2019-02-14 14:18:29,722 : Image to text: 8.08, 25.48, 37.38, 19.0
2019-02-14 14:18:36,420 : Text to Image: 6.224, 20.372, 31.2, 25.0
2019-02-14 14:19:10,312 : Epoch 4 finished
2019-02-14 14:19:10,686 : Image to text: 22.2, 53.0, 70.7, 5.0
2019-02-14 14:19:10,963 : Text to Image: 17.2, 48.16, 64.88, 6.0
2019-02-14 14:19:11,337 : Image to text: 20.9, 50.6, 66.5, 5.0
2019-02-14 14:19:11,614 : Text to Image: 17.34, 46.36, 64.24, 6.0
2019-02-14 14:19:11,988 : Image to text: 22.7, 53.4, 70.2, 5.0
2019-02-14 14:19:12,265 : Text to Image: 16.96, 46.94, 62.84, 6.0
2019-02-14 14:19:12,640 : Image to text: 23.3, 55.4, 72.0, 4.0
2019-02-14 14:19:12,917 : Text to Image: 17.12, 47.22, 64.78, 6.0
2019-02-14 14:19:13,291 : Image to text: 20.7, 54.3, 68.9, 5.0
2019-02-14 14:19:13,568 : Text to Image: 15.92, 46.24, 63.02, 6.0
2019-02-14 14:19:13,568 : Dev mean Text to Image: 16.908, 46.98400000000001, 63.952, 6.0
2019-02-14 14:19:13,568 : Dev mean Image to text: 21.96, 53.339999999999996, 69.66, 4.8
2019-02-14 14:19:13,569 : start epoch
2019-02-14 14:19:53,402 : samples : 64000
2019-02-14 14:20:03,193 : Image to text: 7.48, 23.86, 35.86, 20.0
2019-02-14 14:20:09,889 : Text to Image: 5.832, 19.888, 30.064, 27.0
2019-02-14 14:20:49,702 : samples : 128000
2019-02-14 14:20:59,484 : Image to text: 8.18, 24.92, 36.3, 21.0
2019-02-14 14:21:06,190 : Text to Image: 6.748, 21.312, 31.88, 24.0
2019-02-14 14:21:45,826 : samples : 192000
2019-02-14 14:21:55,573 : Image to text: 7.42, 22.94, 35.8, 21.0
2019-02-14 14:22:02,273 : Text to Image: 5.988, 19.836, 30.18, 26.0
2019-02-14 14:22:41,913 : samples : 256000
2019-02-14 14:22:51,718 : Image to text: 8.92, 25.36, 36.72, 19.0
2019-02-14 14:22:58,416 : Text to Image: 6.888, 21.808, 33.064, 23.0
2019-02-14 14:23:38,123 : samples : 320000
2019-02-14 14:23:47,907 : Image to text: 7.74, 25.7, 37.72, 19.0
2019-02-14 14:23:54,608 : Text to Image: 6.676, 21.356, 32.284, 24.0
2019-02-14 14:24:34,336 : samples : 384000
2019-02-14 14:24:44,102 : Image to text: 7.88, 24.8, 36.82, 19.0
2019-02-14 14:24:50,799 : Text to Image: 6.436, 21.076, 32.144, 24.0
2019-02-14 14:25:30,496 : samples : 448000
2019-02-14 14:25:40,263 : Image to text: 8.18, 23.94, 35.74, 20.0
2019-02-14 14:25:46,959 : Text to Image: 6.36, 20.828, 31.644, 25.0
2019-02-14 14:26:26,792 : samples : 512000
2019-02-14 14:26:36,574 : Image to text: 8.48, 25.76, 38.36, 18.0
2019-02-14 14:26:43,274 : Text to Image: 7.36, 22.124, 33.316, 23.0
2019-02-14 14:27:17,200 : Epoch 5 finished
2019-02-14 14:27:17,576 : Image to text: 22.5, 54.3, 70.4, 5.0
2019-02-14 14:27:17,853 : Text to Image: 17.48, 46.76, 64.46, 6.0
2019-02-14 14:27:18,227 : Image to text: 21.1, 51.8, 67.8, 5.0
2019-02-14 14:27:18,504 : Text to Image: 17.12, 46.32, 64.54, 6.0
2019-02-14 14:27:18,879 : Image to text: 22.7, 54.6, 68.2, 5.0
2019-02-14 14:27:19,157 : Text to Image: 17.6, 47.1, 63.86, 6.0
2019-02-14 14:27:19,532 : Image to text: 21.7, 56.4, 72.0, 4.0
2019-02-14 14:27:19,810 : Text to Image: 18.22, 48.04, 64.9, 6.0
2019-02-14 14:27:20,185 : Image to text: 21.6, 53.0, 68.7, 5.0
2019-02-14 14:27:20,461 : Text to Image: 17.98, 46.62, 63.9, 6.0
2019-02-14 14:27:20,461 : Dev mean Text to Image: 17.68, 46.968, 64.33200000000001, 6.0
2019-02-14 14:27:20,461 : Dev mean Image to text: 21.92, 54.02, 69.42, 4.8
2019-02-14 14:27:20,462 : start epoch
2019-02-14 14:28:00,242 : samples : 64000
2019-02-14 14:28:10,002 : Image to text: 9.02, 26.98, 38.8, 18.0
2019-02-14 14:28:16,699 : Text to Image: 7.484, 22.916, 34.052, 23.0
2019-02-14 14:28:56,376 : samples : 128000
2019-02-14 14:29:06,127 : Image to text: 7.8, 23.0, 35.36, 21.0
2019-02-14 14:29:12,826 : Text to Image: 6.124, 19.956, 30.2, 27.0
2019-02-14 14:29:52,739 : samples : 192000
2019-02-14 14:30:02,512 : Image to text: 8.04, 25.24, 36.74, 20.0
2019-02-14 14:30:09,212 : Text to Image: 7.016, 22.068, 33.192, 23.0
2019-02-14 14:30:49,093 : samples : 256000
2019-02-14 14:30:58,851 : Image to text: 8.4, 24.84, 36.76, 19.0
2019-02-14 14:31:05,557 : Text to Image: 6.376, 20.568, 31.104, 25.0
2019-02-14 14:31:45,513 : samples : 320000
2019-02-14 14:31:55,304 : Image to text: 7.76, 24.74, 37.12, 19.0
2019-02-14 14:32:01,997 : Text to Image: 7.168, 21.98, 33.048, 23.0
2019-02-14 14:32:42,365 : samples : 384000
2019-02-14 14:32:52,140 : Image to text: 7.68, 24.64, 37.2, 19.0
2019-02-14 14:32:58,799 : Text to Image: 6.156, 20.38, 31.2, 25.0
2019-02-14 14:33:39,045 : samples : 448000
2019-02-14 14:33:48,765 : Image to text: 8.78, 25.7, 37.98, 18.0
2019-02-14 14:33:55,419 : Text to Image: 7.136, 22.332, 33.836, 22.0
2019-02-14 14:34:35,673 : samples : 512000
2019-02-14 14:34:45,376 : Image to text: 7.96, 26.04, 37.78, 18.0
2019-02-14 14:34:52,037 : Text to Image: 6.676, 21.46, 32.684, 23.0
2019-02-14 14:35:26,338 : Epoch 6 finished
2019-02-14 14:35:26,711 : Image to text: 23.1, 54.7, 69.0, 5.0
2019-02-14 14:35:26,987 : Text to Image: 19.3, 50.26, 67.96, 5.0
2019-02-14 14:35:27,359 : Image to text: 22.0, 50.8, 67.9, 5.0
2019-02-14 14:35:27,635 : Text to Image: 17.84, 47.54, 65.94, 6.0
2019-02-14 14:35:28,009 : Image to text: 24.4, 54.5, 69.5, 5.0
2019-02-14 14:35:28,284 : Text to Image: 18.24, 49.52, 66.72, 6.0
2019-02-14 14:35:28,657 : Image to text: 23.3, 57.1, 71.5, 4.0
2019-02-14 14:35:28,933 : Text to Image: 19.94, 50.94, 67.88, 5.0
2019-02-14 14:35:29,306 : Image to text: 23.1, 53.2, 69.4, 5.0
2019-02-14 14:35:29,582 : Text to Image: 17.86, 49.48, 67.14, 6.0
2019-02-14 14:35:29,582 : Dev mean Text to Image: 18.636, 49.548, 67.128, 5.6000000000000005
2019-02-14 14:35:29,582 : Dev mean Image to text: 23.18, 54.06, 69.46, 4.8
2019-02-14 14:35:29,582 : start epoch
2019-02-14 14:36:10,031 : samples : 64000
2019-02-14 14:36:19,827 : Image to text: 8.72, 27.12, 39.54, 17.0
2019-02-14 14:36:26,540 : Text to Image: 7.408, 23.128, 34.368, 22.0
2019-02-14 14:37:06,858 : samples : 128000
2019-02-14 14:37:16,654 : Image to text: 8.32, 25.64, 37.82, 19.0
2019-02-14 14:37:23,369 : Text to Image: 6.712, 21.02, 32.148, 24.0
2019-02-14 14:38:03,679 : samples : 192000
2019-02-14 14:38:13,482 : Image to text: 8.68, 26.52, 39.86, 17.0
2019-02-14 14:38:20,201 : Text to Image: 7.592, 23.616, 34.94, 21.0
2019-02-14 14:39:00,521 : samples : 256000
2019-02-14 14:39:10,317 : Image to text: 8.72, 25.6, 37.74, 19.0
2019-02-14 14:39:17,032 : Text to Image: 6.684, 21.088, 32.196, 24.0
2019-02-14 14:39:57,345 : samples : 320000
2019-02-14 14:40:07,131 : Image to text: 8.7, 25.06, 36.9, 19.0
2019-02-14 14:40:13,848 : Text to Image: 7.192, 22.368, 33.228, 23.0
2019-02-14 14:40:54,230 : samples : 384000
2019-02-14 14:41:04,013 : Image to text: 8.5, 26.36, 37.84, 18.0
2019-02-14 14:41:10,728 : Text to Image: 7.3, 22.744, 34.336, 22.0
2019-02-14 14:41:51,034 : samples : 448000
2019-02-14 14:42:00,820 : Image to text: 9.28, 27.1, 39.22, 17.0
2019-02-14 14:42:07,533 : Text to Image: 7.856, 24.048, 35.608, 21.0
2019-02-14 14:42:47,954 : samples : 512000
2019-02-14 14:42:57,745 : Image to text: 8.84, 27.14, 39.12, 18.0
2019-02-14 14:43:04,464 : Text to Image: 7.14, 22.288, 33.016, 23.0
2019-02-14 14:43:38,917 : Epoch 7 finished
2019-02-14 14:43:39,292 : Image to text: 23.9, 55.7, 71.1, 4.0
2019-02-14 14:43:39,570 : Text to Image: 19.28, 50.84, 68.68, 5.0
2019-02-14 14:43:39,945 : Image to text: 24.1, 55.3, 69.1, 4.0
2019-02-14 14:43:40,223 : Text to Image: 18.68, 51.26, 67.54, 5.0
2019-02-14 14:43:40,598 : Image to text: 25.3, 54.8, 70.8, 5.0
2019-02-14 14:43:40,876 : Text to Image: 18.48, 51.06, 69.16, 5.0
2019-02-14 14:43:41,251 : Image to text: 25.8, 58.4, 73.2, 4.0
2019-02-14 14:43:41,529 : Text to Image: 20.04, 51.52, 69.22, 5.0
2019-02-14 14:43:41,904 : Image to text: 23.6, 55.9, 70.4, 4.0
2019-02-14 14:43:42,182 : Text to Image: 18.92, 50.32, 68.32, 5.0
2019-02-14 14:43:42,182 : Dev mean Text to Image: 19.08, 51.0, 68.584, 5.0
2019-02-14 14:43:42,182 : Dev mean Image to text: 24.54, 56.019999999999996, 70.92, 4.2
2019-02-14 14:43:42,182 : start epoch
2019-02-14 14:44:22,382 : samples : 64000
2019-02-14 14:44:32,153 : Image to text: 8.22, 25.32, 37.2, 18.0
2019-02-14 14:44:38,854 : Text to Image: 6.968, 22.068, 33.608, 22.0
2019-02-14 14:45:18,832 : samples : 128000
2019-02-14 14:45:28,598 : Image to text: 8.72, 26.96, 39.4, 17.0
2019-02-14 14:45:35,307 : Text to Image: 7.588, 23.12, 34.464, 22.0
2019-02-14 14:46:15,307 : samples : 192000
2019-02-14 14:46:25,082 : Image to text: 9.36, 26.94, 39.24, 17.0
2019-02-14 14:46:31,786 : Text to Image: 7.628, 22.936, 34.076, 22.0
2019-02-14 14:47:11,875 : samples : 256000
2019-02-14 14:47:21,668 : Image to text: 8.74, 25.44, 38.46, 17.0
2019-02-14 14:47:28,373 : Text to Image: 7.064, 22.34, 33.62, 23.0
2019-02-14 14:48:08,224 : samples : 320000
2019-02-14 14:48:18,044 : Image to text: 9.46, 27.94, 40.32, 17.0
2019-02-14 14:48:24,768 : Text to Image: 7.108, 22.488, 33.94, 22.0
2019-02-14 14:49:05,408 : samples : 384000
2019-02-14 14:49:15,212 : Image to text: 9.02, 26.84, 39.82, 17.0
2019-02-14 14:49:21,930 : Text to Image: 7.424, 23.312, 34.732, 21.0
2019-02-14 14:50:02,353 : samples : 448000
2019-02-14 14:50:12,105 : Image to text: 9.76, 27.4, 39.88, 17.0
2019-02-14 14:50:18,813 : Text to Image: 7.36, 23.476, 35.02, 21.0
2019-02-14 14:50:59,152 : samples : 512000
2019-02-14 14:51:08,963 : Image to text: 8.72, 26.2, 39.06, 17.0
2019-02-14 14:51:15,681 : Text to Image: 7.264, 22.476, 33.852, 22.0
2019-02-14 14:51:49,999 : Epoch 8 finished
2019-02-14 14:51:50,374 : Image to text: 22.9, 56.9, 73.0, 4.0
2019-02-14 14:51:50,651 : Text to Image: 19.0, 51.18, 68.96, 5.0
2019-02-14 14:51:51,026 : Image to text: 23.3, 54.8, 70.4, 5.0
2019-02-14 14:51:51,303 : Text to Image: 18.86, 49.82, 67.96, 6.0
2019-02-14 14:51:51,679 : Image to text: 24.9, 56.4, 72.0, 4.0
2019-02-14 14:51:51,957 : Text to Image: 18.82, 50.3, 67.94, 5.0
2019-02-14 14:51:52,332 : Image to text: 25.6, 60.2, 73.9, 4.0
2019-02-14 14:51:52,610 : Text to Image: 20.9, 50.34, 68.48, 5.0
2019-02-14 14:51:52,985 : Image to text: 24.3, 55.4, 70.8, 4.0
2019-02-14 14:51:53,262 : Text to Image: 18.94, 50.82, 66.72, 5.0
2019-02-14 14:51:53,262 : Dev mean Text to Image: 19.304, 50.492000000000004, 68.01199999999999, 5.2
2019-02-14 14:51:53,263 : Dev mean Image to text: 24.2, 56.739999999999995, 72.02, 4.2
2019-02-14 14:51:53,263 : start epoch
2019-02-14 14:52:33,717 : samples : 64000
2019-02-14 14:52:43,497 : Image to text: 9.18, 26.4, 38.94, 17.0
2019-02-14 14:52:50,216 : Text to Image: 7.624, 23.488, 34.64, 21.0
2019-02-14 14:53:30,480 : samples : 128000
2019-02-14 14:53:40,188 : Image to text: 8.82, 26.28, 38.92, 17.0
2019-02-14 14:53:46,851 : Text to Image: 7.612, 23.008, 34.336, 22.0
2019-02-14 14:54:26,624 : samples : 192000
2019-02-14 14:54:36,320 : Image to text: 9.18, 26.02, 38.96, 18.0
2019-02-14 14:54:42,970 : Text to Image: 7.212, 22.588, 33.968, 22.0
2019-02-14 14:55:22,698 : samples : 256000
2019-02-14 14:55:32,417 : Image to text: 9.52, 26.74, 38.66, 17.0
2019-02-14 14:55:39,059 : Text to Image: 7.62, 23.06, 34.392, 21.0
2019-02-14 14:56:18,808 : samples : 320000
2019-02-14 14:56:28,565 : Image to text: 9.78, 27.24, 39.66, 17.0
2019-02-14 14:56:35,277 : Text to Image: 7.684, 22.848, 34.532, 21.0
2019-02-14 14:57:14,939 : samples : 384000
2019-02-14 14:57:24,729 : Image to text: 8.88, 27.22, 39.48, 17.0
2019-02-14 14:57:31,438 : Text to Image: 7.404, 22.744, 34.276, 22.0
2019-02-14 14:58:11,036 : samples : 448000
2019-02-14 14:58:20,818 : Image to text: 9.2, 27.54, 39.0, 17.0
2019-02-14 14:58:27,522 : Text to Image: 7.58, 23.424, 34.9, 21.0
2019-02-14 14:59:07,241 : samples : 512000
2019-02-14 14:59:17,024 : Image to text: 9.72, 27.94, 39.96, 16.0
2019-02-14 14:59:23,724 : Text to Image: 7.54, 22.992, 34.36, 22.0
2019-02-14 14:59:57,547 : Epoch 9 finished
2019-02-14 14:59:57,922 : Image to text: 25.2, 57.2, 73.0, 4.0
2019-02-14 14:59:58,198 : Text to Image: 21.14, 53.26, 70.8, 5.0
2019-02-14 14:59:58,573 : Image to text: 25.1, 56.2, 72.5, 4.0
2019-02-14 14:59:58,849 : Text to Image: 19.94, 52.28, 70.24, 5.0
2019-02-14 14:59:59,224 : Image to text: 25.8, 60.3, 74.9, 4.0
2019-02-14 14:59:59,501 : Text to Image: 20.72, 53.62, 70.4, 5.0
2019-02-14 14:59:59,875 : Image to text: 25.4, 59.0, 74.9, 4.0
2019-02-14 15:00:00,153 : Text to Image: 22.08, 53.54, 70.7, 5.0
2019-02-14 15:00:00,530 : Image to text: 24.9, 59.2, 72.4, 4.0
2019-02-14 15:00:00,806 : Text to Image: 21.24, 53.34, 70.2, 5.0
2019-02-14 15:00:00,806 : Dev mean Text to Image: 21.024, 53.208, 70.468, 5.0
2019-02-14 15:00:00,806 : Dev mean Image to text: 25.28, 58.379999999999995, 73.54, 4.0
2019-02-14 15:00:00,807 : start epoch
2019-02-14 15:00:40,521 : samples : 64000
2019-02-14 15:00:50,291 : Image to text: 8.84, 26.08, 38.22, 18.0
2019-02-14 15:00:56,984 : Text to Image: 7.62, 23.508, 34.888, 21.0
2019-02-14 15:01:36,627 : samples : 128000
2019-02-14 15:01:46,430 : Image to text: 9.36, 28.3, 39.84, 17.0
2019-02-14 15:01:53,127 : Text to Image: 7.704, 24.136, 35.524, 21.0
2019-02-14 15:02:32,874 : samples : 192000
2019-02-14 15:02:42,655 : Image to text: 9.38, 28.44, 40.6, 16.0
2019-02-14 15:02:49,361 : Text to Image: 7.404, 23.212, 34.712, 22.0
2019-02-14 15:03:29,082 : samples : 256000
2019-02-14 15:03:38,856 : Image to text: 10.4, 27.86, 40.72, 16.0
2019-02-14 15:03:45,560 : Text to Image: 7.532, 23.348, 34.644, 21.0
2019-02-14 15:04:25,226 : samples : 320000
2019-02-14 15:04:35,013 : Image to text: 9.32, 26.88, 39.64, 16.0
2019-02-14 15:04:41,713 : Text to Image: 7.588, 23.34, 34.76, 21.0
2019-02-14 15:05:21,411 : samples : 384000
2019-02-14 15:05:31,187 : Image to text: 9.28, 27.6, 39.88, 16.0
2019-02-14 15:05:37,894 : Text to Image: 7.416, 23.34, 35.14, 21.0
2019-02-14 15:06:17,586 : samples : 448000
2019-02-14 15:06:27,357 : Image to text: 9.34, 27.24, 40.2, 16.0
2019-02-14 15:06:34,065 : Text to Image: 7.436, 22.892, 34.224, 22.0
2019-02-14 15:07:13,649 : samples : 512000
2019-02-14 15:07:23,416 : Image to text: 8.7, 26.8, 39.92, 17.0
2019-02-14 15:07:30,116 : Text to Image: 7.564, 23.652, 35.072, 21.0
2019-02-14 15:08:03,880 : Epoch 10 finished
2019-02-14 15:08:04,254 : Image to text: 24.8, 57.4, 73.0, 4.0
2019-02-14 15:08:04,531 : Text to Image: 20.82, 53.28, 69.88, 5.0
2019-02-14 15:08:04,906 : Image to text: 25.1, 55.0, 70.0, 4.0
2019-02-14 15:08:05,182 : Text to Image: 18.94, 50.26, 68.82, 5.0
2019-02-14 15:08:05,557 : Image to text: 25.1, 57.5, 73.5, 4.0
2019-02-14 15:08:05,834 : Text to Image: 21.22, 52.54, 68.88, 5.0
2019-02-14 15:08:06,208 : Image to text: 25.3, 58.7, 73.4, 4.0
2019-02-14 15:08:06,485 : Text to Image: 21.02, 52.14, 69.34, 5.0
2019-02-14 15:08:06,859 : Image to text: 24.1, 57.2, 72.9, 4.0
2019-02-14 15:08:07,136 : Text to Image: 20.68, 51.74, 69.02, 5.0
2019-02-14 15:08:07,136 : Dev mean Text to Image: 20.535999999999998, 51.992, 69.188, 5.0
2019-02-14 15:08:07,136 : Dev mean Image to text: 24.880000000000003, 57.16000000000001, 72.56, 4.0
2019-02-14 15:08:07,136 : start epoch
2019-02-14 15:08:46,866 : samples : 64000
2019-02-14 15:08:56,632 : Image to text: 9.48, 27.74, 39.68, 16.0
2019-02-14 15:09:03,335 : Text to Image: 8.236, 24.412, 35.924, 20.0
2019-02-14 15:09:43,016 : samples : 128000
2019-02-14 15:09:52,781 : Image to text: 9.42, 28.0, 40.48, 16.0
2019-02-14 15:09:59,487 : Text to Image: 8.148, 24.336, 35.828, 20.0
2019-02-14 15:10:39,105 : samples : 192000
2019-02-14 15:10:48,861 : Image to text: 9.54, 26.48, 39.38, 17.0
2019-02-14 15:10:55,562 : Text to Image: 7.528, 22.844, 34.316, 21.0
2019-02-14 15:11:35,159 : samples : 256000
2019-02-14 15:11:44,910 : Image to text: 10.14, 28.14, 40.52, 16.0
2019-02-14 15:11:51,613 : Text to Image: 8.372, 24.712, 36.352, 20.0
2019-02-14 15:12:31,199 : samples : 320000
2019-02-14 15:12:40,976 : Image to text: 9.72, 27.88, 40.9, 16.0
2019-02-14 15:12:47,677 : Text to Image: 7.64, 23.94, 35.584, 20.0
2019-02-14 15:13:27,196 : samples : 384000
2019-02-14 15:13:36,953 : Image to text: 9.64, 28.42, 40.78, 16.0
2019-02-14 15:13:43,658 : Text to Image: 7.724, 24.34, 35.852, 20.0
2019-02-14 15:14:23,144 : samples : 448000
2019-02-14 15:14:32,918 : Image to text: 9.92, 28.0, 41.22, 16.0
2019-02-14 15:14:39,595 : Text to Image: 7.7, 24.168, 36.172, 20.0
2019-02-14 15:15:19,247 : samples : 512000
2019-02-14 15:15:29,018 : Image to text: 9.98, 28.78, 40.28, 16.0
2019-02-14 15:15:35,727 : Text to Image: 7.848, 24.216, 35.88, 20.0
2019-02-14 15:16:09,535 : Epoch 11 finished
2019-02-14 15:16:09,909 : Image to text: 23.5, 58.1, 71.6, 4.0
2019-02-14 15:16:10,185 : Text to Image: 20.06, 52.3, 70.26, 5.0
2019-02-14 15:16:10,560 : Image to text: 24.5, 55.6, 69.7, 4.0
2019-02-14 15:16:10,837 : Text to Image: 19.0, 50.84, 68.92, 5.0
2019-02-14 15:16:11,212 : Image to text: 26.1, 58.7, 72.4, 4.0
2019-02-14 15:16:11,489 : Text to Image: 19.8, 51.18, 68.32, 5.0
2019-02-14 15:16:11,864 : Image to text: 24.5, 59.9, 74.4, 4.0
2019-02-14 15:16:12,142 : Text to Image: 20.14, 52.56, 68.92, 5.0
2019-02-14 15:16:12,516 : Image to text: 25.2, 58.6, 71.7, 4.0
2019-02-14 15:16:12,794 : Text to Image: 19.7, 51.04, 68.84, 5.0
2019-02-14 15:16:12,794 : Dev mean Text to Image: 19.74, 51.584, 69.05199999999999, 5.0
2019-02-14 15:16:12,794 : Dev mean Image to text: 24.76, 58.18000000000001, 71.96, 4.0
2019-02-14 15:16:12,794 : start epoch
2019-02-14 15:16:52,559 : samples : 64000
2019-02-14 15:17:02,338 : Image to text: 8.8, 27.38, 39.92, 16.0
2019-02-14 15:17:09,041 : Text to Image: 7.972, 24.316, 35.84, 20.0
2019-02-14 15:17:48,804 : samples : 128000
2019-02-14 15:17:58,578 : Image to text: 10.1, 27.94, 40.08, 16.0
2019-02-14 15:18:05,293 : Text to Image: 6.984, 23.18, 34.512, 21.0
2019-02-14 15:18:44,995 : samples : 192000
2019-02-14 15:18:54,754 : Image to text: 9.88, 28.08, 41.5, 15.0
2019-02-14 15:19:01,465 : Text to Image: 8.004, 24.348, 36.052, 20.0
2019-02-14 15:19:41,160 : samples : 256000
2019-02-14 15:19:50,932 : Image to text: 10.0, 28.0, 40.54, 16.0
2019-02-14 15:19:57,627 : Text to Image: 7.992, 23.708, 35.672, 21.0
2019-02-14 15:20:37,275 : samples : 320000
2019-02-14 15:20:47,064 : Image to text: 9.84, 28.56, 41.02, 16.0
2019-02-14 15:20:53,770 : Text to Image: 7.584, 23.504, 34.688, 21.0
2019-02-14 15:21:33,555 : samples : 384000
2019-02-14 15:21:43,341 : Image to text: 8.82, 26.78, 40.3, 16.0
2019-02-14 15:21:50,051 : Text to Image: 8.024, 24.468, 36.124, 20.0
2019-02-14 15:22:30,077 : samples : 448000
2019-02-14 15:22:39,849 : Image to text: 10.4, 29.34, 41.7, 15.0
2019-02-14 15:22:46,557 : Text to Image: 8.336, 24.504, 36.68, 19.0
2019-02-14 15:23:26,733 : samples : 512000
2019-02-14 15:23:36,516 : Image to text: 9.66, 28.24, 40.44, 16.0
2019-02-14 15:23:43,230 : Text to Image: 7.852, 24.384, 36.088, 20.0
2019-02-14 15:24:17,366 : Epoch 12 finished
2019-02-14 15:24:17,742 : Image to text: 25.4, 55.5, 72.0, 4.0
2019-02-14 15:24:18,019 : Text to Image: 19.32, 49.5, 67.62, 6.0
2019-02-14 15:24:18,395 : Image to text: 24.6, 56.3, 71.8, 4.0
2019-02-14 15:24:18,672 : Text to Image: 17.88, 49.66, 67.58, 6.0
2019-02-14 15:24:19,047 : Image to text: 26.9, 58.2, 73.3, 4.0
2019-02-14 15:24:19,325 : Text to Image: 18.98, 50.98, 68.64, 5.0
2019-02-14 15:24:19,701 : Image to text: 23.9, 56.6, 73.1, 4.0
2019-02-14 15:24:19,979 : Text to Image: 18.7, 50.4, 68.28, 5.0
2019-02-14 15:24:20,354 : Image to text: 25.1, 55.1, 72.1, 4.0
2019-02-14 15:24:20,631 : Text to Image: 18.12, 49.94, 66.64, 6.0
2019-02-14 15:24:20,631 : Dev mean Text to Image: 18.6, 50.096, 67.752, 5.6000000000000005
2019-02-14 15:24:20,631 : Dev mean Image to text: 25.179999999999996, 56.34, 72.46, 4.0
2019-02-14 15:24:20,631 : start epoch
2019-02-14 15:25:00,721 : samples : 64000
2019-02-14 15:25:10,512 : Image to text: 9.82, 28.5, 41.26, 16.0
2019-02-14 15:25:17,237 : Text to Image: 8.036, 24.508, 36.528, 20.0
2019-02-14 15:25:57,100 : samples : 128000
2019-02-14 15:26:06,904 : Image to text: 10.2, 28.44, 41.04, 16.0
2019-02-14 15:26:13,617 : Text to Image: 8.364, 25.064, 36.652, 20.0
2019-02-14 15:26:53,387 : samples : 192000
2019-02-14 15:27:03,186 : Image to text: 9.28, 28.34, 40.22, 17.0
2019-02-14 15:27:09,911 : Text to Image: 7.556, 23.232, 34.748, 21.0
2019-02-14 15:27:50,610 : samples : 256000
2019-02-14 15:28:01,352 : Image to text: 10.12, 28.62, 41.76, 15.0
2019-02-14 15:28:09,468 : Text to Image: 8.228, 24.892, 36.368, 20.0
2019-02-14 15:28:51,671 : samples : 320000
2019-02-14 15:29:01,725 : Image to text: 10.06, 28.8, 41.4, 16.0
2019-02-14 15:29:08,979 : Text to Image: 8.06, 24.892, 36.832, 20.0
2019-02-14 15:29:50,041 : samples : 384000
2019-02-14 15:30:00,068 : Image to text: 9.62, 28.42, 40.86, 16.0
2019-02-14 15:30:07,370 : Text to Image: 7.708, 24.396, 35.884, 20.0
2019-02-14 15:30:48,103 : samples : 448000
2019-02-14 15:30:58,170 : Image to text: 8.9, 27.02, 39.58, 17.0
2019-02-14 15:31:05,436 : Text to Image: 7.888, 24.512, 36.132, 20.0
2019-02-14 15:31:48,058 : samples : 512000
2019-02-14 15:31:58,248 : Image to text: 10.1, 28.68, 41.7, 15.0
2019-02-14 15:32:05,511 : Text to Image: 8.22, 25.08, 36.812, 19.0
2019-02-14 15:32:40,816 : Epoch 13 finished
2019-02-14 15:32:41,831 : Image to text: 24.4, 59.6, 73.5, 4.0
2019-02-14 15:32:42,589 : Text to Image: 20.6, 54.02, 70.84, 5.0
2019-02-14 15:32:43,539 : Image to text: 23.8, 55.9, 71.8, 4.0
2019-02-14 15:32:44,360 : Text to Image: 19.72, 52.36, 69.76, 5.0
2019-02-14 15:32:45,301 : Image to text: 27.4, 57.0, 74.3, 4.0
2019-02-14 15:32:46,112 : Text to Image: 21.46, 54.48, 71.1, 5.0
2019-02-14 15:32:47,096 : Image to text: 26.8, 57.2, 73.0, 4.0
2019-02-14 15:32:47,914 : Text to Image: 21.38, 52.88, 70.24, 5.0
2019-02-14 15:32:48,918 : Image to text: 25.3, 57.7, 72.8, 4.0
2019-02-14 15:32:49,710 : Text to Image: 20.02, 52.74, 69.72, 5.0
2019-02-14 15:32:49,710 : Dev mean Text to Image: 20.635999999999996, 53.296, 70.33200000000001, 5.0
2019-02-14 15:32:49,710 : Dev mean Image to text: 25.54, 57.48, 73.08, 4.0
2019-02-14 15:32:58,770 : 
Test scores | Image to text:             25.020000000000003, 57.739999999999995, 73.84, 4.2
2019-02-14 15:32:58,770 : Test scores | Text to image:             21.308, 53.20399999999999, 70.168, 5.0

2019-02-14 15:32:58,892 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 15:32:59,105 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 15:32:59,744 : loading BERT model bert-base-uncased
2019-02-14 15:32:59,744 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:32:59,774 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:32:59,775 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxawwz5yh
2019-02-14 15:33:02,213 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:33:03,762 : Computing embeddings for train/dev/test
2019-02-14 15:35:06,590 : Computed embeddings
2019-02-14 15:35:06,590 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:35:42,015 : [('reg:1e-05', 65.0), ('reg:0.0001', 56.99), ('reg:0.001', 46.06), ('reg:0.01', 29.56)]
2019-02-14 15:35:42,015 : Validation : best param found is reg = 1e-05 with score             65.0
2019-02-14 15:35:42,015 : Evaluating...
2019-02-14 15:35:55,842 : 
Dev acc : 65.0 Test acc : 65.8 for LENGTH classification

2019-02-14 15:35:55,843 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 15:35:56,223 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 15:35:56,270 : loading BERT model bert-base-uncased
2019-02-14 15:35:56,270 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:35:56,298 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:35:56,299 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpig67sn7v
2019-02-14 15:35:58,757 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:36:00,206 : Computing embeddings for train/dev/test
2019-02-14 15:37:49,952 : Computed embeddings
2019-02-14 15:37:49,952 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:38:25,571 : [('reg:1e-05', 0.68), ('reg:0.0001', 0.18), ('reg:0.001', 0.13), ('reg:0.01', 0.11)]
2019-02-14 15:38:25,572 : Validation : best param found is reg = 1e-05 with score             0.68
2019-02-14 15:38:25,572 : Evaluating...
2019-02-14 15:38:39,821 : 
Dev acc : 0.7 Test acc : 0.8 for WORDCONTENT classification

2019-02-14 15:38:39,823 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 15:38:40,138 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 15:38:40,208 : loading BERT model bert-base-uncased
2019-02-14 15:38:40,208 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:38:40,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:38:40,234 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjeo9kwoq
2019-02-14 15:38:42,671 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:38:44,151 : Computing embeddings for train/dev/test
2019-02-14 15:40:37,339 : Computed embeddings
2019-02-14 15:40:37,340 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:41:27,205 : [('reg:1e-05', 27.57), ('reg:0.0001', 22.77), ('reg:0.001', 19.86), ('reg:0.01', 18.07)]
2019-02-14 15:41:27,205 : Validation : best param found is reg = 1e-05 with score             27.57
2019-02-14 15:41:27,205 : Evaluating...
2019-02-14 15:41:39,882 : 
Dev acc : 27.6 Test acc : 27.7 for DEPTH classification

2019-02-14 15:41:39,883 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 15:41:40,264 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 15:41:40,326 : loading BERT model bert-base-uncased
2019-02-14 15:41:40,326 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:41:40,436 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:41:40,436 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn74w7toi
2019-02-14 15:41:42,871 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:41:44,354 : Computing embeddings for train/dev/test
2019-02-14 15:43:37,965 : Computed embeddings
2019-02-14 15:43:37,965 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:44:48,747 : [('reg:1e-05', 64.98), ('reg:0.0001', 46.99), ('reg:0.001', 22.7), ('reg:0.01', 9.19)]
2019-02-14 15:44:48,747 : Validation : best param found is reg = 1e-05 with score             64.98
2019-02-14 15:44:48,747 : Evaluating...
2019-02-14 15:45:06,753 : 
Dev acc : 65.0 Test acc : 65.6 for TOPCONSTITUENTS classification

2019-02-14 15:45:06,754 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 15:45:07,294 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 15:45:07,360 : loading BERT model bert-base-uncased
2019-02-14 15:45:07,360 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:45:07,392 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:45:07,393 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpe9hfg0gm
2019-02-14 15:45:09,836 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:45:11,343 : Computing embeddings for train/dev/test
2019-02-14 15:47:10,050 : Computed embeddings
2019-02-14 15:47:10,051 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:48:15,982 : [('reg:1e-05', 84.39), ('reg:0.0001', 83.32), ('reg:0.001', 80.33), ('reg:0.01', 69.86)]
2019-02-14 15:48:15,982 : Validation : best param found is reg = 1e-05 with score             84.39
2019-02-14 15:48:15,982 : Evaluating...
2019-02-14 15:48:33,247 : 
Dev acc : 84.4 Test acc : 83.7 for BIGRAMSHIFT classification

2019-02-14 15:48:33,248 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 15:48:33,627 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 15:48:33,693 : loading BERT model bert-base-uncased
2019-02-14 15:48:33,693 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:48:33,809 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:48:33,809 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp383hyy4s
2019-02-14 15:48:36,329 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:48:37,832 : Computing embeddings for train/dev/test
2019-02-14 15:50:44,103 : Computed embeddings
2019-02-14 15:50:44,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:51:43,665 : [('reg:1e-05', 89.36), ('reg:0.0001', 88.02), ('reg:0.001', 83.33), ('reg:0.01', 74.91)]
2019-02-14 15:51:43,665 : Validation : best param found is reg = 1e-05 with score             89.36
2019-02-14 15:51:43,665 : Evaluating...
2019-02-14 15:52:02,408 : 
Dev acc : 89.4 Test acc : 88.1 for TENSE classification

2019-02-14 15:52:02,409 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 15:52:02,988 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 15:52:03,051 : loading BERT model bert-base-uncased
2019-02-14 15:52:03,052 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:52:03,080 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:52:03,080 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn9c1uw5r
2019-02-14 15:52:05,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:52:07,054 : Computing embeddings for train/dev/test
2019-02-14 15:54:25,154 : Computed embeddings
2019-02-14 15:54:25,154 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:55:16,045 : [('reg:1e-05', 82.48), ('reg:0.0001', 80.37), ('reg:0.001', 75.27), ('reg:0.01', 66.34)]
2019-02-14 15:55:16,045 : Validation : best param found is reg = 1e-05 with score             82.48
2019-02-14 15:55:16,046 : Evaluating...
2019-02-14 15:55:25,857 : 
Dev acc : 82.5 Test acc : 82.1 for SUBJNUMBER classification

2019-02-14 15:55:25,858 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 15:55:26,461 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 15:55:26,529 : loading BERT model bert-base-uncased
2019-02-14 15:55:26,530 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:55:26,561 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:55:26,561 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr7ap57o_
2019-02-14 15:55:29,011 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:55:30,439 : Computing embeddings for train/dev/test
2019-02-14 15:57:10,697 : Computed embeddings
2019-02-14 15:57:10,697 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 15:58:24,773 : [('reg:1e-05', 77.63), ('reg:0.0001', 74.14), ('reg:0.001', 68.68), ('reg:0.01', 61.53)]
2019-02-14 15:58:24,773 : Validation : best param found is reg = 1e-05 with score             77.63
2019-02-14 15:58:24,773 : Evaluating...
2019-02-14 15:58:50,713 : 
Dev acc : 77.6 Test acc : 79.0 for OBJNUMBER classification

2019-02-14 15:58:50,714 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 15:58:51,129 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 15:58:51,199 : loading BERT model bert-base-uncased
2019-02-14 15:58:51,199 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 15:58:51,228 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 15:58:51,228 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvly45i0l
2019-02-14 15:58:53,679 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 15:58:55,242 : Computing embeddings for train/dev/test
2019-02-14 16:01:29,113 : Computed embeddings
2019-02-14 16:01:29,113 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:02:55,057 : [('reg:1e-05', 62.7), ('reg:0.0001', 61.38), ('reg:0.001', 59.27), ('reg:0.01', 55.31)]
2019-02-14 16:02:55,057 : Validation : best param found is reg = 1e-05 with score             62.7
2019-02-14 16:02:55,057 : Evaluating...
2019-02-14 16:03:16,282 : 
Dev acc : 62.7 Test acc : 62.0 for ODDMANOUT classification

2019-02-14 16:03:16,283 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 16:03:16,696 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 16:03:16,774 : loading BERT model bert-base-uncased
2019-02-14 16:03:16,774 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:03:16,901 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:03:16,901 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4bai_6cq
2019-02-14 16:03:19,358 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:03:20,867 : Computing embeddings for train/dev/test
2019-02-14 16:05:52,439 : Computed embeddings
2019-02-14 16:05:52,439 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:06:58,320 : [('reg:1e-05', 61.73), ('reg:0.0001', 56.1), ('reg:0.001', 50.0), ('reg:0.01', 50.0)]
2019-02-14 16:06:58,320 : Validation : best param found is reg = 1e-05 with score             61.73
2019-02-14 16:06:58,320 : Evaluating...
2019-02-14 16:07:19,347 : 
Dev acc : 61.7 Test acc : 60.4 for COORDINATIONINVERSION classification

2019-02-14 16:07:19,349 : total results: {'STS12': {'MSRpar': {'pearson': (0.3159333718275255, 7.593749215474323e-19), 'spearman': SpearmanrResult(correlation=0.36501511462567093, pvalue=4.718039465667152e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.27271501614316496, 2.945707615453469e-14), 'spearman': SpearmanrResult(correlation=0.3202125811855527, pvalue=2.414521953307395e-19), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.46958590477799106, 1.497405066082265e-26), 'spearman': SpearmanrResult(correlation=0.5577757788033646, pvalue=6.916903571814638e-39), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5121338748629419, 2.2133286567956047e-51), 'spearman': SpearmanrResult(correlation=0.5322965161696521, pvalue=4.2116928656725695e-56), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.572847995395462, 3.588953288794736e-36), 'spearman': SpearmanrResult(correlation=0.4982339629118318, pvalue=2.0121496552473152e-26), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.42864323260141707, 'wmean': 0.4085241240608467}, 'spearman': {'mean': 0.4547067907392145, 'wmean': 0.44014079557857855}}}, 'STS13': {'FNWN': {'pearson': (0.20364845637864512, 0.004944227965974272), 'spearman': SpearmanrResult(correlation=0.2145512785456282, pvalue=0.0030305002078771692), 'nsamples': 189}, 'headlines': {'pearson': (0.45826590086175506, 3.2548955475478845e-40), 'spearman': SpearmanrResult(correlation=0.45357782277736947, pvalue=2.4745444768029854e-39), 'nsamples': 750}, 'OnWN': {'pearson': (0.1457594297219818, 0.000533814719881717), 'spearman': SpearmanrResult(correlation=0.18304746059623112, pvalue=1.2840611631544894e-05), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2692245956541273, 'wmean': 0.309306682650608}, 'spearman': {'mean': 0.28372552063974293, 'wmean': 0.3222821227484243}}}, 'STS14': {'deft-forum': {'pearson': (0.0814599003184559, 0.0843314432251369), 'spearman': SpearmanrResult(correlation=0.09770023297041448, pvalue=0.03829071211287143), 'nsamples': 450}, 'deft-news': {'pearson': (0.539535691583149, 4.63729549708375e-24), 'spearman': SpearmanrResult(correlation=0.5522667639089318, pvalue=2.3710904274856167e-25), 'nsamples': 300}, 'headlines': {'pearson': (0.4195631802139284, 2.4524358770413565e-33), 'spearman': SpearmanrResult(correlation=0.38094158915100174, pvalue=2.5931851993582133e-27), 'nsamples': 750}, 'images': {'pearson': (0.38206870255469094, 1.7745684795122935e-27), 'spearman': SpearmanrResult(correlation=0.39036744399344403, pvalue=1.0379165318628906e-28), 'nsamples': 750}, 'OnWN': {'pearson': (0.3360951434554828, 2.915520133536378e-21), 'spearman': SpearmanrResult(correlation=0.3786474514156983, pvalue=5.586811708824422e-27), 'nsamples': 750}, 'tweet-news': {'pearson': (0.45364645826097166, 2.4026816963048337e-39), 'spearman': SpearmanrResult(correlation=0.45097163690372377, pvalue=7.539469400249765e-39), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.36872817939777974, 'wmean': 0.37121274026188134}, 'spearman': {'mean': 0.3751491863905357, 'wmean': 0.3760909933619379}}}, 'STS15': {'answers-forums': {'pearson': (0.4638792341169102, 2.0867613901434373e-21), 'spearman': SpearmanrResult(correlation=0.4749170641975552, pvalue=1.7098126294600872e-22), 'nsamples': 375}, 'answers-students': {'pearson': (0.5228019752127946, 7.707516565012894e-54), 'spearman': SpearmanrResult(correlation=0.5365133936631351, pvalue=3.947717324149254e-57), 'nsamples': 750}, 'belief': {'pearson': (0.5052954556573342, 1.0737448704590304e-25), 'spearman': SpearmanrResult(correlation=0.5396847721437169, pvalue=9.925103694696733e-30), 'nsamples': 375}, 'headlines': {'pearson': (0.5008045728896219, 7.276066744866947e-49), 'spearman': SpearmanrResult(correlation=0.5086741532681454, pvalue=1.3295173642315023e-50), 'nsamples': 750}, 'images': {'pearson': (0.47895522578260213, 2.8646510447210536e-44), 'spearman': SpearmanrResult(correlation=0.4883602982197516, pvalue=3.30917297234204e-46), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.4943472927318526, 'wmean': 0.49678727969303527}, 'spearman': {'mean': 0.5096299362984609, 'wmean': 0.510212190830417}}}, 'STS16': {'answer-answer': {'pearson': (0.5020973176947334, 1.2567451776550844e-17), 'spearman': SpearmanrResult(correlation=0.53394963880395, pvalue=4.0166049580993267e-20), 'nsamples': 254}, 'headlines': {'pearson': (0.5768616204867375, 1.7327524549025594e-23), 'spearman': SpearmanrResult(correlation=0.5887633802292845, pvalue=1.2680372395265464e-24), 'nsamples': 249}, 'plagiarism': {'pearson': (0.725554781987876, 7.144429529545753e-39), 'spearman': SpearmanrResult(correlation=0.7554946114346508, pvalue=9.317106312318866e-44), 'nsamples': 230}, 'postediting': {'pearson': (0.7687687382968927, 6.924600575359986e-49), 'spearman': SpearmanrResult(correlation=0.7917535149380951, pvalue=1.0104386742551025e-53), 'nsamples': 244}, 'question-question': {'pearson': (0.09928128271478646, 0.1526536618924442), 'spearman': SpearmanrResult(correlation=0.10905017747158356, pvalue=0.1160071214675118), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5345127482362052, 'wmean': 0.545006932786428}, 'spearman': {'mean': 0.5558022645755127, 'wmean': 0.5665838071667125}}}, 'MR': {'devacc': 72.8, 'acc': 72.73, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.23, 'acc': 75.12, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 78.85, 'acc': 82.27, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.6, 'acc': 91.47, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 80.5, 'acc': 80.29, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 39.96, 'acc': 42.22, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 60.2, 'acc': 77.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.74, 'acc': 67.94, 'f1': 75.08, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.0, 'acc': 68.36, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6906131772241485, 'pearson': 0.7045048191840649, 'spearman': 0.6729525904289914, 'mse': 0.5137214283401571, 'yhat': array([2.07366089, 3.61846196, 3.66324951, ..., 2.96932159, 4.38484955,        4.51563741]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.641185482784188, 'pearson': 0.5653916197916385, 'spearman': 0.5575592013832982, 'mse': 1.8883422406589319, 'yhat': array([2.56592089, 2.25740054, 3.19930901, ..., 3.77088552, 3.50169811,        3.06203121]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.23, 'acc': 61.44, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 301.90000000000003, 'acc': [(25.020000000000003, 57.739999999999995, 73.84, 4.2), (21.308, 53.20399999999999, 70.168, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 65.0, 'acc': 65.78, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 0.68, 'acc': 0.75, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 27.57, 'acc': 27.7, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 64.98, 'acc': 65.63, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.39, 'acc': 83.66, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.36, 'acc': 88.09, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 82.48, 'acc': 82.07, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 77.63, 'acc': 79.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.7, 'acc': 61.99, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 61.73, 'acc': 60.43, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 16:07:19,349 : STS12 p=0.4085, STS12 s=0.4401, STS13 p=0.3093, STS13 s=0.3223, STS14 p=0.3712, STS14 s=0.3761, STS15 p=0.4968, STS15 s=0.5102, STS 16 p=0.5450, STS16 s=0.5666, STS B p=0.5654, STS B s=0.5576, STS B m=1.8883, SICK-R p=0.7045, SICK-R s=0.6730, SICK-P m=0.5137
2019-02-14 16:07:19,349 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 16:07:19,349 : 0.4085,0.4401,0.3093,0.3223,0.3712,0.3761,0.4968,0.5102,0.5450,0.5666,0.5654,0.5576,1.8883,0.7045,0.6730,0.5137
2019-02-14 16:07:19,349 : MR=72.73, CR=75.12, SUBJ=91.47, MPQA=82.27, SST-B=80.29, SST-F=42.22, TREC=77.20, SICK-E=68.36, SNLI=61.44, MRPC=67.94, MRPC f=75.08
2019-02-14 16:07:19,349 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 16:07:19,349 : 72.73,75.12,91.47,82.27,80.29,42.22,77.20,68.36,61.44,67.94,75.08
2019-02-14 16:07:19,349 : COCO r1i2t=25.02, COCO r5i2t=57.74, COCO r10i2t=73.84, COCO medr_i2t=4.20, COCO r1t2i=21.31, COCO r5t2i=53.20, COCO r10t2i=70.17, COCO medr_t2i=5.00
2019-02-14 16:07:19,349 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 16:07:19,349 : 25.02,57.74,73.84,4.20,21.31,53.20,70.17,5.00
2019-02-14 16:07:19,349 : SentLen=65.78, WC=0.75, TreeDepth=27.70, TopConst=65.63, BShift=83.66, Tense=88.09, SubjNum=82.07, ObjNum=79.00, SOMO=61.99, CoordInv=60.43, average=61.51
2019-02-14 16:07:19,349 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 16:07:19,349 : 65.78,0.75,27.70,65.63,83.66,88.09,82.07,79.00,61.99,60.43,61.51
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,349 : layer 8
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,349 : ********************************************************************************
2019-02-14 16:07:19,441 : ***** Transfer task : STS12 *****


2019-02-14 16:07:19,454 : loading BERT model bert-base-uncased
2019-02-14 16:07:19,454 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:07:19,471 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:07:19,471 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5n3aub2v
2019-02-14 16:07:21,907 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:07:25,772 : MSRpar : pearson = 0.2778, spearman = 0.3321
2019-02-14 16:07:27,325 : MSRvid : pearson = 0.2012, spearman = 0.2484
2019-02-14 16:07:28,542 : SMTeuroparl : pearson = 0.4351, spearman = 0.5433
2019-02-14 16:07:30,513 : surprise.OnWN : pearson = 0.5107, spearman = 0.5210
2019-02-14 16:07:31,168 : surprise.SMTnews : pearson = 0.6121, spearman = 0.5164
2019-02-14 16:07:31,168 : ALL (weighted average) : Pearson = 0.3817,             Spearman = 0.4123
2019-02-14 16:07:31,168 : ALL (average) : Pearson = 0.4074,             Spearman = 0.4322

2019-02-14 16:07:31,169 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 16:07:31,179 : loading BERT model bert-base-uncased
2019-02-14 16:07:31,179 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:07:31,198 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:07:31,199 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp989n92fx
2019-02-14 16:07:33,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:07:35,710 : FNWN : pearson = 0.1442, spearman = 0.1713
2019-02-14 16:07:36,593 : headlines : pearson = 0.4443, spearman = 0.4461
2019-02-14 16:07:37,277 : OnWN : pearson = 0.2129, spearman = 0.2367
2019-02-14 16:07:37,277 : ALL (weighted average) : Pearson = 0.3200,             Spearman = 0.3332
2019-02-14 16:07:37,277 : ALL (average) : Pearson = 0.2671,             Spearman = 0.2847

2019-02-14 16:07:37,277 : ***** Transfer task : STS14 *****


2019-02-14 16:07:37,293 : loading BERT model bert-base-uncased
2019-02-14 16:07:37,293 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:07:37,312 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:07:37,312 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpaeyupw9l
2019-02-14 16:07:39,761 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:07:41,913 : deft-forum : pearson = 0.1153, spearman = 0.1145
2019-02-14 16:07:42,608 : deft-news : pearson = 0.5886, spearman = 0.5848
2019-02-14 16:07:43,779 : headlines : pearson = 0.4143, spearman = 0.3869
2019-02-14 16:07:44,760 : images : pearson = 0.3334, spearman = 0.3440
2019-02-14 16:07:45,755 : OnWN : pearson = 0.4054, spearman = 0.4348
2019-02-14 16:07:47,063 : tweet-news : pearson = 0.5307, spearman = 0.5019
2019-02-14 16:07:47,063 : ALL (weighted average) : Pearson = 0.3977,             Spearman = 0.3940
2019-02-14 16:07:47,063 : ALL (average) : Pearson = 0.3980,             Spearman = 0.3945

2019-02-14 16:07:47,063 : ***** Transfer task : STS15 *****


2019-02-14 16:07:47,131 : loading BERT model bert-base-uncased
2019-02-14 16:07:47,131 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:07:47,150 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:07:47,150 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb339eb41
2019-02-14 16:07:49,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:07:51,995 : answers-forums : pearson = 0.4412, spearman = 0.4163
2019-02-14 16:07:52,915 : answers-students : pearson = 0.4643, spearman = 0.4762
2019-02-14 16:07:53,787 : belief : pearson = 0.5315, spearman = 0.5614
2019-02-14 16:07:54,818 : headlines : pearson = 0.5033, spearman = 0.5038
2019-02-14 16:07:55,797 : images : pearson = 0.3906, spearman = 0.3985
2019-02-14 16:07:55,797 : ALL (weighted average) : Pearson = 0.4611,             Spearman = 0.4668
2019-02-14 16:07:55,797 : ALL (average) : Pearson = 0.4662,             Spearman = 0.4712

2019-02-14 16:07:55,797 : ***** Transfer task : STS16 *****


2019-02-14 16:07:55,873 : loading BERT model bert-base-uncased
2019-02-14 16:07:55,873 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:07:55,894 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:07:55,894 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsajcvvd2
2019-02-14 16:07:58,392 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:08:00,249 : answer-answer : pearson = 0.5457, spearman = 0.5470
2019-02-14 16:08:00,548 : headlines : pearson = 0.5382, spearman = 0.5572
2019-02-14 16:08:00,943 : plagiarism : pearson = 0.7511, spearman = 0.7667
2019-02-14 16:08:01,586 : postediting : pearson = 0.7573, spearman = 0.7945
2019-02-14 16:08:01,873 : question-question : pearson = 0.1597, spearman = 0.1897
2019-02-14 16:08:01,873 : ALL (weighted average) : Pearson = 0.5595,             Spearman = 0.5797
2019-02-14 16:08:01,873 : ALL (average) : Pearson = 0.5504,             Spearman = 0.5710

2019-02-14 16:08:01,873 : ***** Transfer task : MR *****


2019-02-14 16:08:01,892 : loading BERT model bert-base-uncased
2019-02-14 16:08:01,893 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:08:01,915 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:08:01,915 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6m6f255d
2019-02-14 16:08:04,389 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:08:05,874 : Generating sentence embeddings
2019-02-14 16:08:20,239 : Generated sentence embeddings
2019-02-14 16:08:20,239 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 16:08:28,440 : Best param found at split 1: l2reg = 1e-05                 with score 73.13
2019-02-14 16:08:40,298 : Best param found at split 2: l2reg = 1e-05                 with score 73.33
2019-02-14 16:08:53,124 : Best param found at split 3: l2reg = 0.0001                 with score 74.37
2019-02-14 16:09:05,023 : Best param found at split 4: l2reg = 1e-05                 with score 72.37
2019-02-14 16:09:16,448 : Best param found at split 5: l2reg = 0.0001                 with score 74.51
2019-02-14 16:09:17,315 : Dev acc : 73.54 Test acc : 74.59

2019-02-14 16:09:17,316 : ***** Transfer task : CR *****


2019-02-14 16:09:17,324 : loading BERT model bert-base-uncased
2019-02-14 16:09:17,324 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:09:17,343 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:09:17,343 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkpdw_8l8
2019-02-14 16:09:19,783 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:09:21,219 : Generating sentence embeddings
2019-02-14 16:09:24,944 : Generated sentence embeddings
2019-02-14 16:09:24,944 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 16:09:29,161 : Best param found at split 1: l2reg = 1e-05                 with score 75.46
2019-02-14 16:09:33,989 : Best param found at split 2: l2reg = 0.0001                 with score 76.88
2019-02-14 16:09:38,533 : Best param found at split 3: l2reg = 1e-05                 with score 76.52
2019-02-14 16:09:43,138 : Best param found at split 4: l2reg = 0.0001                 with score 76.23
2019-02-14 16:09:47,575 : Best param found at split 5: l2reg = 1e-05                 with score 74.78
2019-02-14 16:09:47,804 : Dev acc : 75.97 Test acc : 74.65

2019-02-14 16:09:47,804 : ***** Transfer task : MPQA *****


2019-02-14 16:09:47,810 : loading BERT model bert-base-uncased
2019-02-14 16:09:47,810 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:09:47,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:09:47,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpc7kekq_4
2019-02-14 16:09:50,274 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:09:51,750 : Generating sentence embeddings
2019-02-14 16:09:55,471 : Generated sentence embeddings
2019-02-14 16:09:55,472 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 16:10:06,207 : Best param found at split 1: l2reg = 1e-05                 with score 80.6
2019-02-14 16:10:17,451 : Best param found at split 2: l2reg = 1e-05                 with score 80.94
2019-02-14 16:10:30,410 : Best param found at split 3: l2reg = 1e-05                 with score 79.38
2019-02-14 16:10:44,104 : Best param found at split 4: l2reg = 1e-05                 with score 82.37
2019-02-14 16:10:59,228 : Best param found at split 5: l2reg = 1e-05                 with score 82.65
2019-02-14 16:11:00,037 : Dev acc : 81.19 Test acc : 84.89

2019-02-14 16:11:00,038 : ***** Transfer task : SUBJ *****


2019-02-14 16:11:00,052 : loading BERT model bert-base-uncased
2019-02-14 16:11:00,052 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:11:00,076 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:11:00,076 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6wbh09g1
2019-02-14 16:11:02,562 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:11:04,018 : Generating sentence embeddings
2019-02-14 16:11:18,234 : Generated sentence embeddings
2019-02-14 16:11:18,235 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 16:11:28,783 : Best param found at split 1: l2reg = 1e-05                 with score 93.09
2019-02-14 16:11:43,466 : Best param found at split 2: l2reg = 1e-05                 with score 93.05
2019-02-14 16:11:58,380 : Best param found at split 3: l2reg = 1e-05                 with score 92.99
2019-02-14 16:12:13,463 : Best param found at split 4: l2reg = 1e-05                 with score 93.3
2019-02-14 16:12:27,670 : Best param found at split 5: l2reg = 1e-05                 with score 92.81
2019-02-14 16:12:28,771 : Dev acc : 93.05 Test acc : 92.26

2019-02-14 16:12:28,772 : ***** Transfer task : SST Binary classification *****


2019-02-14 16:12:28,916 : loading BERT model bert-base-uncased
2019-02-14 16:12:28,916 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:12:28,943 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:12:28,943 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpriaeebsy
2019-02-14 16:12:31,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:12:32,885 : Computing embedding for train
2019-02-14 16:13:18,413 : Computed train embeddings
2019-02-14 16:13:18,413 : Computing embedding for dev
2019-02-14 16:13:19,454 : Computed dev embeddings
2019-02-14 16:13:19,454 : Computing embedding for test
2019-02-14 16:13:21,636 : Computed test embeddings
2019-02-14 16:13:21,636 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:13:41,951 : [('reg:1e-05', 81.65), ('reg:0.0001', 81.54), ('reg:0.001', 78.33), ('reg:0.01', 64.79)]
2019-02-14 16:13:41,952 : Validation : best param found is reg = 1e-05 with score             81.65
2019-02-14 16:13:41,952 : Evaluating...
2019-02-14 16:13:47,058 : 
Dev acc : 81.65 Test acc : 81.0 for             SST Binary classification

2019-02-14 16:13:47,058 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 16:13:47,113 : loading BERT model bert-base-uncased
2019-02-14 16:13:47,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:13:47,137 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:13:47,137 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwxarpqwz
2019-02-14 16:13:49,627 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:13:51,078 : Computing embedding for train
2019-02-14 16:14:00,600 : Computed train embeddings
2019-02-14 16:14:00,600 : Computing embedding for dev
2019-02-14 16:14:01,811 : Computed dev embeddings
2019-02-14 16:14:01,812 : Computing embedding for test
2019-02-14 16:14:04,223 : Computed test embeddings
2019-02-14 16:14:04,223 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:14:06,864 : [('reg:1e-05', 40.15), ('reg:0.0001', 39.42), ('reg:0.001', 37.24), ('reg:0.01', 26.25)]
2019-02-14 16:14:06,865 : Validation : best param found is reg = 1e-05 with score             40.15
2019-02-14 16:14:06,865 : Evaluating...
2019-02-14 16:14:07,508 : 
Dev acc : 40.15 Test acc : 37.96 for             SST Fine-Grained classification

2019-02-14 16:14:07,509 : ***** Transfer task : TREC *****


2019-02-14 16:14:07,523 : loading BERT model bert-base-uncased
2019-02-14 16:14:07,523 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:14:07,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:14:07,544 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpnaxyguzj
2019-02-14 16:14:09,999 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:14:15,267 : Computed train embeddings
2019-02-14 16:14:15,563 : Computed test embeddings
2019-02-14 16:14:15,563 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 16:14:23,564 : [('reg:1e-05', 67.23), ('reg:0.0001', 59.28), ('reg:0.001', 45.49), ('reg:0.01', 32.38)]
2019-02-14 16:14:23,565 : Cross-validation : best param found is reg = 1e-05             with score 67.23
2019-02-14 16:14:23,565 : Evaluating...
2019-02-14 16:14:24,172 : 
Dev acc : 67.23 Test acc : 78.2             for TREC

2019-02-14 16:14:24,173 : ***** Transfer task : MRPC *****


2019-02-14 16:14:24,194 : loading BERT model bert-base-uncased
2019-02-14 16:14:24,194 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:14:24,214 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:14:24,215 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6jxn2t19
2019-02-14 16:14:26,657 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:14:28,088 : Computing embedding for train
2019-02-14 16:14:37,706 : Computed train embeddings
2019-02-14 16:14:37,707 : Computing embedding for test
2019-02-14 16:14:41,874 : Computed test embeddings
2019-02-14 16:14:41,890 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 16:14:47,047 : [('reg:1e-05', 68.6), ('reg:0.0001', 68.5), ('reg:0.001', 68.42), ('reg:0.01', 67.54)]
2019-02-14 16:14:47,047 : Cross-validation : best param found is reg = 1e-05             with score 68.6
2019-02-14 16:14:47,047 : Evaluating...
2019-02-14 16:14:47,405 : Dev acc : 68.6 Test acc 68.87; Test F1 76.46 for MRPC.

2019-02-14 16:14:47,405 : ***** Transfer task : SICK-Entailment*****


2019-02-14 16:14:47,474 : loading BERT model bert-base-uncased
2019-02-14 16:14:47,474 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:14:47,495 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:14:47,495 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb8reh5rw
2019-02-14 16:14:49,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:14:51,419 : Computing embedding for train
2019-02-14 16:14:56,572 : Computed train embeddings
2019-02-14 16:14:56,572 : Computing embedding for dev
2019-02-14 16:14:57,254 : Computed dev embeddings
2019-02-14 16:14:57,254 : Computing embedding for test
2019-02-14 16:15:02,806 : Computed test embeddings
2019-02-14 16:15:02,834 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:15:04,323 : [('reg:1e-05', 71.0), ('reg:0.0001', 70.0), ('reg:0.001', 57.0), ('reg:0.01', 61.2)]
2019-02-14 16:15:04,323 : Validation : best param found is reg = 1e-05 with score             71.0
2019-02-14 16:15:04,324 : Evaluating...
2019-02-14 16:15:04,675 : 
Dev acc : 71.0 Test acc : 70.1 for                        SICK entailment

2019-02-14 16:15:04,676 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 16:15:04,704 : loading BERT model bert-base-uncased
2019-02-14 16:15:04,704 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:15:04,766 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:15:04,766 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphjf9kyht
2019-02-14 16:15:07,247 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:15:08,748 : Computing embedding for train
2019-02-14 16:15:14,357 : Computed train embeddings
2019-02-14 16:15:14,358 : Computing embedding for dev
2019-02-14 16:15:15,119 : Computed dev embeddings
2019-02-14 16:15:15,119 : Computing embedding for test
2019-02-14 16:15:21,273 : Computed test embeddings
2019-02-14 16:16:27,542 : Dev : Pearson 0.6930347179264527
2019-02-14 16:16:27,542 : Test : Pearson 0.6928476010637008 Spearman 0.6631921667799732 MSE 0.5363103855537937                        for SICK Relatedness

2019-02-14 16:16:27,543 : 

***** Transfer task : STSBenchmark*****


2019-02-14 16:16:27,582 : loading BERT model bert-base-uncased
2019-02-14 16:16:27,582 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:16:27,613 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:16:27,614 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgz9zh7ar
2019-02-14 16:16:30,060 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:16:31,531 : Computing embedding for train
2019-02-14 16:16:39,820 : Computed train embeddings
2019-02-14 16:16:39,820 : Computing embedding for dev
2019-02-14 16:16:42,276 : Computed dev embeddings
2019-02-14 16:16:42,276 : Computing embedding for test
2019-02-14 16:16:44,266 : Computed test embeddings
2019-02-14 16:17:26,380 : Dev : Pearson 0.6250795058638942
2019-02-14 16:17:26,380 : Test : Pearson 0.5593898367103933 Spearman 0.5514142884288264 MSE 1.8727268647617732                        for SICK Relatedness

2019-02-14 16:17:26,381 : ***** Transfer task : SNLI Entailment*****


2019-02-14 16:17:31,296 : loading BERT model bert-base-uncased
2019-02-14 16:17:31,297 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:17:31,428 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:17:31,429 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0z8erj2k
2019-02-14 16:17:33,910 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:17:35,560 : PROGRESS (encoding): 0.00%
2019-02-14 16:18:54,043 : PROGRESS (encoding): 14.56%
2019-02-14 16:20:22,590 : PROGRESS (encoding): 29.12%
2019-02-14 16:21:51,687 : PROGRESS (encoding): 43.69%
2019-02-14 16:23:29,506 : PROGRESS (encoding): 58.25%
2019-02-14 16:25:15,797 : PROGRESS (encoding): 72.81%
2019-02-14 16:27:01,596 : PROGRESS (encoding): 87.37%
2019-02-14 16:28:52,963 : PROGRESS (encoding): 0.00%
2019-02-14 16:29:06,521 : PROGRESS (encoding): 0.00%
2019-02-14 16:29:19,697 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 16:30:05,428 : [('reg:1e-09', 61.69)]
2019-02-14 16:30:05,428 : Validation : best param found is reg = 1e-09 with score             61.69
2019-02-14 16:30:05,428 : Evaluating...
2019-02-14 16:30:50,358 : Dev acc : 61.69 Test acc : 60.95 for SNLI

2019-02-14 16:30:50,359 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 16:30:59,263 : loading BERT model bert-base-uncased
2019-02-14 16:30:59,263 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 16:30:59,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 16:30:59,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpewiuedkg
2019-02-14 16:31:01,759 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 16:31:03,219 : Computing embedding for train
2019-02-14 16:38:42,750 : Computed train embeddings
2019-02-14 16:38:42,750 : Computing embedding for dev
2019-02-14 16:39:02,388 : Computed dev embeddings
2019-02-14 16:39:02,388 : Computing embedding for test
2019-02-14 16:39:23,827 : Computed test embeddings
2019-02-14 16:39:23,842 : prepare data
2019-02-14 16:39:23,904 : start epoch
2019-02-14 16:40:07,581 : samples : 64000
2019-02-14 16:40:17,721 : Image to text: 3.24, 11.76, 20.0, 49.0
2019-02-14 16:40:25,127 : Text to Image: 2.62, 9.748, 16.136, 58.0
2019-02-14 16:41:08,196 : samples : 128000
2019-02-14 16:41:18,384 : Image to text: 4.24, 14.3, 22.3, 44.0
2019-02-14 16:41:25,787 : Text to Image: 1.94, 7.9, 13.672, 74.0
2019-02-14 16:42:08,585 : samples : 192000
2019-02-14 16:42:18,745 : Image to text: 5.4, 17.64, 27.08, 32.0
2019-02-14 16:42:26,121 : Text to Image: 3.788, 13.476, 21.5, 42.0
2019-02-14 16:43:08,623 : samples : 256000
2019-02-14 16:43:18,781 : Image to text: 6.14, 19.38, 29.58, 29.0
2019-02-14 16:43:26,146 : Text to Image: 4.584, 15.388, 24.432, 35.0
2019-02-14 16:44:08,559 : samples : 320000
2019-02-14 16:44:18,908 : Image to text: 5.48, 18.24, 28.36, 30.0
2019-02-14 16:44:26,345 : Text to Image: 4.128, 14.716, 23.624, 37.0
2019-02-14 16:45:08,589 : samples : 384000
2019-02-14 16:45:19,020 : Image to text: 5.66, 19.36, 30.04, 28.0
2019-02-14 16:45:26,531 : Text to Image: 4.268, 15.28, 24.3, 36.0
2019-02-14 16:46:08,814 : samples : 448000
2019-02-14 16:46:19,272 : Image to text: 5.62, 18.16, 29.34, 29.0
2019-02-14 16:46:26,749 : Text to Image: 4.476, 15.76, 24.552, 36.0
2019-02-14 16:47:08,971 : samples : 512000
2019-02-14 16:47:19,383 : Image to text: 6.88, 20.18, 30.56, 26.0
2019-02-14 16:47:26,906 : Text to Image: 4.496, 15.576, 24.556, 35.0
2019-02-14 16:48:03,081 : Epoch 1 finished
2019-02-14 16:48:03,511 : Image to text: 17.0, 45.8, 63.3, 7.0
2019-02-14 16:48:03,827 : Text to Image: 12.58, 37.56, 53.52, 9.0
2019-02-14 16:48:04,255 : Image to text: 16.9, 45.1, 62.3, 7.0
2019-02-14 16:48:04,571 : Text to Image: 12.24, 36.24, 53.42, 9.0
2019-02-14 16:48:04,997 : Image to text: 20.2, 48.5, 64.9, 6.0
2019-02-14 16:48:05,310 : Text to Image: 12.24, 36.22, 52.76, 9.0
2019-02-14 16:48:05,734 : Image to text: 18.2, 45.8, 61.9, 7.0
2019-02-14 16:48:06,046 : Text to Image: 13.22, 37.1, 52.84, 9.0
2019-02-14 16:48:06,470 : Image to text: 17.6, 46.4, 62.3, 6.0
2019-02-14 16:48:06,782 : Text to Image: 12.24, 36.46, 53.32, 9.0
2019-02-14 16:48:06,782 : Dev mean Text to Image: 12.504000000000001, 36.716, 53.172000000000004, 9.0
2019-02-14 16:48:06,782 : Dev mean Image to text: 17.98, 46.32, 62.93999999999999, 6.6000000000000005
2019-02-14 16:48:06,782 : start epoch
2019-02-14 16:48:49,599 : samples : 64000
2019-02-14 16:48:59,665 : Image to text: 7.6, 22.68, 33.9, 23.0
2019-02-14 16:49:06,946 : Text to Image: 5.3, 17.984, 28.264, 29.0
2019-02-14 16:49:49,871 : samples : 128000
2019-02-14 16:49:59,920 : Image to text: 7.24, 21.22, 32.82, 22.0
2019-02-14 16:50:07,187 : Text to Image: 5.768, 18.54, 28.54, 28.0
2019-02-14 16:50:50,282 : samples : 192000
2019-02-14 16:51:00,321 : Image to text: 6.5, 21.68, 32.64, 23.0
2019-02-14 16:51:07,630 : Text to Image: 5.624, 18.628, 28.492, 29.0
2019-02-14 16:51:50,399 : samples : 256000
2019-02-14 16:52:00,455 : Image to text: 6.8, 21.6, 32.64, 24.0
2019-02-14 16:52:07,707 : Text to Image: 5.004, 17.42, 27.128, 31.0
2019-02-14 16:52:50,776 : samples : 320000
2019-02-14 16:53:01,161 : Image to text: 6.92, 22.04, 34.44, 22.0
2019-02-14 16:53:08,580 : Text to Image: 6.148, 19.6, 30.24, 26.0
2019-02-14 16:53:50,289 : samples : 384000
2019-02-14 16:54:00,641 : Image to text: 7.66, 23.8, 35.04, 20.0
2019-02-14 16:54:08,058 : Text to Image: 5.876, 19.616, 29.98, 27.0
2019-02-14 16:54:50,046 : samples : 448000
2019-02-14 16:55:00,428 : Image to text: 7.54, 21.0, 31.44, 25.0
2019-02-14 16:55:07,870 : Text to Image: 5.032, 16.68, 26.792, 31.0
2019-02-14 16:55:49,983 : samples : 512000
2019-02-14 16:56:00,361 : Image to text: 8.06, 23.6, 34.54, 21.0
2019-02-14 16:56:07,780 : Text to Image: 5.86, 19.936, 30.452, 26.0
2019-02-14 16:56:43,492 : Epoch 2 finished
2019-02-14 16:56:43,922 : Image to text: 19.5, 50.4, 66.6, 5.0
2019-02-14 16:56:44,239 : Text to Image: 15.16, 43.34, 60.72, 7.0
2019-02-14 16:56:44,665 : Image to text: 19.8, 49.2, 64.8, 6.0
2019-02-14 16:56:44,980 : Text to Image: 12.72, 41.44, 60.0, 7.0
2019-02-14 16:56:45,406 : Image to text: 21.6, 50.6, 65.4, 5.0
2019-02-14 16:56:45,728 : Text to Image: 14.2, 42.3, 59.6, 7.0
2019-02-14 16:56:46,187 : Image to text: 18.2, 49.8, 65.4, 6.0
2019-02-14 16:56:46,523 : Text to Image: 14.12, 41.86, 59.56, 7.0
2019-02-14 16:56:46,972 : Image to text: 19.2, 48.0, 65.0, 6.0
2019-02-14 16:56:47,311 : Text to Image: 15.22, 41.54, 58.98, 7.0
2019-02-14 16:56:47,311 : Dev mean Text to Image: 14.284, 42.096000000000004, 59.772, 7.0
2019-02-14 16:56:47,311 : Dev mean Image to text: 19.66, 49.6, 65.44, 5.6000000000000005
2019-02-14 16:56:47,311 : start epoch
2019-02-14 16:57:29,658 : samples : 64000
2019-02-14 16:57:39,703 : Image to text: 7.58, 23.34, 35.52, 21.0
2019-02-14 16:57:47,001 : Text to Image: 5.572, 18.98, 29.54, 28.0
2019-02-14 16:58:30,658 : samples : 128000
2019-02-14 16:58:40,718 : Image to text: 7.6, 24.12, 35.82, 20.0
2019-02-14 16:58:48,020 : Text to Image: 5.94, 19.804, 30.852, 26.0
2019-02-14 16:59:30,793 : samples : 192000
2019-02-14 16:59:40,794 : Image to text: 7.56, 23.88, 35.08, 21.0
2019-02-14 16:59:48,052 : Text to Image: 6.32, 21.268, 32.216, 25.0
2019-02-14 17:00:36,300 : samples : 256000
2019-02-14 17:00:46,323 : Image to text: 8.04, 24.76, 35.46, 21.0
2019-02-14 17:00:53,603 : Text to Image: 6.484, 21.52, 32.556, 24.0
2019-02-14 17:01:35,178 : samples : 320000
2019-02-14 17:01:45,517 : Image to text: 7.86, 24.08, 35.82, 20.0
2019-02-14 17:01:52,920 : Text to Image: 6.556, 20.984, 31.696, 25.0
2019-02-14 17:02:33,462 : samples : 384000
2019-02-14 17:02:43,787 : Image to text: 8.32, 25.34, 37.28, 19.0
2019-02-14 17:02:51,212 : Text to Image: 6.38, 20.572, 31.284, 25.0
2019-02-14 17:03:32,349 : samples : 448000
2019-02-14 17:03:42,725 : Image to text: 7.68, 24.52, 36.48, 19.0
2019-02-14 17:03:50,125 : Text to Image: 6.632, 20.976, 31.744, 25.0
2019-02-14 17:04:30,558 : samples : 512000
2019-02-14 17:04:40,918 : Image to text: 8.44, 24.08, 36.26, 21.0
2019-02-14 17:04:48,320 : Text to Image: 5.956, 19.312, 29.82, 27.0
2019-02-14 17:05:23,323 : Epoch 3 finished
2019-02-14 17:05:23,751 : Image to text: 21.8, 55.1, 69.2, 5.0
2019-02-14 17:05:24,064 : Text to Image: 18.18, 48.6, 65.6, 6.0
2019-02-14 17:05:24,490 : Image to text: 21.4, 53.6, 69.8, 5.0
2019-02-14 17:05:24,801 : Text to Image: 16.4, 45.72, 63.56, 6.0
2019-02-14 17:05:25,235 : Image to text: 23.8, 57.4, 72.3, 4.0
2019-02-14 17:05:25,569 : Text to Image: 17.5, 47.22, 64.9, 6.0
2019-02-14 17:05:26,024 : Image to text: 23.9, 53.2, 68.9, 5.0
2019-02-14 17:05:26,357 : Text to Image: 17.64, 46.72, 64.86, 6.0
2019-02-14 17:05:26,791 : Image to text: 21.8, 54.0, 69.6, 5.0
2019-02-14 17:05:27,134 : Text to Image: 17.4, 47.22, 64.68, 6.0
2019-02-14 17:05:27,134 : Dev mean Text to Image: 17.424, 47.096000000000004, 64.72, 6.0
2019-02-14 17:05:27,134 : Dev mean Image to text: 22.54, 54.66, 69.96, 4.8
2019-02-14 17:05:27,135 : start epoch
2019-02-14 17:06:06,615 : samples : 64000
2019-02-14 17:06:16,634 : Image to text: 8.48, 24.36, 36.24, 19.0
2019-02-14 17:06:23,939 : Text to Image: 6.536, 20.696, 31.512, 25.0
2019-02-14 17:07:03,728 : samples : 128000
2019-02-14 17:07:13,727 : Image to text: 8.72, 26.12, 38.06, 19.0
2019-02-14 17:07:20,994 : Text to Image: 6.912, 21.768, 32.668, 23.0
2019-02-14 17:08:00,411 : samples : 192000
2019-02-14 17:08:10,409 : Image to text: 7.86, 25.38, 36.7, 20.0
2019-02-14 17:08:17,695 : Text to Image: 6.588, 21.084, 31.752, 24.0
2019-02-14 17:08:57,313 : samples : 256000
2019-02-14 17:09:07,303 : Image to text: 8.92, 26.34, 38.52, 18.0
2019-02-14 17:09:14,593 : Text to Image: 7.44, 22.848, 34.476, 22.0
2019-02-14 17:09:54,203 : samples : 320000
2019-02-14 17:10:04,475 : Image to text: 8.54, 25.34, 37.82, 19.0
2019-02-14 17:10:11,862 : Text to Image: 6.948, 22.032, 33.22, 23.0
2019-02-14 17:10:54,070 : samples : 384000
2019-02-14 17:11:04,428 : Image to text: 8.24, 26.18, 37.72, 19.0
2019-02-14 17:11:11,838 : Text to Image: 7.212, 22.216, 33.616, 22.0
2019-02-14 17:11:53,170 : samples : 448000
2019-02-14 17:12:03,491 : Image to text: 9.1, 26.26, 38.5, 18.0
2019-02-14 17:12:10,880 : Text to Image: 7.14, 22.364, 33.46, 23.0
2019-02-14 17:12:52,421 : samples : 512000
2019-02-14 17:13:02,736 : Image to text: 9.64, 26.9, 39.32, 17.0
2019-02-14 17:13:10,119 : Text to Image: 7.384, 23.3, 34.54, 22.0
2019-02-14 17:13:44,274 : Epoch 4 finished
2019-02-14 17:13:44,703 : Image to text: 22.7, 55.9, 71.9, 4.0
2019-02-14 17:13:45,020 : Text to Image: 17.28, 47.62, 66.22, 6.0
2019-02-14 17:13:45,469 : Image to text: 21.3, 54.5, 70.4, 5.0
2019-02-14 17:13:45,805 : Text to Image: 17.52, 47.54, 65.06, 6.0
2019-02-14 17:13:46,260 : Image to text: 23.8, 57.1, 72.2, 4.0
2019-02-14 17:13:46,602 : Text to Image: 17.78, 47.36, 63.88, 6.0
2019-02-14 17:13:47,048 : Image to text: 24.0, 56.4, 71.1, 4.0
2019-02-14 17:13:47,397 : Text to Image: 17.18, 46.72, 64.2, 6.0
2019-02-14 17:13:47,840 : Image to text: 21.0, 55.6, 70.3, 5.0
2019-02-14 17:13:48,175 : Text to Image: 17.4, 47.8, 63.56, 6.0
2019-02-14 17:13:48,175 : Dev mean Text to Image: 17.432000000000002, 47.408, 64.584, 6.0
2019-02-14 17:13:48,175 : Dev mean Image to text: 22.56, 55.900000000000006, 71.18, 4.4
2019-02-14 17:13:48,176 : start epoch
2019-02-14 17:14:28,794 : samples : 64000
2019-02-14 17:14:38,783 : Image to text: 9.28, 26.7, 39.02, 17.0
2019-02-14 17:14:46,049 : Text to Image: 7.052, 22.504, 33.828, 23.0
2019-02-14 17:15:28,835 : samples : 128000
2019-02-14 17:15:38,797 : Image to text: 9.0, 26.34, 38.56, 18.0
2019-02-14 17:15:46,048 : Text to Image: 7.256, 22.416, 33.54, 23.0
2019-02-14 17:16:27,975 : samples : 192000
2019-02-14 17:16:37,955 : Image to text: 8.98, 25.24, 36.08, 20.0
2019-02-14 17:16:45,222 : Text to Image: 6.304, 20.156, 30.836, 25.0
2019-02-14 17:17:24,727 : samples : 256000
2019-02-14 17:17:34,707 : Image to text: 9.64, 26.5, 38.76, 18.0
2019-02-14 17:17:41,994 : Text to Image: 7.356, 22.852, 34.432, 22.0
2019-02-14 17:18:21,465 : samples : 320000
2019-02-14 17:18:31,820 : Image to text: 8.74, 26.16, 38.62, 18.0
2019-02-14 17:18:39,250 : Text to Image: 6.872, 22.324, 33.7, 22.0
2019-02-14 17:19:19,407 : samples : 384000
2019-02-14 17:19:29,697 : Image to text: 8.68, 26.84, 38.74, 18.0
2019-02-14 17:19:37,061 : Text to Image: 7.16, 22.896, 34.232, 22.0
2019-02-14 17:20:16,931 : samples : 448000
2019-02-14 17:20:27,265 : Image to text: 10.06, 28.26, 39.92, 17.0
2019-02-14 17:20:34,657 : Text to Image: 7.784, 24.144, 35.328, 21.0
2019-02-14 17:21:15,648 : samples : 512000
2019-02-14 17:21:25,981 : Image to text: 8.6, 26.48, 38.9, 18.0
2019-02-14 17:21:33,370 : Text to Image: 7.152, 22.664, 33.968, 22.0
2019-02-14 17:22:08,471 : Epoch 5 finished
2019-02-14 17:22:08,899 : Image to text: 21.9, 55.8, 71.3, 4.0
2019-02-14 17:22:09,215 : Text to Image: 18.78, 49.66, 67.6, 6.0
2019-02-14 17:22:09,661 : Image to text: 22.9, 54.2, 71.6, 5.0
2019-02-14 17:22:09,993 : Text to Image: 18.76, 49.76, 67.92, 6.0
2019-02-14 17:22:10,438 : Image to text: 23.7, 56.6, 72.1, 4.0
2019-02-14 17:22:10,769 : Text to Image: 18.64, 49.36, 66.92, 6.0
2019-02-14 17:22:11,197 : Image to text: 24.0, 57.0, 72.3, 4.0
2019-02-14 17:22:11,526 : Text to Image: 18.94, 50.84, 67.72, 5.0
2019-02-14 17:22:11,964 : Image to text: 22.6, 56.1, 72.8, 4.0
2019-02-14 17:22:12,293 : Text to Image: 19.26, 50.02, 67.22, 5.0
2019-02-14 17:22:12,293 : Dev mean Text to Image: 18.876, 49.928, 67.476, 5.6
2019-02-14 17:22:12,294 : Dev mean Image to text: 23.02, 55.94, 72.02, 4.2
2019-02-14 17:22:12,294 : start epoch
2019-02-14 17:22:54,803 : samples : 64000
2019-02-14 17:23:04,846 : Image to text: 9.98, 28.0, 40.74, 16.0
2019-02-14 17:23:12,141 : Text to Image: 7.916, 24.504, 36.264, 20.0
2019-02-14 17:23:54,536 : samples : 128000
2019-02-14 17:24:04,549 : Image to text: 8.28, 24.8, 37.06, 19.0
2019-02-14 17:24:11,877 : Text to Image: 6.4, 21.196, 32.404, 24.0
2019-02-14 17:24:53,968 : samples : 192000
2019-02-14 17:25:04,001 : Image to text: 9.42, 26.88, 38.0, 18.0
2019-02-14 17:25:11,321 : Text to Image: 7.072, 22.336, 33.768, 22.0
2019-02-14 17:25:51,805 : samples : 256000
2019-02-14 17:26:01,844 : Image to text: 9.2, 26.84, 38.7, 17.0
2019-02-14 17:26:09,158 : Text to Image: 7.364, 22.784, 33.912, 22.0
2019-02-14 17:26:50,414 : samples : 320000
2019-02-14 17:27:00,723 : Image to text: 9.28, 26.42, 38.72, 17.0
2019-02-14 17:27:08,101 : Text to Image: 7.58, 23.648, 35.232, 21.0
2019-02-14 17:27:48,743 : samples : 384000
2019-02-14 17:27:59,066 : Image to text: 9.58, 27.28, 39.96, 17.0
2019-02-14 17:28:06,454 : Text to Image: 6.876, 22.472, 33.984, 22.0
2019-02-14 17:28:47,434 : samples : 448000
2019-02-14 17:28:57,746 : Image to text: 10.5, 28.18, 41.02, 16.0
2019-02-14 17:29:05,129 : Text to Image: 8.1, 24.668, 36.552, 20.0
2019-02-14 17:29:49,160 : samples : 512000
2019-02-14 17:29:59,521 : Image to text: 9.72, 27.86, 40.12, 17.0
2019-02-14 17:30:06,925 : Text to Image: 7.096, 22.628, 34.46, 22.0
2019-02-14 17:30:42,664 : Epoch 6 finished
2019-02-14 17:30:43,092 : Image to text: 23.9, 56.3, 72.8, 5.0
2019-02-14 17:30:43,404 : Text to Image: 19.76, 51.1, 68.92, 5.0
2019-02-14 17:30:43,865 : Image to text: 21.5, 56.9, 73.1, 4.0
2019-02-14 17:30:44,198 : Text to Image: 18.4, 49.66, 66.54, 6.0
2019-02-14 17:30:44,652 : Image to text: 26.1, 58.1, 73.3, 4.0
2019-02-14 17:30:44,985 : Text to Image: 19.62, 51.64, 68.88, 5.0
2019-02-14 17:30:45,422 : Image to text: 25.2, 57.3, 74.1, 4.0
2019-02-14 17:30:45,758 : Text to Image: 20.96, 51.24, 67.94, 5.0
2019-02-14 17:30:46,196 : Image to text: 22.5, 55.8, 70.7, 4.0
2019-02-14 17:30:46,525 : Text to Image: 18.42, 50.08, 67.56, 5.0
2019-02-14 17:30:46,525 : Dev mean Text to Image: 19.432000000000002, 50.744, 67.968, 5.2
2019-02-14 17:30:46,525 : Dev mean Image to text: 23.84, 56.88000000000001, 72.80000000000001, 4.2
2019-02-14 17:30:46,526 : start epoch
2019-02-14 17:31:28,234 : samples : 64000
2019-02-14 17:31:38,289 : Image to text: 9.5, 28.16, 40.54, 16.0
2019-02-14 17:31:45,574 : Text to Image: 7.768, 23.992, 35.616, 20.0
2019-02-14 17:32:28,096 : samples : 128000
2019-02-14 17:32:38,160 : Image to text: 9.5, 27.76, 39.52, 18.0
2019-02-14 17:32:45,486 : Text to Image: 7.196, 22.884, 34.264, 22.0
2019-02-14 17:33:27,579 : samples : 192000
2019-02-14 17:33:37,587 : Image to text: 10.18, 28.82, 41.24, 16.0
2019-02-14 17:33:44,910 : Text to Image: 8.308, 24.956, 36.84, 20.0
2019-02-14 17:34:24,472 : samples : 256000
2019-02-14 17:34:34,495 : Image to text: 8.88, 27.44, 38.74, 17.0
2019-02-14 17:34:41,783 : Text to Image: 7.384, 23.0, 34.352, 22.0
2019-02-14 17:35:21,434 : samples : 320000
2019-02-14 17:35:31,745 : Image to text: 8.92, 26.4, 39.18, 17.0
2019-02-14 17:35:39,141 : Text to Image: 7.58, 23.284, 35.024, 21.0
2019-02-14 17:36:23,088 : samples : 384000
2019-02-14 17:36:33,437 : Image to text: 9.2, 27.44, 40.5, 17.0
2019-02-14 17:36:40,823 : Text to Image: 7.764, 23.644, 35.224, 21.0
2019-02-14 17:37:21,644 : samples : 448000
2019-02-14 17:37:32,002 : Image to text: 9.58, 27.9, 40.98, 16.0
2019-02-14 17:37:39,418 : Text to Image: 7.764, 24.392, 36.272, 20.0
2019-02-14 17:38:19,798 : samples : 512000
2019-02-14 17:38:30,096 : Image to text: 10.04, 28.58, 40.54, 16.0
2019-02-14 17:38:37,486 : Text to Image: 7.78, 23.616, 35.368, 21.0
2019-02-14 17:39:12,532 : Epoch 7 finished
2019-02-14 17:39:12,961 : Image to text: 25.0, 58.7, 72.9, 4.0
2019-02-14 17:39:13,274 : Text to Image: 20.12, 52.88, 69.74, 5.0
2019-02-14 17:39:13,700 : Image to text: 24.4, 59.6, 73.6, 4.0
2019-02-14 17:39:14,036 : Text to Image: 20.04, 52.74, 70.6, 5.0
2019-02-14 17:39:14,497 : Image to text: 26.7, 59.7, 74.9, 4.0
2019-02-14 17:39:14,832 : Text to Image: 21.02, 53.24, 70.92, 5.0
2019-02-14 17:39:15,275 : Image to text: 24.7, 57.5, 74.3, 4.0
2019-02-14 17:39:15,608 : Text to Image: 20.42, 53.68, 71.0, 5.0
2019-02-14 17:39:16,053 : Image to text: 23.0, 56.7, 72.3, 4.0
2019-02-14 17:39:16,384 : Text to Image: 20.4, 52.3, 69.7, 5.0
2019-02-14 17:39:16,385 : Dev mean Text to Image: 20.4, 52.968, 70.392, 5.0
2019-02-14 17:39:16,385 : Dev mean Image to text: 24.759999999999998, 58.44, 73.6, 4.0
2019-02-14 17:39:16,385 : start epoch
2019-02-14 17:39:56,033 : samples : 64000
2019-02-14 17:40:06,093 : Image to text: 9.04, 26.78, 38.76, 18.0
2019-02-14 17:40:13,367 : Text to Image: 7.14, 22.736, 34.428, 22.0
2019-02-14 17:40:54,742 : samples : 128000
2019-02-14 17:41:04,781 : Image to text: 9.8, 28.7, 40.62, 16.0
2019-02-14 17:41:12,114 : Text to Image: 7.988, 24.296, 36.004, 20.0
2019-02-14 17:41:52,654 : samples : 192000
2019-02-14 17:42:02,662 : Image to text: 10.46, 29.52, 41.6, 15.0
2019-02-14 17:42:09,951 : Text to Image: 7.904, 24.524, 36.372, 20.0
2019-02-14 17:42:49,720 : samples : 256000
2019-02-14 17:42:59,718 : Image to text: 8.7, 27.44, 39.76, 17.0
2019-02-14 17:43:07,040 : Text to Image: 7.64, 24.08, 36.056, 20.0
2019-02-14 17:43:46,873 : samples : 320000
2019-02-14 17:43:57,203 : Image to text: 10.7, 29.54, 42.36, 15.0
2019-02-14 17:44:04,626 : Text to Image: 8.236, 24.808, 36.744, 19.0
2019-02-14 17:44:45,127 : samples : 384000
2019-02-14 17:44:55,503 : Image to text: 10.42, 29.14, 42.14, 15.0
2019-02-14 17:45:02,938 : Text to Image: 8.16, 24.412, 36.484, 19.0
2019-02-14 17:45:44,558 : samples : 448000
2019-02-14 17:45:54,976 : Image to text: 10.14, 29.52, 42.22, 15.0
2019-02-14 17:46:02,420 : Text to Image: 8.264, 24.724, 36.64, 19.0
2019-02-14 17:46:42,975 : samples : 512000
2019-02-14 17:46:53,350 : Image to text: 9.4, 28.22, 41.4, 16.0
2019-02-14 17:47:00,802 : Text to Image: 7.844, 23.988, 35.704, 20.0
2019-02-14 17:47:34,892 : Epoch 8 finished
2019-02-14 17:47:35,322 : Image to text: 25.7, 59.0, 74.0, 4.0
2019-02-14 17:47:35,636 : Text to Image: 19.74, 51.48, 70.22, 5.0
2019-02-14 17:47:36,063 : Image to text: 24.3, 56.8, 73.3, 4.0
2019-02-14 17:47:36,378 : Text to Image: 17.98, 51.42, 69.4, 5.0
2019-02-14 17:47:36,819 : Image to text: 26.3, 59.7, 74.6, 4.0
2019-02-14 17:47:37,156 : Text to Image: 20.54, 51.98, 69.18, 5.0
2019-02-14 17:47:37,615 : Image to text: 25.5, 59.6, 74.7, 4.0
2019-02-14 17:47:37,950 : Text to Image: 21.28, 52.16, 69.24, 5.0
2019-02-14 17:47:38,392 : Image to text: 22.9, 58.4, 73.6, 4.0
2019-02-14 17:47:38,727 : Text to Image: 19.76, 51.94, 68.82, 5.0
2019-02-14 17:47:38,727 : Dev mean Text to Image: 19.86, 51.796, 69.372, 5.0
2019-02-14 17:47:38,727 : Dev mean Image to text: 24.939999999999998, 58.7, 74.03999999999999, 4.0
2019-02-14 17:47:38,727 : start epoch
2019-02-14 17:48:18,634 : samples : 64000
2019-02-14 17:48:28,658 : Image to text: 9.36, 27.5, 40.82, 16.0
2019-02-14 17:48:35,945 : Text to Image: 7.832, 23.952, 35.512, 21.0
2019-02-14 17:49:19,102 : samples : 128000
2019-02-14 17:49:29,151 : Image to text: 9.34, 28.24, 41.32, 15.0
2019-02-14 17:49:36,469 : Text to Image: 7.976, 24.168, 36.308, 20.0
2019-02-14 17:50:19,313 : samples : 192000
2019-02-14 17:50:29,328 : Image to text: 9.36, 28.1, 40.94, 16.0
2019-02-14 17:50:36,613 : Text to Image: 8.04, 24.74, 36.5, 20.0
2019-02-14 17:51:18,723 : samples : 256000
2019-02-14 17:51:28,737 : Image to text: 9.88, 27.5, 40.9, 16.0
2019-02-14 17:51:36,022 : Text to Image: 7.816, 24.424, 36.312, 20.0
2019-02-14 17:52:18,461 : samples : 320000
2019-02-14 17:52:28,804 : Image to text: 10.14, 28.78, 40.58, 16.0
2019-02-14 17:52:36,215 : Text to Image: 7.86, 24.248, 35.444, 20.0
2019-02-14 17:53:18,268 : samples : 384000
2019-02-14 17:53:28,605 : Image to text: 9.6, 27.84, 41.28, 16.0
2019-02-14 17:53:36,043 : Text to Image: 7.66, 24.048, 35.86, 20.0
2019-02-14 17:54:17,430 : samples : 448000
2019-02-14 17:54:27,836 : Image to text: 10.16, 29.18, 41.12, 16.0
2019-02-14 17:54:35,282 : Text to Image: 7.788, 24.856, 36.44, 20.0
2019-02-14 17:55:17,337 : samples : 512000
2019-02-14 17:55:27,714 : Image to text: 9.48, 28.26, 40.86, 16.0
2019-02-14 17:55:35,135 : Text to Image: 7.484, 23.196, 34.816, 21.0
2019-02-14 17:56:09,772 : Epoch 9 finished
2019-02-14 17:56:10,209 : Image to text: 26.3, 60.2, 74.9, 4.0
2019-02-14 17:56:10,540 : Text to Image: 22.1, 55.56, 72.22, 4.0
2019-02-14 17:56:10,981 : Image to text: 26.8, 60.4, 74.4, 4.0
2019-02-14 17:56:11,311 : Text to Image: 21.68, 53.9, 71.74, 5.0
2019-02-14 17:56:11,739 : Image to text: 30.0, 62.7, 77.7, 3.0
2019-02-14 17:56:12,069 : Text to Image: 22.72, 56.08, 72.04, 4.0
2019-02-14 17:56:12,515 : Image to text: 26.1, 60.9, 75.5, 4.0
2019-02-14 17:56:12,846 : Text to Image: 22.42, 54.62, 71.28, 5.0
2019-02-14 17:56:13,292 : Image to text: 26.1, 59.0, 73.1, 4.0
2019-02-14 17:56:13,621 : Text to Image: 22.5, 54.64, 71.14, 5.0
2019-02-14 17:56:13,621 : Dev mean Text to Image: 22.284, 54.959999999999994, 71.684, 4.6
2019-02-14 17:56:13,621 : Dev mean Image to text: 27.060000000000002, 60.64, 75.12, 3.8
2019-02-14 17:56:13,621 : start epoch
2019-02-14 17:56:55,309 : samples : 64000
2019-02-14 17:57:05,264 : Image to text: 9.58, 28.18, 40.8, 16.0
2019-02-14 17:57:12,530 : Text to Image: 8.064, 24.4, 36.472, 20.0
2019-02-14 17:57:54,016 : samples : 128000
2019-02-14 17:58:03,962 : Image to text: 9.9, 29.1, 41.9, 15.0
2019-02-14 17:58:11,198 : Text to Image: 8.704, 25.756, 37.688, 19.0
2019-02-14 17:58:50,611 : samples : 192000
2019-02-14 17:59:00,611 : Image to text: 10.64, 29.86, 42.26, 15.0
2019-02-14 17:59:07,935 : Text to Image: 8.164, 25.08, 36.712, 20.0
2019-02-14 17:59:48,233 : samples : 256000
2019-02-14 17:59:58,174 : Image to text: 9.68, 29.28, 42.22, 15.0
2019-02-14 18:00:05,423 : Text to Image: 7.764, 23.664, 35.024, 21.0
2019-02-14 18:00:50,138 : samples : 320000
2019-02-14 18:01:00,409 : Image to text: 10.0, 28.2, 41.82, 15.0
2019-02-14 18:01:07,759 : Text to Image: 7.764, 23.916, 35.416, 21.0
2019-02-14 18:01:47,407 : samples : 384000
2019-02-14 18:01:57,674 : Image to text: 9.5, 29.52, 42.16, 16.0
2019-02-14 18:02:05,052 : Text to Image: 7.948, 24.4, 36.444, 19.0
2019-02-14 18:02:44,812 : samples : 448000
2019-02-14 18:02:55,093 : Image to text: 9.96, 29.58, 41.44, 16.0
2019-02-14 18:03:02,474 : Text to Image: 8.084, 24.268, 35.908, 20.0
2019-02-14 18:03:45,794 : samples : 512000
2019-02-14 18:03:56,080 : Image to text: 10.1, 29.34, 41.8, 16.0
2019-02-14 18:04:03,450 : Text to Image: 8.24, 25.192, 37.14, 19.0
2019-02-14 18:04:37,280 : Epoch 10 finished
2019-02-14 18:04:37,723 : Image to text: 26.7, 60.3, 74.9, 4.0
2019-02-14 18:04:38,070 : Text to Image: 21.84, 55.84, 73.04, 5.0
2019-02-14 18:04:38,506 : Image to text: 25.9, 59.1, 73.4, 4.0
2019-02-14 18:04:38,835 : Text to Image: 21.34, 53.52, 70.64, 5.0
2019-02-14 18:04:39,279 : Image to text: 27.8, 63.1, 77.9, 4.0
2019-02-14 18:04:39,609 : Text to Image: 22.26, 54.78, 71.46, 5.0
2019-02-14 18:04:40,040 : Image to text: 27.5, 59.8, 74.9, 4.0
2019-02-14 18:04:40,368 : Text to Image: 22.66, 53.52, 71.36, 5.0
2019-02-14 18:04:40,808 : Image to text: 24.3, 58.6, 73.0, 4.0
2019-02-14 18:04:41,136 : Text to Image: 22.04, 54.12, 71.42, 5.0
2019-02-14 18:04:41,136 : Dev mean Text to Image: 22.028, 54.356, 71.584, 5.0
2019-02-14 18:04:41,136 : Dev mean Image to text: 26.439999999999998, 60.18, 74.82000000000001, 4.0
2019-02-14 18:04:41,136 : start epoch
2019-02-14 18:05:20,618 : samples : 64000
2019-02-14 18:05:30,574 : Image to text: 10.36, 30.22, 42.64, 15.0
2019-02-14 18:05:37,880 : Text to Image: 8.964, 26.48, 38.6, 18.0
2019-02-14 18:06:18,382 : samples : 128000
2019-02-14 18:06:28,325 : Image to text: 10.76, 29.96, 42.66, 15.0
2019-02-14 18:06:35,588 : Text to Image: 8.16, 25.216, 37.088, 19.0
2019-02-14 18:07:17,330 : samples : 192000
2019-02-14 18:07:27,236 : Image to text: 9.6, 28.48, 41.3, 16.0
2019-02-14 18:07:34,534 : Text to Image: 7.836, 24.504, 36.2, 20.0
2019-02-14 18:08:13,869 : samples : 256000
2019-02-14 18:08:23,864 : Image to text: 10.56, 29.8, 42.24, 15.0
2019-02-14 18:08:31,113 : Text to Image: 8.572, 25.796, 37.784, 19.0
2019-02-14 18:09:10,303 : samples : 320000
2019-02-14 18:09:20,566 : Image to text: 9.86, 28.94, 42.1, 15.0
2019-02-14 18:09:27,901 : Text to Image: 8.156, 24.876, 36.66, 19.0
2019-02-14 18:10:07,452 : samples : 384000
2019-02-14 18:10:17,715 : Image to text: 10.2, 30.2, 43.3, 14.0
2019-02-14 18:10:25,067 : Text to Image: 8.528, 25.54, 37.772, 19.0
2019-02-14 18:11:05,161 : samples : 448000
2019-02-14 18:11:15,441 : Image to text: 10.34, 29.66, 42.62, 15.0
2019-02-14 18:11:22,806 : Text to Image: 7.856, 24.876, 37.032, 19.0
2019-02-14 18:12:02,603 : samples : 512000
2019-02-14 18:12:12,888 : Image to text: 10.6, 30.12, 41.38, 16.0
2019-02-14 18:12:20,256 : Text to Image: 8.036, 24.532, 36.584, 19.0
2019-02-14 18:12:54,980 : Epoch 11 finished
2019-02-14 18:12:55,422 : Image to text: 25.7, 58.5, 75.3, 4.0
2019-02-14 18:12:55,757 : Text to Image: 21.8, 55.82, 74.06, 4.0
2019-02-14 18:12:56,196 : Image to text: 26.6, 59.0, 73.3, 4.0
2019-02-14 18:12:56,538 : Text to Image: 21.0, 54.92, 71.74, 5.0
2019-02-14 18:12:56,968 : Image to text: 27.3, 60.3, 74.9, 3.0
2019-02-14 18:12:57,296 : Text to Image: 23.26, 55.1, 71.96, 4.0
2019-02-14 18:12:57,735 : Image to text: 25.7, 61.4, 75.8, 3.0
2019-02-14 18:12:58,063 : Text to Image: 22.46, 55.6, 71.52, 4.0
2019-02-14 18:12:58,489 : Image to text: 27.7, 58.9, 73.9, 4.0
2019-02-14 18:12:58,816 : Text to Image: 22.28, 55.32, 72.3, 4.0
2019-02-14 18:12:58,816 : Dev mean Text to Image: 22.16, 55.352, 72.31599999999999, 4.2
2019-02-14 18:12:58,816 : Dev mean Image to text: 26.6, 59.620000000000005, 74.64, 3.6000000000000005
2019-02-14 18:12:58,816 : start epoch
2019-02-14 18:13:40,981 : samples : 64000
2019-02-14 18:13:50,945 : Image to text: 9.76, 28.76, 42.22, 15.0
2019-02-14 18:13:58,182 : Text to Image: 8.388, 25.712, 38.084, 18.0
2019-02-14 18:14:43,148 : samples : 128000
2019-02-14 18:14:53,090 : Image to text: 10.64, 29.34, 42.68, 15.0
2019-02-14 18:15:00,338 : Text to Image: 7.88, 24.2, 35.924, 20.0
2019-02-14 18:15:40,012 : samples : 192000
2019-02-14 18:15:49,986 : Image to text: 10.72, 30.06, 41.94, 15.0
2019-02-14 18:15:57,225 : Text to Image: 8.824, 26.368, 38.736, 17.0
2019-02-14 18:16:38,459 : samples : 256000
2019-02-14 18:16:48,393 : Image to text: 10.68, 29.68, 42.84, 15.0
2019-02-14 18:16:55,689 : Text to Image: 8.544, 25.32, 37.244, 19.0
2019-02-14 18:17:38,407 : samples : 320000
2019-02-14 18:17:48,686 : Image to text: 10.66, 29.4, 42.08, 15.0
2019-02-14 18:17:56,037 : Text to Image: 8.212, 24.692, 36.652, 19.0
2019-02-14 18:18:36,798 : samples : 384000
2019-02-14 18:18:46,972 : Image to text: 10.62, 30.02, 42.48, 15.0
2019-02-14 18:18:54,294 : Text to Image: 8.716, 26.224, 38.636, 18.0
2019-02-14 18:19:35,180 : samples : 448000
2019-02-14 18:19:45,468 : Image to text: 11.08, 31.0, 43.84, 14.0
2019-02-14 18:19:52,869 : Text to Image: 9.012, 26.396, 38.22, 18.0
2019-02-14 18:20:33,260 : samples : 512000
2019-02-14 18:20:43,455 : Image to text: 10.4, 30.02, 42.36, 15.0
2019-02-14 18:20:50,857 : Text to Image: 8.532, 26.14, 38.272, 18.0
2019-02-14 18:21:25,341 : Epoch 12 finished
2019-02-14 18:21:25,766 : Image to text: 25.5, 58.0, 73.2, 4.0
2019-02-14 18:21:26,076 : Text to Image: 21.18, 55.4, 72.68, 5.0
2019-02-14 18:21:26,500 : Image to text: 27.5, 60.3, 73.5, 4.0
2019-02-14 18:21:26,808 : Text to Image: 21.3, 54.72, 72.06, 5.0
2019-02-14 18:21:27,235 : Image to text: 28.6, 62.8, 76.0, 3.0
2019-02-14 18:21:27,569 : Text to Image: 21.24, 56.58, 72.68, 4.0
2019-02-14 18:21:28,012 : Image to text: 25.7, 61.5, 76.1, 4.0
2019-02-14 18:21:28,344 : Text to Image: 21.84, 54.62, 71.86, 5.0
2019-02-14 18:21:28,793 : Image to text: 25.2, 58.9, 74.1, 4.0
2019-02-14 18:21:29,123 : Text to Image: 21.28, 53.72, 71.14, 5.0
2019-02-14 18:21:29,123 : Dev mean Text to Image: 21.368, 55.008, 72.084, 4.8
2019-02-14 18:21:29,123 : Dev mean Image to text: 26.5, 60.3, 74.58, 3.8
2019-02-14 18:21:32,948 : 
Test scores | Image to text:             26.519999999999996, 60.13999999999999, 75.7, 3.8
2019-02-14 18:21:32,948 : Test scores | Text to image:             22.036, 54.8, 71.496, 4.6

2019-02-14 18:21:33,042 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 18:21:33,408 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 18:21:34,074 : loading BERT model bert-base-uncased
2019-02-14 18:21:34,074 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:21:34,106 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:21:34,106 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpv09kah7s
2019-02-14 18:21:36,546 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:21:37,983 : Computing embeddings for train/dev/test
2019-02-14 18:23:11,786 : Computed embeddings
2019-02-14 18:23:11,786 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:23:38,367 : [('reg:1e-05', 63.72), ('reg:0.0001', 58.15), ('reg:0.001', 43.68), ('reg:0.01', 30.26)]
2019-02-14 18:23:38,367 : Validation : best param found is reg = 1e-05 with score             63.72
2019-02-14 18:23:38,367 : Evaluating...
2019-02-14 18:23:46,089 : 
Dev acc : 63.7 Test acc : 64.9 for LENGTH classification

2019-02-14 18:23:46,090 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 18:23:46,436 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 18:23:46,485 : loading BERT model bert-base-uncased
2019-02-14 18:23:46,485 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:23:46,518 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:23:46,518 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpgje4sbrg
2019-02-14 18:23:48,930 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:23:50,357 : Computing embeddings for train/dev/test
2019-02-14 18:25:18,558 : Computed embeddings
2019-02-14 18:25:18,558 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:25:58,110 : [('reg:1e-05', 1.13), ('reg:0.0001', 0.2), ('reg:0.001', 0.13), ('reg:0.01', 0.11)]
2019-02-14 18:25:58,110 : Validation : best param found is reg = 1e-05 with score             1.13
2019-02-14 18:25:58,110 : Evaluating...
2019-02-14 18:26:11,219 : 
Dev acc : 1.1 Test acc : 1.2 for WORDCONTENT classification

2019-02-14 18:26:11,220 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 18:26:11,595 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 18:26:11,664 : loading BERT model bert-base-uncased
2019-02-14 18:26:11,664 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:26:11,768 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:26:11,768 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpohn24by9
2019-02-14 18:26:14,173 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:26:15,590 : Computing embeddings for train/dev/test
2019-02-14 18:27:37,856 : Computed embeddings
2019-02-14 18:27:37,856 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:28:08,842 : [('reg:1e-05', 28.13), ('reg:0.0001', 23.01), ('reg:0.001', 19.56), ('reg:0.01', 18.07)]
2019-02-14 18:28:08,843 : Validation : best param found is reg = 1e-05 with score             28.13
2019-02-14 18:28:08,843 : Evaluating...
2019-02-14 18:28:16,981 : 
Dev acc : 28.1 Test acc : 27.8 for DEPTH classification

2019-02-14 18:28:16,982 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 18:28:17,344 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 18:28:17,406 : loading BERT model bert-base-uncased
2019-02-14 18:28:17,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:28:17,514 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:28:17,515 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ibfz8h0
2019-02-14 18:28:19,903 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:28:21,328 : Computing embeddings for train/dev/test
2019-02-14 18:29:38,659 : Computed embeddings
2019-02-14 18:29:38,659 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:30:17,343 : [('reg:1e-05', 70.19), ('reg:0.0001', 54.43), ('reg:0.001', 26.37), ('reg:0.01', 9.4)]
2019-02-14 18:30:17,343 : Validation : best param found is reg = 1e-05 with score             70.19
2019-02-14 18:30:17,343 : Evaluating...
2019-02-14 18:30:26,974 : 
Dev acc : 70.2 Test acc : 70.3 for TOPCONSTITUENTS classification

2019-02-14 18:30:26,975 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 18:30:27,537 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 18:30:27,607 : loading BERT model bert-base-uncased
2019-02-14 18:30:27,607 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:30:27,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:30:27,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1v_agfjq
2019-02-14 18:30:30,129 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:30:31,584 : Computing embeddings for train/dev/test
2019-02-14 18:31:54,758 : Computed embeddings
2019-02-14 18:31:54,758 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:32:32,256 : [('reg:1e-05', 87.25), ('reg:0.0001', 85.76), ('reg:0.001', 82.94), ('reg:0.01', 60.35)]
2019-02-14 18:32:32,257 : Validation : best param found is reg = 1e-05 with score             87.25
2019-02-14 18:32:32,257 : Evaluating...
2019-02-14 18:32:43,588 : 
Dev acc : 87.2 Test acc : 86.6 for BIGRAMSHIFT classification

2019-02-14 18:32:43,589 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 18:32:44,196 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 18:32:44,266 : loading BERT model bert-base-uncased
2019-02-14 18:32:44,266 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:32:44,301 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:32:44,301 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp50hatlhk
2019-02-14 18:32:46,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:32:48,158 : Computing embeddings for train/dev/test
2019-02-14 18:34:10,178 : Computed embeddings
2019-02-14 18:34:10,178 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:34:37,538 : [('reg:1e-05', 89.83), ('reg:0.0001', 89.15), ('reg:0.001', 85.51), ('reg:0.01', 77.18)]
2019-02-14 18:34:37,538 : Validation : best param found is reg = 1e-05 with score             89.83
2019-02-14 18:34:37,538 : Evaluating...
2019-02-14 18:34:45,084 : 
Dev acc : 89.8 Test acc : 88.3 for TENSE classification

2019-02-14 18:34:45,085 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 18:34:45,543 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 18:34:45,611 : loading BERT model bert-base-uncased
2019-02-14 18:34:45,611 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:34:45,641 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:34:45,641 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmph_9to6yg
2019-02-14 18:34:48,054 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:34:49,509 : Computing embeddings for train/dev/test
2019-02-14 18:36:27,705 : Computed embeddings
2019-02-14 18:36:27,705 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:37:44,491 : [('reg:1e-05', 83.64), ('reg:0.0001', 81.45), ('reg:0.001', 76.16), ('reg:0.01', 71.95)]
2019-02-14 18:37:44,492 : Validation : best param found is reg = 1e-05 with score             83.64
2019-02-14 18:37:44,492 : Evaluating...
2019-02-14 18:37:56,802 : 
Dev acc : 83.6 Test acc : 83.2 for SUBJNUMBER classification

2019-02-14 18:37:56,803 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 18:37:57,209 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 18:37:57,275 : loading BERT model bert-base-uncased
2019-02-14 18:37:57,275 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:37:57,390 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:37:57,390 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpr2k62y84
2019-02-14 18:37:59,833 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:38:01,274 : Computing embeddings for train/dev/test
2019-02-14 18:39:44,807 : Computed embeddings
2019-02-14 18:39:44,807 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:40:51,633 : [('reg:1e-05', 78.73), ('reg:0.0001', 75.48), ('reg:0.001', 68.78), ('reg:0.01', 60.17)]
2019-02-14 18:40:51,633 : Validation : best param found is reg = 1e-05 with score             78.73
2019-02-14 18:40:51,633 : Evaluating...
2019-02-14 18:41:07,387 : 
Dev acc : 78.7 Test acc : 80.0 for OBJNUMBER classification

2019-02-14 18:41:07,388 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 18:41:07,769 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 18:41:07,838 : loading BERT model bert-base-uncased
2019-02-14 18:41:07,838 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:41:07,962 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:41:07,963 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpcy1uz03w
2019-02-14 18:41:10,403 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:41:11,851 : Computing embeddings for train/dev/test
2019-02-14 18:43:11,571 : Computed embeddings
2019-02-14 18:43:11,571 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:44:12,518 : [('reg:1e-05', 62.22), ('reg:0.0001', 61.5), ('reg:0.001', 60.11), ('reg:0.01', 50.19)]
2019-02-14 18:44:12,518 : Validation : best param found is reg = 1e-05 with score             62.22
2019-02-14 18:44:12,518 : Evaluating...
2019-02-14 18:44:22,085 : 
Dev acc : 62.2 Test acc : 62.2 for ODDMANOUT classification

2019-02-14 18:44:22,086 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 18:44:22,676 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 18:44:22,752 : loading BERT model bert-base-uncased
2019-02-14 18:44:22,752 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:44:22,784 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:44:22,784 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2usch7oy
2019-02-14 18:44:25,225 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:44:26,657 : Computing embeddings for train/dev/test
2019-02-14 18:46:22,878 : Computed embeddings
2019-02-14 18:46:22,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:47:17,294 : [('reg:1e-05', 65.55), ('reg:0.0001', 61.5), ('reg:0.001', 50.07), ('reg:0.01', 50.0)]
2019-02-14 18:47:17,294 : Validation : best param found is reg = 1e-05 with score             65.55
2019-02-14 18:47:17,294 : Evaluating...
2019-02-14 18:47:28,472 : 
Dev acc : 65.5 Test acc : 64.1 for COORDINATIONINVERSION classification

2019-02-14 18:47:28,474 : total results: {'STS12': {'MSRpar': {'pearson': (0.27784848350542646, 9.218192451543829e-15), 'spearman': SpearmanrResult(correlation=0.33212047151201324, pvalue=9.023356602326013e-21), 'nsamples': 750}, 'MSRvid': {'pearson': (0.20117930538655004, 2.7406239630503596e-08), 'spearman': SpearmanrResult(correlation=0.24842930138708644, pvalue=5.177050878029563e-12), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.43505508774537544, 1.279677347838663e-22), 'spearman': SpearmanrResult(correlation=0.5432789215399574, pvalue=1.3313973500618152e-36), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.5106953385561811, 4.676175866928071e-51), 'spearman': SpearmanrResult(correlation=0.520968755656903, pvalue=2.0680331760611962e-53), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.6120616482417162, 2.2301472682952138e-42), 'spearman': SpearmanrResult(correlation=0.5164058000083865, pvalue=1.4050636184407346e-28), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.40736797268704983, 'wmean': 0.38165869643168926}, 'spearman': {'mean': 0.4322406500208693, 'wmean': 0.4123390719456849}}}, 'STS13': {'FNWN': {'pearson': (0.1441528019344977, 0.04781885057429608), 'spearman': SpearmanrResult(correlation=0.17130443128296932, pvalue=0.01842959369050109), 'nsamples': 189}, 'headlines': {'pearson': (0.44433785825056343, 1.230777712339271e-37), 'spearman': SpearmanrResult(correlation=0.44613639156630114, pvalue=5.807600480447757e-38), 'nsamples': 750}, 'OnWN': {'pearson': (0.2128991170901366, 3.583896769826679e-07), 'spearman': SpearmanrResult(correlation=0.23672398432717182, pvalue=1.3847373431780777e-08), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.2671299257583992, 'wmean': 0.3199564519607395}, 'spearman': {'mean': 0.28472160239214744, 'wmean': 0.333187324263167}}}, 'STS14': {'deft-forum': {'pearson': (0.11531518424535356, 0.01438228846855131), 'spearman': SpearmanrResult(correlation=0.11449726767986679, pvalue=0.015094868203569279), 'nsamples': 450}, 'deft-news': {'pearson': (0.5886479932575824, 2.306091709633918e-29), 'spearman': SpearmanrResult(correlation=0.5847727700045408, pvalue=6.522421468339562e-29), 'nsamples': 300}, 'headlines': {'pearson': (0.41428808444872056, 1.8188487662881026e-32), 'spearman': SpearmanrResult(correlation=0.3868559967845868, pvalue=3.484703690816567e-28), 'nsamples': 750}, 'images': {'pearson': (0.3334262132536199, 6.236943883076286e-21), 'spearman': SpearmanrResult(correlation=0.34399717094862753, pvalue=2.9362823337952703e-22), 'nsamples': 750}, 'OnWN': {'pearson': (0.405365693863085, 4.975102414990529e-31), 'spearman': SpearmanrResult(correlation=0.4348053588715926, pvalue=6.119886462697438e-36), 'nsamples': 750}, 'tweet-news': {'pearson': (0.5307191621858242, 1.0123110474126817e-55), 'spearman': SpearmanrResult(correlation=0.5018844179923379, pvalue=4.227272356762558e-49), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.3979603885423643, 'wmean': 0.39768949232029893}, 'spearman': {'mean': 0.39446883038025876, 'wmean': 0.39403008264137623}}}, 'STS15': {'answers-forums': {'pearson': (0.4411831908896394, 2.7126365533607015e-19), 'spearman': SpearmanrResult(correlation=0.4162678675909151, pvalue=3.7889148104595377e-17), 'nsamples': 375}, 'answers-students': {'pearson': (0.46429949234107654, 2.282445453779507e-41), 'spearman': SpearmanrResult(correlation=0.4762252556076933, pvalue=1.019122545262546e-43), 'nsamples': 750}, 'belief': {'pearson': (0.5314908294537455, 1.0003815993068027e-28), 'spearman': SpearmanrResult(correlation=0.5614468762021585, pvalue=1.5630485410446553e-32), 'nsamples': 375}, 'headlines': {'pearson': (0.503255768317588, 2.115147330331106e-49), 'spearman': SpearmanrResult(correlation=0.503824784247457, pvalue=1.5854805867995535e-49), 'nsamples': 750}, 'images': {'pearson': (0.39055314150049364, 9.731238325222442e-29), 'spearman': SpearmanrResult(correlation=0.39848019664902096, pvalue=5.9752988351992324e-30), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.46615648450050856, 'wmean': 0.4611113530827126}, 'spearman': {'mean': 0.471248996059449, 'wmean': 0.46684690210017704}}}, 'STS16': {'answer-answer': {'pearson': (0.5456620118839048, 4.150810404416735e-21), 'spearman': SpearmanrResult(correlation=0.5469762409730998, pvalue=3.1996720768202415e-21), 'nsamples': 254}, 'headlines': {'pearson': (0.5382334069646757, 4.2017577523499884e-20), 'spearman': SpearmanrResult(correlation=0.557208674494048, pvalue=1.0369641001863348e-21), 'nsamples': 249}, 'plagiarism': {'pearson': (0.7511387775651408, 5.284432955718461e-43), 'spearman': SpearmanrResult(correlation=0.7667085605058799, pvalue=8.991159676071265e-46), 'nsamples': 230}, 'postediting': {'pearson': (0.7572855097945392, 1.1259318667153825e-46), 'spearman': SpearmanrResult(correlation=0.794530185971821, pvalue=2.3932832708766404e-54), 'nsamples': 244}, 'question-question': {'pearson': (0.1596845879808879, 0.020914306755199105), 'spearman': SpearmanrResult(correlation=0.18968760637241402, pvalue=0.005943992084445739), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.5504008588378297, 'wmean': 0.5594704312568057}, 'spearman': {'mean': 0.5710222536634526, 'wmean': 0.5797048644026109}}}, 'MR': {'devacc': 73.54, 'acc': 74.59, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.97, 'acc': 74.65, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.19, 'acc': 84.89, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.05, 'acc': 92.26, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.65, 'acc': 81.0, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.15, 'acc': 37.96, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 67.23, 'acc': 78.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.6, 'acc': 68.87, 'f1': 76.46, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.0, 'acc': 70.1, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6930347179264527, 'pearson': 0.6928476010637008, 'spearman': 0.6631921667799732, 'mse': 0.5363103855537937, 'yhat': array([2.72160666, 3.96874692, 2.44314905, ..., 2.70767825, 4.29888443,        4.30357378]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6250795058638942, 'pearson': 0.5593898367103933, 'spearman': 0.5514142884288264, 'mse': 1.8727268647617732, 'yhat': array([1.84165192, 2.06797574, 2.88357607, ..., 3.74529505, 3.61948281,        3.40409692]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 61.69, 'acc': 60.95, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 311.74800000000005, 'acc': [(26.519999999999996, 60.13999999999999, 75.7, 3.8), (22.036, 54.8, 71.496, 4.6)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 63.72, 'acc': 64.88, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 1.13, 'acc': 1.22, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.13, 'acc': 27.82, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 70.19, 'acc': 70.28, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.25, 'acc': 86.6, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.83, 'acc': 88.32, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.64, 'acc': 83.21, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 78.73, 'acc': 79.97, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 62.22, 'acc': 62.22, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 65.55, 'acc': 64.1, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 18:47:28,474 : STS12 p=0.3817, STS12 s=0.4123, STS13 p=0.3200, STS13 s=0.3332, STS14 p=0.3977, STS14 s=0.3940, STS15 p=0.4611, STS15 s=0.4668, STS 16 p=0.5595, STS16 s=0.5797, STS B p=0.5594, STS B s=0.5514, STS B m=1.8727, SICK-R p=0.6928, SICK-R s=0.6632, SICK-P m=0.5363
2019-02-14 18:47:28,475 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 18:47:28,475 : 0.3817,0.4123,0.3200,0.3332,0.3977,0.3940,0.4611,0.4668,0.5595,0.5797,0.5594,0.5514,1.8727,0.6928,0.6632,0.5363
2019-02-14 18:47:28,475 : MR=74.59, CR=74.65, SUBJ=92.26, MPQA=84.89, SST-B=81.00, SST-F=37.96, TREC=78.20, SICK-E=70.10, SNLI=60.95, MRPC=68.87, MRPC f=76.46
2019-02-14 18:47:28,475 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 18:47:28,475 : 74.59,74.65,92.26,84.89,81.00,37.96,78.20,70.10,60.95,68.87,76.46
2019-02-14 18:47:28,475 : COCO r1i2t=26.52, COCO r5i2t=60.14, COCO r10i2t=75.70, COCO medr_i2t=3.80, COCO r1t2i=22.04, COCO r5t2i=54.80, COCO r10t2i=71.50, COCO medr_t2i=4.60
2019-02-14 18:47:28,475 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 18:47:28,475 : 26.52,60.14,75.70,3.80,22.04,54.80,71.50,4.60
2019-02-14 18:47:28,475 : SentLen=64.88, WC=1.22, TreeDepth=27.82, TopConst=70.28, BShift=86.60, Tense=88.32, SubjNum=83.21, ObjNum=79.97, SOMO=62.22, CoordInv=64.10, average=62.86
2019-02-14 18:47:28,475 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 18:47:28,475 : 64.88,1.22,27.82,70.28,86.60,88.32,83.21,79.97,62.22,64.10,62.86
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,475 : layer 9
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,475 : ********************************************************************************
2019-02-14 18:47:28,569 : ***** Transfer task : STS12 *****


2019-02-14 18:47:28,582 : loading BERT model bert-base-uncased
2019-02-14 18:47:28,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:47:28,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:47:28,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppxzyhhmk
2019-02-14 18:47:31,130 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:47:34,094 : MSRpar : pearson = 0.2885, spearman = 0.3692
2019-02-14 18:47:34,775 : MSRvid : pearson = 0.1387, spearman = 0.1941
2019-02-14 18:47:35,504 : SMTeuroparl : pearson = 0.2840, spearman = 0.4896
2019-02-14 18:47:36,884 : surprise.OnWN : pearson = 0.2502, spearman = 0.3153
2019-02-14 18:47:37,648 : surprise.SMTnews : pearson = 0.4115, spearman = 0.4841
2019-02-14 18:47:37,648 : ALL (weighted average) : Pearson = 0.2582,             Spearman = 0.3465
2019-02-14 18:47:37,648 : ALL (average) : Pearson = 0.2746,             Spearman = 0.3705

2019-02-14 18:47:37,648 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 18:47:37,656 : loading BERT model bert-base-uncased
2019-02-14 18:47:37,656 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:47:37,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:47:37,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpccvj7cyh
2019-02-14 18:47:40,124 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:47:42,257 : FNWN : pearson = 0.0832, spearman = 0.0689
2019-02-14 18:47:43,477 : headlines : pearson = 0.1277, spearman = 0.3777
2019-02-14 18:47:44,490 : OnWN : pearson = 0.0107, spearman = 0.0633
2019-02-14 18:47:44,490 : ALL (weighted average) : Pearson = 0.0783,             Spearman = 0.2212
2019-02-14 18:47:44,490 : ALL (average) : Pearson = 0.0739,             Spearman = 0.1700

2019-02-14 18:47:44,490 : ***** Transfer task : STS14 *****


2019-02-14 18:47:44,506 : loading BERT model bert-base-uncased
2019-02-14 18:47:44,506 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:47:44,524 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:47:44,524 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4p0f7snm
2019-02-14 18:47:46,980 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:47:49,431 : deft-forum : pearson = 0.0894, spearman = 0.0987
2019-02-14 18:47:50,296 : deft-news : pearson = 0.2023, spearman = 0.4558
2019-02-14 18:47:51,723 : headlines : pearson = 0.2415, spearman = 0.3642
2019-02-14 18:47:53,102 : images : pearson = 0.0865, spearman = 0.2029
2019-02-14 18:47:54,518 : OnWN : pearson = 0.1615, spearman = 0.2611
2019-02-14 18:47:56,174 : tweet-news : pearson = 0.3022, spearman = 0.3834
2019-02-14 18:47:56,174 : ALL (weighted average) : Pearson = 0.1853,             Spearman = 0.2906
2019-02-14 18:47:56,174 : ALL (average) : Pearson = 0.1806,             Spearman = 0.2943

2019-02-14 18:47:56,174 : ***** Transfer task : STS15 *****


2019-02-14 18:47:56,207 : loading BERT model bert-base-uncased
2019-02-14 18:47:56,207 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:47:56,224 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:47:56,225 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0zpkom24
2019-02-14 18:47:58,619 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:48:01,204 : answers-forums : pearson = 0.1603, spearman = 0.2573
2019-02-14 18:48:02,636 : answers-students : pearson = 0.0207, spearman = 0.1832
2019-02-14 18:48:03,693 : belief : pearson = 0.2499, spearman = 0.3967
2019-02-14 18:48:05,225 : headlines : pearson = 0.2135, spearman = 0.4600
2019-02-14 18:48:06,715 : images : pearson = 0.0869, spearman = 0.1869
2019-02-14 18:48:06,716 : ALL (weighted average) : Pearson = 0.1316,             Spearman = 0.2893
2019-02-14 18:48:06,716 : ALL (average) : Pearson = 0.1463,             Spearman = 0.2968

2019-02-14 18:48:06,716 : ***** Transfer task : STS16 *****


2019-02-14 18:48:06,785 : loading BERT model bert-base-uncased
2019-02-14 18:48:06,785 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:48:06,803 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:48:06,803 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmponj23jjz
2019-02-14 18:48:09,200 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:48:11,270 : answer-answer : pearson = 0.3420, spearman = 0.4457
2019-02-14 18:48:11,767 : headlines : pearson = 0.4406, spearman = 0.5409
2019-02-14 18:48:12,319 : plagiarism : pearson = 0.2898, spearman = 0.6065
2019-02-14 18:48:13,074 : postediting : pearson = 0.4699, spearman = 0.6645
2019-02-14 18:48:13,556 : question-question : pearson = 0.0579, spearman = 0.1952
2019-02-14 18:48:13,556 : ALL (weighted average) : Pearson = 0.3288,             Spearman = 0.4977
2019-02-14 18:48:13,556 : ALL (average) : Pearson = 0.3200,             Spearman = 0.4906

2019-02-14 18:48:13,556 : ***** Transfer task : MR *****


2019-02-14 18:48:13,573 : loading BERT model bert-base-uncased
2019-02-14 18:48:13,573 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:48:13,594 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:48:13,594 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7byngni0
2019-02-14 18:48:16,001 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:48:17,577 : Generating sentence embeddings
2019-02-14 18:48:34,408 : Generated sentence embeddings
2019-02-14 18:48:34,409 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 18:48:57,599 : Best param found at split 1: l2reg = 1e-05                 with score 77.57
2019-02-14 18:49:18,872 : Best param found at split 2: l2reg = 1e-05                 with score 77.74
2019-02-14 18:49:33,714 : Best param found at split 3: l2reg = 0.0001                 with score 78.39
2019-02-14 18:49:47,740 : Best param found at split 4: l2reg = 1e-05                 with score 77.1
2019-02-14 18:50:01,520 : Best param found at split 5: l2reg = 0.0001                 with score 78.08
2019-02-14 18:50:02,403 : Dev acc : 77.78 Test acc : 77.52

2019-02-14 18:50:02,404 : ***** Transfer task : CR *****


2019-02-14 18:50:02,411 : loading BERT model bert-base-uncased
2019-02-14 18:50:02,411 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:50:02,431 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:50:02,431 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmppju_1duf
2019-02-14 18:50:04,865 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:50:06,311 : Generating sentence embeddings
2019-02-14 18:50:10,468 : Generated sentence embeddings
2019-02-14 18:50:10,468 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 18:50:15,187 : Best param found at split 1: l2reg = 1e-05                 with score 84.63
2019-02-14 18:50:21,162 : Best param found at split 2: l2reg = 0.0001                 with score 84.56
2019-02-14 18:50:30,758 : Best param found at split 3: l2reg = 0.0001                 with score 84.83
2019-02-14 18:50:38,240 : Best param found at split 4: l2reg = 1e-05                 with score 84.38
2019-02-14 18:50:47,730 : Best param found at split 5: l2reg = 1e-05                 with score 84.31
2019-02-14 18:50:48,297 : Dev acc : 84.54 Test acc : 82.33

2019-02-14 18:50:48,297 : ***** Transfer task : MPQA *****


2019-02-14 18:50:48,302 : loading BERT model bert-base-uncased
2019-02-14 18:50:48,302 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:50:48,322 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:50:48,322 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8ufaz0ti
2019-02-14 18:50:50,766 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:50:52,300 : Generating sentence embeddings
2019-02-14 18:51:01,278 : Generated sentence embeddings
2019-02-14 18:51:01,279 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 18:51:20,623 : Best param found at split 1: l2reg = 1e-05                 with score 85.5
2019-02-14 18:51:45,272 : Best param found at split 2: l2reg = 1e-05                 with score 84.91
2019-02-14 18:52:03,550 : Best param found at split 3: l2reg = 0.0001                 with score 85.7
2019-02-14 18:52:18,125 : Best param found at split 4: l2reg = 0.0001                 with score 86.69
2019-02-14 18:52:32,413 : Best param found at split 5: l2reg = 1e-05                 with score 85.95
2019-02-14 18:52:33,478 : Dev acc : 85.75 Test acc : 85.85

2019-02-14 18:52:33,479 : ***** Transfer task : SUBJ *****


2019-02-14 18:52:33,493 : loading BERT model bert-base-uncased
2019-02-14 18:52:33,493 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:52:33,515 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:52:33,515 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpyotqv2pe
2019-02-14 18:52:35,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:52:37,436 : Generating sentence embeddings
2019-02-14 18:52:51,884 : Generated sentence embeddings
2019-02-14 18:52:51,885 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 18:53:16,288 : Best param found at split 1: l2reg = 0.0001                 with score 93.56
2019-02-14 18:53:42,116 : Best param found at split 2: l2reg = 1e-05                 with score 93.5
2019-02-14 18:54:09,355 : Best param found at split 3: l2reg = 1e-05                 with score 93.26
2019-02-14 18:54:28,452 : Best param found at split 4: l2reg = 1e-05                 with score 93.66
2019-02-14 18:54:43,745 : Best param found at split 5: l2reg = 1e-05                 with score 93.42
2019-02-14 18:54:44,480 : Dev acc : 93.48 Test acc : 92.9

2019-02-14 18:54:44,481 : ***** Transfer task : SST Binary classification *****


2019-02-14 18:54:44,609 : loading BERT model bert-base-uncased
2019-02-14 18:54:44,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:54:44,631 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:54:44,631 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj_hdyzc_
2019-02-14 18:54:47,056 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:54:48,507 : Computing embedding for train
2019-02-14 18:55:43,075 : Computed train embeddings
2019-02-14 18:55:43,075 : Computing embedding for dev
2019-02-14 18:55:44,180 : Computed dev embeddings
2019-02-14 18:55:44,180 : Computing embedding for test
2019-02-14 18:55:46,602 : Computed test embeddings
2019-02-14 18:55:46,602 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:56:25,527 : [('reg:1e-05', 83.83), ('reg:0.0001', 83.37), ('reg:0.001', 81.88), ('reg:0.01', 78.78)]
2019-02-14 18:56:25,527 : Validation : best param found is reg = 1e-05 with score             83.83
2019-02-14 18:56:25,527 : Evaluating...
2019-02-14 18:56:37,162 : 
Dev acc : 83.83 Test acc : 82.15 for             SST Binary classification

2019-02-14 18:56:37,162 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 18:56:37,212 : loading BERT model bert-base-uncased
2019-02-14 18:56:37,212 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:56:37,233 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:56:37,233 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7eluue5w
2019-02-14 18:56:39,663 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:56:41,168 : Computing embedding for train
2019-02-14 18:56:53,209 : Computed train embeddings
2019-02-14 18:56:53,209 : Computing embedding for dev
2019-02-14 18:56:54,673 : Computed dev embeddings
2019-02-14 18:56:54,673 : Computing embedding for test
2019-02-14 18:56:57,741 : Computed test embeddings
2019-02-14 18:56:57,741 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:57:02,436 : [('reg:1e-05', 43.32), ('reg:0.0001', 43.87), ('reg:0.001', 41.78), ('reg:0.01', 35.06)]
2019-02-14 18:57:02,436 : Validation : best param found is reg = 0.0001 with score             43.87
2019-02-14 18:57:02,436 : Evaluating...
2019-02-14 18:57:03,141 : 
Dev acc : 43.87 Test acc : 42.26 for             SST Fine-Grained classification

2019-02-14 18:57:03,141 : ***** Transfer task : TREC *****


2019-02-14 18:57:03,155 : loading BERT model bert-base-uncased
2019-02-14 18:57:03,155 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:57:03,174 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:57:03,174 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkhsh9dwg
2019-02-14 18:57:05,644 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:57:11,250 : Computed train embeddings
2019-02-14 18:57:11,582 : Computed test embeddings
2019-02-14 18:57:11,583 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 18:57:21,763 : [('reg:1e-05', 72.41), ('reg:0.0001', 70.54), ('reg:0.001', 52.24), ('reg:0.01', 37.05)]
2019-02-14 18:57:21,763 : Cross-validation : best param found is reg = 1e-05             with score 72.41
2019-02-14 18:57:21,763 : Evaluating...
2019-02-14 18:57:22,547 : 
Dev acc : 72.41 Test acc : 85.2             for TREC

2019-02-14 18:57:22,548 : ***** Transfer task : MRPC *****


2019-02-14 18:57:22,570 : loading BERT model bert-base-uncased
2019-02-14 18:57:22,570 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:57:22,591 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:57:22,591 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphusbzuxq
2019-02-14 18:57:25,030 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:57:26,490 : Computing embedding for train
2019-02-14 18:57:37,159 : Computed train embeddings
2019-02-14 18:57:37,159 : Computing embedding for test
2019-02-14 18:57:41,761 : Computed test embeddings
2019-02-14 18:57:41,777 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 18:57:48,932 : [('reg:1e-05', 71.22), ('reg:0.0001', 71.17), ('reg:0.001', 70.07), ('reg:0.01', 68.13)]
2019-02-14 18:57:48,932 : Cross-validation : best param found is reg = 1e-05             with score 71.22
2019-02-14 18:57:48,932 : Evaluating...
2019-02-14 18:57:49,341 : Dev acc : 71.22 Test acc 72.87; Test F1 80.99 for MRPC.

2019-02-14 18:57:49,342 : ***** Transfer task : SICK-Entailment*****


2019-02-14 18:57:49,406 : loading BERT model bert-base-uncased
2019-02-14 18:57:49,406 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:57:49,425 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:57:49,426 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvrnadnz4
2019-02-14 18:57:51,861 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:57:53,338 : Computing embedding for train
2019-02-14 18:57:59,261 : Computed train embeddings
2019-02-14 18:57:59,261 : Computing embedding for dev
2019-02-14 18:57:59,848 : Computed dev embeddings
2019-02-14 18:57:59,848 : Computing embedding for test
2019-02-14 18:58:05,436 : Computed test embeddings
2019-02-14 18:58:05,463 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 18:58:08,596 : [('reg:1e-05', 71.8), ('reg:0.0001', 71.0), ('reg:0.001', 71.2), ('reg:0.01', 59.6)]
2019-02-14 18:58:08,596 : Validation : best param found is reg = 1e-05 with score             71.8
2019-02-14 18:58:08,596 : Evaluating...
2019-02-14 18:58:09,395 : 
Dev acc : 71.8 Test acc : 72.11 for                        SICK entailment

2019-02-14 18:58:09,396 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 18:58:09,423 : loading BERT model bert-base-uncased
2019-02-14 18:58:09,423 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 18:58:09,478 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 18:58:09,478 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2vjhzhsu
2019-02-14 18:58:11,894 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 18:58:13,406 : Computing embedding for train
2019-02-14 18:58:21,639 : Computed train embeddings
2019-02-14 18:58:21,639 : Computing embedding for dev
2019-02-14 18:58:22,689 : Computed dev embeddings
2019-02-14 18:58:22,689 : Computing embedding for test
2019-02-14 18:58:32,199 : Computed test embeddings
2019-02-14 19:00:41,928 : Dev : Pearson 0.7431423375783016
2019-02-14 19:00:41,928 : Test : Pearson 0.757480991033089 Spearman 0.7077849446591156 MSE 0.4343256655235998                        for SICK Relatedness

2019-02-14 19:00:41,929 : 

***** Transfer task : STSBenchmark*****


2019-02-14 19:00:41,997 : loading BERT model bert-base-uncased
2019-02-14 19:00:41,997 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:00:42,016 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:00:42,016 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5f25iq8a
2019-02-14 19:00:44,452 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:00:45,871 : Computing embedding for train
2019-02-14 19:00:55,380 : Computed train embeddings
2019-02-14 19:00:55,380 : Computing embedding for dev
2019-02-14 19:00:58,642 : Computed dev embeddings
2019-02-14 19:00:58,642 : Computing embedding for test
2019-02-14 19:01:01,485 : Computed test embeddings
2019-02-14 19:02:09,338 : Dev : Pearson 0.6758162693595525
2019-02-14 19:02:09,338 : Test : Pearson 0.6087472917533145 Spearman 0.5951505545651116 MSE 1.77856188379372                        for SICK Relatedness

2019-02-14 19:02:09,339 : ***** Transfer task : SNLI Entailment*****


2019-02-14 19:02:14,025 : loading BERT model bert-base-uncased
2019-02-14 19:02:14,025 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:02:14,148 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:02:14,148 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps45gy36l
2019-02-14 19:02:16,591 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:02:18,444 : PROGRESS (encoding): 0.00%
2019-02-14 19:04:04,756 : PROGRESS (encoding): 14.56%
2019-02-14 19:05:57,416 : PROGRESS (encoding): 29.12%
2019-02-14 19:07:43,077 : PROGRESS (encoding): 43.69%
2019-02-14 19:09:34,833 : PROGRESS (encoding): 58.25%
2019-02-14 19:11:43,137 : PROGRESS (encoding): 72.81%
2019-02-14 19:14:02,472 : PROGRESS (encoding): 87.37%
2019-02-14 19:16:19,652 : PROGRESS (encoding): 0.00%
2019-02-14 19:16:37,076 : PROGRESS (encoding): 0.00%
2019-02-14 19:16:53,657 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 19:17:39,740 : [('reg:1e-09', 66.53)]
2019-02-14 19:17:39,740 : Validation : best param found is reg = 1e-09 with score             66.53
2019-02-14 19:17:39,740 : Evaluating...
2019-02-14 19:18:37,228 : Dev acc : 66.53 Test acc : 66.52 for SNLI

2019-02-14 19:18:37,228 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 19:18:45,828 : loading BERT model bert-base-uncased
2019-02-14 19:18:45,828 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 19:18:45,862 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 19:18:45,862 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg2eha1vb
2019-02-14 19:18:48,261 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 19:18:49,776 : Computing embedding for train
2019-02-14 19:28:57,523 : Computed train embeddings
2019-02-14 19:28:57,523 : Computing embedding for dev
2019-02-14 19:29:23,600 : Computed dev embeddings
2019-02-14 19:29:23,600 : Computing embedding for test
2019-02-14 19:29:50,610 : Computed test embeddings
2019-02-14 19:29:50,625 : prepare data
2019-02-14 19:29:50,687 : start epoch
2019-02-14 19:30:32,197 : samples : 64000
2019-02-14 19:30:44,731 : Image to text: 4.32, 14.64, 23.26, 39.0
2019-02-14 19:30:54,662 : Text to Image: 2.928, 10.908, 18.476, 49.0
2019-02-14 19:31:36,227 : samples : 128000
2019-02-14 19:31:48,751 : Image to text: 6.18, 19.94, 29.82, 28.0
2019-02-14 19:31:58,688 : Text to Image: 4.544, 15.336, 24.972, 34.0
2019-02-14 19:32:40,637 : samples : 192000
2019-02-14 19:32:53,252 : Image to text: 4.96, 18.16, 28.32, 30.0
2019-02-14 19:33:03,223 : Text to Image: 4.376, 14.72, 23.568, 37.0
2019-02-14 19:33:46,497 : samples : 256000
2019-02-14 19:33:59,153 : Image to text: 5.94, 20.24, 30.72, 27.0
2019-02-14 19:34:09,343 : Text to Image: 4.76, 16.156, 25.44, 33.0
2019-02-14 19:34:51,765 : samples : 320000
2019-02-14 19:35:01,863 : Image to text: 6.42, 20.74, 31.44, 26.0
2019-02-14 19:35:08,989 : Text to Image: 5.092, 16.9, 26.676, 31.0
2019-02-14 19:35:49,503 : samples : 384000
2019-02-14 19:36:01,141 : Image to text: 6.18, 20.52, 30.62, 27.0
2019-02-14 19:36:11,500 : Text to Image: 5.02, 17.456, 27.024, 31.0
2019-02-14 19:36:55,120 : samples : 448000
2019-02-14 19:37:07,960 : Image to text: 5.98, 20.94, 31.26, 26.0
2019-02-14 19:37:18,391 : Text to Image: 5.376, 18.044, 27.992, 30.0
2019-02-14 19:38:02,648 : samples : 512000
2019-02-14 19:38:15,488 : Image to text: 7.34, 22.5, 33.3, 23.0
2019-02-14 19:38:25,966 : Text to Image: 5.528, 18.632, 29.28, 28.0
2019-02-14 19:39:03,507 : Epoch 1 finished
2019-02-14 19:39:04,545 : Image to text: 21.2, 51.2, 68.0, 5.0
2019-02-14 19:39:05,422 : Text to Image: 16.94, 45.16, 61.74, 7.0
2019-02-14 19:39:06,477 : Image to text: 21.0, 51.4, 65.7, 5.0
2019-02-14 19:39:07,294 : Text to Image: 16.76, 45.6, 63.06, 6.0
2019-02-14 19:39:08,377 : Image to text: 23.2, 55.0, 70.7, 5.0
2019-02-14 19:39:09,270 : Text to Image: 16.58, 45.8, 63.22, 7.0
2019-02-14 19:39:10,302 : Image to text: 20.1, 51.9, 67.5, 5.0
2019-02-14 19:39:11,167 : Text to Image: 15.52, 44.86, 62.34, 7.0
2019-02-14 19:39:12,242 : Image to text: 20.8, 51.3, 67.5, 5.0
2019-02-14 19:39:13,164 : Text to Image: 17.02, 46.18, 62.54, 6.0
2019-02-14 19:39:13,164 : Dev mean Text to Image: 16.564, 45.519999999999996, 62.58, 6.6
2019-02-14 19:39:13,164 : Dev mean Image to text: 21.26, 52.16, 67.88, 5.0
2019-02-14 19:39:13,164 : start epoch
2019-02-14 19:39:58,071 : samples : 64000
2019-02-14 19:40:10,922 : Image to text: 7.9, 24.88, 35.86, 20.0
2019-02-14 19:40:21,323 : Text to Image: 6.236, 20.336, 31.096, 25.0
2019-02-14 19:41:06,217 : samples : 128000
2019-02-14 19:41:19,144 : Image to text: 7.52, 24.0, 36.28, 21.0
2019-02-14 19:41:29,617 : Text to Image: 6.388, 20.864, 31.88, 24.0
2019-02-14 19:42:14,049 : samples : 192000
2019-02-14 19:42:26,900 : Image to text: 7.48, 23.74, 35.3, 21.0
2019-02-14 19:42:37,252 : Text to Image: 6.3, 20.58, 31.216, 25.0
2019-02-14 19:43:21,687 : samples : 256000
2019-02-14 19:43:34,412 : Image to text: 7.76, 24.08, 35.86, 20.0
2019-02-14 19:43:44,727 : Text to Image: 6.612, 21.136, 32.172, 24.0
2019-02-14 19:44:27,076 : samples : 320000
2019-02-14 19:44:37,330 : Image to text: 7.88, 24.84, 37.02, 19.0
2019-02-14 19:44:44,747 : Text to Image: 6.308, 20.528, 31.012, 25.0
2019-02-14 19:45:24,764 : samples : 384000
2019-02-14 19:45:35,074 : Image to text: 7.44, 24.34, 35.48, 21.0
2019-02-14 19:45:42,469 : Text to Image: 6.336, 20.18, 31.24, 25.0
2019-02-14 19:46:23,778 : samples : 448000
2019-02-14 19:46:34,084 : Image to text: 7.92, 24.36, 35.82, 21.0
2019-02-14 19:46:41,478 : Text to Image: 6.488, 21.228, 32.548, 23.0
2019-02-14 19:47:21,283 : samples : 512000
2019-02-14 19:47:31,603 : Image to text: 8.28, 25.64, 37.58, 19.0
2019-02-14 19:47:39,001 : Text to Image: 6.804, 21.4, 32.292, 24.0
2019-02-14 19:48:13,570 : Epoch 2 finished
2019-02-14 19:48:13,994 : Image to text: 22.9, 55.0, 72.0, 5.0
2019-02-14 19:48:14,317 : Text to Image: 20.58, 52.0, 68.54, 5.0
2019-02-14 19:48:14,735 : Image to text: 23.2, 53.6, 69.3, 5.0
2019-02-14 19:48:15,059 : Text to Image: 19.7, 51.84, 69.5, 5.0
2019-02-14 19:48:15,493 : Image to text: 25.0, 57.0, 74.2, 4.0
2019-02-14 19:48:15,817 : Text to Image: 20.7, 51.72, 68.34, 5.0
2019-02-14 19:48:16,240 : Image to text: 24.0, 54.6, 69.4, 4.0
2019-02-14 19:48:16,565 : Text to Image: 19.68, 51.72, 69.2, 5.0
2019-02-14 19:48:16,997 : Image to text: 23.4, 56.6, 73.0, 4.0
2019-02-14 19:48:17,320 : Text to Image: 20.52, 51.26, 69.06, 5.0
2019-02-14 19:48:17,320 : Dev mean Text to Image: 20.235999999999997, 51.708, 68.928, 5.0
2019-02-14 19:48:17,320 : Dev mean Image to text: 23.7, 55.36, 71.58, 4.3999999999999995
2019-02-14 19:48:17,321 : start epoch
2019-02-14 19:48:57,076 : samples : 64000
2019-02-14 19:49:07,406 : Image to text: 9.16, 26.5, 38.06, 18.0
2019-02-14 19:49:14,765 : Text to Image: 7.412, 22.872, 34.36, 22.0
2019-02-14 19:49:55,566 : samples : 128000
2019-02-14 19:50:05,886 : Image to text: 7.7, 23.7, 35.1, 21.0
2019-02-14 19:50:13,246 : Text to Image: 6.172, 20.668, 31.792, 24.0
2019-02-14 19:50:53,651 : samples : 192000
2019-02-14 19:51:03,972 : Image to text: 8.68, 25.68, 38.0, 18.0
2019-02-14 19:51:11,337 : Text to Image: 7.172, 22.644, 33.944, 22.0
2019-02-14 19:51:51,978 : samples : 256000
2019-02-14 19:52:02,255 : Image to text: 8.68, 25.88, 37.68, 19.0
2019-02-14 19:52:09,615 : Text to Image: 6.448, 21.328, 32.364, 23.0
2019-02-14 19:52:50,056 : samples : 320000
2019-02-14 19:53:00,361 : Image to text: 8.34, 26.02, 38.14, 18.0
2019-02-14 19:53:07,819 : Text to Image: 6.924, 21.992, 33.236, 23.0
2019-02-14 19:53:48,710 : samples : 384000
2019-02-14 19:53:58,783 : Image to text: 8.96, 26.26, 39.14, 17.0
2019-02-14 19:54:06,161 : Text to Image: 7.352, 22.94, 34.32, 22.0
2019-02-14 19:54:47,144 : samples : 448000
2019-02-14 19:54:57,251 : Image to text: 8.92, 26.22, 39.34, 18.0
2019-02-14 19:55:04,604 : Text to Image: 7.044, 22.212, 33.644, 22.0
2019-02-14 19:55:44,300 : samples : 512000
2019-02-14 19:55:54,587 : Image to text: 8.88, 27.0, 38.74, 18.0
2019-02-14 19:56:01,968 : Text to Image: 7.48, 23.344, 35.176, 21.0
2019-02-14 19:56:37,726 : Epoch 3 finished
2019-02-14 19:56:38,158 : Image to text: 23.1, 57.4, 71.9, 4.0
2019-02-14 19:56:38,483 : Text to Image: 19.74, 51.08, 68.06, 5.0
2019-02-14 19:56:38,904 : Image to text: 23.8, 56.3, 69.1, 4.0
2019-02-14 19:56:39,228 : Text to Image: 18.32, 51.14, 68.68, 5.0
2019-02-14 19:56:39,664 : Image to text: 24.8, 58.4, 73.1, 4.0
2019-02-14 19:56:39,991 : Text to Image: 18.78, 49.42, 67.68, 6.0
2019-02-14 19:56:40,415 : Image to text: 24.1, 55.3, 70.9, 4.0
2019-02-14 19:56:40,754 : Text to Image: 19.08, 50.6, 68.2, 5.0
2019-02-14 19:56:41,179 : Image to text: 23.3, 58.0, 72.8, 4.0
2019-02-14 19:56:41,505 : Text to Image: 19.08, 51.18, 67.9, 5.0
2019-02-14 19:56:41,505 : Dev mean Text to Image: 19.0, 50.684, 68.104, 5.2
2019-02-14 19:56:41,505 : Dev mean Image to text: 23.82, 57.080000000000005, 71.56, 4.0
2019-02-14 19:56:41,505 : start epoch
2019-02-14 19:57:23,582 : samples : 64000
2019-02-14 19:57:33,859 : Image to text: 9.3, 26.7, 39.04, 18.0
2019-02-14 19:57:41,184 : Text to Image: 7.44, 23.556, 35.244, 21.0
2019-02-14 19:58:22,996 : samples : 128000
2019-02-14 19:58:33,229 : Image to text: 9.1, 26.22, 38.24, 18.0
2019-02-14 19:58:40,570 : Text to Image: 7.4, 23.08, 34.648, 21.0
2019-02-14 19:59:22,378 : samples : 192000
2019-02-14 19:59:32,597 : Image to text: 8.78, 25.44, 37.58, 18.0
2019-02-14 19:59:39,909 : Text to Image: 6.888, 21.776, 32.608, 23.0
2019-02-14 20:00:20,768 : samples : 256000
2019-02-14 20:00:30,948 : Image to text: 9.9, 27.68, 39.34, 17.0
2019-02-14 20:00:38,253 : Text to Image: 7.54, 23.44, 35.332, 21.0
2019-02-14 20:01:17,801 : samples : 320000
2019-02-14 20:01:28,092 : Image to text: 9.42, 27.2, 38.86, 17.0
2019-02-14 20:01:35,473 : Text to Image: 7.532, 23.98, 35.844, 20.0
2019-02-14 20:02:15,717 : samples : 384000
2019-02-14 20:02:26,022 : Image to text: 9.38, 27.24, 39.38, 17.0
2019-02-14 20:02:33,479 : Text to Image: 7.528, 23.484, 35.06, 21.0
2019-02-14 20:03:13,384 : samples : 448000
2019-02-14 20:03:23,706 : Image to text: 8.9, 26.86, 38.82, 17.0
2019-02-14 20:03:31,150 : Text to Image: 7.916, 24.1, 35.988, 20.0
2019-02-14 20:04:12,227 : samples : 512000
2019-02-14 20:04:22,496 : Image to text: 9.06, 26.9, 38.94, 17.0
2019-02-14 20:04:29,922 : Text to Image: 7.464, 23.448, 35.348, 21.0
2019-02-14 20:05:04,647 : Epoch 4 finished
2019-02-14 20:05:05,078 : Image to text: 26.7, 58.4, 73.5, 4.0
2019-02-14 20:05:05,403 : Text to Image: 20.64, 53.18, 70.18, 5.0
2019-02-14 20:05:05,824 : Image to text: 25.6, 58.4, 70.9, 4.0
2019-02-14 20:05:06,160 : Text to Image: 19.66, 52.38, 70.02, 5.0
2019-02-14 20:05:06,587 : Image to text: 27.3, 60.8, 75.6, 4.0
2019-02-14 20:05:06,915 : Text to Image: 20.88, 53.44, 70.06, 5.0
2019-02-14 20:05:07,352 : Image to text: 27.3, 59.3, 73.0, 4.0
2019-02-14 20:05:07,678 : Text to Image: 20.38, 52.66, 70.36, 5.0
2019-02-14 20:05:08,100 : Image to text: 26.2, 59.7, 73.1, 4.0
2019-02-14 20:05:08,427 : Text to Image: 19.94, 52.94, 69.06, 5.0
2019-02-14 20:05:08,427 : Dev mean Text to Image: 20.3, 52.92, 69.93599999999999, 5.0
2019-02-14 20:05:08,427 : Dev mean Image to text: 26.620000000000005, 59.31999999999999, 73.22, 4.0
2019-02-14 20:05:08,427 : start epoch
2019-02-14 20:05:48,170 : samples : 64000
2019-02-14 20:05:58,402 : Image to text: 9.86, 27.08, 39.34, 17.0
2019-02-14 20:06:05,742 : Text to Image: 7.476, 23.392, 34.88, 21.0
2019-02-14 20:06:46,222 : samples : 128000
2019-02-14 20:06:56,515 : Image to text: 9.84, 28.66, 40.52, 16.0
2019-02-14 20:07:03,891 : Text to Image: 7.868, 24.288, 36.136, 20.0
2019-02-14 20:07:46,065 : samples : 192000
2019-02-14 20:07:56,376 : Image to text: 9.04, 27.74, 40.24, 17.0
2019-02-14 20:08:03,758 : Text to Image: 7.588, 23.916, 35.8, 20.0
2019-02-14 20:08:43,994 : samples : 256000
2019-02-14 20:08:54,341 : Image to text: 8.78, 25.72, 37.38, 19.0
2019-02-14 20:09:01,726 : Text to Image: 7.396, 22.736, 34.38, 21.0
2019-02-14 20:09:41,344 : samples : 320000
2019-02-14 20:09:51,664 : Image to text: 9.62, 28.28, 40.52, 16.0
2019-02-14 20:09:59,146 : Text to Image: 8.2, 24.64, 36.436, 20.0
2019-02-14 20:10:41,031 : samples : 384000
2019-02-14 20:10:51,333 : Image to text: 8.88, 26.34, 38.84, 17.0
2019-02-14 20:10:58,819 : Text to Image: 7.616, 23.472, 34.924, 21.0
2019-02-14 20:11:40,754 : samples : 448000
2019-02-14 20:11:51,042 : Image to text: 10.08, 27.5, 40.32, 17.0
2019-02-14 20:11:58,541 : Text to Image: 7.752, 24.304, 36.32, 20.0
2019-02-14 20:12:38,345 : samples : 512000
2019-02-14 20:12:48,622 : Image to text: 9.92, 28.54, 41.02, 16.0
2019-02-14 20:12:56,157 : Text to Image: 8.34, 25.352, 37.48, 19.0
2019-02-14 20:13:32,653 : Epoch 5 finished
2019-02-14 20:13:33,091 : Image to text: 25.0, 59.6, 73.2, 4.0
2019-02-14 20:13:33,433 : Text to Image: 21.36, 53.1, 69.98, 5.0
2019-02-14 20:13:33,876 : Image to text: 25.6, 57.1, 71.7, 4.0
2019-02-14 20:13:34,206 : Text to Image: 19.56, 52.72, 69.48, 5.0
2019-02-14 20:13:34,638 : Image to text: 26.2, 60.3, 73.8, 4.0
2019-02-14 20:13:34,981 : Text to Image: 20.82, 53.4, 70.24, 5.0
2019-02-14 20:13:35,407 : Image to text: 26.0, 57.5, 71.7, 4.0
2019-02-14 20:13:35,734 : Text to Image: 20.36, 53.48, 70.96, 5.0
2019-02-14 20:13:36,172 : Image to text: 27.1, 58.8, 74.1, 4.0
2019-02-14 20:13:36,500 : Text to Image: 20.54, 54.2, 70.12, 5.0
2019-02-14 20:13:36,500 : Dev mean Text to Image: 20.528000000000002, 53.379999999999995, 70.156, 5.0
2019-02-14 20:13:36,500 : Dev mean Image to text: 25.980000000000004, 58.66, 72.89999999999999, 4.0
2019-02-14 20:13:36,500 : start epoch
2019-02-14 20:14:16,987 : samples : 64000
2019-02-14 20:14:27,266 : Image to text: 9.8, 28.96, 41.38, 16.0
2019-02-14 20:14:34,650 : Text to Image: 7.992, 24.728, 36.516, 20.0
2019-02-14 20:15:14,848 : samples : 128000
2019-02-14 20:15:25,154 : Image to text: 9.0, 26.54, 39.3, 17.0
2019-02-14 20:15:32,551 : Text to Image: 7.608, 23.736, 35.212, 20.0
2019-02-14 20:16:14,115 : samples : 192000
2019-02-14 20:16:24,501 : Image to text: 10.22, 28.12, 40.16, 16.0
2019-02-14 20:16:31,892 : Text to Image: 7.764, 24.108, 36.172, 20.0
2019-02-14 20:17:12,217 : samples : 256000
2019-02-14 20:17:22,519 : Image to text: 9.74, 26.94, 39.72, 17.0
2019-02-14 20:17:29,902 : Text to Image: 7.7, 23.828, 35.584, 20.0
2019-02-14 20:18:12,057 : samples : 320000
2019-02-14 20:18:22,326 : Image to text: 9.84, 28.54, 41.0, 16.0
2019-02-14 20:18:29,855 : Text to Image: 8.6, 25.056, 37.608, 19.0
2019-02-14 20:19:09,587 : samples : 384000
2019-02-14 20:19:19,811 : Image to text: 10.58, 29.72, 41.84, 16.0
2019-02-14 20:19:27,315 : Text to Image: 8.26, 25.628, 37.776, 19.0
2019-02-14 20:20:06,925 : samples : 448000
2019-02-14 20:20:17,170 : Image to text: 9.86, 29.08, 41.34, 16.0
2019-02-14 20:20:24,667 : Text to Image: 8.132, 24.796, 36.592, 19.0
2019-02-14 20:21:05,259 : samples : 512000
2019-02-14 20:21:15,459 : Image to text: 9.94, 28.18, 41.66, 16.0
2019-02-14 20:21:22,976 : Text to Image: 7.832, 24.368, 36.288, 19.0
2019-02-14 20:21:56,791 : Epoch 6 finished
2019-02-14 20:21:57,212 : Image to text: 27.6, 58.9, 73.2, 4.0
2019-02-14 20:21:57,536 : Text to Image: 21.74, 55.96, 73.06, 4.0
2019-02-14 20:21:57,971 : Image to text: 24.9, 58.0, 73.6, 4.0
2019-02-14 20:21:58,294 : Text to Image: 20.64, 56.26, 72.62, 4.0
2019-02-14 20:21:58,716 : Image to text: 28.7, 60.2, 74.9, 4.0
2019-02-14 20:21:59,041 : Text to Image: 22.56, 55.9, 72.9, 4.0
2019-02-14 20:21:59,474 : Image to text: 25.3, 59.4, 74.6, 4.0
2019-02-14 20:21:59,799 : Text to Image: 22.7, 55.64, 73.18, 5.0
2019-02-14 20:22:00,225 : Image to text: 26.9, 61.3, 74.8, 4.0
2019-02-14 20:22:00,555 : Text to Image: 22.14, 56.28, 72.08, 4.0
2019-02-14 20:22:00,555 : Dev mean Text to Image: 21.956, 56.007999999999996, 72.76800000000001, 4.2
2019-02-14 20:22:00,555 : Dev mean Image to text: 26.680000000000003, 59.559999999999995, 74.22, 4.0
2019-02-14 20:22:00,555 : start epoch
2019-02-14 20:22:40,995 : samples : 64000
2019-02-14 20:22:51,304 : Image to text: 9.68, 28.12, 41.06, 16.0
2019-02-14 20:22:58,665 : Text to Image: 8.112, 24.968, 37.176, 19.0
2019-02-14 20:23:40,982 : samples : 128000
2019-02-14 20:23:51,167 : Image to text: 9.94, 28.34, 41.66, 16.0
2019-02-14 20:23:58,495 : Text to Image: 7.84, 24.056, 36.208, 20.0
2019-02-14 20:24:39,040 : samples : 192000
2019-02-14 20:24:49,200 : Image to text: 10.4, 30.14, 42.66, 15.0
2019-02-14 20:24:56,550 : Text to Image: 8.232, 25.516, 37.38, 19.0
2019-02-14 20:25:37,100 : samples : 256000
2019-02-14 20:25:47,303 : Image to text: 10.2, 29.26, 43.12, 15.0
2019-02-14 20:25:54,627 : Text to Image: 8.252, 25.256, 37.308, 19.0
2019-02-14 20:26:36,121 : samples : 320000
2019-02-14 20:26:46,280 : Image to text: 10.12, 28.16, 41.5, 16.0
2019-02-14 20:26:53,780 : Text to Image: 8.496, 25.512, 37.54, 18.0
2019-02-14 20:27:40,560 : samples : 384000
2019-02-14 20:27:50,815 : Image to text: 10.82, 29.9, 42.4, 15.0
2019-02-14 20:27:58,332 : Text to Image: 8.692, 26.528, 38.868, 18.0
2019-02-14 20:28:38,226 : samples : 448000
2019-02-14 20:28:48,420 : Image to text: 9.92, 28.14, 40.72, 16.0
2019-02-14 20:28:55,935 : Text to Image: 8.132, 25.236, 37.524, 18.0
2019-02-14 20:29:38,013 : samples : 512000
2019-02-14 20:29:48,154 : Image to text: 10.56, 30.2, 42.5, 15.0
2019-02-14 20:29:55,639 : Text to Image: 8.648, 25.964, 38.364, 18.0
2019-02-14 20:30:31,115 : Epoch 7 finished
2019-02-14 20:30:31,541 : Image to text: 24.2, 56.4, 74.3, 4.0
2019-02-14 20:30:31,867 : Text to Image: 21.46, 54.76, 72.62, 5.0
2019-02-14 20:30:32,294 : Image to text: 25.8, 58.0, 73.8, 4.0
2019-02-14 20:30:32,621 : Text to Image: 21.1, 54.26, 71.8, 5.0
2019-02-14 20:30:33,049 : Image to text: 25.7, 59.5, 75.0, 4.0
2019-02-14 20:30:33,374 : Text to Image: 22.68, 54.7, 72.12, 5.0
2019-02-14 20:30:33,805 : Image to text: 25.7, 58.8, 73.4, 4.0
2019-02-14 20:30:34,131 : Text to Image: 22.14, 54.76, 72.42, 5.0
2019-02-14 20:30:34,555 : Image to text: 24.2, 58.4, 74.6, 4.0
2019-02-14 20:30:34,880 : Text to Image: 21.34, 54.6, 71.56, 5.0
2019-02-14 20:30:34,880 : Dev mean Text to Image: 21.744, 54.616, 72.104, 5.0
2019-02-14 20:30:34,880 : Dev mean Image to text: 25.12, 58.22, 74.22, 4.0
2019-02-14 20:30:34,880 : start epoch
2019-02-14 20:31:17,226 : samples : 64000
2019-02-14 20:31:27,439 : Image to text: 9.96, 28.56, 41.42, 16.0
2019-02-14 20:31:34,785 : Text to Image: 8.608, 25.884, 37.692, 18.0
2019-02-14 20:32:15,465 : samples : 128000
2019-02-14 20:32:25,661 : Image to text: 10.44, 29.26, 42.24, 15.0
2019-02-14 20:32:33,014 : Text to Image: 8.492, 25.592, 37.832, 18.0
2019-02-14 20:33:15,932 : samples : 192000
2019-02-14 20:33:26,078 : Image to text: 10.92, 29.24, 42.1, 15.0
2019-02-14 20:33:33,408 : Text to Image: 8.236, 25.32, 37.236, 19.0
2019-02-14 20:34:14,514 : samples : 256000
2019-02-14 20:34:24,732 : Image to text: 9.96, 28.4, 41.74, 15.0
2019-02-14 20:34:32,060 : Text to Image: 8.592, 26.02, 37.948, 18.0
2019-02-14 20:35:18,757 : samples : 320000
2019-02-14 20:35:29,037 : Image to text: 10.92, 30.38, 42.98, 14.0
2019-02-14 20:35:36,551 : Text to Image: 8.732, 25.9, 38.468, 18.0
2019-02-14 20:36:18,156 : samples : 384000
2019-02-14 20:36:28,367 : Image to text: 10.0, 29.24, 41.66, 15.0
2019-02-14 20:36:35,880 : Text to Image: 8.336, 25.28, 37.292, 19.0
2019-02-14 20:37:17,848 : samples : 448000
2019-02-14 20:37:28,054 : Image to text: 10.94, 30.24, 44.06, 14.0
2019-02-14 20:37:35,556 : Text to Image: 8.552, 25.776, 37.94, 18.0
2019-02-14 20:38:17,374 : samples : 512000
2019-02-14 20:38:27,597 : Image to text: 10.26, 29.6, 42.18, 15.0
2019-02-14 20:38:35,125 : Text to Image: 8.544, 25.1, 37.668, 19.0
2019-02-14 20:39:09,315 : Epoch 8 finished
2019-02-14 20:39:09,741 : Image to text: 27.0, 63.1, 76.1, 3.0
2019-02-14 20:39:10,062 : Text to Image: 23.66, 56.28, 73.6, 4.0
2019-02-14 20:39:10,477 : Image to text: 28.1, 60.1, 75.7, 3.0
2019-02-14 20:39:10,799 : Text to Image: 22.08, 55.7, 72.74, 4.0
2019-02-14 20:39:11,229 : Image to text: 28.9, 62.7, 77.8, 3.0
2019-02-14 20:39:11,555 : Text to Image: 23.44, 57.0, 73.18, 4.0
2019-02-14 20:39:11,976 : Image to text: 26.5, 61.7, 74.3, 3.0
2019-02-14 20:39:12,301 : Text to Image: 22.92, 56.24, 73.36, 4.0
2019-02-14 20:39:12,736 : Image to text: 27.8, 61.3, 76.8, 3.0
2019-02-14 20:39:13,062 : Text to Image: 23.44, 56.68, 73.1, 4.0
2019-02-14 20:39:13,062 : Dev mean Text to Image: 23.108000000000004, 56.379999999999995, 73.196, 4.0
2019-02-14 20:39:13,062 : Dev mean Image to text: 27.659999999999997, 61.779999999999994, 76.14, 3.0
2019-02-14 20:39:13,062 : start epoch
2019-02-14 20:39:52,869 : samples : 64000
2019-02-14 20:40:03,165 : Image to text: 10.48, 29.78, 43.7, 14.0
2019-02-14 20:40:10,514 : Text to Image: 8.924, 26.336, 38.684, 18.0
2019-02-14 20:40:50,213 : samples : 128000
2019-02-14 20:41:00,469 : Image to text: 10.98, 29.88, 42.78, 15.0
2019-02-14 20:41:07,819 : Text to Image: 8.352, 24.632, 36.88, 19.0
2019-02-14 20:41:47,681 : samples : 192000
2019-02-14 20:41:57,883 : Image to text: 10.32, 29.52, 42.7, 15.0
2019-02-14 20:42:05,219 : Text to Image: 8.524, 25.508, 37.468, 19.0
2019-02-14 20:42:46,047 : samples : 256000
2019-02-14 20:42:56,311 : Image to text: 9.9, 29.0, 41.82, 16.0
2019-02-14 20:43:03,670 : Text to Image: 8.324, 25.076, 37.428, 19.0
2019-02-14 20:43:43,129 : samples : 320000
2019-02-14 20:43:53,411 : Image to text: 11.06, 30.66, 43.64, 14.0
2019-02-14 20:44:00,938 : Text to Image: 8.956, 26.64, 39.36, 17.0
2019-02-14 20:44:40,736 : samples : 384000
2019-02-14 20:44:51,034 : Image to text: 9.48, 28.16, 40.56, 16.0
2019-02-14 20:44:58,538 : Text to Image: 8.172, 25.032, 37.052, 19.0
2019-02-14 20:45:38,101 : samples : 448000
2019-02-14 20:45:48,384 : Image to text: 10.6, 29.32, 42.02, 15.0
2019-02-14 20:45:55,886 : Text to Image: 8.584, 25.816, 38.36, 18.0
2019-02-14 20:46:37,403 : samples : 512000
2019-02-14 20:46:47,586 : Image to text: 10.76, 30.06, 42.7, 15.0
2019-02-14 20:46:55,104 : Text to Image: 9.108, 27.088, 39.14, 17.0
2019-02-14 20:47:30,700 : Epoch 9 finished
2019-02-14 20:47:31,115 : Image to text: 25.5, 60.4, 76.9, 4.0
2019-02-14 20:47:31,437 : Text to Image: 22.44, 55.84, 73.64, 5.0
2019-02-14 20:47:31,871 : Image to text: 27.3, 60.9, 74.8, 4.0
2019-02-14 20:47:32,196 : Text to Image: 22.14, 55.42, 72.42, 4.0
2019-02-14 20:47:32,620 : Image to text: 30.5, 63.1, 77.1, 3.0
2019-02-14 20:47:32,953 : Text to Image: 23.06, 56.48, 73.26, 4.0
2019-02-14 20:47:33,379 : Image to text: 28.1, 60.7, 74.7, 4.0
2019-02-14 20:47:33,704 : Text to Image: 22.98, 55.5, 72.22, 4.0
2019-02-14 20:47:34,138 : Image to text: 29.1, 61.0, 74.6, 3.0
2019-02-14 20:47:34,462 : Text to Image: 23.32, 56.2, 72.56, 4.0
2019-02-14 20:47:34,462 : Dev mean Text to Image: 22.788000000000004, 55.888000000000005, 72.82000000000001, 4.2
2019-02-14 20:47:34,462 : Dev mean Image to text: 28.099999999999998, 61.22, 75.62, 3.6
2019-02-14 20:47:38,238 : 
Test scores | Image to text:             28.84, 61.74, 76.14, 3.1999999999999997
2019-02-14 20:47:38,238 : Test scores | Text to image:             23.304, 56.559999999999995, 72.77199999999999, 4.0

2019-02-14 20:47:38,341 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 20:47:38,714 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 20:47:39,384 : loading BERT model bert-base-uncased
2019-02-14 20:47:39,384 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:47:39,416 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:47:39,416 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmprtcx3pls
2019-02-14 20:47:41,819 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:47:43,274 : Computing embeddings for train/dev/test
2019-02-14 20:49:18,858 : Computed embeddings
2019-02-14 20:49:18,858 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 20:49:47,776 : [('reg:1e-05', 60.75), ('reg:0.0001', 55.75), ('reg:0.001', 40.88), ('reg:0.01', 29.98)]
2019-02-14 20:49:47,776 : Validation : best param found is reg = 1e-05 with score             60.75
2019-02-14 20:49:47,776 : Evaluating...
2019-02-14 20:49:56,368 : 
Dev acc : 60.8 Test acc : 62.0 for LENGTH classification

2019-02-14 20:49:56,368 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 20:49:56,839 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 20:49:56,885 : loading BERT model bert-base-uncased
2019-02-14 20:49:56,885 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:49:56,914 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:49:56,915 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4yroeskf
2019-02-14 20:49:59,326 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:50:00,738 : Computing embeddings for train/dev/test
2019-02-14 20:51:28,457 : Computed embeddings
2019-02-14 20:51:28,457 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 20:52:06,282 : [('reg:1e-05', 3.24), ('reg:0.0001', 0.39), ('reg:0.001', 0.2), ('reg:0.01', 0.17)]
2019-02-14 20:52:06,283 : Validation : best param found is reg = 1e-05 with score             3.24
2019-02-14 20:52:06,283 : Evaluating...
2019-02-14 20:52:18,079 : 
Dev acc : 3.2 Test acc : 3.2 for WORDCONTENT classification

2019-02-14 20:52:18,080 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 20:52:18,630 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 20:52:18,699 : loading BERT model bert-base-uncased
2019-02-14 20:52:18,700 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:52:18,727 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:52:18,727 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfqihhfa1
2019-02-14 20:52:21,140 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:52:22,580 : Computing embeddings for train/dev/test
2019-02-14 20:53:45,887 : Computed embeddings
2019-02-14 20:53:45,887 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 20:54:15,807 : [('reg:1e-05', 28.53), ('reg:0.0001', 25.78), ('reg:0.001', 19.24), ('reg:0.01', 18.26)]
2019-02-14 20:54:15,807 : Validation : best param found is reg = 1e-05 with score             28.53
2019-02-14 20:54:15,807 : Evaluating...
2019-02-14 20:54:24,611 : 
Dev acc : 28.5 Test acc : 28.2 for DEPTH classification

2019-02-14 20:54:24,612 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 20:54:25,043 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 20:54:25,111 : loading BERT model bert-base-uncased
2019-02-14 20:54:25,111 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:54:25,143 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:54:25,144 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6yh83a50
2019-02-14 20:54:27,588 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:54:29,058 : Computing embeddings for train/dev/test
2019-02-14 20:55:47,232 : Computed embeddings
2019-02-14 20:55:47,233 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 20:56:26,293 : [('reg:1e-05', 68.99), ('reg:0.0001', 56.18), ('reg:0.001', 28.71), ('reg:0.01', 14.11)]
2019-02-14 20:56:26,293 : Validation : best param found is reg = 1e-05 with score             68.99
2019-02-14 20:56:26,293 : Evaluating...
2019-02-14 20:56:35,161 : 
Dev acc : 69.0 Test acc : 68.8 for TOPCONSTITUENTS classification

2019-02-14 20:56:35,162 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 20:56:35,555 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 20:56:35,626 : loading BERT model bert-base-uncased
2019-02-14 20:56:35,626 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:56:35,661 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:56:35,661 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpujtw4kyh
2019-02-14 20:56:38,104 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:56:39,576 : Computing embeddings for train/dev/test
2019-02-14 20:58:04,317 : Computed embeddings
2019-02-14 20:58:04,317 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 20:58:45,151 : [('reg:1e-05', 88.12), ('reg:0.0001', 86.71), ('reg:0.001', 83.14), ('reg:0.01', 77.9)]
2019-02-14 20:58:45,151 : Validation : best param found is reg = 1e-05 with score             88.12
2019-02-14 20:58:45,151 : Evaluating...
2019-02-14 20:58:56,261 : 
Dev acc : 88.1 Test acc : 87.5 for BIGRAMSHIFT classification

2019-02-14 20:58:56,262 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 20:58:56,648 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 20:58:56,714 : loading BERT model bert-base-uncased
2019-02-14 20:58:56,714 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 20:58:56,831 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 20:58:56,831 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpwbhg1d48
2019-02-14 20:58:59,235 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 20:59:00,666 : Computing embeddings for train/dev/test
2019-02-14 21:00:21,641 : Computed embeddings
2019-02-14 21:00:21,641 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:00:53,295 : [('reg:1e-05', 90.21), ('reg:0.0001', 90.05), ('reg:0.001', 89.04), ('reg:0.01', 83.33)]
2019-02-14 21:00:53,295 : Validation : best param found is reg = 1e-05 with score             90.21
2019-02-14 21:00:53,295 : Evaluating...
2019-02-14 21:01:01,935 : 
Dev acc : 90.2 Test acc : 89.0 for TENSE classification

2019-02-14 21:01:01,936 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 21:01:02,365 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 21:01:02,432 : loading BERT model bert-base-uncased
2019-02-14 21:01:02,433 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:01:02,565 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:01:02,565 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvub4_neu
2019-02-14 21:01:04,965 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:01:06,385 : Computing embeddings for train/dev/test
2019-02-14 21:02:32,412 : Computed embeddings
2019-02-14 21:02:32,412 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:03:08,476 : [('reg:1e-05', 85.06), ('reg:0.0001', 83.47), ('reg:0.001', 78.29), ('reg:0.01', 71.87)]
2019-02-14 21:03:08,476 : Validation : best param found is reg = 1e-05 with score             85.06
2019-02-14 21:03:08,477 : Evaluating...
2019-02-14 21:03:17,766 : 
Dev acc : 85.1 Test acc : 84.1 for SUBJNUMBER classification

2019-02-14 21:03:17,767 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 21:03:18,394 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 21:03:18,466 : loading BERT model bert-base-uncased
2019-02-14 21:03:18,466 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:03:18,498 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:03:18,498 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpezmprklv
2019-02-14 21:03:20,879 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:03:22,313 : Computing embeddings for train/dev/test
2019-02-14 21:04:47,790 : Computed embeddings
2019-02-14 21:04:47,790 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:05:17,777 : [('reg:1e-05', 79.64), ('reg:0.0001', 76.92), ('reg:0.001', 72.09), ('reg:0.01', 63.31)]
2019-02-14 21:05:17,778 : Validation : best param found is reg = 1e-05 with score             79.64
2019-02-14 21:05:17,778 : Evaluating...
2019-02-14 21:05:27,842 : 
Dev acc : 79.6 Test acc : 81.0 for OBJNUMBER classification

2019-02-14 21:05:27,843 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 21:05:28,270 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 21:05:28,343 : loading BERT model bert-base-uncased
2019-02-14 21:05:28,343 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:05:28,375 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:05:28,375 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpm01hp6zf
2019-02-14 21:05:30,775 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:05:32,192 : Computing embeddings for train/dev/test
2019-02-14 21:07:10,474 : Computed embeddings
2019-02-14 21:07:10,474 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:07:42,021 : [('reg:1e-05', 64.74), ('reg:0.0001', 61.54), ('reg:0.001', 58.82), ('reg:0.01', 51.66)]
2019-02-14 21:07:42,021 : Validation : best param found is reg = 1e-05 with score             64.74
2019-02-14 21:07:42,021 : Evaluating...
2019-02-14 21:07:52,190 : 
Dev acc : 64.7 Test acc : 64.5 for ODDMANOUT classification

2019-02-14 21:07:52,191 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 21:07:52,603 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 21:07:52,680 : loading BERT model bert-base-uncased
2019-02-14 21:07:52,681 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:07:52,710 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:07:52,710 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8qs8u0id
2019-02-14 21:07:55,053 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:07:56,475 : Computing embeddings for train/dev/test
2019-02-14 21:09:32,707 : Computed embeddings
2019-02-14 21:09:32,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:10:10,363 : [('reg:1e-05', 67.94), ('reg:0.0001', 65.94), ('reg:0.001', 57.95), ('reg:0.01', 50.03)]
2019-02-14 21:10:10,363 : Validation : best param found is reg = 1e-05 with score             67.94
2019-02-14 21:10:10,363 : Evaluating...
2019-02-14 21:10:19,184 : 
Dev acc : 67.9 Test acc : 67.0 for COORDINATIONINVERSION classification

2019-02-14 21:10:19,186 : total results: {'STS12': {'MSRpar': {'pearson': (0.28845919411414744, 7.722493472272243e-16), 'spearman': SpearmanrResult(correlation=0.36924557145440906, pvalue=1.218185190148858e-25), 'nsamples': 750}, 'MSRvid': {'pearson': (0.13872906807281937, 0.00013820474695083612), 'spearman': SpearmanrResult(correlation=0.19411210310530191, pvalue=8.407509824283135e-08), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.2839911086465053, 5.801358310138264e-10), 'spearman': SpearmanrResult(correlation=0.48960098964645943, pvalue=4.8175892830853234e-29), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.25023083882831465, 3.593176247887424e-12), 'spearman': SpearmanrResult(correlation=0.3153136055358491, pvalue=8.950831685361124e-19), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.4115146761592851, 9.722465698880132e-18), 'spearman': SpearmanrResult(correlation=0.4840983566474817, pvalue=7.829354177321414e-25), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.27458497716421437, 'wmean': 0.2582402189246338}, 'spearman': {'mean': 0.37047412527790025, 'wmean': 0.3464882749748198}}}, 'STS13': {'FNWN': {'pearson': (0.08323794368278692, 0.2548206482880998), 'spearman': SpearmanrResult(correlation=0.06894596983419576, pvalue=0.34584221139982707), 'nsamples': 189}, 'headlines': {'pearson': (0.12769979048728844, 0.0004553534234251064), 'spearman': SpearmanrResult(correlation=0.37771759030179014, pvalue=7.612275060117582e-27), 'nsamples': 750}, 'OnWN': {'pearson': (0.010680257920957356, 0.8007245097561342), 'spearman': SpearmanrResult(correlation=0.06326660168119351, pvalue=0.13448317912195384), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.07387266403034425, 'wmean': 0.07833229261011342}, 'spearman': {'mean': 0.16997672060572647, 'wmean': 0.22120769637877016}}}, 'STS14': {'deft-forum': {'pearson': (0.0894473155620664, 0.057961554590859554), 'spearman': SpearmanrResult(correlation=0.09870101586005542, pvalue=0.03634430265324801), 'nsamples': 450}, 'deft-news': {'pearson': (0.20229815845275403, 0.00042220145462827896), 'spearman': SpearmanrResult(correlation=0.45581051991629706, pvalue=8.505924961577205e-17), 'nsamples': 300}, 'headlines': {'pearson': (0.24154055005796882, 2.0372670015423558e-11), 'spearman': SpearmanrResult(correlation=0.3641789180593962, pvalue=6.151223713819035e-25), 'nsamples': 750}, 'images': {'pearson': (0.08646422846858565, 0.01786484071269665), 'spearman': SpearmanrResult(correlation=0.20286467477997067, pvalue=2.0850676914563958e-08), 'nsamples': 750}, 'OnWN': {'pearson': (0.16152815806061951, 8.769723067703521e-06), 'spearman': SpearmanrResult(correlation=0.2611235589799896, pvalue=3.712212606171933e-13), 'nsamples': 750}, 'tweet-news': {'pearson': (0.3021577758833718, 2.6817912601474544e-17), 'spearman': SpearmanrResult(correlation=0.38337497451384867, pvalue=1.1411928125654731e-27), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.18057269774756102, 'wmean': 0.18525567303777746}, 'spearman': {'mean': 0.2943422770182596, 'wmean': 0.29061738876315146}}}, 'STS15': {'answers-forums': {'pearson': (0.16028347854562552, 0.001847768689436778), 'spearman': SpearmanrResult(correlation=0.25732417355570747, pvalue=4.3779029354020146e-07), 'nsamples': 375}, 'answers-students': {'pearson': (0.020733587488424497, 0.5707640921650601), 'spearman': SpearmanrResult(correlation=0.18324495856463432, pvalue=4.352786469933317e-07), 'nsamples': 750}, 'belief': {'pearson': (0.24986705103950285, 9.561842620208698e-07), 'spearman': SpearmanrResult(correlation=0.3967492673269759, pvalue=1.3763438773461858e-15), 'nsamples': 375}, 'headlines': {'pearson': (0.21353461169456064, 3.4969367605252735e-09), 'spearman': SpearmanrResult(correlation=0.4600353165570627, pvalue=1.5012807385659803e-40), 'nsamples': 750}, 'images': {'pearson': (0.08686539819387594, 0.017339059378668607), 'spearman': SpearmanrResult(correlation=0.1868639965253506, pvalue=2.544261580148542e-07), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.1462568253923979, 'wmean': 0.1315522155423563}, 'spearman': {'mean': 0.29684354250594625, 'wmean': 0.2892952480220973}}}, 'STS16': {'answer-answer': {'pearson': (0.3419573968121076, 2.2379293994338262e-08), 'spearman': SpearmanrResult(correlation=0.44569421949036575, pvalue=8.456372268616219e-14), 'nsamples': 254}, 'headlines': {'pearson': (0.4406412060586546, 2.9889844599241326e-13), 'spearman': SpearmanrResult(correlation=0.5408528658943149, pvalue=2.5557184327974754e-20), 'nsamples': 249}, 'plagiarism': {'pearson': (0.2898032297059558, 7.91838644575212e-06), 'spearman': SpearmanrResult(correlation=0.6064742719375158, pvalue=1.7101876699556818e-24), 'nsamples': 230}, 'postediting': {'pearson': (0.4698565611038304, 8.383328054016223e-15), 'spearman': SpearmanrResult(correlation=0.6645440019273958, pvalue=1.8342964098237476e-32), 'nsamples': 244}, 'question-question': {'pearson': (0.057914776853730136, 0.40487731460435816), 'spearman': SpearmanrResult(correlation=0.19519242680214957, pvalue=0.004622393795135092), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.3200346341068557, 'wmean': 0.3288202118069261}, 'spearman': {'mean': 0.4905515572103484, 'wmean': 0.49773333185143326}}}, 'MR': {'devacc': 77.78, 'acc': 77.52, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 84.54, 'acc': 82.33, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 85.75, 'acc': 85.85, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.48, 'acc': 92.9, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.83, 'acc': 82.15, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 43.87, 'acc': 42.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 72.41, 'acc': 85.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 71.22, 'acc': 72.87, 'f1': 80.99, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 71.8, 'acc': 72.11, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7431423375783016, 'pearson': 0.757480991033089, 'spearman': 0.7077849446591156, 'mse': 0.4343256655235998, 'yhat': array([2.59818527, 4.58762737, 2.32076124, ..., 3.07911347, 4.22267674,        4.50918682]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.6758162693595525, 'pearson': 0.6087472917533145, 'spearman': 0.5951505545651116, 'mse': 1.77856188379372, 'yhat': array([2.2012837 , 1.99164629, 2.7602234 , ..., 3.85191961, 3.21238556,        3.77165352]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 66.53, 'acc': 66.52, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 318.264, 'acc': [(28.84, 61.74, 76.14, 3.1999999999999997), (23.304, 56.559999999999995, 72.77199999999999, 4.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 60.75, 'acc': 61.99, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 3.24, 'acc': 3.25, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.53, 'acc': 28.18, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 68.99, 'acc': 68.81, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 88.12, 'acc': 87.55, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.21, 'acc': 89.01, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 85.06, 'acc': 84.1, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 79.64, 'acc': 81.0, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.74, 'acc': 64.49, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 67.94, 'acc': 66.97, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 21:10:19,186 : STS12 p=0.2582, STS12 s=0.3465, STS13 p=0.0783, STS13 s=0.2212, STS14 p=0.1853, STS14 s=0.2906, STS15 p=0.1316, STS15 s=0.2893, STS 16 p=0.3288, STS16 s=0.4977, STS B p=0.6087, STS B s=0.5952, STS B m=1.7786, SICK-R p=0.7575, SICK-R s=0.7078, SICK-P m=0.4343
2019-02-14 21:10:19,186 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 21:10:19,186 : 0.2582,0.3465,0.0783,0.2212,0.1853,0.2906,0.1316,0.2893,0.3288,0.4977,0.6087,0.5952,1.7786,0.7575,0.7078,0.4343
2019-02-14 21:10:19,186 : MR=77.52, CR=82.33, SUBJ=92.90, MPQA=85.85, SST-B=82.15, SST-F=42.26, TREC=85.20, SICK-E=72.11, SNLI=66.52, MRPC=72.87, MRPC f=80.99
2019-02-14 21:10:19,186 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 21:10:19,186 : 77.52,82.33,92.90,85.85,82.15,42.26,85.20,72.11,66.52,72.87,80.99
2019-02-14 21:10:19,186 : COCO r1i2t=28.84, COCO r5i2t=61.74, COCO r10i2t=76.14, COCO medr_i2t=3.20, COCO r1t2i=23.30, COCO r5t2i=56.56, COCO r10t2i=72.77, COCO medr_t2i=4.00
2019-02-14 21:10:19,186 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 21:10:19,186 : 28.84,61.74,76.14,3.20,23.30,56.56,72.77,4.00
2019-02-14 21:10:19,186 : SentLen=61.99, WC=3.25, TreeDepth=28.18, TopConst=68.81, BShift=87.55, Tense=89.01, SubjNum=84.10, ObjNum=81.00, SOMO=64.49, CoordInv=66.97, average=63.54
2019-02-14 21:10:19,186 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 21:10:19,186 : 61.99,3.25,28.18,68.81,87.55,89.01,84.10,81.00,64.49,66.97,63.54
2019-02-14 21:10:19,186 : ********************************************************************************
2019-02-14 21:10:19,186 : ********************************************************************************
2019-02-14 21:10:19,186 : ********************************************************************************
2019-02-14 21:10:19,186 : layer 10
2019-02-14 21:10:19,186 : ********************************************************************************
2019-02-14 21:10:19,187 : ********************************************************************************
2019-02-14 21:10:19,187 : ********************************************************************************
2019-02-14 21:10:19,283 : ***** Transfer task : STS12 *****


2019-02-14 21:10:19,295 : loading BERT model bert-base-uncased
2019-02-14 21:10:19,295 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:19,313 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:19,314 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps42wbl5q
2019-02-14 21:10:21,789 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:10:25,004 : MSRpar : pearson = 0.0704, spearman = 0.1289
2019-02-14 21:10:25,769 : MSRvid : pearson = -0.0035, spearman = 0.0204
2019-02-14 21:10:26,363 : SMTeuroparl : pearson = 0.2055, spearman = 0.4243
2019-02-14 21:10:27,536 : surprise.OnWN : pearson = 0.0769, spearman = 0.1760
2019-02-14 21:10:28,194 : surprise.SMTnews : pearson = 0.1564, spearman = 0.1567
2019-02-14 21:10:28,194 : ALL (weighted average) : Pearson = 0.0851,             Spearman = 0.1613
2019-02-14 21:10:28,194 : ALL (average) : Pearson = 0.1011,             Spearman = 0.1813

2019-02-14 21:10:28,194 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 21:10:28,204 : loading BERT model bert-base-uncased
2019-02-14 21:10:28,205 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:28,222 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:28,223 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7nl78xqd
2019-02-14 21:10:30,589 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:10:32,594 : FNWN : pearson = 0.0243, spearman = 0.0291
2019-02-14 21:10:33,466 : headlines : pearson = -0.0246, spearman = 0.3081
2019-02-14 21:10:34,133 : OnWN : pearson = -0.1112, spearman = 0.0685
2019-02-14 21:10:34,133 : ALL (weighted average) : Pearson = -0.0509,             Spearman = 0.1834
2019-02-14 21:10:34,133 : ALL (average) : Pearson = -0.0372,             Spearman = 0.1353

2019-02-14 21:10:34,133 : ***** Transfer task : STS14 *****


2019-02-14 21:10:34,149 : loading BERT model bert-base-uncased
2019-02-14 21:10:34,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:34,167 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:34,167 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3mmc6598
2019-02-14 21:10:36,509 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:10:38,594 : deft-forum : pearson = 0.0333, spearman = 0.0430
2019-02-14 21:10:39,366 : deft-news : pearson = 0.2710, spearman = 0.3906
2019-02-14 21:10:40,438 : headlines : pearson = 0.0280, spearman = 0.3185
2019-02-14 21:10:41,466 : images : pearson = 0.0275, spearman = 0.0839
2019-02-14 21:10:42,511 : OnWN : pearson = -0.0729, spearman = 0.0915
2019-02-14 21:10:43,879 : tweet-news : pearson = 0.2001, spearman = 0.2616
2019-02-14 21:10:43,879 : ALL (weighted average) : Pearson = 0.0622,             Spearman = 0.1875
2019-02-14 21:10:43,879 : ALL (average) : Pearson = 0.0812,             Spearman = 0.1982

2019-02-14 21:10:43,879 : ***** Transfer task : STS15 *****


2019-02-14 21:10:43,911 : loading BERT model bert-base-uncased
2019-02-14 21:10:43,911 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:43,928 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:43,928 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxan7da7x
2019-02-14 21:10:46,313 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:10:48,562 : answers-forums : pearson = 0.0655, spearman = 0.0684
2019-02-14 21:10:49,476 : answers-students : pearson = -0.0151, spearman = 0.1231
2019-02-14 21:10:50,315 : belief : pearson = 0.1510, spearman = 0.2281
2019-02-14 21:10:51,292 : headlines : pearson = -0.0393, spearman = 0.3952
2019-02-14 21:10:52,225 : images : pearson = 0.0147, spearman = 0.0904
2019-02-14 21:10:52,225 : ALL (weighted average) : Pearson = 0.0171,             Spearman = 0.1892
2019-02-14 21:10:52,225 : ALL (average) : Pearson = 0.0354,             Spearman = 0.1810

2019-02-14 21:10:52,225 : ***** Transfer task : STS16 *****


2019-02-14 21:10:52,289 : loading BERT model bert-base-uncased
2019-02-14 21:10:52,290 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:52,307 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:52,307 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoxq97zxj
2019-02-14 21:10:54,656 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:10:56,453 : answer-answer : pearson = 0.2107, spearman = 0.2770
2019-02-14 21:10:56,769 : headlines : pearson = 0.0670, spearman = 0.4281
2019-02-14 21:10:57,166 : plagiarism : pearson = 0.2611, spearman = 0.4137
2019-02-14 21:10:57,790 : postediting : pearson = 0.4166, spearman = 0.5320
2019-02-14 21:10:58,074 : question-question : pearson = -0.0321, spearman = 0.1058
2019-02-14 21:10:58,074 : ALL (weighted average) : Pearson = 0.1899,             Spearman = 0.3575
2019-02-14 21:10:58,074 : ALL (average) : Pearson = 0.1846,             Spearman = 0.3513

2019-02-14 21:10:58,074 : ***** Transfer task : MR *****


2019-02-14 21:10:58,093 : loading BERT model bert-base-uncased
2019-02-14 21:10:58,093 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:10:58,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:10:58,113 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf2yzo2wj
2019-02-14 21:11:00,518 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:11:01,982 : Generating sentence embeddings
2019-02-14 21:11:15,330 : Generated sentence embeddings
2019-02-14 21:11:15,330 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 21:11:27,026 : Best param found at split 1: l2reg = 0.0001                 with score 78.89
2019-02-14 21:11:39,725 : Best param found at split 2: l2reg = 0.0001                 with score 79.19
2019-02-14 21:11:52,264 : Best param found at split 3: l2reg = 0.001                 with score 79.77
2019-02-14 21:12:04,320 : Best param found at split 4: l2reg = 0.0001                 with score 79.7
2019-02-14 21:12:14,390 : Best param found at split 5: l2reg = 0.0001                 with score 79.24
2019-02-14 21:12:15,124 : Dev acc : 79.36 Test acc : 79.18

2019-02-14 21:12:15,125 : ***** Transfer task : CR *****


2019-02-14 21:12:15,133 : loading BERT model bert-base-uncased
2019-02-14 21:12:15,133 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:12:15,157 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:12:15,157 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpeqf2lva2
2019-02-14 21:12:17,572 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:12:19,002 : Generating sentence embeddings
2019-02-14 21:12:22,680 : Generated sentence embeddings
2019-02-14 21:12:22,680 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 21:12:27,426 : Best param found at split 1: l2reg = 1e-05                 with score 84.9
2019-02-14 21:12:32,740 : Best param found at split 2: l2reg = 0.0001                 with score 85.33
2019-02-14 21:12:37,414 : Best param found at split 3: l2reg = 1e-05                 with score 84.9
2019-02-14 21:12:42,144 : Best param found at split 4: l2reg = 1e-05                 with score 84.84
2019-02-14 21:12:45,896 : Best param found at split 5: l2reg = 1e-05                 with score 84.34
2019-02-14 21:12:46,101 : Dev acc : 84.86 Test acc : 82.76

2019-02-14 21:12:46,102 : ***** Transfer task : MPQA *****


2019-02-14 21:12:46,107 : loading BERT model bert-base-uncased
2019-02-14 21:12:46,107 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:12:46,125 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:12:46,126 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplw6ixgzd
2019-02-14 21:12:48,460 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:12:49,944 : Generating sentence embeddings
2019-02-14 21:12:53,658 : Generated sentence embeddings
2019-02-14 21:12:53,658 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 21:13:03,136 : Best param found at split 1: l2reg = 1e-05                 with score 83.82
2019-02-14 21:13:12,573 : Best param found at split 2: l2reg = 1e-05                 with score 85.57
2019-02-14 21:13:26,403 : Best param found at split 3: l2reg = 1e-05                 with score 85.0
2019-02-14 21:13:38,968 : Best param found at split 4: l2reg = 0.0001                 with score 85.01
2019-02-14 21:13:50,863 : Best param found at split 5: l2reg = 1e-05                 with score 84.23
2019-02-14 21:13:51,470 : Dev acc : 84.73 Test acc : 85.67

2019-02-14 21:13:51,472 : ***** Transfer task : SUBJ *****


2019-02-14 21:13:51,489 : loading BERT model bert-base-uncased
2019-02-14 21:13:51,489 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:13:51,510 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:13:51,510 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps9acyf_w
2019-02-14 21:13:53,921 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:13:55,365 : Generating sentence embeddings
2019-02-14 21:14:08,554 : Generated sentence embeddings
2019-02-14 21:14:08,554 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 21:14:18,924 : Best param found at split 1: l2reg = 1e-05                 with score 94.75
2019-02-14 21:14:28,312 : Best param found at split 2: l2reg = 0.0001                 with score 94.45
2019-02-14 21:14:40,621 : Best param found at split 3: l2reg = 1e-05                 with score 94.86
2019-02-14 21:14:54,369 : Best param found at split 4: l2reg = 0.0001                 with score 94.95
2019-02-14 21:15:06,989 : Best param found at split 5: l2reg = 1e-05                 with score 94.69
2019-02-14 21:15:07,662 : Dev acc : 94.74 Test acc : 94.49

2019-02-14 21:15:07,663 : ***** Transfer task : SST Binary classification *****


2019-02-14 21:15:07,795 : loading BERT model bert-base-uncased
2019-02-14 21:15:07,796 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:15:07,822 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:15:07,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptxe5eow7
2019-02-14 21:15:10,228 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:15:11,671 : Computing embedding for train
2019-02-14 21:15:56,864 : Computed train embeddings
2019-02-14 21:15:56,864 : Computing embedding for dev
2019-02-14 21:15:57,792 : Computed dev embeddings
2019-02-14 21:15:57,792 : Computing embedding for test
2019-02-14 21:15:59,771 : Computed test embeddings
2019-02-14 21:15:59,771 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:16:20,082 : [('reg:1e-05', 84.52), ('reg:0.0001', 84.17), ('reg:0.001', 83.26), ('reg:0.01', 81.19)]
2019-02-14 21:16:20,082 : Validation : best param found is reg = 1e-05 with score             84.52
2019-02-14 21:16:20,082 : Evaluating...
2019-02-14 21:16:25,162 : 
Dev acc : 84.52 Test acc : 84.18 for             SST Binary classification

2019-02-14 21:16:25,163 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-14 21:16:25,231 : loading BERT model bert-base-uncased
2019-02-14 21:16:25,231 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:16:25,256 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:16:25,257 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5ogncqmz
2019-02-14 21:16:27,740 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:16:29,143 : Computing embedding for train
2019-02-14 21:16:39,434 : Computed train embeddings
2019-02-14 21:16:39,434 : Computing embedding for dev
2019-02-14 21:16:40,779 : Computed dev embeddings
2019-02-14 21:16:40,779 : Computing embedding for test
2019-02-14 21:16:43,137 : Computed test embeddings
2019-02-14 21:16:43,137 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:16:45,466 : [('reg:1e-05', 42.42), ('reg:0.0001', 42.51), ('reg:0.001', 41.24), ('reg:0.01', 40.42)]
2019-02-14 21:16:45,466 : Validation : best param found is reg = 0.0001 with score             42.51
2019-02-14 21:16:45,466 : Evaluating...
2019-02-14 21:16:45,961 : 
Dev acc : 42.51 Test acc : 43.26 for             SST Fine-Grained classification

2019-02-14 21:16:45,961 : ***** Transfer task : TREC *****


2019-02-14 21:16:45,974 : loading BERT model bert-base-uncased
2019-02-14 21:16:45,974 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:16:45,993 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:16:45,993 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw6lyo6_5
2019-02-14 21:16:48,432 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:16:53,240 : Computed train embeddings
2019-02-14 21:16:53,502 : Computed test embeddings
2019-02-14 21:16:53,502 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 21:17:02,481 : [('reg:1e-05', 65.52), ('reg:0.0001', 62.84), ('reg:0.001', 48.28), ('reg:0.01', 36.58)]
2019-02-14 21:17:02,481 : Cross-validation : best param found is reg = 1e-05             with score 65.52
2019-02-14 21:17:02,481 : Evaluating...
2019-02-14 21:17:02,880 : 
Dev acc : 65.52 Test acc : 69.2             for TREC

2019-02-14 21:17:02,881 : ***** Transfer task : MRPC *****


2019-02-14 21:17:02,904 : loading BERT model bert-base-uncased
2019-02-14 21:17:02,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:17:02,929 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:17:02,929 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphi3o604n
2019-02-14 21:17:05,318 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:17:06,784 : Computing embedding for train
2019-02-14 21:17:16,378 : Computed train embeddings
2019-02-14 21:17:16,379 : Computing embedding for test
2019-02-14 21:17:20,537 : Computed test embeddings
2019-02-14 21:17:20,554 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-14 21:17:26,708 : [('reg:1e-05', 68.72), ('reg:0.0001', 68.74), ('reg:0.001', 68.28), ('reg:0.01', 67.93)]
2019-02-14 21:17:26,708 : Cross-validation : best param found is reg = 0.0001             with score 68.74
2019-02-14 21:17:26,708 : Evaluating...
2019-02-14 21:17:27,071 : Dev acc : 68.74 Test acc 69.62; Test F1 80.39 for MRPC.

2019-02-14 21:17:27,071 : ***** Transfer task : SICK-Entailment*****


2019-02-14 21:17:27,138 : loading BERT model bert-base-uncased
2019-02-14 21:17:27,138 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:17:27,161 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:17:27,161 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpoh2rb9wm
2019-02-14 21:17:29,583 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:17:31,003 : Computing embedding for train
2019-02-14 21:17:36,582 : Computed train embeddings
2019-02-14 21:17:36,583 : Computing embedding for dev
2019-02-14 21:17:37,322 : Computed dev embeddings
2019-02-14 21:17:37,322 : Computing embedding for test
2019-02-14 21:17:43,149 : Computed test embeddings
2019-02-14 21:17:43,177 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:17:44,939 : [('reg:1e-05', 69.0), ('reg:0.0001', 67.4), ('reg:0.001', 61.4), ('reg:0.01', 56.4)]
2019-02-14 21:17:44,939 : Validation : best param found is reg = 1e-05 with score             69.0
2019-02-14 21:17:44,939 : Evaluating...
2019-02-14 21:17:45,522 : 
Dev acc : 69.0 Test acc : 68.07 for                        SICK entailment

2019-02-14 21:17:45,523 : ***** Transfer task : SICK-Relatedness*****


2019-02-14 21:17:45,549 : loading BERT model bert-base-uncased
2019-02-14 21:17:45,549 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:17:45,605 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:17:45,605 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3rm_8fm5
2019-02-14 21:17:47,972 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:17:49,430 : Computing embedding for train
2019-02-14 21:17:54,481 : Computed train embeddings
2019-02-14 21:17:54,481 : Computing embedding for dev
2019-02-14 21:17:55,172 : Computed dev embeddings
2019-02-14 21:17:55,173 : Computing embedding for test
2019-02-14 21:18:00,674 : Computed test embeddings
2019-02-14 21:19:14,335 : Dev : Pearson 0.6095643658240335
2019-02-14 21:19:14,335 : Test : Pearson 0.6611467655784379 Spearman 0.6225046762207509 MSE 0.5729367137475779                        for SICK Relatedness

2019-02-14 21:19:14,336 : 

***** Transfer task : STSBenchmark*****


2019-02-14 21:19:14,374 : loading BERT model bert-base-uncased
2019-02-14 21:19:14,374 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:19:14,402 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:19:14,402 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp163725ng
2019-02-14 21:19:16,790 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:19:18,210 : Computing embedding for train
2019-02-14 21:19:26,179 : Computed train embeddings
2019-02-14 21:19:26,179 : Computing embedding for dev
2019-02-14 21:19:28,589 : Computed dev embeddings
2019-02-14 21:19:28,589 : Computing embedding for test
2019-02-14 21:19:30,552 : Computed test embeddings
2019-02-14 21:19:53,181 : Dev : Pearson 0.44628606987289704
2019-02-14 21:19:53,181 : Test : Pearson 0.38206788826701005 Spearman 0.39195045625178027 MSE 2.132797179579039                        for SICK Relatedness

2019-02-14 21:19:53,181 : ***** Transfer task : SNLI Entailment*****


2019-02-14 21:19:58,053 : loading BERT model bert-base-uncased
2019-02-14 21:19:58,053 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:19:58,183 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:19:58,183 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmb00701p
2019-02-14 21:20:00,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:20:02,196 : PROGRESS (encoding): 0.00%
2019-02-14 21:21:19,993 : PROGRESS (encoding): 14.56%
2019-02-14 21:22:46,000 : PROGRESS (encoding): 29.12%
2019-02-14 21:24:12,469 : PROGRESS (encoding): 43.69%
2019-02-14 21:25:47,463 : PROGRESS (encoding): 58.25%
2019-02-14 21:27:30,970 : PROGRESS (encoding): 72.81%
2019-02-14 21:29:14,347 : PROGRESS (encoding): 87.37%
2019-02-14 21:31:03,629 : PROGRESS (encoding): 0.00%
2019-02-14 21:31:17,065 : PROGRESS (encoding): 0.00%
2019-02-14 21:31:30,066 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 21:32:15,674 : [('reg:1e-09', 57.76)]
2019-02-14 21:32:15,674 : Validation : best param found is reg = 1e-09 with score             57.76
2019-02-14 21:32:15,674 : Evaluating...
2019-02-14 21:33:01,409 : Dev acc : 57.76 Test acc : 58.43 for SNLI

2019-02-14 21:33:01,409 : ***** Transfer task: Image Caption Retrieval *****


2019-02-14 21:33:10,112 : loading BERT model bert-base-uncased
2019-02-14 21:33:10,113 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 21:33:10,163 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 21:33:10,163 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmptcqluk78
2019-02-14 21:33:12,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 21:33:13,957 : Computing embedding for train
2019-02-14 21:40:44,271 : Computed train embeddings
2019-02-14 21:40:44,271 : Computing embedding for dev
2019-02-14 21:41:04,136 : Computed dev embeddings
2019-02-14 21:41:04,136 : Computing embedding for test
2019-02-14 21:41:24,529 : Computed test embeddings
2019-02-14 21:41:24,544 : prepare data
2019-02-14 21:41:24,609 : start epoch
2019-02-14 21:42:05,431 : samples : 64000
2019-02-14 21:42:15,193 : Image to text: 3.16, 11.42, 19.78, 50.0
2019-02-14 21:42:22,278 : Text to Image: 2.812, 10.192, 16.872, 57.0
2019-02-14 21:43:03,550 : samples : 128000
2019-02-14 21:43:13,288 : Image to text: 4.42, 16.08, 25.74, 35.0
2019-02-14 21:43:20,354 : Text to Image: 3.688, 13.328, 21.38, 42.0
2019-02-14 21:44:02,034 : samples : 192000
2019-02-14 21:44:11,753 : Image to text: 4.58, 15.38, 24.9, 36.0
2019-02-14 21:44:18,832 : Text to Image: 3.948, 13.84, 22.232, 41.0
2019-02-14 21:44:59,100 : samples : 256000
2019-02-14 21:45:08,871 : Image to text: 4.3, 15.68, 25.3, 35.0
2019-02-14 21:45:15,926 : Text to Image: 3.66, 13.104, 21.312, 42.0
2019-02-14 21:45:56,601 : samples : 320000
2019-02-14 21:46:07,598 : Image to text: 5.56, 18.14, 28.86, 30.0
2019-02-14 21:46:17,550 : Text to Image: 4.268, 15.1, 24.172, 37.0
2019-02-14 21:47:00,401 : samples : 384000
2019-02-14 21:47:12,953 : Image to text: 4.74, 17.84, 27.22, 30.0
2019-02-14 21:47:22,854 : Text to Image: 4.42, 15.276, 24.084, 37.0
2019-02-14 21:48:03,488 : samples : 448000
2019-02-14 21:48:13,415 : Image to text: 5.64, 18.68, 29.02, 29.0
2019-02-14 21:48:21,419 : Text to Image: 4.632, 16.12, 25.704, 33.0
2019-02-14 21:49:03,154 : samples : 512000
2019-02-14 21:49:15,643 : Image to text: 5.44, 19.36, 30.04, 28.0
2019-02-14 21:49:25,651 : Text to Image: 4.56, 15.116, 24.088, 36.0
2019-02-14 21:50:00,530 : Epoch 1 finished
2019-02-14 21:50:00,980 : Image to text: 18.0, 47.2, 64.0, 6.0
2019-02-14 21:50:01,345 : Text to Image: 15.22, 42.92, 59.82, 7.0
2019-02-14 21:50:01,795 : Image to text: 20.1, 47.5, 63.1, 6.0
2019-02-14 21:50:02,160 : Text to Image: 14.98, 44.06, 61.7, 7.0
2019-02-14 21:50:02,610 : Image to text: 19.6, 49.5, 66.6, 6.0
2019-02-14 21:50:02,975 : Text to Image: 16.1, 43.8, 60.2, 7.0
2019-02-14 21:50:03,425 : Image to text: 19.1, 48.3, 63.3, 6.0
2019-02-14 21:50:03,789 : Text to Image: 15.62, 43.54, 61.62, 7.0
2019-02-14 21:50:04,238 : Image to text: 18.3, 48.9, 65.2, 6.0
2019-02-14 21:50:04,586 : Text to Image: 15.82, 45.04, 61.36, 7.0
2019-02-14 21:50:04,586 : Dev mean Text to Image: 15.548, 43.872, 60.94, 7.0
2019-02-14 21:50:04,586 : Dev mean Image to text: 19.020000000000003, 48.28, 64.44000000000001, 6.0
2019-02-14 21:50:04,587 : start epoch
2019-02-14 21:50:44,530 : samples : 64000
2019-02-14 21:50:57,088 : Image to text: 6.72, 21.42, 30.78, 26.0
2019-02-14 21:51:07,065 : Text to Image: 5.184, 17.624, 27.244, 31.0
2019-02-14 21:51:50,616 : samples : 128000
2019-02-14 21:52:02,531 : Image to text: 6.06, 20.24, 30.94, 25.0
2019-02-14 21:52:09,659 : Text to Image: 5.684, 18.532, 28.408, 28.0
2019-02-14 21:52:50,767 : samples : 192000
2019-02-14 21:53:03,309 : Image to text: 6.06, 20.2, 30.54, 26.0
2019-02-14 21:53:13,226 : Text to Image: 5.256, 17.74, 27.792, 30.0
2019-02-14 21:53:55,944 : samples : 256000
2019-02-14 21:54:05,945 : Image to text: 6.38, 20.92, 32.68, 24.0
2019-02-14 21:54:13,026 : Text to Image: 5.368, 17.348, 27.168, 30.0
2019-02-14 21:54:54,019 : samples : 320000
2019-02-14 21:55:06,542 : Image to text: 6.86, 21.9, 32.84, 23.0
2019-02-14 21:55:16,498 : Text to Image: 5.48, 18.168, 28.384, 29.0
2019-02-14 21:55:59,047 : samples : 384000
2019-02-14 21:56:09,000 : Image to text: 6.18, 20.94, 31.56, 24.0
2019-02-14 21:56:16,047 : Text to Image: 5.432, 18.22, 28.664, 29.0
2019-02-14 21:56:57,247 : samples : 448000
2019-02-14 21:57:09,725 : Image to text: 6.62, 20.62, 31.68, 25.0
2019-02-14 21:57:19,677 : Text to Image: 5.064, 17.64, 27.632, 30.0
2019-02-14 21:58:02,476 : samples : 512000
2019-02-14 21:58:12,431 : Image to text: 6.66, 22.38, 33.82, 23.0
2019-02-14 21:58:19,492 : Text to Image: 6.036, 19.324, 29.812, 27.0
2019-02-14 21:58:54,224 : Epoch 2 finished
2019-02-14 21:58:55,119 : Image to text: 20.8, 53.0, 68.7, 5.0
2019-02-14 21:58:55,873 : Text to Image: 17.66, 47.68, 64.52, 6.0
2019-02-14 21:58:56,785 : Image to text: 20.3, 47.5, 64.6, 6.0
2019-02-14 21:58:57,524 : Text to Image: 16.44, 48.22, 65.46, 6.0
2019-02-14 21:58:58,442 : Image to text: 20.2, 50.0, 66.7, 5.0
2019-02-14 21:58:59,210 : Text to Image: 16.5, 46.16, 63.16, 6.0
2019-02-14 21:59:00,135 : Image to text: 20.5, 52.2, 68.7, 5.0
2019-02-14 21:59:00,900 : Text to Image: 17.66, 47.32, 64.38, 6.0
2019-02-14 21:59:01,806 : Image to text: 20.8, 52.5, 65.9, 5.0
2019-02-14 21:59:02,572 : Text to Image: 18.26, 47.62, 64.24, 6.0
2019-02-14 21:59:02,572 : Dev mean Text to Image: 17.304000000000002, 47.4, 64.352, 6.0
2019-02-14 21:59:02,572 : Dev mean Image to text: 20.52, 51.040000000000006, 66.92, 5.2
2019-02-14 21:59:02,573 : start epoch
2019-02-14 21:59:45,214 : samples : 64000
2019-02-14 21:59:57,720 : Image to text: 7.2, 22.8, 34.54, 21.0
2019-02-14 22:00:07,644 : Text to Image: 6.18, 20.072, 30.512, 26.0
2019-02-14 22:00:47,909 : samples : 128000
2019-02-14 22:00:59,476 : Image to text: 5.88, 21.38, 32.02, 24.0
2019-02-14 22:01:09,428 : Text to Image: 5.336, 17.936, 28.256, 29.0
2019-02-14 22:01:51,793 : samples : 192000
2019-02-14 22:02:04,302 : Image to text: 7.26, 22.24, 33.82, 22.0
2019-02-14 22:02:14,246 : Text to Image: 6.176, 19.72, 30.348, 26.0
2019-02-14 22:02:54,262 : samples : 256000
2019-02-14 22:03:04,170 : Image to text: 7.22, 23.28, 35.04, 21.0
2019-02-14 22:03:12,210 : Text to Image: 6.372, 19.824, 30.588, 26.0
2019-02-14 22:03:54,829 : samples : 320000
2019-02-14 22:04:07,409 : Image to text: 6.7, 22.72, 34.44, 23.0
2019-02-14 22:04:17,402 : Text to Image: 5.956, 19.62, 29.896, 27.0
2019-02-14 22:04:59,362 : samples : 384000
2019-02-14 22:05:09,288 : Image to text: 7.64, 23.1, 34.14, 21.0
2019-02-14 22:05:16,327 : Text to Image: 6.392, 20.232, 30.764, 25.0
2019-02-14 22:05:57,181 : samples : 448000
2019-02-14 22:06:09,684 : Image to text: 7.26, 22.66, 33.48, 23.0
2019-02-14 22:06:19,652 : Text to Image: 6.204, 19.78, 30.028, 26.0
2019-02-14 22:07:02,451 : samples : 512000
2019-02-14 22:07:12,554 : Image to text: 6.88, 22.48, 33.08, 22.0
2019-02-14 22:07:19,661 : Text to Image: 6.036, 20.236, 30.784, 26.0
2019-02-14 22:07:54,347 : Epoch 3 finished
2019-02-14 22:07:54,695 : Image to text: 20.0, 53.0, 67.0, 5.0
2019-02-14 22:07:54,958 : Text to Image: 16.9, 46.98, 63.92, 6.0
2019-02-14 22:07:55,311 : Image to text: 20.1, 51.6, 66.7, 5.0
2019-02-14 22:07:55,586 : Text to Image: 16.54, 46.5, 64.48, 6.0
2019-02-14 22:07:55,958 : Image to text: 20.2, 52.2, 69.4, 5.0
2019-02-14 22:07:56,224 : Text to Image: 16.2, 45.64, 62.76, 7.0
2019-02-14 22:07:56,585 : Image to text: 21.1, 51.8, 66.5, 5.0
2019-02-14 22:07:56,850 : Text to Image: 16.64, 45.82, 63.42, 6.0
2019-02-14 22:07:57,211 : Image to text: 21.2, 52.8, 68.5, 5.0
2019-02-14 22:07:57,911 : Text to Image: 17.28, 47.34, 64.18, 6.0
2019-02-14 22:07:57,911 : Dev mean Text to Image: 16.712, 46.456, 63.751999999999995, 6.2
2019-02-14 22:07:57,911 : Dev mean Image to text: 20.520000000000003, 52.28, 67.62, 5.0
2019-02-14 22:07:57,911 : start epoch
2019-02-14 22:08:39,490 : samples : 64000
2019-02-14 22:08:49,675 : Image to text: 7.58, 23.46, 35.44, 21.0
2019-02-14 22:08:56,863 : Text to Image: 5.98, 20.084, 30.584, 26.0
2019-02-14 22:09:37,150 : samples : 128000
2019-02-14 22:09:47,461 : Image to text: 6.28, 21.4, 32.28, 24.0
2019-02-14 22:09:55,070 : Text to Image: 5.48, 18.264, 28.34, 28.0
2019-02-14 22:10:35,355 : samples : 192000
2019-02-14 22:10:45,397 : Image to text: 7.86, 23.72, 35.96, 20.0
2019-02-14 22:10:52,568 : Text to Image: 6.484, 20.512, 31.372, 25.0
2019-02-14 22:11:33,409 : samples : 256000
2019-02-14 22:11:44,322 : Image to text: 8.04, 24.84, 36.9, 19.0
2019-02-14 22:11:51,462 : Text to Image: 6.964, 21.12, 32.14, 24.0
2019-02-14 22:12:31,593 : samples : 320000
2019-02-14 22:12:42,586 : Image to text: 7.88, 24.5, 35.62, 20.0
2019-02-14 22:12:52,591 : Text to Image: 6.648, 20.988, 31.808, 24.0
2019-02-14 22:13:34,867 : samples : 384000
2019-02-14 22:13:44,842 : Image to text: 7.78, 24.6, 37.14, 19.0
2019-02-14 22:13:51,936 : Text to Image: 6.9, 21.86, 32.712, 23.0
2019-02-14 22:14:33,127 : samples : 448000
2019-02-14 22:14:45,741 : Image to text: 7.5, 23.88, 35.76, 20.0
2019-02-14 22:14:52,717 : Text to Image: 7.052, 21.468, 32.468, 24.0
2019-02-14 22:15:33,433 : samples : 512000
2019-02-14 22:15:43,244 : Image to text: 7.36, 23.46, 35.2, 21.0
2019-02-14 22:15:53,076 : Text to Image: 6.5, 20.256, 30.836, 26.0
2019-02-14 22:16:27,042 : Epoch 4 finished
2019-02-14 22:16:27,980 : Image to text: 23.9, 53.7, 69.5, 4.0
2019-02-14 22:16:28,710 : Text to Image: 18.4, 48.86, 66.84, 6.0
2019-02-14 22:16:29,589 : Image to text: 22.3, 52.1, 67.6, 5.0
2019-02-14 22:16:30,311 : Text to Image: 18.0, 48.82, 67.8, 6.0
2019-02-14 22:16:31,221 : Image to text: 22.9, 54.4, 71.1, 5.0
2019-02-14 22:16:31,964 : Text to Image: 18.16, 48.44, 65.88, 6.0
2019-02-14 22:16:32,879 : Image to text: 24.4, 56.0, 72.0, 4.0
2019-02-14 22:16:33,601 : Text to Image: 18.84, 48.54, 66.7, 6.0
2019-02-14 22:16:34,497 : Image to text: 24.0, 54.8, 70.7, 4.0
2019-02-14 22:16:35,253 : Text to Image: 18.86, 48.9, 66.42, 6.0
2019-02-14 22:16:35,253 : Dev mean Text to Image: 18.451999999999998, 48.712, 66.72800000000001, 6.0
2019-02-14 22:16:35,253 : Dev mean Image to text: 23.5, 54.199999999999996, 70.18, 4.3999999999999995
2019-02-14 22:16:35,254 : start epoch
2019-02-14 22:17:17,786 : samples : 64000
2019-02-14 22:17:30,226 : Image to text: 8.12, 25.1, 36.78, 19.0
2019-02-14 22:17:40,040 : Text to Image: 6.988, 21.2, 31.7, 24.0
2019-02-14 22:18:22,626 : samples : 128000
2019-02-14 22:18:35,057 : Image to text: 8.52, 25.96, 37.62, 19.0
2019-02-14 22:18:44,847 : Text to Image: 7.136, 22.156, 33.22, 23.0
2019-02-14 22:19:27,363 : samples : 192000
2019-02-14 22:19:39,795 : Image to text: 7.64, 23.56, 35.6, 20.0
2019-02-14 22:19:49,718 : Text to Image: 6.6, 21.236, 32.168, 24.0
2019-02-14 22:20:32,138 : samples : 256000
2019-02-14 22:20:44,632 : Image to text: 7.64, 23.88, 35.06, 21.0
2019-02-14 22:20:54,573 : Text to Image: 6.632, 20.952, 31.764, 25.0
2019-02-14 22:21:37,216 : samples : 320000
2019-02-14 22:21:49,744 : Image to text: 8.26, 25.16, 37.66, 18.0
2019-02-14 22:21:59,700 : Text to Image: 7.252, 22.088, 32.964, 23.0
2019-02-14 22:22:41,739 : samples : 384000
2019-02-14 22:22:54,329 : Image to text: 7.0, 23.06, 34.84, 21.0
2019-02-14 22:23:04,269 : Text to Image: 6.256, 19.952, 30.3, 26.0
2019-02-14 22:23:45,002 : samples : 448000
2019-02-14 22:23:57,504 : Image to text: 7.78, 24.08, 36.36, 20.0
2019-02-14 22:24:07,459 : Text to Image: 6.488, 21.524, 32.1, 24.0
2019-02-14 22:24:48,035 : samples : 512000
2019-02-14 22:25:00,574 : Image to text: 8.28, 25.2, 36.64, 19.0
2019-02-14 22:25:10,504 : Text to Image: 7.036, 21.94, 33.136, 23.0
2019-02-14 22:25:46,001 : Epoch 5 finished
2019-02-14 22:25:46,916 : Image to text: 22.9, 57.0, 71.7, 4.0
2019-02-14 22:25:47,661 : Text to Image: 20.12, 50.7, 67.96, 5.0
2019-02-14 22:25:48,585 : Image to text: 22.9, 52.6, 68.3, 5.0
2019-02-14 22:25:49,323 : Text to Image: 18.76, 49.34, 68.12, 6.0
2019-02-14 22:25:50,256 : Image to text: 23.9, 55.4, 71.5, 4.0
2019-02-14 22:25:51,013 : Text to Image: 18.96, 50.48, 67.74, 5.0
2019-02-14 22:25:51,901 : Image to text: 25.3, 57.2, 70.5, 4.0
2019-02-14 22:25:52,662 : Text to Image: 19.34, 49.88, 67.86, 6.0
2019-02-14 22:25:53,576 : Image to text: 24.5, 58.2, 73.7, 4.0
2019-02-14 22:25:54,325 : Text to Image: 20.42, 50.78, 67.72, 5.0
2019-02-14 22:25:54,325 : Dev mean Text to Image: 19.52, 50.236000000000004, 67.88, 5.4
2019-02-14 22:25:54,325 : Dev mean Image to text: 23.9, 56.08, 71.14, 4.2
2019-02-14 22:25:54,326 : start epoch
2019-02-14 22:26:36,572 : samples : 64000
2019-02-14 22:26:49,147 : Image to text: 8.66, 26.06, 38.64, 18.0
2019-02-14 22:26:59,160 : Text to Image: 7.428, 23.104, 34.492, 22.0
2019-02-14 22:27:42,785 : samples : 128000
2019-02-14 22:27:55,369 : Image to text: 7.46, 24.22, 37.12, 20.0
2019-02-14 22:28:03,921 : Text to Image: 6.732, 20.896, 32.06, 24.0
2019-02-14 22:28:44,616 : samples : 192000
2019-02-14 22:28:54,693 : Image to text: 7.64, 24.16, 35.54, 21.0
2019-02-14 22:29:01,938 : Text to Image: 6.492, 21.096, 32.364, 24.0
2019-02-14 22:29:43,475 : samples : 256000
2019-02-14 22:29:56,308 : Image to text: 7.78, 23.72, 36.28, 20.0
2019-02-14 22:30:06,662 : Text to Image: 6.804, 21.4, 32.636, 24.0
2019-02-14 22:30:50,046 : samples : 320000
2019-02-14 22:31:02,899 : Image to text: 7.82, 24.76, 36.74, 18.0
2019-02-14 22:31:13,142 : Text to Image: 7.248, 22.32, 33.532, 22.0
2019-02-14 22:31:57,231 : samples : 384000
2019-02-14 22:32:10,060 : Image to text: 8.6, 26.12, 37.74, 19.0
2019-02-14 22:32:20,461 : Text to Image: 7.616, 23.248, 34.332, 22.0
2019-02-14 22:33:04,290 : samples : 448000
2019-02-14 22:33:17,133 : Image to text: 8.24, 25.56, 37.44, 19.0
2019-02-14 22:33:27,569 : Text to Image: 7.096, 21.776, 32.984, 22.0
2019-02-14 22:34:11,448 : samples : 512000
2019-02-14 22:34:24,322 : Image to text: 7.14, 22.8, 34.08, 22.0
2019-02-14 22:34:34,738 : Text to Image: 5.992, 19.34, 29.788, 26.0
2019-02-14 22:35:11,839 : Epoch 6 finished
2019-02-14 22:35:12,897 : Image to text: 24.1, 56.4, 71.9, 4.0
2019-02-14 22:35:13,878 : Text to Image: 19.82, 52.5, 69.66, 5.0
2019-02-14 22:35:14,978 : Image to text: 21.7, 53.9, 71.2, 5.0
2019-02-14 22:35:15,847 : Text to Image: 19.24, 51.28, 68.78, 5.0
2019-02-14 22:35:16,798 : Image to text: 23.2, 53.9, 70.9, 5.0
2019-02-14 22:35:17,662 : Text to Image: 19.18, 50.96, 68.6, 5.0
2019-02-14 22:35:18,656 : Image to text: 24.9, 56.4, 71.5, 4.0
2019-02-14 22:35:19,505 : Text to Image: 19.92, 50.98, 68.46, 5.0
2019-02-14 22:35:20,615 : Image to text: 22.8, 55.6, 72.4, 4.0
2019-02-14 22:35:21,453 : Text to Image: 20.44, 51.96, 67.98, 5.0
2019-02-14 22:35:21,453 : Dev mean Text to Image: 19.72, 51.536, 68.696, 5.0
2019-02-14 22:35:21,453 : Dev mean Image to text: 23.340000000000003, 55.239999999999995, 71.58000000000001, 4.3999999999999995
2019-02-14 22:35:21,453 : start epoch
2019-02-14 22:36:05,156 : samples : 64000
2019-02-14 22:36:17,972 : Image to text: 8.24, 24.94, 37.3, 19.0
2019-02-14 22:36:28,333 : Text to Image: 7.268, 22.308, 33.388, 22.0
2019-02-14 22:37:11,953 : samples : 128000
2019-02-14 22:37:24,812 : Image to text: 8.06, 23.86, 36.14, 20.0
2019-02-14 22:37:34,157 : Text to Image: 7.028, 21.496, 32.076, 24.0
2019-02-14 22:38:13,236 : samples : 192000
2019-02-14 22:38:23,050 : Image to text: 9.18, 25.7, 38.22, 18.0
2019-02-14 22:38:30,142 : Text to Image: 7.216, 22.436, 33.796, 22.0
2019-02-14 22:39:09,685 : samples : 256000
2019-02-14 22:39:19,431 : Image to text: 8.84, 26.12, 37.78, 18.0
2019-02-14 22:39:26,493 : Text to Image: 7.532, 22.824, 33.984, 21.0
2019-02-14 22:40:05,482 : samples : 320000
2019-02-14 22:40:15,208 : Image to text: 8.76, 26.4, 38.06, 18.0
2019-02-14 22:40:22,292 : Text to Image: 7.736, 23.26, 34.996, 21.0
2019-02-14 22:41:01,285 : samples : 384000
2019-02-14 22:41:11,007 : Image to text: 8.76, 26.3, 38.28, 18.0
2019-02-14 22:41:18,098 : Text to Image: 7.744, 23.276, 34.9, 21.0
2019-02-14 22:41:57,012 : samples : 448000
2019-02-14 22:42:06,790 : Image to text: 7.98, 25.86, 38.12, 18.0
2019-02-14 22:42:13,847 : Text to Image: 7.612, 23.232, 34.692, 22.0
2019-02-14 22:42:52,749 : samples : 512000
2019-02-14 22:43:02,492 : Image to text: 9.0, 26.4, 39.26, 17.0
2019-02-14 22:43:09,552 : Text to Image: 8.06, 23.408, 35.004, 21.0
2019-02-14 22:43:42,714 : Epoch 7 finished
2019-02-14 22:43:43,124 : Image to text: 24.7, 54.7, 72.0, 5.0
2019-02-14 22:43:43,438 : Text to Image: 20.22, 51.32, 69.74, 5.0
2019-02-14 22:43:43,858 : Image to text: 22.1, 54.7, 69.1, 5.0
2019-02-14 22:43:44,172 : Text to Image: 19.36, 52.32, 70.06, 5.0
2019-02-14 22:43:44,586 : Image to text: 22.6, 54.2, 71.1, 5.0
2019-02-14 22:43:44,902 : Text to Image: 19.66, 52.06, 68.78, 5.0
2019-02-14 22:43:45,336 : Image to text: 27.6, 56.6, 73.2, 4.0
2019-02-14 22:43:45,663 : Text to Image: 20.96, 52.6, 69.6, 5.0
2019-02-14 22:43:46,085 : Image to text: 22.5, 58.6, 74.6, 4.0
2019-02-14 22:43:46,408 : Text to Image: 21.08, 52.92, 69.4, 5.0
2019-02-14 22:43:46,408 : Dev mean Text to Image: 20.256, 52.244, 69.51599999999999, 5.0
2019-02-14 22:43:46,408 : Dev mean Image to text: 23.9, 55.76, 72.0, 4.6
2019-02-14 22:43:46,408 : start epoch
2019-02-14 22:44:26,358 : samples : 64000
2019-02-14 22:44:36,481 : Image to text: 8.92, 26.64, 39.14, 17.0
2019-02-14 22:44:43,705 : Text to Image: 8.032, 23.904, 35.26, 20.0
2019-02-14 22:45:23,875 : samples : 128000
2019-02-14 22:45:33,871 : Image to text: 9.5, 26.6, 39.64, 17.0
2019-02-14 22:45:40,949 : Text to Image: 7.956, 23.52, 34.92, 21.0
2019-02-14 22:46:20,905 : samples : 192000
2019-02-14 22:46:30,592 : Image to text: 8.86, 25.98, 38.36, 18.0
2019-02-14 22:46:37,516 : Text to Image: 7.612, 23.256, 34.84, 21.0
2019-02-14 22:47:20,242 : samples : 256000
2019-02-14 22:47:30,132 : Image to text: 9.0, 27.08, 39.72, 17.0
2019-02-14 22:47:37,288 : Text to Image: 8.068, 24.048, 35.648, 20.0
2019-02-14 22:48:20,442 : samples : 320000
2019-02-14 22:48:30,342 : Image to text: 9.16, 27.76, 40.18, 16.0
2019-02-14 22:48:37,492 : Text to Image: 7.764, 23.636, 35.1, 21.0
2019-02-14 22:49:18,482 : samples : 384000
2019-02-14 22:49:28,461 : Image to text: 8.7, 25.82, 39.36, 17.0
2019-02-14 22:49:35,693 : Text to Image: 7.456, 22.964, 34.328, 22.0
2019-02-14 22:50:18,214 : samples : 448000
2019-02-14 22:50:28,160 : Image to text: 9.56, 28.26, 40.82, 16.0
2019-02-14 22:50:35,394 : Text to Image: 8.016, 23.844, 35.516, 20.0
2019-02-14 22:51:17,435 : samples : 512000
2019-02-14 22:51:27,183 : Image to text: 8.76, 25.56, 38.3, 18.0
2019-02-14 22:51:34,226 : Text to Image: 7.5, 22.916, 33.856, 22.0
2019-02-14 22:52:09,908 : Epoch 8 finished
2019-02-14 22:52:10,320 : Image to text: 27.5, 60.5, 75.5, 4.0
2019-02-14 22:52:10,637 : Text to Image: 21.46, 53.76, 71.9, 5.0
2019-02-14 22:52:11,048 : Image to text: 24.4, 56.1, 71.4, 4.0
2019-02-14 22:52:11,364 : Text to Image: 21.0, 53.86, 70.96, 5.0
2019-02-14 22:52:11,778 : Image to text: 25.9, 57.4, 73.5, 4.0
2019-02-14 22:52:12,096 : Text to Image: 20.92, 53.18, 70.46, 5.0
2019-02-14 22:52:12,525 : Image to text: 26.0, 58.7, 74.4, 4.0
2019-02-14 22:52:12,845 : Text to Image: 20.92, 53.04, 69.98, 5.0
2019-02-14 22:52:13,258 : Image to text: 27.1, 59.9, 75.3, 4.0
2019-02-14 22:52:13,575 : Text to Image: 21.9, 53.76, 70.02, 5.0
2019-02-14 22:52:13,575 : Dev mean Text to Image: 21.240000000000002, 53.519999999999996, 70.664, 5.0
2019-02-14 22:52:13,575 : Dev mean Image to text: 26.18, 58.519999999999996, 74.02, 4.0
2019-02-14 22:52:13,575 : start epoch
2019-02-14 22:52:54,007 : samples : 64000
2019-02-14 22:53:04,042 : Image to text: 8.94, 27.76, 40.28, 16.0
2019-02-14 22:53:11,268 : Text to Image: 7.988, 24.056, 35.516, 20.0
2019-02-14 22:53:51,723 : samples : 128000
2019-02-14 22:54:01,781 : Image to text: 8.36, 26.02, 38.56, 18.0
2019-02-14 22:54:08,978 : Text to Image: 7.608, 23.236, 34.34, 21.0
2019-02-14 22:54:49,631 : samples : 192000
2019-02-14 22:54:59,430 : Image to text: 9.24, 27.02, 40.42, 17.0
2019-02-14 22:55:06,591 : Text to Image: 7.852, 24.04, 35.664, 20.0
2019-02-14 22:55:48,123 : samples : 256000
2019-02-14 22:55:57,903 : Image to text: 8.7, 25.3, 37.98, 18.0
2019-02-14 22:56:05,015 : Text to Image: 6.932, 22.152, 33.712, 22.0
2019-02-14 22:56:46,586 : samples : 320000
2019-02-14 22:56:56,372 : Image to text: 9.44, 27.4, 39.78, 17.0
2019-02-14 22:57:03,433 : Text to Image: 7.952, 23.708, 35.296, 21.0
2019-02-14 22:57:42,673 : samples : 384000
2019-02-14 22:57:52,487 : Image to text: 8.92, 26.08, 38.7, 18.0
2019-02-14 22:57:59,775 : Text to Image: 7.884, 23.3, 34.772, 22.0
2019-02-14 22:58:41,417 : samples : 448000
2019-02-14 22:58:51,360 : Image to text: 9.22, 27.2, 39.62, 17.0
2019-02-14 22:58:58,598 : Text to Image: 7.92, 23.956, 35.392, 21.0
2019-02-14 22:59:40,789 : samples : 512000
2019-02-14 22:59:50,788 : Image to text: 9.56, 27.16, 39.84, 17.0
2019-02-14 22:59:58,006 : Text to Image: 8.056, 24.068, 35.68, 20.0
2019-02-14 23:00:34,537 : Epoch 9 finished
2019-02-14 23:00:34,971 : Image to text: 24.8, 56.7, 73.3, 4.0
2019-02-14 23:00:35,295 : Text to Image: 20.9, 54.08, 71.14, 5.0
2019-02-14 23:00:35,717 : Image to text: 24.2, 56.0, 71.4, 4.0
2019-02-14 23:00:36,041 : Text to Image: 20.64, 52.34, 70.88, 5.0
2019-02-14 23:00:36,474 : Image to text: 25.1, 58.3, 74.4, 4.0
2019-02-14 23:00:36,800 : Text to Image: 20.68, 53.52, 70.76, 5.0
2019-02-14 23:00:37,225 : Image to text: 25.8, 57.9, 75.1, 4.0
2019-02-14 23:00:37,552 : Text to Image: 21.54, 52.7, 70.14, 5.0
2019-02-14 23:00:37,981 : Image to text: 25.8, 58.8, 73.0, 4.0
2019-02-14 23:00:38,308 : Text to Image: 21.9, 53.72, 69.98, 5.0
2019-02-14 23:00:38,309 : Dev mean Text to Image: 21.131999999999998, 53.272, 70.58, 5.0
2019-02-14 23:00:38,309 : Dev mean Image to text: 25.14, 57.54, 73.44, 4.0
2019-02-14 23:00:38,309 : start epoch
2019-02-14 23:01:19,620 : samples : 64000
2019-02-14 23:01:29,784 : Image to text: 8.46, 26.56, 38.8, 18.0
2019-02-14 23:01:37,063 : Text to Image: 7.396, 22.448, 34.524, 22.0
2019-02-14 23:02:17,331 : samples : 128000
2019-02-14 23:02:27,352 : Image to text: 9.2, 27.5, 39.8, 17.0
2019-02-14 23:02:34,570 : Text to Image: 8.128, 24.428, 35.968, 20.0
2019-02-14 23:03:15,547 : samples : 192000
2019-02-14 23:03:25,343 : Image to text: 9.44, 27.98, 40.9, 16.0
2019-02-14 23:03:32,527 : Text to Image: 8.144, 24.056, 35.68, 20.0
2019-02-14 23:04:14,585 : samples : 256000
2019-02-14 23:04:24,410 : Image to text: 9.98, 28.16, 40.68, 16.0
2019-02-14 23:04:31,512 : Text to Image: 8.1, 24.34, 35.732, 20.0
2019-02-14 23:05:13,918 : samples : 320000
2019-02-14 23:05:24,011 : Image to text: 8.84, 25.8, 38.44, 18.0
2019-02-14 23:05:31,345 : Text to Image: 7.568, 23.336, 34.816, 21.0
2019-02-14 23:06:13,504 : samples : 384000
2019-02-14 23:06:23,501 : Image to text: 9.96, 28.28, 40.46, 17.0
2019-02-14 23:06:30,830 : Text to Image: 8.208, 24.56, 36.352, 20.0
2019-02-14 23:07:12,566 : samples : 448000
2019-02-14 23:07:22,553 : Image to text: 8.86, 26.1, 38.3, 18.0
2019-02-14 23:07:29,791 : Text to Image: 7.368, 22.424, 33.212, 23.0
2019-02-14 23:08:11,189 : samples : 512000
2019-02-14 23:08:21,174 : Image to text: 9.6, 27.48, 39.22, 17.0
2019-02-14 23:08:28,390 : Text to Image: 7.756, 23.588, 35.056, 21.0
2019-02-14 23:09:04,029 : Epoch 10 finished
2019-02-14 23:09:04,443 : Image to text: 27.4, 58.3, 73.5, 4.0
2019-02-14 23:09:04,758 : Text to Image: 20.7, 53.82, 71.48, 5.0
2019-02-14 23:09:05,169 : Image to text: 23.3, 56.0, 71.2, 4.0
2019-02-14 23:09:05,484 : Text to Image: 20.66, 53.16, 71.24, 5.0
2019-02-14 23:09:05,905 : Image to text: 25.2, 56.4, 75.5, 4.0
2019-02-14 23:09:06,222 : Text to Image: 22.06, 53.66, 70.98, 5.0
2019-02-14 23:09:06,632 : Image to text: 26.4, 57.4, 73.4, 4.0
2019-02-14 23:09:06,948 : Text to Image: 20.92, 52.54, 70.6, 5.0
2019-02-14 23:09:07,363 : Image to text: 27.2, 60.4, 74.1, 3.0
2019-02-14 23:09:07,681 : Text to Image: 22.18, 53.64, 69.94, 5.0
2019-02-14 23:09:07,682 : Dev mean Text to Image: 21.304, 53.364, 70.848, 5.0
2019-02-14 23:09:07,682 : Dev mean Image to text: 25.9, 57.7, 73.53999999999999, 3.8000000000000003
2019-02-14 23:09:07,682 : start epoch
2019-02-14 23:09:47,189 : samples : 64000
2019-02-14 23:09:57,342 : Image to text: 9.12, 27.74, 39.6, 17.0
2019-02-14 23:10:04,608 : Text to Image: 8.024, 24.212, 35.924, 20.0
2019-02-14 23:10:43,856 : samples : 128000
2019-02-14 23:10:54,011 : Image to text: 10.12, 27.92, 40.34, 16.0
2019-02-14 23:11:01,258 : Text to Image: 8.192, 24.68, 36.66, 19.0
2019-02-14 23:11:40,235 : samples : 192000
2019-02-14 23:11:50,095 : Image to text: 8.5, 25.84, 39.28, 17.0
2019-02-14 23:11:57,175 : Text to Image: 7.784, 23.82, 35.232, 21.0
2019-02-14 23:12:36,884 : samples : 256000
2019-02-14 23:12:46,760 : Image to text: 9.88, 28.22, 41.58, 16.0
2019-02-14 23:12:53,896 : Text to Image: 8.536, 24.896, 36.416, 20.0
2019-02-14 23:13:32,745 : samples : 320000
2019-02-14 23:13:42,595 : Image to text: 9.64, 27.56, 39.62, 17.0
2019-02-14 23:13:49,794 : Text to Image: 7.912, 23.836, 35.416, 21.0
2019-02-14 23:14:28,957 : samples : 384000
2019-02-14 23:14:38,785 : Image to text: 10.22, 28.06, 41.22, 16.0
2019-02-14 23:14:45,893 : Text to Image: 8.032, 24.672, 36.424, 19.0
2019-02-14 23:15:31,556 : samples : 448000
2019-02-14 23:15:41,431 : Image to text: 10.22, 28.94, 41.7, 15.0
2019-02-14 23:15:48,568 : Text to Image: 8.556, 25.304, 37.288, 19.0
2019-02-14 23:16:27,463 : samples : 512000
2019-02-14 23:16:37,308 : Image to text: 9.66, 28.18, 40.38, 16.0
2019-02-14 23:16:44,441 : Text to Image: 8.032, 24.264, 35.776, 20.0
2019-02-14 23:17:17,649 : Epoch 11 finished
2019-02-14 23:17:18,058 : Image to text: 25.2, 59.5, 75.2, 4.0
2019-02-14 23:17:18,372 : Text to Image: 21.9, 55.08, 72.46, 5.0
2019-02-14 23:17:18,792 : Image to text: 23.4, 57.5, 71.5, 4.0
2019-02-14 23:17:19,106 : Text to Image: 20.28, 54.64, 72.22, 5.0
2019-02-14 23:17:19,517 : Image to text: 26.4, 59.5, 74.7, 4.0
2019-02-14 23:17:19,835 : Text to Image: 21.52, 54.28, 71.44, 5.0
2019-02-14 23:17:20,262 : Image to text: 27.7, 58.3, 74.1, 4.0
2019-02-14 23:17:20,581 : Text to Image: 21.56, 54.06, 72.0, 5.0
2019-02-14 23:17:20,995 : Image to text: 25.8, 59.2, 73.9, 4.0
2019-02-14 23:17:21,321 : Text to Image: 22.48, 54.94, 71.54, 4.0
2019-02-14 23:17:21,322 : Dev mean Text to Image: 21.548000000000002, 54.60000000000001, 71.932, 4.8
2019-02-14 23:17:21,322 : Dev mean Image to text: 25.7, 58.8, 73.88, 4.0
2019-02-14 23:17:21,322 : start epoch
2019-02-14 23:18:00,518 : samples : 64000
2019-02-14 23:18:10,613 : Image to text: 9.44, 27.02, 40.24, 16.0
2019-02-14 23:18:17,850 : Text to Image: 8.004, 23.82, 35.292, 20.0
2019-02-14 23:18:57,305 : samples : 128000
2019-02-14 23:19:07,486 : Image to text: 10.06, 29.46, 42.2, 16.0
2019-02-14 23:19:14,694 : Text to Image: 8.564, 24.984, 37.092, 19.0
2019-02-14 23:20:00,885 : samples : 192000
2019-02-14 23:20:10,730 : Image to text: 10.06, 29.28, 41.52, 16.0
2019-02-14 23:20:17,791 : Text to Image: 8.632, 24.928, 36.76, 19.0
2019-02-14 23:20:56,509 : samples : 256000
2019-02-14 23:21:06,379 : Image to text: 10.14, 28.28, 41.12, 15.0
2019-02-14 23:21:13,448 : Text to Image: 8.032, 24.372, 36.332, 19.0
2019-02-14 23:21:52,305 : samples : 320000
2019-02-14 23:22:02,127 : Image to text: 9.44, 27.8, 40.28, 17.0
2019-02-14 23:22:09,189 : Text to Image: 7.804, 23.64, 35.408, 21.0
2019-02-14 23:22:49,675 : samples : 384000
2019-02-14 23:22:59,506 : Image to text: 9.46, 27.2, 40.7, 16.0
2019-02-14 23:23:06,581 : Text to Image: 7.96, 24.112, 35.892, 20.0
2019-02-14 23:23:49,070 : samples : 448000
2019-02-14 23:23:58,887 : Image to text: 10.08, 28.74, 41.38, 16.0
2019-02-14 23:24:06,029 : Text to Image: 8.276, 24.748, 36.336, 20.0
2019-02-14 23:24:48,292 : samples : 512000
2019-02-14 23:24:58,087 : Image to text: 9.44, 27.36, 40.76, 16.0
2019-02-14 23:25:05,202 : Text to Image: 8.048, 24.416, 36.428, 19.0
2019-02-14 23:25:40,908 : Epoch 12 finished
2019-02-14 23:25:41,330 : Image to text: 26.3, 58.4, 74.3, 4.0
2019-02-14 23:25:41,645 : Text to Image: 21.34, 54.4, 72.04, 5.0
2019-02-14 23:25:42,055 : Image to text: 25.3, 56.8, 71.7, 4.0
2019-02-14 23:25:42,375 : Text to Image: 21.5, 54.56, 72.34, 5.0
2019-02-14 23:25:42,791 : Image to text: 25.9, 58.6, 75.0, 4.0
2019-02-14 23:25:43,108 : Text to Image: 21.4, 54.54, 71.8, 5.0
2019-02-14 23:25:43,523 : Image to text: 27.4, 61.1, 74.6, 4.0
2019-02-14 23:25:43,854 : Text to Image: 21.9, 54.26, 70.9, 5.0
2019-02-14 23:25:44,269 : Image to text: 26.1, 61.2, 75.5, 4.0
2019-02-14 23:25:44,586 : Text to Image: 22.52, 55.32, 71.74, 4.0
2019-02-14 23:25:44,586 : Dev mean Text to Image: 21.732, 54.61600000000001, 71.76400000000001, 4.8
2019-02-14 23:25:44,586 : Dev mean Image to text: 26.200000000000003, 59.22, 74.22, 4.0
2019-02-14 23:25:44,587 : start epoch
2019-02-14 23:26:25,897 : samples : 64000
2019-02-14 23:26:36,053 : Image to text: 10.14, 27.46, 40.12, 17.0
2019-02-14 23:26:43,260 : Text to Image: 8.22, 24.26, 35.92, 20.0
2019-02-14 23:27:24,413 : samples : 128000
2019-02-14 23:27:34,525 : Image to text: 10.28, 29.08, 41.7, 15.0
2019-02-14 23:27:41,685 : Text to Image: 8.384, 25.352, 36.936, 19.0
2019-02-14 23:28:23,981 : samples : 192000
2019-02-14 23:28:33,774 : Image to text: 9.18, 26.84, 38.86, 18.0
2019-02-14 23:28:40,770 : Text to Image: 7.512, 23.08, 34.472, 22.0
2019-02-14 23:29:24,531 : samples : 256000
2019-02-14 23:29:34,283 : Image to text: 9.44, 26.92, 39.28, 17.0
2019-02-14 23:29:41,283 : Text to Image: 8.2, 24.108, 35.76, 20.0
2019-02-14 23:30:23,515 : samples : 320000
2019-02-14 23:30:33,269 : Image to text: 10.08, 29.16, 41.08, 16.0
2019-02-14 23:30:40,261 : Text to Image: 8.588, 25.204, 37.328, 19.0
2019-02-14 23:31:22,437 : samples : 384000
2019-02-14 23:31:32,198 : Image to text: 10.1, 29.28, 42.58, 15.0
2019-02-14 23:31:39,189 : Text to Image: 8.26, 24.788, 36.888, 19.0
2019-02-14 23:32:18,826 : samples : 448000
2019-02-14 23:32:28,557 : Image to text: 9.04, 27.56, 40.2, 16.0
2019-02-14 23:32:35,575 : Text to Image: 7.7, 23.808, 35.412, 21.0
2019-02-14 23:33:14,410 : samples : 512000
2019-02-14 23:33:24,215 : Image to text: 9.8, 28.2, 40.5, 16.0
2019-02-14 23:33:31,233 : Text to Image: 8.06, 24.284, 36.336, 20.0
2019-02-14 23:34:04,455 : Epoch 13 finished
2019-02-14 23:34:04,864 : Image to text: 24.0, 57.5, 73.3, 4.0
2019-02-14 23:34:05,178 : Text to Image: 20.96, 54.42, 71.74, 5.0
2019-02-14 23:34:05,598 : Image to text: 24.6, 55.5, 69.6, 4.0
2019-02-14 23:34:05,913 : Text to Image: 20.78, 53.92, 72.52, 5.0
2019-02-14 23:34:06,323 : Image to text: 25.6, 58.9, 73.4, 4.0
2019-02-14 23:34:06,641 : Text to Image: 21.2, 54.88, 71.82, 5.0
2019-02-14 23:34:07,068 : Image to text: 25.1, 59.1, 73.2, 4.0
2019-02-14 23:34:07,386 : Text to Image: 21.44, 53.62, 71.0, 5.0
2019-02-14 23:34:07,800 : Image to text: 24.0, 56.9, 72.8, 4.0
2019-02-14 23:34:08,117 : Text to Image: 22.1, 54.32, 70.42, 5.0
2019-02-14 23:34:08,117 : Dev mean Text to Image: 21.296, 54.232, 71.5, 5.0
2019-02-14 23:34:08,117 : Dev mean Image to text: 24.66, 57.58, 72.46, 4.0
2019-02-14 23:34:11,981 : 
Test scores | Image to text:             26.52, 60.0, 75.55999999999999, 3.5999999999999996
2019-02-14 23:34:11,981 : Test scores | Text to image:             21.943999999999996, 54.772, 71.668, 4.6

2019-02-14 23:34:12,082 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-14 23:34:12,428 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-14 23:34:13,041 : loading BERT model bert-base-uncased
2019-02-14 23:34:13,041 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:34:13,070 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:34:13,070 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7smpkbfj
2019-02-14 23:34:15,426 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:34:16,841 : Computing embeddings for train/dev/test
2019-02-14 23:35:50,557 : Computed embeddings
2019-02-14 23:35:50,557 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:36:25,523 : [('reg:1e-05', 66.45), ('reg:0.0001', 62.71), ('reg:0.001', 52.36), ('reg:0.01', 41.68)]
2019-02-14 23:36:25,523 : Validation : best param found is reg = 1e-05 with score             66.45
2019-02-14 23:36:25,523 : Evaluating...
2019-02-14 23:36:34,020 : 
Dev acc : 66.5 Test acc : 68.6 for LENGTH classification

2019-02-14 23:36:34,021 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-14 23:36:34,401 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-14 23:36:34,450 : loading BERT model bert-base-uncased
2019-02-14 23:36:34,451 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:36:34,483 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:36:34,483 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpb9wk7bf0
2019-02-14 23:36:36,830 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:36:38,275 : Computing embeddings for train/dev/test
2019-02-14 23:38:06,700 : Computed embeddings
2019-02-14 23:38:06,701 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:38:39,728 : [('reg:1e-05', 12.54), ('reg:0.0001', 1.81), ('reg:0.001', 0.37), ('reg:0.01', 0.32)]
2019-02-14 23:38:39,728 : Validation : best param found is reg = 1e-05 with score             12.54
2019-02-14 23:38:39,728 : Evaluating...
2019-02-14 23:38:48,963 : 
Dev acc : 12.5 Test acc : 12.7 for WORDCONTENT classification

2019-02-14 23:38:48,965 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-14 23:38:49,285 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-14 23:38:49,359 : loading BERT model bert-base-uncased
2019-02-14 23:38:49,359 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:38:49,387 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:38:49,387 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzyo1ly_q
2019-02-14 23:38:51,800 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:38:53,254 : Computing embeddings for train/dev/test
2019-02-14 23:40:17,022 : Computed embeddings
2019-02-14 23:40:17,023 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:40:44,957 : [('reg:1e-05', 28.99), ('reg:0.0001', 28.14), ('reg:0.001', 24.93), ('reg:0.01', 20.8)]
2019-02-14 23:40:44,957 : Validation : best param found is reg = 1e-05 with score             28.99
2019-02-14 23:40:44,957 : Evaluating...
2019-02-14 23:40:50,555 : 
Dev acc : 29.0 Test acc : 28.9 for DEPTH classification

2019-02-14 23:40:50,556 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-14 23:40:50,967 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-14 23:40:51,034 : loading BERT model bert-base-uncased
2019-02-14 23:40:51,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:40:51,064 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:40:51,064 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpclgqnop6
2019-02-14 23:40:53,508 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:40:54,969 : Computing embeddings for train/dev/test
2019-02-14 23:42:12,878 : Computed embeddings
2019-02-14 23:42:12,878 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:42:43,950 : [('reg:1e-05', 67.9), ('reg:0.0001', 61.93), ('reg:0.001', 47.77), ('reg:0.01', 25.29)]
2019-02-14 23:42:43,951 : Validation : best param found is reg = 1e-05 with score             67.9
2019-02-14 23:42:43,951 : Evaluating...
2019-02-14 23:42:52,455 : 
Dev acc : 67.9 Test acc : 67.5 for TOPCONSTITUENTS classification

2019-02-14 23:42:52,456 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-14 23:42:52,835 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-14 23:42:52,904 : loading BERT model bert-base-uncased
2019-02-14 23:42:52,904 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:42:53,037 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:42:53,038 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpn53mhgy2
2019-02-14 23:42:55,474 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:42:56,940 : Computing embeddings for train/dev/test
2019-02-14 23:44:21,141 : Computed embeddings
2019-02-14 23:44:21,142 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:45:01,807 : [('reg:1e-05', 87.76), ('reg:0.0001', 87.45), ('reg:0.001', 85.36), ('reg:0.01', 82.34)]
2019-02-14 23:45:01,807 : Validation : best param found is reg = 1e-05 with score             87.76
2019-02-14 23:45:01,807 : Evaluating...
2019-02-14 23:45:13,652 : 
Dev acc : 87.8 Test acc : 86.3 for BIGRAMSHIFT classification

2019-02-14 23:45:13,653 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-14 23:45:14,055 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-14 23:45:14,125 : loading BERT model bert-base-uncased
2019-02-14 23:45:14,125 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:45:14,258 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:45:14,258 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpfjuvfht4
2019-02-14 23:45:16,633 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:45:18,098 : Computing embeddings for train/dev/test
2019-02-14 23:46:39,190 : Computed embeddings
2019-02-14 23:46:39,191 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:47:10,620 : [('reg:1e-05', 90.12), ('reg:0.0001', 90.24), ('reg:0.001', 89.97), ('reg:0.01', 89.46)]
2019-02-14 23:47:10,620 : Validation : best param found is reg = 0.0001 with score             90.24
2019-02-14 23:47:10,620 : Evaluating...
2019-02-14 23:47:17,836 : 
Dev acc : 90.2 Test acc : 89.3 for TENSE classification

2019-02-14 23:47:17,837 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-14 23:47:18,448 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-14 23:47:18,514 : loading BERT model bert-base-uncased
2019-02-14 23:47:18,514 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:47:18,544 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:47:18,545 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7z5pypwg
2019-02-14 23:47:20,936 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:47:22,373 : Computing embeddings for train/dev/test
2019-02-14 23:48:48,691 : Computed embeddings
2019-02-14 23:48:48,692 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:49:23,519 : [('reg:1e-05', 83.24), ('reg:0.0001', 82.99), ('reg:0.001', 79.87), ('reg:0.01', 74.89)]
2019-02-14 23:49:23,520 : Validation : best param found is reg = 1e-05 with score             83.24
2019-02-14 23:49:23,520 : Evaluating...
2019-02-14 23:49:34,985 : 
Dev acc : 83.2 Test acc : 83.3 for SUBJNUMBER classification

2019-02-14 23:49:34,986 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-14 23:49:35,438 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-14 23:49:35,510 : loading BERT model bert-base-uncased
2019-02-14 23:49:35,510 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:49:35,543 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:49:35,543 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2ph9b5sr
2019-02-14 23:49:37,890 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:49:39,357 : Computing embeddings for train/dev/test
2019-02-14 23:51:04,657 : Computed embeddings
2019-02-14 23:51:04,657 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:51:28,570 : [('reg:1e-05', 73.94), ('reg:0.0001', 74.3), ('reg:0.001', 72.7), ('reg:0.01', 62.81)]
2019-02-14 23:51:28,571 : Validation : best param found is reg = 0.0001 with score             74.3
2019-02-14 23:51:28,571 : Evaluating...
2019-02-14 23:51:36,202 : 
Dev acc : 74.3 Test acc : 75.8 for OBJNUMBER classification

2019-02-14 23:51:36,203 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-14 23:51:36,629 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-14 23:51:36,703 : loading BERT model bert-base-uncased
2019-02-14 23:51:36,703 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:51:36,734 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:51:36,734 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsnitu0_c
2019-02-14 23:51:39,103 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:51:40,531 : Computing embeddings for train/dev/test
2019-02-14 23:53:18,596 : Computed embeddings
2019-02-14 23:53:18,596 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:53:52,798 : [('reg:1e-05', 64.21), ('reg:0.0001', 63.89), ('reg:0.001', 62.69), ('reg:0.01', 60.24)]
2019-02-14 23:53:52,798 : Validation : best param found is reg = 1e-05 with score             64.21
2019-02-14 23:53:52,798 : Evaluating...
2019-02-14 23:54:01,681 : 
Dev acc : 64.2 Test acc : 64.1 for ODDMANOUT classification

2019-02-14 23:54:01,682 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-14 23:54:02,083 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-14 23:54:02,159 : loading BERT model bert-base-uncased
2019-02-14 23:54:02,159 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:54:02,279 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:54:02,279 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2n9xmy0v
2019-02-14 23:54:04,662 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:54:06,106 : Computing embeddings for train/dev/test
2019-02-14 23:55:42,292 : Computed embeddings
2019-02-14 23:55:42,292 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-14 23:56:15,882 : [('reg:1e-05', 69.05), ('reg:0.0001', 68.84), ('reg:0.001', 66.73), ('reg:0.01', 62.41)]
2019-02-14 23:56:15,883 : Validation : best param found is reg = 1e-05 with score             69.05
2019-02-14 23:56:15,883 : Evaluating...
2019-02-14 23:56:24,648 : 
Dev acc : 69.0 Test acc : 68.8 for COORDINATIONINVERSION classification

2019-02-14 23:56:24,650 : total results: {'STS12': {'MSRpar': {'pearson': (0.0704117109311826, 0.053919768140639264), 'spearman': SpearmanrResult(correlation=0.1289339028659971, pvalue=0.0004002844056631735), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.003529449886115714, 0.9231258197916514), 'spearman': SpearmanrResult(correlation=0.02039427770422632, pvalue=0.577085651017218), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.2055326748676219, 9.043579516430057e-06), 'spearman': SpearmanrResult(correlation=0.4243266128178254, pvalue=1.7373159752502878e-21), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.07691563422327567, 0.035201871547025775), 'spearman': SpearmanrResult(correlation=0.17602678278715145, pvalue=1.23103970152875e-06), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.1563693114070662, 0.0017304779034054817), 'spearman': SpearmanrResult(correlation=0.15671390544635005, pvalue=0.0016897117213797446), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.10113997630860613, 'wmean': 0.0851284666881965}, 'spearman': {'mean': 0.18127909632431005, 'wmean': 0.16129697106644358}}}, 'STS13': {'FNWN': {'pearson': (0.02430106930992741, 0.7399517902994459), 'spearman': SpearmanrResult(correlation=0.029144811288537426, pvalue=0.6905554156325678), 'nsamples': 189}, 'headlines': {'pearson': (-0.02462558254352889, 0.5007077269788116), 'spearman': SpearmanrResult(correlation=0.30810131019405174, pvalue=5.896154561285888e-18), 'nsamples': 750}, 'OnWN': {'pearson': (-0.11124704553409012, 0.008358159716108475), 'spearman': SpearmanrResult(correlation=0.06853564111744095, pvalue=0.10489263721905238), 'nsamples': 561}, 'all': {'pearson': {'mean': -0.03719051958923053, 'wmean': -0.05085725156846329}, 'spearman': {'mean': 0.13526058753334336, 'wmean': 0.1833552310973045}}}, 'STS14': {'deft-forum': {'pearson': (0.03334110883817964, 0.48049867861605944), 'spearman': SpearmanrResult(correlation=0.04304713976998689, pvalue=0.3622688864689869), 'nsamples': 450}, 'deft-news': {'pearson': (0.27103960454632325, 1.8939349910745655e-06), 'spearman': SpearmanrResult(correlation=0.39059847281852045, pvalue=2.252804354915269e-12), 'nsamples': 300}, 'headlines': {'pearson': (0.028040785544703125, 0.44320135380862236), 'spearman': SpearmanrResult(correlation=0.31849850439325555, pvalue=3.82934431670828e-19), 'nsamples': 750}, 'images': {'pearson': (0.027482622952459314, 0.45233422670121437), 'spearman': SpearmanrResult(correlation=0.08390205121001154, pvalue=0.021563340755662054), 'nsamples': 750}, 'OnWN': {'pearson': (-0.07292818680302134, 0.04587425321807359), 'spearman': SpearmanrResult(correlation=0.09154857598858698, pvalue=0.01213314132443079), 'nsamples': 750}, 'tweet-news': {'pearson': (0.20006510779466163, 3.2793121214774875e-08), 'spearman': SpearmanrResult(correlation=0.2616423755580693, pvalue=3.3229065863295787e-13), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.08117350714555094, 'wmean': 0.06221616732204796}, 'spearman': {'mean': 0.19820618662307177, 'wmean': 0.18753183602786472}}}, 'STS15': {'answers-forums': {'pearson': (0.06554605903441425, 0.205363209726195), 'spearman': SpearmanrResult(correlation=0.06844009441975064, pvalue=0.18601394391083362), 'nsamples': 375}, 'answers-students': {'pearson': (-0.015102851000037033, 0.6796489005809596), 'spearman': SpearmanrResult(correlation=0.12308175017136343, pvalue=0.0007302515606128076), 'nsamples': 750}, 'belief': {'pearson': (0.15096567326973326, 0.0033842682375676046), 'spearman': SpearmanrResult(correlation=0.22807885713565706, pvalue=8.161009258821034e-06), 'nsamples': 375}, 'headlines': {'pearson': (-0.03933643705817047, 0.2819754287592138), 'spearman': SpearmanrResult(correlation=0.39515416713362367, pvalue=1.9445568567227144e-29), 'nsamples': 750}, 'images': {'pearson': (0.01472516221345345, 0.6872319297933948), 'spearman': SpearmanrResult(correlation=0.09044907925302202, pvalue=0.013212139427677978), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.03535952129187869, 'wmean': 0.017135435076829925}, 'spearman': {'mean': 0.18104078962268336, 'wmean': 0.18923611808392823}}}, 'STS16': {'answer-answer': {'pearson': (0.2106581801812752, 0.0007280996785869363), 'spearman': SpearmanrResult(correlation=0.2769515861696119, pvalue=7.462476490693646e-06), 'nsamples': 254}, 'headlines': {'pearson': (0.06704362292309864, 0.29197750168971104), 'spearman': SpearmanrResult(correlation=0.4281230192985527, pvalue=1.6090016125488588e-12), 'nsamples': 249}, 'plagiarism': {'pearson': (0.2610914963597456, 6.128386536662583e-05), 'spearman': SpearmanrResult(correlation=0.41366852858334713, pvalue=6.383846115544008e-11), 'nsamples': 230}, 'postediting': {'pearson': (0.4165834291141557, 1.1628178645215279e-11), 'spearman': SpearmanrResult(correlation=0.5319870914026142, pvalue=3.135806850554545e-19), 'nsamples': 244}, 'question-question': {'pearson': (-0.032146120302701836, 0.6440358928226606), 'spearman': SpearmanrResult(correlation=0.10575639833011187, pvalue=0.1275061935781366), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.18464612165511465, 'wmean': 0.18986500977843696}, 'spearman': {'mean': 0.35129732475684755, 'wmean': 0.3575042443674723}}}, 'MR': {'devacc': 79.36, 'acc': 79.18, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 84.86, 'acc': 82.76, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 84.73, 'acc': 85.67, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.74, 'acc': 94.49, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 84.52, 'acc': 84.18, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 42.51, 'acc': 43.26, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 65.52, 'acc': 69.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 68.74, 'acc': 69.62, 'f1': 80.39, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.0, 'acc': 68.07, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.6095643658240335, 'pearson': 0.6611467655784379, 'spearman': 0.6225046762207509, 'mse': 0.5729367137475779, 'yhat': array([2.37050751, 3.69688166, 1.37433667, ..., 2.92335361, 4.44116705,        4.13801343]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.44628606987289704, 'pearson': 0.38206788826701005, 'spearman': 0.39195045625178027, 'mse': 2.132797179579039, 'yhat': array([2.81015107, 2.88280439, 3.03177561, ..., 3.79448148, 3.14106248,        3.5136546 ]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 57.76, 'acc': 58.43, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 307.752, 'acc': [(26.52, 60.0, 75.55999999999999, 3.5999999999999996), (21.943999999999996, 54.772, 71.668, 4.6)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 66.45, 'acc': 68.64, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 12.54, 'acc': 12.7, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 28.99, 'acc': 28.86, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 67.9, 'acc': 67.48, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 87.76, 'acc': 86.31, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 90.24, 'acc': 89.34, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 83.24, 'acc': 83.28, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 74.3, 'acc': 75.79, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 64.21, 'acc': 64.09, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 69.05, 'acc': 68.77, 'ndev': 10002, 'ntest': 10002}}
2019-02-14 23:56:24,650 : STS12 p=0.0851, STS12 s=0.1613, STS13 p=-0.0509, STS13 s=0.1834, STS14 p=0.0622, STS14 s=0.1875, STS15 p=0.0171, STS15 s=0.1892, STS 16 p=0.1899, STS16 s=0.3575, STS B p=0.3821, STS B s=0.3920, STS B m=2.1328, SICK-R p=0.6611, SICK-R s=0.6225, SICK-P m=0.5729
2019-02-14 23:56:24,650 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-14 23:56:24,650 : 0.0851,0.1613,-0.0509,0.1834,0.0622,0.1875,0.0171,0.1892,0.1899,0.3575,0.3821,0.3920,2.1328,0.6611,0.6225,0.5729
2019-02-14 23:56:24,650 : MR=79.18, CR=82.76, SUBJ=94.49, MPQA=85.67, SST-B=84.18, SST-F=43.26, TREC=69.20, SICK-E=68.07, SNLI=58.43, MRPC=69.62, MRPC f=80.39
2019-02-14 23:56:24,650 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-14 23:56:24,650 : 79.18,82.76,94.49,85.67,84.18,43.26,69.20,68.07,58.43,69.62,80.39
2019-02-14 23:56:24,650 : COCO r1i2t=26.52, COCO r5i2t=60.00, COCO r10i2t=75.56, COCO medr_i2t=3.60, COCO r1t2i=21.94, COCO r5t2i=54.77, COCO r10t2i=71.67, COCO medr_t2i=4.60
2019-02-14 23:56:24,650 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-14 23:56:24,650 : 26.52,60.00,75.56,3.60,21.94,54.77,71.67,4.60
2019-02-14 23:56:24,650 : SentLen=68.64, WC=12.70, TreeDepth=28.86, TopConst=67.48, BShift=86.31, Tense=89.34, SubjNum=83.28, ObjNum=75.79, SOMO=64.09, CoordInv=68.77, average=64.53
2019-02-14 23:56:24,650 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-14 23:56:24,650 : 68.64,12.70,28.86,67.48,86.31,89.34,83.28,75.79,64.09,68.77,64.53
2019-02-14 23:56:24,650 : ********************************************************************************
2019-02-14 23:56:24,651 : ********************************************************************************
2019-02-14 23:56:24,651 : ********************************************************************************
2019-02-14 23:56:24,651 : layer 11
2019-02-14 23:56:24,651 : ********************************************************************************
2019-02-14 23:56:24,651 : ********************************************************************************
2019-02-14 23:56:24,651 : ********************************************************************************
2019-02-14 23:56:24,746 : ***** Transfer task : STS12 *****


2019-02-14 23:56:24,759 : loading BERT model bert-base-uncased
2019-02-14 23:56:24,759 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:56:24,776 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:56:24,776 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp42u72pb5
2019-02-14 23:56:27,145 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:56:30,365 : MSRpar : pearson = -0.0021, spearman = 0.0095
2019-02-14 23:56:31,141 : MSRvid : pearson = -0.0168, spearman = 0.0171
2019-02-14 23:56:31,742 : SMTeuroparl : pearson = 0.1434, spearman = 0.3344
2019-02-14 23:56:32,930 : surprise.OnWN : pearson = 0.0515, spearman = 0.2831
2019-02-14 23:56:33,589 : surprise.SMTnews : pearson = 0.0657, spearman = 0.1049
2019-02-14 23:56:33,589 : ALL (weighted average) : Pearson = 0.0375,             Spearman = 0.1376
2019-02-14 23:56:33,589 : ALL (average) : Pearson = 0.0483,             Spearman = 0.1498

2019-02-14 23:56:33,589 : ***** Transfer task : STS13 (-SMT) *****


2019-02-14 23:56:33,599 : loading BERT model bert-base-uncased
2019-02-14 23:56:33,599 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:56:33,617 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:56:33,617 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp330iwfg_
2019-02-14 23:56:35,975 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:56:37,999 : FNWN : pearson = 0.0497, spearman = 0.0710
2019-02-14 23:56:38,903 : headlines : pearson = -0.0661, spearman = 0.4220
2019-02-14 23:56:39,568 : OnWN : pearson = -0.0585, spearman = 0.1304
2019-02-14 23:56:39,568 : ALL (weighted average) : Pearson = -0.0487,             Spearman = 0.2687
2019-02-14 23:56:39,568 : ALL (average) : Pearson = -0.0250,             Spearman = 0.2078

2019-02-14 23:56:39,568 : ***** Transfer task : STS14 *****


2019-02-14 23:56:39,583 : loading BERT model bert-base-uncased
2019-02-14 23:56:39,583 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:56:39,602 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:56:39,602 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpipsbxts0
2019-02-14 23:56:41,960 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:56:44,069 : deft-forum : pearson = -0.0169, spearman = 0.0448
2019-02-14 23:56:44,746 : deft-news : pearson = 0.2341, spearman = 0.3571
2019-02-14 23:56:45,709 : headlines : pearson = 0.0032, spearman = 0.4285
2019-02-14 23:56:46,667 : images : pearson = -0.0132, spearman = 0.0837
2019-02-14 23:56:47,633 : OnWN : pearson = -0.0654, spearman = 0.2070
2019-02-14 23:56:48,913 : tweet-news : pearson = 0.0997, spearman = 0.3568
2019-02-14 23:56:48,913 : ALL (weighted average) : Pearson = 0.0216,             Spearman = 0.2491
2019-02-14 23:56:48,913 : ALL (average) : Pearson = 0.0403,             Spearman = 0.2463

2019-02-14 23:56:48,913 : ***** Transfer task : STS15 *****


2019-02-14 23:56:48,979 : loading BERT model bert-base-uncased
2019-02-14 23:56:48,979 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:56:48,997 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:56:48,997 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps1x7009l
2019-02-14 23:56:51,348 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:56:53,690 : answers-forums : pearson = -0.0238, spearman = -0.0317
2019-02-14 23:56:54,718 : answers-students : pearson = 0.0499, spearman = 0.1346
2019-02-14 23:56:55,666 : belief : pearson = 0.0669, spearman = 0.1399
2019-02-14 23:56:56,772 : headlines : pearson = -0.0362, spearman = 0.5055
2019-02-14 23:56:57,825 : images : pearson = 0.0290, spearman = 0.0469
2019-02-14 23:56:57,826 : ALL (weighted average) : Pearson = 0.0161,             Spearman = 0.1853
2019-02-14 23:56:57,826 : ALL (average) : Pearson = 0.0172,             Spearman = 0.1590

2019-02-14 23:56:57,826 : ***** Transfer task : STS16 *****


2019-02-14 23:56:57,864 : loading BERT model bert-base-uncased
2019-02-14 23:56:57,864 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:56:57,910 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:56:57,910 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpw4cmrxgw
2019-02-14 23:57:00,290 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:57:02,097 : answer-answer : pearson = 0.1570, spearman = 0.2441
2019-02-14 23:57:02,399 : headlines : pearson = -0.0365, spearman = 0.5639
2019-02-14 23:57:02,782 : plagiarism : pearson = 0.1979, spearman = 0.1751
2019-02-14 23:57:03,392 : postediting : pearson = 0.2721, spearman = 0.3715
2019-02-14 23:57:03,667 : question-question : pearson = 0.0238, spearman = 0.2946
2019-02-14 23:57:03,667 : ALL (weighted average) : Pearson = 0.1245,             Spearman = 0.3330
2019-02-14 23:57:03,667 : ALL (average) : Pearson = 0.1229,             Spearman = 0.3299

2019-02-14 23:57:03,667 : ***** Transfer task : MR *****


2019-02-14 23:57:03,682 : loading BERT model bert-base-uncased
2019-02-14 23:57:03,682 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:57:03,701 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:57:03,701 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp5nnkfn9n
2019-02-14 23:57:06,040 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:57:07,475 : Generating sentence embeddings
2019-02-14 23:57:20,850 : Generated sentence embeddings
2019-02-14 23:57:20,850 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 23:57:29,716 : Best param found at split 1: l2reg = 1e-05                 with score 75.45
2019-02-14 23:57:41,998 : Best param found at split 2: l2reg = 1e-05                 with score 75.05
2019-02-14 23:57:54,686 : Best param found at split 3: l2reg = 0.0001                 with score 74.92
2019-02-14 23:58:05,456 : Best param found at split 4: l2reg = 1e-05                 with score 74.69
2019-02-14 23:58:18,547 : Best param found at split 5: l2reg = 1e-05                 with score 75.35
2019-02-14 23:58:19,282 : Dev acc : 75.09 Test acc : 75.13

2019-02-14 23:58:19,283 : ***** Transfer task : CR *****


2019-02-14 23:58:19,290 : loading BERT model bert-base-uncased
2019-02-14 23:58:19,290 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:58:19,348 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:58:19,348 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp8kgw_3s_
2019-02-14 23:58:21,750 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:58:23,220 : Generating sentence embeddings
2019-02-14 23:58:26,892 : Generated sentence embeddings
2019-02-14 23:58:26,892 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 23:58:32,872 : Best param found at split 1: l2reg = 1e-05                 with score 79.16
2019-02-14 23:58:39,067 : Best param found at split 2: l2reg = 1e-05                 with score 80.06
2019-02-14 23:58:44,144 : Best param found at split 3: l2reg = 1e-05                 with score 79.21
2019-02-14 23:58:49,206 : Best param found at split 4: l2reg = 1e-05                 with score 80.17
2019-02-14 23:58:54,573 : Best param found at split 5: l2reg = 1e-05                 with score 79.24
2019-02-14 23:58:54,854 : Dev acc : 79.57 Test acc : 76.27

2019-02-14 23:58:54,855 : ***** Transfer task : MPQA *****


2019-02-14 23:58:54,860 : loading BERT model bert-base-uncased
2019-02-14 23:58:54,861 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-14 23:58:54,879 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-14 23:58:54,880 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjqvp2roq
2019-02-14 23:58:57,293 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-14 23:58:58,728 : Generating sentence embeddings
2019-02-14 23:59:02,365 : Generated sentence embeddings
2019-02-14 23:59:02,365 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-14 23:59:12,726 : Best param found at split 1: l2reg = 1e-05                 with score 79.82
2019-02-14 23:59:24,288 : Best param found at split 2: l2reg = 1e-05                 with score 80.2
2019-02-14 23:59:38,782 : Best param found at split 3: l2reg = 1e-05                 with score 81.78
2019-02-14 23:59:51,566 : Best param found at split 4: l2reg = 1e-05                 with score 81.84
2019-02-15 00:00:05,821 : Best param found at split 5: l2reg = 1e-05                 with score 81.39
2019-02-15 00:00:06,635 : Dev acc : 81.01 Test acc : 82.71

2019-02-15 00:00:06,636 : ***** Transfer task : SUBJ *****


2019-02-15 00:00:06,653 : loading BERT model bert-base-uncased
2019-02-15 00:00:06,653 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:00:06,674 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:00:06,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmporso8x6l
2019-02-15 00:00:09,114 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:00:10,581 : Generating sentence embeddings
2019-02-15 00:00:23,694 : Generated sentence embeddings
2019-02-15 00:00:23,694 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 00:00:37,341 : Best param found at split 1: l2reg = 1e-05                 with score 93.29
2019-02-15 00:00:51,715 : Best param found at split 2: l2reg = 1e-05                 with score 93.19
2019-02-15 00:01:05,233 : Best param found at split 3: l2reg = 1e-05                 with score 93.64
2019-02-15 00:01:17,988 : Best param found at split 4: l2reg = 1e-05                 with score 93.28
2019-02-15 00:01:32,421 : Best param found at split 5: l2reg = 1e-05                 with score 93.55
2019-02-15 00:01:33,282 : Dev acc : 93.39 Test acc : 93.02

2019-02-15 00:01:33,284 : ***** Transfer task : SST Binary classification *****


2019-02-15 00:01:33,419 : loading BERT model bert-base-uncased
2019-02-15 00:01:33,419 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:01:33,445 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:01:33,445 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpj5oduh3f
2019-02-15 00:01:35,849 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:01:37,300 : Computing embedding for train
2019-02-15 00:02:22,339 : Computed train embeddings
2019-02-15 00:02:22,339 : Computing embedding for dev
2019-02-15 00:02:23,266 : Computed dev embeddings
2019-02-15 00:02:23,266 : Computing embedding for test
2019-02-15 00:02:25,228 : Computed test embeddings
2019-02-15 00:02:25,228 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:02:45,528 : [('reg:1e-05', 83.37), ('reg:0.0001', 81.65), ('reg:0.001', 76.72), ('reg:0.01', 63.88)]
2019-02-15 00:02:45,528 : Validation : best param found is reg = 1e-05 with score             83.37
2019-02-15 00:02:45,528 : Evaluating...
2019-02-15 00:02:50,952 : 
Dev acc : 83.37 Test acc : 83.03 for             SST Binary classification

2019-02-15 00:02:50,952 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 00:02:51,009 : loading BERT model bert-base-uncased
2019-02-15 00:02:51,009 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:02:51,068 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:02:51,069 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpko9mat4c
2019-02-15 00:02:53,483 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:02:54,927 : Computing embedding for train
2019-02-15 00:03:04,363 : Computed train embeddings
2019-02-15 00:03:04,364 : Computing embedding for dev
2019-02-15 00:03:05,595 : Computed dev embeddings
2019-02-15 00:03:05,595 : Computing embedding for test
2019-02-15 00:03:07,967 : Computed test embeddings
2019-02-15 00:03:07,967 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:03:11,180 : [('reg:1e-05', 40.96), ('reg:0.0001', 40.42), ('reg:0.001', 38.6), ('reg:0.01', 32.43)]
2019-02-15 00:03:11,180 : Validation : best param found is reg = 1e-05 with score             40.96
2019-02-15 00:03:11,181 : Evaluating...
2019-02-15 00:03:12,154 : 
Dev acc : 40.96 Test acc : 39.5 for             SST Fine-Grained classification

2019-02-15 00:03:12,154 : ***** Transfer task : TREC *****


2019-02-15 00:03:12,167 : loading BERT model bert-base-uncased
2019-02-15 00:03:12,167 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:03:12,189 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:03:12,189 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplvx0qxui
2019-02-15 00:03:14,576 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:03:19,410 : Computed train embeddings
2019-02-15 00:03:19,675 : Computed test embeddings
2019-02-15 00:03:19,675 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 00:03:26,856 : [('reg:1e-05', 62.51), ('reg:0.0001', 54.85), ('reg:0.001', 44.15), ('reg:0.01', 37.14)]
2019-02-15 00:03:26,856 : Cross-validation : best param found is reg = 1e-05             with score 62.51
2019-02-15 00:03:26,856 : Evaluating...
2019-02-15 00:03:27,574 : 
Dev acc : 62.51 Test acc : 75.2             for TREC

2019-02-15 00:03:27,575 : ***** Transfer task : MRPC *****


2019-02-15 00:03:27,624 : loading BERT model bert-base-uncased
2019-02-15 00:03:27,624 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:03:27,644 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:03:27,645 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpg3awxu5w
2019-02-15 00:03:30,026 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:03:31,447 : Computing embedding for train
2019-02-15 00:03:41,200 : Computed train embeddings
2019-02-15 00:03:41,200 : Computing embedding for test
2019-02-15 00:03:45,246 : Computed test embeddings
2019-02-15 00:03:45,262 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 00:03:50,621 : [('reg:1e-05', 67.74), ('reg:0.0001', 67.66), ('reg:0.001', 67.79), ('reg:0.01', 67.89)]
2019-02-15 00:03:50,621 : Cross-validation : best param found is reg = 0.01             with score 67.89
2019-02-15 00:03:50,621 : Evaluating...
2019-02-15 00:03:50,932 : Dev acc : 67.89 Test acc 65.8; Test F1 78.4 for MRPC.

2019-02-15 00:03:50,933 : ***** Transfer task : SICK-Entailment*****


2019-02-15 00:03:50,957 : loading BERT model bert-base-uncased
2019-02-15 00:03:50,957 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:03:51,016 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:03:51,016 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3e1zz1cd
2019-02-15 00:03:53,455 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:03:54,898 : Computing embedding for train
2019-02-15 00:04:00,008 : Computed train embeddings
2019-02-15 00:04:00,008 : Computing embedding for dev
2019-02-15 00:04:00,699 : Computed dev embeddings
2019-02-15 00:04:00,699 : Computing embedding for test
2019-02-15 00:04:06,186 : Computed test embeddings
2019-02-15 00:04:06,215 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:04:07,364 : [('reg:1e-05', 69.4), ('reg:0.0001', 68.0), ('reg:0.001', 56.6), ('reg:0.01', 56.4)]
2019-02-15 00:04:07,365 : Validation : best param found is reg = 1e-05 with score             69.4
2019-02-15 00:04:07,365 : Evaluating...
2019-02-15 00:04:07,673 : 
Dev acc : 69.4 Test acc : 66.71 for                        SICK entailment

2019-02-15 00:04:07,674 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 00:04:07,701 : loading BERT model bert-base-uncased
2019-02-15 00:04:07,702 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:04:07,723 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:04:07,723 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplrizsyjg
2019-02-15 00:04:10,132 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:04:11,574 : Computing embedding for train
2019-02-15 00:04:16,667 : Computed train embeddings
2019-02-15 00:04:16,667 : Computing embedding for dev
2019-02-15 00:04:17,359 : Computed dev embeddings
2019-02-15 00:04:17,360 : Computing embedding for test
2019-02-15 00:04:22,874 : Computed test embeddings
2019-02-15 00:05:31,870 : Dev : Pearson 0.681500358418841
2019-02-15 00:05:31,871 : Test : Pearson 0.6921481976790592 Spearman 0.6457122512639326 MSE 0.534431821585591                        for SICK Relatedness

2019-02-15 00:05:31,871 : 

***** Transfer task : STSBenchmark*****


2019-02-15 00:05:31,952 : loading BERT model bert-base-uncased
2019-02-15 00:05:31,952 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:05:31,970 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:05:31,970 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmg8hfwfq
2019-02-15 00:05:34,306 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:05:35,707 : Computing embedding for train
2019-02-15 00:05:43,728 : Computed train embeddings
2019-02-15 00:05:43,728 : Computing embedding for dev
2019-02-15 00:05:46,172 : Computed dev embeddings
2019-02-15 00:05:46,172 : Computing embedding for test
2019-02-15 00:05:48,104 : Computed test embeddings
2019-02-15 00:06:16,619 : Dev : Pearson 0.2377149626595009
2019-02-15 00:06:16,619 : Test : Pearson 0.23336685242288113 Spearman 0.3054802326068784 MSE 2.335368654706889                        for SICK Relatedness

2019-02-15 00:06:16,619 : ***** Transfer task : SNLI Entailment*****


2019-02-15 00:06:21,608 : loading BERT model bert-base-uncased
2019-02-15 00:06:21,609 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:06:21,739 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:06:21,739 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpke35vd1d
2019-02-15 00:06:24,135 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:06:25,755 : PROGRESS (encoding): 0.00%
2019-02-15 00:07:43,419 : PROGRESS (encoding): 14.56%
2019-02-15 00:09:09,324 : PROGRESS (encoding): 29.12%
2019-02-15 00:10:35,767 : PROGRESS (encoding): 43.69%
2019-02-15 00:12:10,014 : PROGRESS (encoding): 58.25%
2019-02-15 00:13:54,640 : PROGRESS (encoding): 72.81%
2019-02-15 00:15:37,918 : PROGRESS (encoding): 87.37%
2019-02-15 00:17:27,350 : PROGRESS (encoding): 0.00%
2019-02-15 00:17:40,744 : PROGRESS (encoding): 0.00%
2019-02-15 00:17:54,497 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 00:18:56,369 : [('reg:1e-09', 58.11)]
2019-02-15 00:18:56,369 : Validation : best param found is reg = 1e-09 with score             58.11
2019-02-15 00:18:56,369 : Evaluating...
2019-02-15 00:20:00,042 : Dev acc : 58.11 Test acc : 58.15 for SNLI

2019-02-15 00:20:00,042 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 00:20:09,054 : loading BERT model bert-base-uncased
2019-02-15 00:20:09,054 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 00:20:09,105 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 00:20:09,105 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpkmoxw5dl
2019-02-15 00:20:11,459 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 00:20:12,907 : Computing embedding for train
2019-02-15 00:27:42,715 : Computed train embeddings
2019-02-15 00:27:42,716 : Computing embedding for dev
2019-02-15 00:28:02,217 : Computed dev embeddings
2019-02-15 00:28:02,217 : Computing embedding for test
2019-02-15 00:28:23,362 : Computed test embeddings
2019-02-15 00:28:23,378 : prepare data
2019-02-15 00:28:23,443 : start epoch
2019-02-15 00:29:04,979 : samples : 64000
2019-02-15 00:29:15,060 : Image to text: 3.44, 11.88, 20.04, 51.0
2019-02-15 00:29:22,335 : Text to Image: 2.78, 9.796, 16.424, 58.0
2019-02-15 00:30:03,951 : samples : 128000
2019-02-15 00:30:13,965 : Image to text: 4.42, 15.52, 24.8, 36.0
2019-02-15 00:30:21,190 : Text to Image: 3.696, 12.428, 20.372, 46.0
2019-02-15 00:31:02,845 : samples : 192000
2019-02-15 00:31:12,871 : Image to text: 4.56, 15.16, 24.5, 36.0
2019-02-15 00:31:20,104 : Text to Image: 3.832, 13.164, 21.144, 43.0
2019-02-15 00:31:59,514 : samples : 256000
2019-02-15 00:32:09,553 : Image to text: 4.32, 14.76, 24.34, 38.0
2019-02-15 00:32:16,748 : Text to Image: 3.396, 12.296, 20.224, 45.0
2019-02-15 00:32:56,787 : samples : 320000
2019-02-15 00:33:06,785 : Image to text: 5.9, 18.62, 28.66, 28.0
2019-02-15 00:33:14,001 : Text to Image: 4.428, 15.152, 24.132, 37.0
2019-02-15 00:33:54,006 : samples : 384000
2019-02-15 00:34:03,940 : Image to text: 5.42, 17.52, 27.36, 30.0
2019-02-15 00:34:11,165 : Text to Image: 4.44, 15.2, 24.244, 36.0
2019-02-15 00:34:50,337 : samples : 448000
2019-02-15 00:35:00,273 : Image to text: 4.84, 15.74, 25.76, 34.0
2019-02-15 00:35:07,475 : Text to Image: 4.072, 14.432, 23.04, 39.0
2019-02-15 00:35:46,919 : samples : 512000
2019-02-15 00:35:56,904 : Image to text: 5.78, 18.54, 29.08, 29.0
2019-02-15 00:36:04,159 : Text to Image: 4.772, 15.924, 25.532, 34.0
2019-02-15 00:36:37,738 : Epoch 1 finished
2019-02-15 00:36:38,217 : Image to text: 15.4, 45.2, 59.5, 7.0
2019-02-15 00:36:38,547 : Text to Image: 13.04, 38.36, 54.76, 9.0
2019-02-15 00:36:39,027 : Image to text: 17.1, 45.5, 61.9, 6.0
2019-02-15 00:36:39,357 : Text to Image: 12.6, 38.42, 55.5, 9.0
2019-02-15 00:36:39,836 : Image to text: 18.3, 47.0, 63.8, 6.0
2019-02-15 00:36:40,166 : Text to Image: 11.92, 38.0, 54.48, 9.0
2019-02-15 00:36:40,648 : Image to text: 17.2, 46.1, 61.6, 6.0
2019-02-15 00:36:40,980 : Text to Image: 12.94, 38.46, 55.96, 8.0
2019-02-15 00:36:41,460 : Image to text: 20.6, 47.2, 63.6, 6.0
2019-02-15 00:36:41,790 : Text to Image: 13.98, 39.6, 57.18, 8.0
2019-02-15 00:36:41,790 : Dev mean Text to Image: 12.896, 38.568, 55.576, 8.6
2019-02-15 00:36:41,790 : Dev mean Image to text: 17.72, 46.2, 62.08, 6.2
2019-02-15 00:36:41,790 : start epoch
2019-02-15 00:37:20,888 : samples : 64000
2019-02-15 00:37:31,029 : Image to text: 6.26, 20.6, 31.44, 26.0
2019-02-15 00:37:38,258 : Text to Image: 5.044, 17.46, 26.94, 32.0
2019-02-15 00:38:19,535 : samples : 128000
2019-02-15 00:38:29,410 : Image to text: 6.32, 21.14, 32.1, 24.0
2019-02-15 00:38:36,373 : Text to Image: 5.304, 17.964, 27.848, 30.0
2019-02-15 00:39:18,638 : samples : 192000
2019-02-15 00:39:31,150 : Image to text: 5.12, 18.2, 28.86, 29.0
2019-02-15 00:39:41,126 : Text to Image: 4.9, 16.712, 26.24, 32.0
2019-02-15 00:40:23,119 : samples : 256000
2019-02-15 00:40:33,132 : Image to text: 7.24, 21.52, 32.22, 25.0
2019-02-15 00:40:40,260 : Text to Image: 5.492, 18.068, 27.672, 31.0
2019-02-15 00:41:21,023 : samples : 320000
2019-02-15 00:41:33,475 : Image to text: 6.44, 21.22, 32.28, 23.0
2019-02-15 00:41:43,461 : Text to Image: 4.784, 17.208, 27.012, 31.0
2019-02-15 00:42:26,710 : samples : 384000
2019-02-15 00:42:36,642 : Image to text: 6.06, 20.22, 31.32, 25.0
2019-02-15 00:42:43,638 : Text to Image: 5.448, 18.344, 28.572, 30.0
2019-02-15 00:43:23,922 : samples : 448000
2019-02-15 00:43:36,422 : Image to text: 6.06, 20.42, 31.74, 25.0
2019-02-15 00:43:46,422 : Text to Image: 5.396, 17.852, 27.748, 30.0
2019-02-15 00:44:30,005 : samples : 512000
2019-02-15 00:44:42,195 : Image to text: 6.9, 22.62, 33.82, 23.0
2019-02-15 00:44:49,279 : Text to Image: 5.888, 19.416, 29.684, 27.0
2019-02-15 00:45:24,239 : Epoch 2 finished
2019-02-15 00:45:25,164 : Image to text: 22.6, 54.3, 69.2, 4.0
2019-02-15 00:45:25,930 : Text to Image: 18.7, 48.02, 65.3, 6.0
2019-02-15 00:45:26,828 : Image to text: 18.8, 50.8, 67.5, 5.0
2019-02-15 00:45:27,580 : Text to Image: 16.64, 47.26, 64.46, 6.0
2019-02-15 00:45:28,506 : Image to text: 20.7, 50.7, 68.3, 5.0
2019-02-15 00:45:29,266 : Text to Image: 16.66, 46.66, 64.6, 6.0
2019-02-15 00:45:30,165 : Image to text: 20.5, 51.9, 69.0, 5.0
2019-02-15 00:45:30,946 : Text to Image: 17.5, 47.2, 64.28, 6.0
2019-02-15 00:45:31,858 : Image to text: 22.0, 55.0, 70.4, 4.0
2019-02-15 00:45:32,601 : Text to Image: 19.2, 47.38, 64.5, 6.0
2019-02-15 00:45:32,601 : Dev mean Text to Image: 17.74, 47.304, 64.628, 6.0
2019-02-15 00:45:32,601 : Dev mean Image to text: 20.92, 52.54, 68.88, 4.6
2019-02-15 00:45:32,601 : start epoch
2019-02-15 00:46:14,661 : samples : 64000
2019-02-15 00:46:27,162 : Image to text: 7.44, 23.06, 34.0, 22.0
2019-02-15 00:46:35,462 : Text to Image: 5.904, 19.184, 29.32, 28.0
2019-02-15 00:47:15,931 : samples : 128000
2019-02-15 00:47:28,391 : Image to text: 6.42, 20.54, 31.64, 25.0
2019-02-15 00:47:38,326 : Text to Image: 5.436, 18.296, 28.168, 29.0
2019-02-15 00:48:21,405 : samples : 192000
2019-02-15 00:48:33,387 : Image to text: 7.2, 22.3, 33.14, 22.0
2019-02-15 00:48:40,519 : Text to Image: 5.72, 19.116, 29.336, 28.0
2019-02-15 00:49:21,842 : samples : 256000
2019-02-15 00:49:34,336 : Image to text: 7.34, 23.16, 34.48, 23.0
2019-02-15 00:49:44,300 : Text to Image: 5.944, 18.82, 29.132, 28.0
2019-02-15 00:50:28,055 : samples : 320000
2019-02-15 00:50:39,283 : Image to text: 7.42, 22.06, 33.06, 23.0
2019-02-15 00:50:46,413 : Text to Image: 5.92, 19.24, 29.04, 28.0
2019-02-15 00:51:27,058 : samples : 384000
2019-02-15 00:51:39,590 : Image to text: 7.42, 22.56, 33.78, 22.0
2019-02-15 00:51:49,527 : Text to Image: 6.16, 19.68, 29.696, 27.0
2019-02-15 00:52:32,836 : samples : 448000
2019-02-15 00:52:43,998 : Image to text: 7.02, 22.14, 33.74, 22.0
2019-02-15 00:52:51,176 : Text to Image: 5.896, 18.904, 29.172, 28.0
2019-02-15 00:53:32,354 : samples : 512000
2019-02-15 00:53:44,876 : Image to text: 7.76, 23.22, 34.12, 22.0
2019-02-15 00:53:54,827 : Text to Image: 6.316, 19.728, 29.972, 26.0
2019-02-15 00:54:31,574 : Epoch 3 finished
2019-02-15 00:54:32,533 : Image to text: 22.4, 51.5, 68.1, 5.0
2019-02-15 00:54:33,299 : Text to Image: 16.92, 45.66, 62.96, 7.0
2019-02-15 00:54:34,265 : Image to text: 19.1, 51.7, 66.4, 5.0
2019-02-15 00:54:35,006 : Text to Image: 15.96, 45.78, 62.88, 6.0
2019-02-15 00:54:35,918 : Image to text: 20.8, 51.5, 67.1, 5.0
2019-02-15 00:54:36,659 : Text to Image: 15.64, 44.5, 62.6, 7.0
2019-02-15 00:54:37,600 : Image to text: 19.4, 51.6, 67.9, 5.0
2019-02-15 00:54:38,341 : Text to Image: 17.22, 45.5, 62.7, 7.0
2019-02-15 00:54:39,261 : Image to text: 22.1, 54.3, 69.3, 5.0
2019-02-15 00:54:40,033 : Text to Image: 17.12, 46.84, 63.6, 6.0
2019-02-15 00:54:40,033 : Dev mean Text to Image: 16.572, 45.656000000000006, 62.948, 6.6
2019-02-15 00:54:40,033 : Dev mean Image to text: 20.759999999999998, 52.120000000000005, 67.75999999999999, 5.0
2019-02-15 00:54:40,033 : start epoch
2019-02-15 00:55:20,256 : samples : 64000
2019-02-15 00:55:30,067 : Image to text: 7.8, 23.98, 35.3, 21.0
2019-02-15 00:55:39,062 : Text to Image: 5.788, 19.672, 29.872, 27.0
2019-02-15 00:56:21,466 : samples : 128000
2019-02-15 00:56:33,990 : Image to text: 6.7, 20.94, 32.74, 24.0
2019-02-15 00:56:43,968 : Text to Image: 5.66, 18.484, 28.66, 28.0
2019-02-15 00:57:25,548 : samples : 192000
2019-02-15 00:57:35,510 : Image to text: 7.72, 23.68, 35.56, 20.0
2019-02-15 00:57:42,683 : Text to Image: 6.372, 20.376, 30.724, 26.0
2019-02-15 00:58:23,532 : samples : 256000
2019-02-15 00:58:36,073 : Image to text: 8.46, 24.92, 36.48, 20.0
2019-02-15 00:58:45,994 : Text to Image: 6.66, 21.168, 31.828, 24.0
2019-02-15 00:59:28,841 : samples : 320000
2019-02-15 00:59:39,289 : Image to text: 7.66, 24.1, 36.0, 21.0
2019-02-15 00:59:46,390 : Text to Image: 6.644, 20.94, 31.872, 25.0
2019-02-15 01:00:26,931 : samples : 384000
2019-02-15 01:00:38,343 : Image to text: 7.02, 23.04, 34.42, 22.0
2019-02-15 01:00:45,898 : Text to Image: 6.12, 20.144, 30.404, 26.0
2019-02-15 01:01:27,223 : samples : 448000
2019-02-15 01:01:37,209 : Image to text: 7.92, 23.5, 34.72, 21.0
2019-02-15 01:01:44,385 : Text to Image: 6.696, 21.444, 32.056, 25.0
2019-02-15 01:02:25,128 : samples : 512000
2019-02-15 01:02:34,886 : Image to text: 7.46, 24.12, 36.76, 19.0
2019-02-15 01:02:43,023 : Text to Image: 6.44, 20.396, 31.004, 26.0
2019-02-15 01:03:17,675 : Epoch 4 finished
2019-02-15 01:03:18,130 : Image to text: 21.4, 54.4, 71.4, 5.0
2019-02-15 01:03:18,488 : Text to Image: 17.96, 48.92, 66.42, 6.0
2019-02-15 01:03:18,933 : Image to text: 21.1, 51.8, 67.2, 5.0
2019-02-15 01:03:19,293 : Text to Image: 18.04, 47.8, 65.36, 6.0
2019-02-15 01:03:19,738 : Image to text: 22.1, 53.9, 69.1, 5.0
2019-02-15 01:03:20,096 : Text to Image: 17.98, 47.72, 65.14, 6.0
2019-02-15 01:03:20,543 : Image to text: 21.2, 54.9, 69.8, 4.0
2019-02-15 01:03:20,901 : Text to Image: 17.64, 48.38, 66.3, 6.0
2019-02-15 01:03:21,346 : Image to text: 23.7, 56.2, 72.3, 4.0
2019-02-15 01:03:21,704 : Text to Image: 18.92, 48.12, 65.04, 6.0
2019-02-15 01:03:21,704 : Dev mean Text to Image: 18.108, 48.188, 65.652, 6.0
2019-02-15 01:03:21,704 : Dev mean Image to text: 21.9, 54.24, 69.96, 4.6
2019-02-15 01:03:21,705 : start epoch
2019-02-15 01:04:03,001 : samples : 64000
2019-02-15 01:04:12,966 : Image to text: 7.96, 24.8, 36.88, 19.0
2019-02-15 01:04:20,184 : Text to Image: 6.88, 21.528, 32.74, 24.0
2019-02-15 01:05:00,416 : samples : 128000
2019-02-15 01:05:12,838 : Image to text: 8.12, 24.24, 36.64, 19.0
2019-02-15 01:05:22,924 : Text to Image: 6.728, 21.468, 31.904, 24.0
2019-02-15 01:06:05,204 : samples : 192000
2019-02-15 01:06:15,212 : Image to text: 7.8, 24.38, 35.64, 20.0
2019-02-15 01:06:22,078 : Text to Image: 6.664, 21.192, 31.892, 24.0
2019-02-15 01:07:03,270 : samples : 256000
2019-02-15 01:07:15,461 : Image to text: 7.76, 23.82, 35.2, 21.0
2019-02-15 01:07:22,366 : Text to Image: 6.924, 21.964, 32.704, 24.0
2019-02-15 01:08:03,439 : samples : 320000
2019-02-15 01:08:13,195 : Image to text: 8.04, 26.38, 37.98, 19.0
2019-02-15 01:08:23,110 : Text to Image: 7.04, 21.668, 32.996, 23.0
2019-02-15 01:09:03,611 : samples : 384000
2019-02-15 01:09:16,026 : Image to text: 6.98, 22.96, 34.66, 21.0
2019-02-15 01:09:25,844 : Text to Image: 6.32, 20.144, 30.688, 26.0
2019-02-15 01:10:08,174 : samples : 448000
2019-02-15 01:10:20,632 : Image to text: 8.16, 23.96, 36.62, 19.0
2019-02-15 01:10:30,507 : Text to Image: 6.416, 20.936, 31.488, 25.0
2019-02-15 01:11:13,159 : samples : 512000
2019-02-15 01:11:25,611 : Image to text: 8.16, 25.44, 36.92, 19.0
2019-02-15 01:11:35,507 : Text to Image: 6.984, 21.712, 33.02, 23.0
2019-02-15 01:12:11,796 : Epoch 5 finished
2019-02-15 01:12:12,669 : Image to text: 23.8, 55.2, 71.7, 4.0
2019-02-15 01:12:13,372 : Text to Image: 19.66, 50.2, 67.34, 5.0
2019-02-15 01:12:14,272 : Image to text: 22.1, 52.3, 68.6, 5.0
2019-02-15 01:12:14,998 : Text to Image: 18.58, 48.66, 65.84, 6.0
2019-02-15 01:12:15,901 : Image to text: 23.1, 52.7, 69.5, 5.0
2019-02-15 01:12:16,653 : Text to Image: 18.28, 50.38, 67.06, 5.0
2019-02-15 01:12:17,523 : Image to text: 21.6, 57.1, 72.8, 4.0
2019-02-15 01:12:18,246 : Text to Image: 18.04, 49.36, 67.28, 6.0
2019-02-15 01:12:19,106 : Image to text: 26.2, 57.5, 71.2, 4.0
2019-02-15 01:12:19,841 : Text to Image: 18.92, 50.42, 66.94, 5.0
2019-02-15 01:12:19,841 : Dev mean Text to Image: 18.695999999999998, 49.804, 66.89200000000001, 5.4
2019-02-15 01:12:19,841 : Dev mean Image to text: 23.36, 54.96, 70.75999999999999, 4.3999999999999995
2019-02-15 01:12:19,842 : start epoch
2019-02-15 01:13:02,241 : samples : 64000
2019-02-15 01:13:14,743 : Image to text: 8.5, 26.12, 38.4, 18.0
2019-02-15 01:13:24,689 : Text to Image: 7.412, 22.66, 33.88, 23.0
2019-02-15 01:14:07,518 : samples : 128000
2019-02-15 01:14:20,029 : Image to text: 7.24, 23.64, 35.06, 20.0
2019-02-15 01:14:29,989 : Text to Image: 6.624, 20.86, 31.316, 25.0
2019-02-15 01:15:12,273 : samples : 192000
2019-02-15 01:15:24,789 : Image to text: 6.82, 23.56, 36.18, 21.0
2019-02-15 01:15:34,689 : Text to Image: 6.416, 20.904, 31.32, 25.0
2019-02-15 01:16:15,354 : samples : 256000
2019-02-15 01:16:27,872 : Image to text: 7.56, 24.3, 36.52, 19.0
2019-02-15 01:16:37,825 : Text to Image: 6.776, 21.024, 31.832, 25.0
2019-02-15 01:17:18,472 : samples : 320000
2019-02-15 01:17:31,032 : Image to text: 8.94, 25.34, 37.24, 19.0
2019-02-15 01:17:40,988 : Text to Image: 7.208, 22.084, 33.028, 23.0
2019-02-15 01:18:21,935 : samples : 384000
2019-02-15 01:18:34,460 : Image to text: 8.54, 25.88, 38.46, 18.0
2019-02-15 01:18:44,453 : Text to Image: 7.196, 22.232, 33.492, 23.0
2019-02-15 01:19:26,535 : samples : 448000
2019-02-15 01:19:39,152 : Image to text: 8.0, 25.18, 37.32, 19.0
2019-02-15 01:19:49,145 : Text to Image: 6.868, 21.288, 32.136, 24.0
2019-02-15 01:20:31,898 : samples : 512000
2019-02-15 01:20:41,941 : Image to text: 7.66, 23.4, 34.6, 21.0
2019-02-15 01:20:49,140 : Text to Image: 5.924, 19.3, 30.164, 26.0
2019-02-15 01:21:23,901 : Epoch 6 finished
2019-02-15 01:21:24,348 : Image to text: 22.2, 56.6, 72.3, 4.0
2019-02-15 01:21:24,718 : Text to Image: 19.58, 50.62, 67.72, 5.0
2019-02-15 01:21:25,164 : Image to text: 21.8, 53.5, 68.6, 5.0
2019-02-15 01:21:25,533 : Text to Image: 17.98, 50.4, 67.74, 5.0
2019-02-15 01:21:25,981 : Image to text: 23.0, 53.0, 69.0, 5.0
2019-02-15 01:21:26,353 : Text to Image: 18.84, 49.82, 67.82, 6.0
2019-02-15 01:21:26,799 : Image to text: 24.2, 54.8, 72.1, 5.0
2019-02-15 01:21:27,167 : Text to Image: 18.6, 50.02, 67.06, 5.0
2019-02-15 01:21:27,614 : Image to text: 22.7, 55.5, 71.1, 4.0
2019-02-15 01:21:27,982 : Text to Image: 19.0, 50.28, 67.28, 5.0
2019-02-15 01:21:27,982 : Dev mean Text to Image: 18.8, 50.227999999999994, 67.524, 5.2
2019-02-15 01:21:27,982 : Dev mean Image to text: 22.78, 54.68, 70.62, 4.6
2019-02-15 01:21:27,983 : start epoch
2019-02-15 01:22:08,492 : samples : 64000
2019-02-15 01:22:21,304 : Image to text: 8.34, 24.88, 36.88, 19.0
2019-02-15 01:22:31,679 : Text to Image: 7.496, 22.232, 33.376, 23.0
2019-02-15 01:23:14,921 : samples : 128000
2019-02-15 01:23:27,790 : Image to text: 7.5, 24.14, 35.82, 20.0
2019-02-15 01:23:38,250 : Text to Image: 6.636, 20.82, 31.616, 25.0
2019-02-15 01:24:22,279 : samples : 192000
2019-02-15 01:24:35,126 : Image to text: 8.4, 26.32, 38.5, 18.0
2019-02-15 01:24:45,484 : Text to Image: 6.948, 22.108, 33.012, 23.0
2019-02-15 01:25:29,426 : samples : 256000
2019-02-15 01:25:42,288 : Image to text: 9.18, 26.54, 38.94, 18.0
2019-02-15 01:25:52,739 : Text to Image: 7.232, 22.928, 34.332, 22.0
2019-02-15 01:26:37,110 : samples : 320000
2019-02-15 01:26:49,963 : Image to text: 7.98, 24.84, 37.0, 20.0
2019-02-15 01:27:00,361 : Text to Image: 7.124, 22.22, 33.148, 22.0
2019-02-15 01:27:44,286 : samples : 384000
2019-02-15 01:27:57,113 : Image to text: 8.28, 26.24, 38.64, 18.0
2019-02-15 01:28:07,482 : Text to Image: 7.432, 22.804, 33.812, 22.0
2019-02-15 01:28:51,313 : samples : 448000
2019-02-15 01:29:04,171 : Image to text: 9.0, 25.2, 37.44, 19.0
2019-02-15 01:29:14,556 : Text to Image: 7.4, 22.812, 33.988, 22.0
2019-02-15 01:29:58,523 : samples : 512000
2019-02-15 01:30:11,330 : Image to text: 8.78, 25.64, 38.62, 18.0
2019-02-15 01:30:18,571 : Text to Image: 7.812, 23.292, 34.464, 22.0
2019-02-15 01:30:53,410 : Epoch 7 finished
2019-02-15 01:30:53,892 : Image to text: 24.3, 56.2, 73.6, 4.0
2019-02-15 01:30:54,217 : Text to Image: 21.12, 51.92, 69.22, 5.0
2019-02-15 01:30:54,700 : Image to text: 22.7, 55.8, 70.4, 4.0
2019-02-15 01:30:55,026 : Text to Image: 18.84, 51.1, 69.0, 5.0
2019-02-15 01:30:55,506 : Image to text: 23.3, 56.5, 70.7, 4.0
2019-02-15 01:30:55,837 : Text to Image: 19.2, 51.18, 68.28, 5.0
2019-02-15 01:30:56,319 : Image to text: 24.0, 58.1, 73.0, 4.0
2019-02-15 01:30:56,644 : Text to Image: 20.34, 51.1, 68.52, 5.0
2019-02-15 01:30:57,125 : Image to text: 25.0, 60.3, 73.0, 4.0
2019-02-15 01:30:57,450 : Text to Image: 20.78, 52.0, 68.54, 5.0
2019-02-15 01:30:57,450 : Dev mean Text to Image: 20.056, 51.46, 68.712, 5.0
2019-02-15 01:30:57,450 : Dev mean Image to text: 23.86, 57.38000000000001, 72.14, 4.0
2019-02-15 01:30:57,450 : start epoch
2019-02-15 01:31:38,417 : samples : 64000
2019-02-15 01:31:48,643 : Image to text: 9.2, 27.22, 39.3, 18.0
2019-02-15 01:31:55,898 : Text to Image: 7.66, 23.22, 34.464, 22.0
2019-02-15 01:32:37,211 : samples : 128000
2019-02-15 01:32:47,321 : Image to text: 8.34, 26.1, 37.78, 18.0
2019-02-15 01:32:54,615 : Text to Image: 7.096, 21.992, 32.84, 23.0
2019-02-15 01:33:35,029 : samples : 192000
2019-02-15 01:33:45,025 : Image to text: 8.72, 25.94, 38.88, 18.0
2019-02-15 01:33:52,307 : Text to Image: 7.388, 22.888, 33.96, 22.0
2019-02-15 01:34:33,069 : samples : 256000
2019-02-15 01:34:43,142 : Image to text: 8.78, 26.72, 39.02, 17.0
2019-02-15 01:34:50,508 : Text to Image: 7.86, 23.372, 34.688, 21.0
2019-02-15 01:35:31,801 : samples : 320000
2019-02-15 01:35:41,803 : Image to text: 9.56, 27.22, 39.82, 17.0
2019-02-15 01:35:49,094 : Text to Image: 7.568, 22.6, 33.92, 22.0
2019-02-15 01:36:34,680 : samples : 384000
2019-02-15 01:36:44,789 : Image to text: 8.44, 26.1, 38.18, 18.0
2019-02-15 01:36:52,088 : Text to Image: 7.476, 22.76, 34.204, 22.0
2019-02-15 01:37:33,228 : samples : 448000
2019-02-15 01:37:43,235 : Image to text: 8.94, 25.54, 39.04, 17.0
2019-02-15 01:37:50,554 : Text to Image: 7.56, 22.888, 34.192, 22.0
2019-02-15 01:38:30,930 : samples : 512000
2019-02-15 01:38:40,822 : Image to text: 8.38, 26.56, 38.4, 18.0
2019-02-15 01:38:48,044 : Text to Image: 7.368, 22.44, 33.584, 22.0
2019-02-15 01:39:23,723 : Epoch 8 finished
2019-02-15 01:39:24,198 : Image to text: 26.0, 57.1, 72.2, 4.0
2019-02-15 01:39:24,528 : Text to Image: 20.7, 52.88, 69.96, 5.0
2019-02-15 01:39:25,006 : Image to text: 22.1, 54.7, 70.3, 5.0
2019-02-15 01:39:25,335 : Text to Image: 19.52, 50.9, 68.14, 5.0
2019-02-15 01:39:25,813 : Image to text: 22.7, 54.3, 70.8, 5.0
2019-02-15 01:39:26,144 : Text to Image: 19.6, 50.94, 68.2, 5.0
2019-02-15 01:39:26,623 : Image to text: 23.3, 56.0, 72.4, 4.0
2019-02-15 01:39:26,953 : Text to Image: 20.56, 51.52, 68.54, 5.0
2019-02-15 01:39:27,432 : Image to text: 25.3, 57.3, 73.3, 4.0
2019-02-15 01:39:27,761 : Text to Image: 20.92, 51.58, 68.72, 5.0
2019-02-15 01:39:27,762 : Dev mean Text to Image: 20.26, 51.56399999999999, 68.712, 5.0
2019-02-15 01:39:27,762 : Dev mean Image to text: 23.880000000000003, 55.88, 71.8, 4.3999999999999995
2019-02-15 01:39:27,762 : start epoch
2019-02-15 01:40:09,139 : samples : 64000
2019-02-15 01:40:19,240 : Image to text: 8.5, 26.16, 38.86, 17.0
2019-02-15 01:40:26,493 : Text to Image: 7.232, 22.592, 33.888, 22.0
2019-02-15 01:41:07,747 : samples : 128000
2019-02-15 01:41:17,771 : Image to text: 8.06, 24.44, 37.14, 19.0
2019-02-15 01:41:25,055 : Text to Image: 7.112, 21.984, 32.728, 23.0
2019-02-15 01:42:06,156 : samples : 192000
2019-02-15 01:42:16,167 : Image to text: 8.5, 26.58, 38.14, 18.0
2019-02-15 01:42:23,420 : Text to Image: 7.464, 22.84, 34.108, 22.0
2019-02-15 01:43:04,560 : samples : 256000
2019-02-15 01:43:14,557 : Image to text: 7.78, 25.46, 37.58, 19.0
2019-02-15 01:43:21,783 : Text to Image: 7.26, 22.644, 33.936, 22.0
2019-02-15 01:44:03,131 : samples : 320000
2019-02-15 01:44:13,148 : Image to text: 9.38, 26.62, 38.4, 18.0
2019-02-15 01:44:20,455 : Text to Image: 7.88, 23.636, 34.932, 21.0
2019-02-15 01:45:01,571 : samples : 384000
2019-02-15 01:45:11,757 : Image to text: 8.16, 24.58, 36.96, 20.0
2019-02-15 01:45:19,035 : Text to Image: 7.356, 22.34, 33.628, 23.0
2019-02-15 01:45:58,231 : samples : 448000
2019-02-15 01:46:08,394 : Image to text: 8.88, 26.08, 38.3, 18.0
2019-02-15 01:46:15,704 : Text to Image: 7.636, 23.348, 34.564, 22.0
2019-02-15 01:46:54,866 : samples : 512000
2019-02-15 01:47:04,879 : Image to text: 9.12, 27.28, 39.52, 17.0
2019-02-15 01:47:12,097 : Text to Image: 7.776, 23.868, 35.764, 21.0
2019-02-15 01:47:45,304 : Epoch 9 finished
2019-02-15 01:47:45,778 : Image to text: 24.5, 55.9, 71.5, 4.0
2019-02-15 01:47:46,116 : Text to Image: 20.66, 52.56, 69.74, 5.0
2019-02-15 01:47:46,597 : Image to text: 22.0, 52.6, 71.0, 5.0
2019-02-15 01:47:46,933 : Text to Image: 20.06, 51.24, 68.98, 5.0
2019-02-15 01:47:47,414 : Image to text: 24.5, 54.6, 70.8, 4.0
2019-02-15 01:47:47,746 : Text to Image: 19.16, 51.36, 69.12, 5.0
2019-02-15 01:47:48,223 : Image to text: 23.7, 56.5, 73.7, 4.0
2019-02-15 01:47:48,551 : Text to Image: 19.86, 51.5, 68.82, 5.0
2019-02-15 01:47:49,032 : Image to text: 24.4, 58.5, 73.0, 4.0
2019-02-15 01:47:49,361 : Text to Image: 21.04, 52.06, 68.64, 5.0
2019-02-15 01:47:49,361 : Dev mean Text to Image: 20.156, 51.74400000000001, 69.05999999999999, 5.0
2019-02-15 01:47:49,361 : Dev mean Image to text: 23.82, 55.620000000000005, 72.0, 4.2
2019-02-15 01:47:49,361 : start epoch
2019-02-15 01:48:28,485 : samples : 64000
2019-02-15 01:48:38,633 : Image to text: 8.58, 26.34, 38.48, 18.0
2019-02-15 01:48:45,939 : Text to Image: 7.304, 22.904, 34.044, 22.0
2019-02-15 01:49:24,983 : samples : 128000
2019-02-15 01:49:35,133 : Image to text: 9.18, 27.36, 40.18, 17.0
2019-02-15 01:49:42,382 : Text to Image: 8.244, 24.408, 35.404, 21.0
2019-02-15 01:50:21,486 : samples : 192000
2019-02-15 01:50:31,674 : Image to text: 8.54, 26.96, 39.26, 17.0
2019-02-15 01:50:38,961 : Text to Image: 7.764, 23.172, 34.368, 22.0
2019-02-15 01:51:19,516 : samples : 256000
2019-02-15 01:51:29,663 : Image to text: 8.48, 26.24, 37.76, 18.0
2019-02-15 01:51:36,913 : Text to Image: 7.412, 22.564, 33.916, 22.0
2019-02-15 01:52:17,201 : samples : 320000
2019-02-15 01:52:27,318 : Image to text: 7.86, 25.78, 38.16, 18.0
2019-02-15 01:52:34,669 : Text to Image: 7.144, 22.668, 33.96, 22.0
2019-02-15 01:53:16,000 : samples : 384000
2019-02-15 01:53:26,120 : Image to text: 9.18, 27.16, 39.78, 17.0
2019-02-15 01:53:33,497 : Text to Image: 7.932, 23.972, 35.272, 21.0
2019-02-15 01:54:12,644 : samples : 448000
2019-02-15 01:54:22,775 : Image to text: 8.42, 26.08, 37.98, 19.0
2019-02-15 01:54:30,053 : Text to Image: 7.192, 22.252, 33.22, 23.0
2019-02-15 01:55:09,560 : samples : 512000
2019-02-15 01:55:19,573 : Image to text: 9.3, 26.12, 38.86, 18.0
2019-02-15 01:55:26,820 : Text to Image: 7.536, 22.912, 34.404, 22.0
2019-02-15 01:56:02,535 : Epoch 10 finished
2019-02-15 01:56:03,022 : Image to text: 25.0, 57.3, 73.2, 4.0
2019-02-15 01:56:03,357 : Text to Image: 20.84, 52.68, 70.02, 5.0
2019-02-15 01:56:03,842 : Image to text: 22.5, 54.3, 69.6, 5.0
2019-02-15 01:56:04,168 : Text to Image: 19.72, 50.48, 68.3, 5.0
2019-02-15 01:56:04,650 : Image to text: 24.8, 56.6, 71.4, 4.0
2019-02-15 01:56:04,976 : Text to Image: 18.8, 50.92, 68.96, 5.0
2019-02-15 01:56:05,456 : Image to text: 23.1, 57.9, 72.1, 4.0
2019-02-15 01:56:05,782 : Text to Image: 20.12, 49.88, 68.36, 6.0
2019-02-15 01:56:06,262 : Image to text: 24.1, 58.5, 71.9, 4.0
2019-02-15 01:56:06,588 : Text to Image: 21.08, 51.56, 67.9, 5.0
2019-02-15 01:56:06,588 : Dev mean Text to Image: 20.112000000000002, 51.104, 68.708, 5.2
2019-02-15 01:56:06,588 : Dev mean Image to text: 23.900000000000002, 56.92, 71.64, 4.2
2019-02-15 01:56:10,348 : 
Test scores | Image to text:             24.159999999999997, 56.959999999999994, 73.16, 4.2
2019-02-15 01:56:10,348 : Test scores | Text to image:             20.252, 51.864, 69.24, 5.0

2019-02-15 01:56:10,449 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 01:56:10,649 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 01:56:11,266 : loading BERT model bert-base-uncased
2019-02-15 01:56:11,266 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:56:11,294 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:56:11,294 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp11oxbjn8
2019-02-15 01:56:13,645 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:56:15,038 : Computing embeddings for train/dev/test
2019-02-15 01:57:48,683 : Computed embeddings
2019-02-15 01:57:48,683 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 01:58:26,261 : [('reg:1e-05', 59.29), ('reg:0.0001', 53.43), ('reg:0.001', 42.89), ('reg:0.01', 32.19)]
2019-02-15 01:58:26,261 : Validation : best param found is reg = 1e-05 with score             59.29
2019-02-15 01:58:26,261 : Evaluating...
2019-02-15 01:58:40,230 : 
Dev acc : 59.3 Test acc : 59.8 for LENGTH classification

2019-02-15 01:58:40,231 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 01:58:40,595 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 01:58:40,643 : loading BERT model bert-base-uncased
2019-02-15 01:58:40,644 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 01:58:40,673 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 01:58:40,674 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpo2dpffku
2019-02-15 01:58:43,021 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 01:58:44,461 : Computing embeddings for train/dev/test
2019-02-15 02:00:13,114 : Computed embeddings
2019-02-15 02:00:13,114 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:00:46,647 : [('reg:1e-05', 8.19), ('reg:0.0001', 0.85), ('reg:0.001', 0.15), ('reg:0.01', 0.14)]
2019-02-15 02:00:46,647 : Validation : best param found is reg = 1e-05 with score             8.19
2019-02-15 02:00:46,647 : Evaluating...
2019-02-15 02:00:55,804 : 
Dev acc : 8.2 Test acc : 8.5 for WORDCONTENT classification

2019-02-15 02:00:55,806 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 02:00:56,184 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 02:00:56,254 : loading BERT model bert-base-uncased
2019-02-15 02:00:56,254 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:00:56,281 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:00:56,281 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpvljof3gn
2019-02-15 02:00:58,681 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:01:00,138 : Computing embeddings for train/dev/test
2019-02-15 02:02:23,610 : Computed embeddings
2019-02-15 02:02:23,610 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:02:50,715 : [('reg:1e-05', 25.58), ('reg:0.0001', 22.55), ('reg:0.001', 20.55), ('reg:0.01', 18.37)]
2019-02-15 02:02:50,715 : Validation : best param found is reg = 1e-05 with score             25.58
2019-02-15 02:02:50,715 : Evaluating...
2019-02-15 02:02:58,359 : 
Dev acc : 25.6 Test acc : 24.6 for DEPTH classification

2019-02-15 02:02:58,360 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 02:02:58,761 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 02:02:58,827 : loading BERT model bert-base-uncased
2019-02-15 02:02:58,828 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:02:58,949 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:02:58,949 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmps2hyaxec
2019-02-15 02:03:01,343 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:03:02,772 : Computing embeddings for train/dev/test
2019-02-15 02:04:20,156 : Computed embeddings
2019-02-15 02:04:20,156 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:04:55,030 : [('reg:1e-05', 62.26), ('reg:0.0001', 47.61), ('reg:0.001', 27.53), ('reg:0.01', 10.84)]
2019-02-15 02:04:55,030 : Validation : best param found is reg = 1e-05 with score             62.26
2019-02-15 02:04:55,031 : Evaluating...
2019-02-15 02:05:05,876 : 
Dev acc : 62.3 Test acc : 62.1 for TOPCONSTITUENTS classification

2019-02-15 02:05:05,877 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 02:05:06,393 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 02:05:06,457 : loading BERT model bert-base-uncased
2019-02-15 02:05:06,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:05:06,486 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:05:06,486 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp2bmi2k0h
2019-02-15 02:05:08,915 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:05:10,324 : Computing embeddings for train/dev/test
2019-02-15 02:06:32,921 : Computed embeddings
2019-02-15 02:06:32,922 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:07:12,200 : [('reg:1e-05', 84.95), ('reg:0.0001', 84.23), ('reg:0.001', 82.0), ('reg:0.01', 75.34)]
2019-02-15 02:07:12,201 : Validation : best param found is reg = 1e-05 with score             84.95
2019-02-15 02:07:12,201 : Evaluating...
2019-02-15 02:07:21,993 : 
Dev acc : 85.0 Test acc : 84.2 for BIGRAMSHIFT classification

2019-02-15 02:07:21,995 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 02:07:22,399 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 02:07:22,468 : loading BERT model bert-base-uncased
2019-02-15 02:07:22,468 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:07:22,599 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:07:22,600 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpziszyvbt
2019-02-15 02:07:24,977 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:07:26,404 : Computing embeddings for train/dev/test
2019-02-15 02:08:47,423 : Computed embeddings
2019-02-15 02:08:47,424 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:09:21,264 : [('reg:1e-05', 89.39), ('reg:0.0001', 89.28), ('reg:0.001', 87.92), ('reg:0.01', 86.8)]
2019-02-15 02:09:21,264 : Validation : best param found is reg = 1e-05 with score             89.39
2019-02-15 02:09:21,264 : Evaluating...
2019-02-15 02:09:30,172 : 
Dev acc : 89.4 Test acc : 88.4 for TENSE classification

2019-02-15 02:09:30,173 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 02:09:30,793 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 02:09:30,860 : loading BERT model bert-base-uncased
2019-02-15 02:09:30,860 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:09:30,891 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:09:30,892 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp6jpiovul
2019-02-15 02:09:33,239 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:09:34,696 : Computing embeddings for train/dev/test
2019-02-15 02:11:00,895 : Computed embeddings
2019-02-15 02:11:00,895 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:11:34,824 : [('reg:1e-05', 79.65), ('reg:0.0001', 79.02), ('reg:0.001', 75.61), ('reg:0.01', 60.5)]
2019-02-15 02:11:34,824 : Validation : best param found is reg = 1e-05 with score             79.65
2019-02-15 02:11:34,824 : Evaluating...
2019-02-15 02:11:46,091 : 
Dev acc : 79.7 Test acc : 78.5 for SUBJNUMBER classification

2019-02-15 02:11:46,092 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 02:11:46,719 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 02:11:46,790 : loading BERT model bert-base-uncased
2019-02-15 02:11:46,790 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:11:46,822 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:11:46,822 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpmmub3qt8
2019-02-15 02:11:49,227 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:11:50,656 : Computing embeddings for train/dev/test
2019-02-15 02:13:16,005 : Computed embeddings
2019-02-15 02:13:16,005 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:13:42,070 : [('reg:1e-05', 70.34), ('reg:0.0001', 69.97), ('reg:0.001', 67.18), ('reg:0.01', 55.21)]
2019-02-15 02:13:42,070 : Validation : best param found is reg = 1e-05 with score             70.34
2019-02-15 02:13:42,070 : Evaluating...
2019-02-15 02:13:49,662 : 
Dev acc : 70.3 Test acc : 72.0 for OBJNUMBER classification

2019-02-15 02:13:49,664 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 02:13:50,096 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 02:13:50,169 : loading BERT model bert-base-uncased
2019-02-15 02:13:50,169 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:13:50,200 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:13:50,200 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpjdc2xq27
2019-02-15 02:13:52,601 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:13:53,997 : Computing embeddings for train/dev/test
2019-02-15 02:15:31,759 : Computed embeddings
2019-02-15 02:15:31,760 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:16:11,915 : [('reg:1e-05', 63.73), ('reg:0.0001', 63.88), ('reg:0.001', 63.34), ('reg:0.01', 59.68)]
2019-02-15 02:16:11,916 : Validation : best param found is reg = 0.0001 with score             63.88
2019-02-15 02:16:11,916 : Evaluating...
2019-02-15 02:16:21,462 : 
Dev acc : 63.9 Test acc : 62.7 for ODDMANOUT classification

2019-02-15 02:16:21,463 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 02:16:21,895 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 02:16:21,976 : loading BERT model bert-base-uncased
2019-02-15 02:16:21,976 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:16:22,113 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:16:22,114 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp7yzrgqu_
2019-02-15 02:16:24,470 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:16:25,901 : Computing embeddings for train/dev/test
2019-02-15 02:18:01,804 : Computed embeddings
2019-02-15 02:18:01,804 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:18:36,978 : [('reg:1e-05', 65.87), ('reg:0.0001', 65.7), ('reg:0.001', 62.87), ('reg:0.01', 58.25)]
2019-02-15 02:18:36,978 : Validation : best param found is reg = 1e-05 with score             65.87
2019-02-15 02:18:36,978 : Evaluating...
2019-02-15 02:18:45,725 : 
Dev acc : 65.9 Test acc : 66.6 for COORDINATIONINVERSION classification

2019-02-15 02:18:45,727 : total results: {'STS12': {'MSRpar': {'pearson': (-0.0020736311467958904, 0.9547888993382473), 'spearman': SpearmanrResult(correlation=0.009471880695009413, pvalue=0.7956573975883443), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.016813424200010293, 0.6457178813331201), 'spearman': SpearmanrResult(correlation=0.017149276906136168, pvalue=0.6391386729026772), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.1434159755218335, 0.002069006368971839), 'spearman': SpearmanrResult(correlation=0.33435670791763294, pvalue=1.8864772304068578e-13), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.05152124022525262, 0.15867257724907485), 'spearman': SpearmanrResult(correlation=0.28313904578136817, pvalue=2.713128016697157e-15), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.06565749673776324, 0.19060027393201012), 'spearman': SpearmanrResult(correlation=0.10492651728954794, pvalue=0.036160649837672185), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.04834153142760863, 'wmean': 0.03748420612024579}, 'spearman': {'mean': 0.14980868571793893, 'wmean': 0.13759831462986113}}}, 'STS13': {'FNWN': {'pearson': (0.04970677099259791, 0.496984663819233), 'spearman': SpearmanrResult(correlation=0.0710307330297198, pvalue=0.3314186154514318), 'nsamples': 189}, 'headlines': {'pearson': (-0.06608162136901022, 0.07050096744800699), 'spearman': SpearmanrResult(correlation=0.42203302837194306, pvalue=9.480187182392503e-34), 'nsamples': 750}, 'OnWN': {'pearson': (-0.05852721235398564, 0.16625510666994467), 'spearman': SpearmanrResult(correlation=0.13037171488618102, pvalue=0.0019731807232043026), 'nsamples': 561}, 'all': {'pearson': {'mean': -0.024967354243465984, 'wmean': -0.0486669349598284}, 'spearman': {'mean': 0.2078118254292813, 'wmean': 0.26872540791514793}}}, 'STS14': {'deft-forum': {'pearson': (-0.016920485457097404, 0.7203695042824644), 'spearman': SpearmanrResult(correlation=0.044760154155896265, pvalue=0.3434654428942904), 'nsamples': 450}, 'deft-news': {'pearson': (0.23414189824500198, 4.210765370590917e-05), 'spearman': SpearmanrResult(correlation=0.3571277228534667, pvalue=1.8763548200014383e-10), 'nsamples': 300}, 'headlines': {'pearson': (0.0031596522385580364, 0.9311591564491195), 'spearman': SpearmanrResult(correlation=0.42851227190889773, pvalue=7.542827627043108e-35), 'nsamples': 750}, 'images': {'pearson': (-0.01318658511166525, 0.7184412279255556), 'spearman': SpearmanrResult(correlation=0.0836991608005345, pvalue=0.021882790075829602), 'nsamples': 750}, 'OnWN': {'pearson': (-0.06536976463650053, 0.07359022838699009), 'spearman': SpearmanrResult(correlation=0.20699846969613883, pvalue=1.0558052375853287e-08), 'nsamples': 750}, 'tweet-news': {'pearson': (0.09971925707606406, 0.006272518282530805), 'spearman': SpearmanrResult(correlation=0.3568022315552992, pvalue=6.174146790650887e-24), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.040257328725726814, 'wmean': 0.021565405518039732}, 'spearman': {'mean': 0.2463166684950389, 'wmean': 0.24914386311915895}}}, 'STS15': {'answers-forums': {'pearson': (-0.02378260806239872, 0.6461798465788613), 'spearman': SpearmanrResult(correlation=-0.03168485292504799, pvalue=0.540749665168127), 'nsamples': 375}, 'answers-students': {'pearson': (0.04992393060728018, 0.17200349405571105), 'spearman': SpearmanrResult(correlation=0.1346318158842073, pvalue=0.00021752604115826224), 'nsamples': 750}, 'belief': {'pearson': (0.0668738418505241, 0.1963121616987218), 'spearman': SpearmanrResult(correlation=0.1398652906791396, pvalue=0.006672055757866569), 'nsamples': 375}, 'headlines': {'pearson': (-0.03616493857335794, 0.32261948323611134), 'spearman': SpearmanrResult(correlation=0.5055094373443598, pvalue=6.732263016259959e-50), 'nsamples': 750}, 'images': {'pearson': (0.029006280998880328, 0.4276545157081214), 'spearman': SpearmanrResult(correlation=0.04692195639672663, pvalue=0.19929091038891902), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.01717130136418559, 'wmean': 0.016077722481716312}, 'spearman': {'mean': 0.15904872947587706, 'wmean': 0.18528835712558486}}}, 'STS16': {'answer-answer': {'pearson': (0.15702557151182572, 0.012217605930184384), 'spearman': SpearmanrResult(correlation=0.2441495626287863, pvalue=8.43920160894854e-05), 'nsamples': 254}, 'headlines': {'pearson': (-0.03645399928663765, 0.5669627420176866), 'spearman': SpearmanrResult(correlation=0.5639069542412851, pvalue=2.650661881762141e-22), 'nsamples': 249}, 'plagiarism': {'pearson': (0.1979140198804957, 0.002569441507045376), 'spearman': SpearmanrResult(correlation=0.17512956650115877, pvalue=0.007765350495944233), 'nsamples': 230}, 'postediting': {'pearson': (0.2721078954633558, 1.630750785423585e-05), 'spearman': SpearmanrResult(correlation=0.3714750449444143, pvalue=2.118011543412368e-09), 'nsamples': 244}, 'question-question': {'pearson': (0.02376323281760822, 0.7327054061334248), 'spearman': SpearmanrResult(correlation=0.2945928139917619, pvalue=1.492372416423709e-05), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.12287134407732954, 'wmean': 0.12452657341153786}, 'spearman': {'mean': 0.3298507884614813, 'wmean': 0.3329818127316809}}}, 'MR': {'devacc': 75.09, 'acc': 75.13, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.57, 'acc': 76.27, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 81.01, 'acc': 82.71, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 93.39, 'acc': 93.02, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 83.37, 'acc': 83.03, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 40.96, 'acc': 39.5, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 62.51, 'acc': 75.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 67.89, 'acc': 65.8, 'f1': 78.4, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 69.4, 'acc': 66.71, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.681500358418841, 'pearson': 0.6921481976790592, 'spearman': 0.6457122512639326, 'mse': 0.534431821585591, 'yhat': array([2.99063304, 3.60502735, 3.37623358, ..., 3.38983217, 3.61518835,        4.67811155]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.2377149626595009, 'pearson': 0.23336685242288113, 'spearman': 0.3054802326068784, 'mse': 2.335368654706889, 'yhat': array([2.76958828, 2.80633885, 2.91632935, ..., 3.67968613, 3.16482428,        3.49670234]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 58.11, 'acc': 58.15, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 293.608, 'acc': [(24.159999999999997, 56.959999999999994, 73.16, 4.2), (20.252, 51.864, 69.24, 5.0)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 59.29, 'acc': 59.78, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 8.19, 'acc': 8.54, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.58, 'acc': 24.64, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 62.26, 'acc': 62.08, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 84.95, 'acc': 84.22, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.39, 'acc': 88.4, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 79.65, 'acc': 78.47, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 70.34, 'acc': 71.96, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 63.88, 'acc': 62.72, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 65.87, 'acc': 66.59, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 02:18:45,728 : STS12 p=0.0375, STS12 s=0.1376, STS13 p=-0.0487, STS13 s=0.2687, STS14 p=0.0216, STS14 s=0.2491, STS15 p=0.0161, STS15 s=0.1853, STS 16 p=0.1245, STS16 s=0.3330, STS B p=0.2334, STS B s=0.3055, STS B m=2.3354, SICK-R p=0.6921, SICK-R s=0.6457, SICK-P m=0.5344
2019-02-15 02:18:45,728 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 02:18:45,728 : 0.0375,0.1376,-0.0487,0.2687,0.0216,0.2491,0.0161,0.1853,0.1245,0.3330,0.2334,0.3055,2.3354,0.6921,0.6457,0.5344
2019-02-15 02:18:45,728 : MR=75.13, CR=76.27, SUBJ=93.02, MPQA=82.71, SST-B=83.03, SST-F=39.50, TREC=75.20, SICK-E=66.71, SNLI=58.15, MRPC=65.80, MRPC f=78.40
2019-02-15 02:18:45,728 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 02:18:45,728 : 75.13,76.27,93.02,82.71,83.03,39.50,75.20,66.71,58.15,65.80,78.40
2019-02-15 02:18:45,728 : COCO r1i2t=24.16, COCO r5i2t=56.96, COCO r10i2t=73.16, COCO medr_i2t=4.20, COCO r1t2i=20.25, COCO r5t2i=51.86, COCO r10t2i=69.24, COCO medr_t2i=5.00
2019-02-15 02:18:45,728 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 02:18:45,728 : 24.16,56.96,73.16,4.20,20.25,51.86,69.24,5.00
2019-02-15 02:18:45,728 : SentLen=59.78, WC=8.54, TreeDepth=24.64, TopConst=62.08, BShift=84.22, Tense=88.40, SubjNum=78.47, ObjNum=71.96, SOMO=62.72, CoordInv=66.59, average=60.74
2019-02-15 02:18:45,728 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 02:18:45,728 : 59.78,8.54,24.64,62.08,84.22,88.40,78.47,71.96,62.72,66.59,60.74
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,728 : layer 12
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,728 : ********************************************************************************
2019-02-15 02:18:45,825 : ***** Transfer task : STS12 *****


2019-02-15 02:18:45,837 : loading BERT model bert-base-uncased
2019-02-15 02:18:45,837 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:18:45,855 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:18:45,856 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9dge7ktp
2019-02-15 02:18:48,303 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:18:51,550 : MSRpar : pearson = 0.0163, spearman = 0.0630
2019-02-15 02:18:52,320 : MSRvid : pearson = -0.0103, spearman = 0.0302
2019-02-15 02:18:52,941 : SMTeuroparl : pearson = 0.1663, spearman = 0.3681
2019-02-15 02:18:54,121 : surprise.OnWN : pearson = 0.1408, spearman = 0.3793
2019-02-15 02:18:54,779 : surprise.SMTnews : pearson = 0.1048, spearman = 0.2049
2019-02-15 02:18:54,779 : ALL (weighted average) : Pearson = 0.0735,             Spearman = 0.1947
2019-02-15 02:18:54,779 : ALL (average) : Pearson = 0.0836,             Spearman = 0.2091

2019-02-15 02:18:54,779 : ***** Transfer task : STS13 (-SMT) *****


2019-02-15 02:18:54,789 : loading BERT model bert-base-uncased
2019-02-15 02:18:54,789 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:18:54,807 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:18:54,807 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1b0gybnx
2019-02-15 02:18:57,196 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:18:59,227 : FNWN : pearson = 0.0505, spearman = 0.0720
2019-02-15 02:19:00,177 : headlines : pearson = 0.0838, spearman = 0.5279
2019-02-15 02:19:00,904 : OnWN : pearson = -0.0242, spearman = 0.1483
2019-02-15 02:19:00,904 : ALL (weighted average) : Pearson = 0.0392,             Spearman = 0.3284
2019-02-15 02:19:00,904 : ALL (average) : Pearson = 0.0367,             Spearman = 0.2494

2019-02-15 02:19:00,904 : ***** Transfer task : STS14 *****


2019-02-15 02:19:00,919 : loading BERT model bert-base-uncased
2019-02-15 02:19:00,919 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:19:00,936 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:19:00,936 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzh28m2uo
2019-02-15 02:19:03,360 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:19:05,464 : deft-forum : pearson = 0.0097, spearman = 0.1380
2019-02-15 02:19:06,234 : deft-news : pearson = 0.2956, spearman = 0.4707
2019-02-15 02:19:07,311 : headlines : pearson = 0.1930, spearman = 0.5071
2019-02-15 02:19:08,340 : images : pearson = 0.0163, spearman = 0.1450
2019-02-15 02:19:09,386 : OnWN : pearson = 0.0163, spearman = 0.3209
2019-02-15 02:19:10,759 : tweet-news : pearson = 0.1938, spearman = 0.4368
2019-02-15 02:19:10,759 : ALL (weighted average) : Pearson = 0.1087,             Spearman = 0.3362
2019-02-15 02:19:10,759 : ALL (average) : Pearson = 0.1208,             Spearman = 0.3364

2019-02-15 02:19:10,760 : ***** Transfer task : STS15 *****


2019-02-15 02:19:10,827 : loading BERT model bert-base-uncased
2019-02-15 02:19:10,827 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:19:10,844 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:19:10,844 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpd_65gtrz
2019-02-15 02:19:13,178 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:19:15,425 : answers-forums : pearson = 0.0233, spearman = 0.0370
2019-02-15 02:19:16,346 : answers-students : pearson = 0.0692, spearman = 0.1836
2019-02-15 02:19:17,188 : belief : pearson = 0.1252, spearman = 0.2258
2019-02-15 02:19:18,169 : headlines : pearson = 0.1823, spearman = 0.5869
2019-02-15 02:19:19,110 : images : pearson = 0.0523, spearman = 0.1072
2019-02-15 02:19:19,110 : ALL (weighted average) : Pearson = 0.0945,             Spearman = 0.2523
2019-02-15 02:19:19,111 : ALL (average) : Pearson = 0.0905,             Spearman = 0.2281

2019-02-15 02:19:19,111 : ***** Transfer task : STS16 *****


2019-02-15 02:19:19,150 : loading BERT model bert-base-uncased
2019-02-15 02:19:19,150 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:19:19,204 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:19:19,204 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3y1px2hn
2019-02-15 02:19:21,553 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:19:23,394 : answer-answer : pearson = 0.1940, spearman = 0.3039
2019-02-15 02:19:23,705 : headlines : pearson = 0.3225, spearman = 0.6261
2019-02-15 02:19:24,104 : plagiarism : pearson = 0.2518, spearman = 0.3172
2019-02-15 02:19:24,728 : postediting : pearson = 0.3867, spearman = 0.4923
2019-02-15 02:19:25,018 : question-question : pearson = 0.1809, spearman = 0.4721
2019-02-15 02:19:25,018 : ALL (weighted average) : Pearson = 0.2695,             Spearman = 0.4425
2019-02-15 02:19:25,018 : ALL (average) : Pearson = 0.2672,             Spearman = 0.4423

2019-02-15 02:19:25,018 : ***** Transfer task : MR *****


2019-02-15 02:19:25,034 : loading BERT model bert-base-uncased
2019-02-15 02:19:25,034 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:19:25,054 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:19:25,055 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpp8u1ok50
2019-02-15 02:19:27,472 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:19:28,890 : Generating sentence embeddings
2019-02-15 02:19:42,325 : Generated sentence embeddings
2019-02-15 02:19:42,325 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 02:19:52,050 : Best param found at split 1: l2reg = 1e-05                 with score 76.77
2019-02-15 02:20:03,046 : Best param found at split 2: l2reg = 0.0001                 with score 76.65
2019-02-15 02:20:15,452 : Best param found at split 3: l2reg = 1e-05                 with score 76.35
2019-02-15 02:20:24,935 : Best param found at split 4: l2reg = 0.001                 with score 75.13
2019-02-15 02:20:35,116 : Best param found at split 5: l2reg = 1e-05                 with score 74.15
2019-02-15 02:20:35,659 : Dev acc : 75.81 Test acc : 73.1

2019-02-15 02:20:35,660 : ***** Transfer task : CR *****


2019-02-15 02:20:35,667 : loading BERT model bert-base-uncased
2019-02-15 02:20:35,668 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:20:35,726 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:20:35,726 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf17paipf
2019-02-15 02:20:38,134 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:20:39,625 : Generating sentence embeddings
2019-02-15 02:20:43,303 : Generated sentence embeddings
2019-02-15 02:20:43,304 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 02:20:46,760 : Best param found at split 1: l2reg = 0.001                 with score 80.59
2019-02-15 02:20:50,485 : Best param found at split 2: l2reg = 0.0001                 with score 81.25
2019-02-15 02:20:54,017 : Best param found at split 3: l2reg = 0.01                 with score 82.02
2019-02-15 02:20:57,666 : Best param found at split 4: l2reg = 0.001                 with score 82.36
2019-02-15 02:21:02,015 : Best param found at split 5: l2reg = 0.01                 with score 81.83
2019-02-15 02:21:02,226 : Dev acc : 81.61 Test acc : 79.68

2019-02-15 02:21:02,227 : ***** Transfer task : MPQA *****


2019-02-15 02:21:02,232 : loading BERT model bert-base-uncased
2019-02-15 02:21:02,232 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:21:02,251 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:21:02,251 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp4s2odgo1
2019-02-15 02:21:04,586 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:21:06,000 : Generating sentence embeddings
2019-02-15 02:21:09,612 : Generated sentence embeddings
2019-02-15 02:21:09,613 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 02:21:19,248 : Best param found at split 1: l2reg = 1e-05                 with score 82.17
2019-02-15 02:21:29,190 : Best param found at split 2: l2reg = 0.001                 with score 83.88
2019-02-15 02:21:42,150 : Best param found at split 3: l2reg = 0.001                 with score 83.54
2019-02-15 02:21:55,788 : Best param found at split 4: l2reg = 1e-05                 with score 83.12
2019-02-15 02:22:09,532 : Best param found at split 5: l2reg = 0.0001                 with score 83.89
2019-02-15 02:22:10,156 : Dev acc : 83.32 Test acc : 83.95

2019-02-15 02:22:10,156 : ***** Transfer task : SUBJ *****


2019-02-15 02:22:10,173 : loading BERT model bert-base-uncased
2019-02-15 02:22:10,174 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:22:10,194 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:22:10,194 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp3p2s29wv
2019-02-15 02:22:12,557 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:22:13,992 : Generating sentence embeddings
2019-02-15 02:22:27,102 : Generated sentence embeddings
2019-02-15 02:22:27,103 : Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation
2019-02-15 02:22:38,254 : Best param found at split 1: l2reg = 0.001                 with score 93.98
2019-02-15 02:22:48,615 : Best param found at split 2: l2reg = 0.001                 with score 94.18
2019-02-15 02:23:01,669 : Best param found at split 3: l2reg = 0.0001                 with score 94.13
2019-02-15 02:23:15,055 : Best param found at split 4: l2reg = 1e-05                 with score 94.61
2019-02-15 02:23:28,019 : Best param found at split 5: l2reg = 1e-05                 with score 94.29
2019-02-15 02:23:28,921 : Dev acc : 94.24 Test acc : 94.1

2019-02-15 02:23:28,922 : ***** Transfer task : SST Binary classification *****


2019-02-15 02:23:29,057 : loading BERT model bert-base-uncased
2019-02-15 02:23:29,057 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:23:29,083 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:23:29,083 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpf5r4w6e2
2019-02-15 02:23:31,464 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:23:32,852 : Computing embedding for train
2019-02-15 02:24:17,479 : Computed train embeddings
2019-02-15 02:24:17,480 : Computing embedding for dev
2019-02-15 02:24:18,415 : Computed dev embeddings
2019-02-15 02:24:18,415 : Computing embedding for test
2019-02-15 02:24:20,378 : Computed test embeddings
2019-02-15 02:24:20,378 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:24:44,615 : [('reg:1e-05', 82.91), ('reg:0.0001', 82.8), ('reg:0.001', 80.73), ('reg:0.01', 79.13)]
2019-02-15 02:24:44,615 : Validation : best param found is reg = 1e-05 with score             82.91
2019-02-15 02:24:44,615 : Evaluating...
2019-02-15 02:24:50,728 : 
Dev acc : 82.91 Test acc : 81.77 for             SST Binary classification

2019-02-15 02:24:50,729 : ***** Transfer task : SST Fine-Grained classification *****


2019-02-15 02:24:50,782 : loading BERT model bert-base-uncased
2019-02-15 02:24:50,782 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:24:50,837 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:24:50,837 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp_q980ym2
2019-02-15 02:24:53,209 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:24:54,665 : Computing embedding for train
2019-02-15 02:25:04,062 : Computed train embeddings
2019-02-15 02:25:04,062 : Computing embedding for dev
2019-02-15 02:25:05,288 : Computed dev embeddings
2019-02-15 02:25:05,288 : Computing embedding for test
2019-02-15 02:25:07,664 : Computed test embeddings
2019-02-15 02:25:07,664 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:25:10,946 : [('reg:1e-05', 40.33), ('reg:0.0001', 41.69), ('reg:0.001', 41.33), ('reg:0.01', 37.6)]
2019-02-15 02:25:10,946 : Validation : best param found is reg = 0.0001 with score             41.69
2019-02-15 02:25:10,946 : Evaluating...
2019-02-15 02:25:11,720 : 
Dev acc : 41.69 Test acc : 42.17 for             SST Fine-Grained classification

2019-02-15 02:25:11,721 : ***** Transfer task : TREC *****


2019-02-15 02:25:11,733 : loading BERT model bert-base-uncased
2019-02-15 02:25:11,733 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:25:11,755 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:25:11,755 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp1wxtyk7p
2019-02-15 02:25:14,151 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:25:18,942 : Computed train embeddings
2019-02-15 02:25:19,205 : Computed test embeddings
2019-02-15 02:25:19,205 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 02:25:27,452 : [('reg:1e-05', 63.72), ('reg:0.0001', 63.3), ('reg:0.001', 63.81), ('reg:0.01', 53.4)]
2019-02-15 02:25:27,453 : Cross-validation : best param found is reg = 0.001             with score 63.81
2019-02-15 02:25:27,453 : Evaluating...
2019-02-15 02:25:28,052 : 
Dev acc : 63.81 Test acc : 83.2             for TREC

2019-02-15 02:25:28,053 : ***** Transfer task : MRPC *****


2019-02-15 02:25:28,108 : loading BERT model bert-base-uncased
2019-02-15 02:25:28,108 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:25:28,132 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:25:28,132 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp63xa_tv8
2019-02-15 02:25:30,547 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:25:31,978 : Computing embedding for train
2019-02-15 02:25:42,431 : Computed train embeddings
2019-02-15 02:25:42,431 : Computing embedding for test
2019-02-15 02:25:46,692 : Computed test embeddings
2019-02-15 02:25:46,708 : Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation
2019-02-15 02:25:52,100 : [('reg:1e-05', 69.48), ('reg:0.0001', 68.67), ('reg:0.001', 68.38), ('reg:0.01', 68.87)]
2019-02-15 02:25:52,100 : Cross-validation : best param found is reg = 1e-05             with score 69.48
2019-02-15 02:25:52,100 : Evaluating...
2019-02-15 02:25:52,364 : Dev acc : 69.48 Test acc 66.49; Test F1 79.87 for MRPC.

2019-02-15 02:25:52,364 : ***** Transfer task : SICK-Entailment*****


2019-02-15 02:25:52,389 : loading BERT model bert-base-uncased
2019-02-15 02:25:52,389 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:25:52,449 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:25:52,449 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpulkd8qml
2019-02-15 02:25:54,901 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:25:56,322 : Computing embedding for train
2019-02-15 02:26:01,384 : Computed train embeddings
2019-02-15 02:26:01,384 : Computing embedding for dev
2019-02-15 02:26:02,073 : Computed dev embeddings
2019-02-15 02:26:02,073 : Computing embedding for test
2019-02-15 02:26:07,552 : Computed test embeddings
2019-02-15 02:26:07,581 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:26:09,219 : [('reg:1e-05', 75.8), ('reg:0.0001', 76.8), ('reg:0.001', 74.4), ('reg:0.01', 70.4)]
2019-02-15 02:26:09,219 : Validation : best param found is reg = 0.0001 with score             76.8
2019-02-15 02:26:09,219 : Evaluating...
2019-02-15 02:26:09,752 : 
Dev acc : 76.8 Test acc : 74.41 for                        SICK entailment

2019-02-15 02:26:09,753 : ***** Transfer task : SICK-Relatedness*****


2019-02-15 02:26:09,781 : loading BERT model bert-base-uncased
2019-02-15 02:26:09,781 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:26:09,802 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:26:09,802 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzr5o4qc9
2019-02-15 02:26:12,172 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:26:13,586 : Computing embedding for train
2019-02-15 02:26:18,670 : Computed train embeddings
2019-02-15 02:26:18,671 : Computing embedding for dev
2019-02-15 02:26:19,372 : Computed dev embeddings
2019-02-15 02:26:19,372 : Computing embedding for test
2019-02-15 02:26:24,876 : Computed test embeddings
2019-02-15 02:26:46,544 : Dev : Pearson 0.7558401559974782
2019-02-15 02:26:46,544 : Test : Pearson 0.7636444241811735 Spearman 0.6978110340036805 MSE 0.42466150021616883                        for SICK Relatedness

2019-02-15 02:26:46,544 : 

***** Transfer task : STSBenchmark*****


2019-02-15 02:26:46,625 : loading BERT model bert-base-uncased
2019-02-15 02:26:46,625 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:26:46,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:26:46,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmphxh181ws
2019-02-15 02:26:48,995 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:26:50,411 : Computing embedding for train
2019-02-15 02:26:58,695 : Computed train embeddings
2019-02-15 02:26:58,695 : Computing embedding for dev
2019-02-15 02:27:01,158 : Computed dev embeddings
2019-02-15 02:27:01,158 : Computing embedding for test
2019-02-15 02:27:03,130 : Computed test embeddings
2019-02-15 02:27:25,076 : Dev : Pearson 0.5052051329840743
2019-02-15 02:27:25,076 : Test : Pearson 0.4859946327378523 Spearman 0.48616831403702593 MSE 1.926173191683129                        for SICK Relatedness

2019-02-15 02:27:25,076 : ***** Transfer task : SNLI Entailment*****


2019-02-15 02:27:30,015 : loading BERT model bert-base-uncased
2019-02-15 02:27:30,015 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:27:30,147 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:27:30,147 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiir7ujx6
2019-02-15 02:27:32,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:27:34,118 : PROGRESS (encoding): 0.00%
2019-02-15 02:28:52,028 : PROGRESS (encoding): 14.56%
2019-02-15 02:30:18,116 : PROGRESS (encoding): 29.12%
2019-02-15 02:31:45,330 : PROGRESS (encoding): 43.69%
2019-02-15 02:33:19,012 : PROGRESS (encoding): 58.25%
2019-02-15 02:35:03,699 : PROGRESS (encoding): 72.81%
2019-02-15 02:36:47,347 : PROGRESS (encoding): 87.37%
2019-02-15 02:38:36,742 : PROGRESS (encoding): 0.00%
2019-02-15 02:38:50,232 : PROGRESS (encoding): 0.00%
2019-02-15 02:39:04,093 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 02:40:01,320 : [('reg:1e-09', 52.8)]
2019-02-15 02:40:01,321 : Validation : best param found is reg = 1e-09 with score             52.8
2019-02-15 02:40:01,321 : Evaluating...
2019-02-15 02:40:58,353 : Dev acc : 52.8 Test acc : 53.19 for SNLI

2019-02-15 02:40:58,353 : ***** Transfer task: Image Caption Retrieval *****


2019-02-15 02:41:07,163 : loading BERT model bert-base-uncased
2019-02-15 02:41:07,163 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 02:41:07,212 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 02:41:07,212 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpscqw5xxx
2019-02-15 02:41:09,559 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 02:41:10,978 : Computing embedding for train
2019-02-15 02:48:40,625 : Computed train embeddings
2019-02-15 02:48:40,625 : Computing embedding for dev
2019-02-15 02:48:59,961 : Computed dev embeddings
2019-02-15 02:48:59,961 : Computing embedding for test
2019-02-15 02:49:20,155 : Computed test embeddings
2019-02-15 02:49:20,170 : prepare data
2019-02-15 02:49:20,231 : start epoch
2019-02-15 02:49:59,134 : samples : 64000
2019-02-15 02:50:08,952 : Image to text: 4.84, 15.2, 24.52, 38.0
2019-02-15 02:50:16,017 : Text to Image: 3.572, 12.904, 20.984, 44.0
2019-02-15 02:50:54,858 : samples : 128000
2019-02-15 02:51:04,637 : Image to text: 6.0, 19.88, 29.76, 28.0
2019-02-15 02:51:11,649 : Text to Image: 4.852, 16.256, 25.808, 34.0
2019-02-15 02:51:50,490 : samples : 192000
2019-02-15 02:52:00,292 : Image to text: 5.96, 17.64, 26.88, 32.0
2019-02-15 02:52:07,301 : Text to Image: 4.204, 14.292, 22.872, 38.0
2019-02-15 02:52:46,147 : samples : 256000
2019-02-15 02:52:55,958 : Image to text: 6.14, 19.48, 29.18, 28.0
2019-02-15 02:53:03,073 : Text to Image: 4.672, 15.724, 25.208, 33.0
2019-02-15 02:53:44,336 : samples : 320000
2019-02-15 02:53:54,128 : Image to text: 6.66, 21.16, 32.56, 24.0
2019-02-15 02:54:01,162 : Text to Image: 5.836, 18.588, 28.344, 29.0
2019-02-15 02:54:43,011 : samples : 384000
2019-02-15 02:54:52,772 : Image to text: 6.46, 20.6, 31.54, 24.0
2019-02-15 02:54:59,779 : Text to Image: 5.244, 17.516, 27.2, 32.0
2019-02-15 02:55:40,420 : samples : 448000
2019-02-15 02:55:50,216 : Image to text: 6.86, 21.1, 31.76, 26.0
2019-02-15 02:55:57,238 : Text to Image: 5.82, 18.552, 28.348, 29.0
2019-02-15 02:56:37,162 : samples : 512000
2019-02-15 02:56:46,952 : Image to text: 7.06, 21.7, 33.18, 24.0
2019-02-15 02:56:53,957 : Text to Image: 5.624, 19.144, 29.064, 28.0
2019-02-15 02:57:32,746 : Epoch 1 finished
2019-02-15 02:57:33,179 : Image to text: 20.8, 54.2, 68.7, 5.0
2019-02-15 02:57:33,616 : Text to Image: 16.9, 46.62, 64.16, 6.0
2019-02-15 02:57:34,053 : Image to text: 19.8, 52.4, 69.0, 5.0
2019-02-15 02:57:34,486 : Text to Image: 16.5, 44.92, 62.78, 7.0
2019-02-15 02:57:34,923 : Image to text: 22.8, 51.0, 67.3, 5.0
2019-02-15 02:57:35,355 : Text to Image: 16.9, 45.74, 63.32, 7.0
2019-02-15 02:57:35,792 : Image to text: 23.6, 52.7, 67.8, 5.0
2019-02-15 02:57:36,225 : Text to Image: 17.24, 46.56, 63.1, 6.0
2019-02-15 02:57:36,661 : Image to text: 21.1, 50.9, 68.7, 5.0
2019-02-15 02:57:37,093 : Text to Image: 17.64, 47.24, 63.94, 6.0
2019-02-15 02:57:37,093 : Dev mean Text to Image: 17.035999999999998, 46.216, 63.459999999999994, 6.3999999999999995
2019-02-15 02:57:37,093 : Dev mean Image to text: 21.620000000000005, 52.24, 68.3, 5.0
2019-02-15 02:57:37,094 : start epoch
2019-02-15 02:58:16,288 : samples : 64000
2019-02-15 02:58:26,017 : Image to text: 7.9, 23.98, 35.76, 21.0
2019-02-15 02:58:33,027 : Text to Image: 6.548, 19.96, 30.652, 26.0
2019-02-15 02:59:14,410 : samples : 128000
2019-02-15 02:59:24,153 : Image to text: 6.86, 21.68, 33.3, 23.0
2019-02-15 02:59:31,190 : Text to Image: 6.28, 19.556, 29.92, 27.0
2019-02-15 03:00:13,762 : samples : 192000
2019-02-15 03:00:23,576 : Image to text: 7.12, 22.28, 33.74, 22.0
2019-02-15 03:00:30,596 : Text to Image: 6.216, 19.996, 30.948, 25.0
2019-02-15 03:01:16,796 : samples : 256000
2019-02-15 03:01:26,603 : Image to text: 8.74, 24.68, 37.04, 19.0
2019-02-15 03:01:33,689 : Text to Image: 7.2, 21.756, 32.704, 24.0
2019-02-15 03:02:15,228 : samples : 320000
2019-02-15 03:02:24,990 : Image to text: 8.14, 24.46, 36.96, 20.0
2019-02-15 03:02:32,005 : Text to Image: 6.952, 20.548, 31.052, 25.0
2019-02-15 03:03:12,526 : samples : 384000
2019-02-15 03:03:22,273 : Image to text: 7.98, 25.46, 35.96, 21.0
2019-02-15 03:03:29,305 : Text to Image: 6.736, 21.368, 32.348, 23.0
2019-02-15 03:04:09,516 : samples : 448000
2019-02-15 03:04:19,257 : Image to text: 8.1, 24.06, 35.32, 21.0
2019-02-15 03:04:26,255 : Text to Image: 6.16, 19.884, 30.744, 25.0
2019-02-15 03:05:09,519 : samples : 512000
2019-02-15 03:05:19,326 : Image to text: 8.24, 24.52, 37.0, 20.0
2019-02-15 03:05:26,318 : Text to Image: 7.112, 21.556, 32.216, 24.0
2019-02-15 03:06:06,466 : Epoch 2 finished
2019-02-15 03:06:06,912 : Image to text: 23.8, 55.9, 70.0, 4.0
2019-02-15 03:06:07,298 : Text to Image: 19.06, 50.54, 67.76, 5.0
2019-02-15 03:06:07,737 : Image to text: 21.9, 53.1, 69.6, 5.0
2019-02-15 03:06:08,123 : Text to Image: 18.46, 48.94, 67.18, 6.0
2019-02-15 03:06:08,561 : Image to text: 23.7, 55.7, 71.2, 4.0
2019-02-15 03:06:08,947 : Text to Image: 18.8, 48.9, 66.2, 6.0
2019-02-15 03:06:09,385 : Image to text: 24.9, 57.7, 71.8, 4.0
2019-02-15 03:06:09,772 : Text to Image: 19.66, 49.34, 65.8, 6.0
2019-02-15 03:06:10,211 : Image to text: 23.9, 55.9, 72.7, 4.0
2019-02-15 03:06:10,597 : Text to Image: 18.88, 49.54, 66.58, 6.0
2019-02-15 03:06:10,597 : Dev mean Text to Image: 18.972, 49.452000000000005, 66.70400000000001, 5.800000000000001
2019-02-15 03:06:10,597 : Dev mean Image to text: 23.64, 55.66, 71.06, 4.2
2019-02-15 03:06:10,598 : start epoch
2019-02-15 03:06:49,593 : samples : 64000
2019-02-15 03:06:59,358 : Image to text: 8.68, 25.0, 36.46, 19.0
2019-02-15 03:07:06,391 : Text to Image: 7.224, 21.82, 32.576, 23.0
2019-02-15 03:07:45,394 : samples : 128000
2019-02-15 03:07:55,162 : Image to text: 7.38, 23.04, 35.2, 21.0
2019-02-15 03:08:02,213 : Text to Image: 6.992, 21.512, 32.508, 24.0
2019-02-15 03:08:43,893 : samples : 192000
2019-02-15 03:08:53,686 : Image to text: 8.14, 25.1, 36.58, 20.0
2019-02-15 03:09:00,730 : Text to Image: 7.284, 22.024, 32.796, 23.0
2019-02-15 03:09:42,637 : samples : 256000
2019-02-15 03:09:52,448 : Image to text: 8.96, 25.38, 37.6, 18.0
2019-02-15 03:09:59,458 : Text to Image: 7.02, 21.344, 32.024, 24.0
2019-02-15 03:10:41,689 : samples : 320000
2019-02-15 03:10:51,460 : Image to text: 8.96, 26.16, 38.3, 18.0
2019-02-15 03:10:58,473 : Text to Image: 7.308, 21.984, 33.296, 23.0
2019-02-15 03:11:40,723 : samples : 384000
2019-02-15 03:11:50,502 : Image to text: 9.14, 26.12, 37.96, 19.0
2019-02-15 03:11:57,514 : Text to Image: 7.116, 21.7, 32.572, 23.0
2019-02-15 03:12:39,834 : samples : 448000
2019-02-15 03:12:49,658 : Image to text: 9.22, 26.16, 37.96, 19.0
2019-02-15 03:12:56,687 : Text to Image: 7.696, 22.848, 34.444, 21.0
2019-02-15 03:13:38,934 : samples : 512000
2019-02-15 03:13:48,726 : Image to text: 9.08, 25.56, 37.86, 19.0
2019-02-15 03:13:55,752 : Text to Image: 7.24, 22.34, 33.62, 22.0
2019-02-15 03:14:31,736 : Epoch 3 finished
2019-02-15 03:14:32,153 : Image to text: 25.6, 54.5, 69.0, 5.0
2019-02-15 03:14:32,539 : Text to Image: 19.26, 49.56, 67.88, 6.0
2019-02-15 03:14:32,979 : Image to text: 21.7, 53.0, 69.1, 5.0
2019-02-15 03:14:33,384 : Text to Image: 17.92, 50.38, 67.48, 5.0
2019-02-15 03:14:33,823 : Image to text: 22.9, 54.7, 71.2, 4.0
2019-02-15 03:14:34,228 : Text to Image: 18.02, 49.46, 66.54, 6.0
2019-02-15 03:14:34,667 : Image to text: 23.1, 54.5, 71.8, 4.0
2019-02-15 03:14:35,071 : Text to Image: 19.08, 49.64, 66.5, 6.0
2019-02-15 03:14:35,509 : Image to text: 24.4, 57.0, 71.8, 4.0
2019-02-15 03:14:35,913 : Text to Image: 19.22, 50.68, 68.44, 5.0
2019-02-15 03:14:35,913 : Dev mean Text to Image: 18.700000000000003, 49.944, 67.36800000000001, 5.6000000000000005
2019-02-15 03:14:35,913 : Dev mean Image to text: 23.54, 54.739999999999995, 70.58, 4.3999999999999995
2019-02-15 03:14:35,913 : start epoch
2019-02-15 03:15:17,772 : samples : 64000
2019-02-15 03:15:27,543 : Image to text: 9.62, 26.86, 39.08, 18.0
2019-02-15 03:15:34,677 : Text to Image: 7.544, 22.728, 34.0, 22.0
2019-02-15 03:16:16,969 : samples : 128000
2019-02-15 03:16:26,764 : Image to text: 9.32, 26.58, 38.54, 18.0
2019-02-15 03:16:33,803 : Text to Image: 7.544, 22.98, 34.012, 22.0
2019-02-15 03:17:16,002 : samples : 192000
2019-02-15 03:17:25,754 : Image to text: 9.04, 26.22, 38.96, 18.0
2019-02-15 03:17:32,790 : Text to Image: 7.112, 21.796, 32.9, 23.0
2019-02-15 03:18:14,985 : samples : 256000
2019-02-15 03:18:24,760 : Image to text: 9.26, 27.1, 39.14, 18.0
2019-02-15 03:18:31,837 : Text to Image: 7.932, 23.596, 35.148, 21.0
2019-02-15 03:19:13,667 : samples : 320000
2019-02-15 03:19:23,452 : Image to text: 8.96, 26.92, 38.96, 17.0
2019-02-15 03:19:30,480 : Text to Image: 7.636, 23.228, 34.432, 21.0
2019-02-15 03:20:12,491 : samples : 384000
2019-02-15 03:20:22,251 : Image to text: 9.38, 26.64, 38.62, 17.0
2019-02-15 03:20:29,262 : Text to Image: 7.664, 22.756, 34.32, 22.0
2019-02-15 03:21:12,134 : samples : 448000
2019-02-15 03:21:22,040 : Image to text: 8.86, 25.72, 37.8, 18.0
2019-02-15 03:21:29,072 : Text to Image: 7.544, 22.84, 34.304, 22.0
2019-02-15 03:22:11,283 : samples : 512000
2019-02-15 03:22:21,115 : Image to text: 9.26, 27.66, 40.04, 17.0
2019-02-15 03:22:28,129 : Text to Image: 7.716, 23.232, 34.284, 22.0
2019-02-15 03:23:01,582 : Epoch 4 finished
2019-02-15 03:23:01,990 : Image to text: 24.7, 56.9, 71.3, 4.0
2019-02-15 03:23:02,342 : Text to Image: 19.74, 52.0, 70.08, 5.0
2019-02-15 03:23:02,783 : Image to text: 24.9, 56.7, 71.0, 4.0
2019-02-15 03:23:03,160 : Text to Image: 19.72, 51.04, 69.2, 5.0
2019-02-15 03:23:03,601 : Image to text: 24.9, 56.5, 73.3, 4.0
2019-02-15 03:23:03,979 : Text to Image: 20.28, 51.16, 68.7, 5.0
2019-02-15 03:23:04,420 : Image to text: 25.9, 58.0, 72.9, 4.0
2019-02-15 03:23:04,798 : Text to Image: 19.72, 50.74, 68.82, 5.0
2019-02-15 03:23:05,238 : Image to text: 24.2, 60.5, 73.4, 4.0
2019-02-15 03:23:05,615 : Text to Image: 20.14, 51.12, 68.08, 5.0
2019-02-15 03:23:05,615 : Dev mean Text to Image: 19.92, 51.212, 68.976, 5.0
2019-02-15 03:23:05,615 : Dev mean Image to text: 24.919999999999998, 57.72, 72.38000000000001, 4.0
2019-02-15 03:23:05,615 : start epoch
2019-02-15 03:23:45,161 : samples : 64000
2019-02-15 03:23:54,928 : Image to text: 9.5, 27.6, 40.34, 18.0
2019-02-15 03:24:01,950 : Text to Image: 7.94, 23.816, 35.552, 21.0
2019-02-15 03:24:41,824 : samples : 128000
2019-02-15 03:24:51,665 : Image to text: 9.66, 28.46, 40.18, 17.0
2019-02-15 03:24:58,672 : Text to Image: 7.892, 23.952, 35.208, 21.0
2019-02-15 03:25:38,236 : samples : 192000
2019-02-15 03:25:48,045 : Image to text: 8.48, 25.94, 37.6, 18.0
2019-02-15 03:25:55,121 : Text to Image: 7.62, 22.94, 34.676, 21.0
2019-02-15 03:26:35,317 : samples : 256000
2019-02-15 03:26:45,099 : Image to text: 9.36, 26.06, 38.66, 18.0
2019-02-15 03:26:52,137 : Text to Image: 7.58, 22.952, 33.984, 22.0
2019-02-15 03:27:33,310 : samples : 320000
2019-02-15 03:27:43,067 : Image to text: 9.38, 27.18, 38.94, 17.0
2019-02-15 03:27:50,134 : Text to Image: 7.6, 23.18, 34.652, 21.0
2019-02-15 03:28:31,456 : samples : 384000
2019-02-15 03:28:41,209 : Image to text: 8.32, 25.7, 37.9, 18.0
2019-02-15 03:28:48,231 : Text to Image: 7.884, 23.764, 35.076, 21.0
2019-02-15 03:29:27,335 : samples : 448000
2019-02-15 03:29:37,136 : Image to text: 9.0, 27.62, 39.58, 17.0
2019-02-15 03:29:44,148 : Text to Image: 7.664, 23.32, 35.212, 21.0
2019-02-15 03:30:25,510 : samples : 512000
2019-02-15 03:30:35,288 : Image to text: 9.02, 25.88, 38.08, 18.0
2019-02-15 03:30:42,296 : Text to Image: 7.784, 23.188, 34.592, 21.0
2019-02-15 03:31:16,981 : Epoch 5 finished
2019-02-15 03:31:17,410 : Image to text: 25.5, 58.5, 74.2, 4.0
2019-02-15 03:31:17,807 : Text to Image: 21.2, 53.7, 70.88, 5.0
2019-02-15 03:31:18,247 : Image to text: 23.5, 56.5, 72.6, 4.0
2019-02-15 03:31:18,644 : Text to Image: 20.92, 52.78, 70.34, 5.0
2019-02-15 03:31:19,084 : Image to text: 25.8, 59.2, 72.6, 4.0
2019-02-15 03:31:19,481 : Text to Image: 20.6, 53.38, 70.0, 5.0
2019-02-15 03:31:19,921 : Image to text: 29.6, 58.3, 74.0, 4.0
2019-02-15 03:31:20,319 : Text to Image: 21.5, 54.28, 71.32, 5.0
2019-02-15 03:31:20,759 : Image to text: 26.4, 60.6, 74.8, 4.0
2019-02-15 03:31:21,156 : Text to Image: 21.92, 54.3, 70.98, 5.0
2019-02-15 03:31:21,156 : Dev mean Text to Image: 21.228, 53.688, 70.704, 5.0
2019-02-15 03:31:21,156 : Dev mean Image to text: 26.160000000000004, 58.620000000000005, 73.63999999999999, 4.0
2019-02-15 03:31:21,157 : start epoch
2019-02-15 03:32:03,502 : samples : 64000
2019-02-15 03:32:13,263 : Image to text: 10.4, 28.56, 40.92, 16.0
2019-02-15 03:32:20,281 : Text to Image: 8.292, 24.044, 35.884, 20.0
2019-02-15 03:33:01,453 : samples : 128000
2019-02-15 03:33:11,209 : Image to text: 8.38, 26.02, 38.7, 18.0
2019-02-15 03:33:18,260 : Text to Image: 7.808, 23.292, 34.508, 21.0
2019-02-15 03:34:00,435 : samples : 192000
2019-02-15 03:34:10,230 : Image to text: 9.4, 27.5, 38.82, 17.0
2019-02-15 03:34:17,274 : Text to Image: 7.76, 23.58, 35.22, 21.0
2019-02-15 03:34:59,040 : samples : 256000
2019-02-15 03:35:08,848 : Image to text: 9.16, 26.9, 39.1, 17.0
2019-02-15 03:35:15,906 : Text to Image: 7.864, 23.308, 34.636, 21.0
2019-02-15 03:35:58,695 : samples : 320000
2019-02-15 03:36:08,511 : Image to text: 10.14, 27.78, 40.3, 17.0
2019-02-15 03:36:15,566 : Text to Image: 7.912, 24.064, 35.852, 20.0
2019-02-15 03:36:57,795 : samples : 384000
2019-02-15 03:37:07,584 : Image to text: 10.14, 28.44, 40.58, 16.0
2019-02-15 03:37:14,600 : Text to Image: 8.468, 25.096, 36.636, 20.0
2019-02-15 03:37:55,574 : samples : 448000
2019-02-15 03:38:05,263 : Image to text: 9.42, 27.08, 40.26, 16.0
2019-02-15 03:38:12,224 : Text to Image: 7.764, 23.6, 35.184, 21.0
2019-02-15 03:38:53,875 : samples : 512000
2019-02-15 03:39:03,585 : Image to text: 9.92, 28.34, 39.46, 17.0
2019-02-15 03:39:10,556 : Text to Image: 8.128, 24.66, 36.444, 19.0
2019-02-15 03:39:46,800 : Epoch 6 finished
2019-02-15 03:39:47,224 : Image to text: 25.7, 60.7, 75.0, 4.0
2019-02-15 03:39:47,545 : Text to Image: 21.94, 55.28, 71.9, 5.0
2019-02-15 03:39:47,975 : Image to text: 24.5, 56.5, 72.2, 4.0
2019-02-15 03:39:48,371 : Text to Image: 20.52, 53.7, 71.34, 5.0
2019-02-15 03:39:48,812 : Image to text: 27.6, 58.6, 72.5, 4.0
2019-02-15 03:39:49,208 : Text to Image: 21.24, 53.62, 70.94, 5.0
2019-02-15 03:39:49,648 : Image to text: 27.3, 59.2, 75.0, 4.0
2019-02-15 03:39:50,043 : Text to Image: 21.98, 53.72, 70.62, 5.0
2019-02-15 03:39:50,483 : Image to text: 25.4, 59.8, 74.9, 4.0
2019-02-15 03:39:50,879 : Text to Image: 21.66, 54.98, 70.54, 5.0
2019-02-15 03:39:50,879 : Dev mean Text to Image: 21.468, 54.25999999999999, 71.068, 5.0
2019-02-15 03:39:50,879 : Dev mean Image to text: 26.1, 58.96, 73.92, 4.0
2019-02-15 03:39:50,879 : start epoch
2019-02-15 03:40:30,360 : samples : 64000
2019-02-15 03:40:40,143 : Image to text: 10.14, 28.26, 40.02, 16.0
2019-02-15 03:40:47,216 : Text to Image: 8.56, 24.776, 36.24, 20.0
2019-02-15 03:41:26,155 : samples : 128000
2019-02-15 03:41:35,969 : Image to text: 10.08, 28.06, 40.24, 16.0
2019-02-15 03:41:43,000 : Text to Image: 8.336, 25.092, 36.804, 19.0
2019-02-15 03:42:28,170 : samples : 192000
2019-02-15 03:42:37,981 : Image to text: 10.18, 27.36, 39.76, 16.0
2019-02-15 03:42:45,020 : Text to Image: 8.188, 24.34, 35.84, 20.0
2019-02-15 03:43:23,998 : samples : 256000
2019-02-15 03:43:33,820 : Image to text: 9.8, 28.6, 40.56, 16.0
2019-02-15 03:43:40,849 : Text to Image: 8.428, 24.856, 36.636, 19.0
2019-02-15 03:44:20,874 : samples : 320000
2019-02-15 03:44:30,664 : Image to text: 9.7, 27.72, 40.42, 16.0
2019-02-15 03:44:37,682 : Text to Image: 8.476, 24.576, 36.264, 20.0
2019-02-15 03:45:17,122 : samples : 384000
2019-02-15 03:45:26,904 : Image to text: 9.6, 28.36, 40.4, 16.0
2019-02-15 03:45:33,956 : Text to Image: 8.568, 25.088, 36.912, 19.0
2019-02-15 03:46:12,927 : samples : 448000
2019-02-15 03:46:22,709 : Image to text: 10.2, 28.68, 40.8, 16.0
2019-02-15 03:46:29,755 : Text to Image: 8.604, 25.16, 37.048, 19.0
2019-02-15 03:47:08,639 : samples : 512000
2019-02-15 03:47:18,465 : Image to text: 10.4, 29.4, 42.02, 15.0
2019-02-15 03:47:25,470 : Text to Image: 9.072, 25.548, 37.124, 19.0
2019-02-15 03:48:02,463 : Epoch 7 finished
2019-02-15 03:48:02,878 : Image to text: 26.9, 59.7, 73.4, 4.0
2019-02-15 03:48:03,235 : Text to Image: 22.02, 55.6, 72.74, 5.0
2019-02-15 03:48:03,674 : Image to text: 25.5, 55.1, 71.1, 4.0
2019-02-15 03:48:04,059 : Text to Image: 21.84, 54.28, 72.52, 5.0
2019-02-15 03:48:04,499 : Image to text: 25.2, 57.5, 74.4, 4.0
2019-02-15 03:48:04,883 : Text to Image: 21.7, 53.98, 71.68, 5.0
2019-02-15 03:48:05,322 : Image to text: 27.8, 58.7, 74.0, 4.0
2019-02-15 03:48:05,707 : Text to Image: 22.12, 54.72, 71.72, 5.0
2019-02-15 03:48:06,146 : Image to text: 26.8, 59.0, 75.4, 4.0
2019-02-15 03:48:06,530 : Text to Image: 22.4, 54.32, 70.84, 5.0
2019-02-15 03:48:06,530 : Dev mean Text to Image: 22.016000000000002, 54.58, 71.9, 5.0
2019-02-15 03:48:06,531 : Dev mean Image to text: 26.439999999999998, 58.0, 73.66, 4.0
2019-02-15 03:48:06,531 : start epoch
2019-02-15 03:48:46,658 : samples : 64000
2019-02-15 03:48:56,446 : Image to text: 10.48, 28.36, 40.72, 16.0
2019-02-15 03:49:03,472 : Text to Image: 8.696, 25.352, 37.24, 19.0
2019-02-15 03:49:45,642 : samples : 128000
2019-02-15 03:49:55,402 : Image to text: 10.0, 29.3, 41.18, 16.0
2019-02-15 03:50:02,470 : Text to Image: 8.268, 24.464, 36.432, 20.0
2019-02-15 03:50:44,567 : samples : 192000
2019-02-15 03:50:54,375 : Image to text: 10.26, 28.64, 41.18, 16.0
2019-02-15 03:51:01,409 : Text to Image: 8.484, 25.012, 37.072, 19.0
2019-02-15 03:51:43,338 : samples : 256000
2019-02-15 03:51:53,104 : Image to text: 10.28, 27.64, 39.86, 16.0
2019-02-15 03:52:00,128 : Text to Image: 8.544, 25.056, 37.072, 19.0
2019-02-15 03:52:41,489 : samples : 320000
2019-02-15 03:52:51,245 : Image to text: 10.56, 30.28, 42.42, 15.0
2019-02-15 03:52:58,174 : Text to Image: 8.868, 25.704, 37.624, 19.0
2019-02-15 03:53:39,115 : samples : 384000
2019-02-15 03:53:48,890 : Image to text: 9.88, 28.8, 41.5, 16.0
2019-02-15 03:53:55,817 : Text to Image: 8.776, 25.348, 37.336, 19.0
2019-02-15 03:54:37,432 : samples : 448000
2019-02-15 03:54:47,122 : Image to text: 10.0, 29.34, 42.34, 15.0
2019-02-15 03:54:54,039 : Text to Image: 8.696, 25.708, 37.412, 19.0
2019-02-15 03:55:34,661 : samples : 512000
2019-02-15 03:55:44,373 : Image to text: 10.06, 28.3, 40.7, 16.0
2019-02-15 03:55:51,388 : Text to Image: 8.256, 24.948, 36.748, 19.0
2019-02-15 03:56:26,730 : Epoch 8 finished
2019-02-15 03:56:27,167 : Image to text: 26.9, 59.5, 75.1, 4.0
2019-02-15 03:56:27,599 : Text to Image: 21.96, 55.72, 72.72, 4.0
2019-02-15 03:56:28,034 : Image to text: 24.3, 60.2, 74.6, 4.0
2019-02-15 03:56:28,465 : Text to Image: 21.5, 55.14, 71.98, 5.0
2019-02-15 03:56:28,909 : Image to text: 27.2, 57.9, 73.1, 4.0
2019-02-15 03:56:29,345 : Text to Image: 22.24, 54.34, 72.58, 5.0
2019-02-15 03:56:29,780 : Image to text: 28.1, 58.2, 74.0, 4.0
2019-02-15 03:56:30,222 : Text to Image: 22.7, 53.94, 72.06, 5.0
2019-02-15 03:56:30,667 : Image to text: 27.3, 59.0, 74.0, 4.0
2019-02-15 03:56:30,975 : Text to Image: 22.96, 54.78, 71.04, 4.0
2019-02-15 03:56:30,975 : Dev mean Text to Image: 22.272, 54.784000000000006, 72.07600000000001, 4.6
2019-02-15 03:56:30,975 : Dev mean Image to text: 26.76, 58.96000000000001, 74.16, 4.0
2019-02-15 03:56:30,975 : start epoch
2019-02-15 03:57:12,775 : samples : 64000
2019-02-15 03:57:22,599 : Image to text: 10.22, 29.32, 41.88, 15.0
2019-02-15 03:57:29,533 : Text to Image: 8.824, 25.636, 37.48, 19.0
2019-02-15 03:58:08,519 : samples : 128000
2019-02-15 03:58:18,201 : Image to text: 9.94, 30.0, 41.98, 16.0
2019-02-15 03:58:25,133 : Text to Image: 8.144, 24.392, 36.264, 20.0
2019-02-15 03:59:04,061 : samples : 192000
2019-02-15 03:59:13,838 : Image to text: 10.48, 29.28, 41.94, 15.0
2019-02-15 03:59:20,901 : Text to Image: 8.492, 25.164, 36.876, 19.0
2019-02-15 03:59:59,727 : samples : 256000
2019-02-15 04:00:09,495 : Image to text: 10.44, 27.66, 40.24, 16.0
2019-02-15 04:00:16,499 : Text to Image: 8.232, 24.904, 36.764, 19.0
2019-02-15 04:00:55,421 : samples : 320000
2019-02-15 04:01:05,085 : Image to text: 10.12, 28.48, 40.58, 16.0
2019-02-15 04:01:12,056 : Text to Image: 8.568, 24.648, 36.656, 19.0
2019-02-15 04:01:50,996 : samples : 384000
2019-02-15 04:02:00,737 : Image to text: 9.78, 28.46, 40.94, 17.0
2019-02-15 04:02:07,661 : Text to Image: 8.224, 24.48, 36.3, 19.0
2019-02-15 04:02:46,554 : samples : 448000
2019-02-15 04:02:56,228 : Image to text: 10.24, 28.86, 41.56, 16.0
2019-02-15 04:03:03,259 : Text to Image: 8.44, 24.936, 36.708, 19.0
2019-02-15 04:03:42,141 : samples : 512000
2019-02-15 04:03:51,901 : Image to text: 10.12, 29.14, 42.16, 15.0
2019-02-15 04:03:58,870 : Text to Image: 8.764, 25.544, 37.64, 19.0
2019-02-15 04:04:32,098 : Epoch 9 finished
2019-02-15 04:04:32,534 : Image to text: 26.8, 59.5, 75.8, 4.0
2019-02-15 04:04:32,942 : Text to Image: 22.32, 56.5, 73.88, 4.0
2019-02-15 04:04:33,377 : Image to text: 25.0, 57.4, 73.9, 4.0
2019-02-15 04:04:33,785 : Text to Image: 22.4, 55.62, 72.86, 5.0
2019-02-15 04:04:34,220 : Image to text: 29.6, 60.6, 73.9, 4.0
2019-02-15 04:04:34,628 : Text to Image: 22.4, 56.06, 72.82, 4.0
2019-02-15 04:04:35,063 : Image to text: 29.5, 60.2, 76.4, 4.0
2019-02-15 04:04:35,470 : Text to Image: 23.02, 55.42, 72.24, 4.0
2019-02-15 04:04:35,907 : Image to text: 26.8, 61.1, 76.1, 4.0
2019-02-15 04:04:36,314 : Text to Image: 23.88, 56.42, 72.48, 4.0
2019-02-15 04:04:36,314 : Dev mean Text to Image: 22.804, 56.004, 72.856, 4.2
2019-02-15 04:04:36,314 : Dev mean Image to text: 27.54, 59.76, 75.22, 4.0
2019-02-15 04:04:36,314 : start epoch
2019-02-15 04:05:17,660 : samples : 64000
2019-02-15 04:05:30,182 : Image to text: 10.5, 28.76, 41.68, 16.0
2019-02-15 04:05:40,141 : Text to Image: 8.32, 24.4, 36.576, 20.0
2019-02-15 04:06:22,797 : samples : 128000
2019-02-15 04:06:32,787 : Image to text: 10.52, 30.22, 42.66, 15.0
2019-02-15 04:06:39,956 : Text to Image: 9.288, 26.352, 38.228, 18.0
2019-02-15 04:07:20,542 : samples : 192000
2019-02-15 04:07:32,986 : Image to text: 10.84, 30.48, 43.5, 15.0
2019-02-15 04:07:42,934 : Text to Image: 9.148, 26.116, 37.9, 18.0
2019-02-15 04:08:25,933 : samples : 256000
2019-02-15 04:08:36,887 : Image to text: 10.14, 28.46, 40.72, 16.0
2019-02-15 04:08:43,926 : Text to Image: 8.2, 24.652, 36.392, 20.0
2019-02-15 04:09:23,779 : samples : 320000
2019-02-15 04:09:36,278 : Image to text: 9.92, 29.06, 41.64, 15.0
2019-02-15 04:09:46,247 : Text to Image: 8.612, 25.492, 37.616, 18.0
2019-02-15 04:10:29,606 : samples : 384000
2019-02-15 04:10:42,140 : Image to text: 10.24, 29.8, 42.14, 15.0
2019-02-15 04:10:49,938 : Text to Image: 8.492, 25.292, 37.044, 19.0
2019-02-15 04:11:30,819 : samples : 448000
2019-02-15 04:11:43,256 : Image to text: 10.8, 29.58, 41.88, 15.0
2019-02-15 04:11:53,159 : Text to Image: 8.972, 25.796, 37.82, 18.0
2019-02-15 04:12:36,758 : samples : 512000
2019-02-15 04:12:47,226 : Image to text: 10.5, 30.1, 42.58, 15.0
2019-02-15 04:12:54,393 : Text to Image: 8.768, 25.372, 37.444, 18.0
2019-02-15 04:13:29,793 : Epoch 10 finished
2019-02-15 04:13:30,704 : Image to text: 26.4, 62.5, 77.6, 4.0
2019-02-15 04:13:31,449 : Text to Image: 22.86, 57.6, 74.14, 4.0
2019-02-15 04:13:32,426 : Image to text: 24.5, 56.4, 73.1, 4.0
2019-02-15 04:13:33,193 : Text to Image: 22.52, 55.24, 72.22, 5.0
2019-02-15 04:13:34,088 : Image to text: 28.0, 60.5, 73.1, 3.0
2019-02-15 04:13:34,880 : Text to Image: 21.9, 55.4, 72.1, 5.0
2019-02-15 04:13:35,796 : Image to text: 28.7, 59.7, 75.1, 4.0
2019-02-15 04:13:36,528 : Text to Image: 22.54, 54.48, 72.22, 5.0
2019-02-15 04:13:37,449 : Image to text: 27.6, 60.3, 75.5, 4.0
2019-02-15 04:13:38,198 : Text to Image: 23.26, 55.52, 71.98, 4.0
2019-02-15 04:13:38,198 : Dev mean Text to Image: 22.616, 55.647999999999996, 72.532, 4.6
2019-02-15 04:13:38,199 : Dev mean Image to text: 27.04, 59.88000000000001, 74.88, 3.8
2019-02-15 04:13:38,199 : start epoch
2019-02-15 04:14:21,550 : samples : 64000
2019-02-15 04:14:34,064 : Image to text: 9.98, 29.12, 41.98, 15.0
2019-02-15 04:14:42,063 : Text to Image: 8.716, 25.48, 37.328, 19.0
2019-02-15 04:15:22,784 : samples : 128000
2019-02-15 04:15:35,252 : Image to text: 10.3, 29.48, 41.58, 15.0
2019-02-15 04:15:45,228 : Text to Image: 8.46, 25.508, 37.704, 19.0
2019-02-15 04:16:28,256 : samples : 192000
2019-02-15 04:16:40,753 : Image to text: 10.46, 29.56, 42.82, 15.0
2019-02-15 04:16:49,064 : Text to Image: 8.816, 25.664, 37.448, 19.0
2019-02-15 04:17:29,781 : samples : 256000
2019-02-15 04:17:42,268 : Image to text: 9.94, 30.68, 43.18, 15.0
2019-02-15 04:17:52,170 : Text to Image: 9.16, 26.1, 37.992, 18.0
2019-02-15 04:18:35,570 : samples : 320000
2019-02-15 04:18:48,085 : Image to text: 10.84, 29.42, 42.78, 15.0
2019-02-15 04:18:56,865 : Text to Image: 8.608, 25.704, 37.648, 18.0
2019-02-15 04:19:37,351 : samples : 384000
2019-02-15 04:19:49,800 : Image to text: 11.2, 31.18, 43.42, 14.0
2019-02-15 04:19:59,724 : Text to Image: 9.104, 26.276, 38.152, 18.0
2019-02-15 04:20:43,050 : samples : 448000
2019-02-15 04:20:55,595 : Image to text: 10.8, 29.64, 42.86, 15.0
2019-02-15 04:21:03,429 : Text to Image: 8.98, 26.156, 38.36, 18.0
2019-02-15 04:21:43,558 : samples : 512000
2019-02-15 04:21:55,506 : Image to text: 10.58, 29.34, 41.88, 15.0
2019-02-15 04:22:05,410 : Text to Image: 9.164, 26.104, 38.324, 18.0
2019-02-15 04:22:42,424 : Epoch 11 finished
2019-02-15 04:22:43,371 : Image to text: 27.4, 59.7, 76.3, 4.0
2019-02-15 04:22:44,145 : Text to Image: 22.7, 57.3, 73.78, 4.0
2019-02-15 04:22:45,098 : Image to text: 23.9, 58.5, 73.6, 4.0
2019-02-15 04:22:45,860 : Text to Image: 22.12, 55.46, 72.82, 5.0
2019-02-15 04:22:46,830 : Image to text: 29.3, 59.7, 73.5, 4.0
2019-02-15 04:22:47,582 : Text to Image: 22.32, 55.24, 72.52, 4.0
2019-02-15 04:22:48,551 : Image to text: 28.9, 60.1, 74.8, 4.0
2019-02-15 04:22:49,298 : Text to Image: 21.82, 55.84, 72.96, 4.0
2019-02-15 04:22:50,255 : Image to text: 28.3, 59.9, 75.8, 4.0
2019-02-15 04:22:51,052 : Text to Image: 23.44, 55.56, 72.98, 4.0
2019-02-15 04:22:51,052 : Dev mean Text to Image: 22.480000000000004, 55.88, 73.012, 4.2
2019-02-15 04:22:51,052 : Dev mean Image to text: 27.56, 59.58, 74.8, 4.0
2019-02-15 04:22:51,052 : start epoch
2019-02-15 04:23:33,114 : samples : 64000
2019-02-15 04:23:43,036 : Image to text: 10.64, 30.26, 42.44, 15.0
2019-02-15 04:23:50,211 : Text to Image: 8.92, 25.852, 37.996, 18.0
2019-02-15 04:24:30,995 : samples : 128000
2019-02-15 04:24:43,517 : Image to text: 10.16, 28.78, 41.7, 16.0
2019-02-15 04:24:53,454 : Text to Image: 8.592, 24.936, 37.1, 19.0
2019-02-15 04:25:36,341 : samples : 192000
2019-02-15 04:25:46,811 : Image to text: 10.92, 30.16, 42.56, 15.0
2019-02-15 04:25:54,002 : Text to Image: 8.812, 25.776, 37.876, 18.0
2019-02-15 04:26:34,209 : samples : 256000
2019-02-15 04:26:45,483 : Image to text: 10.74, 30.14, 43.4, 14.0
2019-02-15 04:26:53,402 : Text to Image: 8.86, 26.088, 38.128, 18.0
2019-02-15 04:27:34,351 : samples : 320000
2019-02-15 04:27:44,315 : Image to text: 10.84, 29.22, 40.86, 17.0
2019-02-15 04:27:51,487 : Text to Image: 8.204, 24.564, 36.548, 19.0
2019-02-15 04:28:31,755 : samples : 384000
2019-02-15 04:28:41,486 : Image to text: 11.3, 30.74, 43.0, 15.0
2019-02-15 04:28:49,505 : Text to Image: 9.272, 26.532, 38.644, 18.0
2019-02-15 04:29:29,755 : samples : 448000
2019-02-15 04:29:39,733 : Image to text: 11.38, 30.82, 43.06, 15.0
2019-02-15 04:29:46,863 : Text to Image: 9.656, 26.664, 38.736, 18.0
2019-02-15 04:30:27,901 : samples : 512000
2019-02-15 04:30:37,834 : Image to text: 10.46, 28.8, 42.0, 16.0
2019-02-15 04:30:44,947 : Text to Image: 8.708, 25.548, 37.568, 19.0
2019-02-15 04:31:19,925 : Epoch 12 finished
2019-02-15 04:31:20,903 : Image to text: 29.1, 60.9, 76.2, 4.0
2019-02-15 04:31:21,695 : Text to Image: 22.98, 56.58, 73.84, 4.0
2019-02-15 04:31:22,705 : Image to text: 25.4, 58.2, 74.8, 4.0
2019-02-15 04:31:23,485 : Text to Image: 22.02, 56.14, 73.32, 4.0
2019-02-15 04:31:24,479 : Image to text: 28.2, 59.2, 74.3, 4.0
2019-02-15 04:31:25,268 : Text to Image: 22.92, 55.92, 72.9, 4.0
2019-02-15 04:31:26,249 : Image to text: 28.7, 60.6, 75.1, 3.0
2019-02-15 04:31:27,048 : Text to Image: 22.78, 56.1, 73.14, 4.0
2019-02-15 04:31:28,054 : Image to text: 25.7, 60.2, 75.8, 4.0
2019-02-15 04:31:28,866 : Text to Image: 22.62, 55.94, 72.22, 5.0
2019-02-15 04:31:28,867 : Dev mean Text to Image: 22.664, 56.135999999999996, 73.084, 4.2
2019-02-15 04:31:28,867 : Dev mean Image to text: 27.42, 59.82, 75.24, 3.8000000000000007
2019-02-15 04:31:28,867 : start epoch
2019-02-15 04:32:10,663 : samples : 64000
2019-02-15 04:32:20,562 : Image to text: 10.86, 30.26, 43.84, 14.0
2019-02-15 04:32:29,248 : Text to Image: 8.944, 25.544, 37.888, 18.0
2019-02-15 04:33:10,880 : samples : 128000
2019-02-15 04:33:21,183 : Image to text: 10.68, 29.52, 42.8, 15.0
2019-02-15 04:33:30,345 : Text to Image: 9.02, 26.056, 37.912, 18.0
2019-02-15 04:34:11,069 : samples : 192000
2019-02-15 04:34:23,300 : Image to text: 10.16, 29.38, 42.52, 15.0
2019-02-15 04:34:31,241 : Text to Image: 8.808, 25.548, 37.88, 18.0
2019-02-15 04:35:12,070 : samples : 256000
2019-02-15 04:35:24,497 : Image to text: 11.04, 30.42, 43.1, 15.0
2019-02-15 04:35:34,406 : Text to Image: 9.148, 26.5, 38.792, 18.0
2019-02-15 04:36:16,677 : samples : 320000
2019-02-15 04:36:29,115 : Image to text: 10.3, 29.72, 41.98, 15.0
2019-02-15 04:36:38,974 : Text to Image: 8.852, 26.4, 38.256, 18.0
2019-02-15 04:37:21,337 : samples : 384000
2019-02-15 04:37:33,780 : Image to text: 10.7, 30.12, 42.82, 15.0
2019-02-15 04:37:43,689 : Text to Image: 8.984, 26.004, 38.588, 18.0
2019-02-15 04:38:26,288 : samples : 448000
2019-02-15 04:38:38,751 : Image to text: 11.5, 31.52, 43.96, 14.0
2019-02-15 04:38:48,654 : Text to Image: 9.324, 26.584, 38.52, 18.0
2019-02-15 04:39:31,059 : samples : 512000
2019-02-15 04:39:43,519 : Image to text: 10.82, 29.78, 42.64, 15.0
2019-02-15 04:39:53,468 : Text to Image: 8.984, 25.872, 38.264, 18.0
2019-02-15 04:40:29,001 : Epoch 13 finished
2019-02-15 04:40:29,887 : Image to text: 27.3, 58.8, 75.6, 4.0
2019-02-15 04:40:30,620 : Text to Image: 22.76, 56.7, 73.28, 4.0
2019-02-15 04:40:31,568 : Image to text: 25.0, 56.9, 72.8, 4.0
2019-02-15 04:40:32,332 : Text to Image: 22.5, 55.52, 72.8, 5.0
2019-02-15 04:40:33,265 : Image to text: 27.9, 59.4, 74.1, 4.0
2019-02-15 04:40:34,000 : Text to Image: 21.98, 55.76, 72.44, 4.0
2019-02-15 04:40:34,908 : Image to text: 26.2, 58.9, 75.9, 4.0
2019-02-15 04:40:35,693 : Text to Image: 22.82, 55.46, 73.1, 4.0
2019-02-15 04:40:36,598 : Image to text: 28.8, 60.4, 76.0, 4.0
2019-02-15 04:40:37,366 : Text to Image: 22.72, 56.34, 72.4, 4.0
2019-02-15 04:40:37,366 : Dev mean Text to Image: 22.556, 55.956, 72.804, 4.2
2019-02-15 04:40:37,366 : Dev mean Image to text: 27.04, 58.879999999999995, 74.88, 4.0
2019-02-15 04:40:45,692 : 
Test scores | Image to text:             26.9, 60.9, 75.38000000000001, 3.6
2019-02-15 04:40:45,693 : Test scores | Text to image:             22.644, 55.651999999999994, 72.988, 4.4

2019-02-15 04:40:45,797 : ***** (Probing) Transfer task : LENGTH classification *****
2019-02-15 04:40:46,002 : Loaded 99996 train - 9996 dev - 9996 test for Length
2019-02-15 04:40:46,619 : loading BERT model bert-base-uncased
2019-02-15 04:40:46,620 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:40:46,648 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:40:46,648 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp9i345svs
2019-02-15 04:40:49,007 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:40:50,485 : Computing embeddings for train/dev/test
2019-02-15 04:42:58,202 : Computed embeddings
2019-02-15 04:42:58,202 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:44:04,108 : [('reg:1e-05', 55.96), ('reg:0.0001', 53.74), ('reg:0.001', 45.89), ('reg:0.01', 40.7)]
2019-02-15 04:44:04,108 : Validation : best param found is reg = 1e-05 with score             55.96
2019-02-15 04:44:04,108 : Evaluating...
2019-02-15 04:44:23,223 : 
Dev acc : 56.0 Test acc : 55.8 for LENGTH classification

2019-02-15 04:44:23,223 : ***** (Probing) Transfer task : WORDCONTENT classification *****
2019-02-15 04:44:23,569 : Loaded 100000 train - 10000 dev - 10000 test for WordContent
2019-02-15 04:44:23,615 : loading BERT model bert-base-uncased
2019-02-15 04:44:23,616 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:44:23,643 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:44:23,643 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpzirdmfi7
2019-02-15 04:44:25,989 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:44:27,430 : Computing embeddings for train/dev/test
2019-02-15 04:46:34,407 : Computed embeddings
2019-02-15 04:46:34,407 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:47:39,763 : [('reg:1e-05', 10.72), ('reg:0.0001', 2.7), ('reg:0.001', 0.46), ('reg:0.01', 0.22)]
2019-02-15 04:47:39,763 : Validation : best param found is reg = 1e-05 with score             10.72
2019-02-15 04:47:39,763 : Evaluating...
2019-02-15 04:47:59,047 : 
Dev acc : 10.7 Test acc : 11.1 for WORDCONTENT classification

2019-02-15 04:47:59,048 : ***** (Probing) Transfer task : DEPTH classification *****
2019-02-15 04:47:59,392 : Loaded 100000 train - 10000 dev - 10000 test for Depth
2019-02-15 04:47:59,457 : loading BERT model bert-base-uncased
2019-02-15 04:47:59,457 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:47:59,549 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:47:59,549 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmplur8q74b
2019-02-15 04:48:01,947 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:48:03,392 : Computing embeddings for train/dev/test
2019-02-15 04:49:45,215 : Computed embeddings
2019-02-15 04:49:45,216 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:50:13,106 : [('reg:1e-05', 25.64), ('reg:0.0001', 23.48), ('reg:0.001', 22.56), ('reg:0.01', 20.97)]
2019-02-15 04:50:13,106 : Validation : best param found is reg = 1e-05 with score             25.64
2019-02-15 04:50:13,106 : Evaluating...
2019-02-15 04:50:19,495 : 
Dev acc : 25.6 Test acc : 25.8 for DEPTH classification

2019-02-15 04:50:19,496 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****
2019-02-15 04:50:19,872 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents
2019-02-15 04:50:19,936 : loading BERT model bert-base-uncased
2019-02-15 04:50:19,937 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:50:20,051 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:50:20,051 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpiwx_equ0
2019-02-15 04:50:22,515 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:50:23,967 : Computing embeddings for train/dev/test
2019-02-15 04:51:53,711 : Computed embeddings
2019-02-15 04:51:53,711 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:53:14,746 : [('reg:1e-05', 53.62), ('reg:0.0001', 52.1), ('reg:0.001', 46.33), ('reg:0.01', 26.82)]
2019-02-15 04:53:14,747 : Validation : best param found is reg = 1e-05 with score             53.62
2019-02-15 04:53:14,747 : Evaluating...
2019-02-15 04:53:36,117 : 
Dev acc : 53.6 Test acc : 54.1 for TOPCONSTITUENTS classification

2019-02-15 04:53:36,118 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****
2019-02-15 04:53:36,625 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift
2019-02-15 04:53:36,691 : loading BERT model bert-base-uncased
2019-02-15 04:53:36,691 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:53:36,721 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:53:36,721 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmploi1g79g
2019-02-15 04:53:39,067 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:53:40,546 : Computing embeddings for train/dev/test
2019-02-15 04:55:57,787 : Computed embeddings
2019-02-15 04:55:57,787 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 04:57:11,610 : [('reg:1e-05', 85.08), ('reg:0.0001', 84.96), ('reg:0.001', 85.73), ('reg:0.01', 81.55)]
2019-02-15 04:57:11,610 : Validation : best param found is reg = 0.001 with score             85.73
2019-02-15 04:57:11,610 : Evaluating...
2019-02-15 04:57:30,950 : 
Dev acc : 85.7 Test acc : 85.0 for BIGRAMSHIFT classification

2019-02-15 04:57:30,951 : ***** (Probing) Transfer task : TENSE classification *****
2019-02-15 04:57:31,341 : Loaded 100000 train - 10000 dev - 10000 test for Tense
2019-02-15 04:57:31,405 : loading BERT model bert-base-uncased
2019-02-15 04:57:31,405 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 04:57:31,435 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 04:57:31,435 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpxbseepyd
2019-02-15 04:57:33,784 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 04:57:35,284 : Computing embeddings for train/dev/test
2019-02-15 04:59:52,831 : Computed embeddings
2019-02-15 04:59:52,831 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 05:00:49,763 : [('reg:1e-05', 89.37), ('reg:0.0001', 89.37), ('reg:0.001', 89.34), ('reg:0.01', 88.89)]
2019-02-15 05:00:49,763 : Validation : best param found is reg = 1e-05 with score             89.37
2019-02-15 05:00:49,763 : Evaluating...
2019-02-15 05:00:59,413 : 
Dev acc : 89.4 Test acc : 88.3 for TENSE classification

2019-02-15 05:00:59,415 : ***** (Probing) Transfer task : SUBJNUMBER classification *****
2019-02-15 05:00:59,823 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber
2019-02-15 05:00:59,885 : loading BERT model bert-base-uncased
2019-02-15 05:00:59,886 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 05:00:59,926 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 05:00:59,927 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp56fsz2dh
2019-02-15 05:01:02,329 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 05:01:03,811 : Computing embeddings for train/dev/test
2019-02-15 05:02:50,143 : Computed embeddings
2019-02-15 05:02:50,143 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 05:03:20,368 : [('reg:1e-05', 78.4), ('reg:0.0001', 78.53), ('reg:0.001', 77.64), ('reg:0.01', 72.24)]
2019-02-15 05:03:20,368 : Validation : best param found is reg = 0.0001 with score             78.53
2019-02-15 05:03:20,368 : Evaluating...
2019-02-15 05:03:27,926 : 
Dev acc : 78.5 Test acc : 77.7 for SUBJNUMBER classification

2019-02-15 05:03:27,927 : ***** (Probing) Transfer task : OBJNUMBER classification *****
2019-02-15 05:03:28,372 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber
2019-02-15 05:03:28,445 : loading BERT model bert-base-uncased
2019-02-15 05:03:28,445 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 05:03:28,578 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 05:03:28,578 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpsw4gf2ah
2019-02-15 05:03:31,016 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 05:03:32,402 : Computing embeddings for train/dev/test
2019-02-15 05:04:57,257 : Computed embeddings
2019-02-15 05:04:57,257 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 05:05:27,313 : [('reg:1e-05', 67.78), ('reg:0.0001', 67.33), ('reg:0.001', 63.76), ('reg:0.01', 58.52)]
2019-02-15 05:05:27,313 : Validation : best param found is reg = 1e-05 with score             67.78
2019-02-15 05:05:27,313 : Evaluating...
2019-02-15 05:05:34,928 : 
Dev acc : 67.8 Test acc : 67.6 for OBJNUMBER classification

2019-02-15 05:05:34,929 : ***** (Probing) Transfer task : ODDMANOUT classification *****
2019-02-15 05:05:35,312 : Loaded 100000 train - 10000 dev - 10000 test for OddManOut
2019-02-15 05:05:35,380 : loading BERT model bert-base-uncased
2019-02-15 05:05:35,380 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 05:05:35,500 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 05:05:35,500 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmp0zzi8wuf
2019-02-15 05:05:37,837 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 05:05:39,221 : Computing embeddings for train/dev/test
2019-02-15 05:07:16,466 : Computed embeddings
2019-02-15 05:07:16,466 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 05:07:42,909 : [('reg:1e-05', 60.73), ('reg:0.0001', 60.72), ('reg:0.001', 60.58), ('reg:0.01', 54.75)]
2019-02-15 05:07:42,909 : Validation : best param found is reg = 1e-05 with score             60.73
2019-02-15 05:07:42,909 : Evaluating...
2019-02-15 05:07:48,757 : 
Dev acc : 60.7 Test acc : 60.5 for ODDMANOUT classification

2019-02-15 05:07:48,758 : ***** (Probing) Transfer task : COORDINATIONINVERSION classification *****
2019-02-15 05:07:49,338 : Loaded 100002 train - 10002 dev - 10002 test for CoordinationInversion
2019-02-15 05:07:49,413 : loading BERT model bert-base-uncased
2019-02-15 05:07:49,413 : loading vocabulary file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased-vocab.txt
2019-02-15 05:07:49,443 : loading archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz
2019-02-15 05:07:49,443 : extracting archive file /home/renxuancheng/pytorch-pretrained-BERT/pretrained_bert/bert-base-uncased.tar.gz to temp dir /tmp/tmpq199jrna
2019-02-15 05:07:51,769 : Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-02-15 05:07:53,149 : Computing embeddings for train/dev/test
2019-02-15 05:09:28,894 : Computed embeddings
2019-02-15 05:09:28,894 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..
2019-02-15 05:09:57,338 : [('reg:1e-05', 64.21), ('reg:0.0001', 64.18), ('reg:0.001', 63.33), ('reg:0.01', 62.76)]
2019-02-15 05:09:57,339 : Validation : best param found is reg = 1e-05 with score             64.21
2019-02-15 05:09:57,339 : Evaluating...
2019-02-15 05:10:04,425 : 
Dev acc : 64.2 Test acc : 63.3 for COORDINATIONINVERSION classification

2019-02-15 05:10:04,427 : total results: {'STS12': {'MSRpar': {'pearson': (0.016326714822881307, 0.655301491132485), 'spearman': SpearmanrResult(correlation=0.06298355036682185, pvalue=0.08476017345231386), 'nsamples': 750}, 'MSRvid': {'pearson': (-0.010286582296628755, 0.7785207872981923), 'spearman': SpearmanrResult(correlation=0.030235583199504805, pvalue=0.40832578568553446), 'nsamples': 750}, 'SMTeuroparl': {'pearson': (0.16628662952523934, 0.00034650149576261703), 'spearman': SpearmanrResult(correlation=0.36811890863697594, pvalue=3.5431880263750445e-16), 'nsamples': 459}, 'surprise.OnWN': {'pearson': (0.14084853778715936, 0.00010875898315343231), 'spearman': SpearmanrResult(correlation=0.3792504225710364, pvalue=4.5688992135702756e-27), 'nsamples': 750}, 'surprise.SMTnews': {'pearson': (0.10477794644887634, 0.036426590484931236), 'spearman': SpearmanrResult(correlation=0.20489777485577373, pvalue=3.726347291183914e-05), 'nsamples': 399}, 'all': {'pearson': {'mean': 0.0835906492575055, 'wmean': 0.07345510499364397}, 'spearman': {'mean': 0.20909724792602255, 'wmean': 0.19468241902665637}}}, 'STS13': {'FNWN': {'pearson': (0.050521005716747266, 0.48995372779708624), 'spearman': SpearmanrResult(correlation=0.07200005882319421, pvalue=0.32484561513236676), 'nsamples': 189}, 'headlines': {'pearson': (0.08380882606066581, 0.021709616367506523), 'spearman': SpearmanrResult(correlation=0.5278508142473555, pvalue=4.9290058962112154e-55), 'nsamples': 750}, 'OnWN': {'pearson': (-0.024232573062824003, 0.5668051450618798), 'spearman': SpearmanrResult(correlation=0.14826410144054053, pvalue=0.0004261138294512884), 'nsamples': 561}, 'all': {'pearson': {'mean': 0.036699086238196364, 'wmean': 0.03920707742514689}, 'spearman': {'mean': 0.2493716581703634, 'wmean': 0.32844818847416235}}}, 'STS14': {'deft-forum': {'pearson': (0.009692945753186074, 0.8375320029134687), 'spearman': SpearmanrResult(correlation=0.1380034503071052, pvalue=0.0033527106080123653), 'nsamples': 450}, 'deft-news': {'pearson': (0.2955718537260127, 1.8375561546648654e-07), 'spearman': SpearmanrResult(correlation=0.47067111189312266, pvalue=6.0497230494992505e-18), 'nsamples': 300}, 'headlines': {'pearson': (0.19300443615591878, 9.985247318125186e-08), 'spearman': SpearmanrResult(correlation=0.5071381924284415, pvalue=2.927677398014155e-50), 'nsamples': 750}, 'images': {'pearson': (0.01633547007057031, 0.6551285873811177), 'spearman': SpearmanrResult(correlation=0.14501323017429402, pvalue=6.724933159812128e-05), 'nsamples': 750}, 'OnWN': {'pearson': (0.016293368115695803, 0.6559602102423197), 'spearman': SpearmanrResult(correlation=0.320891986295897, pvalue=2.0094778812770665e-19), 'nsamples': 750}, 'tweet-news': {'pearson': (0.19377774990415497, 8.856437168087983e-08), 'spearman': SpearmanrResult(correlation=0.43676535080987433, pvalue=2.768864682018284e-36), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.12077930395425644, 'wmean': 0.10869110663773132}, 'spearman': {'mean': 0.3364138869847891, 'wmean': 0.3361758549300038}}}, 'STS15': {'answers-forums': {'pearson': (0.0233303898158428, 0.6524623630285488), 'spearman': SpearmanrResult(correlation=0.03703822241973334, pvalue=0.4745515731352521), 'nsamples': 375}, 'answers-students': {'pearson': (0.06918499526410578, 0.058249105199783754), 'spearman': SpearmanrResult(correlation=0.1836249026318792, pvalue=4.116228040036837e-07), 'nsamples': 750}, 'belief': {'pearson': (0.12516653057487007, 0.015296056269633757), 'spearman': SpearmanrResult(correlation=0.22583383127205092, pvalue=1.0062304010830934e-05), 'nsamples': 375}, 'headlines': {'pearson': (0.18230275797190995, 4.997245601747722e-07), 'spearman': SpearmanrResult(correlation=0.5869398801513455, pvalue=1.2408939145577559e-70), 'nsamples': 750}, 'images': {'pearson': (0.05228502278487294, 0.15258086966238107), 'spearman': SpearmanrResult(correlation=0.10721818091100091, pvalue=0.003283481914521053), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.09045393928232032, 'wmean': 0.09450530905406127}, 'spearman': {'mean': 0.22813100347720194, 'wmean': 0.2523047476350294}}}, 'STS16': {'answer-answer': {'pearson': (0.1940433622910459, 0.0018907841576741933), 'spearman': SpearmanrResult(correlation=0.3038506949988019, pvalue=7.978508935754555e-07), 'nsamples': 254}, 'headlines': {'pearson': (0.3224741852845401, 1.965180679882007e-07), 'spearman': SpearmanrResult(correlation=0.6261252133368136, pvalue=1.6406897949195284e-28), 'nsamples': 249}, 'plagiarism': {'pearson': (0.2517515958915923, 0.00011359761116536501), 'spearman': SpearmanrResult(correlation=0.3171586970741023, pvalue=9.055573182997195e-07), 'nsamples': 230}, 'postediting': {'pearson': (0.3867313899012794, 3.975513783357172e-10), 'spearman': SpearmanrResult(correlation=0.4922654287938698, pvalue=2.687856341907611e-16), 'nsamples': 244}, 'question-question': {'pearson': (0.18091987155459538, 0.008754096322729667), 'spearman': SpearmanrResult(correlation=0.47210251191786295, pvalue=5.34438358066745e-13), 'nsamples': 209}, 'all': {'pearson': {'mean': 0.2671840809846106, 'wmean': 0.2695283857535118}, 'spearman': {'mean': 0.4423005092242901, 'wmean': 0.44250585547566895}}}, 'MR': {'devacc': 75.81, 'acc': 73.1, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 81.61, 'acc': 79.68, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 83.32, 'acc': 83.95, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 94.24, 'acc': 94.1, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 82.91, 'acc': 81.77, 'ndev': 872, 'ntest': 1821}, 'SST5': {'devacc': 41.69, 'acc': 42.17, 'ndev': 1101, 'ntest': 2210}, 'TREC': {'devacc': 63.81, 'acc': 83.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 69.48, 'acc': 66.49, 'f1': 79.87, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 76.8, 'acc': 74.41, 'ndev': 500, 'ntest': 4927}, 'SICKRelatedness': {'devpearson': 0.7558401559974782, 'pearson': 0.7636444241811735, 'spearman': 0.6978110340036805, 'mse': 0.42466150021616883, 'yhat': array([3.27560724, 4.03596539, 1.81959416, ..., 3.19238416, 4.3697911 ,        4.93485964]), 'ndev': 500, 'ntest': 4927}, 'STSBenchmark': {'devpearson': 0.5052051329840743, 'pearson': 0.4859946327378523, 'spearman': 0.48616831403702593, 'mse': 1.926173191683129, 'yhat': array([1.72703783, 1.48180866, 2.84115513, ..., 3.96642826, 3.46172743,        2.94848187]), 'ndev': 1500, 'ntest': 1379}, 'SNLI': {'devacc': 52.8, 'acc': 53.19, 'ndev': 9842, 'ntest': 9824}, 'ImageCaptionRetrieval': {'devacc': 314.364, 'acc': [(26.9, 60.9, 75.38000000000001, 3.6), (22.644, 55.651999999999994, 72.988, 4.4)], 'ndev': 25000, 'ntest': 25000}, 'Length': {'devacc': 55.96, 'acc': 55.8, 'ndev': 9996, 'ntest': 9996}, 'WordContent': {'devacc': 10.72, 'acc': 11.07, 'ndev': 10000, 'ntest': 10000}, 'Depth': {'devacc': 25.64, 'acc': 25.8, 'ndev': 10000, 'ntest': 10000}, 'TopConstituents': {'devacc': 53.62, 'acc': 54.08, 'ndev': 10000, 'ntest': 10000}, 'BigramShift': {'devacc': 85.73, 'acc': 85.05, 'ndev': 10000, 'ntest': 10000}, 'Tense': {'devacc': 89.37, 'acc': 88.33, 'ndev': 10000, 'ntest': 10000}, 'SubjNumber': {'devacc': 78.53, 'acc': 77.7, 'ndev': 10000, 'ntest': 10000}, 'ObjNumber': {'devacc': 67.78, 'acc': 67.6, 'ndev': 10000, 'ntest': 10000}, 'OddManOut': {'devacc': 60.73, 'acc': 60.47, 'ndev': 10000, 'ntest': 10000}, 'CoordinationInversion': {'devacc': 64.21, 'acc': 63.27, 'ndev': 10002, 'ntest': 10002}}
2019-02-15 05:10:04,427 : STS12 p=0.0735, STS12 s=0.1947, STS13 p=0.0392, STS13 s=0.3284, STS14 p=0.1087, STS14 s=0.3362, STS15 p=0.0945, STS15 s=0.2523, STS 16 p=0.2695, STS16 s=0.4425, STS B p=0.4860, STS B s=0.4862, STS B m=1.9262, SICK-R p=0.7636, SICK-R s=0.6978, SICK-P m=0.4247
2019-02-15 05:10:04,427 : STS12 p, STS12 s, STS13 p, STS13 s, STS14 p, STS14 s, STS15 p, STS15 s, STS 16 p, STS16 s, STS B p, STS B s, STS B m, SICK-R p, SICK-R s, SICK-P m
2019-02-15 05:10:04,427 : 0.0735,0.1947,0.0392,0.3284,0.1087,0.3362,0.0945,0.2523,0.2695,0.4425,0.4860,0.4862,1.9262,0.7636,0.6978,0.4247
2019-02-15 05:10:04,427 : MR=73.10, CR=79.68, SUBJ=94.10, MPQA=83.95, SST-B=81.77, SST-F=42.17, TREC=83.20, SICK-E=74.41, SNLI=53.19, MRPC=66.49, MRPC f=79.87
2019-02-15 05:10:04,428 : MR, CR, SUBJ, MPQA, SST-B, SST-F, TREC, SICK-E, SNLI, MRPC, MRPC f
2019-02-15 05:10:04,428 : 73.10,79.68,94.10,83.95,81.77,42.17,83.20,74.41,53.19,66.49,79.87
2019-02-15 05:10:04,428 : COCO r1i2t=26.90, COCO r5i2t=60.90, COCO r10i2t=75.38, COCO medr_i2t=3.60, COCO r1t2i=22.64, COCO r5t2i=55.65, COCO r10t2i=72.99, COCO medr_t2i=4.40
2019-02-15 05:10:04,428 : COCO r1i2t, COCO r5i2t, COCO r10i2t, COCO medr_i2t, COCO r1t2i, COCO r5t2i, COCO r10t2i, COCO medr_t2i
2019-02-15 05:10:04,428 : 26.90,60.90,75.38,3.60,22.64,55.65,72.99,4.40
2019-02-15 05:10:04,428 : SentLen=55.80, WC=11.07, TreeDepth=25.80, TopConst=54.08, BShift=85.05, Tense=88.33, SubjNum=77.70, ObjNum=67.60, SOMO=60.47, CoordInv=63.27, average=58.92
2019-02-15 05:10:04,428 : SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, CoordInv, average
2019-02-15 05:10:04,428 : 55.80,11.07,25.80,54.08,85.05,88.33,77.70,67.60,60.47,63.27,58.92
